{"title": "Enhancing Conditional Image Generation with Explainable Latent Space Manipulation", "authors": ["Kshitij Pathania"], "abstract": "In the realm of image synthesis, achieving fidelity to a reference image while adhering to conditional prompts remains a significant challenge. This paper proposes a novel approach that integrates a diffusion model with latent space manipulation and gradient-based selective attention mechanisms to address this issue. Leveraging Grad-SAM (Gradient-based Selective Attention Manipulation), we analyze the cross attention maps of the cross attention layers and gradients for the denoised latent vector, deriving importance scores of elements of denoised latent vector related to the subject of interest. Using this information, we create masks at specific timesteps during denoising to preserve subjects while seamlessly integrating the reference image features. This approach ensures the faithful formation of subjects based on conditional prompts, while concurrently refining the background for a more coherent composition. Our experiments on places365 dataset demonstrate promising results, with our proposed model achieving the lowest mean and median Frechet Inception Distance (FID) scores compared to baseline models, indicating superior fidelity preservation. Furthermore, our model exhibits competitive performance in aligning the generated images with provided textual descriptions, as evidenced by high CLIP scores. These results highlight the effectiveness of our approach in both fidelity preservation and textual context preservation, offering a significant advancement in text-to-image synthesis tasks.", "sections": [{"title": "Introduction", "content": "Image synthesis, particularly in the context of adhering to both reference images and conditional prompts, poses a significant challenge in contemporary research. While numerous approaches exist, achieving fidelity to reference images while integrating specified conditional prompts remains elusive. This paper presents a novel methodology aimed at addressing this challenge through the integration of a diffusion model with latent space manipulation and gradient-based selective attention mechanisms.\nIn recent years, diffusion models have emerged as powerful tools for image generation, allowing for the creation of high-quality images by iteratively denoising a latent representation. However, preserving the fidelity of a reference image while adhering to conditional prompts requires further refinement. State-of-the-art models like Stable Diffusion have demonstrated remarkable capabilities in generating images based on a conditional reference image. However, they face challenges when adhering to both the reference image and an additional text condition simultaneously. These models typically operate by controlling the level of noise added to the latent representation, with lesser noise injection required to retain properties from the reference image. This approach often leads to a compromise in faithfully rendering the subjects specified in the text condition, as the reduced number of denoising timesteps limits the model's ability to accurately generate these subjects.\nTo address this limitation, we propose the integration of Absolute Gradient-based Selective Attention Manipulation for images (Abs-Grad-SAM) motivated from Grad-SAM [1], a technique that analyzes cross-attention maps and gradients within the denoised latent space to generate importance scores. By leveraging Abs-Grad-SAM, we derive importance scores for elements of the denoised latent vector related to the subject of interest. Since we are dealing with images and the Grad-SAM [1] technique is originally designed for language models, here, instead of using ReLU, we utilize the absolute values of gradients. This adaptation is crucial as attention can impact pixel values both positively and negatively, requiring a more comprehensive consideration of gradient magnitudes.\nThis enables the creation of masks at specific timesteps during denoising, facilitating the preservation of subjects derived from the text condition while seamlessly integrating background elements from the reference image. Consequently, our approach ensures faithful adherence to both the text prompt and the reference image, overcoming the trade-off between subject integrity and background preservation faced by existing methods.\nIn this paper, we present preliminary findings. Through comprehensive experiments conducted on the Places 365 dataset [2], we demonstrate the superiority of our proposed method over baseline models. Specifically, our method yields improved Frechet Inception Distance (FID) scores and CLIP scores, indicative of higher-quality image synthesis. Furthermore, our approach generates images that are not only of"}, {"title": "Related Works", "content": "Denoising Diffusion Probabilistic Models: DDPMs [6] have emerged as a powerful framework for high-quality image synthesis. These models operate by gradually removing noise from an initial noisy latent representation, guided by a neural network trained to predict the noise distribution. [Ho et al., 2020] [6] introduced DDPMs and demonstrated their effectiveness in generating high-quality images on datasets like CIFAR-10 and LSUN. [Nichol and Dhariwal, 2021] [11] extended DDPMs to enable class-conditional image generation, while [Rombach et al., 2022] [13] proposed Latent Diffusion Models (LDMs) that apply diffusion models in the latent space of pre-trained autoencoders, enabling high-resolution synthesis with reduced computational requirements.\nText-to-Image Diffusion Models: Building upon the success of DDPMs, researchers have explored techniques to generate images conditioned on text prompts. [Rombach et al., 2022] [13] introduced cross-attention layers into the diffusion model architecture, enabling text-to-image synthesis. The Stable Diffusion model [Rombach et al., 2022] [13] demonstrated state-of-the-art performance in this domain. However, these models often struggle with faithfully rendering all aspects of the text prompt, leading to the phenomenon of \"catastrophic neglect\" [Chefer et al., 2023] [3]. To control the trade-off between sample quality and diversity, classifier guidance [Nichol and Dhariwal, 2021] [11] was introduced, which combines the score estimate of the diffusion model with the estimates of a separately trained image classifier. Alternatively, classifier-free guidance [Ho and Salimans, 2022] [7] jointly trains a conditional and an unconditional diffusion model, and combines their respective score estimates to achieve a similar trade-off without requiring a separate classifier. Nichol et al. (2022) [12] introduced GLIDE, a text-guided diffusion model for photorealistic image generation and editing. They compare two guidance strategies-CLIP guidance and classifier-free guidance-and find that the latter is preferred by human evaluators for both photorealism and caption similarity, often producing photorealistic samples. Additionally, they demonstrate the capability of their models for image inpainting, enabling powerful text-driven image editing.\nImproving Diffusion Model Performance: Several works have aimed to enhance the performance of diffusion models, particularly in the context of text-to-image generation. [Chefer et al., 2023] [3] proposed the Attend-and-Excite technique, which guides the model to attend to all subject tokens in the text prompt and strengthen their activations, improving the faithfulness of the generated images. [Hoogeboom and Salimans, 2022] [8] showed that blurring can be equivalently defined through a Gaussian diffusion process with non-isotropic noise, bridging the gap between inverse heat dissipation and denoising diffusion, and proposed Blurring Diffusion Models that offer the benefits of both standard Gaussian denoising diffusion and inverse heat dissipation. [Lee et al., 2022] [10] introduced a progressive deblurring approach for diffusion models, enabling coarse-to-fine image synthesis by diffusing and deblurring different frequency components of an image at different speeds.. [Ji et al., 2023] [9] proposed Self-Attention Control for Diffusion Models Training (SAT), leveraging attention maps to refine intermediate samples during training. [Ruiz et al. 2023] [15] presented DreamBooth, a method for fine-tuning text-to-image diffusion models to generate personalized images of subjects based on a few reference images. Saharia et al. (2022) [16]"}, {"title": "Methodology", "content": "In our approach, we extend upon the breakthroughs achieved by the state-of-the-art Stable Diffusion model (SD) introduced by Rombach et al. [13] in 2022. Unlike traditional image-based methods, SD operates within the latent space of an autoencoder architecture. Initially, an encoder (E) is trained to transform input images (x \u2208 X) into spatial latent vectors (z = E(x)). We then utilize a denoising diffusion probabilistic model (DDPM) [6] to first add noise to the latent vector through a forward diffusion process, and subsequently denoise the noisy latent vector through a reverse diffusion process over the learned latent space. This process generates a denoised version of the input latent vector z at each timestep and ultimately we get the final denoised latent vector 2d. Throughout the denoising procedure, the diffusion model is conditioned on an additional input condition. Following denoising, a decoder (D) reconstructs the input image, ensuring that the output of the decoder (D(2d)) closely resembles the desired image (x). Leveraging this latent space framework allows for efficient manipulation and enhancement of image representations, laying the foundation for our integration of importance scores based on Abs-Grad-SAM [1] and latent space manipulation techniques."}, {"title": "Mask Creation", "content": "The denoising diffusion probabilistic model (DDPM) utilizes a UNet architecture to predict noise at each timestamp. Conditional guidance is provided through the use of cross-attention [17] modules within the UNet. These cross-attention maps within the UNet has varying spatial dimensions and comprises a query, derived from the transformation of the input latent vector, and a key, derived from the input text embedding. Specifically, we define a nth cross attention map weights as $W_{A,t,n}$ as a tensor in $R^{H\\times(P\\times P)\\times N}$ where H is the number of attention heads, P is the spatial dimension of the latent vector, and N is the input text size.\nFor each subject within the input text processed by the cross-attention layer, we compute the Jacobian of gradients of the predicted noise 2\u2081 with respect to the corresponding attention weights. This calculation is mathematically represented as:\n$\\frac{\\partial z_t}{\\partial W_{A,t,n}} = \\frac{\\partial \\hat{z}_t(c_i, x_i, y_i)}{\\partial W_{A,t,n}(h_j, s_k, v_l)}$\nwhere c\u2081 indexes the channel, h\u2c7c represent the attention-head, (x\u1d62, y\u1d62) represent the spatial coordinates, s\u2096 indexes the token, and v\u2081 indexes the variable in the attention map weight vector.\nNow, based on the Jacobian calculated and the attention scores for a particular subject sm, we calculate the importance score of predicted noise elements corresponding to that subject based on the findings from Grad-SAM [1]. Mathematically, importance score $I_{score}$ is given by:\n$I_{score} (S_m, c_i, t, x_i, t, y_i, t) = W_{A,t} (s_m) \\odot Abs(G_{s_m})$\nwhere $G_{s_m} := \\frac{\\partial z_t}{\\partial W_{A,t} (s_m)}$ represents the gradients of the particular subject sm in the Jacobian of gradients, and \u2299 denotes the Hadamard product.\nBased on the calculated importance scores, we create a mask M(x, y) based on a dynamic threshold \u03b8 as described:\n$M_{s_m,n} (c, x, y) =\\begin{cases} 1, & \\text{if } I_{score} (S_m, c_i, x_i, y_i) \\geq \\theta, \\\\ 0, & \\text{otherwise.} \\end{cases}$\nAdditionally, to smooth these masks, we apply a Gaussian filter and morphological dilation on them. Similarly, we compute the importance score for all the subjects in the input text and generate a mask for each of them. Subsequently, we take union of these masks to create the corresponding mask for the nth cross-attention layer, which selectively masks all the relevant subjects in the latent vector. Mathematically, we denote this final mask as $M_n (x, y)$ is given by,\n$M_n (c, x, y) = \\bigcup_m M_{s_m,n} (c, x, y)$,\nFinally, we compute the intersection of these binary masks from all cross-attention layers to derive the ultimate mask $M_{final} (x, y)$, serving as the composite representation.:\n$M_{final} (c, x, y) = \\bigcap_n M_n (c, x, y)$"}, {"title": "Latent Space Manipulation", "content": "In latent space manipulation, we manipulate the final denoised latent vector $z_{denoised}$ for the particular timestamp t. We utilize the previously formed mask $M_{final}$ to retain the formation of subjects in the image while altering the background from the reference image by replacing the unmasked elements with the corresponding elements from the latent vector $z_{ref}$ obtained by denoising the reference image up to timestep t. Mathematically, the manipulated latent vector"}, {"title": "Mask Refinement and Smoothing", "content": "While the initial masks generated from the importance scores capture the desired regions, their boundaries often exhibit abrupt transitions, which can lead to visual artifacts in the synthesized images. To mitigate this issue and ensure smooth transitions between the preserved subjects and the integrated background elements, we apply two post-processing operations on the importance scores: Gaussian blurring and morphological dilation.\nMathematically, the Gaussian blurring operation can be expressed as:\n$G(x, y) = \\frac{1}{2 \\pi \\sigma^2} exp{-\\frac{x^2 + y^2}{2\\sigma^2}}$\n$I_{blurred}(x, y) = I(x, y) * G(x, y)$\nwhere I(x, y) represents the importance score, G(x, y) is the Gaussian kernel with standard deviation \u03c3, and * denotes the convolution operation. By applying Gaussian blurring to the generated importance score and use them for generating masks, we effectively smooth the boundaries, reducing abrupt transitions and promoting gradual changes between the preserved and integrated regions."}, {"title": "Algorithm 1 Latent Space Manipulation for Improved Context Adherence", "content": "Require: Input image x, input text condition, reference image xref\nEnsure: Manipulated image $x_{adhered}$\n1: z \u2190 E(x)\n2: z \u2190 DDPM Forward process(z, timesteps)\n\\t DDPM Reverse process\n3: for selected timestep t do\n4: $\\frac{\\partial z_t}{\\partial W_{A,t,n}}$ \u2190 ComputeJacobian ($\\hat{z}_t$, $W_{A,t,n}$)\n5: $I_{score}$ ($S_m, c_i, x_i, y_i$) \u2190 CalculateImportanceScores ($\\frac{\\partial z_t}{\\partial W_{A,t,n}}$, $W_{A,t, Sm}$)\n6: $I_s$ \u2190 SmoothScores ($I_{score}$)\n7: $M_{S_m, n}(c, x, y)$ \u2190 CreateMask($I_s (S_m, c_i, x_i, y_i), \\theta$)\n8: $M_n(c, x, y)$ \u2190 $\\bigcup_m M_{S_m, n}(c, x, y)$\n9: $M_{final}(c, x, y)$ \u2190 $\\bigcap_n M_n(c, x, y)$\n10: $\\hat{z}_{ref}$\u2190 DDPM(E(xref), t)\n11: $\\hat{z}_{manipulated} (c, x, y)$\n12: $\\quad$ $\\begin{cases} \\hat{z}_{denoised} (c, x, y), \\quad \\text{if } M_{final} (c, x, y) = 1\\\\ \\hat{z}_{ref} (c, x, y), \\quad \\text{if } M_{final} (c, x, y) = 0 \\end{cases}$\n13: $z_{t-1}$\u2190 $\\hat{z}_{manipulated}$\n14: end for\n15: $\\hat{z}_{denoised}$ \u2190 $z_0$\n15: $X_{adhered}$ \u2190 D($\\hat{z}_{denoised}$)"}, {"title": "Mask Refinement and Smoothing", "content": "where I(x, y) is the importance score, K(s, t) is the structuring element (e.g., a disk or square kernel), and S is the domain of the structuring element. By applying dilation to the the scores regions, we slightly expand their boundaries, ensuring a smoother transition between the preserved subjects and the integrated background.\nThe combination of Gaussian blurring and morphological dilation on the importance scores helps generated masks to mitigate visual artifacts and promote seamless integration of the preserved subjects with the background elements from the reference image. The specific parameters, such as the standard deviation for Gaussian blurring and the structuring element for dilation, are fine-tuned empirically to achieve the desired level of smoothness and boundary preservation."}, {"title": "Experiments and Results", "content": "In this section, we present the results of our experiments on text-to-image generation tasks. Initially, we employed the Stable Diffusion v1-5 model [14] as our baseline model, coupled with CLIP for text embeddings. To enhance the fidelity of generated images and ensure alignment with the provided"}, {"title": "Aggregated Results", "content": "Our model demonstrates notable performance in both fidelity preservation, as indicated by the Frechet Inception Distance (FID) scores, and textual"}, {"title": "Conclusion", "content": "In this study, we explored text-to-image generation tasks us-ing the Stable Diffusion model coupled with Abs-Grad-SAM technique for latent space manipulation. Through experi-mentation, we fine-tuned the baseline model and developed our model by leveraging the dynamic masking technique. We"}]}