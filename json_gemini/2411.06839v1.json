{"title": "LLM-Neo: Parameter Efficient Knowledge Distillation for Large Language Models", "authors": ["Runming Yang", "Taiqiang Wu", "Jiahao Wang", "Pengfei Hu", "Ngai Wong", "Yujiu Yang"], "abstract": "In this paper, we propose a novel LLM-Neo framework that efficiently transfers knowledge from a large language model (LLM) teacher to a compact student. Initially, we revisit the knowledge distillation (KD) and low-rank adaption (LoRA), and argue that they share the same paradigm. Inspired by this observation, we explore the strategy that combines LoRA and KD to enhance the efficiency of knowledge transfer. We first summarize some guidelines for this design and further develop the LLM-Neo. Experimental results on compressing Llama 2 and Llama 3 show that LLM-Neo outperforms various baselines. Further analysis demonstrates the robustness of the proposed LLM-Neo on variants of LoRA. The trained models have been available at this repository.", "sections": [{"title": "I. INTRODUCTION", "content": "Knowledge distillation (KD) [1] for large language models (LLMs) [2] is a predominant method for model compression. The key insight is to train a compact student model by mimicking the behaviors of the teacher model. One mainstreaming way is to align the logits [3], and thus transfer the knowledge from the teacher model to the student model.\nParameter-Efficient Fine-Tuning (PEFT) [4], [5] is another commonly used technique for LLM efficiency [6]. Among various PEFT methods, the Low-rank adapter (LoRA) [7] has gained increasing popularity since it does not introduce any additional parameters for inference. During training, LoRA updates a mergeable low-rank branch instead of updating the original full parameters. Therefore, LoRA can efficiently transfer the knowledge contained in the training examples to the trained models.\nIn this paper, we argue that KD and LoRA follow the same paradigm, i.e., aiming at transferring knowledge while the sources differ [8]. Moreover, LoRA transfers the knowledge efficiently via the low-rank branch, while KD methods update the full parameters and typically cost much more resources. We thus ask: can we combine KD and LoRA to improve the efficiency of knowledge transfer from the teacher model?\nTo this end, we propose a novel LLM-Neo framework which integrates LoRA into KD to achieve parameter-efficient knowledge distillation. Specifically, as shown in Fig. 1, we follow the idea of LoRA to introduce a low-rank branch in the student model, aiming to inherit the knowledge from the"}, {"title": "II. PRELIMINARY AND METHODOLOGY", "content": "In this section, we first mathematically analyze the knowledge distillation (KD) and low-rank adapter (LoRA). We then extend the KD process by applying the LoRA gradient to the KL divergence term, and thus propose the LLM-Neo framework."}, {"title": "A. Knowledge Distillation", "content": "In KD, the student model S learns from both the ground truth labels \\(Y_{dataset}\\) and the teacher model\u2019s predictions \\(T_{output}\\). For the same input, both S model and T model predict the next-token, producing probability distributions. The KD loss combines two parts: cross-entropy loss with the ground truth and Kullback-Leibler (KL) divergence to align the student\u2019s predictions with the teacher's:\n\\(L_{KD} = \\alpha \\cdot L_{CE}(S, Y_{dataset}) + (1 - \\alpha) \\cdot L_{KL}(S, T_{output}),\\) (1)\nwhere \\(\\alpha\\) is the loss weight. In this setup, the model parameters W are typically updated via the gradients:\n\\(W\\leftarrow W - \\eta \\cdot \\frac{\\partial L_{KD}}{\\partial W}\\) (2)"}, {"title": "B. Low-Rank Adaption", "content": "LORA introduces a low-rank branch to model the weight updates \\(\\Delta W\\). The update is formulated as:\n\\(W' = W + \\Delta W,\\) (3)\nwhere \\(\\Delta W = AB^T\\), with \\(A \\in \\mathbb{R}^{d_1 \\times r}\\) and \\(B \\in \\mathbb{R}^{d_2 \\times r}\\), and \\(r < d\\). Therefore, the updated parameters reduce from \\(d_1 \\times d_2\\) to \\((d_1+d_2) \\times r\\). In the Supervised Fine-Tuning (SFT) process, the model learns from the training samples by minimizing the cross-entropy loss \\(L_{SFT}\\), as defined below:\n\\(L_{SFT} = L_{CE}(S, Y_{dataset}).\\) (4)\nHence, we can update the low-rank branch \\(\\Delta W\\) via:\n\\(\\Delta W \\leftarrow \\Delta W - \\eta \\cdot \\frac{\\partial L_{SFT}}{\\partial \\Delta W}\\) (5)"}, {"title": "C. Extending LoRA Gradients to KL Loss", "content": "Comparing the definitions, we can conclude that KD and LORA follow the same paradigm transferring knowledge. In Equation 1, the first term corresponds to the supervised fine-tuning loss in Equation (4), where both update the model using knowledge derived from the ground truth labels. However, the second term in the KD loss, \\(L_{KL}(S, T_{output})\\), originates from the teacher model's outputs, representing a different source of knowledge. Therefore, it is feasible to extend the low-rank updating to the KL term as follows:\n\\(\\Delta W \\leftarrow \\Delta W - \\eta \\cdot \\frac{\\partial L_{KL}(S, T_{output})}{\\partial \\Delta W}\\) (6)\nThis ensures that both the cross-entropy loss and the KL divergence term are optimized using LoRA's low-rank parameters, while maintaining the efficiency of knowledge transfer through different knowledge sources."}, {"title": "LLM-Neo: Combining LoRA and KD", "content": "Finally, we unify the LoRA and the KD to form the LLM-Neo method. The final loss is a weighted combination of the cross-entropy loss from the dataset and the KL divergence between the student and teacher models. Specifically, the optimization process for LLM-Neo is:\n\\(L_{KD} = \\alpha \\cdot L_{CE}(S, Y_{dataset}) + (1 - \\alpha) \\cdot L_{KL}(S, T_{output}),\\) (7)\n\\(\\Delta W \\leftarrow \\Delta W - \\eta \\cdot (\\frac{\\partial L_{CE}(S, Y_{dataset})}{\\partial \\Delta W} + \\frac{\\partial L_{KL}(S, T_{output})}{\\partial \\Delta W}),\\) (8)\nIn LLM-Neo, both terms contribute to updating \\(\\Delta W\\), as expressed in equations (4) and (6). Therefore, LLM-Neo retains LoRA's parameter efficiency by applying low-rank updates across both supervised learning and knowledge distillation, combining the benefits of dataset learning and teacher model knowledge transfer."}, {"title": "III. EXPERIMENT", "content": "For the training data, we employ the BAAI Infinity-Instruct dataset [12] and randomly sample 1,000,000 samples as fine-tuning data, which include around 500M tokens. Due to limited resources, we further randomly sampled several smaller subsets of BAAI Infinity-Instruct, specifically 10,000 and 100,000 samples, corresponding to approximately 5M and 50M tokens, respectively."}, {"title": "IV. EXTENSIVE ANALYSIS", "content": ""}, {"title": "A. Strengthen with LoRA variants", "content": "We evaluate the robustness of LLM-Neo towards LoRA variants. Specifically, we adopt the latest variant of MoSLORA [11], which improves LoRA via mixing the subspaces. As shown in Fig. 3, LLM-Neo-MoSLORA gets better performance than vanilla LoRA consistently, which demonstrates the robustness of LLM-Neo."}, {"title": "B. Scaling law in LLM-Neo", "content": "Following the Llama3 report, we also scale the dataset and apply LLM-Neo to larger datasets progressively, including 100K, 200K, 500K, and 1M training samples. As shown in Fig. 4, the results indicate a consistent improvement in performance as the data size increased, suggesting that LLM-Neo effectively leverages larger datasets and efficiently transfers knowledge, which highlights its scalability and robustness when applied to extensive training tasks."}, {"title": "C. Applied on Minitron", "content": "We further perform distillation from Llama 3.1 to the Nvidia Minitron 4B [25], [26] using the 50M tokens. All the experiments are conducted on 6 A100 40G GPUs. Fig. 5 shows the results on several benchmarks. We can find that our proposed LLM-Neo performs well on the pruned Minitron 4B, demonstrating its robustness. Specifically, LLM-Neo gets an average score of 54.37, which is 0.52 higher than the base model. The trained weight has been available at this repository."}, {"title": "D. Compatibility with More Memory Optimizations", "content": "To test the robustness of LLM-Neo combined with other memory optimizations, we explore the compatibility with existing LLM optimization techniques when distilling the Minitron 4B. Specifically, we further conduct LLM-Neo with ZeRO-1 methods. As shown in Table II, we can find that LLM-Neo works well with ZeRO1 and ZeRO2, highlighting the relationship between different ZeRO levels and their impact on performance metrics such as memory consumption and time efficiency. Specifically, KD would be out-of-memory though we decrease the batch size to 1."}, {"title": "V. CONCLUSION", "content": "In this work, we propose a novel LLM-Neo framework, aiming to efficiently transfer knowledge from a large language model (LLM) teacher to a compact student. We first revisit the knowledge distillation (KD) and low-rank adaption (LoRA), and find that they share the same paradigm. Therefore, we explore the strategy combining LoRA and KD to enhance the efficiency of knowledge transfer. We first summarize some guidelines and further develop the LLM-Neo, i.e., the larger rank around 128 is more suitable. Experimental results on compressing Llama 2 and Llama 3.1 show that LLM-Neo outperforms various baselines. For future work, we would like to explore the performance of LLM-Neo on more training samples and more LoRA variants."}]}