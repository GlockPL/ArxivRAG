{"title": "SeFi-CD: A Semantic First Change Detection Paradigm That Can Detect Any Change You Want", "authors": ["Ling Zhao", "Zhenyang Huang", "Dongsheng Kuang", "Chengli Peng", "Jun Gan", "Haifeng Li"], "abstract": "The existing change detection(CD) methods can be summarized as the visual-first change detection (ViFi-CD) paradigm, which first extracts change features from visual differences and then assigns them specific semantic information. However, CD is essentially dependent on change regions of interest (CRoIs), meaning that the CD results are directly determined by the semantics changes of interest, making its primary image factor semantic of interest rather than visual. The ViFi-CD paradigm can only assign specific semantics of interest to specific change features extracted from visual differences, leading to the inevitable omission of potential CRoIs and the inability to adapt to different CRoI CD tasks. In other words, changes in other CRoIs cannot be detected by the ViFi-CD method without retraining the model or significantly modifying the method. This paper introduces a new CD paradigm, the semantic-first CD (SeFi-CD) paradigm. The core idea of SeFi-CD is to first perceive the dynamic semantics of interest and then visually search for change features related to the semantics. Based on the SeFi-CD paradigm, we designed Anything You Want Change Detection (AUWCD). Experiments on public datasets demonstrate that the AUWCD outperforms the current state-of-the-art CD methods, achieving an average F1 score 5.01% higher than that of these advanced supervised baselines on the SECOND dataset, with a maximum increase of 13.17%. The proposed SeFi-CD offers a novel CD perspective and approach.", "sections": [{"title": "I. INTRODUCTION", "content": "Change detection (CD) is an important remote sensing task and can be used in tasks such as disaster assessment [1], environmental monitoring [2], land management [3] and urban change analysis [4]. The main purpose of the CD task is to detect changes in a specific target; however, not all changes in images of different phases since any location can change over time. The target we focus on can change according to the needs of subsequent tasks. For example, in practical applications, when we wish to investigate changes in living space, we focus on changes in buildings; when we want to monitor changes in traffic, we focus on changes in vehicles. The regions we focus on can be called change regions of interest (CRoIs), while other targets can be called change regions of uninterest (CROUI). Since the change maps of different CRoIs differ, the result of CD is determined by the semantics of interest, and the nature of the interest is considered semantic information.\nTherefore, in this paper, we propose that the semantic of interest is the first image factor in CD tasks..\nIn addition to semantics, visualizations are also an important image factor in CD tasks. The current CD paradigm can be summarized as the visual first CD (ViFi-CD) paradigm, as shown in Fig.1(a), which first extracts specific change features from visual changes and subsequently gives the change features specific semantics in terms of fixation interests. There are two main problems with this paradigm, which we analyze in detail in Section 2. These two problems can be roughly described as follows:\n1) Unsupervised ViFi-CD methods tend to easily interpret visual differences as changes, but visual differences sometimes do not imply changes in the semantic content of interest.\n2) Deep learning-led supervised Learning ViFi-CD methods tend to result in models that can only extract specific visual features from specific CRoI and detect those specific CRoIs after training on them while ignoring other potential CRoIs. Without additional training, the model can not detect different CRoIs. However, conducting additional training on the model requires reannotating the training data, which is a very costly process.\nTo solve the above two problems of the ViFi-CD paradigm, the SeFi-CD paradigm is proposed. As Fig.1(b) shows, we can use any dynamic CRoI semantics to prompt the SeFi-CD model, informing the model about the changes we are interested in, allowing the model to understand our needs, and then actively searching for the corresponding change features. The paradigm can search for dynamic change features based on dynamic semantics and can be adapted to different CROI CD tasks.\nTo our knowledge, no CD method exists for the SeFi-CD paradigm. Therefore, we propose the first CD framework AUWCD for the SeFi-CD paradigm in this paper. Several change scenarios are detected in the CD task due to the diversity in task requirements. This framework can detect corresponding changes according to different CRoI semantic prompts. The proposal of the AUWCD is mainly motivated by the following three factors:\n1) Human describability of CRoI semantics. Different tasks require different CRoI semantics. First, we must be able to obtain an accurate representation of different CROI semantics. Language, as a commonly used means of communication for human beings, covers the linguistic expression of most features; therefore, language can be used as a carrier of different CROI semantics.\n2) The semantic-first principle. In recent years, \"prompt engineering\" has gradually replaced the previous paradigm of \"pretraining fine-tuning\" for downstream tasks [5], which allows the model to perform on large quantities of raw data. Pretraining is performed on the model, and by defining a new prompt function, the model can perform few-shot or even zero-shot learning and adapt to new scenarios with little or no labeled data. Therefore, we can use the CRoI concept described in language as a semantic prompt and prioritize prompts to guide the model to extract corresponding change features;\n3) The ability of the model to understand language. Since we choose human language to express CROI semantics, we also need a model that can understand the language-expressed CRoI semantics. Here, we introduce a large multimodal visual language model to solve this problem.\nIn summary, our contributions are as follows:\n\u2022\tWe summarize the current CD methods as a visual-first CD paradigm and provide a detailed analysis of the issues associated with this paradigm.\n\u2022\tIn response to the issues with ViFi-CD, we propose a new CD paradigm called the SeFi-CD paradigm. This paradigm dynamically locates regions of interest based on user-provided CRoI text prompts. Additionally, we design AUWCD, the first CD framework under the SeFi-CD paradigm.\n\u2022\tWe compare the AUWCD with current state-of-the-art CD methods on two public CD datasets. The experimental results show that when the CROI is modified according to task requirements without additional training, the AUWCD has strong flexibility and better detection accuracy. On the second dataset, the F1 score exceeds that of these advanced supervised baselines by an average of 5.01%, with the highest exceeding 13.17%.\nThe remainder of this article is organized as follows. Section 2 provides a detailed analysis of the issues present in the ViFi-CD paradigm. Section 3 reviews the related works. Section 4 describes the proposed method in detail. To evaluate our method, experiments are designed in Section 5. Finally, Section 6 summarizes this article and then discusses future research directions based on the experimental results."}, {"title": "II. COMPARATIVE ANALYSIS OF VIFI-CD AND SEFI-CD", "content": "In this section, we conduct a detailed analysis of the issues associated with the current ViFi-CD methods. We also compare ViFi-CD methods with SeFi-CD methods to provide a deeper understanding of SeFi-CD.\nIn the Introduction section, we discussed two crucial image factors in CD: semantic and visual. To comprehensively analyze the issues with ViFi-CD, we categorized the changes into four scenarios based on both the semantic and visual perspectives, as shown in Table I: (a) both semantic and visual change, (b) semantic change but no visual change, (c) no change in semantic but visual change, and (d) no semantic or visual changes. It should be noted that \"semantic\" here is the change in an interesting semantic because the change process usually involves a change in some, not all semantics. Moreover, in reality, there is no absolute \u201cunchange\u201d visually due to lighting, imaging, etc. Therefore, visual change and unchange can be understood as visually \"obvious differences\" and \"subtle differences\u201d, respectively. Cases (a), (b) and (c) are usually CRoIs in various CD tasks, and case (d) is a CRoUI in most cases.\nThe methods within the ViFi-CD paradigm can be categorized into unsupervised methods and deep learning-led Supervised Learning Methods. In the following sections, we conduct an issue analysis on these two types of methods separately and compare them with SeFi-CD."}, {"title": "A. Unsupervised CD Methods", "content": "Before the popularization of deep learning technology, unsupervised methods were the mainstream methods for CD tasks. To date, researchers have proposed various unsupervised CD methods, which can be roughly divided into algebra-based methods and transform-based methods [6]. The algebra-based methods obtain the difference map by applying mathematical operations to each image pixel and then obtain the final change map through operations [6]\u2013[12]. The transform-based CD method first transforms the image pixels and then processes the transformed feature map to obtain the change map [13]-[23].\nThese methods focus on automatically recognizing visual differences between images through algorithms to discover change regions and then identifying regions that change the human judgment of semantic information [6]. This process usually involves comparing two temporal images and finding visual change features (e.g., brightness, color, and texture) [9]. This method is effective when it involves obvious visual changes, such as the addition or destruction of buildings. However, these methods suffer from two critical issues:\n\u2022\tMissing regions with minor visual differences but significant semantic disparities. As Table I(b) shows, this region exhibits minor visual differences, appearing as gray-white areas, but undergoes a semantic change from \u201cRoad\u201d to \u201cSnow Field\u201d. Unsupervised methods typically rely only on visual differences for determination, often classifying such regions as unchanged, which can lead to missed changes.\n\u2022\tFalse positives for CRoUI. As Table I(c) shows, this region exhibits significant visual differences; however, when the semantic interest is \u201cLand\u201d, no change exists in this area. However, unsupervised methods may incorrectly identify these obvious visual differences in the region as changes, leading to false positives in CD.\nFor the first issue, SeFi-CD prioritizes semantic changes based on the provided semantic prompt, enabling the identification of change features in the image that are relevant to the specified semantic interest. For the second issue, SeFi-CD focuses only on change features relevant to the specified semantic interest, disregarding changes that are irrelevant to the specified semantics."}, {"title": "B. Deep Learning-Led Supervised Learning Methods", "content": "With its powerful representation capabilities, deep learning technology has become the mainstream method for current CD tasks and has achieved surprising results. Deep learning algorithms, which aim to hierarchically learn representative and discriminative features from datasets, have received widespread attention from the global earth science and remote sensing communities. Researchers have recently proposed various deep network models for CD tasks. These deep networks include convolutional neural networks (CNNs), transformers, generative adversarial networks (GANs), and recurrent neural networks (RNNs) [24]. The CNN and transformer structures are the main model structures in this type of CD method. A CNN is a special form of neural network used to process data with a known grid-like representation, such as image data, which can be thought of as a two-dimensional grid of pixels [25], [25]\u2013[34]. The transformer [35] was first introduced in 2017 and was originally used for sequence-to-sequence learning. Transformers can easily provide long-range dependency modeling; therefore, they have been widely used in natural language processing (NLP) and are starting to show promise in computer vision and CD. [25], [36]-[40].\nThis type of method detects change regions by training CD models using labeled datasets [41], which can somewhat overcome the shortcomings of unsupervised methods, as they can learn complex patterns of specific change types [42]. However, the effectiveness of this method is limited by the type and number of labels in the dataset.\nMoreover, As Table I shows, a more severe issue with these methods is that once trained on specific CRoIs, the model can only extract visual features specific to those specific CRoIs and can not detect other CRoIs, leading to missed detections of other CRoIs. For example, if we train a deep learning model for road CD using a road CD dataset, the model becomes ineffective when tasked with building CD, resulting in significant missed detections of building changes.\nAlthough this problem can be solved by retraining the model by relabelling the data, labeling the training samples is quite time-consuming and laborious [41]. Later, researchers gradually recognized the importance of semantics in CD tasks. For example, Dong et al. proposed ChangeCLIP [43], the first work to apply a multimodal vision-language approach to CD tasks. ChangeCLIP introduces rich semantic information using pre-trained CLIP [44], providing semantic guidance for model training and enhancing the performance compared to using a single visual modality alone. However, this method still starts with visual features and the model cannot dynamically search for change features related to the user's specified semantic interests. Therefore, this method still faces the aforementioned issues.\nAs illustrated in Figure 1, SeFi-CD can dynamically accept user-provided CRoI prompts, enabling it to comprehend diverse task requirements and actively search for corresponding change features. This approach resolves the issues associated with the deep learning-based supervised methods prevalent in ViFi-CD."}, {"title": "III. RELATED WORKS", "content": "In this section, to better explain the contents of VLMS and FSMs in the AUWCD, we summarize the development of vison-language pretrained models and foundation segment models. The details are as follows:"}, {"title": "A. Vision-Language Pretrained Models", "content": "In recent years, with the continued interest in the multimodal field and the rapid development of self-supervised learning, a series of pretrained visual language models represented by CLIP [44] and ALIGN [45] have been created. These models usually consist of modality-specific (image and text) encoders capable of generating embeddings for various modalities. These models are usually pretrained on billion-scale image and text datasets, and contrastive learning methods are used to maximize the alignment of image-text positive sample pairs by randomly sampling image-text pairs. A direct application of this type of model is zero-shot image-text retrieval or zero-shot classification through text prompts [44], which explore shared or mixed architectures between image and text modalities and implement additional features such as zero-shot visual question answering (VQA) and subtitles. There are also several methods, such as locked image tuning(LiT) (Zhai et al., 2022) [46], aligning pretrained encoders(APE) (Rosenfeld et al., 2022) [47], and bootstrapping language-image pretraining(BLIP-2) (Li et al., 2023a) [48], that uses a pretrained single-modal model to reduce the training cost of CLIP-like models [49].\nWhile large pretrained visual language models have shown amazing capabilities, researchers have also conducted in-depth discussions on the interpretability of such models. CLIP surgery [50] refers to the concept of MaskCLIP [51] in open vocabulary semantic segmentation. This task requires neither extra supervision nor additional training. In contrast to MaskCLIP, CLIP surgery aims to improve the interpretability of CLIP [44], while the goal of MaskCLIP is to extract dense predictions for segmentation; CLIP surgery does not delete self-attention in MaskCLIP but rather proposes v-v self-attention. The original attention module is transformed; in addition, CLIP surgery introduces dual paths to merge multiple v-v self-attention mechanisms instead of only modifying the last layer, as in MaskCLIP. CLIP surgery is an interpretable method designed for CLIP models. It uses text prompts to generate accurate explanation maps, which can highlight areas that are similar to text concepts."}, {"title": "B. Foundation Segment Models", "content": "The segment anything model(SAM) [52], proposed by Meta in 2023, has made significant progress in breaking segmentation boundaries and greatly promoted the development of basic computer vision models. SAM also refers to using large model prompts for powerful zero-shot learning and can generate high-quality segmentation masks based on various visual segmentation prompts.\nSince then, many scholars have conducted research to explore the capabilities of SAM and apply it to various tasks [53]\u2013[55]. In addition, some scholars are working on various visual prompts to improve the versatility of SAM. [53], [56] introduced more diverse prompts than SAM does and proposed a more general segmentation system, SEEM, which includes visual prompts (points, boxes, graffiti, masks) and text and quote hints (reference areas of another image). As the authors claimed, the unified prompting scheme introduced in the SEEM [56] can encode different prompts into a joint visual-semantic space to produce strong zero-shot generalization capabilities. These basic visual models can generate high-quality segmentation masks. Here, we regard these foundation visual models, which are designed for semantic segmentation tasks, as the foundation segment models (FSMs)."}, {"title": "IV. METHOD", "content": "In this section, to better understand the working details of the AUWCD, we provide a detailed introduction to the Semantic Align Module, the CRoI Segment Module, and the CD Module, along with the correlations between these three modules.\nFig.2 shows the overall framework structure of the AUWCD. The details are as follows:"}, {"title": "A. Semantic Align Module", "content": "The main function of this module is to align the feature space of the multitemporal image with the semantic space of the CROI text prompt. Visual prompts are generated for areas with a higher degree of alignment based on the alignment degree between the image feature space and the CROI semantic space. The CROI segment module can be utilized to segment the CRoIs in images. VLMs are the core component of this module and usually consist of an image encoder for storing rich knowledge and a text encoder for selecting the CRoI we intend to use. Specifically, the image encoder stores rich knowledge about more possible CRoIs and has a strong ability to distinguish their features clearly. In other words, it can generate similar representations for CRoIs of the same type and different representations for CRoIs of different types. We used CLIP Surgery [50] to generate point prompts as an example to explain this module in detail.\nThe overall process of this module is shown in Fig.3. We describe the four processes of image encoding, text encoding, similarity map generation, and point prompt generation based on similarity maps:\na) Image Encoding: The dual-temporal images $I_{t1}$ and $I_{t2}$ obtain the text encodings $l_1$ and $l_2$ of the two images through the image encoder. The two image encoders used here are from CLIP Surgery, and their weights are shared.\nb) Text Encoding: Both CRoI and CRoUI are represented by text. We can specify CRoUI using textual descriptions to suppress the CRoUI recognition. For example, in a building CD task, we can select textual descriptions such as \"shadows\", \"rivers\", and \"trees\" as CRoUIs to inhibit the model's attention to these CROUIs and enhance CRoI recognition. In cases where CROUI is not explicitly specified, empty text (\" \") can be used. We constructed text prompts for two types of text according to prompt templates and obtained text encodings $T_1$ and $T_2$ through the CLIP surgery's text encoder.\nc) Generate Similarity Map: Because $T_1$ is the text encoding of CRoI and $T_2$ is the CRoUI text encoding, we use formulas (1) and (2) to calculate the similarity vector $Sim1$ between $I_1$ and $T_1$ and the similarity vector $Sim2$ between $I_2$ and $T_2$.\n$Sim1 = Norm[I_1(T_1 - T_2)^T]$\t(1)\n$Sim2 = Norm[I_2(T_1 - T_2)^T]$\t(2)\nIn the above formula, $Norm()$ is the normalization function, and its formula is:\n$Norm(x) = \\frac{x - min(x)}{\u0442\u0430\u0445(x) \u2013 min(x)}$\t(3)\nThe image encoder is usually ViT [57]; thus, $I_1$ and $I_2 \u2208 R^{1\u00d7num\\_token\u00d7token\\_dim}$, $T_1$ and $T_2 \u2208 R^{1\u00d7token\\_dim}$ num_token is the number of tokens in ViT and can be calculated by the formula(4). In addition, token_dim is the dimension of each token.\n$num\\_token = (\\frac{EIS}{PS})^2 + 1$\t(4)\nIn the above equation, EIS is the image size of the input image encoder, and PS is the size of each patch that divides the image into VIT during image preprocessing. The similarity vectors $Sim1$ and $Sim2 \u2208 R^{\\frac{EIS}{PS} \u00d7 \\frac{EIS}{PS}\u00d71}$ are calculated to represent the similarity between each token and the CROI. Finally, the similarity vector Sim is reshaped into a two-dimensional shape to obtain a similarity map (SimM). However, the size of $SimM(SimMS)$ obtained is $(\\frac{EIS}{PS} \u00d7 \\frac{EIS}{PS})$; therefore we use the bilinear interpolation method to obtain the similarity map of the original image size.\nd) Generate Points Prompt based on Similarity Map: We use the threshold segmentation method to select positive/negative prompt points in the downsampled similarity map (in Fig.3, the red points are positive prompt points and the blue points are negative prompt points). The specific steps are as follows.\nFirst, determine the number of positive and negative prompt points num_p and num_n according to following formula(5):\n$num\\_p = mins\\{sum(SimM > t), \\frac{SimMS}{2}\\}$\t(5)\nIn the formula(5), $t$ is the threshold and a hyperparameter. $sum(SimM > t)$ represents the number in the downsampled similarity map that is greater than the threshold t. Additionally, we select the same number of negative prompt points, that is,\n$num\\_p = num\\_n$\t(6)\nAfter obtaining $num_p$ and $num_n$, we select the largest $num_p$ points as positive prompt points and the smallest $num_n$ points as negative prompt points in $SimM$. However, the coordinates of the points selected in this way are the coordinates in SimM, not the coordinates in the original image. Finally, the formulas (7) and (8) are used to convert the coordinates of the prompt points in SimM to the coordinates in the original image:\n$Xo = \\frac{H}{\\sqrt{SimMS}}X$\t(7)\n$Yo = \\frac{W}{\\sqrt{SimMS}}Y$\t(8)\nIn the above formulas, $X$ and $Y$ are the coordinates of the prompt point in $Xo$ and $Yo$ denotes the coordinates of the original image, and H and W are the height and width of the original image, respectively."}, {"title": "B. CRoI Segment Module", "content": "The core component of this module is FSMs, whose main function is to receive the visual prompt of multitemporal images generated by the \"Semantic Align Module\u201d and segment the CRoI masks in the multitemporal images through the foundation segmentation model of shared weights. As mentioned earlier, classic FSMs, such as the segment anything model (SAM) [52], Although the segmentation results of FSMs represented by the SAM do not carry semantic information, it is provided by the Semantic Align Module for guidance; therefore, when the Semantic Align Module is given a visual prompt from a CROI, an extraction boundary about the CRoI can be generated. Moreover, the SAM can further correct boundaries by providing some visual prompts from the CRoUI as negative samples. Through the CRoI Segment Module, we can obtain more accurate segmentation results for the CRoI text description; therefore, we use SAM as the FSM of this module to introduce this module in detail.\nThe SAM is an interactive segmentation framework (as shown in Fig.4) that can segment the corresponding target mask based on given prompts, such as prompt points, bounding boxes or masks. The SAM comprises three main components: an image encoder($\\phi_{ien}$), a prompt encoder($\\phi_{pen}$), and a lightweight mask decoder($\\phi_{mde}$). The SAM uses the masked autoencoder (MAE) pretraining method based on a vision transformer (ViT) [57] to extract image features and encode the previous prompts into embedding tokens. Subsequently, the cross-attention mechanism in the mask decoder can promote interactions between image features and prompt embedding and ultimately generate a mask that the prompt desires. The process is shown in Fig.4 and can be expressed as follows:\n$F_{img} = \\phi_{ien}(I)$\t(9)\n$T_{pmt} = \\phi_{pen}(\\{p\\})$\t(10)\n$M = \\phi_{mde}(F_{img} + Fc\\_mask, [T_{out}, T_{pmt}])$\t(11)\nwhere $I \u2208 R^{H\u00d7W\u00d73}$ represents the original image, $F_{img} \u2208 R^{h\u00d7w\u00d7c}$ represents the image features encoded by the S\u0102M image encoder, and $p$ contains sparse prompts and can contain positive/negative sample points(also in Fig.4, the red points are positive prompt points and the blue points are negative prompt points), bounding boxes or masks. $T_{pmt} \u2208 R^{k\u00d7c}$ represents the sparse prompt token encoded by the S\u0102M prompt encoder. In addition, $Fc\\_mask \u2208 R^{h\u00d7w\u00d7c}$ is the representation of the coarse mask, which is an optional input of the SAM. $T_{out} \u2208 R^{5\u00d7c}$ consists of preinserted learnable tokens representing four different masks and their corresponding IoU predictions. Finally, M is the corresponding prediction mask. With respect to the AUWCD, only one output is needed; thus, we choose the one with the highest prediction score as the final prediction result."}, {"title": "C. CD(CD) Module", "content": "The main function of this module is to perform the change process on the CRoI mask in the multitemporal image and finally obtain the CRoI change map. We assume that we obtain the mask of multiple temporal images CRol ${Mt1, Mt2,..., Mtn}$ in a certain area through the foundation segment module. The change map obtained by the change process for any two phases can be expressed by the formula(12):\n$ChangeMap_{(t_i,t_j)} = (M_{t_i} \u222a M_{t_j}) \\ (M_{t_i} \u2229 M_{t_j})$\t(12)\nIn the formula, $ChangeMap_{(ti,tj)}$ represents the change map of the image at time tj in some area relative to the image at time ti in the same area. \\ is the difference set symbol, and A\\B represents the difference set of A to B.\nNotably, the VLM and VSM include CLIP surgery and SAM. We can also choose different change process methods to obtain a higher-quality change map."}, {"title": "V. EXPERIMENTS", "content": "Experimental validations were conducted using the instantiated framework described above to assess the effectiveness of the AUWCD framework. We first outline the experimental setup, including the dataset, evaluation metrics, and implementation details, followed by presenting the experimental results. Finally, a brief summary and analysis of the experimental outcomes are provided."}, {"title": "A. Databases", "content": "1) BCDD Dataset: The BCDD dataset [58] (as shown in Fig.5) encompasses the region affected by a 6.3 magnitude earthquake in February 2011 in Christchurch, New Zealand. This dataset comprises two image scenes captured at the same location in 2012 and 2016, accompanied by semantic labels and CD labels for buildings. To manage the large image size of 32,507 \u00d7 15,354 pixels, we divided each image into non-overlapping 256 \u00d7 256 pixels image pairs, resulting in a total of 7,434 pairs.\n2) Second Dataset: The SECOND dataset [59] is a semantic CD dataset (as illustrated in Fig.6). It consists of 4,662 pairs of aerial images collected from various platforms and sensors covering cities such as Hangzhou, Chengdu, and Shanghai. Each image in the dataset has a size of 512 \u00d7 512 pixels and is annotated at the pixel level. The dataset focuses on six land cover categories, namely, non-vegetation, trees, low vegetation, water, buildings, and playgrounds. We established a baseline CROI based on the land cover categories in the first temporal phase. Using these six CRols, the dataset was divided into six CRoI categories. Subsequently, the images were utilized to create the training set, validation set, and test set in an 8:1:1 ratio."}, {"title": "B. Metrics", "content": "To quantitatively validate the effectiveness of the proposed method, we employed four common accuracy metrics commonly used in CD tasks [30]: precision, recall, F1 score (F1), and overall accuracy (OA). The formulas for these metrics are as follows:\n$Precision = \\frac{TP}{TP + FP}$\t(13)\n$Recall = \\frac{TP}{TP + FN}$\t(14)\n$F1 = \\frac{2TP \u00b7 Recall}{Precision + Recall}$\t(15)\n$OA = \\frac{TP + TN}{TP + TN + FP + FN}$\t(16)\nwhere TP is the number of true positives, FP is the number of false positives, TN is the number of true negatives, and FN is the number of false negatives.\nIn addition, this paper utilizes the point accuracy (PAcc) metric to quantify the effectiveness of the point prompt generated by the \"Semantic Align Module\u201d. The computation formula for the PAcc is as follows:\n$PAcc = \\frac{Pc + Nc}{P+N}$\t(17)\nwhere P is the total number of positive points generated by CLIP surgery and N is the total number of negative points generated by CLIP surgery, Pc is the number of positive points whose corresponding label is 1 (CROI), Ne is the number of negative points whose corresponding label is 0 (CROUI)."}, {"title": "C. Comparative Experiment", "content": "The purpose of this comparative experiment was to quantitatively compare the effects of the AUWCD framework and advanced methods in the ViFi-CD paradigm on different CROI CD tasks without major modifications to the model or method, the SeFi-CD paradigm can better adapt to complex and changeable CRoI CD than the ViFi-CD paradigm. The experimental details and results are as follows.\n1) Experimental Design Ideas: The idea for the design of this experiment is that in a multicategory CD dataset, various categories represent CRoIs for different tasks. We selected several major supervised baselines dominated by supervised learning in the ViFi-CD paradigm as comparison objects.\nFor these supervised baselines, we provide training samples for only a certain category in the dataset and do not provide training samples for the other categories. Thus, the baseline obtained after training can be regarded as a model dedicated to a specific CROI CD task and other categories can be regarded as different CD tasks focused on CRoIs.\nFinally, we use the trained baselines to perform CD tasks on all CRoIs categories and use the above indicators to quantify the detection effect; for AUWCD, we input only the text of each category in the dataset into the model. To prompt the model to complete different categories of CD tasks, the above indicators are used to quantify the effect of CD; finally, the effects of the AUWCD and these supervised baselines are compared by comparing indicator values.\n2) Implementation Details: For multicategory CD datasets, we choose the SECOND dataset, which contains six classes for semantic CD, as the experimental dataset to verify the effect of the AUWCD framework. We compare the performance of the AUWCD with that of advanced CD methods on the ViFi-CD dataset.\nFor the methods in ViFi-CD, we chose several advanced supervised baselines from the past five years as objects of comparison: FC-EF [27], FC-Siam-conc [27], FC-Siam-diff [27], DASNet [30], BIT [37], SwinSUNet [40], ICIFNet [60], DMINet [61], ELGCNet [62], ChangeBind [63].\nAll supervised baselines were implemented using the Py-Torch backend and were supported by NVIDIA RTX A6000 GPUs. We used the Adam optimizer with an initial learning rate of 0.00001 for all the models and set the batch size to 8.\nFor the AUWCD framework, we needed to change only the CROI text prompt to directly test each of the six CROIS separately. The various hyperparameters of the point prompt generation process in CLIP surgery used the best hyperparameters obtained from subsequent ablation experiments and used the largest and best-performing model, CSVIT-L-14. The FSM module also utilizes the sam_vit_h model. Both CSVIT-L-14 and sam_vit_h use pretrained model parameters. The text prompts for each CRoI category were set as follows: \"low vegetation land\", \"tree\", \"building roofs\", \"waters\", \"non-vegetation land\" and \"football field\u201d. These datasets corresponded to the original low vegetation, tree, building, water, n.v.g. surface, and playground categories, respectively.\nTo eliminate chance, we conducted three independent sets of comparative experiments, and the CRoI categories that provided training samples were divided into \"tree\", \"water\", and \"playground\". Moreover, to demonstrate our effects quantitatively, we chose the F1 score, which is the most important indicator for CD tasks, as a quantitative indicator.\n3) Result:\na) \"Tree\" was used as the training category to provide training samples: The experimental results for the other categories that did not provide training samples are shown in Table II.\nb) \"Water\" was used as the training category to provide training samples: The experimental results for the other categories that did not provide training samples are shown in Table III.\nc) \"Playground\" was used as the training category to provide training samples: The experimental results of other categories that did not provide training samples are shown in Table IV.\nThe experimental results show that the performance of the AUWCD can exceed that of several CD supervision baselines compared with the CRoI categories that provide training samples for each supervision baseline; however, a certain gap remains compared with the current, more advanced supervision baselines. Compared with the performance that is not provided for the training sample categories, the performance of the AUWCD exceeds that of these supervised baselines by simply modifying the CRoI prompt text without additional training. The F1 score exceeds these advanced supervised baselines by an average of 5.01 percentage points, and the highest exceeds it by 13.17 percentage points."}, {"title": "D. Visualization Experiment", "content": "A visual experiment of the AUWCD is performed in this section. The main purpose is to visually demonstrate the process and effect of CD via the AUWCD. The experimental details and results are as follows:\n1) Implementation Details:\na) Visualization effect of the AUWCD on a single CRoI CD task (GT is used for comparison, but the category is single): We display the visual results of building CD on the BCDD dataset to show the intermediate process and effects of AUWCD CD and the final CD results. Consistent with the parameter settings obtained after the subsequent ablation experiment, VLM selected CSVIT-L-14, the threshold t was set to 0.65, and the CRoI prompt word was set to \"iron building house roofs\u201d. For FSM, we also selected sam_vit_h. Both CSVIT-L-14 and sam_vit_h also used pretrained model parameters.\nb) Visualization effect of the AUWCD on multiple CROI CD tasks (no information for comparison but rather multiple categories): In the BCDD dataset, we demonstrate the CD effect of different CRoIs on the same pair of dual-temporal images. In the experiment, we selected several ground objects for display. The selected ground objects and corresponding prompt words are shown in Table V:"}, {"title": "E. Ablation Experiment", "content": "This ablation experiment investigates the impact of VLMs in the semantics align module on the overall performance of the AUWCD. The study explored the following four aspects:\na) Effect of CRoI Text Prompts: This experiment investigated the influence of different prompts on the accuracy of CROI descriptions. Since different prompts can lead to varying levels of precision in describing CRoIs, the objective is to identify prompts that provide more accurate descriptions of the target.\nb) Effect of CRoUI Text Prompts: In the Semantic Align Module, we can design a negative text prompt to suppress the model's recognition of CRoUI. The purpose of this experiment is to demonstrate that negative text prompts for CRoUIs can enhance CRoI detection quality by inhibiting CRoUI recognition.\nc) Effect of Thresholds: As thresholds directly affect the number and prompt point locations, determining the most suitable threshold values is essential. The experiment aims to find optimal thresholds that maximize the CD process performance.\nd) Impact of VLM Model Performance: This analysis aims to demonstrate how VLM performance in the Semantic Align Module affects overall AUWCD performance. The experimental details and results for each of the above aspects are presented below:"}, {"title": "1) Implementation Details:", "content": "We utilize the BCDD dataset as the data source for this experiment. Since the AUWCD does not require training and can directly perform CD tasks, we tested it on the entire BCDD dataset, which contains 7,434 pairs of images. The implementation details for the three aforementioned experiments are as follows:\na) Effect of CRoI text prompts: Considering that the BCDD dataset focuses on CD for buildings, we selected seven text prompts to describe the CRoI as follows: 'buildings', 'houses', 'roofs', 'building roofs', 'house roofs', 'building house roofs', and 'iron building house roofs'. Additionally, to demonstrate that only relevant text prompts are effective, we added other text descriptions unrelated to buildings such as \"tree\", \"road\", and \"river\". To generate point prompts using CLIP surgery, we utilized the largest model size, CSVIT-L-14, with a threshold set to 0.80. For the FSM module, we employed the SAM official demo, which also uses the largest model size, sam_vit_h. Similarly, both CSVIT-L-14 and sam_vit_h also used pretrained model parameters. We conducted a quantitative analysis using five evaluation metrics to assess the impact of different text prompts on the final CD results. Furthermore, since the BCDD dataset contains annotations for buildings in both time frames, we demonstrated the influence of different text pages on building segmentation in dual temporal images. This presentation adds further persuasiveness to the results.\nb) Effect of CROUI Text Prompts: Since this is a building CD task, CRoI refers to buildings. Therefore, we set the CROI text prompt to \"iron building house roofs\". In building CD, common pseudo changes include shadows, roads and so on. Here, we selected common pseudo changes as the CROUI textual prompts, which are \"road\", \"shadow\", and \"tree\". Additionally, to explore the combined effect of multiple CROUIs, we set \"road, shadow, tree\" as the text prompt for multiple CROUI combinations. We compared these CRoUI text prompts above with an empty textual prompt(\" \") to demonstrate the effectiveness of this method. The VLM in the Semantic Align module is CSVIT-L-14, with a threshold set to 0.80. The FSM in the CROI Segment Module is sam_vit_h. Both of them used pretrained model weights.\nc) Effect of thresholds: Based on the results obtained from the previous experiment, where we identified the optimal text prompt for building CD, we explored the impact of varying threshold values on CD and CRoI segmentation. We investigated threshold values in the range of 0.5 to 0.95 with a step size of 0.05 for their effect on performance. Again, we used CLIP surgery with the model size CSVIT-L-14 and the FSM module with the sam_vit_h model while using the best-performing text prompt from the first experiment. CSVIT-L-14 and sam_vit_h used pretrained model weights.\nd) Impact of VLM performance: This experiment explored the impact of VLM performance on the overall results. Larger model sizes have been shown to improve performance [44], [64]; thus, we chose three different CLIP surgery model sizes to represent VLMs with varying capabilities. The selected model sizes, from smallest to largest, were CSVIT-B-32, CSVIT-B-16, and CSVIT-L-14. All CLIP surgery models used pretrained model weights. The text prompt remained consistent, using the best-performing prompt obtained from the previous experiment. The threshold was set to the optimal value identified in the threshold experiment. By conducting these three experiments, we obtained comprehensive insights into the effects of text prompts, thresholds, and VLM model sizes on the performance of the AUWCD using the BCDD dataset. The quantitative analysis, combined with the building segmentation results, provided a more convincing evaluation of the proposed approach."}, {"title": "2) Result:", "content": "a) Ablation experimental results of CRoI text prompts: The building CD indicators for the different CRoI text prompts in the BCDD dataset are shown in TableVII.\nThe building extraction indicators of the different CROI text prompts in the BCDD dataset are shown in TableVIII.\nAccording to the experimental results in the table above, the text prompt was \"iron building house roofs\". Regardless of the building CD task or the building extraction task, the five evaluation indicators were better than other text prompts."}, {"title": "therefore, these roofs were used as a text prompt for later ablation experiments.", "content": "In addition, the experimental results obtained by different text prompts have considerable differences in terms of the indicators. In the building CD and extraction tasks, using text descriptions unrelated to buildings leads to a significant decline in the model's detection performance, which further demonstrates the effectiveness of our Semantic Align Module. Additionally, among the various textual descriptions for buildings, the maximum differences in F1 scores can reach 3.44 and 8.37 percentage points, respectively. In fact, there are so many text prompts that the prompt \"iron building house roofs\u201d may not be the most effective text prompt. Considerable research has shown that the zero-shot ability of large models largely depends on the quality of the prompts. This article provides prompt words according to the categories corresponding to the CROI.\nTherefore, providing higher-quality and more stable CROI semantic prompts for VLMs can also be an important direction for optimizing the AUWCD in the future.\nb) Ablation experimental results of CRoUI text prompts: The building CD indicators of the different CROUI text prompts in the BCDD dataset are shown in TableIX. The Experimental results show that using common pseudo-change text prompts in building CD tasks as CRoUI leads to improvements across four metrics compared to using empty text prompt(\" \"). Therefore, this method indeed enhances the overall detection performance of the model by suppressing the recognition of CROUI. Additionally, combining multiple CROUI text prompts demonstrated better performance than using a single CROUI. This indicates that this method can better optimize the model's detection performance by combining multiple CRoUIs.\nc) Ablation experimental results of thresholds: From the text prompt ablation experiment in the previous section, we obtained the prompt \"iron building house roofs\u201d, which better describes the building. In the threshold ablation experiment in this section, we use this as a fixed prompt word.\nThe evaluation results of different thresholds for building CD in the BCDD dataset are shown in Figure 8.\nThe evaluation results of different thresholds for building extraction in the BCDD dataset are shown in Figure 9."}, {"title": "changes. For example, as the threshold gradually increases, the recall shows an obvious negative correlation.", "content": "This is because, under a high similarity selection threshold, ignoring areas in the image that are less similar to CRoI text descriptions is easy, resulting in missed CRoI detections. The PAcc shows an obvious positive correlation with the threshold. The number of visual prompt points under high similarity conditions can be accurately increased, indicating that the model is more focused on real CRoIs. Moreover, the change patterns of the F1 score and precision index show a parabolic pattern that first increases and then decreases. However, there is great uncertainty in finding the parabolic vertex threshold, which requires more thorough research and more appropriate methods.\nIn addition, the CD error is significantly larger than the extraction error for the building CD and building extraction evaluation indicators. We performed the following analysis on this phenomenon. For the building CD and building extraction indicators under different thresholds, we used $F1_{CD}$ and $F1_{seg}$ to represent the F1 scores of building CD and extraction, respectively, and $1 - F1_{CD}$ and $1 \u2013 F1_{seg}$ to represent their errors, respectively. The error ratio can be calculated using the following formula (18):\n$Error Ratio = \\frac{1 - F1_{CD}}{1 - F1_{Seg}}$\t(18)\nThis error ratio curves under different thresholds are shown in Fig.10:\nFig.10 shows that the error ratio of the CD score to the extracted F1 score of the building remains approximately \u221a2. This error accumulation occurs because the method for obtaining the change map in the experiment (i.e., the change process method in the CD module) involves comparing two masks (operations such as union, intersection, and complement). Therefore, the error of any mask may have an impact on the final result. In statistics and error analysis, when independent error sources are combined, the square of the total error is the sum of the squares of the independent errors, that is:\n$M^2 = m_1^2 + m_2^2 + ... + m_n^2 = \\sum_{i=1}^n m_i^2$\t(19)\nwhere $M$ represents the independent errors that constitute the total error. The CRoI extraction mask errors of the two phases are assumed to be independently and identically distributed, and the size of each mask error is assumed to be $m$. Therefore, when the change map is obtained by merging the CRoI extraction masks of the two phases, according to formula (19), the square of the change map error $m_c^2 = m^2 + m^2 = 2m^2$; therefore, $m_c = \\sqrt{2}m$. In complex systems, error propagation may nonlinearly affect the final output, which is why the actual error ratio curve does not coincide with \u221a2. Therefore, one of the views of this article on the ratio of the error ratio close to \u221a2 is that for obtaining the change map using only the CRoI extraction masks in the two phases, the CRoI extraction errors of the two phases are independent are independent to some extent. The total error contributes similarly. Therefore, changing the change process method in the CD module in the AUWCD is proposed to reduce the error accumulation caused by error propagation as a future research direction."}, {"title": "d) Ablation experimental results of VLM performance:", "content": "The test prompt and threshold selected in this section of the experiment are the optimal results obtained in the above ablation experiment, which are \u201ciron building house roofs\u201d and 0.55, respectively. CLIP surgery of different scales are selected to represent VLMs with different performances.\nThe experimental results of building extraction on the BCDD dataset are shown in Table X.\nThe experimental results of building extraction on the BCDD dataset are shown in Table XI.\nThe experimental results reveal that CSVIT-L-14, the largest model, outperforms the other smaller models in both building CD and extraction tasks. As the model size continues to increase, the performance on detection and extraction tasks also improves. As described in the previous implementation section, an increase in the model size improves the performance. Therefore, this experiment proves that the CD performance of the AUWCD is positively correlated with the VLM performance.\nOn the basis of this conclusion, we propose that an increase in model size leads to stronger language and image representation and understanding capabilities, thereby enabling a better understanding of CRoI text descriptions and their corresponding image representations. However, research is needed to explore this hypothesis. In addition, only CLIP surgery was used for ablation experiments in this article. In fact, the performance of CLIP surgery is not ideal in some scenarios, especially in remote sensing [44]. Therefore, exploring VLMs with better remote sensing performance is also an important direction for optimizing the CD effect of AUWCDs in the future."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this article, we first discussed the nature of the CD task and proposed the view that \u201csemantics is the first imaging factor of the CD task\u201d. Moreover, the current CD method relies on the vision-first ViFi-CD paradigm; however, a serious problem with this paradigm is that it inevitably ignores some potential CRoIs and has difficulty adapting to the CD tasks of different CRoIs. To solve this problem, a \u201csemantic-first\u201d CD paradigm called SeFi-CD was proposed based on the above. To verify the effectiveness of this paradigm, this paper proposes a new CD framework AUWCD under this paradigm. Compared with state-of-the-art methods in the current ViFi-CD paradigm, AUWCD performs well on both the SECOND and BCDD datasets, outperforming the state-of-the-art current CD methods when dynamically switching CRoIs without retraining. These findings verify the effectiveness and necessity of the paradigm shift proposed in this article.\nIn the future, we will conduct further research on more complex CD tasks and simultaneously build a \u201csemantic-first\u201d paradigm CD dataset for optimizing and evaluating subsequent methods to promote further development of this paradigm."}]}