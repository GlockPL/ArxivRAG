{"title": "SeFi-CD: A Semantic First Change Detection\nParadigm That Can Detect Any Change You Want", "authors": ["Ling Zhao", "Zhenyang Huang", "Dongsheng Kuang", "Chengli Peng", "Jun Gan", "Haifeng Li"], "abstract": "The existing change detection(CD) methods can\nbe summarized as the visual-first change detection (ViFi-CD)\nparadigm, which first extracts change features from visual\ndifferences and then assigns them specific semantic information.\nHowever, CD is essentially dependent on change regions of\ninterest (CRoIs), meaning that the CD results are directly\ndetermined by the semantics changes of interest, making its\nprimary image factor semantic of interest rather than visual. The\nViFi-CD paradigm can only assign specific semantics of interest to\nspecific change features extracted from visual differences, leading\nto the inevitable omission of potential CRoIs and the inability to\nadapt to different CRoI CD tasks. In other words, changes in\nother CRoIs cannot be detected by the ViFi-CD method without\nretraining the model or significantly modifying the method. This\npaper introduces a new CD paradigm, the semantic-first CD\n(SeFi-CD) paradigm. The core idea of SeFi-CD is to first perceive\nthe dynamic semantics of interest and then visually search for\nchange features related to the semantics. Based on the SeFi-CD\nparadigm, we designed Anything You Want Change Detection\n(AUWCD). Experiments on public datasets demonstrate that the\nAUWCD outperforms the current state-of-the-art CD methods,\nachieving an average F1 score 5.01% higher than that of these\nadvanced supervised baselines on the SECOND dataset, with a\nmaximum increase of 13.17%. The proposed SeFi-CD offers a\nnovel CD perspective and approach.", "sections": [{"title": "I. INTRODUCTION", "content": "Change detection (CD) is an important remote sensing task\nand can be used in tasks such as disaster assessment [1],\nenvironmental monitoring [2], land management [3] and urban\nchange analysis [4]. The main purpose of the CD task is to\ndetect changes in a specific target; however, not all changes in\nimages of different phases since any location can change over\ntime. The target we focus on can change according to the needs\nof subsequent tasks. For example, in practical applications,\nwhen we wish to investigate changes in living space, we focus\non changes in buildings; when we want to monitor changes\nin traffic, we focus on changes in vehicles. The regions we\nfocus on can be called change regions of interest (CRoIs),\nwhile other targets can be called change regions of uninterest\n(CROUI). Since the change maps of different CRoIs differ, the\nresult of CD is determined by the semantics of interest, and\nthe nature of the interest is considered semantic information.\nTherefore, in this paper, we propose that the semantic of\ninterest is the first image factor in CD tasks..\nIn addition to semantics, visualizations are also an important\nimage factor in CD tasks. The current CD paradigm can be\nsummarized as the visual first CD (ViFi-CD) paradigm, as\nshown in Fig.1(a), which first extracts specific change fea-\ntures from visual changes and subsequently gives the change\nfeatures specific semantics in terms of fixation interests. There\nare two main problems with this paradigm, which we analyze\nin detail in Section 2. These two problems can be roughly\ndescribed as follows:\n1) Unsupervised ViFi-CD methods tend to easily interpret\nvisual differences as changes, but visual differences\nsometimes do not imply changes in the semantic content"}, {"title": "II. COMPARATIVE ANALYSIS OF VIFI-CD AND SEFI-CD", "content": "In this section, we conduct a detailed analysis of the\nissues associated with the current ViFi-CD methods. We also\ncompare ViFi-CD methods with SeFi-CD methods to provide\na deeper understanding of SeFi-CD.\nIn the Introduction section, we discussed two crucial image\nfactors in CD: semantic and visual. To comprehensively\nanalyze the issues with ViFi-CD, we categorized the changes\ninto four scenarios based on both the semantic and visual\nperspectives, as shown in Table I: (a) both semantic and\nvisual change, (b) semantic change but no visual change,\n(c) no change in semantic but visual change, and (d)\nno semantic or visual changes. It should be noted that\n\"semantic\" here is the change in an interesting semantic\nbecause the change process usually involves a change in some,\nnot all semantics. Moreover, in reality, there is no absolute\n\"unchange\u201d visually due to lighting, imaging, etc. Therefore,\nvisual change and unchange can be understood as visually\n\"obvious differences\" and \"subtle differences\u201d, respectively.\nCases (a), (b) and (c) are usually CRoIs in various CD tasks,\nand case (d) is a CRoUI in most cases.\nThe methods within the ViFi-CD paradigm can be cate-\ngorized into unsupervised methods and deep learning-led\nSupervised Learning Methods. In the following sections,\nwe conduct an issue analysis on these two types of methods\nseparately and compare them with SeFi-CD."}, {"title": "A. Unsupervised CD Methods", "content": "Before the popularization of deep learning technology, unsu-\npervised methods were the mainstream methods for CD tasks.\nTo date, researchers have proposed various unsupervised CD\nmethods, which can be roughly divided into algebra-based\nmethods and transform-based methods [6]. The algebra-\nbased methods obtain the difference map by applying math-\nematical operations to each image pixel and then obtain the\nfinal change map through operations [6]\u2013[12]. The transform-\nbased CD method first transforms the image pixels and then"}, {"title": "B. Deep Learning-Led Supervised Learning Methods", "content": "With its powerful representation capabilities, deep learning\ntechnology has become the mainstream method for current\nCD tasks and has achieved surprising results. Deep learn-\ning algorithms, which aim to hierarchically learn represen-\ntative and discriminative features from datasets, have received\nwidespread attention from the global earth science and remote\nsensing communities. Researchers have recently proposed var-\nious deep network models for CD tasks. These deep networks\ninclude convolutional neural networks (CNNs), transformers,\ngenerative adversarial networks (GANs), and recurrent neural\nnetworks (RNNs) [24]. The CNN and transformer structures\nare the main model structures in this type of CD method.\nA CNN is a special form of neural network used to process\ndata with a known grid-like representation, such as image\ndata, which can be thought of as a two-dimensional grid\nof pixels [25], [25]\u2013[34]. The transformer [35] was first"}, {"title": "III. RELATED WORKS", "content": "In this section, to better explain the contents of VLMS\nand FSMs in the AUWCD, we summarize the development\nof vison-language pretrained models and foundation segment\nmodels. The details are as follows:"}, {"title": "A. Vision-Language Pretrained Models", "content": "In recent years, with the continued interest in the mul-\ntimodal field and the rapid development of self-supervised\nlearning, a series of pretrained visual language models repre-\nsented by CLIP [44] and ALIGN [45] have been created. These\nmodels usually consist of modality-specific (image and text)\nencoders capable of generating embeddings for various modal-\nities. These models are usually pretrained on billion-scale\nimage and text datasets, and contrastive learning methods are\nused to maximize the alignment of image-text positive sample\npairs by randomly sampling image-text pairs. A direct appli-\ncation of this type of model is zero-shot image-text retrieval\nor zero-shot classification through text prompts [44], which\nexplore shared or mixed architectures between image and text\nmodalities and implement additional features such as zero-\nshot visual question answering (VQA) and subtitles. There\nare also several methods, such as locked image tuning(LiT)\n(Zhai et al., 2022) [46], aligning pretrained encoders(APE)\n(Rosenfeld et al., 2022) [47], and bootstrapping language-\nimage pretraining(BLIP-2) (Li et al., 2023a) [48], that uses\na pretrained single-modal model to reduce the training cost of\nCLIP-like models [49].\nWhile large pretrained visual language models have shown\namazing capabilities, researchers have also conducted in-depth discussions on the interpretability of such models. CLIP\nsurgery [50] refers to the concept of MaskCLIP [51] in\nopen vocabulary semantic segmentation. This task requires\nneither extra supervision nor additional training. In contrast to\nMaskCLIP, CLIP surgery aims to improve the interpretability\nof CLIP [44], while the goal of MaskCLIP is to extract\ndense predictions for segmentation; CLIP surgery does not\ndelete self-attention in MaskCLIP but rather proposes v-v self-\nattention. The original attention module is transformed; in\naddition, CLIP surgery introduces dual paths to merge multiple\nv-v self-attention mechanisms instead of only modifying the\nlast layer, as in MaskCLIP. CLIP surgery is an interpretable\nmethod designed for CLIP models. It uses text prompts to\ngenerate accurate explanation maps, which can highlight areas\nthat are similar to text concepts."}, {"title": "B. Foundation Segment Models", "content": "The segment anything model(SAM) [52], proposed by Meta\nin 2023, has made significant progress in breaking segmen-\ntation boundaries and greatly promoted the development of\nbasic computer vision models. SAM also refers to using\nlarge model prompts for powerful zero-shot learning and can\ngenerate high-quality segmentation masks based on various\nvisual segmentation prompts.\nSince then, many scholars have conducted research to\nexplore the capabilities of SAM and apply it to various tasks\n[53]\u2013[55]. In addition, some scholars are working on various\nvisual prompts to improve the versatility of SAM. [53], [56]\nintroduced more diverse prompts than SAM does and proposed\na more general segmentation system, SEEM, which includes\nvisual prompts (points, boxes, graffiti, masks) and text and\nquote hints (reference areas of another image). As the authors\nclaimed, the unified prompting scheme introduced in the\nSEEM [56] can encode different prompts into a joint visual-\nsemantic space to produce strong zero-shot generalization\ncapabilities. These basic visual models can generate high-quality segmentation masks. Here, we regard these foundation\nvisual models, which are designed for semantic segmentation\ntasks, as the foundation segment models (FSMs)."}, {"title": "IV. METHOD", "content": "In this section, to better understand the working details\nof the AUWCD, we provide a detailed introduction to the"}, {"title": "A. Semantic Align Module", "content": "The main function of this module is to align the feature\nspace of the multitemporal image with the semantic space of\nthe CROI text prompt. Visual prompts are generated for areas\nwith a higher degree of alignment based on the alignment\ndegree between the image feature space and the CROI semantic\nspace. The CROI segment module can be utilized to segment\nthe CRoIs in images. VLMs are the core component of this\nmodule and usually consist of an image encoder for storing\nrich knowledge and a text encoder for selecting the CRol\nwe intend to use. Specifically, the image encoder stores rich\nknowledge about more possible CRoIs and has a strong ability\nto distinguish their features clearly. In other words, it can\ngenerate similar representations for CRoIs of the same type\nand different representations for CRoIs of different types.\nWe used CLIP Surgery [50] to generate point prompts as an\nexample to explain this module in detail.\nThe overall process of this module is shown in Fig.3. We\ndescribe the four processes of image encoding, text encoding,\nsimilarity map generation, and point prompt generation\nbased on similarity maps:\na) Image Encoding: The dual-temporal images It1 and\nIt2 obtain the text encodings 11 and 12 of the two images\nthrough the image encoder. The two image encoders used here\nare from CLIP Surgery, and their weights are shared.\nb) Text Encoding: Both CRoI and CRoUI are represented\nby text. We can specify CRoUI using textual descriptions to\nsuppress the CRoUI recognition. For example, in a building\nCD task, we can select textual descriptions such as \u201cshadows\u201d,"}, {"title": "B. CRoI Segment Module", "content": "The core component of this module is FSMs, whose main\nfunction is to receive the visual prompt of multitemporal\nimages generated by the \"Semantic Align Module\u201d and seg-\nment the CRoI masks in the multitemporal images through\nthe foundation segmentation model of shared weights. As\nmentioned earlier, classic FSMs, such as the segment any-\nthing model (SAM) [52], Although the segmentation results\nof FSMs represented by the SAM do not carry semantic\ninformation, it is provided by the Semantic Align Module for\nguidance; therefore, when the Semantic Align Module is given\na visual prompt from a CROI, an extraction boundary about\nthe CRoI can be generated. Moreover, the SAM can further\ncorrect boundaries by providing some visual prompts from\nthe CROUI as negative samples. Through the CRoI Segment\nModule, we can obtain more accurate segmentation results for\nthe CRoI text description; therefore, we use SAM as the FSM\nof this module to introduce this module in detail."}, {"title": "C. CD(CD) Module", "content": "The main function of this module is to perform the\nchange process on the CRoI mask in the multitemporal im-\nage and finally obtain the CRoI change map. We assume\nthat we obtain the mask of multiple temporal images CRol\n{Mt1, Mt2,..., Mtn} in a certain area through the foundation\nsegment module. The change map obtained by the change pro-\ncess for any two phases can be expressed by the formula(12):\n\\(ChangeMap_{(t_1,t_j)} = (M_{t_i} \\cup M_{t_i}) \\setminus (M_{t_i} \\cap M_{t_i})\\)   (12)\nIn the formula, \\(ChangeMap_{(ti,t_j)}\\) represents the change map\nof the image at time \\(t_j\\) in some area relative to the image at\ntime \\(t_i\\) in the same area. \\ is the difference set symbol, and\nA\\B represents the difference set of A to B.\nNotably, the VLM and VSM include CLIP surgery and\nSAM. We can also choose different change process methods\nto obtain a higher-quality change map."}, {"title": "V. EXPERIMENTS", "content": "Experimental validations were conducted using the instan-\ntiated framework described above to assess the effectiveness\nof the AUWCD framework. We first outline the experimental\nsetup, including the dataset, evaluation metrics, and implemen-\ntation details, followed by presenting the experimental results.\nFinally, a brief summary and analysis of the experimental\noutcomes are provided."}, {"title": "A. Databases", "content": "1) BCDD Dataset: The BCDD dataset [58] (as shown in\nFig.5) encompasses the region affected by a 6.3 magnitude\nearthquake in February 2011 in Christchurch, New Zealand.\nThis dataset comprises two image scenes captured at the same\nlocation in 2012 and 2016, accompanied by semantic labels\nand CD labels for buildings. To manage the large image size\nof 32,507 \u00d7 15,354 pixels, we divided each image into non-\noverlapping 256 \u00d7 256 pixels image pairs, resulting in a total\nof 7,434 pairs.\n2) Second Dataset: The SECOND dataset [59] is a se-\nmantic CD dataset (as illustrated in Fig.6). It consists of\n4,662 pairs of aerial images collected from various platforms\nand sensors covering cities such as Hangzhou, Chengdu, and\nShanghai. Each image in the dataset has a size of 512 \u00d7\n512 pixels and is annotated at the pixel level. The dataset\nfocuses on six land cover categories, namely, non-vegetation,\ntrees, low vegetation, water, buildings, and playgrounds.\nWe established a baseline CROI based on the land cover\ncategories in the first temporal phase. Using these six CRols,\nthe dataset was divided into six CRoI categories. Subsequently,\nthe images were utilized to create the training set, validation\nset, and test set in an 8:1:1 ratio."}, {"title": "B. Metrics", "content": "To quantitatively validate the effectiveness of the proposed\nmethod, we employed four common accuracy metrics com-\nmonly used in CD tasks [30]: precision, recall, F1 score (F1),\nand overall accuracy (OA). The formulas for these metrics\nare as follows:\n\\(Precision = \\frac{TP}{TP + FP}\\)  (13)\n\\(Recall = \\frac{TP}{TP + FN}\\)  (14)\n\\(F1 = \\frac{2TP \\cdot Recall}{Precision + Recall}\\)  (15)\n\\(OA = \\frac{TP + TN}{TP + TN + FP + FN}\\)  (16)\nwhere TP is the number of true positives, FP is the number of\nfalse positives, TN is the number of true negatives, and FN is\nthe number of false negatives.\nIn addition, this paper utilizes the point accuracy (PAcc)\nmetric to quantify the effectiveness of the point prompt\ngenerated by the \"Semantic Align Module\". The computation\nformula for the PAcc is as follows:\n\\(PAcc = \\frac{P_c + N_c}{P + N}\\)  (17)\nwhere P is the total number of positive points generated by\nCLIP surgery and N is the total number of negative points\ngenerated by CLIP surgery, \\(P_c\\) is the number of positive points\nwhose corresponding label is 1 (CROI), \\(N_c\\) is the number of\nnegative points whose corresponding label is 0 (CROUI)."}, {"title": "C. Comparative Experiment", "content": "The purpose of this comparative experiment was to quan-\ntitatively compare the effects of the AUWCD framework\nand advanced methods in the ViFi-CD paradigm on different\nCROI CD tasks without major modifications to the model or\nmethod, the SeFi-CD paradigm can better adapt to complex\nand changeable CRoI CD than the ViFi-CD paradigm. The\nexperimental details and results are as follows.\n1) Experimental Design Ideas: The idea for the design of\nthis experiment is that in a multicategory CD dataset, various\ncategories represent CRoIs for different tasks. We selected\nseveral major supervised baselines dominated by supervised\nlearning in the ViFi-CD paradigm as comparison objects.\nFor these supervised baselines, we provide training samples\nfor only a certain category in the dataset and do not provide\ntraining samples for the other categories. Thus, the baseline\nobtained after training can be regarded as a model dedicated to\na specific CROI CD task and other categories can be regarded\nas different CD tasks focused on CRoIs.\nFinally, we use the trained baselines to perform CD tasks on\nall CRoIs categories and use the above indicators to quantify\nthe detection effect; for AUWCD, we input only the text of\neach category in the dataset into the model. To prompt the\nmodel to complete different categories of CD tasks, the above\nindicators are used to quantify the effect of CD; finally, the\neffects of the AUWCD and these supervised baselines are\ncompared by comparing indicator values.\n2) Implementation Details: For multicategory CD datasets,\nwe choose the SECOND dataset, which contains six classes for\nsemantic CD, as the experimental dataset to verify the effect of\nthe AUWCD framework. We compare the performance of\nthe AUWCD with that of advanced CD methods on the ViFi-CD\ndataset.\nFor the methods in ViFi-CD, we chose several advanced\nsupervised baselines from the past five years as objects of\ncomparison: FC-EF [27], FC-Siam-conc [27], FC-Siam-diff\n27], DASNet [30], BIT [37], SwinSUNet [40], ICIFNet [60],\nDMINet [61], ELGCNet [62], ChangeBind [63].\nAll supervised baselines were implemented using the Py-\nTorch backend and were supported by NVIDIA RTX A6000\nGPUs. We used the Adam optimizer with an initial learning\nrate of 0.00001 for all the models and set the batch size to 8.\nFor the AUWCD framework, we needed to change only\nthe CROI text prompt to directly test each of the six CROIS\nseparately. The various hyperparameters of the point prompt\ngeneration process in CLIP surgery used the best hyperpa-\nrameters obtained from subsequent ablation experiments and\nused the largest and best-performing model, CSVIT-L-14. The\nFSM module also utilizes the sam_vit_h model. Both CSVIT-\nL-14 and sam_vit_h use pretrained model parameters. The text\nprompts for each CRoI category were set as follows: \"low\nvegetation land\", \"tree\", \"building roofs\", \"waters\", \"non-vegetation land\" and \"football field\u201d. These datasets corre-sponded to the original low vegetation, tree, building, water, n.v.g. surface, and playground categories, respectively.\nTo eliminate chance, we conducted three independent sets\nof comparative experiments, and the CRoI categories that\nprovided training samples were divided into \"tree\", \"water\",\nand \"playground\". Moreover, to demonstrate our effects quan-\ntitatively, we chose the F1 score, which is the most important\nindicator for CD tasks, as a quantitative indicator.\n3) Result:\na) \"Tree\" was used as the training category to provide\ntraining samples: The experimental results for the other\ncategories that did not provide training samples are shown\nin Table II.\nb) \"Water\" was used as the training category to provide\ntraining samples: The experimental results for the other\ncategories that did not provide training samples are shown\nin Table III.\nc) \"Playground\" was used as the training category to\nprovide training samples: The experimental results of other\""}, {"title": "D. Visualization Experiment", "content": "A visual experiment of the AUWCD is performed in this\nsection. The main purpose is to visually demonstrate the\nprocess and effect of CD via the AUWCD. The experimental\ndetails and results are as follows:\n1) Implementation Details:\na) Visualization effect of the AUWCD on a single CRoI\nCD task (GT is used for comparison, but the category is\nsingle): We display the visual results of building CD on the\nBCDD dataset to show the intermediate process and effects\nof AUWCD CD and the final CD results. Consistent with\nthe parameter settings obtained after the subsequent ablation\nexperiment, VLM selected CSVIT-L-14, the threshold t was\nset to 0.65, and the CRoI prompt word was set to \"iron\nbuilding house roofs\u201d. For FSM, we also selected sam_vit_h.\nBoth CSVIT-L-14 and sam_vit_h also used pretrained model\nparameters.\nb) Visualization effect of the AUWCD on multiple CROI\nCD tasks (no information for comparison but rather multiple\ncategories): In the BCDD dataset, we demonstrate the CD\neffect of different CRoIs on the same pair of dual-temporal\nimages. In the experiment, we selected several ground objects\nfor display. The selected ground objects and corresponding\nprompt words are shown in Table V:"}, {"title": "E. Ablation Experiment", "content": "This ablation experiment investigates the impact of VLMs\nin the semantics align module on the overall performance of\nthe AUWCD. The study explored the following four aspects:\na) Effect of CRoI Text Prompts: This experiment inves-\ntigated the influence of different prompts on the accuracy of\nCROI descriptions. Since different prompts can lead to varying\nlevels of precision in describing CRoIs, the objective is to\nidentify prompts that provide more accurate descriptions of\nthe target.\nb) Effect of CRoUI Text Prompts: In the Semantic Align\nModule, we can design a negative text prompt to suppress the\nmodel's recognition of CRoUI. The purpose of this experiment\nis to demonstrate that negative text prompts for CRoUIs\ncan enhance CRoI detection quality by inhibiting CRoUI\nrecognition.\nc) Effect of Thresholds: As thresholds directly affect the\nnumber and prompt point locations, determining the most\nsuitable threshold values is essential. The experiment aims\nto find optimal thresholds that maximize the CD process\nperformance.\nd) Impact of VLM Model Performance: This analysis\naims to demonstrate how VLM performance in the Seman-\ntic Align Module affects overall AUWCD performance. The\nexperimental details and results for each of the above aspects\nare presented below:\n1) Implementation Details: We utilize the BCDD dataset as\nthe data source for this experiment. Since the AUWCD does\nnot require training and can directly perform CD tasks, we\ntested it on the entire BCDD dataset, which contains 7,434\npairs of images. The implementation details for the three\naforementioned experiments are as follows:\na) Effect of CRoI text prompts: Considering that the\nBCDD dataset focuses on CD for buildings, we selected seven\ntext prompts to describe the CRoI as follows: 'buildings',\n'houses', 'roofs', 'building roofs', 'house roofs', 'building\nhouse roofs', and 'iron building house roofs'. Additionally,\nto demonstrate that only relevant text prompts are effective,\nwe added other text descriptions unrelated to buildings such\nas \"tree\", \"road\", and \"river\". To generate point prompts using\nCLIP surgery, we utilized the largest model size, CSVIT-\nL-14, with a threshold set to 0.80. For the FSM module,\nwe employed the SAM official demo, which also uses the\nlargest model size, sam_vit_h. Similarly, both CSVIT-L-14\nand sam_vit_h also used pretrained model parameters. We\nconducted a quantitative analysis using five evaluation metrics\nto assess the impact of different text prompts on the final\nCD results. Furthermore, since the BCDD dataset contains\nannotations for buildings in both time frames, we demonstrated\nthe influence of different text pages on building segmentation\nin dual temporal images. This presentation adds further per-suasiveness to the results.\nb) Effect of CROUI Text Prompts: Since this is a building\nCD task, CRoI refers to buildings. Therefore, we set the\nCROI text prompt to \"iron building house roofs\". In building\nCD, common pseudo changes include shadows, roads and\nso on. Here, we selected common pseudo changes as the\nCROUI textual prompts, which are \"road\", \"shadow\", and\n\"tree\". Additionally, to explore the combined effect of multiple\nCROUIs, we set \"road, shadow, tree\" as the text prompt for\nmultiple CROUI combinations. We compared these CRoUI\ntext prompts above with an empty textual prompt(\" \") to\ndemonstrate the effectiveness of this method. The VLM in the\nSemantic Align module is CSVIT-L-14, with a threshold set\nto 0.80. The FSM in the CROI Segment Module is sam_vit_h.\nBoth of them used pretrained model weights.\nc) Effect of thresholds: Based on the results obtained\nfrom the previous experiment, where we identified the optimal\ntext prompt for building CD, we explored the impact of\nvarying threshold values on CD and CRoI segmentation. We\ninvestigated threshold values in the range of 0.5 to 0.95 with\na step size of 0.05 for their effect on performance. Again, we\nused CLIP surgery with the model size CSVIT-L-14 and the\nFSM module with the sam_vit_h model while using the best-performing text prompt from the first experiment. CSVIT-L-14\nand sam_vit_h used pretrained model weights.\nd) Impact of VLM performance: This experiment ex-plored the impact of VLM performance on the overall results.\nLarger model sizes have been shown to improve performance\n[44], [64]; thus, we chose three different CLIP surgery model\nsizes to represent VLMs with varying capabilities. The se-lected model sizes, from smallest to largest, were CSVIT-B-32, CSVIT-B-16, and CSVIT-L-14. All CLIP surgery models used pretrained model weights. The text prompt remained"}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this article, we first discussed the nature of the CD task\nand proposed the view that \u201csemantics is the first imaging\nfactor of the CD task\u201d. Moreover, the current CD method\nrelies on the vision-first ViFi-CD paradigm; however, a serious\nproblem with this paradigm is that it inevitably ignores some"}]}