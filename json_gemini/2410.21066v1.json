{"title": "Learning to Handle Complex Constraints for Vehicle Routing Problems", "authors": ["Jieyi Bi", "Yining Ma", "Jianan Zhou", "Wen Song", "Zhiguang Cao", "Yaoxin Wu", "Jie Zhang"], "abstract": "Vehicle Routing Problems (VRPs) can model many real-world scenarios and often involve complex constraints. While recent neural methods excel in constructing solutions based on feasibility masking, they struggle with handling complex constraints, especially when obtaining the masking itself is NP-hard. In this paper, we propose a novel Proactive Infeasibility Prevention (PIP) framework to advance the capabilities of neural methods towards more complex VRPs. Our PIP integrates the Lagrangian multiplier as a basis to enhance constraint awareness and introduces preventative infeasibility masking to proactively steer the solution construction process. Moreover, we present PIP-D, which employs an auxiliary decoder and two adaptive strategies to learn and predict these tailored masks, potentially enhancing performance while significantly reducing computational costs during training. To verify our PIP designs, we conduct extensive experiments on the highly challenging Traveling Salesman Problem with Time Window (TSPTW), and TSP with Draft Limit (TSPDL) variants under different constraint hardness levels. Notably, our PIP is generic to boost many neural methods, and exhibits both a significant reduction in infeasible rate and a substantial improvement in solution quality.", "sections": [{"title": "Introduction", "content": "Vehicle routing problems (VRPs) are NP-hard combinatorial optimization problems with complex constraints that model real-world scenarios, such as logistics [1] and supply chains [2]. For decades, traditional solvers relied on hand-crafted rules for VRP optimization and constraint handling. Recently, the learning-to-optimize community [3] has successfully trained deep neural networks to automatically construct VRP solutions in an end-to-end manner [4-6]. These data-driven neural methods offer greater efficiency and high parallelism for batch optimization, making them favorable alternatives.\nIn general, neural methods construct VRP solutions by autoregressively sampling a node from its predicted distribution while masking out nodes that would violate constraints to ensure the solution's feasibility. Despite successes (e.g., on TSP and CVRP), this masking mechanism assumes that 1) the feasibility of the entire solution can be properly decomposed into the feasibility of each node selection step, and 2) ground truth masks are easily obtainable for each step. However, such assumptions may fail in VRPs (e.g., TSPTW) with complex interdependent constraints among decision variables (i.e., nodes). As will be discussed in Section 4.1, this creates a masking dilemma - considering only the local feasibility of node selections does not guarantee the overall feasibility of the constructed solutions, while computing global feasibility masks that account for future impacts transforms masking itself into another intractable NP-hard problem.\nThese observations highlight significant gaps in applying recent neural methods to practical VRPs, necessitating research on new constraint-handling frameworks. In the literature, few studies have focused on novel ways of handling feasibility in neural constructive solvers. Although preliminary methods have attempted to mitigate it by relaxing constraints into soft ones [7, 8] or supplementing networks with more feasibility-related features [9], the former is prone to failure when applied to more complex scenarios, while the latter requires problem-specific features and a large supervised learning dataset, limiting its adaptability to broader VRPs. Consequently, neural methods still show limited flexibility, poor feasibility rates and large optimality gaps in solving those complex VRPs.\nIn this paper, we propose a novel Proactive Infeasibility Prevention (PIP) framework to extend the capabilities of neural constructive methods for VRPs with complex interdependent constraints. Our PIP first integrates the Lagrangian multiplier method into the reinforcement learning framework of neural methods, promoting initial constraint awareness and search guidance. To further address the limitations of the Lagrangian multiplier method on complex constraints, we then introduce preventative infeasibility masking to proactively steer the search to (near-)feasible regions during solution construction. By doing so, PIP significantly enhances feasibility rates and reduces optimality gaps. Moreover, to reduce the costs of obtaining preventative infeasibility information during training, we present PIP-D, which employs an auxiliary decoder to learn and predict masking information. Our PIP-D also incorporates two adaptive strategies: one to balance infeasible and feasible masking information for different problem hardness, and another to periodically update the model so as to balance training efficiency with prediction accuracy. These advancements enable PIP-D to achieve comparable or even better performance than PIP, particularly on larger and more constrained VRP instances, while significantly reducing computational complexity.\nOur contributions are as follows: 1) Conceptually, we represent an early work to address and advance the handling of complex interdependent constraints in VRPs, where the original masking loses effectiveness due to the aforementioned dilemma, thereby extending the applications of neural methods to more practical scenarios. 2) Methodologically, we propose novel PIP and PIP-D approaches that can boost the capabilities of most constructive neural methods. Specifically, we leverage the Lagrangian multiplier method and introduce preventative infeasibility masking, which is further learned by an auxiliary decoder network with two adaptive strategies, to proactively and efficiently steer the search during solution construction. 3) Experimentally, we conduct extensive validation to demonstrate the effectiveness and versatility of PIP across various backbone models (i.e., AM [4], POMO [5], and GFACS [10]) and complex VRP variants (i.e., TSPTW and TSPDL). Notably, PIP achieves both a significant (up to 93.52%) reduction in infeasible rate and a substantial improvement in solution quality."}, {"title": "Related work", "content": "Neural solvers for VRPs. Existing literature on learning to optimize VRPs features two primary paradigms: constructive solvers and iterative solvers. Constructive solvers learn policies to construct solutions from scratch in an end-to-end manner. Early works introduce Pointer Network to approximate the optimal solution to TSP [11, 12] and CVRP [13] in an autoregressive (AR) way. Among all AR solvers, the attention-based model (AM) [4] represents a milestone in solving a series of VRPs. Later, the policy optimization with multiple optima (POMO) [5] further improves upon AM by considering the symmetry property of VRP solutions. Numerous recent studies have then aimed to further enhance their performance [14\u201323] and versatility [24\u201327]. Besides the AR methods, several works construct a heatmap, which indicates the probability distribution of each edge being part of the optimal solution, to solve VRPs in a non-autoregressive (NAR) manner [10, 28-34]. Despite the superior performance on large-scale instances, we note that a recent work [35] questions the effectiveness of heatmap generative methods due to the misalignment of training and testing objectives. Differently, iterative solvers learn policies to iteratively refine an initial solution. The policies are often trained in contexts of classic heuristics or meta-heuristics for obtaining more efficient and effective search components [36-45]. Generally, constructive solvers can efficiently achieve desirable performance levels, whereas iterative solvers hold the potential to search for near-optimal solutions with a prolonged time budget. Additionally, there are also several works studying the scalability [46\u201352], generalization [53\u201358], and robustness [59, 60] of neural VRP solvers, and leveraging large language models (LLMs) to optimize VRPs [61-63].\nConstraint handling for VRPs. Most neural methods for VRPs manage constraints using a feasibility masking mechanism that eliminates actions leading to infeasible solutions during construction or iteration search [4, 28, 32, 41]. However, such a mechanism assumes the availability of accurate masks and often lacks constraint awareness learning during training, which is not always practical or desirable. For example, Zhao et al. [64] highlighted the benefits of learning to modulate agent behaviours in the 3D Bin Packing Problem, and Ma et al. [45] showed that temporary constraint violations could enhance neural iterative solvers. Despite their successes, these approaches are inherently unsuitable for assisting constructive solvers to address the VRPs with complex interdependent constraints studied in this paper. While Tang et al. [8] and Zhang et al. [7] proposed methods to transform hard constraints into soft ones via relaxation techniques and problem redefinition, respectively, they may only be able to yield near-feasible solutions with large infeasible rates for VRPs with complex constraints. More recently, Chen et al. [9] developed a multi-step look-ahead (MUSLA) method specifically tailored for TSPTW, incorporating problem-specific features and a large supervised learning dataset. In contrast, this paper proposes a more flexible and generic PIP framework based on novel ideas of preventative infeasibility masking, learnable decoders, and adaptive strategies to advance a broader range of neural methods without needing labelled training data."}, {"title": "Preliminaries", "content": "In this paper, we mainly consider two VRP variants with complex interdependent constraints (i.e., TSPTW and TSPDL), and neural solvers (i.e., AM [4], POMO [5] and GFACS [10]).\nProblem definitions and notations. A VRP instance can be defined over a complete graph G = {V,E}, where V = {V_0, V_1, ..., V_n} denotes the node set, and E = {e (V_i, V_j)|V_i, V_j \u2208 V,i \u2260 j} denotes the directed edge set among all nodes. The objective is to minimize the total cost (e.g. Euclidean length) of the solution tour. To form a feasible solution, each node in V should be visited exactly once while respecting problem-specific constraints. We consider two types of VRP constraints that are practical in industry: 1) Time window constraint: The arrival time at node V_i, denoted as t_i, must fall within a customer-specific time window [l_i, u_i]. If The vehicle arrives early (i.e., t_i < l_i), it must wait until l_i; 2) Draft limit constraint: Each node V_i represents a port with a non-negative demand d_i and a maximum draft d'_i. We denote the current cumulative load of the freighter at port V_i as a_i in a given solution, which should not exceed the corresponding maximum draft d'_i of the port.\nConstructive solvers for VRPs. Popular neural constructive solvers [4, 5] typically parameterize the policy using an encoder-decoder model with parameter \u03b8, trained with reinforcement learning (RL). Given a VRP instance G = {V, E}, the features of each node V_i are represented as f_i = {x_i, y_i, c_i}, where x_i, y_i are node coordinates, c_i represents constraint-related features (e.g., c_i = {l_i, u_i} for time windows in TSPTW and c_i = {d_i, d'_i} for demand and draft limits in TSPDL). The encoder transforms node features into high-dimensional representation embeddings h_i, which, combined with the context of the partial tour, represent the current state. The decoder takes them as inputs and outputs probabilities for candidate nodes (actions). The reward R(T|G) is the negative tour length. The policy \u03c0_\u03b8 is typically trained using REINFORCE [65] as follows:\n$\\nabla_\\theta L_{RL} (\\theta | G) = \\frac{1}{K} \\sum_{i=1}^{K} (R(T_i | G) - b(G)) \\nabla log \\pi_\\theta (T_i | G),$\nwhere K denotes the number of sampled solutions T_i for a given training instance G, and b(\u00b7) is a baseline function to reduce the variance. Specifically, the baseline is the reward (negative tour length) of the solution derived greedily in AM or the average reward of sampled solutions $\\sum_{i=1}^{K}R(T_i | G)$ in POMO. Notably, POMO stipulates the starting node of each solution for diversification, which, however, may hinder solution feasibility in our studied complex constrained problems. Based on our preliminary experiments, POMO with and without diverse starting nodes achieve around 50.70% and 1.75% infeasible rates on the easy TSPTW-50 datasets, respectively. Therefore, we remove the starting node stipulation in POMO and instead sample n solutions to calculate the baseline."}, {"title": "Methodology", "content": "We now discuss the limitations of existing masking mechanisms in solving VRPs with complex interdependent constraints, followed by a detailed introduction of our PIP and PIP-D frameworks.\nDilemma of feasibility masking\nThe core of feasibility masking in neural constructive solvers is to filter out invalid actions that violate constraints, based on the assumption that the global feasibility can be decomposed into the feasibility of each node selection step, and that ground truth masks are obtainable for each step. Without loss of generality, we illustrate the dilemma of feasibility masking using a TSPTW example. In TSPTW, nodes are masked out if they have been visited or cannot be visited before their time window closes. However, the feasibility of selecting a node at a particular step impacts the current time, thereby affecting all future selections due to the interdependence of time window constraints. Thus, considering only local feasibility does not guarantee overall feasibility and may lead to irreversible infeasibility. For instance, in a 4-node TSPTW instance with time windows {[0, 7], [1, 4], [2, 4], [2, 6]} as illustrated in the left panel of Figure 1, there is a feasible solution \u03c4 = (v_0 \u2192 v_1 \u2192 v_2 \u2192 v_3). Yet, with the partial solution v_0 \u2192 v_1, both v_2 and v_3 appear locally feasible. If the solver selects v_3, the tour becomes infeasible irreversibly. A potential remedy is to compute global feasibility masks that consider all future possibilities, as illustrated in the right panel of Figure 1. However, this makes masking itself an NP-hard problem, which creates a dilemma between ensuring solution feasibility and managing computational complexity. Note that this dilemma is less critical in CVRPTW, which involves multiple vehicles and routes, providing more flexibility. If one route becomes infeasible, another vehicle departing at time 0 can cover the missed nodes, reducing the impact of constraint interdependencies. However, this issue is severe in TSPTW and other variants like TSPDL.\nGuided policy search by PIP\nWe first formulate the solution construction process of VRP as a Constrained MDP (CMDP) defined by the tuple (S, A, P, R, C), where S is the state space, A is the action space that travels from node v_i to node v_j, R : S \u00d7 A \u00d7 S is the reward function, C : S \u00d7 A \u00d7 S is the constraint violation cost (penalty) function, and P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the transition probability function. At each time step, the neural solver outputs the probability of all candidate nodes, and selects one to construct a complete solution \u03c4. The objective of CMDP is to learn a policy $\u03c0_\u03b8 : S \u2192 P(A)$ that maximizes the summation of the state-wise reward subject to certain constraints,\n$\\max_\\theta I(\\pi_\\theta) = E_{\\tau \\sim \\pi_\\theta} [\\sum_{e(v_i, v_j) \\in T} R(e(v_i, v_j))]$\ns.t. $\u03c0_\u03b8 \u2208 \u03a0_F$, $\u03a0_F = {\u03c0 \u2208 \u03a0 | I_{Cm}(\u03c0) \u2264 \u03ba_m, \u2200m \u2208 [1, M]}$,\nwhere I is the expected return of the policy, $\u03a0_F$ denotes the set of all feasible policies, $\u03ba_m$ represents the boundary of the inequality constraints $C_m$, and M is the number of constraints. Specifically, a feasible policy \u03c0 is one whose expected value of constraint violation w.r.t $C_m$, denoted as $I_{Cm}(\u03c0)$, does not exceed $\u03ba_m$. Note that $\u03ba_m$ is set to 0 throughout this paper since we consider the hard constraints that do not tolerate any violation. Moreover, we set the reward function R to the negative value of the Euclidean distance between two nodes, i.e., R (e (v_i, v_j)) = \u2212||v_i - v_j||_2.\nBy applying feasibility masking, the search is confined to only feasible regions, allowing neural methods to focus solely on the objective function in Eq. (2) without explicitly considering constraint awareness or constraint violations. However, these methods lose effectiveness when such masks are unavailable, leading to inefficient searches in large infeasible regions. To address this, we propose PIP, combining a Lagrangian multiplier for constraint awareness and preventative infeasibility masking to confine the search space to near-feasible regions for complex constrained problems.\nLagrangian-assisted constraint awareness. We design a Lagrangian multiplier based method to incorporate constraints C into the reward function R. Based on the Lagrangian Multiplier Theorem, the CMDP formulation in Eq. (2) is transformed into the following MDP formulation for VRPs:\n$\\min_{\\lambda \\geq 0} \\max_\\theta L(\\lambda, \\theta) = \\min_{\\lambda \\geq 0} \\max_\\theta -E_{\\tau \\sim \\pi_\\theta}[\\sum_{e(v_i, v_j) \\in T} ||v_i - v_j||_2 + \\sum_{m=1}^{M} \\lambda_m I_{C_m}(\\tau) + J_{IN}]$,\nwhere L is the Lagrangian function, and $\u03bb_m$ is a non-negative Lagrangian multiplier. Generally, the constraint violation term is calculated as the total violation value of all constraints. In TSPTW, $I_{TW}(\u03c4) = \\sum_{i=1}^{n} max(t_i - u_i, 0)$, and in TSPDL, $I_{DL}(\u03c4) = \\sum_{i=1}^{n} max(a_i \u2013 d'_i, 0)$. Additionally, we introduce the number of infeasible nodes in the solution \u03c4, termed as $J_{IN}$, as an extra term in the Lagrangian function for better constraint awareness, which is empirically found to be effective to reduce the infeasibility rate. While Lagrangian relaxation has been explored in neural iterative methods for soft objectives [8], our approach introduces a distinct constraint violation cost function tailored for neural constructive methods and considers fixing the Lagrangian multiplier \u03bb (the dual variable) and optimizing the primal variable \u03b8, significantly reducing computational overheads.\nPreventative infeasibility (PI) masking. As depicted in Figure 2(b), the customized Lagrangian multiplier guides the neural policy towards a potentially feasible and high-quality space using Eq. (3). However, for more complex cases shown in Figure 2(c), neural solvers may still struggle to navigate the large search space. To further improve training efficiency and solution feasibility, we introduce preventative infeasibility (PI) masking to proactively avoid selecting infeasible nodes during the solution construction process. As shown in the left panel of Figure 3, if selecting a candidate node (i.e., orange node) results in any remaining candidates (i.e., green node) becoming potentially unvisitable in the next step due to constraint violations, it is marked as infeasible (i.e., red node) since selecting it would cause irreversible future infeasibility (see Appendix A.3 for a detailed example). Note that we employ a simple yet effective one-step PI masking in this paper to balance computational costs without iterating over all future possibilities (which is NP-hard). Together with the customized Lagrangian multiplier, our PIP proactively reduces the search space to a near-feasible domain $\u03a0_F$, as shown in Figures 2(d)-(e). Notably, such PIP design is generic and can be applied to enhance most neural constructive solvers for VRPs with complex interdependent constraints."}, {"title": "Learning to prevent infeasibility", "content": "Benefiting from the constraint-aware optimization guided by our PIP, neural methods gain enhanced capabilities to address complex constraints, significantly boosting feasibility and optimality. However, acquiring the above PI information introduces extra computational costs (see Section 5). To alleviate this, we propose an auxiliary decoder network to learn and predict these masks, replacing the time-consuming process of generating PI information with a much faster forward pass of the PIP decoder. This further accelerates the training process, resulting in an enhanced version of our PIP framework, termed PIP-D. The overall framework of PIP-D is illustrated in Figure 3.\nAuxiliary PIP decoder. As presented in Figure 3, we incorporate an auxiliary decoder to learn and predict the PI masks. Our PIP-D simultaneously involves training a routing decoder (the original one) that maximizes the expected reward of solutions in Eq. (3) and a PIP decoder that minimizes the prediction error on the PI masking using a weighted binary cross-entropy loss. The combined loss function is a weighted sum of these two objectives, i.e., L = \u03b1LRL + \u03b2LPIP. To ensure the generality, the PIP decoder mirrors the architecture of the backbone model and only adjusts the final output layer with Sigmoid activation. More details of our PIP decoder are provided in Appendix B.\nPIP-D training with adaptive strategies. Nevertheless, efficiently training the PIP decoder together with the routing decoder necessitates effective designs. We address this with two adaptive strategies. Firstly, training the PIP decoder at every gradient step would result in higher computational complexity than the original PIP, counteracting our goal of reducing training complexity. Hence, we adopt a periodic update strategy that intermittently updates the PIP decoder instead of continuously doing so. This approach is based on the observation that the PI masks recommended by the neural network tend to remain robust over short training periods. Specifically, we first train the PIP decoder with $E_{init}$ epochs, then periodically update $E_u$ epochs per $E_p$ epochs, and finally conduct $E_l$-epoch updates. In this way, the computational costs are reduced and can be adaptively adjusted. Secondly, we consider balancing feasible and infeasible PI signals for instances with different inherent hardness. Given that the proportion of PI signals identified for feasible and infeasible nodes can vary significantly across different VRP variants with different inherent hardness, we employ a weighted balancing strategy to mitigate the influence of label imbalance [66], which is formulated as follows:\n$L_{PIP}(\\theta | G) = -\\frac{1}{T} \\sum_{t=0}^{T} (w_{infsb}g_t \\cdot log(p_\\theta (g_t)) + w_{fsb} (1 - g_t) \\cdot log(1-p_\\theta (g_t)))$,\\nwhere T is the total decoding step to construct a complete solution. The weights of each category are calculated by their corresponding sample number, i.e, $w_{infsb} = \\frac{N_{infsb}}{N_{infsb}+N_{fsb}}$, $w_{fsb} = \\frac{N_{fsb}}{N_{infsb}+N_{fsb}}$, where $N_{infsb}$ and $N_{fsb}$ are the number of infeasible and feasible nodes identified by our PI masking ($g_t$) in a specific decoding step t, respectively. Moreover, beyond the above two critical strategies, we explore additional strategies to accelerate the training of the PIP decoder, including fine-tuning and early-stopping techniques, which are discussed in Appendix D.3."}, {"title": "Experiments", "content": "In this paper, we propose a Proactive Infeasibility Prevention (PIP) framework and its enhanced version, PIP-D, to address the limitations of existing masking mechanisms for handling complex constraints. Notably, our PIP and PIP-D are generic and can be applied to boost various problem variants and neural methods. To evaluate the effectiveness of our method, we apply our PIP frameworks to two representative AR constructive methods, AM [4] and POMO [5], and the latest NAR constructive GFACS [10]. For the benchmark problem, we consider two representative complex VRP variants with strong interdependent constraints that challenge existing neural methods (i.e., TSPTW and TSPDL, each at varying levels of hardness) with small problem scale n = 50, 100 for AM [4] and POMO [5] and large scale n = 500 for GFACS [10]. All the experiments are conducted on servers with NVIDIA GeForce RTX 3090 GPUs and Intel(R) Xeon(R) Gold 6326 CPU at 2.90GHz. Our implementation in PyTorch are publicly available at https://github.com/jieyibi/PIP-constraint.\nImplementation details. We generate instances at different hardness levels following prior works. For TSPTW [7, 9, 30, 67], we generate three types of instances: Easy, Medium and Hard, by adjusting the width and overlap of the time window. For TSPDL [68\u201370], we consider two levels of hardness: Medium and Hard. More details of such instance generation are provided in Appendix A. To ensure a comprehensive comparison, we also train and evaluate the models learned solely using our designed Lagrangian multiplier method. Meanwhile, we mark the models that use the Lagrangian multiplier with an * for clarity. For our proposed approaches, our PIP models build on the Lagrangian multiplier by further incorporating one-step preventative infeasibility masking, while the enhanced version, PIP-D, is trained with a periodically and adaptively updated PIP decoder as previously described. Hyper-parameters for training follow the original settings of the backbone models except for the ones related to the added PIP decoder. Detailed hyper-parameters and additional results are available in Appendix C and D. During inference, we adhere to the settings of the original backbone models. For the AM series models, we sample 1280 solutions per instance; for the POMO series models, we use a greedy strategy with 8\u00d7 augmentation; and for the GFACS series models, we employ 100 ants to generate solutions for each instance over 10 pheromone iterations.\nBaselines. We compare our proposed PIP framework with two types of baselines: 1) heuristic methods, including LKH3 [71], a strong solver designed for multiple VRP variants; OR-Tools [72], a more flexible solver allowing different combinations of multiple diverse constraints; and Greedy Heuristics that selects locally optimal candidates at each step, where Greedy-L picks the nearest candidate and Greedy-C chooses based on complex constraints: in TSPTW, the soonest time window ends relative to the current time; and in TSPDL, the minimal draft limit; 2) Neural methods, including the original AM [4], POMO [5] and GFACS [10], as well as JAMPR [73], adapted by [9] to solve TSPTW from VRPTW; and MUSLA [9], a prior work on TSPTW trained in supervised manner, where OSLA is its one-step version and MUSLA adapt adopts an adaptive inference strategy. More details on the compared baselines are presented in Appendix C.\nEvaluation metrics. In this paper, we report the following metrics to evaluate the performance of our proposed PIP framework: 1) the ratio of infeasible solutions (Infeasible%), which includes the solution-level (Sol.) infeasible rate that considers all generated solutions during inference and the instance-level (Inst.) infeasible rate that considers the comprehensive results of Ns solutions generated by the sampling (Ns = 1,280 in AM series models) or augmentation (Ns = 8 in POMO series models). If at least one feasible solution is found among these Ns solutions, the instance is considered to have feasible solutions; 2) average optimality gap (Gap) w.r.t the strong baseline LKH3 [71] for the best feasible solutions within Ns solutions; 3) average tour length (Obj.) of the feasible best solutions within Ns solutions; and 4) inference time, where we report the total time taken to solve 10,000 (n = 50 and 100) or 128 (n = 500) instances, with batch parallelism enabled on a single GPU. For baselines run in CPU, we exhibit the results in parallel on 16 CPU cores."}, {"title": "Model performance on complex constrained problems", "content": "The performance comparison on TSPTW and TSPDL at various levels of problem hardness is presented in Table 1 and Table 2, respectively. Notably, the original backbone models AM and POMO could not solve the problem even at the easiest level. By incorporating the Lagrangian multiplier (indicated by *), the models begin to generate some feasible solutions. However, this advantage diminishes under more complex constraints. For example, the instance-level infeasibility rates for POMO and AM on Hard TSPTW-100 reach 100% in Table 1, which is dramatically reduced to 6.28% with the addition of PIP-D, while also improving solution quality. Compared to traditional heuristics like ORTools, Greedy-L, and Greedy-C, our PIP-D consistently outperforms these methods and shows favourable results against JAMPR and MUSLA, especially in large-scale problems. Furthermore, compared to PIP, our PIP-D delivers competitive or even better objective values and optimality gaps while significantly enhancing training efficiency (e.g., 1.5 times faster for n = 50 and 5.8 times faster for n = 100, w.r.t POMO* + PIP). Notably, the superiority of PIP-D is more significant on the more constrained hardness levels and larger problem sizes. For TSPDL, we observe similar patterns, where our PIP and PIP-D models consistently outperform other baselines in terms of both infeasibility reduction and solution quality. These results validate that our PIP approach significantly reduces infeasible rates and substantially improves solution quality compared to existing neural methods."}, {"title": "Model performance on large-scale problems", "content": "We further evaluate the capability of solving large- scale problems by implementing our PIP framework on GFACS [10]. As displayed in Table 3, equipping GFACS with our PIP significantly reduces the infeasible rate, for both the solution level and instance level and simultaneously enhances solution quality. Notably, GFACS* + PIP-D almost guarantees to obtain all feasible solutions. Different from AM and POMO, GFACS is a NAR constructive solver, which showcases the generality of our framework."}, {"title": "Further Experiments", "content": "Ablation on each PIP and PIP-D design. We now provide in-depth discussions on the effectiveness of the three proposed designs: the Lagrangian multiplier (*), the PI masking (PIP) and the learnable decoder (PIP-D). As shown in Tables 1 and 2, in Easy datasets, the solution-level infeasible rate for POMO* is 2.11%, improving to 0.06% with PIP-D. This shows that, for less complex constraints, the Lagrangian multiplier alone effectively guides the policy to feasible regions, hedging the impact of PIP and PIP-D, which aligns with Figure 2(b) and (d) where $\u03a0_F$ is relatively large compared to II. However, in more complex scenarios, where $\u03a0_F$ is much smaller relative to II (as in Figure 2(c)), the neural policy struggles even with the Lagrangian multiplier. In such cases, our PIP and PIP-D become crucial, significantly confining the search space as depicted in Figure 2(e). In Medium datasets, the infeasible rate drops from 18.7% in POMO* to 3.34% in POMO* + PIP-D; in Hard datasets, it drops dramatically from 100% in POMO* to 6.48% in POMO* + PIP-D. These results verify that our PIP framework achieves significant improvement, especially as problem complexity increases.\nAblation on the terms in Lagrangian function. In Figure 4, we exhibit the results with and without the TIN in Eq.(3), which validates its efficacy of enhancing the constraint awareness.\nAblation on weighted balancing strategy. Recall that the ratio of infeasible to feasible samples in PIP labels varies with the inherent constraint hardness. Our preliminary experiments suggest that such a ratio can reach up to 20:1 in the case of Hard datasets, causing significant label imbalance. This imbalance may significantly impact the performance of POMO* + PIP-D on several datasets, especially the harder ones, leading to 0% prediction accuracy on the minority class and causing a 100% infeasible rate for the backbone solver without a weighted balancing strategy. This indicates that the hardness-adaptive label balance strategy is essential. Moreover, for the accuracy of PIP-D, please refer to Appendix D.4.\nAblation on periodical update strategy. In Figure 5, we evaluate PIP-D models with fewer updates. Results show that more updates improve performance, despite a slight increase in training time.\nAblation on different step numbers. Instead of iterating over all future possibilities, we use one-step PI masking to approximate NP-hard feasibility mask and reduce computational cost. To provide a"}, {"title": "Conclusions", "content": "In this paper, we study an unsolved challenge in neural VRP solvers and correspondingly propose a novel Proactive Infeasibility Prevention (PIP) framework to advance their capabilities towards addressing VRPs with complex constraints. Technically, we introduce a Lagrangian multiplier method and preventative infeasibility masking to proactively guide the solution construction process. By further incorporating an auxiliary decoder, our PIP framework enhances training efficiency while exhibiting superior performance on more complex datasets. While our PIP is generic and has shown great ability to boost both AR and NAR constructive methods, one potential limitation is that it may not improve performance on all backbone solvers and all VRP variants. Future directions include: 1) exploring other strategies to reduce computational complexity, such as employing a trainable heatmap to confine the candidate space of PI masking calculation, 2) applying PIP to more neural methods at larger scales, 3) extending PIP to neural iterative solvers, 4) applying PIP to more VRP variants with complex constraints, including those hard-constrained VRPs whose feasibility masking is not NP-hard but with large optimality gaps, 5) exploring the applications of PIP in other domains, such as job shop scheduling, where operations need to be completed in a specific order and infeasibility can be proactively prevented using PIP, and 6) developing theoretical justifications for PIP."}, {"title": "Details of considered VRPS", "content": "In this paper", "constraints": "Traveling Salesman Problem with Time Window (TSPTW) and TSP with Draft Limit (TSPDL). We first detail their corresponding data generation process", "properties": 2, "67": "they generate the coordinates following a uniform distribution confined in a square box $(x_i", "100": "while generating time window differently. Concretely", "window": 1, "75": 2, "76": "and 3) generate the time window under a uniform distribution without prior TSP permutation (e.g. in [7, 9", "9": "and employ the third method presented above to generate time window. In specific, the lower bound of the time window l_i follows a uniform distribution, i.e., $l_i \\sim U[0, T_N"}, {"9": ".", "\u03b2": "and \u03b1 and \u03b2 are set to 0.5 and 0.75, respectively.\nMedium TSPTW. It follows the same settings of the easy TSPTW, except that \u03b1 and \u03b2 are set to 0.1 and 0.2, respectively. To decrease \u03b1 and \u03b2, we derive TSPTW instances with tighter time windows, resulting in an increased hardness level"}]}