{"title": "An approach to hummed-tune and song sequences matching", "authors": ["Bao LocPham", "Huong Hoang Luong", "Thien Phu Tran", "Hoang Phuc Ngo", "Hoang Vi Nguyen", "Thinh Nguyen"], "abstract": "Melody stuck in your head, also known as \"earworm\", is tough to get rid of, unless you listen to it again or sing it out loud. But what if you can not find the name of that song? It must be an intolerable feeling. Recognizing a song name base on humming sound is not an easy task for a human being and should be done by machines. However, there is no research paper published about hum tune recognition. Adapting from Hum2Song Zalo AI Challenge 2021 - a competition about querying the name of a song by user's giving humming tune, which is similar to Google's Hum to Search. This paper covers details about the pre-processed data from the original type (mp3) to usable form for training and inference. In training an embedding model for the feature extraction phase, we ran experiments with some states of the art, such as ResNet, VGG, AlexNet, MobileNetV2. And for the inference phase, we use the Faiss module to effectively search for a song that matched the sequence of humming sound. The result comes at nearly 94% in MRR@10 metric on the public test set, along with the top 1 result on the public leaderboard.", "sections": [{"title": "1 Introduction", "content": "Recently, with the development of multimedia on the advance of mobile technology, people listen to music more than ever. People spend most of their time listening to music, while shopping, driving, or studying. As music is listened to more often, we have more \"earworm\" than ever. What if you want to listen to \"that song\" again, how can you find it when you do not know the song's name? To address that issue with current on-the-market products, we got Shazam [22] and Google's Hum to search[21].\nShazam, released in 2002, is an application that allows you to search for a song's name by letting it listen to music sequences. Nevertheless, Shazam[22] has a drawback: it can only search by receiving a song recorded from the studio. With other variants such as remix songs or covers, Shazam does not always guarantee having relative accuracy as the original recorded song, which means when coming to human's humming sound, the program cannot return a correct result.\nGoogle's Hum to search [21], released in 2020, has the ability to return some of the matching songs that come from user's hummed tunes. It returns some of the most likely songs to the humming tune. All popular songs from the 80s to now trending music have really good accuracy.\nAll those mentioned products do not work with Vietnamese songs because the popularity of music genres on the internet is mostly written in English, Spanish, Korean, or Japanese. As a result, Vietnamese people need a search engine for their music, which still can not be fulfilled by Shazam or Google's Hum to search. Fortunately, Zalo - the most popular cross-platform instant messaging application in Vietnam, hosted The Zalo AI Challenge 2021 [19]. In that contest, there was a challenge named Hum2Song, which asked the candidates to develop a Machine Learning model to look for a song using a humming tune. Our purpose for this paper is to present the methodology for solving a music matching problem with the Hum2Song[19] challenge's data.\nThere are some researches for Vietnamese data: [15], [16], [17], [18],etc. All of them focus on the classification, in this paper, we will cover about the Vietnamese music searching pipeline.\nThis paper consists of 5 sections. The next section is about Related works[2]. The Methodology section[3] will cover the details of the data preprocessing, training and inference pipeline. The Experiments section [4] will list all our experiment results. Finally, in the Conclusion section[5], we conclude our paper and summarize other ways to improve or further research for better results."}, {"title": "2 Related Works", "content": "There already some researches for specific tasks in music. There are music classification: [8], [9], [10]. And cover songs identification : [11], [12], [13], etc. Most of them are supervised learning, but labeling data is a time-consuming job. Thankfully, there are many other methods, such as \"contrastive learning\nmethods to train neural networks\" [14]. \"The idea is to make the distance of sequences from the same song close to each other, and sequences from different songs must be far apart.\" [14]\nThere is already a well-known software for music searching - Shazam [22], their method was introduced in \"An Industrial Strength Audio Search Algorithm\" [7]. This software can search for a song, which means it listens to a song sequence and then returns the most related song to that sequence. The query sequences have to be audio recorded songs, which also works with some of the remixes and covers. Shazam[22] \"listens\" to a song, but does not perform very well on hummed tunes.\n\"Hum to Search\" is a feature in Google search whose methodology was introduced in a blog[21] in 2020. This search feature allows you to hum your tune and returns the top likely options based on the tune, and then you can choose what matches the best for you (like a recommended system). However, as mentioned in the Introduction[1], they cannot return accurate results when searching for a Vietnamese song.\nThe research is take place from Zalo AI Challenge 2021 [19]'s first prize solution, the solution was created and published by Wano (a three-member team consisting of Mr Vo Van Phuc, Mr Nguyen Van Thieu, and Mr Lam Ba Thinh) on a Github repository named hum2song[20]."}, {"title": "3 Methodology", "content": "The methodology includes 3 parts: data preprocessing, training embedding model, and inference. The preprocessing of all data for training and inference is only done once. The training embedding models phase is experimented with various state of the arts to find the most sufficient backbone for the task. The inference phase is handled by using Faiss[2] module."}, {"title": "3.1 Data Preprocessing", "content": "Observation about the data\nThe data consists of 3 sets. The training set has 1000 unique song sequences with their unique id within 2901 song sequences along with 2901 hummed tunes, totals of 1.3 GB. The public test used for evaluating on the public leader board has 419 song sequences and 500 hummed tunes, totals of 539MB. The private test used for the final leader board standing has 10153 song sequences and 1067 hummed tune, totals of 12.5GB. You can request the data at Zalo AI Challenge website[19].\nConvert data from mp3 to mel-spectrogram\nAs the data is original in mp3 format, in order to use it, we process all of it into mel-spectrogram and store it in numpy array format for easy training and inference. We convert data to float32 and normalize the sound by dividing by the largest absolute value in the sound. Next, we convert to mel-spectrogram using config of filter length: 1024, hop-length: 256, win-length: 1024. Finally,"}, {"title": "3.2 Training", "content": "Hyperparameters: All of the backbones were trained on 100 epochs, batch size of 32, the convergence of models vary depends on backbone architecture. The loss function used for this training pipeline is ArcFace, which was introduced in \"ArcFace: Additive Angular Margin Loss for Deep Face Recognition\" [1]. The optimizer is Stochastic gradient descent with a learning rate of 1e-2, learning rate decay of 0.5 and a weight decay of le-1."}, {"title": "3.3 Inference", "content": "The inference part contains 3 steps, illustrated in Figure 2:\n\u2022 Step 1: Extract all features from both song sequences.\n\u2022 Step 2: Add song sequences' features to Faiss[2] module to create a vector space of original song sequences.\n\u2022 Step 3: Extract features from the hummed tune and use the Faiss[2] module to query the closest song sequence to the hummed tune.\nThe Faiss[2] module using IndexFlatL2[2] measures the L2 (or Euclidean) distance between all given points between our query vector, and the vectors loaded into the index."}, {"title": "4 Experiments", "content": "After done preprocssing data, We tried on some State of the Art backbones in order to select some of the best candidate for embeddings model based on their performance on public test set and their training time. As the Zalo company's policy, the truth label and the results were only based on the public test, which was only used for the public leader board. Please note that all of the experiments use the same configuration. The metric used for evaluating models' performance is the same as the Zalo AI Challenge, which uses MRR@10 (mean reciprocal rank)."}, {"title": "4.3 Evaluation", "content": "ResNet[3] variants have more training time and As the results shown on the tables above, based on score and training hours trade-off, if it about the mean reciprocal rank or accuracy, ResNet[3] variants and other big networks which create much better feature extraction are recommended. But as the speed of training, which also provides an acceptable score, VGG [4] variants are recommended.\nVGG[4] variants have gradients vanishing problem. To solve that, ResNet[3] variants have \"short cut connection\", and instead of learning the mapping from x \u2192F(x), the network learns the mapping from x \u2192 F(x)+G(x)."}, {"title": "5 Conclusion", "content": "In this paper, we presented our pipeline for preprocessing song sequences and hummed tunes, training embedding models, and using Faiss module to optimize searching for the candidate for a hummed tune. The result on the public leader board is about 90%."}]}