{"title": "BUILDING PRE-TRAIN LLM DATASET FOR THE INDIC\nLANGUAGES: A CASE STUDY ON HINDI", "authors": ["Shantipriya Parida", "Shakshi Panwar", "Kusum Lata", "Sanskruti Mishra", "Sambit Sekhar"], "abstract": "Large language models (LLMs) demonstrated transformative capabilities in many applications that\nrequire automatically generating responses based on human instruction. However, the major challenge\nfor building LLMs, particularly in Indic languages, is the availability of high-quality data for building\nfoundation LLMs. In this paper, we are proposing a large pre-train dataset in Hindi useful for the Indic\nlanguage Hindi. We have collected the data span across several domains including major dialects\nin Hindi. The dataset contains 1.28 billion Hindi tokens. We have explained our pipeline including\ndata collection, pre-processing, and availability for LLM pre-training. The proposed approach can\nbe easily extended to other Indic and low-resource languages and will be available freely for LLM\npre-training and LLM research purposes.", "sections": [{"title": "Introduction", "content": "A certain kind of language model known as pre-trained LLM has been trained on a sizable corpus of text data. Due\nto their capacity to allow machines to comprehend and produce human language, Language Models have grown\nin importance in the field of natural language processing (NLP) Zhao et al. [2023]. Speech recognition, sentiment\nanalysis, machine translation, audio-to-text conversion, and other tasks are all handled by NLP-based systems using\nlanguage models. LLMs go through extensive training and fine-tuning, which can lead to increased dependability and\nperformance. It is simpler to integrate them into current systems and applications because they frequently come with\nwell-documented APIs and integration optionsNaveed et al. [2023].\nDespite the fact that there are over 7,000 languages that are actively spoken worldwide, yet most NLP systems only\nemploy a small number of languages, including English, Chinese, Urdu, Farsi, Arabic, French, and Spanish. Since\nmany cultures are represented and inherited through different languages, there is an increasing interest in creating\nNLP for languages other than English. Text creation, summarization, and language understanding tasks have been\nimpeded by the lack of strong pre-trained language models for non-English languages like Hindi, which has impeded\nthe advancement of the development of NLP applications in these languages. Therefore, to support a variety of NLP\napplications and further the development of multilingual NLP, it is necessary to create more reliable and effective\npre-trained language models for non-English languages. For this purpose, a large dataset is required for training models."}, {"title": "Literature Survey", "content": "Our research article on the Hindi LLM pre-trained model dataset comprises an associated section where we examine\nspecific investigations that make major contributions to the field of large language models and pre-trained model\nadaption to new languages. First off, Zhao et al. [2023] offers a thorough summary of numerous big language models,\ncovering their designs, approaches for training, and uses in a variety of languages. This study provides insightful\ninformation about the state of language modeling, which is essential background knowledge for comprehending the\ngrowth and developments in the area. The important job of adapting pre-trained language models to new languages is\naddressed by Csaki et al. [2023]; this difficulty is especially pertinent to our research on the Hindi LLM model. The\nstudy investigates effective methods for adjusting current models to manage the linguistic subtleties and traits of various\nlanguages.\nThe author Joshi [2022] introduces L3Cube-HindBERT and DevBERT pre-trained models, which symbolize particular\nadvances in the context of Devanagari-based languages, such as Marathi and Hindi. This work is with our goal\nof creating a pre-trained model dataset specifically for the Hindi language since it emphasizes the significance of\nlanguage-specific adaptations in meeting the needs of languages with distinctive scripts and linguistic traits. Altogether,\nthese studies set the foundation and offer insightful information about large language models and how they are tailored\nto different linguistic circumstances, which helps us with our research on the Hindi LLM pre-trained dataset. The\nMuRIL: Multilingual Representations for Indian Languages developed by Khanuja et al. [2021] and discusses the\ndrawbacks of the multilingual language models that are currently in use for Indian languages and presents MuRIL"}, {"title": "Focused Language", "content": "As a member of the Indo-Aryan branch of the Indo-European language family, Hindi is a highly organized language\nPatil et al. [2008]. It is among India's official languages. The subject usually appears at the front of the sentence,\nfollowed by the object and the verb. This subject-object-verb (SOV) word order is what distinguishes the Hindi language\nstructure. However, due to its flexible nature, Hindi allows for variations in word order to emphasize certain elements\nor express different complexities. Hindi has a large lexicon that incorporates words from many different languages,\nincluding English, Persian, Sanskrit, and Arabic. It uses the Devanagari script, a syllabic alphabet with unique letters\nfor vowels and consonants.\nIn terms of grammar, Hindi is an extremely inflected language. Nouns are classified by gender (masculine or feminine),\nnumber (singular or plural), and case (nominative, accusative, dative, genitive, locative, etc.). Adjectives and pronouns\nshare the same gender, number, and case as the nouns they modify. Hindi has a complex system of verb conjugation,\nwith verbs inflected to indicate tense, aspect, mood, person, and number. Like Odia, Hindi makes use of postpositions\nrather than prepositions, which means that these markers are placed after the noun they govern. These postpositions\nplay a crucial role in expressing various relationships and indicate concepts such as location, direction, possession, and\ntime.\nThe process of building the pre-designed LLM dataset involved finding and compiling a variety of textual data from\ndifferent domains, genres, and dialect variations in the Hindi language landscape. This includes literary works, historical\narchives, contemporary discourse, digital content, and other linguistic works that contribute to the richness and diversity\nof the Indian language corpus.\nWith a special focus on Hindi as the primary language of interest, the dataset provides a targeted approach to capture\nunique linguistic features, semantic subtleties, and contextual nuances prevalent in Hindi texts. This targeted research\nenables researchers and practitioners to develop language models and NLP algorithms that match the specific linguistic\nfeatures and requirements of Hindi, thereby increasing their accuracy, flexibility, and applicability in real-world\nscenarios."}, {"title": "Dataset Preparation", "content": "The initial step involves obtaining various texts from different domains, which include news articles, literary works,\nonline content, and social media, among others. It is essential to create a rich and representative dataset that encapsulates\nthe breadth of linguistic nuance and contextual variation present in Hindi."}, {"title": "Data Collection", "content": "It is necessary to compile and select linguistic resources appropriate for Hindi language creation and understanding\ntasks to create datasets for the Hindi language that are specifically designed for pre-trained LLMs. It includes a selection\nof datasets to improve effective pre-training on different content and linguistic nuances and different domains.\nThe first dataset, licensed by cc-by-sa-3.0 from Wikipedia Jones [2017], forms the basis of our corpus. With 43,670,526\ncharacters spread across 1,850,408 sentences, this dataset provides a comprehensive summary of general knowledge and\nlanguage use. The variety of topics and comprehensive coverage make it an invaluable resource for teaching language\nmodels for understanding and creating coherent texts in Hindi.\nComplementing the Dialect Hindi Dataset Bafna [2022], a repository that focuses on language change. This dataset adds\n459,384 characters spread across 63,091 sentences, capturing the complexity of regional language nuances necessary\nfor a robust language model. Covering the vernacular of 26 languages and dialects related to Hindi, the dataset provides\nvaluable linguistic insight into regional variation, dialectal features, and cultural nuances.\nAdditionally, the AI4Bharat/IndicParaphrase dataset Kumar et al. [2022], licensed under cc-by-nc-4.0, adds another\nlayer of linguistic diversity to our corpus. With 55,670,651 astounding numbers distributed across 5,864,552 sentences,\nthis dataset provides a large set of sentences, expanding the dataset to include semantic variations and syntactic\nstructures in Hindi. Completing translation expressions facilitates a deeper study of linguistic semantics and encourages\nthe development of language models that can produce more diverse and contextually relevant results.\nThe Oscar dataset Su\u00e1rez et al. [2019, 2020], available under the CC0-1.0 license, represents a major asset in developing\na robust pre-built dataset that is transparently designed for LLM in Hindi. Boasting a whopping 745,990,971 words and\ncontaining 27,117,459 sentences, this dataset provides unparalleled linguistic data for comprehensive language model\ntraining.\nThe richness of Oscar dataset extends beyond its size; and covers a variety of linguistic sources, including linguistic\nnuances, dialectical changes, and general thematic contexts in the use of Indian languages. This diversity allows for a\ndeeper exploration and understanding of the complexities inherent in the Hindi language, improving the model's ability\nto understand and produce coherent texts across different linguistic domains and styles.\nMiracl-Corpus Zhang et al. [2022], licensed by Apache-2.0, contributed significantly to our dataset by collecting\n33,662,634 items consisting of 2,040,026 sentences. This corpus, licensed under the Apache License, provides a wealth\nof linguistic concepts and domain-specific content, adding valuable diversity to our dataset. Miracl-Corpus increases the\nbreadth of topics and content represented in a pre-trained dataset with a large number and size of vocabulary, improving\nthe model's understanding of the nuances of Indian languages in various domains and linguistic styles.\nIn addition, the addition of the big science/xP3all Muennighoff et al. [2022] dataset further increases the linguistic\ndiversity and domain coverage of our pre-trained dataset. Licensed under the Apache-2.0 license, this dataset contains\n395,323,154 effective numbers distributed over 21,856,987 sentences. A number of linguistic variations, domain-specific\nterminology, and a variety of thematic content increase the flexibility of our dataset. By integrating Bigscience/xP3all,\nwe equip our pre-trained models with language skills necessary for accuracy and adaptability to real-world problems,\nthereby improving their ability to understand and produce coherent texts in Hindi across multiple contexts and domains."}, {"title": "Data Processing", "content": "The initial stage of data processing begins with the exploration of different datasets, each presenting a unique set of\nchallenges and opportunities. From Wikipedia'sJones [2017] knowledge repository to specialized corpora such as Hindi\nDialect Bafna [2022] and the ai4bharat IndicParaphrase dataset Kumar et al. [2022], each source has added a unique\nflavor to the entire tapestry of linguistic information. In addition, the addition of datasets such as Miracl CorpusZhang\net al. [2022], Oscar Su\u00e1rez et al. [2019], Su\u00e1rez et al. [2020], and bigscience/xP3all Muennighoff et al. [2022] have\nadded depth and breadth to the training corpus, enriching it with a variety of text genres, styles, and registers.\nAfter the acquisition, the dataset was intensively developed to standardize its format and improve its suitability for\ntraining large-scale language models. This pre-processing begins by filtering out external metadata such as ID, URL, and\nall kinds of audio and compiling data containing relevant textual content. Special attention is paid to the regularization\nof text elements, including the removal of special characters, punctuation, and number artifacts, thereby creating a\nconsistent and consistent corpus for efficient model training.\nThe culmination of this initial work resulted in the transformation of a raw dataset characterized by two main columns,\nfilename and content, into a coherent and organized form. The former serves as a beacon, clarifying the provenance of\neach document in the corpus, while the latter contains the cleaned textual data content and format. This uniformity not\nonly facilitates seamless integration across different datasets but also provides the basis for the development of robust\nLM models designed to understand and generate natural language text with exceptional fidelity.\nIn addition, advanced techniques such as language processing and domain-specific filtering are used to improve the\nquality and relevance of the training data. This includes identifying and correcting language-related errors, resolving\ntransliteration issues, and filtering out domain-specific arguments or technical terms that may introduce interference or\nbias into the dataset.\nThe culmination of this initial work resulted in a refined and streamlined corpus characterized by uniformity, consistency,\nand relevance. The revised dataset, embellished with detailed textual content, forms the foundation for building an\nadvanced LM model for India. In addition, the pre-processing step not only lays the groundwork for effective\nmodel training, but also facilitates downstream NLP tasks such as text classification, sentiment analysis, and machine\ntranslation."}, {"title": "Analysis and Discussion", "content": "In the Analysis and Discussion section of this paper, we provide a comprehensive overview of various corpora collections\nused in the construction of pre-trained LM datasets."}, {"title": "Domain Coverage", "content": "As the collection of Hindi data spans multiple datasets covering various domains, in this section, we are highlighting\nthe domains covered in our pre-trained Hindi LLM dataset.\nStarting with the Wikipedia dataset Jones [2017], uniqueness includes various general knowledge systems. It serves\nas a treasure trove of information on topics ranging from science and technology to history, culture, and more. The\ndiversity of content in this dataset provides a broad coverage of the humanities, making it an invaluable resource for\nlanguage comprehension problems.\nIn contrast, the Hindi Dialect dataset (HinDialect) Bafna [2022] focuses on regional dialects and local language\nvariations. We reveal the natural linguistic diversity in the dataset by examining the distribution of dialect features and\nlinguistic landscape of Hindi-speaking regions and communities.\nThe HinDialect dataset, with its unique features and contribution to the broader goal of building a pre-built LLM dataset\nfor Indian languages. This HinDialect dataset comprises folksongs from 26 Hindi-related languages and dialects forming\na continuum in North India and nearby regions. These languages include Angika, Awadhi, Baiga, Bengali, Bhadrawahi,\nBhili, Bhojpuri, Braj, Bundeli, Chhattisgarhi, Garhwali, Gujarati, Haryanvi, Himachali, Hindi, Kanauji, Khadi Boli,\nKorku, Kumaoni, Magahi, Malvi, Marathi, Nimadi, Panjabi, Rajasthani, and Sanskrit. The data, originally collected by\nthe Kavita Kosh Project, features languages primarily spoken in North India, with Bengali also spoken in Bangladesh.\nExcept for Korku, all languages are Indic, with most closely related to standard Hindi dialects genealogically.\nThe dataset exclusively uses the Devanagari script. For languages not typically written in Devanagari, such as Bengali\nand Gujarati, content has been transliterated by the Kavita Kosh Project. Each language's data is contained within a\nsingle text file, with folksongs separated by empty lines. The title of each folksong marks the beginning of a new piece,\nwhile line separation within folksongs is maintained.\nBy opening the AI4Bharat IndicParaphrase dataset Kumar et al. [2022], his unique contribution is based on paraphrase\ngeneration and linguistic equivalence. Designed to facilitate the identification of paraphrases, translations, and\ngenealogical research, this dataset consists of pairs of sentences with the same meaning but different linguistic\nformulations. By including various paraphrastic transformations, it leads to advances in machine learning algorithms\nfor natural language understanding and generation."}, {"title": "Use Cases", "content": "Integrating various datasets into a single corpus is a major task in the effort to build a pre-train LLM dataset for the Hindi\nlanguage. This comprehensive dataset, carefully curated from various sources, contains a harmonious combination of\nlinguistic diversity, cultural richness, and thematic depth, thus providing an invaluable resource for developing NLP\ncapabilities in the Indian language landscape.\nPre-training: Large datasets are the main source for training LLM for Indian languages, especially Hindi language.\nBy exposing the model to a variety of linguistic data, the pre-training process allows the model to learn linguistic\npatterns, semantic relationships, and language-specific contextual nuances. These pre-trained models can be well\nadapted to downstream tasks such as sensory analysis, text generation, and machine translation Mujadia et al. [2023],\nthus improving performance and adaptability.\nLanguage Model: The comprehensive dataset facilitates the development of robust language models tailored specifi-\ncally for Hindi and other Indian languages. The dataset's extensive linguistic information may be used for language\nmodeling tasks including text generation, sentence completion, and next-word prediction. By training language mod-\nels on this corpus, researchers and developers can improve the model's understanding of Indian language structure,\nvocabulary, and semantics, making texts more accurate and contextually relevant.\nGenerating synthetic data: The variety of domains, genres, and dialect variations represented in the dataset makes\nit an ideal source for generating synthetic data Roy et al. [2018]. Synthetic data generated from this corpus can be\nused to augment existing datasets, solve data scarcity problems, and increase the power of machine learning models.\nApplications of synthetic data generation include data augmentation to train classifiers, sentence structure generation\nfor data diversity, and generation of artificial training examples for rare linguistic phenomena.\nDomain-Specific Improvement: The dataset covers many domains like literature, history, science, technology,\nand more. Researchers and practitioners can use this domain diversity to tailor pre-engineered language models\nfor specific application domains. For example, in domain-specific tasks, language models can perform better when\nfed domain-specific data from a unified corpus, such as dataset fine-tuning, legal document analysis, medical text\nunderstanding, and financial statement summarization.\nMultilingual NLP Research: Apart from Hindi, the dataset can support multilingual NLP related to other Indian\nlanguages. By extending pre-training and fine-tuning methods to languages such as Bengali, Tamil, Telugu, and Gujarati,\nresearchers can create language models and NLP programs that match the linguistic diversity of the Indian subcontinent.\nThis promotes inclusiveness and accessibility in NLP research and application in various linguistic communities.\nIn total, 1.28 billion tokens serve as a foundation for research, innovation, and application in natural language processing\nfor Indian languages, contributing to the development of language technologies that address the unique linguistic needs\nand challenges of the region."}, {"title": "Availability", "content": "The collected and processed data are available through the Hugging Face:\nhttps://huggingface.co/datasets/Hindi-data-hub/odaigen_hindi_pre_trained_sp\nResearchers and practitioners can freely access the dataset for practice, model training, and further research."}, {"title": "Conclusion and Futurework", "content": "In this research, we describe a complete strategy to addressing the issues of developing large pre-trained language\nmodels (LLMs) for Hindi, with an emphasis on data collecting, preprocessing, and availability. By collecting a broad\nand extensive dataset of 1.28 billion tokens from multiple sources, topics, and dialects, we have created the groundwork\nfor furthering the field of natural language processing (NLP) studies as well as applications. the paper Building a\nPre-Trained LLM Dataset for Hindi Language is an important step in the development of a Hindi-focused, large-scale,\nHindi-specific pre-trained language model (LLM). Comprehensive integration of various datasets and intelligent\npreprocessing efforts lay the foundation for producing reliable and context-aware LLMs. Despite its contribution, the\nstudy recognizes some limitations and areas for future research.\nThe provided dataset provides a comprehensive library of linguistic assets that encapture Hindi's particular subtleties,\ncultural complexity, and thematic depth. We have turned unstructured textual information into a refined dataset sufficient\nfor training strong and context-aware LLMs built specifically for Hindi. The dataset's domain coverage includes a wide\nrange of themes, genres, and language variances namely different dialects and nuances of the vernacular language,\nallowing for the creation of models that are capable of understanding and producing coherent texts across a variety of\nfields and styles.\nIn addition, future work can be explored to refine evaluation criteria and indicators to more comprehensively assess LLM\nperformance. Standardized evaluation procedures and clear benchmarks are needed to effectively compare different\nmodels and approaches. By creating a reliable assessment framework, researchers can gain a deeper understanding of\nthe strengths and weaknesses of LLM trainees, thereby leading to continuous improvement and innovation in the field.\nFuture research can explore ways to increase the availability and accessibility of resources for the wider research\ncommunity. Facilitating easy access to datasets, source code, and pre-engineered models can promote collaboration,\naccelerate research, and democratize access to advanced language processing technologies. In addition, initiatives to\nsupport multilingualism and inclusiveness in the development of language models can contribute to digital inclusion\nand linguistic opportunities in diverse language communities."}, {"title": "Limitation", "content": "While the huge pre-trained LLM dataset described in this study provides substantial prospects for furthering the\nresources available for research opportunities, we need to acknowledge the limitations associated. Despite attempts to\npre-process data, there may still be flaws or inconsistencies that affect the efficacy of trained models. Furthermore,\nbiases inherent in the source data, such as cultural or representational biases, may impact the action patterns of language\nmodels, resulting in unforeseen effects in downstream applications.\n\u2022 Data Accuracy and Prejudice The paper recognizes the complexity of data bias and representation inherent\nin structured datasets. Despite careful efforts to collect diverse linguistic samples, bias can persist which can\nlead to misrepresentation or misrepresentation of certain linguistic and cultural aspects. These constraints\nmay impede the generalization and implementation of trained LLMs, particularly in real-world contexts\ncharacterized by linguistic diversity, because structured datasets may provide insufficient coverage of linguistic\nchange.\n\u2022 Addressing Linguistic Diversity and Evaluation Criteria Although this paper emphasizes the importance of\nlinguistic diversity, it may not be sufficiently represented in some dialects, registers, or domains. As a result,\nLLMs trained in such information may struggle to accurately understand and produce texts across all the\nlinguistic variations that exist in the Indian language landscape. In addition, the paper can benefit from further\nclarification on the evaluation criteria and indicators used to evaluate LLM performance. A standardized\nevaluation procedure is needed to compare the effectiveness of different models and approaches in the field.\nClear standards will allow you to understand the strengths and shortcomings of an LLM.\n\u2022 Language Diversity and Domain Coverage While the dataset seeks to capture a wide range of linguistic\ncontent, it may not fully represent the breadth of language. Certain dialects, vernacular nuances, or specialized\ndomains may be undervalued or absent from the dataset, restricting the applicability of trained models to\na wide range of linguistic settings. Furthermore, the dataset's inclusion of specific areas or topics may be\nunequal, thereby biasing model performance.\n\u2022 Resource Constraints in Developing The development and upkeep of a large pre-trained LLM dataset\nnecessitate enormous resources for data gathering, preparation, storage, and computational infrastructure.\nWhile attempts have been undertaken to put together and consolidate the dataset, resource restrictions may\nlimit its scalability and accessibility to researchers as well as practitioners, especially those working in\nresource-constrained situations.\n\u2022 Ethical and Privacy Considerations The collection and use of huge amounts of textual data raises serious\nethical and privacy concerns, specifically about user authorization, data anonymization, and the potential\nexploitation of sensitive information. While attempts have been made to follow ethical norms and data\nprotection guidelines, extra caution and transparency in managing and securing the information is required to\nguarantee that privacy rights and ethical values are upheld."}]}