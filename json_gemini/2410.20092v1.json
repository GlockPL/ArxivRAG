{"title": "OGBENCH: BENCHMARKING OFFLINE GOAL-CONDITIONED RL", "authors": ["Seohong Park", "Kevin Frans", "Benjamin Eysenbach", "Sergey Levine"], "abstract": "Offline goal-conditioned reinforcement learning (GCRL) is a major problem in reinforcement learning (RL) because it provides a simple, unsupervised, and domain-agnostic way to acquire diverse behaviors and representations from unlabeled data without rewards. Despite the importance of this setting, we lack a standard benchmark that can systematically evaluate the capabilities of offline GCRL algorithms. In this work, we propose OGBench, a new, high-quality benchmark for algorithms research in offline goal-conditioned RL. OGBench consists of 8 types of environments, 85 datasets, and reference implementations of 6 representative offline GCRL algorithms. We have designed these challenging and realistic environments and datasets to directly probe different capabilities of algorithms, such as stitching, long-horizon reasoning, and the ability to handle high-dimensional inputs and stochasticity. While representative algorithms may rank similarly on prior benchmarks, our experiments reveal stark strengths and weaknesses in these different capabilities, providing a strong foundation for building new algorithms.", "sections": [{"title": "1 MOTIVATION", "content": "Why offline goal-conditioned reinforcement learning (RL)? The enduring trend in modern machine learning is to simplify domain-specific assumptions and scale up the data. In computer vision and natural language processing, the strongest general-purpose models are trained via simple unsupervised objectives on raw, unlabeled data, such as next-token prediction, contrastive learning, and masked auto-encoding. What analogous paradigm could enable data-driven unsupervised learning for reinforcement learning? Ideally, such a framework should be able to produce from data a generalist policy that can be directly queried or adapted to solve a variety of downstream tasks, much like how generative language models trained via next-token prediction can be easily adapted to everyday tasks.\nWe posit that a natural analogy to data-driven unsupervised learning in RL is offline goal-conditioned RL (GCRL). The objective of offline goal-conditioned RL is fully unsupervised, remarkably simple, and requires no domain knowledge: it merely aims to learn to reach any state from any other state in the dataset in the fewest number of steps. However, mastering this simple objective is exceptionally difficult: the agent needs to not only acquire diverse skills to efficiently navigate the state space, but also have a deep, complete understanding of the underlying world and dataset. As a result, offline goal-conditioned RL yields a highly capable general-purpose multi-task policy as well as rich, useful representations that can be adapted to solve a variety of downstream tasks. Indeed, for such simplicity and generality, interest in (offline) goal-conditioned RL has recently surged to the extent that even a standalone workshop on goal-conditioned RL was held at a machine learning conference.\nWhy a new benchmark? Despite the importance of and increasing interest in offline goal-conditioned RL, we currently lack a standard benchmark that can systematically assess the capabilities of offline GCRL algorithms, such as the ability to stitch, perform long-horizon reasoning, and handle stochasticity. Prior works in offline goal-conditioned RL have mainly used either existing datasets for standard offline RL tasks without modification (e.g., D4RL), relatively simple goal-conditioned tasks (e.g., Fetch), or tasks tailored to demonstrate the individual abilities of the proposed methods. This often results in limited evaluation. Prior works often evaluate their multi-task policies only on a single task (when using datasets not originally designed for offline GCRL), or learn relatively simple behaviors. While there exist some prior tasks tailored to evaluate individual properties of offline GCRL such as stitching or generalization, we lack a comprehensive, standardized benchmark that exhaustively assesses various properties of offline GCRL algorithms with diverse, challenging tasks.\nTherefore, we introduce a benchmark named Offline Goal-Conditioned RL Benchmark (OGBench) in this work. The primary goals of this benchmark are to facilitate algorithms research in offline goal-conditioned RL and to provide a set of complex tasks that can unlock the potential of offline GCRL. Our benchmark introduces 8 types of environments and 85 datasets across robotic locomotion, robotic manipulation, and drawing, and provides well-tuned reference implementations of 6 representative offline GCRL methods. These datasets, tasks, and implementations are carefully designed. Tasks are designed in a way that complex behaviors can naturally emerge when successfully solved, and that they pose diverse algorithmic challenges in offline GCRL, such as goal stitching, stochastic control, long-horizon reasoning, and more. Dataset and task difficulties are carefully adjusted to highlight stark contrasts between algorithms across multiple criteria. The entire benchmark is designed to minimize unnecessary computational overhead and maximize usability such that any researcher can easily iterate and evaluate new ideas. We believe OGBench serves as a solid foundation for developing ideal algorithms for goal-conditioned and unsupervised RL from data."}, {"title": "2 PROBLEM SETTING", "content": "The offline goal-conditioned RL problem is defined by a controlled Markov process \\(M = (S, \u0391, \u03bc, \u03c1)\\) (i.e., a Markov decision process (MDP) without rewards) and an unlabeled dataset D, where S denotes the state space, A denotes the action space, \\(\u03bc(8) \u2208 \u2206(S)\\) denotes the initial state distribution, and \\(p(s' | s, a): S \u00d7 A \u2192 \u2206(S)\\) denotes the transition dynamics function. \u0394(X) denotes the set of probability distributions defined on a set X. The dataset \\(D = {\\\u03c4^{(n)}\\}_{n\u2208{1,2,...,N}}\\) consists of unlabeled trajectories \\(\u03c4^{(n)} = (s_0^{(n)}, a_0^{(n)}, s_1^{(n)},..., s_{T^{(n)}}^{(n)})\\).\nThe objective of offline goal-conditioned RL is to learn to reach any state from any other state in the minimum number of time steps. Formally, offline GCRL aims to learn a goal-conditioned policy \\(\u03c0(\u03b1 | s, g): S \u00d7 S \u2192 \u2206(A)\\) that maximizes the objective \\(E_{\u03c4\u223cp(\u03c4 | g)} [\u2211_{t=0}^{T}\u03b3^t\u03b4_g(s_t)]\\) for all \\(g \u2208 S\\), where \\(T \u2208 N\\) denotes the episode horizon, \\(\u03b3 \u2208 (0, 1)\\) denotes the discount factor, \\(p(\u03c4 | g)\\) denotes the distribution given by \\(p(\u03c4 | g) = \u03bc(s_0)\u03c0(\u03b1_0 | s_0, g)p(s_1 | s_0, a_0)\u2026\u2026p(s_T | s_{T\u22121}, a_{T-1})\\), and \\(\u03b4_g(\u00b7)\\)"}, {"title": "3 HOW HAVE PRIOR WORKS BENCHMARKED OFFLINE GCRL?", "content": "While many excellent offline GCRL algorithms have been proposed so far, the community currently lacks a standardized way to evaluate their performance, unlike other fields in RL. Tasks used by prior works in offline GCRL are often limited for providing a proper evaluation for various reasons. For example, many works directly use tasks in existing offline RL benchmarks (not necessarily designed for offline goal-conditioned RL), such as D4RL AntMaze and Kitchen. However, since the tasks are designed for single-task offline RL, they often evaluate their multi-task policies only on the single, original goal when using these tasks, which results in limited evaluation. Some employ online GCRL tasks provided by Plappert et al. (2018) (e.g., Fetch tasks) with policy-collected datasets, but these tasks are mostly atomic (e.g., single pick-and-place) and do not sufficiently address various challenges in offline GCRL, such as long-horizon reasoning and goal stitching. While Roboverse provides pixel-based manipulation tasks, the tasks are still relatively atomic and it requires installing multiple, fragmented dependencies, making it less approachable for researchers. Some prior works construct bespoke tasks and datasets to individually study the important and specific features of their algorithms (e.g., stitching and stochasticity), but it is often not entirely clear how algorithms compare to one another or if the new capabilities of new methods (e.g., stitching) come at the loss of other capabilities. These limitations of previous evaluation tasks have motivated us to create a new benchmark. In this work, we introduce a set of diverse tasks that cover various challenges in offline GCRL, enabling a much more thorough and multi-faceted evaluation than previous tasks. We summarize the properties of the previous tasks and our new tasks in Table 1 and refer to Appendix B for further discussion on related work."}, {"title": "4 OVERVIEW OF OGBENCH", "content": "We now introduce our benchmark, Offline Goal-Conditioned RL Benchmark (OGBench). The primary goal of this benchmark is to provide tasks and datasets to unlock the full potential of offline goal-conditioned RL. To this end, we pose diverse challenges in offline GCRL throughout the benchmark in such a way that researchers can easily test and iterate on algorithmic ideas, and that complex, intriguing behaviors can naturally emerge when successful. OGBench consists of 8 types of environments, 85 datasets, and reference implementations of 6 representative offline GCRL algorithms. In the following sections, we first describe the challenges in offline GCRL (Section 5) and then outline our core design philosophies (Section 6). We next introduce the tasks and datasets (Section 7) and present the benchmarking results of the current algorithms (Section 8)."}, {"title": "5 CHALLENGES IN OFFLINE GOAL-CONDITIONED RL", "content": "Offline goal-conditioned RL, despite its simplicity, is a challenging problem. Here, we discuss the major challenges in offline GCRL, which will motivate the design choices in our benchmark tasks.\n(1) Learning from suboptimal, unstructured data: An ideal offline GCRL algorithm should be able to learn an effective multi-task policy from diverse and suboptimal data. This is especially important, considering that suboptimal (yet diverse) data is much cheaper to collect than curated, expert datasets and that the very use of large, diverse, unstructured data is one of the foundations for the success of modern machine learning. Reflecting this challenge, we provide datasets with high diversity and varying suboptimality in this benchmark to challenge the capabilities of offline GCRL algorithms.\n(2) Goal stitching: Another important challenge is to stitch the initial and final states of different trajectories to learn more diverse behaviors. We call this \u201cgoal stitching.\u201d Goal stitching is different from \"regular\" stitching in offline RL, which applies only when the dataset is suboptimal. Unlike regular stitching, goal stitching applies even when the dataset only consists of optimal, expert trajectories, because we can often acquire more diverse goal-reaching behaviors by stitching multiple trajectories together, regardless of their optimality. For instance, an agent can stitch two atomic pick-and-place behaviors to sequentially move two objects in a single episode, even when the dataset does not contain any double pick-and-place behaviors. Goal stitching is crucial to learning diverse behaviors in many real-world applications with high behavioral diversity and large state spaces. In our benchmark, we introduce many tasks with large state spaces to assess the ability to perform goal stitching.\n(3) Long-horizon reasoning: Long-horizon reasoning refers to the capability of navigating from a starting state to a goal state that is many steps apart. This challenge is important in many real-world tasks like autonomous driving or assembly, which may require several hours of continuous control or achieving dozens of subtasks. To substantially challenge the long-horizon reasoning ability of offline GCRL methods, we introduce tasks that are more than 5 times longer than previously used ones in terms of both the episode length and the number of subtasks (Table 1).\n(4) Handling stochasticity: Another prominent challenge in offline GCRL is the ability to deal with stochastic environments. Correctly handling stochasticity is very important in practice, because virtually any real-world environment is stochastic due to partial observability. Yet, many works in offline GCRL assume deterministic dynamics to exploit the metric structure of temporal distances or to enable hierarchical control, at the cost of being optimistically biased in stochastic environments. Correctly handling environment stochasticity while fully exploiting the recursive subgoal structure of GCRL remains an open problem. Since most previous tasks used to evaluate offline GCRL methods have deterministic dynamics (Table 1), we introduce several challenging tasks with stochastic dynamics in this benchmark."}, {"title": "6 DESIGN PRINCIPLES", "content": "Next, we discuss the design principles underlying our benchmark tasks. The tasks are intended to exercise the major challenges in offline GCRL in the previous section, while providing a set of high-quality tasks that provide not only a toolkit for algorithms research and evaluation, but also a platform for vividly illustrating the capabilities of offline GCRL with compelling and complex domains.\n(1) Realistic and exciting tasks: The tasks should be realistic yet exciting enough while posing diverse challenges in offline goal-conditioned RL. Imagine a robot arm watching random movements of a puzzle and then solving it zero-shot at test time, a humanoid robot navigating through a labyrinth, or an agent painting cool pictures using different types of brushes. In this benchmark, we design new tasks such that these kinds of exciting behaviors can naturally emerge when an RL agent properly stitches different trajectory segments together (up to 24; see Table 1) or successfully generalizes. At the same time, we make sure our tasks exhaustively cover major challenges in offline goal-conditioned RL, such as long-horizon reasoning, stochastic control, and combinatorial generalization via goal stitching (Section 5).\n(2) Appropriate difficulty: The tasks and datasets should have appropriate levels of difficulty to properly evaluate different algorithms. In other words, they should be of high quality for benchmarking. No matter how intricate or compelling a task is, it will fail to provide a useful signal for benchmarking if it is too easy, too hard, unsolvable from the given dataset, or does not clearly distinguish between more or less effective methods. In this work, we carefully curate and adjust the difficulty of each task and dataset such that they can provide effective guidance for algorithms research. For some tasks, we provide multiple versions with varying difficulty, all the way up to tasks that are difficult to solve with current methods, so that the same benchmark can continuously be used to develop new methods even in the future. Our rule of thumb is to have, for each type of tasks, at least one task where the current state-of-the-art offline GCRL method achieves a success rate of 20-30%. This ensures that the task is solvable from the dataset while leaving significant room for improvement.\n(3) Controllable datasets: The benchmark should provide tools to control and adjust datasets for scientific research and ablation studies. Verifying the effectiveness of algorithms in real-world problems is surely important. However, for algorithms research, it is equally, if not more, important to provide analysis tools to enable a rigorous, scientific understanding of challenges and algorithms. Hence, instead of employing fixed, human-collected data, which does not always provide clear benchmarking signals for algorithms research, we choose to focus on simulated environments and synthetic datasets that we have full control of, and provide tools to reproduce and adjust them easily. We note that many algorithmic ideas in RL that have made a major impact, such as DQN, PPO, and CQL, were originally developed in simulated environments. Even in natural language processing, studies on synthetic, controlled datasets have revealed the mechanisms and limitations of language models with scientific evidence, and these insights have transferred to real scenarios. We demonstrate how such controllability of datasets reveals challenges and design principles in offline GCRL in Section 8.2.\n(4) Minimal compute requirements: The tasks should be designed to minimize unnecessary computational overhead so that as many researchers as possible, including those from small labs and underprivileged backgrounds, can quickly iterate on their new algorithmic ideas. This does not mean that the tasks should be easy (indeed, some of our tasks are very challenging (but solvable)!); it means the benchmark should focus mainly on algorithmic challenges (e.g., not requiring high-resolution image processing). In our benchmark, we provide both state- and pixel-based observations whenever possible, and minimize the size of image observations (up to 64 \u00d7 64 \u00d7 3) to reduce the computational burden. Moreover, we carefully adjust colors, transparency, and lighting for image-based tasks to enable pixel-based control without high-resolution images or multiple views.\n(5) High code quality: The reference implementations should be very clean and well-tuned so that researchers can directly use our implementations to build their ideas, and the benchmark should be very easy to set up. Our benchmark environments only depend on MuJoCo and do not require any other dependencies (Table 1). For reference implementations, we minimize the number of file dependencies for each algorithm, largely following the spirit of the single-file implementations of recent RL libraries, while maintaining a minimal amount of additional modularity. We also extensively test and tune different design choices and hyperparameters for each offline GCRL algorithm, to the degree that several methods achieve even better performances than their original performances reported on previous benchmarks (Table 3)."}, {"title": "7 ENVIRONMENTS, TASKS, AND DATASETS", "content": "We now introduce the environments, tasks, and datasets in our benchmark. They can be broadly categorized into three groups: locomotion, manipulation, and drawing. Most tasks support both state- and pixel-based observations, and each task accompanies five pre-defined state-goal pairs for evaluation."}, {"title": "7.1 LocOMOTION TASKS", "content": "We provide four types of locomotion environments, PointMaze, AntMaze, HumanoidMaze, and AntSoccer, with diverse variants. These environments are designed to test the agent's long-horizon and hierarchical reasoning abilities. They are based on the MuJoCo simulator.\nPointMaze (pointmaze), AntMaze (antmaze), and HumanoidMaze (humanoidmaze). Maze navigation is one of the most widely used tasks for benchmarking offline GCRL algorithms. We provide three different types of maze navigation tasks: PointMaze, which involves controlling a 2-D point mass, AntMaze, which involves controlling a quadrupedal Ant agent with 8 degrees of freedom (DoF), and HumanoidMaze, which involves controlling a much more complex 21-DoF Humanoid agent. The aim of these tasks is to control the agent to reach a goal location in the given maze. The agent must learn both the high-level maze navigation and low-level locomotion skills that involve high-dimensional control, purely from diverse offline trajectories.\nIn our benchmark, we substantially extend the original PointMaze and AntMaze tasks proposed by D4RL. Unlike the original D4RL tasks, which only support Point and Ant agents and do not challenge stitching, stochasticity, or pixel-based control, we support Humanoid control, pixel-based observations, and multi-goal evaluation, while providing more challenging and diverse types of mazes and datasets. The supported maze types are as follows:\n\u2022 medium: This is the smallest maze, with the same layout as the original medium maze in D4RL.\n\u2022 large: This is a larger maze, with the same layout as the original large maze in D4RL.\n\u2022 giant: This is the largest maze, twice the size of large. It has the same size as the previous antmaze-ultra maze by Jiang et al. (2023), but its layout is more challenging and contains longer paths that require up to 3000 environment steps (in the case of Humanoid). This maze is designed to substantially challenge the long-horizon reasoning capability of the agent.\n\u2022 teleport: This maze is specially designed to challenge the agent's ability to handle environment stochasticity. It has the same size as large, but contains multiple stochastic teleporters. If the agent enters a black hole, it is immediately sent to a randomly chosen white hole. However, since one of the three white holes is a dead-end, there is always a risk in taking a teleporter. The agent therefore must learn to avoid the black holes, without being optimistically biased by \"lucky\" outcomes.\nOn these mazes, we collect datasets with a low-level directional policy trained via SAC and a high-level waypoint controller. For each maze type, we provide three types of datasets that pose different kinds of challenges (the figures below show example trajectories in these datasets):\n\u2022 navigate: This is the standard dataset, collected by a noisy expert policy that navigates the maze by repeatedly reaching randomly sampled goals.\n\u2022 stitch: This dataset is designed to challenge the agent's stitching ability. It consists of short goal-reaching trajectories, where the length of each trajectory is at most 4 cell units. Hence, the agent must be able to stitch multiple trajectories (up to 8) together to complete the tasks.\n\u2022 explore: This dataset is designed to test whether the agent can learn navigation skills from extremely low-quality (yet high-coverage) data. It consists of random exploratory trajectories, collected by commanding the low-level policy with random directions re-sampled every 10 steps, with a large amount of action noise.\nWe provide two types of observation modalities:\n\u2022 States: This is the default setting, where the agent has access to the full low-dimensional state representation, including its current x-y position.\n\u2022 Pixels (visual): This requires pure pixel-based control, where the agent only receives 64\u00d764\u00d73 RGB images rendered from a third-person camera viewpoint. Following Park et al. (2023), we color the floor to enable the agent to infer its location from the images, obviating the need for a potentially expensive memory component. However, unlike Park et al. (2023), which additionally provides proprioceptive information, we do not provide any low-dimensional state information like joint angles; the agent must learn purely from image observations.\nAntSoccer (antsoccer). To provide a more diverse type of locomotion task beyond simple maze navigation, we introduce a new locomotion task, AntSoccer. This task involves controlling an Ant agent to dribble a soccer ball. It is inspired by the quadruped-fetch task in the DeepMind Control suite. AntSoccer is significantly harder than AntMaze because the agent must also carefully control the ball while navigating the environment. We provide two maze types: arena, which is an open space without walls, and medium, which is the same maze as the medium one in AntMaze. For datasets, we provide navigate and stitch. The navigate datasets consist of trajectories where the agent repeatedly approaches the ball and dribbles it to random locations. The stitch datasets consist of two different types of trajectories, maze navigation without the ball and dribbling with the ball near the agent, so that stitching is required to complete the full task. AntSoccer only supports state-based observations."}, {"title": "7.2 MANIPULATION TASKS", "content": "We provide a manipulation suite with three types of robotic manipulation tasks, Cube, Scene, and Puzzle, with diverse difficulties and complexities. They are designed to test the agent's object manipulation, sequential generalization, and combinatorial generalization abilities. These environments are based on the MuJoCo simulator and a 6-DoF UR5e robot arm. On these tasks, we provide \"play\"-style datasets collected by non-Markovian expert policies with temporally correlated noise. To support more diverse types of research (e.g., dataset ablation studies), we additionally provide more noisy datasets collected by Markovian expert policies with uncorrelated Gaussian noise, which we describe in Appendix A.\nFor all manipulation tasks, we support both state-based observations and pixel-based observations with 64 x 64 \u00d7 3 RGB camera images. For pixel observations, we adjust colors and make the arm transparent to ensure full observability. The transparent arm in the figure might appear challenging, but the colors and transparency are carefully adjusted to minimize difficulties in visual perception, to the extent that some methods achieve even better performance with pixels (see Table 2).\nCube (cube). This task involves pick-and-place manipulation of cube blocks, whose goal is to control a robot arm to arrange cubes into designated configurations. We provide four variants, single, double, triple, and quadruple, with different numbers (1-4) of cubes. We provide \"play\"-style datasets collected by a scripted policy that repeatedly picks a random block and places it in other random locations or on another block. At test time, the agent is given goal configurations that require moving, stacking, swapping, or permuting cube blocks. Hence, the agent must learn not only generalizable multi-object pick-and-place behaviors from unstructured random trajectories in the dataset, but also long-term plans to achieve the tasks (e.g., permuting blocks requires non-trivial sequential and logical reasoning).\nScene (scene). This task is designed to challenge the sequential, long-horizon reasoning capabilities of the agent. It involves manipulating diverse everyday objects, such as a cube block, a window, a drawer, and two button locks, where pressing a button toggles the lock status of the corresponding object (the drawer or window). We provide \u201cplay\u201d-style datasets collected by scripted policies that randomly interact with these objects. At test time, the agent is commanded to arrange the objects into a desired configuration. Evaluation tasks require a significant degree of sequential reasoning: for instance, some tasks require unlocking the drawer, opening it, putting the cube in the drawer, and closing it again (see the figure above), and the longest task involves eight atomic behaviors. Hence, the agent must be able to plan and sequentially combine learned manipulation skills.\nPuzzle (puzzle). This task is designed to test the combinatorial generalization abilities of the agent. It requires solving the \u201cLights Out\u201d puzzle with a robot arm. The puzzle consists of a two-dimensional array of buttons (e.g., a 4 \u00d7 6 grid), where pressing a button toggles the colors of the pressed button and the buttons adjacent to it (typically four, except on the edges and corners; see videos). The goal is to achieve a desired configuration of colors (e.g., turning all the buttons blue) by pressing an appropriate combination of buttons. Since these buttons are implemented in the MuJoCo simulator, the agent must control a robot arm to physically press the buttons. We provide four levels of difficulty, 3x3, 4x4, 4x5, and 4x6, with different grid sizes. The datasets are collected by a scripted policy that randomly presses buttons in arbitrary sequences. Given the enormous state space of this task (with up to \\(2^{24} = 16,777,216\\) distinct button states), the agent must achieve combinatorial generalization while mastering low-level continuous control. Some evaluation task in the hardest puzzle requires pressing more than 20 buttons, which also substantially challenges the long-horizon reasoning capabilities of the agent. This might sound very challenging (and it is!), but we provide different levels and enough data to ensure that they provide meaningful research signals and are solvable (see the results in Section 8)."}, {"title": "7.3 DRAWING TASKS", "content": "Powderworld (powderworld). To provide more diverse tasks beyond robotic locomotion or manipulation, we introduce a drawing task, Powderworld, which presents unique challenges with extremely high intrinsic dimensionality. The goal of Powderworld is to draw a target picture on a 32 \u00d7 32 grid using different types of \"powder\" brushes, where each powder brush has a distinct physical property corresponding to a unique element. For example, the \u201csand\u201d brush falls down and piles up, and the \u201cfire\u201d brush burns combustible elements like \u201cplant.\" We provide three versions of tasks, easy, medium, and hard, with different numbers of available elements (2, 5, and 8 elements, respectively). The datasets are collected by a random policy that keeps drawing arbitrary shapes with random brushes. This Powderworld task poses unique challenges that are distinct from the other tasks in the benchmark. First, the agent must deal with the high intrinsic dimensionality of the states, which presents a substantial challenge in representation learning. Second, since the transitions of powder elements are mostly stochastic and unpredictable, the agent must be able to correctly handle environment stochasticity. Third, the agent must achieve a high degree of generalization and sequential reasoning through a deep understanding of the physics, in order to complete symmetrical, orderly test-time drawing tasks from random, chaotic data."}, {"title": "8 RESULTS", "content": "We now present and discuss the benchmarking results of existing offline goal-conditioned RL algorithms on OGBench."}, {"title": "8.1 ALGORITHMS", "content": "We benchmark six representative offline GCRL algorithms: goal-conditioned behavioral cloning (GCBC), goal-conditioned implicit {V, Q}-learning (GCIVL and GCIQL), quasimetric RL (QRL), contrastive RL (CRL), and hierarchical implicit Q-learning (HIQL). GCBC is the simplest goal-conditioned behavioral cloning method. GCIVL and GCIQL are offline GCRL algorithms that approximates the optimal value function using an expectile regression. QRL is a non-traditional GCRL method that fits a quasimetric value function with a dual objective. CRL is a \"one-step\" RL algorithm that fits a Monte Carlo value function via contrastive learning and performs one-step policy improvement. HIQL is a hierarchical RL method that extracts a two-level hierarchical policy from a single GCIVL value function. For benchmarking, we perform a similar amount of hyperparameter tuning for each method to ensure fair comparison. We refer the reader to Appendices C and D for the details."}, {"title": "8.2 BENCHMARKING RESULTS AND Q&AS", "content": "We present the full benchmarking results in Table 2. Figure 2 summarizes the results by showing performances grouped by different task categories. Performances are measured by average (binary) success rates on the five test-time goals of each task. We train the agents for 1M gradient steps (500K for pixel-based tasks), and average the results over 8 seeds (4 seeds for pixel-based tasks). We discuss the results through Q&As.\nQ: Which method works best in general?\nA: While no single method dominates the others across all categories in Figure 2, HIQL (a method that involves hierarchical policy extraction) tends to achieve particularly strong performance among the benchmarked methods, especially in locomotion and visual manipulation tasks. Among the non-hierarchical methods, CRL tends to work best in locomotion tasks and GCIQL tends to work best in manipulation tasks. In the drawing tasks, GCIVL performs the best.\nQ: Which methods are good at goal stitching?\nA: To see this, we can compare the performances on the navigate and stitch datasets from the same locomotion task in Table 2. The results suggest that, as expected, full RL-based methods like HIQL (i.e., methods that fit the optimal value function Q*) are better at stitching than one-step RL methods like CRL (i.e., methods that fit the behavioral value function QB). For example, in visual locomotion tasks, the relative performance between HIQL and CRL is reversed on the stitch datasets (Figure 2).\nQ: Which methods are good at handling stochasticity?\nA: For this, we can compare the performances on the large and teleport mazes in Table 2, where both have the same maze size, but only the latter involves stochastic transitions that incur risk. Table 2 shows that, value-only methods like HIQL and QRL (i.e., methods that do not have a separate Q function), which are optimistically biased in stochastic environments, struggle relatively more in stochastic teleport tasks. In contrast, CRL is generally robust to environment stochasticity, likely because it fits a Monte Carlo value function.\nQ: Which methods are good at handling pixel-based observations?\nA: Although state-based and pixel-based observations generally provide the same amount of information, several methods struggle to handle image observations due to additional representational challenges. We can understand how well a method addresses such representational challenges by comparing the performances of corresponding state- and pixel-based tasks. Table 2 shows that CRL is notably robust to the difference in input modalities, likely because it is based on a pure representation learning objective. HIQL also achieves strong performance in pixel-based tasks, especially in visual manipulation tasks. However, these methods are still not perfect at handling image observations; for example, HIQL achieves relatively weak performance on image drawing tasks. We suspect this is due to the difficulty of learning low-dimensional subgoal representations from states of high intrinsic dimensionality.\nQ: How good are our reference implementations?\nA: We compare the performance of our reference implementations with previously reported numbers on one of the most commonly used tasks in prior work, D4RL antmaze-large (Fu et al., 2020). Table 3 shows the comparison results, with the corresponding numbers taken from the prior works. The results suggest that our implementations generally achieve better performance than previously reported results, sometimes significantly surpassing them (e.g., CRL).\nQ: Why can't I just use D4RL AntMaze instead of OGBench AntMaze for offline GCRL?\nA: D4RL AntMaze is an excellent task for benchmarking offline RL algorithms. However, it is limited for benchmarking offline goal-conditioned RL algorithms because it only involves a single, fixed state-goal pair, and the datasets are tailored to this specific task. In contrast, OGBench supports multi-goal evaluation (and provides much more diverse types of tasks and datasets!). To empirically demonstrate this difference, we compare the benchmarking results on D4RL antmaze-large-{diverse, play} and OGBench antmaze-large-navigate in Table 4. The table suggests that single-goal evaluation is indeed limited, and is potentially prone to inaccurate conclusions: for example, see how the ranking between GCIQL and QRL is reversed with multi-goal evaluation on the same antmaze-large maze. Moreover, the performance differences between methods are more pronounced in OGBench AntMaze, showing that OGBench provides clearer research signals.\nQ: There seem to be a lot of datasets. What should I use for my research?\nA: For general offline GCRL algorithms research, we recommend starting with more \u201cregular\" datasets, such as antmaze-{large, giant}-navigate, humanoidmaze-medium-navigate, cube-{single, double}-play, scene-play, and puzzle-3x3-play. From there, depending on the performance on these tasks, try harder versions of them or more challenging tasks, such as humanoidmaze-giant, antsoccer, puzzle-{4x4, 4x5, 4x6}, and powderworld.\nWe also provide more specialized datasets that pose specific challenges in offline GCRL (Section 5). For stitching, try the stitch datasets in the locomotion suite as well as complex manipulation tasks that require stitching (e.g., puzzle). For long-horizon reasoning, consider humanoidmaze-giant, which has the longest episode length, and puzzle-4x6, which has the most semantic steps. For stochastic control, try antmaze-teleport, which is specifically designed to challenge optimistically biased methods, and powderworld, which has unpredictable, stochastic dynamics. For learning from highly suboptimal data, consider antmaze-explore as well as the noisy datasets in the manipulation suite, which features high suboptimality and high coverage.\nQ: Have you found any insights on data collection for offline GCRL?\nA: One of the main features of OGBench is that every task is accompanied by a reproducible and controllable data-generation script. Here, we show one example of how this controllability can lead to practical insights and raise open research questions. In Figure 3, we ablate the strength of Gaussian action noise \u03c3 added to expert actions on two manipulation tasks (cube-single-noisy and puzzle-3x3-noisy), and measure how this affects performance.\nThe results are quite remarkable: they show that having the right amount of noise (i.e., state coverage) is extremely important for achieving good performance. For example, the performance drops from 99% to 6% if there is no noise in expert actions, even on the most basic cube pick-and-place task. This suggests that, we may need to prioritize coverage much more than optimality when collecting datasets for offline GCRL in the real world as well, and failing to do so may lead to (surprising) failures in learning. Like action noise, we believe there are many other important properties of datasets that significantly affect performance. We believe that our fully transparent, controllable data-generation scripts can facilitate such scientific studies."}, {"title": "9 RESEARCH OPPORTUNITIES", "content": "In this section, we discuss potential research ideas and open questions.\nBe the first to solve unsolved tasks! While all environments in OGBench have at least one variant that current methods can solve to some degree, there are still a number of challenging tasks on which no existing method achieves non-trivial performance, such as humanoidmaze-giant, cube-triple, puzzle-4x5, powderworld-hard, and more. We ensure that sufficient data is available for those tasks (which is estimated from the amount needed to solve their easier versions). We invite researchers to take on these challenges and push the limits of offline GCRL with better algorithms.\nHow can we develop a policy that generalizes well at test time? In our experiments, we found hierarchical RL methods (e.g., HIQL) to work especially well in several tasks. Among several potential explanations, we hypothesize that this is mainly because hierarchical RL reduces learning complexity by having two policies specialized in different things, which makes both policies generalize better at evaluation time. After all, test-time generalization is known to be one of the major bottlenecks in offline RL. But, are hierarchies really necessary to achieve good test-time generalization? Can we develop a non-hierarchical method that enjoys the same benefit by exploiting the subgoal structure of offline GCRL? This would be especially beneficial, not just because it is simpler, but also because it can potentially yield better, unified representations that can potentially serve as a \"foundation model\" for fine-tuning.\nCan we develop a method that works well across all categories? Our benchmarking results reveal that no method"}]}