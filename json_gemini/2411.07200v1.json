{"title": "'Explaining RL Decisions with Trajectories': A Reproducibility Study", "authors": ["Karim Abdel Sadek", "Matteo Nulli", "Joan Velja", "Jort Vincenti"], "abstract": "This work investigates the reproducibility of the paper \"Explaining RL decisions with tra-\njectories\" by Deshmukh et al. (2023). The original paper introduces a novel approach in\nexplainable reinforcement learning based on the attribution decisions of an agent to specific\nclusters of trajectories encountered during training. We verify the main claims from the pa-\nper, which state that (i) training on less trajectories induces a lower initial state value, (ii)\ntrajectories in a cluster present similar high-level patterns, (iii) distant trajectories influence\nthe decision of an agent, and (iv) humans correctly identify the attributed trajectories to the\ndecision of the agent. We recover the environments used by the authors based on the partial\noriginal code they provided for one of the environments (Grid-World), and implemented the\nremaining from scratch (Seaquest and HalfCheetah, Breakout, Q*Bert). While we confirm\nthat (i), (ii), and (iii) partially hold, we extend on the largely qualitative experiments from\nthe authors by introducing a quantitative metric to further support (iii), and new exper-\niments and visual results for (i). Moreover, we investigate the use of different clustering\nalgorithms and encoder architectures to further support (ii). We could not support (iv),\ngiven the limited extent of the original experiments. We conclude that, while some of the\nclaims can be supported, further investigations and experiments could be of interest. We\nrecognize the novelty of the work from the authors and hope that our work paves the way\nfor clearer and more transparent approaches.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL), formalized in the pioneering work of Sutton & Barto (2018), focuses on\nlearning how to map situations to actions, in order to maximize a reward signal. The agent aims to discover\nwhich actions are the most rewarding by testing them. This addresses the problem of how agents should learn\na policy that takes actions to maximize the cumulative reward through interaction with the environment. A\nrecent pivotal focus in RL is the increasing attention on the explainability of these algorithms, a factor for\ntheir adoption in real-world applications. Precedent work in the field of XRL include Puiutta & Veith (2020),\nKorkmaz (2021) and Coppens et al. (2019). This reproducibility report focuses on the work of Deshmukh\net al. (2023), which proposes an innovative approach to enhance the transparency of RL decision-making\nprocesses. Given the rising interest and applications of Offline RL (Levine et al. (2020), Kumar et al. (2020)),\nobtaining explainable decision is an important desideratum. Deshmukh et al. (2023) introduces a novel\nframework in the offline RL landscape. This new approach is based on attributing the decisions of an RL\nagent to specific trajectories encountered during its training phase. It counters traditional methods that\npredominantly rely on highlighting salient features of the state of the agent(Iyer et al. (2018))."}, {"title": "2 Scope of Reproducibility", "content": "Explainability and interpretability have recently become of great interest for the adoption of AI systems in\nreal-world applications. In particular, understanding and explaining the behavior and decisions of RL agents\nis a crucial task considering the plausible large-scale adoption of these systems. On top of the aforementioned\nones in Section 1, other examples of Explainable Reinforcement Learning (XRL) studies include a high-level\ndecision language approach by Puri et al. (2019), Pawlowski et al. (2020) and Madumal et al. (2020). The\ngoal of this report is to analyze the reproducibility of the work by Deshmukh et al. (2023). Given the novelty\nof the work, it follows that there is no existing benchmark to compare the results claimed by the authors.\nOur contribution lies mostly in the implementation, verification, and interpretation per se of these results.\nWe proceed by verifying the claims made by the authors, which we summarize and re-state as follows here\nbelow:\n\u2022 Removing Trajectories induces a lower Initial State Value: Including all relevant trajectories\nin the training data will result in higher or equal initial state value estimates compared to training\nsets where key trajectories are omitted. This holds also for other metrics we may consider. The\ndefinitions can be found in Section 3.3.\n\u2022 Cluster High-Level Behaviours: High-level behaviours are defined as patterns within a trajectory\nwhich lead to the same result and repeat over multiple trajectories. We aim to verify that different\nembedding clusters represent different meaningful high-level and interpretable behaviors.\n\u2022 Distant Trajectories influence Decisions of the Agents: Decisions performed by RL agents\ncan be influenced by trajectories distant from the state under consideration. In such scenarios looking\nonly at the features in the action space may not provide a full understanding of the behaviour of an\nagent.\n\u2022 Human Study: Humans may accurately identify the determinant trajectories that influenced the\ndecision of an RL agent."}, {"title": "3 Methodology", "content": "The original paper code is not yet publicly available. However, we obtained part of the code from the\nauthors: we were given the Grid-World environment, together with part of its related experiments. We\nfollowed their code and expanded upon it, in order to verify the claims. On the other hand, we wrote from\nscratch the implementation for Seaquest, HalfChetaah. Additionally, we added two environments to the\nanalysis of Deshmukh et al. (2023), Breakout and Q*Bert."}, {"title": "3.1 Environments", "content": "The investigations made in the paper regard three different types of Reinforcement Learning environments:\n1. Grid-World, a grid-like environment in which the agent has a discrete state and action space. The\ngame consists of an agent starting from a point in the grid and moving inside it. The goal is to\nreach a \u2019green\u2019 cell while avoiding entering a \u2019red\u2019 cell, while making the smallest number of steps\npossible. The default grid has a size of 7x7.\n2. Seaquest, a video-game-like environment in which the agent has a discrete state and action space.\nThe game consists in a submarine moving underwater. Here are more information on the Atari\nSeaquest environment.\n3. HalfCheetah, a video-game-like environment where the agent has a continuous state and action space.\nThe game consists of a 2-dimensional robot having a number of joints. The goal is to get the cat\nshaped robot to run, by turning its joints using rotational forces (torque). Here are more information\non the HalfCheetah environment.\n4. Breakout, one of the most played atari-produced video-games. The game consists of a paddle and\nhitting a ball into a brick wall at the top of the screen, where the goal is to destroy all the bricks.\nHere are more informations on Breakout environment.\n5. Q*Bert is a video-game like environment. The game consists of an agent which hops on each cube\nof a pyramid one at a time, and the goal is to change their color of into a specific one. Here are\nmore informations on Q*Bert environment."}, {"title": "3.2 Datasets", "content": "Datasets used in the analysis have the same composition between the five environments. Each dataset D\ncomprises of a set of $n_\\tau$ trajectories. Each $\\tau_j$ is a k-step trajectory and each trajectory step is a tuple\n$\\tau_j = [\\tau_{j,1}, \\tau_{j,2}, ..., \\tau_{j,k}]$ where $\\tau_{j,i} = (o_i , a_i , r_i)$. Here $o_i$ is the observation in that step, $a_i$ is the action\ntaken in that step and $r_i$ is the per-step reward. However, collecting data depends on the environment in\nwhich the experiments are made. Regarding Grid-World, agents are trained specifically to generate data\ntrajectories. For Seaquest data is instead downloaded from d4rl-Atari Repository, for Breakout and Q*Bert\nfrom Expert-offline RL Repository, whereas in the case of HalfCheetah from d4rl Repository of Fu et al.\n(2020)."}, {"title": "3.3 Model Description", "content": "Across all three environments, trajectory attributions and explanations are made through the following 5\nsteps, also summarized in Figure 1:\na. In Grid-World trajectories are generated by training different agents using Model-based offline RL\nthrough the Dyna-Q Algorithm (Appendix A.1). Trajectories are then encoded. In Grid-World\nthe authors define a Seq2Seq LSTM based encoder-decoder architecture. After training, only the\noutput of the encoder which corresponds to the trajectory embedding of Figure 1 is kept. On the\nother hand, in all others ( Seaquest, Breakout, Q*Bert and HalfCheetah) the trajectories encoders are\npre-trained. For the former, the model is obtained following the instructions on pre-trained decision\ntransformer. For the latter, the pre-trained model is downloaded from the GitHub repository pre trained trajectory transformer from Janner et al. (2021). Both architectures are GPTs. Last but\nnot least, these encodings are then embedded.\nb. The embeddings are passed through the XMeans clustering algorithm introduced by Pelleg et al.\n(2000). The implementation used by the authors is the one from Novikov (2019). Using XMeans is\nan arbitrary choice and in Section 4.5 we will investigate other options.\nc. The cluster representations are embedded obtaining the representative embedding of given trajecto ries."}, {"title": "3.4 Hyper-parameters", "content": "In order to reproduce the experiments of the paper we strictly used, when available, the same hyperpa rameters used by the authors. This was the case for Grid-World. Regarding Seaquest, Breakout, Q*Bert\nand HalfCheetah we developed the code from scratch. Hence, we cannot be certain about the exact hyper parameters used by the authors. In all instances, we retained the default settings provided by the libraries.\nIf certain essential values were absent, we chose those that aligned with the settings used in Grid-World."}, {"title": "3.5 Experimental Set up and Code", "content": "Our experimental setup follows the approach of Deshmukh et al. (2023) in proving their claims. For claims\nClustering High Level Behaviour and Distant Trajectories influence Decisions of the Agent we visually inspect\nthe trajectories by plotting them, together with additional experiments. Removing Trajectories induces a\nlower IVS claim is carefully taken care of by inspecting the 5 different metrics previously introduced. Human\nStudy claim is verified by replicating the analogous human study."}, {"title": "4 Results", "content": "In order to verify the previously stated claims, we proceed with an empirical evaluation, aiming to reproduce\nthe results obtained by the authors. The result from Grid-Word presents some variance with respect to the\nones reproduced in this article. Concerning the four other environments, they lack reproducibility due to\na total absence of original code. Additionally, we further investigate each claim by conducting additional\nexperiments. Most of these are done using the proposed number of trajectories by Deshmukh et al. (2023)\n(60 in Grid-World 7x7, 717 in Seaquest, 1000 in HalfCheetah), unless specified otherwise."}, {"title": "4.1 Removing Trajectories induces a lower Initial State Value", "content": "Reproducibility: In the Grid-World environment, the authors introduced different metrics to show that\ntrajectories play an important role in obtaining high-quality policies. The values obtained by the authors\nare reported in Table 1 of the original paper. We present our findings in Table 1, providing evidence for\nthe claim of the authors. The results are in general reproducible, with a small variation. The original\npolicy, trained on all the trajectories, achieves the highest ISV among all. We report the reproduced results\nfor the Seaquest environment in Table 2. We observe that the results are not similar to the ones in the\noriginal papers. This discrepancy could be attributed to several factors. The setup process, involving the\ninstallation of numerous packages and the use of outdated libraries, likely introduced minor computational\nvariances. Additionally, changes in game versions, such as upgrading Seaquest from v4 to v5, affected the\ngame-play dynamics, increasing the available action space. The choice of the difficulty level of the game\nalso influenced the dataset, as easier versions had shorter trajectories due to quicker game terminations.\nAnother motive for this is given by the limited amount of training we carried for our agent. In fact, given\ncomputational limitations, we are not able to train for a long horizon of time. Moreover, the settings of\nthe experiments from the authors are unclear. There is no notion or explanation for what it means to train\nan agent until saturation, and no further details on the hyperparameters of the experiments (for additional\ndetails, see Appendix D). On the other hand, even given the differences and limitations explained above,\nthe Claim Removing Trajectories induces a lower Initial State Value still holds. The policy trained on the\nwhole dataset achieves a higher ISV than any of the other ones. At the same time, the other metrics are\nconsistent in terms of conclusions we can draw. Summarizing, the difference in reproducibility is due to the\nlimited details given by the authors, together with the limitation on our computational resources. Results\nfor HalfCheetah are reported in Table 5.\nWe perform a further analysis to highlight the reproducibility of the results. To have a better comparison of\nour results with the one obtained by the authors, we report in Table 3 the average values for the metrics we"}, {"title": "4.2 Cluster High-Level Behaviours", "content": "Reproducibility: In Grid-World, this claim can be verified by either observing their shared high-level\nbehavioural patterns or by using some quantitative metric. We deemed the latter to be a more appropriate\napproach. Thus, we proceeded to define this starting from inspecting trajectories belonging to that cluster\nand calculating the percentage of such manifesting a certain pattern. We show one trajectory for each of\nthe three analyzed clusters in Figure 3. The following high-level behaviours are retrieved: 'Achieving Goal\nin Top right corner', 'Mid-grid journey to goal' and 'Falling into lava'. A practical explanation of this\nself-defined metric can be done analyzing the trajectory behavior of 'Falling into lava'. This is spotted by\nlooking for a -1 reward value in the last but one state, and then calculating the percentage of trajectories\nthat have this wanted characteristic within each cluster. We repeat this for every cluster and pick those\nwith a percentage value greater then 90%. The procedure is then iterated for the other two categories, by\nchanging the characteristics to look for. In the 'Achieving Goal in Top right corner' we look for a +1 reward\nin the last but one state and going towards position 6. Whereas in the 'Mid-grid Journey to goal' category\nwe look for trajectories starting in the middle of the grid and having a positive reward in the last but one\nstate. These align with the behaviours found by the authors. The claim is thus supported. Note that cluster\nlabels vary from those highlighted by the authors.\nIn our Seaquest analysis, we tried to replicate the cluster findings from the original study in Figure 4.\nWe noticed differences in the number of data points and their distribution. Converting 717 trajectories\ninto around 24,000 sub-trajectories for the XMeans algorithm revealed more data points than shown in the\noriginal graph of the authors. This discrepancy could be due to two reasons: (i) the choice of game mode\nand data source might affect the length of observations, which was not detailed by the original authors, and\n(ii) the authors might have used a more complex method to aggregate data post-encoding than the simple\naveraging they described.\nAdditionally, when trying to interpret the high-level meaning of those clusters, we obtained some discrep-\nancies. Results in Figure 5 show a strong link between the 'Filling Oxygen' behavior and cluster 7, while\nthe other behaviors remained unclear, questioning the specific claims of the authors. However, this does\nnot undermine the broader notion of 'meaningful clusters', but, given the scope of this paper, finding those"}, {"title": "4.3 Distant Trajectories influence Decisions of the Agents", "content": "Reproducibility: We start by analyzing claim Distant Trajectories influence Decisions of the Agents in the\ndiscrete Grid-World environment. The authors perform a qualitative analysis to support their claim. We try\nto reproduce the results and the plots given the code provided by the authors. The hyperparameters are set\nequal to the default values. The results of Figure 2 in the paper by Deshmukh et al. (2023) are reproducible\nusing the code given by the authors. Note that it may take more than one attempt to reproduce these results.\nThis is due to the possible variation of clusters from each iteration of the code. Nevertheless, we found that\nthese results were easily reproducible with little effort. Results are shown in Figure 6. The trajectories (i), (ii),\n(iii) are equivalent to the ones indicated in the paper by the authors. We plot an additional trajectory which\nis part of the attributed cluster. It is important to stress that also (iv) is distant from the state (1,1) we\nare considering. This investigation confirms the claim of the paper. However, given the highly qualitative\njustification provided by the authors, we seek a more structured and quantitative way of analyzing this claim.\nWe defer these experiments to Section 4.3."}, {"title": "4.4 Human Study", "content": "In order to verify this claim we reproduce the experiments as well as the study setup of the authors.\nDeshmukh et al. (2023) study is conducted on 10 people, which may not itself be sufficient to support\nthe claim. In trying to improve this, we doubled the interviewees to 20 people, each of whom first received\nan explanation of how Grid-World navigation works. Following this explanation, they all gained a full\nunderstanding of the navigation process. 40% are university graduates in mathematics and computer science.\n45% are student graduates in engineering. The remaining 5% come from different study backgrounds. We\nbegin by showing two Attributed Trajectories (Attr traj 1 and Attr traj 2), one Random Trajectory, and one\nTrajectory belonging to an Alternate cluster for each state. We investigate two questions. (i) Which single\ntrajectory do you believe best explains the action suggested? (ii) Can you point out all the trajectories you\nbelieve explain the action suggested?\nAveraging between both states, the results on Question (i) highlight how almost ~ 72.5% of the interviewees\ncorrectly identify one of the two attributed trajectories. For Question (ii), Figure 8 shows that in ~ 63%\nof the cases, humans can correctly identify all the attributed trajectories. The results obtained are similar\nto the ones of the authors. However, given the very limited sample size of our experiments, we do not have\nenough evidence to support the claim."}, {"title": "4.5 Results beyond original paper", "content": "In this section we go beyond the author claims and try to experiment with the authors design choices like\nthe employed encoder method as well as cluster algorithm.\nImproving clustering algorithm: XMeans has proven to be useful in determining almost accurately the\ncorrect clustering trajectories, we propose a different approach by using DBSCAN algorithm. Introduced\nin Ester et al. (1996), DBSCAN is a non-parametric density-based clustering method that groups together\nsets of points packed together. The density-based characteristic of DBSCAN, differently to X-Means, is"}, {"title": "5 Discussion", "content": "Across this work, we performed several experiments aimed at reproducing key findings of Deshmukh et al.\n(2023). The outcomes of this reproducibility study partially confirm their claims."}, {"title": "A Methodology", "content": null}, {"title": "A.1 Model description", "content": "Dyna-Q Algorithm\nIntroduced in Sutton (1990), Dyna-Q is a Model based Reinforcement Learning Algorithm. Conceptually\nit is an algorithm that illustrates how real and simulated experience can be combined in building a policy.\nDyna-Q algorithm (11) introduced in Section 3.3 starts by initializing a so-called Q table. A table made of\nall possible states vs all possible actions. The model also contains a state, action, next state, and reward\ntuples. This way the model can be both improved and queried to get the next state in the planning part. The\nprocess begins by observing state S (a), and then selecting the next action A, in a greedy manner (b). After\ntaking the action A, we observe a reward R and a state S\n\u2032. These two values are then used in the formula\nin (d) to update the Q table cell corresponding with state s and action a. After these classic Q-Learning\nsteps we perform a loop (f) which consists of the added Dyna-Q part. First, we randomly select a state S\nand an action A and then we deduce a new state S\n\u2032 and a new reward R which will then be used to update\nthe Q-table as before."}, {"title": "A.2 Computational Requirements & Environmental Impacts", "content": "The experiments were performed using a MacBook with Apple M2 Pro silicon chip with 10 CPU cores (1),\nMacBook with Apple M1 silicon chip with 8 CPU cores (2), and a Microsoft Windows 11 Pro with Intel(R)\nCore(TM) i7-10710U with 6 CPU cores (3). Most of the experiments in Grid-World run easily and in seconds\non our local machines. On the other hand the running time for Seaquest and HalfCheetah with machine (1),\ncan vary between 50 minutes (with 10 steps per epoch) and 8 hours (with 100 steps per epoch). We employed\npre-trained models both for Seaquest and HalfCheetah as explained in Sections 3.3 and 3.2. The same is done\nfor some additional experiments on Grid-World (Section C.2). Discussion on the environmental impact of\nthese models has been addressed in previous literature (Rillig et al. (2023)). In addressing our own ecological\nfootprint, we used the Code Carbon Tool (on machine (1)) to estimate our total energy consumption in\nobtaining cluster attributions. In Grid-World the estimated consumption is approximately 0.000170 kWh of\nelectricity. Whereas in Seaquest we use 0.005133 kWh of electricity. Similarly, in HalfCheetah we consume\n0.004783 kWh. We then use the $CO_2e$ equation to obtain the corresponding $CO_2$ emissions. The formula\n$CO_2e = CI \\cdot P UE \\cdot P \\cdot t$\nis comprised of CI Carbon Intensity (fixed value of 0.954), P UE Power Usage Effectiveness (also fixes to\n1.58), P Power required (estimated through Code Carbon Tool) and the training time t in hours. Our final\nemissions are available in Table 4."}, {"title": "B Results reproducing original paper and verification of the\nclaims", "content": null}, {"title": "B.1 Removing trajectories induces a lower ISV", "content": "Despite the challenges in replicating the exact clustering outcomes for Half Cheetah as highlighted earlier,\n5 reveals some interesting patterns. The table provides a quantitative analysis that, despite reproducibility\nissues, still shows consistent trends across different metrics."}, {"title": "B.2 Meaningful Clusters", "content": "In order to obtain Figure 5, we specifically analyzed the oxygen tank indicator at the bottom of the\nscreen, recognizable by its unique color. An increase in the bar is interpreted as the submarine refilling\nits oxygen tank, while a empty tank resulted in a submarine explosion. This might occur either from\nrunning out of oxygen or sustaining damage from enemies. For the \u2019Fighting with head out\u2019 behavior, we\nmonitored the position of the submarine within the top 30 pixels of the screen. We would consider it as\nengaging in surface combat if it remained in this area for more than 10 out of the 30 frames in a sub-trajectory.\nThe analysis highlights distinct behaviors: Cluster 7 is linked to \u2019Filling Oxygen\u2019, and Clusters 2 and 3 to\n\u2019Submarine Burst\u2019, with no clear trend for \u2019Fighting with Head Out\u2019. 12 shows overlapping behaviors across\nclusters, complicating the attribution of specific actions. Consequently, the most representative cluster\nappears to be the one of refueling oxygen: two frames depict the submarine at the surface (directly implying\noxygen refueling), while the remaining three suggest imminent game resets, indirectly associated with oxygen\nrefill."}, {"title": "B.3 Clusters for Q*Bert and Breakout", "content": "Displayed in Figure 13 are the clusters for Q-Bert and Breakout. The choice of five clusters was made for\nboth games, as they independently feature significantly more rewards within the trajectories as well as fewer\naction states (only 6 and 4, respectively), compared to their Seaquest counterpart. Consequently, we opted\nto reduce the length of the sub-trajectories to 15 and to decrease the number of clusters to account for these\ndifferences."}, {"title": "C Results beyond original paper", "content": null}, {"title": "C.1 Improving clustering algorithm", "content": "Table 8 is mirroring the results produced in Table 1. In this section however we produce metrics results\nusing DBSCAN clustering algorithm, instead XMeans. As introduced before the values are similar to those\nattained in the original table."}, {"title": "C.2 Implementing different encoder techniques", "content": null}, {"title": "C.3 Are distant trajectories really important?", "content": "Distance State-Trajectory and its importance Let us formally define the following variables:\n\u2022 S: set containing all states with at least one attributed trajectory\n\u2022 $T_s$: set of all the attributed trajectories for the state $s \\in S$\n\u2022 $t_{i,s}$: i-th trajectory in the attribution set $T_s$. Each trajectory i has length $l_i$\n\u2022 $a^{*}(b; c)$: distance from point b to point c in our grid. It is calculated by implementing the A* search\nalgorithm.\n\u2022 $d(t_{i,s}; s)$: distance from state s to trajectory $t_{i,s}$. Mathematically, for each point $p_j \\in t_{i,s}$,\n$d(t_{i,s}; s) = \\begin{cases}\n0 &: \\text{if } \\exists p_j \\in t_{i,s} \\text{ s.t. } p_j = s \\\\\n\\frac{1}{l_i} \\sum_{j=1}^{l_i} a^{*}(p_j, s) &: \\text{otherwise}\n\\end{cases}$\n\u2022 $d(T_s, s)$: Average distance of the attribution set $T_s$ from its respective state s. We implement it as:\n$d(T_s, s) = \\frac{1}{|T_s|} \\sum_{k=1}^{|T_s|} d(t_{k,s}; s)$\nGiven the elements introduced above, we can calculate the average distance of a state from its attributed\ntrajectories, denoted by $d(T_s, s)$. Note that in the formulation above, if a trajectory $t_{i,s}$ passes through the\nstate s, we set $d(t_{k,s}; s) = 0$. This is a design choice, which can be further justified. In fact, we are interested\nin considering \u2019far\u2019 only the trajectories where there is no interaction with the state s itself.\nWe designed and implemented Algorithm 1. We provide a high-level pseudo-code for a better understanding\nof the steps we perform."}, {"title": "C.4 Are data trajectories important to obtain a good action value? Are some\nmore important than others?", "content": "In this section, we aim to provide further details on experiments on the assumptions of Claim Removing\nTrajectories induces a lower Initial State Value. Values from both the original paper and from Table 1 suggest"}, {"title": "C.5 Additional Hyper-Parameters Experiments", "content": "In this section we investigate the change in metrics when hyperparameters regarding the agent training are\nchanged. We play with the values of alpha, gamma and number of evaluation epochs. We proceed to generate\noffline data for each combination of the above mentioned hyperparameter and successively train the authors\nSeq2Seq model.\nThe best results in terms of loss value are shown in Table 10. However while we reach better loss results we"}, {"title": "D Training setting for Seaquest and HalfCheetah", "content": "As mentioned throughout the paper, we implemented from scratch the code for the Seaquest and HalfCheetah\nenvironments. Due to the lack of details provided in the original study, we provide our own setup for the\ntraining of the aforementioned environments.\nFor Seaquest, we train a Discrete SAC model based on the original work by Christodoulou (2019), developed\nby Seno & Imai (2022). Observations for the game were in the form of 84x84 greyscale frames, which we\nstacked, forming for each observation a 4x84x84 array. This allowed the model to incorporate some degree of\ntemporal awareness, also referred to as context in the original work. Subsequently, in order to preserve spatial\ninformation, we implemented a custom encoder for the model, in the form of a Convolutional Neural Network.\nWe are not aware whether the authors pursued this approach in their study, but due to the nature of the data\nitself, we are sure this implementation helped the training and performance by a significant margin. Once\nmore, due to the nature of the data (images), we implemented a 'pixel' scaler for preprocessing purposes to\nact as a pixel value normalizer.\nFor HalfCheetah, on the other hand, we implemented a SAC model based on the original work by Haarnoja\net al. (2018). We kept the standard hyperparameters provided in the documentation of the library by Seno\n& Imai (2022). One critical missing piece of information we extensively discussed upon implementation was\ngiven by the notion of difference in action between models: due to the continuous nature of the actions in\nthis environment, it becomes almost impossible for two policies to predict the same set of actions. For this\npurpose, we decided to implement a way of comparing actions based through 'numpy.isclose(a,b)', by Harris\net al. (2020). The similarity formula given by the above method is\n$|a - b| \\le (abs. tolerance + rel. tolerance * |b|)$\nUnlike the built-in 'math.isclose', the above equation is not symmetric in a and b: it assumes b is the reference\nvalue so that 'isclose(a, b)' might be different from 'isclose(b, a)'. Furthermore, the default value of 'abs.\ntolerance' is not zero, and is used to determine what small values should be considered close to zero. Once\nagain, we are not aware of what the authors did on this end, but we have reasons to believe this approach\nhas grounds for a correct interpretation. We acknowledge potential sensitivity to the results based on the\nchoice of the hyperparameters of the method, for which we kept the default ones.\nFinally, although the authors mention a training schedule until saturation without further explanation, we\nfollowed the guidelines provided in the DR3RLpy framework as outlined by Fu et al. (2021), training both\nour models for 10 epochs, each taking 100 steps. We are not aware of whether our approach matches the\none suggested by the authors, but results, although not in absolute value, are relatively consistent with\nthose provided in the original study. Thus we can conclude that our hyperparameters setting is sufficient for\nreproducing the results."}]}