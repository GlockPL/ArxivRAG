{"title": "Redefining Proactivity for Information Seeking Dialogue", "authors": ["Jing Yang Lee", "Seokhwan Kim", "Kartik Mehta", "Jiun-Yu Kao", "Yu-Hsiang Lin", "Arpit Gupta"], "abstract": "Information-Seeking Dialogue (ISD) agents aim to provide accurate responses to user queries. While proficient in directly addressing user queries, these agents, as well as LLMs in general, predominantly exhibit reactive behavior, lacking the ability to generate proactive responses that actively engage users in sustained conversations. However, existing definitions of proactive dialogue in this context do not focus on how each response actively engages the user and sustains the conversation. Hence, we present a new definition of proactivity that focuses on enhancing the 'proactiveness' of each generated response via the introduction of new information related to the initial query. To this end, we construct a proactive dialogue dataset comprising 2,000 single-turn conversations, and introduce several automatic metrics to evaluate response 'proactiveness' which achieved high correlation with human annotation. Additionally, we introduce two innovative Chain-of-Thought (CoT) prompts, the 3-step CoT and the 3-in-1 CoT prompts, which consistently outperform standard prompts by up to 90% in the zero-shot setting.", "sections": [{"title": "1 Introduction", "content": "Generally, the aim of Information-Seeking Dialogue (ISD) agents (Dziri et al., 2022; Nakamura et al., 2022) is to generate an informative response which answers the user's query. In these interactions, users typically pose questions to obtain specific pieces of information, and the dialogue agent generates coherent responses which contains the information requested by the user. In recent years, Large Language Models (LLMs) have generally succeeded at achieving this goal (Li et al., 2023a; Braunschweiler et al., 2023). However, current ISD agents, as well as LLMs in general, tend to be more reactive than proactive. An example of a reactive response is provided in Figure 1. Responses generated by a reactive ISD agent would adequately address the user's query but fail to proactively engage the user. Once the requested information is provided, the conversation with the ISD agent naturally concludes.\nIn ISD, existing work on proactivity primarily focuses on generating clarifying questions and eliciting user preferences (Deng et al., 2023), aiming to resolve ambiguity in the user's query or uncover their preference respectively. Current definitions of proactivity in ISD do not emphasize engaging the user or sustaining the conversation once the desired information has been provided. Hence, we introduce a novel definition of ISD proactivity that emphasizes generating responses that aim to sustain the interaction by proactively engaging the user via the introduction of new information pertinent to the initial query. By proactively providing new related information, the agent can stimulate the user's interest, prompting further inquiries and sustaining the conversation. Hence, our definition of ISD proactivity focuses on actively delivering"}, {"title": "2 Related Work", "content": "information related to the initial query in a conversational manner, thereby naturally guiding the conversation towards addressing multiple pieces of information, improving the overall informativeness during interactions with users and further enhancing user satisfaction (Deng et al., 2023; Doherty and Doherty, 2018). Unlike prior definitions, we focus on the proactiveness of each individual response, evaluating them individually rather than as part of the entire conversation. This allows us to evaluate responses on specific criterion (Section 3).\nAccording to our definition, a proactive response consists of the answer to the user's query and a proactive element, which refers to new information related to the initial query. The proactive element can be further categorized as either a Follow-up Question (FQ) or Additional Information(AI). Samples of proactive responses according to our definitions are also provided in Figure 1. It's important to note that this work does not encompass factual accuracy or information correctness. The focus is purely on syntactic and semantic proactivity.\nIn this paper, our contributions are as follows:\n1. We introduce a novel response-level definition of proactivity for ISD.\n2. We construct a proactive dialogue corpus consisting of 2,000 single-turn conversations.\n3. We introduce a set of automatic metrics designed to measure the level of \u2018proactiveness' in a response, according to our definition of proactive dialogue. Our metrics demonstrate high correlation with human annotation.\n4. We propose two in-context Chain-of-Thought (CoT) prompts, namely the 3-step CoT prompt and the 3-in-1 CoT prompt, which outperform standard few-shot prompting. Additionally, utilizing our corpus, we demonstrate the efficacy of instruction-tuning in the context of proactive response generation.\n5. We demonstrate the efficacy of our approach in sustaining user interaction and improving conversational informativeness and in the multi-turn scenarios.\nProactive Dialogue Proactive dialogue encompasses various techniques for engaging users by steering conversations in specific directions. In the"}, {"title": "3 Problem Definition", "content": "context of Open-Domain (OD) dialogue, some popular proactive dialogue tasks include: target-guided dialogue, prosocial dialogue, and non-collaborative dialogue. Target guided dialogue focuses on directing interactions toward predefined topics or entities, using methods such as response planning (Kishinami et al., 2022), event-based knowledge graphs (Xu et al., 2021), and commonsense bridging (Gupta et al., 2022a). Prosocial dialogue involves generating non-offensive responses that adhere to societal norms (Kim et al., 2022). In the context of Task-Oriented (TO) dialogue, proactive dialogue definitions include non-collaborative dialogue as well as enriched TO dialogue. In non-collaborative dialogue, the agent and user have opposing objectives. Some examples include persuasion (Wang et al., 2019; Wu et al., 2021), negotiation (He et al., 2018), and deception-based dialogue (Santhanam et al., 2020). Enriched TO dialogue shares some similarities with our task. However, while enriched TO dialogue focuses on enhancing conversational naturalness through additional information, our goal is to sustain ISD. Rather than prioritizing naturalness, we aim to encourage user engagement by introducing new information (either directly or through a FQ) that prompts the user to continue the conversation.\nWith regard to ISD specifically, response proactivity largely revolves around generating clarifying questions and eliciting user preferences (Deng et al., 2023). Clarifying question generation aims to resolve ambiguity in user queries to provide the user with the requested information (Aliannejadi et al., 2021). Approaches include retrieval and ranking-based frameworks (Aliannejadi et al., 2019), reinforcement learning with clarification utility rewards (Zamani et al., 2020), and multi-step frameworks predicting the need for a clarifying question before generating one (Aliannejadi et al., 2021; Guo et al., 2021). Some methods also combine clarifying questions and conversational QA in multi-turn context (Deng et al., 2022; Guo et al., 2021). User preference elicitation involves proactively reveals the user's interests for better recommendations (Zhang et al., 2018). This task is often treated as a decision-making problem often tackled with reinforcement learning (Zhang et al., 2018; Deng et al., 2021; Jaques et al., 2019). Unlike earlier definitions, we do not concentrate on specific proactive ISD aspects like clarifying question generation or user preference elicitation. Instead, we solely focus on enhancing proactivity by providing relevant information. Moreover, we evaluate the proactiveness of each individual response separately, rather than considering the entire conversation.\nLLM-based ISD In recent years, LLMs have emerged as leading models in language generation tasks, demonstrating state-of-the-art performance. In ISD, recent methods utilize LLMs through in-context learning or supervised fine-tuning. In-context learning refers to learning a new task during inference with a few prompt examples. Approaches leveraging few-shot (Li et al., 2023b; Chada and Natarajan, 2021) and CoT (Yoran et al., 2023; Sultan et al., 2024) prompts have been employed in this context. LLMs are also often trained on dialogue contexts alongside task instructions, which is known as instruction tuning, to enhance zero-shot performance. In the context of dialogue, LLMs such as Flan-T5 (Chung et al., 2022), InstructGPT (Ouyang et al., 2022), and InstructDial (Gupta et al., 2022b) were explicitly trained on dialogue data for chat applications. Likewise, instruction-tuning has also been applied to improve the accuracy and informativeness of conversational QA responses (Jiang et al., 2024; Razumovskaia et al., 2024). These methods excel at achieving the primary aim of ISD to address user queries. However, as highlighted in Section 1, they tend to produce reactive responses that do not proactively engage the user."}, {"content": "We propose a new proactive response definition for ISD that consists of two components: an Answer and a Proactive Element. The Answer directly addresses the user's query, while the Proactive Element actively engages the user by providing related information. The proactive element enriches the user's understanding and can spark further interest, prompting them to further engage the conversation to find out more. We further classify the Proactive Element into two main categories: Additional Information (AI) and Follow-up Questions (FQs).\nAl refers to any knowledge not explicitly requested in the user's query or mentioned in the answer, but that could be of interest to the user. The provision of high-quality AI enriches the conversation by increasing its informativeness, and encouraging the user to continue the interaction. To determine if an AI qualifies, the following criteria must be met:\n1. Relevance. The AI should be relevant to the user's query.\n2. Informativeness. The AI should provide substantial supplementary details beyond the original Answer. It should not be simply a rephrased version of the Answer.\n3. Naturalness. The AI should be natural in a spoken conversational context. It should be introduced in a conversational manner and avoid excessive verbosity.\nIt's important to note that LLMs often have a tendency to include excessive details in a single response, which can hinder naturalness, particularly in spoken context. Our goal is to incorporate AI in a concise and engaging manner that encourages the user to continue the interaction.\nA FQ asks if the user is interested in a specific piece of additional relevant information related to their initial query. The information itself is not explicitly provided in the FQ. By asking appropriate FQs, we can extend the conversation beyond the initial turn. The criteria for a FQ are defined as follows:\n1. Relevance. The FQ should relate to knowledge relevant to the user's query.\n2. Specificity. The FQ should be as specific as possible, referring to a particular piece of information rather than making a broad inquiry. Specific FQs lead to more informative and satisfying interactions."}, {"title": "4 Proactive Response Evaluation", "content": "3. Perspective. The FQ should not request information from the user. It should focus on assisting and informing the user, avoiding information seeking.\nUnlike prior work in ISD, our definition focuses specifically on response proactivity rather than factual accuracy. Therefore, we do not include criteria related to information accuracy or ground responses on external knowledge sources. These factors are often used to prevent hallucination and ensure factual correctness.\nIn this section, we propose several automatic metrics to quantify the proactivity of a response. A reliable automatic metric would enable objective and cost-effective evaluation, ultimately enhancing the reproducibility of our work."}, {"title": "4.1 Baseline Metrics", "content": "We introduce two baseline metrics: a prompt-based metric and a classification-based metric. The prompt-based metric, ranging from 0 to 1, is obtained by prompting an LLM to assess the proactiveness of responses based on our definition. The classification-based metric is calculated using two language models, each evaluating responses as valid or invalid for each Proactive Element type, according to our definition. More details are provided in Appendix A.6."}, {"title": "4.2 Proposed Metrics", "content": "The baseline scores often lack interpretability. They do not provide specific information about which criteria a response violates. Therefore, we propose two additional metrics which evaluate the responses based on the criteria defined in Section 3.\nSemantic similarity-based We design a metric based on semantic similarity to evaluate the Relevance of a proactive response, as well as the Specificity and Informativeness of the FQ and AI respectively.\nThe respective semantic scores for the FQ and AI are computed as follows:\n\u2022 FQ: $\\alpha * BS(Q, R) + (1 - \\alpha)BS(R)$\n\u2022 AI: $\\alpha * BS(Q, R) + (1 - \\alpha)(1 \u2013 BS(R))$\nwhere Q and R denote the input query and generated response respectively. $BS(\u00b7)$ refers to the BERTScore, and $BS(res) = \\frac{1}{n} \\sum_{i,j \\in N, j \\in N} BS(r_i, r_j)$, the mean pair-wise semantic similarity. a is a hyperparameter introduced to control the distribution between both terms. In our implementation, the BertScore is computed using the deberta-base-v3 embeddings.\nIt should also be highlighted that a completely irrelevant or incoherent proactive element would likely result in a lower semantic similarity score compared compared to a generic but related response. This difference is primarily due to the first term in the equations, which involves the BertScore calculation between the query and the response. An entirely irrelevant response would achieve a very low BertScore, whereas a generic but relevant response would obtain a relatively higher score. Conssequently, after appropriately adjusting a, the semantic score for a proactive response containing irrelevant elements would be significantly low."}, {"title": "User Simulation-based", "content": "We also propose a user simulation-based metric to quantify the quality of the Proactive Element based on Relevance and Conversational Naturalness of the AI, as well as the Specificity and Perspective of the FQ. This involves prompting an LLM to generate a simulated user turn in response to a given proactive system response, and then measuring the sentiment of the LLM-generated user response. After analyzing our initial responses, we found that users often react positively when we provide proactive responses paired with custom FQs or seamlessly integrated AI. This approach frequently elicits enthusiastic acknowledgments such as 'Yes, thank you!', 'Wow! That's interesting.', or 'That would be great. Thanks!', contributing to a LLM-generated user response with significantly positive sentiment. Conversely, subpar proactive responses that include generic FQs or conversationally unnatural AI tend to elicit replies with comparatively neutral sentiment. Furthermore, FQs with the wrong Perspective (requesting information from the user) generally lead to more detailed responses containing the requested information, often resulting in a neutral sentiment. Naturally, responses that do not address the user's query will typically elicit responses with negative sentiment. Samples of generated responses and the corresponding LLM-generated user responses for AI and FQ are provided in Figure 3(b) and 3(c) respectively."}, {"title": "5 Corpus Construction", "content": "To create our proactive dialogue corpus, we utilize the Natural Questions Question Answer (NQQA) dataset (Kwiatkowski et al., 2019). Each sample in this dataset includes a query, a short answer, and a long answer. The short answer provides the response to the query, while the long answer contains some relevant information. We selected the NQQA corpus because the query and short answer format resembles a typical single-turn conversation between a human and an ISD agent. However, since the short answer in the NQQA corpus consists of only a single entity, it needed to be modified for conversational naturalness."}, {"title": "5.1 Annotation", "content": "To achieve this, we engaged crowdworkers via Amazon Mechanical Turk (AMT) to modify the short answer to make it sound more like a natural response in a conversation, and to formulate the Proactive Element. AMT instructions are provided in Appendix A.3. These two components were concatenated to form the final proactive response. This process allowed us to construct a proactive dialogue corpus that could be used for training and evaluating proactive ISD agents.\nAnswer The Answer component is obtained by enhancing the short answer found in the NQQA corpus. This short answer, which is the direct answer to the user's query, is modified to ensure conversational naturalness. The crowdworkers were given instructions to integrate the short response, often a single verb or noun, into a coherent and comprehensive sentence that effectively addresses the user's query in a conversational style. For example, for the query in Figure 1 and 2, the short response ('Pom Klementieff') resulted in the following sentence: 'The actress who portrayed Mantis in Guardians of the Galaxy is Pom Klementieff'.\nProactive Element To obatin the Proactive Element (FQ or AI), crowdworkers were provided the long answer for reference. This simplified the task and ensured the accuracy of the Proactive Elements. For FQs, crowdworkers were instructed to create inquiries that assessed whether the user desired a particular piece of information from the long answer. They were encouraged to make their questions as specific as possible, focusing on particular details rather than general inquiries. For AI, crowdworkers were told to identify a single piece of information not already present in the initial answer and rephrase it to sound more natural in a conversational context. Before annotation, we filtered the NQQA dataset based on query length and long answer length. This ensured the clarity of the query and guaranteed that there was sufficient information from which the crowdworkers can formulate either a FQ or AI."}, {"title": "5.2 Corpus Features and Statistics", "content": "Based on the approach described above, we extracted 1000 samples and collect 2,000 proactive dialogue samples (1,000 for each Proactive Element) for our proactive response corpus. Each sample in our corpus constitutes a single-turn dialogue consisting of a user query and a proactive response. After obtaining the annotations, we manually validated each response to ensure fluency and correct any spelling or grammatical errors. The number of samples and average query length are identical for both Proactive Elements as a single query is used to obtain two proactive responses, one for each Proactive Element. Some basic corpus statistics are provided in Appendix A.4."}, {"title": "6 Proactive Response Generation", "content": "In this section, we describe the in-context learning and instruction-tuning approaches we employed for proactive response generation."}, {"title": "6.1 In-context Learning", "content": "In-context learning involves explicitly providing demonstrations of the task at hand to the model as part of a prompt. In this section, we describe three in-context learning prompts we utilize for proactive response generation: the direct prompt, 3-step CoT prompt, and 3-in-1 CoT prompt. For our experiments, we implemented 0-shot, 1-shot, and 3-shot variants of these three prompts. Prompt templates are provided in Appendix A.5.\nDirect Prompt This approach involves direct prompting the LLM to generate answers with the task description and demonstrations of query-proactive response pairs.\n3-step Chain-of-Thought (CoT) Prompt We introduce a 3-step CoT prompting approach designed to effectively generate proactive responses. Our approach involves systematically decomposing the proactive response generation task into three distinct subtasks, each addressed by an independent prompt. This entails three separate inferences. The output from each prompt is used as input for the subsequent prompt. The three prompts corresponding to the three subtasks are as follows:\nP1: Query answering: In this step, the LLM is prompted to generate the precise answer to the user's query.\nP2: Related information generation: Building upon the answer generated in P\u2081, the LLM is directed to identify a specific piece of related information that was not present in the initial answer.\nP3: Proactive Element generation: For the FQ, the LLM is prompted to formulate an inquiry to ask the user if they would like to receive the information generated in P2. Alternatively, for the AI, the LLM is prompted to rephrase the content produced in P2 in a manner that reflects a scenario where the information is being offered to the user.\nThe final proactive response R is obtained by combining the output of P\u2081 and the output of P3, \u0456.\u0435., R = LLM(P1) + LLM(P3), where + refers to the concatenate operation. We conduct simple post processing (rule-based removal of escape characters as well as excess spacing) on the output of each prompt to ensure the quality of the input to the subsequent prompt.\nIn the 1-shot and 3-shot versions, demonstration examples were not provided to P\u2081 as P\u2081 achieved good performance in the 0-shot setting. Additionally, since the reference information from which the response is based on is not readily available in our corpus, P2 and P3 would entail manually deriving the reference information for few-shot prompting.\n3-in-1 Chain-of-Thought (CoT) Prompt A drawback of the previous approach is the necessity for three distinct model inferences, leading to increased latency during generation. To address this, we attempt to consolidate all three prompts into a single 3-in-1 prompt. This unified prompt provides explicit instructions to the LLM to follow the exact same process as before in a step-by-step manner, encompassing all three subtasks within a single inference. We also implement a 0-shot, 1-shot, and 3-shot version of this prompt. Unlike the 3-stop CoT prompt, no manual derivation of specific information is required. Only the query and response, which are readily available, is required.\nDemonstration Selection We also perform demonstration selection using metrics outlined in Section 4.2. Specifically, we identify the top-k and bottom-k responses (for a k-shot prompt) using the following criteria: (1) the user-simulation score, (2) the semantic similarity score, and (3) the sum of both scores. Generally, we observe that using the sum of both scores results in the generation of high-quality responses that achieve high user-simulation and semantic similarity scores. Full results are provided in Appendix A.1."}, {"title": "6.2 Instruction Tuning", "content": "We also instruction tuned an LLM via QLoRA (Dettmers et al., 2023) to generate proactive responses. Leveraging our proposed corpus, we conducted instruction tuning on two distinct tasks corresponding to the generation of proactive responses with either a FQ or AI. We utilized 1000 proactive responses (500 from each proactive element)."}, {"title": "7 Experiments", "content": "Instruction TuningImplementation In our experiments, we utilize the 40b instruction-tuned Falcon LLM (Penedo et al., 2023) and the 13b StableVicuna LLM(Chiang et al., 2023). Results attained using StableVicuna are provided in the Appendix A.2. We utilize a temperature value of 0.2 for all generations. For each Proactive Element, we split our proactive dialogue corpus into two distinct sets: a 500-sample training set and a 500-sample test set. We select demonstration examples for our prompts from the training sets, and then evaluate them on the test set. We instruction-tune the LLM on the training sets for both the FQs and AI concurrently. The instructions used are identical to the direct prompt.\nMetric Correlations Table 1 shows the Point Biserial correlations between our new metrics and human annotations, calculated from a dataset of 500 positive samples from our corpus and 500 negative samples generated by prompting a LLM for subpar proactive responses that lack a proactive element, feature low-quality proactive element or are completely irrelevant with respect to the user's input.\nThe prompt-based baseline yields low correlation scores, highlighting its limitations as a metric. Conversely, the classification-based baseline achieve better, though inconsistent, correlations with human evaluations. Specifically, correlations for AI are higher than those for FQs. This difference arises because negative samples for AI, which mostly violate the Informativeness criteria, are simpler for the model to detect compared to the nuanced, generic responses that characterize negative samples for FQs, which violate the Specificity criteria. Future research could involve improving the correlations through further prompt engineering or by enriching the training dataset with more varied negative examples.\nThe proposed semantic and sentiment scores clearly outperform both baselines. The semantic metric, encompassing Relevance, Informativeness (AI), and Specificity (FQs), achieves the highest correlation scores. This aligns with expectations, as many negative responses lack the required Informativeness and Relevance. Conversely, the sentiment score focuses on Perspective and Conversational Naturalness, which are less common in negative samples. Therefore, we recommend using both metrics together to effectively evaluate response proactiveness, covering the criteria outlined in Section 3 comprehensively."}, {"title": "In-Context Learning", "content": "Scores attained by the direct, 3-step CoT, and 3-in-1 CoT prompts on Falcon-40b-instruct are shown in Table 2. A key finding is that the 3-step CoT prompt generally enhances 0-shot performance, addressing the general lack of proactive element seen in responses in the 0-shot direct and 3-in-1 CoT prompts, which generate fewer tokens in the 0-shot setting. The 3-step prompt resolves this by ensuring the final proactive response includes FQs or AI by concatenating outputs from the 1st and 3rd prompts.\nIt is also evident that the 3-step CoT prompt surpasses both the 3-in-1 CoT and direct prompts when it comes to the FQ. Conversely, for AI, the 3-in-1 CoT prompt outperforms both the 3-step CoT and direct prompts. This could be attributed to the inherent difficulty in generating high-quality FQs for the LLM, which generally excels at generating informative responses. Consequently, the FQ task benefits more from the 3-step CoT prompt since it breaks down the task into three simpler components.\nInstruction Tuning Table 2 also includes results for the instruction-tuned Falcon-40b-instruct, which produced responses similar to the 3-shot variants of the 3-step and 3-in-1 CoT prompts for FQs and AI, respectively. These responses strictly adhere to the structure outlined in Section 3. Compared to prompted responses, there are fewer instances of missing Answers or Proactive Elements. Instead, lower-quality responses lacked Specificity (FQs) or Conversational Naturalness (AI)."}, {"title": "8 Multi-turn Setting", "content": "To demonstrate the efficacy of our approach in the multi-turn conversations, we sampled 50 test cases from our dataset and interactions between a simulated user and an agent using Falcon-40b-instruct. We used 3-step and 3-in-1 CoT prompts with modifications to produce proactive responses, detailed in Appendix A.8.\nAfter conducting 50 simulations, we discovered that when the agent includes AI or FQ, the user is significantly more inclined to continue interacting with the agent. In contrast, responses lacking this proactive element usually consist of the agent merely acknowledging the information provided, naturally ending the conversation (Table 10). From the 50 simulations conducted, we found that approximately 94% of conversations ended after just one turn. In contrast, only 22% and 34% of interactions with the agent generating proactive responses with FQ and AI respectively ended after a single turn. On average, users continued the conversation for 3.9 turns with the FQ agent and 3.2 turns with the AI agent before ending the conversation naturally. For the FQ, the simulated user naturally requests the agent to provide the information suggested by the agent, further sustaining the interaction and improving the informativeness of the whole conversation (Table 11). For AI, the AI provided by the agent would tend to elicit more involved responses from the user rather than a cursory acknowledgement (Table 12) as well as encourage the user to inquire further about the AI provided by the agent."}, {"title": "9 Conclusion", "content": "However, both proactive elements displayed a tendency to repeat the proactive element from earlier in the conversation. We hypothesize that this issue could potentially be alleviated by improving quality of the LLM. To confirm our hypothesis, we repeat the experiment using GPT-4 instead of Falcon-40b-instruct for the Assistant. The sample conversations demonstrate that GPT-4 effectively minimizes such repetitions across up to four dialogue turns (Table 13). In our experiments, we apply our prompts at every conversational turn. However, in real-world ISD, not every turn would warrant a proactive response. Future work could constitute introducing an approach to detect if a proactive response is appropriate.\nIn this work, we propose a novel response-level definition of ISD proactivity. Per our definition, a proactive response includes both an Answer and a Proactive Element (FQ or AI). We compiled a dataset consisting of 2000 single-turn dialogues, and introduced a novel 3-step CoT and 3-in-1 CoT prompt that outperforms standard few-shot prompts in generating proactive responses. Future work could entail exploring finer-grained proactive elements or employing reward modelling and Reinforcement Learning with Human Feedback (RLHF) for fine-tuning. Expanding the current corpus to the multi-turn scenarios could also facilitate further research to improve in-context learning or supervised fine-tuning performance. Existing conversation-level metrics in ISD could also be enhanced to account for response-level proactivity. The performance of different LLMs on our task can also be explored."}, {"title": "10 Limitations", "content": "Firstly, the effectiveness of the generation approaches proposed are highly dependent on the LLMs that underpin them. Hence, different LLMs may display inherent biases or produce unforeseen outputs, resulting in lower quality response sets. Secondly, there are limitations based on the computational resources available. We do not have the capability to conduct in-context learning or instruction tuning experiments with larger or more recent LLMs. Future work could entail the evaluating the zero-shot performance of these LLMs on our proposed task. Thirdly, in this work, we do not assert that our prompt template is the optimal choice for proactive response generation. Our direct, CoT and 3-step CoT prompt templates are intended to form a baseline for researchers to improve upon. Additional work could entail additional, more deliberate prompt engineering."}, {"title": "11 Ethics Statement", "content": "We recruited annotators (\"Turkers\") through Amazon Mechanical Turk to build our dataset. Each Turker received detailed information about the Human Intelligence Task (HIT), including task descriptions, requirements and compensation, before agreeing to participate. They were free to withdraw from the task at any time for any reason. Each Turker was compensated at the rate of 0.20USD per HIT, and each HIT took an average of 55.6 seconds (12.90USD per hour)."}, {"title": "A Appendix", "content": "A.1 Demonstration Selection\nResults for 1-shot, 3-shot and 5-shot demonstration selection are presented in Table 3, 4, and 5 respectively.\nGenerally, the results attained align closely with our expectations. When we select demonstration examples using sentiment or semantic metrics as criteria, the resulting responses tend to achieve higher scores in the user simulation and semantic similarity scores respectively. For example, with regard to the FQ, selecting the bottom-1, 3, or 5 examples based on the semantic score would result in relatively generic FQs, which are reflected in the low semantic similarity scores. Similarly, for the AI, selecting the top-1, 3, or 5 examples based on sentiment score would result in responses with conversationally natural AI and high user simulation scores.\nAlso, while there is a slight decrease in semantic similarity score when bottom examples are selected based on semantic similarity for the AI, this drop is minimal. Especially when compared to the drop in user simulation score brought about by selecting the bottom examples based on sentiment for the FQ. This is primarily due to the fact that the responses in our dataset largely meet the criteria of Informativeness for the AI, leading to an overall high semantic similarity score. On the other hand, there is a relatively larger variance in terms of quality with regard to Specificity for the FQ (eg. 'Would you like to know more about Pom Guardians of the Galaxy 2?' vs 'Would you like to know who portrayed the character of Peter Quill in Guardians of the Galaxy 2?').\nAdditionally, it can be observed that while there is a relatively significant increase in performance between 1 and 3-shot prompts, the 3-shot and 5-shot prompts generally achieve comparable performance. It should also be noted that when we select demonstration examples based on the sum of the sentiment and semantic metrics, the generated responses exhibit balanced improvements across all criteria."}, {"title": "A.2 Stable Vicuna", "content": "The scores attained when direct, 3-step CoT, and 3-in-1 CoT prompting are applied to StableVicuna are provided in Table 6.\nGenerally, the trends observed in the results and responses attained via Falcon-40b-instruct can be observed in the case of StableVicuna. The 3-step CoT and 3-in-1 CoT prompts generally improve on 0-shot performance. Also, for the FQ, the performance of the 3-step CoT prompt exceeds both the 3-in-1 CoT and direct prompts. For AI, the 3-in-1 CoT prompt achieves better performance compared to both the 3-step CoT and direct prompts.\nIn addition, with the exception of the semantic similarity score, Falcon-40b-instruct generally attains higher scores across all metrics. When it comes to the semantic similarity, responses generated by StableVicuna and Falcon-40b-instruct attained comparable scores. This suggests that, in terms of providing AI, StableVicuna's responses exhibit a relatively lower level of Naturalness compared to Falcon-40b-instruct. In other words, the AI in the responses tend to be introduced in a relatively abrupt fashion as opposed to a conversation-ally natural manner. For the FQ, StableVicuna's responses exhibit a comparatively lower level of specificity when compared to those generated by Falcon-40b-instruct. The FQs from Stable Vicuna more often refer to general, broad areas which would likely require further specification from the user."}, {"title": "A.3 AMT Instruction", "content": "Throughout the data collection process, several pilot tests were conducted in order to refine the instructions provided to the turkers via AMT. The final instructions and interface utilized during data collection are provided in Fig 4 and 5 respectively. For both the FQ and AI, three turkers were engaged at a rate of 0.20USD per task (or HIT).\nA.3.1 Answer\nFirstly, turkers were instructed to amend a reference response for conversational naturalness to attain the Answer component. Initially, the turkers were instructed to input the Answer and the Proactive Element in a single input field. However, during the initial pilot tests, we found that numerous turkers simply input the reference response provided as is, without any amendment. The reference response corresponds to the short answer from the Natural Questions QA corpus, which consists of a single entity (eg. 'Pom Kleimentieff', '4th of July', or 'United States of America'). This negatively impacts the naturalness of the overall proactive response. We found that this issue can be addressed by breaking down the task into two distinct components with separate instructions and input fields."}, {"title": "A.3.2 Follow-up Question", "content": "For the FQ, turkers were told to formulate a FQ that references a specific piece of information in the reference text provided. The reference text corresponds to the long answer in the Questions QA corpus. The initial pilot tests revealed a strong tendency for turkers to input extremely short and generic questions (eg. 'Would you like to know more?', 'Are you interested in learning more?'). Hence, the final instructions explicitly highlight the importance of ensuring that the questions are as specific as possible, in addition to emphasizing that the question should not request any information from the user. Positive and negative examples were provided for the user's reference."}, {"title": "A.3.3 Additional Information", "content": "For the AI, the turkers were instructed to formulate a additional relevant information based on the reference text provided. Providing turkers with the reference text serves to ensure the factuality of the AI formulated. For this HIT, the main issue"}]}