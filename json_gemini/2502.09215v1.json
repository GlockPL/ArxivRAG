{"title": "Architecture for Simulating Behavior Mode Changes in Norm-Aware Autonomous Agents", "authors": ["Sean Glaze", "Daniela Inclezan"], "abstract": "This paper presents an architecture for simulating the actions of a norm-aware intelligent agent whose behavior with respect to norm compliance is set, and can later be changed, by a human controller. Updating an agent's behavior mode from a norm-abiding to a riskier one may be relevant when the agent is involved in time-sensitive rescue operations, for example. We base our work on the Authorization and Obligation Policy Language AOPL designed by Gelfond and Lobo for the specification of norms. We introduce an architecture and a prototype software system that can be used to simulate an agent's plans under different behavior modes that can later be changed by the controller. We envision such software to be useful to policy makers, as they can more readily understand how agents may act in certain situations based on the agents' attitudes towards norm-compliance. Policy makers may then refine their policies if simulations show unwanted consequences.", "sections": [{"title": "Introduction", "content": "This paper introduces an architecture for simulating the actions to be taken by an intelligent agent that is aware of norms (i.e., policies\u00b9) governing the domain in which it acts. We assume that different agents may exhibit different behavior modes with respect to norm-compliance: some may be very cautious and norm-abiding, while others may exhibit a riskier behavior. We consider the case in which the behavior mode under which an agent operates is set by a human controller who can update it if needed, for instance in cases when the agent is involved in a time-sensitive rescue operation.\nThis architecture is relevant to modeling physical intelligent agents that act autonomously, for in-stance robots deployed in harsh environments (underwater, on Mars, in mines), and whose settings may be re-adjusted by a human controller if the circumstances require it, but this is done sparingly in emergency situations. The architecture is also crucial to simulating the behavior of human agents with differ-ent norm-abiding attitudes, especially if such attitudes change over time. This can be of value to policy makers as testing their policies on different human agent models can lead to policy improvement, if un-wanted consequences are observed in the simulation (similarly to work by Corapi et al. [4] on creating use cases for policy development and refinement).\nIn our work, we utilize the Authorization and Obligation Policy Language (AOPL) by Gelfond and Lobo [8] for norm specification, due to its close connection to Answer Set Programming (ASP). In fact, the semantics of AOPL and the notion of norm-compliance are defined via a translation into ASP. This allows us to leverage existing ASP methodologies for representing dynamic domains, planning, and cre-ating agent architectures, as well as ASP solvers like CLINGO (https://potassco.org/clingo/) or DLV (https://www.dlvsystem.it/dlvsite/). A reason for using AOPL instead of representing norms directly in ASP (as soft constraints for example) is that AOPL provides policy analysis capa-bilities [10], which are important for checking that a policy imposed on an agent is actually valid and"}, {"title": "Background", "content": "In this section we introduce the norm-specification language AOPL and behavior modes for norm-aware agents. We assume that readers are familiar with ASP and otherwise direct them to outside resources on ASP [7, 11, 6]."}, {"title": "Norm-Specification Language AOPL", "content": "Gelfond and Lobo [8] introduced the Authorization and Obligation Policy Language (AOPL) for spec-ifying policies for an intelligent agent acting in a dynamic environment. A policy is a collection of authorization and obligation statements. An authorization indicates whether an agent's action is permit-ted or not, and under which conditions. An obligation describes whether an agent is obligated or not obligated to perform a specific action under certain conditions. An AOPL policy works in conjunction with a dynamic system description of the agent's environment written in an action language such as ALd [5]. The signature of the dynamic system description includes predicates denoting sorts for the elements in the domain; fluents (i.e., properties of the domain that may be changed by actions); and actions. An $AL_d$ system description defines the domain's transition diagram whose states are complete and consistent sets of fluent literals and whose arcs are labeled by action atoms (shortly actions).\nThe signature of an AOPL policy includes the signature of the associated dynamic system and additional predicates permitted for authorizations, obl for obligations, and prefer for specifying prefer-ences between authorizations or obligations. A prefer atom is created from the predicate prefer; similarly for permitted and obl atoms.\nAn AOPL policy P is a finite collection of statements of the form:\npermitted(e) if cond (1a)\n\u00acpermitted(e) if cond (1b)\nobl(h) if cond (1c)\n\u00acobl(h) if cond (1d)\nd : normally permitted(e) if cond (1e)\nd : normally \u00acpermitted(e) if cond (1f)\nd: normally obl(h) if cond (1g)\nd : normally \u00acobl(h) if cond (1h)\nprefer(di, dj) (1i)"}, {"title": "Behavior Modes in Norm-Aware Autonomous Agents", "content": "Harders and Inclezan [9] introduced an ASP framework for plan selection for norm-aware autonomous agents, where norms were specified in AOPL. They built upon observations by Inclezan [10] indicat-ing that, for categorical AOPL policies, all strongly-compliant actions are also weakly-compliant w.r.t. authorizations and that modality conflicts between authorizations and obligations may occur when the AOPL policy simultaneously contains obligations and prohibitions to execute an action. Instead, the notion of an underspecified event was introduced to denote an event that is not explicitly known to be compliant nor non-compliant w.r.t. authorizations, and a modality ambiguous event as an event arising from a modality conflict. Harders and Inclezan proposed that agents may have different attitudes towards norm compliance that would impact the selection of the \u201cbest\u201d plan. They called these attitudes behav-ior modes and introduced different metrics that can be used to express them. They also presented some predefined agent behavior modes, defined as follows: (a) Safe Behavior Mode \u2013 prioritizes events that are explicitly known to be compliant and does not execute non-compliant actions; (b) Normal Behavior Mode - prioritizes plan length and then actions explicitly known to be compliant, while not executing non-compliant actions; and (c) Risky Behavior Mode \u2013 disregards policy rules, but does not go out of its way to break rules either."}, {"title": "Example", "content": "For illustration purposes, consider a Mining Domain consisting of a 3x3 square grid of locations with an associated risk level (low, medium, or high) and three ores (gold, silver, and iron) with unique locations across the grid. The mining robot can collect ores or move between adjacent locations. The mining robot's goal is to collect all three ores. The norm that is imposed in this domain is that the collection of ores must happen in the sequence: gold first, then silver, and finally iron.\nThe mining robot has three behavior modes: Safe, Normal, and Risky, as defined in Section 2.2, but expanded with some additional policies. The Safe agent is obligated to move only through low-risk locations, the Normal agent is obligated to only move through low or medium-risk locations, and the Risky agent moves freely throughout the grid with no regard for the risk level of locations. Furthermore, as the Risky behavior mode does not have any regard for policies, an agent in this mode will collect ores in whichever order leads to the shortest plan possible.\nWe will now discuss a specific scenario within the mining domain shown in Figure 1. In this illus-tration, locations are labeled 10 to 18, with connected locations indicated by a black line. Each location is colored green, yellow, or red to indicate a low, medium, or high-risk level, respectively. The mining robot is depicted in its initial location and the locations of ores are indicated by their corresponding labels in the periodic table.\nNow let's consider behavior mode changes in this scenario. Table 1 shows the plan that is generated by a mining robot that begins in Safe mode, is switched by the controller to Normal mode at time step 3, and is switched again to Risky mode at time step 7. In case of emergency or changing priorities, a more risky behavior may be desired by the controller of such a robot, even though it does result in a higher degree of danger for the robot. We want our architecture to simulate such behavior mode modifications."}, {"title": "Architecture", "content": "We identified three questions that needed to be answered during the development of this architecture, outlined below together with our design decisions:\n\u2022 How will an agent adjust its plan when its behavior mode is modified?\nDesign Decision: The agent will devise a new plan with its new behavior mode, starting at the time step that the behavior mode modification is set to take effect.\n\u2022 How will the agent's memory mechanism work with respect to already executed actions of a plan? In other words, how will the agent deal with prior actions that may not satisfy the definition of its new behavior mode?\nDesign Decision: The agent remembers the behavior mode under which it operated at each point in time and checks requirement satisfaction w.r.t. to the behavior mode settings in place when the action was executed, to mimic real world situations where new laws are not applied retrospectively.\n\u2022 Does the agent need to be aware that its behavior mode is liable to be modified at later points in time?\nDesign Decision: The agent is not explicitly aware that its behavior mode can be modified. How-ever, we introduce the concept of subgoals so that the agent can strive to partially complete its overall goal if its current behavior mode prevent completing the goal as a whole.\nThe proposed architecture consists of two distinct components that work in conjunction: an ASP Component and a Python Component, discussed in detail in the following subsections. Figure 4 provides an overall view of the proposed architecture."}, {"title": "ASP Component", "content": "The ASP Component consists of five pieces: the dynamic domain encoding, scenario encoding, policy encoding, behavior mode encoding, and learned information.\nThe dynamic domain encoding contains the objects, statics, fluents, actions, and axioms that define the dynamic domain, encoded according to established ASP methodologies [6]. In the Mining Domain, the objects are locations and ores. Statics are used to describe whether two locations are connected, as well as the risk level of a location (has_risk_level(l,level)). Inertial fluents at play are the agent's location (at_loc(l)); whether the agent possesses a certain ore or not (has_ore(o)); and the locations of the ores (ore_loc(o,l)). The agent can perform two actions: move from one location to another (move(l1,l2)); and collect an ore (collect(o)). It has an additional action wait that does not change the state of the domain.\nThe scenario encoding consists of a list of facts that are true at time step 0. For the Mining Domain, this means specifying the risk level of each of the locations, the initial location of the agent, and the location of the ores.\nThe policy encoding contains the ASP translations of the AOPL policies that govern the dynamic domain. For the Mining Domain, there is only one policy that applies by default: the agent is obligated to collect the ores in the sequence gold, silver, iron. This is encoded in two separate AOPL rules one that says the agent is obligated not to collect silver unless it possesses gold and another that says the agent is obligated not to collect iron unless it possesses silver:\nobl(\u00accollect(silver)) if \u00achas_ore(gold)\nobl(\u00accollect(iron)) if \u00achas_ore(silver)"}, {"title": "", "content": "The behavior mode encoding in this architecture considers tree behavior modes: Safe, Normal, and Risky. As previously mentioned, the exact definitions of these behavior modes are meant to be tailored to the dynamic domain's specific needs. For example, in the Mining Domain, the Safe and Normal agents are under additional policies. Specifically, the Safe agent is obligated not to move through high or medium-risk locations, and the Normal agent is obligated not to move through high-risk areas. These rules are written in AOPL, as shown below for the Safe agent, and are translated into ASP to be used in our architecture:\nobl(\u00acmove(L1,L2)) if has_risk_level(L2,high)\nobl(\u00acmove(L1,L2)) if has_risk_level(L2,medium)\nThe behavior mode encoding also contains general ASP rule for planning, such as:\n1 {occurs(A,I) : action(A)} 1:- step(I), I >= n1.\nThis rule says that at each time step I \u2265 n1, the agent must perform exactly one action. The constant, n1, is an integral part of this architecture: it represents the time step in which the new behavior mode is to take effect. For example, if we are in a scenario where we want the agent's initial behavior mode to be b\u2080 and switch to b\u2081 at time step i, then n1 = 0 for each time step t < i, and n1 = i for each time step t \u2265 i. This ensures that only the planned actions at time steps greater than or equal to i have to obey the definition of behavior mode b\u2081.\nIn each of the behavior mode's encodings, we additionally have several metrics that are calculated and considered by the agent when devising its plan, as in work by [9]. What differentiates each of the behavior modes is the priority that is given to each of the metrics in the planning process, as described in Section 2.2. For example, in the Safe behavior mode's encoding, we see the following ASP rule:\n#maximize{ N404 : subgoal_count(N4);\nN303 : percentage_strongly_compliant(N3);\nN202: percentage_underspecified(N2);\nN101: wait_count (N1)}.\nThis says that the metric subgoal_count should be prioritized first, percentage_strongly_comp-liant should be prioritized second, percentage_underspecified should be prioritized third, and wait_count should be prioritized last. The subgoal_count metric is a count of the number of subgoals that the agent completes during the plan. This is a novel inclusion in our proposed architecture. In the Mining Domain, the maximum number of subgoals that the agent can complete is three, one subgoal corresponding to the collection of each of the ores. The ASP encoding for this is:\nsubgoal (has_ore(gold)). subgoal(has_ore(silver)). subgoal(has_ore(iron)).\nsubgoal_count(N) :- #count{F : subgoal(F), holds(F, n)} = N.\nThe percentage_strongly_compliant and percentage_underspecified metrics come from work by Harders and Inclezan [9]. Recall that a strongly-compliant action is one that is explicitly permit-ted by the agent's policies and an underspecified action is one that is neither permitted nor not permitted by the agent's policies. The safe agent prioritizes actions that are explicitly permitted, because it is de-signed to act in an extremely cautious way, even if unnecessary. Finally, the wait_count metric is a count of the number of wait actions in the agent's plan. The higher the wait_count, the shorter the plan. Agents under this proposed architecture only perform waiting actions after they have completed as many subgoals as possible. This is encoded in ASP as:\n:- occurs(wait, I1), occurs (A, I2), I2 > I1, I1 >= n1, A != wait.\nNow that we have an understanding of what each of these metrics represent, let's compare the prior-itization order of the Safe agent to that of the Normal agent.\n#maximize{ N404 : subgoal_count(N4);\nN303 : wait_count(N3);\nN202: percentage_underspecified(N2);\nN101: percentage_strongly_compliant(N1)}."}, {"title": "", "content": "The Normal agent still prioritizes first completing as many subgoals as possible, but instead of also trying to maximize the number of strongly-compliant actions in the plan, it values a shorter plan. Additionally, both the Safe and Normal agent behavior modes have constraints saying that no non-compliant actions w.r.t. obligations are allowed. The Risky agent only considers two metrics in its planning process, subgoal_count and wait_count, in that order. This allows the Risky agent to devise the shortest plan possible while completing as many subgoals as possible, with the trade-off that it completely disregards any policies that are imposed on it. The ASP encoding is:\n#maximize{ N202: subgoal_count(N2); N1@1: wait_count(N1)}.\nFinally, the learned information refers to the facts formed by holds and occurs literals that are true prior to the time step when the behavior mode modification took effect."}, {"title": "Python Component", "content": "The Python Component of the proposed architecture is what allows us to manage the behavior mode modification process. This component utilizes the CLINGO Python API, which allows developers to solve ASP programs and analyze their output using Python code. For the Mining Domain, we present a class called MiningDomainSolver, which takes as input a scenario number, an initial behavior mode, and a list of behavior mode changes and the time steps when they are to take effect. Once this class is instantiated, a user may call the class's function called generate_plan_with_bmode_changes(), which returns the plan as a string. This function follows the control flow outlined below. It is also worth noting that this control flow is not specific to the Mining Domain, and can be applied to any other dynamic domain under this proposed architecture:\n1. n1 is computed. As mentioned previously, n1 = 0, when we are solving the ASP program cor-responding to the initial behavior mode, and n1 is equal to the time step of each behavior mode change after that.\n2. The ASP program is created inside of a string variable by reading the contents of the text files of the dynamic domain (i.e., the dynamic domain encoding, scenario encoding, policy encoding, and behavior mode encoding). Learned information (stored inside of a string variable) is also added to the ASP program.\n3. A temporary text file is created and the ASP program is written to it.\n4. A CLINGO control object is created using the CLINGO API, and the temporary file is loaded into it using its provided load() function.\n5. The ASP program is solved using the solve() function of the control object. This function solves the loaded ASP program and outputs the literals in the answer set that are specified using CLINGO'S #show directive in the ASP program.\n6. If there is a behavior mode change, these literals are saved to a class variable and filtered to produce the learned information for the next iteration."}, {"title": "Software System", "content": "Next, let us discuss the graphical user interface (GUI) that was developed as a proof of concept for a program that allows a controller to make behavior mode modifications of an agent. The software is available at https://github.com/scglaze/MiningRobotDomainGUI."}, {"title": "Evaluation", "content": "Runtime Performance: We ran experiments on the 10 scenarios in the Mining Domain. We measured the runtime performance for each of the three behavior modes by themselves, and for the six combina-tions of first-order behavior mode modifications (i.e. only one modification made during the plan). We varied the time step when the modification occurred from scenario to scenario, based on what we sub-jectively deemed as leading to most illustrative changes in plans. Additionally, we measured the runtime performance of second-order behavior mode modifications for two of the scenarios that are more com-plex. We present the runtime performance of Scenario #4 from Figure 3 in Table 3 and Scenario #9 from Figure 4 in Table 4. Time steps when behavior mode modifications are made are listed in parenthesis. Scenario #9 involves second-order behavior mode modification. The runtime performance that we report does not come from the CLINGO solver itself, but instead, is measured via code that was integrated into the Python component for this experiment. This test code utilizes the time Python library. The reason we went this route instead of measuring the reported runtime from CLINGO, is that our proposed archi-tecture requires an additional solve() for each behavior mode modification that is made. We ran each experiment 10 times, and report the mean in seconds (T (s)) and standard deviation (SD). All experi-ments were performed on a machine with an Intel(R) Core(TM) i7-1065G7 CPU @ 1.30GHz 1.50 GHz processor and 16 GB RAM.\nWe see that the Safe behavior mode has a slightly longer runtime than that of the Normal behavior mode, and that the Risky behavior mode has the shortest runtime overall, which is a trend observed across all 10 scenarios. This is intuitive, as the Safe behavior mode takes the most amount of factors (i.e., aggregates) into consideration during plan generation, and the Risky behavior mode takes significantly less factors into consideration. Similarly, we observe that the runtime of behavior mode modifications (with the time step when the modification occurs specified in parenthesis) follows this same trend modifications involving the Risky mode add less runtime than either of the other behavior modes, and the Safe mode adds the most to runtime. Scenario #9 additionally tests second-order behavior mode modifications. We selected modifying the agent's behavior mode from Safe, to Normal, to Risky because the agent begins in a safe area of the grid, where the gold is also located. The silver is located adjacent to the gold but in a medium-risk location that the Safe agent cannot access. Therefore, the Safe agent will wait indefinitely, unless there is a behavior mode modification. This behavior is mirrored by the Normal agent after it collects the silver. Hence, the plan generated by the agent with behavior mode parameters"}, {"title": "", "content": "\"Safe to Normal (2) to Risky (4)\" is set up to collect the ores without the Safe or Normal agents waiting at all, and the agent with behavior mode parameters \"Safe to Normal (3) to Risky (6)\" is set up for the Safe and Normal agents to wait for exactly 1 time step before its behavior mode is modified to a more Risky one that allows them to complete another subgoal. An interesting observation is that the agent with behavior mode parameters \"Safe to Normal (3) to Risky (6)\" has a faster runtime than that with parameters \"Safe to Normal (2) to Risky (4).\" We speculate that this is because the plan that is generated by the Normal agent at time step 3 has a shorter span of time steps to plan for than when starting at time step 2, and likewise for the Risky agent at time step 6 versus 4.\nWe also ran experiments on the 14 scenarios of the Room Domain by Harders and Inclezan [9]. The results are in Table 5 and they match the observations for the Mining Domain.\nGUI Usability Study: The final evaluation was a usability study for the GUI that we presented in Section 5. Our participants (N = 6) were given a brief explanation of the Mining Domain, and necessary background information on ASP planning. Then, they were asked to download and run an executable file for the GUI seen in Figure 3, and to test all 10 scenarios with different behavior mode parameters. Finally, they answered questions on a 5-point Likert scale. The average score and standard deviation for each question's responses are reported in Table 6. While scores were generally high, especially for question 6, we do note the lower scores for questions 1, 2, 8. This indicates that the prototype GUI can be improved by using more modern-looking widgets, facilitating the process of downloading it, and providing more descriptive error messages that are displayed when input validation checks that are violated."}, {"title": "Related Work", "content": "Our work expands on Harders and Inclezan's [9] notions of behavior modes w.r.t. norm-compliance. Another work on norm-aware agents is that by by Meyer and Inclezan [12] who created the APIA architecture for norm-aware intentional agents. APIA agents operate with activities instead of simple"}, {"title": "Conclusions and Future Work", "content": "We presented an ASP framework that defines how the controller of norm-aware autonomous agents can modify their behavior modes under the plan-choosing framework proposed by [9]. We introduced a Python component that includes a wrapper class that can be used as the behavior mode-changing mechanism and a GUI with the potential for generalization, for other domains as well.\nIn the future, one could generalize this proposed ASP simulation framework so that a controller can manipulate multiple agents' behavior modes as they work toward achieving their goal(s). Optimizing ASP encodings and Python code is another future goal, as it would allow for larger and more complicated dynamic domains to be simulated. Finally, one could continue to develop additional behavior modes, outside of the three that are used in this framework. This would allow for more nuanced agent behavior to be modeled under this framework and generally in ASP."}]}