{"title": "Advanced computer vision for extracting georeferenced vehicle trajectories from\ndrone imagery", "authors": ["Robert Fonoda", "Haechan Cho", "Hwasoo Yeo", "Nikolas Geroliminis"], "abstract": "This paper presents a comprehensive framework for extracting georeferenced vehicle trajectories from high-altitude\ndrone footage, addressing key challenges in urban traffic monitoring and limitations of traditional ground-based systems.\nOur approach employs state-of-the-art computer vision techniques and deep learning models to create an end-to-end\npipeline that significantly enhances vehicle detection, tracking, and trajectory stabilization. Conducted in the Songdo\nInternational Business District, South Korea, the study utilized a multi-drone experiment over 20 intersections, capturing\napproximately 12TB of ultra-high-definition video data over four days. We developed a novel track stabilization method\nthat uses detected vehicle bounding boxes as exclusion masks during image registration, which, combined with advanced\ngeoreferencing techniques, accurately transforms vehicle coordinates into real-world geographical data. Additionally,\nour framework includes robust vehicle dimension estimation and detailed road segmentation, allowing for an in-depth\nanalysis of traffic dynamics. The framework produced two high-quality datasets: the Songdo Traffic dataset, compris-\ning nearly 1 million unique vehicle trajectories, and the Songdo Vision dataset, containing over 5,000 human-annotated\nframes with about 300,000 vehicle instances in four classes. Comparisons between drone-derived data and high-precision\nsensor data from an instrumented probe vehicle highlight the accuracy and consistency of our framework's extraction in\ndense urban settings. By publicly releasing these datasets and the pipeline source code, this work sets new benchmarks\nfor data quality, reproducibility, and scalability in traffic research. The results demonstrate the potential of integrating\ndrone technology with advanced computer vision tools for precise and cost-effective urban traffic monitoring, providing\nvaluable resources for the research community to develop intelligent transportation systems and improve traffic manage-\nment strategies.", "sections": [{"title": "1. Introduction", "content": "Traditional traffic monitoring methods, such as loop detectors and manual counting, are becoming obsolete in the era\nof smart cities due to their inflexibility and limited scope [1]. Stationary camera surveillance, although effective, presents\neconomic challenges due to high installation costs and limited field of view (FoV), leading to blind spots. Data from\nglobal navigation satellite system (GNSS) and connected vehicles also lack the versatility needed for comprehensive\ntraffic monitoring. Several vehicle detection and tracking technologies have emerged, including camera-based [2-4],\nLiDAR-based [5, 6], magnetics-based [7, 8], and radar-based [9, 10] systems. While each offers certain advantages, they\nstill face limitations in adaptability and coverage. Accurate estimation of traffic states allows for adjustments in traffic\ncontrol strategies [11], enhancing network utilization [12, 13], improving road user experiences, and yielding economic\nand ecological benefits.\nThe integration of unmanned aerial vehicles (UAVs), commonly known as drones, and computer vision (CV) tech-\nniques presents a promising solution to these challenges. Originally developed for military purposes [14], UAVs have\nsince found applications in various civilian domains, including urban traffic monitoring and management [15-17]. Ad-\nvances in drone technology, combined with sophisticated CV tools, enable the extraction of detailed vehicle trajectories\nfrom high-altitude drone footage, facilitating spatio-temporal analyses of dynamic entities such as vehicles, pedestrians,\nand cyclists [18-20]. Compared to traditional sensors, these trajectories offer new opportunities for studying complex\ntraffic phenomena, such as large-scale network modeling [21], on-street parking [22], as well as emission [23] and\nnoise [24] estimation. This aerial perspective, free from the constraints of fixed infrastructure, provides an adaptable and\ncost-effective means to observe urban mobility patterns in congested environments, thereby supporting responsive traffic\nmanagement strategies in smart cities [25-27].\nIn a collaborative effort between KAIST and EPFL, a unique multi-drone experiment was conducted in the Songdo\nInternational Business District, South Korea, from October 4 to 7, 2022, excluding Monday due to adverse weather\nconditions. A fleet of 10 commercial-off-the-shelf drones synchronized their operations to monitor 20 busy intersections,\nas depicted in Fig. 1. Advanced flight plans, combining both hovering and transitioning phases, were executed to optimize\ncoverage. The drones maintained an altitude of 150 meters, with some reduced to 140 meters to mitigate collision risks.\nAdopting a bird's-eye view (BEV) perspective, the drones captured footage at 4K ultra-high-definition (UHD) resolution\n(3,840 x 2,160 pixels) at a smooth frame rate of 29.97 frames per second (FPS). Each day included 10 synchronized\nflight sessions per drone during peak morning and afternoon periods, each session lasting approximately 30 minutes\nand followed by a brief battery replacement. This experiment generated 12TB of raw video data and utilized 13 real-\ntime kinematic (RTK)-calibrated ground control points (GCPs) for precise georeferencing. A dedicated drone captured\noverlapping images at an altitude of 75 meters to create a highly accurate orthophoto of the experiment area, providing a\nsolid foundation for advanced transportation analysis.\nThis work introduces a comprehensive end-to-end trajectory extraction pipeline that employs advanced CV algo-\nrithms to derive detailed traffic data from high-altitude drone footage. Although a detailed experiment design and an\nin-depth analysis of the extracted data are outside this paper's scope, our contributions significantly expand the resources\navailable to the research community. Specifically, we introduce the Songdo Traffic dataset [28], comprising nearly 1 mil-\nlion unique vehicle trajectories, making it one of the largest public traffic datasets collected from a smart city worldwide.\nThis extensive collection provides unparalleled insights into urban mobility, serving as a valuable resource for traffic\nmanagement and research. The dataset includes trajectory data at a high frequency of 29.97 points per second, com-\nplemented by various metadata. Songdo Traffic encompasses vehicle trajectories in multiple coordinate systems (CSs),\nincluding global geographic, local Cartesian, and orthophoto CS. Each trajectory is further annotated with metadata such\nas vehicle identifier (ID), type, and, when feasible, vehicle dimension estimates. Additionally, each trajectory point is\ntagged with a precise ISO-formatted timestamp, along with instantaneous speed and acceleration estimates, road section\nand lane number information, and a visibility flag, where available.\nComplementing Songdo Traffic, the Songdo Vision dataset [29] addresses the scarcity of detailed vehicle annotations\nnecessary for training object detection models, particularly from high-altitude BEV aerial perspectives, where smaller\nvehicles, like motorcycles, are often less discernible. This dataset contains over 5,000 human-annotated video frames,\nfeaturing nearly 300,000 vehicle instances across four distinct classes: cars (including vans and light-duty vehicles),\nbuses, trucks, and motorcycles. To facilitate broader use, annotations are provided in multiple formats compatible with"}, {"title": "2. Related work", "content": "This section reviews the SOTA in drone-based traffic data extraction, including object detection, tracking, video stabi-\nlization, georeferencing, and dataset availability. Traditional CV methods for fixed cameras, such as background subtrac-\ntion [33], optical flow (OF) [34], template matching [35], feature matching [36], and histogram-based methods [37, 38],\nface challenges in complex environments [39] and under varying illumination conditions [4, 40, 41]."}, {"title": "2.1. Object detection", "content": "The rise of DL techniques for CV has significantly improved vehicle detection. Leading architectures include\nsingle shot detector (SSD) [42], region-based convolutional neural network (R-CNN) [43], and you only look once\n(YOLO) [44]. R-CNN variants, such as Faster R-CNN [45], have been used for vehicle detection in various stud-\nies [46, 47]. YOLO models (v3-v10) [48\u201355] have also been adapted for accuracy in traffic surveillance [17, 56, 57].\nDespite their effectiveness, these models often rely on non-maximum suppression (NMS), introducing post-processing\ncomplexities. Recent transformer-based detectors (DETRs) [58-61] aim to eliminate NMS but struggle with small object\ndetection and require high computational resources. Specialized detectors, such as YOLO for occluded vehicles [62]\nand the Butterfly detector [63], address challenges specific to aerial perspectives. Integration of multi-modal data, like\ncombining RGB with LiDAR [64] or thermal [65] imaging, has also shown promise in enhancing detection in complex\nurban environments."}, {"title": "2.2. Object tracking", "content": "Accurate tracking is essential for reliable movement data. Traditional methods [66], like Kalman filtering [67] and\nthe Hungarian algorithm [68], have been widely used in traffic analysis [47, 69-71]. However, these approaches often\nstruggle with drone-captured footage complexities, especially in dynamic urban environments where occlusions, varying\nobject sizes, and diverse motion patterns pose significant challenges.\nModern multi-object tracking (MOT) algorithms, such as simple online and realtime tracking (SORT) [72], observation-\ncentric SORT (OC-SORT) [73], ByteTrack [74], and \"bag of tricks\u201d SORT (BoT-SORT) [75], follow the separate de-\ntection and tracking (SDP) paradigm. Hybrid models, including DeepSORT [76], StrongSORT [77], and BoT-SORT-\nReID [75], incorporate appearance information to reduce identity switches and handle occlusions more effectively. End-\nto-end tracking approaches like FairMOT [78] combine detection and tracking but face challenges in BEV perspectives\ndue to limited ground truth data and high computational demands.\nCurrent challenges in MOT include persistent track loss in occluded areas, object similarity, and high computational\ncosts. These issues may be particularly acute in dense urban environments. While benchmarks like MOTChallenge [79\u2013\n81] have advanced pedestrian tracking, vehicle tracking in BEV contexts remains underrepresented."}, {"title": "2.3. Stabilization", "content": "Unlike larger drones equipped with high-performance gimbals, video stabilization is crucial for accurate traffic data\nextraction with smaller commercial drones that are more susceptible to environmental disturbances like wind [82, 83].\nStabilization, a form of video registration [84], involves aligning consecutive frames by matching keypoints to cor-\nrect unwanted motion. Various keypoint detectors such as scale-invariant feature transform (SIFT) [85], root SIFT\n(RSIFT) [86], binary robust invariant scalable keypoints (BRISK) [87], KAZE [88], accelerated-KAZE (A-KAZE) [89],\nand oriented FAST and rotated BRIEF (ORB) [90] are commonly used. OF methods, like the Kanade-Lucas-Tomasi\n(KLT) tracker [91], are faster but can be less reliable [92]. Recent DL-based methods such as SuperPoint [93] and\nSuperGlue [94] enhance keypoint detection and matching, although they often require fine-tuning with ground truth data.\nKeyframe correspondences established through these features allow for the computation of either perspective or affine\ntransformations using the least squares method, optimized by variants of the random sample consensus (RANSAC) al-\ngorithm [95] for outlier rejection. Feature matching algorithms, such as brute force (BF) or fast library for approximate\nnearest neighbors (FLANN) [96], compute similarity scores between extracted keypoints to form correspondences. Pre-\nprocessing steps, like grayscale conversion and contrast limited adaptive histogram equalization (CLAHE), are imple-\nmented to enhance keypoint detection in varying lighting conditions.\nVarious methods can be combined to align video frames, allowing for smoother vehicle tracking. For instance, [82]\nutilized speeded-up robust features (SURF) features for keypoint detection to ensure smooth traffic analysis. However, the\nmain challenge in stabilization is achieving an optimal balance between speed and accuracy while mitigating distortions\ncaused by moving objects."}, {"title": "2.4. Georeferencing", "content": "Georeferencing is crucial for converting trajectories into real-world coordinates. Accurate georegistration often re-\nquires a precise georeferenced image of the area [84, 97, 98]. An orthophoto, a geometrically corrected (orthorectified)\nimage that eliminates distortions caused by camera tilt, lens distortion, and topographic relief, is a prevalent method\nfor this purpose [99]. This correction allows orthophotos to represent true distances accurately, making them essential\nfor measurements and analyses. Nonetheless, previous studies often omit meticulous consideration of accurate georef-\nerencing, relying on simplistic methods based on ground sample distance (GSD) for estimating only a subset of traffic\nparameters, such as vehicle speed and location derived from drone's location and GSD-scaled image coordinates. How-\never, these methods lack the accuracy required for traffic monitoring, particularly since commercial-off-the-shelf drones\noften exhibit positional errors greater than claimed [100].\nMore sophisticated georeferencing involves the use of geographic information system (GIS) software [17] for manual\nalignment of imagery. While more accurate than basic GSD methods, this approach is time-consuming and demands\nsignificant expertise. Alternatively, the use of RTK-GNSS significantly enhances georeferencing accuracy by refining\nboth the GCPs placement and the drone's position during image capture. RTK-GNSS achieves centimeter-level precision\nthrough corrections provided by a nearby base station, thereby facilitating the generation of highly accurate orthophotos\nfor the surveyed area.\nBy combining accurate orthophotos with automatic image matchings, georeferencing becomes more precise, reduc-\ning the reliance on manual interventions. This process not only streamlines the trajectory extraction pipeline but also\naddresses the inaccuracies commonly associated with GSD or GIS-based methods, enabling more reliable mapping of\nvehicle movements in real-world coordinates."}, {"title": "2.5. Availability of datasets", "content": "Recent datasets like HighD [101], HARPY [102], pNEUMA [27], and CitySim [103] focus on specific traffic sce-\nnarios, such as single intersections or highway segments, and often use outdated CV methods. HighD and HARPY, for\ninstance, utilized U-Net and YOLOv2 for detection but lacked robust trajectory validation. PNEUMA, which employed\nstatic drone swarms, provides insights into traffic flow but experiences imprecise BBs and trajectory anomalies [20, 104],\npossibly due to visual limitations or suboptimal CV tools. Meanwhile, CitySim's methodologies, including its georefer-\nencing algorithm, remain undisclosed, and its traffic data lacks validation.\nVehicle annotation datasets, such as CARPK [105], CyCAR [106], HARPY [107], RAI4VD [108], UAVDT [109],\nUIT-ADrone [110], and VisDrone [111], serve as valuable resources for object detection training. However, they often\nemploy basic annotation techniques and cover a limited, sometimes inconsistent, range of vehicle classes. This variability\nin annotation accuracy can impact the data's reliability for training DL models."}, {"title": "2.6. Summary", "content": "In summary, while the potential of drones and CV in urban traffic data extraction is evident, challenges such as di-\nverse object scales, occlusions, and the scarcity of comprehensive datasets persist [4, 112]. Most existing approaches\nare optimized for static cameras and lower altitudes, limiting their effectiveness in BEV perspectives, where objects ap-\npear smaller and less distinctive. Video stabilization and georeferencing are critical for enhancing extraction pipelines.\nHowever, common methods like GSD-based and manual GIS georeferencing often fall short in accuracy. Incorporat-\ning RTK-GNSS for orthorectification with GCPs and advanced stabilization techniques enables more reliable trajectory\nextraction from high-altitude drone imagery. These advancements, coupled with improved datasets, can significantly\ncontribute to effective traffic monitoring and urban planning."}, {"title": "3. Methodology", "content": "Our methodology for transforming raw drone footage into detailed traffic data, which is compiled into the Songdo\nTraffic dataset, involves several key steps, as shown in Fig. 2. First, in the data wrangling step, the raw videos are\nprocessed into meaningful segments. In parallel, selected frames are annotated to identify vehicle classes, which are used\nto train and validate an object detection algorithm, and are released as part of the Songdo Vision dataset. The detection\nalgorithm processes the segmented videos to detect and locate vehicles. Next, a MOT algorithm assigns unique IDs and\ntracks vehicles across frames. Image registration methods stabilize the footage and transform vehicle coordinates into\na fixed reference frame. Georeferencing then converts these coordinates into real-world values using homographic and\naffine transformations, along with an orthophoto. Finally, the extracted trajectories are used to derive useful metadata,\nincluding vehicle dimensions, speed, acceleration, road segment, and lane number. By applying track stabilization after\nobject tracking, we are able to bypass the limitations of traditional video stabilization and achieve more accurate results."}, {"title": "3.1. Data wrangling", "content": "During the data wrangling phase, we segmented 12TB of raw UHD drone videos, saved in .mp4 format at 29.97 FPS,\nalong with their corresponding flight logs, into distinct segments. Each video segment corresponds to a stable hovering\nperiod of an individual drone over a monitored intersection (see Fig. 1)."}, {"title": "3.2. Data annotation", "content": "The unique challenges of the Songdo study, particularly the BEV aerial perspective at 140-150m altitudes, required\nthe creation of a specialized vehicle annotation dataset. Existing public datasets often lack the necessary quality, with\nvarying perspectives, resolutions, and object sizes that do not align with our study's specific conditions. Additionally,\ntheir annotations are frequently inaccurate, with poor-quality BBs that can compromise the performance of detection\nmodels. To ensure reliable object detection directly within the Songdo experiment's environment, we constructed a\ncustom dataset tailored to our specific needs.\nOur dataset categorizes vehicles into four classes: cars (including vans and light-duty vehicles), buses, trucks, and\nmotorcycles, specifically designed to enhance model training and evaluation (see Section 3.3). By integrating diverse\nscenes from multiple public datasets, we aimed to improve the model's accuracy and robustness for our task. A total\nof 5,419 video frames were annotated, capturing nearly 300,000 vehicle instances with tight, axis-aligned BBs, see\nexamples in Fig. 3 (left). The annotations, summarized in Table 2, were conducted by trained annotators using Azure\nML Studio to maximize precision. Following open science principles, the Songdo Vision dataset will be made publicly\navailable alongside the traffic dataset.\nOf the total frames annotated, 5,274 were randomly selected from the merged drone videos (described in Section 3.1),\ncovering both hovering and transitioning sequences. The remaining 145 frames were carefully chosen to include un-\nderrepresented scenarios, such as motorcycles at pedestrian crossings, bicycle lanes, near traffic light poles, and other\ndistinctive road markers, as shown in Fig. 3 (right). This selection process was meticulously managed to include only\nfootage from suitable altitudes and camera angles, ensuring the integrity and accuracy of the dataset.\nThe Songdo Vision dataset, enriched with human-verified annotations, aims to significantly improve vehicle detection\nalgorithms, providing a reliable ground truth for model validation. We plan to release the annotations in widely-used\nformats such as COCO, YOLO, and Pascal VOC, ensuring compatibility with a broad range of applications beyond\ntraffic analysis, including autonomous drone delivery, surveillance, safety systems, and environmental studies."}, {"title": "3.3. Object detection", "content": "Object detection is a pivotal component of our vehicle extraction framework. We employ a DL-based object detector\nthat predicts BBs encoded as (x, y, w, h). Here, (x, y) represent the center coordinates of the BB, while w and h denote\nits width and height, respectively. Each k-th image frame of video v, denoted F, is a $w_{i} \\times h_{i}$ matrix of pixel intensities,\nwhere $w_{i}$ and $h_{i}$ represent the frame's width and height in pixels, respectively.\nOur object detector processes each frame F to identify potential vehicles by predicting BBs. Following this initial\ndetection step, our pipeline employs a class-agnostic NMS algorithm with an intersection over union (IoU) threshold\n$\\tau_{NIOU} > 0$. This crucial step effectively eliminates overlaps among the BBs, thereby refining the overall detection quality in\ndensely populated urban settings. The refined set of detections, $D_{v}[k]$, includes only the most reliable vehicle detections\nand is defined as follows:\n$D_{v}[k] = \\{(x^{(i)}[k], y^{(i)}[k], w^{(i)}[k], h^{(i)}[k], c^{(i)}[k], s^{(i)}[k]) \\ | \\ s^{(i)}[k] \\geq \\tau_{d} \\}_{i=1}^{N_{d}[k]}$\nwhere $N_{d}[k]$ represents the number of vehicle detections in the k-th frame after processing through NMS. For the i-th\ndetection, $x^{(i)}$ and $y^{(i)}$ are the normalized center coordinates of the BB, $w^{(i)}$ and $h^{(i)}$ are its normalized width and height,\n$c^{(i)}$ is the class label restricted to $c^{(i)} \\in C = \\{0, 1, 2, . . ., C - 1\\}$, with C > 0 representing the total number of distinct object\nclasses recognized by the detector, and $s^{(i)}$ is the confidence score constrained within (0, 1]. Detections are retained in\n$D[k]$ only if their confidence scores exceed the detection threshold $\\tau_{d} > 0$. All BB coordinates and dimensions are\nnormalized with respect to the input image size, ensuring that 0$\\leq x^{(i)}, y^{(i)}, w^{(i)}, h^{(i)} \\leq 1$.\nWe employed the anchor-free YOLOv8 detector architecture [53], which has demonstrated superior performance\nacross all metrics, establishing it as one of the top-performing object detectors available at the onset of our research.\nCompared to the anchor-based YOLOv5 [50] and transformer-based real-time DETR (RT-DETR) [61], YOLOv8 offers\nenhanced accuracy and faster inference speeds, particularly for smaller object, making it more suitable for our operational\nrequirements. After extensive trials, the 's' model scale with standard output heads was selected for its optimal balance\nof accuracy and inference speed. While we experimented with an additional P2/4 output layer, as utilized in our previous\nYOLOv5 setup[17], no significant performance gains were observed within the YOLOv8 architecture. This architecture\nwas ultimately configured with $\\tau_{d}$ = 0.25 and $\\tau_{NIOU}$ = 0.7.\nOur object detector was trained using a multi-stage approach. Initially, the model was trained on the large-scale\n\"BASE\" dataset, which consists of a diverse collection of images from various sources, offering broad coverage of\nvehicle types and scenarios. This training commenced with weights pre-trained on the COCO dataset [113], establishing\na solid foundation for transfer learning. Subsequently, the model was fine-tuned on the \u201cFINE\u201d dataset, a curated,\nhigh-quality subset of BASE, specifically chosen for its accurate annotations and higher-resolution images, facilitating\nfine-grained model refinement. The datasets, which include images and vehicle annotations from eight public datasets\nand our own Songdo Vision dataset (described in Section 3.2), are further detailed in Table 1. The BASE and FINE\ndatasets comprised 21,908 and 10,187 images, respectively, with 3,458 images from BASE and 1,183 images from FINE\ndesignated for testing, while the remaining images were utilized for training."}, {"title": "3.4. Object tracking", "content": "Object tracking, specifically MOT, is a crucial component of our framework, assigning unique IDs to each vehicle\nand consistently tracking their movements across video frames. This enables continuous tracking, even during temporary\nocclusions, providing valuable insights into vehicle behavior and interactions over time.\nFor the k-th frame of video v, the MOT algorithm processes the detection set $D_{v}[k]$ to generate a set $T_{v}[k]$ of $N_{v}[k]$\ntracks, each represented as:\n$T_{v}[k] = \\{(id^{(i)}[k], x^{(i)}[k], y^{(i)}[k], w^{(i)}[k], h^{(i)}[k], c^{(i)}[k], s^{(i)}[k]) \\in I_{v}[k] \\times D_{v}[k]\\}_{i=1}^{N_{v}[k]}$\nwhere 'x' denotes the Cartesian product, $I_{v}[k]$ is the set of unique vehicle IDs in v up to frame k, and $id^{(i)}[k] \\in$\n$\\{1, 2, ..., V_{v}[k]\\}$ is the unique ID for each tracked vehicle, consistent across all frames. The number of active tracks\nin $F_{k}$ is $N_{v}[k]$, while $V_{v}[k]$ is the total number of unique vehicles observed up to frame k. The remaining symbols in Eq. 2\nhave the same meanings as in Eq. 1.\nAfter applying tracking to all frames $F_{k}, k \\in \\{1, ..., N_{v}\\}$, where $N_{v}$ is the total number of frames in v, we implement\na class label refinement step for $c_{i}^{(i)}[k]$ in $T_{v}[k]$ to maintain classification consistency across frames for each tracked\nvehicle. Although our observations indicate such inconsistencies are very rare, we formalize this refinement through the\nfollowing aggregation function:\n$f_{c}(id) = arg \\max_{c \\in C} {\\sum_{k=1}^{N_{v}} \\sum_{i=1}^{N_{v}[k]} s^{(i)}[k] \\cdot 1 [c^{(i)}[k] = c] \\cdot 1 [id^{(i)}[k] = id] }, id = 1, ..., V_{v}[N_{v}],$\nwhere 1(\u00b7) is the indicator function that is 1 if the condition is true and 0 otherwise. In this process, for each vehicle ID,\nthe ultimate class label is assigned by selecting the class c \u2208 C that maximizes the sum of confidence scores across all\nframes where that ID appears, that is, we refine the class labels to $c^{(i)}[k] = f_{c}(id[k])$.\nThe MOT algorithm efficiently manages the assignment of new IDs and the removal of vehicles that no longer appear\nin the frame. Consequently, $N_{v}[k] \\leq V_{v}[k]$ is maintained for all frames. In case of intermittent tracking failures, such as\nthose due to prolonged visual occlusions or prolonged misdetections, the resulting vehicle trajectory may exhibit multiple\nassigned IDs. However, this occurrence has minimal impact on the primary goal of our effort, which is the extraction of\nprecise trajectories. Furthermore, such instances are relatively infrequent, predominantly attributed to motorcycles that\ntraverse complex road infrastructure, as illustrated in Fig. 3 (right).\nTo effectively address the challenges posed by intermittent tracking failures and to ensure the continuity and accuracy\nof vehicle tracking, we adopted the BoT-SORT algorithm [75] within our tracking framework. As part of the SDP class,\nBOT-SORT, an enhancement over the original ByteTrack [74], incorporates a suite of optimizations, notably including\nan improved Kalman filter state vector and efficient ego motion compensation (EMC). These enhancements elevate\nBOT-SORT to a SOTA status, especially noted for its adaptability in dynamic environments and its exceptional capability\nto reduce identity switches and tracking failures [75]. Crucially, BoT-SORT has shown remarkable efficacy in managing\nsporadic misdetections, a frequent issue with fast-moving and smaller vehicles such as motorcycles, which are often\nunderrepresented in aerial datasets. This made it the ideal choice for our tracking needs, particularly in scenarios with\nminimal occlusions, where the comprehensive integration of a Re-ID module would not deliver proportional benefits.\nTo regulate the frequency of misdetections leading to ID deletion, we employ a track buffer of $T_{buff}$ = 30 frames,\nwhich retains information about tracks that may temporarily disappear, for example, due to occlusion or miss-detection.\nThis buffer is essential for managing tracks in environments prone to visual interruptions, ensuring that vehicles that may\nreappear are not prematurely removed from tracking. However, in our Songdo experiment, the importance of this buffer\nis minimal since obstructions are limited to a few small bridges. These are situated away from the main road arteries and\nintersections, as shown in Fig. 1, typically occurring at exit roads from underground garages that lead onto the main road.\nAdditionally, the BoT-SORT algorithm employs several thresholds to manage track initiation and termination. A new\ntrack is initiated when a detection's confidence score exceeds $\\tau_{n}$, ensuring only high-confidence detections contribute\nto track formation. For our implementation, we set $\\tau_{n}$ = 0.6. For track termination, a dual-threshold strategy is imple-\nmented: high-confidence matches are made above $\\tau_{h}$ = 0.5, while low-confidence matches are allowed above $\\tau_{l}$ = 0.1.\nFurthermore, associations between detections and existing tracks are ensured by a matching threshold of $\\tau_{m}$ = 0.8,\nproviding robust track management. Additionally, BoT-SORT employs an efficient EMC method based on sparse OF\nto adjust the tracking during camera movements, compensating for the global background motion that might otherwise\nintroduce errors. This capability allows us to apply track stabilization as a subsequent step after object tracking, rather\nthan relying on traditional video stabilization earlier in the pipeline, as shown in Fig. 2. The advantages of this approach\nto trajectory stabilization are further discussed in Section 3.5.\nAlthough BoT-SORT was originally validated on pedestrian tracking in street-level perspectives through the MOT\nChallenge dataset [75], multiple factors support its reliability in our aerial vehicle tracking application. First, our setup\nbenefits from favorable tracking conditions: the combination of 4K resolution footage captured at high-frequency 29.97\nFPS and vehicles moving at speeds normally not exceeding 80 km/h ensures sufficient frame-to-frame overlap for robust\ntracking. Second, qualitative validation through manual inspection of randomly sampled trajectories showed perfect\ntracking consistency across all examined cases. Finally, our AV experiment (detailed in Section 4.2) provides quantitative\nvalidation of the tracking precision, as we successfully matched multiple ground truth trajectories of the AVs with their\ncorresponding tracked sequences."}, {"title": "3.5. Track stabilization", "content": "To mitigate the effect of drone movements on the extracted trajectories, we \u201cstabilize\" each track $T_{v}[k]$ by aligning the\ncoordinates of each frame $F_{k}$ (k = 2, . . ., $N_{v}$) with a reference frame $F_{ref}$, which is set as the first frame of the video v, i.e.,\n$F_{ref} = F_{1}$. This ensures consistent alignment throughout the footage. As illustrated in Fig. 5, we achieve this alignment\nthrough image registration by identifying corresponding keypoints between pairs of grayscale frames ($F_{ref}$ and $F_{k}$), with\noptional CLAHE applied to improve contrast. The matched keypoints are used to compute a 3 \u00d7 3 homography matrix\n$H_{k \\rightarrow ref}$ using the least squares method, enhanced with a variant of the RANSAC algorithm to reject outliers. Despite\nthe marginally higher computational cost, we choose a projective transformation over an affine one to more effectively\naccount for potential camera tilting caused by wind."}, {"title": "3.5.1. Implementation details", "content": "We extract up to K and $K_{ref}$ keypoints from each frame $F_{k}$ (k = 2, . . ., $N_{v}$) and the reference frame $F_{ref}$, respectively,\nusing a local feature detector. To ensure stable frame alignment, we focus keypoint detection on static background\nelements by leveraging the BBs from $D_{v}[k]$ as \u201cexclusion masks", "85": "also known as second nearest neighbors (SNN) test", "1": "resulting in a refined match set $m_{k \\rightarrow ref"}, {"y": {"is": "n$p^{ref} = [x_{ref} \\ y_{ref}", "y'/z": {"z": {"1": {"T_{v}[k": "resulting in a new\nset of stabilized tracks, $T_{v}^{ref} [k"}}}}}]}