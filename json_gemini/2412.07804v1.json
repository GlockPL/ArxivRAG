{"title": "XLSTM-HVED: CROSS-MODAL BRAIN TUMOR SEGMENTATION AND MRI\nRECONSTRUCTION METHOD USING VISION XLSTM AND HETEROMODAL\nVARIATIONAL ENCODER-DECODER", "authors": ["Shenghao Zhu", "Yifei Chen", "Shuo Jiang", "Weihong Chen", "Chang Liu", "Yuanhan Wang", "Xu Chen", "Yifan Ke", "Feiwei Qin", "Zhu Zhu", "Changmiao Wang"], "abstract": "Neurogliomas are among the most aggressive forms of can-\ncer, presenting considerable challenges in both treatment and\nmonitoring due to their unpredictable biological behavior.\nMagnetic resonance imaging (MRI) is currently the preferred\nmethod for diagnosing and monitoring gliomas. However, the\nlack of specific imaging techniques often compromises the ac-\ncuracy of tumor segmentation during the imaging process. To\naddress this issue, we introduce the XLSTM-HVED model.\nThis model integrates a hetero-modal encoder-decoder frame-\nwork with the Vision XLSTM module to reconstruct missing\nMRI modalities. By deeply fusing spatial and temporal fea-\ntures, it enhances tumor segmentation performance. The key\ninnovation of our approach is the Self-Attention Variational\nEncoder (SAVE) module, which improves the integration\nof modal features. Additionally, it optimizes the interac-\ntion of features between segmentation and reconstruction\ntasks through the Squeeze-Fusion-Excitation Cross Aware-\nness (SFECA) module. Our experiments using the BraTS\n2024 dataset demonstrate that our model significantly outper-\nforms existing advanced methods in handling cases where\nmodalities are missing. Our source code is available at\nhttps://github.com/Quanato607/XLSTM-HVED.", "sections": [{"title": "1. INTRODUCTION", "content": "Neurogliomas rank among the deadliest cancers and are the\nmost common malignant primary brain tumors in adults, ac-\ncounting for approximately 25% of all primary brain tumors\nand 80% of malignant primary brain and central nervous sys-\ntem tumors. Within this category, diffuse gliomas are the most\nfrequently occurring malignant subtype. However, their di-\nverse biological behavior and unpredictable response to treat-\nment present substantial challenges for treatment [1].\nMagnetic resonance imaging (MRI) is the preferred\nmethod for diagnosing diffuse gliomas, offering crucial in-\nsights into tumor size, location, and morphology over time\n[2]. Various MRI modalities, such as T1, T1-Gd, T2, and\nFLAIR, provide complementary information about brain\ntumors. For instance, T1 and T2 modalities help identify\nvasogenic edema in subacute strokes; T1-Gd, enhanced with\ncontrast, reveals details about vasculature and the blood-brain\nbarrier; and FLAIR provides general information about stroke\nlesions. Consequently, using multimodal MRI for segmenta-\ntion reduces uncertainty and enhances performance compared\nto relying on single modalities. However, in practice, missing\nmodalities are common. This issue arises due to patients'\nlimited tolerance for scans, time constraints, or image corrup-\ntion. As a result, accurate MRI segmentation in the presence\nof missing modalities has garnered significant research inter-\nest in recent years. For instance, Doren et al. [3] developed\na Heteromodal Variational Encoder-Decoder (HVED) for\ntumor segmentation and modal complementation under con-\nditions of missing modalities. Similarly, Chen et al. [13]\nproposed a multimodal brain segmentation method based on\na feature decoupling and gated fusion framework to enhance\nrobustness when modalities are absent. Furthermore, Liu et\nal. [5] introduced a multimodal mask autoencoder (M3AE)\nfor brain tumor segmentation in scenarios where multiple\nmodalities are missing."}, {"title": "2. METHODS", "content": null}, {"title": "2.1. Model Overview", "content": "As illustrated in Figure 1, we introduce the XLSTM-HVED\nmodel, designed to improve the interaction of features be-\ntween different MRI modalities. This model employs the Het-\neromodal Variational Encoder-Decoder (HVED) to capitalize\non the capabilities of XLSTM. It merges channel and spatial\ninformation through the SFECA module, effectively utilizing\ncomplementary information from various modalities. Conse-"}, {"title": "2.2. Self-Attention Variational Encoder", "content": "As shown in Figure 1, our proposed Self-Attention Varia-\ntional Encoder (SAVE) encodes multimodal inputs using the\nMultimodal Variational Autoencoder (MVAE) [6]. The en-\ncoded features are processed through a convolutional layer,\nfollowed by spatial attention, to enhance focus on relevant\nmultimodal features. These encoded features are simultane-\nously linked to the corresponding decoder. Developed un-\nder the framework of conditionally independent modalities\nX = X1,..., In given a shared latent variable z, MVAE ex-\ntends the Variational Autoencoder (VAE) [7] to manage miss-\ning data by incorporating multimodal inputs.\nThe module fuses the means \u00b5 and covariances \u2211 of each\nmodality using a Product of Gaussians (PoG) [8] to form a\nlatent variable z, effectively excluding any missing modali-\nties. The sampled z is then decoded into image space via\nthe reparameterization trick, with feature representation fur-\nther refined by the Dimension Reduction Block (DRB) [9]. In\nthe final downsampling layer, multimodal features are merged\nwith unimodal latent features using MVAE and DRB, and\nare subsequently fed into the Vision-LSTM Attention (ViLA)\nmodule. This integration ensures that encoded multimodal\nfeatures are effectively combined with latent variable features\nderived from unimodal encoding."}, {"title": "2.3. Vision XLSTM Attention Module", "content": "As shown in Figure 1, our proposed ViLA module processes\nfeatures through the Vision-LSTM (ViL) module [10] and ap-"}, {"title": "2.4. Squeeze-Fusion-Excitation Cross Awareness Module", "content": "The features generated by the ViLA module are directed into\nboth the segmentation and reconstruction decoders. Concur-\nrently, the downsampled features from the SAVE module are\nintegrated into the decoding process. During upsampling,\nthe 8- and 16-dimensional features from the decoders inter-\nact through the DuSFE module [11], effectively merging their\noutputs for final feature extraction tailored to different tasks.\nIn the pre-training phase, only the reconstruction decoder\nis actively trained, while layers 2, 3, and the final module\nare kept static. The DuSFE module plays a crucial role in\ncombining segmentation and reconstruction features. It em-\nploys channel-squeeze-fusion-excitation (cSFE) and spatial-\nsqueeze-fusion-excitation (sSFE) techniques for spatial recal-\nibration, as illustrated in Figure 2 (right). Within cSFE, fea-\ntures F\u2081 and F2 are compressed into vectors V\u2081 and V2 using\nglobal average pooling. These vectors are then concatenated"}, {"title": "3. EXPERIMENT RESULT", "content": null}, {"title": "3.1. Dataset and Implementation Details", "content": "Our study utilizes the multimodal Brain Tumor Segmentation\nChallenge (BraTS) 2024 dataset [12]. This dataset includes\nMRI scans from T1, T1ce, T2, and FLAIR modalities of ap-\nproximately 4,500 patients diagnosed with various gliomas.\nThe images have been resampled to a uniform one mm\u00b3 res-\nolution, aligned to anatomical templates, and pre-processed\nto remove cranial structures. Expert radiologists manually la-\nbeled the images, focusing segmentation on three key regions.\nFor our experiments, we divided the dataset randomly into\ntraining and testing sets, comprising 80% and 20% of the total\ndata, respectively. We conducted our experiments using the\nPyTorch framework, leveraging an NVIDIA Tesla V100 for\ncomputational support. During training, we utilized a batch\nsize of 2, set a learning rate of 0.0001, and carried out the\ntraining over a period of 72 hours."}, {"title": "3.2. Comparative Experiment", "content": "For evaluation, we utilized the RA-HVED [9] and RMBTS\n[13] models as baselines. All the baselines and our model\nwere trained and tested using the same backbone network\nto ensure consistency in the evaluation phase. As detailed\nin Table 1, our model's performance is compared to these\nbaselines using Dice scores and HD95 scores. We selected\nU-HVED [3] as the backbone network. Our model demon-\nstrates strong performance across all metrics and is particu-"}, {"title": "3.3. Ablation Study", "content": "We investigated the impact of the SAVE, ViLA, and SFECA\nmodules on model performance. The findings are summa-\nrized in Table 2. Removing the ViLA module led to a notice-\nable decline in segmentation performance for the T1 modal-\nity, with both HD95 and Dice metrics worsening. Similarly,\nexcluding the SAVE module reduced the overall performance\nof multimodal fusion, particularly affecting the segmentation\nof the whole tumor. Omitting the SFECA module resulted in\nsignificant degradation in segmentation performance for the\nT1c and T1 modalities, especially in the HD95 metrics. These\nresults underscore the importance of these three modules."}, {"title": "4. CONCLUSION", "content": "In this paper, we introduce the XLSTM-HVED model to\naddress brain tumor segmentation challenges, particularly\nin scenarios where some MRI modalities are unavailable.\nOur model enhances segmentation accuracy and MRI data\nreconstruction quality by integrating cross-modal encoding,\nmulti-task learning, and attention mechanisms. The SAVE\nmodule facilitates effective multimodal information fusion,\nwhile the SFECA module optimizes the coordination be-\ntween segmentation and reconstruction tasks. Our extensive\nexperiments on the BraTS 2024 dataset reveal that the model\nis highly robust across various tasks, even when dealing with\nthe loss of certain modalities. Additionally, ablation studies\nconfirm that removing any module significantly diminishes\nthe model's overall performance, highlighting the critical role\neach component plays in ensuring the model's effectiveness."}, {"title": "5. COMPLIANCE WITH ETHICAL STANDARDS", "content": "This research study was conducted retrospectively using hu-\nman subject data made available in open access by BraTS\n2024 dataset [12]. Ethical approval was not required as con-\nfirmed by the license attached with the open access data."}]}