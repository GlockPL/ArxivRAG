{"title": "Resolving Multiple-Dynamic Model Uncertainty in Hypothesis-Driven Belief-MDPs", "authors": ["Ofer Dagan", "Tyler Becker", "Zachary N. Sunberg"], "abstract": "When human operators of cyber-physical systems encounter surprising behavior, they often consider multiple hypotheses that might explain it. In some cases, taking information-gathering actions such as additional measurements or control inputs given to the system can help resolve uncertainty and determine the most accurate hypothesis. The task of optimizing these actions can be formulated as a belief-space Markov decision process that we call a hypothesis-driven belief MDP. Unfortunately, this problem suffers from the curse of history similar to a partially observable Markov decision process (POMDP). To plan in continuous domains, an agent needs to reason over countlessly many possible action-observation histories, each resulting in a different belief over the unknown state. The problem is exacerbated in the hypothesis-driven context because each action-observation pair spawns a different belief for each hypothesis, leading to additional branching. This paper considers the case in which each hypothesis corresponds to a different dynamic model in an underlying POMDP. We present a new belief MDP formulation that: (i) enables reasoning over multiple hypotheses, (ii) balances the goals of determining the (most likely) correct hypothesis and performing well in the underlying POMDP, and (iii) can be solved with sparse tree search.", "sections": [{"title": "1 INTRODUCTION", "content": "In sequential decision-making problems modeled as a Partially Ob-servable Markov Decision Processes (POMDPs), an autonomous agent must account for uncertainty. To reason about uncertainty, the agent maintains a belief, or a probability distribution over the unknown state of the system, by updating its prior belief using Bayes' rule with noisy transition and measurement models. When planning in continuous domains, this results in a different belief for each possible action-observation pair, making the POMDP problem intractable, also known as the 'curse of history'. Further growth in complexity occurs due to ambiguity, e.g., uncertainty in which ob-ject generated the measurement (i.e., the data association problem), or uncertainty in which dynamic model generated the state tran-sition. In these cases, for the same observation or action, multiple 'branches' of the belief must be reasoned over, or estimated.\nIn the Bayesian estimation literature, multi-hypothesis algo-rithms are separated based on the origin of the hypothesis. In the first set of problems branching of the belief space occurs in the prediction step, due to multiple-dynamic models. These are addressed by algorithms such as the multiple-model (MM) and the interactive-MM (IMM) algorithms [3], [20]. In the second set of problems, branching occurs in the measurement update step, due to ambiguity, where there is uncertainty in which object generated the measurement signal, also known as the data association problem. Algorithms for this set of problems include multi-hypothesis track-ing (MHT) [5], and the probabilistic data association filter (PDAF) and joint probabilistic data association (JPDA) filters [2]. While these problems are similar in that they handle uncertainties and multiple potential scenarios, they are complimentary problems, and the solutions are not interchangeable. A similar distinction can be made in multi-hypothesis planning algorithms, sometimes referred to as 'hybrid belief planning, referring to the belief containing continuous and discrete variables, between methods that consider measurement ambiguity, and ones that consider multiple-dynamic models.\nMeasurement ambiguity: Planning with measurement am-biguity has received increased attention in the active SLAM (si-multaneous localization and mapping) community. A Monte-Carlo (MC)-based algorithm for hybrid belief POMDPs (HB-MCP) is pre-sented by Barenboim et al. [4]. It builds on the DA-BSP and D2A-BSP algorithms [19], [21], and extends it to prune unlikely hypotheses and keeping the most promising ones. The ARAS algorithm [12] uses factor graphs with multi-hypothesis factors [11] for ambiguity-aware active SLAM, but chooses only one hypothesis to plan the next path segment.\nMultiple-dynamic models: Planning with hybrid dynamics is addressed by Brunskill et al. [7] and Jain and Niekum [13], where the idea is to approximate a non-linear dynamics model using a set of linear models. The approach of Brunskill et al. [7] is limited to Gaussian sum filters for belief updates, and assumes discrete actions and observations spaces, thus does not consider the full breadth of the problem. Similarly, Jain and Niekum [13] represent the belief with a Gaussian mixture, but consider continuous ac-tion space with maximum likelihood observations. In both types of multi-hypothesis planning, the key objective is based on the system's underlying state, such as the agent's position, the position of a tracked target, or a map of the environment. This is done by taking"}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 Belief-MDP", "content": "One of the main building blocks for sequential decision-making under uncertainty is the Markov decision process (MDP). An MDP is defined by the tuple $(S, A, T, R, \\gamma)$, where S, A are the sets of all possible states and actions, respectively. $T (s, a, s') = p(s'|s, a)$ is a stochastic state transition model, which defines the probability of transitioning to state $s' \\in S$ from state $s \\in S$ after taking action $a \\in A$. The reward function R(s, a) determines the immediate reward the agent receives when taking action a at state s, and $\\gamma \\in [0, 1)$ is a discount factor.\nA solution for an MDP is an optimal policy $\\pi*(s)$ that maps every state to an action a*, and maximizes the expected cumulative reward. However, in most real-world problems, the state s is not perfectly known, but can only be inferred by a sequence of noisy measurements o. A partially observable MDP (POMDP) takes into account this uncertainty by adding two elements, (O, Z) to the MDP tuple $(S, A, T, R, \\gamma)$. Where O is the set of all possible observations, and $Z(s', a, o) = p(ols', a)$ is the stochastic observation function,\ndefined as the conditional probability of seeing observation o after taking action a and reaching state s'.\nAs the true state s of a POMDP is unknown, an agent maintains a belief over states $s \\in S$, which summarizes the history $h_t$ of all actions taken and observations received up to and including time step t and starting from a prior belief $b_0$,\n$b_t(s) = p(s_t = s|h_t)$, where $h_t = (b_0, a_0, o_1, a_1, ..., o_t)$.\nA difficulty is that the reward function of a POMDP depends directly on the state of the system and, thus this reward cannot be used to account for state uncertainty or to motivate information-gathering actions [1]. A solution to this problem is to formulate the problem as a belief-MDP and reason about belief-states instead of states. Note that any POMDP is a belief-MDP, with a set of belief states $b \\in B$, a transition model $T (b, a, b')$ [16], which accounts for observations using Bayes' rule,\n$T(b, a, b') = p(b'|b, a) = \\int_{O \\in O} p(b'|a, b, o) \\cdot p(o|a, b),$   (1)\nand a belief-dependent reward function [1]\n$\\rho(b,a) = E_{s \\sim b} [R(s, a)] + \\phi(b,a)$.   (2)\nHere $\\phi(b, a)$ is a belief-dependent reward such as Shannon's entropy or Kullback-Leibler divergence, and the first term is the expected reward when the state s is distributed according to b,\n$E_{s \\sim b} [R(s,a)] = \\int_{S \\in S} b(s)R(s,a)$.   (3)"}, {"title": "2.2 MCTS", "content": "A sequential decision process (loop) is built out of four main steps: plan - act - observe \u2013 estimate\u00b9. The left diagram in Figure 1 de-scribes a cycle in an online planning loop, where an agent plans based on its prior belief b to choose an action a, acting and interact-ing with the environment, and generating a measurement o. The last step is a Bayesian belief update,\n$b' (s') = p(s'|h_t) \\propto p(o|a, s') \\cdot \\int_s b(s) \\cdot p(s'|s, a)$, (4)\nwhere s is the state at the current time step and s' represents the state at the next time step.\nMonte Carlo tree search (MCTS) is a widely used technique to solve various reasoning problems. In the context of Belief-MDP problems, at each planning step, MCTS builds a tree, consisting of belief (circle) and action (square) nodes, as shown in Figure 1 (middle), up to a predefined horizon. The result of the simulation is the (current step) action that maximizes the action value function,\n$Q(s, a) = R(s, a) + \\gamma \\sum_{s'} T(s'|s, a)U(s')$,   (5)\nwhere $U(s) = \\max_a Q(s, a)$ is the value function [17]. Since the MCTS algorithm is not the focus of this paper, we now discuss the elements of the algorithm that are relevant in the context of this paper and refer the interested reader to the survey by Browne et al. [6] for more details about the MCTS algorithm and to the documentation of the POMDPs.jl framework [8] for implementation details."}, {"title": "3 PROBLEM STATEMENT", "content": "Consider an autonomous agent, tasked with solving an underlying partially observable Markovian planning problem P, that can be framed as a POMDP, and thus a belief-MDP. Denote the underlying state space of the system as $S_X$ and the corresponding belief space as $B_X$. Now assume that there is uncertainty regarding the transition model that derives the underlying state space (or a subset of it), that is, there are $n_H$ possible transition models $T_i$ (with no transitions between the models). We define the set of questions of whether model i is correct or not as the set of hypotheses H, where each hypothesis $H_i$ corresponds to transition model $T_i$. Then the belief over all possible hypotheses is represented by $B_H$.\nWe define the new multiple-dynamics hypothesis (MDH) prob-lem P, as a planning problem over both the underlying belief state $B_X$ and the hypothesis state $B_H$, with the MDH-belief-MDP (MDH-BMDP) defined by the tuple,\n$(B = B_X \\times B_H, A, \\tilde{T}, \\tilde{\\rho}, \\gamma)$,   (6)\nwhere A and $\\gamma$ are the same as in the underlying problem P, B is the augmented belief-state set and includes the underlying belief-state space and the belief over all possible hypotheses. The transition model $\\tilde{T}$ includes a set of transition models $T_i$ for i = 1 : $n_H$ for all dynamics hypotheses, and $\\tilde{\\rho}$ is the new reward function, which depends on both the underlying problem reward, $\\rho_x$ and the hypothesis-based reward $\\rho_H$,\n$\\tilde{\\rho} = \\rho_X (b_X, a) + \\rho_H(b_H, a)$.   (7)\nThe observation space and function, O, Z, are not explicitly defined in the Belief-MDP tuple, as the MDP formulation assumes perfect state knowledge. Instead, they are implicitly included within the transition model $\\tilde{T}$.\nThe planning problem P considered in this paper searches for an optimal policy $\\pi*$ that balances two potentially competing require-ments - deciding which hypothesis $H_i$, corresponding to transition model $T_i$, is (most likely) correct, while still solving the original underlying problem P. This is different from existing works such as [13], [7] which search for a policy assuming the state can transition according to different dynamics and do not explicitly try to reason which one is correct."}, {"title": "4 TECHNICAL APPROACH", "content": "Typically, Belief-MDP (and POMDP) problems reason about one transition model T, and one observation function Z associated with the problem definition tuple P. This section describes the MDH-BMDP framework to enable the use of existing algorithms to solve the hypothesis-driven problem where several transition models are considered. We first define how the MDH-BMDP tuple, defined in Equation (6), is constructed given an underlying POMDP, and a set of hypotheses. Second, we describe some important implementation details and then provide further details on the definition of the transition model $\\tilde{T}$ and the reward function $\\tilde{\\rho}$."}, {"title": "4.1 Constructing the MDH-BMDP", "content": "Consider an underlying POMDP problem P, defined by $(S_X, A, T,O, Z, R_X, \\gamma)$, a set of $n_H$ hypotheses that we want to reason about defined by their transition models,\n${H_1 (T_1), H_2 (T_2), ..., H_{n_H} (T_{n_H})}$,\nand an hypotheses belief reward function $\\rho_H$. The elements of the MDH-BMDP definition $(B, A, \\tilde{T}, \\tilde{\\rho}, \\gamma)$ (Equation (6)), and their dependencies on the underlying POMDP P, the hypotheses set and the hypotheses belief reward function, are illustrated in Figure 2(top) and defined as follows:\n*   $B = B_X \\times B_H$ is the joint belief space, based on the under-lying state space $S_X$ and the hypotheses state space $S_H = {1, 2, ..., n}$. As a result a belief state $b \\in B$ is given by,\n$b(s) = p(s_X, s_H) = p(s_H) \\cdot p(s_X|s_H)$.   (8)\n$b_H$ $b_X$\nWhere $b_H$ is a categorical distribution, and $b_X$ is defined as an array of conditional distributions, e.g. Gaussian, one for each hypothesis $H_i$. Thus the joint belief b is a mixture of distributions, weighted by the probability of each hypothesis $H_i$ being correct.\n*   A is the action space, inherited from the underlying POMDP problem P.\n*   $\\tilde{T}$ is a generative transition model, defining how to gener-ate a new joint belief b', from b', after taking an action a. More details about $\\tilde{T}$ are given in Section 4.3, but notice that $\\tilde{T}$ depends on the underlying transition model T, ob-servation space O, and observation function Z, and the set of hypotheses transition models $T_1, ... T_{n_H}$\n*   $\\tilde{\\rho}$ is the joint belief-dependent reward function defined in Equation (7). It transitions the underlying state-dependent reward $R_X$ to a belief-dependent reward using Equation (3) and adds the hypothesis reward $\\rho_H$. Further discussion on the reward function is given in Section 4.4.\n*   $\\gamma$ is the discount factor, inherited from the underlying POMDP problem P."}, {"title": "4.2 Implementation Details", "content": "The key idea in the implementation of the MDH-BMDP framework is to define an array P of $n_H$ 'hypothesis conditioned'-POMDPs $P_i$,\n$(S_X, A, T_i, O, Z, R_X, \\gamma)$,   (9)\nwhere, $T_i$ is based on the underlying POMDP problem P, but with a dynamics function $f_i$ corresponding to hypothesis $H_i$. Note the subtle difference between the dynamics function $f_i$ and the tran-sition model T - while $f_i$ describes the equations of motion, T depends on $f_i$ and represents a distribution over possible states s. The rest of the tuple elements are inherited from P. To be able to use the different POMDPs $P_i$ with existing solvers within the POMDPs.jl ecosystem [8], during construction, the MDH-BMDP is augmented with the POMDP array P and an array of belief up-daters, one for each model $P_i$. We denote this new structure as P = {$(B, A, \\tilde{T}, \\tilde{\\rho}, \\gamma)$, P, Updaters}. The full structure of the MDH-BMDP is demonstrated in Figure 2(bottom). As will be discussed"}, {"title": "4.3 Generative Transition Model", "content": "Planning in belief-MDPs requires reasoning belief-states instead of over states, thus a generative model as defined in algorithm 1 has to be modified to propagate the belief b and not the state s, as described in algorithm 2. Propagating a prior belief b to a pos-terior belief b' in a Bayesian belief updater (estimator) includes two main steps: prediction and correction (Equation 4). In the pre-diction step, the belief is propagated according to the transition model T (s, a, s') = p(s'|s, a). In the correction step, also known as measurement update, Bayes' rule is used to fuse an observation according to the model's observation model Z(s', a, o) = p(o|s', a).\nHowever, since we are reasoning over a belief state, we don't have a unique state s' to sample an observation, as in line 4, algorithm 1. Instead, we need to first sample a state from the belief, by first sampling a hypothesis state $s_H$ out of $b_H$, and then an underlying state $s_X$ is sampled from the conditional distribution $p(s_X|s_H)$ (lines"}, {"title": "4.4 Reward Function", "content": "The last element in the definition of the MDH-BMDP P is the reward function $\\tilde{\\rho}$. The goal is to construct a reward function that will balance two requirements: (i) making a decision regarding the most probable hypothesis, and (ii) solving the original underlying problem P with minimal effect on the original policy.\nSimilar to [10], we define the reward function as the sum of the expected state-action reward and a belief-dependent reward as in Equation 7,\n$\\tilde{\\rho}(b,a) = \\int_{S_X \\in S_X} b_X (s_X) R_X (s_X, a)ds_X + w \\cdot \\rho_H(b,a)$,   (10)\nwhere we used Equation (3) for the underlying POMDP reward, and added the parameter w to allow weighting to balance between the two requirements. Since the reward function, $R_X (s_X, a)$ is defined by the POMDP $P_i$, w allows tuning for the requested behavior of the MDH-BMDP policy. This is especially useful since for different underlying reward functions $R_X$ the reward values might be scaled significantly differently.\nWe noe focus our attention on the hypothesis-related reward function $\\rho_H(b_H, a)$. In a realistic scenario, where the agent is ex-pected to generate a plan to support deciding which hypothesis is most likely, there are two parameters that could be of interest. First,\nthe user might want to make the decision within a time limit $\\tau$. Second, it is unlikely that the probability of one hypothesis will be exactly 1, so we want to set a decision-making threshold, such that the probability of hypothesis $H_i$ being correct is at least p = 1 \u2212 $\\epsilon$. Common belief-dependent reward functions often depend on some measure of information [1], [10], e.g., negative Shannon's entropy,\n$\\rho_H(b,a) = \\sum_{s_H \\in S_H} b_H(s_H) \\log b_H(s_H)$.   (11)\nHowever, while this type of reward function can incentivize information-gathering actions that reduce entropy, thus trying to 'push' one of the hypotheses probabilities in the direction of 1, it does not explicitly set a time limit on making a decision. We suggest a new simple sparse reward function that explicitly accounts for making a decision of which hypothesis is correct with probability p > 1-$\\epsilon$ within time $\\tau$.\n$\\rho_H(b_H, a) = \\begin{cases}\n1.0 & \\text{if } \\max(b_H) \\geq 1 - \\epsilon \\& t \\leq \\tau \\& !resolved\\\\\n0.0 & \\text{otherwise,}\n\\end{cases}$   (12)\nwhere '!resolved' should read 'not resolved', and indicates that a decision had not been made yet, to avoid collecting the reward more than once. As might happen in cases when the probability fluctuates above and below the threshold."}, {"title": "5 EXPERIMENTS", "content": "To test the suggested MDH-BMDP framework, and evaluate the effect of the reward function on hypothesis-driven BMDP prob-lems, we performed simulation experiments on two problems \u2013 the Van Der Pol (VDP) track problem, and a space domain awareness problem. For both problems, we define an underlying POMDP, and then augment it with three hypotheses based on variations in the system dynamics."}, {"title": "5.1 Multiple-Dynamic Model VDP-Track", "content": "This test problem is a modification of the Van Der Pol (VDP) tag problem [22] to a tracking application with multiple-dynamic model hypothesis. The underlying, or base planning problem of the au-tonomous agent is to plan an observation schedule for a sensor located at the origin, with the goal of estimating the unknown 2D position of three moving objects. The initial position of the objects is within a square with 0.5 unit side length, and they move according to the 2D VDP oscillation differential equations,\n$\\ddot{x} = \\mu (x - \\frac{x^3}{3}) - \\dot{x}$,   (13)\nwith $\\mu$ = 0.6/2.0/1.4 for objects 1/2/3, respectively.\nA zero mean Gaussian process noise, with $\\sigma_x = \\sigma_y$ = 0.05, is added to the object's position at the end of each Runge-Kutta inte-gration step (4th order). The observation space is continuous and consists of 8 noisy range beams (O \u2208 $R^8$) with $\\sigma$ = 2. In each time step the agent has one accurate measurement ($\\sigma$ = 0.5) at its disposal, and it needs to decide to which object it directs the sensor (A = {1, 2, 3} \u2208 $R^3$), where there is a different probability of detection for each object, with $P_{detect}$ = 0.95/0.8/0.65. The reward for taking an accurate measurement of the object i \u2208 {1, 2, 3} is the Euclidean distance d from the object. Now consider a scenario"}, {"title": "5.2 Space Domain Awareness", "content": "One of the core problems in space domain awareness (SDA) is catalog maintenance. It refers to the problem of tasking a set of sensors to maintain custody over an existing 'catalog' of space objects (SOs). This multi-sensor tasking problem can be formulated as a POMDP and solved via MCTS [9]. Consider a scenario when during routine sensor tasking operation for catalog maintenance an anomaly in the orbit of one of the SOs is detected. As a result, the operator is interested in determining the origin of the anomaly, e.g., due to an engine misfiring, a deployment of solar panels, or possibly an intentional maneuver.\nTo demonstrate how MDH-BMDP is used for hypothesis-driven planning, we consider the following scenario: there are 5 SOs in low Earth orbit (LEO), and an anomaly is detected on one of them, the OOI, which spawns three hypotheses regarding the dynamic model: (i) nominal dynamics (i.e. nothing happened), (ii)-(iii) the OOI deployed a small/large solar panel which results in small/high amount of drag addition, respectively. Given a nominal catalog maintenance sensor-tasking plan P for 3600 seconds, with a time step of 60 seconds, the MDH-BMDP problem P seeks to determine the correct hypothesis with probability p = 0.8 within 1600 seconds, with minimal change to the original plan. In this problem, when transitioned into an MDH-BMDP, the belief over the OOI state is approximated as a Gaussian distribution and propagated using an unscented Kalman filter (UKF) [15], one for each hypothesis."}, {"title": "5.3 Results", "content": "In hypothesis-driven planning, the planning agent optimizes for actions that both reduce uncertainty and still perform well in the underlying problem. The simulation analysis then focuses on pa-rameters relating to decision quality was the decision correct and was it made in time, and how much was the performance of the underlying problem affected as measured by the state-action dependent cumulative discounted reward ('base reward').\nFigure 3 presents 20 Monte-Carlo (MC) simulation results of the multi-hypothesis VDPTrack and SDA problems, respectively. The upper row of the figures shows a discrete search for the weight-ing factor w of the hypothesis reward functions \u2013 resolution time (Equation 12) and negative entropy (Equation 11). For each weight w, subfigures (a)-(b) and (d)-(e) show (i) the discrete probabilities of the 3 different dynamic models, denoted by markers with error bars,\nat the time a decision was made, (ii) the threshold for decision mak-ing, 0.8, in a dashed line, and (iii) success rate for making the right decision for a decision made in time (dots), and a decision made by the end of the simulation, including after the deadline, which we call 'late' (dash-dot). We can see then for the VDPTrack problem, the best results with respect to hypothesis decision are achieved with w = 50 and w = 75 for the resolution time reward and w = 50 for the entropy reward. For the SDA problem, w = 150 results in the best hypothesis decision results for both reward functions. Subfigures 3(c) and (f) show the mean probability against time for w = 50 and w = 150, respectively, for both reward functions. For the SDA problem, after a decision is made the resolution time reward function does not motivate the agent to take observation actions to the OOI, the probability stays constant. On the other hand, we can see that the entropy-based reward still causes fluctuations in the hypothesis probability. For the VDPTrack problem, since taking observations actions to the OOI might still be advantageous per the base reward function, we see that it might cause a decrease in the hypothesis probability, depending on the actual observations made.\nTo decide on a reward function and its best weight w, we con-sider how taking hypothesis-driven actions affects the underlying POMDP performance, we examine the base reward in Table 1. The baseline for comparison is a simulation with w = 0, that is actions are taken based only on the base reward $\\rho_x$. For VDPTrack, we performed 50 MC simulations with the best-performing weights w = [50,75] (for resolution time) and w = 50 (for entropy). From the table, we see that for both problems the resolution time reward outperforms the entropy-based reward since it leads to (i) a higher success rate for deciding on the correct hypothesis in time and (ii) a higher base reward. The reward weight for the VDPTrack problem is chosen to be w = 50, as it accumulates a higher base reward, thus having a smaller effect on the underlying problem, while still meeting the decision threshold and making a decision on time.\nFor the SDA problem, the difference in the base reward can be explained by the number of unnecessary plan changes made when planning with the entropy reward. In that case, there are 36.75"}, {"title": "6 SUMMARY", "content": "This paper defines the hypothesis-driven POMDP problem as the set of multi-hypothesis POMDP problems where explicitly determining"}]}