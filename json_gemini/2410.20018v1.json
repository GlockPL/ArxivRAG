{"title": "GHIL-Glue: Hierarchical Control with Filtered Subgoal Images", "authors": ["Kyle B. Hatch", "Ashwin Balakrishna", "Oier Mees", "Suraj Nair", "Seohong Park", "Blake Wulfe", "Masha Itkina", "Benjamin Eysenbach", "Sergey Levine", "Thomas Kollar", "Benjamin Burchfiel"], "abstract": "Image and video generative models that are pre-trained on Internet-scale data can greatly increase the generalization capacity of robot learning systems. These models can function as high-level planners, generating intermediate sub-goals for low-level goal-conditioned policies to reach. However, the performance of these systems can be greatly bottlenecked by the interface between generative models and low-level controllers. For example, generative models may predict photo-realistic yet physically infeasible frames that confuse low-level policies. Low-level policies may also be sensitive to subtle visual artifacts in generated goal images. This paper addresses these two facets of generalization, providing an interface to effectively \"glue together\" language-conditioned image or video prediction models with low-level goal-conditioned policies. Our method, Generative Hierarchical Imitation Learning-Glue (GHIL-Glue), filters out subgoals that do not lead to task progress and improves the robustness of goal-conditioned policies to generated subgoals with harmful visual artifacts. We find in extensive experiments in both simulated and real environments that GHIL-Glue achieves a 25% improvement across several hierarchical models that leverage generative subgoals, achieving a new state-of-the-art on the CALVIN simulation benchmark for policies using observations from a single RGB camera. GHIL-Glue also outperforms other generalist robot policies across 3/4 language-conditioned manipulation tasks testing zero-shot generalization in physical experiments. Code, model checkpoints and videos can be found at https://ghil-glue.github.io.", "sections": [{"title": "I. INTRODUCTION", "content": "As Internet-scale foundation models achieve success in computer vision and natural language processing, a central question arises for robot learning: how can Internet-scale models enable embodied behavior generalization? While one approach is to collect increasingly large action-labeled robot manipulation training datasets [1]\u2013[3], video datasets (without actions) from the Internet are vastly larger. This action-free video data can provide robotic control policies with a wide array of common sense capabilities. However, while videos may be useful for inferring the steps in a task, such as how the objects should be moved, or which parts of an object to manipulate (e.g., grabbing a cup by the handle), they are less useful for learning details about low-level control. For example, it is difficult to infer the action commands for controlling a robot's fingers from videos of humans performing manipulation tasks. One promising solution to this challenge is to employ a hierarchical approach: infer high-level subgoals in the form of goal images using models"}, {"title": "II. RELATED WORK", "content": "Generative Models for Robotic Control: Prior works have explored diverse ways to leverage generative models, such as diffusion models [12], [13] and Transformers [14], for robotic control. They have employed highly expressive generative models, potentially pre-trained on Internet-scale data, for low-level control [15]\u2013[21], data augmentation [22]\u2013[24], object detection [25], [26], semantic planning [27]\u2013[31], and visual planning [4]\u2013[9]. Among them, our work is most related to prior works that employ image or video prediction models to generate intermediate subgoal images for the given language task [4]\u2013[9], [32]. These works use diffusion models to convert language instructions into visual subgoal plans, which are then fed into low-level subgoal-conditioned policies to produce actions. While sensible, this configuration leads to failures due to the misalignment of the generative models and the low-level policies that control the robot behavior, as shown in our experiments (Section V).\nRejection Sampling: One of our key ideas in this paper is based on rejection sampling, where we sample multiple subgoal proposals from an image or video prediction model and pick the best one based on a learned subgoal classifier. The idea of test-time rejection sampling has been widely used in diverse areas of machine learning, such as filtering-based action selection in offline reinforcement learning (RL) [33]\u2013[37], response verification in natural language processing [38]\u2013[40], and planning and exploration in robotics [29], [30], [41]\u2013[43]. Previous works in robotics have proposed several ways to filter out infeasible plans generated by pre-trained foundation models [29], [30], [41], [42], [44], [45]. Unlike these works, we focus on filtering visual subgoals instead of language plans [29], [42], [44], and do not involve any planning procedures [30] or structural knowledge [41]. While the subgoal classifier we train resembles the classifier from [46], our classifier differs in two key ways. First, we use our classifier to filter out \"off-task\" subgoals, whereas the classifier in [46] is used as a reward function for training downstream policies. Second, the classifier from [46] is conditioned on the initial state s\u2080 and the current state s, whereas our classifier is conditioned on the current state s and a generated subgoal g.\nGoal-Conditioned Policy Learning: Our method is broadly related to goal-conditioned policy learning [47]\u2013[49], language-conditioned policy learning [50]\u2013[55], and hierarchical control [4], [5], [56]\u2013[59]. Most prior works in hierarchical policy learning either train a high-level policy from scratch that produces subgoals or latent skills [57], [60]\u2013[73] or employ subgoal planning [70], [74]\u2013[86]. Unlike these works, we do not train a high-level subgoal prediction model from scratch nor involve a potentially complex planning procedure. Instead, we sample multiple potential subgoals from a pre-trained (or potentially fine-tuned) image or video prediction model and pick the best one based on a trained subgoal classifier. Among hierarchical policy methods, perhaps the closest work to ours is IRIS [56], which trains a conditional variational autoencoder to generate subgoal proposals and selects the best subgoal that maximizes the task value function. While conceptually similar, our method differs from IRIS in that we do not assume access to a reward function in order to train a value function. Our classifier is trained on trajectories consisting only of images and language descriptions.\nDiffusion Model Guidance: The generative models we consider in our paper [87], [88] are diffusion-based models trained using classifier-free guidance (CfG) [89]. Although we use a large value for the language-prompt guidance parameter at inference in our experiments, we find that producing \"off-task\u201d subgoals is still a common failure mode that is not solved by increasing this parameter alone.\nClassifier guidance [12], [90], [91] is also a plausible alternative to rejection sampling, but there are some practical challenges in training a subgoal classifier for this purpose. First, the diffusion models we consider use latent diffusion [92], and therefore would require training the subgoal classifier to operate in the latent space of the diffusion model. Second, the subgoal classifier would need to be trained on noised data in order to guide the diffusion denoising process of the generative model. Nevertheless, classifier guidance is a potentially appealing direction for future work."}, {"title": "III. PRELIMINARIES", "content": "We consider the same problem setting as [4], where the goal is for a robot to perform a task described by some previously unseen language command l. To do this, we consider the same three dataset categories as in [4]: (1) language-labeled video clips Di which contain no robot actions; (2) language-labeled robot data D\u1d62,\u2090 that includes both language labels and robot actions; (3) unlabeled robot data that only includes actions D\u2090. The dataset D\u1d62,\u2090 consists of a set of trajectory and task language pairs, {(\u03c4\u2099, l\u2099)}\u1d3a\u2099=1, and a trajectory contains a sequence of state, s \u2208 S, and action, a \u2208 A, pairs, \u03c4\u2099 = (s\u2070, a\u2070, s\u00b9, a\u00b9,\u2026). Given these datasets, we assume access to two learned modules:\n1) a subgoal generation module from which we can sample multiple possible future subgoals. This can be trained on D\u1d62 and D\u1d62,\u2090.\n2) a low-level goal-reaching policy that chooses actions to reach generated subgoals. This can be trained on D\u2090 and/or D\u1d62,\u2090.\nOur contribution is a set of approaches to robustify the interface between these two modules.\nWhile GHIL-Glue can be applied to any hierarchical imitation learning method consisting of the two components mentioned above, in this work we apply GHIL-Glue to two specific algorithms: (1) UniPi [5], in which a high-level model generates a subgoal video, and a low-level inverse-dynamics model predicts the actions needed to \"connect\" the images in the video, and (2) SuSIE [4], in which a high-level model generates a subgoal image by \"editing\" the current image observation, and a goal-conditioned policy predicts actions to achieve the subgoal image. We define subgoals, g \u2208 G, as video or image samples from the high-level models used in these algorithms."}, {"title": "IV. GHIL-GLUE", "content": "Many modern hierarchical policy methods aim to improve generalization by using language-conditioned image or video models to generate intermediate subgoal images for a given task. The interface between these image or video models and the low-level policies that choose actions to reach generated subgoals is a major performance bottleneck for these hierarchical policy methods. GHIL-Glue improves the robustness of this interface (see Fig. 1). In Section IV-A, we propose a simple method to filter subgoals that do not progress towards completing the task specified by language instruction l. Then, in Section IV-B, we describe a simple yet non-obvious data augmentation practice to robustify the low-level policy and our subgoal classifier to harmful visual artifacts in the generated subgoals. We note that the two components of GHIL-Glue work together synergistically: when applied together, the resulting performance improvement is larger than the sum of improvements that results from applying each component individually (see Section V).\nA. Subgoal Filtering\nThe image and video generative models we consider are first pre-trained on general Internet-scale image and video data, and then fine-tuned on a modest amount of robot data. Despite being fine-tuned on robot data, a common failure mode we observe across different models is that, over the course of executing a task, the model begins to go \"off-task,\" meaning that it starts generating subgoals that are consistent with the current image observation but that do not progress towards completing the language instruction l. We hypothesize that this is due to the distribution shift between the Internet data these image or video prediction models are pre-trained on and the robot data they are fine-tuned on.\nTo address this challenge, we train a subgoal classifier f\u03b8(s,g,l) on D\u1d62 and/or D\u1d62,\u2090 that predicts the probability that the transition between the current image observation s and the next subgoal g makes progress towards completing language instruction l. Note that although we train the subgoal classifier on robot data in our experiments, action labels are not used in the training of the classifier, and the subgoal classifier can be trained on action-free data, including large, non-robotics Internet video datasets. During training, we sample positive examples of state-goal transitions for l from the set of trajectories that successfully complete the instruction. We construct negative examples in the following three ways:\n1) Wrong Instruction: (s, g, l') where l' is sampled from a different transition than s and g.\n2) Wrong Goal Image: (s,g',l) where g' is sampled from a different transition than s and l.\n3) Reverse Direction: (g, s, l), where the order of the current image observation and the subgoal image have been switched. This is important for learning whether a candidate goal image is making temporal progress towards completing the language instruction.\nWe refer to this dataset of negative examples constructed from D\u1d62,\u2090 as D\u1d62,\u2090\u0304. We then train the subgoal classifier by minimizing the binary cross entropy loss between the positive examples and the constructed negative examples (see Appendix A for additional training details):\nJ(\u03b8) = \\mathbb{E}_{(s,g,l)\\sim D_{i,a}} [log (f_\u03b8 (s, g, l))] + \\mathbb{E}_{(s^-,g^-,l^-)\\sim D_{i,a}^-} [log (1 \u2013 f_\u03b8(s^-, g^-,l^-))].\nGiven a set of K subgoals predicted by the image or video model, GHIL-Glue uses the classifier to select the subgoal with the highest progress probability and passes that subgoal to the low-level policy for conditioning.\nB. Image Augmentation De-Synchronization\nWhile the method proposed in Section IV-A increases robustness to predicted subgoals that do not make task"}, {"title": "V. EXPERIMENTS", "content": "We study the degree to which GHIL-Glue improves existing hierarchical imitation learning algorithms across a number of tasks in simulation and physical experiments that assess zero-shot generalization. On the CALVIN [10] simulation benchmark, we find that applying GHIL-Glue to two different hierarchical methods that leverage generative subgoals yields an average increase in relative performance of 27%. In experiments on a physical robot, GHIL-Glue increases the relative success rate of a SOTA generative hierarchical imitation learning method [4] by 23%. The improvement in average relative performance yielded by GHIL-Glue across both simulated and real experiments is 25%. We analyze the influence of each component of GHIL-Glue on task performance and also perform extensive qualitative analysis in Appendix C.\nA. Experimental Domains\nWe evaluate our method on the CALVIN [10] simulation benchmark and the Bridge V2 [11] physical experiment setup with a WidowX250 robot.\nSimulation Experiment Setup: Simulation experiments are performed in the CALVIN [10] benchmark, which focuses on long-horizon language-conditioned robot manipulation. We follow the same protocol as in [4], and train on data"}, {"title": "VI. CONCLUSION", "content": "We present GHIL-Glue, a method for better aligning image and video prediction models and low-level control policies for hierarchical imitation learning. Our key insight is that while image and video foundation models can generate highly realistic subgoals for goal-conditioned policy learning, when generalizing to novel environments, the generated images are prone to containing visual artifacts and can be inconsistent with the task the robot is commanded to perform. GHIL-Glue provides two simple ideas to address these challenges, leading to a significant increase in zero-shot generalization performance over prior work both in the CALVIN simulation benchmark and in physical experiments.\nOne exciting avenue for future work would be to explore training image or video prediction models for subgoal generation on a broader distribution of robot data, such as the data available in the Open-X embodiment dataset [2]. Another interesting direction would be to filter subgoals based on the capability of the low-level policy to actually achieve them, for example, by training a goal-conditioned value function for the low-level policy and using it to evaluate subgoal feasibility. Finally, while we trained the subgoal classifiers on robot datasets, in principle these could be trained in the same way on much larger, non-robotics video datasets in order to improve generalization."}]}