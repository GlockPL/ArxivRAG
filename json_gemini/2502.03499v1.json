{"title": "Omni-DNA: A Unified Genomic Foundation Model for Cross-Modal and Multi-Task Learning", "authors": ["Zehui Li", "Vallijah Subasri", "Yifei Shen", "Dongsheng Li", "Yiren Zhao", "Guy-Bart Stan", "Caihua Shan"], "abstract": "Large Language Models (LLMs) demonstrate remarkable generalizability across diverse tasks, yet genomic foundation models (GFMs) still require separate finetuning for each downstream application, creating significant overhead as model sizes grow. Moreover, existing GFMs are constrained by rigid output formats, limiting their applicability to various genomic tasks. In this work, we revisit the transformer-based auto-regressive models and introduce Omni-DNA, a family of cross-modal multi-task models ranging from 20 million to 1 billion parameters. Our approach consists of two stages: (i) pretraining on DNA sequences with next token prediction objective, and (ii) expanding the multi-modal task-specific tokens and finetuning for multiple downstream tasks simultaneously. When evaluated on the Nucleotide Transformer and GB benchmarks, Omni-DNA achieves state-of-the-art performance on 18 out of 26 tasks. Through multi-task finetuning, Omni-DNA addresses 10 acetylation and methylation tasks at once, surpassing models trained on each task individually. Finally, we design two complex genomic tasks, DNA2Function and Needle-in-DNA, which map DNA sequences to textual functional descriptions and images, respectively, indicating Omni-DNA's cross-modal capabilities to broaden the scope of genomic applications. All the models are available through https://huggingface.co/collections/zehui127.", "sections": [{"title": "1. Introduction", "content": "The volume of genomic data has been increasing at an exponential rate over the past decades (Lathe et al., 2008), making it infeasible to annotate these sequences manually.Genomic Foundation Models (GFMs) (Nguyen et al., 2024; Zhou et al., 2023; Dalla-Torre et al., 2024; Schiff et al., 2024), a type of Deep Neural Networks (DNNs) for genomic sequence modeling, has emerged as essential tools to automate the annotation process. GFMs has been used for associating mutation in the genome with diseases (Benegas et al., 2023; Cheng et al., 2023), revealing regulatory effects of genomic sequences (Avsec et al., 2021a; Linder et al., 2025), and genomic elements annotation (de Almeida et al., 2024). Although these GFMs achieve great performance on genomic tasks (Zhou & Troyanskaya, 2015; Gre\u0161ov\u00e1 et al., 2023), they are still far away from generalist models, which can handle multiple tasks simultaneously. This contrasts with Large Language Models (LLMs) (Radford et al., 2019; Team et al., 2023; Touvron et al., 2023), which have seen remarkable success at solving general tasks from question answering to theorem proofs (Xin et al., 2024). The success of LLMs is pretraining transformer-based auto-regressive models (Vaswani, 2017) on internet-scale data, followed by post-training to increase instruction-following abilities (Lambert et al., 2024). Inspired by this, we ask: is it possible to develop a generalist genomic foundation model for addressing diverse genomic tasks simultaneously?Existing GFMs vary significantly in architecture and tokenization strategies, but they all follow a \"pretrain + task-specific finetune\" paradigm: first pretraining on unlabeled DNA sequences, and task-specific Multi-Layer Perceptrons(MLPs) are attached for finetuning. This paradigm limits the generalization of the model in two ways. (i) The pretrained model needs to be separately finetuned K times given K tasks. This results in the additional cost of storing O(K) copies of model weights, incurring significant I/O latency, memory costs, and context-switching penalties. This also prevents models from leveraging shared biological patterns (e.g., conserved regulatory motifs or chromatin accessibility signatures (Van Roey et al., 2012; Yang et al., 2023)) across finetuning tasks. (ii) The output space of existing models is rigidly constrained to DNA sequences or pre-defined genomic labels, limiting their ability to generate diverse, semantically meaningful outputs. As a result, current GFMs struggle for multi-modal downstream tasks such as DNA2Text and DNA2Image.To overcome these limitations, we introduce Omni-DNA, a family of cross-modal multi-task models ranging from 20 million to 1 billion parameters. This is achieved through carefully ablated pretraining of transformer-based auto-regressive models on DNA sequences and cross-modal multi-task finetuning.The pretraining stage explores previously overlooked key configurations and their impact on training dynamics. This includes a comparison of non-parametric LayerNorm (Ba, 2016) vs. RMSNorm (Zhang & Sennrich, 2019), ROPE (Su et al., 2024) vs. ALiBi (Press et al., 2021) positional embeddings, and other configurations, ultimately leading Omni-DNA to achieve state-of-the-art performance on 18 out of 26 tasks on the Nucleotide Transformer (Dalla-Torre et al., 2024) and GB benchmarks (Gre\u0161ov\u00e1 et al., 2023). When evaluated with a conventional paradigm, Omni-DNA outperforms existing bidirectional models.Cross-modal multi-task finetuning expands the vocabulary in the model's tokenizer by dynamically modifying the embedding matrix. To address the distribution shift of existing tokens due to vocabulary expansion (Hewitt, 2021), we propose Important Key Repetition and adopt NEFTune (Jain et al., 2023). Additionally, we implement a task unification strategy, aligning multiple genomic tasks under a unified format. The combination of these techniques enables a single model to solve multiple acetylation and methylation tasks at once, surpassing models trained on each task individually, as demonstrated in Figure 2. Furthermore, beyond conventional genomic tasks, Omni-DNA explores direct mapping from DNA sequences to functional descriptions and images, unlocking cross-modal capabilities.In summary, our contributions are three-fold:(1) We revisit the auto-regressive transformers for GFMs, indicating the potential of next token prediction paradigm.(2) We present Omni-DNA, a unified cross-modal and multi-task GFM ranging from 20M to 1B parameters, achieving SOTA performance on multiple conventional benchmarks."}, {"title": "2. Preliminaries and Notations", "content": ""}, {"title": "2.1. Genomic Sequence Modeling", "content": "DNA is a polymer made of up four types of nucleotides: Adenine (A), Thymine (T), Guanine (G), and Cytosine (C). Let N\u2081 = {A, T, G, C}. A DNA sequence of length T, denoted as x = (X1, X2, ..., XT) \u2208 N, follows a natural distribution x ~ po(x). We use p\u00f4(x) to represent an estimate to the true distribution. The dataset of unlabeled genomic sequences is given in the form of {x()}1.Genomic sequence modeling aims to learn a function f that maps input sequences to biological annotations using a labeled dataset D = {x(i),y(i)}{1. The type of y(i) varies depending on the types of tasks: y(i) is a class label in DNA Sequence Classification (Gre\u0161ov\u00e1 et al., 2023; Dalla-Torre et al., 2024). Or a real value vector in Genomic Assay Prediction tasks (Avsec et al., 2021a; Linder et al., 2025). Current genomic sequence models typically follow a two-stage strategy. In the pretraining phase, we learn the data distribution p\u00f4(x) on a unlabeled dataset from unlabeled data using losses such as masked language modeling (MLM) p(x) = \u03a0t\u0454\u043c \u0440\u03b8(xt | X1:t\u22121, Xt+1:T) or next token prediction (NTP) p(x) = \u03a0=1 Po(xt X1:t-1)."}, {"title": "2.2. Supervised Finetuning", "content": "Supervised Finetuning (SFT) plays a key role in enhancing the instruction-following (Mishra et al., 2021; Sanh et al., 2021; Wei et al., 2022) and reasoning capabilities (Lambert et al., 2024). For a pretrained auto-regressive model with a fixed vocabulary V and a labeled dataset D, SFT maximizes the likelihood: = arg maxe i=1 log pe (y(i) | x(i)) . This process retains the model's pretrained knowledge while aligning its outputs with task-specific objectives, typically using cross-entropy loss on the target tokens:pe(y(2)x(2)) = \u03a3\u03a3 log pe (y) (y1, x()).Notably, y could include new set of vocabulary V\u2082 that do not overlap with V. Therefore, each term in Equation (1) is computed through Equation (2).Po (yty1:t-1, x) =exp(ht-1eyt) / \u03a3meVo exp(-1em) + \u03a3n\u2208Vz exp(h-1en),where ht-1 is the neural representation of the prefix sequence (y1:t-1,x), em is the embedding of vocab m.As a result, during the finetuning stage, the embeddings for each new vocabulary token n \u2208 V\u2082 must be initialized, and the original token probabilities are shifted due to the expanded output space, as detailed in Hewitt (2021)."}, {"title": "3. Approach", "content": "In this section, we introduce Omni-DNA with various model sizes for genomic tasks. The pretraining process is present in Section 3.1, followed by cross-modal multi-task finetuning in Section 3.2. The framework is illustrated in Figure 4."}, {"title": "3.1. Pretraining", "content": "Pretraining Dynamics: we pretrain a series of auto-regressive transformer-based models using unlabeled DNA data, with log-uniformly spaced sizes ranging from 20 Million to 1 Billion. The pretraining procedure are conducted with minimum loss spike as shown in Figure 3(a) across models with various sizes. These model are trained on 300 billion nucleotides. The full hyperparameters used by our model and a comparison with prior genomic models are shown in Appendix A, below highlights key configurations.Model Architecture: We build on Open Language Model (OLMO) (Groeneveld et al., 2024) for DNA Sequence Modeling. Compared to the vanilla transformer (Vaswani, 2017), several improvements introduced by LLaMA family (Touvron et al., 2023) and OLMo family are adopted to our model. (i) LayerNorm The Non-parametric layer norm (Ba, 2016) without Bias are applied to our 116M and 1B model. RMSNorm (Zhang & Sennrich, 2019) are applied to the remaining fours models. We find while both types of LayerNorm result in a stable pretraining process, the Non-parametric layer norm tends to maintain more stable average weights on the feed forward layer as shown in Figure 3, which in turns help to achieve higher accuracy when performing fullsize finetuning on the downstream tasks (Section 4). (ii) Relative Positional Embeddings Although previous work (Zhou et al., 2023) indicates that relative positional embedding methods such as ALiBi (Press et al., 2021) may offer improved extrapolation capabilities, the context length of the pretrained model can still be easily extended during finetuning. We observed slower convergence when using ALiBi in our pretraining experiments (see Section 6). (iii) BPE Tokenizer We adopt Byte-Pair Encoding (BPE) (Sennrich, 2015) with an initial vocabulary of 4096, following Zhou et al. (2023). During the vocabulary expansion, newly added tokens are always recognized as standalone tokens rather than retraining the tokenizer.Pretraining Dataset Deduplication Duplicate data can slow model convergence, making deduplication standard in LLM pretraining (Rae et al., 2021; Groeneveld et al., 2024). Because genomic data is highly duplicated, we removed exact duplicates from NCBI's multi-species genome dataset (Schoch et al., 2020), reducing it to 30 billion nucleotides. Despite this reduction, multiple epochs can still be used in training."}, {"title": "3.2. Cross-modal Multi-task Finetuning", "content": "Here we extend pretrained models to handle multiple modalities beyond DNA tokens and enable learning of diverse downstream tasks through a single finetuning process. We first introduce the whole process of finetuning and then describe the detailed modules.Whole Process As illustrated in Algorithm 1, we need to finetune K task-specific datasets with different token sets and modalities. We first merge K task-specific datasets into a unified dataset Duni, and the tokenizer vocabulary are expanded to include unique new tokens from each task. During each training iteration, a batch of (x(i), y(i))}_1 is first sampled from Duni, y(i) is replicated by a factor a to emphasize class labels, and the loss is computed. The parameters @ are then updated via gradient-based optimization.By unifying multiple tasks under a single loop, the algorithm encourages the model improves the average performance across diverse genomic tasks.Multi-task Dataset Unification We integrate K genomic tasks with labeled datasets {Dk}K-1 of various formats into a unified dataset Duni = {x(i), \u1ef9(i)}. Specifically, each sample from task k is modified by appending a task-specific prompt Prompt to its input x(i): (i) = (x(i), Promptk). The corresponding response \u1ef9(i) is treated as tokens and thetypical transformation is shown in Table 1.Vocabulary Expansion As the multi-task dataset unification introduces new tokens (i.e., prompt and response tokens), it is necessary to expand the vocabulary during SFT stage. This is a key difference between genomic language model and normal language models. A direct result of vocabulary expansion is distribution shift. Based on (Hewitt, 2021), the distribution shift after adding a new set of vocabulary V\u2082 to pretrained model @ results in a new model \u03b8', let Z= exp(h1em). Then the distribution shift are:Po' (Xt | X1:t-1) = Po (Xt | X1:t-1).*exp(h_1en) / (1+\u03a3n\u2208Vzexp(h_1en)).In other words, all the words probability are reduced. To alleviate it, we should minimize the size of added tokens.NEFTune & Key Token Replication To mitigate catastrophic forgetting (Zheng et al., 2024) caused by distribution shifts from vocabulary expansion, we employ two simple yet effective techniques: NEFTune (Press et al., 2021) and Key Token Replication. NEFTune introduces random noise to embedding vectors during finetuning, reducing overfitting. Key Token Replication, applied to classification tasks, replicates label tokens in y(i) by a factor of a. This ensures stronger signal propagation, facilitating the model's transition from DNA sequence generation to classification by reinforcing the association between input sequences and their corresponding labels.Response Discretization In many genomic tasks such as genomic assay prediction (Avsec et al., 2021a) and structure prediction (Abramson et al., 2024), the response is a high-dimensional continuous variable y \u2208 Rd1xd2...\u00d7dk. To adopt Omni-DNA, we need to discretize the response first.Here we consider a three-dimensional continuous variable y\u2208 Rd1xd2Xd3. Response discretization is achieved by training a VQ-VAE (Van Den Oord et al., 2017), which learns a latent embedding space of dimension [K, D], where K represents the number of quantized embeddings, and D denotes the dimensionality of each latent embedding vector e\u017c \u2208 RD. The VQ-VAE compresses the input y(i) to a sequence of discrete latent variables \u1ef9(i) = {eo, e1, ..., eL} \u2208 Nk. Here, L = d'\u2081 \u00d7 d'\u2082 corresponds to the spatial dimensions of the encoded variable \u1ef9(i), where d\u2081 = d\u2081/r and d2 = d2/r. The compression ratio r quantifies the reduction in dimensionality achieved during encoding. The decoder then reconstructs the input y from discrete latent sequences \u1ef9. By employing response discretization, any DNA-to-continuous-variable task can be converted into a DNA-to-discrete-token task."}, {"title": "4. Results on Conventional Genomics Tasks", "content": "Setup To evaluate the quality of Omni-DNA and establish a baseline for multi-tasking, we first follow the conventional evaluation process in single task mode. We compare Omni-DNA with DNABERT-2 (Zhou et al., 2023), NT-Transformer (Dalla-Torre et al., 2024), HyenaDNA (Nguyen et al., 2024), and Caduceus models (Schiff et al., 2024), across two widely used benchmarks: NT-Downstream Tasks (Dalla-Torre et al., 2024) and Genomic Benchmark (Gre\u0161ov\u00e1 et al., 2023). Evaluation is performed using a conventional approach, where a MLP head is attached to the pretrained models, followed by full-size finetuning. The detailed set up of the evaluation is in Appendix D."}, {"title": "4.1. Nucleotide Transformer Downstream Tasks", "content": "NT Downstream Tasks include 18 distinct binary or three-class classification tasks. These tasks can be categorized into four categories: epigenetic marker (Pokholok et al., 2005), promoter (Oubounyt et al., 2019), enhancer and splice site prediction. Following the prior evaluation setup (Schiff et al., 2024), Matthews Correlation Coefficient (MCC) is used for histone marker and enhancer, while F1-score is used for splice and promoter classification. We finetune each model with 10-fold cross-validation (Schiff et al., 2024), with maximum epochs set to 20.Table 2 shows the average performance of 16 models across different tasks types and the average performance across 18 tasks. Omni-DNA (1B) and Omni-DNA (116M) achieve the best average performance of 0.767 and the second of 0.755. Both models are trained with non-parametric norm.We provide the task-specific performance of 16 models in Table 8, Omni-DNA achieves superior performance on 13 out of 18 benchmark tasks, surpassing competing methods.Task-specific results in Table 8 show that Omni-DNA outperforms competing methods on 13 out of 18 benchmarks. For the remaining five tasks, Omni-DNA (1B) ranks second and third in Promoter: ALL and Promoter: NonTATA. In the three splice site classification tasks, Omni-DNA models perform lower than NT models but still surpass other models."}, {"title": "4.2. Genomic Benchmark Results", "content": "Genomic Benchmark (GB) (Gre\u0161ov\u00e1 et al., 2023) contains eight DNA regulatory element classification tasks, similar to NT downstream tasks, with additional tasks on species classification and gene regulatory element classification on mouse. The number of sequences from seven tasks in GB was 10 times larger compared to NT downstream tasks. The details on the finetuning setting are included in Appendix D. Each model was finetuned for a max of 10 epochs. We compare the performance of smaller size model from each type of the model in Table 3.Omni-DNA (116M) achieved the highest average score, ranking first in five out of eight tasks and second in the remaining three. Compared to DNABERT-2, which has a similar model size, Omni-DNA (116M) outperformed DNABERT-2 in seven out of eight tasks."}, {"title": "5. Building Cross-modal Multi-task Genomic Foundation Models", "content": "In this section, we demonstrate the cross-modal multi-task capability of Omni-DNA in using DNA sequences to three distinct modalities: discrete labels, textual descriptions, and images. The first two are derived from real-world datasets, while the third is based on a synthetic dataset called Needle-in-DNA. Despite the significant differences in task formats, a unified approach is employed to address all three.For classification problems, Omni-DNA finetuned with multiple tasks not only has the ability to solve more than one task simultaneously but also improves overall model performance on related NT downstream tasks: a phenomenon we refer to as the synergistic effect. Additionally, we extend beyond conventional genomic modeling tasks to tackle more challenging and general problems: (1) DNA-to-function and (2) DNA-to-image. These efforts underscore the potential of training a unified genomic foundation model capable of acting as a generalist across both tasks and modalities."}, {"title": "5.1. Multi-Task Model for Acetylation and Methylation Task", "content": "In the NT downstream tasks, 10 related tasks-H3, H3K14AC, H3K36ME3, H3K4ME1, H3K4ME2, H3K4ME3, H3K79ME3, H3K9AC, H4, and H4AC-are considered. These tasks are interconnected due to the biological correlation between acetylation and methylation effects (Pokholok et al., 2005). The objective here is to train a model capable of addressing all 10 tasks with a single fine-tuning step. While conventional fine-tuning approaches with a classification head struggle to address this challenge, cross-modal multi-task finetuning unifies these tasks into a single model. At inference time, having an omni-model that solves all tasks eliminates the need for context switching and repeated fine-tuning.We use Omni-DNA (1B) to complete 10 acetylation and methylation tasks using multi-task finetuning with NEFTune noise level = 5 and repeating factor = 10. The baselines include the best-performing single-task models: Omni-DNA (1B) and CADUCEUS-PH (1.9M), as presented in Section 4, along with DNABERT-2 and NT (500M) as references. Conventionally, multi-task finetuning may lead to performance drop compared with single-task finetuning due to the challenge of balancing generalizability across tasks. However, by grouping similar tasks, we observe significant improvements. The results, shown in Table 4 and Figure 2, demonstrate that Omni-DNA@mult achieves dramatic improvements in 4 tasks, and obtains the highest average accuracy. This indicates that the resulting model not only reduces the burden of context switching and repetitive finetuning but also leverages the internal relationships between tasks to achieve higher performance than its single-task counterparts. We refer to this phenomenon as the synergistic effect, which emerges in finetuning genomic foundation models in a multi-task setting."}, {"title": "5.2. Functional Annotation Generation (DNA2Func)", "content": "Dataset and Task As shown in Figure 10, we have constructed a large-scale DNA sequence dataset enriched with text-based functional annotations. This dataset comprises 330,000 sequences from various mammalian species, and each sequence is annotated with a concise short annotation followed by a more detailed natural language description. We split the dataset into finetuning and test sets with a 9:1 ratio. The full workflow for dataset construction is in Appendix F. This is a multi-task scenario with two objectives: (1) generating concise short annotations for DNA sequences and (2) further explaining the corresponding functions by natural language.Baselines We finetuned our proposed Omni-DNA, using the finetuning dataset, denoted as Omni-DNA@ft. Since conventional genomic foundation models cannot handle these tasks, we employed GPT-40 (GPT4o@zeroshot) to directly predict DNA functions using the prompt in Table 11. In addition, we included OLMO-1B as a baseline, a similar architecture model pre-trained on natural language, and finetuned on the same finetuning dataset with Omni-DNA, referred to as OLMO-1B@ft.Evaluation & Metrics To evaluate the accuracy of free-form text is challenging. Thus, we leveraged an LLM (GPT40) to judge whether the generated text and ground-truth are matched. We first utilized GPT4o to classify the generated text into seven function labels, as shown in Figure 10. Then the F1 scores and Matthews Correlation Coefficient (MCC) are computed between the classified labels and ground-truth.Quantitative Results As shown in Table 5, Omni-DNA obtains the best result compared to GPT4o and OLMO-1B, highlighting the potential for solving this domain-specific problem. Tables 12 to 14 present representative responses from the three models. Among them, Omni-DNA@ft consistently demonstrates the strongest performance. In contrast, GPT4o fails to understand the meaning of the DNA sequences in all three examples, whereas the other two models provide more meaningful responses. Notably, Omni-DNA is capable of producing grammatical and novel sentences, rather than merely reproducing funetuning datasets."}, {"title": "5.3. Needle-in-DNA Task (DNA2Image)", "content": "Dataset and Task The identification of short functional motifs in DNA is pivotal for understanding gene regulation because these conserved subsequences often serve as transcription factor binding sites or other key regulatory elements (Tompa et al., 2005; Avsec et al., 2021b). Over the past decade, deep learning methods have further advanced motif discovery by more accurately predicting sequence specificities of DNA- and RNA-binding proteins (Alipanahi et al., 2015), thus refining our ability to pinpoint critical regulatory regions within the genome. Leveraging motif-aware learning for training LLMs, has proven effective for uncovering regulatory and structural elements in genomic sequences (Wang et al., 2024; Sanabria et al., 2024). Motivated by this, we generate a dataset consisting of 48,000 synthetic DNA sequences by a Hidden Markov Model. Each sequence contains one of four functional motifs (referred to as \"needles\"): {TATAAA, CAAT, GGGCGG, TTAGGG}, and is labeled 0, 1, 2, or 3 accordingly. Notably, the label is represented by an image of a handwritten digit {0, 1,2,3}, sampled from the MNIST dataset (Yadav & Bottou, 2019). The dataset is split into finetuning, validation, and test sets in an 8:1:1 ratio.Following data discretization in Section 3.2, we train a VQ-VAE (Van Den Oord et al., 2017) with six quantized vectors of dimension 32 and a compression ratio of 4. This model converts the grayscale images of size 28 \u00d7 28 into 49 discrete tokens. It leverages a multi-task setting with two tasks: (1) classify the DNA sequences, and (2) generate the corresponding handwritten digit image. The details of dataset construction can be found in Appendix E.Evaluation and Baselines To assess the benefit of pretraining on DNA sequences, we employ two baselines: OLMo-1B, a natural language model pretrained on text with instruction tuning, and the Vanilla Model, which shares Omni-DNA's architecture but is randomly initialized without pretraining. The baselines and Omni-DNA (1B) are all fine-tuned on the fine-tuning dataset for 10 epochs.The evaluation is conducted by two human annotators who independently assess the images generated by the three models. They provide two labels: (1) validity indicating whether the image represents a number from {0,1,2,3}, and (2) the corresponding digit if valid. Inconsistent answers between annotators are discarded. This labeling process identifies two types of errors: vague or non-meaningful images, and incorrect classification. Thus, two metrics are used to measure model performance: (1) invalid percentage, the proportion of generated images that are non-numeric or not in {0,1,2,3}, and (2) macro F1 scores, averaged across all valid samples, for each class.Results Figure 5 shows that Omni-DNA achieves a Macro F1 score of 0.987 and an invalid percentage of 1% on average, significantly outperforming baselines. This indicates that Omni-DNA nearly solves the task perfectly. In contrast,while OLMO-1B has a high F1 score in the motifs TATAAA (class 0) and CAAT (class 1), it struggles with GGGCGG (class 2) and TTAGGG (class 3). Since its invalid percentages are similar across these classes, the primary issue is generating incorrect digit images, indicating that OLMo-1B fails to classify DNA sequences accurately.Beyond Memorization Our visualization results demonstrate that Omni-DNA does not merely memorize the finetuned images, but actually generates novel handwritten digits with various shapes. We verify that these examples are not present in the fine-tuning set, indicating that the model learns general digit patterns in {0,1,2,3} conceptually rather than memorizing discretized tokens. See Figure 9 for generated examples."}, {"title": "6. Ablation Study", "content": ""}, {"title": "6.1. Ablation on Positional Embedding Methods", "content": "We utilized Omni-DNA (116M) to test the effect of two positional embedding methods: ALiBi (Press et al., 2021) and ROPE (Su et al., 2024). We perform pretraining on 300 billion nucleotides with keeping the remaining hyperparameters unchanged. The training and test losses during pretraining are shown in Figure 6. RoPE achieves a lower test loss and converges faster, indicating that it better captures the contextual and relational properties of DNA compared to ALiBi."}, {"title": "6.2. Larger Models Mitigate Distribution Shifts", "content": "Equation (3) shows that introducing new vocabulary leads to a distribution shift of existing tokens. While synergistic effects in multi-task settings can mitigate this shift (Son et al., 2024), single-task scenarios experience performance degradation when new tokens are added compared to using a classification head. Figure 7 shows how performancedegradation varies with model size in the promoter TATA classification task. Notably, increasing the model size alleviates this issue, and Omni-DNA (1B) maintains performance comparable to those, prior to vocabulary expansion."}, {"title": "6.3. Impact of Token Replication Factor", "content": "Figure 8 illustrates the effect of the token replication factor a on the promoter TATA classification task for Omni-DNA sized 116M and 1B. Without token replication (a = 1), classification performance is poor. We find that setting a within the range [8, 11] is effective across various task types."}, {"title": "7. Conclusion", "content": "We have demonstrated that pretraining an auto-regressive transformer on DNA sequences, followed by cross-modal multi-task finetuning, is an effective strategy for constructing a unified genomic foundation model. This model canhandle diverse genomic tasks across multiple modalities, including discrete labels, textual descriptions, and other multi-dimensional outputs. Specifically, we demonstrate that: (i) The auto-regressive model can match or outperform the bidirectional transformer on genomic sequence modeling. (ii) The model pretrained solely on genomic sequences can generalize to unseen tokens through finetuning, achieving the SOTA performance. Our proposed Omni-DNA not only handles multiple classification tasks within a single model, but also tackles more complex challenges, such as converting DNA into coherent, contextually relevant natural language, and mapping DNA to multi-dimensional representations, as showcased in DNA2Funtion and DNA2image.We hope that this paper with available codes for pretraining and finetuning, will serve as a starting point for researchers to explore more complex genomic tasks and develop more comprehensive genomics foundation models.Limitations Our work primarily focuses on transformer-based architectures, leaving the exploration of alternative approaches, such as state-space models like Mamba (Gu & Dao, 2023) and Hyena (Nguyen et al., 2024), and recurrent neural network architectures like xLSTM (Schmidinger et al., 2024) for future research. Additionally, while our model demonstrates the feasibility of mapping DNA to multi-dimensional outputs, such as structure or genomic assay signals, achieving high-fidelity cross-modality translation remains an open challenge. Future improvements in tokenization strategies (Li et al., 2024a; Pagnoni et al., 2024; Qiao et al., 2024), including more robust vector quantization (VQ-VAE) techniques, could potentially improve the model's ability to handle complex modalities with greater precision. Moreover, the DNA pretraining corpus, while extensive at 300B nucleotides, may not fully capture the diversity of regulatory elements across all species and tissue types (Andrews et al., 2023). Finally, while we have demonstrated strong performance on regulatory element classification and selected tasks, our evaluation does not encompass important genomic challenges like long-range interaction prediction (Cheng et al., 2025) or variant effect prediction (Li et al., 2024b)."}, {"title": "A. Pretraining Model Configurations", "content": "The family of Omni-DNA has six models, and their parameters and architectures are shown in Table 6 and Table 7."}, {"title": "B. Task-specific Nucleotide Transformer Benchmark Results", "content": "We present the detailed task-specific results for NT-Downstream tasks in Table 8 and Table 9."}, {"title": "C. Detailed Deduplication Process", "content": ""}, {"title": "C.1. Handling Non-{A, T, G, C} Characters", "content": "The raw genome data from the reference genome includes additional characters beyond {A, T, G, C}, which represent ambiguities or gaps. These characters are defined as follows:\u2022 N: Represents any nucleotide (A, T, G, or C).\u2022 R: Represents purines (A or G)."}, {"title": "C.2. Chunk-Based Deduplication", "content": "The deduplication procedure was performed as follows:1. Chunking: The genome was divided into non-overlapping chunks of 1024 base pairs.2. Exact Matching: Identical sequences across the dataset were identified and removed.3. Efficiency: This step utilized hashing techniques and optimized string comparison algorithms to handle the large dataset efficiently."}, {"title": "C.3. Results", "content": "The raw genome dataset initially contained approximately 170 billion nucleotides. After applying the deduplication process, the dataset was reduced to 30 billion nucleotides, representing unique sequences across multiple species."}, {"title": "C.4. Rationale for Multiple Epochs", "content": "Although deduplication significantly reduced the size of the dataset, the smaller dataset was iterated over multiple epochs during pretraining. This approach ensured that repeated sequences were temporally separated during the training process. By maximizing the temporal distance between occurrences of the same sequence, the risk of overfitting was mitigated, leading to better generalization performance."}, {"title": "D. Finetuning with Classificatoin Head", "content": "Basic Setup We closely follow the setup from Caduceus (Schiff et al.", "compared": "Omni-DNA, DNABERT-2 (Zhou et al., 2023), NT-Transformer (Dalla-Torre et al., 2024), HyenaDNA (Nguyen et al., 2024), and the Caduceus models (Schiff et al., 2024).For NT Downstream tasks, we use a maximum"}]}