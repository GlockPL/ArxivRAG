{"title": "Operating System And Artificial Intelligence: A Systematic Review", "authors": ["Yifan Zhang", "Xinkui Zhao", "Jianwei Yin", "Lufei Zhang", "Zuoning Chen"], "abstract": "In the dynamic landscape of technology, the convergence of Artificial Intelligence (AI) and Operating Systems (OS) has emerged as a pivotal arena for innovation. Our exploration focuses on the symbiotic relationship between AI and OS, emphasizing how AI-driven tools enhance OS performance, security, and efficiency, while OS advancements facilitate more sophisticated AI applications. We delve into various AI techniques employed to optimize OS functionalities, including memory management, process scheduling, and intrusion detection. Simultaneously, we analyze the role of OS in providing essential services and infrastructure that enable effective AI application execution, from resource allocation to data processing. The article also addresses challenges and future directions in this domain, emphasizing the imperative of secure and efficient AI integration within OS frameworks. By examining case studies and recent developments, our review provides a comprehensive overview of the current state of AI-OS integration, underscoring its significance in shaping the next generation of computing technologies. Finally, we explore the promising prospects of Intelligent OSes, considering not only how innovative OS architectures will pave the way for groundbreaking opportunities but also how AI will significantly contribute to advancing these next-generation OSs.", "sections": [{"title": "1. Introduction", "content": "Operating systems (OSes) have long been central to computer systems, efficiently managing hardware resources and providing secure environments for application execution. However, the increasing complexity of modern OSes, the rapid diversification of hardware, and the continuous evolution of Artificial Intelligence (AI) present new opportunities to explore AI's potential throughout the lifespan of OSes, spanning both development and runtime. As the AI wave continues to surge, AI systems grow increasingly massive and complex, necessitating optimization and efficiency enhancements at the lower layers of the stack, particularly within the operating system.\nThis review delves into the intricate relationship between AI and OS, examining how AI-driven tools enhance OS performance, security, and efficiency. Conversely, we explore how advancements in OS design facilitate the deployment and optimization of AI applications. By scrutinizing various AI techniques employed to augment OS functionalities such as memory management, process scheduling, and intrusion detection\u2014this review provides a comprehensive overview of the current state of AI-OS integration. Additionally, we discuss the critical role of the OS in providing essential services and infrastructure necessary for effective AI application operation, from resource allocation to data processing. The article also addresses challenges and future research directions in this domain, emphasizing the imperative for secure and efficient integration of Al capabilities within OS frameworks. Through systematic examination of case studies and recent developments, this review underscores the significance of the AI-OS nexus in propelling the next generation of computing technologies.\nIn this paper, we embark on a comprehensive exploration of the intersection between AI and OS. Our approach involves collecting and analyzing 108 primary studies in the field, aiming to uncover key insights and trends. The rest of this paper is organized as follows. We provide the research background.Then we propose our research questions, report on the paper selection process, and analyze the distribution of research popularity. Collected papers in the fields of AI4OS and OS4AI are classified and summarized in Sections 4, 5 and 6. Furthermore, we discuss how and what novel OS architectures may create opportunities for AI4OS in section 7 and how LLM can create oppotunies for OS in section 8.\nSubsequently, in Section 9, we consolidate these in-"}, {"title": "2. Background", "content": "Operating systems have been providing a crucial layer of abstraction between applications and hardware resources. The design of an OS can significantly impact hardware resource management efficiency and, consequently, the performance of all applications running on it. Additionally, the development of OSes requires extensive engineering efforts from experts. And the complexity of modern OSes poses challenges for developers to effectively improve and optimize performance. The scale and dynamics of computing systems further contribute to this complexity.Moreover, the escalating scale and intricacy of AI systems themselves necessitate optimizations from the OS. As the complexity increases and the need for efficient resource management grows, the integration of AI techniques has emerged as a promising avenue for enhancing OS functionality and performance.\nIn recent years, researchers have explored various dimensions of AI integration in operating systems, leading to notable advancements."}, {"title": "3. Methodology", "content": "We present the research questions, introduce the paper selection process, and statistically analyze the selected papers in this section."}, {"title": "3.1. Research questions", "content": "The definition of research questions is the core innovation of a secondary study, as they clearly convey the authors' perspective on the subject under investigation and the study's goal. We characterize the purpose, the major research topics, and the scope of this paper as follows.\nRQ1: What OS sub-domains are researchers inclined to enhance using AI?\nRQ2: How can OS be optimized to improve the efficiency of AI systems?\nRQ3: How and which AI techniques are used to improve OS?\nRQ4: How and what novel OS architectures create opportunities for AI4OS?\nRQ5: How LLMs create opportunities for OS?"}, {"title": "3.2. Paper Selection", "content": "We identify four AI-related terms and two OS-related terms, then combine these two categories of terms by logical ORs to create nine search strings. After specifying the range the papers were published: 2019-March 2024, we deliver these nine search strings to Google Scholar, to collect related papers in the intersection of AI and OS. The defined search terms are as follows:\nAI or Machine Learning or Deep Learning or Large Language Model\nAnd\nOS or Operating System\nWe began with an initial pool of 212 papers related to the intersection of AI and Operating Systems (OS). To narrow down our focus, we applied the following inclusion criteria (IC) and exclusion criteria (EC):\nIC1: Peer-Reviewed Research Paper: We considered only peer-reviewed research papers.\nIC2: Primary Study: We included primary studies that directly addressed AI-OS integration.\nIC3: Relevance to AI and OS: Papers needed to discuss AI techniques enhancing OS or propose novel approaches for improving Al systems using OS methods.\nIC4: Publication Date: We restricted our selection to papers published within the past 5 years.\nEC1: Avoiding Duplicates: Only the most comprehensive or recent version of each study was included.\nEC2: Length Constraint: Papers shorter than 6 pages were excluded.\nEC3: Language: We considered papers written in English.\nAs a result, 108 papers met our criteria and are within the scope of our research. Excluded papers merely mentioned the terms but did not delve into the actual intersection of OS and AI."}, {"title": "3.3. Paper Analysis", "content": "3.3.1. The main venues. Figure 1 depicts the distribution of publication years for research 1. Notably, the number of papers published in the AI4OS and OS4AI domains indicates a growing trend, increased attention in this field. Notably, the number of papers published(including pre-print) in the field of integrating LLM and OS indicate a rapidly growing trend,"}, {"title": "4. RQ1: What OS sub-domains are researchers\ninclined to enhance using AI?", "content": "Currently, OSes employ global, static policies based on heuristics. However, AI techniques, adaptable to different application behaviors, hold promise for outperforming these traditional policies. This research question explores and quantifies the diverse applications of AI approaches in enhancing or automating various OS tasks."}, {"title": "4.1. AI for OS Tuning", "content": "Growing OS complexity challenges configuration and decision-making, prompting the exploration of AI and ML for enhancements in auto-tuning tasks, including scheduling, energy efficiency, and memory management.\nScheduling. Scheduling, a core OS function, balances fairness, responsiveness, and throughput by efficiently managing resources. [2] [16] Advancements through machine learning can refine ticket distribution, enable adaptability, and adjust scheduling variables precisely.\nSpringborg et al. [86] introduce Chronus, a Python application that collaborates with the Simple Linux Utility for Resource Management (SLURM) scheduler, prevalent in global supercomputers. Chronus executes comprehensive benchmark tests on HPC clusters, varying parameters like core numbers, processor speeds, and hyperthreading. Throughout, it logs performance metrics and energy use for\neach setup. This data trains a machine learning model to recognize patterns, aiming to forecast the most energy-efficient configuration for specific jobs and systems. Proven through notable energy savings in benchmarks, this approach holds potential for broader application in HPC systems, promoting eco-friendliness and financial prudence without sacrificing computational power.\nChen et al. [3] presents a ML-based resource-aware load balancer for the Linux kernel with a low-overhead method for collecting training data,an ML model based on a multi-layer perceptron that imitates the Linux's Completely Fair Scheduler (CFS) load balancer based on the collected training data and an in-kernel implementation of inference on the model. The authors argue that CFS approach maximizes the utilization of processing time but overlooks the contention for lower-level hardware resources.Using eBPF for dynamic tracing, an MLP model replicates CFS decisions, with in-kernel inference for real-time balancing, the model achieves high accuracy and small latency increase, demonstrating effective, low-overhead load management.\nGoodarzy et al. [14] also questioned CFS in its ability for proper allocation of CPU, memory, I/O, and network bandwidth. In response to this, the authors propose SmartOS, an operating system that automatically learns what tasks the user deems to be most important at that time. ased on the learned user preferences, SmartOS adjusts the allocation of system resources such as CPU, memory, I/O, and network bandwidth. It prioritizes the resources for the tasks that the user is currently focused on. The authors demonstrate an implementation of such a learning-based OS in Linux and present evaluation results showing that a reinforcement learning-based approach can rapidly learn and adjust system resources to meet user demands.\nStorage. Storage systems, along with their associated OS components, are engineered to cater to a broad spectrum of applications and fluctuating workloads. The storage elements within the OS incorporate a range of heuristic algorithms, ensuring optimal performance and adaptability across diverse workloads.\nPredictable latency can be very useful for data-center"}, {"title": "4.2. AI for OS security", "content": "With the widespread application of deep learning in recent years, using deep learning technologies for OS security has emerged, and the effectiveness of threat detection has been dramatically improved.\nQin et al. [35]presents MSNdroid, a novel malware detector designed specifically for Android applications, leverages a combination of native API calls, permissions, system API call features, and a Deep Belief Network. By applying deep learning techniques to native code features, MSNdroid effectively detects Android malware. This approach involves extracting features from a comprehensive dataset comprising malicious applications and benign applications. Notably, MSNDroid achieves an impressive accuracy while maintains an impressively low false-negative rate.\nDe Wit et al. [39] emphasize the value of incorporating machine learning in malware detection strategies for Android platforms. By leveraging accessible hardware data and sophisticated classification techniques, the study demonstrates the feasibility of identifying malware with a reasonable degree of precision, highlighting the potential of app-specific metrics in enhancing detection rates. This research contributes to the growing body of knowledge on AI-integrated security measures within operating systems, particularly pertinent to the Android ecosystem."}, {"title": "4.3. Findings", "content": "In response to Research Question 1, which explores the inclination of researchers to utilize AI in enhancing specific OS sub-domains, our comprehensive analysis of existing literature highlights two prominent areas: auto-tuning and security. These domains have attracted considerable attention due to their potential for transformative improvements through AI integration.\nAuto-tuning emerges as a key area where AI is being employed to overcome the shortcomings of conventional heuristic-based methods. These traditional approaches often struggle to cope with the ever-evolving demands of modern computing environments. Researchers are harnessing machine learning to dynamically adjust OS parameters, such as scheduling, energy management, memory allocation, and storage optimization, aiming to boost system performance and resource efficiency. Al's predictive abilities are particularly crucial in enhancing scheduling mechanisms to strike a balance between fairness, responsiveness, and throughput, effectively managing critical resources, ensuring they can handle the intricate and fluctuating workloads common in today's computing landscapes.\nThe security domain has witnessed a surge in the adoption of deep learning technologies to combat malware threats, showcasing substantial advancements in detection efficacy. Al-powered malware detectors have proven superior in terms of accuracy and reduced false negatives compared to traditional machine learning models. Furthermore, AI frameworks are being developed to create adversarial malware, underscoring AI's dual role in fortifying OS security while also exposing vulnerabilities in AI-based detection systems."}, {"title": "5. RQ2: How can OS be optimized to improve\nthe efficiency of AI systems?", "content": "AI accelerators are different from traditional hardware, affecting all aspects of system design, from data-center scale to single-chip scale. They also add high requirement for system architecture, management, and programming [13].\nPrevious work has shown AI jobs critically demand high-speed I/O and low-latency and high-bandwidth data communication [18] [21].Various attempt on hardware has been done to improve ML application performance,for exmaple, using newly NVMe SSD [89], relying on hardware FPGA for the I/O communication control instead of relying on OS-level interrupts that can significantly reduce both total I/O latency and its variance and algorithm level. Accessing hardware through the kernel introduces a performance bottleneck. To mitigate this bottleneck, one effective approach is to bypass the kernel altogether, enabling userspace programs to directly access hardware [20].\nBateni et al. [89] presents NeuOS, a comprehensive system solution aimed at providing latency predictability for executing multi-DNN workloads in autonomous systems while simultaneously managing energy optimization and dynamic accuracy adjustments. It coordinates system- and application-level solutions intelligently to ensure that multiple DNN instances operate efficiently and meet their respective deadlines to guarantee latency predictability. It manages energy consumption by dynamically adjusting parameters such as voltage and frequency scaling to minimize energy usage without compromising the latency predictability or accuracy requirements. Based on specific system constraints, NeuOS adjusts the accuracy level of DNN computations in real-time. This allows for trade-offs between computational precision and resource efficiency, ensuring that the system operates within its given constraints while maintaining an acceptable level of performance.\nWang et al. [21] showed what network for GPU AI remoting,a technique where the execution of GPU APIs is managed remotely through a network on a remote proxy instead of running GPU computations locally on the machine. The study takes a GPU-centric perspective to derive minimum latency and bandwidth requirements and aims to ensure that unmodified AI applications can run on remoting setups using commodity networking hardware without performance degradation. The paper introduces a novel theoretical framework that quantifies the minimum network requirements essential for efficient GPU API remoting. By formalizing the relationship between network latency, bandwidth, and remoting efficiency, this model provides foundational insights.\nSerizawa et al. [19]proposed an solution focused on I/O bandwidth. The method aims to improve the reading performance of large training datasets by using high-performance I/O storage devices. The authors discusses the problem of copying datasets between local storage and shared storage and proposes a solution to conceal the time spent on copying by overlapping the copying and reading of training data."}, {"title": "5.1. Findings", "content": "Addressing Research Question 2, which delves into the ways OSes can elevate the efficiency of Al systems, our analysis underscores the pivotal function of OSes in handling the distinctive needs of AI tasks.\nTo optimize AI efficiency through the OS, several strategic approaches emerge as essential. Firstly, there is a need to tailor the OS to cater specifically to the requirements of AI accelerators. This includes facilitating high-speed I/O operations, minimizing latency in communications, and enabling autonomous operation of AI tasks. Secondly, the development of specialized runtime systems and schedulers becomes crucial, ensuring optimal allocation of resources and efficient execution of AI processes. Thirdly,optimizing I/O bandwidth further enhances the performance of AI applications. Collectively, these strategies form a comprehensive framework for improving the efficiency and effectiveness of AI systems through optimized OS integration."}, {"title": "6. RQ3: How and which AI techniques are\ntypically used to improve OS?", "content": "To answer RQ3, we review all studies that investigate AI techniques applied in OS in this section. Among 38 AI4OS previous work,those did not clear clarify the AI tools excluded, we list the AI tools that are used more than twice. In general, RF, RNN (LSTM), KNN, RL, MLP,etc. are the most widely-used techniques."}, {"title": "6.1. Tools analysis", "content": "RF(DT). A decision tree is a type of machine learning model used when the relationship between a set of predictor variables and a response variable is non-linear, while random forest is essentially a collection of decision trees. It is quick to fit to a dataset and easy to interpret. Chowdhury et al. [32] used RF as one if the models to accurately detect the attack from the network traffic. To construct the random forest classifier, the authors employed the Random Forest Regressor, which serves as both a regressor and a meta estimator. It accomplishes this by fitting multiple decision trees to different subsets of the dataset. For our specific model, we opted for a forest containing 1000 trees.Ahmed et al. [23] used RF to build a device fitness model, based on the Dataset collected during runtime and statically. De Wit et al. [39] trained a statistical classifier able to recognize malware signatures in any log data collected on a smartphone. The classifier was trained, cross-validated, and tested using the dataset described above and RF classifier had a better performance.Metzge el at. [49] used RF to get a optimal kernel runtime switching slice size. The model is a random forest regressor with 50 decision trees with a depth of two for the GPU model. Ongun et al. [36] used RF to get the probability of a command being malicious,based on labels dataset.\nRNN(LSTM). RNNs are a class of neural networks designed to handle sequential data. They have feedback connections, allowing them to maintain an internal state or memory. Each step in an RNN processes an input and updates its hidden state based on the current input and the previous hidden state.While LSTM,a type of RNN architecture with a more complex cell structure,were introduced to address the vanishing gradient problem. Motivated by the problem that exploiting 3D-stacking memory's performance is challenging because bandwidth utilization heavily depends on address mapping in the memory controller, Zhang et al. [17] used a software-defined address mapping, allowing user programs to directly control low-level memory hardware in an intelligent and fine-grained manner.LSTM is used to in a method to get access pattern information to select an address mapping. It identify the major variables that significantly contribute to external memory access and have a substantial impact on memory traffic and data movement.\nWordEmbedding. Word embeddings provide a way to achieve this by mapping words to dense vectors in a multi-dimensional space. Fu et al. [34] proposed a AI-based approach to help under-resourced security analysts to find, detect, and localize vulnerabilities. They utilized a word-level Clang tokenizer with a copy mechanism. This tokenizer broke down a C function into a sequence of tokens.Then word embedding was used to generate vector representations for each token in the sequence, capturing the semantic information among the input tokens. And further classifier was, Ongun et al. [36] explore techniques for representing command-line text using word embedding methods.Based on this, they devise ensemble boosting classifiers to differentiate between malicious and non-malicious commands.\nKNN. KNN is a supervised machine learning algorithm used for both classification and regression tasks. The fundamental idea behind KNN is simple: neighbors influence each other. If you're surrounded by similar things, you're likely similar too. It is widely applicable in pattern recognition, data mining, and intrusion detection. Yang et al [42] introduce a KNN-based machine learning algorithms can accurately predict the Turnaround-time(TaT) of a process. It can effectively reduce the TaT of the process and reduce the number of process context switches.\nMLP. An MLP is a type of feedforward neural network used for supervised learning tasks, such as classification and regression. Chen et al. [3] argues that traditional Linux CFS scheduler maximizes the utilization of processing time but overlooks the contention for lower-level hardware resources and try to solve the above problem using an ML-based resource-aware load balancer.They employed supervised imitation learning to replace a portion of its internal logic with an MLP model. This trained MLP model emulates the kernel's load balancing decisions. MLP is chosen because this current work doesn't require a very complex model and MLP has a relatively simple implementation compared to the other models. Based on this work, Qiu et al. [78] propose the concept of reconfigurable kernel datapaths that enables kernels to self-optimize dynamically to reduce the cost of kernel. The authors also used MLP ML model that can mimic Linux CFS decisions."}, {"title": "6.2. Findings", "content": "In addressing Research Question 3, which explores the methods by which AI techniques are utilized to enhance OS, our analysis reveals a variety of approaches tailored to meet distinct OS requirements. These AI techniques are strategically chosen to address a spectrum of OS challenges, encompassing security enhancements, performance forecasting, resource management, and process scheduling. The widespread adoption of these methods attests to their versatility and underscores the profound influence of AI in augmenting OS functionality and performance. This integration of AI into core OS components not only boosts operational efficiency but also paves the way for more intelligent and adaptable systems, capable of meeting the evolving demands of modern computing environments."}, {"title": "7. RQ4: How and what novel OS architectures\ncreate opportunities for AI4OS?", "content": "Comprehensive research has revealed that utilizing the standard kernel pathway for hardware interaction results in notable efficiency losses, rendering conventional approaches less than ideal for the current AI-driven environment. In response, kernel bypass tactics are gaining prominence, aiming to optimize hardware utilization and enhance real-time capabilities specifically for AI tasks. Nonetheless, the absence of an OS means these strategies fall short in providing the necessary tailoring to exploit AI applications' full potential.\nIn essence, the interplay between modern application design and OS innovation fosters a fertile ground for the conception of systems that not only meet but also anticipate the evolving needs of AI-driven environments. This confluence of advancements signals a pivotal moment in the trajectory of computing, where the synergy between AI and OS architectures could redefine the boundaries of what is achievable in high-performance computing."}, {"title": "7.1. Kernel-bypass OS Structure for AI", "content": "Prior research has successfully demonstrated the potential of leveraging hardware acceleration for machine learning within the kernel space, showcasing the feasibility of such an approach [4]. This study introducES an API remoting system, which facilitates access to specialized accelerator interfaces for kernel space applications. Moreover, it simplifies the integration by offering high-level APIs directly to the kernel space, eliminating the necessity for kernel-specific adaptations of complex libraries. The API remoting mechanism transmits commands between kernel and user space. This innovative design not only enhances the performance of AI applications within the kernel but also provides a compelling perspective on the capabilities of modern operating systems.\nRaza et al. [53] propose integrating unikernel optimizations into Linux, known for creating secure, compact OS images for single applications. Unikernel Linux (UKL) reduces the number of executed instructions and improves instructions-per-cycle efficiency. Tail latency tests on Memcached-a multi-threaded key-value store-show that UKL achieves a significant performance improvement. It introduces a configuration option that allows a single, optimized process to link directly with the kernel, bypassing the traditional system call overhead, and significantly cuts latency for system call payloads, showcasing the benefits of unikernel-inspired enhancements in refining Linux's performance.\nCadden et al. [59] introduces Serverless Execution via Unikernel SnapShots (SEUSS), a method that leverages unikernel snapshots for rapid deployment and high-density caching of serverless functions. The authors describe the use of unikernel contexts, which consist of a high-level language interpreter configured to import and execute function code, providing isolation and security. This minimalistic approach leads to a reduced memory footprint and faster startup times compared to traditional operating systems like Linux, which is beneficial for serverless environments where rapid function instantiation is crucial. By using unikernels, the SEUSS system can cache a large number of function instances in memory due to the reduced memory footprint."}, {"title": "7.2. Library OS", "content": "One significant obstacle that OSs face in effectively leveraging the potential of AI lies in the absence of a comprehensive, universally applicable strategy for adapting AI technologies to the wide array of heterogeneous devices in use.\nThe Demikernel project [52] unveils an OS architecture optimized for datacenter systems with microsecond-scale requirements, emphasizing low-latency I/O. Adopting a LibOS strategy, it side-steps the traditional kernel in I/O paths, significantly enhancing performance. This design supports kernel-bypass devices, allowing seamless application operation with negligible overhead at the nanosecond scale. For I/O efficiency, Demikernel applies zero-copy techniques for large buffers, optimizes resource use, and maintains system stability through periodic LibOS interaction. Security is bolstered with controlled data placement and potential advanced memory integrity measures. LibOS components employ hardware acceleration to offload critical tasks, minimizing latency and maximizing throughput. Kernel bypass in I/O paths reduces overhead from context switching, system calls, and memory duplication."}, {"title": "7.3. Other Stuctures", "content": "Skiadopoulos et al. [60] present DBOS as a superior alternative to conventional cluster OS components, offering comparable functionality but enhanced analytics and reduced code complexity. DBOS matches current systems in scheduling, file management, and inter-process communication, yet excels in analytics and simplifies code through database query-based OS services. It efficiently implements low-latency transactions and ensures high availability. DBOS's integrated DBMS approach is especially advantageous for ML, delivering a cohesive platform for efficient resource management and analytics in large-scale distributed environments, adeptly managing parallel computation and workload dynamics across various hardware.\nShan et al. [61] developed LegoOS-a splitkernel architecture for hardware disaggregation, decentralizing OS functions for scalable, distributed management. It fundamentally breaks down traditional OS functionalities into loosely-coupled monitors that each run on and manage a distinct hardware component. The splitkernel model distributes responsibilities such as scheduling, memory management, and I/O operations across these monitors, effectively creating a distributed set of hardware components that can be independently managed and scaled. The splitkernel model in LegoOS allows for independent scaling of compute and memory resources, essential for ML scenarios where large"}, {"title": "7.4. Findings", "content": "Addressing Research Question 4, our analysis highlights the transformative role of novel OS architectures in advancing AI4OS. Pioneering designs like LibOS and unikernels have sparked renewed excitement among researchers, demonstrating their potential to outshine traditional OSes. These cutting-edge architectures enhance the symbiosis between software and hardware, reducing latency to boost A\u0399 application responsiveness and real-time processing capabilities. Their streamlined construction optimizes resource allocation, ensuring efficient management of computational assets. Beyond performance, these architectures establish a secure foundation for AI deployment. By simplifying the environment, they minimize security risks, vital for AI applications managing sensitive data. DBOS and LegoOS exemplify this by simplifying distributed AI setup and maintenance, freeing experts to focus on algorithmic innovation. They also adapt nimbly to the variable demands and hardware diversity of modern ML, enhancing deployment efficiency.\nIn essence, these advanced OS architectures elevate performance and security, fostering an innovative landscape for AI4OS. By catering to AI application requirements, they promise to rewrite the rules of computing, blending AI and OS synergies for smarter, more efficient, and secure computing futures."}, {"title": "8. RQ5: How LLMs create opportunities for\nOS?", "content": "The integration of LLMs into OSes presents a significant opportunity to enhance the user experience and overall system functionality [67] [68]. LLMs, with their advanced natural language processing capabilities, [62] [63] [64] can transform the way users interact with and manage their computing environments. [76] We simply divide these works into two big categories: LLM AS OS and LLM4OS."}, {"title": "8.1. LLM AS OS", "content": "\"LLM AS OS\" refers to the integration of LLMs into the core of an OS, effectively serving as the \"brain\" of the system,make OS capable of understanding and responding to natural language commands, thereby enabling more intuitive and flexible human-computer interaction [70] [69].\nKamath et al.'s LLaMaS [54] addresses the complexities of diverse computing environments by leveraging LLMs to ease OS challenges in hardware adaptation and resource management. It adapts to new devices by interpreting plain text specs, recommending optimized OS tactics. The LLM frontend deciphers system characteristics, converting them into actionable embeddings. The backend uses these for on-the-fly OS decisions, like memory tier data movement. An experiment with ChatGPT demonstrated its skill in adjusting memory allocation for CPU and GPU tasks based on usage. Aimed at reducing administrative and research costs, LLaMaS autonomously aligns with hardware changes via LLMs' innate zero-shot learning, eliminating the need for manual adjustments or detailed device-specific coding.\nAIOS-Agent ecosystem [55] [56], where LLMs act essentially as an \"operating system with soul\".LLMs are embedded in the OS kernel for intelligent decision-making and resource allocation. Its context window acts as memory, managing relevant data, while external storage serves as a file system with enhanced retrieval capabilities. Hardware and software are treated as peripherals and libraries, enabling agent-environment interaction. Natural language becomes the primary programming interface, democratizing software development. This ecosystem supports single and multi-agent applications for executing a broad range of tasks.\nMemGPT [57] presents a solution to overcome the fixed-length context window limitation in LLMs, a hurdle for tasks demanding deep analysis of lengthy dialogues or extensive texts. Drawing parallels with traditional OS hierarchical memory systems, MemGPT introduces virtual context management, echoing virtual memory principles. It enables LLMs to access information beyond immediate capacity, mirroring OS memory management through strategic 'paging'. This system reacts to events like user messages, system alerts, and timers, appending pertinent data to the primary context buffer before processing. Additionally, it supports function chaining for uninterrupted, sequential task execution, accommodating intricate operations and long-term planning within the LLM's context limitations."}, {"title": "8.2. LLM40S", "content": "SecRepair [58], designed to address the challenge of identifying and repairing code vulnerabilities in software development. This system is powered by a LLM and incorporates reinforcement learning and semantic rewards to enhance its capabilities.To support the training process and prepare a robust model for vulnerability analysis, the authors have compiled and released a comprehensive instruction-based vulnerability dataset.They also propose a reinforcement learning technique with a semantic reward mechanism to generate concise and appropriate code commit comments. This technique is inspired by how humans fix code issues and provides developers with a clear understanding of the vulnerability and its resolution.\nRahman et al. [71] introduce ChronoCTI, an automated pipeline for mining temporal dynamics between attacker actions from text-based accounts of cyber incidents. The focus is on pinpointing repetitive action sequences, termed temporal attack patterns, crucial for preemptive defense strategies by security professionals against impending cyber threats. ChronoCTI is anchored by a curated dataset linking sentences to adversary maneuvers, temporal linkages across 94 attack narratives, and large language models refined on cybersecurity topics. The research delineates a structured"}, {"title": "8.3. Findings", "content": "Addressing Research Question 5, our analysis highlights the incorporation of LLMs within operating systems heralds a transformative era, reshaping system dynamics and enhancing user engagement. They can enrich user interfaces by facilitating natural language processing for voice commands and text inputs, leading to more intuitive and personalized user experiences. In the realm of development, LLMs can automate code generation and optimization, streamlining coding processes and reducing errors. Additionally, they contribute to dynamic system management by analyzing usage patterns to optimize resource allocation, boosting system responsiveness. LLMs also play a pivotal role in intelligent troubleshooting, diagnosing system issues more accurately and recommending preventive measures to minimize downtime. Furthermore, they bolster OS security by detecting anomalies indicative of malicious activities and assisting in secure configuration management. Lastly, LLMs act as intelligent aids for developers, offering insights into function queries, suggesting best practices, and even generating documentation, thus accelerating the development process and enhancing code quality. Through continuous learning, LLMs ensure that OS performance and efficiency improve over time, adapting to user needs and operational contexts."}, {"title": "9. Opportunities for Future Work", "content": "The escalating sophistication of AI workloads, characterized by high-velocity I/O, minimal latency, and bandwidth-intensive requirements, is catalyzing an urgent need for AI-specialized OS optimization. As AI permeates diverse sectors, the imperative for a finely calibrated infrastructure escalates, poised to support the burgeoning demands of next-generation applications. The emergence of AI-dedicated hardware has dramatically reshaped AI computation landscapes. These technological marvels, while delivering unrivaled performance, necessitate software ecosystems meticulously attuned to their singular attributes.\nIn this multifaceted hardware ecosystem [20] [23], the need for an OS that can seamlessly manage and orchestrate AI tasks across a spectrum of devices is more pressing than ever. Al has transcended beyond the realm of traditional neural networks, with innovative models leading the change in natural language processing and graph neural networks adeptly handling intricate relational data. These advanced applications are calling for specialized optimizations that a one-size-fits-all approach cannot provide. Juggling performance, energy conservation, and scalability constitutes a multifaceted challenge, demanding an OS equipped for"}, {"title": "9.1. Optimized OS for AI", "content": "dynamic adjustment to cater to the singular requisites of every workload.\nPortatble OS for AI. In industrial settings, IoT devices generate massive amounts of data. Deploying deep learning models directly on edge devices (such as sensors, gateways, or edge servers) in equally important as relying solely on cloud-based processing. In a certain sense, deploying on edge devices can be a more challenging scenario because the computational power and bandwidth, of edge devices are more constrained [77] [12] [1]. Due to the diversity of edge devices,current AI platforms' lack of portability across different edge platforms hinders widespread adoption ,and bypassing the kernel and directly accessing hardware while adapting to various hardware configurations can be extremely challenging. Therefore, an operating system that can accommodate multiple hardware types and is specifically tailored for AI applications is a potential solution [7] [8].\nFast deployment and standard API interface. Simplified deployment means that even non-experts can utilize AI applications effectively. Developers, data scientists, and industrial engineers can quickly integrate the framework into their existing edge devices without extensive configuration or manual setup. And a standard API interface ensures that software components can communicate seamlessly with each other.\nMinimized OS consumption. Resource-constrained environments, such as edge devices or embedded systems, often operate with tight constraints on memory, processing power, and energy while in scenarios like cloud computing or data centers, minimizing resource consumption directly impacts operational expenses. As OS and applications share the resources, the OS consumption determines the actual operational performance of the algorithmic model. So there is a need to design an efficient OS that can achieve small memory and fast training and inference. There are several promising approaches to achieve this. Apart from bypassing the kernel, unikernel OSes are designed to be fast and lightweight [11] [32].\nPrivacy and security. The problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Increasingly, edge devices are equipped with AI applications. This trend comes with privacy risks as models can leak information about their training data [24]. OS need to find ways to provide safeguards for ensuring the confidentiality and integrity of its data and programs."}, {"title": "9.2. Intelligent OS", "content": "9.2.1. Potential for Intelligent LibOS. A recently emerging trend of Internet-based software systems is \"resource adaptive,\" i.e., software systems should be robust and intelligent enough to the changes of heterogeneous resources [9], both physical and logical, provided by their running environment [26]. The key principle of such an OS is based"}, {"title": "9.2.2. Multi-agent LLM for OS Development", "content": "Previous research has demonstrated the efficacy of employing AI in optimizing, safeguarding, and evaluating system software including operating systems [10] [22] [28]. Our study reveals a notable dearth of efforts dedicated to leveraging LLMs in the context of operating systems. Nonetheless, we identify at least one distinct domains where LLMs can potentially revolutionize the functioning and capabilities of OSes as Multi-agent LLM for OS Development."}, {}]}