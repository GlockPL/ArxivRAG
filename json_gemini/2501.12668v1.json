{"title": "NBDI: A Simple and Efficient Termination Condition for Skill Extraction from Task-Agnostic Demonstrations", "authors": ["Myunsoo Kim", "Hayeong Lee", "Seong-Woong Shim", "JunHo Seo", "Byung-Jun Lee"], "abstract": "Intelligent agents are able to make decisions based on different levels of granularity and duration. Recent advances in skill learning enabled the agent to solve complex, long-horizon tasks by effectively guiding the agent in choosing appropriate skills. However, the practice of using fixed-length skills can easily result in skipping valuable decision points, which ultimately limits the potential for further exploration and faster policy learning. In this work, we propose to learn a simple and efficient termination condition that identifies decision points through a state-action novelty module that leverages agent experience data. Our approach, Novelty-based Decision Point Identification (NBDI), outperforms previous baselines in complex, long-horizon tasks, and remains effective even in the presence of significant variations in the environment configurations of downstream tasks, highlighting the importance of decision point identification in skill learning.", "sections": [{"title": "1. Introduction", "content": "The ability to make decisions based on different levels of granularity and duration is one of the key attributes of intelligence. In reinforcement learning (RL), temporal abstraction refers to the concept of an agent reasoning over a long horizon, planning, and taking high-level actions. Each high-level action corresponds to a sequence of primitive actions, or low-level actions. For example, in order to accomplish a task with a robot arm, it would be easier to utilize high-level actions such as grasping and lifting, instead of controlling every single joint of a robot arm. Temporal abstraction simplifies complex tasks by reducing the number of decisions the agent has to make, thereby alleviating the challenges that RL faces in long-horizon, sparse reward tasks.\nDue to the advantages of temporal abstraction, there has been active research on developing hierarchical RL algorithms, which structure the agent's policy into a hierarchy of two policies: a high-level policy and a low level policy. The option framework (Sutton, 1998) was proposed to achieve temporal abstraction by learning options, which are high-level actions that contain inner low level policy, initiation set and termination conditions. Termination conditions are used to figure out when to switch from one option to another, enabling the agent to flexibly respond to changes in environment or task requirements. While the option framework can achieve temporal abstraction without any loss of performance when the options are optimally learned, it is usually computationally challenging to optimize for the ideal set of options within complex domains.\nIn this case, the skill discovery framework, which aims to discover meaningful skills (fixed-length executions of low-level policy) from the dataset through unsupervised learning techniques, has been used as an alternative. Recently, notable progress has been made in skill-based deep RL models, showing promising results in complex environments and robot manipulations (Pertsch et al., 2021a; Hakhamaneshi et al., 2021; Park et al., 2023). However, the use of fixed-length skills and the absence of appropriate termination conditions often restrict them from making decisions at critical decision points (e.g., crossroads), which can result in significant loss in performance. While there have been some studies incorporating the option framework into deep RL as is, the algorithmic complexity and unstable performance in large environments limit its widespread adoption (Kulkarni et al., 2016; Hutsebaut-Buysse et al., 2022).\nIn this paper, we present NBDI (Novelty-based Decision Point Identification), a simple state-action novelty-based decision point identification method that allows the agent to learn terminated skills from task-agnostic demonstrations. In this context, the term task-agnostic refers to the collection of trajectories from a diverse set of tasks, excluding the one we are specifically interested in (see Appendix E for visualizations). Identifying critical decision points promote"}, {"title": "2. Related Works", "content": "Option Framework One major approach of discovering good options is to focus on identifying good terminal states, or sub-goal states. For example, landmark states (Kaelbling, 1993), reinforcement learning signals (Digney, 1998), graph partitioning (Menache et al., 2002; \u015eim\u015fek et al., 2005; Machado et al., 2017a;b), and state clustering (Srinivas et al., 2016) have been used to identify meaningful sub-goal states. (Digney, 1998; Simsek et al., 2005) and (Kulkarni et al., 2016) focused on detecting bottleneck states, which are states that appear frequently within successful trajectories, but are less common in unsuccessful trajectories (e.g., a state with access door). (\u015eim\u015fek & Barto, 2004) tried to identify access states, which are similar to bottleneck states, but determined based on the relative novelty score of predecessor states and successor states. Access states are found based on the intuition that sub-goals will exhibit a relative novelty score distribution with scores that are frequently higher than those of non sub-goals. These studies motivated us to search for states with meaningful properties to terminate skills. However, these methods frequently face challenges in scaling to large or continuous state spaces.\nSkill-based deep RL As extending the classic option framework to high-dimensional state spaces through the adoption of function approximation is not straightforward, a number of practitioners have proposed acquiring skills, which are fixed-length executions of low-level policies, to achieve temporal abstraction. For example, skill discovery (Gregor et al., 2016; Achiam et al., 2018; Mavor-Parker et al., 2022; Park et al., 2023) and skill extraction (Yang et al., 2021; Singh et al., 2020; Pertsch et al., 2021b; Hakhamaneshi et al., 2021) frameworks have proven to be successful in acquiring meaningful sets of skills. Especially, Pertsch et al. (2021a) showed promising results in complex, long-horizon tasks with sparse rewards by extracting skills with data-driven behavior priors. The learned prior enables the agent to explore the environment in a more structured manner, which leads to better performance in downstream tasks. However, we believe that their performances are greatly constrained by the use of fixed-length skills, which restricts them from making decisions at critical decision points. There are prior works introducing variable-length skill extraction methods. Salter et al. (2022) focus on learning an option-level transition model leveraging predictability to compress offline behaviors into options that terminate at bottleneck states. However, due to its model-based design, the extracted skills are not suitable for transferring to downstream tasks with significantly different environment configuration. Jiang et al. (2022) is an option framework based model that learns both options and termination condition from the task-agnostic demonstrations in terms of minimum description length (Rissanen, 1978). As it serves as an appropriate benchmark for evaluating our approach, we selected Jiang et al. (2022) as one of the baselines to compare with.\nNovelty-based RL Novelty has been utilized in reinforcement learning for various purposes. Depending on its design, novelty can be used for curiosity-driven exploration (Burda et al., 2018; Pathak et al., 2019; Sekar et al., 2020), or data coverage maximization (Bellemare et al., 2016; Hazan et al., 2019; Seo et al., 2021). It has been also used to identify sub-goals in discrete environments (Goel, 2003; \u015eim\u015fek & Barto, 2004). However, to the best of our knowledge, there has been no research that has utilized state-action novelty for identifying decision points in the context of deep RL."}, {"title": "3. Background", "content": "Markov Decision Process (MDP) MDP is a mathematical framework to model decision making problems with discrete-time control processes. It is defined by a tuple {S, A, P, R, y}, where S denotes a state space, A denotes a set of actions the agent can execute, P(s' s, a) denotes a transition probability, R(s, a) is a reward function and y is a discount factor. In a MDP, the probability of transitioning to a future state depends solely on the current state, which is known as the Markov property. Given a MDP, we aim to find an optimal policy \\(\\pi^*\\) that maximizes the expected discounted sum of reward \\(E_{\\pi} [\\Sigma_{t=0}^{\\infty} \\gamma^{t}R(s, a)]\\). The state value function \\(V^{\\pi}(s)\\) and the action value function \\(Q^{\\pi}(s, a)\\) denote the conditional expectation of discounted sum of reward following policy \\(\\pi\\)."}, {"title": "4. Simple and Efficient Identification of Decision Points", "content": "The option framework aims to achieve temporal abstraction by learning good options, and good options can be learned through the identification of meaningful sub-goal states (Menache et al., 2002; \u015eim\u015fek & Barto, 2004), i.e., the critical decision points. In this work, we propose to use state-action novelty to identify critical decision points for skill termination, which leads to the execution of variable-length skills. In particular, we use intrinsic curiosity module (ICM) (Pathak et al., 2017) as our state-action novelty estimator (more details in Section 6.1). However, any state-action novelty estimation mechanism that measures the joint novelty of state-action pairs can be used for our approach."}, {"title": "4.1. State-action Novelty-based Decision Point Identification", "content": "Our proposed method classifies a state-action pair with high joint state-action novelty as a decision point. A more insightful perspective on this choice can be obtained by breaking down the novelty estimator (Equation 1). By interpreting joint novelty \\(\\chi(s, a)\\) as the reciprocal of joint visitation count N(s, a), we can decompose a state-action joint novelty \\(\\chi\\) into the product of a state novelty and a conditional action novelty. The proposed method combines the strength of both novelty estimates.\n\\(\\chi(s, a) = \\frac{1}{N(s,a)} = \\frac{1}{N(s)} \\frac{1}{N(a|s)} = \\chi(s) \\chi(a|s) \\)\n        (1)\nThe state novelty \\(\\chi(s)\\) will seek for a novel state, which refers to a state that is either challenging to reach or rare in the dataset of agent experiences. As the skills are derived from the same pool of experiences that we use to estimate novelty, a high state novelty implies a potential lack of diverse skills to explore neighboring states effectively. Thus, increasing the frequency of decision-making in such unfamiliar states will lead to improved exploration and broader coverage of the state space when solving downstream tasks.\nA conditional action novelty \\(\\chi(a|s)\\) will seek for a novel action. With the state conditioning, action novelty will be high in a state where a multitude of actions have been frequently executed. For example, unlike straight roads, crossroads provide the agent with options to move in multiple directions. In such states, the agent may need to perform different actions to accomplish the current goal, rather than solely depending on the current skill. This necessity arises because the current skill may have been originally designed for different goals, making it potentially less than ideal for"}, {"title": "4.2. Termination Improvement from State-action Novelty-based Terminations", "content": "We provide an alternative interpretation on the potential benefits of identifying decision points based on state-action novelty. While maximizing skill length is advantageous in terms of temporal abstraction, extended skills can result in suboptimal behavior, especially when the skills are derived from task-agnostic trajectories. Such suboptimality of extended skills (or options) can be theoretically quantified using the termination improvement theorem (Sutton, 1998).\nTheorem 4.1. [Termination Improvement, (Sutton, 1998), informal] For any meta-control policy u on set of options O, define a new set of options O', which is a set of options that we can additionally choose to terminate whenever the value of a state \\(V^{\\mu}(s)\\) is larger than the value of a state given that we keep the current option o, \\(Q^{\\mu}(s, o)\\). With \\(\\mu'\\), which has the same option selection probability as \\(\\mu\\) but over a new set of options O', we have \\(V^{\\mu'}(s) \\geq V^{\\mu}(s)\\).\nThe termination improvement theorem implies that we should terminate an option when there are much better alternatives available from the current state. When the skills (or options) are discovered from diverse trajectories (e.g., trajectories gathered from a diverse set of goals), termination improvement is typically observed in states where a multitude of actions have been executed, such as crossroads.\nTo identify the states where termination improvement occurs, we plotted the relative frequency of termination improvement occurrences in a small 8 \u00d7 8 grid maze with three different goal settings (Figure 3 (left)). It shows that termination improvement frequently occurs in states where diverse plausible actions exist. In states with a single available option, \\(V^{\\mu}(s)\\) would be equal to \\(Q^{\\mu}(s, o)\\). On the other hand, as more actions/options are plausible, \\(Q^{\\mu}(s, o)\\) would exhibit a broader range of values, thereby increasing the likelihood of satisfying \\(Q^{\\mu}(s,o) < V^{\\mu}(s)\\).\nHowever, terminating skills based on the termination improvement theorem can be challenging when the downstream task is unknown, as it requires \\(Q^{\\mu}(s, o)\\) and \\(V^{\\mu}(s)\\) to be computed in advance with the skills extracted from the downstream task trajectories. Thus, by leveraging the data collected across a diverse set of tasks, we can use conditional action novelty as a tool for pinpointing the states where a multitude of plausible actions can be taken (Figure 3 (middle)). Through experiments, we also found state novelty to be useful in terminating skills, as it encourages the agent to sufficiently explore unfamiliar parts of the state space (Figure 3 (right)). As a result, we propose to use state-action novelty, which combines the strength of both conditional action novelty and state novelty as in Equation 1, as our skill termination condition. In Section 6, we also demonstrate how these different novelty measures, utilized as termination conditions, lead to different performance outcomes."}, {"title": "5. Learning Termination Conditions through State-action Novelty Module", "content": "Our goal is to improve the learning of a new complex and long-horizon task by identifying critical decision points through a state-action novelty module. While fixed-length skills have been mostly considered for temporal abstractions in recent studies (Pertsch et al., 2021a; Hakhamaneshi et al., 2021), utilizing fixed-length skills can easily skip valuable decision points, ultimately reducing the opportunities for further exploration.\nIn this work, we propose to use state-action novelty as a termination condition to effectively capture critical decision points and execute terminated skills. Our approach consists of two major steps. First, we train the state-action novelty module and then the low-level policy using an offline trajectory dataset for skill extraction. Next, we perform online reinforcement learning with the pre-trained state-action novelty module to solve an unseen task."}, {"title": "5.1. Unsupervised Learning of State-action Novelty Module", "content": "In the process of unsupervised learning, our goal is to pre-train the low-level policy \\(\\pi(a|z, s)\\) and the state-action novelty module \\(p(\\beta|s, a)\\). We define a skill \\(z \\in Z\\) as an embedding of state-action pairs \\(\\tau = \\{(s_i, a_i)\\}_{i=t}^{t+H-1}\\) and termination conditions \\(B = \\{p_i\\}_{i=t}^{t+H-1}\\). The termination conditions B are Bernoulli random variables that decide when to stop the current skill. Through the classification of state-action pairs demonstrating significant novelty \\(\\chi(s, a)\\), \\(\\beta\\) are trained to predict the critical decision points. The point at which novelty is considered significant varies depending on the environment. In downstream tasks, the skill being executed will be terminated either when \\(\\beta\\) = 1 is sampled or when the maximum skill length H is reached. In all experiments, we set H = 30. The low-level policy is trained following standard practices (Pertsch et al., 2021a; Hakhamaneshi et al., 2021) (see Appendix B for details)."}, {"title": "5.2. Reinforcement Learning with Skill Termination Conditions", "content": "In downstream learning, our objective is to learn a skill policy \\(\\mu_\\theta (z|s)\\) that maximizes the expected sum of discounted rewards, parameterized by \\(\\theta\\). The pre-trained low-level policy \\(\\pi(a|z, s)\\) decodes a skill embedding z into a series of actions, which persists until the skill is terminated by the predicted termination condition \\(\\beta\\).\nThe downstream learning can be formulated as a SMDP which is an extended version of MDP that supports actions of different execution lengths. We aim to maximize discounted sum of rewards \\(E_{\\tau \\in \\mathcal{T}} r(s_t, z_t)\\) where T is set of time steps where we execute skills, i.e., \\(T = \\{0, k_0, k_0 + k_1, k_0 + k_1 + k_2, ...\\}\\) and \\(k_i\\) is the variable skill length of i-th executed skill. The RL learning loop is described in Algorithm 1. In downstream RL tasks, we use SAC (Haarnoja et al., 2018) to update the high-level policy. More details of the learning procedure are in Appendix B."}, {"title": "6. Experiments", "content": "We design the experiments to address the following questions: (i) Does learning state-action novelty-based termination condition improve policy learning in unseen tasks? (ii) How does each component of state-action novelty contribute to the identification of critical decision points? (iii) Have we successfully identified the decision points that match our intuition? Additional studies are in Appendix A."}, {"title": "6.1. State-action Novelty Module", "content": "We utilize ICM (Pathak et al., 2017) to calculate state-action novelty for both image-based and non-image-based observations."}, {"title": "6.3. Results", "content": "We use the following models for comparison: Flat RL (SAC): Soft Actor-Critic (Haarnoja et al., 2018) agent that does not leverage prior experience. This comparison illustrates the effectiveness of temporal abstraction. SAC+Novelty: Flat RL that uses state-action novelty as intrinsic rewards. This comparison demonstrates the importance of incorporating state-action novelty in skill learning. Flat Offline Learning w/ Finetuning (BC+SAC, IQL+SAC): The supervised behavioral cloning (BC) policy and Implicit Q-Learning (IQL) (Kostrikov et al., 2021) policy that are trained on offline data and subsequently fine-tuned for the downstream task using SAC. Fixed-length Skill Policy (SPiRL): The agent that learns a fixed-length skill policy (Pertsch et al., 2021a) by leveraging prior experience. This comparison demonstrates the benefit of critical decision points identification through state-action novelty. NBDI (Ours): The agent that learns a terminated skill policy through state-action novelty \\(\\chi(s, a)\\). It learns a state-action novelty based termination distribution \\(p(\\beta|z, s)\\) to predict skill termination at current step. State Novelty Decision Point Identification (NBDI-\\(\\chi(s)\\)): The agent that learns a terminated skill policy through state novelty. To exclusively assess the influence of the novelty type, we distilled the state-action novelty module used in NBDI into a separate network, \\(\\chi(s)\\), which solely depends on the current state. Conditional Action Novelty Decision Point Identification (NBDI-\\(\\chi(a|s)\\)): The agent that learns a terminated skill policy through conditional action novelty \\(\\chi(s,a), \\chi\\), where \\(\\chi(s)\\) is the distilled state novelty module used for NBDI-\\(\\chi(s)\\). Implementation details of the baselines are in Appendix G.\nIn both the robot manipulation tasks and the navigation tasks, executing terminated skills through state-action novelty (NBDI-\\(\\chi(s, a)\\)) facilitates convergence toward a more effective policy (Figure 5). While NBDI manages to accomplish all four subtasks in the kitchen environment, others never achieves the maximum return. Furthermore, as shown the table in Figure 5, NBDI surpasses SPiRL even within a challenging robotic simulation environment where there are no clearly defined subtasks (Sparse block stacking). However, SAC, SAC+Novelty, BC+SAC and IQL+SAC show poor performance due to their lack of temporal abstraction, which limits their ability to explore unseen tasks effectively.\nIn alignment with our motivation for state-action novelty, conditional action novelty (NBDI-\\(\\chi(a|s)\\)) appears to play a crucial role in identifying decision points (Figure 6a). While it appears that terminating skills solely based on state novelty doesn't lead to better performance, combining it with conditional action novelty (resulting in state-action novelty) leads to better exploration and better convergence.\nFigure 7 compares decision points made by SPiRL and NBDI in the maze environment. This result provides the answer to our third question. While the SPiRL agent makes decisions in random states, our model tends to make decisions in crossroad states or states that are unfamiliar. For instance, in the lower-right area of the maze, SPiRL shows periodic skill terminations due to its fixed-length of skills, whereas our approach tends to make decisions in states characterized by high conditional action novelty or state novelty."}, {"title": "6.4. Comparison to Other Variable-length Skill Extraction Methods", "content": "We compare the performance of NBDI, relative novelty (\u015eim\u015fek & Barto, 2004) and LOVE (Jiang et al., 2022) in the maze and kitchen environment (Figure 6b). Relative novelty identifies termination condition based on the assumption that sub-goals typically show a relative novelty score distribution with higher scores than those of non-sub-goals. LOVE is an option framework based model that learns both options and termination condition from the task-agnostic demonstrations in terms of minimum description length. Since LOVE is designed for discrete action space, we used an MLP layer as the action decoder to handle continuous actions. Furthermore, since LOVE was only evaluated in"}, {"title": "6.5. Critical Decision Points with Suboptimal Data", "content": "We investigate how the quality of offline data affects the performance of our approach. We trained a behavior cloning (BC) policy on expert-level trajectories to generate mediocre quality demonstrations. We additionally added weighted Gaussian random noise (\\(\\sigma\\)) to actions of BC policy to add stochasticity to the generated dataset. Table 1 shows that even in a less challenging goal setting compared to Figure 7, SPiRL fails to reach the goal, while NBDI achieves a success rate of 28% and 22%. However, as the policy generating the trajectory becomes more stochastic (Table 1), it gathers data primarily around the initial state, leading to an overall reduction in the scale of prediction errors. Thus, we can see that the level of stochasticity in the dataset influences critical decision point detection."}, {"title": "6.6. Model capacity and critical decision point detection", "content": "To assess how the model capacity and dataset utilization influence critical point detection, we varied the width and depth of the neural network across different settings. Figure 8 (left) shows that the estimated state-action novelty by ICM is not affected by the number of parameters used to train the model. Figure 8 (right) demonstrates that the scale of the estimated state-action novelty remains consistent even as the dataset size decreases. We can see that our proposed approach for detecting critical decision points is robust to various number of parameters or size of datasets."}, {"title": "7. Conclusion", "content": "We present NBDI, an approach for learning terminated skills through a state-action novelty module that leverages offline, task-agnostic datasets. Our approach significantly outperforms previous baselines in solving complex, long-horizon tasks and shows effectiveness even under significant changes in environment configuration of downstream tasks. A promising direction for future work is to use novelty-based decision point identification to learn variable-length skills in meta-reinforcement learning (Nam et al., 2022)."}, {"title": "A. Ablation", "content": "Figure 9a compares the performance of our model (NBDI-th0.3) in the kitchen environment with different state-action novelty threshold values. We can see that there is no significant improvement in performance compared to SPiRL when the threshold value is not appropriately chosen. For example, as illustrated in Figure 10, termination distributions learned with low threshold values can disturb the policy learning by terminating skills in states that lack significance. It illustrates that threshold value needs to be appropriately chosen to capture meaningful decision points.\nFigure 9b shows the performance drop when we do not learn the termination distribution in advance (see Appendix B for more details). NBDI-NoTermDistr directly uses the state-action novelty module in the downstream learning phase to terminate skills. The performance gap indicates that the skill embedding space in offline skill extraction learning needs to be learned with terminated skills to effectively guide the agent in choosing variable-length skills. Thus, it is necessary to jointly optimize the termination distribution, skill embedding space, and skill prior using the deep latent variable model in offline skill extraction.\nFigure 9c shows the performance difference when we use cumulative sum of state-action novelties to learn decision points. NBDI-CumulativeSum terminates skills once the cumulative sum of state-action novelty reaches or surpasses a predefined threshold. This comparison implies that accumulating novelties does not lead to the identification of significant termination points."}, {"title": "A.4. Comparison to SPiRL with Fixed Average Skill Length of NBDI", "content": "Figure 11 shows whether NBDI still outperforms SPiRL when SPiRL uses average skill length of NBDI (SPiRL (avg-fixed)). The average lengths of skills of NBDI was 26 in the maze environment, 22 in the sparse block stacking environment, and 25 in the kitchen environment. We found NBDI still outperforms SPiRL with those fixed average skill lengths. We also observed that SPiRL, when set to those average skill lengths, performs worse than SPiRL configured with a fixed skill length of 10 in the block stacking environment, and better in the maze environment. However, across any fixed skill length ranging from 10 to 30, there was no instance where SPiRL outperformed NBDI. This demonstrates that our model can effectively leverage critical decision points in the environment compared to fixed length approaches.\nTo investigate how frequently the maximum skill length gets reached, we tracked the skill lengths of high-level policy for each environment during the downstream reinforcement learning process. The percentages of executed skill lengths shorter than H are as follows: 19.8% in the maze environment, 30.8% in the sparse block stacking environment, and 17.4% in the kitchen environment. When categorizing the skill lengths into intervals of 1-10, 11-20, and 21\u201330, the distributions are as follows: 13.2%, 2.8%, and 84% for the maze environment; 28.6%, 1.6%, and 69.8% for the sparse block stacking; and 16.8%, 0%, and 83.2% for the kitchen.\nIt can be observed that, for the majority of the time, our high-level policy maximizes temporal abstraction by executing longer skills (ranging from 21 to 30). However, our method also allows the high-level policy to capture important decision points through shorter skills (ranging from 1 to 10), promoting more efficient exploration of the state space and enhancing the transfer of knowledge across various tasks."}, {"title": "B.1. Learning the Skill Prior, Skill Embedding Space and Termination Distribution", "content": "To learn the skill embedding space Z, we train a latent variable model consisting of a Long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) encoder \\(q_\\phi(z|\\tau, \\beta)\\) and a decoder \\(p_\\psi(a_t, \\beta_t|z, s_t)\\). To learn model parameters \\(\\phi\\) and \\(\\psi\\), the latent variable model receives a randomly sampled experience \\(\\tau\\) from the training dataset D along with a termination condition vector \\(\\beta\\) from the state-action novelty module, and tries to reconstruct the corresponding action sequence and its length (i.e., point of termination) by maximizing the evidence lower bound (ELBO):\n\\(logp(a_t, \\beta_t| s_t) > E_{z \\sim q_{\\phi}(z|\\tau,\\beta), \\tau \\sim D} [log p_{\\psi}(a_t, \\beta_t|z, s_t) + \\alpha (log p(z) \u2013 log q_{\\phi}(z|\\tau, \\beta)]\\)\n\\(\\mathcal{L}_{rec}(\\phi,\\psi)\\)  \\(\\mathcal{L}_{reg}(\\phi)\\)\n(2)\nwhere \\(\\alpha\\) is used as the weight of the regularization term (Higgins et al., 2016). The Kullback-Leibler (KL) divergence between the unit Gaussian prior \\(p(z) = \\mathcal{N}(0, I)\\) and the posterior \\(log q_{\\phi}(z|\\tau, \\beta)\\) makes smoother representation of skills.\nTo offer effective guidance in selecting skills for the current state, the skill prior \\(p_n (z|s_t)\\), parameterized by \\(\\eta\\), is trained by minimizing its KL divergence from the predicted posterior \\(q_\\phi(z|\\tau, \\beta)\\). In the context of the option framework, it can also be viewed as the process of obtaining an appropriate initiation set \\(\\mathcal{I}\\) for options/skills. This will lead to the minimization of the prior loss:\n\\(\\mathcal{L}_{prior}(\\eta) = E_{\\tau \\sim D} [D_{KL}(q_{\\phi}(z|\\tau,\\beta)||p_{\\eta}(z|s_t))]\\)  (3)\nThe basic architecture for skill extraction and skill prior follows prior works (Pertsch et al., 2021a; Hakhamaneshi et al., 2021), which have proven to be successful. In summary, termination distribution, skill embedding space, and skill prior are jointly optimized with the following loss:\n\\(\\mathcal{L}_{total} = \\mathcal{L}_{rec}(\\phi,\\psi) + \\alpha \\mathcal{L}_{reg}(\\phi) + \\mathcal{L}_{prior}(\\eta) \\)  (4)"}, {"title": "B.2. Reinforcement Learning with NBDI", "content": "In downstream learning, our objective is to learn a skill policy \\(\\pi_{\\theta}(z|s_t)\\) that maximizes the expected sum of discounted rewards, parameterized by \\(\\theta\\). The pre-trained decoder \\(p_{\\psi}(a_t, \\beta_t|z, s_t)\\) decodes a skill embedding z into a series of actions, which persists until the skill is terminated by the predicted termination condition \\(\\beta_t\\). The downstream learning can be formulated as a SMDP which is an extended version of MDP that supports actions of different execution lengths.\nAdapted from Soft Actor-Critic (SAC) (Haarnoja et al., 2018), we aim to maximize discounted sum of rewards while minimizing its KL divergence from the pre-trained skill prior on SMDP. The regularization weighted by \\(\\omega\\) effectively reduces the size of the skill latent space the agent needs to explore.\n\\(J(\\theta) = E_{\\pi_{\\theta}} [\\sum_{t \\in T} r(s_t, z_t) \u2013 \\omega D_{KL}(\\pi_{\\theta}(z_t|s_t), p_n(z_t|s_t))]\\)  (5)\nwhere T is set of time steps where we execute skills, i.e., \\(T = \\{0, k_0, k_0 + k_1, k_0 + k_1 + k_2, ...\\}\\) where \\(k_i\\) is the variable skill length of i-th executed skill.\nTo handle actions of different execution lengths, the following Q-function objective is used:\n\\(J_Q(\\xi) = E_{(s_t, z_t, r_t, s_{t+k+1}, k) \\sim D, z_{t+k+1} \\sim \\pi_{\\theta}(\\cdot|s_{t+k+1})} [\\frac{1}{2} (Q_{\\xi}(s_t, z_t) \u2013 Q)^2]\\)\nwhere \\(Q = r_t+\\gamma^k [Q_{\\xi}(s_{t+k+1}, z_{t+k+1}) \u2013 \\omega D_{KL}(\\pi_{\\theta}(z_{t+k+1}|s_{t+k+1})||p_n(z_{t+k+1}|s_{t+k+1}))]\\)\n\\(\\omega\\) represents the temperature for KL-regularization, k denotes the number of time steps elapsed from the start state \\(s_t\\) to the termination state \\(s_{t+k+1}\\), and r represents the cumulative discounted reward over the k time steps. The detailed RL learning loop is described in Algorithm 2 and Figure 12."}, {"title": "C. Visualization of Critical Decision Points with Suboptimal Data", "content": "Figure 13 and Figure 14 (top, middle) show that with suboptimal dataset, NBDI is still able to learn termination points characterized by high conditional action novelty or state novelty. Figure 13a and Figure 13e shows that SPiRL can only navigate around the initial state using fixed-length skills extracted from the suboptimal dataset, whereas NBDI can successfully reach the goal efficiently (Figure 13b and Figure 13f).\nHowever, with dataset generated by random walk (Figure 14 (bottom)), it becomes challenging to learn meaningful decision points. As the policy generating the trajectory becomes more stochastic, it gathers data primarily around the initial state, leading to an overall reduction in the scale of prediction errors. Thus, we can see that the level of stochasticity in the dataset influences critical decision point detection."}, {"title": "D. Visualization of Critical Decision Points in Complex Physics Simulation Tasks", "content": "We investigated whether meaningful decision points can be found in complex physics simulation tasks. We trained ICM using different offline datasets provided by D4RL (Fu et al., 2021) (halfcheetah-medium-expert, halfcheetah-medium-replay, ant-medium-expert, ant-medium-replay) to assess its ability to detect critical decision points. Figure 15 illustrates the presence of critical decision points in complex physics simulation tasks. For instance, the cheetah has the option of spreading its hind legs or lowering them to the ground, and the ant has the choice of flipping to the right or lowering themselves to the ground. However in completely random datasets (halfcheetah-random, ant-random), we were not able to find any meaningful decision points. Similar to Appendix C, it shows that the degree of"}]}