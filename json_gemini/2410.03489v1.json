{"title": "Gradient-based Jailbreak Images for Multimodal Fusion Models", "authors": ["Javier Rando", "Hannah Korevaar", "Erik Brinkman", "Ivan Evtimov", "Florian Tram\u00e8r"], "abstract": "Augmenting language models with image inputs may enable more effective jailbreak attacks through continuous optimization, unlike text inputs that require discrete optimization. However, new multimodal fusion models tokenize all input modalities using non-differentiable functions, which hinders straightforward attacks. In this work, we introduce the notion of a tokenizer shortcut that approximates tokenization with a continuous function and enables continuous optimization. We use tokenizer shortcuts to create the first end-to-end gradient image attacks against multimodal fusion models. We evaluate our attacks on Chameleon models and obtain jailbreak images that elicit harmful information for 72.5% of prompts. Jailbreak images outperform text jailbreaks optimized with the same objective and require 3x lower compute budget to optimize 50\u00d7 more input tokens. Finally, we find that representation engineering defenses, like Circuit Breakers, trained only on text attacks can effectively transfer to adversarial image inputs.", "sections": [{"title": "1 Introduction", "content": "Adapter-based vision language models were an early attempt to augment large language models (LLMs) with image inputs (Liu et al., 2024). They use a pretrained image embedding model, like CLIP (Radford et al., 2021), and train adapters to map image embeddings directly into the embedding space of a pretrained LLM. However, separate input spaces can limit multimodal understanding and do not support native generation of images. In contrast, early-fusion multimodal models have been introduced as a more general approach that supports unlimited modalities as both input and output (Chameleon Team, 2024; Gemini Team, 2023; OpenAI, 2024). These models project all modalities into a shared tokenized space and are pretrained from scratch on multimodal inputs. In this work, we will refer to early-fusion multimodal models as multimodal fusion models.\nJust like LLMs, most vision language models are trained to behave safely and reject harmful requests (Bai et al., 2022). Carlini et al. (2024) demonstrated that bypassing safeguards in adapter-based vision language models is easy because input images can be continuously optimized to maximize harmful outputs. This is in contrast to text input optimization, which requires less efficient discrete optimization methods (Zou et al., 2023). Unlike adapter-based models, multimodal fusion models tokenize all modalities, creating a non-differentiable step between the input and the output token spaces. As a result, optimizing any modality becomes again a discrete optimization problem.\nBuilding upon prior work on adversarial examples against quantized image classifiers (Athalye et al., 2018), we introduce the notion of a tokenizer shortcut that approximates image tokenization with a differentiable function. Backpropagating the model loss through the shortcut provides surrogate gradients to enable continuous end-to-end optimization. We propose different tokenizer shortcut designs, and use them to introduce the first end-to-end attack against multimodal fusion models. We evaluate it on Chameleon models (Chameleon Team, 2024) under white-box access.\nWe optimize jailbreak images and evaluate their success in eliciting harmful responses for prompts in JailbreakBench (Chao et al., 2024). We find successful jailbreak images for more than 70% of the prompts."}, {"title": "2 Preliminaries", "content": "Multimodal models. Adapter-based vision language models combine pretrained architectures by training adapters to map image embeddings into the embedding space of a language model (Liu et al., 2024). On the other hand, multimodal fusion models which we focus on are trained from scratch on multimodal inputs. For this, all modalities are mapped into a shared tokenized space that can serve as both input and output for autoregressive architectures. In this work, we evaluate the family of Chameleon models (Chameleon Team, 2024), the only open-source early-fusion multimodal models at the time of writing. To convert images into tokens, Chameleon trains a separate VQ-VAE (Gafni et al., 2024) that encodes the image into 1024 vectors that are then quantized into discrete tokens.\nJailbreaking language models. Researchers continuously find jailbreaks to bypass safeguards and extract unsafe information from language models (Wei et al., 2024). On one hand, some attacks use specific prompting strategies (Liu et al., 2023b; Shah et al., 2023; Liu et al., 2023a; Wei et al., 2024) that do not require access to model weights. On the other hand, Zou et al. (2023) proposed GCG, a discrete optimization algorithm that"}, {"title": "3 Our Attack: Tokenizer Shortcut for Continuous Optimization", "content": "The goal of our attack is finding adversarial images that, when appended to harmful prompts, can elicit a harmful response from the model. For this, inspired by the GCG objective (Zou et al., 2023), we optimize the image using gradient descent to maximize the probability of model generations to start with a non-refusal prefix (e.g. \"Sure, I can help you with that\"). We can additionally enhance the loss to simultaneously minimize the probability of generic refusal tokens. More formally, we optimize the image to minimize the following loss\n$L(image) = \\mathbb{E}_{prompts}[ I \\cdot p(refusal\\ prefix\\_i | i) - p(non\\_refusal\\ prefix\\_i | i)].$\nto be minimized to be maximized\nAt each step t of the optimization, we update our image to minimize the loss using gradient descent\u00b9:\n$image_{t+1} = image_t - \\alpha sign(\\delta L/\\delta image).$\nHowever, image tokenization in multimodal fusion models relies on quantization which is non-differentiable, and it is thus not possible to compute $\\delta L/\\delta image$ naively. Previous work on image classification showed that even rough approximations of quantization can provide gradients to guide adversarial optimization (Athalye et al., 2018). We introduce the notion of a tokenizer shortcut that maps the image embeddings before quantization to a continuous model input space, creating a fully differentiable path between the image and the output space. See Figure 2 for an illustration. These shortcuts enable end-to-end continuous optimization of images with respect to the model prediction loss.\nTokenizer shortcut. We use a 2-layer fully connected network as our tokenizer shortcut to approximate tokenization with a differentiable function. We propose two shortcuts that take as input each of the 1024 latent vectors from the VQ-VAE (R1024\u00d7256). The first shortcut maps the VQ-VAE embeddings directly to the LLM embedding space (R1024\u00d74096); we will refer to this shortcut as the embedding shortcut. The second shortcut maps the VQ-VAE embeddings to the one-hot encoding over the vocabulary tokens, producing a soft one-hot encoding over tokens (R1024\u00d716384) that is then used as input to the model; we will refer to this shortcut as the 1-hot shortuct. Figure 2 depicts our proposed shortcuts.\nSince the original Chameleon pretraining dataset is proprietary, we train both shortcuts on a subset of the open-source MIMIC-IT dataset (Li et al., 2023). The embedding shortcut is trained to minimize the cosine similarity between the shortcut predictions and the embedding that would be obtained for the original image tokens. The 1-hot shortcut produces a distribution over the vocabulary and minimizes the cross-entropy loss with the token that would be assigned to each vector through quantization. After training the shortcuts, we use them to compute end-to-end gradients and update the image pixels according to Equation 2."}, {"title": "4 Experimental Setup", "content": "Datasets. We use JailbreakBench (Chao et al., 2024) to evaluate our attacks. It provides a curated collection of harmful prompts from different sources. We evaluate our attacks on 80 prompts and keep a held-out set of 20 prompts for transferability experiments. While there can be reasonable debate over what is considered violating and harmful in AI generations, JailbreakBench offers a framework for comparing different methods using common prompts and metrics.\nOptimizing adversarial images. We optimize our jailbreak images using gradient descent (see Equation 2) for 500 steps. We initialize $\\alpha$ = 0.01 and divide it by 2 every 100 steps until reaching a minimum of 0.001. We keep the image with the lowest loss on the training images. We start from a random image from the MIMIC-IT dataset (Li et al., 2023), illustrated in Figure 1.\nBaseline attacks. We compare image jailbreaks against text-only and representation engineering attacks. More specifically, we use GCG (Zou et al., 2023) -optimizes text prompts to maximize probability of non-refusal and Refusal Direction (Arditi et al., 2024)-identifies a refusal direction in the model's activation space and subtracts it during the forward pass. For GCG, we run all our attacks for 200 steps with a suffix of 20 tokens, 256 replacement candidates per token and 512 total candidate suffixes per step."}, {"title": "5 Direct Attacks", "content": "In this set of experiments, we assume an attacker that optimizes a jailbreak image for each of the test prompts to maximize effectiveness. First, in Section 5.1, we report the most relevant ablations to find our best attack. Then, in Section 5.2, we compare the performance of our attack with text-only optimization. Finally, in Section 5.3, we measure the effectiveness of our jailbreak images against popular white-box protections."}, {"title": "5.1 Finding the Best Attack", "content": "Comparing shortcuts. We compare the performance of the embedding and 1-hot shortcuts in Table 1. Both shortcuts perform similary and find successful jailbreak images for over 70% prompts. However, their performance drastically changes if the shortcut is turned-off and the image follows the default tokenization forward path (e.g. if an adversary creates the images in a white-box setup and wants to use them against the same model deployed under black-box access). In this case, images optimized with an embedding shortcut are no longer successful but images from the 1-hot shortcut retain an attack success rate close to 50%.\nTemperature in shortcut predictions. The mapping to the one-hot encoding space can be easily learned by the shortcut network and it produces very confident predictions (i.e. probability 1 on the correct token and 0 elsewhere). This skewed distribution causes vanishing gradients for all other tokens. We thus find that applying high softmax temperature of 6 to the predictions of the shortcut is essential to be able to optimize the attack. Appendix A shows different ablations of the softmax temperature and their attack success."}, {"title": "5.2 Comparison with Text Optimization", "content": "We compare the performance and efficiency of optimization over text and image tokens. We use GCG as the best-known method to update text tokens with gradient information. For GCG, we optimize an adversarial prefix -instead of an image for each test prompt. Both successful image jailbreaks and GCG suffixes require around 100 optimization steps, but their computational complexities differ. For each step, jailbreak images require 1 forward and 1 backward pass on 1024 image tokens; totaling ~ 100,000 forward and backward token operations per successful jailbreak image. In contrast, GCG uses 20 additional tokens with 1 backward pass and 512 forward passes per iteration, resulting in ~ 20,000 backward and ~ 1M forward token operations. Assuming constant FLOPs per token operation and that backward operations require 3 times more FLOPs than forward operations, we estimate that jailbreak images require an additional 400,000 FLOPs/token, while GCG needs an extra 1,060,000 FLOPs/token to obtain a successful attack4."}, {"title": "5.3 Effectiveness Against White-Box Protections", "content": "First, we create jailbreak images and text suffixes with GCG on models enhanced with Circuit Breakers (Zou et al., 2024) (see Table 3). Circuit Breakers protections optimized over text inputs can generalize against images. Embedding shortcuts provide a more flexible representation to circumvent the protections. Moreover, unlike text attacks, jailbreak images do not increase the average token perplexity in the prompt, making them"}, {"title": "6 Transfer Attacks", "content": "We now evaluate the transferability of jailbreak images to unseen prompts and models. Instead of optimizing a separate image for each test prompt, we create a universal jailbreak image optimized over N held-out train prompts, and evaluate the attack success rate on the unseen test prompts.\nWe use 2 text-only baselines to contextualize our findings. We again use GCG, but, instead of optimizing for a single propmt, we optimize text suffixes over the same set of train prompts to increase transferability. Additionally, we use refusal suppression (Arditi et al., 2024), a representation engineering method that requires a set of train prompts to detect a direction in the model's activation space that is responsible for refusal5. This direction is subtracted at inference time, disabling the model's ability to refuse harmful requests. We summarize the main findings from the results in Table 5.\nIncreasing the number of train prompts improves generalization. Jailbreak images optimized on a single image exhibit good generalization to unseen prompts (~30%) but computing the loss over a larger number of prompts improves generalization for both 1-hot and embedding shortcuts. However, after a certain point, increasing the number of prompts does not result in improved attack success rates. For both shortcuts, we find very similar success for 10 and 20 training prompts.\nEmbedding shortcut can find more transferable jailbreak images. Although generalization for images optimized on a single prompt is similar for 1-hot and embedding shortcuts, the embedding shortcut can obtain much higher attack success rates on unseen prompts when optimized over more than 10 prompts (53.8% vs. 72.5%). In fact, the performance obtained with the jailbreak image optimized with embedding shortcut on 10 prompts can jailbreak as many prompts as optimizing a single image per test prompt (see Section 5), reducing computational cost significantly."}, {"title": "7 Discussion and Future Work", "content": "Image jailbreaks are promising but have a long way to go. Our work is the first attempt to jailbreak multimodal architectures using end-to-end gradient attacks. We present promising results that suggest that optimization might be smoother and more efficient than on text tokens. However, we think there is still significant room for improvement upon our methods. While we ablated most of the relevant hyperparameters we identified, our results indicate that attack success can still vary drastically with their choice. Future work may explore the optimization dynamics in more detail to come up with more effective ways of using the gradients. Similarly, future work may explore more flexible target functions (Thompson & Sklar, 2024); e.g. combining the output and activation space.\nTransferability of jailbreak images remains an open problem. Concurrent work on adapter-based models (Schaeffer et al., 2024) demonstrated that jailbreak images do not generalize across models even when optimizing on an ensemble of models. Our results indicate that this problem also persists in multimodal fusion models. Future work may focus on looking for ways to regularize the optimization to improve transferability."}, {"title": "8 Related Work", "content": "Multimodal Models. Adapter-based vision language models (VLMs) are a popular approach for integrating image understanding into language models (Liu et al., 2024; Karamcheti et al., 2024). They typically combine pre-trained language models with image embedding models. VLMs train an adapter to transform pre-trained image embeddings (e.g., from CLIP) into a representation compatible with language models. However, state-of-the-art proprietary models are shifting towards early-fusion multimodal models (Gemini Team, 2023; OpenAI, 2024). These models autoregressively process tokens representing multiple modalities in a joint tokenized space. Our work focuses on Chameleon models (Chameleon Team, 2024), currently the only open-source family of early-fusion multimodal models. Chameleon models support image inputs, are safety-tuned, and are available in 7B and 30B parameter sizes.\nAdversarial Examples. Adversarial examples are inputs designed to fool machine learning models, first explored in the context of image classification (Szegedy, 2013; Madry, 2017). Adversarial images examples are created by adding perturbations to valid inputs. These perturbations are optimized using the gradient of the target loss with respect to the input.\nJailbreaks. Jailbreaks are adversarial inputs designed to bypass the safeguards implemented in large language models (LLMs) and get them to generate harmful content. Jailbreaks are often black-box-they do not require access to model weights\u2014 and involve specific conversational strategies (Liu et al., 2023b; Shah et al., 2023; Liu et al., 2023a; Wei et al., 2024). On the other hand, there are white-box jailbreaks that use model weights and gradients to guide the attack. However, unlike in image classification, text inputs cannot be directly optimized using gradients with respect to the target loss because the input space is discrete and sparse and require approximate methods. GCG is the most prominent white-box attack (Zou et al., 2023). GCG optimizes a suffix that can be appended after harmful instructions to prevent refusal. Interestingly, GCG suffixes optimized on open-source models transfer to black-box and proprietary models.\nJailbreaks against vision language models. Carlini et al. (2024) demonstrated that incorporating image inputs can enable more effective and efficient jailbreaking attacks. Concurrent to our work, Schaeffer et al. (2024) explored whether optimizing jailbreak images on adapter-based vision language models exhibit the same transferability properties as GCG prompts. Their results indicate that jailbreak images are effective against adapter-based vision language models under white-box access but they do not generalize across models."}, {"title": "Impact Statement", "content": "Our research contributes to the safety and responsible development of future AI systems by exposing limitations in current models. While acknowledging the potential for misuse in adversarial research, we believe our methods do not introduce any new risks or unlock dangerous capabilities beyond those already accessible through existing attacks or open-source models without safety measures. Finally, we believe that identifying vulnerabilities is essential for addressing them. By conducting controlled research to uncover these issues now, we proactively mitigate risks that could otherwise emerge during real-world deployments."}]}