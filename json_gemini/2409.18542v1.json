{"title": "MIMII-Gen: Generative Modeling Approach for Simulated Evaluation of Anomalous Sound Detection System", "authors": ["Harsh Purohit", "Tomoya Nishida", "Kota Dohi", "Takashi Endo", "Yohei Kawaguchi"], "abstract": "Insufficient recordings and the scarcity of anomalies present significant challenges in developing and validating robust anomaly detection systems for machine sounds. To address these limitations, we propose a novel approach for generating diverse anomalies in machine sound using a latent diffusion-based model that integrates an encoder-decoder framework. Our method utilizes the Flan-T5 model to encode captions derived from audio file metadata, enabling conditional generation through a carefully designed U-Net architecture. This approach aids our model in generating audio signals within the EnCodec latent space, ensuring high contextual relevance and quality. We objectively evaluated the quality of our generated sounds using the Fr\u00e9chet Audio Distance (FAD) score and other metrics, demonstrating that our approach surpasses existing models in generating reliable machine audio that closely resembles actual abnormal conditions. The evaluation of the anomaly detection system using our generated data revealed a strong correlation, with the area under the curve (AUC) score differing by 4.8% from the original, validating the effectiveness of our generated data. These results demonstrate the potential of our approach to enhance the evaluation and robustness of anomaly detection systems across varied and previously unseen conditions.", "sections": [{"title": "I. INTRODUCTION", "content": "Anomaly detection in machine operations is critical, as even minor deviations in machine sounds can indicate significant faults in industrial applications [1]. These systems must effectively identify irregularities across various machines, operational changes, and recording environments. However, evaluating anomaly detection models poses a significant challenge due to the scarcity of recorded data, particularly abnormal samples, which are often difficult to obtain in real-world settings. This limitation hinders the development of robust and accurate detection systems.\nTo address this, we propose an approach that leverages audio generation to create abnormal samples, enabling a simulated evaluation of anomaly detection systems. Unlike current audio generation methods such as Tango, AudioLDM, MusicLM [2]-[7] have shown remarkable success in domains like speech and music, generating high-quality machine audio remains an under-explored area. Existing techniques fail to accurately replicate the fine-grained audio differences and complexities of machine sounds due to different operational environments. Addressing this gap, our research named as MIMII-Gen seeks to advance machine sound generation similar to recorded MIMII-DG data [8], enabling practical applications, particularly in evaluating anomaly detection systems.\nWe develop and train a latent diffusion model that integrates an encoder-decoder approach of EnCodec [9] for converting between audio clips and their latent representations. To list our contributions, (i) We enhance the descriptive quality of weak metadata associated with sound clips by converting them into rich, human-like captions. (ii) We have carefully designed the U-Net for improved quality while guiding"}, {"title": "II. RELATED WORK", "content": "In this section, we review the recent advancements in audio generation using diffusion-based models and unsupervised anomaly detection in machine sounds.\n\nA. Text-to-Audio Generation Using Diffusion Models\nText-to-audio (TTA) generation has garnered considerable attention, with approaches like AudioGen [11] focusing on learning audio representations by leveraging paired audio-text data to overcome the challenges of data scarcity and quality variability. AudioGen employs a Transformer-based decoder to generate discrete tokens that represent compressed waveform features. By implementing data augmentation techniques, such as mixing audio samples and distilling language descriptions into simplified labels, AudioGen increases the diversity of training data. However, this comes at the cost of losing intricate spatial and temporal relationships in the text descriptions, which can impact the fidelity and contextual richness of the generated audio. On the other hand, diffusion models have become a dominant framework for generative tasks, including text-to-audio conversion. Diffsound [7], uses a non-autoregressive decoder based on discrete diffusion model to generate audio using text by refining mel-spectrogram tokens through iterative steps rather than sequential predictions typical of autoregressive decoders.\nGeneration approaches like AudioLDM [12], Make-An-Audio [13], and Tango [6] typically employ pre-trained text encoders (e.g., CLAP [14], T5) and VAEs to extract text embeddings and latent audio features. Using a latent diffusion model (LDM) architecture, these systems generate audio latent features conditioned on text inputs, which are subsequently transformed into mel-spectrograms and waveforms using VAEs and neural vocoders.\nDespite these advances, current TTA models often fall short when applied to machine sound generation, where the complexity of acoustic environments and subtle variations in sound are critical. The existing methods primarily focus on speech and music, with limited exploration of industrial machine sounds. This gap underscores"}, {"title": "B. Unsupervised Anomaly Detection in Machine Sounds", "content": "Anomalous sound detection (ASD) [15]\u2013[19] aims to identify deviations from normal operational sounds in machines, a task complicated by the rarity and variability of anomalous events. Traditional ASD approaches often rely on labeled anomalous data, which is scarce in real-world applications, limiting their ability to generalize to new or unseen conditions. Consequently, unsupervised ASD, which trains only on normal sounds, has emerged as a viable yet challenging alternative.\nThe DCASE-2023 Challenge Task-2 introduced a first-shot (FS) approach to unsupervised ASD, targeting the detection of anomalies in machine types not seen during training [20]. However, unsupervised ASD methods struggle to adapt to first-shot scenarios due to a lack of diverse training data encompassing unseen machine types and operational conditions. Limited anomalous data for evaluation also hinders adaptability and reliability in real-world scenarios where anomalies vary widely.\nRecent work by Zhang et al. [21] uses audio generation of anomalous sounds for training and enhancing anomaly detection systems, whereas our approach focuses on generating anomalies to evaluate the robustness and effectiveness of existing anomaly detection system. Our approach combines generative modeling with EnCodec and Flan-T5 embeddings to produce machine audio that captures subtle variations crucial for anomaly detection. Tailored for industrial acoustic environments, it generates diverse audio data for anomaly detection evaluation when real-world data is scarce."}, {"title": "III. PROPOSED APPROACH", "content": "Our approach employs a condition-based latent diffusion model to generate machine sounds under various operational and environmental conditions. The crucial parts of the generation model as well as diffusion process are explained below.\n\nA. Overview of Diffusion Models\nDiffusion models are probabilistic generative models [22] that learn the data distribution p(x) by progressively denoising a normally"}, {"title": "distributed variable", "content": "The diffusion process involves learning the reverse of a fixed Markov chain of length T, where the forward process adds noise step-by-step, and the reverse process denoises iteratively. This approach can be conceptualized as a sequence of denoising autoencoders, denoted as $\\epsilon_{\\theta} (x_t, t)$ is a noisy version of the input x at time step t.\nThe objective function for diffusion models is expressed as:\n\n$L_{DM} = E_{x,\\epsilon~N(0,1),t} [||\\epsilon - \\epsilon_{\\theta} (x_t, t)||^2]$,\n\nwhere t is uniformly sampled from {1, ..., T}. This formulation mirrors denoising score matching, enabling the model to predict clean data from noisy observations effectively. Instead of high-dimensional data space, diffusion models can operate in latent space using low-dimensional representations from an encoder, thus reducing computations and focusing on semantically relevant aspects of the data.\nThe training objective within the latent diffusion framework is defined as:\n\n$L_{LDM} = E_{\\varepsilon(x),\\epsilon~N(0,1),t} [||\\epsilon - \\epsilon_{\\theta} (z_t, t)||^2]$,\n\nwhere zt represents the noisy latent variable derived from the encoder, and the reverse process is modeled through a time-conditional U-Net backbone as shown in Fig. 1.\nDiffusion models can also model conditional distributions of the form p(zy) by projecting the conditioning variable y to representations $T_{\\theta}(y)$ through encoder $T_{\\theta}$. The loss function for the conditional latent diffusion model is expressed as follows:\n\n$L_{LDM} = E_{\\varepsilon(x),y,\\epsilon~N(0,1),t} [||\\epsilon - \\epsilon_{\\theta} (z_t, t, T_{\\theta}(y)) ||^2]$,\n\nwhere both the latent encoder $e_{\\theta}$ and the conditioning encoder $T_{\\theta}$ can be jointly optimized according to the loss function.\n\nB. Model architecture\nAs shown in Fig.1, we generate captions from the metadata of audio files, which describe operational settings, environmental conditions, anomaly types, and machine models. These captions"}, {"title": "are then encoded into dense vector representations that serve as condition vectors for the diffusion model to generate realistic machine sounds across combinations of different conditions", "content": "EnCodec is used to obtain latent space representations of audio signals, capturing essential features in a compressed form. The diffusion model is trained using a Denoising Diffusion Probabilistic Model (DDPM) framework utilizing a wide-channel denoising U-Net where the condition embeddings are combined with audio representations.\n\n1) Text condition encoding: We utilize the Flan-T5 model [10] as a text encoder to obtain condition embeddings from generated captions. These encoded caption embeddings from 768-dimensional projection layer of the text encoder are then used as conditions for the U-Net model.\n\n2) Generation of latent space: Many audio generation approaches, such as AudioLDM and Tango, rely on a combination of VAE and vocoder architectures. These methods apply diffusion to spectrogram representations and require additional training of a VAE [5] and a separate vocoder (e.g., HiFi-GAN [23]) to reconstruct waveforms from generated spectrograms. In contrast, we use EnCodec, an off-the-shelf VQ-GAN model, known for its effective performance in audio generation similar to AudioJourney [24]. The EnCodec model comprises an encoder, a vector quantizer, and a decoder, enabling efficient audio generation thus reducing the overall model size and complexity.\nIn Encodec, given a continuous latent represention that comes out of the encoder, residual vector quantization (RVQ) is applied to convert it into discrete set of indexes of codebooks. The dimension N of latent representation thus becomes Ng which is equal to the number of codebooks selected. Variable bandwidth training is performed in Encodec where the authors select randomly a number of codebooks as a multiple of 4, i.e. corresponding to a bandwidth 1.5, 3, 6, 12 or 24 kbps at 24 kHz. We experimented with these different bandwidth values and selected 24 kbps for our use case based on generation quality. This discrete representation can changed again to a vector by summing the corresponding codebook entries, which is done just before going into the decoder by the dequantization block shown as De-Q in Fig.1. We use this continuous latent vector as input for diffusion process during training. During inference, the sampled latent vector is then directly given as input to the decoder for generating the audio clip.\n\n3) U-Net design and noise scheduling: In order to effectively utilize the latent space from the encoder of Encodec model, we use a wide channel U-Net in the diffusion process, that is designed for a 16-channel input rather than the typical one or three channels seen in many audio generation methods. To counter the minimal variance within the 16-dimensional latent encodings from encoder, we reshape the latent vectors from a single-channel 128 \u00d7 750 format to a 16-channel 8 \u00d7 750 format. With this new structure, the convolutional blocks encompass the entire latent representation within their receptive field without losing detail, resulting in higher fidelity audio generation. This reshaping allowed each channel to be separately normalized to zero mean and unit variance, aiding the U-Net in learning the noise distribution, N(0, I). The approach of reshaping the encoder output is inspired from Audiojourney although the resulting dimension due to reshaping, number of channels and U-Net design are different due to our specific data characteristics. Post-generation, these transformations are fully reversible, enabling decoding back into waveforms using decoder of Encodec model. Utilizing cross-attention rather than embedding addition in the U-Net architecture allows original audio embeddings to remain unchanged throughout each layer, thereby retaining their integrity and enhancing conditional guidance.\nThe generated samples are used to evaluate generation quality and as input into an auto-encoder based anomaly detection system [20]."}, {"title": "IV. EXPERIMENTS", "content": "A. Dataset\nThe data used for this research is originally from MIMII-DG dataset [8] that has sounds of various machines recorded in different operating conditions. This dataset is then used for anomaly detection Task-2 in DCASE [20] which is composed of three parts: the development dataset, an additional training dataset, and an evaluation dataset. The development dataset includes seven machine types (fan, gearbox, bearing, slide rail, ToyCar, ToyTrain, valve), with each type featuring one section that comprises complete training and test data. Various attributes related to operational and environmental conditions as well as model types of the machines are available as metadata. We used recordings of the audio sounds of five machines (fan, gearbox, bearing, slide rail, valve) from MIMII-DG corresponding to this development dataset part for training and evaluating our generation model. Each audio recording is a single-channel file lasting about 10 seconds with a sampling rate of 16 kHz.\n\nB. Experimental setup\n1) Audio generation: In order to train the diffusion model, total of 35,146 training samples from all machines are used and 8,787 samples are used for evaluation of the generation quality. \nThe metadata associated with all the recorded audio clips are given as input to the T5-large [25] model to obtain descriptive captions that are saved in order to be used later for audio generation. These captions are then encoded into 768-d embedding vectors using another model Flan-T5 to give as input condition during training and inference from diffusion model. During training, the parameters of Encodec and the Flan-T5 model are frozen, only the denoising U-Net is trained.\n2) Anomaly detection: In order to train the anomaly detection system, we used 990 normal audio clips from each machine type for training. Two evaluation datasets are created, one set contains the originally recorded 50 normal and 50 anomalous clips, while other set consists of 50 original normal and 50 generated anomalous clips for all 5 machine types. The anomaly detection system is same as the auto-encoder trained as baseline for task-2 of DCASE [20]"}, {"title": "V. RESULTS AND DISCUSSION", "content": "We employ four objective metrics to evaluate our generated audio: Frechet Audio Distance (FAD) [26], [27], calculated using"}, {"title": "sounds", "content": "Spectrogram provides a visual representation of the audio data highlighting the unique patterns associated with each machine type's . We could also successfully generate the audio samples for combinations of different conditions which were not seen during training of the diffusion model.\nThe anomaly detection system is then evaluated on this generated anomalous data. The AUC scores for generated data have an average difference of 4.8% and are correlated to that of the original anomalous data for all machines."}, {"title": "TABLE IV: AUC scores for all machine types", "content": "VI. CONCLUSION\nWe presented a method for generating high-quality machine audio with fine-grained variations using metadata, demonstrating strong alignment with real recordings. Our approach effectively generates anomalous data across various conditions, enhancing downstream task performance. Leveraging an EnCodec-based approach over VAE and vocoder methods, we achieve superior performance with a simplified pipeline. The anomaly detection system tested on our generated audio shows only a 4.8% AUC deviation from its performance on original data, underscoring our method's practical value in industrial scenarios."}]}