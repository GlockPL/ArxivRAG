{"title": "URECA: The Chain of Two Minimum Set Cover Problems exists behind Adaptation to Shifts in Semantic Code Search", "authors": ["Seok-Ung Choi", "Joonghyuk Hahn", "Yo-Sub Han"], "abstract": "Adaptation is to make model learn the patterns shifted from the training distribution. In general, this adaptation is formulated as the minimum entropy problem. However, the minimum entropy problem has inherent limitation-shifted initialization cascade phenomenon. We extend the relationship between the minimum entropy problem and the minimum set cover problem via Lebesgue integral. This extension reveals that internal mechanism of the minimum entropy problem ignores the relationship between disentangled representations, which leads to shifted initialization cascade. From the analysis, we introduce a new clustering algorithm, Union-find based Recursive Clustering Algorithm (URECA). URECA is an efficient clustering algorithm for the leverage of the relationships between disentangled representations. The update rule of URECA depends on Thresholdly-Updatable Stationary Assumption to dynamics as a released version of Stationary Assumption. This assumption helps URECA to transport disentangled representations with no errors based on the relationships between disentangled representations. URECA also utilize simulation trick to efficiently cluster disentangled representations. The wide range of evaluations show that URECA achieves consistent performance gains for the few-shot adaptation to diverse types of shifts along with advancement to State-of-The-Art performance in COSQA in the scenario of query shift.", "sections": [{"title": "1 Introduction", "content": "Code search is a task to retrieve code snippets from the given query whose intent is to find implementations of specific functionality. This task is important for both human developers' productivity and LLM's hallucination with RAG (Brown et al. (2020); Lewis et al. (2020); Huang et al. (2025)). Although the scale-up of programming language models boosts performance of this task, the models still are vulnerable to diverse types of shifts (Arakelyan et al. (2023)), which comes from the evolutions that human developers continuously write new codes for debug or performance improvements. Adaptation to these evolutions relies on generalization, a key mechanism of human intelligence that also drives the application of neural networks across wide range of fields.\nWe can utilize supervised signals as catalysts for the generalization. In spite of the effectiveness, only insufficient supervised signals are available as guidances to generalization due to severity of shifts. Therefore, it is necessary to analyze adaptation itself for effective utilization of these supervised signals. Adaptation to shifts is formulated as minimum entropy problem in code search. However, minimum entropy problem cannot help shifted initialization lead to bad solution (shifted initialization cascade phenomenon). (Press et al. (2024)) demonstrates that this phenomenon occurs in entropy minimization (Grandvalet & Bengio (2004)) which is the unsupervised instantiation of minimum entropy problem."}, {"title": "2 Problem Analysis", "content": "At first, we analyze the minimum entropy problem in the lens of Lebesgue integral (Theorem 2.1). After then, we show that model learns to cluster the elements by minimum entropy problem (Theorem 2.2). These theorems drive us to notice that the ignorance of disentangled relationships leads to shifted initialization cascade.\nTheorem 2.1. Minimum entropy problem is dual to the combination of chains of two minimum set cover problems with greedy algorithm for disentangled representations.\n$\\begin{aligned}\n\\min _{i} \\frac{1}{p(\\mathbf{E} \\alpha)} &=\\int \\frac{1}{p(\\mathbf{E} \\alpha)} d p(\\mathbf{E} \\alpha) \\\\\n&=\\sup {\\sum{n=1}^{[p(\\mathbf{E} {\\alpha})]} p(\\mathbf{E} {\\alpha})} \\\\\n&=\\sup {\\min \\gamma(\\mathbf{c}) d x(\\mathbf{c} \\in \\gamma)}\n\\end{aligned}$  (s.t. $p(\\mathbf{E} {\\alpha}) \\leq p(\\mathbf{u})$)\nAs we mentioned, we examine minimum entropy problem in the lens of Lebesgue integral. The proof of Theorem 2.1 is provided in Appendix D.1. u is the subset of U which is the universe of disentangled representations e for the predecessor minimum set cover problem. The predecessor minimum set cover problem is the first problem of the chain of two minimum set cover problems (We call the last problem of the chain as successor minimum set cover problem). $\\gamma(e)$ is the characteristic function which describes whether e is the element of u (Appendix B). $s(e)$ is the simple function which measures the size of e.\nThe analysis starts from equation (2.1) which describes the self-information in terms of Lebesgue integral to probability. We can rewrite the integral form of equation (2.1) to supremum form of equation (2.2) based on the definition of Lebesgue integral for non-negative measure (Appendix B). With properties of probability as Lebesgue measure, this supremum form hints that self information"}, {"title": "3 Union-find based Recursive Evidence Clustering Algorithm (URECA)", "content": "URECA is a new algorithm to cluster disentangled representations based on the relationships between disentangled representations. URECA implements the iterative steps of initialization, update and recursion in other clustering algorithms (k-means etc.) with the combination of calculations for numeric values based on simulation trick. Simulation trick is the trick to replace specific task to simple numeric computations. For example, a simulation implements moving a box only by changing"}, {"title": "3.1 Initializatoin Step", "content": "At first, URECA calculates weight of evidence for initial cluster (logits) like equation (4). $(q_i, C_i)$ is the given pair of (query, code) and f is neural network.\n$\\operatorname{evid}\\left(C_{i, j}^{0}\\right)=\\ln \\frac{P\\left(q_{i}, c_{j}\\right)}{p(y)}=f\\left(q_{i}\\right) \\cdot f\\left(c_{j}\\right)$\nIn the context of code search, the estimation of weight of clusters leads to Fragment Initialization and Cluster Initialization. We can justify this statement since the estimation of logits leads to estimation of conditional probability and entropy, which respectively correspond to Fragment Initialization and Cluster Initialization.\nFragment Initialization: We can apply the analysis of Theorem 2.1 and Theorem 2.2 to conditional probability as output of neural network in the scenario of training neural network with minimum entropy problem. Neural network calculates conditional probability based on logits and softmax. Conditioning random variable in this conditional probability prerequisites the inverse image as subset of universe of fragments. The conditional probability configures this subset of universe of fragments as universe of the predecessor minimum set cover problem. In other words, forward propagation and dot product adopt the only query-relevant fragments in code search and do fragment"}, {"title": "3.2 Update Step", "content": "Update step consists of dynamics estimation and transport computation. URECA esitmates dynamics of disentangled representations with attention scores of queries. After then, URECA estimtates the transported parts of logits through linear combination of logits with these dynamics. Then, it subtracts the parts of logits from logits of dropped cluster and adds the value to logits of survived cluster. The subtraction and addition simulate to transport evidences/fragments from dropped cluster to survived cluster.\nSimulation trick enables URECA to simulate the transport of fragments with numeric calculations. The following properties of Lebesgue measure enable simulation trick to operate with no errors (equation (3.2)).\n$\\mu(A \\cup B)=\\mu(A)+\\mu(B)-\\mu(A \\cap B) \\\\\n\\mu(A-B)=\\mu(A)-\\mu(A \\cap B)$\nThese properties naturally connects the numeric calculations for Lebesgue measure to transport elements between measurable set A and B. We split the transport into two sub-units, separation from $C_s$ (subtraction) and clump into $C_t$ (addition). To begin with, we can formalize the separation to separate semantic fragment $R_{s,t}$ from $C_s$ as follows.\n$\\mu\\left(C_{s}-R_{s, t}\\right)=\\mu\\left(C_{s}\\right)-\\mu\\left(C_{s} \\cap R_{s, t}\\right) \\\\\n=\\mu\\left(C_{s}\\right)-\\mu\\left(R_{s, t}\\right)$\nThen, we can also derive the following formula for addition to clump $R_{s,t}$ into $C_t$.\n$\\mu\\left(C_{t} \\cup R_{s, t}\\right)=\\mu\\left(C_{t}\\right)+\\mu\\left(R_{s, t}\\right)-\\mu\\left(C_{t} \\cap R_{s, t}\\right)$\nThe estimation of $\\mu(R_{s,t})$ enables to subtract the estimation from $\\mu(C_s)$ and add it to $\\mu(C_t)$, which simulates the transport subset of semantic fragments($R_{s,t}$) from $C_s$ to $C_t$ (In this case, $\\mu(C_t \\cap R_{s,t})$ should be zero by the assumption that specific fragment/evidence belongs to the only one cluster at specific time). In other words, we can accurately implement the update of general clustering algorithm with the sequence of operations for Lebesgue measure. Probability meets the basic properties in equation (3.2) since it is one of the Lebesgue measures. Therefore, we can formulate the transport as follows with Bayesian theorem.\n$\\operatorname{evid}\\left(C_{i, i}^{t+1}\\right)=E_{j \\sim p\\left(C_{i, i}^{t+1} \\mid C_{j, j}^{t}\\right)}\\left[\\frac{p\\left(C_{j, j}^{t}\\right)}{p(y)}\\right]$"}, {"title": "3.3 Recursion Step", "content": "As mentioned earlier, we use logits instead of probabilities which impede training of neural network due to excessive regularization. However, logits do not meet the properties of Lebesgue measure. This unsatisfaction causes errors in the process of transport-based estimation for next time step's logits. Probability-based transport decides next time step's logits as follows.\n$\\ln \\frac{P_{t+1}\\left(C_{i, i}^{t+1}\\right)}{p(y)}=\\ln E_{j \\sim p\\left(C_{i, i}^{t+1} \\mid C_{j, j}^{t}\\right)} \\frac{p_{t}\\left(C_{j, j}^{t}\\right)}{p(y)}$\nIn contrast, logit-based transport estimates next time step's logits as follows.\n$\\operatorname{evid}\\left(C_{i, i}^{t+1}\\right)=E_{j \\sim p\\left(C_{i, i}^{t+1} \\mid C_{j, j}^{t}\\right)} \\frac{p_{t}\\left(C_{j, j}^{t}\\right)}{p(y)}$"}, {"title": "4 Experiments", "content": "In this section, we focus on the results and analyses of our experiments for few shot adaptation to shifts in code search. To make the best use of the available space, we have included the details of the experimental setup such as the datasets, baselines, and metrics in Appendix F.1~F.4. Furthermore, Appendix F also provides a detailed explanation of the reasons behind the occurrence of specific types of shifts (e.g., query shift, code shift, and task shift) in each experimental setting."}, {"title": "4.1 Adaptation to Shifts", "content": "Table 1, 2 and Appendix F.5's Tables show that URECA generally exhibits performance gains over InfoNCE. This demonstrates the necessity of reflection on disentangled representations to address shifted initialization cascade across diverse types of shifts. Experiment section reports results which is average of three different seeds for CSN-Python and CoSQA, while results for the remaining programming languages in CSN-Ruby, CSN-Javascript, CSN-Java, CSN-Go and CSN-PHP are provided in Appendix F.5."}, {"title": "4.2 Shifted Initialization and Dynamics Estimation", "content": "Shifted Initialization: Figure 3 demonstrates that minimum entropy problem leads to bad solutions due to shifted initialization as mentioned in Section 2. Figure 3 represents the results of training CodeT5p-220M for 100 epochs with 120 few-shot examples of CSN-Go and 80 few-shot examples of CSN-PHP. For CSN-Go, InfoNCE shows extremely low performance for first 10 epochs (MRR: 0.1%) while URECA shows superior performance (MRR: 48.1%). Although the performance gaps decrease in the process of training, it is still significant even after 100 epochs for 59.1 % (InfoNCE) and 70.3% (URECA). For CSN-PHP in Figure 3, InfoNCE still shows little performance improvements again even after 100 epochs of training. Although URECA also exhibits little improvements during the first 10 epochs, the performance of URECA spikes after then. These"}, {"title": "5 Conclusion", "content": "In this paper, we analyze adaptation as minimum entropy problem. We demonstrate that the problem decomposes into the combination of minimum set cover problems, which reveals minimum entropy problem ignores the relationships between disentangled representations. In this context, we introduce URECA, a clustering algorithm to leverage relationships between disentangled representations. Then, we design new auxiliary loss based on the result of URECA to lead the model to reflect on the relationships between disentangled representations. Our experiments show that URECA achieves SOTA for CoSQA and consistent performance gains across programming languages of CSN. These results necessitate to leverage relationships of disentangled representation for robust adaptation, paving the way for code retrieval and large-scale RAG systems."}, {"title": "6 Impact Statement", "content": "This work theoretically analyzes adaptation as minimum entropy problem in terms of Lebesgue integral. Minimum entropy problem is one of the most widely used optimization problems across diverse fields like statistics, information theory and machine learning). This analysis has great potentials to improve data analyses, network communication and machine due to the general use of minimum entropy problem However, it also brings about the ethical considerations for the privacy about neural network since we discover the internal mechanism of minimum entropy problem and connects it to deep learning context.\nIn addition, our proposed method, URECA leverages the relationships between disentangled representations with the clusters estimated from the clustering process of URECA which simulates clustering disentangled representations based on simulation trick with transported logits. This clustering algorithm introduces novel insights for efficient clustering since it simulates clustering with numeric calculations. Although the efficiency makes rapid adaptation to distribution shift, it also accelerates the development of attack from malicious users.\nAlthough our work pushes the boundaries of machine learning, the distinctions also introduce new challenges,which people should contemplate on for the proper utilization of advancements."}, {"title": "A Related Works", "content": null}, {"title": "A.1 Disentangled Representation Learning", "content": "Disentangled representation learning has emerged as a critical area of study within the broader field of representation learning. This can be categorized into dimension-wise and vector-wise approaches according to representation structure (Wang et al. (2022)). Dimension-wise methods assign a single scalar dimension to represent fine-grained generative factors, making them suitable for synthetic and simple datasets (Chen et al. (2016); Jeon et al. (2021); Lin et al. (2019); Xiao et al. (2017)). In contrast, vector-wise methods use multiple dimensions (vectors) to represent coarse-grained factors, making them more applicable to complex real-world tasks such as identity swapping, image classification and video understanding (Tran et al. (2017); Liu et al. (2021); Denton (2017); Wang et al. (2022)). In this paper, we focus on vector-wise approaches since flexible approaches are suitable to code search as one of the most realistic tasks.\n(Wiles et al. (2022); Lee et al. (2024)) explore the disentangled representations and (Lee et al. (2024)) highlights that minimum entropy problem ignores the influence of disentangled representations, which leads to unreliable prediction. In addition, (Wiles et al. (2022)) assists that it is important for model to understand disentangled representations for distribution shift. However, most methodologies for disentangled representation learning do not focus on the learning process itself which drives model to learn entanglement of fine-grained representations In contrast, this paper analyzes adaptation as minimum entropy problem in terms of Lebesgue integral and proposes a new algorithm for clustering disentangled representations to reflect on the internal structure of representation"}, {"title": "A.2 Semantic Code Search", "content": "Semantic Code Search (Gu et al. (2018); Cambronero et al. (2019)) is a task to retrieve the most relevant code snippets in response to given natural language query. This semantic code search is especially useful for hallucination of LLM with leverage for the result of retrieval (Niu et al. (2024); Chen et al. (2024)). It has significantly evolved in accordance with the advancements in deep learning. Some datasets such as CodeSearchNet (Husain et al. (2019)) and CoSQA (Huang et al. (2021)) are introduced and other foundational works did significant efforts to advance the frontiers to the next level (Shi et al. (2023); Li et al. (2023, 2024)). These datasets not only support the development of code-specific pretrained language model (Guo et al. (2021); Feng et al. (2020); Wang et al. (2021b); Guo et al. (2022); Wang et al. (2023)) but also enable the fine-tuning of these models, leading to notable improvements in code search tasks (Guo et al. (2022); Wang et al. (2023)). GraphCodeBERT (Feng et al. (2020)), UniXCoder (Guo et al. (2022)) and CodeT5+ (Wang et al. (2023)) stand out as these programming language models that are the ones of the most successful models in code search task.\nAs long as we know, there is no single paper that treats the distribution shift in code search except for docprompting (Zhou et al. (2023)) which only partially treats code search under the setting of distribution shift since the authors focus on the code generation. However, the retrieval module is enough deserved to be spotlighted, considering the sensitivity of LLMs to given information and the hallucination as the problem deep-seated into LLMs. Disentangled representation learning is well-suited to the solution of this distribution shift, especially in the domain of programming language in that PL is structured language and pattern in this domain is sensitive to the structure of disentangled fragments. This learning paradigm is out of sight of the researchers in code domain"}, {"title": "A.3 Minimum Entropy Problem", "content": "Entropy is originally introduced as optimal length of code by (Shannon (1948); Mao et al. (2023)). This entropy is formally defined as the expectation of self-information across different events. The self-information can be interpreted as a measure for the amount of information since it is inversely proportional to the possibility of specific event. If that event is likely to happen, the occurrence of that event does not update much references for decision. In contrast, the occurrence of unlikely event updates many parts of codebooks. These correlations makes self-information proportional to the amount of updates, which is the reason why we use this measure for information.\nMinimum entropy problem is the problem to minimize the expectation of this amount of updates by exploring the space of parameters. This problem is widely used in the field of machine learning for classification with cross entropy. In addition, entropy minimization plays a crucial role in Test Time Adaptation that can improve the generalization to shifted distributions in unsupervised manner (Wang et al. (2021a)). (Press et al. (2024)) provides experimental supports for the hypothesis that entropy minimization is related to clustering. Unlike (Wang et al. (2021a)) and (Press et al. (2024)), we theoretically shed on the light into the internal mechanism of minimum entropy problem which can be used to train model regardless of the existence of supervised signals. This makes us realize the reason that shifted initialization results in the bad solution of clustering.\nThe minimum set cover problem is one of the most fundamental optimization problems, which is NP-complete. Given universe U, a collection T as a power set of U, and cost function c, this problem is defined to find a sub-collection of T whose union of all elements covers U whose cost is minimum. Greedy algorithm selects subsets whose price is minimum for each iteration until all elements of U are covered. It is well known that this algorithm becomes best approximation unless NP has polynomial time algorithm. This means that the minimum set cover as solution constructed by greedy algorithm has tight upper bound for the cost, $c(O P T)(\\ln n+\\gamma)$ when $\\gamma$ is Euler-Mascheroni constant and OPT is the optimal solution of the minimum set cover problem.\nAccording to (Halperin & Karp (2005); Cardinal et al. (2012)), the minimum set cover problem is dual form of the minimum entropy problem, which is called the minimum entropy set cover problem. However, existing researches only focus on the superficial parts of this problem. With the analysis of the minimum set cover problem in terms of Lebesgue integral, we extend the connection of the minimum set cover problem and minimum entropy problem into the unknowns. We unveil another minimum set cover problem behind the minimum entropy set cover problem."}, {"title": "B Lebesgue Integral", "content": "Lebesgue integral is extension of riemannian integral (Kwon & Yoon (2012)). Intuitively, lebesgue integral is integral across the partitions of image for the given function, while riemannian integral is across the partitions of domain. Lebesgue integral for the non-negative measurable function f is defined as follows, under the condition 0 \u2264 s(x) \u2264 f(x).\n$\\int f(x) d \\mu(x)=\\sup {\\int s(x) d \\mu(x)}$\nIn this formula, s is measurable simple function, which is linear combination of characteristic function $\\chi_{E_{k}}$, ak is the element in range and $E_k$ is pre-image of s. Measurable function is function of which all pre-images are in the domain of that function. Definition of simple function is as follows.\n$s(x)=\\sum_{k} a_{k} \\chi_{E_{k}}(x)$\nCharacteristic function $\\chi_{E_{k}}$ can be called as indicator function, which is defined as follows.\n$\\chi_{E_{k}}(x)=\\left\\{\\begin{array}{ll}1 & (x \\in E k) \\\\0 & (x \\notin E k)\\end{array}\\right.$"}, {"title": "C Pseudo Code of URECA", "content": "Algorithm 1 URECA\nInput: Q, D, GT\nOutput: NONE\n1: E = Q. D\n2: DYN = softmax(QQ)\n3: for k\u2190 1 to max_recursion_num do\n4: S = argsort(E)\n5: p = S.find_position(GT)\n6: F = E.split(p)\n7: F=F+FDY M\n8: E=F\n9: end for"}, {"title": "D Proofs", "content": null}, {"title": "D.1 Proof for Theorem 2.1", "content": "The outline of proof is as follows. At first, we prove the base case of Theorem 2.1 from the relationship of self-information and the minimum set cover problem which is described by Lebesgue integral. Base case means the probability of each element in universe is $\\frac{1}{|U_a|}$ and we extends the relationship to general case in terms of probability and the cardinality of universe. At last, we illuminate the relationship between minimum entropy problem and the minimum set cover problem from self-information's one.\nProof. For the first, we explain the basics of the minimum set cover problem (Halim & Gilbert (2016)). The minimum set cover problem is one of the most fundamental optimization problems, which is NP-complete. Given universe Ua, a collection T as a power set of $U_a$(a is the identifier for specific event), and cost function c, this problem is defined to find a sub-collection of T whose"}]}