{"title": "CAN MAMBA ALWAYS ENJOY THE \u201cFREE LUNCH\u201d?", "authors": ["Ruifeng Ren", "Zhicong Li", "Yong Liu"], "abstract": "Transformers have been the cornerstone of current Large Language Models (LLMs); however, its linear growth in overhead during inference with respect to sequence length poses challenges for modeling long sequences. In this context, Mamba has gradually attracted attention due to its constant-level size during inference and existing empirical results have shown that it can perform comparably to Transformers in sequence modeling while offering significant savings. However, one may ask that, can Mamba always enjoy the \"free lunch\"? In this paper, we focus on analyzing the expressive ability of Mamba from a theoretical standpoint. First, inspired by the connection between Mamba and linear attention, we investigate potential shortcomings of the Mamba when performing the COPY operation. Our results indicate that Mamba with constant size may encounter bottlenecks when handling COPY, while it can achieve perfect performance when the size scales linearly with sequence length. Based on this observation, we analyze Mamba's ability to tackle DP problems when equipped with Chain of Thought (CoT). Our findings suggest that to solve arbitrary DP problems, the total cost of Mamba is comparable to standard and efficient Transformers. However, similar to efficient Transformers, when facing DP problems with favorable properties such as locality, Mamba can provide savings in overhead. Our results contribute to a deeper understanding of Mamba.", "sections": [{"title": "1 INTRODUCTION", "content": "Reccently, Transformer-based large language models (LLMs) have become the mainstream of modern neural network architectures due to their outstanding performance across a wide range of tasks (Vaswani et al., 2017; Kenton & Toutanova, 2019; Brown et al., 2020; Dosovitskiy et al., 2020; Min et al., 2022). However, the core component of Transformers-the attention layer-while providing excellent performance, also leads to emerging drawbacks: during training, the computational cost scales quadratically with sequence length, and during inference, the cost scales linearly with sequence length. This limitation becomes increasingly unacceptable when dealing with long sequence tasks. To address this issue, many works have attempted to improve the attention mechanism to reduce its time and storage costs (Tay et al., 2023; Choromanski et al., 2020; Katharopoulos et al., 2020; Beltagy et al., 2020; Child et al., 2019). However, these improved structures often achieve efficiency in the attention layer at the expense of some performance.\nFaced with the scaling challenges of Transformers, the exploration of new model architectures to replace Transformers has gradually come into focus, leading to the development of modern RNN architectures, including RWKV (Peng et al., 2023), RetNet (Sun et al., 2023), and Mamba (Gu & Dao, 2023). Among them, the Mamba architecture (Gu et al., 2021; Gu & Dao, 2023), based on the state space model (SSM), has garnered attention for its performance comparable to Transformers in many sequence modeling tasks Dao & Gu (2024) and vision tasks (Zhu et al., 2024; Xu et al., 2024). These models utilize hardware-aware algorithms during training, resulting in computational costs that scale linearly with sequence length, and require constant-level computation and storage during inference. Mamba's strong performance and computational efficiency make it a strong competitor to Transformers."}, {"title": "2 RELATED WORK", "content": "SSMs and Attention Mechanism: The attention mechanism is a core component of LLMS (Brown et al., 2020; Touvron et al., 2023). Drawing connections between SSMs and attention is a fascinating direction as it not only aids in our understanding of the Mamba structure but also facilitates the transfer of well-established acceleration techniques from attention mechanisms to Mamba (Dao, 2023; Katharopoulos et al., 2020). Based on observations of the similarities between them, Dao & Gu (2024) proposed the state space dual (SSD) layer based on SSMs to achieve significant improvements in training efficiency. Sieber et al. (2024) introduce the Dynamical Systems Framework (DSF), under which attention and SSMs can be directly compared. Additionally, Han et al. (2024) reformulate the structure of SSMs to establish links with linear attention, aiming to investigate the key factors behind success in vision tasks. We follow this convenient reformulation and comparison, based on which we furthermore explore Mamba's ability to perform the COPY operation.\nComparisons between Transformers and Mamba: More recent works compare the performance of Mamba and Transformers across various tasks from different perspectives. Merrill et al. (2024) theoretically demonstrate that, similar to Transformers, Mamba is also unable to solve state tracking problems such as permutation composition. Jelassi et al. (2024) find that Transformers significantly surpass SSMs when facing tasks related to copying and retrieving information from context. In"}, {"title": "3 PRELIMINARIES", "content": "In this section, we introduce the Mamba structure that we focus on and its reformulated form firstly introduced by Han et al. (2024), which facilitates a better understanding of the connection between Mamba and linear attention as illustrated in Section 4.1. It should be noted that to better distinguish different types of variables, in this paper, we use bold uppercase letters to represent matrices such as A, bold lowercase letters to represent vectors such as a, and all non-bold letters to represent scalars such as a and \u2206. This may differ slightly from the notations used in some Mamba-related literatures (Dao & Gu, 2024; Zhu et al., 2024), where uppercase letters are used to describe A, B, C in SSMs.\nState Space Model: The state space model (SSM) is inspired by the continuous system that maps a scalar input x(t) \u2208 R to its output y(t) \u2208 R through a high-dimensional hidden state h \u2208 \\R^{d_h} (Gu & Dao, 2023; Dao & Gu, 2024; Han et al., 2024; Zhu et al., 2024; Han et al., 2024). Specifically, this system can be written as:\n$h'(t) = Ah(t) + bx(t), \\ y(t) = ch(t) + dx(t),$\nwhere A \u2208 \\R^{d_h \\times d_h} denotes the evolution parameters, b, c \u2208 \\R^{d_h} are projection parameters and d is a scalar parameter. The above continuous system can be discretized using transformation called zero-order hold (ZOH), resulting in a discrete version that can be used for neural networks. In this process, A, b will be transformed as A, b. The discrete version for SSM can be written as:\n$h_i = \\bar{A}h_{i-1} + \\bar{b}x_i, \\ Y_i = c^Th_i + dx_i,$\nwhere $\\bar{A} = exp(\\Delta A), \\bar{b} = (\\Delta A)^{-1}(exp(\\Delta A) \u2013 I) \u00b7 Ab \\approx \\Delta b$ and \u2206 \u2208 R is a timescale parameter. The matrix A is typically assumed to have certain structures such as being diagonal, leading to the structured SSMs (Gu et al., 2022; Gupta et al., 2022).\nSelective State Space Module: To enhance the SSM, Mamba makes the parameters $b_i, c_i, A_i$ dependent on different inputs $x_i$. More specifically, A is set to be diagonal resulting in that"}, {"title": "4 CAN MAMBA ALWAYS PERFORM COPY PERFECTLY?", "content": "In this section, we firstly interpret the reformulated SSM module introduced in Section 3 as a special linear attention. Then based on this observation, we explore the capability of Mamba to execute COPY operations during inference, which is highly related to the ability to perform in-context learning and retrieve information.\n4.1 VIEWING MAMBA AS LINEAR ATTENTION\nThe attention mechanism is the key to the success of the Transformer architecture. Recent works has explored the relationship between Mamba and attention mechanisms, particularly linear attention from different perspectives Han et al. (2024); Dao & Gu (2024); Sieber et al. (2024). The linear causal attention mechanism can be formalized as:\n$Y_i = \\sum_{j=1}^{i} v_jq_j^Tk_i = \\sum_{j=1}^{i}(q_j^Tk_i)v_j = \\sum_{j=1}^{i}\\alpha_{ij}v_j,$\nwhere $q_i, k_i, v_i$ are usually interpreted as query, key, value respectively and $\u03b1_{ij}$ denotes the attention scores of the i-th token to the j-th one. In attention mechanisms in Transformers, there exists $\u03b1_{ij} > 0$ for all $j < i$ and $\\sum_{j=1}^{i}\\alpha_{ij} = 1$, which can be implemented by Softmax function, or approximated by kernel methods (Katharopoulos et al., 2020; Choromanski et al., 2020).\nOn the other hand, given the input sequence $[x_i]_{i=1}^{i=N}$ and recalling Eq (1), that the output of the SSM module will have the following form when we set $H_0 = O$ and d = 0:\n$Y_i = (\\Delta_i \\odot x_i)b_i c_i + \\sum_{j=1}^{i-1} [\\prod_{l=j+1}^{i}(\\Delta_j \\odot x_j)b_j ] c_i,$"}, {"title": "4.2 THE TRADE-OFF OF MAMBA WHEN PERFORMING COPY", "content": "Based on the observation of the connection between the SSM module and attention mechanism, we investigate the capability of Mamba to recover historical inputs, which is foundational for the model to process information based on context. The COPY operation we focus on is defined as follows:\nDefinition 1 (COPY operation). Given the considered input sequence $X = [x_1]_{i=1}^N$, we define the L-local matching set $S_i = \\{i \u2013 L + 1 \\leq j \\leq i : |c_i^Tb_j| \\geq \\delta\\}$ and denote $v_i = \\Delta_i x_i$ as historical records. Then the output of COPY operation is a sequence of vectors $0_1, 0_2, ..., 0_N$ with $0_i = v_{pos(i)}$ where $pos(i) \u2208 S_i$ is the position we want to copy.\nThe L-local matching set $S_i$ describes the indices of historical keys $b_j$ that is highly relevant to the current query $c_i$ within a local window of length L, that is, the \u201cattention scores $|c_i^Tb_j|$\u201d is lower-bounded by \u03b4. Thus, the position of the historical record we most want to replicate should be within this highly relevant set. It should be noted that here we do not specify particular conditions for the positions we want to copy for a given $i \u2208 [N]$, which are usually determined by specific tasks. Therefore, our analysis can be applied to general scenarios. We then make the following assumption:\nAssumption 1. For the given sequence $X = [x_i]_{i=1}^N$, the following conditions holds:"}, {"title": "5 THE EXPRESSIVE POWER OF MAMBA EQUIPPED WITH COT", "content": "Although Mamba may face certain bottlenecks when handling copy tasks, another interesting question is: when augmented with other techniques, such as Chain of Thought (CoT), will Mamba see an improvement in its capabilities? Now, we turn our attention to the expressive ability of Mamba equipped with CoT to solve dynamic programming (DP) problems. Specifically, following the setup by Feng et al. (2024); Yang et al. (2024), a DP problem can be described by input sequences {$(1), s(2), ..., s(N)$}, state space I, transition function fr, and aggregation function fa. Each of component can be described as follows:\n\u2022 Input sequences: We use {$s(1), s(2), . . ., s(N) $} to denote the input of the sequences and the vector $[|s(1)|, |s(2)|, ..., |s(N)|]^\u2191$ to describe the scale of the problem, where |s(i)| denotes the length of the i-th sequence.\n\u2022 State Space: For a given DP problem, the state space In (and its size) will be determined based on the problem size n. Each state $i \u2208 I_n$ corresponds to an intermediate value dp(i) that needs to be computed, and i < j means that state i needs to be solved before state j. There exists a function $f_1: I_n\u2192 I_n$ to calculate the next state, that is, $j = f_1(i)$ if j is the next state to solve after i.\n\u2022 Transition function: The intermediate DP value can be calculated by the transition function fr as $dp(i) = f_r(n, s, \\{(j, dp(j)) : j < i\\})$ where s is the concatenation of the input tokens which corresponds to all elements of all input sequences. Furthermore, this can be formulated as $dp(i) = f(n, \\{s_j : j \u2208 D_i\\}, \\{dp(k) : k \u2208 V_{dp(i)}\\})$ where $D_i$ and $V_{dp(i)}$ are the sets of input tokens indices and DP values needed to solve state i respectively.\n\u2022 Aggregation function: To produce the final answer, the aggregation function needs to collect the required intermediate DP values and calculate the final result, which can be formalized as $anwser = f_a(\\{dp(i) : i \u2208 A_n\\})$ where $A_n$ is the set of DP values needed in the aggregation according to the problem size n.\nIt should be noted that in the above definition, we use $s(i)$ to denote the i-th input sequence and $s_i$ to denote the i-th input token, where s is all input tokens transformed from the concatenated input sequences (s(1), s(2), ..., s(N)). It can be referenced from Section 4.1 of Feng et al. (2024) for more detailed examples for DP problems. We consider the process by which the Mamba layer defined as Eq (2) gradually generates the solution to DP problems when using CoT. The format of the generated sequence can be written as:\n$s(1) | s(2) | ... | s(N) | (i_1, dp(i_1)) ... (i_{|I_n|}, dp(i_{|I_n|})) final answer$\nwhere the input sequence is separated using the symbol | as a delimiter.\nAssumption 2. Given the input sequences $s(1), s(2), ..., s(N)$, we consider the following constraints for the DP problem:"}, {"title": "6 DISCUSSION", "content": "In this paper, inspired by the similarity between the SSM module in Mamba and linear attention, we explore Mamba's potential bottlenecks in the COPY operation and show that Mamba with linear size can complete it. Additionally, we present that Mamba has the same cost as standard or efficient Transformers when solving DP problems using CoT. Our findings contribute to a deeper understanding of Mamba. However, we would like to illustrate that while Mamba may slightly underperform Transformers in certain tasks, it offers advantages in others like sparse parity learning(Park et al., 2024) and can achieve comparable performance with lower costs(Gu & Dao, 2023). Therefore, as shown in Park et al. (2024); Waleffe et al. (2024); Wen et al. (2024), exploring hybrid architectures and deeper theoretical analysis for them is a promising direction for future work."}, {"title": "A APPENDIX", "content": "A.1 PROOF OF THEOREM 1\nTheorem 5 (Perform COPY operation). Given a input sequence $X_1, X_2, ..., X_N$ and for any \u0454 > 0, there exists an SSM module with constant size that can approximate the COPY operation defined above when the following condition is satisfied for $i \u2208 [N]$:\n$\\rho \\geq 1 - \\frac{\\epsilon}{2M||v||} \\frac{1}{A_{pos(i)}} ( \\frac{\\delta}{\\frac{\\alpha_{max,<}}{1 - \\alpha_{max,<}} + \\frac{1}{\\alpha_{min,>}^{L-1} -1} + \\frac{1}{1 - \\alpha_{min,>}} }^2,$\nwhere $\\alpha_{max,<} = max_{10,$\nwhere $\u03a0_j^i = \\bar{A_i} \\bar{A_{i-1}}... \\bar{A_{j+1}}$. We notice that since in practice all elements of $\\bar{A}$ are positive and $\\bar{A}$ is set to be negative (Gu & Dao, 2023; Dao & Gu, 2024; Han et al., 2024), so that the elements of $\\bar{A_i}$ in Eq (1) belong to the interval [0, 1]. For the sake of simplicity in analysis, we replace the matrix $\\bar{A_i}$ with a constant $a_i$ (i.e., considering the case where all elements of $\\bar{A_i}$ are the same), where $a_i \u2208 [0, 1]$ (Dao & Gu, 2024). In fact, the subsequent analysis can be easily extended to the normal case where the elements of matrix $\\bar{A_i}$ are different. Then, Eq (5) can be rewritten as\n$y_i = \\sum_{j=1}^{i}a_j [\\prod_{l=j+1}^i \\Delta_j x_j] b_i c_i = \\sum_{j=1}^{i} \\alpha_j (c_i^T b_j) (\\Delta_j x_j),$\nwhere $\u03b1_j = \\prod_{k=j+1}^i a_k$ for $j < i \u2212 1$ and $\u03b1_i = 1$. In this form, we can observe that it bears similarities to linear attention without normalization in Eq (4), where $(\\Delta_j x_j), b_j, c_i$ corresponds to $v_j, k_j$ and $q_i$ respectively and $c_i^T b_j$ acts like attention scores $\u03b1_{ij}$. Considering $a_{j\u22121} \\geq a_j$ and $a_j \u2208 [0, 1]$ for all $j < i$, the main difference is that each term in Eq (6) is weighted by a coefficient $a_j$ to achieve the forgetting of inputs at longer distances while the attention mechanism uses the constraints for attention scores imposed by Softmax function to make sure the scaling of outputs.\n4.2 THE TRADE-OFF OF MAMBA WHEN PERFORMING COPY\nBased on the observation of the connection between the SSM module and attention mechanism, we investigate the capability of Mamba to recover historical inputs, which is foundational for the model to process information based on context. The COPY operation we focus on is defined as follows:\nDefinition 1 (COPY operation). Given the considered input sequence $X = [x_1]_{i=1}^N$, we define the L-local matching set $S_i = \\{i \u2013 L + 1 \\leq j \\leq i : |c_i^T b_j| \\geq \\delta\\}$ and denote $v_i = \\Delta_i x_i$ as historical records. Then the output of COPY operation is a sequence of vectors $0_1, 0_2, ..., 0_N$ with $0_i = v_{pos(i)}$ where $pos(i) \u2208 S_i$ is the position we want to copy.\nThe L-local matching set $S_i$ describes the indices of historical keys $b_j$ that is highly relevant to the current query $c_i$ within a local window of length L, that is, the \u201cattention scores $|c_i^T b_j|$\u201d is lower-bounded by \u03b4. Thus, the position of the historical record we most want to replicate should be within this highly relevant set. It should be noted that here we do not specify particular conditions for the positions we want to copy for a given $i \u2208 [N]$, which are usually determined by specific tasks. Therefore, our analysis can be applied to general scenarios. We then make the following assumption:\nAssumption 1. For the given sequence $X = [x_i]_{i=1}^N$, the following conditions holds:"}]}