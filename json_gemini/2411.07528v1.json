{"title": "SecEncoder: Logs are All You Need in Security", "authors": ["Muhammed Fatih Bulut", "Yingqi Liu", "Naveed Ahmad", "Maximilian Turner", "Sami Ait Ouahmane", "Cameron Andrews", "Lloyd Greenwald"], "abstract": "Large and Small Language Models (LMs) are typically pre-trained using extensive volumes of text, which are sourced from publicly accessible platforms such as Wikipedia, Book Corpus, or through web scraping. These models, due to their exposure to a wide range of language data, exhibit impressive generalization capabilities and can perform a multitude of tasks simultaneously. However, they often fall short when it comes to domain-specific tasks due to their broad training data. This paper introduces SecEncoder, a specialized small language model that is pretrained using security logs. SecEncoder is designed to address the domain-specific limitations of general LMs by focusing on the unique language and patterns found in security logs. Experimental results indicate that SecEncoder outperforms other LMs, such as BERT-large, DeBERTa-v3-large and OpenAI's Embedding (text-embedding-ada-002) models, which are pretrained mainly on natural language, across various tasks. Furthermore, although SecEncoder is primarily pretrained on log data, it outperforms models pretrained on natural language for a range of tasks beyond log analysis, such as incident prioritization and threat intelligence document retrieval. This suggests that domain-specific pretraining with logs can significantly enhance the performance of LMs in security. These findings pave the way for future research into security-specific LMs and their potential applications.", "sections": [{"title": "1 Introduction", "content": "Transformers [65] are a breakthrough AI architecture that have facilitated the development of various Language Models (LMs) [3,27]. These models can harness huge amounts of data to perform diverse tasks across language and other modalities such as audio, image and video. Some examples of LMs are BERT, ROBERTa, GPT, Gemini and PaLM [4, 5, 17, 19, 23-25, 48]. LMs can differ in their size, data sources, learning objectives, and are trained on large collections of text, such as Wikipedia, books, news articles, code, social media posts or web scraping. Certain language models, such as encoder-only models, are designed to encode the semantic and syntactic information of natural language into high-dimensional vectors, enabling them to perform downstream tasks like search, classification, summarization, translation, and question answering. Decoder-only models, on the other hand, are capable of generating natural language text by sampling from their probability distributions, producing coherent and fluent outputs. Some models, such as GPT-o and Gemini 1.5, extend these capabilities to multimodal content generation, creating images, audio, and video by utilizing a shared latent space across different modalities.\nLanguage models (LMs) have achieved state-of-the-art results across a wide range of benchmarks in natural language processing, coding, mathematics, and reasoning. They have also demonstrated impressive abilities to generate realistic and creative content, including stories, poems, songs, and jokes. These advancements in LMs have unlocked new opportunities and introduced challenges for AI research and applications, spanning areas such as natural language understanding, natural language generation, multimodal fusion, and knowledge extraction.\nHowever, LMs are not ideally suited to address domain-specific challenges, such as specialized vocabulary, terminology, knowledge, and logic, due to their design for broad and diverse applications. Previous studies highlight these limitations across several domains. In Biomedicine [31,49], LMs struggle to capture complex relationships and semantics of biomedical entities and concepts. In Finance [66], LMs underperform compared to domain-specific counterparts. In Medicine [40], LMs lack alignment with clinical utility. In Security [21,38], LMs fall short in domain-specific security knowledge. Similarly, in Software [33], LMs face challenges in interpreting and making sense of operational logs.\nDespite these limitations, the potential of Artificial Intelligence (AI), particularly generative AI, continues to attract significant interest from the security community [2, 6, 11]. These generative models hold potential as valuable tools for security professionals, serving as copilots to navigate the com-"}, {"title": "2 Related Work", "content": "plexities of security tasks such as identifying phishing emails, crafting detections, or analyzing and summarizing incidents. However, the field of security presents unique challenges that can only be partially addressed by generative models. A notable challenge is the need for security professionals to handle a variety of data types beyond natural language texts, including logs and telemetries. These data are often heterogeneous, noisy, and voluminous, necessitating efficient and scalable processing and analysis methods.\nIn this paper, we present SecEncoder, a small language model that is trained with security logs. SecEncoder is an encoder-only model, pretrained on a large corpus of security logs, which capture various events and activities related to security incidents and operations. SecEncoder aims to demonstrate the feasibility and utility of training a domain-specific language model on security logs at scale, and to provide a versatile and powerful model that can be applied to various security use cases. We evaluate SecEncoder on both intrinsic and extrinsic tasks, such as log analysis, anomaly detection, log search and incident classification. Our main contributions are:\n\u2022 We pretrain a security-specific small language model from scratch on a large and diverse corpus of security logs, which capture various events and activities related to security incidents and operations. We aim to train a versatile and powerful model that can generalize to various type of security use cases.\n\u2022 We evaluate SecEncoder using both intrinsic and extrinsic measures, using both internal and publicly available benchmarks. We also compare SecEncoder to the other LMs, such as BERT, DeBERTa and OpenAI's embedding model (text-embeddings-ada-002), and show that SecEncoder outperforms the best results on most of the tasks, and also exhibits some unique and novel capabilities.\n\u2022 We present four real-world use cases for SecEncoder. Notably, some of these use cases such as incident classification and threat intelligence document retrieval demonstrate that, despite SecEncoder being primarily trained on logs, it can effectively generalize to other data modalities without specific training on them. This finding suggests that logs could serve as valuable data sources for pretraining language models across domains beyond security.\n\u2022 We discuss the limitations and future directions for SecEncoder, focusing on areas such as data quality and diversity, as well as improvements in robustness and inference speed.\nThe remainder of this paper is structured as follows: Section 2 discusses related work, while Section 3 introduces the overall architecture and design. Section 4 details the various experiments conducted for testing and evaluation. Section 5 explains multiple real world use cases of SecEncoder and the corresponding results. Section 6 delves into limitations of SecEncoder and discusses future work, and finally, Section 7 provides the conclusion."}, {"title": "2.1 Language Models", "content": "Large and small Language Models are a type of deep neural network that contain a substantial number of parameters. These models are trained using vast amounts of data in a self-supervised manner, which enables them to perform a wide range of tasks. These tasks include, but are not limited to, generating text, images, and videos, summarizing content, answering questions, and reasoning.\nLMs are typically categorized into three types based on their use of the transformer architecture: encoder-only, decoder-only, or both encoder and decoder models. Encoder-only models, such as BERT [23] and DeBERTa [34, 35], are primarily used as representational models. They are capable of converting a given input into numerical values, such as vector representations. This feature makes them particularly useful for finetuning to different tasks.\nOn the other hand, some of the most popular LMs are decoder-only generative models. Examples of these include Gemini [24, 25], GPT [4, 5, 17], LLama [26, 63], and PaLM [19, 28]. These models rely on the now well-known Transformer architecture [65], with various modifications such as the mixture of experts [39] to enhance their performance. Beyond the realm of transformers, new architectures are also emerging. One such example is the Mamba model [30], which represents the continuous evolution and innovation in the field of Language Models."}, {"title": "2.2 Domain Adaptation", "content": "While LMs are generally versatile and capable of handling a wide range of tasks without finetuning, domain adaptation may be necessary to achieve optimal performance in specialized fields.\nBeyond pretraining a language model from scratch, there are three main techniques for domain adaptation: finetuning, prompt tuning, and in-context learning, which includes few-shot and zero-shot learning. Each approach has its own limitations. Finetuning a pretrained LM for a new domain can result in the loss of important token weights, especially if they are not part of the original vocabulary and the new domain has a different corpus. Additionally, finetuning may lead to catastrophic forgetting [50], where the model begins to forget tasks it previously performed well.\nPrompt tuning [44,45] also presents challenges, particularly in deciding which parts to update with gradient descent while"}, {"title": "2.3 Security specific LMs", "content": "keeping the rest intact. Lastly, in-context learning, while convenient and straightforward, has its limitations. A pretrained LM may not be able to learn new, unseen knowledge and may still lack domain-specific capabilities.\nIn the literature, there are several works that propose domain-specific large and small LMs. These include models for biomedicine [31,49], molecular biology [57], finance [66], and security [16, 32, 43, 51, 54, 56].\nIn the study by [16], the authors introduce a small language model that is pretrained with natural language security texts, as opposed to logs, which is the focus on this paper. Another study, [54], demonstrates how to finetune a LM for the purpose of detecting malicious software, highlighting a specific use case. There are also other specific applications of finetuning, which include phishing and threat detection [43, 51], as well as log anomaly detection for specific types of logs such as system logs [32,37,58,70].\nSecurity is a dynamic domain where attackers are constantly evolving and using different behaviors and vocabularies compared to natural language. SecEncoder stands out from previous approaches as it focuses on pretraining from scratch at a scale not attempted with other security-specific LMs [16, 38]. The ultimate goal of SecEncoder is to serve as a general-purpose tool for security logs, rather than being tailored for specific use cases, as targeted in some of the previous work [32, 37, 38, 54, 58, 70]."}, {"title": "3 System Design", "content": "Figure 1 illustrates the comprehensive architecture of the SecEncoder system, detailing the process from initial data extraction to final deployment. In the following section, each component is thoroughly explained, providing in-depth insights into their individual functions and interactions within the overall framework."}, {"title": "3.1 Data", "content": "SecEncoder leverages two distinct types of data sources for pretraining: public and private. Public data sources are carefully mined from the Internet, with a focus on those that have permissible licenses to ensure compliance and ethical use. The second type of data utilized for training SecEncoder comprises private data sources, specifically proprietary data owned by Microsoft (not customer data). This includes security-related data that is instrumental in helping Microsoft operate its business securely and efficiently. Microsoft possesses a vast and rich repository of such telemetry data, as referenced in [9]. Appendix A offers a comprehensive overview of the datasets"}, {"title": "3.2 Tokenizer", "content": "utilized during the pretraining phase of SecEncoder. In total, we utilized 1 terabyte of data for training SecEncoder. This data encompasses a diverse set of logs from a wide array of sources, including hosts, devices, systems, and users. It covers various domains such as identity, operating systems (Windows, Linux), cloud, networks, and applications, ensuring a comprehensive and robust training dataset.\nSecEncoder leverages the robust capabilities of Azure Synapse Analytics to efficiently process raw data [52]. By harnessing the parallel processing power of Synapse, we can perform comprehensive data preprocessing tasks, including deduplication, for which SecEncoder employs advanced techniques inspired by the methods described in [17]. Specifically, our deduplication process encompasses both exact and approximate deduplication strategies, where exact deduplication identifies and removes duplicate records that are identical in every aspect, while approximate deduplication uses Min-Hash to detect and eliminate records that are not identical but highly similar. By employing these combined methods, we effectively reduce the dataset size from an initial 1TB to approximately 270GB, which equates to around 77 billion tokens. This reduction ensures a more manageable and efficient dataset for pretraining, especially considering the limited GPU budget.\nThe SecEncoder tokenizer is based on the Byte-Pair Encoding (BPE) algorithm [61]. We choose to train the SecEncoder tokenizer with BPE for several reasons as we outline below:\n\u2022 Handling Rare Tokens and Flexibility: Security logs often contain elements that are challenging for traditional NLP tokenizers, such as IP addresses, timestamps, unique identifiers, or error codes. BPE excels in handling these rare tokens by breaking them down into more manageable subwords, ensuring that even uncommon or unique sequences are effectively tokenized.\n\u2022 Vocabulary Size Reduction: Security logs are typically vast in volume, containing a myriad of unique terms and phrases. By decomposing words into subwords, BPE significantly reduces the overall vocabulary size. This reduction is crucial for efficient processing and storage, as it minimizes the memory footprint and enhances the tokenizer's performance.\n\u2022 Generalization to Unseen Data: One of the key advantages of BPE is its ability to generalize to unseen data. By learning subwords, the SecEncoder tokenizer can effectively handle new and previously unseen terms. This capability is particularly important in the context of security logs, where usernames, IP addresses, and other entities frequently change. BPE ensures that the tokenizer remains robust and adaptable to these dynamic elements."}, {"title": "3.3 Pretraining", "content": "The SecEncoder tokenizer is being trained on a substantial dataset, comprising 100GB of data that has been equally and randomly sourced from our pretraining dataset. This extensive and diverse dataset ensures that the tokenizer is exposed to a wide range of scenarios and variations, further enhancing its effectiveness and reliability. The resulting SecEncoder tokenizer has 29,952 number of tokens in its vocabulary. Table 1 illustrates an example tokenization by SecEncoder and other tokenizers. Notably, Natural Language (NL-based) tokenizers struggle to recognize security specific terms like TTY, USER and PWD often breaking them into incorrect segments.\nSecEncoder is a transformer-based encoder-only architecture. We envision SecEncoder as a representational model designed to tackle complex cybersecurity challenges and complement universal generative models such as GPT models. SecEncoder employs the same transformer architecture as DeBERTa-v2 [35]. The DeBERTa (Decoding-enhanced BERT with disentangled Attention) architecture enhances BERT and ROBERTa by incorporating a disentangled attention mechanism and an improved mask decoder. This architecture represents each word with two vectors for content and position, computing attention weights separately for these aspects. The enhanced mask decoder integrates absolute positions, improving the model's ability to predict masked tokens and boosting overall performance on NLP tasks. This architecture has demonstrated superior performance across various NLP downstream tasks. SecEncoder can handle up to 48,000 tokens as an input, enabling it to process long log lines effectively."}, {"title": "3.3.1 Pretraining Loss", "content": "SecEncoder is pretrained using a customized version of masked language modeling (MLM) loss. The MLM objective involves randomly masking tokens in the input sequence and training the model to predict these masked tokens. This task encourages the model to learn contextual representations and understand the relationships between different tokens.\nA portion of the logs in our dataset are structured and transformed into a JSONL format, which includes a large number of delimiter tokens such as double quotes. If we randomly mask tokens in these logs without considering the type of tokens, the model will spend a significant amount of time learning to predict the delimiters, which are not as important for understanding the log content.\nTo address this and improve training efficiency, we propose a customized MLM loss. This customized loss is designed to mask only the content tokens and not the delimiter tokens. This modification ensures that the model focuses on learning the content of the logs and the relationships between the content tokens, rather than the delimiters. This approach has been shown to improve the quality of the learned representations and enhance the model's performance on downstream tasks.\nHere is the formula for the customized MLM loss:\n$X_{masked} = {x_i \\sim X \\backslash X_{delimiter}}$\n$L_{MLM} = \\sum_{i=1}^N log P(x_i | X_{masked})$         (1)\nFormula 1 outlines the customized MLM loss, where $x_i$ represents the token at position i, $X_{masked}$ denotes the masked tokens and $X_{masked}$ is sampled from the input token set that excludes the delimiters. This loss function encourages the model to predict the masked content tokens, enhancing its ability to learn the content of the logs and the relationships between the content tokens. With this customized MLM loss, SecEncoder can effectively learn to generate high-quality representations of security logs and perform well on a wide range of downstream tasks."}, {"title": "3.3.2 Pretraining infrastructure", "content": "SecEncoder leverages Azure Machine Learning (AML) pipelines to streamline and optimize the pretraining process [1]. By utilizing these pipelines, SecEncoder ensures a robust and scalable infrastructure capable of handling large-scale data and complex computations. During the pretraining phase, SecEncoder integrates several advanced tools and frameworks to enhance performance and efficiency as explained next:\n\u2022 HuggingFace, Pytorch-Lightning and PyTorch: This library provides state-of-the-art models and tools for natural language processing. SecEncoder utilizes Hugging-Face and Pytorch-Lightning libraries to easily pretrain SecEncoder along with PyTorch [7, 14, 29].\n\u2022 DeepSpeed: This deep learning optimization library is used to scale up training, allowing SecEncoder to handle larger models and datasets more efficiently while reducing training time and resource consumption [55].\n\u2022 ONNX Runtime for PyTorch: By incorporating ONNX Runtime, SecEncoder accelerates PyTorch models, ensuring faster inference and reduced latency [13].\nTogether, these technologies enable SecEncoder to achieve faster, more efficient, and scalable machine learning workflows, ultimately leading to better performance and outcomes. We pretrain SecEncoder using 64xA100 GPUs. Even though we pretrain multiple models, the largest SecEncoder model, 1.1B billion parameters, is trained on 77B tokens of security logs, and take approximately 4 weeks to complete the pretraining process."}, {"title": "3.4 Evaluation", "content": "SecEncoder utilizes Azure Machine Learning (AML) pipelines to conduct thorough benchmarking and evaluation processes. At the core of this system is a master pipeline, which orchestrates the initiation of multiple benchmarking pipelines in a distributed manner. This distributed approach ensures that SecEncoder can be evaluated across a wide range of benchmarks efficiently and effectively. The results from these evaluations are meticulously aggregated and compiled into a CSV file, facilitating straightforward analysis, interpretation and comparison over time. Further details on the evaluation can be found in Section 4."}, {"title": "3.5 Finetuning", "content": "SecEncoder leverages Azure Machine Learning (AML) pipelines for finetuning. The finetuning pipeline is primarily used to generate models based on the pretrained SecEncoder model for evaluation purposes. The components within these pipelines facilitate the easy development of additional models, enabling the system to address a wide range of use cases efficiently."}, {"title": "3.6 Deployment", "content": "Once the SecEncoder model is pretrained, it is deployed to Azure Machine Learning (AML) as an endpoint to serve various use cases. These use cases, which some of them we discuss in Section 5, include log subsampling, log pattern detection and incident classification. AML endpoints offer a versatile and scalable solution for deploying machine learning models, allowing for seamless integration with other services and applications. The endpoints support autoscaling, ensuring that the model can handle varying loads efficiently. Additionally, AML provides an intuitive interface and comprehensive tools that make the deployment process straightforward and user friendly."}, {"title": "4 Experiments", "content": "SecEncoder is pretrained in various sizes: base (110M), large (350M), xlarge (700M), and xxlarge (1.1B), following the DeBERTa naming conventions. The base model is trained on 160GB of data (47B tokens), while the large, xlarge, and xxlarge models are each trained on 270GB of data (77B tokens). In general, the training process is smooth, with the loss decreasing steadily over time, despite occasional spikes. Figure 2 shows the training loss trajectory for the xxlarge model. Overall, the training experience is consistent, showing a gradual reduction in loss over time.\nSecEncoder is assessed through various dimensions. Section 4.1 delves into the intrinsic evaluations of SecEncoder. Section 4.2 examines the extrinsic evaluations of SecEncoder across different downstream tasks. Lastly, Section 4.3 discusses the inference time of SecEncoder."}, {"title": "4.1 Intrinsic Evaluation", "content": "This assessment utilizes two key metrics: perplexity and masked token prediction accuracy. Perplexity, a conventional metric for appraising language models, is used to evaluate how well a model predicts a sample of text. The formula to calculate perplexity is given in Equation 2, where (w1,W2,...,WN) represents the sequence of words, and P(W1,W2,..., wn) denotes the probability of the sequence of words according to the model. Perplexity gauges the uncertainty of a language model in predicting the next token (such as a word or character) in a sequence \u2013the lower the perplexity, the more capable the model.\nPP(W) = exp(\\frac{1}{N} \\sum_{i=1}^N log P(w_i | w_1, w_2,..., w_{i-1}))        (2)\nAccuracy, as an instrinsic metric, on the other hand, quantifies the proportion of masked tokens that are correctly predicted -the higher the accuracy, the better the model performs. The formula to calculate the accuracy is given in Equation 3.\nAccuracy = $\\frac{Number \\, of \\, Correct \\, Predictions}{Total \\, Number \\, of \\, Predictions}$               (3)\nWe conducted intrinsic evaluations on two datasets: the In-distribution test set (IDTS) and the Out-of-distribution test set (ODTS). The total size of IDTS is 2.2GB (~10M samples), whereas ODTS is 0.03GB (~ 200K samples). The key difference between these datasets is that ODTS is sampled from a different distribution than the training dataset, whereas IDTS shares the same distribution as the training dataset \u2013 albeit none is being used while training SecEncoder."}, {"title": "4.2 Extrinsic Evaluation", "content": "For extrinsic evaluation, we assess how well SecEncoder performs on specific downstream tasks. We focus on three primary tasks: log similarity, log search and log anomaly detection. For these tasks, we evaluate the performance of SecEncoder embeddings. This helps determine whether SecEncoder's embeddings represent security logs and can be effectively used for log-related tasks. We also compare SecEncoder's embeddings with those from natural language-trained (NL-based) models. To represent a single embedding vector for a given log, we average the embeddings of all tokens."}, {"title": "4.2.1 Log similarity", "content": "In this task, we evaluate whether the SecEncoder embeddings can help distinguishing between similar and different logs. We evaluate on the linux system logs because they do not have a fixed template, and the logs are more diverse. To test this, logs with the same log template, parsed by the drain parser [36], are randomly grouped together as positive pairs, while logs with different templates are randomly grouped together as negative pairs. We use the drain parser's default settings, tree depth parameter (depth=4) and a similarity threshold parameter (st=0.4). In total, 1,000 positive and 1,000 negative pairs are generated. Difference in cosine similarity between positive and negative pairs using models' embeddings, is used as a metric to evaluate the performance -the greater the difference, the better the performance is.\nTable 3 shows the performance of SecEncoder in comparison to different NL-based models. The row PMean represents the mean cosine similarity of positive pairs, NMean represents the mean cosine similarity of negative pairs, and Diff is the difference between PMean and NMean. The greater the Diff, the better the results. For the log similarity task, SecEncoder shows substantial improvement over NL-based models (0.46 vs. 0.10). Generally, larger SecEncoder models perform better than smaller ones."}, {"title": "4.2.2 Log search", "content": "The second extrinsic task used to evaluate SecEncoder is log search. This task tests whether the model's embeddings can help find logs with the same template in a reference set, given a query log. Similar to the log similarity experiment, we use linux syslogs and the Drain parser [36] to parse the logs into templates. The query set contains 4.7k samples, and the reference set contains 5.4k samples. Notably, the query and reference sets do not share templates with SecEncoder's training set.\nAs a variation, instead of using the Drain parser, we adopt a different approach to find similar logs. First, we create natural language (NL) descriptions of the logs using GPT-4 and then cluster the logs into different groups based on these descriptions. We assume that logs within the same clusters are relevant and should be returned in the search. This NL-cluster approach differs from the template-based approach and may better capture semantic similarity.\nTo evaluate both template and nl-cluster based approaches, Mean reciprocal rank (MRR) and mean average precision (MAP) are used. MRR measures the average of the reciprocal ranks of results for a set of queries as shown in Equation 4, where Q is the total number of queries and rank; is the rank position of the first relevant log for the (i)-th query. On the other hand, MAP calculates the mean of the average precision scores for a set of queries, as shown in Equation 5 where Q is the total number of queries and AP(i) is the average precision for the i-th query.\nMRR = $\\frac{1}{|Q|} \\sum_{i=1}^Q \\frac{1}{rank_i}$                (4)\nMAP = $\\frac{1}{|Q|} \\sum_{i=1}^Q AP(i)$                (5)\nTable 3 compares the performance of SecEncoder with various NL-based models. In the template-based search, SecEncoder shows a slight improvement over NL-based models. However, in the NL-cluster-based search, OpenAI's text-embedding-ada-002 performs slightly better in MAP and similarly in MRR. Although the specifics are unclear, using GPT-4 descriptions for clustering might have given an unfair advantage to OpenAI's embedding model if it is based on the same underlying model. Nonetheless, SecEncoder still outperforms other NL-based models, such as BERT and DeBERTa, in most metrics."}, {"title": "4.2.3 Log Anomaly Detection", "content": "In this task, we evaluate SecEncoder on the anomaly detection task using the datasets, as detailed in Table 4. These datasets include both private and public sources. The M365 dataset is sourced from M365 data center machines, contains red teaming activities labeled as anomalies. The HDFS [67,71], BGL [53,71] and Thunderbird [53, 71] are obtained from LogHub [8, 71], featuring various types of logs for anomaly detection. We follow a similar methodology to that described in [42] to split the dataset into training and testing sets. For the HDFS dataset, we utilize only a subset of the full dataset. The F5 dataset consists of syslogs from a single F5 device in Azure, with some logs identified as anomalous by experts. Additionally, two more datasets, curated by Microsoft security researchers, include structured Windows logs and unstructured syslogs. Overall, these datasets represent a diverse set, encompassing both structured and unstructured logs, and various types of logs for comprehensive anomaly detection evaluation.\nEmbedding quality evaluation First, we evaluate the quality of the models' embeddings. To achieve this, embeddings are used as features for the log lines. More specifically, given a dataset, each log line is fed into the model as a text, and an embedding is generated as the average of all token embeddings. These embeddings are then used to evaluate SecEncoder in both supervised and unsupervised settings.\nIn the supervised setting, we utilize a simple Long Short-Term Memory (LSTM) network [60] and Graph Neural Networks (GNN) [59]. Each sample of data consists of a set of logs. We first process these logs through SecEncoder, obtaining sets of embeddings. For the LSTM model, we treat each set of logs as a sequence annotated with timestamps and train the LSTM on these sequences of SecEncoder embeddings. For the supervised GNN model, we represent each set of logs as a graph. We construct the graph based on the Euclidean distance in the embedding space, drawing edges only between nodes that are within the top 1% of the smallest Euclidean distances. We then train the GNN on these graphs."}, {"title": "4.3 Runtime performance of SecEncoder", "content": "We evaluate SecEncoder's runtime performance using a single V100 GPU with 16GB of memory. We processed 2,000 logs from six different tables in a demo Microsoft Sentinel environment [12]. These logs are semi-structured, containing both categorical data and open text. The average number of tokens per line ranges from 513 to 1052. Table 7 presents the inference speed (tokens per minute) for different model sizes.\nAs illustrated in Table 7, SecEncoder's inference speed decreases as the model size increases. This is expected due to the increased computational complexity associated with larger models. Specifically, for the same model, longer inputs result in slower processing speeds due to the O(n\u00b2) complexity of the transformer architecture. This quadratic complexity means that as the input length (n) increases, the time required for processing grows quadratically.\nFor real-world deployment, it is essential to balance SecEncoder's speed and performance. While larger models may offer better accuracy and more nuanced understanding, they also require more computational resources and time. Therefore, selecting the appropriate model size and optimizing input lengths are crucial steps to ensure efficient and effective deployment.\nIn summary, our evaluation highlights the trade-offs between model size, input length, and processing speed. These"}, {"title": "5 SecEncoder Use Cases", "content": "In this section, we delve into the application of SecEncoder to real-world challenges through two innovative services: LogSubsampling and LogPatternDetection. Additionally, we explore how SecEncoder effectively generalizes to various modalities, including incidents, alerts and threat intelligence documents."}, {"title": "5.1 LogSubsampling", "content": "Logs are a rich source of security data, providing valuable insights into system activities and potential threats. However, security analysts face significant challenges when investigating large quantities of raw logs and transforming them into actionable insights and recommendations. Generative models like GPT can assist in this transformation by turning raw data into meaningful insights and recommendations. However, due to size limitations, LMs cannot effectively process vast amounts of raw logs in their entirety.\nWhile generic methods exist for breaking large datasets into smaller chunks and combining results, these methods often fail to account for the varying importance of different data chunks. This is where LogSubsampling, powered by SecEncoder embeddings, comes into play. LogSubsampling effectively selects chunks of data that retain the critical information needed for an investigation, while pruning away redundant and uninformative data. This ensures that the most relevant and diverse set of logs is chosen for analysis.\nWe hypothesize that SecEncoder embeddings semantically represent security logs and will help in selecting a diverse and informative set of logs for analysis. To prove this we design and experiment with the following details:"}, {"title": "5.1.1 Data", "content": "We evaluate LogSubsampling across various log types, including Windows logs, Identity logs, Device logon events, Sign-in logs, and Alert Info logs. From each dataset, we select 2000 logs and further subsample to 10 logs each."}, {"title": "5.1.2 Methodology", "content": "For subsampling, we employ a greedy algorithm based on SecEncoder embeddings. This algorithm selects logs that maximize the variance of the embeddings, ensuring the chosen logs are both diverse and informative. Initially, the algorithm selects the log closest to the center of all logs. It then iteratively selects the log with the maximum minimum distance to the embeddings of the already selected logs. This Max-Min strategy ensures the selected logs are maximally diverse and avoids choosing logs that are too similar to those already selected."}, {"title": "5.1.3 Evaluation", "content": "To assess the performance of SecEncoder embeddings on Log-Subsampling, we compare them against two baseline methods: random sampling and Term Frequency-Inverse Document Frequency with K-Nearest Neighbors (TF-IDF+KNN). For TF-IDF+KNN, we first apply TF-IDF to transform the text data into feature vectors. Then, we use K-Nearest Neighbors (KNN) to cluster the data into N clusters, where N represents the target number of subsampled samples. Finally, we select the samples closest to each cluster center, representing the most central examples within each cluster.\nWe assess the performance of SecEncoder embeddings using two metrics: entity count and Levenshtein distance. The entity count shows the number of unique entities in the subsampled logs, while the Levenshtein distance measures the edit distance among the subsampled logs. Higher entity counts and Levenshtein distances indicate more diverse and informative subsampled logs. Entity count is measured at the entity level, where each unique value in each column represents a unique entity count. Levenshtein distance is measured at the character level.\nAs shown in Table 8, SecEncoder's greedy approach, on average, returns 9% more entities than random sampling and 12% more than tf-idf+knn. While entity counts may consider similar entities as different, Levenshtein distance measures similarity at the character level. Additionally, SecEncoder's greedy approach outperforms random sampling by 2.2x and tf-idf+knn by 1.7x in Levenshtein distance on average. This demonstrates that SecEncoder embeddings are effective in selecting diverse and informative logs for analysis."}, {"title": "5.2 LogPatternDetection", "content": "LogPatternDetection is a tool designed to identify different patterns in log data. It employs an unsupervised implementation of the IsolationForest algorithm, utilizing SecEncoder embeddings as a featurizer. This section expands on the results discussed in Section 4 with a real world use case.\nThe IsolationForest algorithm operates by creating a random subsample of the dataset, which is then used to construct"}, {"title": "5.3 Incident classification", "content": "an isolation tree. Each tree is built using SecEncoder embedding as a feature, with split values selected randomly. The resulting binary tree is analyzed to identify isolated points, which are expected to have fewer splits, indicating potential anomalies. This method is effective for anomaly detection in logs because it does not require labeled data and can handle high-dimensional datasets efficiently.\nWe hypothesize that SecEncoder embeddings semantically represent security logs and will help in finding anomalies within a given set. To prove this we design and experiment with the following details:"}, {"title": "5.3.1 Data", "content": "Incidents and alerts are essential components of cybersecurity operations. Incidents refer to probable security breaches or attacks, whereas alerts are notifications of potential security issues that require further investigation. In this context, SecEncoder is applied to both incidents and alerts, with the dataset primarily consisting of natural language descriptions of these events. Although these data types are not specifically used to train SecEncoder or are relatively rare, one could argue that they still contain natural language elements similar to those found in some logs.\nTo evaluate SecEncoder's performance in incident classification, we use real world incident data. Each incident is associated with a set of alerts, and each alert is linked"}]}