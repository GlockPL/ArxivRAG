{"title": "GS2Pose: Tow-stage 6D Object Pose Estimation Guided by Gaussian Splatting", "authors": ["Jilan Mei", "Junbo Li", "Cai Meng"], "abstract": "This paper proposes a new method for accurate and robust 6D pose estimation of novel objects, named GS2Pose. By introducing 3D Gaussian splatting, GS2Pose can utilize the reconstruction results without requiring a high-quality CAD model, which means it only requires segmented RGBD images as input. Specifically, GS2Pose employs a two-stage structure consisting of coarse estimation followed by refined estimation. In the coarse stage, a lightweight U-Net network with a polarization attention mechanism, called Pose-Net, is designed. By using the 3DGS model for supervised training, Pose-Net can generate NOCS images to compute a coarse pose. In the refinement stage, GS2Pose formulates a pose regression algorithm following the idea of reprojection or Bundle Adjustment (BA), referred to as GS-Refiner. By leveraging Lie algebra to extend 3DGS, GS-Refiner obtains a pose-differentiable rendering pipeline that refines the coarse pose by comparing the input images with the rendered images. GS-Refiner also selectively updates parameters in the 3DGS model to achieve environmental adaptation, thereby enhancing the algorithm's robustness and flexibility to illuminative variation, occlusion, and other challenging disruptive factors. GS2Pose was evaluated through experiments conducted on the LineMod dataset, where it was compared with similar algorithms, yielding highly competitive results. The code for GS2Pose will soon be released on GitHub.", "sections": [{"title": "1. INTRODUCTION", "content": "Accurate 6D object pose estimation is a fundamental problem in the field of computer vision, with broad application prospects in technologies such as robot navigation[1, 2] and virtual reality [3, 4]. However, classical pose estimation algorithms[5-7] lack robustness against environmental interference, such as non-uniform lighting, varying degrees of occlusion, and dynamic blur. Moreover, the lightweight of the algorithm is also demanding in the field of embodied intelligence[8-10].\nWith the widespread application of deep learning methods, the robustness of related algorithms[11-15] against interference has continually improved. Early works[16\u201319] have achieved high-precision instance-level pose estimation. However, these models can only handle a specific object after training session and cannot generalize to others. Additionally, they require datasets with precise ground truth pose, which is difficult to obtain in practical applications.\nWith the emergence of novel pose representation methods[20] such as NOCS, breakthroughs have been made in category-level pose estimation methods[21-26], achieving notable intra-class generalization. Trained models can perform high-precision pose estimation on objects with similar geometric and color features. However, these methods typically require a substantial amount of CAD models of the same category during training phase, results in huge time expenditure. Additionally, since the 6D pose of the target object is bound to the objects coordinate system under the CAD model, which can lead to issues, such as parameter ambiguity in the estimation results during the inference phase.\nIn recent years, with the development of large models[27-29], some research[30\u201332] have introduced the concept of pre-training on large datasets into the field of 6D pose estimation. These methods construct large datasets by collecting numerous CAD models of common objects from different categories, enabling effective generalization to unseen objects. They require only the CAD model of the target object during inference, allowing for the artificial setting of strict coordinate relationships without the need for additional training on the target object. However, these models also have drawbacks, such as the inability to generalize to uncommon objects, high computational resource consumption, and their accuracy being heavily dependent on the quality of CAD modeling.\nTo address the aforementioned shortcomings of these algorithms, we propose a novel pose estimation method that does not require artificial designed CAD models. This method is designed for application scenarios where high-quality CAD models of the target object are unavailable, and only un-textured scanned models or SFM point cloud models can be obtained. To achieve lightweight training, accurate reference relationships, and robustness to interference, GS2POSE consists of a two-stage pose estimation approach comprising coarse estimation followed by pose refinement.\nThe detailed process of GS2Pose is illustrated in in Fig. 1 The 3DGS point cloud model of the object (hereafter referred to as the 3DGS model) is obtained using existing 3DGS reconstruction techniques, with the object coordinate system manually specified. Inspired by the GS-SLAM model[33], we introduce the commonly used reprojection-based pose optimization iterative approach from the SLAM domain[34\u201336], also known as Bundle Adjustment (BA). By representing object poses using Lie algebra and integrating it with the differentiable 3DGS rendering pipeline, we developed an approach that utilizes reprojection and backpropagation. It enables an iterative optimization algorithm that can regress both the object pose and the camera pose, referred to as GS-Refiner.\nSince this iterative optimization algorithm requires a reasonable initial pose as a starting point for optimization, we also need to design an algorithm that can provide a rough"}, {"title": "2. RELATED WORKS", "content": "This section provides a brief summary of the development on the 6D pose estimation. We first review the 6D pose prediction about known rigid objects. Then we focus on the progress of 6D pose estimation about novel objects in recent years. We summarize the recent development of Gaussian models finally."}, {"title": "A. 6D pose estimation of known objects", "content": "Traditional 6D pose estimation methods [5, 38-40] rely on extracting local invariant features and establishing correspondences by template matching. Researchers have made innovative explorations in the features robustness and the template matching performance in complex occlusion scenarios. However, these traditional methods still struggle to solve challenges related to large variations in lighting and the accurate pose estimation of symmetric objects. As a result, the 6D pose estimation becomes inefficient and unsuitable for widespread development and practical applications.\nConversely, deep learning methods have gained attention in 6D pose estimation due to their powerful ability to automatically learn features from datasets. The PoseCNN model[19] introduced a novel loss function, enabling the network to better handle symmetric objects, thereby enhancing the robot's ability to interact with the real world. As for the applications without depth information, BB8[41] model proposed a classifier to restrict the range of poses, which can compensates the lack of depth information. Moreover, RADet[42] proposed a rigidity-aware detection method to better address occlusion issues, which created a visibility map using the minimum barrier between each pixel in the detection bounding box and the box boundary.\nRecently, Generative Adversarial Networks (GAN) have demonstrated exceptional capabilities in denoising and recovering missing parts of images. UnrealDA[43] proposed a GAN-based network, which transformed real depth maps with background occlusion into synthetic depth maps to improve pose estimation performance. Apart from that, the Pix2Pose[44] model based on GAN network, introduced a transformer loss to guide predictions toward the closest pose, addressing pose estimation for symmetric objects."}, {"title": "B. 6D pose estimation of novel objects", "content": "To improve the generalization ability and robustness of 6D pose estimation, some researchers aim to address pose estimation for novel objects. MegaPose[31] network proposed a 6D pose estimator based on a rendering and comparison strategy, which trains the network on a large synthetic dataset. Moreover, GigaPose[30] network proposed a novel solution by leveraging templates to recover out-of-plane rotations, then utilizing patches correspondences to estimate the four remaining pose parameters. Although above foundation methods have strong generalization capabilities, their robustness remains insufficient for specialized devices in industries and medical, such as surgical instruments and precision constructions. We proposed a course-refine 6D pose estimation network. For each new object, coarse estimation network provides an approximate pose by rapid training, followed by precise correction in the refine estimation network."}, {"title": "C. 3D Gaussian Representation", "content": "3DGS[45] demonstrates significant advantages in high-quality and real-time rendering. This work represents scene with 3D Gaussian ellipsoids and efficiently renders by rasterizing the Gaussian ellipsoids into images, achieving state-of-the-art (SOTA) level visual quality. At the same time, 3DGS employs an explicit construction method, possessing clear geometric structure and appearance. This technology has already been applied in multiple fields, including autonomous navigation[33, 36, 46, 47], virtual human body reconstruction[48, 49] and 3D generation[50\u201352].\nHowever, there are few works that apply 3D Gaussian Splatting (3DGS) to the field pose estimation currently. Although GSPose network attempts to apply 3DGS model, it still requires training a DINO network to create a database, which rely on dataset training. So this research is difficult to fine-tune for new objects. Moreover, it does not fully utilize the differentiable advantages of 3D Gaussian."}, {"title": "3. METHODOLOGY", "content": ""}, {"title": "A. Overview", "content": "In this chapter, we provide a detailed overview of the framework and principles of pose estimation methods. Our objective is to determine the relative pose of an object with respect to the camera, based on the input RGB-D image $I_{in}$ and the 3D geometric reconstruction model $G_m$ of the object. This involves computing the transformation matrix $T_{cm}$ from the coordinate system of the reconstructed model $m$ to the camera coordinate system $c$, which is composed of a translation vector $t_{cm}$ and a rotation matrix $R_{cm}$."}, {"title": "B. 3D Gaussian Splatting", "content": "3D Gaussian Spheres (3DGS) is a scene representation method that describes objects in the world coordinate system using Gaussian spheres. All attributes of the 3D Gaussian Spheres are learnable, including the position parameters $\u00b5$, opacity $\u03b1$, the 3D covariance matrix $\u03a3$, and the spherical harmonics $sh$. Given any point $x$ in the world coordinate system, the 3D Gaussian sphere defined at point x according to the Gaussian distribution is as follows:"}, {"title": "C. Coarse Pose Estimation Network", "content": "Inspired by the NeRF-Pose model[37], which is currently the state-of-the-art approach in 6D pose estimation, we have designed a lightweight NOCS image generation network ( Pose-Unet) to predict the coarse pose of objects. The 3DGS method generates RGB images from the camera viewpoint along with the corresponding NOCS images, which are used as training inputs for Pose-Unet. Through fine-tuning, the model can rapidly generalize to new objects. Subsequently, the test RGB images (segmented using the CNOS model) are input to obtain the corresponding NOCS images, from which a coarse pose is estimated. Since the NOCS image predictions exhibit significant deviations along the z-axis, the improved ICP algorithm is utilized to align the point cloud model in the observed viewpoint (acquired from RGB-D images) with the Gaussian model, correcting the z-axis in the coarse pose.\nPose-Unet utilizes ResNet50 as the encoder. While in the decoder stage, three transposed convolution layers are employed for up sampling. As most encoder-decoder based network models, Pose-Unet incorporates skip connections ( Mobile-ASPP) during the down sampling to minimize information loss. Mobile-ASPP optimizes the ASPP structure, which consists of three parallel atrous convolutions. Specifically, the dilated convolution layers have kernel sizes of 1 \u00d7 1, 3\u00d73, and 3\u00d73, with corresponding dilation rates of 1, 1, and 2, respectively. This module enables the network to fully capture shallow information while reducing computational resource consumption. In the deep feature extraction module, based on the PPM structure, four parallel pooling layers with different kernel sizes are constructed to effectively capture dependencies between pixels."}, {"title": "D. Refine Pose Estimation Network", "content": ""}, {"title": "1) Overview", "content": "After obtaining a coarse estimation $T_{coarse}$ with limited accuracy, we designed a multi-stage refinement algorithm, termed GS-refiner, which leverages the 3DGS representation model of the object. This algorithm employs an iterative reprojection method to provide a precise pose estimation of the object.\nInspired by 3D Gaussian Splatting SLAM[33], we represent the pose changes between coordinate systems using Lie algebra. We compute the error through reprojection for backpropagation, aiming to regress the precise pose of the object.\nThanks to the differentiable rendering pipeline of 3DGS, we can differentiate most parameters of the 3DGS, including the rendering pose, by calculating the differences between the reprojection images $I_{pred}(T_{iter})$ under the coarse estimated pose and the ground truth images $I_{in}$. Following the approach of 3D Gaussian Splatting, we design the loss function as follows:"}, {"title": "2) Camera Refiner", "content": "In the Camera Refiner, the object being updated is the camera coordinate system c. During each iteration, a new camera coordinate system c' is obtained, and the coordinates of any object point $p_m$ in the new camera coordinate system are given by:"}, {"title": "3) Object Refiner", "content": "In the second stage, the object of the update changes from the pose of the camera relative to the object to the pose of the object relative to the camera. Let the updated object coordinate system be m'. Since each object point $p_m$ on the object is rigidly attached to the object coordinate system, its coordinate values in the object coordinate system will not change, that is:"}, {"title": "4) Environment adoption", "content": "Since the 3DGS model is a type of self-emissive model, the lighting and shading characteristics of the model are not derived from its relative pose to the light source. Instead, they are obtained through the superposition of the RGB colors of each Gaussian sphere. This is a distinctive feature of the RayCast rendering algorithm. To address issues such as reflections and shadows under varying lighting conditions, we leverage the learnable nature of the 3DGS color parameters and the anisotropic properties of color parameters expressed by spherical harmonics. This allows the model to adapt to changes in lighting while adjusting its pose, thereby enhancing the accuracy of the model and preventing angle miscorrections due to lighting or shadow issues.\nIn this step, we set the 16 spherical harmonic parameters of the Gaussian model as learnable parameters, along with the rotational pose parameter, rot, of the Gaussian spheres. We have observed that during the model's color learning process, there is a tendency for the back side of the Gaussian spheres to be assigned a black color. Allowing the Gaussian spheres to rotate freely can accelerate the learning efficiency and prevent overfitting of the colors, as well as mitigate the issue of vanishing gradients.\nAdditionally, we lock other parameters, such as the scale parameter of the Gaussian spheres, their position parameters (xyz) relative to the object coordinate system, and their transparency. This is to prevent the model from compromising its original structure during iterations in an attempt to forcefully fit the target image. Such compromises could negatively impact the accuracy of angle estimation.\nBy carefully managing these parameters, we ensure that the model retains its integrity while effectively adapting to various lighting conditions. This approach not only enhances the model's performance but also maintains the precision required for accurate angle calculations, ultimately leading to improved results in rendering."}, {"title": "4. EXPERIMENTAL RESULTS AND ANALYSES", "content": "In order to evaluate the effectiveness of the proposed model, this section conducts a comparative analysis of its performance against a range of state-of-the-art deep-learning 6D pose estimation models, including Pix2Pose [44], SSD-6D [53], Lienet [54], Cai [55], DPOD [56], PVNet [57], CDPN [13]"}, {"title": "A. Experimental Dataset and Settings", "content": "Experiments were conducted on two publicly accessible datasets for 6D pose estimation: Linemod (LM) [7] and Linemod (LM) [7]: The LM dataset consists of 15 registered video sequences, each containing over 1100 frames. The object scales range from 100 mm to 300 mm. There are significant variations in illumination intensity of the images captured under the same model, along with minimal occlusion phenomena. We referenced the majority of 6D pose estimation methods [31, 37] and selected 13 categories to evaluate the performance of the model, including ape, bvise, cam, can, cat, driller, duck, eggbox, glue, holep, iron, lamp and phone."}, {"title": "5. CONCLUSION", "content": "In conclusion, this paper presents GS2Pose, a novel method for accurate and robust 6D pose estimation of novel objects that effectively addresses the limitations of traditional approaches reliant on high-quality CAD models. By leveraging 3D Gaussian splatting and segmented RGBD images, GS2Pose demonstrates a significant advancement in the efficiency and accessibility of pose estimation. The two-stage architecture, comprising the coarse estimation via the Pose-Net and the refined estimation through the GS-Refiner, showcases a well-integrated approach that enhances the precision of pose estimation. The experimental results on the LineMod dataset confirm the effectiveness of GS2Pose, positioning it as a competitive alternative to existing algorithms in the field."}]}