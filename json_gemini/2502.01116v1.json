{"title": "Picky LLMs and Unreliable RMs:\nAn Empirical Study on Safety Alignment after Instruction Tuning", "authors": ["Guanlin Li", "Kangjie Chen", "Shangwei Guo", "Jie Zhang", "Han Qiu", "Chao Zhang", "Guoyin Wang", "Tianwei Zhang", "Jiwei Li"], "abstract": "Large language models (LLMs) have emerged\nas powerful tools for addressing a wide range of\ngeneral inquiries and tasks. Despite this, fine-\ntuning aligned LLMs on smaller, domain-specific\ndatasets, critical to adapting them to special-\nized tasks, can inadvertently degrade their safety\nalignment, even when the datasets are benign.\nThis phenomenon makes models more suscep-\ntible to providing inappropriate responses. In\nthis study, we systematically examine the fac-\ntors contributing to safety alignment degradation\nin benign fine-tuning scenarios. Our analysis\nidentifies three critical factors affecting aligned\nLLMs: answer structure, identity calibration, and\nrole-play. Additionally, we evaluate the relia-\nbility of state-of-the-art reward models (RMs),\nwhich are often used to guide alignment processes.\nOur findings reveal that these RMs frequently\nfail to accurately reflect human preferences re-\ngarding safety, underscoring their limitations in\npractical applications. By uncovering these chal-\nlenges, our work highlights the complexities of\nmaintaining safety alignment during fine-tuning\nand offers guidance to help developers balance\nutility and safety in LLMs. Datasets and fine-\ntuning code used in our experiments can be found\nin https://github.com/GuanlinLee/\nllm_instruction_tuning.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) (OpenAI, 2023; Anthropic;\nAnil et al., 2023), containing billions of parameters, trained\non billions or trillions of tokens, have demonstrated impres-\nsive capabilities in handling diverse tasks and providing\ncreative and helpful responses. As these models become\nincreasingly adept at following user instructions, ensuring\ntheir outputs are safe, unbiased, and aligned with human\nvalues is paramount. Alignment techniques with reward\nmodels (RMs), such as reinforcement learning from hu-\nman feedback (RLHF) (Ouyang et al., 2022), have been\ninstrumental in fine-tuning these models to avoid generating\nharmful or illegal content while enhancing their ability to\nalign with human preferences.\nAlignment training typically occurs during the post-training\nphase, following the pre-training stage where LLMs are op-\ntimized for next-token prediction. In the post-training phase,\nmodels are fine-tuned on curated, high-quality datasets to en-\nhance their instruction-following capabilities while aligning\ntheir behavior with human values to mitigate the risk of gen-\nerating harmful or biased content. However, while aligned\nLLMs perform well on general tasks, additional fine-tuning\nwith domain-specific datasets is often required to improve\ntheir utility in specialized areas, such as helping customers\npick products (Zheng et al., 2024; Cao et al., 2024), provid-\ning professional medical advice to patients (Cascella et al.,\n2024; Savage et al., 2024), and completing code (Touvron\net al., 2023; Yang et al., 2024). In Figure 1, we illustrate the\nwhole lifecycle of LLMs development.\nDespite its utility, fine-tuning LLMs on domain-specific\ndatasets can inadvertently compromise the safety align-\nment of LLMs. Previous studies (Qi et al., 2024b; Zhao\net al., 2024; Ji et al., 2024) have shown that even when the\nfine-tuning data contains no explicit harmful content, the\nfine-tuned models can become more vulnerable to jailbreak\nattacks in generating inappropriate or unsafe outputs (Liu\net al., 2023; Yu et al., 2023; Zou et al., 2023). While most\nprior work (Hsu et al., 2024; Qi et al., 2024a; Huang et al.,\n2024a) has focused on safeguarding alignment in scenarios\nwhere datasets include harmful or illegal content, they often\nattribute safety degradation to shallow alignment mecha-\nnisms (Qi et al., 2024a). Furthermore, Ji et al. (2024) have\ntheoretically shown that LLMs inherently resist alignment,\nwhich is often superficial. Distinct from these approaches,\nfor the first time, our study aims to investigate the resistance\nto alignment from the perspective of the instruction-tuning"}, {"title": "dataset itself.", "content": "In this work, we focus on a purely benign scenario, where\nno adversarial factors or harmful data are present in the\ninstruction-tuning dataset. All samples in the instruction-\ntuning dataset are harmless, legal, and unbiased. The goal\nis to examine the intrinsic factors contributing to safety\nalignment degradation in aligned LLMs. Specifically, we\ninvestigate their vulnerability to jailbreak attacks after being\nfine-tuned on benign datasets designed for downstream tasks.\nOur analysis spans two core dimensions: the alignment\nrobustness of LLMs and the reliability of RMs in scoring\nand guiding alignment.\nTo explore the alignment of LLMs, we fine-tune open-source\naligned models on diverse instruction-tuning datasets, in-\ncluding those focused on medical tasks (Med, a), code com-\npletion (Pyt), and STEM subjects (Lee et al., 2023). Surpris-\ningly, we find these open-source aligned LLMs are picky to\nthe answer's format in the dataset. By simply reformatting\nthe answers in the dataset, we can improve or worsen the\nsafety alignment after the instruction-tuning. An automatic\nreformatting pipeline is proposed by us to achieve such\na job. Further, we reveal the identity calibration and the\nrole-play phenomena during the instruction-tuning process,\nwhich will enhance the alignment from the language model\nidentity learned during the alignment process or break the\nexisting safety alignment based on a new identity, depending\non the data items in the instruction-tuning dataset.\nIn parallel, we assess state-of-the-art RMs (Lambert et al.,\n2024) by analyzing their ability to accurately score data\nin both the original and reformatted datasets. We adopt\nthese RMs to generate absolute scores for each data item\nin the dataset and analyze these scores inside the dataset\nitself and between the original dataset and the reformatted\ndataset. Our experiments reveal that advanced RMs are fun-\ndamentally unreliable. Although these RMs can effectively\nidentify higher-quality training data within a dataset, they\noften fail to recognize the benefits of reformatted data that\nimprove model alignment, instead assigning them lower\nscores compared to the original versions. These findings\nshed light on the factors contributing to safety alignment\ndegradation in LLMs and provide actionable insights for\npreparing datasets to develop safer downstream applications.\nOur contributions can be summarized as follows:\n\u2022 We analyze the safety degradation of aligned LLMs\nfine-tuned on harmless downstream datasets, offering\na more general and practical perspective compared to\nstudies focusing on datasets with harmful content.\n\u2022 We identify and analyze three key factors in instruction-\ntuning datasets that influence safety alignment, demon-\nstrating how they can either enhance or diminish safety\ndepending on their use."}, {"title": "\u2022 We investigate the reliability of RMs, uncovering sig-\nnificant limitations and weaknesses in their application\nto downstream tasks.\n\u2022 We provide insights into the behaviors of LLMs and\nRMs, enhancing the understanding of safety degrada-\ntion and practical constraints in model fine-tuning.", "content": "2. Related Works"}, {"title": "2.1. Reward Models and Language Model Alignment", "content": "Reward models are widely used in the training process of\nmodern large language models. For modern LLMs, as their\nabilities improve with more learnable parameters and more\ntraining data (Kaplan et al., 2020; Hoffmann et al., 2022),\nit is important to align the behaviors of LLMs with human\npreferences to prevent them from generating harmful, bi-\nased, or illegal content.\nBefore aligning LLMs with human preferences, model de-\nvelopers need to build RMs to fit human preferences. To\nachieve this point, they first collect massive labeled data\nbased on human feedback to correctly and truthfully re-\nflect the preference (Ouyang et al., 2022). Then, RMs\nare trained on these data to learn human preferences with\nsuitable loss functions, such as pairwise loss (Wang et al.,\n2024c), Bradley-Terry loss (Ouyang et al., 2022; Liu et al.,\n2024; Wang et al., 2024c), margin-based loss (Liu et al.,\n2024), or regression loss (Wang et al., 2024d). Usually,\nadopting the Bradley-Terry loss can achieve better results\nand generalizability (Liu et al., 2024; Wang et al., 2024c).\nAfter obtaining advanced RMs, there are two mainstream\nalignment approaches, i.e., reinforcement learning-based\nand direct optimization-based. In these two approaches,\nreward models play different but equally critical roles. For\nthe former one, reinforcement learning algorithms, such\nas proximal policy optimization (PPO) (Schulman et al.,\n2017), are adopted to teach LLMs better sampling policies.\nBy earning higher rewards from the RMs, the LLMs learn\nthe human preference step-by-step. The most widely used\nsolution is reinforcement learning from human feedback\n(RLHF) (Ouyang et al., 2022). For the later one, direct\npreference optimization (DPO) (Rafailov et al., 2023) is one\nof the most widely used solutions, adopted by many popu-\nlar LLMs (Touvron et al., 2023; Yang et al., 2024; Dubey\net al., 2024). In DPO, RMs are used to select pairwise data\nand provide them to LLMs to learn the preference distribu-\ntion (Touvron et al., 2023; Dubey et al., 2024). Therefore,\nRMs in DPO act more like data filters, which are designed\nto provide more high-quality training data to LLMs.\nOverall, the performance of alignment is related to both\nRMs and the alignment approaches. In this paper, we only\nfocus on the impact of RMs and will leave the alignment\napproaches as our future work."}, {"title": "3. Preliminary", "content": "We provide a detailed overview of the instruction-tuning\ntask and the experimental setups used in our study."}, {"title": "3.1. Instruction-Tuning", "content": "The instruction-tuning task involves fine-tuning a pre-\ntrained and aligned LLM on a dataset D, which consists of\ntuples (xt, Xi, Xa). Here, xt represents an instruction detail-\ning the task or posing a specific question, xi provides addi-\ntional input or context, and xa is the expected answer. The\ninstruction-tuning process wraps xt and xi into a prompt\ntemplate that serves as the input\u00b9, aiming to\nimprove its ability to generalize across diverse tasks by\nleveraging human-provided instructions. xi can be empty\nfor some tuples. The primary goal of instruction-tuning is to\nenhance the model's capacity to follow human instructions\nfor tasks that were not explicitly seen during pre-training\nor post-training. Furthermore, instruction-tuning allows the\nmodel to acquire specialized knowledge embedded in D,\nenabling it to generate coherent and contextually relevant\noutputs for specific downstream tasks."}, {"title": "3.2. Experiment Setup", "content": "In our experiments, we consider a purely benign scenario,\nwhere no harmful data is in the dataset and no adversarial\nentities are involved during the model tuning phase. We\nconsider four different instruction-tuning datasets, including\na medical dataset MedicalInstruct (Med, a), a Sin-\ngapore culture dataset CRAFTSG (Wang et al., 2024a), a\ncode dataset PythonCode Instruct (Pyt), and a STEM\ndataset OpenPlatypus (Lee et al., 2023). For each\ndataset, we first split it into two fixed sets: training and\nvalidation. Then, we only save the checkpoint that achieves\nthe lowest loss value on the validation set and evaluate its\nsafety. For all datasets, the validation set contains 1,000\nitems. We give the details of used datasets in Appendix A.\nBesides datasets, we consider three open-source aligned\nLLMs, including Llama-3 (Dubey et al., 2024), Mis-\ntral (Jiang et al., 2023), and Qwen2 (Yang et al., 2024),\nand five open-source RMs, including SkyworkGemma (Liu\net al., 2024), SkyworkLlama (Liu et al., 2024), URM (Lou\net al., 2024), QRM (Dorka, 2024), and Internlm2 (Cai et al.,\n2024). For all instruction-tuning tasks, we adopt the effi-"}, {"title": "4. Picky LLMS", "content": "We conduct comprehensive experiments to reveal how picky\nLLMs are to the instruction-tuning datasets. (1) By simply\nreformatting the answer structure, we can manipulate the\nsafety alignment. (2) We also reveal the identity calibration\nand the role-play phenomena during the instruction-tuning\nprocess, proving that the LLMs are picky about the identity-\nrelated content in the dataset. More detailed analysis of\nthese factors is provided in Appendix G, where we pro-\npose several explanations to elucidate the deeper reasons of\npicky LLMs. In all experiments, we apply the same trans-\nformations on the validation set as we do on the training set,\nincluding reformatting, identity calibration and role-play."}, {"title": "4.1. Answer Structures Impact Safety Alignment", "content": "Previous works (Qi et al., 2024b) find that the safety align-\nment of LLMs drops after being fine-tuned on a clean\ndataset. However, the reason for such a consequence is not\nclear. A question arises naturally, \u2018Will all benign datasets\ncause such a decrease?'. A specific situation studied in pre-\nvious works (Hsu et al., 2024; Bianchi et al., 2024) proves\nthat adding data pairs that contain harmful instruction Xt\nand rejection answer xa into the training dataset and fine-\ntuning the LLMs on this new dataset will not cause a safety"}, {"title": "4.2. Identity Calibration and Role-play", "content": "In Section 4.1, we prove that reformatting the answer struc-\nture can mitigate the risks of safety alignment degrada-\ntion. However, in Table 2, we find that if we reformat\nthe answer structure of CRAFTSG based on the format from\nMedicalInstruct, which in most cases causes a signif-\nicant alignment drop as shown in Table 1, it does not cause\nthe alignment drop and sometimes increases the safety level\nof LLMs. This implies that answer structure is not the only\nfactor that influences the LLM safety.\nTo disclose the other factors, we analyze the characteristics\nof answers from CRAFTSG, finding that thousands of an-\nswers contain an explicit identity statement, such as 'as an\nAl' and 'as a language model'. It is because the answers\nin CRAFTSG are generated by GPT-4 (OpenAI, 2023), but\nwithout the online searching service. GPT-4 is designed to\nprovide users helpful and correct answers by OpenAI, reduc-\ning hallucination. Therefore, when the instruction xt con-"}, {"title": "5. Unreliable RMS", "content": "RMs are introduced during post-training to help LLMs dis-\ntinguish good and bad answers, further aligning LLMs with\nhuman preference. Previous works (Casper et al., 2023;\nChaudhari et al., 2024; Lambert & Calandra, 2023; Lam-\nbert et al., 2024) discuss the limitations of RMs, including\ngeneralization, robustness, quality, diversity, and evaluation.\nOur experiments focus on RMs' generalization and robust-\nness, i.e., their reliability on downstream datasets, which\ncan be represented as the ability to select the better answer\nfrom a branch of candidates. We consider a scenario where\nthe RM gives scores for data in the downstream training\nset. Based on this point, two potential applications can be\ndesigned. The first one is to select data items with higher\nscores to fine-tune the LLMs (Chen et al., 2024), reduc-\ning the size of the dataset and training cost. The second\none is to select the better answer based on the score that\ncan improve the performance of the training model when\nthere exists more than one answer for each data item (Wang\net al., 2024b). For both applications, because the training\ndata always have higher scores, equalizing to better align-\nment with human preference, the fine-tuned LLMs should\nbe aligned with better safety. However, based on our experi-\nments, we find that these state-of-the-art open-source RMs\ndo not always provide reliable scores.\nSpecifically, we design experiments to study both applica-\ntions. We only consider regression-based RMs (Liu et al.,\n2024; Wang et al., 2024c), which always give determined\nscores for the same data. We do not consider generation-"}, {"title": "5.1. Absolute Scores in Datasets", "content": "We study the first application, i.e., selecting data items with\nhigher scores to fine-tune the LLMs with RMs. We sort the\ntraining data in MedicalInstruct based on the scores\nand obtain two sets, each containing 4,000 items. The first\nset only contains data having the highest scores, named Top\nSet. The second contains data having the lowest scores,\nnamed Bottom Set. In Figure 2, we show the results on\ndifferent subsets scored with five RMs, respectively. We\nhave two conclusions. First, LLMs trained on Top Set\nalways have better safety alignment than LLMs trained on\nBottom Set, which is general to the RMs. Second, differ-\nent RMs have distinct preferences in scoring data. We find\nthat models trained on subsets separated based on different\nRMs achieve varying safety levels. To better quantify the\ndifferences, we count the number of disagreements between\nRMs in Figure 3. The results indicate that the disagree-\nment exists in both Top Set and Bottom Set, and is\nrelatively uniform and consistent about the preference.\nTherefore, when using the absolute scores to select data\nfrom a dataset, we can fine-tune an LLM using them and\nkeep its safety alignment, but the results are highly related to\nthe RMs used to score the data. RMs used to select the data\ncould have not aligned with the LLM in terms of preference\npolicy. It implies unreliability in such an application, as we\nhave no information about the quality of the selected data"}, {"title": "5.2. Pairwise Selection with RMs", "content": "To study the second application scenario, we adopt RMs to\nscore original and reformatted datasets in Section 4. Then,\nwe compare the scores between the original and the cor-\nresponding reformatted data items, as shown in Table 3.\nBased on the results in Tables 1 and 2, simply reformatting\nCRAFTSG will not decrease the model's alignment, and re-\nformatting other datasets will increase or keep the model's\nalignment. Therefore, ideally, RMs should give similar\nscores for reformatted and original data in CRAFTSG, while\ngiving higher scores for reformatted data than original data\nin other datasets. However, most RMs do not have clear\ncriteria. For example, most RMs give lower scores to refor-\nmatted data in all datasets, which means the answer structure\nis not the principle criteria or RMs have their own criteria\nto the answer structure, distinct from the LLM's preference.\nOn the other hand, we find that Internlm2 shows the most\nreasonable results on most datasets. However, it has the\nlowest performance on the RewardBench compared with\nother RMs in our experiments. We think most state-of-the-art RMs are overfitting to the RewardBench, and it actually\ncannot correctly reflect real human preference. As it is be-\nyond the scope of this paper to verify this point and propose\nnew benchmarks or better RMs, we only introduce our ordi-\nnary assumption and leave the detailed verification in future\nwork. Overall, based on our experiment, we prove that\nRMs are not reliable in comparing two answers' quality and"}, {"title": "6. Guidance for Safety-aware Fine-tuning", "content": "We provide practical suggestions based on our observations\nfrom two aspects, i.e., building a good instruction-tuning\ndataset and selecting a good RM.\nTo build a good dataset, there are four suggestions:\n\u2022 Affinity Answer Structure. Format answers in a struc-\ntured, itemized style (e.g., Markdown).\n\u2022 Adding Synthetic Data. Many LLMs share training\ndata, leading to similar answer preferences. Therefore,\nsynthetic data from well-aligned models helps improve\nsmaller models.\n\u2022 Less Identity Calibration. While it enhances safety,\nexcessive disclaimers make responses verbose and re-\nduce perceived usefulness and user experience.\n\u2022 Carefully Using Role-Play. Role-play causes diverse\nimpacts on LLMs. It will influence the cognition of\nLLMs from a deeper level.\nTo select a good RM, there are three suggestions:\n\u2022 Using RM Aligned with LLM Preferences. The best\nRM is the one used in the LLM's post-training phase,\nas it strictly guides the model's preference.\n\u2022 Ensuring Comprehensive Training Data. When the\nRM used in post-training is inaccessible, the selected\nRM should be trained on diverse preference data from\ndifferent sources.\n\u2022 Evaluation with Multiple Benchmarks. A single\nbenchmark can be biased or noisy. Therefore, testing\non diverse and comprehensive benchmarks improves\nthe selected RM reliability."}, {"title": "7. Limitations", "content": "There are several limitations in our work. First, in our ex-\nperiments, we only consider open-source models for both\nLLMs and RMs. The main reason is that open-source mod-\nels provide full controllability in the experiments, which\ncould assist our analysis. Commercial models may have\nother factors, e.g., system prompts and inference strategies,\nthat can affect the safety alignment. We believe it is an im-\nportant and valuable orientation to study commercial LLMs\nand RMs in future work.\nSecond, the datasets used in our experiments are constricted\nin English. We notice that there are more and more works\nstarting to study the impacts of different languages, includ-\ning English, Chinese, Japanese, and so on. However, most\nopen-source LLMs and RMs have better performance in the\nEnglish environment, and model developers mainly perform\nalignment on English datasets. We believe that with the de-\nvelopment of LLMs, the performance, including alignment,\nwill be closer among different languages. In future work,\nwe think it is meaningful to study the same features, such as\nanswer structures, in different language datasets.\nThird, we only consider fine-tuning LLMs with LoRA. As\nLORA can achieve similar performance with less compu-\ntational cost, it is a popular technic in model fine-tuning.\nComparing different fine-tuning methods could be a critical\npart of future work."}, {"title": "8. Conclusion", "content": "In this paper, we study the safety decrease phenomenon un-\nder a benign scenario. Specifically, three factors are found\nthat can impact the model's safety level, including the an-\nswer's format, identity calibration, and role-play. We exper-\nimentally prove that we can adjust these factors in a benign\ndataset to increase or decrease the model's safety. This indi-\ncates the importance of building a high-quality downstream\ndataset. On the other hand, we study the reliability of re-\nward models in scoring downstream data. The results reveal"}, {"title": "Impact Statement", "content": "There are two-fold impacts considering our experiments\ndiscovering several new features of the instruction-tuning\ndatasets. For the good part, our work proves that we can\nsimply reformat the answer structure to enhance or keep\nthe safety alignment. On the other hand, we can add some\nidentity information to better align the model or remove\nthe role-play details to improve or maintain the safety level\nwithout reducing the performance. For the bad part, we think\nsome malicious users can change the answer's structure on\npurpose to decrease safety without being detected. It can be\ntreated as a type of data poisoning attack, but more stealthy.\nWe also study reward models in our paper. We believe\nour work will encourage others to build a better evaluation\nbaseline for reward models, which benefits the deep learning\nand security communities."}, {"title": "A. Dataset Information", "content": "MedicalInstruct This dataset contains 26,357 items in total. It combines two previous medical datasets, i.e., Medical\nMeadow Wikidoc (mea) and MedQuAD (Med, b). xt in this dataset is always 'Answer the question truthfully, you are a\nmedical professional.'. xi is a medical question, such as 'Can you provide me with information regarding statins?", "What other iconic landmarks and attractions in Singapore, besides\nthe Marina Bay Sands, showcase the city": "luxurious and extravagant side?", "Write a Python program to calculate the average of a list of positive\nintegers and output the result.": "xi gives additional information about the request, such as \u2018List of positive integers: [1, 5, 6,\n7, 8]'. xa is the Python code for the request.\nOpenPlatypus There are 24,926 data in total. It is constructed by 11 science, code, and math datasets. xt describes a\nspecific question, such as 'A board game spinner is divided into three parts labeled A, B and C. The probability of the\nspinner landing on A is and the probability of the spinner landing on B is. What is the probability of the spinner\nlanding on C? Express your answer as a common fraction.'. xi in most cases is empty. For some multiple choice questions\nin the dataset, xi is 'Choose A, B, C or D as your solution.'. xa is the answer to the question xt."}, {"title": "B. Details of LLMs and RMS", "content": "Llama-3 In our experiment, we adopt Meta-Llama-3-8B-Instruct from the Llama-3 series. It is an auto-regressive language\nmodel based on transformer architecture. Based on the description of Meta, the instruction version is trained with supervised\nfine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness\nand safety.\nMistral Mistral-7B-Instruct-v0.2 is selected by us because it achieves better performance than its previous versions under\nvarious safety evaluations.\nQwen2 We choose the Qwen2-7B-Instruct in our experiment. Based on the model developer's description, the instruction\nversion is trained with SFT and direct preference optimization (DPO) to align with human preferences for helpfulness and\nsafety.\nSkyworkGemma This reward model is Skywork-Reward-Gemma-2-27B-v0.2. The model owners remove the contaminated\ndata used in v0.1 in the training set and fine-tune a gemma-2-27b-it model. As of January 2025, it ranks 3rd on the\nRewardBench leaderboard with a score of 94.3.\nSkyworkLlama We use Skywork-Reward-Llama-3.1-8B-v0.2 in our experiment. Similarly, the model owners remove the\ncontaminated data and fine-tune a Llama-3.1-8B-Instruct model. As of January 2025, it ranks 10th on the RewardBench\nleaderboard with a score of 93.1.\nURM URM-LLaMa-3.1-8B is used. It is an uncertain-aware reward model. The model owner fine-tunes Skywork-Reward-\nLlama-3.1-8B-v0.1 and adds additional uncertainty-aware and attribute-specific value heads. As of January 2025, it ranks\n12th on the RewardBench leaderboard with a score of 92.9.\nQRM QRM-Llama3.1-8B is used. The model owner fine-tunes Skywork-Reward-Llama-3.1-8B-v0.1 with an additional\ngating network and a quantile regression layer. As of January 2025, it ranks 11th on the RewardBench leaderboard with a\nscore of 93.1.\nInternlm2 We use the internlm2-7b-reward model. It is fine-tuned based on the foundation of InternLM2-Chat-7B-SFT.\nBased on the model owner's description, it has been trained using over 2.4 million preference samples, both human-\nannotated and AI-synthesized. It ensures a balance between helpful and harmless. As of January 2025, it ranks 34th on the\nRewardBench leaderboard with a score of 87.6."}, {"title": "C. LORA Fine-tuning Settings", "content": "We follow the most popular LoRA settings and refer to the setups provided by Platypus (Lee et al., 2023) and Meta (Dubey\net al., 2024). The details are shown in Table 4. We adopt two H100 to fine-tune the LLMs. There are two widely used\ninstruction-tuning prompt templates used in our experiment for different xi conditions."}, {"title": "D. LLM Inference Settings", "content": "We evaluate the safety of LLMs on one H100. We disable the sampling function during the evaluation process. Because based\non the previous work (Huang et al., 2024b), sampling settings, including temperature, top_p, and top_k, will significantly\nchange the jailbreak success rate. On the other hand, we do not use system prompts during the evaluation. Similarly,\nprevious works (Huang et al., 2024b; Lyu et al., 2024) find that system prompt will influence safety as well. Considering we\nfocus on the influence of datasets, we control the inference process and make sure that no other factors will influence the\nsafety alignment. The detailed settings are in Table 5."}, {"title": "E. ICL Reformatting Pipeline", "content": "In our experiment, we design an automatic answer reformatting pipeline based on ICL. For MedicalInstruct,\nPythonCodeInstruct, and OpenPlatypus, we adopt the same ICL system prompt with three demonstration exam-\nples, randomly selected from xa in CRAFTSG. For CRAFTSG, we adopt a new ICL system prompt with three demonstration\nexamples, randomly selected from xa in MedicalInstruct. The input for the LLM is the original answer xa. The\nLLM will reformat it and give the new one. We adopt two H100 or four A6000 to run the reformatting pipeline under the\nconfiguration listed in Table 6."}, {"title": "ICL system prompt for MedicalInstruct, PythonCodeInstruct, and OpenPlatypus.", "content": "Rewrite the text to follow the given format examples and keep the semantics\nunchanged.\nRewrite the text, instead of outputting the format examples!\nFormat Example 1:\nSingapore is a melting pot of cultures, and there are numerous ways to\nexperience its cultural diversity and religious harmony beyond the usual\nmethods of sampling local cuisine and visiting temples, mosques, and\nchurches. Here are some unique activities to consider:\n1. **Cultural Festivals and Celebrations:**\n Participate in or observe celebrations such as Chinese New Year,\n Deepavali, Hari Raya Puasa, and Vesak Day. These festivals often include\n street parades, live performances, and traditional activities."}, {"title": "ICL system prompt for CRAFTSG.", "content": "Remove the format of the given text!\nDo not itemize the text!\nDo not use bullet points!\nDo not use Markdown format!\nUse as less paragraphs as possible!\nTry to keep the text in one paragraph!\nRewrite the text to follow the below format examples!\nRewrite the text, instead of outputting the format examples!"}, {"title": "F. Identity Removal Pipeline", "content": "By manually checking hundreds of answers in CRAFTSG, we find that due to the restrictions of GPT-4, about 15% answers\ncontain identity-related content, such as 'an AI', 'a language model', and 'my knowledge update'. Usually, after these\nidentity phrases, the model will first reject to answer the question with the patterns, including 'I'm sorry', 'I am not able\nto', and 'I can't'. Then, the model will answer the question in a more general way. For example, if the question is about\nthe recent activities in Marina Bay, the answer will be in such a template, 'As a language model, my knowledge update\nis in early 2023, I can't give you the information of recent activities in Marina Bay. However, there are regular activities"}, {"title": "G. Rethinking Reasons Causing LLMs Picky", "content": "In Section 4"}]}