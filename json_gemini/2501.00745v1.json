{"title": "DYNAMICS OF ADVERSARIAL ATTACKS ON LARGE LANGUAGE MODEL-BASED SEARCH ENGINES", "authors": ["Xiyang Hu"], "abstract": "The increasing integration of Large Language Model (LLM) based search engines\nhas transformed the landscape of information retrieval. However, these systems are\nvulnerable to adversarial attacks, especially ranking manipulation attacks, where\nattackers craft webpage content to manipulate the LLM's ranking and promote\nspecific content, gaining an unfair advantage over competitors. In this paper, we\nstudy the dynamics of ranking manipulation attacks. We frame this problem as\nan Infinitely Repeated Prisoners' Dilemma, where multiple players strategically\ndecide whether to cooperate or attack. We analyze the conditions under which\ncooperation can be sustained, identifying key factors such as attack costs, discount\nrates, attack success rates, and trigger strategies that influence player behavior. We\nidentify tipping points in the system dynamics, demonstrating that cooperation\nis more likely to sustain when players are forward-looking. However, from a\ndefense perspective, we find that simply reducing attack success probabilities can,\nparadoxically, incentivize attacks under certain conditions. Furthermore, defensive\nmeasures to cap the upper bound of attack success rates may prove futile in some\nscenarios. These insights highlight the complexity of securing LLM-based systems.\nOur work provides a theoretical foundation and practical insights for understanding\nand mitigating their vulnerabilities, while emphasizing the importance of adaptive\nsecurity strategies and thoughtful ecosystem design.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have revolutionized natural language processing, with widespread\napplications in search engines and information retrieval systems. Examples include ChatGPT search,\nPerplexity AI, Google Search, and Microsoft Bing, where LLMs significantly enhance the interaction\nof search results through their advanced language understanding and generation capabilities.\nLLM-based search engines typically operate using the Retrieval-Augmented Generation (RAG)\nframework (Fan et al., 2024). In this framework, the system retrieves top-related documents from a\nknowledge base, database, or the web based on their relevance to the user's query. These retrieved\ndocuments are combined with the query into an augmented input, forming a coherent prompt that\nprovides a comprehensive context for the LLM's response generation. This integrated prompt ensures\nthat the LLM aligns effectively with the user's intent as expressed in the query while incorporating\nrelevant external information retrieved from knowledge sources.\nHowever, the integration of LLMs into these search engines introduces new vulnerabilities, particu-\nlarly ranking manipulation attacks (Pfrommer et al., 2024; Nestaas et al., 2024). These attacks exploit\nthe sensitivity of LLMs to input variations by embedding crafted instructions or misleading content\nwithin documents or webpages, enabling attackers to influence LLMs into favoring their content or\nproduct over competing ones (Kumar & Lakkaraju, 2024; Aggarwal et al., 2024).\nSecuring LLM-based search engines against ranking manipulation attacks presents a multifaceted\nchallenge that differs significantly from traditional search engine optimization (SEO). Traditional SEO\noperates within a relatively transparent framework, where the ranking criteria\u2014such as keywords,\nmeta tags, and backlinks\u2014are explicitly defined. Manipulative practices, such as keyword stuffing,\nare generally easier to detect and address (Sharma et al., 2019). In contrast, ranking manipulation\nin LLM-based systems exploits the models' advanced contextual understanding and generative"}, {"title": "2 RELATED LITERATURE", "content": "Our work draws upon and connects several streams of research: (1) vulnerabilities and security\nchallenges in LLM, (2) ranking manipulation in LLM-based search engines, (3) applications of game\ntheory to security problems, and (4) strategic interactions in AI-driven markets. We discuss each of\nthese areas and position our contributions within the existing literature."}, {"title": "2.1 VULNERABILITIES IN LARGE LANGUAGE MODELS", "content": "The vulnerability of LLMs to adversarial inputs forms the technical foundation for ranking manip-\nulation attacks in LLM-based search engines. Early work by Wallace et al. (2019) demonstrated\nuniversal adversarial triggers could dramatically alter language models' outputs, establishing the\nbasis for exploiting these models' sensitivity to carefully crafted inputs. This sensitivity was further\nexplored by Zhu et al. (2023), who developed a comprehensive framework for evaluating LLM\nrobustness against various types of adversarial prompts. Perez et al. (2022) conducted red teaming of\nlanguage models at scale, revealing systematic vulnerabilities across different attack types.\nThe most prevalent method of executing an attack on large language models is through prompt\ninjection. Prompt injection attacks represent a substantial threat, involving carefully crafted malicious\ninputs designed to subtly but effectively manipulate the model's output (Yi et al., 2023). Building on\nthis idea, Greshake et al. (2023) investigated the domain of indirect prompt manipulation, demon-\nstrating how seemingly harmless inputs can be exploited to influence LLM behavior in unexpected\nways.\nRecent research has advanced increasingly sophisticated attack techniques. Jones et al. (2023)\nand Wen et al. (2024) demonstrated how gradient-informed optimization techniques can generate\nadversarial inputs that consistently bypass LLM safety measures. The transferability of these attacks\nacross different models, as shown by Zou et al. (2023), highlights the systemic nature of these\nvulnerabilities. Additionally, Wang et al. (2024) introduced reinforcement learning approaches"}, {"title": "2.2 RANKING MANIPULATION IN LLM-ENHANCED SEARCH", "content": "The integration of LLMs into search engines has introduced novel attack surfaces for ranking manipu-\nlation. Nestaas et al. (2024) provided one of the first comprehensive analyses of how adversaries can\nuse carefully crafted external content, such as website text or plugin documentation, to manipulate\nLLMs into promoting specific preferences or products, raising concerns about the security and reliabil-\nity of LLM-driven search engines. Pfrommer et al. (2024) extended this work by identifying specific\ncontent structures and semantic patterns that effectively influence LLM-based ranking systems.\nFurther advancements into efficient attack techniques were proposed by Aggarwal et al. (2024) and\nKumar & Lakkaraju (2024), who developed optimization-based techniques for manipulating LLM\npreferences in search contexts. These works demonstrate that ranking manipulation in LLM-based\nsystems differs fundamentally from traditional SEO, requiring more sophisticated approaches that\nexploit the models' deep language understanding and generation capabilities.\nWhile these studies establish the technical feasibility of ranking manipulation attacks, they primarily\nfocus on one-time individual attack instances. Our work complements this literature by examining\nthe long-term strategic dynamics when multiple attackers repeatedly interact within the market."}, {"title": "2.3 GAME THEORY IN SECURITY APPLICATIONS", "content": "Our game-theoretic approach builds upon established frameworks for analyzing security challenges.\nManshaei et al. (2013) provided a comprehensive foundation for applying game theory to security\nproblems, demonstrating how game theory modeling can illuminate attacker-defender dynamics.\nAlpcan & Ba\u015far (2010) provided foundational work on applying game theory to network security.\nKamhoua et al. (2021) extended these game theory principles along with machine leanring to cyber-\nsecurity systems, offering insights relevant to our analysis of LLM-based search engines.\nRecent works have examined specific security scenarios using game theory. Particularly relevant\nis work by Pawlick et al. (2019) on defensive deception strategies, which demonstrates how game\ntheory can model complex security interactions where attackers and defenders must reason about\neach other's strategies. Roy et al. (2010) further showed how game-theoretic models can capture the\nstrategic nature of security investments and defense mechanisms.\nOur work extends these approaches to the novel context of LLM-based search engines, incorporating\nunique elements such as stochastic attack success rates, LLM degradation under universal attacks,\nand the interconnected nature of multiple attackers' actions."}, {"title": "2.4 STRATEGIC INTERACTIONS IN AI-DRIVEN MARKETS", "content": "The competitive dynamics we model align closely with broader strategic interactions observed in\nAI-driven markets. For instance, Mikl\u00f3s-Thal & Tucker (2019) explored how improved demand\nforecasting from AI affects collusion sustainability. While better forecasts allow firms to tailor prices\nmore precisely, they also increase incentives to undercut rivals during high-demand periods. Similarly,\nCalvano et al. (2020) explored pricing algorithms using Q-learning and showed that such algorithms\ncan independently develop collusive strategies, sustaining supracompetitive prices without direct\ncommunication. Together, these studies reveal how AI disrupts traditional competitive paradigms,\nfostering novel forms of coordination in market environments.\nAnother area of interest lies in cooperation among self-interested agents. Bi et al. (2023) analyzed\npartnership formation in federated learning environments, demonstrating how to prompt sustainable\ncollaboration in repeated interactions. Complementing this, Banchio & Mantegazza (2023) proposed\na theoretical framework for collusion between adaptive learning algorithms, demonstrating how\nspontaneous coupling can lead to profitable coordination beyond static Nash equilibria.\nOne more relevant research direction is on humans' strategic reaction to AI adoption is crucial. Wang\net al. (2023) examined the impact of algorithmic transparency on firm and user surplus in markets\nwith strategic users, showing that while transparency can enhance a firm's predictive power and"}, {"title": "3 MODEL SETUP", "content": "We model the problem of ranking manipulation attacks on LLM-based search engines as an infinitely\nrepeated game, a common approach for studying scenarios where players interact over time and\nlearn from each other's actions (Abreu, 1988; Dal B\u00f3 & Fr\u00e9chette, 2018). In each time period\n(t = 1,2,3,...), two content providers (i = 1, 2) simultaneously decide whether to launch a ranking\nmanipulation attack. The problem takes the structure of an Infinitely Repeated Prisoners' Dilemma,\nwhere individuals face incentives to deviate from cooperation. In our context, the short-term incentive\nfor each player is to launch an attack to gain a temporary advantage over the competitors. Players\ndiscount future profits by a common discount rate \\( \\delta \\in (0, 1) \\), reflecting a preference for immediate\ngains over long-term returns.\nThe ranking manipulation attack against large language models (LLMs) is characterized by an attack\nsuccess rate (ASR), denoted as \\( p \\), which represents the probability that the attack will successfully\nachieve its goal-altering the model's response to elevate the rank of a targeted product. This\nconcept is a central measure in LLM safety research, where ASR is commonly used to evaluate the\neffectiveness of various adversarial attacks (Shayegani et al., 2023). The value of \\( p \\) is influenced by\nseveral factors, including the sophistication of the attack, the strength of the model's defenses, and\nthe resources invested by the attacker. Generally, more sophisticated attacks and greater resources\nlead to a higher \\( p \\), while robust defenses and countermeasures implemented by the LLM or search\nengine reduce the probability of success.\nWhen launching a preference manipulation attack, the player incurs a cost, denoted as \\( c \\). This cost\nrepresents the resources needed to develop, deploy, and maintain the manipulative content, such as\nresearch and development effort, data acquisition, and server resources. We examine different cost\nfunction forms, including constant cost, linear cost (\\( c \\times p \\)), and quadratic cost (\\( c \\times p^k, k > 1 \\)). The\nlatter one reflects the increasing difficulty and diminishing return of developing more effective attacks.\nThis cost structure suggests that attackers must balance the potential gains from successful attacks\nagainst the resources required to implement them.\nFor the market, in each period, there is one unit mass of potential consumers, with total demand\nnormalized to 1. The goods supplied by the providers are perfect substitutes. This assumption\nsimplifies the analysis while capturing the competition for market share between the players. It allows\nus to focus on how the division of market share is influenced by the players' strategic choices.\nEach player can observe the actions of the other at the end of each period, including whether they\nchoose to cooperate or attack. This forms a perfect monitoring game, a standard assumption in\nrepeated game theory, which enables players to make strategic adjustments based on the observed\nbehaviors of others (Fudenberg, 1991)."}, {"title": "3.1 PAYOFF STRUCTURE", "content": "The strategic interactions between the players are represented by a payoff matrix, structured as a\nPrisoners' Dilemma, as summarized in Table 1. Within each cell of the table, the first element denotes\nthe payoff of player 1, and the second element denotes that of player 2. The payoff values depend on\nwhether each player chooses to cooperate or launch an attack:\n\u2022 R = \\( \\frac{1}{2} \\) (Mutual Cooperation): If both players refrain from launching attacks, they equally\nshare the market demand, resulting in a payoff of \\( \\frac{1}{2} \\) each.\n\u2022 T = \\( p + (1 - p) \\frac{1}{2} - c \\) (Temptation Payoff): When one player launches an attack while the\nother cooperates, the attacker potentially captures the entire market if the attack is successful\nwith probability \\( p \\). If the attack fails (with probability \\( 1 - p \\)), the market demand is split\nevenly, but the attacker still bears the cost \\( c \\) of launching the attack.\n\u2022 S = \\( (1-p) \\times 0 \\) (Sucker Payoff): If a player cooperates while the other attacks, the cooperator\nretains some market share only if the attack fails. Otherwise, the cooperator loses all market\nshare.\n\u2022 Q = \\( p^2 \\beta + p(1 - p) + (1 - p)^2 \\frac{1}{2} - c \\) (Mutual Attack): If both players launch attacks, the\noutcomes depend on the success rate of the attacks. (1) If both attacks succeed (probability\n\\( p^2 \\)), they equally share the market but at a degraded value due to reduced LLM output quality,\nrepresented by \\( \\beta < 1 \\). A smaller \\( \\beta \\) indicates a larger degradation. (2) If only one player\nsucceeds in attacking (probability \\( p(1 - p) \\)), that player monopolizes the market. (3) If both\nattacks fail, the market is split evenly. The condition \\( \\beta < 1 \\), which represents the degraded\noutput quality when both parties launch attacks, aligns with empirical findings showing that\nwhen all parties engage in ranking manipulation attacks, it results in detrimental outcomes\nfor everyone involved (Nestaas et al., 2024).\nThe condition \\( T > R > Q > S \\) preserves the structure of a Prisoner's Dilemma, indicating that while\nmutual cooperation is preferable, individual incentives drive the players to defect. This condition\nholds when \\( c < p + (\\beta - 1)p^2 \\), ensuring that the temptation to attack outweighs the costs but\nresults in a worse outcome for both when both attack."}, {"title": "4 ANALYSIS", "content": "In this section, we analyze the Infinitely Repeated Prisoners' Dilemma (IRPD) model to examine\nthe conditions under which cooperation can be sustained among players. We answer key questions\nregarding what factors influence the sustainability of cooperation, and how varying parameters such\nas attack costs, attack success rates, and future profits discount rates affect the decision to either co-\noperate or defect. This analysis provides insights into the mechanisms that can encourage cooperation\nand mitigate ranking manipulation attacks in LLM-driven information retrieval systems.\nWe consider the grim trigger strategy, a classic trigger strategy in repeated games where players\ncooperate until one defects; after a defection, all players respond by always defecting. This strategy\nserves as a baseline for understanding how cooperation might be enforced in an environment where\ndeviations are possible. It creates a strong deterrent for initial defection since any deviation leads to\npermanent mutual defection, which can degrade outcomes for all parties involved. In Section 5, we\nextend our analysis to alternative trigger strategies.\nWe first derive the discounted payoffs for continuous cooperation \\( V(C) \\) and for a one-time defection\nfollowed by mutual defection \\( V(D) \\). If both players always cooperate, the discounted payoff for\neach player is:\n\\begin{equation}\nV(C) = R + \\delta R + \\delta^2 R + \\dots = \\frac{R}{1 - \\delta}\n\\end{equation}"}, {"title": "4.1 CONDITION FOR COOPERATION", "content": "For cooperation to be a rational strategy for each player, the payoff from continuous cooperation\nmust be at least as high as the payoff from defection followed by mutual defection \\( V(C) \\geq V(D) \\).\nTheorem 1 provides a summary of the result.\nTheorem 1 (Cooperation Condition). Two players prefer long-term cooperation over engaging in\nranking manipulation attacks if and only if:\n\\begin{equation}\n\\delta > \\delta^* = \\frac{T-R}{T-Q} = \\frac{p-2c}{p - \\beta p^2 + p^2}\n\\end{equation}\nwhere \\( \\delta^* \\) is the critical discount factor.\nThe discount factor reflects how much players value future profits relative to immediate gains. Higher\nvalues of \\( \\delta \\) imply that players are more forward-looking and are thus more willing to cooperate\nbecause attacking now would mean losing significant future profits. Conversely, a lower \\( \\delta \\) makes\nattacking more appealing, as players prioritize immediate rewards gained from ranking attacks\nover long-term benefits. Cooperation is only viable if \\( \\delta > \\delta^* \\), emphasizing the need for creating\nenvironments where long-term outcomes are valued.\nProof. For cooperation to be sustainable, we need \\( V(C) \\geq V(D) \\), which is\n\\begin{equation}\n\\frac{R}{1 - \\delta} > T + \\frac{\\delta Q}{1 - \\delta}\n\\end{equation}\nReorganizing this inequality gives the critical condition for sustaining cooperation:\n\\begin{equation}\n\\delta \\geq \\frac{T-R}{T-Q} \\overset{\\text{def}}{=} \\delta^*\n\\end{equation}\nwhere \\( \\delta^* \\) is the critical discount factor. This threshold represents the minimum value of \\( \\delta \\) required for\nplayers to prioritize long-term cooperation over the short-term gains from defection.\nLet's expand this condition using our given payoff functions:\n\\begin{equation}\nT - R = p + (1 - p) \\left( \\frac{1}{2} \\right) - \\frac{1}{2} - c\n\\end{equation}\n\\begin{equation}\nT - Q = \\left( p + (1 - p) \\frac{1}{2} - c \\right) - \\left( p^2 \\beta + p(1 - p) + (1 - p)^2 \\frac{1}{2} - c \\right)\n\\end{equation}\n\\begin{equation}\n= \\frac{1 + p}{2} - \\frac{p^2 \\beta + p(1 - p) + (1 - p)^2 \\frac{1}{2}}{2}\n\\end{equation}\n\\begin{equation}\n= \\frac{1 + p}{2} - p^2 - \\beta p - \\frac{1}{2} p - \\frac{1}{2} + \\frac{1}{2} p^2\n\\end{equation}\n\\begin{equation}\n= - \\frac{p^2}{2} \\beta + \\frac{p^2}{2} + p\n\\end{equation}\nTherefore, the players prefer cooperation over launching an attack when:\n\\begin{equation}\n\\delta \\geq \\delta^* = \\frac{p-2c}{-\\frac{p^2}{2} \\beta + \\frac{p^2}{2} + p} = \\frac{p-2c}{p - p^2 \\beta + p^2}\n\\end{equation}"}, {"title": "4.2 COOPERATION FORMATION REGION", "content": "Due to the lack of a closed-form solution for the cooperation formation condition in terms of \\( p \\),\nwe employ numerical visualization to investigate the parameter space where cooperation can be\nsustained.\nFigure 1 visualizes the regions of the \\( \\delta \\) (discount factor) and \\( p \\) (attack success probability) space that\nresults in long-term cooperation under various values of \\( \\beta \\) and different cost functions. The region to\nthe right of the curve (the blue region) is where \\( \\delta > \\frac{p-2c}{p - \\beta p^2 + p^2} \\), i.e., the cooperation formation region.\nWe can derive several key observations from these plots.\nFirst, across all cost functions, the region where cooperation is possible tends to shrink as \\( \\beta \\) increases.\nThis is evident as the blue regions, where the inequality condition is satisfied, become smaller moving\nfrom the first row (where \\( \\beta = 0.2 \\)) to the fourth row (where \\( \\beta = 0.8 \\)). The decrease in the size of\nthe cooperation region with increasing \\( \\beta \\) can be attributed to the increasing sensitivity of the players\nto defection payoffs. As \\( \\beta \\) increases, the loss of degradation decreases, therefore the incentive for\ndefection grows, thereby requiring higher values of \\( \\delta \\) to sustain cooperation."}, {"title": "4.3 PAYOFF ANALYSIS OF COOPERATION AND DEFECTION IN LLM SYSTEMS", "content": "In this section", "Evidence": "In Figure 2", "success.\nExplanation": "This counter-intuitive outcome arises from the interplay between immediate and long-\nterm payoffs. A decrease in \\( p \\) lowers the immediate rewards from a successful attack, but it also\nreduces the attack cost and the long-term risks associated with mutual defection. As a result, the\noverall reduction in long-term risk and cost can outweigh the diminished short-term benefit, making\nd"}]}