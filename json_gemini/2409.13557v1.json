{"title": "Trustworthy Hate Speech Detection Through Visual Augmentation", "authors": ["Ziyuan Yang", "Ming Yan", "Yingyu Chen", "Hui Wang", "Zexin Lu", "Yi Zhang"], "abstract": "The surge of hate speech on social media platforms poses a significant challenge, with hate speech detection (HSD) becoming increasingly critical. Current HSD methods focus on enriching contextual information to enhance detection performance, but they overlook the inherent uncertainty of hate speech. We propose a novel HSD method, named trustworthy hate speech detection method through visual augmentation (TrusV-HSD), which enhances semantic information through integration with diffused visual images and mitigates uncertainty with trustworthy loss. TrusV-HSD learns semantic representations by effectively extracting trustworthy information through multi-modal connections without paired data. Our experiments on public HSD datasets demonstrate the effectiveness of TrusV-HSD, showing remarkable improvements over conventional methods.", "sections": [{"title": "1 Introduction", "content": "The widespread use of social media has both facilitated freedom of individual expression and intensified concerns over the rise of hate speech, emphasizing the urgent need for effective automatic hate speech detection (HSD) methods (Rawat et al., 2024; Fortuna et al., 2022). Typically, hate speech mostly appears in short text with very limited contextual information, which increases the detection difficulty (Li et al., 2021). Previous HSD methods mostly focused on designing advanced feature representations to improve detection performance: kernel-based methods (Gamb\u00e4ck and Sikdar, 2017; Abro et al., 2020), pre-trained language models (Sarkar et al., 2021; Daouadi et al., 2023), graph attention networks (Mishra et al., 2019; Miao et al., 2024). All these methods solely extract features from textual speech (see Figure 1(a)), which constrained the capability of detection model under limited textual information.\nAnother stream of HSD methods (see Figure 1(b)) seeks to enrich feature representation"}, {"title": "2 Methodology", "content": "Figure 2 shows the overview of our method, which contains visual cues generation module, hate speech detector and trustworthy loss. TrusV-HSD will be detailed introduced in the following.\nVisual Cues Generation Module The traditional HSD paradigm can be formulated as $F(X_{txt}) \\rightarrow Y$, where $X_{txt}$ is the textual speech and Y is the prediction. This paradigm only extract features from textual modality, and its effectivenss is limited by the short speech length. To mitigate this issue, we propose a novel HSD paradigm that generates visual cues to complement the textual data. Notably, differ with previous multimodal HSD methods, our method is free from image-text paired data, ensuring higher flexibility and broader applicability. Our paradigm is formulated as $F(X_{txt},G(X_{txt})) \\rightarrow Y$, where G is the generator that produces imagery from the speech. Concretely, a image generator (Rombach et al., 2022) is utilized to achieve the above purpose, which consists of a text encoder, a diffusion model, and an autoencoder. The generated imagery is not merely illustrative but also interprets the speech and presents certain metaphors explicitly. Consequently, TrusV-HSD enriches information through visual augmentation, allowing it to learn concrete image concepts"}, {"title": "Hate Speech Detector", "content": "To efficiently model textual hate speech and its visual agumentation, our TrusV-HSD incorporates a lightweight hate speech detector. This design alleviates the data-intensive reliance and circumvents the needs for fine-tuning heavyweight vison-language backbones, making it ideal for the low-data multimodal HSD. Additionally, our proposed detector aligns long-range multimodal features while maintaining linear computational complexity.\nSpecifically, we embed the text and image modalities to get $f_{t\u00e6t} \\in \\mathbb{R}^{1\\times768}$ and $f_{img} \\in \\mathbb{R}^{1\\times512}$ using the pretrained fBERT (Sarkar et al., 2021) and CLIP (Radford et al., 2021), respectively. Then, a connection block is designed to connect those two embeddings, represented $f_{alg}^{tart}$ and $f_{alg} \\in \\mathbb{R}^{1\\times M}$, where M is the alignment dimension. To efficient align multimodal features, we use a Squeeze-and-Excitation Layer (Hu et al., 2018) to encourage attentive learning across multimodal embeddings in TrusV-HSD.\nSimillarly, we also introduce the state space sequence model (SSM) (Gu and Dao, 2024) in this block, which maps the multimodal feature into a sequence $x_{t} \\rightarrow y_{t}$ through a hidden state $h_{t} \\in \\mathbb{R}^{N}$, where t denotes the timestep and N represents the state size. A represents the evolution parameter, B, C denote the projection parameters. The whole process of SSM is formulated as follows:\n$h_{t} = \\bar{A}h_{t-1} + Bx_{t}$,\n$Y_{t} = Cx_{t}$,\nwhere A and B are the discrete paramters. Based on the zero-order hold method (Zhu et al., 2024), $\\bar{A} = exp(\\Delta A)$ and $B = (\\Delta A)^{-1}exp(\\Delta A - I) \\cdot \\Delta B$, and $\\Delta$ denotes the timescale parameter.\nLastly, our hate speech detector delivers the output through a global convolution operation, expressed as $g = \\alpha * K$. $K ="}, {"title": "Trustworthy Loss", "content": "$(CB, \\bar{CB}, CAB,..., CA^{\\text{-T-1-}}B)$ is a casual convolutional kernel, and T denotes the input length.\nDue to subjective or biased factors of speakers and domain independence among individual speeches, it is critical need for network reliability against uncertainties to ensure trustworthy detections. Inspired by Sensoy et al., 2018 and Qin et al., 2022, we introduce a trustworthy loss to emphasize highly reliable predictions while weakening the impact of unreliable ones. More discussions can be found in Appendix A.2.\nGiven an input $x_{i} = {X_{txt}, X_{img}}$, and the evidence vector $e_{i}$ is obtained through the network M, formulated as $e_{i} = M(x_{i})$, where $e_{i} \\geq 0$. Following the principle of Subjective Logic (SL) (Jsang, 2018) and the uncertainty quantification can be written as $u_{i} + \\sum_{k=1}^{K} b_{i} = 1$, where K is the number of classes, $b_{i}$ is the belief mass and $b_{i} = \\frac{e_{i}}{\\mathbb{S}_{i}}$, $u_{i}$ is the uncertainty quantification and $u_{i} = \\frac{K}{\\mathbb{S}_{i}}$, where $S_{i} = \\sum_{k=1}^{K}(e_{i} + 1)$. The belief mass assignment corresponds to a Dirichlet distribution with parameters $\\alpha_{i}$, where $\\alpha_{i} = e_{i} + 1$. Then, we reformulatethe cross-entropy loss as:\n$L_{digamma} = \\sum_{k=1}^{K} Y_{i} (\\psi (S_{i}) - \\psi (\\alpha_{i}))$,\nwhere $\\psi (\\cdot)$ is the digamma function. Moreover, Kullback-Leibler (KL) divergence is incorporated to penalize the divergence from negative samples, which can be defined as:\n$L_{KL} = KL [D (p_{i} | \\tilde{\\alpha_{i}}) ||D (p_{i} |1)]$,\nwhere $p_{i}$ is the classification probability and $p_{i} = \\frac{e_{i}}{\\mathbb{S}_{i}} $.D($p_{i} |1$) is the uniform Dirichlet distribution, and $\\tilde{\\alpha_{i}} = Y_{i} + (1 - Y_{i}) \\odot e_{i}$. $L_{KL}$ encourages the model to give negative samples higher uncertainty. Finally, the overall loss can be formulated as:\n$L_{trust} = L_{digamma} + \\lambda L_{KL}$,\nwhere $\\lambda$ is a annealing coefficient, which is set to $\\chi (t) = min(1, \\frac{t}{c})$. c is current epoch number, and C is the total training epoch number."}, {"title": "3 Experiments", "content": "Experimental Environment TrusV-HSD is implemented on PyTorch, optimized by Adam (Kingma and Ba, 2017) with a learning rate of 0.001. The training epoch is set to 800. The experimental environment includes an AMD Ryzen 7 5800X CPU and four NVIDIA GTX 3080 Ti GPUs."}, {"title": "Datasets and Metrics", "content": "Two public explicit HSD datasets are used to validate the proposed method, including SemEval2019 task-5 (SE) (Basile et al., 2019) and Fox News User Comments (FNUC) (Gao and Huang, 2017) datasets. We also validate our method in the implicit HSD task using the public Implicit Hate Speech (IHC) (ElSherief et al., 2021) dataset. SE, FUNC and IHC contain 12,000, 1,528 and 22,056 samples, respectively. We use 10-fold cross-validation to validate the methods. Due to serious sample imbalance, we use F1, Precision, and Recall as evaluation metrics. The average results and standard deviations are presented for each experiment."}, {"title": "Explicit Hate Speech Detection", "content": "The results of different methods on SE and FNUC are presented in Table 4. Benifiting from integration of visual cues and trustworthy loss, our TrusV-HSD achieves the best F1 scores in the comparisons with other methods. Our TrusV-HSD also achieves optimal or near-optimal precision and recall, with notable improvements in SE dataset. In FNUC dataset, TrusV-HSD exhibits superior F1 performance, along with a significant increase in recall (+2.80%), although the precision is slightly lower (-0.78%) compared to RSGNN. This variation is likely due to the small number of samples in FNUC. Besides, our method outperforms AADT-NLP, a multi-modal HSD method, demonstrating the effectiveness of our proposed detector and the trustworthy loss."}, {"title": "Implicit Hate Speech Detection", "content": "The above experiments validate our method's effectiveness in detecting explicit hate speech. Implicit hate speech, masked by subtle tricks, is more challenging to detect. To assess our method comprehensively, we also evaluate its performance in implicit hate speech detection task, focusing on reasoning ability and robustness (see Table 2). Due to severe data imbalance in IHC dataset, BERT tends to predict samples as positive, resulting in high precision but low recall. Despite this, our significant F1 score improvement demonstrates TrusV-HSD's superiority in large-scale datasets by effectively learning connections between modalities."}, {"title": "Ablation Study", "content": "The ablation study is tested in SE dataset, and the results are shown in Table 3, where \"Detector\u2020\" and \"Detector\u2021\" denote our detector without and with the connection block, respectively\u00b9. We treat fBERT as the baseline, an significant improvement can be observed when im-"}, {"title": "Visualization Examples", "content": "Several visualized samples are shown in Figure 4, it can be noticed that our generation module enrich semantic information by generating concrete imagery from textual speech. Besides, the generated images could present certain metaphors explicitly, as shown in the final sample of Figure 4. Thereby, the imagery cues help the model learn concrete image concepts alongside abstract textual knowledge."}, {"title": "4 Conclusion", "content": "In this paper, we propose a novel trustworthy hate speech detection method via visual augmentation. Concretely, our method can generate imagery cues to enrich information from brief speech, and the designed lightweight detector can effectively alleviate the data-intensive reliance issue. Finally, the trustworthy loss enhances robustness and ensures reliable predictions across diverse speech lengths, alleviating the domain independence problem. In the future work, one possible direction is modeling user-specific features to detect hate speech."}, {"title": "5 Limitations", "content": "While our proposed TrusV-HSD shows promising results in both explicit and implicit hate speech detection tasks, it is not without limitations.\nComputational Overhead: The process of generating images from the speech adds computational complexity and time.\nPotential Risk: Like many AI systems, TrusV-HSD could be susceptible to adversarial attacks designed to manipulate its outcomes.\nLanguage Limitation: Our method has only been tested on English speech. Its effectiveness and accuracy in detecting hate speech in other languages remain unverified, which may limit its applicability in multilingual settings.\nBias and Fairness: While the trustworthy loss aims to reduce subjectivity and bias, it may not completely eliminate them. One possible solution is to provide the speaker's prior, but related dataset is not collected yet."}, {"title": "6 Ethical Considerations", "content": "In this study, we emphasize that all datasets used are publicly available and have been sourced from established repositories. The content within these datasets, including any hate speech examples, does not reflect the views or opinions of the authors. Our research aims to advance the field of hate speech detection for the benefit of the community, and the use of such data is solely for academic and research purposes. Any offensive or harmful content present in the datasets is used only to develop and test our detection models, and we do not endorse or support any of the sentiments expressed in the data.\nThe image generator was primarily trained on subsets of LAION-2B (Schuhmann et al., 2022), which consists mainly of images with English descriptions. Consequently, texts and images from non-English communities and cultures are underrepresented, often defaulting to white and western perspectives. This limitation affects the model's ability to generate content with non-English prompts, producing significantly poorer results. Stable Diffusion v2 amplifies these biases to a degree that viewer discretion is advised, regardless of the input or its intent."}, {"title": "A.1 Motivation", "content": "Our main motivation is to introduce a novel learning paradigm for the hate speech detection task by leveraging feature learning from both text and vision modalities simultaneously, rather than proposing a specific framework. This approach differs from previous paradigms that rely solely on the text modality. Additionally, our learning paradigm does not require paired text-image data, ensuring higher flexibility and broader applicability.\nWe also want to highlight that all the pre-trained feature extraction components in the proposed TrusV-HSD are pluggable and replaceable. In other words, the performance of our method could be further improved with more advanced text or visual pre-trained backbones in the future."}, {"title": "A.2 Trustworthy Loss", "content": "Different speech lengths can lead to various distributions, causing the model to struggle with this issue. A similar problem also occurs with other parameters, such as for people of different regions or genders. If the test distribution differs from the training distribution (e.g., different races, genders, religions), it results in a typical out-of-distribution (OOD) problem. Numerous studies (Sensoy et al., 2018; Chen et al., 2024) have demonstrated that the trustworthy loss can effectively handle this problem by replacing this parameter set with the parameters of a Dirichlet density. The trustworthy loss transforms the model's predictions into a distribution over possible softmax outputs, rather than a point estimate of a softmax output. In this way, our model could measure that if the test data is in or out of the distribution.\nSpecifically, the traditional classification loss classifies the test data into known classes based on the prior knowledge of the training data. However, if the test and training data follow different distributions, this process may lead to misclassification. The trustworthy loss can effectively alleviate this problem by quantifying the uncertainty (Li et al., 2022). This approach allows our model to reduce"}, {"title": "A.3 Experiment Setting", "content": "In our paper, 10-fold cross-validation is used to evaluate the robustness and performance of our model comprehensively. This way reduces the variance associated with random partitioning of the data and provides a more thorough assessment of our model's performance across different subsets, leading to more reliable and stable results. Meanwhile, 10-fold cross-validation is a classical validation protocol in hate speech detection (Song et al., 2022), so we chose to validate both our method and the compared methods using this approach.\nIn the ablation study, our multi-modal baseline has linear layers (without Connection block and Mamba block) after the feature concatenation operation. \"Detector\u2020\" refers to replacing one linear layer with a Mamba Block but without the connection block. \"Detector\u2021\" refers to use both Mamba and connection blocks. \"Detector\u2020\" and \"Detector\u2021\" are optimized with cross-entropy loss, not the trustworthy loss."}, {"title": "A.4 Generation Experiment", "content": "To evaluate the generalization performance and illustrate that our proposed method is a pretrained backbone-free method, we evaluated our methods with different backbones: TrusV-HSD# (with BERT) and TrusV-HSD* (with fBERT). The results, as shown in Table 1, clearly indicate that our"}]}