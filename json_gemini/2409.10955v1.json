{"title": "Investigating Context-Faithfulness in Large Language Models: The Roles of Memory Strength and Evidence Style", "authors": ["Yuepei Li", "Kang Zhou", "Qiao Qiao", "Bach Nguyen", "Qing Wang", "Qi Li"], "abstract": "Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by incorporating external information into the response generation process. However, how context-faithful LLMs are and what factors influence LLMs' context-faithfulness remain largely unexplored. In this study, we investigate the impact of memory strength and evidence presentation on LLMs' receptiveness to external evidence. We introduce a method to quantify the memory strength of LLMs by measuring the divergence in LLMs' responses to different paraphrases of the same question, which is not considered by previous works. We also generate evidence in various styles to evaluate the effects of evidence in different styles. Two datasets are used for evaluation: Natural Questions (NQ) with popular questions and popQA featuring long-tail questions. Our results show that for questions with high memory strength, LLMs are more likely to rely on internal memory, particularly for larger LLMs such as GPT-4. On the other hand, presenting paraphrased evidence significantly increases LLMs' receptiveness compared to simple repetition or adding details.", "sections": [{"title": "1 Introduction", "content": "Retrieval-Augmented Generation (RAG) (Fan et al., 2024; Zhao et al., 2023) has gained increasing popularity as it improves the performance of Large Language Models (LLMs) by integrating external information during the generation process, particularly when the model's internal knowledge is insufficient or outdated (Bianchini et al., 2024; Procko, 2024; Siriwardhana et al., 2023; Vakayil et al., 2024; Wang et al., 2024; Jeong, 2023). It raises the importance of the study of how context-faithful LLMs are, as RAG relies on effectively incorporating retrieved external information into the generated responses to ensure accuracy and contextual relevance. In this study, we explore whether LLMs are context-faithful when encountering external information, particularly when that information conflicts with the LLMs' internal memory.\nTo investigate the issue of context-faithfulness, one approach (Longpre et al., 2021; Chen et al., 2022) is to create conflicts through entity substitution. Longpre et al. (2021) concluded that models frequently rely on their parametric knowledge, generating responses not found in the provided evidence. In contrast, Chen et al. (2022) found that LLMs rely almost exclusively on evidence passages rather than internal memory when multiple evidence passages are presented. Another approach (Xie et al., 2024; Jin et al., 2024) involves generating counter-memory evidence with LLMs, and these studies have shown that LLMs are generally receptive to external evidence as long as it is coherent.\nThese methods, however, overlook some important aspects of the task. On one hand, LLMs have different memory strengths for different knowledge, and their behavior towards external evidence may vary significantly depending on how strongly it resistant to that knowledge. On the other hand, the characteristics of the evidence, such as evidence length, expressions, and detailed information involved, can influence how persuasive the evidence is. The behavior of LLMs is also expected to vary with evidence of different persuasiveness. Overlooking these factors could result in conflicting conclusions.\nTo address these issues, we introduce a method to quantify the memory strength of LLMs. We also generate evidence in various styles to evaluate the different effects of evidence in different styles. Inspired by Zhao et al. (2024), we assess memory strength by measuring the divergence in LLMs' responses to different paraphrases of the same question. Intuitively, an LLM demonstrates high memory strength when it consistently provides the same answer across all paraphrased ver-"}, {"title": "2 Related Work", "content": "2.1 Context Faithfulness of LLM\nTo update static factual knowledge (Lazaridou et al., 2021; Karpukhin et al., 2020; Kasai et al., 2023) in LLMs, the retrieval-based method has been introduced to involve external information to LLMS (Lazaridou et al., 2022; Izacard et al., 2024; Khattab et al., 2022; Santhanam et al., 2022; Gao and Callan, 2022). However, these methods can introduce knowledge conflicts between the introduced context and pre-existing memories from LLMs.\nLLMs often persist in relying on their pre-existing memories, overlooking newly provided contextual evidence (Longpre et al., 2021). To address this, recent studies (Neeman et al., 2023; Li et al., 2023) fine-tune LLMs on counterfactual contexts, where the original facts are replaced with counterfactual ones. Another work (Zhou et al., 2023) proposes a novel approach using prompting to improve context faithfulness in LLMs without additional fine-tuning.\nA related area of research focuses on prediction with abstention. Neeman et al. (2023); Zhou et al. (2024) introduced the concept of answerability augmentation, where LLMs are trained to respond with \"Unanswerable\" when presented with irrelevant or randomly generated contexts. This ensures that the models do not make incorrect predictions in the absence of reliable evidence. Further studies (Wang et al., 2023, 2022) have developed confidence calibration techniques, which encourage LLMs to avoid overly confident predictions in ambiguous or uncertain situations.\nIn our work, we investigate the context-faithfulness of LLMs when faced with conflicting knowledge. We define a model as context-faithful if it demonstrates strong receptiveness to new facts and evidence, adapting its responses accordingly. This capability is essential for ensuring the reliability of LLMs in the RAG system.\n2.2 Construction of Knowledge Conflicts\nIn controlled experiments, knowledge conflicts are typically simulated by constructing counterfactual memories based on a model's parametric memory. Various heuristic approaches have been proposed for this purpose, such as negation injection (Kassner et al., 2021; Petroni et al., 2020; Pan et al., 2021), which alters facts by introducing negations, and entity substitution (Longpre et al., 2021; Chen et al., 2022; Si et al., 2023; Zhou et al., 2023), which replaces mentions or entities in the parametric memory with alternatives to generate counter-memory answers. However, these techniques are constrained to word-level edits, which can lead to low coherence across the constructed counter-memory. To address this limitation, recent studies (Xie et al., 2024; Jin et al., 2024) have explored generating counter-memories using LLMs, producing more coherent and consistent counterfactual content. We adopt this approach in generating our dataset, ensuring the counter-memories maintain a higher level of coherence."}, {"title": "3 Methodology", "content": "3.1 Problem Definition\nFollowing prior work (Longpre et al., 2021; Chen et al., 2022; Xie et al., 2024), we adopt question answering (QA) task as the testbed for knowledge conflict experiments. For a given question Q, if the answer generated by the LLM relies solely on its internal parameters, it is referred to as the memory answer (MA). If an evidence passage E is provided with question Q, then ideally, LLM should generate an answer based on E, even if E conflicts with memory answers. We call the answers that conflict with MA as counter memory answer (CMA).\n3.2 Datasets\nWe use two datasets for our experiments: the long-tail, entity-based QA dataset popQA, and the popular, human-written question dataset Natural Questions (NQ). Specifically:\n\u2022 popQA (Mallen et al., 2023) is an entity-centric question-answering dataset comprising 14,000 questions. The dataset is derived from knowledge triples in Wikidata, where questions are generated using question templates specific to different relationship types. popQA aims to capture a realistic, long-tail distribution of entity popularity, making it a valuable resource for studying the performance of lesser-known entities. Xie et al. (2024) use popQA to test the receptiveness of LLMs by eliciting high-quality parametric memory from LLMs and constructing the corresponding counter-memory. We reuse MA and CMA generated by Xie et al. (2024) for our experiments.\n\u2022 Natural Questions (Kwiatkowski et al., 2019) is widely used in open-domain QA research. It consists of manually crafted questions based on selected paragraphs from Wikipedia, and the subjects in questions of the NQ dataset are generally more popular and commonly known. Longpre et al. (2021) provide a test set that is used to test the context-faithfulness of LLMs by substituting entity of the NQ dataset. The entity substitute involves five categories: person (PER), date (DAT), numeric (NUM), organization (ORG), and location (LOC). The test set contains 4,685 samples, including 1,667 unique questions.\n3.3 Memory Strength\nInspired by Zhao et al. (2024), we use the consistency of answers to different paraphrases of the same question Q to measure the LLM's memory strength $S_Q$ for the knowledge $K_Q$ associated with the question. This method is motivated by the intuition that if an LLM does not have a strong memory of a question, it often produces varying answers when presented with different paraphrases but semantically equivalent questions, as shown in Table 8 in Appendix. In contrast, it can produce consistent answers if the LLM has a strong memory of a question. The process involves two key steps: First, several paraphrased versions of the original question are generated with ChatGPT\u00b9, and the answers to those paraphrased questions are clustered (Section 3.3.1). Then, memory strength $S_Q$ is calculated using answer consistency (Section 3.3.2).\n3.3.1 Question Paraphrases and Answer Clustering\nThe prompt used for paraphrasing the question is provided in Tabel 9 (index 1) in the Appendix. For each question Q, we generate n paraphrases {P1,\u2026\u2026,Pn}Q. For the NQ dataset, we paraphrase the question in each data sample directly. For the popQA dataset, we paraphrase the question template for each relation type since all questions of the same relation type share the same question template. To ensure the paraphrased questions are proper to use, we check if two paraphrased questions are semantically equivalent with an LLM\u00b2.\nNext, LLMs answer the paraphrased questions {P1,\u2026, Pn}Q in a closed-book setting. We denote the answers as {A1,\u2026, An}Q. The answers are grouped into several clusters based on their consistency. The clustering is done by checking answers incrementally. If an answer matches any answer within an existing cluster, this answer is added to this cluster; if not, a new cluster is created with this answer. We use an LLM\u00b2 to determine whether two answers are consistent. The prompt used for this answer inconsistency detection is shown in Table 9 (index 3). We denote the clusters for question Q as {C1,\u2026\u2026, Cm}Q."}, {"title": "3.3.2 Calculating Memory Strength", "content": "Once answer clusters {C1,\u2026\u2026, Cm}Q are identified, memory strength S(Q) can be obtained by calculating the entropy of cluster distribution. The formula is\n$S(Q) = \\sum_{i=1}^{m} \\frac{N(c_i)}{n} -log\\frac{N(c_i)}{n}$  (1)\nwhere $N(c_i)$ is the number of answers in the cluster $c_i$, and n is the number of paraphrases for question Q. In the experiments, we set n = 7 for all the questions in the NQ and popQA datasets. Memory strength reflects how well the LLM remembers the required knowledge: the weaker the memory, the more random and inconsistent the answers are."}, {"title": "3.4 \u041c\u0410, \u0421MA, and Evidence Generation", "content": "3.4.1 MA and CMA Generation\nFor the popQA dataset, both MA and CMA are obtained following the method described in Xie et al. (2024). For the MA of the NQ dataset, we also use a closed-book approach, similar to Xie et al. (2024). While, the process for generating CMA differs. Unlike the popQA dataset, the NQ dataset does not provide relation types for the questions or offer sets of subject and object entities for substitution. To address this issue, we propose an approach using an LLM to substitute entities in MA to generate CMA. First, we identify which \u201cwh-\u201d question type\u00b3 the question belongs to using string matching. Then, based on the question type, we determine the type of entity to be replaced in the MA. Finally, we use\n3.4.2 Evidence Generation\nIn this section, we explain how to generate different styles of evidence. We classify evidence into two categories: direct evidence and indirect evidence.\nDirect evidence is a semantically equivalent statement of the CMA, providing the clearest support for the claim made by the CMA. We generate the direct evidence by paraphrasing the CMA with ChatGPT, following the prompt shown in Table 9 (index 6). For example, in Figure 1, the CMA \"there are 15 episodes in Chicago Fire season 4\u201d is paraphrased to \u201cseason 4 of Chicago Fire consists of a total of 15 episodes\u201d. These two statements"}, {"title": "4 Experiments", "content": "In this study, we aim to investigate two key research questions. 1) Whether memory strength has an impact on the context faithfulness of LLMs. 2) If the style of evidence affects the context faithfulness of LLMs. These research questions are explored in Section 4.2 and 4.3, respectively. We also provide additional studies in Appendix A, which includes a study about the impact of option order and a case study.\n4.1 Experiment Setup\nLLM Models. Our experiments were conducted using four well-known language models: ChatGPT (OpenAI, 2023a), GPT-4 (OpenAI, 2023b),\nEvaluation Metrics. Following previous work (Longpre et al., 2021; Xie et al., 2024; Chen et al., 2022), we transform the free-form QA to a multiple-choice QA format by providing a few options as possible answers. This limits the generation space and helps determine the answer provided by LLMs with certainty. Specifically, for each question from both datasets, LLMs are instructed to select one answer from MA, CMA, and \"Uncertain\" (UCT). We report the ratio of MA, CMA, and UCT, and the formulas are\n$R_m = \\frac{f_m}{f_m + f_c + f_u}$\n$R_c = \\frac{f_c}{f_m + f_c + f_u}$\n$R_u = \\frac{f_u}{f_m + f_c + f_u}$ (2)\nwhere fm, fc, fu are the count of questions with MA, CMA, and UCT answers, respectively."}, {"title": "4.2 Impact of Memory Strength", "content": "4.2.1 Memory Strength on Different Datasets\nWe first illustrate the distributions of memory strength across the popQA and NQ datasets for LLaMA2.7B, LLaMA2.70B, ChatGPT, and GPT-4, respectively (shown in Figure 2). From the analysis, two key insights can be drawn.\nLLMs demonstrate stronger memory for the NQ dataset than the popQA dataset. For the NQ dataset, most questions fall within the bin of (0.25, 0]. Only a few questions fall within bins of weaker memory strength. In contrast, the popQA dataset has a greater number of questions in bins with\n4.2.2 Context-Faithfulness with Memory Strength\nTo demonstrate the relationship between context-faithfulness and memory strength, we categorize the questions in each dataset into four groups according to memory strength for each LLM. The four groups are low, mid-low, mid-high, and high, corresponding to the memory strength intervals [-2, -1], (-1, -0.5], (-0.5, -0.25] and [-0.25, 0], respectively. We use the direct evidence. The results"}, {"title": "4.3 Impact of Evidence Style", "content": "Evidence Styles. We formulate four types of evidence styles: 1) Direct Evidence. This is the most straightforward form of evidence and serves as our baseline. To assess the impact of evidence length, we also create versions where the direct evidence is repeated twice and three times for comparison. 2) Direct Evidence Combined with Paraphrases of CMA (Direct + Paraphrase). To examine the effect of evidence phrasing and expression, we combine the direct evidence with one paraphrase of the CMA to form a two-sentence evidence and with two paraphrases to form a three-sentence evidence. 3) Indirect Evidence. We generate indirect evidence consisting of two sentences and three sentences, respectively7. 4) Direct Evidence Combined with Indirect Evidence (Direct + Indirect). We combine the direct evidence with the first sen-"}, {"title": "5 Conclusion", "content": "We investigate how context-faithful LLMs are to external evidence across two datasets, PopQA and NQ datasets, using LLaMA2.7B, LLaMA2.70B, ChatGPT, and GPT-4. Our findings highlight the critical role of memory strength in shaping LLM behavior. There is a clear positive correlation between memory strength and memory answer ratio. Furthermore, we demonstrate that paraphrasing significantly enhances the context-faithfulness of LLMs across various models and datasets. In conclusion, our results emphasize the influence of memory strength on the context-faithfulness of LLMs and show that paraphrasing direct evidence improves the receptiveness of LLMs to external evidence. These findings offer valuable insights for advancing research in retrieval-augmented generation and context-based LLM applications."}, {"title": "Limitations", "content": "Our framework does not process all types of questions in the NQ dataset. Although it effectively handles the majority of NQ questions, it currently lacks the capability to address \"what,\" \"how,\" and \"why\" question types. The omission of these questions may introduce some bias into our results. Similar to previous studies, our study also focuses on knowledge conflict for extractive QA tasks, where the answer must appear in the evidence. Our conclusion may not be extendable to other types of QA tasks, such as abstractive QA and generative QA.\nWe employed a Natural Language Inference (NLI) model to detect and filter the generated data. Although the NLI model demonstrates high accuracy and the quality of generated data is high, it still cannot guarantee complete correctness. Further, since the NLI model is also trained using language models, which may be biased with parametric memory, it may introduce biases facing knowledge conflicts."}, {"title": "Appendix", "content": "A Additional Study\nA.1 Order of Options\nTo test the effect of the order of options on Rm, we conduct an experiment with one sentence direct evidence by changing the order of options (MA option and CMA option). We define the scenario where the CMA option is presented first in the prompt as \"CMA first\", and the scenario where the MA option is presented first as \u201cMA first\u201d. Figure 4 shows the results.\nAcross all four models (LLaMA2.7B, LLaMA2.70B, ChatGPT, and GPT-4), we observe a consistent trend: MA ratio (Rm) under \"CMA first\" is significantly higher than that under \"MA first\". Evaluations under \"CMA first\" demonstrate that LLMs are less context-faithful.\nTo further demonstrate the effect of the order of options on Rm, we compare the performance of experiments with \u201cCMA first\u201d and \u201cMA first\u201d under two evidence styles: direct evidence with one sentence and direct + paraphrase with three sentences. The results are presented in Table 3. The results show that, for different evidence styles, Rm is consistently higher in the \u201cCMA first\" compared to the \"MA first\". Comparing the results under the \"\u0421\u041c\u0410 first\", the Rm of direct + paraphrase with three sentences is significantly lower than that with direct evidence with one sentence. This demonstrates that paraphrasing direct evidence is an effective method for decreasing Rm. Our conclusion remains unchanged.\nA.2 Case Study\nThe higher Rm of LLaMA2.7B may be attributed to its weakness of reasoning ability. From Table 2, we observe that on the NQ dataset, the Rm of LLaMA2.7B is higher than that of LLaMA2.70B. This phenomenon is counter-intuitive, as a weaker memory strength in LLMs corresponds to a lower Rm for the other three LLMS (LLaMA2.70B, ChatGPT, and GPT-4). To investigate the cause of this issue, we prompt LLaMA2.7B to provide a rationale alongside the answer. We discover that LLaMA2.7B seems to exhibit reasoning errors. Below is an example that shows this phenomenon.\nIn this case, option A represents MA, and option B represents CMA. While the model select MA as the final answer, its rationale indicates that it successfully received and processed the information from CMA. However, for some unknown reason, it still provides the MA as the final answer. This reasoning process is highly confusing. We suspect that the higher Rm of LLaMA2.7B can be attributed to its weakness in reasoning ability. This finding raises an interesting question about the relationship between reasoning ability and memory strength of LLMs. We leave this for future work."}, {"title": "B Methodology and Experiment Details", "content": "B.1 CMA Generation for NQ dataset\nWe generate CMA from MA with three steps: 1)identity question type, 2) determine entity type in MA to change, and 3) generate CMA with LLMs.\nIdentity Question Type: We first build a typing tree using rules to categorize questions. Figure 5 illustrates the typing tree, which consists of a two-layer structure. In the typing process, we first determine if a question begins with one of the following words: \u201cwhat\u201d, \u201cwhen\u201d, \u201cwhere\u201d, \u201cwhich\", \"who\", \u201cwhy\u201d, or \u201chow\u201d. If it does, the question is categorized accordingly; if not, it is classified as \"other\". However, this approach can still group different types of questions together. To address this, we use a second layer to refine the typing by analyzing two specific words in the question. For example, the question shown in Figure 1 falls into the \"how_many\" category."}, {"title": "B.3 Human Evaluation for Model Reliability", "content": "To ensure the reliability of the NLI model, Xie et al. (2024) randomly sample 200 generated examples and manually annotate whether the generated content entails the corresponding claim. The labels are supportive (entailment in the NLI task) or not supportive (either neutral or contradiction in the NLI task). The accuracy is 99%.\nFollowing this process, we evaluate how reliable the generated CMA is. We randomly sample 200 generated examples in the NQ dataset and manually annotate whether the correct entity in MA is found and replaced with a same type alternative. The accuracy is 98%, which means the generated CMA is reliable."}, {"title": "B.4 Impact of Memory Strength with Different Evidence Styles", "content": "To demonstrate the relationship between context-faithfulness and memory strength with other evidence styles, we categorize the questions in each dataset into four groups according to the memory strength intervals [-2, -1], (-1, -0.5], (-0.5, -0.25] and [-0.25, 0], The evidence styles are direct + paraphrase evidence with two sentences and indirect evidence with two sentences. Figures 6,7 show the result. The figures show that there is a clear positive correlation between memory strength and MA ratio for both evidence styles, which implies that this positive correlation between memory strength and MA ratio is general.\nTo demonstrate the relationship between context-faithfulness and memory strength with \"CMA first\" scenario, we show MA, CMA, and UCT ratios with direct evidence with one sentence under \"CMA first\" scenario in Figure 8. The positive correlation between memory strength and MA ratio stays unchanged."}]}