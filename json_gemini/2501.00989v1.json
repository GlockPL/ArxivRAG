{"title": "Bootstrapped Reward Shaping", "authors": ["Jacob Adamczyk", "Volodymyr Makarenko", "Stas Tiomkin", "Rahul V. Kulkarni"], "abstract": "In reinforcement learning, especially in sparse-reward domains, many environment steps are required to observe reward information. In order to increase the frequency of such observations, \"potential-based reward shaping\" (PBRS) has been proposed as a method of providing a more dense reward signal while leaving the optimal policy invariant. However, the required \"potential function\" must be carefully designed with task-dependent knowledge to not deter training performance. In this work, we propose a \"bootstrapped\" method of reward shaping, termed BSRS, in which the agent's current estimate of the state-value function acts as the potential function for PBRS. We provide convergence proofs for the tabular setting, give insights into training dynamics for deep RL, and show that the proposed method improves training speed in the Atari suite.", "sections": [{"title": "Introduction", "content": "The field of reinforcement learning has continued to enjoy successes in solving a variety of problems in both simulation and the physical world. However, the practical use of reinforcement learning in large-scale real-world problems is hindered by the enormous number of environment interactions needed for convergence. Furthermore, even defining the reward functions for such problems (\u201creward engineering\") has proven to be a significant challenge. Improper design of the reward function can inadvertently change the optimal policy, leading to suboptimal or undesirable behaviors, while attempts to create more dense or interpretable reward signals often come at the cost of task complexity. Historically, earlier attempts to adjust the reward function through \"reward shaping\" indeed resulted in unpredictable and negative changes to the corresponding optimal policy (Randl\u00f8v and Alstr\u00f8m 1998).\n\nA significant breakthrough in the field of reward shaping came with the introduction of Potential-Based Reward Shaping (PBRS) from (Ng, Harada, and Russell 1999). PBRS provided a theoretically-grounded method for changing the reward function while keeping the optimal policy fixed. This guarantee framed PBRS as an attractive method of injecting prior knowledge or domain expertise into the reward function through the use of the so-called \"potential function\"."}, {"title": "Background", "content": "In this section we will introduce the relevant background material for reinforcement learning and potential-based reward shaping (PBRS)."}, {"title": "Reinforcement Learning", "content": "We will consider discrete or continuous state spaces and discrete action spaces (this restriction is for an exact calculation of the state-value function; in the final sections we discuss extensions for continuous action spaces). The RL problem is then modeled by a Markov Decision Process (MDP), which we represent by the tuple $(S, A, p, r, \\gamma)$ with state space $S$; action space $A$; potentially stochastic transition function (dynamics) $p: S \\times A \\rightarrow S$; bounded, real reward function $r : S \\times A \\rightarrow \\mathbb{R}$; and the discount factor $\\gamma \\in [0, 1)$.\n\nThe defining objective of reinforcement learning (RL) is to maximize the total discounted reward expected under a policy $\\pi$. That is, to find a policy $\\pi^*$ which maximizes the following sum of expected rewards:\n$\\pi^* = \\arg \\max_{\\pi} \\mathbb{E}_{\\tau \\sim p,\\pi}  \\sum_{t=0}^{\\infty} \\gamma^t r(S_t, a_t)$"}, {"title": "Potential-Based Reward Shaping", "content": "With the goal of efficiently learning an optimal policy for a given reward function, one may wonder how the reward function can be adjusted to enhance training efficiency. Arbitrary \"reward engineering\" may improve performance (Hu et al. 2020) but is not guaranteed to yield the same optimal policy. Indeed, arbitrary changes to the reward function may result in the agent performing reward \"hacking\" or \"hijacking\", having detrimental effects on solving the originally posed task. Examples of this in prior work include the discussion on cycles in (Ng, Harada, and Russell 1999) and more involved examples were later given by (Zhang et al. 2021; Skalse et al. 2022). Instead of arbitrary changes, a specific additive method of changing the reward function, proved by (Ng, Harada, and Russell 1999) to leave the optimal policy invariant, is given by the following result: potential-based reward shaping (PBRS).\n\nTheorem 1 (Ng, Harada, and Russell (1999)). Given task $T = (S, A, p, r, \\gamma)$ with optimal policy $\\pi^*$, then the task $T$ with reward function\n$\\tilde{r}(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p} [\\Phi(s')] - \\Phi(s)$\nhas the optimal policy $\\tilde{\\pi}^* = \\pi^*$, and its optimal value functions satisfy\n$\\tilde{Q}^*(s, a) = Q^*(s, a) - \\Phi(s)$\n$\\tilde{V}^*(s) = V^*(s) - \\Phi(s)$\nfor a bounded, but otherwise arbitrary function $\\Phi : S \\rightarrow \\mathbb{R}$.\n\nIntuitively, the form $\\gamma\\Phi(s') - \\Phi(s)$ represents a discrete-time derivative along trajectories in the MDP (Jenner, van"}, {"title": "Prior Work", "content": "The field of reward shaping in reinforcement learning has a rich history, with roots tracing back to early work on accelerating learning through reward design (Mataric 1994; Randl\u00f8v and Alstr\u00f8m 1998). However, the seminal work of (Ng, Harada, and Russell 1999) marked a significant turning point by introducing Potential-Based Reward Shaping (PBRS). This approach provided a theoretical foundation for modifying rewards without altering the optimal policy, a crucial property for maintaining the \u201ccorrect\u201d or \u201cdesirable\u201d agent behavior at convergence.\n\nThe key insight of (Ng, Harada, and Russell 1999) was that shaping rewards based on a potential function as in Theorem 1 preserves the optimal policy. This result was further extended by (Wiewiora 2003), who proved the equivalence between PBRS and Q-value initialization, providing additional theoretical justification and understanding of the approach. Our work builds directly on these foundations, leveraging the policy invariance property of PBRS while introducing a novel, adaptive approach to defining the potential function.\n\nFollowing the work of Ng, Harada, and Russell, several studies have explored extensions and applications of PBRS which we detail below. Firstly, Dynamic PBRS (Devlin and Kudenko 2012) extended PBRS to a dynamic setting where the potential function is time-dependent within the MDP.\n\nIn the setting of entropy-regularized (\u201cMaxEnt\") RL,(Centa and Preux 2023; Adamczyk et al. 2023a) established connections between the prior policy, PBRS, compositionality, and soft Q-learning; broadening the theoretical understanding of reward shaping. Furthermore, their analysis shows that the degree of freedom used for shaping can be derived from the normalization condition on the optimal policy, or equivalently from an arbitrary \u201cbase task\". Because of these results, our analysis readily extends to the more general entropy-regularized setting. For simplicity, this work will focus on the un-regularized case.\n\nPBRS has also assisted in furthering the theoretical understanding of bounds on the value function.\nDespite the simplicity and applicability of PBRS, a persistent challenge has been the design of effective potential functions without heuristics. Ng, Harada, and Russell suggested using the optimal state-value function $V^*(s)$ as the potential, further explored by (Zou et al. 2021; Cooke et al. 2023) in the meta-learning multi-task setting. However, in the single-task setting this approach presupposes knowledge of the solution, limiting its practical applicability.\nOther approaches have included heuristic-based potentials (Ng, Harada, and Russell 1999), learning-based potentials (especially in the hierarchical setting) (Grze\u015b and Kudenko 2010; Gao and Toni 2015; Ma et al. 2024), and random dynamic potentials (Devlin and Kudenko 2012). Although these approaches have found utility in their respective problem settings, BSRS provides a universally applicable potential function which can be computed without requiring additional samples or training steps."}, {"title": "Theory", "content": "In this section, we derive some theoretical properties of BSRS. Specifically, we show that under appropriate scaling values (\"shape-scales\") $\\eta$, continual shaping in fact converges despite constant changes in the potential function"}, {"title": "Experiments", "content": "In our experiments, we consider the \u201cself-shaped\u201d version of the vanilla value-based algorithm DQN (Mnih et al. 2015; Raffin et al. 2021). We evaluate the performance of our self-shaped algorithm (BSRS) on a variety of environments in a tabular setting, the Atari suite, and on a continuous control task with TD3 (Fujimoto, Hoof, and Meger 2018)."}, {"title": "Tabular Setting", "content": "In the tabular setting, we can exactly solve the MDP and compare the shaped results to un-shaped learning curves and value functions for verification of the theory. We find that the Q-values diverge near the correct boundaries of $\\eta$, but our experiments suggest that the allowed range for $\\eta$ can potentially be expanded (in the positive direction) beyond the values given above.\n\nWe plot the performance (in terms of number of steps until convergence) in Figure 3. The shaded region indicates the standard deviation across 5 random initializations of the Q-table. The inset plot shows the 7\u00d77 gridworld with sparse rewards used for this experiment (arrows indicate the optimal policy). Interestingly, we find that for much larger values of $\\eta$ (\u2248 3 times larger than the allowed maximum value) the performance continually improves, with an optimal value achieving roughly 20% reduction in the number of required steps. The relative location of the optimal performance is"}, {"title": "Continuous Setting", "content": "For more complex environments, we use the Arcade Learning Environment suite (Bellemare et al. 2013). Across many environments, BSRS leads to an improvement over the baseline DQN performance ($\\eta$ = 0). In 21/40 environments we find an improvement of more than 10% and 5/40 environments show an improvement of over 100%. In only 6/40 environments does self-shaping have a negative impact. If $\\eta$ is to be tuned over, one can simply choose $\\eta$ = 0 for these environments. In Figure 2 we find an improvement in the aggregate reward curves of 45% \u2192 60% human normalized score. We find that the value of $\\eta$ can only be increased up until a point ($\\eta$ \u2248 2) until the performance deteriorates. Although our Theorem 2 suggests that $\\eta$ can be negative, we found that the performance in this regime is even worse than that observed for $\\eta$ = 10. Overall, we find a substantial speedup at small times (~ 1.5M steps) and a lasting improvement in rewards at long times for multiple $\\eta$ values.\n\nTo test BSRS in the continuous action setting, we use Pendulum-v1 by extending an implementation of TD3 (Raffin et al. 2021). Since TD3 directly maintains an estimate of both the policy and the value function, the latter is used"}, {"title": "Discussion", "content": "In this work, we provided a theoretically-grounded choice for a dynamic potential function. This work fills the gap in existing literature by providing a universally applicable form for the potential function in any environment. Notably, rather than attempting to tune over the function class $\\Phi : S \\rightarrow \\mathbb{R}$, we instead suggest to tune over the significantly simpler scalar class $\\eta \\in \\mathbb{R}$. This idea simplifies the problem of choosing a potential function from a high-dimensional search problem to a single hyperparameter optimization.\n\nFuture work can naturally extend the results presented. For instance, one may study techniques to learn the optimal value of $\\eta$ over time, perhaps analogous to the method of learning the $\\alpha$ value in (Haarnoja et al. 2018). Further theoretical work can be pursued for understanding the convergence properties of BSRS. For instance, it appears numerically that values of $\\eta$ beyond the proven bounds can be used. Also, it is straightforward to see in the proof of Theorem 2 that all instances of $\\eta$ may be replaced with the functional $\\eta(s)$, giving further control over the self-shaping mechanism. Future work may study such state-dependent shape scales, e.g. dependent on visitation frequencies or the loss experienced in such states (cf. (Wang et al. 2024)), which can further connect to the problem of exploration.\n\nOverall, our work provides a practically relevant implementation of PBRS which provides an advantage in training for both tabular and deep RL."}, {"title": "Additional Experiments", "content": ""}]}