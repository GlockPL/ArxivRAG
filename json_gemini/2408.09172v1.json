{"title": "Unc-TTP: A Method for Classifying LLM Uncertainty\nto Improve In-Context Example Selection", "authors": ["Hsiu-Yuan Huang", "Zichen Wu", "Yutong Yang", "Junzhao Zhang", "Yunfang Wu"], "abstract": "Nowadays, Large Language Models (LLMs) have demon-strated exceptional performance across various downstreamtasks. However, it is challenging for users to discern whetherthe responses are generated with certainty or are fabricated tomeet user expectations. Estimating the uncertainty of LLMsis particularly challenging due to their vast scale and the lackof white-box access. In this work, we propose a novel Un-certainty Tripartite Testing Paradigm (Unc-TTP) to classifyLLM uncertainty, via evaluating the consistency of LLM out-puts when incorporating label interference into the sampling-based approach. Based on Unc-TTP outputs, we aggregateinstances into certain and uncertain categories. Further, weconduct a detailed analysis of the uncertainty properties ofLLMs and show Unc-TTP's superiority over the existingsampling-based methods. In addition, we leverage the ob-tained uncertainty information to guide in-context exam-ple selection, demonstrating that Unc-TTP obviously outper-forms retrieval-based and sampling-based approaches in se-lecting more informative examples. Our work paves a newway to classify the uncertainty of both open- and closed-source LLMs, and introduces a practical approach to exploithis uncertainty to improve LLMs performance.", "sections": [{"title": "1 Introduction", "content": "\u201cReal knowledge is to know the extent of one\u2019s ignorance.\u201d\nConfucius\nIn the rapidly evolving field of artificial intelligence (AI),\nthe quest for generic AI assistants to solve problems with\ntruthful and reliable information remains challenging. Al-though Large Language Models (LLMs) are capable of\nfulfilling user requests for various downstream tasks, they\nstruggle with admitting their knowledge boundary (Xu et al.\n2024). Additionally, they sometimes fabricate statements\nthat are difficult to discern from falsehoods, posing trustwor-thiness issues (Maynez et al. 2020; Alkaissi and McFarlane\n2023; Ji et al. 2023). These challenges push us to considertwo critical questions: (1) How can we determine whetherthe LLM is certain about its answer? (2) Can we leverage\nthe uncertainty property of the model to improve its perfor-mance?"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 The Manifestations of LLM Uncertainty", "content": "While LLMs have performed well in various downstreamtasks, their reliability is still questioned, particularly inhighly specialized domains (Alkaissi and McFarlane 2023;\nShen et al. 2023). Previous work has identified sycophancyand hallucinations as two main reasons for the unreliabilityof LLMs, which can be attributed to the manifestations ofmodel uncertainty.\nSycophancy The sycophancy of LLMs is that they fre-quently wrongly admit mistakes and give biased feedbackwhen questioned by the user (Sharma et al. 2023). By us-ing LLMs for debates (Wang, Yue, and Sun 2023), introduc-ing misleading elements into prompts (Turpin et al. 2023b,a;\nXie et al. 2024; Wei et al. 2024a), or injecting the answerbefore engaging in Chain-of-Thought (CoT) reasoning (Xu,\nQi, and Xu 2024), these findings collectively suggest thatsycophancy may be a prevalent trait among LLMs, indicat-ing their inability to maintain a consistent stance.\nThe patterns of sycophancy have been consistently ob-served across various LLMs and targeted for reduction inprevious works (Sharma et al. 2023; Yang et al. 2023; Wei\net al. 2024a,b). To take a step forward, we recognize the wa-vering behavior behind sycophancy as an indicator of uncer-tainty within LLMs.\nHallucination LLMs often generate content that conflictswith sources or lacks factual verification, known as halluci-nations (Li et al. 2023a; Huang et al. 2023). Xu et al. (2024)\nargue that it occurs when LLMs attempt to answer questionsbeyond their knowledge boundaries. To address this, they\ntrain a reward model to enable the LLM to refuse to answer\nquestions about which it is uncertain. Similarly, Xiong et al.\n(2024) attempt to get LLMs to verbalize their confidence\nin their answers but reveal a tendency towards overconfi-dence. We attribute these issues to the LLMs' inability toexpress their uncertainty without additional assistance. Nev-ertheless, we can still unveil their uncertainty through theirextrinsic manifestations."}, {"title": "2.2 ICL and Example Selection Strategy", "content": "ICL is a paradigm that allows LLMs to learn tasks given onlya few examples as demonstrations without updating modelparameters (Dong et al. 2023; Brown et al. 2020).\nThe most critical factor affecting a model's ICL perfor-mance is the demonstration examples (Kumar and Talukdar\n2021; Lu et al. 2022). Significant efforts have been made\nto select the best in-context examples for LLMs. Liu et al.\n(2021) retrieve in-context examples based on their semanticsimilarity to test samples. Nguyen and Wong (2023) sug-gest selecting examples by calculating their influence. Qin"}, {"title": "3 Classifying LLM Uncertainty", "content": "Despite the sycophantic tendencies of LLMs when con-fronted with users' skepticism or misinformation, they still\nexhibit varying degrees of resistance in their decision-making processes. Therefore, we propose an UncertaintyTripartite Testing Paradigm (Unc-TTP), a method designedto assess the uncertainty of LLMs, which can be applied to"}, {"title": "3.1 The Uncertainty Tripartite Testing Paradigm", "content": "Unc-TTP involves asking LLMs to answer a question un-der three independent settings: (1) providing with no label,(2) providing with the right label, and (3) providing withthe wrong label, where the labels are sourced from bench-mark datasets. We denote this tripartite paradigm as {no-label, right-label, wrong-label} for the sake of brevity.\nUnder the no-label setting, we instruct the LLM with\u201cYour job is to determine whether the text is A or B\u201d, where\u201cA\u201d and \u201cB\u201d are the placeholder for the actual labels in thedataset. We thereby get the model answer that is derived en-tirely from the LLM\u2019s own knowledge and reasoning ability,without any external guiding interference.\nUnder the right-label and wrong-label settings, we pro-vide LLM with either a right or wrong label, intending tolure the model into making the decision we are steering ittowards. These settings expose areas where the model isnot confident and wavers between answers. However, recentwork has shown that LLMs can be easily misled into be-lieving in falsehoods and providing answers that align withthe user\u2019s misguided intentions (Wang, Yue, and Sun 2023).\nTherefore, slavishly guiding the model by concatenating theexternally injected correct (or incorrect) label with the taskprompt does not necessarily reflect the true uncertainty ofthe model but rather an inclination toward sycophancy. To"}, {"title": "3.2 Uncertainty Category Labeling", "content": "After testing all three settings, we obtain three results corre-sponding to {no-label, right-label, wrong-label}, each withtwo possible outcomes: the model answering correctly or in-correctly. Marking the instances that the model answers cor-rectly as 1 and incorrectly as 0, we get a total of eight pos-sible outcomes, denoted as {000, 001, 010, 011, 100, 101,\n110, 111}. These eight categories can be further groupedinto two types:\n\u2022 Certain When the model provides consistent answers,for an instance under all three settings, we consider themodel is certain about the instance. Category 000 (de-noted as Certainw), and 111 (denoted as CertainR), fallwithin this type, where the model outputs unwaveringlywrong in the former and unwaveringly right in the latter.\n\u2022 Uncertain In the rest of the cases where the model haswavering answers, including 001, 110, 011, 100, 101,\nand 110, we consider the model to be uncertain aboutthe instance. Taking 011 as an example, it represents adata instance where the model answers incorrectly withthe no-label setting and correctly with the right-label andwrong-label settings."}, {"title": "4 Assessing LLM Uncertainty", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Model Configurations We perform our experiments withGPT-3.5 and GPT-4 (OpenAI 2024), llama-2-7b-chat (Tou-vron et al. 2023) and mistral-7b-instruct (Jiang et al. 2023),\nwhere all generations are done via greedy decoding.\nDatasets We select three text classification tasks that in-volve subjective decision-making for our experiments, ac-cording to the subjectivity ranking (Li et al. 2023b). Sar-casm Headlines (SH) (Misra and Arora 2023), a high-quality news headlines sarcasm detection dataset collectedfrom two online news websites. Humor Speech (HS) (An-namoradnejad and Zoghi 2020), a dataset consisting of shorttexts with both humorous and non-humorous content. Fi-nancial Phrasebank (FP) (Malo et al. 2014), a sentimentclassification task that divides finance-related sentences aspositive, negative, or neutral.\nComparison Method Following Qin et al. (2024), we ap-ply self-consistency (Wang et al. 2023) with 3 decodingpaths (temperature 0.7) as sampling-based method. We la-bel instances where all three sampling results are right asCertainR. We additionally label instances where all threesamplings are wrong as Certainw, and those who sometimesget one or two right as Uncertain, which aligns with the un-certainty classification of Unc-TTP."}, {"title": "4.2 Results and Analysis", "content": "Unc-TTP Uncertainty Performance The evaluation re-sults of Unc-TTP on three benchmarks are shown in Fig-ure 2. LLM shows more compliance with correct labels. Forthe setting that provides labels, all models except Llama-2on the SH dataset show gullibility to the given labels. Partic-ularly, in the right-label setting, some models nearly reach100% agreement with the given label. However, for thewrong-label setting, the model exhibits varying levels of re-sistance to enticement. Besides, the more capable the modelis, the better it maintains accuracy in the wrong-label setting,although it does not converge to the accuracy achieved in theno-label setting. This indicates that unless the LLM is verycertain of its answer, it reacts quite prudently when the la-bels provided deviate considerably from its own answers, asadopting the user's point of view seems to be a safer option.\nLLMs tend to make compromises when conservative op-tions are offered, choosing the neutral option to accommo-date the user's preference. In the FP dataset, all models ex-"}, {"title": "5 Applying Uncertainty to Example Selection", "content": "In this section, we provide an insight into the real-worldapplications of Unc-TTP through ICL. Inspired by the un-certainty sampling strategy in active learning, where modelsimprove their learning efficiency by actively selecting infor-mative data for training, we creatively select instances fromthe uncertainty categories identified by Unc-TTP to serveas ICL demonstrations, aiming to verify the impact of Unc-TTP-guided uncertainty on LLMs."}, {"title": "5.1 Experiment Settings", "content": "Example Selection Details We select the instances inde-pendently from each of the categories classified by Unc-TTPas the in-context examples to perform K-way N-shot ICL,\nwhere K denotes the number of the labels, and N denotes\nthe sample numbers. Due to the uneven distribution of sam-ple quantities across different categories, some categories\nmight have insufficient samples to complete the K-way N-shot ICL. In such cases, we will supplement with randomlyselected examples. If one category has no corresponding in-stance at all, we will drop it. Therefore, we choose 1-shot asour main experiment since a smaller N can better reflect thetrue impact of uncertainty categories.\nAs for the uncertainty category, we enumerate all possi-ble combinations of outcomes in the three samplings, withat most six for the Unc-TTP method. We choose the onecategory with the highest accuracy on the validation set asthe final in-context examples for testing. It should be notedthat our approach is a one-example-for-all method. Once theselection of the uncertainty category is made, the same ex-ample is applied to all test instances, eliminating the needto retrieve the most compatible one for each test instanceas in previous approaches (Robertson and Zaragoza 2009;\nLiu et al. 2021; Nguyen, Shaker, and H\u00fcllermeier 2022; Qin\net al. 2023)."}, {"title": "5.2 Main Results: 1-Shot ICL", "content": "The test results for 1-shot experiments on three datasets areshown in Table 1. According to an analysis of the uncertaintycategory used for Unc-TTP and the sampling-based method,\nas described in Section 4.2, 010 and 011 are the most infor-mative categories for small models including Llama-2 andMistral.\nThe results demonstrate that the Uncertainty category ofUnc-TTP and the sampling-based method outperform theirCertain ones (mean of Certain and Certainw) by an aver-age of 2.55 points and 1.16 points across three models anddatasets. This highlights the significant informativeness ofuncertain instances when used as in-context examples.\nUncertainty-based sampling methods also show a clearadvantage over other baselines. Compared to retrieval-basedmethods like BM25 and Similarity, Unc-TTP achieves su-perior performance with an average improvement of 3.27"}, {"title": "5.3 N-Shot Results", "content": "We conduct N-shot experiments with Mistral to evaluatewhether Unc-TTP-guided uncertainty sampling can main-tain its superiority as the number of shots increases. The re-sults, depicted in Figure 5, reveal that uncertain examplesidentified by Unc-TTP are generally more informative thanthose selected by the sampling-based method or chosen ran-domly. Unc-TTP consistently outperforms other baselinesacross 1 to 4 shots, highlighting the informativeness of theexamples selected under Unc-TTP's guidance. However, thebenefits of uncertainty sampling are potentially marginal.\nThe advantages of uncertainty sampling diminish for the SHand FP datasets when the number of shots exceeds 8. We ar-gue that the marginal benefits of uncertainty sampling varydepending on the dataset and the length of the selected in-stances. The advantages of uncertain examples identified byUnc-TTP are evident in the smaller number of shots."}, {"title": "5.4 Discussion", "content": "More capable LLMs have demonstrated a stronger ability toresist skepticism or incorrect information provided by users,and they maintain their stance while achieving a higher ac-curacy rate. Therefore, we seek to investigate whether the"}, {"title": "6 Conclusion", "content": "We propose a novel uncertainty testing paradigm, Unc-TTP,\nfor classifying the uncertainty levels of both open- and\nclosed-source LLMs. This paradigm involves enumerating\nall possible LLM output combinations under three testing\nscenarios and classifying uncertainty levels based on out-\nput consistency, thus exposing areas where LLM are not\ncertain and potentially hallucinated. We provide a detailed\nanalysis of uncertainty properties classified by Unc-TTP\nand the sampling-based method, noting that more capable\nmodels exhibit fewer proportions of wavering and greater\ncertainty. Moreover, we introduce a self-guided, in-context\nexample selection strategy that utilizes the uncertainty ob-\ntained by Unc-TTP. Our approach selects more informative\nexamples than the sampling-based and retrieval-based ap-"}, {"title": "A Dataset Implementation Details", "content": "Since the SH dataset does not provide official data splits, werandomly selected 500 samples as the training set, 1500 asthe validation set, and 200 as the test set balancing budgetand time constraints.\nFor the FP dataset, each sentence is annotated between 5to 8 times. To ensure high data quality, we included onlythose sentences with 100% annotation agreement, result-ing in 2,264 examples available for training and evaluation.Among these, only 303 examples were labeled as negative.Consequently, we randomly selected 100 instances for eachlabel to form the training set and 200 instances for each la-bel for the validation set. We randomly sample 200 instancesfrom the split that reach 75% annotation agreement for thetest set.\nOther data splits are detailed in Table 3 for reference. Itshould be noted that all divisions have an equal number ofexamples for each label. The labels we use in the prompt areas follows:\n\u2022 SH: sarcastic / non-sarcastic\n\u2022 HS: humorous / not humorous\n\u2022 FP: positive / neutral / negative"}, {"title": "B Detailed Results of the LLM UncertaintyAssessment", "content": "In this section, we present the data distribution of Unc-TTPon the FP dataset in Table 4."}, {"title": "CUnc-TTP-Guided Uncertainty SamplingResult on Validation Set", "content": "The Unc-TTP-guided uncertainty sampling result on val-idation set are shown in the Table 5. For smaller models, theinformative uncertainty categories are predominantly cen-tered around 011 and 010. However, for larger models, thereis no discernible pattern in the informative uncertainty cat-egories. We speculate that this lack of pattern may be dueto two reasons: first, the larger model may rely less on in-context learning (ICL) compared to smaller models; and sec-ond, the increased parameter size of the larger model intro-duces greater stochasticity, even when using greedy decod-ing."}, {"title": "D Limitations", "content": "Firstly, our proposed Unc-TTP inherits the prompt-sensitive\nnature of the LLMs, which may introduce risks when apply-ing Unc-TTP-guided uncertainty sampling.\nSecondly, we have only evaluated Unc-TTP on subjectivetasks. Given highly subjective tasks often require a deep un-derstanding of contextual subtleties, their labels are some-times \u201cbiased\u201d themselves depending on the annotator de-composition (Li, Lu, and Yin 2022; Li et al. 2023b). Thischaracteristic is more compatible with Unc-TTP, as it can ef-fectively expose areas where LLMs exhibit uncertainty com-pared to objective reasoning tasks. For the implementationof Unc-TTP on subjective tasks will be a primary focus inour future work.\nThirdly, we have only tested Unc-TTP on classificationtasks due to the limited time. However, it should be notedthat our approach is not limited to the categorization task. Onthe generative task, we can similarly transform the model\u2019sanswer into a binary fact-checking task and obtain the tri-partite paradigm by means of different interrogative formu-lations. This will be our future work.\nFinally, due to limitations in time and resources, we havenot yet tested the combination of uncertainty-based exam-ples and randomly selected examples in K-way N-shot ICLsettings. This aspect will also be addressed in our futurework."}]}