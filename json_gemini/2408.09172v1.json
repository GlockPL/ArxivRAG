{"title": "Unc-TTP: A Method for Classifying LLM Uncertainty to Improve In-Context Example Selection", "authors": ["Hsiu-Yuan Huang", "Zichen Wu", "Yutong Yang", "Junzhao Zhang", "Yunfang Wu"], "abstract": "Nowadays, Large Language Models (LLMs) have demonstrated exceptional performance across various downstream tasks. However, it is challenging for users to discern whether the responses are generated with certainty or are fabricated to meet user expectations. Estimating the uncertainty of LLMs is particularly challenging due to their vast scale and the lack of white-box access. In this work, we propose a novel Uncertainty Tripartite Testing Paradigm (Unc-TTP) to classify LLM uncertainty, via evaluating the consistency of LLM outputs when incorporating label interference into the sampling-based approach. Based on Unc-TTP outputs, we aggregate instances into certain and uncertain categories. Further, we conduct a detailed analysis of the uncertainty properties of LLMs and show Unc-TTP's superiority over the existing sampling-based methods. In addition, we leverage the obtained uncertainty information to guide in-context example selection, demonstrating that Unc-TTP obviously outperforms retrieval-based and sampling-based approaches in selecting more informative examples. Our work paves a new way to classify the uncertainty of both open- and closed-source LLMs, and introduces a practical approach to exploit this uncertainty to improve LLMs performance.", "sections": [{"title": "1 Introduction", "content": "\"Real knowledge is to know the extent of one's ignorance.\u201d\nConfucius\nIn the rapidly evolving field of artificial intelligence (AI), the quest for generic AI assistants to solve problems with truthful and reliable information remains challenging. Although Large Language Models (LLMs) are capable of fulfilling user requests for various downstream tasks, they struggle with admitting their knowledge boundary (Xu et al. 2024). Additionally, they sometimes fabricate statements that are difficult to discern from falsehoods, posing trustworthiness issues (Maynez et al. 2020; Alkaissi and McFarlane 2023; Ji et al. 2023). These challenges push us to consider two critical questions: (1) How can we determine whether the LLM is certain about its answer? (2) Can we leverage the uncertainty property of the model to improve its performance?"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 The Manifestations of LLM Uncertainty", "content": "While LLMs have performed well in various downstream tasks, their reliability is still questioned, particularly in highly specialized domains (Alkaissi and McFarlane 2023; Shen et al. 2023). Previous work has identified sycophancy and hallucinations as two main reasons for the unreliability of LLMs, which can be attributed to the manifestations of model uncertainty."}, {"title": "Sycophancy", "content": "The sycophancy of LLMs is that they frequently wrongly admit mistakes and give biased feedback when questioned by the user (Sharma et al. 2023). By using LLMs for debates (Wang, Yue, and Sun 2023), introducing misleading elements into prompts (Turpin et al. 2023b,a; Xie et al. 2024; Wei et al. 2024a), or injecting the answer before engaging in Chain-of-Thought (CoT) reasoning (Xu, Qi, and Xu 2024), these findings collectively suggest that sycophancy may be a prevalent trait among LLMs, indicating their inability to maintain a consistent stance.\nThe patterns of sycophancy have been consistently observed across various LLMs and targeted for reduction in previous works (Sharma et al. 2023; Yang et al. 2023; Wei et al. 2024a,b). To take a step forward, we recognize the wavering behavior behind sycophancy as an indicator of uncertainty within LLMs."}, {"title": "Hallucination", "content": "LLMs often generate content that conflicts with sources or lacks factual verification, known as hallucinations (Li et al. 2023a; Huang et al. 2023). Xu et al. (2024) argue that it occurs when LLMs attempt to answer questions beyond their knowledge boundaries. To address this, they train a reward model to enable the LLM to refuse to answer questions about which it is uncertain. Similarly, Xiong et al. (2024) attempt to get LLMs to verbalize their confidence in their answers but reveal a tendency towards overconfidence. We attribute these issues to the LLMs' inability to express their uncertainty without additional assistance. Nevertheless, we can still unveil their uncertainty through their extrinsic manifestations."}, {"title": "2.2 ICL and Example Selection Strategy", "content": "ICL is a paradigm that allows LLMs to learn tasks given only a few examples as demonstrations without updating model parameters (Dong et al. 2023; Brown et al. 2020).\nThe most critical factor affecting a model's ICL performance is the demonstration examples (Kumar and Talukdar 2021; Lu et al. 2022). Significant efforts have been made to select the best in-context examples for LLMs. Liu et al. (2021) retrieve in-context examples based on their semantic similarity to test samples. Nguyen and Wong (2023) suggest selecting examples by calculating their influence. Qin"}, {"title": "3 Classifying LLM Uncertainty", "content": "Despite the sycophantic tendencies of LLMs when confronted with users' skepticism or misinformation, they still exhibit varying degrees of resistance in their decision-making processes. Therefore, we propose an Uncertainty Tripartite Testing Paradigm (Unc-TTP), a method designed to assess the uncertainty of LLMs, which can be applied to both open- and closed-source models. The schematic of our proposed paradigm is illustrated in Figure 1."}, {"title": "3.1 The Uncertainty Tripartite Testing Paradigm", "content": "Unc-TTP involves asking LLMs to answer a question under three independent settings: (1) providing with no label, (2) providing with the right label, and (3) providing with the wrong label, where the labels are sourced from benchmark datasets. We denote this tripartite paradigm as {no-label, right-label, wrong-label} for the sake of brevity.\nUnder the no-label setting, we instruct the LLM with \"Your job is to determine whether the text is A or B\", where \"A\" and \"B\" are the placeholder for the actual labels in the dataset. We thereby get the model answer that is derived entirely from the LLM's own knowledge and reasoning ability, without any external guiding interference.\nUnder the right-label and wrong-label settings, we provide LLM with either a right or wrong label, intending to lure the model into making the decision we are steering it towards. These settings expose areas where the model is not confident and wavers between answers. However, recent work has shown that LLMs can be easily misled into believing in falsehoods and providing answers that align with the user's misguided intentions (Wang, Yue, and Sun 2023). Therefore, slavishly guiding the model by concatenating the externally injected correct (or incorrect) label with the task prompt does not necessarily reflect the true uncertainty of the model but rather an inclination toward sycophancy. To"}, {"title": "3.2 Uncertainty Category Labeling", "content": "After testing all three settings, we obtain three results corresponding to {no-label, right-label, wrong-label}, each with two possible outcomes: the model answering correctly or incorrectly. Marking the instances that the model answers correctly as 1 and incorrectly as 0, we get a total of eight possible outcomes, denoted as {000, 001, 010, 011, 100, 101, 110, 111}. These eight categories can be further grouped into two types:\n\u2022 Certain When the model provides consistent answers for an instance under all three settings, we consider the model is certain about the instance. Category 000 (denoted as Certainw), and 111 (denoted as CertainR), fall within this type, where the model outputs unwaveringly wrong in the former and unwaveringly right in the latter.\n\u2022 Uncertain In the rest of the cases where the model has wavering answers, including 001, 110, 011, 100, 101, and 110, we consider the model to be uncertain about the instance. Taking 011 as an example, it represents a data instance where the model answers incorrectly with the no-label setting and correctly with the right-label and wrong-label settings."}, {"title": "4 Assessing LLM Uncertainty", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Model Configurations We perform our experiments with GPT-3.5 and GPT-4 (OpenAI 2024), llama-2-7b-chat (Touvron et al. 2023) and mistral-7b-instruct (Jiang et al. 2023), where all generations are done via greedy decoding.\nDatasets We select three text classification tasks that involve subjective decision-making for our experiments, according to the subjectivity ranking (Li et al. 2023b). Sarcasm Headlines (SH) (Misra and Arora 2023), a high-quality news headlines sarcasm detection dataset collected from two online news websites. Humor Speech (HS) (Annamoradnejad and Zoghi 2020), a dataset consisting of short texts with both humorous and non-humorous content. Financial Phrasebank (FP) (Malo et al. 2014), a sentiment classification task that divides finance-related sentences as positive, negative, or neutral.\nComparison Method Following Qin et al. (2024), we apply self-consistency (Wang et al. 2023) with 3 decoding paths (temperature 0.7) as sampling-based method. We label instances where all three sampling results are right as CertainR. We additionally label instances where all three samplings are wrong as Certainw, and those who sometimes get one or two right as Uncertain, which aligns with the uncertainty classification of Unc-TTP."}, {"title": "4.2 Results and Analysis", "content": "Unc-TTP Uncertainty Performance The evaluation results of Unc-TTP on three benchmarks are shown in Figure 2. LLM shows more compliance with correct labels. For the setting that provides labels, all models except Llama-2 on the SH dataset show gullibility to the given labels. Particularly, in the right-label setting, some models nearly reach 100% agreement with the given label. However, for the wrong-label setting, the model exhibits varying levels of resistance to enticement. Besides, the more capable the model is, the better it maintains accuracy in the wrong-label setting, although it does not converge to the accuracy achieved in the no-label setting. This indicates that unless the LLM is very certain of its answer, it reacts quite prudently when the labels provided deviate considerably from its own answers, as adopting the user's point of view seems to be a safer option.\nLLMs tend to make compromises when conservative options are offered, choosing the neutral option to accommodate the user's preference. In the FP dataset, all models exhibit relatively steady performance under the wrong-label setting. A possible explanation for this observation is that FP is a classification dataset with three labels, where the distinctions between neutral and positive or negative are subtle. LLMs inherently tend to favor the more conservative neutral answers. When their initial beliefs are challenged in the wrong-label setting, they are inclined to prefer the neutral option, potentially aligning with user preferences. We observe that most LLMs exhibit a tendency to produce conservative answers in the no-label setting. This tendency becomes pronounced in the wrong-label setting, where the number of neutral answers doubles, further substantiating our conclusion.\nUncertainty Properties of LLMS To further investigate the degree of uncertainty across different LLMs, we present the distribution of certain and uncertain instances for each model in Figure 3.\nStronger LLMs exhibit greater certainty. Intuitively, the more robust the model, the less likely it is to waver between answers. In Unc-TTP, despite external interference, GPT-4 maintains its stance to a large extent, whereas the other models exhibit resilience towards interference but are not as pronounced. A similar trend is observed in the sampling-based method, where stronger models exhibit a lower tendency to waver. We attribute this to the model's confidence in its answers. Stronger models possess better logical reasoning and greater certainty, allowing them to maintain their stance regardless of label injection or repetitive sampling.\nLabel interference improves the strictness of classifying certainty standards. Compared to the sampling-based method, Unc-TTP injects right/wrong label into the prompt, making it much more difficult for LLMs to persist in its answer given how much it wavers in Figure 2. Therefore, the standard of Unc-TTP's certainty is stricter than that of the sampling-based method. It is worth noting that we are not disapproving of the certainty delineated by the sampling-based approach; it is just that such a loose criterion might lead to an inadequate exposition of uncertain instances.\nUncertainty Category Distribution We visualize the respective quantities of instances for each category in Figure 4. The distribution reveals significant unevenness, indicating that each LLM and dataset possesses a unique uncertainty property. Interestingly, the number of instances in the categories that do not match human intuition, such as 001, 100, and 101, is notably low, as expected. For instance, if a model misclassifies an instance both without guidance and with correct guidance, it should have a very low likelihood of answering correctly when provided with incorrect labels.\nMoreover, the categories we anticipate being the most informative-those that could stimulate deep thinking in the model when presented with a label, such as 011-are also scarce. This scarcity is likely due to the high level of capability required to generate such categories. The next most informative categories, in our view, are 010 and 110. These three categories represent the uncertainty types of primary interest to us."}, {"title": "5 Applying Uncertainty to Example Selection", "content": "In this section, we provide an insight into the real-world applications of Unc-TTP through ICL. Inspired by the uncertainty sampling strategy in active learning, where models improve their learning efficiency by actively selecting informative data for training, we creatively select instances from the uncertainty categories identified by Unc-TTP to serve as ICL demonstrations, aiming to verify the impact of Unc-TTP-guided uncertainty on LLMs."}, {"title": "5.1 Experiment Settings", "content": "Example Selection Details We select the instances independently from each of the categories classified by Unc-TTP as the in-context examples to perform K-way N-shot ICL, where K denotes the number of the labels, and N denotes the sample numbers. Due to the uneven distribution of sample quantities across different categories, some categories might have insufficient samples to complete the K-way N-shot ICL. In such cases, we will supplement with randomly selected examples. If one category has no corresponding instance at all, we will drop it. Therefore, we choose 1-shot as our main experiment since a smaller N can better reflect the true impact of uncertainty categories.\nAs for the uncertainty category, we enumerate all possible combinations of outcomes in the three samplings, with at most six for the Unc-TTP method. We choose the one category with the highest accuracy on the validation set as the final in-context examples for testing. It should be noted that our approach is a one-example-for-all method. Once the selection of the uncertainty category is made, the same example is applied to all test instances, eliminating the need to retrieve the most compatible one for each test instance as in previous approaches (Robertson and Zaragoza 2009; Liu et al. 2021; Nguyen, Shaker, and H\u00fcllermeier 2022; Qin et al. 2023)."}, {"title": "5.2 Main Results: 1-Shot ICL", "content": "The test results for 1-shot experiments on three datasets are shown in Table 1. According to an analysis of the uncertainty category used for Unc-TTP and the sampling-based method, as described in Section 4.2, 010 and 011 are the most informative categories for small models including Llama-2 and Mistral.\nThe results demonstrate that the Uncertainty category of Unc-TTP and the sampling-based method outperform their Certain ones (mean of Certain and Certainw) by an average of 2.55 points and 1.16 points across three models and datasets. This highlights the significant informativeness of uncertain instances when used as in-context examples.\nUncertainty-based sampling methods also show a clear advantage over other baselines. Compared to retrieval-based methods like BM25 and Similarity, Unc-TTP achieves superior performance with an average improvement of 3.27"}, {"title": "5.3 N-Shot Results", "content": "We conduct N-shot experiments with Mistral to evaluate whether Unc-TTP-guided uncertainty sampling can maintain its superiority as the number of shots increases. The results, depicted in Figure 5, reveal that uncertain examples identified by Unc-TTP are generally more informative than those selected by the sampling-based method or chosen randomly. Unc-TTP consistently outperforms other baselines across 1 to 4 shots, highlighting the informativeness of the examples selected under Unc-TTP's guidance. However, the benefits of uncertainty sampling are potentially marginal. The advantages of uncertainty sampling diminish for the SH and FP datasets when the number of shots exceeds 8. We argue that the marginal benefits of uncertainty sampling vary depending on the dataset and the length of the selected instances. The advantages of uncertain examples identified by Unc-TTP are evident in the smaller number of shots."}, {"title": "5.4 Discussion", "content": "More capable LLMs have demonstrated a stronger ability to resist skepticism or incorrect information provided by users, and they maintain their stance while achieving a higher accuracy rate. Therefore, we seek to investigate whether the"}, {"title": "6 Conclusion", "content": "We propose a novel uncertainty testing paradigm, Unc-TTP, for classifying the uncertainty levels of both open- and closed-source LLMs. This paradigm involves enumerating all possible LLM output combinations under three testing scenarios and classifying uncertainty levels based on output consistency, thus exposing areas where LLM are not certain and potentially hallucinated. We provide a detailed analysis of uncertainty properties classified by Unc-TTP and the sampling-based method, noting that more capable models exhibit fewer proportions of wavering and greater certainty. Moreover, we introduce a self-guided, in-context example selection strategy that utilizes the uncertainty obtained by Unc-TTP. Our approach selects more informative examples than the sampling-based and retrieval-based ap-"}, {"title": "A Dataset Implementation Details", "content": "Since the SH dataset does not provide official data splits, we randomly selected 500 samples as the training set, 1500 as the validation set, and 200 as the test set balancing budget and time constraints.\nFor the FP dataset, each sentence is annotated between 5 to 8 times. To ensure high data quality, we included only those sentences with 100% annotation agreement, resulting in 2,264 examples available for training and evaluation. Among these, only 303 examples were labeled as negative. Consequently, we randomly selected 100 instances for each label to form the training set and 200 instances for each label for the validation set. We randomly sample 200 instances from the split that reach 75% annotation agreement for the test set.\nOther data splits are detailed in Table 3 for reference. It should be noted that all divisions have an equal number of examples for each label. The labels we use in the prompt are as follows:\n\u2022 SH: sarcastic / non-sarcastic\n\u2022 HS: humorous / not humorous\n\u2022 FP: positive / neutral / negative"}, {"title": "B Detailed Results of the LLM Uncertainty Assessment", "content": "In this section, we present the data distribution of Unc-TTP on the FP dataset in Table 4."}, {"title": "CUnc-TTP-Guided Uncertainty Sampling Result on Validation Set", "content": "The Unc-TTP-guided uncertainty sampling result on validation set are shown in the Table 5. For smaller models, the informative uncertainty categories are predominantly centered around 011 and 010. However, for larger models, there is no discernible pattern in the informative uncertainty categories. We speculate that this lack of pattern may be due to two reasons: first, the larger model may rely less on in-context learning (ICL) compared to smaller models; and second, the increased parameter size of the larger model introduces greater stochasticity, even when using greedy decoding."}, {"title": "D Limitations", "content": "Firstly, our proposed Unc-TTP inherits the prompt-sensitive nature of the LLMs, which may introduce risks when applying Unc-TTP-guided uncertainty sampling.\nSecondly, we have only evaluated Unc-TTP on subjective tasks. Given highly subjective tasks often require a deep understanding of contextual subtleties, their labels are sometimes \"biased\" themselves depending on the annotator decomposition (Li, Lu, and Yin 2022; Li et al. 2023b). This characteristic is more compatible with Unc-TTP, as it can effectively expose areas where LLMs exhibit uncertainty compared to objective reasoning tasks. For the implementation of Unc-TTP on subjective tasks will be a primary focus in our future work.\nThirdly, we have only tested Unc-TTP on classification tasks due to the limited time. However, it should be noted that our approach is not limited to the categorization task. On the generative task, we can similarly transform the model's answer into a binary fact-checking task and obtain the tripartite paradigm by means of different interrogative formulations. This will be our future work.\nFinally, due to limitations in time and resources, we have not yet tested the combination of uncertainty-based examples and randomly selected examples in K-way N-shot ICL settings. This aspect will also be addressed in our future work."}]}