{"title": "The Hitchhiker's Guide to Human Alignment with *PO", "authors": ["Kian Ahrabian", "Xihui Lin", "Barun Patra", "Vishrav Chaudhary", "Alon Benhaim", "Jay Pujara", "Xia Song"], "abstract": "With the growing utilization of large language models (LLMs) across domains, alignment towards human preferences has become one of the most critical aspects of training mod- els. At the forefront of state-of-the-art human alignment methods are preference optimiza- tion methods (*PO). However, prior research has often concentrated on identifying the best- performing method, typically involving a grid search over hyperparameters, which can be im- practical for general practitioners. In this paper, we aim to identify the algorithm that, while be- ing performant, is simultaneously more robust to varying hyperparameters, thereby increasing the likelihood of achieving better results. We fo- cus on a realistic out-of-distribution (OOD) sce- nario that mirrors real-world applications of hu- man alignment, offering practical insights into the strengths and weaknesses of these methods. Furthermore, to better understand the shortcom- ings of generations from the different methods, we analyze the model generations through the lens of KL divergence of the SFT model and the response length statistics. Our analysis reveals that the widely adopted DPO method consis- tently produces lengthy responses of inferior quality that are very close to the SFT responses. Motivated by these findings, we propose an embarrassingly simple extension to the DPO algorithm, LN-DPO, resulting in more concise responses without sacrificing quality compared to the policy obtained by vanilla DPO.", "sections": [{"title": "1 Introduction", "content": "In recent years, the quality of large language mod- els (LLMs) has been constantly increasing (Chi- ang et al., 2024), achieving impressive results across tasks and benchmarks (Abdin et al., 2024; AI@Meta, 2024; Achiam et al., 2023; Team, 2023; Yang et al., 2024). However, even with the most rigorous filtering heuristics, the training data (Com- puter, 2023; Penedo et al., 2024) is typically"}, {"title": "2 Related Work", "content": "Since the introduction of DPO (Rafailov et al., 2024), there has been a body of works with new optimization objectives improving the performance and efficiency (Azar et al., 2024; Tang et al., 2024; Hong et al., 2024; Rosset et al., 2024; Meng et al., 2024; Xu et al., 2024a; Ethayarajh et al., 2024) (see Table 2 for examples). These methods can be partitioned into two groups: reference-free (Meng et al., 2024; Hong et al., 2024) and reference- dependent (Rafailov et al., 2024; Park et al., 2024). Reference-free methods generally benefit from fast training runs, while reference-dependent methods have terms baked into their objective to control di- vergence from the reference model. In this work, we compare SimPO (Meng et al., 2024), a re- cent state-of-the-art reference-free method, with the DPO as the reference-dependent method, pro- viding practical insights into their performance (see Appendix A for extended related work)."}, {"title": "3 Experimental Setup", "content": "For our datasets, we follow the setup introduced by Xu et al. (2024b). Specifically, we use the double safe/unsafe filtered train subset of SafeRLHF (Dai et al., 2024) for training2, and the test subset of HH-RLHF (Ganguli et al., 2022) for evaluation. This setup closely resembles real-world scenarios where even though models are trained on various domains (e.g., safety and helpfulness in our experiments), they have to generalize to similar un-seen queries while interacting with the users. For example, in our setup, both datasets focus on safety; however, SafeRLHF is human-labeled, while LLM models label HH-RLHF."}, {"title": "3.2 Models", "content": "For all our experiments we use the Phi-3 Medium model\u00b3 (Abdin et al., 2024). We chose this model due to its high performance across benchmarks and small size to ensure computational tractability. To evaluate the trained models, we use the OpenAs- sistant reward model\u2074 (K\u00f6pf et al., 2024) as an oracle to score the quality of their generated re- sponses. We chose this model due to its small size and high performance on RewardBench (Lambert et al., 2024), ensuring fast and correct evaluations for our experiments."}, {"title": "3.3 Metrics", "content": "Our analysis focuses on the following five metrics:\n\u2022 Mean Score: The average score of the gener- ated responses, as judged by the gold reward model.\n\u2022 Win vs. Chosen: The fraction of samples where the gold reward model assigns a higher score to the generated response compared to"}, {"title": "3.4 Optimization Objectives", "content": "Considering the performances reported by Meng et al. (2024), we choose DPO as our reference- dependent method and SimPO as our reference-free method. While DPO has an implicit length normal- ization through the reference model, the variance of the reward (i.e., $log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_0(y_w|x)}$) increases with response length. As such, inspired by explicit length regular- ization in SimPO and R-DPO (Park et al., 2024), we further normalize it with the response length similar to SimPO, which we call LN-DPO. Note"}, {"title": "4 Experimental Results", "content": "Following the common prac- tice of comparing the best performance achieved by each method, Table 1 showcases the best re- sult of all three models across our metrics. As evident, at their peaks, SimPO and LN-DPO beat DPO across the board. Moreover, we can observe the effectiveness of the length regularizer, dras- tically decreasing the mean response length (as much as 22.9%) while also having a higher perfor- mance. We also notice a significant decrease in KL divergence (as much as 26%). However, KL and length-normalized KL for SimPO decrease less than LN-DPO, showcasing a more significant di- vergence from SFT, which we will further analyze in the following. Overall, both SimPO and LN- DPO seem to be suitable replacements for DPO. Note that due to being reference-free, SimPO takes almost half the time to train."}, {"title": "4.1 Hyperparameter Sensitivity", "content": "Best Performance."}, {"title": "4.2 Response Length", "content": "Since length exploitation is a critical issue (Park et al., 2024), we compare the response lengths across samples generated by the top k% (k\u2208 {1, 10, 25}) of each method's best-performing hy- perparameters. As illustrated in Figure 2, on the best set of hyperparameters (i.e., top 1%), the non- DPO methods showcase a left shift in length distri- bution (compared to DPO), which is a desired ef- fect. However, this phenomenon starts to diminish as we include worse-performing hyperparameters. For example, LN-DPO has a higher rate than DPO in the tail-end of the top 25% distribution. Overall, we observed that both length-regularized models performed superior to DPO, with SimPO producing the shortest responses across the distribution."}, {"title": "4.3 KL Divergence (vs. SFT)", "content": "Since reference-free methods are not regularized against a reference policy (e.g., the SFT model), there is a possibility of reward hacking occurring (i.e., lower loss with degraded performance). There- fore, we compare the KL divergence across sam- ples generated by the top k% (k \u2208 {1, 10, 25}) of each method's best-performing hyperparameters. Figure 3 illustrates the KL distributions. As evi- dent, both SimPO and LN-DPO achieve lower KLs at their peak. However, as we move toward worse- performing models, DPO achieves lower KL (at 10%). While at first, this seems like a pitfall of SimPO and LN-DPO, in reality, we believe it show-cases that many DPO runs fail to learn beyond the SFT model, which is a critical shortcoming.\nSince KL divergence is calculated as a summa- tion, there is the possibility that lower values are achieved purely due to responses being shorter. As such, we also calculate a length-normalized KL to account for the effect of longer sequences, which roughly quantifies the average per token distribu- tional difference. Notably, on this metric, SimPO always has distributionally higher values, even at the top 1% (see Figure 9, Appendix D). This ob- servation showcases that SimPO diverges the most from the SFT on a token level, which could be explained by the lack of reference dependence."}, {"title": "4.4 Hyperparameter Tuning Considerations", "content": "As presented in Figure 4, lower $\\beta$ leads to higher performances; however, as $\\beta$ decreases, the performance variance increases, which showcases the method's instability. Overall, $\\beta = 0.05$ pro- vides the best balance of stability and performance values. While we initially borrowed $\\beta$'s range from SimPO (Meng et al., 2024), more experi- ments showed benefits in further decreasing its value. Figure 5 presents the performance spread across different runs. From these experiments, $\\beta\\in [1.0, 2.0]$ contains most of the best-performing models. Moreover, we observe the relatively low (compared to DPO) variance across the perfor- mances, showcasing another benefit of LN-DPO. In contrast to the other two methods, SimPO has two method-specific hyperparameters: $\\beta$ and $\\gamma$. As illustrated in Figure 6, on average, lower $\\beta$ values lead to better performance. We be- lieve the performance uptick in the lower range is due to a difference in the average length of this work's and the original work's training sets. More- over, as showcased in Figure 7, the best performing models have a $\\gamma\\in [1.0, 1.4]$, in line with the sug- gestion by Meng et al. (2024). Notably, $\\beta$ and $\\gamma$ have a relatively low variance across experiments, another upside of SimPO."}, {"title": "42 The Answer to the Ultimate Question", "content": "Based on our collective empirical results, we be- lieve SimPO to be the best starting point among the three methods, mainly due to its robustness toward hyperparameter variations and effective length re- duction. As for SimPO's hyperparameters, we rec- ommend $\\beta\\in {1.0, 1.5}$ and $\\gamma \\approx 1.2$. Moreover, while LN-DPO is consistently second-best in most of our experiments, we discuss scenarios for choos- ing it over SimPO in Appendix F."}, {"title": "5 Conclusion", "content": "In this work, we introduce LN-DPO, a length- normalized variation of DPO that reduces the average response length while staying reference- dependent. Moreover, we present a thorough anal- ysis of LN-DPO and two state-of-the-art reference- dependent and reference-free preference optimiza- tion methods in a simulated real-world scenario for safety and helpfulness domains. Specifically, we cover the behavior of these methods across a wide range of hyperparameters under metrics such as mean response length, KL divergence (vs. SFT), and win rate (vs. chosen and SFT). Our experi- ments showcase state-of-the-art methods' strengths and weaknesses along with providing insights for other practitioners. Finally, we present our empiri- cal insights into tuning *PO hyperparameters, po- tentially helping to reduce future efforts and costs."}, {"title": "Adaptive margin defined as", "content": "$$\\gamma_{w,l} = \\log \\frac{\\tau_{ref}(y_w|x)}{\\pi_0(y_w|x)} - \\log \\frac{\\tau_{ref}(y_l|x)}{\\pi_0(y_l|x)}$$"}]}