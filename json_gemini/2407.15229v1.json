{"title": "The Hitchhiker's Guide to Human Alignment with *PO", "authors": ["Kian Ahrabian", "Xihui Lin", "Barun Patra", "Vishrav Chaudhary", "Alon Benhaim", "Jay Pujara", "Xia Song"], "abstract": "With the growing utilization of large language models (LLMs) across domains, alignment towards human preferences has become one of the most critical aspects of training models. At the forefront of state-of-the-art human alignment methods are preference optimization methods (*PO). However, prior research has often concentrated on identifying the best-performing method, typically involving a grid search over hyperparameters, which can be impractical for general practitioners. In this paper, we aim to identify the algorithm that, while being performant, is simultaneously more robust to varying hyperparameters, thereby increasing the likelihood of achieving better results. We focus on a realistic out-of-distribution (OOD) scenario that mirrors real-world applications of human alignment, offering practical insights into the strengths and weaknesses of these methods. Furthermore, to better understand the shortcomings of generations from the different methods, we analyze the model generations through the lens of KL divergence of the SFT model and the response length statistics. Our analysis reveals that the widely adopted DPO method consistently produces lengthy responses of inferior quality that are very close to the SFT responses. Motivated by these findings, we propose an embarrassingly simple extension to the DPO algorithm, LN-DPO, resulting in more concise responses without sacrificing quality compared to the policy obtained by vanilla DPO.", "sections": [{"title": "1 Introduction", "content": "In recent years, the quality of large language models (LLMs) has been constantly increasing (Chiang et al., 2024), achieving impressive results across tasks and benchmarks (Abdin et al., 2024; AI@Meta, 2024; Achiam et al., 2023; Team, 2023; Yang et al., 2024). However, even with the most rigorous filtering heuristics, the training data (Computer, 2023; Penedo et al., 2024) is typically contaminated with undesirable content, that can lead to unacceptable utterances (Bender et al., 2021; Gehman et al., 2020). Moreover, LLMs are pre-trained using an unsupervised next-token log-likelihood objective, which is not necessarily aligned with target behaviors.\nPrior works have utilized supervised fine-tuning (SFT) as a post-training step to align LLMs with desired behaviors such as instruction-following (Zhou et al., 2024; Taori et al., 2023; Chowdhery et al., 2023). To further improve the model's alignment with human preferences, the de-facto approach has been to learn from human/AI generated preference data (e.g., a chosen and a rejected response for each prompt). In particular, off-policy preference optimization methods (*PO) have been especially popular given their good performance and ease of implementation (Rafailov et al., 2024; Hong et al., 2024; Meng et al., 2024).\nOne commonly occurring practice when reporting the performance of new methods is to compare their best-performing variant (after a hyperparameter grid search) to a default baseline with a fixed set of hyperparameters. However, from a practical perspective for future users, these comparisons do not provide a good answer to the problem of which method is expected to achieve higher performance, given a fixed budget for hyperparameter search, as doing broad grid searches is oftentimes computationally infeasible for many practitioners. To this end, in this work, we aim to identify the algorithm that is more robust to hyperparameter variations while still being competitive in performance. We set up our experiments in a realistic out-of-distribution (OOD) setting, focused on safety and helpfulness domains, where the train and test datasets share a common core goal, but their samples are generated from different distributions (e.g., AI and human expert). This setting resembles real-world scenarios as it simulates the release of large generative models for public use.\nTo better understand the behavior of vanilla DPO, we analyze the response length w.r.t. the KL and notice long responses that have low quality (i.e., very low KL). To overcome this issue, we introduce LN-DPO, which adds a simple length-regularizer (similar to SimPO) to vanilla DPO's objective, encouraging the generation of shorter responses\u00b9.\nIn summary, our contributions are as follows:\n\u2022 We comprehensively examine DPO, LN-DPO, and SimPO across a wide range of hyperparameters in a real-world setup, providing insights into the effect of their hyperparameters on performance.\n\u2022 We analyze the performance of these methods on critical metrics such as mean response length, mean score on a gold reward model, win rate vs. chosen and SFT, and KL vs. SFT.\n\u2022 We introduce LN-DPO, a length-normalized version of DPO that is more stable across hyperparameters, effectively reduces the average response length, and improves the performance of the trained models."}, {"title": "2 Related Work", "content": "Since the introduction of DPO (Rafailov et al., 2024), there has been a body of works with new optimization objectives improving the performance and efficiency (Azar et al., 2024; Tang et al., 2024; Hong et al., 2024; Rosset et al., 2024; Meng et al., 2024; Xu et al., 2024a; Ethayarajh et al., 2024) (see Table 2 for examples). These methods can be partitioned into two groups: reference-free (Meng et al., 2024; Hong et al., 2024) and reference-dependent (Rafailov et al., 2024; Park et al., 2024). Reference-free methods generally benefit from fast training runs, while reference-dependent methods have terms baked into their objective to control divergence from the reference model. In this work, we compare SimPO (Meng et al., 2024), a recent state-of-the-art reference-free method, with the DPO as the reference-dependent method, providing practical insights into their performance (see Appendix A for extended related work)."}, {"title": "3 Experimental Setup", "content": ""}, {"title": "3.1 Datasets", "content": "For our datasets, we follow the setup introduced by Xu et al. (2024b). Specifically, we use the double safe/unsafe filtered train subset of SafeRLHF (Dai et al., 2024) for training\u00b2, and the test subset of HH-RLHF (Ganguli et al., 2022) for evaluation. This setup closely resembles real-world scenarios where even though models are trained on various domains (e.g., safety and helpfulness in our experiments), they have to generalize to similar unseen queries while interacting with the users. For example, in our setup, both datasets focus on safety; however, SafeRLHF is human-labeled, while LLM models label HH-RLHF."}, {"title": "3.2 Models", "content": "For all our experiments we use the Phi-3 Medium model\u00b3 (Abdin et al., 2024). We chose this model due to its high performance across benchmarks and small size to ensure computational tractability. To evaluate the trained models, we use the OpenAssistant reward model\u2074 (K\u00f6pf et al., 2024) as an oracle to score the quality of their generated responses. We chose this model due to its small size and high performance on RewardBench (Lambert et al., 2024), ensuring fast and correct evaluations for our experiments."}, {"title": "3.3 Metrics", "content": "Our analysis focuses on the following five metrics:\n\u2022 Mean Score: The average score of the generated responses, as judged by the gold reward model.\n\u2022 Win vs. Chosen: The fraction of samples where the gold reward model assigns a higher score to the generated response compared to the chosen response in the dataset.\n\u2022 Win vs. SFT: The fraction of samples where the gold reward model scores the generated response higher than the initial SFT model's response.\n\u2022 KL divergence: The summed difference of log-probabilities between the SFT and the trained models over the samples.\n\u2022 Response length: The number of tokens in the generated response under the tokenization space of the base model."}, {"title": "3.4 Optimization Objectives", "content": "Considering the performances reported by Meng et al. (2024), we choose DPO as our reference-dependent method and SimPO as our reference-free method. While DPO has an implicit length normalization through the reference model, the variance of the reward (i.e., $log \\frac{\\pi_\\theta(y_w|x)}{\\pi_o (y_w|x)}$) increases with response length. As such, inspired by explicit length regularization in SimPO and R-DPO (Park et al., 2024), we further normalize it with the response length similar to SimPO, which we call LN-DPO. Note that this change makes LN-DPO close to an adaptive margin version of SimPO with per sample margin defined as\n$Y_{w,1} = log \\frac{\\pi_{ref}(Y_w|x)}{|Y_w|} - log \\frac{\\pi_{ref}(Y_l|x)}{|Y_l|}$       (1)\nEssentially, this adaptive margin encourages larger margins for pairs with large margins in the reference policy. Depending on the quality of the reference model and the labels, this change could be beneficial compared to SimPO's constant margin. The adaptive margin focuses more on \"easier\" pairs (i.e., pairs that have some prior evidence to be different) while less on \"harder\" pairs (i.e., pairs that are closer), which means that LN-DPO is potentially less prone to overfitting and less sensitive to wrong labels."}, {"title": "4 Experimental Results", "content": ""}, {"title": "4.1 Hyperparameter Sensitivity", "content": "Best Performance. Following the common practice of comparing the best performance achieved by each method, Table 1 showcases the best result of all three models across our metrics. As evident, at their peaks, SimPO and LN-DPO beat DPO across the board. Moreover, we can observe the effectiveness of the length regularizer, drastically decreasing the mean response length (as much as 22.9%) while also having a higher performance. We also notice a significant decrease in KL divergence (as much as 26%). However, KL and length-normalized KL for SimPO decrease less than LN-DPO, showcasing a more significant divergence from SFT, which we will further analyze in the following. Overall, both SimPO and LN-DPO seem to be suitable replacements for DPO. Note that due to being reference-free, SimPO takes almost half the time to train.\nExpected Performance. Given the limited resources that most users have, it is extremely difficult to run broad hyperparameter searches to find the best-performing set. As such, it becomes crucial to understand the hyperparameter sensitivity, which provides insights into the expectation of finding good hyperparameters set from a limited search. Figure 1 presents the performance distribution *PO methods following a grid search over the hyperparameters denoted in Table 2 and Appendix B. As evident, SimPO and LN-DPO effectively increase the average performance across hyperparameters, showcasing their superiority to DPO. Moreover, we observe that DPO has a relatively high rate of achieving performances worse than the SFT model, which makes the search problem more difficult. Overall, SimPO outperforms the other two models, on average and peak (i.e., the tail-end of the distribution).\nHead-to-head Performance. While comparing the pure performances achieved on the desired met-"}, {"title": "4.2 Response Length", "content": "Since length exploitation is a critical issue (Park et al., 2024), we compare the response lengths across samples generated by the top k% (k\u2208 {1, 10, 25}) of each method's best-performing hyperparameters. As illustrated in Figure 2, on the best set of hyperparameters (i.e., top 1%), the non-DPO methods showcase a left shift in length distribution (compared to DPO), which is a desired effect. However, this phenomenon starts to diminish as we include worse-performing hyperparameters. For example, LN-DPO has a higher rate than DPO in the tail-end of the top 25% distribution. Overall, we observed that both length-regularized models performed superior to DPO, with SimPO producing the shortest responses across the distribution."}, {"title": "4.3 KL Divergence (vs. SFT)", "content": "Since reference-free methods are not regularized against a reference policy (e.g., the SFT model), there is a possibility of reward hacking occurring (i.e., lower loss with degraded performance). Therefore, we compare the KL divergence across samples generated by the top k% (k \u2208 {1, 10, 25}) of each method's best-performing hyperparameters. Figure 3 illustrates the KL distributions. As evident, both SimPO and LN-DPO achieve lower KLs at their peak. However, as we move toward worse-performing models, DPO achieves lower KL (at 10%). While at first, this seems like a pitfall of SimPO and LN-DPO, in reality, we believe it showcases that many DPO runs fail to learn beyond the SFT model, which is a critical shortcoming.\nSince KL divergence is calculated as a summation, there is the possibility that lower values are achieved purely due to responses being shorter. As such, we also calculate a length-normalized KL to account for the effect of longer sequences, which roughly quantifies the average per token distributional difference. Notably, on this metric, SimPO always has distributionally higher values, even at the top 1% (see Figure 9, Appendix D). This observation showcases that SimPO diverges the most from the SFT on a token level, which could be explained by the lack of reference dependence."}, {"title": "4.4 Hyperparameter Tuning Considerations", "content": "DPO. As presented in Figure 4, lower $ \\beta $ leads to higher performances; however, as $ \\beta $ decreases, the performance variance increases, which showcases the method's instability. Overall, $ \\beta = 0.05 $ provides the best balance of stability and performance.\nLN-DPO. While we initially borrowed $ \\beta $'s range from SimPO (Meng et al., 2024), more experiments showed benefits in further decreasing its value. Figure 5 presents the performance spread across different runs. From these experiments, $ \\beta \\in [1.0, 2.0] $ contains most of the best-performing models. Moreover, we observe the relatively low (compared to DPO) variance across the performances, showcasing another benefit of LN-DPO.\nSimPO. In contrast to the other two methods, SimPO has two method-specific hyperparameters: $ \\beta $ and $ \\gamma $. As illustrated in Figure 6, on average, lower $ \\beta $ values lead to better performance. We believe the performance uptick in the lower range is due to a difference in the average length of this work's and the original work's training sets. Moreover, as showcased in Figure 7, the best performing models have a $ \\gamma \\in [1.0, 1.4] $, in line with the suggestion by Meng et al. (2024). Notably, $ \\beta $ and $ \\gamma $ have a relatively low variance across experiments, another upside of SimPO."}, {"title": "42 The Answer to the Ultimate Question", "content": "Based on our collective empirical results, we believe SimPO to be the best starting point among the three methods, mainly due to its robustness toward hyperparameter variations and effective length reduction. As for SimPO's hyperparameters, we recommend $ \\beta \\in [1.0, 1.5] $ and $ \\gamma \\approx 1.2 $. Moreover, while LN-DPO is consistently second-best in most of our experiments, we discuss scenarios for choosing it over SimPO in Appendix F."}, {"title": "5 Conclusion", "content": "In this work, we introduce LN-DPO, a length-normalized variation of DPO that reduces the average response length while staying reference-dependent. Moreover, we present a thorough analysis of LN-DPO and two state-of-the-art reference-dependent and reference-free preference optimization methods in a simulated real-world scenario for safety and helpfulness domains. Specifically, we cover the behavior of these methods across a wide range of hyperparameters under metrics such as mean response length, KL divergence (vs. SFT), and win rate (vs. chosen and SFT). Our experiments showcase state-of-the-art methods' strengths and weaknesses along with providing insights for other practitioners. Finally, we present our empirical insights into tuning *PO hyperparameters, potentially helping to reduce future efforts and costs."}, {"title": "A Extended Related Work", "content": "Online Algorithms. Reinforcement learning from human/AI feedback (RLHF/RLAIF) is among the common approaches for aligning LLMs to human preferences (Christiano et al., 2017; Bai et al., 2022a; Stiennon et al., 2020; Bai et al., 2022b), and has been used to train models such as GPT-4 (Achiam et al., 2023) and Llama-3 (AI@Meta, 2024). In most cases, these approaches are comprised of three stages: 1) supervised fine-tuning (Taori et al., 2023; Zhou et al., 2024; Xia et al., 2024), 2) reward modeling (Gao et al., 2023; Chen et al., 2024; Lightman et al., 2023), and 3) policy optimization (Schulman et al., 2017). The prominent method for policy optimization is Proximal Policy Optimization (PPO) which is an online on-policy approach (Schulman et al., 2017). While PPO has shown promising performances (Stiennon et al., 2020; Ouyang et al., 2022; Achiam et al., 2023), it suffers from problems such as 1) having too many subtle details for reproducibility (Huang et al., 2024b), 2) taking a long time for training (Huang et al., 2024a), and 3) reward over-optimization (Skalse et al., 2022).\nOffline Algorithms. To address the drawbacks of RLHF/RLAIF, recent works have proposed simpler and more efficient offline algorithms, particularly Direct Preference Optimization (DPO) (Rafailov et al., 2024) which is based on the Bradley-Terry model (Bradley and Terry, 1952). These offline algorithms, directly optimize an objective on the preference data with an implicit reward model, without needing to have separate stages. Some recent works have focused on making a broad comparison between PPO and DPO. Specifically, they showcase the potential for PPO with a gold reward model (~ +10%) while underlying the similarity to DPO (~ +1% averaged across benchmarks) when trained on the same data (Ivison et al., 2024; Xu et al., 2024b)."}, {"title": "B Training Regimen", "content": "Following the common practice, before the preference optimization step we do a supervised fine-tuning (SFT) step. Specifically, we first run a grid search over the following hyperparameters: epochs \u2208 {1,3} and learning rate \u2208 {$1e - 6, 3e - 6, 1e - 5, 2e-5$}. Then we evaluate the final checkpoints against the test set and choose the one with the highest performance. This procedure ensures that the preference optimization methods are initialized from a good checkpoint. For the preference optimization methods, we run a grid search using 1) the same ranges as SFT for epochs and learning rate and 2) common values for method-specific hyperparameters as used in prior works (Meng et al., 2024; Rafailov et al., 2024; Hong et al., 2024). Ta-"}, {"title": "C Implementation Details", "content": "We generate all the responses by sampling with a temperature = 0.7, and top_p = 0.95. Moreover, max_generation_length is set to 256 across all experiments, following the setup by Xu et al. (2024b). All our experiments are carried out on a cluster with 256\u00d7A100 80GB GPUs. Finally, we implemented our code using the Transformers (Wolf et al., 2020), TRL (von Werra et al., 2020), and PyTorch (Paszke et al., 2019) libraries."}, {"title": "D Length-Normalized KL", "content": "Figure 9 presents the length normalized KL values across samples generated by the top k% (k\u2208 {1, 10, 25}) of each method's best-performing hyperparameters."}, {"title": "E Mean Length-Normalized KL vs. Mean Score", "content": "Figure 8 presents the spread of mean length normalized KL against mean scores, across runs with different hyperparameters."}, {"title": "F When to use LN-DPO over SimPO?", "content": "While SimPO achieves superior performance on most metrics compared to LN-DPO, the lack of a reference policy regularization could lead to drastic divergence from the initial checkpoint, as also shown in our experiments. This issue then could cause a degradation of performance on other benchmarks, which is a critical pitfall (as also observed in Korbak et al. (2022)). As such, we believe there are various scenarios where LN-DPO should be preferred to SimPO. We leave further experiments over this direction to future works.."}]}