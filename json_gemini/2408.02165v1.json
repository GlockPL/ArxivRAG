{"title": "SelfBC: Self Behavior Cloning for Offline Reinforcement Learning", "authors": ["Shirong Liu", "Chenjia Bai", "Zixian Guo", "Hao Zhang", "Gaurav Sharma", "Yang Liu"], "abstract": "Policy constraint methods in offline reinforcement learning employ additional regularization techniques to constrain the discrepancy between the learned policy and the offline dataset. However, these methods tend to result in overly conservative policies that resemble the behavior policy, thus limiting their performance. We investigate this limitation and attribute it to the static nature of traditional constraints. In this paper, we propose a novel dynamic policy constraint that restricts the learned policy on the samples generated by the exponential moving average of previously learned policies. By integrating this self-constraint mechanism into off-policy methods, our method facilitates the learning of non-conservative policies while avoiding policy collapse in the offline setting. Theoretical results show that our approach results in a nearly monotonically improved reference policy. Extensive experiments on the D4RL MuJoCo domain demonstrate that our proposed method achieves state-of-the-art performance among the policy constraint methods.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) [28] has made great progress in recent years, driven by breakthroughs such as deep Q-learning [19] and policy gradient methods [26]. However, these progressions rely heavily on massive online interactions with the environment, which can be impractical, inefficient, and even risky in real-world scenarios such as autonomous driving [7] and healthcare [36]. Therefore, the ability to learn a policy from existing datasets becomes crucial for practical applications, highlighting the importance of offline reinforcement learning (offline RL) [18].\nTraditional off-policy RL methods often encounter challenges such as value overestimation and policy collapse when applied directly in offline scenarios. These issues arise from the distribution shift between the learned policy and the offline dataset; the learned Q function may suffer from extrapolation errors and overestimate the value of actions that are not observed in the offline dataset, further prompting the learned policy to produce out-of-distribution (OOD) actions. To address these challenges, current offline RL methods integrate regularization techniques to prevent the production of OOD actions. One popular way of regularization is to use policy constraints that explicitly restrict the discrepancy between the learned policy and the offline dataset. Conventional policy constraint methods mainly explore different divergence metrics to restrict the distribution shift, such as behavior cloning (BC) [10, 8], dataset support"}, {"title": "2 Related Works", "content": "TD3+BC with relaxed constraint TD3+BC with policy refinement [4] relaxes the constraint by adding a policy refinement step after the original TD3+BC training is completed, where the weightage of the BC term is reduced during the refinement step. ReBRAC [30] improves performance by relaxing the BC constraint of TD3+BC and integrating several design elements such as larger batches, layer norm for the critic networks, critic penalty, deep networks, and hyperparameter tuning. We focus on dynamic reference policies instead.\nWeighted Behavior Cloning Methods Weighted behavior cloning (WBC) methods aim to address the limitation of BC. As BC is a form of imitation learning, which requires high-quality datasets that perform similar to human experts, wBC can have some sample selective effect on the imperfect datasets. In particular, methods that employ the policy constraint with reverse-KL regularization can derive an advantage-weighted behavior policy as the optimal solution [32, 23, 21]. One line of wBC methods contains three parts: learning a value function for advantage estimation, transforming advantages to weights for dataset samples, and learning a policy to perform weighted behavior cloning. For instance, BAIL [6] learns a value function that evaluates the upper envelope of dataset actions to select high-quality actions from the dataset, and CRR [33] learns a value function that evaluates the current learned policy and designs identifier and exponential functions to compute weights from advantages. WPC [24] integrates the weighted behavior cloning loss into the TD3+BC framework. ABM [27] learns a policy prior via advantage-weighted behavior cloning to formulate the reverse KL constraint. Our method directly updates the reference policy towards current learned policy instead of constructing the reference policy by weighting dataset samples.\nConservative Policy Iteration Conservative Policy Iteration (CPI) [14] guarantees monotonic improvement by incrementally updating the learned policy as stochastic mixtures of consecutive policies. CPI has been integrated into deep reinforcement learning [31]. CPI can be extended to general stochastic policy classes by directly constraining the discrepancy of consecutive policies, such as TRPO [25] and PPO [26]. Behavior Proximal Policy Optimization [40] (BPPO) can improve a previously learned policy by learning from the offline dataset. The constraint of BPPO is the KL divergence between consecutive policies, which is similar to the online PPO. CPI-RE [13] iteratively"}, {"title": "3 Preliminaries", "content": "We consider the standard RL formulation [28] in terms of a Markov Decision Process (MDP) defined as $(\\mathcal{S}, \\mathcal{A}, P, r, \\gamma, \\rho_0)$, where $\\mathcal{S}$ is the state space, $\\mathcal{A}$ is the action space, $P: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{S})$ is the transition function, where $\\Delta(\\mathcal{S})$ is the probability simplex over $\\mathcal{S}$, $r: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is the reward function, $\\gamma$ is the discount factor, and $\\rho_0 \\in \\Delta(\\mathcal{S})$ is the initial state distribution. RL aims to find the optimal policy $\\pi^*: (a|s)$ that maximizes the expected discounted return $J(\\pi) = \\mathbb{E}_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)]$. The Q function and the value function of a policy $\\pi$ are defined as $Q^{\\pi}(s, a) = \\mathbb{E}_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) | s_0 = s, a_0 = a]$ and $V^{\\pi}(s) = \\mathbb{E}_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)| s_0 = s]$, respectively. And the advantage function is $A^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s)$. In the deep RL setting, $\\pi_{\\theta}$ and $Q_{\\phi}$ are used to denote policy and Q networks, respectively, with $\\theta$ and $\\phi$ representing corresponding network parameters.\nIn offline RL, the agent cannot collect samples by interacting with the environment; instead, it must learn the policy from a previously collected dataset $\\mathcal{D} = \\{(s_i, a_i, r_i, s'_i)\\}_{i=1}^N$, where the policy used to collect the dataset is unknown and can be single or mixed. Due to the difficulty of policy evaluation without online sample collection, offline RL is challenging."}, {"title": "4 Analysis of Over Conservatism", "content": "In this section, we examine the issue of over-conservatism and analyze why it persists within the policy constraint framework.\nTo examine the influence of the policy constraint on the asymptotic performance, we add a hyperparameter $\\beta$ into Eq. (1), yielding\n$\\mathbb{E}_{(s, a) \\sim \\mathcal{D}} [Q_{\\phi_1}(s, \\pi_{\\theta}(s)) - \\beta (\\pi_{\\theta}(s) - a)^2]$\nwhere the coefficient $\\alpha$ is dropped since we do not change it."}, {"title": "5 Proposed Method", "content": "We first introduce the general behavior cloning framework, which learns a deterministic policy $\\pi_{\\theta}$ to clone the samples produced by a general reference policy $\\pi_{ref}$. Then we demonstrate that behavior-cloning-based approaches used in existing policy constraint methods to learn a deterministic policy [10, 6, 33, 24] are special cases of cloning a general reference policy."}, {"title": "5.1 General Behavior Cloning Framework", "content": "We start by introducing the weighted behavior cloning objective:\n$\\mathbb{E}_{s \\sim \\mathcal{D}, a \\sim \\pi^\\rho(\\cdot|s)} [w(s, a) \\cdot (\\pi_{\\theta}(s) - a)^2]$\nwhere $\\pi_D$ denotes the ground truth behavior policy of the offline dataset $\\mathcal{D}$, and $w(s, a)$ is the weight function. This formulation involves an expectation over $(s, a)$ pairs. By moving the expectation over $a$ inside the behavior cloning term, we obtain the following equivalent objective with the expectation solely over $s$ (a simplified proof is provided below, full proof is provided in Appendix A):\n$\\mathbb{E}_{s \\sim \\mathcal{D}} [(\\pi_{\\theta}(s) - \\mathbb{E}_{a \\sim q_w(\\cdot|s)} [a])^2]$\nwhere $q_w(a|s) = \\frac{\\pi_D(a|s) \\cdot w(s, a)}{\\int \\pi_D(a|s) w(s, a) da}$ represents the normalized weighted behavior policy of the dataset.\n**Proof.** Considering given a state $s$,\n$\\begin{aligned}\n& (\\pi_{\\theta}(s) - \\mathbb{E}_{a \\sim q_w(:|s)} [a])^2 = (\\mathbb{E}_{a \\sim q_w(:|s)} [\\pi_{\\theta}(s) - a])^2 \\\\\n= & \\mathbb{E}_{a \\sim q_w(:|s)} [(\\pi_{\\theta}(s) - a)^2] - \\mathrm{Var}_{a \\sim q_w(:|s)} [a] \\\\\n= & \\int \\frac{\\pi_D(a|s) w(s, a)}{\\int \\pi_D(a|s) w(s, a)da} (\\pi_{\\theta}(s) - a)^2 da = \\frac{1}{\\int \\pi_D(a|s) w(s, a)da} \\mathbb{E}_{a \\sim \\pi_D(:|s)} [w(s, a) \\cdot (\\pi_{\\theta}(s) - a)^2]\n\\end{aligned}$\nwhere the variance in the second equation is independent of $\\pi_{\\theta}$.\nNow we consider two particular cases of Eq. (6).\n**Expected Behavior Cloning (EBC)** First, note that when we set $w(s, a) = 1$ for all $(s, a) \\in \\mathcal{D}$, we get an equivalent of the naive behavior cloning objective, given by\n$\\mathbb{E}_{s \\sim \\mathcal{D}} [(\\pi_{\\theta}(s) - \\mathbb{E}_{a \\sim \\pi_D(\\cdot|s)} [a])^2]$\nTo implement Eq. (8), we do not need to precisely model $\\pi_D(\\cdot|s)$, as it is often multi-modal and complex. Instead, we learn the expected action $\\mathbb{E}_{a \\sim \\pi_D(\\cdot|s)} [a]$, which is deterministic given a state $s$ and can be easily learned by behavior cloning. We can learn a neural network-modeled behavior policy $\\pi_b$ through behavior cloning:\n$\\pi_b = \\arg \\min_{\\pi} \\mathbb{E}_{(s, a) \\in \\mathcal{D}} [(\\pi(s) - a)^2]$\nAnd Eq. (8) can be rewritten as:\n$\\mathcal{L}_{EBC}(\\theta) = \\mathbb{E}_{s \\sim \\mathcal{D}} [(\\pi_{\\theta}(s) - \\pi_b(s))^2]$\nAs $\\pi_b(s)$ approximates the expected action $\\mathbb{E}_{a \\sim \\pi_D(\\cdot|s)} [a]$, we refer to $\\mathcal{L}_{EBC}(\\theta)$ as the Expected Behavior Cloning (EBC) objective. Here, $\\pi_b$ is a special case of reference policy restricting $\\pi_{\\theta}$."}, {"title": "5.2 Self Behavior Cloning", "content": "Expected Weighted Behavior Cloning (EWBC) Second, note that when w(s, a) is an arbitrary function, the reference policy corresponding to Eq. (6) is $q_w$, which is a special reference policy corresponding to w and $\\pi_D$. We call such a reference policy, $q_w$, the expected weighted behavior policy. This indicates that the weighted behavior cloning method in Eq. (5) is actually a special case of modifying the reference policy by designing a special weight function.\nGeneral Behavior Cloning We introduce the general behavior cloning objective that involves a free-design reference policy $\\pi_{ref}$ as\n$\\mathcal{L}_{General BC}(\\theta) = \\mathbb{E}_{s \\sim \\mathcal{D}} [(\\pi_{\\theta}(s) - \\mathbb{E}_{a \\sim \\pi_{ref}(\\cdot|s)} [a])^2]$\nThis objective covers the two cases mentioned above ($\\pi_{ref} = \\pi_b$ for EBC, $\\pi_{ref} = q_w$ for EWBC). Both EBC and EWBC clone reference policies that derive from static behavior of offline dataset. We propose that, in the case of general behavior cloning, it is not necessary to design the reference policy that explicitly follows the static behavior of offline dataset, which may lead to suboptimal performance as shown in Section 4. Rather, the design of $\\pi_{ref}$ can be flexible as long as it maintains a low distribution shift from the offline dataset behavior, ensuring the preservation of valuable in-distribution information from the offline dataset. We introduce our proposed reference policy design in the next section."}, {"title": "5.3 Ensemble Reference Policies", "content": "We first introduce our design of the reference policy for Eq. (11), and then integrate our policy constraint into an off-policy algorithm. The underlying logic of our reference policy design is as follows:\n* The reference policy should be dynamically updated. Based on Section 4, the static reference policy is not suitable for the policy search with a large distribution shift, which restricts the algorithm performance.\n* The performance of the reference policy should surpass the offline dataset and it would be advantageous if the performance of the reference policy is monotonically increased.\nTo this end, we propose the Self Behavior Cloning (SelfBC) objective. In the general BC objective in Eq. (11), our approach employs a novel reference policy $\\pi_{\\tilde{\\theta}}$, which is the EMA of previously learned policies, yielding the SelfBC objective\n$\\mathcal{L}_{SelfBC}(\\theta) = \\mathbb{E}_{s \\sim \\mathcal{D}} [(\\pi_{\\theta}(s) - \\pi_{\\tilde{\\theta}}(s))^2]$\nNote that $\\pi_{\\tilde{\\theta}}$ is deterministic here. To progressively increase the performance of such reference policy, we integrate our SelfBC into the off-policy algorithm TD3 and obtain our TD3+SelfBC objective:\n$\\mathcal{L}_{TD3+SelfBC}(\\theta) = \\mathbb{E}_{s \\sim \\mathcal{D}} [\\alpha Q(s, \\pi_{\\theta}(s)) - (\\pi_{\\theta}(s) - \\pi_{\\tilde{\\theta}}(s))^2]$\nwhere the reference policy $\\pi_{\\tilde{\\theta}}$ is progressively updated towards the latest learned policy $\\pi_{\\theta}$ by EMA:\n$\\tilde{\\theta} = \\tau_{ref} \\tilde{\\theta} + (1 - \\tau_{ref}) \\theta$\nwhere $\\tau_{ref}$ represents the soft update ratio, set to be near zero."}, {"title": "5.4 Theoretical Analysis", "content": "Noise in the reference policy updating process can destabilize the reference policy and lead to policy collapse. We, therefore, proposed to mitigate the effect of the noise using an ensemble training approach that we outline here. The full details are in Appendix D.\nIn the proposed TD3+SelfBC, the reference policy is influenced by two factors: the pre-trained policy and the learned policy. The former initializes the reference policy, while the latter updates it. However, both factors may inject noise into the reference policy, even though the EMA process may reduce the impact of the noise from the learned policy. To reduce the noise, we propose to simultaneously train an ensemble of TD3+SelfBC trainers. Note that although each trainer maintains its own Q networks, we do not ensemble all the Q networks of all trainers for uncertainty estimation as existing uncertainty-based methods for offline RL [3, 2]. Most parts of each trainer are independent of other trainers, such as Q learning, target network updating, and reference policy updating, except the policy objective uses a shared reference action for all trainers. Specifically, the shared reference action is computed by averaging the reference actions from all trainers. Assume that the number of ensemble is Nens and ensemble learned policies and ensemble reference policies are $\\{\\pi_{\\theta}^i, \\pi_{\\tilde{\\theta}}^i\\}_{i = 1, 2, ..., N_{ens} \\}$. Given a state $s$, the shared reference action $\\tilde{a}$ is defined as:\n$\\tilde{a} = \\frac{1}{N_{ens}} \\sum_{i=1}^{N_{ens}} \\pi_{\\tilde{\\theta}}^i(s)$\nWe train each individual trainer separately using the same $\\tilde{a}$:\n$\\mathcal{L}_{TD3+ESBC} (\\theta^i) = \\mathbb{E}_{s \\sim \\mathcal{D}} [\\alpha Q_{\\phi^i}(s, \\pi_{\\theta^i}(s)) - (\\pi_{\\theta^i}(s) - \\tilde{a})^2]$\nBy combining the conservative policy iteration (CPI) [14, 31] and the behavior proximal policy optimization (BPPO) [40] we develop theoretical analysis for TD3+SelfBC that can be seen as an extension of the CPI to the offline setting. In our analysis, which closely follows [40], we consider the stochastic policy case; the deterministic policy can be seen as a special stochastic policy with near-zero variance. We first formulate our dynamic reference policy as CPI. Then we discuss the performance bound of the reference policy in the offline setting. Finally, we show that the reference policy in our TD3+SelfBC can achieve nearly monotonic improvement."}]}