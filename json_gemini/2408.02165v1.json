{"title": "SelfBC: Self Behavior Cloning for Offline\nReinforcement Learning", "authors": ["Shirong Liu", "Chenjia Bai", "Zixian Guo", "Hao Zhang", "Gaurav Sharma", "Yang Liu"], "abstract": "Policy constraint methods in offline reinforcement learn-\ning employ additional regularization techniques to constrain the dis-\ncrepancy between the learned policy and the offline dataset. How-\never, these methods tend to result in overly conservative policies that\nresemble the behavior policy, thus limiting their performance. We\ninvestigate this limitation and attribute it to the static nature of tradi-\ntional constraints. In this paper, we propose a novel dynamic policy\nconstraint that restricts the learned policy on the samples generated\nby the exponential moving average of previously learned policies.\nBy integrating this self-constraint mechanism into off-policy meth-\nods, our method facilitates the learning of non-conservative policies\nwhile avoiding policy collapse in the offline setting. Theoretical re-\nsults show that our approach results in a nearly monotonically im-\nproved reference policy. Extensive experiments on the D4RL Mu-\nJoCo domain demonstrate that our proposed method achieves state-\nof-the-art performance among the policy constraint methods.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) [28] has made great progress in recent\nyears, driven by breakthroughs such as deep Q-learning [19] and pol-\nicy gradient methods [26]. However, these progressions rely heavily\non massive online interactions with the environment, which can be\nimpractical, inefficient, and even risky in real-world scenarios such\nas autonomous driving [7] and healthcare [36]. Therefore, the ability\nto learn a policy from existing datasets becomes crucial for practi-\ncal applications, highlighting the importance of offline reinforcement\nlearning (offline RL) [18].\nTraditional off-policy RL methods often encounter challenges\nsuch as value overestimation and policy collapse when applied di-\nrectly in offline scenarios. These issues arise from the distribution\nshift between the learned policy and the offline dataset; the learned\nQ function may suffer from extrapolation errors and overestimate the\nvalue of actions that are not observed in the offline dataset, further\nprompting the learned policy to produce out-of-distribution (OOD)\nactions. To address these challenges, current offline RL methods in-\ntegrate regularization techniques to prevent the production of OOD\nactions. One popular way of regularization is to use policy con-\nstraints that explicitly restrict the discrepancy between the learned\npolicy and the offline dataset. Conventional policy constraint meth-\nods mainly explore different divergence metrics to restrict the distri-\nbution shift, such as behavior cloning (BC) [10, 8], dataset support"}, {"title": "2 Related Works", "content": "TD3+BC with relaxed constraint TD3+BC with policy refine-\nment [4] relaxes the constraint by adding a policy refinement step af-\nter the original TD3+BC training is completed, where the weightage\nof the BC term is reduced during the refinement step. ReBRAC [30]\nimproves performance by relaxing the BC constraint of TD3+BC and\nintegrating several design elements such as larger batches, layer norm\nfor the critic networks, critic penalty, deep networks, and hyperpa-\nrameter tuning. We focus on dynamic reference policies instead.\nWeighted Behavior Cloning Methods Weighted behavior cloning\n(WBC) methods aim to address the limitation of BC. As BC is a\nform of imitation learning, which requires high-quality datasets that\nperform similar to human experts, wBC can have some sample se-\nlective effect on the imperfect datasets. In particular, methods that\nemploy the policy constraint with reverse-KL regularization can de-\nrive an advantage-weighted behavior policy as the optimal solution\n[32, 23, 21]. One line of wBC methods contains three parts: learn-\ning a value function for advantage estimation, transforming advan-\ntages to weights for dataset samples, and learning a policy to per-\nform weighted behavior cloning. For instance, BAIL [6] learns a\nvalue function that evaluates the upper envelope of dataset actions\nto select high-quality actions from the dataset, and CRR [33] learns\na value function that evaluates the current learned policy and de-\nsigns identifier and exponential functions to compute weights from\nadvantages. WPC [24] integrates the weighted behavior cloning loss\ninto the TD3+BC framework. ABM [27] learns a policy prior via\nadvantage-weighted behavior cloning to formulate the reverse KL\nconstraint. Our method directly updates the reference policy towards\ncurrent learned policy instead of constructing the reference policy by\nweighting dataset samples.\nConservative Policy Iteration Conservative Policy Iteration (CPI)\n[14] guarantees monotonic improvement by incrementally updating\nthe learned policy as stochastic mixtures of consecutive policies. CPI\nhas been integrated into deep reinforcement learning [31]. CPI can be\nextended to general stochastic policy classes by directly constraining\nthe discrepancy of consecutive policies, such as TRPO [25] and PPO\n[26]. Behavior Proximal Policy Optimization [40] (BPPO) can im-\nprove a previously learned policy by learning from the offline dataset.\nThe constraint of BPPO is the KL divergence between consecutive\npolicies, which is similar to the online PPO. CPI-RE [13] iteratively"}, {"title": "3 Preliminaries", "content": "3.1 Offline Reinforcement Learning\nWe consider the standard RL formulation [28] in terms of a Markov\nDecision Process (MDP) defined as (S, A, P, r, \u03b3, \u03c10), where S is\nthe state space, A is the action space, \\(P: S \\times A \\rightarrow \\Delta(S)\\)\nis the transition function, where \\(\\Delta(S)\\) is the probability simplex\nover S, \\(r: S\\times A \\rightarrow \\mathbb{R}\\) is the reward function, \u03b3 is the dis-\ncount factor, and \u03c10 \u2208 \\(\\Delta S\\) is the initial state distribution. RL\naims to find the optimal policy \u03c0\u03b8(a|s) that maximizes the ex-\npected discounted return \\(J(\\pi) = \\mathbb{E}_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)]\\). The\nQ function and the value function of a policy \u03c0 are defined as\n\\(Q^{\\pi}(s, a) = \\mathbb{E}_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) | s_0 = s, a_0 = a]\\) and \\(V^{\\pi}(s) =\n\\mathbb{E}_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)| s_0 = s]\\), respectively. And the advantage\nfunction is \\(A^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s)\\). In the deep RL setting,\n\u03c0\u03b8 and Q\u03c6 are used to denote policy and Q networks, respectively,\nwith \u03b8 and \u03c6 representing corresponding network parameters.\nIn offline RL, the agent cannot collect samples by interacting with\nthe environment; instead, it must learn the policy from a previously\ncollected dataset \\(D = \\{(s_i, a_i, r_i, s'_i)\\}_{i=1}^N\\), where the policy used to\ncollect the dataset is unknown and can be single or mixed. Due to\nthe difficulty of policy evaluation without online sample collection,\noffline RL is challenging.\n3.2 TD3+BC\nTD3+BC is a minimalist solution to offline RL, based on TD3, it\nonly adds a behavior cloning (BC) term to the policy objective and\nbalances the BC term and the original Q term by a hyperparameter\n\u03b1. TD3+BC maintains two learned Q networks \\(Q_{\\phi_1}, Q_{\\phi_2}\\), a learned\npolicy network \u03c0\u03b8 and their target networks \\(Q_{\\overline{\\phi}_1}, Q_{\\overline{\\phi}_2}, \\pi_{\\overline{\\theta}}\\). The pol-\nicy optimization objective of TD3+BC is:\n\n\n\n\\(L_{TD3+BC} (\\theta) = \\mathbb{E}_{(s,a)\\sim D} [A Q_{\\phi_1} (s, \\pi_{\\theta} (s)) - (\\pi_{\\theta} (s) - a)^2]\\)\n\n\n\nTo unify the hyperparameter settings in different offline datasets,\nTD3+BC additionally normalizes the Q term in Eq. (1).\nTD3+BC learns the Q networks in the same way as TD3:\n\n\n\n\\(C_{TD3+BC} (\\phi_i) = \\mathbb{E}_{(s,a,r,s')\\sim D} [(Q_{\\phi_i}(s, a) - y)^2], i = 1,2\\)\n\n\n\nwhere y is the target and its computation incorporates techniques of\nclip double q-learning and target policy smooth regularization [11]:\n\n\n\n\\(y = r + \\gamma \\min_{i=1,2} Q_{\\overline{\\phi}_i}(s', a')\\)\n\n\\(a' = \\pi_{\\overline{\\theta}}(s') + \\epsilon, \\epsilon \\sim clip(\\mathcal{N} (0, \\sigma), -c, c)\\)\n\n\n\nwhere \u03c3 is the action noise scale and c is the clip range.\n4 Analysis of Over Conservatism\nIn this section, we examine the issue of over-conservatism and ana-\nlyze why it persists within the policy constraint framework.\nTo examine the influence of the policy constraint on the asymptotic\nperformance, we add a hyperparameter \u03b2 into Eq. (1), yielding\n\n\n\n\\(\\mathbb{E}_{(s,a)\\sim D} [Q_{\\phi_1} (s, \\pi_{\\theta} (s)) - \\beta (\\pi_{\\theta} (s) - a)^2]\\)\n\n\nwhere the coefficient \u03b1 is dropped since we do not change it."}, {"title": "5 Proposed Method", "content": "5.1 General Behavior Cloning Framework\nWe first introduce the general behavior cloning framework, which\nlearns a deterministic policy \u03c0\u03b8 to clone the samples produced by\na general reference policy \u03c0ref. Then we demonstrate that behavior-\ncloning-based approaches used in existing policy constraint methods\nto learn a deterministic policy [10, 6, 33, 24] are special cases of\ncloning a general reference policy.\nWe start by introducing the weighted behavior cloning objective:\n\n\n\n\\(\\mathbb{E}_{s\\sim D,a\\sim\\pi^{\\rho}(\\cdot|s)} [w(s, a) \\cdot (\\pi_{\\theta} (s) - a)^2]\\)\n\n\n\nwhere \\(\\pi^{\\rho}\\) denotes the ground truth behavior policy of the offline\ndataset D, and w(s, a) is the weight function. This formulation in-\nvolves an expectation over (s, a) pairs. By moving the expectation\nover a inside the behavior cloning term, we obtain the following\nequivalent objective with the expectation solely over s (a simplified\nproof is provided below, full proof is provided in Appendix A):\n\n\n\n\\(\\mathbb{E}_{s\\sim D} [(\\pi_{\\theta}(s) - \\mathbb{E}_{a\\sim q_w(\\cdot|s)} [a])^2]\\)\n\n\n\nwhere \\(q_w(a|s) = \\frac{\\pi^{\\rho}(a|s).w(s,a)}{\\int \\pi^{\\rho}(a|s) w(s,a)da}\\) represents the normalized\nweighted behavior policy of the dataset.\nProof. Considering given a state s,\n\n\n\n\\[(\\pi_{\\theta}(s) - \\mathbb{E}_{a\\sim q_w(:|s)} [a])^2 = (\\mathbb{E}_{a\\sim q_w(:|s)} [\\pi_{\\theta} (s) - a])^2\\]\n\\[\\mathbb{E}_{a\\sim q_w(:|s)} [(\\pi_{\\theta} (s) - a)^2] - Var_{a\\sim q_w(:|s)} [a]\\]\n\\[\\mathbb{E}_{a\\sim \\pi^{\\rho}(:|s)} [w(s, a) \\cdot (\\pi_{\\theta} (s) - a)^2]\\]\n\n\n\nwhere the variance in the second equation is independent of \u03c0\u03b8.\nNow we consider two particular cases of Eq. (6).\nExpected Behavior Cloning (EBC) First, note that when we set\nw(s, a) = 1 for all (s, a) \u2208 D, we get an equivalent of the naive\nbehavior cloning objective, given by\n\n\n\n\\(\\mathbb{E}_{s\\sim D} [(\\pi_{\\theta} (s) - \\mathbb{E}_{a\\sim \\pi^{\\rho}(\\cdot|s)} [a])^2]\\)\n\n\n\nTo implement Eq. (8), we do not need to precisely model \\(\\pi^{\\rho} (\\cdot|s)\\), as\nit is often multi-modal and complex. Instead, we learn the expected\naction \\(\\mathbb{E}_{a\\sim \\pi^{\\rho}(\\cdot|s)} [a]\\), which is deterministic given a state s and can\nbe easily learned by behavior cloning. We can learn a neural network-\nmodeled behavior policy \u03c0b through behavior cloning:\n\n\n\n\\(\\pi_\\text{b} = arg \\min_{\\pi} \\mathbb{E}_{(s,a)\\in D} [(\\pi_{\\text{b}} (s) - a)^2]\\)\n\n\n\nAnd Eq. (8) can be rewritten as:\n\n\n\n\\(L_{EBC} (\\theta) = \\mathbb{E}_{s\\sim D} [(\\pi_{\\theta} (s) - \\pi_{\\text{b}} (s))^2]\\)\n\n\n\nAs \u03c0b(s) approximates the expected action \\(\\mathbb{E}_{a\\sim \\pi^{\\rho}(\\cdot|s)} [a]\\), we re-\nfer to \\(\\mathcal{L}_{EBC} (\\theta)\\) as the Expected Behavior Cloning (EBC) objective.\nHere, \u03c0b is a special case of reference policy restricting \u03c0\u03b8."}, {"title": "5.2 Self Behavior Cloning", "content": "We first introduce our design of the reference policy for Eq. (11), and\nthen integrate our policy constraint into an off-policy algorithm. The\nunderlying logic of our reference policy design is as follows:\n\u2022 The reference policy should be dynamically updated. Based on\nSection 4, the static reference policy is not suitable for the policy\nsearch with a large distribution shift, which restricts the algorithm\nperformance.\n\u2022 The performance of the reference policy should surpass the offline\ndataset and it would be advantageous if the performance of the\nreference policy is monotonically increased.\nTo this end, we propose the Self Behavior Cloning (SelfBC) objec-\ntive. In the general BC objective in Eq. (11), our approach employs\na novel reference policy \\(\\pi_{\\tilde{\\theta}}\\), which is the EMA of previously learned\npolicies, yielding the SelfBC objective\n\n\n\n\\(L_{SelfBC} (\\theta) = \\mathbb{E}_{s\\sim D} [(\\pi_{\\theta} (s) - \\pi_{\\tilde{\\theta}} (s))^2]\\)\n\n\n\nNote that \\(\\pi_{\\tilde{\\theta}}\\) is deterministic here. To progressively increase the per-\nformance of such reference policy, we integrate our SelfBC into the\noff-policy algorithm TD3 and obtain our TD3+SelfBC objective:\n\n\n\n\\(L_{TD3+SelfBC} (\\theta)=\\mathbb{E}_{s\\sim D} [A Q (s, \\pi_{\\theta} (s)) - (\\pi_{\\theta} (s) -\\pi_{\\tilde{\\theta}} (s))^2]\\)\n\n\n\nwhere the reference policy \\(\\pi_{\\tilde{\\theta}}\\) is progressively updated towards the\nlatest learned policy \u03c0\u03b8 by EMA:\n\n\n\n\\(\\tilde{\\theta} = \\tau_{ref} \\tilde{\\theta} + (1 - \\tau_{ref}) \\theta\\)\n\n\n\nwhere \u03c4ref represents the soft update ratio, set to be near zero."}, {"title": "5.3 Ensemble Reference Policies", "content": "Noise in the reference policy updating process can destabilize the ref-\nerence policy and lead to policy collapse. We, therefore, proposed to\nmitigate the effect of the noise using an ensemble training approach\nthat we outline here. The full details are in Appendix D.\nIn the proposed TD3+SelfBC, the reference policy is influenced\nby two factors: the pre-trained policy and the learned policy. The for-\nmer initializes the reference policy, while the latter updates it. How-\never, both factors may inject noise into the reference policy, even\nthough the EMA process may reduce the impact of the noise from\nthe learned policy. To reduce the noise, we propose to simultane-\nously train an ensemble of TD3+SelfBC trainers. Note that although\neach trainer maintains its own Q networks, we do not ensemble all\nthe Q networks of all trainers for uncertainty estimation as existing\nuncertainty-based methods for offline RL [3, 2]. Most parts of each\ntrainer are independent of other trainers, such as Q learning, target\nnetwork updating, and reference policy updating, except the policy\nobjective uses a shared reference action for all trainers. Specifically,\nthe shared reference action is computed by averaging the reference\nactions from all trainers. Assume that the number of ensemble is Nens\nand ensemble learned policies and ensemble reference policies are\n{\\(\\pi^i_{\\theta}, \\pi^i_{\\tilde{\\theta}}\\)_{i = 1, 2, ..., N_{ens}}\\)}. Given a state s, the shared reference ac-\ntion \u00e3 is defined as:\n\n\n\n\\tilde{a} = \\frac{1}{N_{ens}} \\sum_{i=1}^{N_{ens}} \\pi_{\\tilde{\\theta}}^{i}(s)\n\n\n\nWe train each individual trainer separately using the same \u00e3:\n\n\n\n\\(L_{TD3+ESBC} (\\theta^{i}) = \\mathbb{E}_{s\\sim D} [A Q_{\\phi^{i}} (s, \\pi_{\\theta^{i}} (s)) - (\\pi_{\\theta^{i}} (s) -\\tilde{a})]\\)\n\n\n5.4 Theoretical Analysis\nBy combining the conservative policy iteration (CPI) [14, 31] and\nthe behavior proximal policy optimization (BPPO) [40] we develop\ntheoretical analysis for TD3+SelfBC that can be seen as an exten-\nsion of the CPI to the offline setting. In our analysis, which closely\nfollows [40], we consider the stochastic policy case; the determinis-\ntic policy can be seen as a special stochastic policy with near-zero\nvariance. We first formulate our dynamic reference policy as CPI.\nThen we discuss the performance bound of the reference policy in\nthe offline setting. Finally, we show that the reference policy in our\nTD3+SelfBC can achieve nearly monotonic improvement.\nDefinition 1. We denote \u03c0\u03ba as the reference policy of the k-th itera-\ntion, \u03c0' as the new learned policy, and \\(\\tilde{\\pi}\\) as the new reference policy.\nWe update the reference policy by CPI:\n\n\n\n\\(\\tilde{\\pi} (\\cdot|s) = (1 - \\kappa) \\pi_k (\\cdot|s) + \\kappa \\pi' (\\cdot|s)\\)\n\n\n\nwhere hyperparameter \u03ba controls the conservative update process.\nLemma 1. [14, 40] The difference in expected discounted return\nbetween two arbitrary policies \u03c0', \u03c0is\n\n\n\n\\(J_{\\Delta} (\\pi',\\pi) = J(\\pi') - J(\\pi) = \\mathbb{E}_{s\\sim\\rho_{\\pi'}(\\cdot),a\\sim\\pi'(\\cdot|s)} [A^{\\pi} (s, a)]\\)\n\n\n\nwhere \\(\\rho_{\\pi}(s) = \\sum_{t=0}^{\\infty}\\gamma^tP(s_t = s | \\pi)\\) represents the unnormalized\ndiscounted state visitation frequencies.\nWe then define the performance difference of two consecutive ref-\nerence policies using Lemma 1, we also approximate the difference\non the offline dataset:\nDefinition 2. The performance difference of consecutive reference\npolicies \\(J_{\\Delta} (\\pi, \\pi_k)\\), and its approximation \\(\\hat{J}_{\\Delta} (\\pi, \\pi_k)\\) on offline\ndataset D are defined as:\n\n\n\n\\(J_{\\Delta} (\\pi, \\pi_k) = \\mathbb{E}_{s\\sim\\rho_{\\pi}(\\cdot), a\\sim\\pi(\\cdot|s)} [A^{\\pi_k} (s,a)]\\)\n\n\n\n\\(\\hat{J}_{\\Delta} (\\pi, \\pi_k) = \\mathbb{E}_{s\\sim\\rho_D(\\cdot),a\\sim\\pi(\\cdot|s)} [A^{\\pi_k} (s,a)]\\)\n\n\n\nWe state performance bounds for our conservatively updated ref-\nerence policy as the following theorem, whose proof is deferred to\nAppendix F due to space limitations.\nTheorem 1. The performance difference between two consecutive\nreference policies influenced by the offline approximation satisfies\n\n\n\n\\(J_{\\Delta} (\\pi, \\pi_k) > \\hat{J}_{\\Delta} (\\pi, \\pi_k)\\)\n\n\n\n\\[- \\frac{2\\gamma \\kappa^2}{1-\\gamma} A_{\\pi',\\pi_k} \\mathbb{E}_{s\\sim\\rho_{\\pi_k}} [D_{TV} (\\pi' ||\\pi_k) [s]]\\]\n\\[- \\frac{2\\gamma \\kappa}{1-\\gamma} A_{\\pi',\\pi_k}. \\mathbb{E}_{s\\sim\\rho_{\\pi_k}} [D_{TV} (\\pi_k||\\pi_b) [s]]\\]\n\\[- \\frac{\\gamma \\kappa}{1-\\gamma} A_{\\pi',\\pi_k} \\mathbb{E}_{s\\sim\\rho_D} [1_{\\pi_b} (a|s)]\\]\n\n\n\nwhere \u03c0, \u03c0\u03ba, \u03c0' are as in Definition 1,\n\n\n\n\\(A_{\\pi',\\pi_k} = 2 \\max_{s,a} |A^{\\pi_k} (s, a)| \\max_s D_{TV} (\\pi' ||\\pi_k) [s]\\)\n\n\n\nand,\n\n\n\n\\[\\hat{J}_{\\Delta} (\\pi,\\pi_k) = \\mathbb{E}_{s\\sim\\rho_D(\\cdot),a\\sim\\pi(\\cdot|s)} [A^{\\pi_k} (s, a)]\\]\n\\[= (1 - \\kappa)\\mathbb{E}_{s\\sim\\rho_D(\\cdot),a\\sim\\pi_k(\\cdot|s)} [A^{\\pi_k} (s, a)]\\]\n\\[+ \\kappa\\mathbb{E}_{s\\sim\\rho_D(\\cdot),a\\sim\\pi'(\\cdot|s)} [A^{\\pi_k} (s, a)]\\]\n\\[= \\kappa\\mathbb{E}_{s\\sim\\rho_D(\\cdot),a\\sim\\pi'(\\cdot|s)} [A^{\\pi_k} (s, a)]\\]\n\n\n\nTheorem 1 establishes that the performance difference \\(J_{\\Delta} (\\pi, \\pi_k)\\)\nbetween two consecutive reference policies can be lower bounded by\nits offline approximation \\(\\hat{J}_{\\Delta} (\\pi,\\pi_k)\\) and some total variance (TV)\ndivergence terms. The first of the TV terms represents the discrep-\nancy between the new learned policy \u03c0' and the old reference policy\n\u03c0\u03ba, the second TV term represents the distribution shift of the old\nreference policy \u03c0\u03ba from the approximated behavior policy \u03c0b, and\nthe third TV term can be seen as the approximation error of the be-\nhavior policy \u03c0b. Note that only the first of the TV terms is related to\n\u03c0', while the other two TV terms can be seen as constants. Thus, we\nhave the following conclusion.\nConclusion 1. To ensure monotonic improvement in the expected\ndiscounted return under the offline setting, it suffices to maintain pos-\nitive and maximize the lower bound in Eq. (21), which corresponds\nto simultaneously maximizing \\(\\mathbb{E}_{s\\sim D,a\\sim\\pi'(\\cdot|s)} [A^{\\pi_k} (s, a)]\\) and mini-\nmizing \\(\\max_s [D_{TV} (\\pi'||\\pi_k) [s]]\\).\nThe connection between our TD3+SelfBC and Conclusion 1\n\u2022 Conservative policy update: the EMA process of TD3+SelfBC in\nEq. (14) can be seen as a conservative update step of the reference\npolicy in Definition 1.\n\u2022 Policy learning: the TD3+SelfBC objective in Eq. (13) can be\ntransformed to \\(\\mathbb{E}_{s\\sim D} [A_{Q_{\\phi}} (s, \\pi' (s)) - (\\pi' (s) -\\pi_k (s))^2]\\)\nusing Definition 1, which learns a policy \u03c0' in almost the same way\nas Conclusion 1. Thus, TD3+SelfBC can achieve a nearly mono-\ntonically improved reference policy."}, {"title": "7 Conclusion", "content": "In this work, we empirically demonstrate the limitation of TD3+BC\nand attribute it to the reliance on the static reference policy. We\nsummarized previously weighted behavior cloning as a special de-\nsign of the reference policy for the general behavior cloning. To ad-\ndress these limitations, we propose Self Behavior Cloning (SelfBC),\na novel dynamic policy constraint for offline reinforcement learn-\ning. SelfBC progressively updates the reference policy towards\nthe learned policy during training. By integrating this dynamic\nself-constraint mechanism into the off-policy algorithm TD3, our\nTD3+SelfBC method can learn an non-conservative policy and avoid\nthe policy collapse. We theoretically show that our conservatively\nupdated reference policy can achieve nearly monotonic improve-\nment. Extensive experiments on the D4RL MuJoCo benchmark have\ndemonstrated the excellent performance of our approach. In the fu-\nture, we will explore alternative mechanisms for updating the refer-\nence policy and extend SelfBC to learn stochastic policies."}]}