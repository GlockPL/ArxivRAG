{"title": "GaussianAD: Gaussian-Centric End-to-End Autonomous Driving", "authors": ["Wenzhao Zheng", "Junjie Wu", "Yao Zheng", "Sicheng Zuo", "Zixun Xie", "Longchao Yang", "Yong Pan", "Zhihui Hao", "Peng Jia", "Xianpeng Lang", "Shanghang Zhang"], "abstract": "Vision-based autonomous driving shows great potential due to its satisfactory performance and low costs. Most existing methods adopt dense representations (e.g., bird's eye view) or sparse representations (e.g., instance boxes) for decision-making, which suffer from the trade-off between comprehensiveness and efficiency. This paper explores a Gaussian-centric end-to-end autonomous driving (GaussianAD) framework and exploits 3D semantic Gaussians to extensively yet sparsely describe the scene. We initialize the scene with uniform 3D Gaussians and use surrounding-view images to progressively refine them to obtain the 3D Gaussian scene representation. We then use sparse convolutions to efficiently perform 3D perception (e.g., 3D detection, semantic map construction). We predict 3D flows for the Gaussians with dynamic semantics and plan the ego trajectory accordingly with an objective of future scene forecasting. Our GaussianAD can be trained in an end-to-end manner with optional perception labels when available. Extensive experiments on the widely used nuScenes dataset verify the effectiveness of our end-to-end GaussianAD on various tasks including motion planning, 3D occupancy prediction, and 4D occupancy forecasting.", "sections": [{"title": "1. Introduction", "content": "Vision-based autonomous driving emerges as a promising direction due to its resemblance with human driving and economic sensor configuration [20, 31, 32, 42, 45]. Despite the lack of depth inputs, vision-based methods exploit deep networks to infer structural information from RGB cameras and demonstrate strong performance in various tasks, such as 3D object detection [20, 31, 32], HD map construction [30, 34, 38, 62], and 3D occupancy prediction [21, 22, 49, 50, 55, 56, 66].\nRecent autonomous driving research is undergoing a shift from the modular [21, 32, 62] to the end-to-end paradigm, which aims to plan the future trajectory directly from image inputs [18, 19, 28, 59, 64]. The key advantage of the end-to-end pipeline is less information loss from the inputs to the outputs, making it important to design the intermediate 3D scene representation of 2D images. Conventional methods compress the 3D scene in the height dimension to obtain the bird's eye view (BEV) representation [19, 59]. Recent methods explore sparse queries (e.g. instance boxes, map elements) to describe the surrounding scene [48, 61]. Despite their efficiency, they cannot capture the fine-grained structure of the 3D environment, providing less knowledge to the decision-making process. Furthermore, some methods employ tri-perspective view [21, 47, 66] or voxels [50, 56, 63] to represent scenes as 3D occupancy to capture more comprehensive details. However, the dense modeling leads to large computation overhead and thus fewer resources to reason about decision-making. This raises a natural question: can we design a comprehensive yet sparse intermediate representation to pass information through the end-to-end model?\nThis paper proposes a Gaussian-centric autonomous driving (GaussianAD) framework as a positive answer, as shown in Figure 1. We employ a sparse set of 3D semantic Gaussians [23] from 2D images as the scene representations. Despite the sparsity, it benefits from fine-grained modeling resulting from the universal approximation of Gaussian mixtures and explicit 3D structure facilitating various downstream tasks. We further explore perception, prediction, and planning from the 3D Gaussian representation. For perception, we treat 3D Gaussians as semantic point clouds and employ sparse convolutions and sparse prediction heads to efficiently process the 3D scene. We propose 3D Gaussian flow to comprehensively and explicitly model the scene evolution, where we predict a future displacement for each Gaussian. We then integrate all available information to plan the ego trajectory accordingly. Due to the explicitness of 3D Gaussian representation, we can straightforwardly compute the forecasted future scenes observed by the ego car using affine transformations. We compare the forecasted scenes with ground-truth scene observations as explicit supervision for both prediction and planning. To the best of our knowledge, our GaussianAD is the first to explore the explicitly sparse point-based architecture for vision-centric end-to-end autonomous driving. We conduct extensive experiments on the nuScenes [3] dataset to evaluate the effectiveness of the proposed Gaussian-centric framework. Experimental results demonstrate that our GaussianAD achieves state-of-the-art results on end-to-end motion planning with high efficiency."}, {"title": "2. Related Work", "content": "Perception for Autonomous Driving. Accurately perceiving the surrounding environment from sensor inputs is the fundamental step for autonomous driving. As the two main conventional perception tasks, 3D object detection aims to obtain the 3D position, pose, and category of each agent in the surrounding scene [20, 31, 32, 42, 45, 62], which are important for trajectory prediction and planning. Semantic map reconstruction aims to recover the static map elements in the bird's eye view (BEV) to provide additional information for further inference [30, 34, 38, 62]. Both tasks can be efficiently performed in the BEV space, yet they cannot describe the fine-grained 3D structure of the surrounding scene and arbitrary-shape objects [21, 56]. This motivates recent methods to explore other 3D representations like voxel and tri-perspective view (TPV) [21] to perform the 3D occupancy prediction task [49, 50, 55, 56, 66]. 3D occupancy provides more comprehensive descriptions of the surrounding scene including both dynamic and static elements, which can be efficiently learned from sparse LiDAR [21] or video sequences [4]. Gaussianformer [23] proposed to use 3D semantic Gaussians to represent the scene for 3D occupancy sparsely. However, it is still not clear whether the 3D Gaussian representation can be used for general autonomous driving.\nPrediction for Autonomous Driving. Predicting the scene evolution is also vital to the safety of autonomous driving vehicles. Most existing methods focus on predicting the movement of traffic agents given their past positions and semantic map information [13, 16, 27, 37, 41, 58, 62]. Early methods projected agent and semantic map information onto BEV images and employed 2D image backbones to process them to infer future agent motions [5, 41]. Subsequent methods adopted a more efficient tokenized representation of dynamic agents and used graph neural networks [33] or transformers [37, 40, 52] to aggregate information. Recent works began to explore motion prediction directly from sensor inputs in an end-to-end manner [13, 16, 27, 28, 62]. They usually first perform BEV perception to extract relevant information (e.g., 3D agent boxes, semantic maps, tracklets) and then exploit them to infer future trajectories. Different from existing methods which only model dynamic object motions, we propose Gaussian flows to predict the surrounding scene evolutions including both dynamic and static elements.\nPlanning for Autonomous Driving. Planning is the essential component of autonomous driving systems, which can be categorized into rule-based [1, 12, 51] and learning-based [8, 43, 46] methods. While the traditional rule-based methods can achieve satisfactory results with high interpretability [12], learning-based methods have received increasing attention in recent years due to their great potential to scale up to large-scale training data [2, 11, 24, 36, 57, 65]. As simple yet effective learning-based solutions, imitation-based planners have been the preferred choices for end-to-end methods [9, 14, 25, 26, 53, 65]. As early attempts, LBC [6] and CILRS [10] employed convolutional neural networks (CNNs) to learn from expert driving data. The following methods incorporated more data [59] or extracted more intermediate features [18, 19, 28, 64] to provide more information for the planner, which achieved remarkable performance. Still, most existing end-to-end autonomous driving methods adopt high-level scene descriptions (e.g., 3D boxes, maps) for downstream prediction and planning and may omit certain critical information. This paper proposes a Gaussian-centric autonomous driving pipeline and uses 3D Gaussians as sparse yet comprehensive information carrier."}, {"title": "3. Proposed Approach", "content": "3.1. 3D Scene Representation Matters for Driving\nAutonomous driving aims to produce safe and consistent control signals (e.g., accelerator, brake, steer) given a series of scene observations {0}. While the scene observations {0} can be obtained from multiple sensors such as cameras and LiDAR, we mainly target vision-based autonomous driving from surrounding cameras due to its high information density and low sensor costs [18, 19, 28, 50, 59, 64]. Assuming a good-performing controller, most autonomous driving models mainly focus on learning the mapping f from the current and history observations {0} to the future ego trajectories {w}:\n$\\{o^{T-H},...,o^{T}\\} \\overset{f}{\\rightarrow} \\{w^{T+1},..., w^{T+F}\\},$ (1)\nwhere T denotes the current time stamp, H is the number of history frames, and F is the number of predicted future frames. Each waypoint w = {x,y, 4} is determined by the 2D position {x,y} and the yaw angle 4 (i.e., the advancing direction of the ego vehicle) in the bird's eye view (BEV).\nConventional autonomous driving methods decompose f into perception, prediction, and planning modules and train them separately before connecting [5, 12, 21, 32, 41]:\nPerception: $\\{o^{T-H},...,o^{T} \\} \\rightarrow d^{T},$\nPrediction: $d^{T} \\rightarrow \\{d^{T+1},...,d^{T+F}\\},$\nPlanning: $\\{d^{T+1},...,d^{T+F}\\} \\rightarrow \\{w^{T+1},...,w^{T+F}\\},$ (2)\nwhere d is the scene description such as instance bounding boxes of other agents or map elements of the surroundings. The scene description d usually only provides a partial representation of the scene, resulting in information loss. The separate training of these modules further aggravates this issue as different tasks focus on extracting different information. The incomprehensive information provided to the planning module might bias the decision-making process of the autonomous driving model. This motivates the shift from the modular framework to the end-to-end framework [19, 61, 64], which differentiably bridges and jointly learns the perception, prediction, and planning modules:\n$\\{o^{T-H},...,o^{T} \\} \\rightarrow r^{T} \\rightarrow r^{T}, d^{T} \\rightarrow$\\\n$r^{T}, \\{d^{T+1},...,d^{T+F}\\} \\rightarrow \\{w^{T+1},...,w^{T+F}\\},$ (3)\nwhere r is the scene representation. r is usually composed of a set of continuous features and provides a more comprehensive representation of the 3D scenes than d.\nThe scene representation r conveys information throughout the model, making the choice of r critical to the performance of the end-to-end system. As autonomous driving needs to make decisions in the 3D space, the scene representation should be 3D-structured and contain 3D structural information inferred from the input images. On the other hand, 3D space is usually sparse, resulting in a tradeoff between comprehensiveness and efficiency when designing r. For comprehensiveness, the conventional bird's eye view (BEV) representation [31, 32, 62] uses dense grid features in the map view and compresses the height dimension to reduce redundancy. Subsequent methods further explore more dense representations such as voxels [56] or tri-"}, {"title": "3.2. Gaussian-Centric Autonomous Driving", "content": "3D Gaussian Representation. Existing methods typically build a dense 3D feature to represent the surrounding environment and processes every 3D voxel with equal storage and computation resources, which often leads to intractable overhead because of unreasonable resource allocation. At the same time, this dense 3D voxel representation cannot distinguish objects of different scales. Unlike these methods, we follow GaussianFormer [23] which represents an autonomous driving scene with a number of sparse 3D semantic Gaussians. Each Gaussian instantiates a semantic Gaussian distribution characterized by mean, covariance, and semantic logits. This sparse explicit feature representation is more beneficial for downstream tasks.\nGaussians From Images. We first represent 3D Gaussians and their high-dimensional queries as learnable vectors. We then employ a Gaussian encoder to iteratively enhance these representations. Each Gaussian Encoder block is composed of three modules: a self-encoding module facilitating interactions between Gaussians, an image cross-attention module for aggregating visual information, and a refinement module to fine-tune Gaussian properties. Different from GaussianFormer [23], we utilize a temporal encoder consisting of 4D sparse convolutions to integrate Gaussian features from the previous frame with the corresponding features in the current frame.\nSparse 3D Detection from Gaussians. As 3D Gaussian representation is a sparse scene representation, we follow VoxelNeXt [7] which predicts 3D objects directly based on sparse voxel features. Specially, we conduct a 3D sparse CNN network V to encode 3D Gaussian representation r. Following GenAD [64], we decode 3D objects a with a set of agent tokens D on V(r):\n$a = f_{a}(D, V(r)),$ (4)\nwhere $f_{a}$ represents a combine of global cross-attention mechanism to learn 3D objects tokens and a 3D object decoder head $d_{a}$ on learned 3D objects tokens.\nSparse Map Construction from Gaussians. Similar to the representation of 3D detection from Gaussian, we adopt a set of map tokens M to represent semantic maps. We focus on three categories of map elements(i.e., lane divider, road boundary, and pedestrian crossing).\n$m = f_{m}(M, V(r)),$ (5)\nwhere $f_{m}$ represents a combination of a global cross-attention mechanism to learn map tokens and a semantic map elements decoder head $d_{m}$ on learned map tokens.\nMotion Prediction. Motion prediction module assists ego trajectories planning by forecasting the future trajectories of other traffic participants [19]. We obtain motion tokens Mo by make agent tokens D interact with map tokens M through cross-attention layers CA:\n$M = CA(D, M).$ (6)\nA motion decoder $d_{mo}$ can be applied on motion tokens Mo, meanwhile the learned motion tokens Mo are fed to ego trajectory planning head.\nGaussian Flow for Scene Prediction. Furthermore, it shows that the scene prediction of the intermediate representation r plays a significant role in end-to-end autonomous driving [63]. We predict the future Gaussian representation as Gaussian flow $r^{T+N}$ from current Gaussian representation $r^{T}$ and the predicted ego trajectories $w^{T+N}$:\n$r^{T+N} = f_{r}(r^{T}, w^{T+N}).$ (7)\nWe then feed the predicted future Gaussian representation $r^{T+N}$ to an occupancy decoder $d_{occ}$ [23] to predict future occupancy. The supervision of future occupancy on the intermediate Gaussian representation guarantees the scene forecasting ability which finally improves the performance of ego trajectory prediction."}, {"title": "3.3. End-to-End GaussianAD Framework", "content": "This subsection presents the overall end-to-end framework of our proposed GaussianAD. We first initialize the scenes with a set of uniformly distributed 3D Gaussians $G_{0}$ and then progressively refine them by incorporating information from the surrounding-view images o to obtain the Gaussian scene representation r. We can then optionally extract various scene descriptions d from r as auxiliary tasks if the corresponding annotations are available. Concretely, we employ Gaussian-to-voxel splatting [23] to obtain dense voxel features for dense descriptions (e.g., 3D occupancy prediction) and fully sparse convolutions [7] to obtain sparse queries for sparse descriptions (e.g., 3D bounding boxes, map elements). The use of auxiliary perception supervisions introduces additional constraints and prior knowledge on the scene representation r to guide its learning process. Still, we predict future evolutions directly on the 3D Gaussians r to reduce information loss and plan the ego trajectory {w} accordingly. GaussianAD passes information throughout the model with the sparse yet comprehensive 3D Gaussian representation, providing more knowledge to the decision-making process. The overall framework of our GaussianAD is formulated as follows:\n$\\{o^{T-H},...,o^{T}\\} \\rightarrow r^{T} (\\rightarrow r^{T}, d^{T}) \\rightarrow$\\\n$\\{r^{T},r^{T+1},...,r^{T+F}\\} \\rightarrow \\{w^{T+1},..., w^{T+F}\\},$ (8)\nwhere $( \\rightarrow r^{T}, d^{T})$ means that it is optional to incorporate additional perception supervision with d when available.\nFor training, we adaptively impose different perception losses on the scene descriptions d extracted from r:\n$J_{perc}(d, d) = \\lambda_{occ}J_{occ}(d, d) + \\lambda_{det} J_{det} (d, d)$\\\n$+ \\lambda_{map}J_{map}(d, d) + \\lambda_{motion} J_{motion} (d, d),$ (9)\nwhere $\\lambda_{occ}, \\lambda_{det}, \\lambda_{map},$ and $\\lambda_{motion}$ are balance factors and equal 0 if the supervision is not available. d denotes the ground-truth descriptions. We use 3D occupancy prediction loss [23] as $J_{occ}$, 3D detection loss [32] as $J_{det}$, semantic map loss [62] as $J_{map}$, and motion loss [28] as $J_{motion}$.\nDue to the explicit representation of 3D Gaussians, we can use global affine transformation t to simulate the scene representation r observed at a certain given ego position w. Having obtained the predicted future scene representations $\\{r^{T},r^{T+1},...,r^{T+F}\\}$ with the proposed Gaussian flows, we simulate the future ego scene representations using the planned waypoints $\\{w^{T+1},...,w^{T+F}\\}$:\n$\\{\\hat{r} = t(r, w)\\}^{F},$ (10)\nwhere F denotes the future F frames. We then use the discrepancy between the simulated representations $\\{\\hat{r}\\}^{F}$ and ground truth representations $\\{r\\}^{F}$ as the loss:\n$J_{pred}(\\{r\\}, \\{\\hat{r}\\}, \\{d\\}^{F}) = \\lambda_{re}J_{re}(\\{r\\}, \\{\\hat{r}\\}^{F})$\n$+ \\lambda_{perc}J_{perc}(\\{d(\\hat{r})\\}, \\{d\\}^{F}),$ (11)\nwhere $\\lambda_{re}$ and $\\lambda_{perc}$ are balance factors, and $J_{re}$ computes the discrepancy between two Gaussian representations. $\\{\\hat{r}\\}^{F}$ can be computed from future observations {o}. $d(\\hat{r})$ denotes the predicted descpritions d extracted from $\\hat{r}$. The predicted future ego scene representations $\\{\\hat{r}\\}^{F}$ also depend on the planned trajectories $\\{w\\}^{F}$ as in (10). Therefore, we further adopt the prediction loss (11) for planning in addition to the conventional trajectory loss:\n$J_{plan}(\\{w\\}^{F}, \\{\\hat{w}\\}^{F}) = \\lambda_{tra}J_{tra}(\\{w\\}^{F},\\{\\hat{w}\\}^{F})$\n$+ \\lambda_{pred}J_{pred}(\\{\\hat{r}\\}^{F}, \\{r\\}^{F}, \\{d\\}^{F}),$ (12)\nwhere $\\lambda_{tra}$ and $\\lambda_{pred}$ are balance factors, and $\\hat{w}$ denotes the ground truth waypoint. We adopt the trajectory losses from GenAD [64] as $J_{tra}$.\nThe proposed GaussianAD is a flexible framework and can accommodate various cases with different available supervisions, as shown in Figure 3. We train GaussianAD jointly with the following overall objective:\n$J_{GaussianAD} = J_{perc} + J_{pred} + J_{plan},$ (13)\nwhere $J_{perc}, J_{pred}$, and $J_{plan}$ can be customized for different scenarios.\nFor inference, GaussianAD accomplishes end-to-end driving using 3D Gaussian representation to efficiently pass information throughout the pipeline. It provides comprehensive knowledge for the decision-making process and maintains high efficiency with sparse computing."}, {"title": "4. Experiments", "content": "4.1. Datasets\nWe conducted a series of experiments using the widely used nuScenes [3] dataset to evaluate our GaussianAD. The nuScenes dataset consists of 1000 driving sequences, each providing 20 seconds of video captured by both RGB and LiDAR sensors. They provide data with a rate of 20Hz but only supply annotations for the keyframes at 2Hz, including labels for the semantic map construction and 3D object detection tasks. The recent SurroundOcc [56] further complements nuScenes with 3D semantic occupancy annotations. It assigns each voxel with a label of 18 categories including 16 semantic classes, 1 empty class, and 1 unknown class.\n4.2. Evaluation Metrics\nWe evaluate the planning performance of our GaussianAD using the L2 displacement error and collision rate for fair comparisons with existing end-to-end methods [18, 19, 63,"}, {"title": "4.3. Implementation Details", "content": "We employ ResNet101-DCN [15] with pre-trained weights from FCOS3D [54] as the backbone and additionaly use a feature pyramid network [35] to generate multi-scale image features. Our model takes as input images with a resolution of 1600 x 900 and sets the default number of Gaussians to 25600. In the training stage, we take the AdamW [39] with a weight decay of 0.01 as the optimizer. The learning"}, {"title": "4.4. Results and Analysis", "content": "End-to-End Planning Results. We provide comparisons with state-of-the-art end-to-end autonomous driving models in Table 1. Bold numbers and underlined numbers denote the best and next-best results, respectively. We also report the metrics used in VAD [28], which computes the average results of all the previous frames at each time stamp.\nNote that different methods use different input modalities and auxiliary supervision signals that may influence the performance. Generally, LiDAR provides additional depth information that is critical for planning, especially when measuring the collision rate. However, the LiDAR point clouds, though accurate, are usually sparse and lack more fine-grained information, yielding inferior performance. For auxiliary supervision, motions are usually considered the most effective labels as they provide ground truth for safety-critical future predictions. Still, motions are relatively expensive to annotate while 3D occupancy labels can be automatically annotated using multi-frame LiDAR and 3D bounding boxes [56]. Though our GaussianAD can accommodate different supervision signals, we replace motion with 3D occupancy as the most practical setting."}, {"title": "4D Occupancy Forecasting.", "content": "By predicting a 3D flow for each Gaussian and performing the affine transformation using the planned trajectory, GaussianAD is able to forecast future scenes and perform perception on them. We evaluate the prediction ability of GaussianAD on the 4D occupancy forecasting task [63] and measure the 3D occupancy quality (mIoU and IoU) at the future 1s, 2s, and 3s.\nTable 3 shows that our GaussianAD can effectively predict forecast future 3D occupancy. Note that our GaussianAD is an end-to-end model that performs multiple tasks simultaneously, while OccWorld [63] specifically targets this task. Also, our forecasting does not consider the completion of newly observed areas (due to the ego car moving forward), leading to inferior performance. GaussianAD still demonstrates non-trivial 4D forecasting results, verifying the effectiveness of the proposed Gaussian flows.\nEffect of Different Supervision Signals. As our model can adapt to different training signals for different tasks, we conducted an ablation study to analyze the effect of using different auxiliary supervision, as shown in Table 4. We study the planning performance with a combination of 3D occupancy, 3D detection, map construction, motion prediction, and scene prediction supervision. We see that our GaussianAD delivers consistent performance with different supervision combinations, and using more supervision signals generally improves the performance. The use of motion supervision is particularly effective for the collision rate metric since it provides guidance on potential future overlap of trajectories. Still, using the proposed flow-based scene prediction supervision achieves similar improvements, which only requires future perception labels and introduces no additional annotations.\n3D Gaussian Pruning. We also analyze the effect of further pruning the Gaussians to reduce redundancy, as shown in Table 5. We perform pruning by ordering the Gauss-"}, {"title": "5. Conclusion", "content": "We have presented a Gaussian-centric framework for vision-based end-to-end autonomous driving. To preserve more comprehensive information, we employ 3D Gaussians as the scene representation and adopt Gaussian flows to effectively predict future evolutions. Our framework offers flexibility to accommodate different training data with various annotations. We have conducted extensive experiments on the widely used nuScenes and demonstrated competitive performance on various tasks including ent-to-end planning and 4D occupancy forecasting. It is interesting to explore larger-scale end-to-end models based on 3D Gaussian scene representation trained with more diverse data.\nLimitations. GaussianAD cannot predict accurate scene evolutions since it does not consider newly observed areas."}]}