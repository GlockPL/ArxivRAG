{"title": "Zero-shot and Few-shot Learning with Instruction-following LLMs for\nClaim Matching in Automated Fact-checking", "authors": ["Dina Pisarevskaya", "Arkaitz Zubiaga"], "abstract": "The claim matching (CM) task can benefit an\nautomated fact-checking pipeline by putting\ntogether claims that can be resolved with the\nsame fact-check. In this work, we are the\nfirst to explore zero-shot and few-shot learn-\ning approaches to the task. We consider\nCM as a binary classification task and ex-\nperiment with a set of instruction-following\nlarge language models (GPT-3.5-turbo, Gemini-\n1.5-flash, Mistral-7B-Instruct, and Llama-3-\n8B-Instruct), investigating prompt templates.\nWe introduce a new CM dataset, ClaimMatch,\nwhich will be released upon acceptance. We\nput LLMs to the test in the CM task and find\nthat it can be tackled by leveraging more ma-\nture yet similar tasks such as natural language\ninference or paraphrase detection. We also pro-\npose a pipeline for CM, which we evaluate on\ntexts of different lengths.", "sections": [{"title": "1 Introduction", "content": "Claim matching (CM) is the task of determining\nif two claims can be verified using the same piece\nof evidence or fact-check, which can make the au-\ntomated components of the fact-checking process\nmore efficient (Zeng et al., 2021). Barr\u00f3n-Cede\u00f1o\net al. (2020) stated the problem, and Shaar et al.\n(2020) defined the task, introducing the first dataset\nand baselines. While some see CM as a ranking\ntask that finds the claims most related to a given\nclaim (Shaar et al., 2020, 2021b, 2022; Kazemi\net al., 2021, 2022), others tackle it as a binary classi-\nfication task determining if a pair of claims should\nbe matched to each other or not (Kazemi et al.,\n2021, 2022; Choi and Ferrara, 2024a).\nLarge language models (LLMs) have become\na powerful tool in Natural Language Processing\n(NLP) in recent years to solve a wide variety of\ntasks (OpenAI et al., 2024). The suitability of\ninstruction-following LLMs to CM is yet to be stud-\nied. Finding out if LLMs can be used to determine\nsemantic similarity (SS) between statements has\nrecently received a great deal of attention from the\nNLP community (Bubeck et al., 2023). However,\ntwo matching claims can have low SS. A complex\ninput claim can contain multiple sub-claims (Shaar\net al., 2020) or encompass reasoning between state-\nments. LLMs have shown high results without\ntask fine-tuning but based only on hand-crafted\ntask descriptions with instructions, and zero or lim-\nited number of task examples (prompts) (Brown\net al., 2020). Zero-shot and few-shot learning ap-\nproaches can be particularly useful in the context\nof CM, given the expensive and time-consuming\nnature of manually labelling data where annota-\ntors need to go through long lists of claims to find\nmatches. Hence, in our work, we explore the effec-\ntiveness of zero- and few-shot approaches to CM\nwith instruction-following LLMs, where state-of-\nthe-art methods (Kazemi et al., 2021, 2022) rely on\nlarger training data for fine-tuning.\nGiven the newness of the CM task, little is\nknown about how to approach the task: as a new\ntask, or do approaches from close, more mature\ntasks suffice (e.g. natural language inference (NLI)\nor paraphrase detection (PD)). We investigate, for\nthe first time, prompt-based, instruction-following\nLLMs for CM, in turn exploring the suitability\nof NLI, PD or bespoke CM prompts. We ex-\nperiment on two datasets; (1) where there is a\ndearth of suitable datasets, we compile Claim-\nMatch, a new dataset extending that from Check-\nThat 2022 (Nakov et al., 2022), for claim matching\nwith short texts, and (2) dataset based on Kazemi\net al. (2022) for claim matching with long texts.\nClaim matching is a specific and compli-\ncated fact-checking task, but, with contemporary\ninstruction-following LLMs, it can be resolved as a\nnatural language inference or paraphrase detection\ntask. We find that LLMs yield superior results com-\npared to the SS baselines, and state-of-the-art clas-\nsification results with fine-tuned XLM-ROBERTa."}, {"title": "2 Related Work", "content": "Claim matching. The task can be formulated for\ndifferent settings: at the short / claim level or at the\nlonger / document level. The task can be addressed\nas a ranking or as a classification task. Regarding\nthe matching of short claims with other short texts,\nsentence-based BERT-like embeddings cosine sim-\nilarity methods for estimating textual similarity of\nclaims at the claim level are usually used. Shaar\net al. (2020) proposed to handle CM as a rank-\ning task, using methods based on BM25 ranking\nand BERT-based cosine similarity. Textual simi-\nlarity methods were addressed further for the CM\ntask, using pre-trained LMs (Universal Sentence\nEncoder, ROBERTa, sentence-BERT, etc.), for En-\nglish (i.e. Barr\u00f3n-Cede\u00f1o et al., 2020; Mihaylova\net al., 2021; Pritzkau, 2021).\nSS detection methods can be considered as the\nstate-of-the-art baseline. Shaar et al. (2021a) in-\nvestigated the task from a document-level perspec-\ntive: given an input document, how to identify\nall sentences that contain a claim that can be ver-\nified by previously fact-checked claims. State-\nof-the-art similarity measures and ranking mea-\nsures between possible input-verified pairs were\nimplemented: BM25, NLI Score (Nie et al., 2020),\nBERTScore (Zhang et al., 2020), Sentence-BERT\n(SBERT) cosine similarity scores (Reimers and\nGurevych, 2019), and SimCSE (Gao et al., 2021b).\nFor non-English data, Kazemi et al. (2021) trained\nIndian XLM-R model, performing SS-based rank-\ning, and classification experiments. To the best of\nour knowledge, we are the first to explore CM in\nzero-shot and few-shot settings, using pre-trained\ninstruction-following LLMs.\nKazemi et al. (2022) studied how to find\nmatches from longer fact-checks for short claims\nmade in tweets. The task was addressed as a rank-\ning and as a binary classification task. Multilingual\nXLM-ROBERTa pre-trained LM was fine-tuned on\nthe dataset (with different settings: all data together\nor data in different languages separately). Choi\nand Ferrara (2024a,b) utilised domain fine-tuning\n(based on a synthetic training set) of generative pre-\ntrained LLMs (GPT-3.5-Turbo, Llama-2-13b-chat-hf, and Llama-2-7b-chat-hf) for CM, considering\nit as a RTE (recognising textual entailment) classi-\nfication task with 3 classes: entailment (the truth\nof the first claim implies the truth of the second\nclaim), neutral, contradiction between two claims\n(truth of the first claim implies that second claim is\nfalse). This task setup differs from our task setup\nwhere CM is examined as a binary classification\ntask to check if two claims can be checked with the\nsame piece of evidence. In our setup, matches are\nnot necessarily related to entailment (e.g. \"the sky\nis blue\" and \"the sky is red\" are definite matches in\nour setup, while one doesn't entail the other). Fol-\nlowing the recent efforts, we also tackle CM as a bi-\nnary classification task in hitherto unexplored zero-\nshot and few-shot settings, studying it for shorter\nand for longer texts.\nZero-shot and few-shot learning Let models\nperform on new tasks without fine-tuning. Zero-\nshot and few-shot setups assume addition of extra\nprompt text to each input example, so a model can\nlearn the task based on such train examples and\ninstructions (Schick and Sch\u00fctze, 2021b; Le Scao\nand Rush, 2021) and provide the answer that cor-\nresponds to a final label, filling the unfilled slots\nin a prompt. This method depends on manually"}, {"title": "3 Datasets", "content": "We use two datasets in our research: ClaimMatch,\nwhich we create by extending and adapting data\nfrom Nakov et al. (2022) to experiment with short\ntexts; and longer texts (LT) dataset based on\nKazemi et al. (2022) to experiment with long texts.\nShort texts. To create the ClaimMatch dataset,\nwe rely on texts from the fully available public\ndataset (Nakov et al., 2022), which was in turn\ncreated by building on and extending Shaar et al.\n(2020). We use the multi-domain English subset of\nthe data and get rid of the remainder data pertaining\nto the political domain only. It consists of tweets\nas input claims and previously verified claims from\nthe corresponding articles from the Snopes website.\nIt contains 1,398 claim pairs (training, development\nand dev-test sets combined), and 14,245 previously\nchecked claims in general.\nWe extend this data as it originally contained\nonly positive cases of claim matches, with no nega-\ntive cases. For positive class examples, we chose\n500 claim pairs from the dataset. For the negative\nclass examples, we created them: for each of the se-\nlected input claims, we took another verified claim\nnot from their pair (each verified claim from the\ndataset could be used only once). We used such\nnew pairs as negative examples.\u00b9 Table 4 in the\nappendix shows samples of positive and negative\nclaim matches.\nAll texts were preprocessed: urls, retweet mark-\ners (\u201cRT\u201d) and emojis\u00b2 were removed. Username\n(@) and hashtag (#) markers were removed while\nleaving the mentions, to keep the content of short\ntexts. Text of a verified claim includes its title,\nsubtitle and main text, taking into account the im-\nportance of the titles of articles about claims in\nprevious research (Mansour et al., 2022).\nOur resulting test set, used in the LLMs experi-\nments, contains 1,000 examples: 500 for the posi-\ntive class and 500 for the negative class. Both input\nclaims and verified claims are short. For the input\nclaims, the average length after preprocessing is\n194 characters (39 tokens). For verified claims, av-\nerage length after preprocessing is 303 characters\n(56 tokens). This new dataset is specified for CM\nas a binary classification task, and will be released.\nLonger texts. We use the Kazemi"}, {"title": "4 Experimental Setup", "content": "We use four popular instruction-following LLMs\nfor prompt-based experiments, including 2 mod-\nels with paid API, and 2 freely available models\n(accessed by request):\n1. Gpt-3.5-turbo-0125 (GPT-3.5): the latest and\ncost effective model in the GPT-3.5 family al-\nlows instruction following and works for tradi-\ntional completions non-chat tasks as well\u00b3. To-\nkens context length was limited by the model\ncontext length.\n2. Gemini-1.5-flash (Gemini): the fastest and\ncheapest model from the Gemini family\u2074 al-\nlows instruction following. Default context\nlength was used.\n3. Mistral-7B-Instruct-v0.3 (Mistral): the lat-\nest instruct fine-tuned version of Mistral-7B-\nv0.35, the popular example of open-source\nLLMs of such size. Maximal new tokens is\nset to 400, as it places the full model's an-\nswers.\n4. Llama-3-8B-Instruct (Llama): the last open-\naccess Llama model of such size. The hyper-\nparameters were: temperature 0.6, top_p 0.9,\nmaximum of new tokens 400.\nThe latter two models were set up to fit the Colab\nPro+ resources (A100 GPU with 40 GB RAM). For\nall four models, their zero-shot and few-shot setups\nwere examined with default system instruction &\nspecified user instruction (\u201csingle instruction", "ensemble instruction\").\nWe list the selected CM, PD and NLI prompt\ntemplate sets for zero-shot and few-shot learning in\nTable 1. CM templates were selected after empiri-\ncal tests from a broader initial list of CM templates.\nPD and NLI template sets are based on Prompt-\nSource base (Bach et al., 2022). For Mistral and\nLlama, questions were moved to the beginning of\nthe template to adhere to model requirements.\nIn zero-shot settings, only templates were given\nto models. For few-shot, we used the priming\nmethod (in-context learning): together with a tem-\nplate, labeled examples are included in an input\nsequence, so a model makes a prediction (Brown\net al., 2020). We chose a sample of 5 positive ex-\namples from the data. As negative examples, we\ntook 5 random input claims and selected for them\n5 verified claims not from their pairs. All these\nclaims were not used in the test set. So the set of\ntrain examples was balanced. Based on a prompt\nwith these selected examples, the models were able\nto produce binary classification labels for the new\nexamples. We used \\\"yes\\\"/\\\"no\\\" (templates except-\ning NLI 1-2) or \u201ctrue": "false", "corresponding\nclasses": "match\" (positive)/\"not match\u201d (negative).\nThe final order of positive and negative examples\nwas mixed.\nIn addition to the instruction-following LLMs,\nwe include in the evaluation:\n1. State-of-the-art model (SOTA): multilingual\nXLM-ROBERTa-base7 (XLM-R) model that,\nafter fine-tuning on data, showed the best re-\nsults for CM as classification task in (Kazemi\net al., 2022). In our study, it was fine-tuned\nfor the CM task on the dataset of remaining"}, {"title": "5 Zero-shot and few-shot experiments", "content": "To identify the most effective user instructions for\na prompt, we investigate different templates on\nClaimMatch and compare results in Table 2.\nFew-shot results with a \u201csingle instruction\u201d.\nOverall, our results show that PD and NLI tem-\nplates outperform CM templates, suggesting that\nreformulating the CM task as a PD and NLI helps.\nThe improvement is primarily noticeable with PD,\nsuggesting that, with the datasets at hand, matching\nclaims can be handled as paraphrases. This is how-\never not consistent; certain templates such as PD-6\nand NLI-5 can outperform CM templates, but not\nall PD and NLI templates do.\nDifferent LLMs require different prompt engi-\nneering approaches and differ if CM / PD / NLI\nsetup is the best one for them, but most of them are\nconsistent in best prompts within each of the CM,\nPD and NLI setups: for GPT-3.5, Gemini and Mis-\ntral, templates CM-1, PD-6 and NLI-5 are the most\neffective for their sets. Only for Llama, templates\nCM-2, PD-4 and NLI-4 are better. For GPT-3.5 and\nMistral, PD-6 template performs better, for Gemini\n- NLI-5, for Llama - NLI-4.\nPrompt templates in the same set can have sim-\nilar meanings yet provide distinct results: for ex-\nample, PD-1 and PD-2 differ slightly, but all the\nmodel results yield significant differences. The re-\nsults of the models also vary in different ways in\nthe template sets and among the models and the\ntemplates. Wording-dependent prompt engineering\nseparately for each model is required here. Mistral\nyields the highest overall metrics in all template\nsets. For this model, PD templates show highest\nscores, and the CM templates work better than the\nNLI templates. Llama has the lowest scores, and\nits results with the NLI-2 template demonstrate that\nthe model understood the task wrongly (as a claim\nverification rather than as a CM task). GPT-3.5\nand Gemini are not so consistent along results for\ntemplate sets. Although Llama is very good with\nNLI-4, it performs much worse with NLI-5 where\nexplicit inference understanding is needed. Mis-\ntral shows high consistency and impressive scores\nalong PD templates results, but for NLI they vary\nmuch. These examples demonstrate that there are"}, {"title": "6 Error Analysis", "content": "We studied all model outputs, to deeper understand\nperformance and why different models perform bet-\nter with different prompts. For all templates except\nfor NLI 3-5, in most of the cases the answers were\naccompanied with model explanations. Gemini\nfew-shots provided more structured answers (start-\ning with the class label, e.g. \u201cyes\u201d/\u201cno\u201d), with less\nexplanations than other models.\nFact-checking instead of CM. \u201cSingle instruc-\ntion\" CM templates work for Mistral much better\nthan for other models, and it understood the task\nvery well (due to the model's argumentation). But\nNLI 1-2 templates were problematic: the model did\nnot understand the task and incorrectly addressed\nit as a fact-checking task, due to \"true\"/\"false\" in\ninstruction question. For other models, such cases\nwere also present for these tasks, so \u201ctrue\u201d/\u201cfalse\"\nquestions should be excluded for CM. NLI 3-4\ntemplates did not cause as many fact-checking an-\nswers: they contain \u201ctrue\u201d, but focus on two claims\ninference is expressed more clearly.\nFocus on train examples can also lead to er-\nrors. For Llama, \"single instruction\" templates\nperformed worse than for other models (excluding\nNLI-4). In few-shot, it could be confused with train\nexamples and start comparing two test claims not\nto each other, but to train claims (e.g. in NLI-5 few-\nshot recall for negative class was 21.0%). But its\nzero-shot with \u201censemble\u201d instructions got results"}, {"title": "7 Discussion & Conclusion", "content": "We propose a novel direction to tackle claim match-\ning as a classification task: prompt-based zero-shot\nand few-shot learning for pre-trained instruction-\nfollowing LLMs, suitable when there is no or lim-\nited train data. We create the ClaimMatch dataset\nwith short texts and perform extensive experiments\non it. We use manually created prompt templates\nfor zero-shot and few-shot (10 train examples) set-\ntings, aimed specifically for CM or for two similar\ntasks: PD and NLI. We investigate if the latter two\ntasks can help tackle CM task. Different LLMs\nfrom our benchmark comparison require different\nprompt engineering approaches, but in general PD\nand NLI work better than CM templates.\nOur results help better understand that zero-\nshot and few-shot methods with templates crafted\nfor PD and NLI tasks are appropriate for CM.\nWith instruction-following LLMs, it can be re-\nsolved on the existing data with using NLI and\nPD tasks approaches. Few-shot training exam-\nples are still needed to experiment with the bet-\nter settings. Hence, advanced prompt engineering\nmethods should be further carefully developed for\nCM. We suggest the pipeline for zero-shot and few-\nshot CM, and test it with best prompt templates on\nlonger texts. Our work makes the first contribution\nof prompt-based zero-shot and few-shot learning\nwith instruction-based LLMs for CM, and provides\npossible further research questions and directions.\nThe creation of more datasets for claim matching\nwill greatly benefit further development of the task.\nOne possible reason why PD templates are well\nsuited to the CM task is that existing datasets are\nnot challenging enough. For example, two match-\ning claims \"London is the capital city of the UK\"\nand \"UK's capital is London\" can be easily identi-\nfied as a paraphrase of each other. However, match-\ning claims are not necessarily paraphrases of each\nother, e.g. \"The president of the USA is Joe Biden\"\nand \"The president of the United States is Barack\nObama\". The former is correct at the time of writ-\ning, whereas the latter is not correct, while they\nare matching claims because they can both be fact-\nchecked with the same piece of evidence.\nAs an avenue for future work, to enhance model\nexplainability, chain-of-thought methods can be im-\nplemented to let a model decompose the input and\nbetter understand nested sub-claims. Combinations\nof prompts could be also further examined."}, {"title": "Limitations", "content": "The results depend on the choice of prompt tem-\nplates and their formulations on surface level.\nUse of hand-crafted templates depends on the re-\nsearcher's intuition, leading to limitations in the\nexperimentation and automating the prompt gener-\nation that is left for future work. Likewise, choice\nof few-shot train examples is also important. Alter-\native prompt-learning and -engineering methods\nshould be implemented, automatically searching\nfor best prompts and templates. Different LLMs\nrequire different prompt styles/selection of exam-\nples/prompt formatting. More fine-grained tech-\nniques, depending on a particular LLM, could im-\nprove their results and help understand why dif-\nferent prompt templates work better for different\nmodels. Fine-tuning approaches to LLMs, with\nmore training data, could also reduce in the future\nthe impact of the train examples selection. More de-\ntailed analysis of model error types is also the focus\nof further research. Using \u201censemble\u201d instructions,\nas a method, should be further investigated. On\nshort texts, models in their explanations demon-\nstrate that they mostly consider both system and\nuser instruction combined.\nReproducibility of classification results, with the\nsame LLM parameters, remains a challenging issue.\nBut, for example, for five runs of GPT-3.5 and\nGemini with PD-6 template, the results differed\nonly slightly: mean F1 scores 84.7% and 90.5%,\nwith standard error 0.25 and 0.10, respectively. The\nborderline examples are ambiguous, despite the\ngold label about their match.\nOur experiments were conducted with English\ndata only, which could be further extended to other\nlanguages using multilingual LLMs. CM is a new\ntask, and only limited datasets are available. We\nused all two existing datasets, as a basis for our re-\nsearch. Dataset from (Nakov et al., 2022) contains\nEnglish and Arabic parts. Dataset from (Kazemi\net al., 2022) includes English, Hindi, Spanish, and\nPortuguese. We provide the first study on our CM\napproach and use only English parts, for prompt\nengineering and evaluation purposes, as we do\nnot have native speakers in the aforementioned\nlanguages. There is indeed need to create more\ndatasets, and we are planning to create more CM\ndatasets, including the multilingual ones."}]}