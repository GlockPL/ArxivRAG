{"title": "Mixing Any Cocktail with Limited Ingredients: On the Structure of Payoff Sets in Multi-Objective MDPs and its Impact on Randomised Strategies", "authors": ["James C. A. Main", "Mickael Randour"], "abstract": "We consider multi-dimensional payoff functions in Markov decision processes, and ask whether a given expected payoff vector can be achieved or not. In general, pure strategies (i.e., not resorting to randomisation) do not suffice for this problem. We study the structure of the set of expected payoff vectors of all strategies given a multi-dimensional payoff function and its consequences regarding randomisation requirements for strategies. In particular, we prove that for any payoff for which the expectation is well-defined under all strategies, it is sufficient to mix (i.e., randomly select a pure strategy at the start of a play and committing to it for the rest of the play) finitely many pure strategies to approximate any expected payoff vector up to any precision. Furthermore, for any payoff for which the expected payoff is finite under all strategies, any expected payoff can be obtained exactly by mixing finitely many strategies.", "sections": [{"title": "1 Introduction", "content": "Markov decision processes (MDPs) are a classical framework for decision making in uncertain environments. MDPs are notably used in the fields of formal methods (e.g., [BK08, FBB+23]) and reinforcement learning (e.g., [SB18]). An MDP models the interaction of a controllable system with a stochastic environment: at each step of the process, the system selects an action and the state of the process is updated following a probability distribution that depends only the current state and chosen action. This interaction goes on forever and yields a play (i.e., an execution) of the process. The quality of a play is quantified by a payoff function, which assigns a numeric value to all plays. The goal is then to compute an optimal strategy, i.e., a decision-making procedure for the system that maximises the expected payoff. Such a strategy constitutes a formal blueprint for a controller of the modelled system [Ran13].\nMulti-objective MDPs. More complex specifications, such as simultaneous constraints on the response time and energy consumption of a system, are usually modelled using multi-dimensional payoff functions. In this setting, some expected payoff vectors may be incomparable; an analysis of trade-offs between the different dimensions may be necessary.\nThe goal is generally to determine, given a vector, whether it is achievable, i.e., whether there exists a strategy whose expected payoff is greater than or equal to the vector in\n*James C. A. Main is an F.R.S.-FNRS Research Fellow and Mickael Randour is an F.R.S.-FNRS Research Associate. Both authors are members of the TRAIL institute. This work has been supported by the Fonds de la Recherche Scientifique - FNRS under Grant n\u00b0 T.0188.23 (PDR ControlleRS)."}, {"title": "Strategy complexity", "content": "all components (e.g., [EKVY08, CFW13, RRS17]). A related problem is to compute or approximate the Pareto curve of the set of expected payoffs, i.e., the expected payoffs that are Pareto-optimal (e.g., [FKP12, CKK17, QK21]). Intuitively, an expected payoff is Pareto-optimal if there is no strategy whose expected payoff is as good on all dimensions and strictly better on one dimension. Alternatively, one can look for strategies with expected payoffs that are optimal for the lexicographic order over vectors (e.g., [HPS+21, CKM+23, BCM+23]). For instance, in a two-dimensional setting, this equates to finding strategies that maximise the expected payoff on the second dimension among the strategies that maximise the expected payoff on the first dimension.\nStrategy complexity. For controller design, simpler strategies, i.e., using limited memory and randomisation, are preferable. In the case of many classical (one-dimensional) specifications, there exist optimal strategies that use no memory and no randomisa- tion. For instance, such strategies suffice to maximise the probability of a parity ob- jective [CJH04] or the expectation of a mean [Bie87] or discounted-sum payoff [Sha53].\nIn contrast to the one-dimensional case, Pareto-optimal strategies in multi-objective MDPs may require both memory and randomisation (see, e.g., [RRS17, BGMR23]). For this reason, a major problem is understanding what are the simplest relevant strategies, e.g., whether randomisation is necessary, or what is the minimum amount of memory required for the application at hand. A related approach consists in directly searching for simple strategies that satisfy the desired constraints (e.g., [DKQR20]).\nThere is an extensive body of work aimed at understanding memory requirements for strategies in various game-based models. These works provide sufficient conditions on payoffs for which memoryless or finite-memory strategies suffice, or even complete characterisations of such payoffs. For instance, see [LPR18, BLO+22, BRV23, CO23] for games on deterministic graphs, [BORV23, GK23] for stochastic games and [Gim07] for MDPs. Most of these works focus on pure strategies.\nIn this work, our focus is on randomisation requirements in multi-objective MDPs. Instead of identifying bounds on memory, we study whether we can achieve vectors while using strategies with limited randomisation power. We remark that in general, it is not possible to jointly minimise memory and randomisation requirements. Indeed, there can be a trade-off between memory and randomisation, even in the one-dimensional case [CdH04, Hor09, CRR14, MPR20].\nRandomised strategies. A strategy that does not use randomisation is called pure. Formally, a pure strategy is a function that assigns actions to histories (i.e., sequences of observations). Randomised strategies can be defined in two ways. On the one hand, a behavioural strategy is a function that assigns distributions over actions to histories. In other words, when following a behavioural strategy, the random choices of actions are made at each step of the process. On the other hand, a mixed strategy is a distribution over the set of pure strategies. When playing according to a mixed strategy, a pure strategy is randomly chosen at the start of the process and is followed for the entire play.\nIn MDPs, behavioural and mixed strategies are equivalent; any strategy from one class can be emulated by a strategy of the other. This result is known as Kuhn's theo- rem [Aum64], and it also shows that mixed and behavioural strategies are the most general type of randomised strategies.\nWhen limiting ourselves to finite-memory strategies, the natural analogues of mixed strategies are strictly weaker than the analogue of behavioural strategies [MR24]. In fact, the emulation of a behavioural strategy that flips a coin at each round requires a"}, {"title": "A simple example", "content": "mixed strategy that randomises over uncountably many pure strategies, infinitely many of which require infinite memory to account for all patterns. It follows that mixed strategies with a finite support, i.e., that mix finitely many pure strategies, are a strict sub-class of the set of randomised strategies (regardless of memory). Arguably, this is one of the simplest ways of integrating randomness in strategies. Considering such strategies instead of general behavioural (or mixed) strategies amounts to limiting the randomisation power of strategies.\nA simple example. For the sake of illustration, let us consider the MDP depicted in Figure 1.1a. This MDP models a situation where a person wants to go to work. They must choose between riding their bicycle or taking the train. However, the train may be delayed with high probability due to an ongoing strike. The goal of the commuter is twofold: maximise the likelihood of reaching work within 40 time units (to reach work on time) and do so as fast as possible on average.\nWe illustrate the set of expected payoffs for this situation in Figure 1.1b. We label expected payoffs with their corresponding strategies. On the one hand, \\(\\sigma_{\\text{train}}\\) and \\(\\sigma_{\\text{bike}}\\) denote the pure strategies that always choose the action in the subscript. On the other hand, we denote by \\(\\sigma_{\\ell t+b}\\) the strategy that attempts to take the train \\(\\ell\\) times before choosing the bicycle.\nThis simple example highlights the need for randomisation and memory in multi- objective MDPs. When limited to pure strategies, for instance, it is not possible to reach work on time with probability exceeding 90% while guaranteeing an expected commute time lower than 27: any pure strategy whose payoff is not represented will reach work on time with a smaller probability than \\(\\sigma_{\\text{train}}\\), and thus will not be satisfactory. However, as suggested by the point labelled by \\(\\sigma_{\\text{mix}}\\) on the illustration, these constraints can be satisfied by mixing \\(\\sigma_{\\text{train}}\\) with \\(\\sigma_{6t+b}\\). In fact, here, all expected payoffs can be obtained by finite-support mixed strategies. We explain below how this property generalises. We remark however that, in general, the set of expected payoffs need not be a convex polytope like in this example (see Section 4).\nOur contributions. We study the structure of sets of expected payoffs in multi- objective MDPs, focusing on the relationship between what can be obtained with pure"}, {"title": "Our contributions", "content": "and with randomised strategies. Our goal is to provide general results, i.e., that apply to a broad class of payoffs. We only consider universally unambiguously integrable payoffs (Section 3), i.e., payoffs that have a well-defined (possibly infinite) expectation no matter the strategy, a natural requirement when aiming to reason about expectations. We obtain finer results for universally integrable payoffs, i.e., payoffs whose expectation is finite under all strategies.\nFirst, we prove that for universally integrable (multi-dimensional) payoffs, for all strate- gies, there exists a pure strategy with a greater or equal expected payoff in the lexicographic sense (Theorem 5.1).\nThis first result serves as a building block to one of our main results: any expected payoff vector of a universally integrable (multi-dimensional) payoff is a convex combination of expected payoffs of pure strategies (Theorem 6.2). From a strategic perspective, this means that in multi-objective MDPs, finite-support mixed strategies suffice to exactly obtain any (Pareto-optimal) expected payoff vector. As a corollary, we obtain that any extreme point of a set of expected payoffs of a universally integrable payoff can be obtained by a pure strategy (Corollary 6.3). These results generalise known properties for classical combinations of objectives for which the set of expected payoffs is a convex polytope (e.g., combinations of \\(\\omega\\)-regular specifications [EKVY08] and universally integrable total- reward payoffs [FKP12]). While none of the previous properties generalise to the whole class of universally unambiguously integrable payoffs (Example 5.1 and 6.3), we prove that, for such payoffs, convex combinations of pure strategies can be used to approximate any expected payoff (Theorem 6.5).\nIn both cases, we can bound the number of strategies that are mixed depending on the number of dimensions d. Depending on the setting, we can match or approximate expected payoffs by mixing no more than d + 1 strategies (Theorem 6.6).\nWe also provide a sufficient condition to guarantee that the set of expected payoffs is closed. We show that for continuous universally square-integrable payoffs (which generalise real-valued continuous payoffs), the set of expected payoffs is closed (Theorem 7.6).\nTo prove our results, we borrow techniques from several fields. We mainly rely on topology, properties of convex sets (separating and supporting hyperplanes) and the theory of the Lebesgue integral. We assume familiarity with basic properties of the Lebesgue integral, and otherwise endeavour to keep the text self-contained by recalling most required notions, notably through an extensive appendix.\nApplicability. The class of universally integrable payoffs is large: it contains most classical payoffs. Indeed, all bounded payoffs are de facto in this class. For example, all indicators of objectives are in it. This means that all settings where one considers the probabilities of sets of plays (i.e., either an inherently qualitative objective or one arising from fixing a threshold for a quantitative payoff) are in it, for any definition of such sets of plays. Classical examples include \\(\\omega\\)-regular objectives [EKVY08], window objectives [BDOR20] or percentiles queries [RRS17]. Bounded payoffs also encompass discounted-sum [Sha53] and mean-payoff [BBC+14, CKK17] functions. Heterogeneous combinations, such as, e.g., combinations of energy and mean-payoff [BHRR19] also fit under this umbrella.\nPayoffs that are completely out of the scope of our results i.e., not even universally unambiguously integrable include total payoff [FBB+23], average-energy [BMR+18] and shortest path [RRS17], all with both positive and negative weights: the expectation is ill-defined when plays of positive and negative infinite payoffs have non-zero measure. Let us focus on shortest-path objectives. When only positive weights are used, a classical"}, {"title": "Related work", "content": "restriction [RRS17], the associated payoff is universally unambiguously integrable (but not universally integrable as plays of infinite payoffs may exist). Finally, when targets are almost-surely visited (as in the above example), the payoff is continuous and universally square integrable (Appendix C).\nFor many payoffs that are at least universally unambiguously integrable, extreme points of the set of expected payoffs can be obtained via pure finite-memory strategies (e.g., \\(\\omega\\)-regular objectives [EKVY08] or mean-payoff [BBC+14]). It follows that for these objectives, mixing over finitely many pure finite-memory strategies is sufficient to fulfil any achievability requirement. In other words, one of the least expressive models of ran- domised finite-memory strategies (see the classification of [MR24]) suffices. Furthermore, the blow-up in memory from this mixing argument is small: it suffices to mix, at most, one more strategy than the number of payoffs.\nOur results also highlight the role of randomisation in strategies for multi-objective MDPs: it is useful only to balance the payoffs on different dimensions in many cases.\nRelated work. We provide a few references, in addition to the main references cited previously. In our proof of Theorem 6.2, we invoke the separating hyperplane theorem. This theorem also plays a role in approximation schemes of the set of achievable vectors: see, e.g., [FKP12, QK21]; a unifying approach is presented in [Qua23].\nSpecifications with multiple objectives have also been considered in the context of two- player (non-stochastic) games on graphs (e.g., [FH13, CRR14]) and two-player stochastic games (e.g., [CFK+13, ACK+20]). Closely related to multi-objective specifications are approaches that provide guarantees simultaneously in the worst case and the expected case [BFRR17].\nRegarding randomisation in strategies, [CDGH15] studies when randomisation is help- ful in strategies or in transitions of games. In particular, the authors show that if there exists an optimal strategy to maximise the probability of an event in an MDP, then there exists a pure optimal strategy. This property is generalised by our result on lexicographic MDPs.\nOutline. Section 2 overviews notation and preliminary notions. We introduce pay- off functions in Section 3 and establish relationships between expected payoffs of general strategies and expected payoffs of pure strategies. In Section 4, we provide an example highlighting the potential complexity of sets of expected payoffs in multi-objective MDPs. We study randomisation requirements for the lexicographic optimisation of several objec- tives in MDPs in Section 5. Section 6 presents our results regarding expected payoff sets and achievable sets. Finally, we study continuous payoff functions in Section 7.\nAppendices A and B complement Sections 2 and 3 respectively. Appendix C shows that the results of Section 7 apply to universally integrable continuous shortest-path payoffs."}, {"title": "2 Preliminaries", "content": "This section defines the objects occurring in the main part of this paper. Additional definitions and results have been deferred to Appendix A.\nSet-theoretic notation. We let \\(\\mathbb{N}\\) and \\(\\mathbb{R}\\) respectively denote the sets of natural and real numbers. We let \\(\\mathbb{N}_0 = \\mathbb{N} \\setminus \\{0\\}\\). We denote the extended real line by \\(\\overline{\\mathbb{R}} = \\mathbb{R} \\cup \\{-\\infty, +\\infty\\}\\)."}, {"title": "Probability", "content": "Let \\(A\\) be a countable set. We write \\(\\mathcal{D}(A)\\) for the set of distributions over \\(A\\), i.e., the set of functions \\(\\mu: A \\rightarrow [0,1]\\) such that \\(\\sum_{a \\in A} \\mu(a) = 1\\). The support of a distribution \\(\\mu \\in \\mathcal{D}(A)\\) is \\(\\text{supp}(\\mu) = \\{a \\in A \\mid \\mu(a) > 0\\}\\).\nGiven a set \\(B\\) and a \\(\\sigma\\)-algebra \\(\\mathcal{A}\\) over \\(B\\), we denote by \\(\\mathcal{D}(B,\\mathcal{A})\\) the set of probability distributions over the measurable space \\((B, \\mathcal{A})\\). Let \\(\\mu \\in \\mathcal{D}(B,\\mathcal{A})\\) and \\(f: B \\rightarrow \\mathbb{R}\\) be a measurable function. We say that \\(f\\) is \\(\\mu\\)-integrable it is integrable with respect to \\(\\mu\\), i.e., if \\(\\int_B |f| d\\mu \\in \\mathbb{R}\\). We extend the Lebesgue integral to non-positive functions in the following way: if \\(f\\) is non-positive, we let \\(\\int_B f d\\mu = - \\int_B -f d\\mu\\). If \\(f\\) is non-negative, non-positive or \\(\\mu\\)-integrable, we say that \\(\\int_B f d\\mu\\) is the \\(\\mu\\)-integral of \\(f\\)."}, {"title": "Vector spaces", "content": "Throughout this text, vectors are written in boldface to distinguish them from scalars. Let \\(d \\in \\mathbb{N}_0\\). We let \\(0_d\\) and \\(\\mathbf{1}_d \\in \\mathbb{R}^d\\) respectively be the vectors of \\(\\mathbb{R}^d\\) where all components are zero and one respectively. We omit the dimension subscript or superscript whenever there is no ambiguity on the dimension of the space.\nGiven \\(\\mathbf{v} = (v_j)_{1 \\leq j \\leq d}, \\mathbf{w} = (w_j)_{1 \\leq j \\leq d} \\in \\mathbb{R}^d\\), we let \\(\\langle \\mathbf{v}, \\mathbf{w} \\rangle = \\sum_{j=1}^d v_j w_j\\) denote the scalar product of \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\). We let \\(|| \\cdot ||_2\\) denote the Euclidean norm on \\(\\mathbb{R}^d\\), defined by \\(||\\mathbf{v}||_2 = \\sqrt{\\langle \\mathbf{v}, \\mathbf{v} \\rangle}\\) for all \\(\\mathbf{v} \\in \\mathbb{R}^d\\).\nThe affine span of a set \\(D \\subseteq \\mathbb{R}^d\\), which we denote by \\(\\text{aff}(D)\\), is the smallest affine set (i.e., translation of a vector subspace of \\(\\mathbb{R}^d\\)) in which \\(D\\) is included.\nGiven a linear map \\(L: \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d'}\\) (where \\(d' \\in \\mathbb{N}_0\\)), we let \\(\\text{ker}(L)\\) denote the kernel of \\(L\\), i.e., the set \\(\\{\\mathbf{v} \\in \\mathbb{R}^d \\mid L(\\mathbf{v}) = \\mathbf{0}_{d'}\\} \\). A linear form is a linear map whose co-domain is \\(\\mathbb{R}\\)."}, {"title": "Convexity", "content": "A convex combination of vectors \\(\\mathbf{v}_1, ..., \\mathbf{v}_n \\in \\mathbb{R}^d\\) is a linear combination \\(\\sum_{m=1}^n \\alpha_m \\mathbf{v}_m\\) such that \\(\\alpha_1, ..., \\alpha_n \\in [0,1]\\) and \\(\\sum_{m=1}^n \\alpha_m = 1\\). We refer to a sequence of coefficients \\(\\alpha_1, ..., \\alpha_n \\in [0, 1]\\) such that \\(\\sum_{m=1}^n \\alpha_m = 1\\) as convex combination coefficients. Given \\(\\mathbf{v}, \\mathbf{w} \\in \\mathbb{R}^d\\), we let \\([\\mathbf{v}, \\mathbf{w}] = \\{\\alpha \\cdot \\mathbf{v} + (1-\\alpha)\\mathbf{w} \\mid \\alpha \\in [0,1]\\}\\) denote the (closed) segment from \\(\\mathbf{v}\\) to \\(\\mathbf{w}\\); it is the set of convex combinations of \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\). Open and half-open segments are defined analogously.\nLet \\(D \\subseteq \\mathbb{R}^d\\). The convex hull of \\(D\\), denoted by \\(\\text{conv}(D)\\), is the set of all convex combinations of elements of \\(D\\). The set \\(D\\) is convex if for all \\(\\mathbf{v}, \\mathbf{w} \\in D\\), \\([\\mathbf{v}, \\mathbf{w}] \\subseteq D\\), or, equivalently, if \\(D = \\text{conv}(D)\\). If \\(D\\) is convex, we say that \\(\\mathbf{q} \\in D\\) is an extreme point of \\(D\\) if \\(\\mathbf{q} \\notin \\text{conv}(D \\setminus \\{\\mathbf{q}\\})\\), i.e., if \\(\\mathbf{q}\\) is not a convex combination of elements of \\(D\\) other than \\(\\mathbf{q}\\) and we let \\(\\text{extr}(D)\\) denote the set of extreme points of \\(D\\). Extreme points generalise the notion of vertices of polytopes.\nThe definition of a convex combination does not bound the number of involved vectors. However, in \\(\\mathbb{R}^d\\), it is sufficient to only consider convex combinations involving no more than \\(d + 1\\) vectors. This is formalised by the following theorem.\nTheorem 2.1 (Carath\u00e9odory's theorem for convex hulls (e.g., [Roc70, Thm. 17.1])). Let \\(D \\subseteq \\mathbb{R}^d\\) and \\(\\mathbf{q} \\in \\text{conv}(D)\\). There exists \\(D' \\subseteq D\\) such that \\(|D'| \\leq d + 1\\) and \\(\\mathbf{q} \\in \\text{conv}(D')\\).\nConvexity does not generalise to \\(\\overline{\\mathbb{R}}^d\\). Most notably, convex combinations of vectors of \\(\\overline{\\mathbb{R}}^d\\) (defined in the same way as above) may be ill-defined. Although we consider convex"}, {"title": "Topology", "content": "combinations of elements of \\(\\overline{\\mathbb{R}}^d\\) in the sequel, these are always guaranteed to be well- defined, for reasons explained below.\nTopology. We only provide notation in this section. For convenience, we recall some definitions in Appendix A.1, including the definitions of the product topology, continuity, compactness and the usual topology of \\(\\mathbb{R}\\).\nLet \\((X, \\mathcal{T})\\) be a Hausdorff topological space. For all \\(D \\subseteq X\\), we let \\(\\text{cl}(D)\\) and \\(\\text{int}(D)\\) denote the closure and interior of \\(D\\). The border of \\(D \\subseteq X\\) is the set \\(\\text{bd}(D) = \\text{cl}(D) \\setminus \\text{int}(D)\\).\nIn addition to the three notions above, we introduce an additional one for subsets of \\(\\mathbb{R}^d\\) (with its usual topology). The relative interior of a set \\(D \\subseteq \\mathbb{R}^d\\), denoted by \\(\\text{ri}(D)\\), is the interior of \\(D\\) as a subset of \\(\\text{aff}(D)\\) (with the induced topology). The relative interior of a subset of \\(\\mathbb{R}^d\\) includes its interior and these sets may differ. For instance, the segment \\([\\mathbf{0}_2,\\mathbf{1}_2]\\) \\subseteq \\mathbb{R}^2\\) has empty interior. However, we have \\(\\text{ri}([\\mathbf{0}_2,\\mathbf{1}_2]) = ]\\mathbf{0}_2,\\mathbf{1}_2[\\) (because \\(\\text{aff}([\\mathbf{0}_2,\\mathbf{1}_2])\\) is the line of equation \\(x = y\\))."}, {"title": "Hyperplane separation", "content": "Hyperplane separation. A hyperplane \\(H\\) of \\(\\mathbb{R}^d\\) is a set of the form \\(\\{\\mathbf{v} \\in \\mathbb{R}^d \\mid x^*(\\mathbf{v}) = a\\}\\) for some non-zero linear form \\(x^*\\) and \\(a \\in \\mathbb{R}\\). Let \\(D_1\\) and \\(D_2 \\subseteq \\mathbb{R}^d\\). The sets \\(D_1\\) and \\(D_2\\) are strongly separated by a hyperplane if there exists a non-zero linear form \\(x^*\\) such that \\(\\inf_{\\mathbf{q} \\in D_1} x^*(\\mathbf{q}) > \\sup_{\\mathbf{p} \\in D_2} x^*(\\mathbf{p})\\). A convex set \\(D \\subseteq \\mathbb{R}^d\\) is supported by a hyperplane at \\(\\mathbf{q} \\in D\\) if there exists a non-zero linear form \\(x^*\\) such that, for all \\(\\mathbf{p} \\in D\\), \\(x^*(\\mathbf{p}) \\leq x^*(\\mathbf{q})\\) (a supporting hyperplane in this case is \\(H = \\{\\mathbf{v} \\in \\mathbb{R}^d \\mid x^*(\\mathbf{v}) = x^*(\\mathbf{q})\\}\\)). We provide an illustration of the notions of separating and supporting hyperplanes in Figure 2.1.\nWe recall a variant of the hyperplane separation theorem and the supporting hyper- plane theorem. We first outline a sufficient condition such that two disjoint convex sets can be strongly separated.\nTheorem 2.2 (Hyperplane separation theorem [Roc70, Cor. 11.4.2]). Let \\(D_1\\) and \\(D_2\\) be two convex subsets of \\(\\mathbb{R}^d\\). If \\(\\text{cl}(D_1) \\cap \\text{cl}(D_2) = \\emptyset\\) and \\(D_1\\) or \\(D_2\\) is bounded, then there exists a hyperplane strongly separating \\(D_1\\) and \\(D_2\\).\nFigure 2.1a illustrates a setup in which we can apply the theorem. We often apply Theorem 2.2 in the case where \\(D_1\\) is a singleton set \\(\\{\\mathbf{q}\\}\\) (it is bounded) and \\(\\mathbf{q} \\notin \\text{cl}(D_2)\\). The"}, {"title": "Ordering vectors", "content": "next theorem provides a sufficient condition for the existence of a supporting hyperplane at a given point of a convex set.\nTheorem 2.3 (Supporting hyperplane theorem [Roc70, Thm. 11.6]). Let \\(D \\subseteq \\mathbb{R}^d\\) be convex and \\(\\mathbf{q} \\in D\\). If \\(\\mathbf{q} \\notin \\text{ri}(D)\\), then there exists a hyperplane \\(H\\) supporting \\(D\\) at \\(\\mathbf{q}\\) such that \\(D \\nsubseteq H\\).\nOrdering vectors. We consider two order relations on \\(\\mathbb{R}^d\\): the component-wise order and the lexicographic order. Let \\(\\mathbf{q} = (q_j)_{1 \\leq j \\leq d}\\) and \\(\\mathbf{p} = (p_j)_{1 \\leq j \\leq d} \\in \\mathbb{R}^d\\). For the component-wise order, we write \\(\\mathbf{q} \\preceq \\mathbf{p}\\) if and only if \\(q_j \\leq p_j\\) for all \\(1 \\leq j \\leq d\\). For the lexicographic ordering over \\(\\mathbb{R}^d\\), we write \\(\\mathbf{q} \\leq_{\\text{lex}} \\mathbf{p}\\) if and only if \\(\\mathbf{q} = \\mathbf{p}\\) or \\(q_j \\leq p_j\\) where \\(j = \\min \\{j' \\leq d \\mid q_{j'} \\neq p_{j'}\\}\\). We write \\(\\mathbf{q} <_{\\text{lex}} \\mathbf{p}\\) if \\(\\mathbf{q} \\leq_{\\text{lex}} \\mathbf{p}\\) and \\(\\mathbf{q} \\neq \\mathbf{p}\\). We recall that the component-wise order is partial, whereas the lexicographic order is a total order.\nLet \\(D \\subseteq \\mathbb{R}^d\\). We say that \\(\\mathbf{q} \\in D\\) is a Pareto-optimal element of \\(D\\) if it is maximal for the component-wise order, i.e., if there does not exist \\(\\mathbf{p} \\in D\\) such that \\(\\mathbf{q} \\preceq \\mathbf{p}\\) and \\(\\mathbf{q} \\neq \\mathbf{p}\\). We say that \\(D\\) is downward-closed if for all \\(\\mathbf{q} \\in D\\) and \\(\\mathbf{p} \\in \\mathbb{R}^d\\), \\(\\mathbf{p} \\preceq \\mathbf{q}\\) implies \\(\\mathbf{p} \\in D\\). We let \\(\\text{down}(D)\\) denote the downward closure of \\(D\\), which is defined as the smallest (with respect to set inclusion) downward-closed set in which \\(D\\) is included. A set and its downward closure have the same set of Pareto-optimal elements."}, {"title": "Markov decision processes", "content": "Markov decision processes. A Markov decision process models the interaction of a system with a stochastic environment. At each step of the process, an action is taken by the system and the state of the process is updated in a stochastic fashion, where the relevant distribution depends only on the current state and chosen action.\nFormally, a Markov decision process (MDP) is a tuple \\(\\mathcal{M} = (S, A, \\delta)\\) where \\(S\\) is a finite non-empty set of states, \\(A\\) is a finite set of actions and \\(\\delta: S \\times A \\rightarrow \\mathcal{D}(S)\\) is a partial probabilistic transition function. For \\(s \\in S\\), we let \\(A(s)\\) denote the set of actions \\(a \\in A\\) such that \\(\\delta(s, a)\\) is defined. If \\(a \\in A(s)\\), we say that \\(a\\) is enabled in \\(s\\). We assume that MDPs have no deadlocks, i.e., there is an action enabled in each state.\nA play of \\(\\mathcal{M}\\) is an infinite sequence \\(\\pi = s_0a_0s_1a_1 \\ldots \\in (SA)^{\\omega}\\) such that \\(a_{\\ell} \\in A(s_{\\ell})\\) and \\(s_{\\ell+1} \\in \\text{supp}(\\delta(s_{\\ell}, a_{\\ell}))\\) for all \\(\\ell \\in \\mathbb{N}\\). A history is a finite prefix of a play ending in a state. Let \\(\\pi = s_0 a_0 s_1 a_1 \\ldots\\) be a play. We write \\(\\pi_{<r}\\) for the history \\(s_0 a_0 s_1 \\ldots a_{r-1} s_r\\). We also use this notation for prefixes of histories. We denote the suffix \\(s_r a_r s_{r+1} a_{r+1} \\ldots\\) of \\(\\pi\\) by \\(\\pi_{>r}\\). For any history \\(h = s_0 a_0 s_1 \\ldots s_r\\), we let \\(\\text{last}(h) = s_r\\). We write \\(\\text{Plays}(\\mathcal{M})\\) and \\(\\text{Hist}(\\mathcal{M})\\) for the sets of plays and histories of \\(\\mathcal{M}\\).\nWe fix an MDP \\(\\mathcal{M} = (S, A, \\delta)\\) for the rest of the section.\nStrategies. A strategy is a function that describes how to select actions based on the history of the ongoing play. A strategy may resort to randomisation. Formally, a (behavioural) strategy of \\(\\mathcal{M}\\) is a function \\(\\sigma: \\text{Hist}(\\mathcal{M}) \\rightarrow \\mathcal{D}(A)\\) such that for all \\(h \\in \\text{Hist}(\\mathcal{M})\\), \\(\\text{supp}(\\sigma(h)) \\subseteq A(\\text{last}(h))\\). A strategy is pure if it uses no randomisation, i.e., if it maps all histories to Dirac distributions. We view pure strategies as functions \\(\\sigma: \\text{Hist}(\\mathcal{M}) \\rightarrow A\\).\nA strategy is memoryless if the distribution it assigns to a history depends only on its last state, i.e., if for all histories \\(h, h' \\in \\text{Hist}(\\mathcal{M})\\), \\(\\text{last}(h) = \\text{last}(h')\\) implies \\(\\sigma(h) = \\sigma(h')\\). Memoryless (resp. pure memoryless) strategies can be viewed as functions \\(S \\rightarrow \\mathcal{D}(A)\\) (resp. \\(S \\rightarrow A\\)). We let \\(\\Sigma(\\mathcal{M})\\) and \\(\\Sigma_{\\text{pure}}(\\mathcal{M})\\) respectively denote the set of all strategies of \\(\\mathcal{M}\\) and the set of pure strategies of \\(\\mathcal{M}\\).\nA play \\(\\pi = s_0 a_0 s_1 \\ldots\\) is consistent with a strategy \\(\\sigma\\) if for all \\(\\ell \\in \\mathbb{N}\\), we have \\(a_{\\ell} \\in \\text{supp}(\\sigma(\\pi_{<\\ell}))\\). An outcome of a strategy is any play consistent with the strategy. Consistency of a history with a strategy is defined similarly."}, {"title": "3 Payoffs and multi-"}]}