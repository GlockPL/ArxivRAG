{"title": "DOMBA: DOUBLE MODEL BALANCING FOR\nACCESS-CONTROLLED LANGUAGE MODELS VIA\nMINIMUM-BOUNDED AGGREGATION", "authors": ["Tom Segal", "Asaf Shabtai", "Yuval Elovici"], "abstract": "The utility of large language models (LLMs) depends heavily on the quality and quantity of their\ntraining data. Many organizations possess large data corpora that could be leveraged to train or\nfine-tune LLMs tailored to their specific needs. However, these datasets often come with access\nrestrictions that are based on user privileges and enforced by access control mechanisms. Training\nLLMs on such datasets could result in exposure of sensitive information to unauthorized users. A\nstraightforward approach for preventing such exposure is to train a separate model for each access\nlevel. This, however, may result in low utility models due to the limited amount of training data per\nmodel compared to the amount in the entire organizational corpus. Another approach is to train a\nsingle LLM on all the data while limiting the exposure of unauthorized information. However, current\nexposure-limiting methods for LLMs are ineffective for access-controlled data, where sensitive\ninformation appears frequently across many training examples. We propose DOMBA\u2013 double model\nbalancing - a simple approach for training and deploying LLMs that provides high utility and access-\ncontrol functionality with security guarantees. DOMBA aggregates the probability distributions\nof two models, each trained on documents with (potentially many) different access levels, using a\n\"min-bounded\" average function (a function that is bounded by the smaller value, e.g., harmonic\nmean). A detailed mathematical analysis and extensive evaluation show that DOMBA safeguards\nrestricted information while offering utility comparable to non-secure models.", "sections": [{"title": "Introduction", "content": "Organizations can benefit greatly from training dedicated LLMs, such as coding assistants, email writers or question-\nanswering models, on their data (Tiwari et al. 2023). While the benefits can be substantial, such data often contains\nrestricted information, and an access-control mechanism ensuring users can only access information according to their\naccess rights is usually in place. However, LLMs inherently lack such access-control mechanisms, which can lead to\nthe exposure of sensitive information to unauthorized users (Carlini et al. 2021; Kandpal et al. 2024; Pan et al. 2020).\nA basic approach for introducing access control to LLMs is to train a separate LLM for each access level (Tiwari\net al. 2023). However, as our experiments show, this approach can substantially reduce model utility, since the amount\nof data for each access level is limited. For example, training a model on emails from only one department in an\norganization may be insufficient for constructing effective organizational emails. To overcome this limitation, sufficient\ndata (including restricted data) must be included in the model's training set. This means that any secure and high utility\nmethod should limit the exposure of the training data to users of the model (according to their access rights).\nIn this paper, we propose DOMBA, a method for training and deploying LLMs that incorporates an access-control\nmechanism while maintaining high utility. An overview of DOMBA is presented in Figure 1. To protect sensitive\ninformation, DOMBA \u201cbalances\" two submodels (trained on two different data partitions, each including different\naccess levels) during inference, using a min-bounded average function; intuitively, each submodel \"knows\" different\nrestricted information. During text generation, the min-bounded function makes it unlikely for information known to\njust one submodel to be generated. Assuming that restricted information is not shared between the two partitions, this"}, {"title": "Related Work", "content": "Very few studies have addressed the use of LLMs in the access-control scenario: Tiwari et al. (2023) proposed using\nmixture of experts (MoE) in conjunction with training a separate model for each access level in order to support users\nwith multiple access rights. However, this approach relies solely on non-restricted documents, which substantially\nreduces its utility. Wutschitz et al. (2023) proposed using retrieval-augmented generation (RAG) with access rights,\nwhich prevents the retrieval of unauthorized documents. As illustrated by Tiwari et al. (2023), using RAG by itself,\nwithout training the model on the access-controlled data, may be insufficient to achieve a substantial adjustment of LLM"}, {"title": "Methodology", "content": "In this section, we define the concept of exposing a secret and describe DOMBA's training and aggregation processes.\nUsing formal mathematical language, we establish a bound to DOMBA's exposure of secrets. We also show that no\nother aggregation method could ever achieve a better bound."}, {"title": "Training", "content": "DOMBA-INIT: Let $d_1, ..., d_k$ be the datasets corresponding to access levels 1, ..., k. We randomly assign each access\nlevel to one of two data partitions and train a submodel on each partition separately, denoted as $M_1$ and $M_2$.\nDOMBA-FT: For each access level AL, let $M_1$ and $M_2$ be the resulting submodels of DOMBA-INIT. If AL was\nassigned to $M_1$ during DOMBA-INIT, we fine-tune $M_2$ on $d_{AL}$. Otherwise, we fine-tune $M_1$ on $d_{AL}$. We then save\nthe states of $M_1$ and $M_2$, which will be used during inference for users with access level AL. If a user has multiple\naccess rights, an MoE can be used as demonstrated by Tiwari et al. (2023) (this scenario is not explored in this study).\nWe note that PEFT (parameter-efficient fine-tuning) methods such as LORA (Hu et al. 2021) can be used to efficiently\ntrain and store the different states of $M_1$ and $M_2$."}, {"title": "Preliminaries", "content": "Let $\\Sigma$ be a set of tokens, and let $n = |\\Sigma|$. We use $t$ to refer to a token and $c$ to refer to a context (i.e., a sequence of\ntokens preceding $t$). We use $M, M_1, M_2$ to refer to next-token prediction language models and denote the probability\nassigned by $M$ to token $t$ given context $c$ as $p_M(t|c)$. We use $\\sum$ (sum) without specification to indicate summation over"}, {"title": "Exposure of Secrets", "content": "We begin by defining exposing a secret in a formal sense. As highlighted by Brown et al. (2022), secrecy is relative -\nsomething is deemed secret if it is known by some but unknown to others. Therefore, our concept of secrecy involves\ncomparing probabilities assigned to a token by two models. One possible approach is to use the ratio of the probabilities\nassigned by the models to assess secrecy. However, this method has drawbacks. Consider the following probability\ndistributions over the tokens a, b, c, d: $p_1 = (0.7, 0.1, 0.1, 0.1)$ and $p_2 = (0.97, 0.01, 0.01, 0.01)$. The probabilities'\nratios $(p_1/p_2)$ are $(0.72, 10, 10, 10)$. This implies that tokens b, c, and d are \u201csecret\" in $p_1$ compared to $p_2$. However,\nit seems more appropriate to consider a as secret in $p_2$ compared to $p_1$, because $p_2$ assigns a a probability that is 97\nhigher than all other tokens, whereas $p_1$ assigns it a probability that is only seven times higher. To address this, we\ncompare the probability ratio of a token t (between two models) to a \"typical probability ratio\" (TPR).\nDefinition 1 (Geometric mean). Let $f : \\Sigma \\rightarrow \\mathbb{R}^+$. The geometric mean of $f$ is $GM(f(t)) := exp(\\sum log(f(t)))$.\nDefinition 2 (TPR). Let $c$ be a context, and let $M_1, M_2$ be language models. We define the \"TPR at c\" of $M_1, M_2$ as\n$tpr_c(M_1, M_2) = GM(\\frac{p_{M_1}(t|c)}{p_{M_2}(t|c)})$.\nDefinition 3 (Token exposure). Let $c$ be a context, $t$ be a token, and $M_1, M_2$ be language models. We call $t$ \"$\\alpha$-exposed\nby $M_1$ over $M_2$ at $c$ if $\\frac{p_{M_1}(t|c)}{p_{M_2}(t|c)\\cdot tpr_c(M_1,M_2)} = \\alpha$. We also say that $t$ is \u201c<$ \\alpha$-exposed\" if $t$ is $\\beta$-exposed for some $\\beta < \\alpha$.\nIn other words, instead of directly dividing $p_{M_1}(t|c)$ by $p_{M_2}(t|c)$, we adjust $p_{M_2}(t|c)$ by multiplying it by the TPR.\nIn the example discussed, the TPR is 5.18, which results in the following exposures of tokens a, b, c, d: $M_1$ over $M_2$:\n$(0.14, 1.93, 1.93, 1.93)$ and $M_2$ over $M_1$: $(7.19, 0.52, 0.52, 0.52)$. These values better reflect our intuition that a is\nsecret and not b, c, and / or d."}, {"title": "Exposure Properties", "content": "In this subsection, we explore certain properties of exposure that are essential for later discussions.\nDefinition 4 (Typical and relative probability). Let $c$ be a context, and let $M$ be a language model. We define the\n\u201ctypical probability at $c$\u201d of $M$ as $tp_c(M) = GM(p_M(t|c))$.\nLet $t$ be a token, we further define the \u201crelative probability of $t$ at $c$ by $M$\u201d as $rp_M(t|c) := \\frac{p_M(t|c)}{tp_c(M)}$\nLemma 1. $tpr_c(M_1, M_2) = \\frac{tp_c(M_1)}{tp_c(M_2)}$\nProof. $tpr_c(M_1, M_2) = exp(\\sum log(\\frac{p_{M_1}(t|c)}{p_{M_2}(t|c)})) = \\frac{exp(\\sum log(p_{M_1}(t|c)))}{exp(\\sum log(p_{M_2}(t|c)))} = \\frac{tp_c(M_1)}{tp_c(M_2)}$\nBy this lemma, $\\alpha$-exposed is equivalent to $\\frac{rp_{M_1}(t|c)}{rp_{M_2}(t|c)} = \\alpha$.\nLemma 2 (Exposure-multiplicity). If $t$ is $\\alpha$-exposed by $M_1$ over $M_2$ at $c$ and $\\beta$-exposed by $M_2$ over $M_3$ at $c$, then $t$ is\n$\\alpha\\beta$-exposed by $M_1$ over $M_3$ at $c$.\nProof. $\\frac{rp_{M_1}(t|c)}{rp_{M_3}(t|c)} = \\frac{rp_{M_1}(t|c)}{rp_{M_2}(t|c)} \\cdot \\frac{rp_{M_2}(t|c)}{rp_{M_3}(t|c)} = \\alpha\\beta$."}, {"title": "Aggregation", "content": "In this subsection, we provide a formal definition of the notion of a min-bounded function and describe how DOMBA\naggregates two submodels.\nDefinition 5 (Proper-avg function). Let $f : \\mathbb{R}^{+^2} \\rightarrow \\mathbb{R}^+$. We call $f$ a proper-avg function if $\\forall x, y : min(x, y) \\leq\nf(x, y) \\leq max(x, y)$.\nDefinition 6 (Min-bounded function). Let $f$ be a proper-avg function. we call $f$ min-bounded if $\\forall x, y, f(x, y) \\leq\n\\Lambda_f min(x, y)$ for some constant $\\Lambda_f$.\nIn practice we use the generalized mean (Sykora 2009) with $\\alpha < 0$ for min-bounded functions, that is, $f(x, y) = \\frac{1}{(\\frac{x^{\\alpha}+y^{\\alpha}}{2})^{\\frac{1}{\\alpha}}}$, $\\Lambda_f = 2^{-\\frac{1}{\\alpha}}$. Two special cases are:"}, {"title": "Bounding the Exposure of DOMBA", "content": "In this subsection, we establish the bounds on DOMBA's exposure over both submodels (Theorem 2) as well as over\nany other model (Corollary 1). We begin by introducing several definitions and lemmas that will be used for proving\nthe main theorem later on.\nDefinition 8. Let $M_1, M_2$ be language models, and let $f$ be a min-bounded function. Let $M = DAGG_f(M_1, M_2)$. We\ndefine $f_c(M_1, M_2) = GM(M(t|c)^{-1})$.\nWhile it might be unclear how to interpret $f_c(M_1, M_2)$, it is related to a notion of \u201cmean exposure\" between $M_1$ and\n$M_2$:\nDefinition 9 (Mean absolute exposure). Let $c$ be a context and $M_1, M_2$ be language models. We define the \u201cmean\nabsolute exposure between $M_1$ and $M_2$ at $c$\u201d as $MAE_c(M_1, M_2) = GM(max(\\frac{rp_{M_1}(t|c)}{rp_{M_2}(t|c)}, \\frac{rp_{M_2}(t|c)}{rp_{M_1}(t|c)}))$.\nLemma 3. $f_c(M_1, M_2) \\leq \\sqrt{MAE_c(M_1, M_2)}$.\nProof. Let $x := \\sum log(min(rp_{M_1}(t|c), rp_{M_2}(t|c)))$, $y := \\sum log(max(rp_{M_1}(t|c), rp_{M_2}(t|c)))$. We observe that\n$x + y = \\sum log(rp_{M_1}(t|c)) + \\sum log(rp_{M_2}(t|c)) = 0+0 = 0$. by definition, $y - x = n\\cdot log(MAE_c(M_1, M_2))$, which\nimplies, $x = -\\frac{y}{2} = -\\frac{n}{2}log(MAE(M_1, M_2))$, we conclude that $f_c(M_1, M_2) \\leq exp(\\frac{-y}{2}) = \\sqrt{MAE_c(M_1, M_2)}$.\nLemma 4. $rp_M(t|c) = M(t|c) \\cdot f_c(M_1, M_2)$.\nProof. $rp_M(t|c) = \\frac{p_M(t|c)}{tp_c(M)} = \\frac{p_M(t|c)}{exp(\\sum log(p_M(t|c)))} = \\frac{M(t|c)}{exp(\\sum log(M(t|c)))} = M(t|c) \\cdot f_c(M_1, M_2)$\nIn the following theorem, we provide a lower bound to the minimum token exposure achievable over two models.\nTheorem 1. Let $c$ be a context and $M, M_1, M_2$ be language models. There exists a token $t$ that is $\\geq \\sqrt{MAE_c(M_1, M_2)}$-\nexposed by $M$ over either $M_1$ or $M_2$.\nProof. By the proof of lemma 3: $\\sqrt{MAE_c(M_1, M_2)} = exp(\\frac{-y}{n}) = GM(\\frac{1}{min(\\frac{rp_{M_1}(t|c)}{rp_{M_2}(t|c)}, \\frac{rp_{M_2}(t|c)}{rp_{M_1}(t|c)}))}$. The right end\nside is an average over tokens. Therefore there exists a token for which the term inside is greater than or equal to the\nleft end side, which finishes the proof.\nIn the following theorem, we demonstrate that using DOMBA provides a bound on the exposure that is a constant\nmultiple of the best possible bound (Theorem 1). This constant is solely dependent on $f$ and can even reach a value of 1\n($f$ = Minim\u0438\u0442).\nTheorem 2. Let $f$ be a min-bounded function, $t$ a token and $M = DAGG_f(M_1, M_2)$. t is $\\leq \\gamma$-exposed over both $M_1$\nand $M_2$ for $\\gamma = \\frac{\\Lambda_f}{f_c(M_1, M_2)} \\sqrt{MAE_c(M_1, M_2)} \\leq \\Lambda_f \\sqrt{MAE_c(M_1, M_2)}$.\nProof. $\\frac{rp_M(t|c)}{rp_{M_1}(t|c)} = \\frac{M(t|c) \\cdot f_c(M_1,M_2)}{rp_{M_1}(t|c)} < \\frac{\\Lambda_f min(rp_{M_1}(t|c),rp_{M_2}(t|c))\\cdot f_c(M_1,M_2)}{rp_{M_1}(t|c)} < \\Lambda_f f_c(M_1, M_2)$.\nWe note that by assuming $M_1$ and $M_2$ assign similar relative probabilities to most tokens, we can anticipate the mean\nabsolute exposure to be low. Essentially, we achieve average case behavior for all tokens.\nIn the following corollary, we informally think of $M_b$ as our base model (although the corollary holds in general)."}, {"title": "Evaluation", "content": "Since access-controlled datasets are not publicly available, we required datasets that mimic the access-control scenario.\nThese datasets need to be divided into different topics (which serve as access levels), with many data records per topic.\nAdditionally, to use two of our security evaluation metrics, the data records should contain phrases that we refer to as\n\u201csensitive-mimicking phrases\u201d \u2013 phrases unique to the topic that could be considered sensitive / secret.\nMovie Reviews The first dataset we utilized is the IMDB Spoiler reviews dataset (Misra 2019). We randomly selected\n50 reviews of different movies released after 2015 and considered the movie of each review selected as an access\nlevel. Then, we collected all of the reviews for each of the 50 movies. We note that some reviews contain details about\nthe movie's plot, cast members, or characters, which mimic sensitive information. We utilized the Movies Metadata\ndataset (Banik 2018) to retrieve cast members' names and used them as sensitive-mimicking phrases. The number of\nreviews totaled 22,742, with 10% of each movie's reviews set aside for evaluation. The number of reviews per movie\nranged between 160 and 751.\nRecipes The second dataset used is the Food.com Recipes and Interactions dataset (Li 2019). We utilized class labels\nof the Food-101 dataset (Bossard, Guillaumin, and Van Gool 2014) to partition the recipes into multiple sets. Each set\nincludes recipes with titles containing a specific class label (e.g., pizza). We selected the 10 most frequent classes as the\naccess levels. We note that the recipes include specific details about the process of creating each dish, which can mimic,\nfor example, sensitive detailed descriptions of product manufacturing processes. We use the ingredients of each recipe\nas sensitive-mimicking phrases. However, since some ingredients are common among many classes, we only consider\ningredients that appear in recipes of a certain class with a frequency at least 10 times greater than the frequency in all of\nthe recipes. The number of recipes totaled 10,829, with 10% of each class put aside for evaluation. The number of\nrecipes per class ranged between 408 and 2283."}, {"title": "Training", "content": "For training we used LORA (Hu et al. 2021). LORA is a fine-tuning technique that uses a small number of trainable\nparameters. Training is relatively fast with LORA, and the resulting model requires minimal storage space. These\nqualities were crucial for our experiments, as we conducted numerous trials with limited computational resources.\nHowever, it is important to note that the theoretical analysis is not dependent on the training method, and we anticipate\nthat the experiments can be replicated using other training techniques as well. The base model used was OpenAI-GPT\n(Radford et al. 2018) which has 117 million parameters and a vocabulary size of 40,478. This model's original\ntraining data is a books dataset from 2015 (Zhu et al. 2015). This limits the prior knowledge the model possesses\nregarding movies and recipes. Since recent LLMs are trained on more recent and diverse datasets, evaluating them on\nsensitive information from the movie and recipe datasets would be challenging, as the models are probably familiar\nwith some of the information. We note that although recent LLMs are larger and perform better than OpenAI-GPT,\nmany are still based on the same underlying principles. Our theoretical analysis and proposed approach generalizes to\nany language model based on next-token prediction and does not rely on the specifics of any particular architecture.\nRegarding training parameters, we conducted experiments with varying numbers of training epochs (1, 2, and 4). The\nhyperparameters for LORA were set to default values and were not explored: r=64, lora_alpha=32, lora_dropout=0.05,"}, {"title": "Metrics", "content": "In this section, we describe the metrics used to evaluate the models' utility and security. For utility we use perplexity,\nwhich measures the model's ability to predict the next token in a text. For security we use four different metrics:\nexposure, secret perplexity, a secret inference attack AUC-ROC, and the canary technique score (Carlini et al. 2019).\nWe note that for access-controlled models, we evaluate the security of each variant (corresponding to an access level)\nusing data with a different access level than the one that the variant was trained for.\nUtility Evaluation We evaluate utility in terms of perplexity on two evaluation sets as follows: 1. HOPPL:\nperplexity on held out data with access levels that were not used for training. This metric provides a \u201cfair\u201d way\nof comparing secure and non-secure models, as the non-secure models are not expected to gain by \u201cknowing\u201d\nrestricted information. 2. AUPPL: perplexity on held out data of the access levels used for training (for access-\ncontrolled models - the corresponding variant is used for each access level). The main purpose of this metric is\nto compare the utility of secure and access-controlled models. We expect the access-controlled models to gain\nutility by \u201cknowing\u201d authorized restricted information. For both metrics above, we calculate the perplexity as:\n$perp_M(D_e) = exp(\\frac{\\sum_{r \\in D_e} \\sum_{i=1}^{|r|} - log(p_M(r_i|r_{<i}))}{|D_e|})$. where $|D_e|$ is the amount of tokens in $D_e$, $r$ is a data record,\n$r_i$ is the i'th token in the record, $r_{<i}$ are the tokens preceding it, and $p_M$ is the probability assigned by the model.\nExposure (EXP) In our theoretical analysis (Theorem 2), we established that the exposure of $M = DAGG_f(M_1, M_2)$\nover both $M_1$ and $M_2$ is bounded for any token by $\\Lambda_f f_c(M_1,M_2) < \\Lambda_f\\sqrt{MAE_c(M_1, M_2)}$. To validate this, we\nmeasure \u201cextreme case\u201d exposure of $M$ over $M_1$ and $M_2$. We report the maximum and 99th percentile exposure\n(= $min(\\frac{rp_{M_1}(t|c)}{rp_M(t|c)},\\frac{rp_{M_2}(t|c)}{rp_M(t|c)})$) for all tokens observed in the data, given the previous tokens as context.\nSecret Perplexity (SPPL) One way of measuring the model's ability to handle sensitive information is by eval-\nuating perplexity specifically on sensitive-mimicking phrases. Given a model M, we measure the perplexity of\neach instance of a sensitive-mimicking phrase in the evaluation dataset. Specifically, let $x := x_1, ..., x_k$ be the\ntoken representations of a sensitive-mimicking phrase and c be the tokens preceding this phrase, we measure\n$perp_M(x|c) = exp(\\frac{\\sum_{i=1}^{k} -log(p_M(x_i|c, x_1, ..., x_{i-1}))}{k})$. We report the average of the mean perplexity of each\naccess level. This metric aims to provide a basic, rough evaluation of a model's ability to handle sensitive information.\nSecret Inference Attack (SIA) This attack is based on a membership inference attack with a reference model\n(Mireshghallah et al. 2022; Kumar Murakonda, Shokri, and Theodorakopoulos 2021). The original attack works as\nfollows: Given a reference model $M_b$, a target model $M$, and a potential training data record $r$ of $M$, measure the log\nratio of the probabilities of $r$ according to $M$ and $M_b$, that is $log(\\frac{p_M(r)}{p_{M_b}(r)})$. If this value is above a certain threshold,\nconsider $r$ as belonging to the training data of $M$. In our scenario, instead of inferring the membership of any data\nrecord, the adversary tries to infer secrets. Therefore, we only consider probabilities assigned to sensitive-mimicking\nphrases: cast members' names for the movie review dataset and secret ingredients for the recipe dataset. The attack\ndataset consists of tuples $(c, t, label)$, where c is a context, t is a phrase, and label is true if t is sensitive and false"}, {"title": "Results", "content": "The results with two epochs of training are presented in Table 1. In terms of utility, FT-ALL performed the best across\nboth metrics, as expected. Among the secure and access-controlled models, D-I-H achieved the highest utility on the\nHOPPL metric, while DOMBA achieved the highest utility on the AUPPL metric, with a substantial gap compared to\nsecure models, highlighting the importance of the DOMBA-FT step. Comparing access-controlled models, DOMBA\nexhibited substantially better utility than PER-AL across both metrics and datasets.\nRegarding security, non-secure models performed substantially worse, compared to secure models, on all metrics.\nAmong the secure and access-controlled models, SUBMIX obtained the worst values for all metrics and datasets, D-I-M\nand DOMBA obtained the best values, and D-I-H was slightly worse. Although secure models provide substantially\nbetter security compared to non-secure models, they are not perfect. For instance, a perfectly secure model would score\n0.5 on SIA and one on CAN. This does not imply that secure models are not actually secure. For example, the values\nobtained by all secure and access-controlled models for the canary technique metric are considered impractical for\nextracting useful information (Carlini et al. 2019).\nFigure 2 shows the worst-case and 99th percentile exposure of models employing different aggregation methods on\nthe recipe and movie review datasets for 1, 2, and 4 training epochs. The maximum exposure of DOMBA, D-I-H,\nand D-I-M is 4.69. In comparison, SUBMIX reaches a maximum exposure of 8.5e4 and AGG-A reaches a maximum\nexposure of 1.3e10. We observe that DOMBA's 99th percentile exposure is similar to its maximum exposure, supporting\nthe theoretical bound established by our analysis (Theorem 2). Regarding the effect of the nubmer of epochs, increasing\nit generally leads to higher exposure (except for SUBMIX's 99th percentile exposure on the recipe dataset). However,\nthe increase in exposure for DOMBA, D-I-H, and D-I-M is moderate compared to AGG-A for both datasets, while for\nSUMBIX, the change in exposure is inconsistent between the two datasets.\nFigure 3 illustrates the trade-off between utility and security for different methods across both datasets. For most models,\nas the number of training epochs increases, security tends to worsen while utility improves. However, non-secure\nmodels experience a much greater decline in security. DOMBA achieves the best trade-off, providing superior security\nwhile maintaining utility levels similar to those of the non-secure models."}, {"title": "Discussion", "content": "Our results show that D-I-H and D-I-M achieve a good utility-security trade-off. This suggests that our method may\nbe suitable for general privacy-preserving purposes beyond access control. In future research, it will be interesting\nto develop a variation of DOMBA for non-access-controlled private datasets and compare its performance to that of\nstate-of-the-art privacy-preserving methods which are not focused on the access-control scenario.\nOne limitation of DOMBA is that its security cannot be increased further (assuming minimum is used as the aggregation\nfunction). This is opposed to DP methods which can achieve any security level (with a cost on utility) by adjusting a\nprivacy parameter. Future research could explore hybrid approaches, combining DOMBA with DP techniques to offer\nsecurity beyond DOMBA's current maximum level of security.\nAdditionally, DOMBA relies on a strict separation of access levels into two distinct partitions without shared sensitive\ninformation. Such separation could be challenging to implement in some scenarios. To make DOMBA more robust to\nsensitive information shared between access levels, future research could explore the separation of the access levels into\nmore than two partitions.\nResource overhead is incurred with DOMBA's deployment due to the use of two LLMs instead of one, which may be\nimpractical for some applications. One potential solution is to employ DOMBA as a teacher model to train a student\nmodel via knowledge distillation (Xu et al. 2024), where the student model serves as a deployed model mimicking\nDOMBA."}, {"title": "Conclusion", "content": "In this paper, we proposed DOMBA, a novel approach for training and deploying access-controlled LLMs with high\nutility. We formalized the concept of exposed secrets and bounded DOMBA's exposure. We evaluated DOMBA's\nperformance on two access-controlled datasets, reflecting real world organizations' needs. Our evaluation showed that\nDOMBA achieves a better security-utility trade-off than existing methods, across both datasets, two utility metrics and\nfour security metrics. Finally, we believe that the principles of min-bounded aggregation and relative probabilities,\nwhich serve as DOMBA's core, have substantial potential to serve as foundational elements in a wide range of future\nmachine learning research, extending beyond the scope of security."}]}