{"title": "Runtime Tunable Tsetlin Machines for Edge Inference on eFPGAs", "authors": ["Tousif Rahman", "Gang Mao", "Bob Pattison", "Sidharth Maheshwari", "Marcos Sartori", "Adrian Wheeldon", "Rishad Shafik", "Alex Yakovlev"], "abstract": "Embedded Field-Programmable Gate Arrays (eFPGAs) allow for the design of hardware accelerators of edge Machine Learning (ML) applications at a lower power budget compared with traditional FPGA platforms. However, the limited eFPGA logic and memory significantly constrain compute capabilities and model size. As such, ML application deployment on eFPGAs is in direct contrast with the most recent FPGA approaches developing architecture-specific implementations and maximizing throughput over resource frugal-ity. This paper focuses on the opposite side of this trade-off: the proposed eFPGA accelerator focuses on minimizing resource usage and allowing flexibility for on-field recalibration over throughput. This allows for runtime changes in model size, architecture, and input data dimensionality without offline resynthesis. This is made possible through the use of a bitwise compressed inference archi-tecture of the Tsetlin Machine (TM) algorithm. TM compute does not require any multiplication operations, being limited to only bitwise AND, OR, NOT, summations and additions. Additionally, TM model compression allows the entire model to fit within the on-chip block RAM of the eFPGA. The paper uses this accelerator to propose a strategy for runtime model tuning in the field. The proposed approach uses 2.5x fewer Look-up-Tables (LUTs) and 3.38x fewer registers than the current most resource-fugal design and achieves up to 129x energy reduction compared with low-power microcontrollers running the same ML application.", "sections": [{"title": "1 INTRODUCTION", "content": "Deploying Machine Learning (ML) applications on the edge in-volves finding an adequate trade-off between the performance of the ML model, in terms of accuracy and the compute and memory constraints of the edge device [20, 25]. The trade-off is made more challenging when considering the compute cost of floating-point multiply-accumulate (MAC) arithmetic and 32-bit model sizes of traditional Deep Neural Networks (DNNs). Fortunately, these chal-lenges can be somewhat alleviated by quantizing DNN weights and data, often to a single bit producing Binary Neural Networks (BNNs) [2, 5, 6, 17, 28]. This means that the MAC is simplified to XNOR and popcount operations and weights in memory now use 1-bit each. This approach is particularly effective when trans-lated to accelerators targeting Field Programmable Gate Arrays (FPGAs). The most recent works follow one of two strategies: ef-ficiently mapping quantized computation to the FPGA's Look-up-Tables (LUTs) by converting the memory elements into custom compute [2, 18, 23, 26]. Or, they develop architecture-specific ap-proaches for the ML model that are centered around a parameterized compute engine [5, 17, 22, 28]. Often both types of approach are also wrapped in design automation flows.\nThis paper focuses on the alternative side of this trade-off: mini-mizing resource utilization as much as possible to target smaller"}, {"title": "2 SPARSE TSETLIN MACHINES", "content": "For inference, the state inside each TA in a TM model is represented only by a 1-bit Include or Exclude action (see Fig. 2 if needed). During TM training, this leads to a very sparse model where the number of Excludes vastly outnumbers the Includes [1, 3, 15]. This sparsity can be exploited when examining Fig 3.2. It shows the impact of an Include and an Exclude on the clause output. The Exclude eliminates the contribution of an input Boolean literal (X"}, {"title": "3 DESIGN AND RUNTIME TUNABILITY", "content": "The proposed architecture performs TM inference using the com-pressed instructions described in the previous section. Through Fig. 4 the main attributes of the accelerator design will be high-lighted: Real-time adaptability of the model and task, customization options, and resource frugality:\nProgramming for Real-time Adaptability: The accelerator can be reconfigured for different TM models and input data sizes in real-time using a data stream as seen in Fig. 4.1. The data packets start with a header to configure the architecture. The header is either an Instruction Header (Fig. 4.2) to send a new TM model, or a Feature Header (Fig. 4.3) to send Boolean features for inference. Headers can be configured as 16,32 or 64-bits. The MSB bit indicates that this is a new stream and resets the accelerator. The second bit from the MSB bit indicates whether the following data packets will contain either the Include instructions (Instruction Header), or Boolean Features for inference (Feature Header).\nThe remainder of the Instruction Header contains the model architecture parameters: the number of classes and the number of clauses. This is used later in the accumulation counters (Clause Counter and Class Counter). The remainder of the Feature header contains the number of Inference data packets the accelerator will need to process to generate classifications. Real-time architecture change is made possible due to the simplicity of the TM structure. Only three parameters are sufficient to update the accelerator to a new TM model size (instruction number in Instruction Header) or a new task and new data dimensionality (Class number in Instruction Header and Inference packet number in Data Header)."}, {"title": "5 CONCLUSION", "content": "This work explored the possibilities of LUT frugal and flexible accelerator development for applications that require runtime recal-ibration. The Tsetlin Machine algorithm leads to minimal bitwise compute, and its sparsity enables the instruction-based compression to allow models to fit well with the BRAMs of of-the-shelf eFPGA platforms. While current FPGA works leverage reconfigurability to build the fastest possible custom architectures, this work uses it to allow customization options in memory, batching, interconnect and number of cores. The proposed accelerator implementations offer far better energy efficiency than the low-power MCUs running the same compressed model inference."}]}