{"title": "Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives", "authors": ["Elliot Meyerson", "Xin Qiu"], "abstract": "Decomposing hard problems into subproblems often makes them easier and more efficient to solve. With large language models (LLMs) crossing critical reliability thresholds for a growing slate of capabilities, there is an increasing effort to decompose systems into sets of LLM-based agents, each of whom can be delegated sub-tasks. However, this decomposition (even when automated) is often intuitive, e.g., based on how a human might assign roles to members of a human team. How close are these role decompositions to optimal? This position paper argues that asymptotic analysis with LLM primitives is needed to reason about the efficiency of such decomposed systems, and that insights from such analysis will unlock opportunities for scaling them. By treating the LLM forward pass as the atomic unit of computational cost, one can separate out the (often opaque) inner workings of a particular LLM from the inherent efficiency of how a set of LLMs are orchestrated to solve hard problems. In other words, if we want to scale the deployment of LLMs to the limit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM primitives should be used to reason about and develop more powerful decompositions of large problems into LLM agents.", "sections": [{"title": "1. Introduction", "content": "The turn to agents is well underway (Guo et al., 2024; Wang et al., 2024b). Now that large language models (LLMs) have crossed critical thresholds of general capability and reliability (Bubeck et al., 2023), it is natural to ask how they can be integrated into computational systems larger than themselves, in order to do things: \"Stop speaking and act!\" The term agent has quickly crystallized as the term for LLM-based programs that carry out subprocesses within larger systems or act \u201cautonomously\u201d in virtual or physical environments. Unlike the general purpose LLMs from which they descend, LLM agents usually have a specific scope in which they are intended to operate, i.e., specific kinds of inputs they are expected to receive and corresponding kinds of outcomes they are expected to produce. Agent scopes vary in the precision with which they are specified, but the very act of specifying a scope, or role, for an agent allows it to be treated as a computational object that can serve as a component in the development of a larger system. The promise of LLM agents is not only that they have the capacity to affect change in the real world, but that by constructing increasingly larger systems of many agents, where each has its own complementary focus, we can dramatically expand the scale at which LLM-based AI is applied.\nThere has been a rush to start building such systems (Xi et al., 2025). In academia, LLM agents have been used in applications across the board, for example, in improving performance on standard benchmarks (Du et al., 2024), building teams for more effective software development (Qian et al., 2024; Wu et al.), simulating communities and human behavior (Park et al., 2023; Yan et al., 2024), and tackling the research process itself (Huang et al.; Lu et al., 2024; Yu et al., 2024). In industry, the impact (at least on strategy and priorities) has been arguably greater: while academics usually prefer minimal systems in which agent behavior can be most clearly understood, industry leaders find themselves faced daily with huge organizations with countless interconnected actors and components, and thus countless opportunities for agents to be applied (Hodjat, 2024; Urlana et al., 2024; Sypherd & Belle, 2024).\nThe core focus thus far in the development of such systems has been \"What is possible with the resources we have today?\", not \"How efficient can it be in the limit of scale?\" Efficiency at scale can be understandably an afterthought when the goal is to build something that works and is impressive as quickly as possible, but this paper argues that understanding the efficiency of LLM-agent-based systems at a fundamental algorithmic level is critical to achieving the scale such systems promise. Specifically, missing from the study of such systems are asymptotic analyses that give a fundamental characterization of the cost (temporal, energy,"}, {"title": "2. Asymptotic Analysis with LLM Primitives", "content": "In order to make the argument of this paper concrete, this section sketches a minimal framework for asymptotic analysis with LLM primitives (AALPs). Although it elides many phenomena critical to the complete understanding of agentic LLM systems, this basic framework is sufficient to instantiate clear examples of the advantages of an asymptotic analysis-driven approach to system development, as will be shown in Section 3. This framework can then serve as a seed for future research that develops a more complete approach, as discussed in Section 5.\nLet us first define the central object of interest:\nDefinition 2.1 (Language-based Algorithm). A language-based algorithm (or LLM-based Algorithm (Chen et al., 2024)), LbA for short, is any algorithm in which one or more of the computational steps are performed by an LLM.\nIn other words, an LbA consists of a set of LLM-based agents that work together to complete tasks.\nIn the classical asymptotic analysis of algorithms, the basic unit of computation is a primitive, an atomic operation whose executions can be counted characterize the behavior of the algorithm at scale. Standard choices of a primitive include addition or variable assignment, i.e., operations that can be performed in a single CPU cycle. This paper argues that the appropriate primitive for asymptotic analysis of agentic LLM-based systems is a single execution of an LLM, i.e., a forward pass that generates a single token $v \\in V$. By treating LLMs as atomic, we can focus on the fundamental behavior of the LbA, orthogonal to ongoing improvements inside the LLMs themselves. We count only LLM operations because the computational resources to run LLMs are usually the most salient limiting factor in scaling LbAs. Since our focus is on scaling the deployment of LbAs, we can also ignore training cost, with the expectation that the cost of running inference with fixed models over a long period of time and massive scale will rapidly dominate the cost of training (Sardana et al., 2024) (a loosening of this assumption is discussed in Section 5.5).\nFor convenience, we assume the cost of an LLM execution depends linearly on the number of input tokens n and the size of the model m. This assumption is based on the fact that industry APIs like those from OpenAI and Anthropic charge linearly per token (OpenAI, 2025; Anthropic, 2025). Although in principle the cost of a single forward pass through a transformer model scales quadratically with the input size (Vaswani, 2017), core optimizations like sparse attention (Tay et al., 2020) and representation caching (Luohe et al., 2024) have moved the practical cost closer to linear, as reflected in the API prices. If the cost is in fact superlinear, asymptotic separations between LbAs will generally be even sharper. So, for the purposes of this paper, we use the following definition:\nDefinition 2.2 (LLM Primitive). An LLM primitive is an operation defined by an LLM M of size m, such that a single application of M to a string of length n has cost mn.\nKey to this definition is the idea that different LLMs $M_1, M_2,...$ can have drastically different sizes $m_1, m_2, ...$ and thus drastically different costs. Notice that we use \"cost\" here in a general sense, it could refer to an estimate of the raw FLOPS, the economic cost (Chen et al., 2023; Shekhar"}, {"title": "3. Example Asymptotic Separations", "content": "This section provides concrete examples of how AALPs can be applied to demonstrate massive asymptotic separations between LbA's when applied at scale. The goal of these examples is not to show challenging math, but simple results that immediately highlight the importance of such analysis. Our hope is that these simple examples will inspire future work that tackles more complex LLM-agent systems.\nThe three example problems in this section highlight different kinds of opportunities that would arise from having a solid complexity theory for LLM agents. The first highlights the criticality of understanding where to focus LLM size optimization in many-agent systems; the second highlights critical pitfalls of na\u00efve anthropomorphic multi-agentization; and the third highlights issues that arise from applying LLMs to large and creative optimization tasks. Note that the analysis in this section is intentionally loose, and intended to be as rudimentary as possible, as the goal is simply to highlight the kinds of insights that can emerge, and argue that further development of AALPs will be critical in understanding how to scale real-world systems. To that end, we invite the reader to critique any assumption (implicit or explicit) made in the following analyses, with the hope that such critique will lead to more practical AALPs methodologies and thus more powerful LbAs.\n3.1. k-Task Routing\nThe section highlights potential scaling advantages of having specialized agents of asymptotically smaller size than highly-capabable generalist agents. For this problem, suppose the input size is n and the output size is $O(1)$. Many common tasks like information retrieval, classification, and agentic single-action tasks fall under this specification. Suppose there are k such distinct tasks of this form that we would like the system to be able to perform.\n3.1.1. GENERALIST APPROACH\nThe simplest approach would be to use a single generalist many-task LLM agent $M_g$ of size $m_g$ capable of solving any of the tasks on its own. Processing the entire input and returning a constant sized answer will then cost\n$\\Theta(m_gn)$.\n3.1.2. DELEGATOR AND SPECIALISTS APPROACH\nSuppose instead that we have a system consisting of a delegator $M_d$ of size $m_d$ and k specialists $M_1, ..., M_k$, one for each of the k distinct tasks. For simplicity, assume the specialists are all of equal size $m_s$. Suppose the delegator can determine which of the k tasks the input string belongs to by observing only a constant number of metadata tokens prepended to the input. Then, the cost of identifying the task and solving it with the designated specialist is\n$\\Theta(m_d) + \\Theta(m_sn)$.\n3.1.3. IMPLICATIONS\nWe can safely assume $m_d \\le m_g$, since solving all k tasks almost certainly requires (implicitly or explicitly) identifying which task is being solved as a sub-capability. So, as n grows, the comparison of interest is $\\Theta(m_gn)$ vs. $\\Theta(m_sn)$, i.e., $m_g$ vs. $m_s$. From analysis of the neural scaling laws of LLMs (Kaplan et al., 2020), we can assume the required size of the LLM scales with the number of required capabilities. For example, let us assume that in the present setting, the required size of the LLM scales linearly with the number of tasks it can solve. Then,\n$\\Theta(m_g) = O(km_s) \\Rightarrow \\Theta(m_gn) = O(km_sn)$.\nThat is, the speed-up from delegation and specialization scales linearly with the number of tasks. As LLM-agent-based systems become larger and larger, many hope that they will be able to tackle thousands of distinct tasks, if not more. So, the advantage of having many specialist agents becomes enormous.\nImportantly, notice that this analysis shows that, for those developing the LLMs themselves, it should be much more impactful to focus on minimizing the size of the specialist LLMs than minimizing the size of the delegator. This is because, even if the delegator is relatively huge, it is called so few times relative to the specialists. In a real-world team of human developers, without such analysis, equal research and development resources might be allocated to both reducing the size of the delegator and reducing the size of specialists. For example, it could be very tempting to try to develop a relatively tiny classifier as the delegator, since it might seem like a relatively simple classification problem. Analysis like the above can preempt research on shrinking the delegator: It would be okay to use a giant off-the-self maximum-capability LLM as the delegator, and focus on shrinking the specialists. This observation supports the more general idea that, in an optimal LLM ecology, we should expect orders of magnitude more executions of smaller models than of larger models (Nisioti et al., 2024).\n3.2. Iterative Code Debugging\nThis section looks at another increasingly common and critical task for LLM-agents: code debugging. At a high level, the problem is that we have a large code-base with many bugs, and we would like to clean it up. For clarity of analysis, suppose the code is $n = kl$ tokens long, consisting of k functions, the implementation of each being l tokens long, and suppose there are b bugs somewhere in the code, with no more than one bug per function. The goal is to provide an efficient LbA that fixes all the bugs.\n3.2.1. NA\u00cfVE MULTI-AGENT APPROACH\nSuppose we take an anthropomorphic multi-agent approach, akin to the kind of approach that has become currently quite popular for pushing the problem-solving performance of coding LLMs (Park et al., 2023; Qian et al., 2024; Wu et al.; Song et al., 2024). In such an approach, we instantiate multiple agents by prompting them with different roles, mapping each on to an employee in a human organization. For simplicity in the present analysis, we will use only two roles: quality assurance (QA) engineer $M_q$ and debugging specialist $M_d$ (we leave it as an exercise to the reader to extend/revise the analysis for other common roles found in multi-agent coding systems).\nIn this approach, we alternate between $M_q$ (of size $m_q$) identifying a bug and $M_d$ (of size $m_d$) fixing it. Suppose $M_q$ has the capacity to identify exactly one bug at a time, and $M_d$ has the capacity to fix exactly one identified bug. In the standard approach, the entire conversation history, including prior versions of the code are fed as input to each agent at each iteration. From an asymptotic perspective, such accumulation in the input may seem clearly inefficient, but this is an approach taken in popular multi-agent frameworks today, such as autogen (Wu et al.). Suppose the QA engineer identifies bugs by producing constant size messages, while the debugging specialist rewrites the entire code every time it fixes a bug (as is also standard in current systems). Then, for the ith bug, the QA agent looks at all i previous versions of the code and previous QA comments, yielding a total input length of $(in)$. So, the total cost of QA over all b bugs is\n$m_q(\\Theta(n) + \\Theta(2n) + . . . + \\Theta(bn)) = \\Theta(m_qb^2n)$.\nMeanwhile, the debugging expert looks at in tokens at each iteration and produces n new tokens of code, yielding\n$m_d(in + (in + 1) + . . . + (in + n)) = \\Theta(m_din^2)$\nat each iteration, and over all b bugs:\n$\\Theta(m_dn^2) + ... \\Theta(m_dbn^2) = \\Theta(m_db^2n^2)$.\nSo, the total cost of QA plus writing the debugged code is\n$\\Theta(m_qb^2n) + \\Theta(m_db^2n^2) =$\n$\\Theta(m_qb^2kl) + (m_db^2k^2l^2)$.\nWe notice immediately that, due to the additional factor of n for the debugging expert compared to QA engineer, similar to the implication highlighted in Section 3.1, it is much more important to focus on minimizing the size of the debugging expert than the QA engineer. It could be fine to use the biggest LLM available for QA, since its relative asymptotic cost is so small, especially when the code grows extremely large. However, the central implication from this section comes not from relative model size but from comparison to a more focused approach to the multi-agent problem decomposition, as is discussed next.\n3.2.2. FOCUSED MULTI-AGENT APPROACH\nLet's suppose all functions in the code have completely correct, precise and constant size specification (e.g., in their docstring). Now, instead of having the QA agent look at the entire code every iteration, suppose it looks only at a single function. Then, it will overall look at the entire code only a single time, resulting in a cost of\n$\\Theta(m_qkl)$.\nWhenever a bug is identified in a function, the debugging expert then fixes the bug by looking at and rewriting only that single function, yielding a total debugging expert cost of\n$\\Theta(m_dl^2b)$,\nand thus the cost for the whole system is\n$\\Theta(m_qkl) + \\Theta(m_dl^2b)$.\n3.2.3. IMPLICATIONS\nSince both the QA engineer and debugging expert in the focused version of the system have smaller jobs and less to keep track of, it would be reasonable to assume that they could be of a smaller size than their na\u00efve counterparts. However, even supposing they are no smaller, we have relative speed-ups of $O(b^2)$ for QA and $O(bk^2)$ for the debugging expert. Since n \u2265 b, and the job of the debugging expert is intuitively more challenging than that of QA (since it has to actually produce correct code), the performance improvement that is likely to dominate the relative cost of the two approaches is that of the debugging expert:\n$\\Theta(bk^2)$.\nThis is a huge speed-up, and highlights the power of breaking down solutions into the minimal possible bite-sized"}, {"title": "3.3. Evolutionary Optimization", "content": "Another area where LLM agents are increasingly being applied is in optimization, where the LLM is used as the engine of variation, i.e., given some existing solutions it is used to generate variations on these solutions that have a chance of being improvements with respect to some evaluation function (Lehman et al., 2023; Meyerson et al., 2024; Bradley et al., 2024; Yang et al., 2024; Romera-Paredes et al., 2024; Lee et al., 2025). Many of these applications have been developed under evolutionary optimization frameworks (B\u00e4ck et al., 1997; Wu et al., 2024). In such scenarios, there is some evaluation, or fitness, function f(x) that we would like to maximize, and the LLM is responsible for generating solutions x \u2208 X. If x is representable by text, then an LLM can be applied for this role, and, in theory, any solution type is representable by text (Meyerson et al., 2024).\nLet us suppose f is of a special form that is common for analysis of evolutionary algorithms, i.e., it is a variant of the ONEMAX function (Doerr, 2020; Witt, 2013). Specifically, suppose each solution x has length n = kl tokens, such that it consists of k blocks each of length l. Suppose each block is either \u201ccorrect\u201d or \u201cincorrect\u201d, and the value of f(x) is the number of correct blocks in x. Then, the maximum value of f(x) is k, in the case that all blocks are correct. Note that in practice there may be many ways for a specific block to be considered correct.\n3.3.1. GLOBAL MUTATION\nSuppose we have an LLM-based mutation agent $M_u$ of size $m_u$, that, when applied to an input string x, alters"}, {"title": "4. Alternative Views", "content": "Now that we have made the case for the adoption of asymptotic analysis with LLM primitives in the understanding and development of scaled LLM-based agentic systems, it is worth reflecting on possible counterarguments to this position. Such an engagement can yield the identification of"}, {"title": "5. Research Directions", "content": "The goal of this paper is to serve as a catalyst for the adoption and development of asymptotic analysis with LLM primitives into the research and development of LLM-based agentic systems. As a result, the discussion has focused on high-level principles and minimal motivating examples to highlight the central advantages of such analysis as clearly as possible, and many critical considerations have been thus far ignored. This section enumerates several such considerations, to serve as a scaffolding for future research.\n5.1. Stochasticity and Error-correction\nFor simplicity, the analysis in Sections 2 and 3 assumed that the constituent LLMs can execute capabilities in their scope reliably, i.e., the scenario where the LLM does not successfully fulfill its role is not considered. Of course, LLMs today do make errors, even surprisingly simple ones (Basmov et al., 2023; Williams & Huckle, 2024; Lehman et al., 2025). A more complete analytical framework will take the likelihood of such errors into account. Initial work has developed core ideas in how error probabilities can compound even in the roll-outs of single LLMs (Dziri et al., 2024); further work is needed to extend such insights to the realm of LLM agents. For example, ideas of error correction, e.g., from information theory (Hamming, 1950; Pless, 2011), could be integrated into agentic systems to reduce the impact of errors at scale, akin to the centrality of error-correction in quantum computing (Lidar & Brun, 2013; Roffe, 2019).\nOne open problem key to making such error correction work is developing LLM agents whose errors are decorrelated. Established theory of randomized algorithms will be an essential resource for this project (Motwani & Raghavan, 1996; Mitzenmacher & Upfal, 2017).\n5.2. Asynchrony and Distributed Algorithms\nSimilarly, the examples in Section 3 did not consider potential (temporal) cost benefits from agents running asynchronously, in a distributed manner, or otherwise in parallel. Such parallelization is extremely natural, especially if the central mechanism for achieving asymptotic improvements is problem decomposition. Similar to the case of randomized algorithms above, existing methodologies for asymptotic analysis in parallel, asynchronous, and distributed systems (Akl, 1989; Baudet, 1978; Santoro, 2006), as well as classical multi-agent systems (Ferber & Weiss, 1999; Van der Hoek & Wooldridge, 2008; Hodjat et al., 1998), will be an invaluable resource, and the incorporation of such aspects will be required to clarify the full scope of the advantages of carefully designed LLM-agent systems.\n5.3. Automatic Decomposition\nA central motivating theme throughout this paper is the expectation that the jobs of large and complex LLM-based agentic systems can be usefully decomposed into subproblems, and some such decompositions lead to asymptotic improvements over others. In the examples in Section 3 we assumed that effective decompositions were available a priori or could be deduced programmatically (i.e., without the use of LLMs). In practice, especially in more open-ended systems where the full spectrum of tasks the system might need to solve is not known beforehand, having automatic decomposition methods will be critical to maintain-ing/maximizing asymptotic performance. Such methods will likely rely on LLMs themselves, in which case their cost must be incorporated into AALPs. There has been some initial work on automatic agent decomposition (Wu et al.; Song et al., 2024), but most is based on intuitive zero-shot approaches; much more asymptotically advantageous approaches should be possible.\n5.4. Extensions to Other Modalities\nThis paper argues that AALPs is required to scale LLM agents, but as agentic systems naturally grow alongside AI models that incorporate a greater and greater range of modalities (Team et al., 2023; Liang et al., 2024), the capabilities afforded by these modalities will naturally be incorporated into agentic systems; this process has already begun (Jiang et al., 2024; Gao et al., 2024; Sarch et al., 2024). The same techniques used for AALPs will naturally extend to other modalities, by encapsulating the usage of each modality as a primitive with an assigned cost. A simple implication could be that many visual tasks (when performed at scale) should be executed by compact vision-only models instead of full-fledged multi-modal foundation models.\n5.5. Online Learning and Adaptation\nThis paper has focused on the case where all LLM training happens beforehand, and thus can be viewed as a fixed cost in comparison to the inference (forward-pass) costs of running an LLM-agent system at scale over a long timeframe. However, for more adaptive systems, it may be essential to allow some form of agent learning during deployment. Such learning could consist of the collection and refinement of memories (represented in text) as the system encounters new scenarios (Park et al., 2023; Wang et al., 2024a), or fine-tuning updates through gradient descent (Hu et al., 2023; Rannen-Triki et al., 2024; Ding et al., 2023; Han et al., 2024). In any case, the computational cost of such learning, dependent on the size of the constituent LLM, will need to be carefully incorporated into AALPs.\n5.6. Ethics and Sentience\nFinally, one side benefit of highly-decomposed modular many-agent LLM systems is a potential massive reduction in future machine suffering. As centralized AI systems become larger and more capable, there are compelling arguments that at some threshold of capabilities sentience has to emerge, at which point the AI system will likely endure astronomical levels of suffering (Metzinger, 2021; Saad & Bradley, 2022; Chiang, 2021). One avenue to minimizing the likelihood of such suffering is to build highly-decomposed many-agent systems that achieve the same positive impact of a centralized system, but, by minimizing the capabilities and scope of each AI, reducing the chance that sentience will emerge. This argument is related to a position paper from last year, arguing that reducing the scope of AI memory will reduce the chance of suffering, since memory is such a critical ingredient for suffering in humans (Tkachenko, 2024). If complemented by further research in how sentience can emerge, AALPs could thus play a role in reducing the chance of immeasurable machine suffering."}, {"title": "6. Conclusion", "content": "This paper has claimed that asymptotic analysis with LLM primitives will be critical to the project of scaling LLM-based agentic systems. The argument for this position was developed through contextualization with existing work, concrete examples of how such analysis can lead to impactful insights, and engagement with alternative views. Several important directions were then highlighted as a guide for future research. Our hope is that this discussion will serve as a catalyst, motivating developers of LLM agents to care-"}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of artificial intelligence. There are many societal consequences that potentially follow from such work. One main advantage of the approach advocated in this paper is the potential reduction in future machine suffering, as discussed in Section 5.6. Another advantage of the fact that AALPs leads naturally to maximally decomposed problem-solving is that decomposed systems can be much more interpretable, auditable, and therefore safer than a single opaque centralized AI carrying out the same task, a point that has been acknowledged in prior work (Khot et al., 2021; Sharkey et al., 2025)."}]}