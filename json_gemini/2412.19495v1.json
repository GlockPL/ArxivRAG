{"title": "Disparate Model Performance and Stability in Machine Learning Clinical Support for Diabetes and Heart Diseases", "authors": ["Ioannis Bilionis", "Ricardo C. Berrios", "Luis Fernandez-Luque, PhD", "Carlos Castillo, PhD"], "abstract": "Machine Learning (ML) algorithms are vital for supporting clinical decision-making in biomedical informatics. However, their predictive performance can vary across demographic groups, often due to the underrepresentation of historically marginalized populations in training datasets. The investigation reveals widespread sex- and age-related inequities in chronic disease datasets and their derived ML models. Thus, a novel analytical framework is introduced, combining systematic arbitrariness with traditional metrics like accuracy and data complexity. The analysis of data from over 25,000 individuals with chronic diseases revealed mild sex-related disparities, favoring predictive accuracy for males, and significant age-related differences, with better accuracy for younger patients. Notably, older patients showed inconsistent predictive accuracy across seven datasets, linked to higher data complexity and lower model performance. This highlights that representativeness in training data alone does not guarantee equitable outcomes, and model arbitrariness must be addressed before deploying models in clinical settings.", "sections": [{"title": "Introduction", "content": "In the realm of biomedicine, Artificial Intelligence (AI) methodologies, particularly Machine Learning (ML) models, are used as clinical support tools to systematically discern patterns and interdependencies among factors and outcomes within large datasets. ML has the potential to enhance healthcare provision by complementing, rather than supplanting, clinical judgment. It has demonstrated efficacy in the detection of skin cancer and diabetic retinopathy, among many other medical conditions [1, 2]. A paramount objective when deploying ML models is the assurance of health equity [3, 4]; thus, researchers and practitioners typically aim at attaining uniform model efficacy across diverse patient demographics [5]. The academic literature recommends an array of analytical tools for detecting biases, e.g., determining statistical dependencies between model outcomes, model errors, and specific subgroups [6], particularly those experiencing both historical and ongoing discrimination. Disparities detected in model performance are frequently attributed to deficiencies within the training datasets, typically lack of sufficient samples from those groups [7].\nAlgorithmic fairness, as a research field, studies how and to which extent algorithmic decision support systems can be free from discriminatory biases [8, 9]. Discrimination, in this context, means systematic disadvantages affecting socially salient groups [10]. These disadvantages arise from a complex combination of design choices made at different points in the construction of an ML processing pipeline. Discriminatory biases have been documented in basically all applications of ML and AI [11], including recruitment [12], machine translation [13] and face recognition [14], just to name a few.\nIn healthcare applications, prior research has identified algorithmic bias as a factor contributing to health disparities, highlighting the need for including Social Determinants of Health (SDoH) in ML to achieve health equity [15, 16]. For instance, in computer vision applications for medical imaging, biased data has been found to be a source of disparities in algorithmic outcomes [17, 18]. Differences in mortality prediction and X-ray diagnosis have been identified across racial/ethnic groups [19, 20], including discrepancies in burn identification and diabetic retinopathy identification in dark-skinned versus lighter-skinned patients [21, 22], and in an opioid misuse classifier, with more errors (false negatives) for dark-skinned patients [23]. In other cases, ML algorithms have predicted similar risk scores in both light- and dark-skinned patients, even though the dark-skinned patients had higher risk [24, 25]. There are many other examples, as this is an active research topic that to some extent is in its early stages [26, 27, 28, 29].\nAn in-depth knowledge of an ML application and of its context should inform this analysis [30]. In healthcare, the generalizability of AI algorithms across subgroups is critically dependent on training datasets, including factors such as representativeness, missing data, and outliers [31]. This suggests that some biases can be traced to datasets that underrepresent certain populations; using these unbalanced datasets as training data yields algorithmic models that"}, {"title": "Methods", "content": "exhibit systematically unbalanced errors [32]. In this context, the augmentation of the dataset with additional samples from the underrepresented group, which frequently corresponds to groups that are socioeconomically disadvantaged or medically underserved, has been empirically demonstrated to mitigate discrepancy in model accuracy. This is the case of the seminal \u201cGender Shades\" study [7, 33]. Similar results have been observed in the training set of a popular face detection benchmark dataset [34].\nDifferences in algorithmic performance are not always due to lack of representativeness. Signs and symptoms of many conditions vary between different populations [35, 36, 37, 38]. Crucially, the features included in a dataset may be more or less useful for predicting different outcomes (e.g., being clinically diagnosed with a condition or not). The analysis of a dataset under this perspective is known as data complexity analysis, and it encompasses multiple aspects. A significant body of research has been dedicated to the formulation of various metrics that encapsulate the multifaceted aspects of dataset complexity [39]. Beyond disparities in model accuracy and data complexity, recent work highlights the importance of variance in model predictions. This variance is related to the extent to which model predictions can \"flip\u201d under minor changes in the training data, and it becomes an aspect of algorithmic fairness when high-variance predictions are concentrated in a demographic subgroup. This is called systematic arbitrariness [40]\nThis paper describes a multifaceted analysis of training datasets pertinent to chronic diseases aimed at uncovering potential discrepancies that could lead to biases in the resulting ML models. Our research substantiates the premise that demographic parity within datasets does not inherently ensure uniformity in algorithmic performance. That is to say, even datasets that are ostensibly equitable in terms of demographic attributes may still yield models with performance discrepancies. Initiating our analysis with a common ML performance metric, the Area Under the Receiving Operating Characteristic curve (AUROC or AUC), we measure the predictive efficacy of the models. Subsequently, our examination extends to more profound dataset attributes impacting model behavior, particularly data complexity and systematic arbitrariness. Our methodology provides a comprehensive approach for the assessment of training data from the perspective of algorithmic fairness. To the best of our knowledge, this investigation is the first to test systematic model arbitrariness in the healthcare domain."}, {"title": "Datasets", "content": "We use a list of datasets identified and reported in a survey of publicly accessible datasets related to chronic diseases [41]. Within this selection, two datasets pertain to diabetes (D1, D2), while five are related to cardiac conditions (D3... D7). Dataset sizes vary widely (see Table 1), and for the purpose of this study, we segmented two of the large datasets into smaller subsets (D2a, D26, D7a, D76) by randomly selecting two samples, each sized 100 times larger than the number of attributes. For the purpose of analysis, sex and age variables are binarized. In the case of age, the individuals within the lowest two quintiles are categorized as \"young\u201d, and those within the highest two quintiles are categorized as \u201cold\u201d, with the median quintile remaining unassigned. Dataset D\u2081 does not include sex. Dataset D7{a,b} was made available in 2020, but the specific year of data collection is not explicitly documented."}, {"title": "Model Performance", "content": "For the evaluation of model performance, we used three gradient boosting algorithms (XGBoost [42], LGBoost [43], HGBoost [44]) that support missing values. We considered two sets of attributes: including the protected attributes (\"aware model\"), and excluding them (\u201cunaware model\u201d). The performance metrics were similar across both models, which means that the datasets contain proxy variables for the protected attributes. Model training was done using a 3-fold cross validation schema, which involves partitioning the dataset into three subsets and cyclically using two-thirds for training and one-third for testing. This evaluation was further complemented by repeated bootstrapping, wherein each iteration involved a novel partitioning of the dataset. Hence, each reported Area Under ROC Curve value (AUROC, or simply ROC) is the average of 66 models: 3 algorithms times 22 runs (19 random runs plus 3 cross-validation runs).\nLearning curves were obtained through an analogous process. We extrapolated learning curves to deduce an estimate of the number of additional data points that would enable the group with lower performance to attain the benchmark set by the group with higher performance. Figure 1a illustrates our method. Let $f(n)$ be the superior learning curve, and $g(n)$ be the inferior learning curve, with $h(n)$ being an extrapolation of the learning curve. Conservatively, if we assume the upper curve reaches a saturation point $f(Np) = AUCp$ (which is not always the case, hence the conservative estimate), we attempt the following minimization:\nmin Nadd\ns.t.\n$h(Np + Nadd) = f(Np)$\ni.e., we calculate the minimal number of additional data points that would be required from the group with the lower AUC to match the AUC of the group with higher performance. We consider three different functions $h\u2081(\u00b7)$, $h\u2082(\u00b7)$, $h\u2083(\u00b7)$, which are linearly constructed based on different segments of the concluding portion of the learning curve, choosing the one that necessitates the smallest increase in data points (i.e., the most conservative scenario, to avoid exaggerating the discrepancy). The greater the value of Nadd, the larger the performance disparity."}, {"title": "Data Complexity Metrics", "content": "Data complexity analysis is a systematic effort to understand discrepancies in classification accuracy by relating them to intrinsic characteristics of a dataset. This is a large research topic, and the interested reader can consult any of various surveys about it [45, 46, 47]. We used a fairly standard categorization of data complexity metrics [39], and picked one popular complexity metric within each category, as shown in Table 2."}, {"title": "Systematic Arbitrariness", "content": "A family of models (e.g., various models built using the same learning scheme but different portions of the training data) may exhibit arbitrariness. Model arbitrariness corresponds to discrepancies in the predicted label for some elements across models of the same family, and it tends to be systematic, i.e., concentrated on specific items.\nA recent study introduces a metric of Self-Consistency (SC) [40], which is computed at the level of an item as the probability that two models of the same family agree on the label for an item. For instance, an item with self-consistency of 1.0 is an item for which any model of a family yields the same predicted label. In binary classification, the minimum self-consistency is 0.5, indicating that half of the models yield one predicted label, and half of the models yield the opposite label. Note that self-consistency is independent of the \u201ctrue\u201d label of an element.\nTo compare self-consistency scores between groups, as recommended in [40] we use the Cumulative Distribution Function (CDF) of self-consistency. Often, one curve is above another, similarly to what we see in Figure 1b. We measure the disparity by performing a statistical test of the difference between the two curves."}, {"title": "Results", "content": "Variations in model performance\nUsing datasets collected in prior research, which include patient demographic details such as sex and age [41], our approach leverages three distinct gradient boosting algorithms to infer ML models from the training data. The validation methodology used herein incorporates cross- validation complemented by iterative bootstrapping, thereby generating a multitude of models each informed by different subsets of the training data. By examining the Area Under the ROC Curve (AUC), we determine the model's proficiency in distinguishing between patient cohorts with different clinical outcomes. Table 3 presents a disaggregated view of AUC discrepancies across sex and age demographics. These results account for models that incorporate age and sex as predictive attributes (\"aware modeling\"). Similar results are observed when these variables are omitted (\u201cunaware modeling\u201d).\nIn our analysis, we observe disparities between sexes within several models, and across age groups in all but one model. Regarding sex-based disparities, approximately 10% of validation results reveal a higher AUC for males compared to females, wheras a mere 1% of results show higher female AUC relative to male AUC. Regarding age-related variances, these disparities exceed the sex-related ones with 32% of validation results demonstrating that the AUC for younger patients exceeds that of older patients, and conversely, in 5% of the cases, the AUC is greater for older patients compared to the younger patients.\nLearning curves and the expected impact of additional data\nLearning curves are a standard tool for monitoring changes in model performance with the incremental addition of training data points. These curves graphically depict a performance metric, such as AUC, against the volume of training data utilized to build the model. Through this visual representation, one can appreciate trends like the"}, {"title": "Alignment of data complexity with some disparities", "content": "We considered sixteen complexity metrics grouped into five categories, each corresponding to a unique conceptual framework, and computed the disparity of each metric between protected subgroups regarding sex and age. For each data set, AUC disparity divided by complexity metric disparity creates a ratio reflecting the consistency between model performance and data complexity as follows (where CM: Complexity Metric and A,B: Sub-groups A and B, i.e. Female-Male and Old-Young):\n$ \\frac{AUCA - AUC B}{CMB - CMA} \\begin{cases} 1 \\text{ (Consistency)} & \\text{if } x > 0 \\\\ -1 \\text{ (Inconsistency)} & \\text{if } x \\leq 0 \\end{cases}$\nFigure 2 presents the results in a heatmap visualization highlighting with light color the cases in which higher complexity and lower AUC values are observed for a specific sub-group in comparison with the other, while dark color indicates inconsistent AUC and complexity patterns.\nNo obvious patterns can be observed across data in the results. Indeed, there are some situations of complementarity in which complexity metrics that are well aligned with AUC in some datasets are not aligned with AUC for another dataset and vice versa. Nevertheless, more complex data could potentially be linked to lower model performance, as homogeneous behavior is observed for some categories of metrics (especially in feature, dimensionality and class imbalance) within datasets regarding age. In addition, several sets of databases (e.g. those related to diabetes D1, D2a, D26) show consistent disparities between performance and i) feature-based and ii) dimensionality complexity metrics. However, these experiments suggest that complexity metrics cannot be relied upon as a predictor of AUC disparities in specific clinical conditions."}, {"title": "Systematic arbitrariness and model stability", "content": "In analyzing a family of models, each trained on distinct yet equivalently-sized partitions of the training data, we define an individual's self-consistency as the probability that two models within this family will yield the same label [40]. In our case, the minimum self-consistency is attained by subjects for which half of the models predict that they will be diagnosed with a condition, while the other half predict that they will not. Evidently, this is a situation we would like to avoid as much as possible. Hence, all other things equal, a model with higher self-consistency for most items is preferable.\nSystematic arbitrariness is observed when items with low self-consistency are concentrated within a particular group, and can be measured by comparing CDFs. To quantify disparities, we use the Kolmogorov-Smirnov (KS) statistical test (Figure 1b), with the results shown in Table 5."}, {"title": "Discussion", "content": "Our findings uncover sex- and age-related disparities in model performance as evidenced by the AUC of the models. It is pertinent to recall that the representation of each group within the datasets is equal. This observation underscores that mere demographic parity in training datasets does not mean model equity. The analysis of learning curves provides insights into the potential benefits of data augmentation. Sex-related disparities are observed to occasionally favor males over females, with a marginal predominance for male patients as indicated by both AUC and the requisite additional data to attain performance parity. Regarding age differences, the findings are more pronounced, with models generally predicting better for younger patients across most datasets (higher AUC), and requiring a large volume of additional training data to potentially achieve performance parity.\nFurthermore, upon examining disparities in data complexity and systematic arbitrariness, we observe that predictions for older patients tend to be less consistent than those for their younger counterparts in several datasets. These disparities, to some extent, correlate with the model performance (AUC) and data complexity findings, suggesting a linkage between increased data arbitrariness for older patients and heightened complexity, leading to lower model performance. These correlations suggest but do not determine model disparities, as there are exceptions within our observations, where greater arbitrariness is sometimes associated with comparable or superior AUC values. This highlights the necessity of a multifaceted metric consideration encompassing performance, complexity, and stability, rather than relying exclusively on performance metrics.\nWithin the healthcare domain, the legal and ethical dimensions of decision-making are of paramount importance [48]. The findings of this study highlight some characteristics of model performance that are not typically reported, but that hold considerable potential to influence clinical practice. Specifically, systematic arbitrariness in model outputs could undermine clinician confidence in ML and diminish the acceptability of such models. We propose datasets are tested for systematic arbitrariness before being used in clinical settings. In nearly half of the datasets we studied, older patients with chronic diseases face the risk of health inequities [49, 50] due to data that is suboptimal for modeling their health outcomes as compared to younger patients.\nHospital data, such as the one used in this study, may be indicative solely of the population with healthcare system access, thus potentially engendering bias against certain subpopulations [51, 52, 53, 54]. Future efforts should aim to extend these analyses to include additional databases. To address situations where systematic arbitrariness is detected, we must consider both technical and human factors [55]. This includes designing systems that minimize potential technology- induced disparities, taking into account the data and algorithmic literacy of the users of these systems, i.e., clinicians. Arbitrariness is not a new concept in the health domain, as evidenced by the existence of cost-effective pharmacological treatments that exhibit suboptimal efficacy in particular patient subgroups. Systems are in place"}, {"title": "Conclusion", "content": "to educate and safeguard against potential patient harm, including rigorous and multiphase pharmaceutical clinical studies and pharmacovigilance protocols. Data quality audits should scrutinize performance differentials impacting specific subgroups, whose data characteristics may differ from other populations with the same condition. Future research could explore the creation of monitoring processes for ML models in healthcare, analogous to those applied to pharmacological drugs.\nIn this study, we identified significant age-related and mild sex-related disparities in the performance of ML models for chronic disease prediction. Older patients, in particular, experienced inconsistent and arbitrary predictions across several datasets due to increased data complexity and lower model performance, while sex-based differences slightly favored male predictions. These findings demonstrate that representativeness in training data alone is insufficient for ensuring equitable outcomes. Therefore, addressing model arbitrariness, especially for older individuals, is essential before deploying ML models in clinical settings to ensure fairness and reliability."}]}