{"title": "Improving AlphaFLOW for Efficient Protein Ensembles Generation", "authors": ["Shaoning Li", "Mingyu Li", "Yusong Wang", "Xinheng He", "Nanning Zheng", "Jian Zhang", "Pheng Ann Heng"], "abstract": "Investigating conformational landscapes of proteins is a crucial way to understand their biological functions and properties. AlphaFLOW stands out as a sequence-conditioned generative model that introduces flexibility into structure prediction models by fine-tuning AlphaFold under the flow-matching framework. Despite the advantages of efficient sampling afforded by flow-matching, AlphaFLOW still requires multiple runs of AlphaFold to finally generate one single conformation. Due to the heavy consumption of AlphaFold, its applicability is limited in sampling larger set of protein ensembles or the longer chains within a constrained timeframe. In this work, we propose a feature-conditioned generative model called AlphaFLOW-Lit to realize efficient protein ensembles generation. In contrast to the full fine-tuning on the entire structure, we focus solely on the light-weight structure module to reconstruct the conformation. AlphaFLOW-Lit performs on-par with AlphaFLOW and surpasses its distilled version without pretraining, all while achieving a significant sampling acceleration of around 47 times. The advancement in efficiency showcases the potential of AlphaFLOW-Lit in enabling faster and more scalable generation of protein ensembles.", "sections": [{"title": "1. Introduction", "content": "Exploring conformational landscapes is essential to capture the dynamic nature of protein structures, offering insights into their flexibility, biological function, and interactions. Traditionally, ensembles of conformational changes are collected through molecular dynamics (MD) simulations (Karplus & McCammon, 2002). While MD-base methods adhere to physical laws and will theoretically explore the entire landscape, they are time- and resource-intensive. To expedite this process, some methods focus on increasing the diversity of AlphaFold (Jumper et al., 2021), which is a powerful deep learning model for crystal structure prediction but falls short in accounting for conformational divergence. Specifically, these methods sample different multiple sequence alignments (MSAs) as input (Wayment-Steele et al., 2024) or enable the dropout function in AlphaFold (Wallner, 2023) during the inference process. Despite these inference interventions indeed bring some diversity to AlphaFold, they fall significantly short of generating enough conformational heterogeneity to thoroughly explore the protein landscape.\nRecently, (Jing et al., 2024) harnessed the power of generative methods flow matching, and integrated this framework into AlphaFold, called AlphaFLOW. To be concrete, it treats AlphaFold as a powerful sequence-conditioned denoising model, which receives the noisy structures as templates and samples the protein ensembles from harmonic prior under a flow field. AlphaFLOW inherits the weights of AlphaFold, and was trained on general PDB then fine-tuned on different protein MD trajectories as a regression model, using loss functions similar to those in the original AlphaFold. Due to these enhancements, AlphaFLOW is much more flexible and diverse than the aforementioned inference intervention methods. It is the first method to ingeniously combine both the advantages of accurate structure prediction and the generative capability for conformation sampling.\nHowever, limitations persist in sampling consumption. As shown in Fig. 1, since AlphaFLOW is trained by fully fine-tuning AlphaFold, generating the final structure $x_0$ requires $T$ denoising steps, which means running $T$ times AlphaFold with additional embedders. Although flow matching method is relatively faster compared with other diffusion methods, as shown in Fig. 2(A), AlphaFLOW showcases cubic growth with the chain length, which leads to unacceptable time consumption and hinders its application for generating larger set of protein ensembles. While AlphaFLOW adopts diffusion distillation to reduce the generative process to a single forward pass, this approach compromises the level of the sampling performance."}, {"title": "2. Preliminary", "content": "In this section, we briefly introduce the flow matching framework and some details of AlphaFLOW.\nFlow matching The flow matching framework begins with the continuous normalizing flow (CNF) $V_t$, defined as the solution of an ordinary differential equation (ODE) governed by a time-dependent vector field $u_t : \\dot{V_t} = u_t(V_t)$. Let $x$ be a data point on a specific manifold, the CNF has an initial condition $\\psi_0(x) = x$. Given two distributions $p_0$ and $p_1$, we can define a probability path $p_t$ as their interpolation, which can be viewed as paths generated by $u_t$. To effectively learn the CNF, we make $u_t$ tractable and by adopting the conditional probability path $p_t(x | x_1)$, which samples $x_0$ from prior distribution $p_0$ and interpolating it linearly with the data point $x_1$:\n$x_t = (1 - t)x_0 + t \\cdot x_1$ (1)\nwith the corresponding vector field:\n$u_t(x_t | x_1) = (x_1 - x_t) / (1-t)$ (2)\nThis method is referred as conditional flow matching (CFM). We employ a neural network $v_\\theta$ to learn the vector field. The objective of CFM can be written as:\n$L_{CFM} = E_{t, p(x_0), p(x_1)} || v_\\theta (x_t) - u_t(x_t | x_1) ||^2$ (3)"}, {"title": "3. Method", "content": "AlphaFLOW To integrate the AlphaFold, which is a regression model that directly outputs $x_1$, into the flow matching framework, AlphaFLOW reparameterizes the neural vector field as:\n$v_t(x_t) = (AlphaFold(x_t) - x_t) / (1-t)$ (4)\nIt allows the objective to be rewritten as learning the expectation of $x_1$. Consequently, AlphaFLOW can employ the similar regression loss function (e.g. FAPE) to optimize the neural network. AlphaFLOW introduces two key innovations: (a) It employs the 3D coordinates of its $\\beta$-carbons ($\\alpha$-carbon for glycine) to describe the noisy structure and the prior distribution is defined over the $\\beta$-carbons coordinates as a harmonic prior (Jing et al., 2023); (b) AlphaFLOW treats $x_t$ as features (similar to templates), and the denoising process does not directly apply to the spatial domain as in prevailing SE(3) generative models (Yim et al., 2023; Bose et al., 2023; Li et al., 2024). Instead, it starts from the identity rigids, which is the same as AlphaFold. These contributions make AlphaFLOW as a new paradigm for utilizing AlphaFold within different frameworks or applications.\nAlphaFLOW-Lit follows the same ideas of AlphaFLOW but introduces some modifications to the input pipeline. AlphaFLOW-Lit is a feature-conditioned generative model, that is to say, it is conditioned on the single and pair features after the Evoformer blocks to generate diverse conformations. As illustrated in Fig. 1, the AlphaFold embedders (including the original input embedder, recycling embedder, extra MSA embedder, and extra MSA stack) and Evoformer are kept frozen. The input embedding module for noisy structures $x_t$ and timesteps is similar to that of AlphaFLOW but with little modifications. The single and pair output of input embedding module are derived from the torsion angles (if designated) and contact map of the noisy structure, respectively. These outputs are followed by a Linear layer initialized with zeros before summation with the features after Evoformer blocks. This is similar to the zero convolution in ControlNet (Zhang et al., 2023), designed to minimally disrupt the pretrained weights at the outset. The detailed algorithm for input embedding module is described in Appendix A Algorithm 2. It is worth noting that the torsion angles in AlphaFold are represented in 8 rigids groups with sin-cos formats, indicating rotation towards the coordinates of the former group. As a result, these angles are invariant to rigid transformations, eliminating the need to rotate the predicted structure after RMSD alignment. The training procedure for AlphaFLOW-Lit is the same as for AlphaFLOW, and the inference procedure is provided in Algorithm 1. We keep the Algorithm notations same as (Jing et al., 2024). The acceleration primarily results from the pre-computation of single and pair features. In contrast to"}, {"title": "4. Experiments", "content": "We directly train AlphaFLOW-Lit on ALTAS MD trajectories (Vander Meersche et al., 2024) without pretraining on the PDB. Similar to AlphaFLOW, we use 1265/39/82 ensembles for the training, validation, and test splits, respectively. All multiple sequence alignments (MSAs) are derived from OpenProteinSet (Ahdritz et al., 2024). For sequences not present in OpenProteinSet, we use MMseqs2 (Steinegger & S\u00f6ding, 2017) to search the UniRef30 and ColabDB databases (Mirdita et al., 2022). The initial weight of AlphaFLOW-Lit is from the AlphaFold's publicly available weights. ALTAS provides three parallel trajectories with 10,001 frames for each protein. We subsample the trajectories with a stride of 100 frames to create the training set (300 frames in total). During training, we uniformly sample one frame at each step. Since the Evoformer blocks are frozen, we set the weight of masked MSA loss to 0.\nWe generate 250 samples for 82 targets in the test set. AlphaFLOW-Full refers to AlphaFLOW with 10 consecutive denoising steps. AlphaFLOW-Distilled denotes its distilled with a single forward denoising step. AlphaFLOW-Lit employs the full denoising steps. The protein ensembles of AlphaFLOW-Full and AlphaFLOW-Distilled are downloaded from its public repository. We assess the sampling runtime based on the protein length for these methods and approximate their consumption curve. To evaluate the effectiveness of each method, we first investigate the protein dynamics, considering both the general dynamics indicated by the pairwise root-mean-square deviation (RMSD) and the essential dynamics uncovered through principal components analysis (PCA) (Amadei et al., 1993). Furthermore, we assess the detailed capability of each method at the residue resolution of protein dynamics by systematic comparisons of local arrangements within residues and motional correlations among residues.\nRuntime comparison In Figure 2, we depict the relation between runtime of sampling and sequence length ranging from 100AA to 1,000AA in increments of 100. AlphaFLOW-Lit demonstrate superior scalability, maintaining consistently low runtime across increasing protein lengths. AlphaFLOW exhibit cubic growth in runtime, indicating its inefficiency for longer chains. This inefficiency could be attributed to the cubic complexity of attention in the Evoformer block. While AlphaFLOW-Distilled performs better than AlphaFLOW-Full due to its single forward inference, it still shows moderate increases in runtime as protein length grows. In summary, AlphaFLOW-Lit surpasses AlphaFLOW by 6 to 51 times (47 times in average) and AlphaFLOW-Distilled by 2 to 4 times (3.8 times in average), making it the most efficient configuration and highlighting its potential for generating a larger set of protein ensembles.\nProtein dynamics analysis For each conformational ensemble, the general dynamics are quantified as the average $C_\\alpha$-RMSD between any pair of conformations. Using this measurement, the AlphaFLOW-Lit ensembles demonstrate the strongest Pearson correlation with the ground truth ensembles produced by classic MD, while maintaining a comparable level of diversity in the conformational ensembles relative to AlphaFLOW-Full. In contrast, AlphaFLOW-Distilled loosely match the general dynamics with the ground truth and does not achieve the same level of diversity. Also, we assess the essential dynamics of proteins by projecting the ensembles onto the first two principal components (PCs) derived from PCA. Two common featurization methods for proteins are utilized: aligned $C_\\alpha$ absolute coordinates and pairwise $C_\\alpha$ internal distances. The differences in the distributions are quantified using the mean Jensen-Shannon divergence (JSD) for each PC"}, {"title": "5. Conclusion", "content": "We propose AlphaFLOW-Lit, an improved version of AlphaFLOW for efficient protein ensembles generation. Compared with AlphaFLOW, AlphaFLOW-Lit is a feature-conditioned generative model that eliminates the heavy reliance on MSAs encoding blocks and utilizes computed features to produce a diverse range of conformations. By directly training on ATLAS, AlphaFLOW-Lit performs on-par with AlphaFLOW while outperforming its distilled version, all while achieving a substantial acceleration in sampling speed of around 47 times. In addition, we conduct a thorough analysis of protein dynamics, local arrangements, and long-range coupling within the generated ensembles. The advantages of AlphaFLOW-Lit make it capable of generating a larger set of protein ensembles, enabling us to more effectively explore the protein landscape using deep learning techniques."}, {"title": "Limitation and future work", "content": "As illustrated in the Experiment section, AlphaFLOW-Lit exhibits less diversity compared to AlphaFLOW-Full, likely due to the absence of pretraining on the PDB or insufficient training on MD trajectories. This will be addressed in future work. Additionally, in the PCA analysis of example 6q9c_A, both models fail to capture the additional conformation present in the ground truth MD distribution. Enhancing their capability to capture such nuances will be a focus of our future research."}, {"title": "A. Method Details.", "content": "We highlight the difference with AlphaFLOW in yellow. init = 'final' indicates the weight and bias (if has) of Linear is initialized with 0. Other notations are kept same as (Jumper et al., 2021) and (Jing et al., 2024)."}]}