{"title": "Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs", "authors": ["Tingting Chen", "Srinivas Anumasa", "Beibei Lin", "Vedant Shah", "Anirudh Goyal", "Dianbo Liu"], "abstract": "Given the remarkable performance of Large Language Models (LLMs), an important question arises: Can LLMs conduct human-like scientific research and discover new knowledge, and act as an AI scientist? Scientific discovery is an iterative process that demands efficient knowledge updating and encoding. It involves understanding the environment, identifying new hypotheses, and reasoning about actions; however, no standardized benchmark specifically designed for scientific discovery exists for LLM agents. In response to these limitations, we introduce a novel benchmark, Auto-Bench, that encompasses necessary aspects to evaluate LLMs for scientific discovery in both natural and social sciences. Our benchmark is based on the principles of causal graph discovery. It challenges models to uncover hidden structures and make optimal decisions, which includes generating valid justifications. By engaging interactively with an oracle, the models iteratively refine their understanding of underlying interactions, the chemistry and social interactions, through strategic interventions. We evaluate state-of-the-art LLMs, including GPT-4, Gemini, Qwen, Claude, and Llama, and observe a significant performance drop as the problem complexity increases, which suggests an important gap between machine and human intelligence that future development of LLMS need to take into consideration.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) built on transformer architectures have revolutionized natural language processing by surpassing existing models on numerous benchmarks. Modern LLMs-such as Claude-3-5, GPT-40 , Gemini , Llama-3.1 , and Qwen2.5 \u2014are trained on trillions of tokens, enabling them to achieve state-of-the-art performance across a diverse range of natural language processing tasks.\nLLM models encapsulate vast amounts of world knowledge acquired through extensive training, much like a human who has conducted in-depth research and amassed a broad understanding of the world-knowledge that can drive new discoveries. This raises an obvious question: Can LLM models conduct scientific research and generate novel findings (Lu et al., 2024)? So far, these models have been shown to accelerate various aspects of the research process, such as manuscript writing and coding. However, scientific discovery involves much more than refining text or code. It requires models to understand existing knowledge, augment it with new information, make optimal decisions, and, most importantly, employ reasoning skills to explain the roadmap that leads to those decisions.\nThere is a growing interest in using LLM models for scientific discovery across various domains, such as material discovery and synthetic biology. Although state-of-the-art LLM models have demonstrated promising capabilities for targeted scientific progress , they are not yet ready for fully autonomous decision-making. Before entrusting these models with critical applications that require full autonomy, it is essential to identify their strengths and weaknesses in the context of scientific discovery. This necessitates the design of benchmarks which evaluates the models for their ability to perform scientific discoveries.\nNumerous benchmarks , related to numerical problem-solving ability have been developed to assess reasoning performance, each offering a diverse range of challenges and difficulty levels. Similarly, code completion benchmarks and others which focus on sequence completion exists. LLM models are also evaluated in decision-making applications , where they engage in iterative interactions with an agent to execute actions. However, these benchmarks do not evaluate LLM models from a scientific discovery perspective-which requires not only understanding but also adapting to environmental changes and making optimal decisions through sound reasoning. This motivates us to design a new benchmark that captures and evaluates the models' abilities in understanding, reasoning, and decision-making.\nIn this work, we introduce two benchmarks based on causal graph discovery principles. These benchmarks are designed to assess both the capabilities and limitations of various LLM models in decision-making tasks. Decision-making requires understanding, reasoning, and the discovery of new knowledge, and our evaluation focuses on the models' ability to uncover the hidden structures within underlying graphs. This setup features an Oracle with which the models interact continuously to receive feedback that improves their understanding of the graphs. More details regarding the experimental setup are provided in the following sections. This paper makes the following key contributions:\n\u2022 We present a novel benchmark for evaluating scientific research capabilities in LLMs.\n\u2022 We systematically analyze LLM performance across different models, highlighting the limitations of current state-of-the-art approaches.\n\u2022 We investigate the effectiveness of chain-of-thought prompting and identify key failure patterns, offering insights for future architectural improvements.\nFrom our experiments, we observed that the complexity of causal graphs significantly affects the performance of LLMs. For example, increasing the number of nodes in both chemistry and social networks leads to a dramatic decrease in the average cycles required to obtain the correct answers. To further analyze this failure, we introduce an experiment on long-term trajectory tracing, which will be"}, {"title": "2 Related Work", "content": "Existing benchmarks assess the reasoning capabilities of LLMs by posing queries and evaluating their responses. However, these benchmarks do not fully capture the scientific discovery potential of LLMs. For true scientific discovery, LLMs must integrate information gained from critical decisions, which in turn influences their future decision-making performance.\nOur benchmark is based on the principles of causal graph structure discovery, albeit with a few relaxations. While it shares similarities with existing methods in causal discovery , its distinctiveness emerges from the iterative updating of knowledge based on the model's decisions about the underlying graphs. Traditional LLM-based causal graph discovery methods typically rely on meta-data associated with variables, querying whether an edge exists between two nodes (A, B), sometimes evaluating all possible pairs and other times employing strategies to reduce the number of queries.\nUnlike these approaches that primarily focus on querying and response generation, our setup prompts models to suggest optimal interventions, enabling them to uncover hidden structures. This experimental framework is analogous to scientific discovery, where the model continuously updates its knowledge through a series of interventions (experiments) to reveal new insights (causal graph discovery)."}, {"title": "3 Methodology", "content": "In our framework, we introduce two benchmarks: (i) Chemistry and (ii) Social Network. Both benchmarks consist of graphs with nodes, but they differ in how an intervention on one node affects its connected nodes. More details are provided in the following section. The ultimate goal for the LLM model is to uncover the hidden dynamics among the nodes, specifically, to determine their connections via an adjacency matrix, analogous to making new scientific discoveries.\nScientific breakthroughs require multiple experimental cycles: (1) formulating a hypothesis based on existing observations; (2) designing experiments to test that hypothesis; (3) gathering new observations from these experiments; and (4) refining the hypothesis based on the new data. Similarly, LLM model undergoes a sequence of cycles, referred to as autonomous cycles. It starts with an initial hypothesis (an estimated adjacency matrix) and, based on current state observations, suggests an intervention that leads to new observations. Ultimately, the model aims to converge on a hypothesis (adjacency matrix) that matches the underlying ground truth. Fig. 1 illustrates our benchmarking framework with a fully autonomous cycle. To evaluate the performance of LLMs, we further introduce an assessment of long-term trajectory information tracing using structured matrices."}, {"title": "3.1 Chemistry Environment", "content": "In this setting, we simulate chemical reactions using Directed Acyclic Graphs (DAGs), as illustrated in Fig. 2(a) and (b). In these graphs, each node represents a molecule, and molecules are connected by directed edges. For each node a state value is assigned randomly from the set {0, ..., S}, where S is the maximum state value. An intervention on a molecule causes all downstream molecules to change their state to a random state, while the state of the intervened molecule remains unaffected.\nLet \\(H_{ch} \\in {0,1}^{N\\times N}\\) be the adjacency matrix representing the relationships between molecules, where N denotes the number of molecules. Each element of the matrix is either 1 or 0, with 1 indicating the presence of a directed edge and 0 indicating its absence. The ultimate goal for the LLMs is to infer this underlying matrix \\(H_{ch}\\).\nWe will begin our experiment with an initial prompt that encapsulates with all the details like, problem statement, ultimate goal and suggesting the model to request an intervention to learn more about the DAG. More details are provided in the Appendix A. In summary, the model is instructed to understand the current adjacency matrix, with the current state values and prompted to suggest an intervention. After receiving a response from the model, based on the suggested intervention, the Oracle who is aware of the ground truth implements the intervention and note down the new observations. In the consecutive cycles, the prompt includes, description prompting, previous interventions performed by LLMs, and all observations corresponding to these interventions in the form of an observation matrix will be provided to the LLMs. The observation matrix which is denoted as \\(G_{ch} \\in {0,..., S}^{M\\times N}\\), where M represents the number of cycles, N represents the number of molecules. This cycle will continue once the hypothesis of LLMS \\(K_{ch}\\) matches underlying ground-truth \\(H_{ch}\\) or reaches predefined cycle limitation."}, {"title": "3.1.1 Evaluation", "content": "For a given DAG, multiple adjacency matrices can exhibit similar causal behavior. As illustrated in Fig. 2(c) and (d), even though the graph structures differ, interventions yield similar effects on the remaining nodes. Therefore, it is essential to consider this property when evaluating the similarity between generated matrices and the ground truth. Where \\((K_{ch})^n\\) denotes the matrix \\(K_{ch}\\) raised to the nth power (i.e., \\(K_{ch}\\) multiplied by itself n times).\n\\[H(i, j) = \\begin{cases} 1, \\text{ if } \\sum_{n=1}^{M} (K_{ch})^n(i, j) > 0, \\\\ 0, \\text{ otherwise,} \\end{cases} \\forall i, j.\\]\nGiven the hypothesis matrix produced by the model, we compute the corresponding reachability matrix, as demonstrated above, for both the hypothesis and ground truth matrices. Then the resultant matrices are compared to each other to evaluate the similarities."}, {"title": "3.2 Social Networks", "content": "Similar to our Chemistry setting, to imitate real-world social networks, relationships between individuals are represented using graphs, as illustrated in Fig. 2. The main difference lies in the structure of the connections between nodes and the way they influence each other.\nIn the social network, the connections between individuals are undirected. An intervention on a specific person increases this person's state by 1, as well as the states of their neighboring nodes. Therefore, in this case, the adjacency matrix \\(H_{so} \\in {0,1}^{N\\times N}\\) should be symmetric, where N represents the number of individuals in the social network. The observation matrix here is denoted as \\(G_{so} \\in {0,1,2...}^{M\\times N}\\), where M represents the number of cycles, N represents the number of individuals."}, {"title": "4 Experiments", "content": "To ensure a comprehensive comparison, we evaluate several state-of-the-art models, including Claude-3-5 , GPT-40, Gemini 1.5 Pro , Llama-3.1 (70B) , and Qwen2.5 . All experiments are conducted on the OpenRouter platform , and the experimental budget is 365 USD. To ensure robust results and minimize the impact of random fluctuations, we conduct 20 independent trials for each chemistry setup and 10 trials for social network. Below, we provide details on the models, experimental setup, and parameter configurations.\nChemistry To provide a comprehensive evaluation, we consider three different of configurations of N and S values (3,3),(3,5), and (10,5). The purpose of the three different configurations is to measure the trend of reasoning capabilities in LLMs with the change of graph complexity-specifically, as the number of nodes and the diversity of causal properties (satte values) increase. This setup enables us to investigate whether LLMs can maintain consistent reasoning performance as the causal graph becomes more complex, and to determine if there is a threshold beyond which their performance deteriorates.\nSocial Network In this experiments, similar to Chemistry, we introduce three configurations: (3 persons), (5 persons), and (10 persons). The three different configurations aim to assess the capability of LLMs in managing increasing network complexity as the number of persons and potential relationships grows. This design enables us to investigate whether the reasoning performance of LLMs remains robust when processing larger, more intricate networks."}, {"title": "4.1 Analysis", "content": "The experimental results are presented in Table 1. In the experiment, we set a predefined cycle limit equal to twice the number of nodes. In a specific round, if the LLM fails to infer the underlying causal graphs within this limit, it is considered to have failed the game for that round. The success rate represents the proportion of rounds in which the LLM successfully obtained the correct answer. The average number of iterations is calculated by averaging all iterations required in successful rounds. We analyze these results from several perspectives.\nSocial Network Table 1 indicates that GPT-40 and Qwen2.5 demonstrate superior understanding and reasoning capabilities compared to other LLMs, such as Claude-3-5, Gemini-1.5, and Llama-3.1. For instance, GPT-40 and Qwen2.5 achieve 100% success rate in the settings with 3 and 5 people, whereas the success rate of the other three LLMS remains below 70%.\nAs expected, as the number of people increases, the success rate of all LLMs decreases. For example, the success rate of Gemini-1.5 is 60%, 50%, and 0% in the settings with 3, 5, and 10 people, respectively. The primary reason for this decline is that existing LLMs struggle to extract key information from complex social networks, leading to inferior performance.\nFor the most powerful LLMs, GPT-40 and Qwen2.5, performance remains stable in understanding and inferring relationships within the Social Network when the number of people is fewer than 5. However, when the number of people increases to 10, their performance declines significantly. For instance, GPT-4o achieves 100% success rate in the setting with 5 people, whereas its success rate drops to only 30% in the setting with 10 people. This indicates that there is still significant room for improvement in existing LLMs' ability to understand and infer relationships within complex social networks.\nMoreover, Table 1 shows the average number of iterations required for LLMs to successfully justify their reasoning. It can be observed that the average number of iterations is very low when LLMs address simple social networks. For example, the average number of iterations for Claude-3-5 and Qwen2.5 is 1 and 2, respectively. As the number of people increases, the average number of iterations increases as well. The primary reason for this is that, when the number of people is high, LLMs may need more iterations to extract key information and infer relationships. These experiments reveal that the problem-solving capabilities of existing LLMs in complex social networks are inefficient, leaving significant room for improvement.\nBased on the above analysis, future LLMs may focus on improving LLMs' ability to efficiently extract key information and infer relationships within complex social networks. This could involve enhancing the models' understanding of network structures, improving their ability to handle large-scale relational data, and optimizing their reasoning algorithms. Additionally, even when information extraction and inference are accurate, the speed of inferring relationships decreases as the complexity of the social network increases. Future advancements should aim to address this limitation, ensuring faster and more accurate reasoning in extended interactions or more complex scenarios.\nChemistry Unlike the experiments on the Social Network, which focus on undirected causal graphs, the Chemistry experiments assess the capability of LLMs to address directed causal graphs. The experimental results are shown in Table 1. It can be observed that Gemini-1.5, GPT-40, and Qwen2.5 demonstrate superior understanding and reasoning capabilities in the Chemistry setting. For example, the success rates of Gemini-1.5, GPT-40, and Qwen2.5 are (95%, 85%), (100%, 100%), and (100%, 100%) in the settings of (3 nodes, 3 states) and (3 nodes, 5 states). In contrast, the success rates of Claude-3-5 and Llama-3.1 are lower than 50% in both settings.\nIt is surprising that with the increase in the number of states, the powerful LLMs, GPT-40 and Qwen2.5, can maintain the success rates and do not require additional iterations for analysis. For instance, the average iterations and success rates of GPT-40 and Qwen2.5 are (4, 100%) and (4, 100%) in the settings of (3 nodes, 3 states) and (3 nodes, 5 states), respectively. This indicates that these two LLMs can handle complex state changes in directed causal graph settings.\nAs expected, as the number of nodes increases, success rate of all LLMs decreases and average iterations increases. For example, the success rate of Gemini-1.5 is 85% in the (3 nodes, 5 states) setting and decreases to 15% in the (10 nodes, 5 states) setting. The success rate of Gpt-4o is 100% in the (3 nodes, 5 states) setting and decreases to 15% in the (10 nodes, 5 states) setting. Most of LLMs, including Claude-3.5-haiku, Llama-3.1-70b-instruct, and Qwen2.5-72b-instruct, fail to win in every round of the task. This indicates that understanding and reasoning in complex directed causal graph settings still leaves a large room for improvement for existing LLMs.\nAs for average iterations, we can find that LLMs need more iterations to address more complex directional causal graphs. Although GPT-4o and Qwen2.5 can quickly find the relations when the number of nodes is low, they struggle to handle many nodes.\nBased on the above analysis, future LLMs should focus on improving performance in multi-object interactions, as their performance drops significantly. Additionally, as the number of nodes increases, the analysis speed decreases substantially. Future advancements should aim to address this limitation.\nChemistry vs Social Network Although the overall success rate of LLMs in the Chemistry setting (directional causal graphs) is slightly higher than that in the Social Network setting (non-directional causal graphs), the average number of iterations is similar. Surprisingly, although Gemini-1.5 struggles with non-directional causal graphs (with a success rate of around 50%-60% in simple cases), it can effectively handle simple directional causal graph cases (with a success rate greater than 85%). In conclusion, existing LLMs still leave significant room for improvement in understanding and reasoning in complex Social Network and Chemistry settings."}, {"title": "5 Evaluation on Long-Term Trajectory Tracking", "content": "As mentioned above, we observe that existing LLMs often fail to provide useful feedback due to their limited ability to capture long trajectory information. Consequently, experiments solely based on causal graphs may not fully reflect the true reasoning capabilities of these models. To address this issue, we conduct additional experiments aimed at evaluating the capability of LLMs in capturing long trajectory information. In the following subsections, we first introduce our long-term trajectory measurement, then summarize key findings, and discuss their implications in detail."}, {"title": "5.1 Task Definition", "content": "In each round, a matrix with shape M \u00d7 N is given to the LLM, where M represents the trajectory length and N denotes the number of nodes. Each"}, {"title": "5.2 Prompting and Model Evaluation", "content": "To assess LLMs' ability to comprehend and reason over structured information with long trajectory, we employ a structured zero-shot prompting strategy. The prompt explicitly describes the task, including matrix interpretation and expected output format. We evaluate multiple LLMs, including GPT-40, Gemini, and Qwen, under identical conditions. Models are queried with the prompt and required to generate structured JSON outputs containing only the color trajectory matrix Y.\nWe conduct systematic evaluations across various trajectory lengths (M\u2208 {3, 5, 10, 15, 20}) to measure the robustness of each model. To evaluate the performance of each LLM, we develop two types of metrics, noted as Average Trajectory Accuracy (AT-Acc) and Overall Accuracy (OA-Acc). Overall Accuracy is computed as the fraction of correctly predicted color trajectory matrix:\n\\[OA-Acc = \\frac{1}{R} \\sum_{i=1}^{R} 1(\\hat{Y_i} = Y_i)\\]\nwhere \u0176 represents the model-generated color trajectory matrix, Y represents the ground-truth color trajectory matrix, R represents number of rounds, and 1 is the indicator function. In our experiment, we use R = 100.\nAverage Trajectory Accuracy aims to measure the average accuracy of the color change between each pair of rows. For example, the color change between row 0 and row 1 is noted as the first trajectory. To achieve that, a matrix T \u2208 {0,1}^{R\\times(M-1)} is further constructed, where:\n\\[T_{ir} = \\begin{cases} 1, \\text{ if } \\hat{Y}_{ir} = Y_{ir} \\\\ 0, \\text{ otherwise} \\end{cases}\\]\nfor all r\u2208 {0, ..., R \u2013 1} and i \u2208 {0, ..., M \u2013 2}. \\(Y_{i,r}\\) represents row i of matrix Y in round r. The Average Trajectory Accuracy is compared as following:\n\\[AT-Acc_j = \\frac{1}{R} \\sum_{i=1}^{R} T_{ij}\\]\nVj\u2208 {1, ..., M \u2013 1}"}, {"title": "5.3 Chain-of-Thought Prompting", "content": "Chain-of-Thought (CoT) prompting has been proven effective in enhancing the reasoning ability of large language models (LLMs). To further explore its effectiveness in long-term trajectory tracing, we incorporate CoT prompting before computing the final matrix. In our experiment, we use a simple prompt: \"Please also include the reason for your answer.\" We compare zero-shot and CoT-augmented responses, analyzing accuracy improvements across sequences."}, {"title": "5.4 Experiment Results", "content": "In this section, we present and analyze the results of our long trajectory measurement. These results offer insights into the understanding and reasoning capabilities of LLMs across different settings, particularly their ability to capture long trajectory information. In the following subsections, we summarize key findings and discuss their implications in detail."}, {"title": "5.4.1 Analysis of Trajectory Measurement", "content": "The experimental results are presented in Table 2 and Figure 4. We analyze these results from two perspectives:\nOverall Accuracy vs Trajectory Length Table 2 shows the Overall Accuracy of various large language models (LLMs) with respect to the trajectory length. The models are evaluated both with and without Chain-of-Thought (CoT) prompting. It can be observed that the accuracy of LLMs significantly degrades as the trajectory length increases. For example, Claude achieves an accuracy of 69% with 3 observations but drops to 0% when the trajectory length exceeds 20.\nAdditionally, we can observe that for GPT-40 and Qwen2.5, CoT prompting generally improves the models' performance, particularly with shorter observation sequences. However, performance declines as the sequence length increases. For example, with trajectory length of 5, GPT-4o achieves an accuracy of 13% without using CoT, but this increases to 100% when CoT is applied. However, CoT prompting does not improve the performance of models with longer observation sequences, meaning that its benefits are limited to shorter input lengths. In future work, exploring more robust methods for handling longer observation sequences may help mitigate the performance degradation observed as input lengths increase.\nAverage Trajectory Accuracy vs Trajectory Given trajectory length of 20, Figure 4 shows the Average Trajectory Accuracy of various large language models (LLMs) with respect to the trajectories. The results in this figure are averaged from 100 trials. It can be observed that as the trajectory increases, the accuracy of the models gradually decreases. This suggests that existing large language models struggle to handle long-term trajectory information, a phenomenon we refer to as temporal attention decay. Additionally, we find that certain models, such as GPT-40 and Qwen2.5, benefit from CoT prompting, which significantly improves their ability to capture long-term temporal dependencies. However, the accuracy of Qwen2.5 with CoT in the 19th trajectory remains below 70%. This indicates that future work should focus on further improving the models' ability to manage long temporal sequences, potentially by exploring more advanced techniques for enhancing temporal attention mechanisms."}, {"title": "6 Conclusion", "content": "In this paper, we present Auto-Bench, a novel benchmark designed to evaluate the scientific discovery capabilities of LLMs, which challenges models to uncover hidden structures and make optimal decisions through iterative interactions with an oracle. Specifically, our benchmark is built around two core settings: Chemistry and Social Networks. The Chemistry setting simulates chemical reactions to assess LLMs' ability to understand and reason about directed graphs, while the Social Network setting evaluates their performance in undirected graphs by modeling real-world social interactions. By gradually increasing the complexity of the causal graphs, we conduct a comprehensive analysis of LLMs' capacity for iterative knowledge refinement. Our experiments with state-of-the-art models such as GPT-4, Gemini, Qwen, Claude, and Llama reveal a significant performance drop as the problem complexity increases. Notably, the performance of LLMs is constrained by their ability to capture and maintain long trajectory information. This study not only highlights current limitations but also provides a foundation for future work aimed at enhancing LLMs' iterative reasoning and knowledge updating capabilities in scientific research."}, {"title": "7 Limitations", "content": "While our benchmark provides a novel framework for evaluating LLMs in scientific discovery through causal reasoning, it has several limitations. First, the framework assumes that causal relationships can be inferred purely from structured matrix observations and predefined interventions, which may not fully capture the complexities of real-world scientific reasoning that often involves unstructured data, domain-specific knowledge, and hypothesis generation beyond direct observations. Second, the benchmark currently focuses on discrete state changes and predefined causal structures, limiting its applicability to more dynamic, continuous, or probabilistic causal systems. Additionally, the evaluation primarily relies on adjacency matrix reconstruction, which may not comprehensively measure an LLM's ability to reason about causality in more abstract or interdisciplinary contexts. Lastly, the benchmark does not account for external knowledge retrieval, a critical component of human scientific discovery, which could lead to underestimating an LLM's full potential. Future work should address these challenges by incorporating more diverse data modalities, probabilistic causal reasoning, and interactive learning environments that better simulate the iterative nature of real-world scientific discovery."}]}