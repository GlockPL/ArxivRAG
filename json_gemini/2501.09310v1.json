{"title": "A Study of In-Context-Learning-Based Text-to-SQL Errors", "authors": ["Jiawei Shen", "Chengcheng Wan", "Ruoyi Qiao", "Jiazhen Zou", "Hang Xu", "Yuchen Shao", "Yueling Zhang", "Weikai Miao", "Geguang Pu"], "abstract": "Large language models (LLMs) have been adopted to perform text-to-SQL tasks, utilizing their in-context learning (ICL) capability to translate natural language questions into structured query language (SQL). However, such a technique faces correctness problems and requires efficient repairing solutions. In this paper, we conduct the first comprehensive study of text-to-SQL errors. Our study covers four representative ICL-based techniques, five basic repairing methods, two benchmarks, and two LLM settings. We find that text-to-SQL errors are widespread and summarize 29 error types of 7 categories. We also find that existing repairing attempts have limited correctness improvement at the cost of high computational overhead with many mis-repairs. Based on the findings, we propose MAPLEREPAIR, a novel text-to-SQL error detection and repairing framework. The evaluation demonstrates that MAPLEREPAIR outperforms existing solutions by repairing 13.8% more queries with neglectable mis-repairs and 67.4% less overhead.", "sections": [{"title": "Introduction", "content": "With the emergence of artificial intelligence (AI), converting natural language (NL) questions into structured query language (SQL) has become a practical data management approach for non-expert users. Such text-to-SQL techniques allow users to retrieve answers from databases through conversational interaction, eliminating the need of understanding database concepts and complex SQL syntax. Consequently, these techniques have been widely applied in data analysis [5, 7, 28, 29, 40, 73, 75, 76, 85], business intelligence [27, 39, 55, 59, 83], and other data-intensive domains, providing a user-friendly interface for accessing databases. Recently, in-context learning (ICL) of large language models (LLMs) has been adopted for text-to-SQL tasks [12, 13, 17, 20, 45, 50\u201352, 57, 65-67, 72, 84]. However, they still face correctness problems and require fixing solutions.\nTo better understand the correctness problem, consider DIN-SQL [51], an ICL-based text-to-SQL technique. As shown in Figure 1, it generates a problematic SQL query for a simple question of \"What is the release date and legal play format of the oldest mythic card?\", due to the hallucination and non-deterministic characteristics of LLMs. It wrongly searches for an undefined value in an enumeration-style column status, violating the value specification. It also tries to find the minimum value of a column that allows NULL value. These errors are obvious even without examining the question, indicating the inner flaw of ICL-based techniques.\nWhile recent work has proposed preliminary solutions for detecting and repairing SQL errors, these solutions are often straightforward and only achieve limited improvements. One line of work [12, 13, 50, 65, 66] categorizes errors of their generation results according to SQL clauses and designs rule-based detection for a small subset of these surface-level patterns. Another line of work [51, 72] simply asks LLMs to validate and repair all the generated SQL queries, providing error descriptions or repairing examples. Another work [49] categorizes SQL errors from non-LLM techniques, failing to understand the unique errors introduced by LLMs (details in Section 3). It also lacks error detection and repairing solutions.\nGiven these limitations, it is desirable to conduct an empirical study to understand the text-to-SQL errors of ICL-based techniques,"}, {"title": "Motivation", "content": "based on which design a systematic, robust, and automated repairing solution. There exist several challenges:\n1) How to obtain a high-quality error taxonomy that assists in repairing? While text-to-SQL errors could be easily classified by their clause locations, such taxonomy does not help in understanding error root causes and lacks symptom information for error detection. In addition, the taxonomy should characterize as many errors as possible, and provide definitions without ambiguity. How to design a helpful, comprehensive, and clear taxonomy is an open question.\n2) How to avoid introducing new errors during the repairing process? As a practical error detection and repairing solution, it is unacceptable to have a high false alarm rate or introduce many new errors. Due to the generative nature of LLMs, we cannot fully rely on LLMs themselves to identify and resolve their own errors [23]. Instead, we should design a robust repairing solution with few false alarms, which is challenging.\n3) How to detect and repair error with low-overhead? Text-to-SQL is a typical human-computer interaction scenario, where the high overhead significantly degrades the user experience. While invoking LLMs is a relatively effective solution, it is impractical to apply such expensive solutions to each SQL query."}, {"title": "Contribution", "content": "In this work, we perform the first comprehensive study of text-to-SQL errors of ICL-based techniques and the effectiveness of their repairing methods.\nThrough our study on four ICL-based techniques, two benchmarks, and two LLM settings, we observe that 37.3% generated SQL queries contain errors, and summarize 29 error types of 7 categories. Among them, 4 categories and 13 error types are format-related and can be detected even without the knowledge of NL question or database schema. We also find that 26.0% errors are related to format problems, and 30.9% to semantic problems, showing a great improvement space (Section 3).\nIn addition, we study 5 basic repairing methods from existing ICL-based techniques. We discover that these repairing attempts have limited correctness improvement - they fix 10.9-23.3% errors, at the cost of introducing 5.3-40.1% more errors. They also introduce 1.03-3.82x latency overhead. We also find that supplementary information such as execution information and specifications is beneficial for LLM repairing (Section 4).\nBased on these findings, we propose a novel framework, MAPLEREPAIR, for efficiently detecting and repairing SQL queries generated by ICL-based techniques. MAPLEREPAIR consists of five components, each targeting a set of errors. To minimize computational overhead, we design symptom-guided rule-based error detection solutions. Once an error is detected, MAPLEREPAIR first attempts to resolve it through rule-based query edits. When such an attempt fails, it provides error description, database information and repairing instructions to LLM for SQL query regeneration. Our evaluation shows that MAPLEREPAIR outperforms the state-of-the-art methods by repairing 13.8% more SQL queries and reducing 84.9% mis-repairs, with 67.4% less overhead(Section 5)."}, {"title": "Background", "content": "Text-to-SQL techniques aim to translate natural language questions into SQL queries with strict syntax. Given the effectiveness of LLMs, several in-context learning techniques are proposed, which provide data schema and other information in prompt to assist LLMs in generating correct SQL queries. Figure 2 shows a typical ICL-based text-to-SQL workflow with three major stages.\nGiven a question from the user, at the pre-generation stage, condensed data schema information is created by only describing related tables and columns. Concrete column values that are related to the question are also included [12, 50, 65], for a better match between the text-format question and enumeration-style columns (i.e., \"Legal\" value in Figure 2). ICL-based techniques also statically [12, 17, 20, 45, 50, 52, 65, 66] or dynamically [43, 51, 57, 67, 72, 84] provide several text-SQL pairs, serving as few-shot examples to enhance LLM capability on hard tasks. Then, at the generation stage, the SQL query is generated with the prompt constructed with all the information from the earlier stage. Finally, the post-generation stage tries to detect and repair the potential errors in the generated query, as introduced in Section 2.2. Sometimes, it also refines utilizing consistency between multiple generated SQL queries [17, 20, 31, 38]."}, {"title": "Repairing Text-to-SQL Errors", "content": "Several existing ICL-based techniques propose detection and repairing solutions for text-to-SQL errors. These solutions vary in their error identification algorithms and supplementary information for LLM to understand and repair errors.\nError identification. Judging the correctness of SQL queries is inherently hard. While the compiler is able to report syntax errors, semantic errors are likely to escape. One line of solution adopts chain-of-thought design, including DIN-SQL [51], CHESS [65] and DEA-SQL [72]. After generating the SQL query, they additionally ask LLM to validate the correctness, providing error pattern information. One line of solution, including DIN-SQL [51], CHESS [65] and DEA-SQL [72], utilizes LLMs to validate the correctness of generated SQL queries, providing error pattern information. However, they suffer from high computation costs and are likely to regard a correct query as wrong. Another line of solution applies simple rule-based solutions to identify error symptoms when executing the SQL query. For example, MAC-SQL [66] focuses on SQL execution errors, empty results of SQL query, and NULL values retrieved from the database. These techniques could only solve a small subset of erroneous queries.\nSupplementary information. After identifying an error, the LLM is then asked to generate the SQL query again. Several techniques"}, {"title": "Understanding Text-to-SQL Errors", "content": "To understand text-to-SQL errors, we collect and analyze the errors of SQL queries generated by ICL-based techniques."}, {"title": "Methodology", "content": "The empirical study is conducted with various benchmarks and techniques, as summarized in Table 1.\nBenchmark. We adopt the dev split of two representative text-to-SQL benchmarks, SPIDER [82] and BIRD [37]. SPIDER offers a diverse range of database schema and tasks, including 20 database settings and 1034 text-SQL pairs in its dev split. BIRD is a larger benchmark with more challenging tasks and more complex SQL queries, including 11 database settings and 1534 text-SQL pairs in its dev split (BIRD offers databases with more tables and data entries than that of SPIDER). It covers a broader spectrum of question types and SQL clauses, and provides additional context of evidence and database descriptions. All experiments are conducted in SQLite [6].\nSubject ICL-based techniques. We use the following criteria to select the subject techniques: (1) publicly-validated performance; (2) variance of error detection and repairing solutions; and (3) availability of open-source implementations. We select 4 representative techniques: MAC-SQL [66], DIN-SQL [51], CHESS [65], and DEA-SQL [72]. As their public results are evaluated with different versions of LLMs, we re-evaluate them with GPT-3.5-Turbo-0125 (GPT-3.5 for short) and GPT-40-2024-05-13 (GPT-40 for short) to make a fair comparison, as shown in Table 1. Note that, we only evaluate DIN-SQL and CHESS with GPT-3.5, due to their huge token consumption (i.e. CHESS takes about $2,500 to execute the dev set of BIRD With GPT-40). DEA-SQL only provides its version for SPIDER.\nMetrics. As one NL question may be associated with multiple equivalent SQL queries, we adopt execution match (EM) to judge the correctness of the generated SQL query, instead of the exact string match. EM regards a query as correct only when its query result set is the same as that of the gold query (i.e. ground-truth). We adopt the implementation of BIRD benchmark."}, {"title": "Error analysis", "content": "While all errors could lead to execution failure or incorrect query results, we focus on the errors on the \"edit path to correctness\", which provides clear guidance for correcting the SQL query. That is, we aim to create a taxonomy for the error symptoms that each is associated with a concrete mistake, as those shown in Figure 1. Such taxonomy assists in understanding the root causes of text-to-SQL errors, providing guidance for repairing. The error analysis is conducted in two phases.\nIn the first phase, we built the initial error taxonomies. All four ICL-based techniques were executed with a weaker LLM (i.e. GPT-3.5) on the more challenging benchmark (i.e. BIRD), aiming to capture a wider spectrum of error types. We then focused on the incorrect SQL queries and label error categories based on symptoms. We adopted the open card sorting approach [60]. Specifically, the analysis was done using an iterative process. In each iteration, 100 incorrect queries were randomly selected, and three co-authors independently studied each of them and labeled their error categories. They then cross-validated and discussed the labels until they reached a consensus on the categorized results. Such an iteration was repeated until all the 2460 incorrect SQL queries were analyzed.\nIn the next phase, we used the initial taxonomies to label SQL queries from the rest of the settings. When an error could not be adequately described by existing taxonomy, or is hard to recognize, three co-authors convened to discuss it. If a new category is added to the taxonomy, then all labeled errors are re-labeled according to the new taxonomy. Note that, we only record the errors that have actual impact on SQL query results, neglecting \"potential errors\". This manual analysis process requires considerable domain-specific"}, {"title": "Error Taxonomy", "content": "After manually studying 4602 incorrect SQL queries generated by ICL-based techniques, we build a two-level error taxonomy with 29 types, as illustrated in Figure 3.\nThe SQL query could not be parsed into a valid abstract syntax tree (AST), and thus fail to execute. We observe three types of syntax errors.\nThe generated SQL query invokes a function which seems semantically correct but does not exist in SQL standard or a certain database management system (DBMS). All the observed hallucinated functions are designed for arithmetic and date processing. For example, CHESS sometimes wrongly invokes a non-existent function DIVIDE(a, b), which should be instead of operator /. As another example, even after being told to be executed in SQLite, CHESS still invokes YEAR(x), a function only implemented by MySQL, time by time. In fact, it should invoke STRFTIME('%Y', x) instead.\nThe delimiters (e.g., single/double quotes, backtick and square brackets) do not properly enclose certain text strings and identifiers. Usually, it occurs on column names, table names, as well other identifiers that contain spaces, special characters, or SQL reserved keywords. The order table from the financial database of BIRD benchmark is an example, whose name would be regarded as a keyword unless quoted.\nAll other syntax errors, including unbalanced parentheses, incorrect clause ordering, missing necessary keywords, and others.\nThe SQL query is successfully parsed into an AST, but fails in schema resolution stage and thus cannot execute. We observe 4 types of schema errors.\nThe SQL query references a column that does not exist in the specified table of query, but whose name appears in other tables of the database. Broken down further, it appears in two scenarios: (1) explicit mismatch, where the column is explicitly associated with a table (i.e. SELECT T1.C1 FROM T1, ...), either through name or alias; and (2) implicit mismatch, where the SQL query only specified the column without table information (i.e. SELECT C1 FROM T1, ...).\nThe SQL query references a table or column whose name does not exist in the entire database. This type of error has two major subcategories: (1) spelling error, such as omitting character; (2) schema hallucination, where the LLM generates a semantically plausible but non-existent table or column name. We use edit distance to differentiate between these two subcategories, as spelling error typically has smaller string differences.\nSurprisingly, not all non-existent schema errors would trigger error messages. For the non-existent table or column names in quotes, SQLite will interpret them as a literal value rather than a reference. For the correlated sub-queries (i.e., an inner-level sub-query that references columns from an outer one), SQLite will try to resolve the non-existent column by matching it to the table in the outer query. Unfortunately, such matching is semantically incorrect in most cases, leading to a schema error. \nThe LLM-generated SQL query uses a nested loop and disp_id column of client table to link two tables. Although disp_id does not exist in the client table, SQLite matches it with the card table in the outer level, making the outer-level WHERE clause always true. Since these two scenarios involve SQLite's misinterpretation of non-existent columns, we put them into this category.\nA table or a column is assigned an alias, but subsequent references continue using its original name, which is no longer valid.\nA query references a column that exists in multiple selected tables, but does not explicitly specify the corresponding table. For example, the referred column C1 exists in both table T1 and T2, but the generated SQL query is SELECT C1 FROM T1, T2 ... without evidence of the exact table.\nThe SQL query passes schema resolution but contains logic errors which is obvious even unaware of the database and NL question. We observe 3 types of logic errors.\nDatabase implicitly makes type conversion on data of incompatible type, which typically happens on multi-variate operators and aggregate functions. Its automatic conversion mechanism may lead to unexpected results. For example, when the DIVIDE operator calculates two integer numbers, its output, which usually is a float number, will always be converted into an integer number.\nThe LLM generates a SQL query that performs the division operation between two integer numbers, while it should explicitly turn one of them into float type to ensure a correct response. In this example, the correct response is 1.416, while the SQL query outputs a floored value of 1.\nThe = operator is performed between a single value and a set of values, which actually should be IN operator. In such a case, SQLite will silently take the first element of the set and make the comparison, leading to a stricter condition.\nWhen sorting a column that contains NULL value, the NULL value will typically be regarded as the smallest value, which is likely to cause a NULL output when a concrete number is expected. Specifically, the ascending sort and MIN() function in SQLite could easily trigger this problem. A straightforward fix is to remove all the NULL values before invoking these two operations.\nThe SQL query passes schema resolution but contains errors that can be identified only with the knowledge of database schema, regardless of the NL question. We observe 3 types of convention errors.\nIn a column with value specifications, the SQL query attempts to find a value that violates the specification. It typically occurs when the column is of enumeration type or its value specification follows the enumeration style. It also occurs on column with strict format, e.g., date and time. This type of error makes the query condition unsatisfiable, leading to empty query response. There are two typical causes: (1) wrong value: the LLM correctly selects a column to be examined but expects an impossible value; and (2) wrong column: LLM selects a wrong column, making the expected values meaningless. \nThe SQL query applies numerical aggregate functions (e.g., SUM and AVG), ordering aggregate functions (e.g., MAX and MIN), and comparison operators (e.g., <, =, and >) and ORDER BY) on an improper column.\nFor a numerical column that serves as a unique ID, it is inappropriate to use a numerical aggregation or an ordering aggregate function. Some enumeration-style column uses integer format, which also should not apply numerical aggregation to.\nFor a non-numerical column whose comparison lacks practical significance (e.g., free text), it is also unreasonable to use a numerical aggregate function. Moreover, such function typically attempts to convert incompatible data into a numeric format. If this conversion fails, the system often defaults to treating the value as zero, which may cause misleading results.\nParticularly, for a text column, applying a comparison operator will trigger string-based comparisons, which can sometimes yield unexpected outcomes. \nThe LLM mistakenly selects two unrelated columns and generates a SQL query that compares them with IN, ON, or comparison operators. Such error is inherently illogical and particularly common in the JOIN clause generated by LLMs, leading to either an empty set or a joined table with irrational relationship between columns.\nThe SQL query passes schema resolution but contains semantic errors that could only be identified with the knowledge of database schema and the NL question.\nThe generated SQL query selects inappropriate tables. It contains three primary forms: (1) a required table is omitted; (2) an unrequired table is included; and (3) a wrong table is selected in place of the correct one.\nThe columns or expressions in the SELECT statement are improper, including containing wrong columns and having incorrect formats. For example, when answering the question of \"Whose post has the highest popularity?\", the SQL query erroneously selects MAX (ViewCount), retrieving the highest popularity. However, the question actually asks for people, which should be obtained from DisplayName.\nThe scope of a sub-query is not aligned with that of the main query, leading to unexpected query results. Typically, the sub-query attempts to find a max/min value or to filter results, but only focuses on a subset of the data"}, {"title": "Not an error.", "content": "The generated SQL query is regarded as incorrect due to the error of benchmark, ambiguity of NL question, or improper implementation of correctness judgment.\nThe benchmark provides an incorrect ground-truth (gold SQL query). The gold error is only caused by benchmark quality, regardless of the actual correctness of the generated SQL query.\nThe generated SQL query produces results that contain all the required information, but in a different format from that of the ground-truth. Typically, the results may have different column order, include additional column, or equivalent content in another representation. For example, when answering the question of \"who are the users that ...\", if the SQL query returns usernames instead of user IDs, we regard it as a correct output as long as the user ID is not explicitly required.\nThe database violates the foreign key constraints and causes unexpected query results. This problem commonly happens in SQLite as it disables the foreign key validation by default [2]. The toxicology database of BIRD benchmark is a concrete example. The molecule_id column of molecule table is the foreign key of molecule_id column of bond table. However, the latter contains values that are not recorded in the former. Therefore, SQLite cannot correctly join these two tables.\nIn some databases, the max/min value of a column refers to multiple data entries. However, most of the NL questions do not specify the expected answer of finding the items with max/min value. There are two types of solutions: selecting one of them (i.e., SELECT * FROM T1 ORDER BY C1 LIMIT 1) or all of them (i.e., SELECT * FROM T1 WHERE C1= (SELECT MAX(C1) FROM T1)). While most of the existing datasets and metrics treat them as different, we regard both are correct unless the NL question explicitly specifies the number of the selected data entries.\nThe query result contains a NULL value. We regard containing NULL value as incorrect only when the NL question explicitly requires non-NULL outputs."}, {"title": "Errors of ICL-based techniques", "content": "With the taxonomy, we have categorized and summarized the errors in SQL queries generated by all four studied ICL-based text-to-SQL techniques, as shown in Table 2. Around 15.4% of all queries have not-an-error problems, which indicates the unreliability of their correctness judgment. Therefore, we omit these queries when discussing the error statistics.\nOverall, among all the 6136 generated SQL queries in BIRD benchmark that have reliable correctness judgment, 47.8% of them contain at least one true error. As the text-to-SQL tasks in SPIDER dataset are simpler, only 26.8% of the queries are erroneous. In many cases, an incorrect SQL query contains multiple errors belonging to different categories, which we labeled with all these error types."}, {"title": "Format-related errors.", "content": "We first investigate the format-related errors that can be identified without the knowledge of NL questions, which are syntax, schema, logic, and convention errors. These errors reflect the inner flaws of ICL-based techniques in constructing \"format-correct\" SQL queries that are executable and seemly feasible on a concrete database. In our study, around 26.0% (682 of 2620) errors in BIRD belong to this category, and 25.8% (267 of 1035) in SPIDER. The prevalence of these errors shows that LLMs still face great challenges in generating format-correct SQL queries.\nSyntax errors constitute 18.8% (73 out of 389) of all errors that cause execution failures. More than 40% of the syntax errors in BIRD are function hallucination, indicating the hallucination problem of LLM also spread to the function invocation task in SQL query generation. Particularly, different DBMSs have different standards and libraries. For example, the YEAR(x) function is unsupported in SQLite, but is valid in MySQL [4]. As the LLMs are not able to differentiate among DBMS, this function is wrongly invoked in SQLite and fails to execute. In other words, when performing domain-specific tasks, the general LLMs suffer from negative impacts from out-of-domain knowledge.\nMeanwhile, missing quote and other syntax violations happen from time to time, due to the generative nature of LLMs. Unlike rule-based solutions, there is no guarantee for LLMs to generate responses in a strict format [56]. Therefore, it is inevitable for ICL-based techniques to generate SQL queries with syntax errors."}, {"title": "Semantic errors.", "content": "Semantic errors account for 36.1% and 17.7% of the total errors observed in the BIRD and SPIDER benchmark, respectively. These errors are caused by LLM's misinterpretation of the NL question and misunderstanding of the database schema. It indicates that existing ICL-based techniques fail to augment LLMs in mapping natural languages to concrete objects of a database.\nAround 40% semantic errors are projection errors. We observe that the generated SQL queries frequently use a wrong numerator or denominator in a division operation, especially for ratio calculations. Similar problems are also observed in other arithmetic-based logic. In addition, projection errors are more common in BIRD benchmark than SPIDER benchmark, as the latter requires simpler operations (i.e. basic aggregate functions that do not require sub-queries). These demonstrate the limitations of existing LLMs in mathematical reasoning within the context of relational structures and query formulations.\nIncorrect table selection errors account for 22.4% and 35.0% of semantic errors in BIRD and SPIDER, respectively. In complex databases, LLMs are likely to select wrong tables from a set of correlated ones. For example, if two tables share similar column names or overlapped content, it is hard for LLMs to distinguish them from the semantic space (e.g., yearmonth.date and transactions_1k.date column in the debit_card_specializing database of BIRD). The high frequency of incorrect table selection highlights the gap between natural languages and programming languages.\nComparatively, ICL-based techniques rarely make mistakes of comparing wrong columns, sub-query scope inconsistency, and ORDER BY errors, indicating its capability of understanding single-column information.\nThe impact of LLM capability is more obvious on the semantic errors. Replacing GPT-3.5 with the more powerful GPT-40 model, the studied ICL-based techniques have 48.7-80.4% less semantic errors, due to the improved understanding and reasoning capability of GPT-40. It indicates that semantic errors are not inherent flaws of ICL-based techniques, and highlights the benefit of LLM advancements."}, {"title": "Others.", "content": "As introduced in Section 3.2.7, around 39.9% errors cannot be classified by our taxonomy to provide insights for detection and repairing solution. It indicates that the ICL-based techniques fail to solve these difficult NL questions, and thus generate completely wrong SQL queries. We believe that these errors should be resolved by technique re-design or LLM enhancement, instead of run-time repairing.\nThere are also 120 errors failing to be clustered into error categories, which rarely occur. Therefore, this study skims over these rare error types (miscellaneous error)."}, {"title": "Discussion", "content": "The significant amount of not-an-error problems highlights the importance of benchmark qualities. In all the 12340 generated queries, the correctness judgment of 1905 of them is unreliable. Particularly, 52.1% of them are caused by the incorrect ground-truth (i.e., gold errors), indicating the prevalence of human annotation mistakes. The imprecise database schema description and ambiguity of NL questions also cause not-an-error problems. We observe severe foreign key violations in the flight_2 database of SPIDER, which compromise the reliability of correctness judgment within"}, {"title": "Effectiveness of Existing Repairing Solutions", "content": "We create a fixing framework on which to evaluate the effectiveness of existing repairing methods. We modify MAC-SQL to create this base model, due to its straightforward pipeline. Specifically, its prompt template consists of five components: (1) schema representation (slightly enhanced from MAC-SQL's design by clearer foreign key information), (2) question and evidence, (3) general instructions (CHESS's design due to its wide coverage of error scenarios), (4) SQL query to be repaired, and (5) supplementary information if needed.\nSchemes. The four techniques evaluated in Section 3 also have repairing modules. We summarize five basic repairing methods from them, as shown in Table 3.\n\u2022 Rule-Exe: It applies rule-based error detection according to the SQL execution results. It asks LLMs to regenerate the query if the original one can not be executed, returns empty result, or results contain NULL value.\n\u2022 LLM-Plain: For each generated SQL query, the LLM is asked to identify and fix the errors without additional information.\n\u2022 LLM-Exe: For each query, the LLM identifies and fixes the errors according to the execution results.\n\u2022 LLM-Value: It extends LLM-Exe by providing value specification.\n\u2022 LLM-Extr: It extends LLM-Plain by appending a second round fixing with instructions and few-shot repairing examples for extremum questions.\nTargets. For the SQL queries to be required, we reuse those generated by MAC-SQL with GPT-3.5 on the BIRD benchmark, which contains the richest error types.\nError analysis. We use the approach of Section 3.1.2."}, {"title": "ICL-Based Repairing Result", "content": "The performance of each scheme is shown in the right part of Table 2. Rule-Exe, LLM-Exe, and LLM-Value successfully repair 52-70% syntax and schema errors. In contrast, LLM-Plain and LLM-Extr only fix 37.5% and 38.5% SQL queries containing these errors, due to lack of DBMS feedback. It highlights the significant impact of incorporating DBMS feedback in LLM inference process.\nHowever, these methods only resolve less than half of logic and convention errors. Particularly, most implicit type convention errors remain after the repairing. For the violating value specification errors, LLM-Value fixes 44% of them, while other schemes fix 24-32%. It demonstrates the effect of providing targeted information in prompt is helpful for ICL-based repairing.\nAll five methods face trouble in tackling semantic errors. Most methods repair 23.7-27.3% of them, while Rule-Exe only repairs 9.5%. In addition, most unfocused errors remain, or even introduced more, after the repairing. Error types of other categories, like implicit type convention and ascending sort with NULL, also share a similar phenomenon. It shows the inner incapability of ICL-based methods to understand and repair some types of errors."}, {"title": "Incorrect repairing.", "content": "Not all repairing attempts will succeed. In fact, there are three types of incorrect repairing: (1) only repairing a subset of errors; (2) transforming one type of error into another; and (3) introducing errors into a correct SQL query. Table 4 shows the correct repairs (i.e., reduced errors) and introduced errors of each scheme.\nTake LLM-Value as a representative example, whose repairing results are detailed in Figure 11. ICL-based methods are effective in repairing format-related errors. While other errors are transformed into them from time to time. Due to repairing all SQL queries without examining their correctness, LLM-Value introduces errors into 8.1% of the correct SQL queries, including 2.3% into semantic errors and 3.4% into others category. Note that, these two types of errors are also extremely hard to repair, of which the majority remain after repairing. \nWe observe similar trends on the other repairing schemes except Rule-Exe on average, they repair 147 incorrect queries and meanwhile introduce 51 more erroneous ones. It implies that simple errors (i.e., syntax, schema, logic, and convention errors), if mis-repaired, are often transformed into complex ones that are hard to detect and repair (i.e., semantic errors and others). That is, an improper repairing attempt would exacerbate the errors! We"}, {"title": "MAPLEREPAIR: Mitigating Text-to-SQL errors", "content": "Based on our findings in Section 3&4, we propose MAPLEREPAIR, an automated error detection and repairing tool for text-to-SQL tasks, with minimal mis-repairs. To achieve low overhead, all of MAPLEREPAIR's detection algorithms and most of the repairing algorithms are rule-based. It only adopts LLM-based solutions for errors that are not eliminated by the rule-based solutions.\nAs shown in Figure 13, MAPLEREPAIR repairs SQL query with several repairer modules, each targeting a set of errors. The input of MAPLEREPAIR is the generated SQL query, as well as the NL question, database schema, and evidence (if available) from benchmark. First, the Execution Failure Repairer repairs the syntax and schema errors, making the SQL query executable. MAPLEREPAIR moves to the next step only when the repaired SQL query is executable. The Logic Repairer then detects and repairs logic errors with rule-based solutions. Next, the Convention Repairer, Semantic Repairer, and Output Aligner detect convention errors, semantic errors, and several output-style-related errors respectively. After the rule-based repairing attempts, MAPLEREPAIR invokes the LLM once to resolve all the remaining errors in a unified manner. Finally, MAPLEREPAIR outputs the repaired SQL query."}, {"title": "Execution Failure Repairer.", "content": "As DBMS returns error messages for non-executable SQL queries, Execution Failure Repairer detects syntax and schema errors by keywords from error messages. Particularly, function hallucination is detected by the invalid function information, and repaired by function transformation through a predefined table. This table is obtained through empirical study and online documents, covering common numeric and date functions. Table-column mismatch is repaired by providing error message to LLM. Missing JOIN with missing table information is resolved by including the joining operation. Unused alias is resolved by replacing the table identifier. For non-existent schema, MAPLEREPAIR only resolves the spelling errors with respect to edit distance."}, {"title": "Logic Repairer.", "content": "It resolves all the three types of logic errors. For implicit type conversion, it focuses on the missing CAST scenario, which is detected by examining the operands of DIVIDE operations and resolved by including CAST operation. MAPLEREPAIR detects using = instead of IN through sub-query analysis, and replaces the = operator when the sub-query may return multiple rows. Ascending sort with NULL is detected by examining the existence of NULL values in the column operands of MIN function and ORDER BY clause, and eliminated by"}]}