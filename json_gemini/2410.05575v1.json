{"title": "ClaimBrush: A Novel Framework for Automated\nPatent Claim Refinement Based on Large Language\nModels", "authors": ["Seiya Kawano", "Hirofumi Nonaka", "Koichiro Yoshino"], "abstract": "Abstract\u2014Automatic refinement of patent claims in patent\napplications is crucial from the perspective of intellectual prop-\nerty strategy. In this paper, we propose \u201cClaimBrush,\u201d a novel\nframework for automated patent claim refinement that includes\na dataset and a rewriting model. We constructed a dataset\nfor training and evaluating patent claim rewriting models by\ncollecting a large number of actual patent claim rewriting cases\nfrom the patent examination process. Using the constructed\ndataset, we built an automatic patent claim rewriting model by\nfinetuning a large language model. Furthermore, we enhanced\nthe performance of the automatic patent claim rewriting model\nby applying preference optimization based on a prediction\nmodel of patent examiners' Office Actions. The experimental\nresults showed that our proposed rewriting model outperformed\nheuristic baselines and zero-shot learning in state-of-the-art large\nlanguage models. Moreover, preference optimization based on\npatent examiners' preferences boosted the performance of patent\nclaim refinement.", "sections": [{"title": "I. INTRODUCTION", "content": "Patent claims are part of a patent document that defines\nthe technical scope protected by the patent, and patent claims\nconsist of one or more claims [1], [2]. In patent applications,\nit is important to effectively refine the content of patent claims\nin order for the invention to receive a patent grant or to obtain\nstronger rights compared to existing patents [3], [4]. With\nthe increasing reliance on data-driven approaches in various\nindustries, automating the refinement process has become a\ncritical step in improving the efficiency of patent applications.\nHowever, refining patent claims is often a time-consuming and\nlabor-intensive task, so effective tools to support the patent\nclaim refinement are in demand [5], [6].\nWhile research has been conducted on basic patent infor-\nmation processing tasks such as patent retrieval and patent\nmining [7]\u2013[9], as well as on supporting the understanding\nof patent claims [1], more work is needed on frameworks that\nsuggest patent claim rewriting plans from various perspectives\nto provide more effective patent revision support. To realize\nsuch rewriting, a dataset containing rewriting examples from\nthe viewpoints of what kinds of rewrites are effective in\nobtaining patent approval or strengthening the scope of rights\nis essential. This is because the state-of-the-art in rewriting\nresearch is based on statistical models such as language\nmodels, and data is indispensable for training and fine-tuning\nsuch models.\nIn this paper, we propose \u201cClaimBrush,\u201d a novel framework\nthat includes a rewriting model and a dataset for training and\nevaluation, with the goal of automatically generating rewrites\nof patent claims. We focus on the process before and after\napproval in patent examination and construct the dataset from\nthe perspective of rewriting patent claims into patent claims\nthat are more likely to be approved. Specifically, we focus on\nthe examination process in patent applications and regard the\npairs of claims described in the publication before examination\nand the patent publication tied to a particular patent application\nas examples of desirable rewrites of patent claims. These\nrewriting examples reflect the results of all amendments (both\nvoluntary amendments and amendments in response to Office\nActions\u00b9) made by the applicant during the patent examination\nprocess in order for the filed invention to receive a patent. In\nother words, from the perspective of patent examination, the\nrewritten claims are more refined than the original ones and\ncan be used as training and evaluation data for the rewriting\nmodel.\nWe built an automatic patent claim rewriting model by fine-\ntuning a large language model (LLM) using the constructed\ndataset. Furthermore, we enhanced the performance of the\nautomatic patent claim rewriting model by applying preference\noptimization based on the prediction results of patent examin-\ners' Office Actions. We developed an automatic discrimination\nmodel to determine whether the claims generated by the model\nare likely to be accepted or rejected, taking into account the\npatent claims of patents cited by the examiner as prior art,\nand then used these prediction results for model optimization.\nIn other words, we optimized the model to align with the\npreferences of patent examiners. We applied a method based\non Kahneman-Tversky Optimization (KTO) for preference\noptimization. Our experimental results showed that our pro-"}, {"title": "II. RELATED WORK", "content": "With the advent of large language models (LLMs) based\non Transformer language models, methods using LLMs have\nshown state-of-the-art performance in various natural language\nprocessing tasks such as text summarization, question an-\nswering, machine translation, and grammatical error correc-\ntion [10], [11]. While the performance of these applied tasks\ncontinues to evolve along with the improvement of LLMs, the\ndevelopment of patent language models specialized for the\npatent domain (especially U.S. patents), which have different\nvocabulary from general corpora, and their application to\nvarious tasks in the patent domain are progressing [6], [12],\n[13]. In evaluating the performance of such patent language\nmodels, the focus is often on automatically generating patent\nclaims. However, there is also the task that generates practical\nrewrites for given claim [5].\nIn the patent claim rewriting task, unlike simple generation\ntasks, it is necessary to generate rewrites that maintain the\nmeaning content of the original claim, in other words, the es-\nsential information of the original invention, while improving\nits quality. Regarding the patent claim rewriting task, there\nis research based on rule-based methods that decompose a\nmulti-multi claim (a claim that cites two or more other claims\nthat selectively cite two or more other claims) into multiple\nclaims and rewrite them. Nevertheless, this research is limited\nto rewriting support for very specific purposes [14]. More\nresearch must be done that automatically generates consistent\nrewrites, considering not only a single claim but also the entire\nstructure of patent claims, for various purposes. However,\nexisting publicly available datasets are insufficient for training\nand evaluating such rewriting generation models.\nAs research similar to our study, there is research on the\nproofreading task of scientific papers, which is approached\nfrom various aspects such as grammatical error correction and\nrewriting [15], [16], and review comment generation [17]\u2013\n[20]. Typically, these studies utilize the papers submitted\nto sites such as OpenReview and their review comments\nfor dataset construction. However, the pre-processing and\nannotation work of unstructured texts, such as papers and\nreview comments, is costly, and there are still challenges in\nbuilding models that correspond to various technical domains\ndue to the limitations of data sources. On the other hand,\npatent documents are highly structured; their format is unified\nregardless of the domain, and they cover various technical\ndomains. In addition, since patent documents contain rich\nmeta-information and examination progress information, it\nmay be possible to generate datasets for proofreading tasks\nacross various technical domains automatically.\nGiven such background, as the first step toward realizing\nautomatic proofreading models for technical documents, in-\ncluding patents, in this study, we automatically constructed\na rewriting model for generating refined rewrites of patent\nclaims and a training and evaluation dataset containing rich\nauxiliary information."}, {"title": "III. DATASET OF CLAIM REFINEMENT", "content": "In order to train and evaluate rewriting generation models\naimed at the automatic refinement of patent claims, it is\nnecessary to prepare a reliable parallel corpus as a dataset\nthat collects a large number of patent claim rewriting cases\nconsisting of pairs of patent claims before and after rewriting.\nHowever, since patent claims are highly technical and legal\ndocuments, creating datasets manually is practically difficult\nfrom a cost perspective, as it requires workers to be well-\nversed in both the technical content and intellectual property\npractice.\nIn this paper, we collected pairs of patent claims described\nin the publication before examination and the patent publi-\ncation associated with actual patent applications in Japan as\nrewriting examples for constructing such a dataset. In general,\nin patent examination, many patent applications are initially\nrefused and then granted registration after applying necessary\nmodifications (amendments) (see Fig. 1). In addition, minor\namendments necessary for procedural purposes are also made\nregardless of whether the application is refused or not. The\npublication before examination is a patent document at the\ntime of filing that is published after a certain period regardless\nof whether the patent is registered or not, and the patent\npublication is a patent document that has actually been granted\nregistration. In Japan, these are known as \u7279\u8a31\u516c\u958b\u516c\u5831\n(A) and \u7279\u8a31\u516c\u5831 (B9), and are available as open data.\nConsidering the patent examination process, the patent claims\nin the patent publication have been modified as necessary\nfrom the patent document at the time of filing (corresponding\nto the publication before examination) in order to receive a\np patent. Thus, from the perspective of the patent system, the\npatent claims in the patent publication (=B9) can be considered"}, {"title": "A. Dataset Construction", "content": "We used the dump data of patent information provided by\nthe Japan Patent Office for dataset construction\u00b2. Specifically,\nfrom the set of publications before examination (\u7279\u8a31\u516c\u958b\n\u516c\u5831(A)) and patent publications (\u7279\u8a31\u516c\u5831(B9)) published\nthrough the Patent Office's system from 2004 to 2022, we ex-\ntracted pairs of patent claims with the same patent application\nnumber. Furthermore, based on the patent application numbers,\nwe linked the examination history information\u00b3 provided by\nthe Patent Office and the IIP Patent Database [21] to establish\nan integrated database available for patent information pro-\ncessing research.\nAmong the additional information assigned to the rewriting\npairs, we particularly utilized the legal provisions that served\nas the basis for refusing the patent application during exami-\nnation4. We also utilized the claim information of prior patents\ncited by the patent examiner when notifying the reasons for\nrefusal (the so-called Office Action). Currently, there are about"}, {"title": "B. Dataset Statistics", "content": "Table II shows the statistics of valid pairs of patent claims\nbetween the publication before examination (A) and the patent\npublication (B9) included in our dataset. This includes cases\nwhere there are differences between the two, cases where\nthere are no differences, cases with reasons for rejection,\ncases without reasons for rejection, and patent claims from the\npublication before examination (A) without a corresponding\np patent publication (B9) (Type 1). The average number of\ncharacters and claims are based on the patent claims in the\np patent publication (B9). The values in parentheses indicate\nthe average percentage change compared to the publication\nbefore examination (A) prior to amendment. The percentage\nchange for each application is calculated individually, and then\naveraged across all applications in each type. For ungranted\npatents (Type 1), the statistics are based on the publication\nbefore examination (A).\nFrom Table II, we can observe that the dataset includes\ntext pairs of the publication before examination (A) and\nthe patent publication (B9) for a total of 2,245,640 patent\napplications. Among these, there were 1,839,880 cases where\nthere were differences in the text of the patent claims between\nthe two (Types 4 and 5), and 405,760 cases where there\nwere no differences (Types 2 and 3). Specifically, Type 4\n(628,882 cases) represents applications where the texts differ\nbut there are no reasons for rejection, indicating voluntary\namendments made by applicants. Type 5 (1,210,998 cases)\nrepresents applications with differences and reasons for re-\njection, reflecting amendments made in response to Office\nActions. We confirmed that the average number of characters\nand claims tends to decrease for patent publications compared"}, {"title": "IV. PATENT CLAIM REFINEMENT MODEL BASED ON\nLARGE LANGUAGE MODEL", "content": "In this section, we define the task of patent claim refine-\nment and describe the training approach of the patent claim\nrewriting generation model based on large language models\n(LLMs).\nWe define the patent claim refinement task as follows:\n\u2022 Input: Text x = [c;r] concatenating the patent claims c\nbefore rewriting and the additional information r.\n\u2022 Output: Patent claims c' after rewriting.\nHere, c is the patent claims at the time of filing the patent\napplication where refusal is expected, c' is the patent claims\nto which rewriting has been applied so that it can receive a\np patent grant, and r is the expected reason for refusal of patent\napplication for c. Although multiple claims can be set for a\nsingle patent application, in this paper, we treat them together\nas a single text. In this task, given the input x, we generate\nc' that maximizes the following probability using a language\nmodel:\n$\\\\underset{c'}{argmax} P(c_t|c_{1:t}, x)$"}, {"title": "B. Supervised Fine-Tuning (SFT)", "content": "To train the patent claim rewriting generation model, we ap-\nplied Supervised Fine-Tuning (SFT) based on minimizing the\nfollowing objective function to a pre-trained auto-regressive\nlarge language model [22]:\n$L = - \\\\underset{(x,c')\\\\in D}{ \\\\Sigma} \\\\underset{t=1}{T} log P(c_t|c_{1:t}, x; \\\\theta)$"}, {"title": "C. Preference Optimization", "content": "We further refine the LLM fine-tuned by Supervised Fine-\nTuning (SFT) for the proposed task using preference opti-\nmization techniques, which are known as a powerful way to\nstrengthen language models based on human preferences. In\nthis paper, we propose an approach using Kahneman-Tversky\nOptimization (KTO) [24] based on patent examiners' pref-\nerences. As a conventional alignment approach for language\nmodels, Reward Learning from Human Feedback (RLHF)\nbased on Proximal Policy Optimization (PPO) is considered\na promising option [25], [26]. However, RLHF requires se-"}, {"title": "D. Kahneman-Tversky Optimization", "content": "KTO is a preference optimization method based on prospect\ntheory that incorporates human decision-making biases into\nthe loss function. The Kahneman-Tversky Optimization (KTO)\nprocess directly maximizes the utility of generations instead of\nmaximizing the log-likelihood of the preferences. KTO only\nrequires a binary signal of whether output is desirable or not,\nwhich is a kind of data easier to obtain than paired preference\ndata. The loss function of KTO is defined as follows:\n$L_\\\\kappa\u03c4\u03bf(\\pi_\\\\theta, \\\\pi_{ref}) = E_{(x,y)\\\\sim D}[\\\\Delta_y - v(x, y)]$"}, {"title": "1) Automatic Feedback From Preference Model", "content": "To model\np patent examiners' preferences, we fine-tuned a pre-trained\nLLM for binary classification using our dataset of examples\nwhere patent claims were rewritten in response to reasons\nfor refusal. Specifically, we consider two versions of patent\nclaims: c, the claims from the publication before examination,\nand c', the claims from the patent publication after amend-\nments. We construct a prompt text by concatenating the patent\nclaims $\\\\hat{c} \\\\in {c, c'}$ to be evaluated, where $\\\\hat{c}$ represents either the\noriginal claims c or the amended claims c', the given reason for\nrefusal r, and the claims a of a prior patent related to $\\\\hat{c}$. When\nthere are multiple prior patents cited, we randomly sample one\nto use as a to simplify the input and reduce computational"}, {"title": "V. EXPERIMENT SETTINGS", "content": "In this section, we describe the experimental settings for\nevaluating the patent claim refinement performance of the\nproposed model."}, {"title": "A. Dataset", "content": "Among the rewriting pairs included in the dataset we\nconstructed, we used pairs where the patent was able to\nbe granted after being refused once for some reason during\nthe examination of the patent application. However, since\ntraining including pairs with very long sentences requires a\ncertain amount of computational resources, we constructed\ndata for training (317,182 cases), validation (1,000 cases), and\nevaluation (1,000 cases) of language models and the preference\nmodel using only pairs with not too long contexts. In addition,\nfor preference optimization, we prepared 1,000 samples that\ndo not overlap with these data."}, {"title": "C. Training Setting", "content": "In the SFT of the language model and the training of the\npreference model, we set the learning batch size to 64 and\nthe model's Optimizer to AdamW with a learning rate of 5e-\n5. The number of learning epochs for the model was set to\n1, and the learning rate was linearly decayed to zero at the\nend. Also, in the training of the preference model, we set\nthe learning batch size to 64 and the model's Optimizer to\nAdamW with a learning rate of 5e-5. In KTO, we set the\nlearning batch size to 64 and the model's Optimizer to AdamW\nwith a learning rate of 1e-5. The number of learning epochs\nfor the model was set to 3, and a warmup of 200 steps was\napplied, after which the learning rate was linearly decayed to\nzero at the end. In generating the training samples for KTO,\nwe randomly generated 12 rewriting candidates (top_p=0.95,\ntemperature=0.7) for each sample from the training dataset and\nautomatically assigned the labels of desirable and undesirable\nbased on the evaluation from the preference model. Here, we\nadded the case of outputting the input text as is as undesirable\nand the correct rewrite as desirable. Also, we set Ap to 3.0,\n\u03bb\u03c5 to 1.0, and $ \\\\beta$ to 0.2. In LoRA training, we set r = 8 and\n$\\\\alpha$ = 16 for the linear layers in the self-attention layer of the\nmodel."}, {"title": "D. Evaluation Metrics", "content": "The patent claim rewriting task is essentially a machine\ntranslation task within the same language. However, con-\nsidering the amendment requirement provisions for patents\nstipulated in Article 17 of the Patent Act, evaluation metrics\nthat take into account the input text itself are necessary.\nTherefore, we adopted standard metrics used in the fields\nof grammatical error correction and text simplification [29],\n[30], where consideration of not only the reference and hy-\npothesis but also the input is necessary. Furthermore, using\nthe preference model trained for KTO, we determine the\nappropriateness of the output patent claims of the rewriting\nmodel. The evaluation metrics used in this research are as\nfollows:\n\u2022 GLEU: An evaluation metric that improves BLEU, a\nstandard evaluation metric for machine translation, for\nthe task of grammatical error correction [31]. GLEU is\ncalculated by subtracting the number of n-grams that\nappear in the original text but not in the reference text\nfrom the number of n-grams that match between the\ncorrected text and the reference text."}, {"title": "B. Comparison Models", "content": "We compared the performance of heuristic baseline models,\nzero-shot learning in state-of-the-art large language models\n(LLMs), and proposed models based on fine-tuning of LLMs.\nThe models for comparison are as follows:\n\u2022 Copy: The case where the patent claims are not rewritten.\n\u2022 RDC (Random Delete of Claims): A case where one of\nthe claims, excluding claim 1, is randomly deleted. Any\nclaims dependent on the deleted claim are also excluded.\nThis is based on the observation that the number of claims\ntends to decrease from the time of application to patent\nregistration (see Section III-B).\n\u2022 DMMC (Delete of Multi-Multi Claims): A case where\nall multi-multi claims are excluded. Similarly, any claims\ndependent on the deleted multi-multi claims are also\nexcluded. This is based on the fact that citing multi\nclaims in the form of multi-multi claims is prohibited\nor discouraged by patent offices in various countries7.\n\u2022 LLM (Zero-shot): We evaluated the zero-shot perfor-\nmance of state-of-the-art LLMs. While these LLMs are\nnot specifically designed for patent claim refinement,\nthey have shown the ability to solve various tasks in a\nzero-shot setting [28]. These models were included as\nbaselines to assess their performance on the patent claim\nrefinement task without fine-tuning. The input format\nprovided to the models was the same as in the fine-tuning\nexperiments, including the patent claims and expected\nreasons for rejection, and the models were asked to\ngenerate refined patent claims without additional training.\n\u2022 LLM (SFT): The case where the LLM is fine-tuned using\nfull-parameter supervised fine-tuning.\n\u2022 LLM (LORA): The case where LoRA (Low-Rank Adap-\ntation) is applied to the LLM, and only specific layers\nare fine-tuned.\n\u2022 LLM (KTO): The case where the supervised fine-tuned\nLLM is further optimized using preference optimization\nvia Kahneman-Tversky Optimization (KTO)."}, {"title": "A. Performance of preference prediction", "content": "Table IV shows the prediction results of the preference\nmodel for patent examiners' in the evaluation set. The results\nshow that both the prediction of desirable and undesirable\nachieved an F1 score of over 73%. Since the frequency of each\nlabel is uniform, it can be seen that it significantly exceeds the\nchance rate accuracy of 50%. In other words, the preference\nprediction model can predict with high confidence whether the\nrewritten patent claims will be judged as acceptable or unac-\nceptable by the examiner against the claims of prior patent,\nsuggesting the possibility of substituting patent examiners'\njudgment of the appropriateness of patent claims by automatic\nevaluation."}, {"title": "B. Performance of patent claim refinement", "content": "Table V shows the performance of each patent claim\nrewriting generation model in the evaluation set. Here, w/o\nr denotes the case where the reasons for patent refusal are\nnot used as input to the model. The proposed method based\non LLMs surpasses the heuristics baseline models in SARI\n(word, phrase) using word-level and phrase-level segmentation\ncriteria. The Copy model, which adopts the original patent\nclaims as output, outperforms the proposed method based\non LLMs in GLEU (word) using word-level segmentation\ncriteria. On the other hand, RDC based on random deletion\nof claims shows lower GLEU than the method based on\nLLMs, except for the LoRA model based on low-parameter-size LLMs. DMMC based on deletion of multi-multi claims\nresulted in competing results with Copy because there were\nonly a few multi-multi claims in the evaluation set results, but\nthe GLEU decreased from copy. This reflects the characteristic\nof GLEU that overestimates copying from the input text and\nunderestimates the bold strategy of deleting long sentences.\nOn the other hand, for SARI, which reflects each of the\nrewriting operations of addition, deletion, and substitution in\nthe evaluation, Copy and RDC show low performance. Also,\nthe acceptance rate, which is the automatic evaluation result\nof the refusal judgment of the generated patent claims, never\nexceeded 50% for the baseline models, in contrast to the\nmethod based on LLMs.\nRegarding the effect of different learning approaches in the\ntraining of the rewriting model, SFT outperformed LoRA.\nDue to the nature of the LoRA learning method, it strongly\ndepends on the performance of the language model used as\nthe base. The language model used in this experiment is a\nmultilingual LLM, but there is room for improvement in its\nability to generate Japanese patent documents. when using\nLORA learning, a 7B model shows results that finally compete\nwith the SFT of a 0.5B model, suggesting the importance of\nadapting the pre-trained model to both Japanese and the patent\ndomain. Also, the model to which KTO was applied achieved a\n96% acceptance rate of the rewriting results of patent claims"}, {"title": "VII. CONCLUSION", "content": "In this paper, we proposed ClaimBrush, a novel framework\nthat includes a rewriting model and a dataset for training\nand evaluation, aimed at automatic patent claim refinement.\nWe constructed an automatic patent claim rewriting model\nby fine-tuning a large language model using the constructed\ndataset. Furthermore, we enhanced the performance of the\nautomatic patent claim rewriting model by applying preference\noptimization based on the prediction results of patent examin-\ners' Office Actions. The experimental results showed that our\nproposed model not only surpasses heuristic-based baseline\nmodels but also significantly outperforms zero-shot learning\nin state-of-the-art large language models. Furthermore, we\ndemonstrated that preference optimization based on patent\nexaminers' preferences boosts the performance of patent claim\nrewriting.\nIn terms of future work, efforts should focus on the im-\nprovement of the model' s efficiency in handling the long\nand complex nature of patent claims, potentially through the\nintegration of more efficient architectures such as encoder\nnetworks. Additionally, the adaptation of pre-trained models\nto the patent domain, particularly for Japanese patent-related\ntasks, remains crucial for achieving better performance. The\nexploration of advanced methods such as Retrieval-Augmented\nGeneration (RAG) [33] to incorporate external information\nfrom prior arts and patent body text could offer further\nimprovements. Finally, the development of more accurate\nevaluation metrics that align with human judgments is essential\nfor refining the effectiveness of patent claim rewriting."}]}