{"title": "Continual Learning for Adaptable Car-Following in Dynamic Traffic Environments", "authors": ["Xianda Chen", "PakHin Tiu", "Xu Han", "Junjie Chen", "Yuanfei Wu", "Xinhu Zheng", "Meixin Zhu"], "abstract": "The continual evolution of autonomous driving technology requires car-following models that can adapt to diverse and dynamic traffic environments. Traditional learning-based models often suffer from performance degradation when encountering unseen traffic patterns due to a lack of continual learning capabilities. This paper proposes a novel car-following model based on continual learning that addresses this limitation. Our framework incorporates Elastic Weight Consolidation (EWC) and Memory Aware Synapses (MAS) techniques to mitigate catastrophic forgetting and enable the model to learn incrementally from new traffic data streams. We evaluate the performance of the proposed model on the Waymo and Lyft datasets which encompass various traffic scenarios. The results demonstrate that the continual learning techniques significantly outperform the baseline model, achieving 0% collision rates across all traffic conditions. This research contributes to the advancement of autonomous driving technology by fostering the development of more robust and adaptable car-following models.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid development of autonomous driving technology promises to revolutionize transportation by enhancing both safety and efficiency. At the core of this technology lies the ability to navigate complex traffic scenarios, requiring models that can capture the dynamics of real-world traffic flow and make human-like decisions [1]\u2013[4]. Car-following models play a crucial role in achieving this by simulating longitudinal control. These models enable autonomous vehicles to maintain safe distances from leading vehicles (LVs) in traffic streams [5]-[7]. Adaptive Cruise Control (ACC) has been a valuable advancement in driver assistance, promoting safety and efficiency on highways by automating longitudinal control [8], [9]. However, traditional ACC systems often rely on pre-defined rules or control models. These approaches can be limited in dynamic traffic which is calibrated for typical car behavior, ACC might not adapt well to slower vehicles (trucks) or faster ones (motorcycles).\nLearning-based methodologies, such as Q-learning, Deep Learning, and Reinforcement Learning, have emerged as promising solutions for car-following models [10]\u2013[14]. However, these methods often rely heavily on the training data distribution (see Fig. 1). This dependence can lead to performance degradation when encountering unforeseen situations that deviate from the training data. This vulnerability stems from the lack of continual learning capabilities, which refers to the ability to effectively adapt to new traffic patterns while retaining previously learned safe driving behaviors [15], [16]. Retraining a car-following model from scratch for each novel scenario is not only computationally expensive and time-consuming but also risks catastrophic forgetting, where previously acquired knowledge is overwritten during the retraining process [17], [18].\nIn light of these challenges, we propose a novel continual learning-based car-following model. By incorporating continual learning techniques, the proposed model aims to achieve robust performance by seamlessly adapting to new traffic situations without sacrificing previously learned safe driving behaviors. This approach fosters a more adaptable and generalizable car-following model, capable of thriving in the dynamic and diverse nature of real-world traffic environments. This research contributes to the advancement of autonomous driving technology by introducing:\n\u2022 We propose the implementation of continual learning approaches, specifically Elastic Weight Consolidation (EWC) and Memory Aware Synapses (MAS), for car-following models in autonomous vehicles. This enables the models to continuously learn and adapt to new traffic patterns"}, {"title": "II. RELATED WORK", "content": "Developing robust car-following models for autonomous vehicles is crucial. This section explores existing approaches and highlights the potential of continual learning."}, {"title": "A. Car-Following Models", "content": "Traditionally, car-following models fall into two categories: rule-based and data-driven (learning-based). Rule-based models offer efficiency and interpretability but struggle with dynamic traffic. Examples include Gipps' model (incorporates reaction time) [19], the GM model (considers speed and spacing) [6], and the IDM (captures driver behavior) [20]. Data-driven models learn from historical data to predict following vehicle (FV) behavior, offering flexibility for diverse conditions. Examples include: KNN [21], Neural Networks (NN) [11], [12], [22], and Reinforcement Learning (RL) [1], [10]. KNN models identify similar past situations but can suffer from the curse of dimensionality. NN-based models learn complex relationships from data but can be computationally expensive. RL-based models learn through trial and error in a simulated environment but can be complex to implement. However, data-driven models face a challenge: distributional shift. Real-world traffic can deviate from training data, leading to performance degradation in unseen scenarios. This necessitates continual learning."}, {"title": "B. Continual Learning", "content": "Continual learning (also lifelong learning) [23], [24] allows models to adapt to new information continuously, overcoming limitations of static datasets. In computer vision, continual learning allows models to recognize new objects/scenes without forgetting old categories [25]. Similarly, in natural language processing, it allows models to learn new vocabulary/writing styles while retaining proficiency in previously encountered patterns [26]. In autonomous driving, continual learning offers significant potential for car-following models. Traditional models struggle with unseen traffic scenarios. Continual learning equips them to learn from new data streams while preserving safe driving behaviors. Bao et al. [27] proposed a framework using generative replay to mitigate forgetting in vehicle trajectory prediction. Verwimp et al. [28] introduced CLAD, a continual learning benchmark for object classification/detection in autonomous driving. Continual learning holds promise for car-following models. By facilitating adaptation to evolving traffic patterns, it enhances model transferability and fosters safer, more reliable autonomous driving systems. Our research builds upon these advancements by presenting a novel continual learning framework for car-following models."}, {"title": "III. CONTINUAL LEARNING CAR-FOLLOWING MODEL", "content": "This section introduces our innovative approach to car-following model development, focusing on continual learning to overcome the shortcomings of traditional retraining methods. Instead of starting from scratch with each update, our framework adopts an incremental learning strategy."}, {"title": "A. Baseline Model and Training", "content": "The foundation of our framework lies in a well-established car-following model, a Long Short-Term Memory (LSTM) network [12]. This baseline model is initially trained on a comprehensive traffic dataset encompassing diverse traffic scenarios. The training process aims to minimize a fundamental loss function, commonly the L2 loss. The L2 loss measures the difference between the model's predicted FV dynamic information and the actual behavior observed in the training data.\nIn our baseline model settings, the loss function is defined as follows:\n$L_{Baseline}(\\theta) = MSELoss(s, s^*) + Collision Penalty$ \nHere, s represents the state of the simulated vehicle (SV), while s* denotes the state of the FV from the dataset. The collision penalty term accounts for the severity of potential collisions, encouraging the model to prioritize safe driving behaviors during training."}, {"title": "B. Continual Learning Techniques", "content": "Our framework incorporates two prominent techniques in the domain of continual learning for comparison: Elastic Weight Consolidation [24] and Memory Aware Synapses [23]. These techniques address the challenge of catastrophic forgetting by prioritizing the preservation of previously learned knowledge during the learning of new traffic patterns."}, {"title": "1) Elastic Weight Consolidation", "content": "EWC incorporates a regularization term into the loss function to mitigate substantial alterations in the model's weights during the learning of new tasks. This additional term penalizes the model for making drastic weight updates that were pivotal for prior tasks, as illustrated in Fig. 2. By integrating a penalty factor based on the significance of each weight parameter, EWC ensures that previously acquired knowledge remains resilient to being overwritten."}, {"title": "2) Memory Aware Synapses", "content": "MAS addresses catastrophic forgetting by dynamically adapting the learning rate of each weight parameter according to its significance in previously learned tasks. Weights identified as essential for past tasks are allocated lower learning rates during the training of new tasks. This tactic effectively decelerates their updates, reducing the risk of forgetting previously acquired safe driving behaviors. Specifically, The MAS method introduces a weight importance modulation factor for each weight parameter. This factor dynamically adjusts the learning rate applied to the weight throughout the training process. Weights with greater importance for past tasks receive a lower modulation factor (closer to 0), leading to slower updates and a decreased likelihood of forgetting the knowledge stored in those weights. The weight importance in MAS can be represented as:\n$\\Omega_i = \\frac{1}{N}\\sum_{k=1}^N ||g_i(x_k)||$ \nwhere $g_i (x_k)$ denotes the gradient of the learned function concerning the parameter $\\theta_i$ evaluated at the data point $x_k$, and N represents the total number of data points in a given phase. After determining the weight importance $\\Omega_i$, the MAS loss function is formulated as:\n$L_{MAS} (\\theta) = L_{new} (\\theta) + \\lambda \\sum_i \\Omega_i (\\theta_i - \\theta_i^*)^2$ \nThrough the dynamic adjustment of learning rates based on weight importance, MAS ensures that the model concentrates on updating less critical weights during the learning of new tasks. This approach helps alleviate catastrophic forgetting and facilitates the continuous learning of new traffic patterns while safeguarding previously acquired safe driving behaviors."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "We've chosen the Waymo and Lyft datasets with 20,724 car-following events based on criteria outlined in our previous work [29]. These datasets offer rich sensor data derived from autonomous vehicles with diverse sensors and cameras, providing a comprehensive view of traffic environments; and diverse traffic scenarios encompassing urban and expressway scenarios, which is essential for training a robust model.\nTo evaluate the continual learning capabilities of our model, we further divided the dataset into three distinct task sets based on different speed characteristics. This division is achieved by leveraging the percentiles of the mean speed of the FV across all events in the dataset, with the 33.3rd and 66.7th percentiles serving as splitting points. The 33.3rd and 66.7th percentiles of the mean FV speed are 9.86 m/s and 12.46 m/s, respectively, resulting in three task sets with the following characteristics:\n\u2022 Task Set 1: Mean FV speed range of (12.46,\u221e] m/s.\n\u2022 Task Set 2: Mean FV speed range of (9.86, 12.46] m/s.\n\u2022 Task Set 3: Mean FV speed range of [0,9.86] m/s.\nThe distributions of all task sets are depicted in Fig. 3. This approach ensures each task set focuses on a distinct speed range, enabling assessment of the model's adaptability to new traffic patterns with differing speed characteristics. Finally, adhering to standard training practices, each task set is further divided into training (70%), validation (15%), and testing (15%) sets."}, {"title": "B. Evaluation Metrics", "content": "This section details the metrics employed to evaluate the performance of the model compared to the baseline models."}, {"title": "1) Metrics for Learning Performance", "content": "We employ Mean Squared Errors (MSEs) in spacing and speed to quantify the difference between the predicted values of the FV generated by the model and the corresponding ground-truth values from the dataset.Lower MSE values indicate better agreement between the model's predictions and the actual traffic behavior, reflecting the model's ability to learn effectively. The MSE for a set of predictions $Y_{pred}$ and corresponding ground-truth values $Y_{true}$ can be calculated using the following formula:\n$MSE = \\frac{1}{N}\\sum_{i=1}^N (Y_{pred,i} - Y_{true,i})^2$ \nwhere N represents the total number of events in the testing dataset and $Y_{pred,i}$ and $Y_{true,i}$ represent the i-th predicted value and ground-truth value, respectively.\nTo evaluate the continual learning capabilities of our model, we calculate MSEs for all trained task sets with both the baseline model and the continual learning model at different stages of training:\n\u2022 Stage 1: After training on Task Set 1 (initial learning)\n\u2022 Stage 2: After training on Task Sets 1 and 2 (incremental learning)\n\u2022 Stage 3: After training on all three task sets (final performance)\nThis approach allows us to assess how well the model learns from each task set and retains its knowledge when introduced to new traffic patterns."}, {"title": "2) Metrics for Safety Performance", "content": "We evaluate the safety performance of the car-following models by calculating the collision rate. This metric represents the frequency of collisions occurring during simulated test runs with the model controlling the SV. A lower collision rate indicates a safer car-following behavior by the model. Mathematically, the collision rate can be expressed as:\n$Collision Rate = \\frac{No. of Collisions}{Total No. of Events} \\times 100\\%$"}, {"title": "C. Results", "content": "This section presents the evaluation results of the proposed continual learning car-following models: continual learning with Elastic Weight Consolidation (CL-EWS) and continual learning with Memory Aware Synapses (CL-MAS). These models take the speed of the SV, the speed of the LV, the relative speed between them, and the spacing between them as inputs, and predict the appropriate acceleration value for the SV. We compare them to a continual learning baseline (CL-Baseline) and a standard LSTM model trained on all tasks simultaneously.\nFigure 4 illustrates the training flow for each model. The CL-Baseline is trained incrementally on the three task sets one by one. However, it lacks any additional mechanism to address catastrophic forgetting. In contrast, CL-EWS and CL-MAS also undergo incremental training on the three tasks, but they incorporate an additional loss term during training. The specific loss terms used by CL-EWS and CL-MAS are detailed in Equations 3 and 4, respectively."}, {"title": "1) Training Hyperparameters", "content": "For all LSTM models (regular LSTM, CL-Baseline, CL-EWS, and CL-MAS), we employed the following hyperparameters to ensure a consistent evaluation environment:\n\u2022 Total epochs: 5\n\u2022 Historical horizon: 10 (considering past 10 timesteps)\n\u2022 Learning rate: 0.001\n\u2022 Batch size: 32"}, {"title": "2) Performance on Individual Tasks", "content": "Tables I and II show the MSEs in spacing and speed for all models across the three tasks at different training stages.\nThe CL-Baseline suffers from catastrophic forgetting, with significant MSE increases in both spacing and speed after each new task. Compared to stage 1, the final MSEs increase by 253.5% and 292.1% for spacing and speed, respectively. This highlights the limitations of traditional continual learning approaches.\nBoth CL-EWS and CL-MAS models exhibit substantially lower MSE increases compared to the baseline. However, some interesting observations can be made regarding the differences between CL-EWS and CL-MAS. CL-EWS shows generally lower MSE in both spacing and speed across all stages, particularly for Task Set 1. This suggests that CL-EWS might be more effective in mitigating forgetting for entirely new tasks. On the other hand, CL-MAS performs better for Task Set 2, potentially indicating its ability to adapt to variations within previously encountered scenarios.\nWhile both CL-EWS and CL-MAS achieve good overall performance, a more in-depth analysis is needed to fully understand the reasons behind the observed differences. Here are some potential factors to consider:\n\u2022 Hyperparameter tuning: The effectiveness of EWC and MAS can be sensitive to hyperparameter settings. It's"}, {"title": "3) Key Observations", "content": "CL-EWS and CL-MAS maintain safe following distances: As evident in Fig. 5, the trajectories of CL-EWS (purple) and CL-MAS (brown) consistently maintain a safe distance behind the LV throughout the scenarios. This aligns with the 0% collision rates observed in Table III.\n\u2022 CL-EWS might be more cautious in unfamiliar situations: While both models achieve safe following, CL-EWS trajectories appear slightly more conservative, particularly in areas with sudden changes (e.g., lane merging in Fig. 5c). This reinforces the idea that CL-EWS prioritizes retaining knowledge of entirely new tasks.\n\u2022 CL-MAS demonstrates adaptability within safe margins: CL-MAS trajectories (brown) show a slightly more dynamic behavior compared to CL-EWS, particularly in Task Set 2 scenarios (Fig. 5b). This suggests CL-MAS's ability to adapt to variations in following distances while maintaining safety.\nThese observations are consistent with the findings from the MSE analysis. CL-EWS might be more conservative in its learning approach, prioritizing safety in entirely new situations. CL-MAS demonstrates a stronger ability to adapt to variations within similar scenarios while also achieving safe following behavior. This suggests a potential benefit of CL-MAS for handling continuously evolving traffic conditions, as it can adapt to new situations while maintaining performance on previously learned ones.\nWhile both CL-EWS and CL-MAS exhibit significant safety improvements, it's important to acknowledge limitations. The current analysis focuses on simulated scenarios. Real-world traffic conditions can be far more complex and unpredictable. Future work can involve testing these models in more comprehensive driving simulation environments or potentially even on real-world datasets (if available while ensuring privacy concerns are addressed). Additionally, research can explore techniques to further enhance the safety and adaptability of these continual learning models for car-following tasks in autonomous driving systems."}, {"title": "V. CONCLUSION", "content": "This work addressed the challenge of catastrophic forgetting in car-following models by proposing a novel continual learning framework. Our approach leverages EWC and MAS to enable continual learning from new traffic data while retaining previously acquired knowledge of safe driving behaviors. Evaluations on a comprehensive dataset demonstrate that the proposed CL-EWS and CL-MAS models significantly outperform the baseline, achieving lower errors and maintaining a perfect safety record (0% collision rate) across all traffic scenarios. These results highlight the potential of continual learning techniques in developing robust and adaptable car-following models, paving the way for safer and more reliable autonomous driving systems. Future research directions include exploring additional continual learning methods and integrating this framework with other components of an autonomous driving system."}]}