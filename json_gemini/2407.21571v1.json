{"title": "PMoE: Progressive Mixture of Experts with Asymmetric Transformer for Continual Learning", "authors": ["Min Jae Jung", "JooHee Kim"], "abstract": "Large Language Models (LLMs) encounter significant challenges in continual learning due to catastrophic forgetting, where new information overwrites previously acquired knowledge. This limitation leads to substantial environmental and economic waste. In this study, we introduce the PMOE, Progressive Mixture of Experts with Asymmetric Transformer, which aims to minimize forgetting by utilizing an asymmetric design with shallow layers dedicated to general knowledge and deep layers for new knowledge. PMOE incorporates progressively added experts in deep layers and a router that allocates new knowledge to the appropriate experts efficiently. The router, positioned adjacent to the deep layers, utilizes deep features aggregating consolidated information. This enables the router to perform efficiently, allocating new knowledge to the appropriate experts, which progressively increase in the deep layers. Extensive experiments on TRACE datasets and general language understanding datasets demonstrate that the proposed PMOE outperforms previous state-of-the-art approaches.", "sections": [{"title": "Introduction", "content": "Although Large Language Models (LLMs) have advanced significantly (Brown et al., 2020; Touvron et al., 2023), unlike humans, they often forget previously acquired knowledge during continuous learning (Wu et al., 2022; Luo et al., 2023; Dou et al., 2024). This phenomenon, known as catastrophic forgetting (McCloskey and Cohen, 1989), results in substantial environmental and economic inefficiencies, particularly for generative LLMs that require extensive data for training according to scaling laws (Kaplan et al., 2020). Consequently, research in continuous learning is crucial for the efficient recycling of language models. This paper focuses on developing techniques that preserve existing knowledge while simultaneously acquiring new information.\nExisting continual learning approaches can be divided into three approaches: replay-based (Lopez-Paz and Ranzato, 2017; de Masson D'Autume et al., 2019a), regularization-based (Kirkpatrick et al., 2017), and architecture-based (Wang et al., 2023d; Razdaibiedina et al., 2023). Replay-based and regularization-based methods use a memory buffer with examples from previous tasks or add regularization constraints to penalize changes in important weights. However, they still suffer from forgetting due to continuous adjustments in parameters. Recently proposed architecture-based approaches (Wang et al., 2023b; Razdaibiedina et al., 2023) enhance performance in a parameter-efficient manner but still struggle with the trade-off between learning plasticity and memory stability (Wang et al., 2024) or require prior information such as task-ID to classify input text into task-specific parameters.\nIn this study, we propose the Progressive Mixture of Experts with Asymmetric Transformer (PMoE), which efficiently adapts to new tasks while minimizing the forgetting of existing knowledge within a continual learning framework. The key idea of PMOE is its asymmetric depth design: shallow layers retain general knowledge, while deeper layers acquire new task-specific knowledge. To achieve this, PMOE progressively adds experts (Rusu et al., 2016) and employs a routing network to classify input text.\nWe take inspiration from (Wang et al., 2023a), which shows that LLMs aggregate information in shallow layers around important words and distribute it in deep layers. We hypothesize that the performance of the router is improved when using features from deep layers rather than shallow layers. In P\u041c\u041e\u0415, specialized experts are absent in shallow layers but present in deep layers, enabling the model to benefit from parameter efficiency and robust routing performance.\nThe main contributions of our work are summarized as follows:\n\u2022\n\u2022\nWe propose PMOE, which efficiently adapts to new tasks for continual learning by introducing an asymmetric depth design.\n\u2022\nOur method outperforms LoRA with replay-based and prior state-of-the-art methods on the TRACE benchmark.\n\u2022 Experimental results demonstrate that our asymmetric design is both effective and parameter-efficient in preserving prior knowledge while adapting to new knowledge."}, {"title": "Background", "content": ""}, {"title": "Problem Statement", "content": "In a continual learning setup, a model sequentially encounters datasets $D_1,..., D_T$, where each dataset $D_t = (x_t, y_t)$ consists of data points specific to task t. Concretely, the model is provided only with $D_t$ during training for task t. The primary challenge in continual learning is to enable the model to retain previously acquired knowledge while effectively learning new tasks. In this study, we adopt the general setup of continual learning, which involves maximizing the cumulative performance across all tasks as follows:\n$\\max_{\\theta} \\sum_{k=1}^{T} \\sum_{x,y\\in D_k} log p_\\theta (y|x)$ (1)\nThe overall performance (OP) and backward transfer score (BWT) are measured immediately following the completion of training for the t-th task, as follows:\n$OP_t = \\frac{1}{t} \\sum_{i=1}^{t} R_{t,i}$ (2)\n$BWT_t = \\frac{1}{t} \\sum_{i=1}^{t} (R_{t,i} - R_{i,i})$"}, {"title": "LORA", "content": "Recently, many Parameter Efficient Tuning (PET) approaches have been proposed and widely adopted in fine-tuning settings. These methods aim to efficiently adapt pre-trained models by updating a smaller subset of parameters than updating all. Among these approaches, Low-Rank Adaptation (LoRA) (Hu et al., 2021) hypothesizes that weight updates in pre-trained models have a \"low intrinsic dimension.\" Instead of updating the entire pre-trained weight $W_o\\in R^{d \\times k}$, LoRA updates decomposed bottleneck weights $B\\in R^{d \\times r}$ and $A\\in R^{r\\times k}$, where $W_o + \\Delta W = W_o + BA$ and the rank $r < min(d, k)$. Thus, the forward pass $h = W_ox$ becomes\n$h = W_ox + \\Delta W x = W_ox + BAx$. (3)\nIn this paper, we utilize LoRA as the expert component of our model."}, {"title": "Asymmetric Mixture-of-Experts", "content": ""}, {"title": "Overall Architecture", "content": "To address the issue of information flow within transformer blocks, we propose an asymmetric Mixture-of-Experts (MoE) architecture as illustrated in Figure 1. By introducing a threshold T, the overall network $f_\\theta$ is divided into shallow layers $f_{\\theta | 1=1,...,\\tau}$ and deep layers $f_{\\theta | 1=(\\tau+1),...,n}$. The shallow transformer blocks employ a single LoRA $(BA)^l$ aligned with general human instruction-following, whereas deeper blocks utilize a multi-head LORA $(BA)^l$ aligned with specialized abilities, assigning experts $k \\in 1, ..., T$.\nAt the boundary between shallow and deep layers, the router distributes hidden features"}, {"title": "Experimental Setup", "content": ""}, {"title": "TRACE Dataset for Continual Learning Evaluation", "content": "To evaluate fine-tuning as a scheme for continual learning, we adopt the TRACE"}, {"title": "General Ability Evaluation", "content": "To assess the impact of domain-specific tasks on the generality performance of pre-trained models, we evaluate five well-known benchmark datasets: MMLU (Hendrycks et al., 2021), GSM (Cobbe et al., 2021), BBH (Suzgun et al., 2022), BoolQ (Clark et al., 2019), and PiQA (Bisk et al., 2019). Following the experimental setup of previous work (Wang et al., 2023c), we measure the general ability delta, which represents the performance difference between pre-continual learning and post-continual learning, as follows:\n$\\Delta R_t^G = \\frac{1}{M} \\sum_{i=1}^{M} (R_t^G - R_{t_i}^G)$, (6)\nwhere M refers to the total number of benchmarks, and $R_{t_i}^G$ refers to the result on benchmark i after the t-th tuning."}, {"title": "Implementation Detail", "content": "Our code implementation builds upon the PEFT (Mangrulkar et al., 2022) library. We employ Llama2-7b for our pre-trained weights and LORA for our experts. We optimize our model using AdamW with a minibatch size of 128, a learning rate of 3e-4, and a cosine annealing scheduler. The hyper-parameter rank and \u03c4 are set to 4 and 24, respectively. At inference time, the model generates text sequences without knowing the task ID to which it belongs. All results are averaged over 3 runs.\nFollowing the settings of previous studies (Wang et al., 2023c), we incorporate alignment data from LIMA (Zhou et al., 2023) into the replay memory, replaying only 1% of historical data."}, {"title": "Baselines", "content": "We compare PMOE with six baseline approaches. For consistency in experimental comparison, the foundation of all baselines is pre-trained Llama2-7b (Touvron et al., 2023). In Parameter Efficient Tuning (PET) comparisons, the number of trainable parameters is approximately consistent (\u2264 0.1%).\n\u2022\nLlama2-7b* (Touvron et al., 2023): uses in-context learning with 6-shot on the current task without fine-tuning.\n\u2022\nFull-ft (de Masson D'Autume et al., 2019b): fine-tunes full parameters on sequential tasks.\n\u2022\nLORA (Hu et al., 2021): fine-tunes fixed-size LORA on sequential tasks.\n\u2022\nO-LORA (Wang et al., 2023b): incrementally learns new tasks in a direction orthogonal to the LORA subspace of past tasks while fixing the previous parameters.\n\u2022\nFull-ft-RE: fine-tunes full parameters with a memory buffer and replays samples from old tasks.\n\u2022\nLORA-RE: fine-tunes fixed-size LORA with a memory buffer and replays samples from old tasks, following the same memory setting as Full-ft-RE."}, {"title": "Main Result", "content": "The comparative analysis is presented in Table 1. The upper part of the table represents the general capabilities of models that have undergone continuous learning to completion, while the lower part represents the specialized performance of models using the evaluation set from the data employed in continual learning. Furthermore, the left side of the table presents models that utilize in-context learning and those without the replay-based approach, whereas the right side presents models that incorporate the replay-based method.\nUtilizing replay generally enhances performance, even with only 1% of examples."}, {"title": "Discussion", "content": ""}, {"title": "Asymmetric Design", "content": "The core strategy of the PMoE architecture is characterized by its asymmetric design, where shallow layers are dedicated to general abilities and deeper layers are tailored for newly acquired abilities. To assess the efficacy of this asymmetric design, we analyze the performance as functions of varying \u03c4. A lower \u03c4 results in the model possessing a greater number of task-specific parameters, with the routing mechanism utilizing shallow features. Conversely, a higher 7 leads to a model enriched with parameters for general abilities, where the router utilizes deep features. As illustrated in Figure 2, the model demonstrates optimal average performance at a moderate 7, and a decrease in computational cost proportional to \u03c4. Thus, if performance levels are comparable, it is advantageous to increase T to reduce computational demands."}, {"title": "Router Analysis", "content": "To investigate the router in PMOE, we show matrices representing the allocation probability by the router. As shown in Figure 3, when the router is located near the shallow layer (i.\u0435., \u0442 = 6), the router mainly chooses only a few experts. This imbalance problem is a well-known phenomenon where input x is assigned to a specific superior expert, rather than being distributed across a diverse range of experts. Previous works have attempted to address the router imbalance problem by introducing auxiliary losses (Shazeer et al., 2017), token capacity (Lepikhin et al., 2020), or expert dropout (Zhu et al., 2023). However, when the router is located near the deep layer (i.\u0435., \u0442 = 24), the imbalance problem is automatically solved, well distributing all experts. As mentioned in the motivation of our study, this result supports the hypothesis that LLMs aggregate information in shallow layers and distribute it in deep layers, leading to better router performance when using features from deep layers rather than shallow layers.\nTo further investigate, we conducted a qualitative evaluation of the token allocation by the router, as illustrated in Figure 5. The evaluation utilized test prompts that include three distinct types: Python code, mathematical questions and answers, and lines from Shakespeare's Hamlet. With a\u0442 of 6, most tokens are predominantly allocated to experts 2 or 8. In contrast, at 7 of 24, the allocation patterns are more varied, indicating a comprehensive use of experts. Concretely, the router seems to allocate tokens to experts based on identifiable patterns related to the role of the tokens. For example, special characters are predominantly assigned to expert 3, elements of Python syntax are mainly assigned to expert 4, and plain text is typically assigned to expert 1.\nDrawing from the aforementioned analysis, if Tis 24 and the task-ID is treated as the ground truth for the router during the training stage, PMOE is capable of employing each expert as a task-specific expert without the need for task-ID during the inference stage. We incorporate cross-entropy loss to guide the routing of input x to a suitable expert, as follows:\n$L(x, k) = - log G_k (x)$ (7)\nwhere $G_k(x)$ denotes the router probability of assigning input x to its corresponding task-ID k. This loss provides a simple yet effective mechanism to prevent inputs from being pushed to a single expert by guiding the task assignment corresponding to input x.\nThe comparison with the application of Equation 7 is shown in Figure 4. This auxiliary loss function experimentally demonstrates its effectiveness in making experts task-specific without task-ID in the inference stage. However, in Table 2, applying the auxiliary loss slightly degrades overall performance. This result suggests that a combination of experts rather than task-specific experts has a positive effect on overall performance. Future research should explore extending task-specific experts for asymmetric design."}, {"title": "Comparison with Trainable Parameter Size", "content": "We assessed the detailed comparison between PMOE and LoRA with the replay method at different scales of adaptation parameters, as shown in Table 3. In our evaluation, we control the number of parameters by the rank of the adapter from under 0.1% to over 0.3%. In all parameter size settings, the general ability of PMOE is higher than that of LoRA by 0.7%-2.2%. In specialization performance, PMOE is higher than LoRA by 0.6%-0.8%. Interestingly, there is only a slight difference in performance along with the parameter size."}, {"title": "Related Works", "content": ""}, {"title": "Parameter Efficient Fine Tuning", "content": "As larger pre-trained models become more prevalent, efficiently fine-tuning their parameters has gained significant importance. Parameter Efficient Fine Tuning (PEFT) approaches are a set of methods that update only a small portion of parameters while freezing the pre-trained parameters during the fine-tuning stage. Various approaches have been proposed, including inserting additional modules or parameters, such as Adapter (Houlsby et al., 2019), Prompt Tuning (Lester et al., 2021), and Low-Rank Adaptation (LoRA) (Hu et al., 2021).\nIn this paper, we focus primarily on LoRA, which hypothesizes that weight updates in pre-trained models have a \"low intrinsic dimension.\" Building upon LoRA, AdaLoRA parameterizes incremental updates in the form of singular value decomposition (Zhang et al., 2023), O-LORA involves layering low-rank adapters on the key and value projection matrices (Wang et al., 2023b), SIRA leverages the Sparse Mixture of Experts to boost the performance of LORA (Zhu et al., 2023), and PMoE employs an asymmetric design with shallow layers dedicated to general knowledge and deeper layers for new knowledge."}, {"title": "Continual Learning", "content": "Existing continual learning approaches can be categorized into three main types: replay-based, regularization-based, and architecture-based. Replay-based approaches(Lopez-Paz and Ranzato, 2017; de Masson D'Autume et al., 2019a) store examples from previous tasks for rehearsal. Although these approaches achieve strong performance, storing large samples can be inefficient and may not be feasible for data privacy settings.\nRegularization-based approaches (Kirkpatrick et al., 2017; Li and Hoiem, 2017) update only a few weights trained on previous tasks. These approaches are memory efficient but can suffer from catastrophic forgetting, especially in long sequence tasks. Architecture-based approaches isolate weights trained on previous tasks and progressively add weights for new tasks. Although the recently proposed progressive prompt (Razdaibiedina et al., 2023) has achieved state-of-the-art results, it has the limitation of requiring the task-ID during the inference stage. Among the continual learning techniques that do not require task-ID, O-LORA (Wang et al., 2023b)"}, {"title": "Limitations", "content": "In our study, we conducted experiments across eight distinct tasks. Expanding the number of tasks could enhance the robustness and generalizability of our findings. Our research focuses solely on generative models, particularly Llama2-7b. However, the foundational concepts of our model hold potential for broader applicability in task-agnostic settings. Further research will assess the efficacy of our proposed approach across diverse benchmarks, aiming to broaden its application to multimodal continual learning."}, {"title": "Conclusion", "content": "In this paper, we enhanced the performance of LLM in continual learning by using PMOE. The PMOE features an asymmetric architecture between shallow and deep layers. Specifically, the shallow part employs a single expert to enhance general ability, whereas the deep part progressively utilizes an increasing number of experts to focus on tuned performance. Empirical experiments on comprehensive continual learning benchmarks demonstrate that the proposed PMoE outperforms previous state-of-the-art approaches. Furthermore, to investigate the asymmetric structure, we experimented by varying the asymmetric hyperparameter \u315c, measuring the performance and efficiency of the model, and analyzing the router."}]}