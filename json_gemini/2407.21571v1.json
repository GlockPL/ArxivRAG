{"title": "PMoE: Progressive Mixture of Experts with Asymmetric Transformer for Continual Learning", "authors": ["Min Jae Jung", "JooHee Kim"], "abstract": "Large Language Models (LLMs) encounter significant challenges in continual learning due to catastrophic forgetting, where new information overwrites previously acquired knowledge. This limitation leads to substantial environmental and economic waste. In this study, we introduce the PMOE, Progressive Mixture of Experts with Asymmetric Transformer, which aims to minimize forgetting by utilizing an asymmetric design with shallow layers dedicated to general knowledge and deep layers for new knowledge. PMOE incorporates progressively added experts in deep layers and a router that allocates new knowledge to the appropriate experts efficiently. The router, positioned adjacent to the deep layers, utilizes deep features aggregating consolidated information. This enables the router to perform efficiently, allocating new knowledge to the appropriate experts, which progressively increase in the deep layers. Extensive experiments on TRACE datasets and general language understanding datasets demonstrate that the proposed PMOE outperforms previous state-of-the-art approaches.", "sections": [{"title": "1 Introduction", "content": "Although Large Language Models (LLMs) have advanced significantly (Brown et al., 2020; Touvron et al., 2023), unlike humans, they often forget previously acquired knowledge during continuous learning (Wu et al., 2022; Luo et al., 2023; Dou et al., 2024). This phenomenon, known as catastrophic forgetting (McCloskey and Cohen, 1989), results in substantial environmental and economic inefficiencies, particularly for generative LLMs that require extensive data for training according to scaling laws (Kaplan et al., 2020). Consequently, research in continuous learning is crucial for the efficient recycling of language models. This paper focuses on developing techniques that preserve existing knowledge while simultaneously acquiring new information.\nExisting continual learning approaches can be divided into three approaches: replay-based (Lopez-Paz and Ranzato, 2017; de Masson D'Autume et al., 2019a), regularization-based (Kirkpatrick et al., 2017), and architecture-based (Wang et al., 2023d; Razdaibiedina et al., 2023). Replay-based and regularization-based methods use a memory buffer with examples from previous tasks or add regularization constraints to penalize changes in important weights. However, they still suffer from forgetting due to continuous adjustments in parameters. Recently proposed architecture-based approaches (Wang et al., 2023b; Razdaibiedina et al., 2023) enhance performance in a parameter-efficient manner but still struggle with the trade-off between learning plasticity and memory stability (Wang et al., 2024) or require prior information such as task-ID to classify input text into task-specific parameters.\nIn this study, we propose the Progressive Mixture of Experts with Asymmetric Transformer (PMoE), which efficiently adapts to new tasks while minimizing the forgetting of existing knowledge within a continual learning framework. The key idea of PMOE is its asymmetric depth design: shallow layers retain general knowledge, while deeper layers acquire new task-specific knowledge. To achieve this, PMOE progressively adds experts (Rusu et al., 2016) and employs a routing network to classify input text.\nWe take inspiration from (Wang et al., 2023a), which shows that LLMs aggregate"}, {"title": "2 Background", "content": "2.1 Problem Statement\nIn a continual learning setup, a model sequentially encounters datasets D1,..., DT, where each dataset Dt = (xt, yt) consists of data points specific to task t. Concretely, the model is provided only with Dt during training for task t. The primary challenge in continual learning is to enable the model to retain previously acquired knowledge while effectively learning new tasks. In this study, we adopt the general setup of continual learning, which involves maximizing the cumulative performance across all tasks as follows:\n$\\max \\sum_{k=1}^T log p_\\theta (y|x)$\n           \\n$\\x,y\\in D_k$\nThe overall performance (OP) and backward transfer score (BWT) are measured immediately following the completion of training for the t-th task, as follows:\n$OP_t = \\frac{1}{t}\\sum_{i=1}^t R_{t,i}$\n$BWT_t = \\frac{1}{t}\\sum_{i=1}^t (R_{t,i}^t - R_{t,i})$\nwhere $R$ refers to the score of the i-th task of the model after completing training on the t-th task (i < t). Furthermore, we simulate a realistic setting by having the model generate text sequences without task ID at test time.\n2.2 LORA\nRecently, many Parameter Efficient Tuning (PET) approaches have been proposed and widely adopted in fine-tuning settings. These methods aim to efficiently adapt pre-trained models by updating a smaller subset of parameters than updating all. Among these approaches, Low-Rank Adaptation (LoRA) (Hu et al., 2021) hypothesizes that weight updates in pre-trained models have a \"low intrinsic dimension.\" Instead of updating the entire pre-trained weight Wo\u2208 Rd\u00d7k, LoRA updates decomposed bottleneck weights B\u2208 Rdxr and A\u2208 Rr\u00d7k, where Wo + AW = Wo + BA and the rank r < min(d, k). Thus, the forward pass h = Wox becomes\nh = Wox + \\Delta Wx = Wox + BAx\nIn this paper, we utilize LoRA as the expert component of our model."}, {"title": "3 Asymmetric Mixture-of-Experts", "content": "3.1 Overall Architecture\nTo address the issue of information flow within transformer blocks, we propose an asymmetric Mixture-of-Experts (MoE) architecture as illustrated in Figure 1. By introducing a threshold T, the overall network fe is divided into shallow layers fo1=1,..., and deep layers 01=(1+1),...,n. The shallow transformer blocks employ a single LoRA (BA)\u00b9 aligned with general human instruction-following, whereas deeper blocks utilize a multi-head LORA (BA) aligned with specialized abilities, assigning experts k \u2208 1, ..., T.\nAt the boundary between shallow and deep layers, the router distributes hidden features"}, {"title": "3.2 Experimental Setup", "content": "3.2.1 TRACE Dataset for Continual Learning Evaluation\nTo evaluate fine-tuning as a scheme for continual learning, we adopt the TRACE"}, {"title": "3.2.2 General Ability Evaluation", "content": "To assess the impact of domain-specific tasks on the generality performance of pre-trained models, we evaluate five well-known benchmark datasets: MMLU (Hendrycks et al., 2021), GSM (Cobbe et al., 2021), BBH (Suzgun et al., 2022), BoolQ (Clark et al., 2019), and PiQA (Bisk et al., 2019). Following the experimental setup of previous work (Wang et al., 2023c), we measure the general ability delta, which represents the performance difference between pre-continual learning and post-continual learning, as follows:\n$ARG = \\frac{1}{M} \\sum_{i=1}^M (R_{t_0}^i - R_{t_1}^i)$,\nwhere M refers to the total number of benchmarks, and Re refers to the result on benchmark i after the t-th tuning."}, {"title": "3.2.3 Implementation Detail", "content": "Our code implementation builds upon the PEFT (Mangrulkar et al., 2022) library. We employ Llama2-7b for our pre-trained weights and LORA for our experts. We optimize our model using AdamW with a minibatch size of 128, a learning rate of 3e-4, and a cosine annealing scheduler. The hyper-parameter rank and \u03c4 are set to 4 and 24, respectively. At inference time, the model generates text sequences without knowing the task ID to which it belongs. All results are averaged over 3 runs.\nFollowing the settings of previous studies (Wang et al., 2023c), we incorporate alignment"}, {"title": "3.2.4 Baselines", "content": "We compare PMOE with six baseline approaches. For consistency in experimental comparison, the foundation of all baselines is pre-trained Llama2-7b (Touvron et al., 2023). In Parameter Efficient Tuning (PET) comparisons, the number of trainable parameters is approximately consistent (\u2264 0.1%).\n\u2022 Llama2-7b* (Touvron et al., 2023): uses in-context learning with 6-shot on the current task without fine-tuning.\n\u2022 Full-ft (de Masson D'Autume et al., 2019b): fine-tunes full parameters on sequential tasks.\n\u2022 LORA (Hu et al., 2021): fine-tunes fixed-size LORA on sequential tasks.\n\u2022 O-LORA (Wang et al., 2023b): incrementally learns new tasks in a direction orthogonal to the LORA subspace of past tasks while fixing the previous parameters.\n\u2022 Full-ft-RE: fine-tunes full parameters with a memory buffer and replays samples from old tasks.\n\u2022 LORA-RE: fine-tunes fixed-size LORA with a memory buffer and replays samples from old tasks, following the same memory setting as Full-ft-RE."}, {"title": "3.3 Main Result", "content": "The comparative analysis is presented in Table 1. The upper part of the table represents the general capabilities of models that have undergone continuous learning to completion, while the lower part represents the specialized performance of models using the evaluation set from the data employed in continual learning. Furthermore, the left side of the table presents models that utilize in-context learning and those without the replay-based approach, whereas the right side presents models that incorporate the replay-based method.\nUtilizing replay generally enhances performance, even with only 1% of examples."}, {"title": "4 Discussion", "content": "4.1 Asymmetric Design\nThe core strategy of the PMoE architecture is characterized by its asymmetric design, where shallow layers are dedicated to general"}, {"title": "4.2 Router Analysis", "content": "To investigate the router in PMOE, we show matrices representing the allocation probability by the router. As shown in Figure 3, when the router is located near the shallow layer (i.\u0435., \u0442 = 6), the router mainly chooses only a few experts. This imbalance problem is a well-known phenomenon where input x is assigned to a specific superior expert, rather than being distributed across a diverse range of experts. Previous works have attempted"}, {"title": "4.3 Comparison with Trainable Parameter Size", "content": "We assessed the detailed comparison between PMOE and LoRA with the replay method at different scales of adaptation parameters, as shown in Table 3. In our evaluation, we control the number of parameters by the rank of the adapter from under 0.1% to over 0.3%. In all parameter size settings, the general ability of PMOE is higher than that of LoRA by 0.7%-2.2%. In specialization performance, PMOE is higher than LoRA by 0.6%-0.8%. Interestingly, there is only a slight difference in performance along with the parameter size."}, {"title": "5 Related Works", "content": "5.1 Parameter Efficient Fine Tuning\nAs larger pre-trained models become more prevalent, efficiently fine-tuning their"}, {"title": "5.2 Continual Learning", "content": "Existing continual learning approaches can be categorized into three main types: replay-based, regularization-based, and architecture-based. Replay-based approaches(Lopez-Paz and Ranzato, 2017; de Masson D'Autume et al., 2019a) store examples from previous tasks for rehearsal. Although these approaches achieve strong performance, storing large samples can be inefficient and may not be feasible for data privacy settings.\nRegularization-based approaches (Kirkpatrick et al., 2017; Li and Hoiem, 2017) update only a few weights trained on previous tasks. These approaches are memory efficient but can suffer from catastrophic forgetting, especially in long sequence tasks.\nArchitecture-based approaches isolate weights trained on previous tasks and progressively add weights for new tasks. Although the recently proposed progressive prompt (Razdaibiedina et al., 2023) has achieved state-of-the-art results, it has the limitation of requiring the task-ID during the inference stage. Among the continual learning techniques that do not require task-ID, O-LORA (Wang et al., 2023b)"}, {"title": "7 Conclusion", "content": "In this paper, we enhanced the performance of LLM in continual learning by using PMOE. The PMOE features an asymmetric architecture between shallow and deep layers. Specifically, the shallow part employs a single expert to enhance general ability, whereas the deep part progressively utilizes an increasing number of experts to focus on tuned performance.\nEmpirical experiments on comprehensive continual learning benchmarks demonstrate that the proposed PMoE outperforms previous state-of-the-art approaches. Furthermore, to investigate the asymmetric structure, we experimented by varying the asymmetric hyperparameter \u315c, measuring the performance and efficiency of the model, and analyzing the router."}]}