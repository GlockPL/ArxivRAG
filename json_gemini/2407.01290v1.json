{"title": "Hypformer: Exploring Efficient Hyperbolic Transformer Fully in Hyperbolic Space", "authors": ["Menglin Yang", "Harshit Verma", "Delvin Ce Zhang", "Jiahong Liu", "Irwin King", "Rex Ying"], "abstract": "Hyperbolic geometry have shown significant potential in modeling complex structured data, particularly those with underlying tree-like and hierarchical structures. Despite the impressive performance of various hyperbolic neural networks across numerous domains, research on adapting the Transformer to hyperbolic space remains limited. Previous attempts have mainly focused on modifying self-attention modules in the Transformer. However, these efforts have fallen short of developing a complete hyperbolic Transformer. This stems primarily from: (i) the absence of well-defined modules in hyperbolic space, including linear transformation layers, Layer-Norm layers, activation functions, dropout operations, etc. (ii) the quadratic time complexity of the existing hyperbolic self-attention module w.r.t the number of input tokens, which hinders its scalability. To address these challenges, we propose, Hypformer, a novel hyperbolic Transformer based on the Lorentz model of hyperbolic geometry. In Hypformer, we introduce two foundational blocks that define the essential modules of the Transformer in hyperbolic space. Furthermore, we develop a linear self-attention mechanism in hyperbolic space, enabling hyperbolic Transformer to process billion-scale graph data and long-sequence inputs for the first time. Our experimental results confirm the effectiveness and efficiency of Hypformer across various datasets, demonstrating its potential as an effective and scalable solution for large-scale data representation and large models.", "sections": [{"title": "1 Introduction", "content": "In many real-world scenarios, data frequently exhibit a hierarchical or tree-like structure, either implicitly or explicitly [32, 49, 95]. This is evident in complex networks [8, 38, 78, 94], the semantics of words in natural language processing [50, 51, 63], and conceptual hierarchies in vision tasks [15, 32]. As illustrated in Figure 1, such data can be organized into large and abstract groups that encompass small and specific subgroups, which can further be subdivided into even smaller and more specific sub-subgroups, and so on. The relationships between these groups and subgroups can be effectively approximated by tree-like structures [13]. This hierarchical representation mirrors human cognitive processes [14, 28], making it an intuitive approach to data representation.\nRecent initiatives have explored the use of hyperbolic learning spaces to encode complex non-Euclidean data, achieving impressive performance in representing tree-like data [8, 24, 43, 50, 51, 62, 76, 79, 80, 83]. This success is attributed to the unique property of hyperbolic space, which expands exponentially compared to the polynomial expansion of Euclidean spaces. This property aligns hyperbolic space with the metric of trees, making it particularly suitable for representing tree-like or hierarchically structured data [38]. Despite the growing interest in hyperbolic representation and deep learning, the Transformer [1, 65, 70], a cornerstone model in the various domains, was seldom explored within the realm of hyperbolic space. Despite preliminary attempts in hyperbolic Transformers [9, 26, 61], numerous challenges remain to be addressed.\nChallenge (1): Insufficient definitions for operations in the hyperbolic Transformer. Prior works of HAN [26] and HNN++ [61] primarily concentrated on the self-attention module, yet they fell short of constructing a comprehensive Transformer architecture, lacking basic components such as LayerNorm layer and positional encoding layer. This is primarily due to the inadequate definition of fundamental operations in previous studies.\nChallenge (2): Inefficient and ineffective definitions for linear transformation in the hyperbolic Transformer. While some techniques [8, 24] employ the tangent space to achieve the linear transformation, they often necessitate frequent logarithmic and exponential mappings, heavily dependent on the tangent space at the origin. This leads to an increased computational load, accumulation of mapping errors, and unstable training procedures. Although Chen et al. [9] introduced a fully Lorentz linear transformation in hyperbolic space, it is constrained by its immutable curvature and normalization term.\nChallenge (3): Absence of a linear attention mechanism in hyperbolic Transformer. The hyperbolic self-attention mechanisms proposed by Gulcehre et al. [26], Shimizu et al. [61], and Chen et al. [9] exhibit quadratic time complexity, posing a significant challenge when handling long-sequence input and large-scale graph data.\nProposed work: In this work, we propose an efficient hyperbolic Transformer, referred to as Hypformer. In particular, to address Challenges (1) and (2), we propose two foundational blocks, Hyperbolic Transformation with Curvatures (HTC) and Hyperbolic Readjustment and Refinement with Curvatures (HRC), to build all essential modules in the hyperbolic Transformer. HTC and HRC are built on the Lorentz model of hyperbolic geometry, working directly on the hyperbolic space without frequently mapping. HTC defines the linear transformation and facilitates mapping from a hyperbolic space with one curvature to another different curvature while preserving the relative distance. HRC further enables the definition of basic operations commonly used in the Transformer, such as LayerNorm layer, activation function, dropout, and concatenation, within a hyperbolic context. To tackle Challenge (3), we introduce a self-attention mechanism in Hypformer with linear complexity, enabling efficient large-scale data processing.\nTo validate the effectiveness of the proposed methodology, we have undertaken extensive experiments across a diverse range of tasks. These include graph analysis [40, 42, 77, 85, 89], text classification [90, 91], and image classification [17, 72]. The empirical evidence gathered from these experiments indicates that the proposed method significantly reduces the GPU computation cost by a factor of 10 and concurrently halves the training time compared with the existing hyperbolic softmax attention. Furthermore, the proposed method consistently surpasses the performance of competitive baselines, yielding substantial improvements on both tree-like and non-tree-like datasets.\nContributions. In summary, this study offers the following contributions: First, we introduce two fundamental hyperbolic blocks, HTC and HRC. Building upon these, we have formulated fundamental modules for linear transformation, LayerNorm, activation function, dropout, and concatenation operations within a hyperbolic context. Second, we propose the first hyperbolic linear attention mechanism, which enables the hyperbolic Transformer to be scalable and efficient. Based on the above efforts, we construct a Hypformer\u00b9, the first comprehensive and efficient hyperbolic Transformer model fully designed to operate within hyperbolic space. Last, we extend the hyperbolic model to handle billion-level graph data for the first time, laying a crucial foundation for the application of big data and large-scale models."}, {"title": "2 Related Work", "content": "2.1 Hyperbolic Neural Networks\nRecent studies have demonstrated that hyperbolic space is particularly adept at capturing the hierarchical and tree-like structures [23, 25, 39, 41, 50, 51, 53, 63, 83]. Building on hyperbolic space, a variety of hyperbolic neural networks, HNN [24], HAN [26], HNN++ [61], HGCN [8], HGNN [43], F-HNN [9], Poincar\u00e9 Resnet [64], HGTM [91] have been developed to leverage the advantages of the hyperbolic geometry. These neural networks have obtained an impressive performance in domains like computer vision [2, 29, 32], natural language processing [4, 7, 37, 47], recommender systems [11, 62, 68, 69, 76, 80], graph learning [3, 8, 41, 43, 78, 81, 92] and so on [41, 75].\n2.2 Transformer and Hyperbolic Transformer\nIntroduced by Vaswani et al. [65], Transformer models have brought about a paradigm shift in the field of artificial intelligence. Transformer [5, 16, 18, 65] has made a tremendous impact in many fields, such as language understanding [5, 16, 54], image processing [6, 52] and graph learning [33, 55]. A well-known concern with self-attention is the quadratic time complexity, which can hinder model scalability in many settings. Efficient self-attention models are crucial in applications that model long sequences [27, 31, 36, 59].\nDespite these advancements, existing Transformer architectures predominantly operate within the Euclidean domain. There have been limited attempts to extend these models to hyperbolic and other non-Euclidean spaces. Gulcehre et al. [26] proposed hyperbolic attention networks, which replace the dot-product between the query and key in self-attention with a function of negative hyperbolic distance. They then utilize the Einstein midpoint to compute the attentive output with value. Similarly, Chen et al. [9] and Shimizu et al. [61] adopt similar strategies that result in the attentive output with key being based on the Lorentzian midpoint and gyromidpoint, respectively. However, these methods exhibit quadratic time complexity, limiting their scalability. Besides, they focused more on the self-attention module and did not define the essential modules, like LayerNorm in Transformer. Recently, Cho et al. [12] proposed a fully Product-Stereographic Transformer, presenting a kernelized approach to non-Euclidean attention, which is linear time complexity. However, this method heavily relies on the tangent space, necessitating frequent mappings between the tangent space and manifolds. Ermolov et al. [19] proposed mapping the last layer features obtained from a Euclidean Transformer to hyperbolic space, which essentially does not establish a true Hyperbolic Transformer. Our work aims to address these challenges and further the development of hyperbolic Transformers."}, {"title": "3 Preliminaries", "content": "In this section, we introduce concepts related to Lorentz model of hyperbolic geometry and self-attention module briefly.\n3.1 Lorentz Model of Hyperbolic Geometry\nThere are several isometric models [23, 26, 50, 51, 56, 63] of hyperbolic geometry that have been employed in prior research. In this study, we choose the Lorentz model as the foundational framework due to the numerical stability it offers [46, 51]. Also, the proposed Hypformer can be easily adapted to other hyperbolic models, as they are isometrically equivalent.\nLorentz Model. An n-dimensional Lorentz model with negative constant curvature \u03ba(\u03ba < 0) is a Riemannian manifold denoted by $\\mathbb{L}^{n,\\kappa}$. The corresponding Riemannian metric is given by $g = \\text{diag}(1/\\kappa, 1,..., 1)$. Each point in $\\mathbb{L}^{n,\\kappa}$ can be represented as $x = \\begin{bmatrix} x_t\\\\ x_s \\end{bmatrix}$ where $x \\in \\mathbb{R}^{n+1}$, $x_t \\in \\mathbb{R}$ and $x_s \\in \\mathbb{R}^{n}$. The set of points, $\\mathbb{L}^{n,\\kappa}$, that constitute the manifold are defined as\n$\\mathbb{L}^{n,\\kappa} := \\{x \\in \\mathbb{R}^{n+1} | (x, x)_\\mathcal{L} = 1/\\kappa, x_t > 0\\} .$ (1)\nHere, $(x, y)_\\mathcal{L} = -x_ty_t + x_s^T y_s = x^Tgy$ represents the Lorentzian inner product. Lorentz model, also known as the hyperboloid model, is an upper hyper-surface in an $(n + 1)$ dimensional Minkowski space with the origin point $(\\sqrt{-1/\\kappa}, 0, \u00b7 \u00b7 \u00b7, 0)$. Lorentz model has its roots in the theory of special relativity [57] and employs terminology borrowed from this field. The hyperboloid's axis of symmetry, represented by the 0-th element $x_t$, is referred to as the time-like dimension, while all other axes $x_s$ are called space-like dimensions.\nTangent Space of Lorentz Model. Given $x \\in \\mathbb{L}^{n,\\kappa}$, the tangent space $\\mathbb{L}^{n,\\kappa} := \\{u \\in \\mathbb{R}^{n+1} | (u, x)_\\mathcal{L} = 0\\}$ is the orthogonal space of $\\mathbb{L}^{n,\\kappa}$ at $x$ with respect to the Lorentzian inner product. To achieve the mapping from the Lorentz model to the tangent space at x, we can use the logarithmic map, $\\log_x: \\mathbb{L}^{n,\\kappa} \\rightarrow T_x\\mathbb{L}^{n,\\kappa}$. The exponential map defines the inverse process, $\\exp_x: T_x\\mathbb{L}^{n,\\kappa} \\rightarrow \\mathbb{L}^{n,\\kappa}$. For the details about exponential, logarithmic maps and the relevant distance functions, please refer to Appendix A.\n3.2 Self-Attention Module\nWe first examine the general form of self-attention in Euclidean Transformers. Given the input of N tokens $X \\in \\mathbb{R}^{N\\times d}$, within each head, self-attention can be expressed as:\n$Q = XW^Q, K = XW^K, V = XW^V, \\\\\nZ_i = \\sum_{j=1}^N \\frac{\\text{Sim}(Q_i, K_j)}{\\sum_{k=1}^N \\text{Sim}(Q_i, K_k)}V_j,$ (2)\nwhere $W^Q, W^K, W^V \\in \\mathbb{R}^{d\\times d'}$ are projection matrices and $\\text{Sim}(\\cdot, \\cdot)$ denotes the similarity function. Modern Euclidean Transformers primarily use Softmax attention [65] where similarity is calculated as $\\text{Sim}(Q_i, K_j) = \\exp(Q_iK_j^T/\\sqrt{d})$. In this scenario, the attention map is derived by computing the similarity between all query-key pairs, which results in a computational complexity of $O (N^2)$.\nThe concept of hyperbolic self-attention, as defined by previous works [9, 26, 61], bears a similar idea to Equation (2). Figure 2 presents an illustration for this hyperbolic operation on Lorentz model. It can be expressed as follows:\n$Q = X \\otimes^* WQ, K = X \\otimes^* WK, V = X \\otimes^* WV, \\\\\nZ_i = \\sum_{j=1}^N \\frac{\\text{Sim}^*(Q_i, K_j)}{\\sum_{k=1}^N \\text{Sim}^*(Q_i, K_k)} \\otimes V_j.$ (3)\nIn this equation, $\\otimes^*$ denotes the hyperbolic linear transformation, which can be computed using Equations (6) and (7) given in the following section. The symbol $\\otimes$ represents the weighted sum"}, {"title": "4 Method", "content": "in hyperbolic space. Let $\\text{Att}_i$ denotes the $i$-th row of the attention matrix in the Lorentz model, it can be computed by Lorentzian midpoint [39]:\n$\\text{Att}_i \\otimes^* V_j := \\frac{\\sum_{j=1}^N d_{ij}V_j}{\\sqrt{\\sum_{k=1}^N A_{ik} V_k}}$ (4)\nThe function $\\text{Sim}^*(\\cdot,\\cdot)$ denotes the similarity function defined by the hyperbolic distance $d_H$ [9, 26, 61] or the tangent inner product [44]. Specifically, Chen et al. [9] defined the similarity function as:\n$\\text{Sim}^*(Q_i, K_j) = \\exp(-d_H(Q_i, K_j)/\\sqrt{\\alpha}).$ (5)\nBoth Gulcehre et al. [26] and Shimizu et al. [61] utilized similar forms of this function. They all employ negative distance to define similarity, and each has a computational complexity of $O (N^2)$.\n3.3 Lorentz Transformation\nLorentz Tangent Space Transformation. Previous works [8, 24, 43, 82, 93] mainly define hyperbolic linear transformations by the tangent space method, termed as $LTT$. Given the Lorentz embedding vector $x$ and operation function $f$, the tangent space method maps $x$ to the tangent space at a local reference point $o$ by the logarithmic map. Then, the transformation operation $f$ is applied in this tangent space. Finally, the resulting vector is mapped back to the Lorentz model using the exponential mapping, that is\n$LTT(x; f, \\kappa_1, \\kappa_2) := \\exp_o (f(\\log_o^x(x))).$ (6)\nwhere $o$ is the local reference point (generally the origin point), and the curvatures $\u03ba_1$ and $\u03ba_2$ could be different since they share the same tangent space. Using this method, previous works define the linear transformation, neighbor's aggregation, dropout, and non-linear activation [8, 43].\nLimitations. While this method is intuitive, it has notable limitations. First, parallel computation is feasible if the same reference point is used for the entire embedding. However, this approach can lead to significant mapping errors for distant points due to the point-specific nature of the tangent space. Conversely, using local-specific points enhances accuracy but increases computational load by requiring separate mappings. Second, frequent use of hyperbolic functions like $\\text{cosh}$ or $\\text{cosh}^{-1}$ can destabilize learning. While the clamp function can mitigate this issue, its use may compromise computational precision.\nFully Lorentz Transformation. To overcome the above limitations, Chen et al. [9] defined an alternative Lorentz transformation without using tangent space, termed as $LTF$:\n$LTF (x;f, W, \\kappa) := (\\sqrt{\\|f(Wx, v)\\|^2 - 1/\\kappa}, f (Wx, v)),$ (7)\nwhich involves a function $f$ that operates on vectors $v \\in \\mathbb{R}^{n+1}$ and $W \\in \\mathbb{R}^{m\\times(n+1)}$, $f(Wx, v) = \\lambda \\sigma(\\frac{||W h(x)+b||}{\\||W h(x)+b||} (W h(x) + b))$. Here, $\\sigma$ is the sigmoid function, $b$ and $b'$ are bias terms, $\\lambda > 0$ controls the scaling range, and $h$ is the activation function. Depending on the type of function, it can perform different operations. For instance, for dropout, the operation function is $f(Wx, v) = W \\text{dropout}(x)$.\nLimitations. There are several limitations to this method. First, the curvature is unchangeable. Although it appears that $LTF$ provides a way to directly modify \u03ba in Equation (7), this modification results in a loss of previously learned information, introducing distortions. Direct alteration of curvature cannot guarantee the preservation of relative distance relationships within the learned embedding. The derivation is shown as follows:\nLet $x' = f(Wx, v)$, and $g(x') = (\\sqrt{\\|x'\\|^2 - 1/\\kappa'}, x')$. Then:\n$d(g(x'), g(y')) = \\sqrt{1/|\\kappa'|} \\text{arcosh} (\\kappa' \\langle g(x'), g(y') \\rangle_\\mathcal{L}) \\\\\n= \\sqrt{1/|\\kappa'|} \\text{arcosh} (\\kappa' \\langle x', y' \\rangle_\\mathcal{L})$ (8)\nwhere $a_{time} = \\sqrt{(\\|x'\\|^2 - 1/\\kappa') (\\|y'\\|^2 - 1/\\kappa')}.$\nIt can be observed that changing \u03ba results in a non-linear transformation of the Lorentz distance $d$. Consequently, the relative distances between data points may not be preserved as they were in the original \u03ba Lorentz space. Even small changes in the parameter \u03ba can significantly affect the resulting distances, potentially distorting the previously learned hierarchical structure.\nSecond, the requirement for the $W$ matrix and normalization term pose another challenge. In [9], $W$ is applied to both time-like and space-like dimensions, in order to achieve Lorentz boosts and rotations simultaneously. However, its introduction constrains the usage of certain functions. For instance, dropout, activation operation do not necessarily interact with the matrix $W$. Taking the ReLU activation function as an example, it only requires filtering out negative values without needing matrix multiplication in Euclidean space. Additionally, Chen et al. [9] introduced a normalization term that constrains the value within a limited range, thereby limiting the expressiveness of the transformation.\nLastly, some basic operations, such as LayerNorm and Concatenation, cannot be achieved within this definition.\nThe proposed method is designed to overcome the limitations of the existing attempts in hyperbolic Transformer, as outlined in the Section 1. To address Challenges (1) and (2), we designed two foundational blocks, namely HTC and HRC in Section 4.1, and 4.2, respectively. To overcome Challenge (3), we developed a hyperbolic linear attention module in Section 4.3, which equips the Transformer with linear time complexity.\n4.1 Hyperbolic Transformation with Curvatures (HTC)\nNovelty. Unlike the design of linear transformation using the tangent space method in Equation (6), we build the transformation fully in hyperbolic space. Besides, compared with Lorentz transformation defined by Equation (7), we have two improvements:"}, {"title": "4.2 Hyperbolic Readjustment and Refinement with Curvatures (HRC)", "content": "(1) making the curvature changeable with preserving the relative ordering; (2) being disentangled with normalization term.\nGiven a point x in Lorentz model, $x \\in \\mathbb{L}^{d,\\kappa_1}$ (implies $x \\in \\mathbb{R}^{d+1}$), and transformation matrix $W \\in \\mathbb{R}^{(d+1)\\times d'}$ and bias $b \\in \\mathbb{R}^{d'}$, the HTC is given as the following equation:\n$\\text{HTC}(x; f_t, W, \\kappa_1, \\kappa_2) := (\\sqrt{\\frac{\\| f_t(x; W) \\|_\\mathcal{L}^2 -1}{\\kappa_2}}, \\frac{\\sqrt{\\frac{\\kappa_1}{\\kappa_2}} f_t(x; W)}{\\sqrt{\\| f_t(x; W) \\|_\\mathcal{L}^2}} )$ (9)\nwhere the $f_t(x; W) = W^T x + b$ denotes the linear transformation with bias addition and $\\kappa_1, \\kappa_2$ represent the curvatures before and after the transformation. Note that HTC does not entangle a normalization term with this linear transformation. Besides, It is easy to prove that the defined transformation also satisfies the Lorentz rotation and boost operations, described in [9]. The proof is similar in [9], we omit for brevity.\nThe proposed HTC avoids the use of tangent space and minimizes the usage of logarithmic and exponential mappings in comparison to Equation (6). When contrasted with Equation (7), the variable curvature of the HTC enhances the flexibility of the transformation. This is because linear transformations generally alter the feature dimension, and varying curvatures can express more than a fixed one.\nNext, we study the theoretical aspects of the proposed HTC. First and foremost, we prove that HTC is closed in hyperbolic space in Proposition (4.1) with different dimensions and curvatures so that the mapping is done correctly. Next, in Proposition (4.2), we show that the curvature-changing strategy of the proposed HTC, along with the subsequent HRC, maintains the relative distance among any points between pre and post-curvature changing.\nPROPOSITION 4.1. Let $x \\in \\mathbb{L}^{d_a,\\kappa_a}$ and $W \\in \\mathbb{R}^{(d_a+1)\\times d_b}$. The $LTC$ operation, defined as $LTC(x; ft; W, \\kappa_a, \\kappa_b)$, correctly transforms $x$ from the Lorentz model with curvature $\\kappa_a$ to the Lorentz model with curvature $\\kappa_b$, such that\n$LTC(x; ft; W, \\kappa_a, \\kappa_b) \\in \\mathbb{L}^{d_b,\\kappa_b} .$ (10)"}, {"title": "4.3 Hyperbolic Linear Attention", "content": "PROPOSITION 4.2. Let $z_i, z_j, z_k \\in \\mathbb{L}^{\\kappa_a}$ be points in the Lorentz model with curvature $\u03ba_a$. Consider the curvature changing transformations defined in HTC (Equation (7)) and HRC (Equation 13). Let $z'_i, z'_j, z'_k \\in \\mathbb{L}^{\\kappa_b}$ denote the transformed points in the Lorentz model with curvature $\u03ba_b$. The relative distances within $(z_i, z_j, z_k)$ are preserved after the curvature alteration. Specifically, if\n$d_{\\kappa_a} (z_i, z_j) \\geq d_{\\kappa_a} (z_i, z_k),$ (11)\nthen\n$d_{\\kappa_b} (z'_i, z'_j) \\geq d_{\\kappa_b} (z'_i, z'_k).$ (12)\n4.2 Hyperbolic Readjustment and Refinement with Curvatures (HRC)\nNovelty. Within the Transformer, we have several basic operations beyond linear transformation, which include Dropout and Concatenation, Activation function (e.g., ReLU), and LayerNorm. We interpret these operations within the hyperbolic space as a readjustment or refinement process, referred to as HRC. Similarly, given a point x in Lorentz model, the proposed operation HRC is defined as:\n$\\text{HRC}(x; f_r, \\kappa_1, \\kappa_2) := (\\sqrt{\\frac{\\kappa_1}{\\kappa_2} ||f_r(x[1:]) ||^2 -1/\\kappa_2}, \\frac{\\sqrt{\\frac{\\kappa_1}{\\kappa_2}} f_r(x[1:])}{\\sqrt{\\|f_r(x[1:]) \\|_\\mathcal{L}^2}} )$ (13)\nHere, $f_r$ represents a function applied to the space-like dimensions. It is evident that HRC shares similar advantages with HTC, which we will not repeat for the sake of brevity. However, unlike HTC, HRC performs the transformation only in space-like dimensions. The primary motivation is as follows: HTC involves a Lorentz boost, essential for mapping between different inertial reference frames, tied to causality and affecting the observed sequence of events in relativity. However, operations such as LayerNorm, activation functions, dropout, and concatenation serve as readjustments or refinements within the same frame of reference, acting on space-like features to standardize, activate, or regularize them. Applying these to the space-like dimension ensures the causal structure remains intact. In practical, it ensures dimensional consistency, improves interpretability, and allows for more efficient computation. Nonetheless, it is important to note that HRC does not completely discard the time-like information. According to the definition in Equation (1), the time-like dimension is determined by the space-like dimensions. By operating on the space-like dimensions, HRC implicitly utilizes the time-like information.\nIn hyperbolic space, the traditional way of calculating self-attention is quadratic time complexity, which hinders scalability. Therefore, we defined a linear attention through HTC and HRC modules.\nSpecifically, given the N input token feature with dimension d, $X \\in \\mathbb{L}^{N\\times d,\\kappa_1}$ in the Lorentz model with transformation matrix"}, {"title": "4.4 Hyperbolic Positional Encoding", "content": "$WQ, WK, WV \\in \\mathbb{R}^{(d+1)\\times d'}$, we first transform it to $Q, K$ and $V$, that is\n$Q = \\text{HTC}(X; f_t, W^Q, \\kappa_1, \\kappa_2), \\\\\nK = \\text{HTC}(X; f_t, W^K, \\kappa_1, \\kappa_2), \\\\\nV = \\text{HTC}(X; f_t, W^V, \\kappa_1, \\kappa_2),$ (14)\nwhere $Q, K$ and $V \\in \\mathbb{L}^{N\\times d', \\kappa_2}$. Given that the subsequent pairwise similarity computation and aggregation essentially constitute a weighted sum, and their calculation does not involve transformations on the time-like dimension, we adopted the idea of HRC to achieve this. Specifically, we first slice the values of the space-like dimension,\n$Q_s, K_s, V_s = \\Phi(Q[1:]), \\Phi(K[1:]), \\Phi(V[1:]).$ (15)\nTo achieve linearity, we alter the computation sequence, i.e., transitioning from $(Q(\\mathbb{K}^T V)$ to $Q(\\mathbb{K}^T)V)$, inspired by [27]. Our innovation lies in defining this operation in space-like dimensions and recalibrating the time-like value to respect the Lorentz constraint,\n$Z_s = \\frac{Q_s (K^TV_s)}{Q_s (K^T \\mathbb{1})}$ (16)\nBefore recalibrating, we incorporate the following residual connection:\n$\\tilde{Z}_s = Z_s + \\Phi(V_s),$ (17)\nand then do the time-like calibration and concatenation,\n$\\text{Z}_t = \\sqrt{\\frac{1}{\\kappa_3} ||\\tilde{Z}_s ||^2 - 1/\\kappa_3},$ (Time-like Calibration)\n$\\text{Z} = \\begin{bmatrix} \\text{Z}_t\\\\ \\sqrt{\\frac{\\kappa_2}{\\kappa_3}} Z_s \\end{bmatrix} $(Re-concatenation) (18)\nIn Equation (15, 16, 17), $\\mathbb{1}$ denotes an all \"1\" vector, $\\Psi$ is a linear layer and $\\Phi$ signifies the functions employed to enhance the focus of the linear attention, i.e.,\n$\\Phi(\\text{e}) = \\frac{\\text{ReLU}(\\text{e}) / t}{\\|\\text{ReLU}(\\text{e}) / t\\|} \\text{e}, where $\\text{e}_i = \\text{ReLU}(e)/t,$ (19)\nwhere $\\text{e} \\in \\mathbb{L}^{d', \\kappa_2}$ represents the transpose of row in $Q, K$ and $V$. In this case, $t$ represents a scaling factor, which we set as a trainable parameter in the experiments. The focused strategy is inspired"}, {"title": "4.5 Hyperbolic LayerNorm, Dropout, Activation, and Concatenation", "content": "by the work in [27]. A $p > 1$ sharpens the paired points, i.e., it enhances the similarity within each group while diminishing the similarity between the groups. Conversely, a $p < 1$ has the opposite effect.\nThis linear attention approach allows us to handle large datasets and long sequences more efficiently while respecting the properties of the Lorentz model.\n4.4 Hyperbolic Positional Encoding\nPositional encoding in a Transformer model is instrumental in preserving the sequence of input tokens. In what follows, we introduce a relative positional encoding with a trainable model inspired by [39, 65].\n$x = \\frac{x + e.p}{\\sqrt{\\kappa ||x + e. PL}}$ (20)\nHere, $p := \\text{HTC}(x)$ functions as a Lorentz position vector, and $e$ specifies the magnitude of p and we use 1 in our experiments. This definition calculates the midpoint between $x$ and $ep$, with respect to the Lorentz constraint. We add the positional encoding before the linear transformation in the self-attention block. We reserve the exploration of more advanced positional encoding for future works.\n4.5 Hyperbolic LayerNorm, Dropout, Activation, and Concatenation\nLayerNorm, Dropout, Activation, and Concatenation are fundamental components of the Transformer architecture. For these operations, we employ HRC in our definitions. This choice is motivated by the fact that these functions are performed within the same reference system and do not involve a time-like dimension. Consequently, we define our operations as follows:\nHypLayerNorm(X) = HRC(X, fLayerNorm),\nHypBatchNorm(X) = HRC(X, fBatchNorm),\nHypDropout(X) = HRC(X, fDropout),\nHypActivation (X) = HRC(X, f\\sigma),\nHypConcatnation(X) = HRC((X_i, X_j), fconcatenation), (21)"}, {"title": "4.6 Overall Architecture", "content": "The framework of Hypformer is shown in Figure 4, it can accept a variety of data types, such as text, images, and graphs. During the data preparation phase, the input data is mapped to the Lorentz model using an exponential map7. This mapped embedding is then transformed using a HTC layer. In the encoder part of Hypformer, the transformed data is processed through a hyperbolic linear attention block with hyperbolic position encoding. This is followed by the Feedforward layer implemented by HTC, and LayerNorm layer built by HRC. For graph-based inputs, we incorporate the graph neural networks and adopt the parallel paradigm [45] for Transformer and GNN encoder to form a graph Transformer model. The processed data is then forwarded to the decoder. The decoder can either be the similar structure of encoder, hyperbolic multinomial logistic regression (HypMLR) [24, 61] or a tailored design, we leave it in future exploration. In this research, the decoder is a fully connected layer used for classification tasks.\nTime complexity. In the proposed Hypformer, the linear attention module is the main computational bottleneck. The complexity comes from two key operations. In Equation (16), we perform a space-like inner product computation of K\u1d40 and V within the Lorentz model, which incurs a complexity of $O(d'^2N)$. Following this, we calculate the inner product of these results with Q, which also has a complexity of $O(d'^2N)$. Given that d' << N, the total"}, {"title": "5 Experiments", "content": "computational complexity of our method is $O(N)$. When dealing with graph inputs"}, {"title": "Hypformer: Exploring Efficient Hyperbolic Transformer Fully in Hyperbolic Space", "authors": ["Menglin Yang", "Harshit Verma", "Delvin Ce Zhang", "Jiahong Liu", "Irwin King", "Rex Ying"], "abstract": "Hyperbolic geometry have shown significant potential in modeling complex structured data, particularly those with underlying tree-like and hierarchical structures. Despite the impressive performance of various hyperbolic neural networks across numerous domains, research on adapting the Transformer to hyperbolic space remains limited. Previous attempts have mainly focused on modifying self-attention modules in the Transformer. However, these efforts have fallen short of developing a complete hyperbolic Transformer. This stems primarily from: (i) the absence of well-defined modules in hyperbolic space, including linear transformation layers, Layer-Norm layers, activation functions, dropout operations, etc. (ii) the quadratic time complexity of the existing hyperbolic self-attention module w.r.t the number of input tokens, which hinders its scalability. To address these challenges, we propose, Hypformer, a novel hyperbolic Transformer based on the Lorentz model of hyperbolic geometry. In Hypformer, we introduce two foundational blocks that define the essential modules of the Transformer in hyperbolic space. Furthermore, we develop a linear self-attention mechanism in hyperbolic space, enabling hyperbolic Transformer to process billion-scale graph data and long-sequence inputs for the first time. Our experimental results confirm the effectiveness and efficiency of Hypformer across various datasets, demonstrating its potential as an effective and scalable solution for large-scale data representation and large models.", "sections": [{"title": "1 Introduction", "content": "In many real-world scenarios, data frequently exhibit a hierarchical or tree-like structure, either implicitly or explicitly [32, 49, 95]. This is evident in complex networks [8, 38, 78, 94], the semantics of words in natural language processing [50, 51, 63], and conceptual hierarchies in vision tasks [15, 32]. As illustrated in Figure 1, such data can be organized into large and abstract groups that encompass small and specific subgroups, which can further be subdivided into even smaller and more specific sub-subgroups, and so on. The relationships between these groups and subgroups can be effectively approximated by tree-like structures [13]. This hierarchical representation mirrors human cognitive processes [14, 28], making it an intuitive approach to data representation.\nRecent initiatives have explored the use of hyperbolic learning spaces to encode complex non-Euclidean data, achieving impressive performance in representing tree-like data [8, 24, 43, 50, 51, 62, 76, 79, 80, 83]. This success is attributed to the unique property of hyperbolic space, which expands exponentially compared to the polynomial expansion of Euclidean spaces. This property aligns hyperbolic space with the metric of trees, making it particularly suitable for representing tree-like or hierarchically structured data [38]. Despite the growing interest in hyperbolic representation and deep learning, the Transformer [1, 65, 70], a cornerstone model in the various domains, was seldom explored within the realm of hyperbolic space. Despite preliminary attempts in hyperbolic Transformers [9, 26, 61], numerous challenges remain to be addressed.\nChallenge (1): Insufficient definitions for operations in the hyperbolic Transformer. Prior works of HAN [26] and HNN++ [61] primarily concentrated on the self-attention module, yet they fell short of constructing a comprehensive Transformer architecture, lacking basic components such as LayerNorm layer and positional encoding layer. This is primarily due to the inadequate definition of fundamental operations in previous studies.\nChallenge (2): Inefficient and ineffective definitions for linear transformation in the hyperbolic Transformer. While some techniques [8, 24] employ the tangent space to achieve the linear transformation, they often necessitate frequent logarithmic and exponential mappings, heavily dependent on the tangent space at the origin. This leads to an increased computational load, accumulation of mapping errors, and unstable training procedures. Although Chen et al. [9] introduced a fully Lorentz linear transformation in hyperbolic space, it is constrained by its immutable curvature and normalization term.\nChallenge (3): Absence of a linear attention mechanism in hyperbolic Transformer. The hyperbolic self-attention mechanisms proposed by Gulcehre et al. [26], Shimizu et al. [61], and Chen et al. [9] exhibit quadratic time complexity, posing a significant challenge when handling long-sequence input and large-scale graph data.\nProposed work: In this work, we propose an efficient hyperbolic Transformer, referred to as Hypformer. In particular, to address Challenges (1) and (2), we propose two foundational blocks, Hyperbolic Transformation with Curvatures (HTC) and Hyperbolic Readjustment and Refinement with Curvatures (HRC), to build all essential modules in the hyperbolic Transformer. HTC and HRC are built on the Lorentz model of hyperbolic geometry, working directly on the hyperbolic space without frequently mapping. HTC defines the linear transformation and facilitates mapping from a hyperbolic space with one curvature to another different curvature while preserving the relative distance. HRC further enables the definition of basic operations commonly used in the Transformer, such as LayerNorm layer, activation function, dropout, and concatenation, within a hyperbolic context. To tackle Challenge (3), we introduce a self-attention mechanism in Hypformer with linear complexity, enabling efficient large-scale data processing.\nTo validate the effectiveness of the proposed methodology, we have undertaken extensive experiments across a diverse range of tasks. These include graph analysis [40, 42, 77, 85, 89], text classification [90, 91], and image classification [17, 72]. The empirical evidence gathered from these experiments indicates that the proposed method significantly reduces the GPU computation cost by a factor of 10 and concurrently halves the training time compared with the existing hyperbolic softmax attention. Furthermore, the proposed method consistently surpasses the performance of competitive baselines, yielding substantial improvements on both tree-like and non-tree-like datasets.\nContributions. In summary, this study offers the following contributions: First, we introduce two fundamental hyperbolic blocks, HTC and HRC. Building upon these, we have formulated fundamental modules for linear transformation, LayerNorm, activation function, dropout, and concatenation operations within a hyperbolic context. Second, we propose the first hyperbolic linear attention mechanism, which enables the hyperbolic Transformer to be scalable and efficient. Based on the above efforts, we construct a Hypformer\u00b9, the first comprehensive and efficient hyperbolic Transformer model fully designed to operate within hyperbolic space. Last, we extend the hyperbolic model to handle billion-level graph data for the first time, laying a crucial foundation for the application of big data and large-scale models."}, {"title": "2 Related Work", "content": "2.1 Hyperbolic Neural Networks\nRecent studies have demonstrated that hyperbolic space is particularly adept at capturing the hierarchical and tree-like structures [23, 25, 39, 41, 50, 51, 53, 63, 83]. Building on hyperbolic space, a variety of hyperbolic neural networks, HNN [24], HAN [26], HNN++ [61], HGCN [8], HGNN [43], F-HNN [9], Poincar\u00e9 Resnet [64], HGTM [91] have been developed to leverage the advantages of the hyperbolic geometry. These neural networks have obtained an impressive performance in domains like computer vision [2, 29, 32], natural language processing [4, 7, 37, 47], recommender systems [11, 62, 68, 69, 76, 80], graph learning [3, 8, 41, 43, 78, 81, 92] and so on [41, 75].\n2.2 Transformer and Hyperbolic Transformer\nIntroduced by Vaswani et al. [65], Transformer models have brought about a paradigm shift in the field of artificial intelligence. Transformer [5, 16, 18, 65] has made a tremendous impact in many fields, such as language understanding [5, 16, 54], image processing [6, 52] and graph learning [33, 55]. A well-known concern with self-attention is the quadratic time complexity, which can hinder model scalability in many settings. Efficient self-attention models are crucial in applications that model long sequences [27, 31, 36, 59].\nDespite these advancements, existing Transformer architectures predominantly operate within the Euclidean domain. There have been limited attempts to extend these models to hyperbolic and other non-Euclidean spaces. Gulcehre et al. [26] proposed hyperbolic attention networks, which replace the dot-product between the query and key in self-attention with a function of negative hyperbolic distance. They then utilize the Einstein midpoint to compute the attentive output with value. Similarly, Chen et al. [9] and Shimizu et al. [61] adopt similar strategies that result in the attentive output with key being based on the Lorentzian midpoint and gyromidpoint, respectively. However, these methods exhibit quadratic time complexity, limiting their scalability. Besides, they focused more on the self-attention module and did not define the essential modules, like LayerNorm in Transformer. Recently, Cho et al. [12] proposed a fully Product-Stereographic Transformer, presenting a kernelized approach to non-Euclidean attention, which is linear time complexity. However, this method heavily relies on the tangent space, necessitating frequent mappings between the tangent space and manifolds. Ermolov et al. [19] proposed mapping the last layer features obtained from a Euclidean Transformer to hyperbolic space, which essentially does not establish a true Hyperbolic Transformer. Our work aims to address these challenges and further the development of hyperbolic Transformers."}, {"title": "3 Preliminaries", "content": "In this section, we introduce concepts related to Lorentz model of hyperbolic geometry and self-attention module briefly.\n3.1 Lorentz Model of Hyperbolic Geometry\nThere are several isometric models [23, 26, 50, 51, 56, 63] of hyperbolic geometry that have been employed in prior research. In this study, we choose the Lorentz model as the foundational framework due to the numerical stability it offers [46, 51]. Also, the proposed Hypformer can be easily adapted to other hyperbolic models, as they are isometrically equivalent.\nLorentz Model. An n-dimensional Lorentz model with negative constant curvature \u03ba(\u03ba < 0) is a Riemannian manifold denoted by $\\mathbb{L}^{n,\\kappa}$. The corresponding Riemannian metric is given by $g = \\text{diag}(1/\\kappa, 1,..., 1)$. Each point in $\\mathbb{L}^{n,\\kappa}$ can be represented as $x = \\begin{bmatrix} x_t\\\\ x_s \\end{bmatrix}$ where $x \\in \\mathbb{R}^{n+1}$, $x_t \\in \\mathbb{R}$ and $x_s \\in \\mathbb{R}^{n}$. The set of points, $\\mathbb{L}^{n,\\kappa}$, that constitute the manifold are defined as\n$\\mathbb{L}^{n,\\kappa} := \\{x \\in \\mathbb{R}^{n+1} | (x, x)_\\mathcal{L} = 1/\\kappa, x_t > 0\\} .$ (1)\nHere, $(x, y)_\\mathcal{L} = -x_ty_t + x_s^T y_s = x^Tgy$ represents the Lorentzian inner product. Lorentz model, also known as the hyperboloid model, is an upper hyper-surface in an $(n + 1)$ dimensional Minkowski space with the origin point $(\\sqrt{-1/\\kappa}, 0, \\cdot \\cdot \\cdot, 0)$. Lorentz model has its roots in the theory of special relativity [57] and employs terminology borrowed from this field. The hyperboloid's axis of symmetry, represented by the 0-th element $x_t$, is referred to as the time-like dimension, while all other axes $x_s$ are called space-like dimensions.\nTangent Space of Lorentz Model. Given $x \\in \\mathbb{L}^{n,\\kappa}$, the tangent space $\\mathbb{L}^{n,\\kappa} := \\{u \\in \\mathbb{R}^{n+1} | (u, x)_\\mathcal{L} = 0\\}$ is the orthogonal space of $\\mathbb{L}^{n,\\kappa}$ at $x$ with respect to the Lorentzian inner product. To achieve the mapping from the Lorentz model to the tangent space at x, we can use the logarithmic map, $\\log_x: \\mathbb{L}^{n,\\kappa} \\rightarrow T_x\\mathbb{L}^{n,\\kappa}$. The exponential map defines the inverse process, $\\exp_x: T_x\\mathbb{L}^{n,\\kappa} \\rightarrow \\mathbb{L}^{n,\\kappa}$. For the details about exponential, logarithmic maps and the relevant distance functions, please refer to Appendix A.\n3.2 Self-Attention Module\nWe first examine the general form of self-attention in Euclidean Transformers. Given the input of N tokens $X \\in \\mathbb{R}^{N\\times d}$, within each head, self-attention can be expressed as:\n$Q = XW^Q, K = XW^K, V = XW^V, \\\\\nZ_i = \\sum_{j=1}^N \\frac{\\text{Sim}(Q_i, K_j)}{\\sum_{k=1}^N \\text{Sim}(Q_i, K_k)}V_j,$ (2)\nwhere $W^Q, W^K, W^V \\in \\mathbb{R}^{d\\times d'}$ are projection matrices and $\\text{Sim}(\\cdot, \\cdot)$ denotes the similarity function. Modern Euclidean Transformers primarily use Softmax attention [65] where similarity is calculated as $\\text{Sim}(Q_i, K_j) = \\exp(Q_iK_j^T/\\sqrt{d})$. In this scenario, the attention map is derived by computing the similarity between all query-key pairs, which results in a computational complexity of $O (N^2)$.\nThe concept of hyperbolic self-attention, as defined by previous works [9, 26, 61], bears a similar idea to Equation (2). Figure 2 presents an illustration for this hyperbolic operation on Lorentz model. It can be expressed as follows:\n$Q = X \\otimes^* WQ, K = X \\otimes^* WK, V = X \\otimes^* WV, \\\\\nZ_i = \\sum_{j=1}^N \\frac{\\text{Sim}^*(Q_i, K_j)}{\\sum_{k=1}^N \\text{Sim}^*(Q_i, K_k)} \\otimes V_j.$ (3)\nIn this equation, $\\otimes^*$ denotes the hyperbolic linear transformation, which can be computed using Equations (6) and (7) given in the following section. The symbol $\\otimes$ represents the weighted sum"}, {"title": "4 Method", "content": "in hyperbolic space. Let $\\text{Att}_i$ denotes the $i$-th row of the attention matrix in the Lorentz model, it can be computed by Lorentzian midpoint [39]:\n$\\text{Att}_i \\otimes^* V_j := \\frac{\\sum_{j=1}^N d_{ij}V_j}{\\sqrt{\\sum_{k=1}^N A_{ik} V_k}}$ (4)\nThe function $\\text{Sim}^*(\\cdot,\\cdot)$ denotes the similarity function defined by the hyperbolic distance $d_H$ [9, 26, 61] or the tangent inner product [44]. Specifically, Chen et al. [9] defined the similarity function as:\n$\\text{Sim}^*(Q_i, K_j) = \\exp(-d_H(Q_i, K_j)/\\sqrt{\\alpha}).$ (5)\nBoth Gulcehre et al. [26] and Shimizu et al. [61] utilized similar forms of this function. They all employ negative distance to define similarity, and each has a computational complexity of $O (N^2)$.\n3.3 Lorentz Transformation\nLorentz Tangent Space Transformation. Previous works [8, 24, 43, 82, 93] mainly define hyperbolic linear transformations by the tangent space method, termed as $LTT$. Given the Lorentz embedding vector $x$ and operation function $f$, the tangent space method maps $x$ to the tangent space at a local reference point $o$ by the logarithmic map. Then, the transformation operation $f$ is applied in this tangent space. Finally, the resulting vector is mapped back to the Lorentz model using the exponential mapping, that is\n$LTT(x; f, \\kappa_1, \\kappa_2) := \\exp_o (f(\\log_o^x(x))).$ (6)\nwhere $o$ is the local reference point (generally the origin point), and the curvatures $\u03ba_1$ and $\u03ba_2$ could be different since they share the same tangent space. Using this method, previous works define the linear transformation, neighbor's aggregation, dropout, and non-linear activation [8, 43].\nLimitations. While this method is intuitive, it has notable limitations. First, parallel computation is feasible if the same reference point is used for the entire embedding. However, this approach can lead to significant mapping errors for distant points due to the point-specific nature of the tangent space. Conversely, using local-specific points enhances accuracy but increases computational load by requiring separate mappings. Second, frequent use of hyperbolic functions like $\\text{cosh}$ or $\\text{cosh}^{-1}$ can destabilize learning. While the clamp function can mitigate this issue, its use may compromise computational precision.\nFully Lorentz Transformation. To overcome the above limitations, Chen et al. [9] defined an alternative Lorentz transformation without using tangent space, termed as $LTF$:\n$LTF (x;f, W, \\kappa) := (\\sqrt{\\|f(Wx, v)\\|^2 - 1/\\kappa}, f (Wx, v)),$ (7)\nwhich involves a function $f$ that operates on vectors $v \\in \\mathbb{R}^{n+1}$ and $W \\in \\mathbb{R}^{m\\times(n+1)}$, $f(Wx, v) = \\lambda \\sigma(\\frac{||W h(x)+b||}{\\||W h(x)+b||} (W h(x) + b))$. Here, $\\sigma$ is the sigmoid function, $b$ and $b'$ are bias terms, $\\lambda > 0$ controls the scaling range, and $h$ is the activation function. Depending on the type of function, it can perform different operations. For instance, for dropout, the operation function is $f(Wx, v) = W \\text{dropout}(x)$.\nLimitations. There are several limitations to this method. First, the curvature is unchangeable. Although it appears that $LTF$ provides a way to directly modify \u03ba in Equation (7), this modification results in a loss of previously learned information, introducing distortions. Direct alteration of curvature cannot guarantee the preservation of relative distance relationships within the learned embedding. The derivation is shown as follows:\nLet $x' = f(Wx, v)$, and $g(x') = (\\sqrt{\\|x'\\|^2 - 1/\\kappa'}, x')$. Then:\n$d(g(x'), g(y')) = \\sqrt{1/|\\kappa'|} \\text{arcosh} (\\kappa' \\langle g(x'), g(y') \\rangle_\\mathcal{L}) \\\\\n= \\sqrt{1/|\\kappa'|} \\text{arcosh} (\\kappa' \\langle x', y' \\rangle_\\mathcal{L})$ (8)\nwhere $a_{time} = \\sqrt{(\\|x'\\|^2 - 1/\\kappa') (\\|y'\\|^2 - 1/\\kappa')}.$\nIt can be observed that changing \u03ba results in a non-linear transformation of the Lorentz distance $d$. Consequently, the relative distances between data points may not be preserved as they were in the original \u03ba Lorentz space. Even small changes in the parameter \u03ba can significantly affect the resulting distances, potentially distorting the previously learned hierarchical structure.\nSecond, the requirement for the $W$ matrix and normalization term pose another challenge. In [9], $W$ is applied to both time-like and space-like dimensions, in order to achieve Lorentz boosts and rotations simultaneously. However, its introduction constrains the usage of certain functions. For instance, dropout, activation operation do not necessarily interact with the matrix $W$. Taking the ReLU activation function as an example, it only requires filtering out negative values without needing matrix multiplication in Euclidean space. Additionally, Chen et al. [9] introduced a normalization term that constrains the value within a limited range, thereby limiting the expressiveness of the transformation.\nLastly, some basic operations, such as LayerNorm and Concatenation, cannot be achieved within this definition.\nThe proposed method is designed to overcome the limitations of the existing attempts in hyperbolic Transformer, as outlined in the Section 1. To address Challenges (1) and (2), we designed two foundational blocks, namely HTC and HRC in Section 4.1, and 4.2, respectively. To overcome Challenge (3), we developed a hyperbolic linear attention module in Section 4.3, which equips the Transformer with linear time complexity."}, {"title": "4.2 Hyperbolic Readjustment and Refinement with Curvatures (HRC)", "content": "(1) making the curvature changeable with preserving the relative ordering; (2) being disentangled with normalization term.\nGiven a point x in Lorentz model, $x \\in \\mathbb{L}^{d,\\kappa_1}$ (implies $x \\in \\mathbb{R}^{d+1}$), and transformation matrix $W \\in \\mathbb{R}^{(d+1)\\times d'}$ and bias $b \\in \\mathbb{R}^{d'}$, the HTC is given as the following equation:\n$\\text{HTC}(x; f_t, W, \\kappa_1, \\kappa_2) := (\\sqrt{\\frac{\\| f_t(x; W) \\|_\\mathcal{L}^2 -1}{\\kappa_2}}, \\frac{\\sqrt{\\frac{\\kappa_1}{\\kappa_2}} f_t(x; W)}{\\sqrt{\\| f_t(x; W) \\|_\\mathcal{L}^2}} )$ (9)\nwhere the $f_t(x; W) = W^T x + b$ denotes the linear transformation with bias addition and $\\kappa_1, \\kappa_2$ represent the curvatures before and after the transformation. Note that HTC does not entangle a normalization term with this linear transformation. Besides, It is easy to prove that the defined transformation also satisfies the Lorentz rotation and boost operations, described in [9]. The proof is similar in [9], we omit for brevity.\nThe proposed HTC avoids the use of tangent space and minimizes the usage of logarithmic and exponential mappings in comparison to Equation (6). When contrasted with Equation (7), the variable curvature of the HTC enhances the flexibility of the transformation. This is because linear transformations generally alter the feature dimension, and varying curvatures can express more than a fixed one.\nNext, we study the theoretical aspects of the proposed HTC. First and foremost, we prove that HTC is closed in hyperbolic space in Proposition (4.1) with different dimensions and curvatures so that the mapping is done correctly. Next, in Proposition (4.2), we show that the curvature-changing strategy of the proposed HTC, along with the subsequent HRC, maintains the relative distance among any points between pre and post-curvature changing.\nPROPOSITION 4.1. Let $x \\in \\mathbb{L}^{d_a,\\kappa_a}$ and $W \\in \\mathbb{R}^{(d_a+1)\\times d_b}$. The $LTC$ operation, defined as $LTC(x; ft; W, \\kappa_a, \\kappa_b)$, correctly transforms $x$ from the Lorentz model with curvature $\\kappa_a$ to the Lorentz model with curvature $\\kappa_b$, such that\n$LTC(x; ft; W, \\kappa_a, \\kappa_b) \\in \\mathbb{L}^{d_b,\\kappa_b} .$ (10)"}, {"title": "4.3 Hyperbolic Linear Attention", "content": "PROPOSITION 4.2. Let $z_i, z_j, z_k \\in \\mathbb{L}^{\\kappa_a}$ be points in the Lorentz model with curvature $\u03ba_a$. Consider the curvature changing transformations defined in HTC (Equation (7)) and HRC (Equation 13). Let $z'_i, z'_j, z'_k \\in \\mathbb{L}^{\\kappa_b}$ denote the transformed points in the Lorentz model with curvature $\u03ba_b$. The relative distances within $(z_i, z_j, z_k)$ are preserved after the curvature alteration. Specifically, if\n$d_{\\kappa_a} (z_i, z_j) \\geq d_{\\kappa_a} (z_i, z_k),$ (11)\nthen\n$d_{\\kappa_b} (z'_i, z'_j) \\geq d_{\\kappa_b} (z'_i, z'_k).$ (12)\n4.2 Hyperbolic Readjustment and Refinement with Curvatures (HRC)\nNovelty. Within the Transformer, we have several basic operations beyond linear transformation, which include Dropout and Concatenation, Activation function (e.g., ReLU), and LayerNorm. We interpret these operations within the hyperbolic space as a readjustment or refinement process, referred to as HRC. Similarly, given a point x in Lorentz model, the proposed operation HRC is defined as:\n$\\text{HRC}(x; f_r, \\kappa_1, \\kappa_2) := (\\sqrt{\\frac{\\kappa_1}{\\kappa_2} ||f_r(x[1:]) ||^2 -1/\\kappa_2}, \\frac{\\sqrt{\\frac{\\kappa_1}{\\kappa_2}} f_r(x[1:])}{\\sqrt{\\|f_r(x[1:]) \\|_\\mathcal{L}^2}} )$ (13)\nHere, $f_r$ represents a function applied to the space-like dimensions. It is evident that HRC shares similar advantages with HTC, which we will not repeat for the sake of brevity. However, unlike HTC, HRC performs the transformation only in space-like dimensions. The primary motivation is as follows: HTC involves a Lorentz boost, essential for mapping between different inertial reference frames, tied to causality and affecting the observed sequence of events in relativity. However, operations such as LayerNorm, activation functions, dropout, and concatenation serve as readjustments or refinements within the same frame of reference, acting on space-like features to standardize, activate, or regularize them. Applying these to the space-like dimension ensures the causal structure remains intact. In practical, it ensures dimensional consistency, improves interpretability, and allows for more efficient computation. Nonetheless, it is important to note that HRC does not completely discard the time-like information. According to the definition in Equation (1), the time-like dimension is determined by the space-like dimensions. By operating on the space-like dimensions, HRC implicitly utilizes the time-like information.\nIn hyperbolic space, the traditional way of calculating self-attention is quadratic time complexity, which hinders scalability. Therefore, we defined a linear attention through HTC and HRC modules.\nSpecifically, given the N input token feature with dimension d, $X \\in \\mathbb{L}^{N\\times d,\\kappa_1}$ in the Lorentz model with transformation matrix"}, {"title": "4.4 Hyperbolic Positional Encoding", "content": "$WQ, WK, WV \\in \\mathbb{R}^{(d+1)\\times d'}$, we first transform it to $Q, K$ and $V$, that is\n$Q = \\text{HTC}(X; f_t, W^Q, \\kappa_1, \\kappa_2), \\\\\nK = \\text{HTC}(X; f_t, W^K, \\kappa_1, \\kappa_2), \\\\\nV = \\text{HTC}(X; f_t, W^V, \\kappa_1, \\kappa_2),$ (14)\nwhere $Q, K$ and $V \\in \\mathbb{L}^{N\\times d', \\kappa_2}$. Given that the subsequent pairwise similarity computation and aggregation essentially constitute a weighted sum, and their calculation does not involve transformations on the time-like dimension, we adopted the idea of HRC to achieve this. Specifically, we first slice the values of the space-like dimension,\n$Q_s, K_s, V_s = \\Phi(Q[1:]), \\Phi(K[1:]), \\Phi(V[1:]).$ (15)\nTo achieve linearity, we alter the computation sequence, i.e., transitioning from $(Q(\\mathbb{K}^T V)$ to $Q(\\mathbb{K}^T)V)$, inspired by [27]. Our innovation lies in defining this operation in space-like dimensions and recalibrating the time-like value to respect the Lorentz constraint,\n$Z_s = \\frac{Q_s (K^TV_s)}{Q_s (K^T \\mathbb{1})}$ (16)\nBefore recalibrating, we incorporate the following residual connection:\n$\\tilde{Z}_s = Z_s + \\Phi(V_s),$ (17)\nand then do the time-like calibration and concatenation,\n$\\text{Z}_t = \\sqrt{\\frac{1}{\\kappa_3} ||\\tilde{Z}_s ||^2 - 1/\\kappa_3},$ (Time-like Calibration)\n$\\text{Z} = \\begin{bmatrix} \\text{Z}_t\\\\ \\sqrt{\\frac{\\kappa_2}{\\kappa_3}} Z_s \\end{bmatrix} $(Re-concatenation) (18)\nIn Equation (15, 16, 17), $\\mathbb{1}$ denotes an all \"1\" vector, $\\Psi$ is a linear layer and $\\Phi$ signifies the functions employed to enhance the focus of the linear attention, i.e.,\n$\\Phi(\\text{e}) = \\frac{\\text{ReLU}(\\text{e}) / t}{\\|\\text{ReLU}(\\text{e}) / t\\|} \\text{e}, where $\\text{e}_i = \\text{ReLU}(e)/t,$ (19)\nwhere $\\text{e} \\in \\mathbb{L}^{d', \\kappa_2}$ represents the transpose of row in $Q, K$ and $V$. In this case, $t$ represents a scaling factor, which we set as a trainable parameter in the experiments. The focused strategy is inspired"}, {"title": "4.5 Hyperbolic LayerNorm, Dropout, Activation, and Concatenation", "content": "by the work in [27]. A $p > 1$ sharpens the paired points, i.e., it enhances the similarity within each group while diminishing the similarity between the groups. Conversely, a $p < 1$ has the opposite effect.\nThis linear attention approach allows us to handle large datasets and long sequences more efficiently while respecting the properties of the Lorentz model.\n4.4 Hyperbolic Positional Encoding\nPositional encoding in a Transformer model is instrumental in preserving the sequence of input tokens. In what follows, we introduce a relative positional encoding with a trainable model inspired by [39, 65].\n$x = \\frac{x + e.p}{\\sqrt{\\kappa ||x + e. PL}}$ (20)\nHere, $p := \\text{HTC}(x)$ functions as a Lorentz position vector, and $e$ specifies the magnitude of p and we use 1 in our experiments. This definition calculates the midpoint between $x$ and $ep$, with respect to the Lorentz constraint. We add the positional encoding before the linear transformation in the self-attention block. We reserve the exploration of more advanced positional encoding for future works.\n4.5 Hyperbolic LayerNorm, Dropout, Activation, and Concatenation\nLayerNorm, Dropout, Activation, and Concatenation are fundamental components of the Transformer architecture. For these operations, we employ HRC in our definitions. This choice is motivated by the fact that these functions are performed within the same reference system and do not involve a time-like dimension. Consequently, we define our operations as follows:\nHypLayerNorm(X) = HRC(X, fLayerNorm),\nHypBatchNorm(X) = HRC(X, fBatchNorm),\nHypDropout(X) = HRC(X, fDropout),\nHypActivation (X) = HRC(X, f\\sigma),\nHypConcatnation(X) = HRC((X_i, X_j), fconcatenation), (21)"}, {"title": "4.6 Overall Architecture", "content": "The framework of Hypformer is shown in Figure 4, it can accept a variety of data types, such as text, images, and graphs. During the data preparation phase, the input data is mapped to the Lorentz model using an exponential map7. This mapped embedding is then transformed using a HTC layer. In the encoder part of Hypformer, the transformed data is processed through a hyperbolic linear attention block with hyperbolic position encoding. This is followed by the Feedforward layer implemented by HTC, and LayerNorm layer built by HRC. For graph-based inputs, we incorporate the graph neural networks and adopt the parallel paradigm [45] for Transformer and GNN encoder to form a graph Transformer model. The processed data is then forwarded to the decoder. The decoder can either be the similar structure of encoder, hyperbolic multinomial logistic regression (HypMLR) [24, 61] or a tailored design, we leave it in future exploration. In this research, the decoder is a fully connected layer used for classification tasks.\nTime complexity. In the proposed Hypformer, the linear attention module is the main computational bottleneck. The complexity comes from two key operations. In Equation (16), we perform a space-like inner product computation of K\u1d40 and V within the Lorentz model, which incurs a complexity of $O(d'^2N)$. Following this, we calculate the inner product of these results with Q, which also has a complexity of $O(d'^2N)$. Given that d' << N, the total"}, {"title": "5 Experiments", "content": "computational complexity of our method is $O(N)$. When dealing with graph inputs, the computational complexity of a GNN model is typical O(N+E), where E represents the number of edges. Owing to the typical sparsity of graphs (i.e., E << N\u00b2), the proposed method can scale linearly with respect to the number of nodes in a graph. This design make Hypformer operate on graphs with billion-level nodes.\nIn this work, we propose a novel hyperbolic Transformer with linear complexity, which is especially well-suited for processing graph-structured data. Graphs often exhibit intricate topological and hierarchical relationships, making them an ideal testbed for evaluating the effectiveness of our proposed hyperbolic Transformer. As such, we primarily focus on comparing our model's performance with other state-of-the-art graph models.\n5.1 Experiments on Large Graphs\nExperimental Settings. We first evaluate Hypformer on diverse large-scale graphs for node classification, with node counts ranging from millions to billions, including ogbn-arxiv, ogbn-protein, and Papers100M (for dataset details, see Appendix C.1). To our knowledge, this represents the first application of hyperbolic or non-Euclidean transformations to graphs of this scale. Our comparative analysis focuses on state-of-the-art Euclidean GNNs and graph Transformers. We evaluate Hypformer against a spectrum of baselines, including MLP, GCN [35], SGC [71]), advanced GNN variants (SIGN [22], GCN-NSampler, GAT-NSampler), recent graph Transformer architectures (GraphFormer [84], GraphTrans [74], GraphGPS [55], NodeFormer [72], SGFormer [73]) and hyperbolic models HAN [26], HNN++ [61] and F-HNN [9].\nExperimental Findings. Table 1 summarizes the results of"}]}]}