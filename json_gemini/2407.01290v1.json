{"title": "Hypformer: Exploring Efficient Hyperbolic Transformer Fully in Hyperbolic Space", "authors": ["Menglin Yang", "Harshit Verma", "Delvin Ce Zhang", "Jiahong Liu", "Irwin King", "Rex Ying"], "abstract": "Hyperbolic geometry have shown significant potential in modeling complex structured data, particularly those with underlying tree-like and hierarchical structures. Despite the impressive performance of various hyperbolic neural networks across numerous domains, research on adapting the Transformer to hyperbolic space remains limited. Previous attempts have mainly focused on modifying self-attention modules in the Transformer. However, these efforts have fallen short of developing a complete hyperbolic Transformer. This stems primarily from: (i) the absence of well-defined modules in hyperbolic space, including linear transformation layers, Layer-Norm layers, activation functions, dropout operations, etc. (ii) the quadratic time complexity of the existing hyperbolic self-attention module w.r.t the number of input tokens, which hinders its scalability. To address these challenges, we propose, Hypformer, a novel hyperbolic Transformer based on the Lorentz model of hyperbolic geometry. In Hypformer, we introduce two foundational blocks that define the essential modules of the Transformer in hyperbolic space. Furthermore, we develop a linear self-attention mechanism in hyperbolic space, enabling hyperbolic Transformer to process billion-scale graph data and long-sequence inputs for the first time. Our experimental results confirm the effectiveness and efficiency of Hypformer across various datasets, demonstrating its potential as an effective and scalable solution for large-scale data representation and large models.", "sections": [{"title": "1 Introduction", "content": "In many real-world scenarios, data frequently exhibit a hierarchical or tree-like structure, either implicitly or explicitly [32, 49, 95]. This is evident in complex networks [8, 38, 78, 94], the semantics of words in natural language processing [50, 51, 63], and conceptual hierarchies in vision tasks [15, 32]. As illustrated in Figure 1, such data can be organized into large and abstract groups that encompass small and specific subgroups, which can further be subdivided into even smaller and more specific sub-subgroups, and so on. The relationships between these groups and subgroups can be effectively approximated by tree-like structures [13]. This hierarchical representation mirrors human cognitive processes [14, 28], making it an intuitive approach to data representation.\nRecent initiatives have explored the use of hyperbolic learning spaces to encode complex non-Euclidean data, achieving impressive performance in representing tree-like data [8, 24, 43, 50, 51, 62, 76, 79, 80, 83]. This success is attributed to the unique property of hyperbolic space, which expands exponentially compared to the polynomial expansion of Euclidean spaces. This property aligns hyperbolic space with the metric of trees, making it particularly suitable for representing tree-like or hierarchically structured data [38]. Despite the growing interest in hyperbolic representation and deep learning, the Transformer [1, 65, 70], a cornerstone model in the various domains, was seldom explored within the realm of hyperbolic space. Despite preliminary attempts in hyperbolic Transformers [9, 26, 61], numerous challenges remain to be addressed.\nChallenge (1): Insufficient definitions for operations in the hyperbolic Transformer. Prior works of HAN [26] and HNN++ [61] primarily concentrated on the self-attention module, yet they fell short of constructing a comprehensive Transformer architecture, lacking basic components such as LayerNorm layer and positional encoding layer. This is primarily due to the inadequate definition of fundamental operations in previous studies.\nChallenge (2): Inefficient and ineffective definitions for linear transformation in the hyperbolic Transformer. While some techniques [8, 24] employ the tangent space to achieve the linear transformation, they often necessitate frequent logarithmic and exponential mappings, heavily dependent on the tangent space at the origin. This leads to an increased computational load, accumulation of mapping errors, and unstable training procedures. Although Chen et al. [9] introduced a fully Lorentz linear transformation in hyperbolic space, it is constrained by its immutable curvature and normalization term.\nChallenge (3): Absence of a linear attention mechanism in hyperbolic Transformer. The hyperbolic self-attention mechanisms proposed by Gulcehre et al. [26], Shimizu et al. [61], and Chen et al. [9] exhibit quadratic time complexity, posing a significant challenge when handling long-sequence input and large-scale graph data.\nProposed work: In this work, we propose an efficient hyperbolic Transformer, referred to as Hypformer. In particular, to address Challenges (1) and (2), we propose two foundational blocks, Hyperbolic Transformation with Curvatures (HTC) and Hyperbolic Readjustment and Refinement with Curvatures (HRC), to build all essential modules in the hyperbolic Transformer. HTC and HRC are built on the Lorentz model of hyperbolic geometry, working directly on the hyperbolic space without frequently mapping. HTC defines the linear transformation and facilitates mapping from a hyperbolic space with one curvature to another different curvature while preserving the relative distance. HRC further enables the definition of basic operations commonly used in the Transformer, such as LayerNorm layer, activation function, dropout, and concatenation, within a hyperbolic context. To tackle Challenge (3), we introduce a self-attention mechanism in Hypformer with linear complexity, enabling efficient large-scale data processing.\nTo validate the effectiveness of the proposed methodology, we have undertaken extensive experiments across a diverse range of tasks. These include graph analysis [40, 42, 77, 85, 89], text classification [90, 91], and image classification [17, 72]. The empirical evidence gathered from these experiments indicates that the proposed method significantly reduces the GPU computation cost by a factor of 10 and concurrently halves the training time compared with the existing hyperbolic softmax attention. Furthermore, the proposed method consistently surpasses the performance of competitive baselines, yielding substantial improvements on both tree-like and non-tree-like datasets.\nContributions. In summary, this study offers the following contributions: First, we introduce two fundamental hyperbolic blocks, HTC and HRC. Building upon these, we have formulated fundamental modules for linear transformation, LayerNorm, activation function, dropout, and concatenation operations within a hyperbolic context. Second, we propose the first hyperbolic linear attention mechanism, which enables the hyperbolic Transformer to be scalable and efficient. Based on the above efforts, we construct a Hypformer, the first comprehensive and efficient hyperbolic Transformer model fully designed to operate within hyperbolic space. Last, we extend the hyperbolic model to handle billion-level graph data for the first time, laying a crucial foundation for the application of big data and large-scale models."}, {"title": "2 Related Work", "content": "2.1 Hyperbolic Neural Networks\nRecent studies have demonstrated that hyperbolic space is particularly adept at capturing the hierarchical and tree-like structures [23, 25, 39, 41, 50, 51, 53, 63, 83]. Building on hyperbolic space, a variety of hyperbolic neural networks, HNN [24], HAN [26], HNN++ [61], HGCN [8], HGNN [43], F-HNN [9], Poincar\u00e9 Resnet [64], HGTM [91] have been developed to leverage the advantages of the hyperbolic geometry. These neural networks have obtained an impressive performance in domains like computer vision [2, 29, 32], natural language processing [4, 7, 37, 47], recommender systems [11, 62, 68, 69, 76, 80], graph learning [3, 8, 41, 43, 78, 81, 92] and so on [41, 75].\n2.2 Transformer and Hyperbolic Transformer\nIntroduced by Vaswani et al. [65], Transformer models have brought about a paradigm shift in the field of artificial intelligence. Transformer [5, 16, 18, 65] has made a tremendous impact in many fields, such as language understanding [5, 16, 54], image processing [6, 52] and graph learning [33, 55]. A well-known concern with self-attention is the quadratic time complexity, which can hinder model scalability in many settings. Efficient self-attention models are crucial in applications that model long sequences [27, 31, 36, 59].\nDespite these advancements, existing Transformer architectures predominantly operate within the Euclidean domain. There have been limited attempts to extend these models to hyperbolic and other non-Euclidean spaces. Gulcehre et al. [26] proposed hyperbolic attention networks, which replace the dot-product between the query and key in self-attention with a function of negative hyperbolic distance. They then utilize the Einstein midpoint to compute the attentive output with value. Similarly, Chen et al. [9] and Shimizu et al. [61] adopt similar strategies that result in the attentive output with key being based on the Lorentzian midpoint and gyromidpoint, respectively. However, these methods exhibit quadratic time complexity, limiting their scalability. Besides, they focused more on the self-attention module and did not define the essential modules, like LayerNorm in Transformer. Recently, Cho et al. [12] proposed a fully Product-Stereographic Transformer, presenting a kernelized approach to non-Euclidean attention, which is linear time complexity. However, this method heavily relies on the tangent space, necessitating frequent mappings between the tangent space and manifolds. Ermolov et al. [19] proposed mapping the last layer features obtained from a Euclidean Transformer to hyperbolic space, which essentially does not establish a true Hyperbolic Transformer. Our work aims to address these challenges and further the development of hyperbolic Transformers."}, {"title": "3 Preliminaries", "content": "In this section, we introduce concepts related to Lorentz model of hyperbolic geometry and self-attention module briefly.\n3.1 Lorentz Model of Hyperbolic Geometry\nThere are several isometric models [23, 26, 50, 51, 56, 63] of hyperbolic geometry that have been employed in prior research. In this study, we choose the Lorentz model as the foundational framework due to the numerical stability it offers [46, 51]. Also, the proposed Hypformer can be easily adapted to other hyperbolic models, as they are isometrically equivalent.\nLorentz Model. An n-dimensional Lorentz model with negative constant curvature \\(\\kappa(\\kappa < 0)\\) is a Riemannian manifold denoted by \\(L^{n,\\kappa}\\). The corresponding Riemannian metric is given by \\(g = diag(1/\\kappa, 1,..., 1)\\). Each point in \\(L^{n,\\kappa}\\) can be represented as \\(x = \\begin{bmatrix}x_t\\\\ x_s\\end{bmatrix}\\) where \\(x \\in \\mathbb{R}^{n+1}\\), \\(x_t \\in \\mathbb{R}\\) and \\(x_s \\in \\mathbb{R}^n\\). The set of points, \\(L^{n,\\kappa}\\), that constitute the manifold are defined as\n\\[L^{n,\\kappa} := \\{x \\in \\mathbb{R}^{n+1} | \\langle x, x \\rangle_{\\pounds} = 1/\\kappa, x_t > 0 \\}.\\qquad(1)\\]\nHere, \\(\\langle x, y \\rangle_{\\pounds} = -x_ty_t + x_s^T y_s = x^T g y\\) represents the Lorentzian inner product. Lorentz model, also known as the hyperboloid model, is an upper hyper-surface in an \\((n + 1)\\) dimensional Minkowski space with the origin point \\((\\sqrt{-1/\\kappa}, 0, \u00b7 \u00b7 \u00b7, 0)\\). Lorentz model has its roots in the theory of special relativity [57] and employs terminology borrowed from this field. The hyperboloid's axis of symmetry, represented by the 0-th element \\(x_t\\), is referred to as the time-like dimension, while all other axes \\(x_s\\) are called space-like dimensions.\nTangent Space of Lorentz Model. Given \\(x \\in L^{n,\\kappa}\\), the tangent space \\(T_x L^{n,\\kappa} := \\{u \\in \\mathbb{R}^{n+1} | \\langle u, x \\rangle_{\\pounds} = 0\\}\\) is the orthogonal space of \\(L^{n,\\kappa}\\) at x with respect to the Lorentzian inner product. To achieve the mapping from the Lorentz model to the tangent space at x, we can use the logarithmic map, \\(log_x : L^{n,\\kappa} \\rightarrow T_x L^{n,\\kappa}\\). The exponential map defines the inverse process, \\(exp_x : T_x L^{n,\\kappa} \\rightarrow L^{n,\\kappa}\\). For the details about exponential, logarithmic maps and the relevant distance functions, please refer to Appendix A.\n3.2 Self-Attention Module\nWe first examine the general form of self-attention in Euclidean Transformers. Given the input of N tokens \\(X \\in \\mathbb{R}^{N \\times d}\\), within each head, self-attention can be expressed as:\n\\[\\begin{aligned}Q &= XW^Q, K = XW^K, V = XW^V,\\\\Z_i &= \\sum_{j=1}^N \\frac{Sim (Q_i, K_j)}{\\sum_{k=1}^N Sim (Q_i, K_k)}V_j, \\end{aligned}\\qquad(2)\\]\nwhere \\(W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d'}\\) are projection matrices and \\(Sim(\\cdot, \\cdot)\\) denotes the similarity function. Modern Euclidean Transformers primarily use Softmax attention [65] where similarity is calculated as \\(Sim(Q_i, K_j) = exp (Q_iK_j^T/\\sqrt{d})\\). In this scenario, the attention map is derived by computing the similarity between all query-key pairs, which results in a computational complexity of \\(O (N^2)\\).\nThe concept of hyperbolic self-attention, as defined by previous works [9, 26, 61], bears a similar idea to Equation (2). Figure 2 presents an illustration for this hyperbolic operation on Lorentz model. It can be expressed as follows:\n\\[\\begin{aligned}Q &= X \\otimes^* W^Q, K = X \\otimes^* W^K, V = X \\otimes^* W^V, \\\\Z_i &= \\sum_{j=1}^N \\frac{Sim^* (Q_i, K_j)}{\\sum_{k=1}^N Sim^* (Q_i, K_k)} \\otimes V_j.\\end{aligned}\\qquad(3)\\]\nIn this equation, \\(\\otimes^*\\) denotes the hyperbolic linear transformation, which can be computed using Equations (6) and (7) given in the following section. The symbol \\(\\otimes\\) represents the weighted sum"}, {"title": "4 Method", "content": " scaling range, and h is the activation function. Depending on the type of function, it can perform different operations. For instance, for dropout, the operation function is \\(f (Wx, v) = W dropout (x)\\).\nLimitations. There are several limitations to this method. First, the curvature is unchangeable. Although it appears that LTF provides a way to directly modify \\(\\kappa\\) in Equation (7), this modification results in a loss of previously learned information, introducing distortions. Direct alteration of curvature cannot guarantee the preservation of relative distance relationships within the learned embedding. The derivation is shown as follows:\nLet \\(x' = f (Wx, v)\\), and \\(g(x') = (\\sqrt{||x' ||^2 - 1/\\kappa'}, x')\\). Then:\n\\[d_H (g(x'), g(y')) = \\sqrt{1/|\\kappa'|}arcosh (\\kappa' \\langle g(x'), g(y') \\rangle_{\\pounds})\\\\\t \\qquad = \\sqrt{1/|\\kappa'|}arcosh (\\kappa' \\langle x', y' \\rangle_{\\pounds} / (t_{time} \\cdot t_{time})),\\qquad(8)\\]\nwhere \\(t_{time} = \\sqrt{(\\|x' \\|^2 - 1/\\kappa') (\\|y' \\|^2 - 1/\\kappa')}\\).\nIt can be observed that changing \\(\\kappa\\) results in a non-linear transformation of the Lorentz distance \\(d_H\\). Consequently, the relative distances between data points may not be preserved as they were in the original \\(\\kappa\\) Lorentz space. Even small changes in the parameter \\(\\kappa\\) can significantly affect the resulting distances, potentially distorting the previously learned hierarchical structure.\nSecond, the requirement for the W matrix and normalization term pose another challenge. In [9], W is applied to both timelike and space-like dimensions, in order to achieve Lorentz boosts and rotations simultaneously. However, its introduction constrains the usage of certain functions. For instance, dropout, activation operation do not necessarily interact with the matrix W. Taking the ReLU activation function as an example, it only requires filtering out negative values without needing matrix multiplication in Euclidean space. Additionally, Chen et al. [9] introduced a normalization term that constrains the value within a limited range, thereby limiting the expressiveness of the transformation.\nLastly, some basic operations, such as LayerNorm and Concatenation, cannot be achieved within this definition.\nThe proposed method is designed to overcome the limitations of the existing attempts in hyperbolic Transformer, as outlined in the Section 1. To address Challenges (1) and (2), we designed two foundational blocks, namely HTC and HRC in Section 4.1, and 4.2, respectively. To overcome Challenge (3), we developed a hyperbolic linear attention module in Section 4.3, which equips the Transformer with linear time complexity.\n4.1 Hyperbolic Transformation with\nCurvatures (HTC)\nNovelty. Unlike the design of linear transformation using the tangent space method in Equation (6), we build the transformation fully in hyperbolic space. Besides, compared with Lorentz trans-formation defined by Equation (7), we have two improvements:\nPROPOSITION 4.2. Let \\(z_i, z_j, z_k \\in L^{\\kappa_a}\\) be points in the Lorentz model with curvature \\(\\kappa_a\\). Consider the curvature changing transformations defined in HTC (Equation (7)) and HRC (Equation 13). Let \\(z'_i, z'_j, z'_k \\in L^{\\kappa_b}\\) denote the transformed points in the Lorentz model with curvature \\(\\kappa_b\\). The relative distances within \\((z_i, z_j, z_k)\\) are preserved after the curvature alteration. Specifically, if\n\\[d_{\\kappa_a} (z_i, z_j) \\geq d_{\\kappa_a} (z_i, z_k),\\qquad(11)\\]\nthen\n\\[d_{\\kappa_b} (z'_i, z'_j) \\geq d_{\\kappa_b} (z'_i, z'_k).\\qquad(12)\\]\n4.2 Hyperbolic Readjustment and Refinement\nwith Curvatures (HRC)\nNovelty. Within the Transformer, we have several basic operations beyond linear transformation, which include Dropout and Concatenation, Activation function (e.g., ReLU), and LayerNorm. We interpret these operations within the hyperbolic space as a readjustment or refinement process, referred to as HRC. Similarly, given a point x in Lorentz model, the proposed operation HRC is defined as:\nHere, \\(f_r\\) represents a function applied to the space-like dimensions. It is evident that HRC shares similar advantages with HTC, which we will not repeat for the sake of brevity. However, unlike HTC, HRC performs the transformation only in space-like dimensions. The primary motivation is as follows: HTC involves a Lorentz boost, essential for mapping between different inertial reference frames, tied to causality and affecting the observed sequence of events in relativity. However, operations such as LayerNorm, activation functions, dropout, and concatenation serve as readjustments or refinements within the same frame of reference, acting on space-like features to standardize, activate, or regularize them. Applying these to the space-like dimension ensures the causal structure remains intact. In practical, it ensures dimensional consistency, improves interpretability, and allows for more efficient computation. Nonetheless, it is important to note that HRC does not completely discard the time-like information. According to the definition in Equation (1), the time-like dimension is determined by the space-like dimensions. By operating on the space-like dimensions, HRC implicitly utilizes the time-like information.\n4.3 Hyperbolic Linear Attention\nIn hyperbolic space, the traditional way of calculating self-attention is quadratic time complexity, which hinders scalability. Therefore, we defined a linear attention through HTC and HRC modules.\nSpecifically, given the N input token feature with dimension d, \\(X \\in L^{N \\times d,K_1}\\) in the Lorentz model with transformation matrix"}, {"title": "4.4 Hyperbolic Positional Encoding", "content": "Positional encoding in a Transformer model is instrumental in preserving the sequence of input tokens. In what follows, we introduce a relative positional encoding with a trainable model inspired by [39, 65].\n\\[x = \\frac{x + e \\cdot p}{\\sqrt{\\kappa \\|x + e \\cdot p\\|_{\\pounds}^2 }}.\\qquad(20)\\]\nHere, \\(p := HTC(x)\\) functions as a Lorentz position vector, and e specifies the magnitude of p and we use 1 in our experiments. This definition calculates the midpoint between x and ep, with respect to the Lorentz constraint. We add the positional encoding before the linear transformation in the self-attention block. We reserve the exploration of more advanced positional encoding for future works."}, {"title": "4.5 Hyperbolic LayerNorm, Dropout, Activation,\nand Concatenation", "content": "LayerNorm, Dropout, Activation, and Concatenation are fundamental components of the Transformer architecture. For these operations, we employ HRC in our definitions. This choice is motivated by the fact that these functions are performed within the same reference system and do not involve a time-like dimension. Consequently, we define our operations as follows:\nHypLayerNorm(X) = HRC(X, fLayerNorm),\nHypBatchNorm(X) = HRC(X, fBatchNorm),\nHypDropout(X) = HRC(X, fDropout),\nHypActivation (X) = HRC(X, f\\sigma),\nHypConcatnation(X) = HRC((X_i, X_j), fconcatenation),\n4.6 Overall Architecture\nThe framework of Hypformer is shown in Figure 4, it can accept a variety of data types, such as text, images, and graphs. During the data preparation phase, the input data is mapped to the Lorentz model using an exponential map. This mapped embedding is then transformed using a HTC layer. In the encoder part of Hypformer, the transformed data is processed through a hyperbolic linear attention block with hyperbolic position encoding. This is followed by the Feedforward layer implemented by HTC, and LayerNorm layer built by HRC. For graph-based inputs, we incorporate the graph neural networks and adopt the parallel paradigm [45] for Transformer and GNN encoder to form a graph Transformer model. The processed data is then forwarded to the decoder. The decoder can either be the similar structure of encoder, hyperbolic multinomial logistic regression (HypMLR) [24, 61] or a tailored design, we leave it in future exploration. In this research, the decoder is a fully connected layer used for classification tasks.\nTime complexity. In the proposed Hypformer, the linear attention module is the main computational bottleneck. The complexity comes from two key operations. In Equation (16), we perform a space-like inner product computation of \\(K^T\\) and \\(V\\) within the Lorentz model, which incurs a complexity of \\(O(d'^2N)\\). Following this, we calculate the inner product of these results with Q, which also has a complexity of \\(O(d'^2N)\\). Given that \\(d' << N\\), the total computational complexity of our method is O(N). When dealing with graph inputs, the computational complexity of a GNN model is typical O(N+E), where E represents the number of edges. Owing to the typical sparsity of graphs (i.e., E << \\(N^2\\)), the proposed method can scale linearly with respect to the number of nodes in a graph. This design make Hypformer operate on graphs with billion-level nodes."}, {"title": "5 Experiments", "content": "In this work, we propose a novel hyperbolic Transformer with linear complexity, which is especially well-suited for processing graph-structured data. Graphs often exhibit intricate topological and hierarchical relationships, making them an ideal testbed for evaluating the effectiveness of our proposed hyperbolic Transformer. As such, we primarily focus on comparing our model's performance with other state-of-the-art graph models.\n5.1 Experiments on Large Graphs\nExperimental Settings. We first evaluate Hypformer on diverse large-scale graphs for node classification, with node counts ranging from millions to billions, including ogbn-arxiv, ogbn-protein, and Papers100M (for dataset details, see Appendix C.1). To our knowledge, this represents the first application of hyperbolic or non-Euclidean transformations to graphs of this scale. Our comparative analysis focuses on state-of-the-art Euclidean GNNs and graph Transformers. We evaluate Hypformer against a spectrum of baselines, including MLP, GCN [35], SGC [71]), advanced GNN variants (SIGN [22], GCN-NSampler, GAT-NSampler), recent graph Transformer architectures (GraphFormer [84], GraphTrans [74], GraphGPS [55], NodeFormer [72], SGFormer [73]) and hyperbolic models HAN [26], HNN++ [61] and F-HNN [9].\nExperimental Findings. Table 1 summarizes the results of our experiments. Hypformer consistently outperforms other models across various large-scale graph datasets, demonstrating substantial improvements. It is worth noting that models, such as GraphFormer [84], GraphTrans [74], and GraphGPS [55], HAN [26], HNN++ [61] and F-HNN [9], have difficulty operating effectively on large-scale graph data. In addition, our method significantly outperforms the recent approaches such as, SGFormer and NodeFormer accross all tested scenarios, highlighting its superior effectiveness. Importantly, Hypformer exhibits robust scalability, maintaining its performance advantage even on the largest dataset, ogbn-papers100M, where previous Transformer-based models have encountered limitations.\n5.2 Experiments on Small/Medium Graphs\nTo complement our large-scale evaluations, we assessed Hypformer on small- and medium-scale graph datasets. This additional testing allows for a more comprehensive comparison against current state-of-the-art models, including GNNs, graph transformers, and hyperbolic approaches that may not scale effectively to larger datasets. By expanding our evaluation scope, we aim to isolate Hypformer's effectiveness in graph learning from its scalability advantages.\nExperimental Settings. We conducted experiments on five small/medium-scale graph datasets, adhering closely to the settings used in HGCN works [8]. These datasets included three low-degree"}, {"title": "6 Analysis", "content": "Scalability of Hypformer. We conducted additional tests on the model's scalability regarding the number of nodes in a single batch. The Amazon2M dataset was used, and we randomly selected a subset of nodes, with the number of nodes varying from 10K to 200K. We made a comparison between softmax attention defined by Equation (3) and linear attention defined by Equation (16), keeping all other parameters the same. As depicted in Figure 5, the memory usage of the proposed method exhibits a linear increase with the size of the graph. When the node count exceeds 40K, the softmax attention experiences an out-of-memory (OOM) issue. However, the proposed method continues to function effectively, resulting in a 10X reduction in GPU cost.\nEfficiency and Effectiveness of Hypformer. The linear attention designed for Hypformer enhances its efficiency significantly. Table 4 presents the efficiency of both softmax attention and linear attention within Hypformer. As indicated in Table 4, the proposed linear attention mechanism significantly reduces the training time by half compared to the softmax attention in Hypformer. Furthermore, The left subfigure in Figure 6 presents the performance comparison between Hypformer equipped with Softmax attention (Hypformer(S)) and Linear attention (Hypformer(L)). The results demonstrate that both models perform well, with the linear attention exhibiting better accuracy.\nEffectiveness of Curvature \\(\\kappa\\). In this work, we propose that both the HTC and HRC basic blocks involve two variable curvatures. In our experiment, we set these as trainable parameters. In the right Figure 6, we compare the impact of varying \\(\\kappa\\) and fixed curvature on the Hypformer. Experiments show that varying \\(\\kappa\\) can always perform better than the unified one.\nAblation Study. To gain a deeper understanding of the proposed Hyperbolic Transformer's effectiveness, we conducted an ablation study on three diverse datasets. We compared the performance of the original Hyperbolic Transformer with two variants: one without the graph component (W/o Graph) and another without"}, {"title": "7 Conclusion", "content": "In this work, we introduce a efficient hyperbolic Transformer, Hypformer. This method operates directly and fully on hyperbolic representations and employs a linear attention mechanism, enabling it to be both scalable and effective. Furthermore, this study introduces two basic blocks, HTC and HRC, which are foundational in constructing hyperbolic models. Nonetheless, the research presented is an initial exploration and numerous challenges warrant further investigation. These include the initial determination of a curvature that better reflects the data geometry, the setting of curvature at different levels for Hypformer, and the design of effective decoders for different downstream tasks. We plan to address these issues in our future work."}, {"title": "A Exponential and Logarithmic Map", "content": "Exponential Map. The exponential map, denoted as \\(exp : T_xL^{n,\\kappa} \\rightarrow L^{n,\\kappa}\\), is a function that project any tangent vector u from the tangent space at point x, \\(T_xL^{n,\\kappa}\\), to the manifold \\(L^{n,\\kappa}\\), which is given as\n\\[exp(u) = cosh (\\sqrt{\\kappa} \\|u\\|_{\\pounds}) x+\\frac{sinh (\\sqrt{\\kappa} \\|u\\|_{\\pounds})}{\\sqrt{\\kappa} \\|u\\|_{\\pounds}}u.\\qquad(22)\\]\nLogarithmic Map. The logarithmic map \\(log : L^{N,\\kappa} \\rightarrow L^{n,\\kappa}\\) plays an opposite role, more specifically,\n\\[log^x(u) = \\frac{cosh^{-1} (\\kappa\\langle u,x\\rangle_{\\pounds})}{\\sqrt{\\kappa} \\|u\\|_{\\pounds}}(x-\\kappa\\langle u,x\\rangle_{\\pounds} u). \\qquad(23)\\]\nLorentz Distance. The Lorentz distance between two points \\((x \\in L^{n,\\kappa}, y \\in L^{n,\\kappa})\\) is given as:\n\\[d(x,y) = \\frac{1}{\\sqrt{|\\kappa|}} cosh^{-1}(\\kappa\\langle x,y\\rangle_{\\pounds})\\qquad(24)\\]"}, {"title": "B Proof", "content": "Proof of Proposition 4.1\nPROOF. Let \\(L_x = LTC(x; f_t; W, \\kappa_a, \\kappa_b)\\) and \\(\\langle L_x, L_x \\rangle_{\\pounds} = 1/\\kappa_b\\) holds. Besides, \\(f_t(x; W) : \\mathbb{R}^{d_a+1} \\rightarrow \\mathbb{R}^{d_b}\\). With the time-like dimension re-calibration and concatenation, \\(L_x \\in \\mathbb{R}^{d_b+1}\\). Therefore \\(L_x \\in L^{d_b,\\kappa_b}\\)\nProof of Proposition 4.2\nPROOF. First, let\n\\[Z = \\sqrt{|| f (x; W)||^2 - 1/\\kappa_1}, f (x; W)\\qquad(25)\\]\nand then\n\\[HTC(x, W, \\kappa_1, \\kappa_2) = \\frac{Z}{\\sqrt{K_2}}.\\]\nWe know \\(z \\in L^{K_1}\\) and \\(z' = HTC(x, W, \\kappa_1, \\kappa_2) \\in L^{K_2}\\). Consider the distance between any pair of points in \\(L^{K_1}\\) and \\(L^{K_2}\\):\n\\[\\begin{array}{c}\\\\{d}^{k_1} (z_i, z_j) = \\sqrt{1/|k_1|}arcosh (k_1 \\langle z_i, z_j \\rangle_{\\pounds}),\\end{array}\\]"}, {"title": "C Data Processing and Experimental Details", "content": "C.1 Data Processing for Large-graph Data\nWe employ the public splits offered by OGB [30] for ogbn-proteins and ogbn-arxiv datasets. Additionally, we assess our approach using models on the Amazon2M item co-occurrence network, which comprises 2.45 million nodes and 61.86 million edges. For Amazon2M, we follow the same splits used in recent studies [72, 73]. The largest dataset we employ is ogbn-papers100M, boasting an impressive 0.11 billion nodes and 1.61 billion edges. We also adhere to the publicly available OGB splits for this dataset.\nC.2 Data Processing for Medium-graph Data\nWe used standard splits [34] for the citation networks. For the Airport and Disease datasets, the train/val/test splits were 70%/15%/15% and 30%/10%/60%, respectively, which is the same as [8]. We report the results of five runs on the node classification task. For DISEASE and AIRPORT, which are imbalanced, we report the F1-score. For the other datasets, we report the accuracy. Baselines. For the baselines, we compare Hypformer against the basic GNN models, including"}]}