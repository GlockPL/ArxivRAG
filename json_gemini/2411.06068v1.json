{"title": "Zyda-2: a 5 Trillion Token High-Quality Dataset", "authors": ["Yury Tokpanov", "Paolo Glorioso", "Quentin Anthony", "Beren Millidge"], "abstract": "In this technical report, we present Zyda-2: a five trillion token dataset for language model pretraining. Zyda-2 was used to train our Zamba2 series of models which are state-of-the-art for their weight class. We build Zyda-2 by collating high-quality open-source tokens such as Fine Web and DCLM, then distilling them to the highest-quality subset via cross-deduplication and model-based quality filtering. Zyda-2 is released under a permissive open license, and is available at https://huggingface.co/datasets/Zyphra/Zyda-2.", "sections": [{"title": "I. INTRODUCTION", "content": "Two of the primary determinants of the quality of a language model are the quality and scale of the datasets used to train it. For data quality, initial models such as GPT3 (Brown et al., 2020) were trained purely on web datasets coarsely filtered from Common Crawl*. From there, improving quality via diversity of data sources and the importance of filtering and deduplicating sources have been championed by datasets such as The Pile (Gao et al., 2020) and FineWeb (Penedo et al., 2024), respectively. The scale of datasets used for pretraining has also increased dramatically: while GPT3 (Brown et al., 2020) was trained on only a few hundred billion tokens, current much-smaller models are trained on trillions of tokens. One example of this is Llama3-8b (Meta, 2024), which was trained on 15 trillion tokens. For open-source models to remain on par with closed ones, open-source datasets also need to remain competitive in both quality and scale. To meet this need for open-source, large-scale, and high-quality datasets, sophisticated data pipelines have emerged which scrape, filter, deduplicate, and mix data sources.\nOpen-source datasets have been a key driver of model quality, with C4 (Raffel et al., 2020) and the Pile (Gao et al., 2020) being the first open language modeling dataset of sufficient size to serve as a pretraining dataset. More recent datasets such as Dolma (Soldaini et al., 2024) and RefinedWeb (Penedo et al., 2023) have added additional levels of syntactic filtering and deduplication, raising their quality. A more recent trend has been the use of small classifier models to perform syntactic filtering. For instance, filtering for some proxy of 'educational content' has been used to great effect in the Phi (Abdin et al., 2024) series of models which-although most details are not published-appear to utilize both heavy filtering and a large amount of synthetic data generation. Open datasets utilizing this model-based filtering approach include FineWeb-Edu (Penedo et al., 2024) and DCLM (Li et al., 2024), which perform extremely well compared to prior datasets.\nAt Zyphra, one of our key priorities is producing the highest-quality and most efficient models for a given parameter budget, with a special focus on small, highly powerful models which can automate many tasks cheaply and be utilized on consumer and edge devices. This has driven our innovations in model architecture (Glorioso et al., 2024; Anthony et al., 2024) and also necessitates a focus on constructing strong pretraining and annealing datasets in order to maximize the performance per FLOP and per parameter during training. High-quality datasets appear especially important for smaller models, since they have less total capacity and hence are more affected by significant quantities of noise or low-quality tokens in their training datasets.\nOur general approach to dataset creation is to collect all openly available and highly-performing open-source datasets and improve their quality further by removing duplicates and adding additional filtering steps. We then weight the resulting dataset mixture to obtain highest-quality subset that meets our training budget. Our previous dataset constructed with this approach was Zyda-1 (Tokpanov et al., 2024), which was used to train Zamba1-7B (Glorioso et al., 2024). Zyda-1 outperformed all major language modeling datasets at the time, such as Dolma-1.6 Soldaini et al. (2024), FineWeb (Penedo et al., 2024), and RefinedWeb(Penedo et al., 2023). Since then, however, new datasets utilizing model-based filtering have been released which gave a significant boost to performance and eclipsed Zyda-1. For Zyda-2, we aimed to significantly increase the scale and quality of the dataset beyond Zyda-1, to enable us to reach the frontier of performance for small language models.\nZyda-2 was used to train our Zamba2 series of models (Glorioso, 2024) that achieve state-of-the-art performance at all of the 1.2B, 2.7B, and 7B parameter brackets, beating strong comparable models such as Meta's Llama3 series (Meta, 2024) and Google's Gemma series (Gemma Team et al., 2024). As such, Zyda-2 provides important information about what kind of pretraining dataset is necessary to reach the performance frontier at present.\nZyda-2 is released under a permissive open-source license (ODC-BY) and can be found on HuggingFace at https://huggingface.co/datasets/Zyphra/Zyda-2"}, {"title": "II. DATASET CONSTRUCTION", "content": "Zyda-2 was built upon the following data sources: DCLM-baseline-1.0 (DCLM for short), FineWeb-Edu-score-2 (FineWeb-Edu2 for short), Zyda-1 and the Common Crawl portion of Dolma v1.7 (Dolma-CC for short). We chose the Fine Web-Edu2 version of FineWeb-Edu in order to have a much bigger starting pool of documents. These datasets were put through a two-stage pipeline: a cross-deduplication phase between all datasets, followed by a model-based filtering stage applied to the Zyda-1 and Dolma-CC datasets (see Fig. 1).\nA. Cross-Deduplication\nGiven that all open datasets ultimately originated from similar Common Crawl scrapes, we expect them to contain significant fractions of duplicated documents. Deduplication has been found to generally improve language modeling performance (Lee et al., 2021), although recent papers have claimed its effects can be neutral and perhaps negative when applied in the wrong context (Li et al., 2024).\nWe used approximate minhash LSH deduplication for our deduplication pipeline with the following parameters: minhash with signature size of 128 computed on character-based 25-grams signatures and split into 8 bands, giving roughly 85% Jaccard similarity threshold. We then constructed an undirected graph with nodes being documents and edges being duplicates, and found connected components in it, which provided us with clusters of duplicates. From each cluster, we selected the top document to keep and removed the rest. We selected the remaining document according its origin in the following ranking of datasets: FineWeb-Edu2 > DCLM > Zyda-1 > Dolma-CC; that is, when choosing a document to keep, we kept the one from the highest-ranked dataset.\nAs expected, we found a significant number of duplicated documents across datasets, which resulted in the removal of approximately 11% of the total tokens (or 13% of total documents) from Zyda-2 compared to its component datasets. Additionally, we performed a full intra-dataset deduplication of both DCLM and Fine Web-Edu, since we found that both datasets contained a very large number of internal duplicates (about 80% for both datasets). Both the DCLM and FineWeb-Edu papers claim that internal dataset deduplication did not"}, {"title": "B. Model-based filtering", "content": "The second phase of processing was model-based filtering. We only applied model-based filtering to Zyda-1 and Dolma-CC. This is because Zyda-1 and Dolma-CC are less filtered and contain a higher variety of internet documents which are not all designed to be educational. DCLM and FineWeb-Edu, however, have already undergone significant quality filtering as a core component of their creation, and indeed we did not observe benefits in performing additional filtering from our training ablations.\nFor this step, we experimented with the quality-classifier-deberta model provided in NeMo Curator (Nvidia, 2024). We applied this model to filter both Zyda-1 and Dolma-CC, and experimented with either removing only the 'low' quality documents or keeping only the 'high' quality ones. In an ablation study where we trained a 1.4B parameter transformer for 50B tokens, we found that keeping only the highest quality 10-20% of the documents significantly improved model performance for both Zyda-1 and Dolma-CC (see Figure 2), while only removing 'low' quality documents had less effect.\nThis performance improvement likely occurs because these datasets are not pre-filtered by similar classifiers and demonstrates that, even though both datasets have undergone thorough syntactic filtering, significant gains can be had by using"}, {"title": "C. Composition", "content": "Following the dataset processing steps that we described above resulted in a total dataset consisting of approximately five trillion tokens. The number of tokens removed in each step of our pipeline is presented in Table I. Additionally, the fraction of the total dataset comprised by each of the component datasets is presented in Figure 3.\nThe majority of documents come from DCLM, which is expected given its large size compared to the other datasets."}, {"title": "III. PERFORMANCE", "content": "Rigorously evaluating the quality of datasets is challenging without training large-scale models upon them, which costs significant compute per experiment. One typical alternative to get a reliable signal of dataset quality is to train small models on small slices of the dataset (for instance a 300 million-parameter model on 50 billion tokens) and then compare the evaluation performance of such models. This approach is taken by the majority of recent datasets released. This approach has advantages in that it is now computationally feasible to run many ablation experiments even on relatively constrained"}, {"title": "A. Subset Weightings", "content": "To boost performance further, we investigated the optimal weightings of Zyda-2's component datasets. We conducted a series of experiments to determine this using our annealing scheme described previously. We found that a uniform weighting scheme, where each dataset is weighted by its total number of tokens, is suboptimal. Instead, upweighting FineWeb-Edu such that it became of equal proportion to DCLM exceeded the performance of the uniform weighting approach. With this weighting, due to their smaller sizes, Zyda-1 and Dolma-CC in total make up approximately 5% of the total dataset. However, we found that removing Zyda-1 and Dolma-CC entirely worsened performance, demonstrating that although their total token count is small, adding these datasets brings much-needed diversity of sources to the Zyda-2 dataset."}, {"title": "IV. DISCUSSION", "content": "In this paper we have presented the Zyda-2 dataset, which achieves leading performance across many language evaluations. At 5 trillion tokens, Zyda-2 is perfect for large-scale pretraining. Zyda-2 is primarily focused on natural language capabilities, and, for a generalist language model, we recommend augmenting it with specialized datasets especially for coding, mathematical reasoning, and any other niche capabilities required. We constructed Zyda-2 by collecting the best existing open datasets and applying a two-stage process of cross-deduplication and model-based filtering. We found that model-based filtering significantly improved the performance of the existing unfiltered datasets while having little effect on the already filtered datasets, showing that performing additional educational quality filtering on an already-filtered dataset is not helpful.\nWhile performing deduplication of DCLM and FineWeb-Edu, we found that both datasets contained large fractions of internal duplicates. This result raises several interesting questions. First, it is unclear why removing duplicates does not improve performance. This implies that documents with large numbers of identical tokens provide approximately the same benefit as fresh tokens from the same distribution. We can perhaps consider a highly duplicated dataset like FineWeb-Edu as equivalent to performing a 2-5 epoch shuffled run on a much smaller deduplicated dataset, implying that a small number of epochs do not particularly harm language model performance at these scales. However, this effect may not hold at larger scales, where language models are more sample-efficient at memorization of the training dataset, thus requiring more regularization to counter overfitting. On the other hand, an argument can be made that samples that are repeated more may be of higher quality, although having looked at such highly repetitive samples we doubt this since most seem to be related to common preambles or other features of websites. If this is true, however, it is unclear why earlier results showed positive effects of deduplication (Lee et al., 2021). One possible hypothesis is that the early datasets where deduplication improved performance were not strongly filtered by model-based classifiers like DCLM and FineWeb-Edu, and thus the deduplication step may simply be removing many low-quality spam documents that occur often in unfiltered web data. In more stringently filtered datasets, the magnitude of this effect could diminish. However, we do note that large clusters of duplicates in DCLM still contain low-quality documents: the most frequent duplicate in DCLM is indeed a spam message. Additionally, we notice that on supposedly duplicate data, the scores assigned by the quality-filtering model often vary significantly, despite only very minor differences in text. This is perhaps indicative of the relative lack of robustness in some of these classifiers or the presence of significant fractions of false-positives in the deduplication step.\nModel-based quality filtering based on 'educational quality' as performed by FineWeb-Edu and DCLM has created a marked increase in language modeling performance, at least on commonly used language modeling benchmarks such as MMLU and the ARC tasks, which is where we observe the largest increases. Interesting questions remain around the degree to which model-based filtering can be further improved beyond current methods. Synthetic data (Abdin et al., 2024), including augmentations of existing datasets (Maini et al., 2024), shows considerable promise (Maini et al., 2024), as does methods for using other filtering approaches such as language modeling perplexity (Ankner et al., 2024)."}, {"title": "APPENDIX", "content": "A. Analysis of Global Duplicates\nWe present histograms depicting the distribution of duplicate cluster sizes in all the datasets (see Fig.6-10). Please, note that all the figures uselog-log scale. We see a significant drop in the number of duplicate clusters starting from around a size of 100 duplicates. This drop is present both in DCLM and FineWeb-Edu2 (see Fig. 7 and 8 respectively), and most likely is explained by a combination of the deduplication strategy when creating both datasets: DCLM deduplication was done individually within 10 shards, while FineWeb-Edu2 was deduplicated within every Common Crawl snapshot. We find that large clusters usually contain low quality material (repeated advertisements, license agreements templates, etc), so it's not surprising that such documents were removed. Notably, DCLM still contained one cluster with the size close to 1 million documents, containing low quality documents seemingly coming from the advertisements, which was somehow accepted by their quality filter.\nWe find that both Zyda-1 and Dolma-CC contain only a small amount of duplicates, which is expected, since both datasets were deduplicated globally by their authors. Remaining duplicates are likely false negatives from the initial deduplication procedure, or false positives from our own deduplication. Note, that the distribution of duplicate clusters sizes of these two datasets (Fig. 9 and 10) don't contain any sharp drops, but rather hyper exponentially decreases with cluster size."}, {"title": "B. Largest cluster in DCLM", "content": "Below is an example of the document from the largest cluster (roughly 1M documents) of duplicates in DCLM (quality score 0.482627):\nIs safe? Is scam? Is safe for your PC? Is\nsafe or is it scam? Domain is Safe Safe\nscore: 1\nThe higher the number, the more dangerous\nthe website. Any number higher than 1\nmeans DANGER.\nPositive votes: Negative votes: Vote Up\nVote Down review\nHave you had bad experience with Warn us,\nplease!\nAs can be observed this appears to be some kind of fairly\nlow quality advertisement for some computer security product."}, {"title": "C. Examples of varying quality score in DCLM in a cluster", "content": "To get a sense of what the DCLM quality filter is doing, we present below a few documents, which are selected from the same duplicate cluster, but with different quality scores from DCLM. Quality score varies from 0.2 (high quality) to 0.04 (low quality).\nQuality score of: 0.19616\nThrill Jockey instrumental duo Rome\nare, like many of the acts on the\nChicago-based independent label,\ngenerally categorized as loose adherents\nof \"post-rock,\" a period-genre arising in\nthe mid-'90s to refer to rock-based bands\nutilizing the instruments and structures\nof music in a non-traditionalist or\notherwise heavily mutated fashion. Unlike\nother Thrill Jockey artists such as\nTortoise and Trans-Am, however, Rome\ndraw less obviously from the past,\nusing instruments closely associated\nwith dub (melodica, studio effects),\nambient (synthesizers, found sounds),\nindustrial (machine beats, abrasive\nsounds), and space music (soundtrack-y\natmospherics), but fashioning from\nthem a sound which clearly lies beyond\nthe boundaries of each. Perhaps best\ndescribed as simply \"experimental,\" Rome\nformed in the early '90s as the trio of\nRik Shaw (bass), Le Deuce (electronics),\nand Elliot Dicks (drums). Based in\nChicago, their Thrill Jockey debut\nwas a soupy collage of echoing drums,\nlooping electronics, and deep, droning\nbass, with an overwhelmingly live feel\n(the band later divulged that much of\nthe album was the product of studio\njamming and leave-the-tape-running-styled\nimprovisation). Benefiting from an early\nassociation with labelmates Tortoise as\nrepresenting a new direction for American\nrock, Rome toured the U.S. and U.K. with\nthe group (even before the album had\nbeen released), also appearing on the\nGerman Mille Plateaux label's tribute\ncompilation to French philosopher Gilles\nDeleuze, In Memoriam. Although drummer\nDicks left the group soon after the first\nalbum was released, Shaw and Deuce wasted\nno time with new material, releasing the\n\"Beware Soul Snatchers\" single within\nweeks of its appearance. An even denser\nslab of inboard studio trickery, \"Soul\nSnatchers\" was the clearest example\nto date of the group's evolving sound,\nthough further recordings failed to\nmaterialize. Sean Cooper, Rovi\nQuality score: 0.091928\nThrill Jockey instrumental duo Rome\nare, like many of the acts on the\nChicago-based independent label,\ngenerally grouped in as loose adherents\nof \"post-rock,\" a period-genre arising in\nthe mid-'90s to refer to rock-based bands\nutilizing the instruments and structures\nof the music in a non-traditionalist\nor otherwise heavily mutated fashion.\nUnlike other Thrill Jocky artists such as\nTortoise and Trans-Am, however, Rome\ndraw less obviously from the past,\nusing instruments closely associated\nwith dub (melodica, studio effects),\nambient (synthesizers, found sounds),\nindustrial (machine beats, abrasive\nsounds), and space music (soundtrack-y\natmospherics), but fashioning from\nthem a sound which lay clearly beyond\nthe boundaries of each. Perhaps best\ndescribed as simply experimental, Rome\nformed in the early '90s as the trio of\nRik Shaw (bass), Le Deuce (electronics),\nand Elliot Dick (drums). Based in\nChicago, their Thrill Jockey debut\nwas a soupy collage of echoing drums,\nlooping electronics, and deep, droning\nbass, with an overwhelmingly live feel\n(the band later divulged that much of\nthe album was the product of studio\njamming and leave-the-tape-running styled\nimprovisation). Benefiting from an early\nassociation with labelmates Tortoise as\nrepresenting a new direction for American\nrock, Rome toured the U.S. and U.K. with\nthe group (even before the album had\nbeen released), also appearing on the\nGerman Mille Plateaux label's tribute\ncompilation to French philosopher Gilles\nDeleuze, In Memoriam. Although drummer\nElliot Dick left the group soon after\nthe first album was released, Shaw and\nDeuce wasted no time with new material,\nreleasing the \"Beware Soul Snatchers\"\nsingle within weeks of its appearance.\nAn even denser slab of inboard studio"}, {"title": "Quality score: 0.072259", "content": "trickery, \"Soul Snatchers\" was the\nclearest example to date of the group's\nevolving sound, though further recordings\nfailed to materialize. Sean Cooper, Rovi\nMore Rome\nYou may also like...\nQuality score: 0.072259\nrecent on-air advertisers\nNow Playing\nYou Control the\nArtist Snapshot:\nThrill Jockey instrumental duo Rome\nare, like many of the acts on the\nChicago-based independent label,\ngenerally grouped in as loose adherents\nof \"post-rock,\" a period-genre arising in\nthe mid-'90s to refer to rock-based bands\nutilizing the instruments and structures\nof the music in a non-traditionalist\nor otherwise heavily mutated fashion.\nUnlike other Thrill Jocky artists such as\nTortoise and Trans-Am, however, Rome\ndraw less obviously from the past,\nusing instruments closely associated\nwith dub (melodica, studio effects),\nambient (synthesizers, found sounds),\nindustrial (machine beats, abrasive\nsounds), and space music (soundtrack-y\natmospherics), but fashioning from\nthem a sound which lay clearly beyond\nthe boundaries of each. Perhaps best\ndescribed as simply experimental, Rome\nformed in the early '90s as the trio of\nRik Shaw (bass), Le Deuce (electronics),\nand Elliot Dick (drums). Based in\nChicago, their Thrill Jockey debut\nwas a soupy collage of echoing drums,\nlooping electronics, and deep, droning\nbass, with an overwhelmingly live feel\n(the band later divulged that much of\nthe album was the product of studio\njamming and leave-the-tape-running styled\nimprovisation). Benefiting from an early\nassociation with labelmates Tortoise as\nrepresenting a new direction for American\nrock, Rome toured the U.S. and U.K. with\nthe group (even before the album had\nbeen released), also appearing on the\nGerman Mille Plateaux label's tribute\ncompilation to French philosopher Gilles\nDeleuze, In Memoriam. Although drummer\nElliot Dick left the group soon after\nthe first album was released, Shaw and\nDeuce wasted no time with new material,\nreleasing the \"Beware Soul Snatchers\"\nsingle within weeks of its appearance.\nAn even denser slab of inboard studio\ntrickery, \"Soul Snatchers\" was the\nclearest example to date of the group's\nevolving sound, though further recordings\nfailed to materialize. Sean Cooper,\nRovi"}, {"title": "Quality score: 0.0424", "content": "18 June 2015\nROME self titled 1996\nby request\nArtist Biography by\nThrill Jockey instrumental duo Rome\nare, like many of the acts on the\nChicago-based independent label,\ngenerally categorized as loose adherents\nof \"post-rock,\" a period-genre arising in\nthe mid-'90s to refer to rock-based bands\nutilizing the instruments and structures\nof music in a non-traditionalist or\notherwise heavily mutated fashion. Unlike\nother Thrill Jockey artists such as\nTortoise and Trans-Am, however, Rome\ndraw less obviously from the past,\nusing instruments closely associated\nwith dub (melodica, studio effects),\nambient (synthesizers, found sounds),\nindustrial (machine beats, abrasive\nsounds), and space music (soundtrack-y\natmospherics), but fashioning from\nthem a sound which clearly lies beyond\nthe boundaries of each. Perhaps best\ndescribed as simply \"experimental,\" Rome\nformed in the early '90s as the trio of\nRik Shaw (bass), Le Deuce (electronics),\nand Elliot Dicks (drums). Based in\nChicago, their Thrill Jockey debut\nwas a soupy collage of echoing drums,\nlooping electronics, and deep, droning\nbass, with an overwhelmingly live feel\n(the band later divulged that much of\nthe album was the product of studio\njamming and leave-the-tape-running-styled\nimprovisation). Benefiting from an early\nassociation with labelmates Tortoise as\nrepresenting a new direction for American\nrock, Rome toured the U.S. and U.K. with\nthe group (even before the album had\nbeen released), also appearing on the\nGerman Mille Plateaux label's tribute\ncompilation to French philosopher Gilles\nDeleuze, In Memoriam. Although drummer\nDicks left the group soon after the first\nalbum was released, Shaw and Deuce wasted\nno time with new material, releasing the\n\"Beware Soul Snatchers\" single within\nweeks of its appearance. An even denser\nslab of inboard studio trickery, \"Soul\nSnatchers\" was the clearest example\nto date of the group's evolving sound,\nthough further recordings failed to\nmaterialize.\n1 Leaving Perdition 8:10 2 Intermodal\n3:39 3 Lunar White 3:25 4 She's A Black\nBelt 3:14 5 Rohm 1:09 6 Radiolucence\n(Version) 5:31 7 Deepest Laws 14:14\nNo comments:"}]}