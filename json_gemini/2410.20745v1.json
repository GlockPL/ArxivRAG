{"title": "Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large Language Models", "authors": ["Yilun Jin", "Zheng Li", "Chenwei Zhang", "Tianyu Cao", "Yifan Gao", "Pratik Jayarao", "Mao Li", "Xin Liu", "Ritesh Sarkhel", "Xianfeng Tang", "Haodong Wang", "Zhengyang Wang", "Wenju Xu", "Jingfeng Yang", "Qingyu Yin", "Xian Li", "Priyanka Nigam", "Yi Xu", "Kai Chen", "Qiang Yang", "Meng Jiang", "Bing Yin"], "abstract": "Online shopping is a complex multi-task, few-shot learning problem with a wide\nand evolving range of entities, relations, and tasks. However, existing models and\nbenchmarks are commonly tailored to specific tasks, falling short of capturing\nthe full complexity of online shopping. Large Language Models (LLMs), with\ntheir multi-task and few-shot learning abilities, have the potential to profoundly\ntransform online shopping by alleviating task-specific engineering efforts and by\nproviding users with interactive conversations. Despite the potential, LLMs face\nunique challenges in online shopping, such as domain-specific concepts, implicit\nknowledge, and heterogeneous user behaviors. Motivated by the potential and\nchallenges, we propose Shopping MMLU, a diverse multi-task online shopping\nbenchmark derived from real-world Amazon data. Shopping MMLU consists of 57\ntasks covering 4 major shopping skills: concept understanding, knowledge reason-\ning, user behavior alignment, and multi-linguality, and can thus comprehensively\nevaluate the abilities of LLMs as general shop assistants. With Shopping MMLU,\nwe benchmark over 20 existing LLMs and uncover valuable insights about practices\nand prospects of building versatile LLM-based shop assistants. Shopping MMLU\ncan be publicly accessed at https://github.com/KL4805/ShoppingMMLU. In\naddition, with Shopping MMLU, we host a competition in KDD Cup 2024 2 with\nover 500 participating teams. The winning solutions and the associated workshop\ncan be accessed at our website https://amazon-kddcup24.github.io/.", "sections": [{"title": "1 Introduction", "content": "Machine learning (ML) has been applied to various user-oriented online services, such as online\ncommunities, streaming services, etc, with online shopping being among the most successful ones.\nIn recent years, ML methods are applied to various online shopping tasks, such as user queries\n[25, 19, 15], sessions [45, 24], reviews [28, 27], product attributes [53, 38], etc. To facilitate the\ndevelopment of ML methods, many benchmarks are designed [16, 33] to lower the barrier for\nresearchers and engineers to develop and evaluate novel solutions to real-world online shopping tasks."}, {"title": "2 Related Work", "content": "Online Shopping Datasets We summarize related online shopping datasets in Table 1. Previously,\nonline shopping datasets often focus on one or several closely related tasks, e.g. MAVE [48] for\nattribute value extraction, Amazon-M2 [16] for session-based recommendation, Amazon-ESCI [33]\nfor query-product matching, etc. Consequently, they fail to reflect the multi-task nature of online\nshopping as their coverage of tasks and skills is limited.\nMore recently, multi-task online shopping datasets are curated to build versatile LLM-based shop\nassistants, such as EComInstruct for EcomGPT [20] and ECInstruct for eCeLLM [30]. Both EComIn-\nstruct and ECInstruct reformulate online shopping tasks into text-to-text generation and fine-tune a\nsingle LLM to perform all tasks. However, despite being multi-task datasets, EComInstruct solely\nfocuses on shopping concept understanding (12 out of 12 tasks), while ECInstruct primarily tackles\nuser behavior alignment (8 out of 10 tasks). Therefore, their coverage of skills in online shopping is\nstill limited compared to Shopping MMLU, especially in reasoning and multi-lingual abilities.\nLLMs for Online Shopping Shopping websites house various texts such as product titles, descrip-\ntions, reviews, ads, etc., motivating an extensive study of LLM solutions to online shopping tasks,\nsuch as recommendation [42, 18], ranking [12], named entity recognition [39], etc. However, these\nmethods are limited to specific tasks without fully exploring the multi-task nature of LLMs.\nMore recent works leverage instruction fine-tuning (IFT) to adapt general domain LLMs to online\nshopping, such as EcomGPT and eCeLLM. However, as shown in Table 1, the capabilities of\nEcomGPT and eCeLLM may be limited as their training data cover a limited range of shopping tasks\nand skills.\nWeb Agent Benchmarks for Online Shopping LLMs bring about exciting prospects in developing\nagents that can perform sequential decision making and task execution following text instructions.\nAs online shopping websites are diverse, realistic, and interactive environments, many benchmarks\nare developed upon them, such as WebShop [49] and WebArena [54] where agents are required"}, {"title": "3 Dataset and Task Description", "content": "In this section, we present the overall design of Shop-\nping MMLU, featuring 57 tasks across 4 key skills\nbased on real-world Amazon data. We present the raw\ndata sources used, the task taxonomy, task designs,\nand evaluation metrics. Finally, we describe our ef-\nforts to improve the quality of Shopping MMLU."}, {"title": "3.1 Raw Data Sources", "content": "Shopping MMLU is curated primarily with real-\nworld, internal or public [11, 8, 9, 16, 33] Amazon\ndata, such as product catalogs, reviews, browse ses-\nsions, queries, etc. We remove all IDs (e.g. userID,\nsessionID, etc.) to ensure anonymity. We also use\nClaude 2 [1] to synthesize data for some tasks that\ndo not involve concrete product or user data. Details\nof raw data sources are given in Appendix A.2."}, {"title": "3.2 Online Shopping Tasks", "content": "In this section, we introduce the task designs of Shop-\nping MMLU, including the taxonomy, task types, and evaluation metrics."}, {"title": "3.2.1 Task Taxonomy", "content": "Shopping MMLU consists of 57 tasks across 4 shopping skills corresponding to Figure 1. Moreover,\nwe divide each skill into sub-skills to enable more fine-grained evaluation. A simplified taxonomy is\nshown in Figure 2, with the full taxonomy in Figure 9 in the Appendix. We introduce each skill and\ntheir sub-skills as follows, and leave more details in Appendix A.3.\nShopping Concept Understanding (\"Concept\" for short). Online shopping concepts such as\nbrands and product models are domain-specific and not often seen in pre-training. Moreover, they\noften appear in short texts (e.g. queries, attribute-value pairs) and thus no sufficient contexts are given\nto help understand them. Hence, failing to understand these concepts compromises the performance of\nLLMs on downstream tasks. We include the following sub-skills in this skill: concept normalization,\nelaboration, relational inference, sentiment analysis, information extraction, and summarization.\nShopping Knowledge Reasoning (\"Reasoning\" for short). This skill focuses on understanding\nand applying various implicit knowledge to perform reasoning over products and their attributes. For\nexample, calculations such as the total volume of a product pack require numeric reasoning, and\nfinding compatible products requires multi-hop reasoning among various products over a product\nknowledge graph. Based on the specific type of reasoning required, we split this skill into three\nsub-skills, numeric, commonsense, and multi-hop reasoning.\nUser Behavior Alignment (\"Behavior\" for short). Accurately modeling user behaviors is a crucial\nskill in online shopping. A large variety of user behaviors exist in online shopping, including queries,\nclicks, add-to-carts, purchases, etc. Moreover, these behaviors are generally implicit and not expressed\nin text. Consequently, LLMs trained with general texts encounter challenges in aligning with the\nheterogeneous and implicit user behaviors as they rarely observe such inputs during pre-training. We\nfurther design the following sub-skills to reflect such heterogeneous behaviors: query-query relation,\nquery-product relation, sessions, purchases, and reviews & QAs."}, {"title": "3.2.2 Task Types", "content": "We include 5 types of tasks in Shopping MMLU for a comprehensive evaluation of shopping skills,\nincluding multiple choice, retrieval, ranking, named entity recognition, and generation. Due to\ndifferent format requirements, each type of task requires specific prompts such that the evaluated\nLLMs follow the instructions and generate valid answers, which we show in Appendix A.5."}, {"title": "Evaluation Metrics", "content": "We use accuracy for multiple choice tasks, hit rate@3 for retrieval tasks,\nnormalized discounted cumulative gain (NDCG) for ranking tasks, and micro F1 for named entity\nrecognition tasks. For generation tasks, we apply ROUGE-L scores for extraction tasks (i.e. the\nanswer is a sub-string of the input), BLEU scores for translation tasks, and sentence transformer\nsimilarity [34] for other generation tasks. Details of the metrics are introduced in Appendix A.4. We\ntake an average of all task-wise metrics (i.e. macro average) as the score of a skill."}, {"title": "3.3 Data Quality Control", "content": "Datasets of online shopping are either defined by human behaviors or are human-labeled, and thus\nmay contain noise or errors. To address the issue, we manually inspect all data samples to ensure the\nvalidity of the questions. We also remove potentially offensive contents and all links to images and\nvideos in product descriptions and reviews. Details of data filtering are described in Appendix A.6."}, {"title": "4 Experiments and Analyses", "content": "In this section, we present our experimental setup, results, and analyses based on Shopping MMLU.\nOur experiments uncover the following insights:\n\u2022 Proprietary LLMs remain the state-of-the-arts on Shopping MMLU, with Claude-3 Sonnet\nperforming the best overall. However, strong open-source LLMs have caught up with proprietary\nones like ChatGPT.\n\u2022 Tasks and skills in Shopping MMLU, and hence online shopping share much knowledge in\ncommon, as indicated by the highly positive correlations between pairwise tasks and skills in\nShopping MMLU.\n\u2022 General knowledge transfers well to the specific domain of online shopping. Strong models on\ngeneral LLM benchmarks remain strong on Shopping MMLU.\n\u2022 IFT improves the performance on Shopping MMLU in most cases. However, general domain\nIFT may lead to overfitting and hence compromise the contained knowledge in strong base\nmodels, while domain-specific IFT works only on strong base models and observed tasks and\nskills.\n\u2022 Few-shot learning remains challenging on Shopping MMLU. In-context examples lead to worse\nperformances for many models and tasks."}, {"title": "4.1 Experimental Setup", "content": "We apply zero-shot evaluation for Shopping MMLU for three main reasons. First, zero-shot evaluation\nresembles the real-world scenario where customers directly enter their questions without creating\nfew-shot examples. Second, zero-shot evaluation rules out variances brought by different few-shot\nexamples. Finally, all evaluated models achieve non-trivial results under zero-shot evaluation on\nShopping MMLU. All models are tested with the same prompts."}, {"title": "4.2 Evaluated Models", "content": "We evaluate LLMs with various sizes and training methods to uncover insights about how to build\ndomain-specific LLMs. Details of model access is given in Appendix B.1. Evaluated models include:"}, {"title": "4.3 Overall Performance", "content": "We show the scores of all evaluated models on each skill of Shopping MMLU in Table 2. Due to\nspace limitations, we omit detailed task-wise scores. We draw the following insights from Table 2.\nFirst, proprietary LLMs remain the state-of-the-art, while open-source LLMs are catching up.\nClaude-3 Sonnet performs the best across all models, followed by Claude-2 and ChatGPT. Overall,\nthese proprietary LLMs remain the strongest even in the specific domain of online shopping. We also\nobserve that LLaMA3-70B-Instruct and QWen1.5-72B perform on par with ChatGPT and Claude-2,\ndemonstrating the potential of building powerful LLM shop assistants with public resources.\nSecond, Shopping MMLU is a challenging benchmark. While eCeLLMs outperform GPT-4 on\ntheir dataset ECInstruct [30], they are still far behind ChatGPT on Shopping MMLU, showing that\nShopping MMLU is a more complex and challenging benchmark for online shopping than ECInstruct."}, {"title": "4.4 How 'Multi-task' is Online Shopping?", "content": "According to [52], the key of multi-task learning is to leverage useful information in multiple tasks to\nimprove the performances of all tasks. Consequently, the more the shared knowledge, the more likely\nwe can jointly improve all tasks in Shopping MMLU and build versatile LLM-based shop assistants.\nThus, in this section, we analyze the extent to which knowledge is shared among tasks in Shopping\nMMLU by analyzing the score correlations between pairwise tasks and skills.\nWe first analyze the task-wise score correlations. Let $s_i$ be the scores achieved by all evaluated LLMs\non task i, the score correlation between tasks i and j is defined as $C_{ij} = PearsonCorr(s_i, s_j)$. The\ndistribution of $C_{ij}$ is shown in Figure 3(a). As shown, the scores of most task pairs (1589 out of 1596)\nare positively correlated. Moreover, with an average of 0.557 and a standard deviation of 0.209, the\nscore correlations are significantly positive, indicating a notable amount of shared knowledge among\ntasks in Shopping MMLU. We analyze task pairs with negative correlations in Appendix B.3.\nWe similarly compute the score correlations between pairwise skills and plot them in Figure 3(b).\nAs shown, all skills are positively correlated with each other with correlations of at least 0.9. The\nobservation further underscores the multi-task nature of Shopping MMLU and the potential of jointly\nimproving online shopping skills as a whole with unified solutions."}, {"title": "4.5 How to Build LLM-based Shop Assistants?", "content": "In this section, we analyze various LLM moderation techniques, including model scaling, IFT, and\nin-context learning, to see whether and how they are helpful in improving the performances of LLMs\non the specific domain of online shopping."}, {"title": "4.5.1 General Knowledge Transfers Well to Online Shopping", "content": "The field of LLMs advances at a rapid pace, yielding models with increasingly powerful capabilities.\nTherefore, we analyze whether the specific domain of online shopping benefits from the advancing\nLLMs and their increasing general knowledge and abilities. We calculate the score correlations\nbetween each skill in Shopping MMLU and the Open LLM Leaderboard [4], consisting of MMLU,\nGSM8K, Winogrande, HellaSwag, TruthfulQA, and ARC [10, 7, 23, 50, 36, 6]. The correlations are\nshown in Figure 4(a), where all skills show strongly positive correlations with the Open LLM Leader-\nboard scores. The high correlations indicate that general knowledge transfers well to the specific\ndomain of online shopping, and that powerful LLM-based shop assistants should be established upon\nstrong base models."}, {"title": "4.5.2 Effects of Instruction Fine-tuning", "content": "In this section, we analyze the effects of IFT [40] on\nShopping MMLU. We analyze both general domain\nand domain-specific IFT to understand whether the in-\nstruction following ability transfers from the general\ndomain to online shopping, and how domain-specific\nIFT achieves further improvement."}, {"title": "5 Conclusion and Future Work", "content": "This paper presents Shopping MMLU, a multi-task online shopping benchmark for LLMs aiming\nto facilitate LLMs-based solutions to a unified, multi-task modeling of online shopping. Shopping\nMMLU features a wide range of online shopping skills, tasks, and entities, and thus is suitable for\nresearchers and practitioners to comprehensively evaluate their solutions of domain-specific LLM\nonline shop assistants. With Shopping MMLU, we perform extensive experiments on over 20 LLMs,\nwhose results uncover valuable insights on building domain-specific LLMs for online shopping, such\nas task- and skill-wise relations, general knowledge, instruction fine-tuning, and in-context learning.\nShopping MMLU triggers a series of future work. In Appendix C we show that state-of-the-\nart proprietary LLMs still lags behind task-specific methods on some tasks of Shopping MMLU,\nmotivating advanced training recipes and data for LLMs in online shopping. We also discuss broader\nimpacts and limitations in Appendix C."}, {"title": "A More Dataset and Task Details", "content": "We summarize the data sources we use in this Section."}, {"title": "A.1 License", "content": "Following the licenses of similar datasets [16, 8], Shopping MMLU can be freely used under the\nlicense of Apache 2.0."}, {"title": "A.2 Data Sources", "content": "We summarize the data sources we use in this Section."}, {"title": "A.2.1 Public Data from Amazon", "content": "Some tasks of Shopping MMLU are created with public data from Amazon, including:\n\u2022 Amazon-M2 [16], including browse sessions and product metadata from 6 marketplaces\n(United Kingdom (UK), Spain (ES), Germany (DE), Japan (JP), France (FR), and Italy (IT)).\n\u2022 Amazon-ESCI [33], including user queries, related products, as well the relevance between\nthe query and the products. The data come in 3 languages, English, Japanese, and Spanish.\n\u2022 Amazon Product Keyphrases [8], including product metadata (title, descriptions) as well\nas product keyphrases derived from users' queries. The data come in 5 languages, English,\nGerman, Spanish, French, and Italian.\n\u2022 Amazon Reviews [11], including product metadata, user reviews to products, as well as\nvarious tags such as the number of upvotes received by each review, product-product\nrelations (also-buy, also-view), etc.\n\u2022 Amazon QA [9], including user-generated questions and answers about products, as well as\nproduct reviews that may be related to the question. The goal of the dataset is to automatically\ngenerate answers to user questions based on contexts in the reviews."}, {"title": "A.2.2 Internal Data", "content": "Shopping MMLU is primarily constructed with internal data from Amazon. They can be roughly\ncategorized into four classes.\n\u2022 Catalog Data, which contains the ontology of Amazon to organize products. Catalog data\ncontains the hierarchy, meanings, and relationships (e.g. applicability, complementarity) of\nproduct categories, attributes, attribute values, etc.\n\u2022 Product Data, which contains product metadata such as attributes and values, product\ncategories, titles, descriptions, etc.\n\u2022 Review Data, which contains user reviews to products along with fine-grained labels like\naspects, sentiments, and keyphrases.\n\u2022 Browse Data, which contains user behaviors such as sessions, queries, as well as datasets\nderived from user behaviors (e.g. query-product category relations, query-attribute relations,\netc.).\nWe remove all identifiers (e.g. sessionID, userID, productID, reviewID) for all internal data to\nmaintain anonymity. We have obtained approval from the Amazon legal team to publish the data."}, {"title": "A.2.3 Synthetic Data", "content": "We use Claude-2 to synthesize data for tasks that do not involve specific products, users, etc.,\nincluding:\n\u2022 Unit conversion. We sample a set of units that are used in shopping and ask Claude-2 to\ngenerate unit conversion questions within these units.\n\u2022 Shopping Commonsense. We use Claude-2 to sample questions from ATOMIC10X [43]\nthat are related to shopping and products."}, {"title": "A.3 Full Task Taxonomy", "content": "We provide detailed introduction of each sub-skill as follows.\nShopping Concept Understanding\nWe include the following sub-skills in this skill.\n1. Concept Normalization, which measures the ability to unify terms with the same meanings.\nFor example, 'USB3.0', 'USB3.1Gen 1', and 'USB3.2 Gen1' refer to the same USB standards.\n2. Elaboration, which tests the model's ability to explain products in plain and understandable\nlanguages to facilitate customer shopping decisions.\n3. Extraction and Summarization, which focuses on the model's ability to extract specific details\nor provide concise and informative summaries from long product descriptions.\n4. Relational Inference, which focuses on the compatibility and interactions between concepts\n(e.g. product category and attribute, attribute and attribute value, etc.).\n5. Sentiment Analysis, which requires the model to extract fine-grained aspects and sentiments\nfrom customer reviews, and thus recommend users with high-quality products.\nShopping Knowledge Reasoning (\"Reasoning\" for short). Based on the type of reasoning required,\nthis skill is divided into:\n1. Numeric Reasoning, where the LLM extracts necessary numeric information from product\nmetadata and perform calculations to derive results.\n2. Commonsense Reasoning, which tests the model's ability to infer and reason over common-\nsense knowledge of daily products (e.g. intended usage and purpose).\n3. Multi-hop Reasoning, which requires the model to draw connections across multiple entities\nand relations in online shopping to get the answer (e.g. similarity, compatibility, complementar-\nity, etc.).\nUser Behavior Alignment (\"Behavior\" for short). Accurately modeling user behaviors is a crucial\nskill in online shopping. Various kinds of user behaviors exist in online shopping, including queries,\nclicks, add-to-carts, purchases, etc. Moreover, these behaviors are generally implicit and not expressed\nin languages. Consequently, LLMs trained with general texts encounter challenges in aligning with\nthe heterogeneous and implicit user behaviors as they rarely observe such inputs during pre-training.\nWe further design the following sub-skills, each of which focuses on a specific type of user behavior.\n1. Queries. Most online shopping experiences starts with a user query that reflect the user's initial\nintentions. Afterwards, the user may either initiate other related queries, or browse products\nthat meet his intentions. We thus include two sub-skills corresponding to both scenarios,\nquery-query relation and query-product relation.\n2. Sessions, which evaluates how well the model understands a user's short-term shopping interests\nand recommend the user with the next possible product or query.\n3. Purchases, which focuses the model's ability to help users directly make purchase decisions\nwithout the arduous process of searching and browsing.\n4. Reviews and QAs, which requires the model to provide helpful feedbacks to various user-\ngenerated contents on an online shopping platform, such as answering product-related questions,\nand casting votes to informative reviews.\nMulti-lingual Abilities (\"Multi-lingual\" for short). We include sub-skills of multi-lingual shop-\nping concept understanding, and multi-lingual user behavior alignment in this skill, which are\nmulti-lingual versions of the corresponding skills, respectively.\nThe full taxonomy containing skills, sub-skills and tasks are shown in Figure 9. Detailed task\ndescriptions are shown in Table 3, 4, 5, and 6 for each skill."}, {"title": "4.4 How 'Multi-task' is Online Shopping?", "content": "We first analyze the task-wise score correlations. Let $s_i$ be the scores achieved by all evaluated LLMs\non task i, the score correlation between tasks i and j is defined as $C_{ij} = PearsonCorr(s_i, s_j)$.\nThe distribution of $C_{ij}$ is shown in Figure 3(a). As shown, the scores of most task pairs (1589 out of 1596)\nare positively correlated. Moreover, with an average of 0.557 and a standard deviation of 0.209, the\nscore correlations are significantly positive, indicating a notable amount of shared knowledge among\ntasks in Shopping MMLU. We analyze task pairs with negative correlations in Appendix B.3.\nWe similarly compute the score correlations between pairwise skills and plot them in Figure 3(b).\nAs shown, all skills are positively correlated with each other with correlations of at least 0.9. The\nobservation further underscores the multi-task nature of Shopping MMLU and the potential of jointly\nimproving online shopping skills as a whole with unified solutions."}, {"title": "Evaluation Metrics", "content": "hit rate@3 = $\\frac{|truth \\bigcap retr|}{|truth|}$"}, {"title": "Evaluation Metrics", "content": "DCG = $\\sum_{i=1}^{5} \\frac{rel(rank_i)}{log_2(i+1)}$"}, {"title": "Evaluation Metrics", "content": "NDCG = $\\frac{DCG}{iDCG}$"}, {"title": "Evaluation Metrics", "content": "Precision = $\\frac{TP}{TP + FP}$, Recall = $\\frac{TP}{TP + FN}$, F1 = $\\frac{2 \\cdot Precision \\cdot Recall}{Precision + Recall}$"}]}