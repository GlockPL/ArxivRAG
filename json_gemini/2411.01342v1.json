{"title": "Adaptive World Models: Learning Behaviors by Latent Imagination Under Non-Stationarity", "authors": ["Emiliyan Gospodinov", "Vaisakh Shaj", "Philipp Becker", "Stefan Geyer", "Gerhard Neumann"], "abstract": "Developing foundational world models is a key research direction for embodied intelligence, with the ability to adapt to non-stationary environments being a crucial criterion. In this work, we introduce a new formalism, Hidden Parameter-POMDP, designed for control with adaptive world models. We demonstrate that this approach enables learning robust behaviors across a variety of non-stationary RL benchmarks. Additionally, this formalism effectively learns task abstractions in an unsupervised manner, resulting in structured, task-aware latent spaces.", "sections": [{"title": "1 Introduction", "content": "Recent advances in foundational models have achieved remarkable success in NLP and vision tasks [18, 19]. Still, they fall short in addressing the complexities faced by embodied agents in dynamic, real-world environments. For embodied intelligence, we argue that it is essential to develop foundational world models [7, 21] that capture the causal nature of the world we live in and can make counterfactual predictions. Furthermore, these models should adapt dynamically to non-stationary environments.\nCurrent state-of-the-art approaches for Model-Based Reinforcement Learning (MBRL) [8, 9, 13] often use probabilistic state-space models [17, 2, 22] as a backbone. They learn behaviors by making counterfactual predictions in the latent space of world models. Often these approaches focus on agents mastering a specific, narrow task. Throughout this work, a \"task\" refers to a particular schema of environment dynamics or a specific reward function. In real-world settings, however, tasks are frequently non-stationary and subject to change over time. Thus, a truly intelligent agent must (1) understand the current task and (2) dynamically adapt its perception, model, and behavior to new tasks with minimal interaction.\nWe identified a gap in the literature regarding MBRL in latent spaces that address multitask learning and adaptation under non-stationarity. This work makes two key contributions: (1) highlighting the limitations of current state-of-the-art model-based agents in non-stationary settings, and (2) proposing a new formalism that models non-stationarity as an additional causal latent variable, resulting in robust policies."}, {"title": "2 Non-Stationary RL Formalisms In Latent Spaces", "content": ""}, {"title": "2.1 POMDP formalism", "content": "Existing state-of-the-art MBRL agents [8, 9, 11, 13, 15] that learn in latent spaces typically rely on the partially observable Markov decision process (POMDP) formalism. In this framework, incoming sensory signals are used to update the agent's belief about the hidden state of the environment, enabling the agent to make decisions under uncertainty. Theoretically, the POMDP formalism could handle non-stationarity by treating slowly changing, unobserved tasks as part of the latent states [31]. Here the assumption is that the underlying environment is assumed to be stationary, but the agent has an incomplete view of it [16]. Consequently, single-task frameworks that rely on latent dynamics models for learning should, in theory, be applicable in streaming settings. However, an unstructured latent state, without inductive biases, may hinder the learning of sample-efficient adaptive policies."}, {"title": "2.2 HiP-POMDP formalism", "content": "A complementary but more popular view in literature for non-stationary RL is that the components of the RL (transition, reward, observation functions, action space, etc.) may depend upon time [16]. We build our formalism, the HiP-POMDP, upon this non-stationary function view [5, 32, 31, 23, 4, 24]. We start by providing a formal definition of a HiP-POMDP and demonstrate that this simple modification, along with a scalable variational inference scheme, enables learning adaptive policies across a wide range of non-stationary scenarios where the changing tasks are unknown to the agent.\nDefinition 2.1. A HiP-POMDP is given by a tuple\n$\\{S, A, O, C, L, p_s(s_{t+1}|a_t, s_t, l), p_o(o_t|s_t,l), r(s_t, a_t, l), p_c(C_1|l)\\}$,\nwhere $S, A$, and $O$ are the standard state, action and observation spaces. Additionally, we introduce a space of latent task variables $L$, where $l \\in L$, and a space of context observations $C$, where $C_1 \\in C$. Context observations $C_1$ are generated from $l$ according to $p(C_1|l)$. Finally, the transition model $p_s$, observation model $p_o$, and the reward function $r$ all depend on the latent task $l$.\nThis general definition does not specify how exactly the context can be observed. Throughout this work, we assume $C_1 = \\{(o, a, r, o')_n\\}_{n=1}^N$, i.e., a set of N recent transitions. However, more expressive $C_1$ including temporal embeddings, task metadata, or any other available information about the task could be used in the future. In a HiP-POMDP, the agent's objective is to infer a latent task distribution $p(l | C_1)$ based on the context observations $C_1$ and learn a latent task conditional policy $\\pi(a_t | s_t,l)$ that maximizes the expected cumulative discounted reward, $E_{\\[\\tau\\]} \\[\\sum_{t=0}^{T-1} \\gamma^t r_{t+1}\\]$; where T is the total number of time steps and $\\gamma \\in [0, 1]$ is the discount factor."}, {"title": "3 Adaptive Latent Space Models for HiP-POMDPs", "content": "In line with standard practices in model-based reinforcement learning (MBRL), we alternate between representation learning, behavior learning, and environment interactions to learn policies in the latent space of a world model. However, unlike existing approaches, we make each of these stages adaptive by conditioning them on an inferred task representation or abstraction. Thus, our work goes in the direction of building foundational multi-task world models and subsequent behavior policies. For efficient learning and inference, we adopt a two-phase approach, where we first infer the latent task, which we then use to condition the model, actor, and critic."}, {"title": "3.1 Inferring Latent Task Abstractions via Aggregation", "content": "As stated, throughout this work, we choose $C_\\iota$ to be a collection of N recently observed transition tuples $\\{(o, a, r, o')_n\\}_{n=1}^N$, practically implemented as a FIFO buffer. Note that"}, {"title": "3.2 Learning Adaptive Representations", "content": "In this stage, we learn representations of generative world models that can make counter-factual predictions of the world states based on imagined actions. We make these learned representations adaptive to the task at hand based on the generative model shown in Figure 2. We achieve this by maximizing the conditional data log-likelihood and subsequently deriving an evidence lower bound, as in Equation 1. A detailed derivation can be found in Appendix A.\n$\\ln p (o_{1:T}, r_{1:T} | a_{1:T}, C_{\\iota}) \\ge E_{p(l|C_{\\iota})q(s_{t}|o_{<t},a_{<t},l)} \\[ \\sum_{t=1}^T \\ln p (o_t, r_t | s_t, l) + E_{p(l|C_{\\iota})q(s_{t-1}|o_{<t-1},a_{<t-1},l)} \\[D_{KL} (q (s_t | o_{\\le t}, a_{<t},l) || p (s_t | s_{t-1}, a_{t-1},l))]]\\tag{1}$\nDiscussion: Though we use the generative model from [8, 9], the HiP-POMDP formalism can be used in conjunction with any model-based RL framework in latent spaces [1, 11\u201314, 20] for multitask learning."}, {"title": "3.3 Learning Adaptive Behaviors", "content": "The agent optimizes long-term rewards using a context-sensitive actor-critic approach [25, 8], conditioning both actor and critic on the latent task representation $l$. The actor $\\pi_{\\phi}(a_t | s_t, l)$ selects actions to maximize expected values along imagined trajectories, while the critic $v_{\\psi} (s_{\\tau}, l)$ regresses"}, {"title": "4 Evaluation", "content": "In this section, we evaluate the performance of two competing formalisms\u2014POMDP and HiP-POMDP\u2014in handling non-stationarity within an episodic evaluation setting. We focus on three broad categories of non-stationarity: (1) changing transition functions, (2) changing rewards, and (3) a combination of both. For each category, we further consider two scenarios:\n\u2022 Inter-Episodic Non-Stationarity: Changes remain fixed within an episode but vary between episodes.\n\u2022 Intra-Episodic Non-Stationarity: Non-stationary changes can occur within a single episode.\nA more detailed description of these scenarios is provided in Appendix C. In all experiments, proprioceptive sensors are used as the source of observations.\nAlgorithms Compared: We use the Dreamer [8] as our baseline for the POMDP formalism. For the HiP-POMDP, we modify Dreamer by incorporating latent task abstractions, ensuring a fair comparison between the two approaches. Additionally, we include an \"Oracle\" baseline where the task is assumed to be directly observed. In this setup, the known task replaces the inferred latent task variable $l$, serving as an upper bound on performance. This helps illustrate the potential gains if perfect task information were available.\nWe evaluate the agents in all experiments by calculating the mean return from 10 trajectories every 25 epochs, each with randomly sampled environmental changes. The performance curves are computed by averaging the results over 10 different random seeds. Our evaluation answers the following questions:\nCan HiP-POMDP agents handle changing dynamics? To evaluate the effectiveness of HiP-POMDP formalism under changing dynamics, here we introduce two tasks: 1) We modify the standard HalfCheetah agent by adding joint perturbations of varying magnitudes randomly, and 2) the Hopper agent by randomly changing the body mass and inertia for random number of body parts. Additional evaluation is introduced in Appendix C.3.\nAs seen in Figure 3 HiP-POMDP agent results in robust performance gains, especially under challenging intra-episodic changes and even competing with the Oracle.\nCan HiP-POMDP agents handle changing objectives? To create non-stationarity with changing reward functions/objectives, we modify the standard HalfCheetah such that a target velocity needs to be reached which changes randomly. Additionally, we evaluate the agents on custom-designed multi-task benchmarks using pre-defined tasks from [15], where each task requires the agent to perform different skills (e.g., standing, running, flip) in various directions. As such multi-skill objective changes are more challenging, the experiments are run over 5M steps.\nAs seen in Figure 4 the vanilla POMDP agent fails to deal with objective changes in all cases. On the other hand side, the HiP-POMDP agent with inferred task abstractions resolves the"}, {"title": "5 Conclusion", "content": "In this work, we introduced the HiP-POMDP formalism to learn adaptive world models and behavior policies in latent state spaces. The formalism resulted in algorithms that learn meaningful task abstractions and improved performance on a variety of non-stationary benchmarks. Future work would extend these models to more high-dimensional sensory inputs like images and point clouds."}, {"title": "A Objective Derivation", "content": "$\\ln p (o_{1:T}, r_{1:T} | a_{1:T}, C_{\\iota})\\newline= \\ln E_{\\[p(l | C_{\\iota})\\]} \\[P (o_{1:T}, r_{1:T} | a_{1:T}, l)\\] (Data aggregation for latent task inference given the context $C_{\\iota}$)\\newline= \\ln E_{\\[p(l | C_{\\iota})\\]} \\[\\frac{\\[ p(l | C_{\\iota}) \\underset{\\[p(s_{1:T}|a_{1:T}, l)\\]}E \\[ p (o_{1:T}, r_{1:T} | s_{1:T}, l) ] ds_{1:T} \\]}{\\[p(l | C_{\\iota}) P(s_{1:T}|a_{1:T},l)\\]}\\] (Posterior predictive log-likelihood using Latent Variable Model)\\newline= \\ln E_{\\[p(l | C_{\\iota})\\]} \\[\\underset{\\[p(s_{1:T}|a_{1:T}, l)\\]}E \\[ p (s_{1:T} | a_{1:T}, l) p (o_{1:T}, r_{1:T} | s_{1:T},l) ] ds_{1:T} \\] (Expectation definition)\\newline= \\ln E_{\\[p(l | C_{\\iota})\\]} \\[\\underset{\\[p(s_{1:T}|a_{1:T}, l)\\]}E \\[ \\prod_{t=1}^T p (s_t | s_{t-1}, a_{t-1}, l) p (o_t, r_t | s_t,l) ] ds_{1:T} \\] (Factorization due to Markov property)\\newline= \\ln E_{\\[p(l | C_{\\iota})\\]} \\[\\frac{\\[ \\underset{\\[q(s_t | o_{\\le t}, a_{\\<t}, l)\\]}E \\[ \\prod_{t=1}^T  q(s_t | o_{\\le t}, a_{\\<t}, l) p (s_t | s_{t-1}, a_{t-1}, l) p (o_t, r_t | s_t,l) ] ds_{1:T} \\]}{\\[ \\underset{\\[q(s_t | o_{\\le t}, a_{\\<t}, l)\\]}E \\[ \\prod_{t=1}^T  q(s_t | o_{\\le t}, a_{\\<t}, l) \\]}\\] (Variational approximate posterior)\\newline\\approx \\ln E_{\\[p(l | C_{\\iota})\\]} \\[\\frac{\\[  \\prod_{t=1}^T \\underset{\\[q(s_t | o_{\\le t}, a_{\\<t}, l)\\]}E \\[ p (s_t | s_{t-1}, a_{t-1}, l) p (o_t, r_t | s_t,l) ] \\]}{\\[ \\underset{\\[q(s_t | o_{\\le t}, a_{\\<t}, l)\\]}E \\[ \\prod_{t=1}^T  q(s_t | o_{\\le t}, a_{\\<t}, l) \\]}\\] (Expectation w.r.t approximate posterior + exchange order of nominator factors)\\newline\\ge E_{\\[p(l | C_{\\iota})\\]} \\[\\frac{\\[   \\prod_{t=1}^T E_{\\[q(s_t | o_{\\le t}, a_{\\<t}, l)\\]} \\[ p (s_t | s_{t-1}, a_{t-1}, l) p (o_t, r_t | s_t,l) ] \\]}{\\[  \\prod_{t=1}^T q(s_t | o_{\\le t}, a_{\\<t}, l) \\]}\\] (Jensen's inequality w.r.t. outer expectation)\\newline\\ge  E_{\\[p(l | C_{\\iota})\\]} \\[   \\prod_{t=1}^T \\frac{\\[  E_{\\[q(s_t | o_{\\le t}, a_{\\<t}, l)\\]} \\[ p (s_t | s_{t-1}, a_{t-1}, l) p (o_t, r_t | s_t,l) ] \\]}{\\[  q(s_t | o_{\\le t}, a_{\\<t}, l) \\]}\\] (Jensen's inequality w.r.t. inner expectation)\\newline= - E_{\\[p(l | C_{\\iota})\\]} \\[\\underset{\\[q(s_{1:T}|o_{1:T},a_{1:T},l)\\]}E \\[\\sum_{t=1}^T \\ln \\[\\frac{\\[ p (o_t, r_t | s_t,l) p (s_t | s_{t-1}, a_{t-1}, l) \\]}{\\[ q(s_t | o_{\\<t}, a_{\\<t}, l) \\]}\\]  \\]] (Product rule of logarithms)\\newline=- E_{\\[p(l | C_{\\iota})\\]} \\[\\underset{\\[q(s_{1:T}|o_{1:T},a_{1:T},l)\\]}E \\[\\sum_{t=1}^T \\ln p (o_t, r_t | s_t,l) + \\ln p (s_t | s_{t-1}, a_{t-1}, l)  - \\ln q(s_t | o_{\\<t}, a_{\\<t}, l) \\]  \\] (Quotient logarithm rule)\\newline=  \\sum_{t=1}^T - E_{\\[p(l | C_{\\iota})\\]} \\[\\underset{\\[q(s_{t}|o_{\\<t},a_{\\<t},l)\\]}E \\[ \\ln p (o_t, r_t | s_t,l) + \\ln p (s_t | s_{t-1}, a_{t-1}, l)  - \\ln q(s_t | o_{\\<t}, a_{\\<t}, l) ]] (Product logarithm rule)\\newline=\\sum_{t=1}^T E_{\\[p(l | C_{\\iota})\\]} \\[\\underset{\\[q(s_{t}|o_{\\<t},a_{\\<t},l)\\]}E \\[ \\ln p (o_t, r_t | s_t,l) \\]\\] +  \\sum_{t=1}^T E_{\\[p(l | C_{\\iota})\\]} \\[\\underset{\\[q(s_{t}|o_{\\<t},a_{\\<t},l)\\]}E \\[ \\ln p (s_t | s_{t-1}, a_{t-1}, l) - \\ln q(s_t | o_{\\<t}, a_{\\<t}, l) \\]\\] (Linearity of expectation)\\newline=\\sum_{t=1}^T E_{\\[p(l | C_{\\iota})\\]} \\[\\underset{\\[q(s_{t}|o_{\\<t},a_{\\<t},l)\\]}E \\[ \\ln p (o_t, r_t | s_t,l) \\]\\] +  \\sum_{t=1}^T E_{\\[p(l | C_{\\iota})\\]} \\[  E_{\\[q(s_{t-1}|o_{\\<t-1},a_{\\<t-1},l)\\]} \\[ \\ln p (s_t | s_{t-1}, a_{t-1}, l) - \\ln q(s_t | o_{\\<t}, a_{\\<t}, l) \\]\\] (Linearity of expectation)\\newline=\\sum_{t=1}^T E_{\\[p(l | C_{\\iota})\\]} \\[\\underset{\\[q(s_{t}|o_{\\<t},a_{\\<t},l)\\]}E \\[ \\ln p (o_t, r_t | s_t,l) \\]\\] +  E_{\\[p(l | C_{\\iota})\\]} \\[\\underset{\\[q(s_{t-1}|o_{\\<t-1},a_{\\<t-1},l)\\]} \\[ \\ln q(s_t | o_{\\<t}, a_{\\<t}, l) \\] (Expectation is constant for $\\[t \\in \\{t, t - 1\\}\\])\\newline=\\sum_{t=1}^T E_{\\[p(l | C_{\\iota})\\]} \\[\\underset{\\[q(s_{t}|o_{\\<t},a_{\\<t},l)\\]}E \\[ \\ln p (o_t, r_t | s_t,l) \\]\\] + \\sum_{t=1}^T E_{\\[p(l | C_{\\iota})\\]} \\[\\underset{\\[q(s_{t}|o_{\\<t},a_{\\<t},l)\\]}E \\[ \\ln p (s_t | s_{t-1}, a_{t-1}, l) - \\ln q(s_t | o_{\\<t}, a_{\\<t}, l) \\]\\] (Expectation split)\\newline= \\sum_{t=1}^T E_{\\[p(l | C_{\\iota})\\]} \\[\\underset{\\[q(s_{t}|o_{\\<t},a_{\\<t},l)\\]}E \\[ \\ln p (o_t, r_t | s_t,l) \\]\\] + \\sum_{t=1}^T E_{\\[p(l | C_{\\iota})\\]} \\[ D_{\\[KL\\]} \\[q (s_t | o_{\\le t}, a_{\\<t}, l) || p (s_t | s_{t-1}, a_{t-1},l)]\\] (Quotient rule for logarithms)\\newline= \\sum_{t=1}^T \\underset{\\[p(l | C_{\\iota})\\]}E \\[E_{\\[q(s_{t}|o_{\\<t},a_{\\<t},l)\\]} \\[ \\ln p (o_t, r_t | s_t,l) \\]\\] +   \\sum_{t=1}^T E_{\\[p(l | C_{\\iota})\\]} \\[ D_{\\[KL\\]} \\[q (s_t | o_{\\le t}, a_{\\<t}, l) || p (s_t | s_{t-1}, a_{t-1},l)]\\] (KL-Divergence definition)"}, {"title": "B Algorithm and Implementation Details", "content": ""}, {"title": "B.1 Task Abstractions Via Bayesian Aggregation", "content": "Given a set of encoded interaction tuples and their corresponding variances $\\{\\chi_n, \\sigma_n\\}_{n=1}^N$, using the prior and observation model assumptions in Section 3.1 of the main paper, we infer the latent task abstraction $p (l | C_\\iota) = N(\\mu_\\iota, \\Sigma_\\iota) = N(\\mu_\\iota, diag(\\sigma_{\\iota}))$ as a Bayesian aggregation [30] of these using the following closed-form equations:\n$\\sigma_\\iota^2 = \\[(\\frac{\\[1\\]}{\\[ \\sigma_0^2 \\]}) + ( \\sum_{n=1}^N (\\frac{\\[1\\]}{\\[ \\sigma_n^2 \\]}) )^2\\]^{-1}$\n$\\mu_\\iota = \\mu_0 + \\sigma_\\iota^2 \\odot \\sum_{n=1}^N (\\chi_n - \\mu_0)(\\sigma_n)^{-2}$"}, {"title": "B.2 HiP-Dreamer", "content": ""}, {"title": "B.3 Hyperparameters.", "content": "This section details only those hyperparameter values that differ from the original architectures or correspond to new architectural components, such as the set encoder. For additional hyperparameters related to all algorithm stages not explicitly mentioned here, we refer to [10].\nSet encoder. The set encoder includes a shared fully connected layer with 240 units, followed by two separate fully connected layers of 240 units each. One layer computes the latent task observation $\\chi_n$, while the other computes the latent task variance $\\sigma_n$. All set encoder layers use the ELU activation function. The latent task posterior $p_\\theta (l | C_\\iota)$ is modelled as a multivariate Gaussian with 20 dimensions. Due to computational and time constraints, extensive hyperparameter optimization was not conducted.\nLearning updates. To train the world model, we use batches of 50 sequences of length 50, as in [8]. For the context data, we sample batches of 50 sequences from the replay buffer, each consisting of 20 prior interaction transitions. If fewer than 20 prior transitions are available, we use zero-padding to fill the context data before concatenating it with any available transition data. The set encoder updates use the same learning rate as the world model.\nObjective and Critic. In our objective we use the KL-Balancing technique introduced in [11], which we found essential for stabilizing learning when using proprioceptive inputs. Additionally, we experienced instability problems w.r.t. critic network in many environments. To stabilize the critic training further, we use target network to calculate the critic targets, using a soft-update, realized as $\\theta_{t+1} = \\tau \\theta_t + (1-\\tau)\\theta_{t-1}$ with $\\tau = 0.05$ for Gymnasium-based environments. For DMC multi-task benchmarks, we use a hard update, copying the critic's weights every 100 gradient steps."}, {"title": "C Evaluation Details", "content": "Despite the prevalence of non-stationarity in real-world environments and the existence of some non-stationary benchmarks such as [3] and [6], a detailed categorization of environmental changes remains absent, along with analyses of model-based reinforcement learning agent's adaptability across various non-stationary scenarios.\nWe categorize environmental changes into three primary types based on the temporal and structural aspects of the environment's evolution:\n\u2022 Dynamical Changes: Alterations affecting the robot's system or the physical properties of the environment.\n\u2022 Objective Changes: Modifications of objectives, such as changes in target velocity or task requirements.\n\u2022 Combined Changes: Concurrent occurrences of multiple dynamical changes or a combination of dynamical and objective changes.\nEach category of change can occur either inter-episodically (between episodes) or intra-episodically (within episodes). The temporal dimension specifies when changes occur, while the structural dimension identifies which environmental aspects are affected. Addressing these types of changes enables the design of robust model-based reinforcement learning agents capable of adapting to a broad range of environmental dynamics."}, {"title": "C.1 Dynamical Changes", "content": "We implemented various dynamical changes by modifying Gymnasium [26] and DMC Control Suite [27] environments, particularly focusing on Half Cheetah, Walker, and Hopper robots. The dynamical change values were chosen to ensure that MuJoCo's models remained valid while still challenging pre-trained model-free and model-based agents."}, {"title": "C.2 Objective changes", "content": "Half Cheetah, Hopper, and Walker\n\u2022 Target Velocity: A target velocity is randomly chosen within the range [-6, 6]m.\nIn these environments, the Oracle agent receives the target velocity as a scalar.\nDMC Multi-Task Benchmarks\n\u2022 Objective Change: At the beginning of each episode, a new objective is selected, requiring the agent to acquire multiple skills within the same domain.\nWe implement these multi-task benchmarks using pre-defined tasks from [15], creating a variety of benchmarks across different domains and categorizing them by task count, as shown in Table 1.\nIn all DMC multitask environments, the Oracle agent receives a unique one-hot vector corresponding to each environment."}, {"title": "C.3 Further Evaluation Under Dynamical Changes", "content": "This section presents additional evaluations of environments with both inter-episodic and intra-episodic dynamical changes.\nFigure 7 illustrates additional experiments involving dynamical changes in the environment. Across both inter- and intra-episodic changes, task-conditioned agents exhibit similar performance to the vanilla agent, with only minor differences observed.\nOn one hand, this result suggests that the vanilla agent is capable of adapting to various dynamic changes, indicating that approaches based on the POMDP formalism can indeed learn representations that support adaptive behavior. On the other hand, the HiP-POMDP formalism generally performs comparably or slightly better, highlighting the benefits and expressiveness of a learned latent task representation.\nInterestingly, in some cases, the task-inference agent even outperforms the Oracle agent, despite the former having to learn and utilize a task representation for each task, while the Oracle agent is directly provided with task change information. This advantage may be due to the learned task representation capturing additional task-relevant information beyond the task changes alone, or it could suggest that the ground-truth task representation provided to the Oracle agent is not optimally structured for adaptation. This observation warrants further investigation in future work."}, {"title": "C.4 Further Evaluation Under Objective Changes", "content": "In this section, we conduct additional experiments under various reward/objective changes and identify the points at which different algorithms begin to fail.\nFigure 8 provides further empirical evidence that the vanilla agent struggles to handle objective changes, while task-conditioned agents demonstrate adaptive behavior, consistent with the results shown in Figure 4.\nBreaking point of Vanilla Dreamer under objective changes. To investigate the vanilla agent's failures under target reward-changing experiments, we adjust the reconstruction loss by giving more weight to reward reconstruction to offset potential higher loss weights from the multi-dimensional observations. Figure 9 illustrates the effect of scaling the reward reconstruction loss differently.\nOur observations show that increasing the reward reconstruction weight factor also raises the KL divergence between the prior and posterior distributions over the latent state, along with an increased observation reconstruction loss, ultimately degrading performance. These findings suggest the optimization process struggles to balance reconstruction and regularization loss terms, possibly leading to overfitting to specific rewards within each mini-batch."}, {"title": "C.5 Evaluation under Combined Environmental Changes", "content": "In this section, we evaluate all agents in the most challenging setting, involving both multiple dynamical changes and combined dynamical and objective changes. Figure 12 illustrates the evaluation results for all agents on the Half Cheetah task, where changes evolve in an inter-episodic manner.\nAs seen in Figure 12, the vanilla agent demonstrates adaptability under multi-modal dynamical changes, however, it struggles with combined dynamical and objective changes, largely due to its limited ability to handle shifts in objectives.\nIntroducing a task representation notably enhances performance, as both the task-inference and oracle agents successfully adapt across all combined change scenarios. However, under scenarios involving only combined dynamical changes, the oracle agent exhibits slower learning compared to the vanilla agent. This slower adaptation raises questions about the potential influences of the task representation, warranting further investigation."}, {"title": "C.6 Latent Space Visualizations", "content": "This section presents 2D projections of both the world model's latent state and the latent task spaces. The visualizations are generated by recording trajectories of posterior latent states (incorporating both deterministic and stochastic components) and latent task representations during evaluations across various tasks, with a final 2D projection achieved using t-SNE [28].\nThe primary objective of these visualizations is to explore two key questions: (1) Does conditioning the MBRL agent on either a ground truth or learned task representation introduce any structural changes in the latent state space? (2) How does the structure of the latent state space correlate with the agent's performance?"}, {"title": "C.6.1 Dynamics Changes", "content": "Figure 13 displays 2d projections of the latent state space under different dynamic environmental conditions.\nIn the visualizations, the vanilla agent\u2014grounded in the POMDP formalism\u2014demonstrates task-dependent latent space structuring under different dynamic changes. Examining these projections along with empirical results from Figure 7 reveals a positive correlation between the structured latent space and the agent's performance. Notably, conditioning the agent on task representations produces an even more task-specific latent space structure.\nTask conditioning appears to provide two main advantages: (1) a more structured latent space enables enhanced data modeling, as task-specific representations minimize interferences, thus improving data reconstruction; and (2) task conditioning helps disambiguate overlapping latent states, especially when a state recurs across tasks. This disambiguation contributes to better data modeling and aids in achieving adaptive behaviors."}, {"title": "C.6.2 Objective Changes", "content": "Figure 14 presents 2D projections of latent task and state spaces during evaluations in reward-altered environments, where the objective is to reach various target velocities or master multiple skills.\nFigure 14 reveals two critical observations. When the reward's structure changes, the vanilla agent's latent space does not organize itself by task, leading to substantial interference and making it challenging for all agent's components to infer task information, contributing to suboptimal performance as evidenced in Figure 11 for changing rewards.\nIn contrast, conditioning the agent on task representations produces a latent space that is not only more task-aware but also better suited for concurrent multi-task learning. Similarly to the findings in Section C.6.1 under dynamics changes, a positive correlation emerges between the structured latent space and the agent's final performance. However, for the most challenging skill-learning experiments, the task inference approach organizes latent space with some tasks represented jointly, which can hinder unique task identification and contribute to the observed performance gap in skill-learning tasks as shown in Figure 11."}]}