{"title": "HELVIPAD: A Real-World Dataset for Omnidirectional Stereo Depth Estimation", "authors": ["Mehdi Zayene", "Jannik Endres", "Albias Havolli", "Charles Corbi\u00e8re", "Salim Cherkaoui", "Alexandre Kontouli", "Alexandre Alahi"], "abstract": "Despite considerable progress in stereo depth estimation, omnidirectional imaging remains underexplored, mainly due to the lack of appropriate data. We introduce HELVIPAD, a real-world dataset for omnidirectional stereo depth estimation, consisting of 40K frames from video se- quences across diverse environments, including crowded indoor and outdoor scenes with diverse lighting condi- tions. Collected using two 360\u00b0 cameras in a top-bottom setup and a LiDAR sensor, the dataset includes accurate depth and disparity labels by projecting 3D point clouds onto equirectangular images. Additionally, we provide an augmented training set with a significantly increased label density by using depth completion. We benchmark lead- ing stereo depth estimation models for both standard and omnidirectional images. The results show that while re- cent stereo methods perform decently, a significant chal- lenge persists in accurately estimating depth in omnidirec- tional imaging. To address this, we introduce necessary adaptations to stereo models, achieving improved perfor- mance. Our dataset is available at https://vita-epfl.github.io/Helvipad/.", "sections": [{"title": "1. Introduction", "content": "Mobile robots and autonomous agents are increasingly used in real-world applications such as autonomous driving [16], healthcare [18], or agriculture [10], where accurate 3D rep- resentations of dynamic human environments are crucial for navigation and interaction. Depth estimation [19, 32, 34] aims to help build these representations by assessing the distance of surrounding objects from the ego agent. Historically, LiDAR sensors have been the gold standard for capturing high-quality depth information due to their precision and their 360\u00b0 coverage. They come, however, with significant limitations [33], such as providing point clouds that are sparse, especially for objects in far range, and with high deployment costs. This has driven grow-"}, {"title": "2. Related Work", "content": "Stereo datasets for depth estimation. Following early contributions from synthetic datasets like SceneFlow [7] and real-world benchmarks like KITTI [12, 23], large-scale driving datasets, such as Apolloscape [15], nuScenes [4], and Waymo [26] expanded the scope to more complex driv- ing scenarios, but often lack pixel-wise depth labels or are limited to frontal views or multi-view configurations, as summarized in Tab. 1. In omnidirectional imaging, public datasets with 360\u00b0 images have enabled deep learning meth- ods to improve performance in tasks such as 2D and 3D object detection [3, 21, 38], semantic segmentation [3, 37], tracking [21], video summarization and monocular depth estimation [25]). OmniHouse and OmniThings [31] provide synthetic indoor scenes captured using a cross configuration of four fisheye cameras. Although this setup offers a wide field of view, it is more challenging and costly to imple- ment than the simpler top-bottom setup of two 360\u00b0 cameras in HELVIPAD. MP3D [5] and SF3D [3] offer 360\u00b0 images but are limited to synthetic indoor scenes with single view. Wang et al. [28] built two stereo datasets with top-bottom setup based on these two synthetic databases. The JRDB dataset [21] spans a variety of tasks in real-world outdoor and indoor environments, but does not include pixel-wise depth annotations. In this context, HELVIPAD emerges as a comprehensive real-world stereo dataset of 360\u00b0 images, covering indoor and outdoors scenes with varying lighting conditions and pixel-wise annotations.\nStereo matching. Deep learning-based methods have in- creasingly dominated stereo matching since the pioneering work of Zbontar and LeCun [35], who introduced the use of convolutional neural networks (CNNs) to describe image patches for stereo matching. Since then, many approaches have incorporated 3D convolutional kernels to construct cost volumes [14, 17], enhancing architectures with spa- tial pyramidal pooling, such as PSMNet [6]. Recently,"}, {"title": "3. The HELVIPAD Dataset", "content": "This section details the collection process of the dataset (Sec. 3.1). We address the challenge of producing accu- rate depth labels by projecting LiDAR point clouds onto equirectangular images (Sec. 3.2) and applying depth com- pletion techniques to increase label density for an aug- mented training set (Sec. 3.3). Finally, we present data split statistics and depth information for the dataset (Sec. 3.4).\n3.1. Data Acquisition\nTo assemble a dataset capturing human pedestrian activity, we developed a custom-built rig with which we collected data across distinct scenes, encompassing indoor (e.g., cor- ridors, halls) and outdoor (e.g., squares, external parking, footpaths) settings in various lighting conditions on a uni- versity campus. The rig features two Ricoh Theta V 360\u00b0 cameras in a top-bottom setup with a 19.1 cm baseline, capturing equirectangular images at 30 fps, paired with a Ouster OS1-64 LiDAR sensor with a vertical field of view of 42.4\u00b0, operating at 10 fps, and mounted 45.0 cm below the bottom camera. A central processor manages data cap- ture and ensures device synchronization. In total, we col- lected 29 video sequences. More details on data collection are available in Appendix A.\n3.2. Mapping LiDAR to 360\u00b0 Images\nThe main challenge in processing our raw data is to ensure a robust and accurate projection of the LiDAR point clouds on the 360\u00b0 camera images. The following is a summary of the algorithm's approach, presented in Algorithm 1). At the beginning of each recorded session, we select a set of key 3D points in the point cloud obtained by the LiDAR scans on a 19x19 chessboard, focusing on corners and edge mid- points to obtain their coordinates. The corresponding im- age pixel positions are selected in the equirectangular bot- tom image. We apply initial rotation and translation trans- formations $\\text{R}\\_{\\text{init}}, \\text{T}\\_{\\text{init}}$ to make the camera lens the coordi- nate system's center, ensuring alignment with the camera's"}, {"title": "3.3. Depth Completion from LiDAR Point Clouds", "content": "Given the sparse nature of LiDAR point clouds compared to high-resolution camera images, ground-truth depth la- bels cover only a fraction of the total pixels. To address this issue, we develop a fully automated depth completion pipeline tailored for 360\u00b0 images. Existing depth comple- tion methods share the idea of aggregating point clouds but either need manual annotations [12, 23] or are limited to rectilinear images [33]. Our approach interpolates depth on spherical grid points by using temporally aggregated point clouds and applies a filtering process to exclude points with high uncertainty or lacking nearby measurements, ensuring minimal error before mapping the remaining points.\nTemporal aggregation. To increase the number of valid points, we aggregate the point cloud of the current frame with those from the four preceding and four succeeding frames. Given this limited temporal window, the error in- troduced by the movement of the robot, or within the scene, is negligible, given the LiDAR's high frame rate and the rig's sub-pedestrian moving speed (see Appendix C.3).\nInterpolation. A point on a spherical grid is defined by its polar angle $\\theta\\in [0, \\pi]$, azimuthal angle $\\varphi\\in [-\\pi, \\pi]$, and radial distance r. For a query point $(\\theta\\_q, \\varphi\\_q)$ within the LiDAR's field of view, we estimate its depth through a weighted average of its k-nearest neighbors, $r\\_q =$"}, {"title": "4. Adapting Stereo Matching for 360\u00b0 Imaging", "content": "Omnidirectional images with equirectangular projections differ from traditional rectilinear images due to spherical distortions and the continuity at the left and right edges. As most existing stereo depth estimation models are designed for two rectilinear cameras in a left-right setup, we propose two key adaptations \u2013 polar angle incorporation and circular padding - to enable these models to handle omnidirectional stereo datasets effectively. In this section, we illustrate how these adaptations were applied to adapt the recent state-of- the-art model, IGEV-Stereo [32].\nIncorporating polar angle. In the top-bottom setup, im- ages are warped vertically to form cost volumes. Distor- tions in omnidirectional images vary with the polar angle \u03b8, which affects the calculations of the disparity in the vertical"}, {"title": "5. Experiments", "content": "In this section, we evaluate the performance of multiple state-of-the-art and popular stereo matching methods, both for standard and 360\u00b0 images. We also analyze their cross- scene generalization within the dataset and include ablation studies to evaluate the proposed adaptations and the depth completion pipeline as training data augmentation.\n5.1. Setup and Baselines\nAll models are trained on a single NVIDIA A100 GPU with the largest possible batch size to ensure comparable use of computational resources. Model selection employed early stopping based on validation performance. Implementation details are provided in Appendix C.2.\nBaselines. We implemented two well established learning based stereo depth estimation models, the popular PSM- Net [6] and IGEV-Stereo [32] presented above, the latter also enabling to assess the effectiveness of our proposed adaptations in Sec. 4. For omnidirectional imaging, we include 360SD-Net [28], the only 360\u00b0 stereo method de- signed for top-bottom cameras setups.\nEvaluation metrics. We measure each method's perfor- mance by comparing their disparity and depth estimates to the ground-truth labels using the Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Ab- solute Relative Error (MARE). MAE indicates overall error magnitude, while RMSE captures the impact of outliers. In particular, depth errors are prone to increasing significantly due to the non-linear relationship between depth and dispar- ity. MARE provides a scale-invariant measure of error, use- ful for assessing performance across varying depth ranges."}, {"title": "5.2. Comparative Results", "content": "Sec. 4 presents the performance of various stereo depth esti- mation methods evaluated on the HELVIPAD test set. While 360SD-Net improve every metrics over its rectilinear stereo counterpart PSMNet, it falls short of matching the depth accuracy of more recent models like IGEV-Stereo in met- rics such as MAE and RMSE. This indicates that IGEV- Stereo has a notable advantage in minimizing larger depth errors compared to 360SD-Net. However, in terms of depth MARE and disparity metrics, 360SD-Net performs com- parably to IGEV-Stereo, which suggests that 360SD-Net design for top-bottom 360\u00b0 configurations helps to estimate accurately disparity."}, {"title": "5.3. Ablations", "content": "Depth completion as data augmentation. As shown in Fig. 5, training models with the depth-completed aug- mented set helps to achieve performance gains. For in- stance, 360SD-Net's depth MARE dropped from 0.17 to 0.15 with augmentation, while PSMNet also shows a simi- lar reduction. Notably, the performance gains are less pro- nounced for more advanced models like IGEV-Stereo and 360-IGEV-Stereo.\nOmnidirectional adaptations. During inference, circular padding is applied to account for the continuous 360\u00b0 view, which helps to reduce the depth RMSE from 4.44 to 4.36, as shown in Tab. 3. This enhancement is visually notice- able in Fig. 7, where padding contributes to smoother depth transitions across the left and right image boundaries.\nFurther, the application of photometric augmentation helps models to generalize across the diverse lighting con-"}, {"title": "5.4. Cross-Scene Generalization", "content": "To safely deploy learning systems in real-world settings, it is essential to assess their ability to generalize to unseen or underrepresented conditions, like outdoor environments at night. The bar chart in Fig. 6 shows the performance of our proposed omnidirectional adaptation, 360-IGEV- Stereo, alongside other evaluated methods, using depth MARE as the metric. We trained model variants on differ- ent subsets (Indoor, Outdoor, Indoor+Outdoor) and tested them under various conditions (Indoor, Outdoor, Night Out- door). While models trained on specific environments per- form well within their respective domains, indoor-trained models show a clear limitation in generalizing to outdoor scenes, particularly at night. Interestingly, this perfor- mance drop is less pronounced in omnidirectional models, such as 360-IGEV-Stereo and 360SD-Net, than in conven- tional models, like IGEV-Stereo and PSMNet. This results suggests that omnidirectional approaches benefit from im- proved cross-scene generalization capacities.\nModels trained on Indoor+Outdoor\u00b9 subset outperform those trained solely on singular environments. Finally, best results are obtained with models trained on the whole training set, except for indoor test setting with PSMNet where the model trained on only indoor and daytime out- door scenes performs slightly better. This underscores the importance of diverse training data for robust learning."}, {"title": "6. Conclusion", "content": "In this paper, we build HELVIPAD, a real-world stereo dataset for omnidirectional depth estimation, and demon- strate that incorporating polar angle as input and circular padding helps to adapt stereo matching deep learning mod- els to equirectangular images. We also introduce a method for increasing depth label density from LiDAR point clouds through depth completion tailored to equirectangular pro- jections. Despite achieving promising results, accurate depth estimation in omnidirectional contexts remains chal- lenging. We hope this work encourages further research into omnidirectional stereo matching, especially for real- time applications. Moreover, this dataset is an ideal testbed for assessing the robustness of depth estimation methods to diverse lighting conditions and depth ranges."}, {"title": "A.1. Sequences", "content": "The HELVIPAD dataset includes 29 video sequences captured between December 2023 and February 2024., recorded at a frame rate of 10 Hz.\nEach sequence is synchronized with its corresponding LiDAR point clouds, which are projected on frames to ob- tain depth maps, and subsequently disparity maps. Top im- ages, bottom images, depth maps, and disparity maps are then cropped to remove unnecessary borders and downsized from a width of 1024 pixels to 512 to enable more efficient training.\nThe sequences span a diverse array of settings and con- ditions, as detailed in Tab. 4. The dataset includes record- ings taken at various times of the day, from early morn- ing to night, under a variety of weather conditions, includ- ing cloudy and sunny skies. These recordings were made in multiple indoor and outdoor locations, from pedestrian squares and footpaths to corridors and parking areas, offer- ing a wide spectrum of environmental contexts. The table also provides information on the duration of each sequence, with an average of 2 minutes and 41 seconds. Furthermore, the dynamic nature of the recorded scenes is emphasized by the presence of pedestrians, with an average of 13.33 pedes- trians in indoor sequences and 17.65 pedestrians in outdoor sequences."}, {"title": "B.2. Synchronization between sensors device", "content": "To ensure accurate synchronization between the sensors, we use a hardware-triggered synchronization method. At the start of each recording, an external flash lasting 33 ms is activated in front of the cameras, creating a visible synchro- nization marker in the video streams. Simultaneously, the precise ROS timestamp of the flash event is recorded in the LiDAR data, which provides a precise timestamp. During post-processing, we identify the blinded frames and corre- sponding ROS timestamps, re-aligning all data streams to start from this synchronized reference, as shown in Fig. 11\nTo match the LiDAR's frame rate (10 fps), we retain one out of every three camera frames. The 33ms flash duration ensures it is captured in at least one camera frame, with a maximum potential de-synchronization of half a frame in- terval (16.67 ms) if the flash occurs just after a frame is cap- tured. This reasoning extends to the LiDAR-camera syn- chronization, resulting in a maximum de-synchronization of 16.67 ms across all sensors."}, {"title": "B.3. LiDAR-Image Projection Quality Assessment", "content": "In the absence of a reliable, standardized method to evaluate the accuracy of LiDAR point projections onto equirectangu- lar image planes, manual validation serves as a practical and precise alternative. Visual inspection and manual selection of corresponding points have been highlighted in studies such as [8], where the authors emphasized the role of man- ual evaluation in aligning data when automated methods are insufficient. Similarly, in [9], the challenges of achieving accurate projections without ground truth were addressed, underscoring the importance of visual assessment for high- precision tasks.\nTherefore, in this work, we adopt a manual point selec- tion approach to evaluate the projection of LiDAR points onto 2D equirectangular image planes. This process in- volves manually selecting corresponding points, such as ob- ject edges, on both the LiDAR-projected data and the im- ages. These selected points are then used to compute the pixel-wise error, which quantifies the projection's accuracy.\nFor each selected point pair, we calculate the Euclidean distance between the projection point $(x\\_{\\text{proj}}, y\\_{\\text{proj}})$ and the corresponding real point $(x\\_{\\text{real}}, y\\_{\\text{real}})$:\n$\\text{Error} = \\sqrt{(x\\_{\\text{proj}} - x\\_{\\text{real}})^2 + (y\\_{\\text{proj}} - y\\_{\\text{real}})^2}$.\nThis error is averaged across multiple images to provide a comprehensive assessment of the projection's accuracy. Additionally, a relative error metric is computed by normal- izing the pixel error by the image diagonal, enabling a con-"}, {"title": "C. Depth Completion", "content": "This section provides an in-depth evaluation of our depth completion pipeline, detailing the evaluation methods, hy- perparameter selection, and comparison of temporal aggre- gation techniques. Quantitative and qualitative results are also presented to demonstrate the effectiveness of our ap- proach.\nC.1. Evaluation Method\nThe evaluation of our depth completion method follows a structure akin to standard machine learning. The dataset is split into training and test sets, then performance metrics are computed on the test set within the 3D space.\nCreation of training and test set. To identify the optimal hyperparameters for depth completion, the points of each measured point cloud are divided into a training and test set using a classical 80-20-split. Points are sampled from a uni- form distribution over all input points without replacement.\nThis approach primarily evaluates metrics over points with low distances to its neighbors, which are easier to es- timate. When the pipeline generates points on a uniform grid (e.g., an image), the metrics reflect lower bounds on actual errors due to this distribution shift. While this bias limits direct comparison to image-based errors, it is accept- able for hyperparameter optimization and data augmenta- tion purposes.\nEvaluation metrics. To evaluate the merits of different options for the depth completion pipeline, we calculate the following metrics: mean absolute error (MAE), root mean squared error (RMSE), mean absolute relative error (MARE), inlier ratio (IR) and actual ratio of interpolated points (ARIP). The calculation of the first three mentioned metrics is the same as for depth estimation methods and can be found in Appendix D.1. The IR corresponds to the ratio of estimated depth labels that have an absolute error of less than $t\\_{\\text{inlier}} = 1\\%$. Given the number N of estimated depths among points of all point clouds and sequences, the IR can be calculated in the following way:\n$\\text{IR} = \\frac{1}{N} \\sum\\_{i=1}^{N} \\mathbb{I}(\\vert r\\_{\\text{est}, i} - r\\_{\\text{true}, i}\\vert < t\\_{\\text{inlier}})$.\nIn Sec. 3.3, the ratio of interpolated points (RIP) is defined as the ratio of interpolated points after filtering. As the un- certainty estimates of all query points within one sequence"}, {"title": "C.2. Choice of Hyperparameters", "content": "This section describes how the hyperparameters for the depth completion pipeline, introduced in Sec. 3.3, have been chosen.\nNumber of aggregated point clouds. For temporal ag- gregation, we fuse the m previous and m following point clouds. The minimum MARE for all sequences is observed for m = 4 (Fig. 15a). However, the MAE continues to de- crease for all sequences except indoor, where it increases for higher m values (Fig. 15b). As the total number of points for all indoor sequences is lower than for all out- door and night outdoor train sequences, the error for all sequences is less influenced by indoor sequences. We set"}, {"title": "C.3. Temporal Aggregation Comparison", "content": "To aggregate multiple point clouds, previous and following scans can be aggregated directly (no movement) or trans- formed based on odometry information of the robot. The HELVIPAD dataset provides only omnidirectional stereo im- ages and LiDAR point clouds but no odometry information. KISS-ICP [27] is one of the state-of-the-art approaches for LiDAR odometry. It is based on the ICP algorithm and cre- ates a local map of the environment. As it is not possible to evaluate the quality of estimated odometry data with the dataset, using a robust method that does not need hyperpa- rameter optimization, such as KISS-ICP, is a suitable choice to obtain odometry information for the dataset.\nWe compare no temporal aggregation (no aggregation), temporal aggregation without transforming point clouds (no movement) and temporal aggregation based on odom- etry information obtained with the KISS-ICP (KISS-ICP) in Tab. 6. The KISS-ICP approach yields the least favor- able results in most of the metrics. This may be attributed to the presence of moving people in the scene, coupled with the employed interpolation method. No movement is clearly better than no aggregation in RMSE and slightly bet- ter in MAE. No aggregation is the best approach in terms of MARE and IR. However, it must be noted that the ARIP is also the lowest for this method. Consequently, the MARE and IR for the same ratio may be lower than the ones of the no movement method. Overall, the results of the no move- ment temporal aggregation method are the most favorable. Thus, we use it for the temporal aggregation."}, {"title": "D. Benchmark Specifications", "content": "This section outlines the evaluation framework and bench- mark specifications for assessing model performance on the HELVIPAD dataset. Furthermore, it details the architecture of 360-IGEV-Stereo adaptations.\nD.1. Evaluation Metrics\nTo assess the performance of models, we rely on several metrics, each providing insights into different aspects of the model's disparity and depth estimation accuracy. Given the sparse nature of our ground-truth data for disparity and depth, we apply a masking technique to evaluate models' predictions only in areas with available ground truth values. The metrics are computed by summing over all pixels for which ground truth is available.\nMore formally, let us define I as the set of test set images and $p\\_{ij}$ a pixel j within. We denote the depth and dispar- ity ground truth values for a pixel j in image $i \\in I$ as $r\\_{ij}$ and $d\\_{ij}$ respectively. Similarly, the corresponding values of this pixel j in image i predicted by the model are denoted respectively as $\\hat{r}\\_{ij}$ and $\\hat{d}\\_{ij}$. Among all pixels of the image i, we denote $A\\_i$ the subset of pixels with available ground truth values in the image.\n\u2022 Mean Absolute Error (MAE): MAE measure the aver- age magnitude of errors between the predicted and actual disparity in degrees (and depth in meters), offering a di- rect assessment of overall error. For disparity and depth respectively, the MAE is defined as:\n$\\text{MAE} = \\frac{1}{\\vert I\\vert} \\sum\\_{i \\in I} \\frac{1}{\\vert A\\_i\\vert} \\sum\\_{j \\in A\\_i} \\vert y\\_{ij} - \\hat{y}\\_{ij} \\vert$,\nwhere y can represent either the depth (r) or disparity (d) values.\n\u2022 Root Mean Square Error (RMSE): RMSE measures the square root of the average squared differences, em- phasizing larger errors. It is defined as:\n$\\text{RMSE} = \\sqrt{\\frac{1}{\\vert I\\vert} \\sum\\_{i \\in I} \\frac{1}{\\vert A\\_i\\vert} \\sum\\_{j \\in A\\_i} \\Vert y\\_{ij} - \\hat{y}\\_{ij}\\Vert^2}$,\nwhere y can represent either the depth (r) or disparity (d) values.\n\u2022 Mean Absolute Relative Error (MARE): Considering the varying range of disparity (and depth) values, MARE is crucial. The metrics normalizes the error against the actual depth values, offering a nuanced measure of accu- racy. The MARE is defined as:\n$\\text{MARE} = \\frac{1}{\\vert I\\vert} \\sum\\_{i \\in I} \\frac{1}{\\vert A\\_i\\vert} \\sum\\_{j \\in A\\_i} \\frac{\\vert y\\_{ij} - \\hat{y}\\_{ij}\\vert}{y\\_{ij}}$,\nwhere y can represent either the depth (r) or disparity (d) values."}, {"title": "D.2. Implementation Details", "content": "In the following, we elaborate on the implementation and training details of each model included in the benchmark. All our experiments are conducted using Nvidia A100- SXM4-80GB GPUs.\nPSMNet. Despite its age, PSMNet is a robust and popular method for conventional stereo depth estimation that we in- cluded in our benchmark. Our implementation is based on the code provided by the authors\u00b2. We initialize our model with weights from a SceneFlow-pretrained model and fine- tune it on our dataset for 24 epochs. The model is trained with a batch size of 20 images, with an Adam optimizer, an initial learning rate of 0.0001, and no weight decay.\n360SD-Net. The model is trained from scratch for 40 epochs using an Adam optimizer with an initial learning rate of 0.001, no weight decay, and a batch size of 16. It under- goes further fine-tuning for 10 epochs at a reduced learning rate of 0.0001 to enhance performance. Our implementa- tion is based on the code provided by the authors\u00b3.\nIGEV-Stereo. Again, our implementation is based on the code provided by the authors. We initialize the model from SceneFlow-pretrained weights and fine-tune it on our dataset employing an AdamW optimizer and a one-cycle learning rate schedule with a maximum learning rate of 3e-5, alongside a weight decay of le-5. The training spans 200k steps with a batch size of 16, equivalent to approxi- mately 92 epochs.\n360-IGEV-Stereo. We adapt the IGEV-Stereo code to in- clude our architecture modification stated in the paper and further detailed in Appendix D.3. The implementation de- tails of 360-IGEV-Stereo are similar to IGEV-Stereo, with some small modifications.\nWe convert the disparity, given in degree, to pixels to be able to warp the appropriately for constructing the cost volume:\n$d\\_{\\text{pix}} = \\frac{960\\ \\text{px} \\times d\\_{\\text{deg}}}{180^\\circ}$.\nHereby, 960 px is the height of the downsampled image be- fore cropping.\nDuring training 360-IGEV-Stereo had some instabilities. To mitigate this issue, we clamp the disparity $d\\_{\\text{deg}}$ in each"}, {"title": "D.3. 360-IGEV-Stereo Architecture", "content": "To construct 360-IGEV-Stereo, we introduce three key enhancements to the IGEV-Stereo architecture.\nFirstly, the polar map is added as an additional input to the network. It has the same size as the input image and consists of repeated columns within a range of [48\u00b0, 144\u00b0], corresponding to the vertical field of view of the input im- age. Its encoder is shared between feature and context net- work. The encoder contains convolutional layers with a stride of 2 to decrease the polar map size gradually. For"}, {"title": "E. Additional Results", "content": "In this section, we further study the impact of pretrained weight initialization and cross-dataset generalization.\nE.1. Effect of Pretrained Weights\nIn addition to the ablative studies detailed in the main pa- per, we report in Tab. 12 a detailed comparison of each model performances using randomly initialization versus fine-tuning from pretrained weights.\nBoth 360-IGEV-Stereo and IGEV-Stereo show signifi- cant improvements when initialized with Scene Flow pre- trained weights, outperforming their randomly initialized counterparts across all depth and disparity metrics. For ex- ample, 360-IGEV-Stereo achieves a reduced depth MAE of 1.77 m and RMSE of 4.36 m, demonstrating the ef- fectiveness of leveraging models trained on standard im- ages like Scene Flow for omnidirectional image train- ing. Surprisingly, this pattern does not hold for PSM- Net, where the model initialized randomly achieves bet-"}, {"title": "E.2. Cross-Dataset Generalization", "content": "We study the cross-dataset generalization of 360SD-Net by first training the model on the HELVIPAD dataset and then fine-tuning it on the Stereo-MP3D [28] dataset, which shares a similar top-bottom camera configuration. Results are available in Tab. 13. Compared to a baseline trained from random initialization on Stereo-MP3D, fine-tuning on HELVIPAD significantly improved all depth and disparity metrics. For instance, the depth MAE decreased from 0.09 m to 0.07 m. Note that although we used the authors' pro- vided code\u2075 and the reported hyperparameters, our repro- duced results differ from those reported in the original pa- per. We present our outcomes for a fair comparison.\nFine-tuning on HELVIPAD offers a broader diversity of scenes, particularly outdoor environments. These results suggest that HELVIPAD not only captures a wider range of scenarios but also provides robust features that enhance generalization to datasets with overlapping characteristics."}]}