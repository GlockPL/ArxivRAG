{"title": "FuncEvalGMN: Evaluating Functional Correctness of SQL via Graph Matching Network", "authors": ["Yi Zhan", "Dongchi Huang", "Han Weng", "Guifeng Wang", "Jiajun Xie", "Yu Tian", "Boyi Liu", "Yang Sun"], "abstract": "In this paper, we propose a novel graph-based methodology to evaluate the functional correctness of SQL generation. Conventional metrics for assessing SQL code generation, such as matching-based and execution-based methods (e.g., exact set match and execution accuracy), are subject to two primary limitations. The matching-based method does not effectively assess functional correctness, as different SQL queries may possess identical functionalities. The execution-based method is susceptible to producing false positive samples in evaluations. Our proposed evaluation method, FuncEvalGMN, does not depend on the sufficient preparation of the test data, and it enables precise testing of the functional correctness of the code. Firstly, we parse SQL using a relational operator tree (ROT) called RelNode, which contains rich semantic information from the perspective of logical execution. Then, we introduce a GNN-based approach for predicting the functional correctness of generated SQL. This approach incorporates global positional embeddings to address the limitations with the loss of topological information in conventional graph matching frameworks. As an auxiliary contribution, we propose a rule-based matching algorithm, RelNode Partial Matching (RelPM) as a baseline. Finally, we contribute a dataset, Spider-Pair with a training set and two testing sets, each comprising pairs of SQL codes to simulate various SQL code evaluation scenarios.", "sections": [{"title": "1 Introduction", "content": "Automatic code generation evaluation holds significant importance and broad prospects in the fields of natural language processing and software engineering. Owing to the powerful capabilities of Large Language Models (LLMs), there has been a marked surge in attention towards code generation directly from natural language prompts. However, evaluating the performance of different code generation models remains challenging (Xu et al., 2022), which hinders the development of advanced code generation technologies. Thus, it is imperative to develop more precise and reliable evaluation methods.\nIn code generation tasks, text-to-SQL is a specific category that aims to interpret text semantics to predict accurate SQL queries (Yu et al., 2018). In essence, there are three core types of metrics utilized for the evaluation of SQL. The most commonly used one is Execution Accuracy, which entails the comparison of the denotations of the predicted with the gold SQL based on executions. Unlike PassRatio and Pass@k used in imperative languages, which are continuous by assessing code's pass rate on test cases, Execution Accuracy can only get a binary score (correct or incorrect), failing to reflect partial correctness, and is prone to high false positives when the test data is not comprehensive Zhong et al. (2020). Furthermore, it incurs a significant setup cost for the test environment and computational load. Another prevalent evaluation metric is matching-based, such as BLEU (Papineni et al., 2002), which relies on counting overlapping n-grams between the generated code and reference code (Zhou et al., 2023). However, they focus on basic and lexical-level features, failing to fully capture the wide range of functionally equivalent program variations (Dong et al., 2023). Conversely, an evaluation approach based on pre-trained models (Dong et al.,"}, {"title": "2 Related Work", "content": "Code Evaluation Metrics: The execution-based method becomes prevalent code evaluation harness Chen et al. (2021b); Xu et al. (2022); Kulal et al. (2019). Execution Accuracy compares the denotations of the ground truth and predicted SQL via execution on a database (Yu et al., 2018). This approach requires databases that cover data distributions to prevent false positives, whereby functionally different SQL happens to be incorrectly identified as correct. Test suite accuracy (Zhong et al. (2020)) involves distilling datasets to create compact, high-coverage database test suites for effective SQL semantic accuracy testing.\nOn the other hand, Matching-based evaluation metrics assess surface-form differences in code by considering the code's syntactic rules and structured nature, CrystalBLEU (Eghbali & Pradel, 2022) evaluates predicted codes quality by empirically eliminating the most common n-grams lacking semantic significance in code, while CodeBLEU (Ren et al., 2020) incorporates syntactic structure and semantic information of code through Abstract Syntax Trees (AST) and data flow analysis. However, these methods fail to correctly evaluate the code snippets that are semantically equivalent but lexically diverse, causing serious false negatives."}, {"title": "3 RelNode Partial Matching", "content": "Ren et al. (2020); Eghbali & Pradel (2022) suggest that merely interpreting code as natural language for evaluation of model generation may neglect its intricate syntactic architecture, whereas graph-based methods can be effective to abstract the syntactic structure of code without losing important information (Mi et al., 2023; Allamanis et al., 2018; Wang et al., 2020). To this end, we convert the SQL query to a Relational Operator Tree (ROT), which represents SQL's logic execution plan using relational algebra. It effectively illustrates the structural order and hierarchical organization of relational operators such as join, selection, and projection (Cyganiak, 2005). By leveraging Apache Calcite (Begoli et al., 2018), an"}, {"title": "4 RelNode Graph Matching Network", "content": "As a method based on matching, Re1PM remains susceptible to recognize identical semantics of the SQL with syntactic structure change. Therefore, we propose a novel approach, FuncEvalGMN, based on the graph matching network, to further capture the functional correctness of generated SQLs.\n4.1 Program Graph Construction\n4.2 Node Feature Embedding\nComputing Nodes: In SQL, many equivalent syntactic structures can perform the same functional operations. For instance, a combination of ORDER BY and LIMIT indicate an equivalent MAX or MIN operation. Assuming there are k distinct types in Computing Nodes, we compute the embeddings of each $x_{i} \\in [0, k - 1]$ as\n$h_{i}^{(0)} = Embedding(x_{i})$,\nwhere the superscript (0) represents the initial state, which will be updated as message propagates through the graph neural network.\nContent Nodes: The Content Nodes represents the query's parameter variables, which is also the precise database schema elements. A common challenge faced in text-to-SQL is that SQL queries, despite being structurally sound, may fail to execute as intended due to the misuse of parameter variables, such as table names and column names. The main reason is that word embedding models such as Word2Vec Mikolov et al. (2013) and FastText Bojanowski et al. (2017) is adopted for representing the semantics, with their tendency to represent two semantically similar but distinct entities (i.e., column name 'kid' and 'child') with similar embeddings. To address this challenge, a string-aware embedding method is introduced to enhance our model's understanding of different entities. Specifically, we first characterize each Content Node as a string $S = (S_{1}, S_{2}, ..., S_{n})$ of fixed length n, with each"}, {"title": "4.3 Positional Embedding", "content": "Nodes with the same neighborhood often exhibit similar representations. However, their actual functions might vary due to unique hierarchical positions or locations within subtrees. To address this, we incorporate positional encoding, specifically using the Random Walk Positional Encoding (RWPE) (Dwivedi et al., 2021). It's defined using a k-step random walk:\n$P^{w} = [T, T^{2},...,T^{k}] \\in \\mathbb{R}^{n \\times k}, k\\in [1, K]$\nwhere T is the state transition matrix of the random walk, and Tk represents the k-th power of T. A and D represent the adjacency matrix and degree matrix of the graph. This method utilizes a simple random walk matrix approach, focusing solely on the probability of node i returning to itself, denoted by Tii. This strategy offers a distinct node representation, based on the premise that each node possesses a unique k-hop topological neighborhood, particularly when k is sufficiently large."}, {"title": "4.4 Graph Embedding", "content": "Consider two graphs constructed from SQL, $G_{1} = (V, E)$ and $G_{2} = (V',E')$. We create a Graph Matching Network (GMN) based on Li et al. (2019) to generate graph representations and assess their similarity. In this section, we concentrate on enhancing the GMN's message-passing mechanism, as shown in Figure 4. More details are deferred to Appendix C."}, {"title": "5 Experiment", "content": "5.1 Dataset\nBased on the SQL-generated benchmark dataset Spider (Yu et al., 2018), we constructed a new dataset, Spider-Pair, for code evaluation purposes. This dataset consists of a training set and two testing sets, each containing pairs of SQLs to simulate diverse scenarios for SQL code evaluation. Each pair is composed of the golden SQL and generated SQL as well as a ground truth label indicating the correctness of the generated SQL. The training set (train) and one testing dataset (test) is composed of data from NL2SQL code generation using large language models, while the other (test-aug) is generated from SQL equivalence rewriting. To mitigate the issue of data leakage within the evaluation dataset, we ensure that the training and testing sets originate from distinct databases. The more details are shown in Appendix F.\n5.2 Ablation Studies\nWe evaluate code evaluation metrics against the code's functional correctness using correlation evaluation metrics such as AUC, Spearman R (rs) and Pearson R (rp) Dong et al. (2023), as discussed in Appendix I. In this section, we conduct ablation studies across four key areas to assess the impact of various components within our model."}, {"title": "5.3 Comparison with Other Evaluation Metrics", "content": "To comprehensively evaluate our model, we compare it with Matching-based Metrics including CrystalBLEU (Eghbali & Pradel, 2022), CodeBLEU (Ren et al., 2020), RelPM, ASTPM and pre-trained model-based methods including CodeScore (Dong et al., 2023), CodeBERTScore (Zhou et al., 2023) on two datasets. In addition, to evaluate from the perspective of SQL equivalence, we compare our approach with the state-of-the-art SQL"}, {"title": "5.4 Generalization to Other NL2SQL Dataset", "content": "BIRD Li et al. (2024) is a Dataset which has more complex schemas which is more suitable for the LLM SQL generation task. We simulate the BIRD-pair dataset with the same guidelines in Appendix F.1.\nThe performance of FuncEvalGMN Table 3 remains superior compared to other methods, and hence generalizibility is evident. Also, we evaluate complexities of two data sets, Spider-"}, {"title": "6 Conclusion", "content": "In our work, we present FuncEvalGMN, a novel graph-based approach for enhancing the evaluation of functional correctness in code, particularly SQL. This method involves transforming SQL into a graph that captures its syntactic and semantic essence, using a combination of node types and encoding strategies to represent SQL structures and parameters. We leverage Graph Matching Networks to assess graph similarity, incorporating positional embedding for improving structural understanding. Our experiments include a unique dataset, Spider-Pair, and baseline methods for comprehensive evaluation. Looking forward, we aim to extend this methodology to additional programming languages, exploring its potential to generalize across diverse coding paradigms and enhance code evaluation techniques further."}, {"title": "A RelNode Partial Matching (RelPM)", "content": "RelPM calculates the matched nodes and their scores for each RelNode, using the F-beta score to determine the similarity between the two SQLs. We divide the process into three parts: Node Matching, RelNode Scoring, and Similarity Evaluation.\nA.1 Node Matching\nWhen matching two SQL queries' RelNodes, we can designate one as the source tree and the other as the target tree, then use depth-first search to find their matching node sets. When evaluating the match for the target tree, the source tree is treated as a reference to measure how well the nodes of the target tree align with it. The success of node matching is determined by scoring against l candidate matching nodes, with the highest-scoring node being selected as the final match. The formula for this process is $m = max{m_{i}}, j \\in {0,1,...l}$ where mi represents matching score of node n with a candidate matching node n; in another tree, which can be calculated as:\n$m^{i} =  \\alpha  \\times m^{i}_{self} + (1 -  \\alpha )   \\frac{\\sum^{N}_{i=0} m^{m}}{N}  \\alpha \\in (0,1)$\nThe $m^{i}_{self}$ represents the matching score of the node itself with the candidate matching node, and $m^{m}_{child}$ indicates the maximum matching score of child node between two trees. The calculation of $m_{child}$ is recursive, following the same method as m, until it reaches leaf nodes. Additionally, as Figure 5 illustrates that \u03b1 serves as a weighting coefficient to balance the significance between the scores of a node and those of its children. A node colored in red signifies a successful match, whereas a gray node denotes an unsuccessful one. When \u03b1 falls below 0.5, the emphasis shifts towards the matching outcomes of the child nodes, leading us to circumvent the \"OR\" and \"AND\" logic in favor of matching a greater number of child nodes. Conversely, when \u03b1 exceeds 0.5, the focus is placed on matching the parent node, resulting in the failure to match for all corresponding subtree nodes beneath it.\n$m^{self} = \\begin{cases} 1 & \\text{if } val(n) = val(n_{j}) \\\\ 0 & \\text{otherwise} \\end{cases}$\nFor a given node, its matching score is determined by the candidate matching node nj. If the attributes of both nodes are identical, it is considered that a match can be established."}, {"title": "A.2 RelNode Scoring", "content": "For trees that exhibit matching information, we can adjust the weights of clauses and key nodes according to their importance, thereby influencing the overall score. The scoring equation is defined as:\n$s = \\omega  \\times s_{self} + \\frac{\\sum^{N}_{i=0} \\omega  \\times s^{i}_{child}}{N}, \\text{where }   \\omega   + s_{self} = \\sigma , \\sigma \\in (0,1)$\nHere, $w$ represents the weighting factor, and $s_{self}$ denotes the node's own score, which is calculated during the process of matching stage. The calculation starts from the root node and proceeds recursively, combining the scores of the node itself and its children through a weighted sum."}, {"title": "A.3 Similarity Evaluation", "content": "By performing a cross-comparison between the source and target trees, \"Precision\u201d calculates the percentage of nodes in the source tree that successfully find matches in the target tree, while \"recall\" measures the percentage of nodes in the target tree that are matched in the source tree. We compute the weighted geometric mean,\n$F_{\\beta} = \\frac{(1 + {\\beta}^{2})  \\times Precision  \\times Recall }{{\\beta}^{2} \\times Precision + Recall}$\nWhen assessing code generation models, our main focus is on verifying if the generated code fully aligns with the semantics of the reference code. In this context, the reference code acts as the target tree, while the generated code represents the source tree. Hence, we give priority to recall in our evaluation, indicating that we assign a relatively high value to \u03b2."}, {"title": "B Content Node Feature Embedding", "content": ""}, {"title": "C RelNode Graph Matching Network", "content": "In addition to message passing, the other implementation details of GMN are as follows:\nC.1 Update Function\nThe update function integrates all gathered messages to update each node's representation at every iteration step. This process is mathematically formulated as:\n$h_{v}^{(t+1)} = f_{update}(h_{v}^{(t)}, m_{v}^{(t+1)},  \\rho_{v}^{(t+1)}),$\nIn this equation, $h_{v}^{(t+1)}$ is the updated representation of node v at step t + 1. The function $f_{update}$, which in our implementation is a Gated Recurrent Unit (GRU), updates the node's feature representation using its previous state $h_{v}^{(t)}$, the inner-graph message $m_{v}^{(t+1)}$, and cross-graph communication $\\rho_{v}^{(t+1)}$.\nC.2 Aggregator\nAggregation is to calculate a representation for the entire graph. Following T propagation steps, an aggregation function processes the set of node representations to produce a graph-level representation hg. The aggregation method we utilize is proposed in (Li et al., 2015):\n$h_{G} = MLP_{G} \\sum_{v} \\sigma (MLP_{gate}(h_{v}^{(T)})) \\odot MLP(h_{v}^{(T)}))$ DEG (v) MLPSD"}, {"content": "In this method, a gated weighted sum, which aggregates information across all nodes and filters out irrelevant data, proves to be more effective than a simple summation.\nC.3 Similarity Metric\nOnce we obtain the graph representations, hG\u2081 and hG\u2082, for the graph pair (G1, G2), we evaluate their similarity using a vector space metric. Suitable metrics include Euclidean, cosine, or Hamming similarities. In our case, we employ the Euclidean distance as the similarity metric, which is defined as:\n$s(h_{G_{1}},h_{G_{2}}) = ||h_{G_{1}} \u2013 h_{G_{2}} ||_{2}.$\nC.4 Loss Function\nIn our work, we need to evaluate the distance between the graph similarity we calculated and the true labels. To achieve this, we utilize a margin-based pairwise loss:\n$L = max{0,  \\gamma  \u2013 t(1 \u2013 s(h_{G_{1}}, h_{G_{2}}))}$"}, {"title": "D Equivalent Conversion Capabilities of RelNode", "content": "Calcite converts relational algebra expressions generated by the parser into an execution plan, applying several optimization rules in the process. These optimization rules enable equivalent transformations of relational expressions, such as Predicate Pushdown, Constant Folding and Column Pruning.\nIn our work, we utilize execution plans generated by Calcite to construct graphs. It abstracts the syntactic structure of SQL and provides rich semantic information from the perspectives of logical execution and variable usage. Furthermore, its optimization of execution plans standardizes SQL, uncovering the same functionality under different syntactic structures, thereby reducing the difficulty of model judgment in determining functional equivalence."}, {"title": "E False Positive Cases", "content": "In this section, we will introduce two cases illustrated in Figure 9 to discuss the issue of false positives in measuring the correctness of SQL queries, specifically cases where functionally incorrect SQL queries are mistakenly deemed correct.\nIn the first case, the sole semantic difference between the two SQL queries lies in the range specified by their filtering conditions. The reference SQL uses the conditions weight > 3000 AND weight < 4000, while the generated SQL specifies weight >= 3000 AND weight <=\n4000. Since there are no records in the database with a weight field exactly equal to 3000 or 4000, these two SQL queries retrieve the same set of data, incorrectly treating these inequivalent SQLs as equivalent.\nIn the second case, whether the two SQL statements are equivalent depends on the DDL settings. If capacity is set to be unique, then there will only be one record in the database with the maximum capacity value, resulting in both SQL queries producing the same single record. However, if capacity is not set to unique, there could be multiple records sharing the maximum capacity value. This means that the reference SQL and the generated SQL are not equivalent, leading to an erroneous execution match."}, {"title": "F Spider-Pair Dataset", "content": "Text-to-SQL refers to the process of translating natural language queries into precise SQL commands Yu et al. (2018); Iyer et al. (2017); Deng et al. (2021); Yaghmazadeh et al. (2017); Finegan-Dollak et al. (2018). Numerous datasets, including Spider Yu et al. (2018), have been developed for this purpose, where each entry comprises a reference SQL query, a corresponding natural language question, and the relevant database. However, there is no dataset available to validate the consistency between the quality of generated SQL code and evaluation metrics. Our proposed Spider-Pair fills this gap. It consists of a training set and two testing sets, which we refer to as train, test, and test-aug. Each entry in the dataset includes a pair of SQL queries (reference and generated SQL), a prompt, and the functional correctness of the generated SQL. In the following sections, we will introduce our construction approach.\nF.1 SQL Pairs Auto-generated by LLMs\nTo generate SQL pairs, we utilize Spider Yu et al. (2018), as our source dataset. It comprises 10, 181 queries and 5, 693 unique, complex SQL queries spanning 200 databases across 138 distinct domains. We utilized 8, 659 examples from 146 databases as our source train set,"}, {"title": "F.2 Data Augmentation by Equivalent Rewriting", "content": "In Section F.1, we utilize the same LLMs in both the train and test sets to simulate the code generation process. Given that the same model tends to utilize similar syntactic structures to address the same type of queries, SQL pairs in both sets share identical data distributions. However, such data biases can impact the effectiveness of evaluations. To address this issue, we perform equivalence rewriting on the reference SQL, employing diverse syntactic structures to construct SQL pairs. Additionally, we verify their functional equivalence using same method in Section F.1. Figure 10 illustrates two Hard Positive Cases by our equivalence rewriting, where the SQL pairs, despite achieving identical outcomes, employ significantly varied syntactic structures."}, {"title": "G Attention Visualization For Explanation", "content": "In this section, we analyze a hard positive case that two SQLs are functionally equivalent but differ in their syntactic structure. We visualize the initial and final attention graphs propagated by our FuncEvalGMN to examine the its capacity to identify key features within the graph. Furthermore, by observing the modifications in the attention graph from initial to final propagation, we elucidate how FuncEvalGMN detect and match nodes with strong correlations. This analysis demonstrates that our model is adept at recognizing equivalent substructures and functional nodes across both graphs, thereby facilitating a thorough comprehension of the SQL's syntactic structure and semantics."}, {"title": "G.3 Attention Map Comparison", "content": "In the following, we compare the attention maps from the initial and final propagations to analyze the trends in the attention map changes throughout the model's propagation process, ultimately discerning the capabilities and characteristics of the model's attention component in feature extraction.\nInitially, the node embeddings have only been processed by the encoder layer and have not yet integrated neighborhood information and structural features. At this stage, the model quickly captures the SQL's core features, ts_date and MAX, but overlooks other SQL details. However, after the final propagation, by observing the direction and opacity of the attention edges, we can discern that the attention map exhibits the following four characteristics:\n1. The distribution of attention is more uniform.\n2. The attention weights are more balanced.\n3. Equivalent substructures within the two graphs are captured.\n4. Functionally equivalent nodes across different structures are identified.\nThese observations indicate that as propagation progresses, the model begins to consider a broader range of features within the SQL graph, moving beyond the initial focus on key elements to a more comprehensive understanding of the SQL's structure and semantics. Additionally, the model is capable of extracting functionally consistent information from both equivalent functional nodes and equivalent substructures. This capability is, to some extent, due to the use of the relational operator tree (ROT), as each subtree (substructure) within the ROT represents an atomic functional operation in the execution plan."}, {"title": "H Test Set AUC Trends During Training", "content": ""}, {"title": "I Correlation Evaluation", "content": "The performance of different models can be evaluated using the following metrics:\nArea Under the Curve (AUC)(Huang & Ling, 2005) refers to the area under the Receiver Operating Characteristic (ROC) curve. The ROC curve is a graphical representation that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n$AUC = \\int TPR( \\xi )FPR'( \\xi ) d \\xi$"}, {"title": "J Evaluation of Code LMs", "content": "Our training and testing datasets are sourced from GPT3.5, GPT-4, and CodeLlama. To validate the effectiveness of our FuncEvalGMN against other models, we conducted inference using the DeepSeek model on the Spider dataset. The obtained AUC, rs, and rp are 91.64%, 51.55%, and 59.94%, respectively, demonstrating that our approach also exhibits strong evaluation capabilities on other large models.\nFinally, we evaluated four Code Large Models (LMs) on the Spider dataset using three evaluation metrics: FuncEvalGMN, Test Suite (Zhong et al., 2020), and Execution Accuracy Yu et al. (2018). The original output range of FuncEvalGMN, denoted as y, ranged from negative infinity to zero. We normalized the output results to the range of 0 to 1 using the formula $y_{norm} = \\frac{y+3}{max [3,0]}$.\nAs shown in Figure 15, our FuncEvalGMN can also serve as a good metric for evaluating SQL generation. Compared to Execution Accuracy and Test Suite, we do not incur the cost of maintaining and executing databases."}, {"title": "K Keywords Distribution in BIRD and SPIDER Dataset", "content": "In our study, we analyzed two datasets, BIRD and SPIDER, to understand the distribution of SQL keywords. This analysis provides insight into the complexity and nature of queries present in each dataset.\nThe BIRD dataset exhibits a high frequency of the WHERE keyword, present in 90.29% of the queries. This indicates a strong emphasis on filtering conditions in the queries. JOIN operations are prevalent, appearing in 74.32% of the queries, suggesting a significant number of queries involve combining data from multiple tables.\nThe SPIDER dataset presents a different distribution of SQL keywords. The WHERE keyword is used in 47.68% of the queries, significantly less frequent than in the BIRD dataset, indicating fewer conditions are applied to filter data. The JOIN keyword appears in 39.46% of the queries, showing less reliance on combining tables compared to BIRD.\nInterestingly, Aggregation (53.29%) and Counting (39.85%) are more common in SPIDER, suggesting a higher focus on data summarization. The Order By keyword is used in 22.34% of the queries, and LIMIT appears in 17.70%, both slightly higher than in BIRD.\nThe analysis of these datasets reveals distinct patterns in SQL keyword usage, reflecting the different types of queries they encompass. BIRD has a higher emphasis on filtering and joining data, while SPIDER shows a greater focus on data aggregation and union operations. These differences highlight the varied query complexities and use cases catered to by each dataset."}]}