{"title": "ReachAgent: Enhancing Mobile Agent via Page Reaching and Page Operation", "authors": ["Qinzhuo Wu", "Wei Liu", "Jian Luan", "Bin Wang"], "abstract": "Recently, mobile AI agents have gained increasing attention. Given a task, mobile AI agents can interact with mobile devices in multiple steps and finally form a GUI flow that solves the task. However, existing agents tend to focus on the most task-relevant elements at each step, leading to local optimal solutions and ignoring the overall GUI flow. To address this issue, we constructed a training dataset called MobileReach, which breaks the task into page reaching and operation subtasks. Furthermore, we propose ReachAgent, a two-stage framework that focuses on improving its task-completion abilities. It utilizes the page reaching and page operation subtasks, along with reward-based preference GUI flows, to further enhance the agent. Experimental results show that ReachAgent significantly improves the Intersection over Union (IoU) Accuracy and Text Accuracy by 7.12% and 7.69% on step-level and 4.72% and 4.63% on task-level compared to the SOTA agent. Our data and code will be released upon acceptance.", "sections": [{"title": "1 Introduction", "content": "With the quick advancement of visual language models (VLMs) (Wang et al., 2023; Bai et al., 2023), it has become more feasible to create mobile AI agents that can operate mobile devices (Yang et al., 2023; Ding, 2024; Li et al., 2020). Some early works (Yang et al., 2023; Zhang et al., 2024; Yan et al., 2023) have attempted to combine powerful general VLM models, such as GPT-4V (OpenAI, 2023), with prompt engineering and retrieval modules to generate UI actions for mobile control. Other works have used GUI navigation datasets (Rawles et al., 2023; Zhang et al., 2024) to fine-tune the base VLMs to improve their stability in generating UI actions. Subsequent works (Baechler et al., 2024; Wu et al., 2024) have built mobile-specific datasets to pre-train VLMs so that the model can better understand graphical user interface (GUI) elements and page structure. Specifically, mobile AI agents use the VLM as their base model and the mobile device as the environment. The agent iteratively interacts with the mobile's GUI for each mobile control task, by generating actions based on environmental information and executing those actions to update the GUI environment. This interactive process forms a GUI flow.\nAlthough existing mobile AI agents have shown good performance, they still have some limitations. Existing agents focus on interacting with the most task-relevant elements of the current page, and ignore whether the entire GUI flow solves the task. This method sometimes causes them to focus on single-step action accuracy and select tasks-related actions greedily, thus falling into a local optimal solution. As shown in Figure 2, for the task of adding Xiaomi 14 to the shopping cart, the agent should first reach the product page, but the homepage does not contain elements related to the product name. In this case, it might go directly from the homepage to the shopping cart page.\nTherefore, we break down the task into several subtasks and focus on the agent's subtask completion abilities. Intuitively, these subtasks can be divided into two categories, reach and operate. Reach only requires the agent to reach a specific page, regardless of the path taken. Operate requires the agent to reach a specific page and perform some specific operations. Figure 2 shows a task and its GUI flow, consisting of a 9-step chain, the key to solving this task is to reach the product page first, and then click the \"Add to Cart\" pop-up window and select the attributes and \"OK\" button in the pop-up window in sequence, as shown in Figure 1. For the product page reach subtask, it can be done either through product search or by browsing the product catalog. Similarly, the operate subtask only requires the agent to select the \"B\" attribute in the pop-up window, and browsing other attributes does not affect the task completion.\nTo address these limitations, we collected a training dataset called MobileReach, which contains three types of tasks: page navigation, page reaching, and page operation. In addition, we propose ReachAgent, a two-stage framework that focuses on the ability to complete subtasks. In the first stage, in addition to page navigation abilities, it also learns how to reach a specified page and complete operations on a specified page. In the second stage, a 4-level reward function is used to weigh different GUI flows and construct preference data to further reinforce ReachAgent, Furthermore, an action alignment mechanism is proposed to reduce the difficulty of action generation. The main contributions of this paper can be summarized as follows:\n\u2022 We break down the mobile control task into page reaching and page operation subtasks, and construct a mobile control dataset called MobileReach consisting of three types of tasks.\n\u2022 We proposed ReachAgent, a two-stage framework that utilizes page reaching and page operation to enhance the agent's subtask completion abilities. In addition, it uses reinforcement learning to further enhance the agent's overall task completion ability. ReachAgent includes an action alignment mechanism that decreases the number of candidate actions, thereby reducing the difficulty of the task.\n\u2022 Experimental results show that ReachAgent outperforms existing state-of-the-art agents. It also demonstrates stronger page reaching and operation abilities."}, {"title": "2 Related Work", "content": "Mobile AI Agent. Many recent studies have proposed mobile AI agents for device control. Appagent (Yang et al., 2023) and Mobileagent (Ding, 2024) use prompt engineering methods and rely on existing closed-source models (e.g., GPT-4V, GPT-40) to achieve mobile control. Other studies like CogAgent (Hong et al., 2023) and Agenttuning (Zeng et al., 2023) use data-driven methods to fine-tune the open-source VLMs. In order to improve the device control abilities of mobile AI agents, large-scale datasets with diverse scenarios and accurate annotations are needed.\nRico (Deka et al., 2017) and AITW (Rawles et al., 2023) are two publicly available large-scale GUI datasets, containing 72,219 single-step GUI tasks and 715,142 multi-step page navigation tasks, respectively. They are widely used in multiple GUI modeling works (Wang et al., 2021; Li et al., 2021; Hsiao et al., 2022). However, they are constructed by combining crowdsourcing workers and automated annotation, which also leads to noise and wrong labels. To this end, a series of published works clean and filter these datasets to improve their quality. Enrico (Leiva et al., 2020), UI-Bert (Bai et al., 2021), Vins (Bunian et al., 2021) extended the Rico dataset and proposed new GUI tasks such as UI layout classification and UI element retrieval. AutoUI (Zhan and Zhang, 2023) further filtered the GooglePlay tasks in the AitW dataset. However, these datasets mainly contain screenshots and OCR text, lacking GUI raw data such as xml documents, which limits the agent to further obtain mobile environment information. MobileVLM (Wu et al., 2024) and ScreenAI (Baechler et al., 2024) build large-scale pre-training datasets to enhance the agent's UI understanding ability. Here, Mobile3M (Wu et al., 2024) proposed by MobileVLM uses a breadth-first approach to explore the GUI pages of each APP and forms these pages into a graph structure. This allows us to obtain environmental information about different paths and operations, for example, the next pages after performing different actions in a page, and how many paths there are from one page to another. However, as a pre-training dataset, MobileVLM lacks fine-tuning tasks. Most of their tasks are UI understanding and single-step action generation. A small number of multi-step tasks can be summarized as navigate to a page with a certain button.\nTherefore, based on Mobile3M, we extracted available GUI flows and annotated a page navigation dataset. Using the graph structure environment, we further extracted page reaching and page operation datasets, and scored and paired preference datasets for RL learning. We used the three page datasets for one-stage SFT fine-tuning of ReachAgent and the preference datasets for two-stage RL optimization.\nReinforcement Learning (RL). RL-based fine-tuning methods have shown great potential in"}, {"title": "3 Dataset Construction", "content": "3.1 Data Definition\nGUI flow: Given a user's instruction, an agent will conduct multiple rounds of interactions with the mobile device to complete the task and record the process as GUI flow. Specifically, the GUI flow is a chain structure with the GUI page as the head node, the agent's action as the edge, and the resulting GUI page after executing the action as the tail node. Figure 2 shows a GUI flow. It is defined as a 9-step chain sequence, which includes a complete set of 9 GUI Pages and 9 actions.\nGUI page: Our algorithm uses an Android emulator to run the apps and display the GUI pages. A typical GUI page consists of a screenshot and an XML document containing multiple elements such as buttons, text boxes, icons, and images. These elements have unique identifiers, such as text, bounds, and resource IDs, used to interact with them programmatically. Figure 4 shows a GUI page example.\nAction: Our action set includes four types of actions: Click, Scroll, Input, and Complete. Figure 3 summarizes the parameters and examples of these actions. When the agent interacts with the UI page, such as clicking a button or entering text, Appium\u00b9 sends an action command to the Android emulator, which then performs the corresponding action. The changes of the GUI page are then captured as a new\nscreenshot and XML document, which are used as the environment for the next step.\n3.2 Page Navigation Dataset\nOur page navigation dataset is built using Mobile3M's GUI flows. Mobile3M (Wu et al., 2024) uses random walks to explore the page jump paths of 49 apps and combines them into a graph format. We sample GUI flows from the graph and generate corresponding tasks to build the page navigation dataset. The construction pipeline is as follows:\n1. GUI flow extraction: Select GUI flows with path lengths of 3-10 steps from Mobile3M.\n2. Image Caption: Generate image captions for each GUI page in the flow with InternVL(Chen et al., 2024).\n3. Task Generation: Use GPT-4V to generate a step-by-step description and a brief task for the GUI flow based on the captions of each page and the actions between pages, as shown in Figure 3. The prompt for task generation is in Appendix B.\n4. Data cleaning: Filter out low-quality GUI flows with duplicate tasks or invalid GUI pages.\nAfter the above steps, we have a total of 53,832 GUI flows and 259,742 action steps, approximately 4.8 steps per task. Each task includes a brief task, GUI pages, and corresponding actions, along with a step-by-step description of the task.\n3.3 Page Reaching and Operation Dataset\nTo improve the model's ability to reach and operate mobile GUI pages, we break down the GUI flow into multiple subtasks. There are two types of subtasks described as follows:\nPage reaching task. It takes an instruction for reaching a specific page as input and a GUI flow from the app homepage to that page as output. Here, each GUI page is named in three ways.\n1. If the corresponding step in the step-by-step description mentions the name of a page, we use that name directly, such as \"search results page.\"\n2. We extract the element name or text input from the action as the page name, such as \"white\" or \"add to cart\".\n3. If the name is invalid or too common, we use GPT-4 to summarize the page name based on the GUI page's caption.\nWe use page names and templates to construct page reaching tasks. The pre-defined task templates are shown in Appendix B.\nPage operation task. It requires the agent to complete a specific task on a page, which requires"}, {"title": "4 Models", "content": "4.1 Action Alignment Mechanism & Action Space Generation\nThe mobile control task can be represented as a multi-round interaction process, which generates an action sequence \\(A = {a_1,a_2, ..., a_t }\\) based on a given user instruction X and GUI pages \\(P = {P_1, P_2, ..., P_t }\\) in the mobile environment. This multi-round interaction process forms a GUI flow.\nAction Alignment Mechanism. As shown in Figure 4, the GUI page Pt contains two forms: screenshot \\(P_t^{vis}\\) and XML document \\(P_t^{xml}\\). The screenshot shows multiple elements of the GUI page, while the XML document provides text descriptions and attributes for these elements. While users can touch any point on the screen, their goal is to interact with an element on the page. To narrow down the range of candidate actions, we propose an action alignment mechanism. That is, we suggest that the agent only clicks on the center point of an element or scrolls along its central axis. For a candidate box [273,84][324,180], we can click or enter at position (298, 132). For a candidate box [0,528][720,960], we can scroll left from (360, 744) to (180, 744).\nAction Space Generation. With the action alignment mechanism, we can extract all the candidate actions in the GUI page as the candidate action space \\(P_t^{acts}\\). Each element in the screenshot corresponds to an element description in the XML document. We extract the element's bounding box and its attributes, such as whether it is clickable, scrollable, and inputtable, from the description and form an action. As shown in Figure 4, the \"search box\" element can be formed into an input action and a click action. The \"RecyclerView\" element can be formed into scrolling actions in 4 directions.\n4.2 First Stage SFT Framework\nGiven a task X, in the t-th step, the screenshot \\(P_t^{vis}\\) is encoded into the image representations \\(h_t^s\\) by a frozen visual encoder as follows:\n\\(h_t^s = ViT(P_t^{vis}).\\) (1)\nHere, visual encoder ViT is ViT-bigG (Dosovitskiy et al., 2020). The candidate action space \\(P_t^{acts}\\) is extracted from \\(P_t^{xml}\\) and encoded into the text representation \\(h_t^{acts}\\) as follows:\n\\(h_t^{acts} = E(ActionSpace(P_t^{xml})).\\) (2)\nHere, E() is a wording embedding layer. The candidate action space \\(ActionSpace(P_t^{xml})\\) is a list of all possible actions that can be used to interact with the page, as described in Section 4.1.\nSimilarly, instruction X and action history \\(A_{<t} = {a_1, ..., a_{t-1}}\\) are also encoded as \\(h_t^x\\) and \\(h_{<t}^{act}\\). ReachAgent uses a position-aware visual language adapter to align visual representations with text representations, and generate the the action \\(a_t\\):\n\\(h_t^x, h_{<t}^{act} = E(X), E(A_{<t}), \\)\n\\(h_t^{vis} =Adapter(h_t^s),\\)\n\\(P(a_t) =ReachAgent(h_t^x, h_t^{vis}, h_t^{acts}, h_{<t}^{act}).\\) (3)\nAfter generating the action \\(a_t\\), we execute \\(a_t\\) in the mobile environment to update the GUI page \\(P_t\\) to \\(P_{t+1}\\). Therefore, in the next step, ReachAgent has instruction X, screenshot \\(P_{t+1}^{vis}\\), candidate action space \\(P_{t+1}^{acts}\\) and action history \\(A_{<t+1}\\) as inputs for action \\(a_{t+1}\\) generation. ReachAgent needs to go through multiple rounds of action generation and execution loops until it finally generates the action \"STATUS_TASK_COMPLETE\" to end the mobile-control process, or until it exceeds the maximum number of interaction steps.\nIn the first stage, ReachAgent fine-tuned its ability to reach pages and perform actions on them by using all three datasets. The cross-entropy loss is defined as:\n\\(L = \\sum_t  log P(a_t|X, P_t^{vis}, P_t^{acts}, A_{<t}).\\) (4)"}, {"title": "4.3 Second Stage RL Framework", "content": "Preference data construction. To further reinforce ReachAgent, we constructed a preference dataset \\(D = {(X, P_t, a^u, a^\\lambda)}\\). Here, \\(a^u\\) is the chosen action at step t, and \\(a^\\lambda\\) is the rejected action. Our construction approach is based on two key principles: 1. The GUI flow should reach the required page and complete the required operation in the instruction; 2. The GUI flow should be as short as possible.\nGiven an instruction X and the current page Pt, we can divide all potential actions into four levels:\nGolden: The action is in the golden GUI flow, or in a GUI flow that can complete the instruction and has the same length as the golden flow.\nLonger: The action is in a GUI flow longer than golden, but can still complete instruction X.\nIncomplete: The action is not in a GUI flow that can complete the instruction.\nInvalid: The action cannot be executed or is not in the action space of the current page.\nNaturally, for these four levels, we expect their reward scores to be:\n\\(R(Golden)>R(Longer)>R(Incomplete)>R(Invalid)\\) (5)\nGiven a task, \"add product A to the shopping cart\", the subtasks it needs to complete are simplified into Reach(P2): Go to the \"Product A\" page, and Operate(P2, \"add to cart\"): Add Product A to the cart on the product page. Figure 4 shows several GUI flows for this task. \"P0, P1, P2, P3\" and \"P0, P1, P2, P3\" are the 4-step golden GUI flows. \"P0, P1, P2, P2, P3\" takes 5 steps to complete the task and marked as Longer. \"P0, P1, P3\" doesn't reach the P2 page and marked as Incomplete.\nBased on the 4-level reward function, we identify the chosen and rejected actions for the shared history flow. Start with GUI page P0, \\(a^\\lambda\\) and \\(a^u\\) are in the Golden flow, while \\(a^\\lambda\\) leads to a Longer flow. Therefore, \\(R(a^u|P_0)>R(a^\\lambda|P_0)\\) and \\((X, P_0, a^u, a^\\lambda)\\) is a preference data. Start with page P1, \\(a_1^u\\), \\(a_1^\\lambda\\), and \\(a_1^\\lambda\\) lead to a Golden, Longer, and Incomplete flow respectively. The preference data could be \\((X, P_1, a_1^u, a_1^\\lambda)\\) and \\((X, P_1, a_1^u, a_1^\\lambda)\\).\nTo collect preference data, we use the SFT agent to regenerate the training split of the page navigation dataset. If the reward of an action is less than the golden action, we use it to construct the preference data. Otherwise, we randomly select an action with a lower reward in the action space of each page to pair with the golden action.\nDPO Optimization. Direct Policy Optimization (DPO) (Rafailov et al., 2024) does not require an explicit reward score, but only requires a preference for paired data, which is more suitable for our 4-level reward function. Therefore, we adopt DPO to"}, {"title": "5 Experiment", "content": "5.1 Datasets and Benchmarks\nThe MobileReach dataset contains 3 splits: page navigation, page reaching, and page operation. We used the GUI graph from Mobile3M to build the MobileReach dataset. The detailed information of the MobileReach dataset is shown in Table 2. For preference data, 48,013 rejected actions are generated by the agent, and 211,729 rejected actions are sampled from the action space.\nWe used the page navigation split of the MobileReach dataset and the Auto-UI dataset (Zhan and Zhang, 2023) for testing. For the Auto-UI dataset, we follow the official split and method for finetuning. For the MobileReach dataset, we randomly selected 5% of the GUI flows as a test set before data construction. Therefore, there was no overlap between the training set and the test set.\n5.2 Evaluation Metrics\nWe use two objective metrics to evaluate the position and text of the generated action. IoU Acc evaluates the bounding box accuracy. Text Acc evaluates the text accuracy. Step-level accuracy evaluates whether each action in a GUI flow is correct, while task-level accuracy evaluates whether all actions in an entire GUI flow are correct. See more metric details in Appendix A.\n5.3 Implementation Details and Baselines\nReachAgent chose MobileVLM as the backbone model. We use 8 80GB Nvidia A100 GPUs for fine-tuning. The learning rate is 1e-5, and the agent's max length is 4,096. For the MobileReach dataset, in the first stage, ReachAgent was trained for 2 epochs on all three splits. In the second stage, it was trained for 2 epochs on 259,742 preference data. For the Auto-UI dataset, ReachAgent is fine-tuned for 1 epoch on its training data using the first-stage framework. For specific information on parameters and baselines, refer to Appendix A.\n5.4 Main results\nThe main experimental results are presented in Table 1 and 4. We can observe that:\n\u2022 For MobileReach dataset, ReachAgent improves the IoU Accuracy, and Text Accuracy by +7.12%, +7.69% on step-level and +4.72% and +4.63% on task-level compared to fine-tuned MobileVLM. It also raises +6.59% on Task Success Rate. This proves that ReachAgent is not only good at generating actions at each step, but also provides a more efficient GUI flow to successfully complete the task.\n\u2022 At both step-level and task-level, ReachAgent-stagel has a significantly higher IoU and Text accuracy than other SFT baselines. We attribute this to our action alignment mechanism and the introduction of page reaching and page operation"}, {"title": "5.5 Ablation Study", "content": "Table 5 shows several ablation experiment results. Effects of Action Alignment Mechanism: Through the action alignment mechanism, the performance of the agent (+Al) is improved by 5.55% and 1.46% on average at the step level and task level. This is because the action alignment mechanism helps to narrow down the range of possible actions that the agent can take, reducing the generation difficulty.\nEffects of Page Reaching and Page Operation Subasks: Adding the page reaching subtask and the page operation subtask both improve the agent's performance. The accuracy of the \"+ Al & Re & Op\" increases by an average of 1.7% and 1.87% at the step level and task level compared to \"+ Al\". These subtasks enhance the agent's understanding of the task goal and the order of actions, making the agent generate actions that are more aligned with the overall goal of the task. This leads to better performance at both the step and task levels.\nEffects of Reinforcement Learning: ReachAgent further adds RL to \"+ Al & Re & Op\". It mainly improves the task-level accuracy. This is because our reward focuses more on whether the agent can generate a short GUI flow that can complete all subtasks, rather than whether the action of each step is exactly the same as the golden answer.\nAbility to complete subtasks: We also compared the ability of different ablations to complete the Reach and Operate subtasks. As shown in Table 3, ReachAgent improves the overall IoU accuracy by 1.9% and the Text accuracy by 2.18%, outperforming the agent without RL in all subtasks. This shows that the RL mechanism improves the agent's ability to complete various subtasks. In addition, adding the page operation subtask contributes more to the agent's performance than adding the page reaching subtask. This is because the page operation subtask not only improves the agent's ability to operate pages, but also requires the agent to be able to reach the specified page."}, {"title": "5.6 Case Study", "content": "Figure 5 shows two generation cases. MobileVLM tends to greedily choose task-related actions at each step, leading to local optima. In the first case, it goes directly to the \"Public Transportation\" page on the homepage, or clicks on \"Running Routes\" before finding nearby restaurants. In the second case, when the task is to search for a property that is about to be sold, MobileVLM keeps scrolling down, trying to browse directly to find the required property, and gets stuck in an endless loop. In contrast, ReachAgent improves the ability to reach and operate on task-related pages, and further completes the task based on these subtasks."}, {"title": "6 Conclusion", "content": "In this work, we propose ReachAgent, a two-stage framework that leverages page reaching and page operation tasks to enhance the agent's subtask completion abilities. Besides, we use a 4-level reward function to collect preference GUI flows, and further enhance the agent's overall task completion ability with reinforcement learning. In addition, we construct a mobile control dataset called MobileReach, which contains 3 categories of tasks: page navigation, page reaching, and page manipulation. Experimental results show that ReachAgent significantly improves IoU and Text accuracy by 7.12% and 7.69% at the step level and 4.72% and 4.63% at the task level, respectively.\nOur ReachAgent shows strong task completion abilities by addressing the challenge of mobile AI agents focusing more on single-step action accuracy rather than completing the entire task flow. We hope that MobileReach can serve as a useful resource for breaking down tasks, solving page reach and operation subtasks, and provide assistance for future research."}, {"title": "Limitations", "content": "Our training data is built on a graph consisting of GUI flows of 49 commonly used apps. Since the method of exploring the GUI graph is random walk, this may not cover all the functions of the app. In addition, we select GUI flows by random sampling, which may result in many invalid GUI flows that do not have corresponding tasks."}, {"title": "Ethics Statement", "content": "This paper was conducted in accordance with the ACM Code of Ethics. Our MobileReach dataset is constructed using publicly available platforms and data sources, ensuring that there are no privacy issues or violations. All data used in our research was obtained following legal and ethical standards, and we do not collect any personally identifiable information."}, {"title": "A Experiment Settings", "content": "A.1 Baselines\nReachAgent was compared to four other baselines as follows: GPT-4o, Qwen-VL, Auto-UI, MobileVLM.\n\u2022 GPT-4o, GPT-4V (OpenAI, 2023) are most advanced VLMs currently available.\n\u2022 Qwen-VL, Qwen-VL-Max (Bai et al., 2023) are large-scale visual language models designed to perceive and understand text and images. They have demonstrated significant performance in tasks such as image captioning, question answering, visual or document visual question answering, and localization, and have been applied as the base model for multiple mobile AI agents.\n\u2022 Auto-UI (Zhan and Zhang, 2023) is derived from AITW (Rawles et al., 2023), which uses a chain-of-action composed of a series of action histories and future action plans to improve the agent's action prediction ability.\n\u2022 MobileVLM (Wu et al., 2024) builds a large-scale Mobile3M dataset in the graph structure and uses multiple pre-training tasks to enhance the agent's UI understanding ability.\n\u2022 MobileAgent (Ding, 2024) builds a GUI agent based on prompt engineering based on GPT-4V.\n\u2022 CoCo-LLaVA (Ma et al., 2024) leverages comprehensive environment perception (CEP) and conditional action prediction (CAP) to enhance the agent\u2019s understanding of GUI pages and tasks.\n\u2022 CogAgent (Hong et al., 2023) is built on CogVLM, adding pre-training tasks and supporting higher-resolution images.\nFor in-context learning, we provided them with several few-shot examples. For SFT learning, we use the page navigation split of the MobileReach dataset to fine-tune them for two epochs. We maintain consistent hyperparameters across all the baselines for fair comparisons."}, {"title": "A.2 Metric", "content": "We use two objective metrics to evaluate the model at the step-level:\nIOU Acc: Intersection over Union (Cheng et al., 2021) evaluates if the bounding box of the generated action intersects with the golden action. We allow a 14% margin of error relative to the screen size.\nText Acc: It evaluates if the text in the generated action matches the golden one. This includes the name of the click action, the direction of the scroll action, the name and input text of the input action, and the text of the complete action. For the input text, which may have varied descriptions, we require an F1 value greater than 0.8 to be considered consistent. The remaining text must be entirely consistent.\nWe use two metrics to evaluate the model at the task-level:\nTask-level Acc: As described in Section 5.2, task-level accuracy evaluates whether all actions in an entire GUI flow are correct. That is, all actions in the reasoning process are exactly the same as all actions in the ground truth. This is a very strict method for judging task success and has been applied in previous works.\nTask Success Rate: This metric considers a GUI flow that completes all subtasks as correct. In our reward scoring principle, if the model can complete all subtasks, it does not need to be exactly the same as the ground truth to be considered complete. However, since the Auto-UI dataset does not contain explored paths other than the golden answer, we only use this evaluation metric in the Mobile3M dataset."}, {"title": "A.3 HyperParameter", "content": "We present the hyperparameters for the first stage SFT framework and second stage RL framework in Table 6. ReachAgent chose MobileVLM as the backbone model. We use 8 80GB Nvidia A100 GPUs for fine-tuning. The learning rate is 1e-5, and the agent\u2019s max length is 4,096.\nFor the MobileReach dataset, in the first stage, the model was trained for 2 epochs on all three splits with a batch size of 4. In the second stage, it was trained for 2 epochs on 259,742 preference data with a batch size of 1. During testing, the max step is set to 15.\nFor the Auto-UI dataset, we fine-tuned the MobileReach-stagel on its training split for 1"}, {"title": "B Prompt", "content": "For the page navigation task, we use the few-shot prompt to guide the GPT-4 in building tasks, as shown in Figure 6. The inputs are image captions generated by Intern-VL and actions between GUI pages. The outputs are step-by-step description and brief task.\nFor the page reaching task, we use the following pre-defined task templates to build tasks. We replace the {text} in the template with page names."}, {"title": "C Page Reaching and Operation Dataset Construction", "content": "C.1 Subflow Filtering\nFigure 7 shows the page reaching subtasks and page operation subtasks extracted from the GUI flow example in Figure 3.\nHere, we provide a step-by-step data construction process from GUI flow to task and subtask generation. Take the case in Figure 3 and Figure 7 as an example:\n1. First, we will observe such a GUI flow:\n2. Then, we need to check the validity of this flow, generate corresponding tasks, and filter.\nCheck the validity\n\u2022 Whether this path of P1, P2, ..., P9 has already appeared in the current training set.\n\u2022 Whether all actions in the action sequence have already appeared in the current training set.\n\u2022 Whether the action sequence contains consecutive repeated actions.\n\u2022 Whether there is an action ar that is not included in the action space of Pr\nGenerate Corresponding Tasks. We use InternVL to generate image captions for each GUI page in the flow and use GPT-4V to generate a step-by-step description and a brief task as follows:\nStep-by-step Description:\n1. On the homepage of Xiaomi Mall, click the search icon to enter the search page.\n2. On the search page, enter \u201cxiaomi 14\u201d in the search box to search.\n3. On the search page, click the search icon to search.\n4. On the search results page, select the detailed information of \u201cxiaomi 14\u201d.\n5. On the detailed information page, select and click \u201cAdd to Cart\u201d to enter the parameter page of the phone.\n6. On the parameter page, scroll the page to view more parameter information."}, {"title": "C.2 Subtask Generation", "content": "After obtaining the GUI flow", "example": "nDescription: 4. On the search results page", "xiaomi 14": "nTask: Visit the search result page.\nGUI Flow: P1 -> a1 -> P2 -> a2 -> P3 -> a3 -> P4\n2. If the element name of a click action does not appear in the existing dataset"}, {"example": "nAction: a4", "425,1074": [628, 1125], "nTask": "Help me navigate to \u201cXiaomi 14\u201d interface.\nGUI Flow: P1 -> a1 -> P2 -> a2 -> P3 -> a3 -> P4 -> a4 -> P5\nPage Operation Subtask Generation\n1. If the action type is scroll or input", "action.\nAction": "a6, scroll([0,585", "720,1088": ""}]}