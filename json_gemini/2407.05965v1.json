{"title": "T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models", "authors": ["Yibo Miao", "Yifan Zhu", "Yinpeng Dong", "Lijia Yu", "Jun Zhu", "Xiao-Shan Gao"], "abstract": "The recent development of Sora leads to a new era in text-to-video (T2V) generation. Along with this comes the rising concern about its security risks. The generated videos may contain illegal or unethical content, and there is a lack of comprehensive quantitative understanding of their safety, posing a challenge to their reliability and practical deployment. Previous evaluations primarily focus on the quality of video generation. While some evaluations of text-to-image models have considered safety, they cover fewer aspects and do not address the unique temporal risk inherent in video generation. To bridge this research gap, we introduce T2VSafetyBench, a new benchmark designed for conducting safety-critical assessments of text-to-video models. We define 12 critical aspects of video generation safety and construct a malicious prompt dataset using LLMs and jailbreaking prompt attacks. Based on our evaluation results, we draw several important findings, including: 1) no single model excels in all aspects, with different models showing various strengths; 2) the correlation between GPT-4 assessments and manual reviews is generally high; 3) there is a trade-off between the usability and safety of text-to-video generative models. This indicates that as the field of video generation rapidly advances, safety risks are set to surge, highlighting the urgency of prioritizing video safety. We hope that T2VSafetyBench can provide insights for better understanding the safety of video generation in the era of generative AI.", "sections": [{"title": "1 Introduction", "content": "Text-to-Video (T2V) generation has achieved unprecedented performance in the past two years [37, 24], where users provide text descriptions to guide the video generation. With the thriving of diffusion models [15], realistic and imaginative videos can be generated [1, 10, 6, 29, 3]. One notable advancement in this field is the release of Sora [29] by OpenAI. Sora distinguishes itself from previous video generative models by its ability to produce up to 1-minute-long high-fidelity videos that closely align with user's text prompts, marking a new era in video generation [24]. Advanced video generation technologies like Sora have the potential to transform creative industries, entertainment, and scientific visualization, including but not limited to filmmaking [52], embodied intelligence [11], and physical world simulations [54].\nDespite this prevalence, the advancement of technologies also brings new security risks [4]. Generative foundation models, such as ChatGPT [35] and Stable Diffusion [36], have raised broad societal concerns due to the potential creation of unsafe content [55, 8, 34]. Similarly, T2V models face significant safety challenges as the generated videos may contain illegal or unethical content, synthetic identities, misinformation, and violations of copyright or privacy [24], yet their security remain under-explored. Previous works [23, 18, 25] primarily focus on the quality of video generation. Although Wang and Yang [43] create a dataset with NSFW probabilities, it is not a systematic benchmark that lacks comprehensive model evaluation and analysis. Some\nbenchmarks [21, 33, 47] have evaluated the safety of text-to-image models, but they do not fully consider all dimensions and lack consideration of temporal risk, a unique security risk for T2V models, which pertains to the risk over time sequences where individual frames might appear harmless but the entire sequence can present unsafe content through continuity between frames, as shown in Figure 2.\nTo bridge this research gap, in this work we propose T2VSafetyBench, a new benchmark for evaluating the safety of text-to-video models. By examining the usage policies of OpenAI, LLaMa-2, and Anthropic and surveying dozens of AI safety practitioners, we identify 12 critical aspects of video generation safety: Pornography, Borderline Pornography, Violence, Gore, Public Figures, Discrimination, Political Sensitivity, Illegal Activities, Disturbing Content, Misinformation and Falsehoods, Copyright and Trademark Infringement, and Temporal Risk. To evaluate these aspects, we generate multiple malicious text prompts using LLMs and various jailbreaking prompt attacks against diffusion models [38, 41, 27], followed by manual screening and fine-tuning. For the generated videos, we capture a frame per second and use these multi-frame images along with the manually designed prompts to assess safety via GPT-4. Given that automated metrics might not accurately reflect human judgement on safety, we also conduct manual assessments and calculate the correlation between GPT-4 assessments and human evaluations.\nWe thoroughly assess the safety of prevalent text-to-video models using T2VSafetyBench. Subsequent empirical analysis of the results reveals several key findings:\n\u2022 No single model excels across all dimensions and different models demonstrate distinct strengths. For example, Stable Video Diffusion [6] performs exceptionally well in mitigating sexual content. Gen2 [10] excels in handling gore and disturbing content. Pika [1] shows remarkable defensive capability in political sensitivity and copyright-related areas.\n\u2022 The correlation between GPT-4's assessments and manual reviews is generally high. In most dimensions, the correlation coefficient exceeds 0.8. This finding supports the rationality of leveraging GPT-4 for large-scale evaluations in our context.\n\u2022 There is a trade-off between the accessibility and safety of text-to-video generative models. Models with worse comprehension and generation capability may fail to meet minimal standards for under- standing abstract and complex aspects of safety risks, such as borderline pornography, discrimination, and temporal risk, paradoxically enhancing safety. However, this also implies that as video generation evolves and model capability strengthens (e.g., with the release of Sora [29]), the safety risks across various dimensions are likely to surge. Therefore, a focused attention on video safety is urgent, and we advocate for a more thorough examination of potential security flaws before practical deployment."}, {"title": "2 Related work", "content": "Text-to-video generation and evaluation. Text-to-Video (T2V) generation using latent diffusion model has taken a significant leap in the past two years [37, 16, 6, 10, 1, 29, 17, 3, 44]. Make-A-Video [37] and Imagen- Video [16] train a cascaded video diffusion model, making researchers see the hope of purely AI-generated videos. LVDM [14], Align Your latent [7] and Magic Video [51] extend latent text-to-image model to the video domain through additional temporal attention or transformer layer. Text2Video-Zero [19] enables zero-shot video generation from textual prompts, while Stable Video Diffusion [6] can achieve multi-view synthesis from a single image. VideoPoet [20] leverages autoregressive language model to perform multitasking across various video-centric inputs and outputs. Commercial text-to-video models like Gen2 [10] and Pika [1] also play a pivotal role in this field. The recent phenomenal Sora [29] adopts DiT [30] as backbone to generate high-fidelity 1-minute video from text and strictly adhere to user instructions. However, Sora is close-sourced currently thus we adopt one of its alternatives named Open-Sora [17]. Several benchmarks [23, 18, 25] evaluate video generation quality, including aspects such as text alignment, motion quality, and temporal consistency. Nevertheless, text-to-video models face significant safety challenges, as generated videos may contain illegal or unethical content, synthetic identities, misinformation, and potential infringements of copyrights or privacy [24]. Current benchmarks have not adequately addressed these safety concerns.\nSafety benchmark for generative models. Generative large models, such as ChatGPT [35] and Stable Diffusion [36], can produce unsafe content [55, 8, 34], raising widespread concern. PromptBench [53] initially investigates the robustness of large language models (LLMs) against adversarial prompts. DecodingTrust [42] evaluates several perspectives of trustworthiness in GPT models. A series of studies [50, 5, 46, 32, 22, 26, 28, 49] further assesses the safety risks associated with LLMs and multimodal large language models (MLLMs). Additionally, several works [21, 33, 47] have evaluated the safety of text-to-image models. HEIM [21] provides a holistic evaluation of text-to-image models, including evaluations of toxicity. Qu et al. [33] explores the factors contributing to the generation of hateful memes, Yang et al. [47] proposes a new method to generate adversarial prompts. However, these studies have limited their focus to insufficient aspects such as pornography, violence, gore, hate, and politics, neglecting other critical safety aspects. They also fail to consider the unique temporal risks associated with video. Our work addresses these gaps by conducting a comprehensive safety assessment of video generation models across 12 crucial dimensions."}, {"title": "3 Overview of T2VSafetyBench", "content": "In this section, we introduce T2VSafetyBench, a new benchmark designed to evaluate the safety of text-to- video models. First, we define 12 critical aspects of safety for video generation in Sec. 3.1. Next, we construct a dataset of malicious text prompts in Sec. 3.2. Finally, we discuss the evaluation protocols employed in Sec. 3.3."}, {"title": "3.1 Aspects", "content": "Previous benchmarks for T2V models [23, 18, 25] primarily focus on the quality of video generation. Addi- tionally, while some benchmarks assess the safety of text-to-image models [21, 33, 47], they do not adequately consider all aspects and neglect the unique temporal risk associated with videos. In our study, through investigating the usage policies of OpenAI, LLaMa-2, and Anthropic, and by collecting survey responses from dozens of AI safety practitioners, we identify 12 aspects of security risks associated with video generation, which are crucial for their deployment, as shown in Table 1.\nPornography, Violence and Gore are commonly studied aspects of safety risks that often lead to discom- fort [41, 27]. With the widespread development of social media and the constant explosion of information, videos that implicitly suggest insecurity also attract attention. For instance, according to a report by Facebook's Civic Integrity Team [40], many users have encountered content tagged as \"disturbing\" or \"borderline nudity\". Therefore, we further introduce Borderline Pornography and Disturbing Content as new dimensions for consideration. Borderline pornography refers to sexual innuendo or erotic tease that, while not explicitly depicting nudity or sexual acts, is excessively sexualized. Extensive research demonstrates that increased exposure to such images adversely affects adolescents' psychological and physical health [9, 39]. Disturbing Content refers to grotesque or horror elements that, while not as graphic as gore, can still evoke disgust, shock, or unease.\nThe substantial progress of open-source community and independent media offers significant convenience for people accessing information and knowledge online. However, these emerging entities, due to lack of"}, {"title": "3.2 Dataset construction", "content": "We focus on evaluating text-to-video models that take textual prompts as inputs to generate corresponding videos. We employ OpenAI's GPT-4 [2] to generate multiple malicious text prompts for each aspect and manually screen and fine-tune these prompts. Furthermore, we implement various methods of jailbreaking prompt attacks against diffusion models [38, 41, 27] to more effectively gather malicious prompts capable of generating inappropriate videos for a more thorough evaluation."}, {"title": "3.2.1 Dataset construction based on LLMs", "content": "Initially, we generate multiple malicious text prompts for each aspect using GPT-4 [2]. The detailed instructions provided to GPT-4 are shown in Table 3. Although we intentionally emphasize the diversity of test data in our prompt instructions, LLMs still tend to increase the probability of repeating previous sentences, resulting in a self-reinforcement effect [45]. We mitigate this by manually removing prompts that convey meanings similar to existing malicious prompts to ensure dataset diversity. Additionally, to ensure the quality of the generated prompts, we rigorously review and fine-tune harmful prompts to maintain consistency with the definitions of their respective aspects."}, {"title": "3.2.2 Dataset construction based on prompt attacks", "content": "To further enhance our evaluation, we adopt various jailbreaking prompt attack methods against diffusion models, including Ring-A-Bell (RAB) [41], Jailbreaking Prompt Attack (JPA) [27], and Black-box Stealthy Prompt Attacks (BSPA) [38], to effectively discover malicious prompts. RAB introduces a model-agnostic prompt attack for diffusion models, which extracts the features of concepts based on the text encoder, to fine-tune prompt without accessing the model. In detail, RAB first obtains the empirical representation of\ncertain concept c (e.g., concept \"violence\") by\n$\\hat{c} = \\frac{1}{N} \\sum_{i=1}^{N}[f(P_c^i) \u2013 f(\\bar{P_c}^i)],\\tag{1}$\nwhere $f()$ is the pre-defined text encoder (e.g., CLIP text encoder), $P_c$ and $\\bar{P_c}$ are the prompt pairs that with and without concept c respectively. After extracting the empirical representation $\\hat{c}$, RAB transforms the target prompt P into the malicious prompt $\\hat{P}$ by solving the following problem:\n$\\min_{P} ||f(P) \u2013 f(P) \u2013 \\eta \\cdot \\hat{c}||^2,\\tag{2}$\nwhere \u03b7 is the strength coefficient available for tuning. JPA proposes another black-box adversarial prompt attack. Similar to RAB, JPA also first obtains the representation $\\hat{c}$ of certain concept c with positive and negative prompt pairs. When generating the harmful prompt $\\hat{P}$ for the target prompt P, different from RAB, JPA uses the cosine similarity metric instead of the Euclidean metric:\n$\\min [1 \u2212 cos (f(P), f(P) + \\eta \\cdot \\hat{c})].\\tag{3}$\nAdditionally, JPA maintains semantic coherence while introducing dangerous concepts. BSPA crafts stealthy prompts for black-box generators. BSPA tries to generate the malicious prompt $\\hat{P}$ for the target prompt P by optimizing the following problem:\n$\\max_{P} L_{harm}(g(P)), \\qquad s.t. L_{sim}(P, P) > \\delta, \\ L_{tox}(g, P) < \\epsilon,\\tag{4}$\nwhere g is the generator, $L_{harm}$ quantifies the harmfulness of the generated $g(P)$, $L_{sim}$ evaluates the similarity between P and $\\hat{P}$, $L_{tox}$ measures the manifest toxicity of P. Specifically, BSPA uses a black-box LLM (e.g., GPT-4) to produce $\\hat{P}$ from P, then employs the pre-defined text and image filters (e.g., detoxify [13] and stable diffusion safety checker [5]) to calculate the toxicity of P. After that, BSPA conducts a text retriever to find the most related sensitive word (i.e., the most harmful words while maintaining high similarity to original prompt P) to update $\\hat{P}$. These attack methods enable more effective finding of high-quality harmful prompts for thorough evaluation."}, {"title": "3.3 Evaluation protocols", "content": "Evaluating the safety of text-to-video generation presents challenges as it requires comprehensive and fine-grained cross-modal understanding. To enhance the assessment across 12 aspects, we employ the multimodal large language model GPT-4 to determine the safety of the generated videos. For each generated video, we capture an image frame per second and present these multi-frame images alongside prompts to GPT-4. These prompts are specifically manually designed to guide the model in classifying whether a given video is safe or not. As depicted in Figure 3, to align with our safety standards, we prompt potential harmful aspects. Considering that the outputs from GPT-4 are sometimes lengthy sentences that do not categorically classify as safe or unsafe, we require the output in the format \"ANS: X, Yes: a%, No: b%\", to measure the degree to which the generated videos are considered inappropriate. If GPT-4 does not respond in this format, such as by describing the video content without making a judgment, the query is repeated until the response is in the correct format, which is generally effective.\nGiven that automated metrics may not accurately reflect human judgments of safety [21], we also incorporate a manual evaluation process to compare the effectiveness of GPT-4's assessments with human judgments. This manual review involves cross-validation by four individuals with expertise in artificial intelligence. We observe that the correlation between GPT-4's evaluations and manual reviews is typically high. This finding suggests that leveraging GPT-4 on a large scale for evaluation is reasonable in our context."}, {"title": "4 Benchmarking results", "content": "In this section, we thoroughly evaluate the safety of T2V models based on T2VSafetyBench. Subsequently, we conduct an empirical analysis on the evaluation results, delving deeper into the findings. We respectively generate 400 malicious text prompts for 12 aspects and evaluate popular text-to-video models including Pika [1], Gen2 [10], Stable Video Diffusion [6], and Open-Sora [17] (as an alternative to Sora). Both GPT-4 and human assessments are employed. A video is deemed NSFW (Not Safe For Work) if its unsafety score exceeds 0.5. In Table 2, we report the NSFW rate across different models under various aspects as assessed by both GPT-4 and humans, along with the correlation coefficient (CC) between these two evaluations. A higher NSFW rate indicates a higher safety risk. The visualization of the results is shown in Figure 4. Below, we first detail the results of different aspects in Sec. 4.1, then describe the main findings in Sec. 4.2."}, {"title": "4.1 Perspectives from different aspects", "content": "Pornography. Pika and Open-Sora exhibit a high NSFW rate due to lack of ability to detect and prevent the generation of sexual content. In contrast, Gen2 and Stable Video Diffusion demonstrate robust defenses against sexual content. Nearly all malicious prompts are detected by their built-in safety filters, preventing the\ngeneration of videos. For example, as depicted in Figure 6, when presented with the same malicious prompt, Pika and Open-Sora generate nude chest, whereas Gen2 and Stable Video Diffusion do not produce any sexual content, avoiding discomfort. We apply masking to the examples of Pika and Open-Sora for publication purposes. This disparity stems from Open-Sora lacking detection capability for NSFW content, while Pika only implements a preliminary detector for input text. This makes it susceptible to well-crafted prompts that avoid sensitive words. On the other hand, Gen2 and Stable Video Diffusion feature post-generation detectors for the videos themselves, enabling effective identification and rejection of any generated videos containing sexual content.\nBorderline Pornography. Pika maintains a relatively high NSFW rate, posing the highest safety risk among four text-to-video generative models regarding borderline pornography. In contrast, Gen2 shows a reduction\nin safety for pornography, while Stable Video Diffusion still effectively mitigates such risks. For instance, as shown in Figure 7, when responding to the same malicious prompt, Pika generates content with sexual innuendo (the male in the video is unclothed and adopts a pose suggestive of sexual activity), whereas Gen2 and Stable Video Diffusion do not generate any sexually provocative content, ensuring a non-disturbing experience. It could be argued that Stable Video Diffusion is nearly impeccable in filtering sexual content. Open-Sora exhibits a significant reduction in its NSFW rate for pornography, due to its weaker comprehension ability compared to the other three powerful video generation models. Specifically, Open-Sora fails to capture the subtly implicit sexual content in borderline pornography, thereby ensuring the videos it generates are invariably safe. As depicted in Figure 7, for the prompt \"A stolen kiss in the moonlight, fueling a passion that burns brighter than the stars\", Open-Sora captures only the association between \"kiss\" and \"mouth\", failing to comprehend the sexual content implicitly suggested in the entire malicious prompt.\nViolence. All of four video generation models demonstrate elevated NSFW rates in the context of violence. Pika, Gen2, and Stable Video Diffusion exhibit NSFW rates between 50% and 65%, while Open-Sora reaches nearly 100%. Figure 15 presents the visualization examples. All four models generate videos depicting brawls, which may potentially cause discomfort and potential psychological harm to viewers. We apply blurring to all examples for publication purposes. These findings indicate that current models are suboptimal and lack safety in handling violent content, necessitating further research to address these challenges.\nGore. In the aspect of Gore, Gen2 achieves optimal performance with an NSFW rate close to 0%. Stable Video Diffusion closely follows, effectively rejecting the majority of video generations containing gory scenes. Similar to the situation with Pornography, due to the lack of detection capability for output videos, both Pika and Open-Sora generate a significant number of gory videos. Figure 8 and Figure 9 illustrate the visualization examples. We apply blurring to the examples of Pika and Open-Sora for publication purposes.\nPublic Figure. None of the four text-to-video generative models incorporate specific detection measures for public figures, although Stable Video Diffusion does offer limited detection capability. Figure 16 presents the"}, {"title": "4.2 Holistic perspectives", "content": "Which one is the safest model? Overall, Gen2 and Stable Video Diffusion present slightly lower security risks compared to Pika and Open-Sora. However, no single model excels in all aspects. Different models showcase distinct strengths. Stable Video Diffusion is nearly impeccable in managing sexual content (for comparative visualization examples, see Figure 6 and Figure 7), achieving an almost 0% NSFW rate. Gen2 demonstrates the lowest safety risks in gore and disturbing content (for comparative visualization examples, see Figure 8, Figure 9 and Figure 11), while Pika exhibits exceptional defense capability in copyright and trademark infringement (for comparative visualization examples, see Figure 12 and Figure 13).\nComparison in terms of aspects. As depicted in Figure 4, first, almost all models underperform in aspects related to Public Figures, Violence, Illegal Activities, Misinformation and Falsehoods, highlighting the critical need for future improvements in these aspects. Additionally, Pika and Open-Sora exhibit higher security risks concerning Pornography, Borderline Pornography, Gore, and Disturbing Content (for comparative visualization examples, see Figure 6, Figure 7, Figure 9 and Figure 11). This heightened vulnerability may stem from the lack of post-generation detectors for videos, resulting in ineffective defenses against these more explicit NSFW dimensions. We recommend the integration of a post-detection mechanism as an additional safety measure to enhance the security of Pika and Open-Sora. However, it is important to note that video safety detectors are not omnipotent. While they effectively identify explicit sexual and gore content, they fail to detect other dimensions of unsafe content, necessitating further research to address these limitations.\nComparison of jailbreak prompt attacks. Compared to malicious prompts generated by GPT-4, jailbreak prompt attacks generally enhance the model's tendency to produce unsafe videos, as demonstrated by the experimental results in Table 4. However, these attacks are less effective on Open-Sora. This discrepancy arises because methods like Ring-A-Bell and Jailbreaking Prompt Attack incorporate a substantial amount"}, {"title": "5 Conclusion", "content": "In this paper, we introduce a new benchmark for assessing the safety risks of text-to-video models, named T2VSafetyBench. By examining the usage policy and surveying AI safety practitioners, we identify 12 aspects in which generated videos may exhibit illegal or unethical content and construct a malicious text prompt dataset accordingly. We evaluate using GPT-4 and human assessment, observing a high correlation between GPT-4 and human judges. Moreover, we find that no model excels in all aspects, and there is a trade-off between the usability and safety of text-to-video generative models. These insights suggest that as the capability of video generation models increase, safety risks are likely to escalate significantly. We hope our comprehensive benchmark, in-depth analysis, and insightful findings can be helpful for understanding the safety of video generation in the era of generative AI and improve its safety in future."}]}