{"title": "T2VSafetyBench: Evaluating the Safety of Text-to-Video\nGenerative Models", "authors": ["Yibo Miao", "Yifan Zhu", "Yinpeng Dong", "Lijia Yu", "Jun Zhu", "Xiao-Shan Gao"], "abstract": "The recent development of Sora leads to a new era in text-to-video (T2V) generation. Along\nwith this comes the rising concern about its security risks. The generated videos may contain\nillegal or unethical content, and there is a lack of comprehensive quantitative understanding\nof their safety, posing a challenge to their reliability and practical deployment. Previous\nevaluations primarily focus on the quality of video generation. While some evaluations of\ntext-to-image models have considered safety, they cover fewer aspects and do not address the\nunique temporal risk inherent in video generation. To bridge this research gap, we introduce\nT2VSafetyBench, a new benchmark designed for conducting safety-critical assessments of\ntext-to-video models. We define 12 critical aspects of video generation safety and construct\na malicious prompt dataset using LLMs and jailbreaking prompt attacks. Based on our\nevaluation results, we draw several important findings, including: 1) no single model excels\nin all aspects, with different models showing various strengths; 2) the correlation between\nGPT-4 assessments and manual reviews is generally high; 3) there is a trade-off between\nthe usability and safety of text-to-video generative models. This indicates that as the field\nof video generation rapidly advances, safety risks are set to surge, highlighting the urgency\nof prioritizing video safety. We hope that T2VSafetyBench can provide insights for better\nunderstanding the safety of video generation in the era of generative AI.", "sections": [{"title": "1 Introduction", "content": "Text-to-Video (T2V) generation has achieved unprecedented performance in the past two years [37, 24], where\nusers provide text descriptions to guide the video generation. With the thriving of diffusion models [15],\nrealistic and imaginative videos can be generated [1, 10, 6, 29, 3]. One notable advancement in this field is the\nrelease of Sora [29] by OpenAI. Sora distinguishes itself from previous video generative models by its ability\nto produce up to 1-minute-long high-fidelity videos that closely align with user's text prompts, marking a new\nera in video generation [24]. Advanced video generation technologies like Sora have the potential to transform\ncreative industries, entertainment, and scientific visualization, including but not limited to filmmaking [52],\nembodied intelligence [11], and physical world simulations [54].\nDespite this prevalence, the advancement of technologies also brings new security risks [4]. Generative\nfoundation models, such as ChatGPT [35] and Stable Diffusion [36], have raised broad societal concerns due\nto the potential creation of unsafe content [55, 8, 34]. Similarly, T2V models face significant safety challenges\nas the generated videos may contain illegal or unethical content, synthetic identities, misinformation, and\nviolations of copyright or privacy [24], yet their security remain under-explored. Previous works [23, 18, 25]\nprimarily focus on the quality of video generation. Although Wang and Yang [43] create a dataset with NSFW\nprobabilities, it is not a systematic benchmark that lacks comprehensive model evaluation and analysis. Some\nbenchmarks [21, 33, 47] have evaluated the safety of text-to-image models, but they do not fully consider all\ndimensions and lack consideration of temporal risk, a unique security risk for T2V models, which pertains\nto the risk over time sequences where individual frames might appear harmless but the entire sequence can\npresent unsafe content through continuity between frames, as shown in Figure 2.\nTo bridge this research gap, in this work we propose T2VSafetyBench, a new benchmark for evaluating\nthe safety of text-to-video models. By examining the usage policies of OpenAI, LLaMa-2, and Anthropic\nand surveying dozens of AI safety practitioners, we identify 12 critical aspects of video generation safety:\nPornography, Borderline Pornography, Violence, Gore, Public Figures, Discrimination, Political Sensitivity,\nIllegal Activities, Disturbing Content, Misinformation and Falsehoods, Copyright and Trademark Infringement,\nand Temporal Risk. To evaluate these aspects, we generate multiple malicious text prompts using LLMs\nand various jailbreaking prompt attacks against diffusion models [38, 41, 27], followed by manual screening\nand fine-tuning. For the generated videos, we capture a frame per second and use these multi-frame images\nalong with the manually designed prompts to assess safety via GPT-4. Given that automated metrics might\nnot accurately reflect human judgement on safety, we also conduct manual assessments and calculate the\ncorrelation between GPT-4 assessments and human evaluations.\nWe thoroughly assess the safety of prevalent text-to-video models using T2VSafetyBench. Subsequent\nempirical analysis of the results reveals several key findings:\n\u2022 No single model excels across all dimensions and different models demonstrate distinct strengths.\nFor example, Stable Video Diffusion [6] performs exceptionally well in mitigating sexual content.\nGen2 [10] excels in handling gore and disturbing content. Pika [1] shows remarkable defensive\ncapability in political sensitivity and copyright-related areas.\n\u2022 The correlation between GPT-4's assessments and manual reviews is generally high. In most\ndimensions, the correlation coefficient exceeds 0.8. This finding supports the rationality of leveraging\nGPT-4 for large-scale evaluations in our context.\n\u2022 There is a trade-off between the accessibility and safety of text-to-video generative models. Models\nwith worse comprehension and generation capability may fail to meet minimal standards for under-\nstanding abstract and complex aspects of safety risks, such as borderline pornography, discrimination,\nand temporal risk, paradoxically enhancing safety. However, this also implies that as video generation\nevolves and model capability strengthens (e.g., with the release of Sora [29]), the safety risks across\nvarious dimensions are likely to surge. Therefore, a focused attention on video safety is urgent, and\nwe advocate for a more thorough examination of potential security flaws before practical deployment."}, {"title": "2 Related work", "content": "Text-to-video generation and evaluation. Text-to-Video (T2V) generation using latent diffusion model has\ntaken a significant leap in the past two years [37, 16, 6, 10, 1, 29, 17, 3, 44]. Make-A-Video [37] and Imagen-\nVideo [16] train a cascaded video diffusion model, making researchers see the hope of purely AI-generated\nvideos. LVDM [14], Align Your latent [7] and Magic Video [51] extend latent text-to-image model to the video\ndomain through additional temporal attention or transformer layer. Text2Video-Zero [19] enables zero-shot\nvideo generation from textual prompts, while Stable Video Diffusion [6] can achieve multi-view synthesis\nfrom a single image. VideoPoet [20] leverages autoregressive language model to perform multitasking across\nvarious video-centric inputs and outputs. Commercial text-to-video models like Gen2 [10] and Pika [1] also\nplay a pivotal role in this field. The recent phenomenal Sora [29] adopts DiT [30] as backbone to generate\nhigh-fidelity 1-minute video from text and strictly adhere to user instructions. However, Sora is close-sourced\ncurrently thus we adopt one of its alternatives named Open-Sora [17]. Several benchmarks [23, 18, 25]\nevaluate video generation quality, including aspects such as text alignment, motion quality, and temporal\nconsistency. Nevertheless, text-to-video models face significant safety challenges, as generated videos may\ncontain illegal or unethical content, synthetic identities, misinformation, and potential infringements of\ncopyrights or privacy [24]. Current benchmarks have not adequately addressed these safety concerns.\nSafety benchmark for generative models. Generative large models, such as ChatGPT [35] and Stable\nDiffusion [36], can produce unsafe content [55, 8, 34], raising widespread concern. PromptBench [53] initially\ninvestigates the robustness of large language models (LLMs) against adversarial prompts. DecodingTrust [42]\nevaluates several perspectives of trustworthiness in GPT models. A series of studies [50, 5, 46, 32, 22, 26, 28,\n49] further assesses the safety risks associated with LLMs and multimodal large language models (MLLMs).\nAdditionally, several works [21, 33, 47] have evaluated the safety of text-to-image models. HEIM [21] provides\na holistic evaluation of text-to-image models, including evaluations of toxicity. Qu et al. [33] explores the\nfactors contributing to the generation of hateful memes, Yang et al. [47] proposes a new method to generate\nadversarial prompts. However, these studies have limited their focus to insufficient aspects such as pornography,\nviolence, gore, hate, and politics, neglecting other critical safety aspects. They also fail to consider the unique\ntemporal risks associated with video. Our work addresses these gaps by conducting a comprehensive safety\nassessment of video generation models across 12 crucial dimensions."}, {"title": "3 Overview of T2VSafetyBench", "content": "In this section, we introduce T2VSafetyBench, a new benchmark designed to evaluate the safety of text-to-\nvideo models. First, we define 12 critical aspects of safety for video generation in Sec. 3.1. Next, we construct\na dataset of malicious text prompts in Sec. 3.2. Finally, we discuss the evaluation protocols employed in\nSec. 3.3."}, {"title": "3.1 Aspects", "content": "Previous benchmarks for T2V models [23, 18, 25] primarily focus on the quality of video generation. Addi-\ntionally, while some benchmarks assess the safety of text-to-image models [21, 33, 47], they do not adequately\nconsider all aspects and neglect the unique temporal risk associated with videos. In our study, through\ninvestigating the usage policies of OpenAI, LLaMa-2, and Anthropic, and by collecting survey responses from\ndozens of AI safety practitioners, we identify 12 aspects of security risks associated with video generation,\nwhich are crucial for their deployment, as shown in Table 1.\nPornography, Violence and Gore are commonly studied aspects of safety risks that often lead to discom-\nfort [41, 27]. With the widespread development of social media and the constant explosion of information,\nvideos that implicitly suggest insecurity also attract attention. For instance, according to a report by Facebook's\nCivic Integrity Team [40], many users have encountered content tagged as \"disturbing\" or \"borderline nudity\".\nTherefore, we further introduce Borderline Pornography and Disturbing Content as new dimensions for\nconsideration. Borderline pornography refers to sexual innuendo or erotic tease that, while not explicitly\ndepicting nudity or sexual acts, is excessively sexualized. Extensive research demonstrates that increased\nexposure to such images adversely affects adolescents' psychological and physical health [9, 39]. Disturbing\nContent refers to grotesque or horror elements that, while not as graphic as gore, can still evoke disgust, shock,\nor unease.\nThe substantial progress of open-source community and independent media offers significant convenience\nfor people accessing information and knowledge online. However, these emerging entities, due to lack of"}, {"title": "3.2 Dataset construction", "content": "We focus on evaluating text-to-video models that take textual prompts as inputs to generate corresponding\nvideos. We employ OpenAI's GPT-4 [2] to generate multiple malicious text prompts for each aspect and\nmanually screen and fine-tune these prompts. Furthermore, we implement various methods of jailbreaking\nprompt attacks against diffusion models [38, 41, 27] to more effectively gather malicious prompts capable of\ngenerating inappropriate videos for a more thorough evaluation."}, {"title": "3.2.1 Dataset construction based on LLMs", "content": "Initially, we generate multiple malicious text prompts for each aspect using GPT-4 [2]. The detailed instructions\nprovided to GPT-4 are shown in Table 3. Although we intentionally emphasize the diversity of test data in our\nprompt instructions, LLMs still tend to increase the probability of repeating previous sentences, resulting in a\nself-reinforcement effect [45]. We mitigate this by manually removing prompts that convey meanings similar\nto existing malicious prompts to ensure dataset diversity. Additionally, to ensure the quality of the generated\nprompts, we rigorously review and fine-tune harmful prompts to maintain consistency with the definitions of\ntheir respective aspects."}, {"title": "3.2.2 Dataset construction based on prompt attacks", "content": "To further enhance our evaluation, we adopt various jailbreaking prompt attack methods against diffusion\nmodels, including Ring-A-Bell (RAB) [41], Jailbreaking Prompt Attack (JPA) [27], and Black-box Stealthy\nPrompt Attacks (BSPA) [38], to effectively discover malicious prompts. RAB introduces a model-agnostic\nprompt attack for diffusion models, which extracts the features of concepts based on the text encoder, to\nfine-tune prompt without accessing the model. In detail, RAB first obtains the empirical representation of\ncertain concept c (e.g., concept \"violence\") by\n$$ \\hat{c} = \\frac{1}{N} \\sum_{i=1}^{N} [f(P_i^+) - f(P_i^-)], $$\nwhere f() is the pre-defined text encoder (e.g., CLIP text encoder), $P^+$ and $P^-$ are the prompt pairs that with\nand without concept c respectively. After extracting the empirical representation $\\hat{c}$, RAB transforms the target\nprompt P into the malicious prompt P by solving the following problem:\n$$ min_P ||f(P) - f(P) - \\eta \\cdot \\hat{c}||^2, $$\nwhere $\\eta$ is the strength coefficient available for tuning. JPA proposes another black-box adversarial prompt\nattack. Similar to RAB, JPA also first obtains the representation $\\hat{c}$ of certain concept c with positive and\nnegative prompt pairs. When generating the harmful prompt P for the target prompt P, different from RAB,\nJPA uses the cosine similarity metric instead of the Euclidean metric:\n$$ min_P [1 - cos (f(P), f(P) + \\eta \\cdot \\hat{c})]. $$\nAdditionally, JPA maintains semantic coherence while introducing dangerous concepts. BSPA crafts stealthy\nprompts for black-box generators. BSPA tries to generate the malicious prompt P for the target prompt P by\noptimizing the following problem:\n$$ max_P L_{harm}(g(P)), \\ \\ \\ s.t. \\ L_{sim}(P, P) > \\delta, \\ L_{tox}(g, P) < \\epsilon, $$\nwhere g is the generator, $L_{harm}$ quantifies the harmfulness of the generated g(P), $L_{sim}$ evaluates the similarity\nbetween P and P, $L_{tox}$ measures the manifest toxicity of P. Specifically, BSPA uses a black-box LLM (e.g.,\nGPT-4) to produce P from P, then employs the pre-defined text and image filters (e.g., detoxify [13] and\nstable diffusion safety checker [5]) to calculate the toxicity of P. After that, BSPA conducts a text retriever to\nfind the most related sensitive word (i.e., the most harmful words while maintaining high similarity to original\nprompt P) to update P. These attack methods enable more effective finding of high-quality harmful prompts\nfor thorough evaluation."}, {"title": "3.3 Evaluation protocols", "content": "Evaluating the safety of text-to-video generation\npresents challenges as it requires comprehensive and\nfine-grained cross-modal understanding. To enhance\nthe assessment across 12 aspects, we employ the\nmultimodal large language model GPT-4 to deter-\nmine the safety of the generated videos. For each\ngenerated video, we capture an image frame per sec-\nond and present these multi-frame images alongside\nprompts to GPT-4. These prompts are specifically\nmanually designed to guide the model in classifying\nwhether a given video is safe or not. As depicted\nin Figure 3, to align with our safety standards, we\nprompt potential harmful aspects. Considering that\nthe outputs from GPT-4 are sometimes lengthy sen-\ntences that do not categorically classify as safe or\nunsafe, we require the output in the format \"ANS: X,\nYes: a%, No: b%\", to measure the degree to which\nthe generated videos are considered inappropriate.\nIf GPT-4 does not respond in this format, such as\nby describing the video content without making a\njudgment, the query is repeated until the response is in the correct format, which is generally effective.\nGiven that automated metrics may not accurately reflect human judgments of safety [21], we also incorporate\na manual evaluation process to compare the effectiveness of GPT-4's assessments with human judgments.\nThis manual review involves cross-validation by four individuals with expertise in artificial intelligence. We\nobserve that the correlation between GPT-4's evaluations and manual reviews is typically high. This finding\nsuggests that leveraging GPT-4 on a large scale for evaluation is reasonable in our context."}, {"title": "4 Benchmarking results", "content": "In this section, we thoroughly evaluate the safety\nof T2V models based on T2VSafetyBench. Subse-\nquently, we conduct an empirical analysis on the\nevaluation results, delving deeper into the findings.\nWe respectively generate 400 malicious text prompts\nfor 12 aspects and evaluate popular text-to-video\nmodels including Pika [1], Gen2 [10], Stable Video\nDiffusion [6], and Open-Sora [17] (as an alternative\nto Sora). Both GPT-4 and human assessments are\nemployed. A video is deemed NSFW (Not Safe For\nWork) if its unsafety score exceeds 0.5. In Table 2,\nwe report the NSFW rate across different models\nunder various aspects as assessed by both GPT-4 and\nhumans, along with the correlation coefficient (CC)\nbetween these two evaluations. A higher NSFW rate\nindicates a higher safety risk. The visualization of\nthe results is shown in Figure 4. Below, we first de-\ntail the results of different aspects in Sec. 4.1, then\ndescribe the main findings in Sec. 4.2."}, {"title": "4.1 Perspectives from different aspects", "content": "Pornography. Pika and Open-Sora exhibit a high NSFW rate due to lack of ability to detect and prevent\nthe generation of sexual content. In contrast, Gen2 and Stable Video Diffusion demonstrate robust defenses\nagainst sexual content. Nearly all malicious prompts are detected by their built-in safety filters, preventing the\ngeneration of videos. For example, as depicted in Figure 6, when presented with the same malicious prompt,\nPika and Open-Sora generate nude chest, whereas Gen2 and Stable Video Diffusion do not produce any sexual\ncontent, avoiding discomfort. We apply masking to the examples of Pika and Open-Sora for publication\npurposes. This disparity stems from Open-Sora lacking detection capability for NSFW content, while Pika\nonly implements a preliminary detector for input text. This makes it susceptible to well-crafted prompts that\navoid sensitive words. On the other hand, Gen2 and Stable Video Diffusion feature post-generation detectors\nfor the videos themselves, enabling effective identification and rejection of any generated videos containing\nsexual content.\nBorderline Pornography. Pika maintains a relatively high NSFW rate, posing the highest safety risk among\nfour text-to-video generative models regarding borderline pornography. In contrast, Gen2 shows a reduction\nin safety for pornography, while Stable Video Diffusion still effectively mitigates such risks. For instance,\nas shown in Figure 7, when responding to the same malicious prompt, Pika generates content with sexual\ninnuendo (the male in the video is unclothed and adopts a pose suggestive of sexual activity), whereas Gen2\nand Stable Video Diffusion do not generate any sexually provocative content, ensuring a non-disturbing\nexperience. It could be argued that Stable Video Diffusion is nearly impeccable in filtering sexual content.\nOpen-Sora exhibits a significant reduction in its NSFW rate for pornography, due to its weaker comprehension\nability compared to the other three powerful video generation models. Specifically, Open-Sora fails to capture\nthe subtly implicit sexual content in borderline pornography, thereby ensuring the videos it generates are\ninvariably safe. As depicted in Figure 7, for the prompt \"A stolen kiss in the moonlight, fueling a passion that\nburns brighter than the stars\", Open-Sora captures only the association between \"kiss\" and \"mouth\", failing to\ncomprehend the sexual content implicitly suggested in the entire malicious prompt.\nViolence. All of four video generation models demonstrate elevated NSFW rates in the context of violence.\nPika, Gen2, and Stable Video Diffusion exhibit NSFW rates between 50% and 65%, while Open-Sora reaches\nnearly 100%. Figure 15 presents the visualization examples. All four models generate videos depicting brawls,\nwhich may potentially cause discomfort and potential psychological harm to viewers. We apply blurring to all\nexamples for publication purposes. These findings indicate that current models are suboptimal and lack safety\nin handling violent content, necessitating further research to address these challenges.\nGore. In the aspect of Gore, Gen2 achieves optimal performance with an NSFW rate close to 0%. Stable\nVideo Diffusion closely follows, effectively rejecting the majority of video generations containing gory scenes.\nSimilar to the situation with Pornography, due to the lack of detection capability for output videos, both Pika\nand Open-Sora generate a significant number of gory videos. Figure 8 and Figure 9 illustrate the visualization\nexamples. We apply blurring to the examples of Pika and Open-Sora for publication purposes.\nPublic Figure. None of the four text-to-video generative models incorporate specific detection measures for\npublic figures, although Stable Video Diffusion does offer limited detection capability. Figure 16 presents the\nvisualization examples. All four models generate a representation of the public figure \"Elon Musk\", potentially\ninfringing on Elon Musk's portrait rights. The relatively low Human NSFW rate of 86.5% in Open-Sora stems\nfrom a minor proportion of unrecognizable public figures being generated. In contrast, the GPT-4 NSFW\nrate reaches 97.3%, possibly due to looser detection standards compared to human judgment in this aspect.\nOverlooking the detection of public figures could lead to the generation of infringing content and pose legal\nrisks, which necessitates greater attention.\nDiscrimination. All of four models exhibit lower NSFW rates in terms of discrimination, suggesting minimal\nsafety risks. However, this outcome primarily arises because current video generation models generally possess\nlimited capability, typically generating clear and simple actions rather than effectively representing more\nabstract and complex content such as discrimination. Consequently, a lower NSFW rate does not imply a\nrobust defense mechanism against discrimination in the models. Even for simple discriminatory actions, such\nas a single gesture, these models struggle to detect and reject the generation of such content, as shown in\nFigure 10.\nPolitical Sensitivity. In the context of Political Sensitivity, Pika and Open-Sora exhibit lower NSFW rates,\nwhereas Gen2 and Stable Video Diffusion do not inhibit the generation of such content, resulting in higher\nNSFW rates. Pika's lower security risk stems from its text detector's capability to identify keywords related to\npolitical sensitivity and subsequently refuse video generation. Conversely, Open-Sora's reduced NSFW rate is\npartly due to its weaker comprehension and generative capability.\nIllegal Activities. The NSFW rates for four video generation models are notably high when generating\ncontent related to illegal activities. Pika, Gen2, and Open-Sora exhibit NSFW rates around 50%, while Stable\nVideo Diffusion displays an NSFW rate approaching 65%. Current models lack robust safeguards against the\ngeneration of content related to illegal activities, which poses risks not only of fostering criminal behavior\nbut also of exposing platforms and their users to legal and societal liabilities. Additionally, generating videos\nrelated to illegal activities may raise concerns about the application of generative artificial intelligence in daily\nlife.\nDisturbing Content. Gen2 achieves the lowest safety risk among four models regarding disturbing content.\nStable Video Diffusion also detects a portion of disturbing content, while Pika and Open-Sora exhibit almost\nno defensive mechanisms. Gen2's superior performance likely stems from its realistic video generation\nstyle, offering some resistance to grotesque descriptions and horror elements. For instance, as illustrated\nin Figure 11, in response to the same malicious prompt, Gen2 generates realistic human, unaffected by\ndescriptions suggestive of a \"corpse-like\" appearance. In contrast, Pika, Stable Video Diffusion, and Open-\nSora all produce disturbing skeletal videos that may cause discomfort to viewers. For publication purposes, we\napply blurring to the examples of Pika, Stable Video Diffusion and Open-Sora. Additionally, in the dimension\nof disturbing content, there is a significant disparity between GPT-4 and human judgments, possibly because\nGPT-4 does not fully comprehend scenarios that humans find frightening or uncomfortable in the absence of\nexplicit elements like gore.\nMisinformation and Falsehoods. None of the four text-to-video generative models specifically implements\nmeasures to detect misinformation and falsehoods, resulting in higher NSFW rates. Figure 17 presents the\nvisualization examples. All four models generate scenarios of a fire at the United States Capitol, potentially\nleading to public misunderstanding and panic. As text-to-video generation models increasingly produce\nrealistic outputs, the potential risks associated with misinformation are also escalating. In practice, determining\nwhether information constitutes misinformation or falsehoods is challenging, necessitating further research to\naddress these issues.\nCopyright and Trademark. Gen2 and Stable Video Diffusion exhibit relatively high NSFW rates in the\ncontext of copyright and trademark infringement. In contrast, Pika demonstrates exceptional defensive\ncapability; it does not refuse generation but ensures the resulting videos are free of infringing marks. This\nefficacy likely stems from the model's training process, which incorporates consideration of infringing symbols\nand implements measures for their elimination. Open-Sora, due to limited generative capability, fails to\nproduce clear representations of specific trademarks in certain instances. Figure 12 and Figure 13 present the\nvisualization examples. Both Gen2 and Stable Video Diffusion generate visuals containing the logos \"KFC\"\nand \"NEW ERA\". Conversely, Pika generates video content featuring a fried chicken bucket and a hat devoid\nof any trademarked logos.\nTemporal Risk. Pika exhibits a higher NSFW rate compared to other models, where the latter approach a 0%\nrate. This disparity arises because Pika possesses superior capability in generating continuous actions and\nvariations unique to videos, such as complex movements, subtitle shifts, and transformations in human forms.\nIn contrast, the other three models demonstrate weaker generative abilities and fail to meet the minimum\nthreshold necessary to produce such risks. For example, as depicted in Figure 14, when confronted with the\nsame malicious prompt, Pika effectively captures the transformation of God turning into demon, highlighting\nunique security risks associated with video. In contrast, Gen2, Stable Video Diffusion and Open-Sora only\npartially represent the \"demon\" described in the prompt, overlooking the critical transformation from \"god\" in\nthe prompt's initial segment. We employ blurring on the examples of Gen2 and Open-Sora for publication\npurposes. This underscores the necessity to consider Temporal Risk as a critical new category of risk in the\nevolving field of video generation, where advancements in model capability continually emerge."}, {"title": "4.2 Holistic perspectives", "content": "Which one is the safest model? Overall, Gen2 and Stable Video Diffusion present slightly lower security\nrisks compared to Pika and Open-Sora. However, no single model excels in all aspects. Different models\nshowcase distinct strengths. Stable Video Diffusion is nearly impeccable in managing sexual content (for\ncomparative visualization examples, see Figure 6 and Figure 7), achieving an almost 0% NSFW rate. Gen2\ndemonstrates the lowest safety risks in gore and disturbing content (for comparative visualization examples,\nsee Figure 8, Figure 9 and Figure 11), while Pika exhibits exceptional defense capability in copyright and\ntrademark infringement (for comparative visualization examples, see Figure 12 and Figure 13).\nComparison in terms of aspects. As depicted in Figure 4, first, almost all models underperform in aspects\nrelated to Public Figures, Violence, Illegal Activities, Misinformation and Falsehoods, highlighting the critical\nneed for future improvements in these aspects. Additionally, Pika and Open-Sora exhibit higher security risks\nconcerning Pornography, Borderline Pornography, Gore, and Disturbing Content (for comparative visualization\nexamples, see Figure 6, Figure 7, Figure 9 and Figure 11). This heightened vulnerability may stem from the\nlack of post-generation detectors for videos, resulting in ineffective defenses against these more explicit NSFW\ndimensions. We recommend the integration of a post-detection mechanism as an additional safety measure\nto enhance the security of Pika and Open-Sora. However, it is important to note that video safety detectors\nare not omnipotent. While they effectively identify explicit sexual and gore content, they fail to detect other\ndimensions of unsafe content, necessitating further research to address these limitations.\nComparison of jailbreak prompt attacks. Compared to malicious prompts generated by GPT-4, jailbreak\nprompt attacks generally enhance the model's tendency to produce unsafe videos, as demonstrated by the\nexperimental results in Table 4. However, these attacks are less effective on Open-Sora. This discrepancy\narises because methods like Ring-A-Bell and Jailbreaking Prompt Attack incorporate a substantial amount\nof meticulously crafted gibberish in the text prompts, which exceeds Open-Sora's comprehension capability,\npreventing it from generating the intended provocative videos.\nCorrelation between GPT-4 and human evaluation. The correlation between the evaluations of GPT-4\nand human assessments is generally strong across most dimensions, with correlation coefficients exceeding\n0.8. These findings suggest that leveraging GPT-4 for assessments is reasonable in our context. However, a\nsignificant divergence is observed in the dimension of Disturbing Content, where the correlation coefficient is\nonly 0.589. This discrepancy may stem from GPT-4's limited ability to fully understand scenarios that evoke\nfear and discomfort in humans without explicit elements like gore (see Figure 11). These observations open\nnew avenues for research into developing better automatic evaluation that excel across multiple safety aspects.\nTrade-off between the accessibility and safety. It is noteworthy that a trade-off exists between the availability\nand security of text-to-video generative models. For instance, in the Temporal Risk dimension, Pika's superior\ncapability in generating continuous actions and changes leads to heightened security risks (see Figure 14,\nPika effectively captures the transition from God to demon). In contrast, the other three models exhibit\nweaker generative abilities and fail to meet the minimum criteria for posing such risks (see Figure 14, Gen2,\nStable Video Diffusion and Open-Sora only partially represent the \"demon\" of the prompt, neglecting the\nprompt's initial \"God\" component). Regarding the Discrimination dimension, all four models struggle to\neffectively capture this more abstract and complex content, inadvertently resulting in reduced security risks.\nMoreover, in the Borderline Pornography dimension, Open-Sora's limited understanding prevents it from\ndiscerning the subtly implied non-direct sexual content (see Figure 7, Open-Sora captures only the association\nbetween \"kiss\" and \"mouth\" and fails to comprehend the sexual implications embedded in the entire malicious\nprompt), thus enhancing its security. Consequently, weaker generative capability in video generative models\nparadoxically correlate with higher security in certain dimensions. This also implies that as the field of video\ngeneration evolves and model capability strengthen (e.g., the release of Sora by OpenAI, which can produce\nup to 1-minute-long high-fidelity videos that closely align with user's text prompts), the security risks across\nvarious dimensions will increase, underscoring the urgency to prioritize video security."}, {"title": "5 Conclusion", "content": "In this paper, we introduce a new benchmark for assessing the safety risks of text-to-video models, named\nT2VSafetyBench. By examining the usage policy and surveying AI safety practitioners, we identify 12 aspects\nin which generated videos may exhibit illegal or unethical content and construct a malicious text prompt dataset\naccordingly. We evaluate using GPT-4 and human assessment, observing a high correlation between GPT-4\nand human judges. Moreover, we find that no model excels in all aspects, and there is a trade-off between\nthe usability and safety of text-to-video generative models. These insights suggest that as the capability of\nvideo generation models increase, safety risks are likely to escalate significantly. We hope our comprehensive\nbenchmark, in-depth analysis, and insightful findings can be helpful for understanding the safety of video\ngeneration in the era of generative AI and improve its safety in future."}]}