{"title": "Perspective Transition of Large Language Models for Solving Subjective Tasks", "authors": ["Xiaolong Wang", "Yuanchi Zhang", "Ziyue Wang", "Yuzhuang Xu", "Fuwen Luo", "Yile Wang", "Peng Li", "Yang Liu"], "abstract": "Large language models (LLMs) have revolutionized the field of natural language processing, enabling remarkable progress in various tasks. Different from objective tasks such as commonsense reasoning and arithmetic question-answering, the performance of LLMs on subjective tasks is still limited, where the perspective on the specific problem plays crucial roles for better interpreting the context and giving proper response. For example, in certain scenarios, LLMs may perform better when answering from an expert role perspective, potentially eliciting their relevant domain knowledge. In contrast, in some scenarios, LLMS may provide more accurate responses when answering from a third-person standpoint, enabling a more comprehensive understanding of the problem and potentially mitigating inherent biases. In this paper, we propose Reasoning through Perspective Transition (RPT), a method based on in-context learning that enables LLMs to dynamically select among direct, role, and third-person perspectives for the best way to solve corresponding subjective problem. Through extensive experiments on totally 12 subjective tasks by using both closed-source and open-source LLMs including GPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single fixed perspective based methods such as chain-of-thought prompting and expert prompting, highlights the intricate ways that LLMs can adapt their perspectives to provide nuanced and contextually appropriate responses for different problems.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have exhibited substantial advancements (Brown et al., 2020; OpenAI, 2023, 2022; Touvron et al., 2023; Jiang et al., 2023) in recent years, demonstrating remarkable performance across various tasks such as mathematical reasoning (Luo et al., 2023; Yang et al., 2023), code generation (Chen et al., 2021; Roziere et al., 2023), and commonsense question answering (Talmor et al., 2019). Meanwhile, research in the realm of subjective tasks remains relatively nascent (Rottger et al., 2022; Kanclerz et al., 2023; Sun et al., 2023). Unlike objective tasks, which are typically well-defined and directly solvable, subjective tasks such as metaphor recognition (Mohler et al., 2016) and dark humor detection (Meaney et al., 2021) require an understanding of context, linguistic subtleties, and varying individual perspectives. These elements are challenging to quantify or measure objectively, thus presenting significant obstacles for current LLMs (Jentzsch and Kersting, 2023; Wachowiak and Gromann, 2023; Mao et al., 2024).\nChain-of-thought (CoT) prompting style methods have been widely used to elicit the reasoning ability of LLMs (Wei et al., 2022; Kojima et al., 2022; Zhang et al., 2023; Yao et al., 2023). However, in subjective-leaning questions, it is difficult to find a chain-of-thought pathway similar to that in conventional reasoning tasks. Also, due to the nature of the subjective tasks, manually handwriting the reasoning paths for subjective tasks is more challenging and less consistent. Therefore, directly using CoT prompting techniques may not be practical for subjective tasks. Furthermore, the generated reasoning pathways can even mislead the model to provide incorrect answers, as example shown in Therefore, we are motivated to propose a general method to enhance the ability of LLMs to solve various subjective tasks."}, {"title": "2 Related Work", "content": "Subjective Tasks in NLP. Compared with objective tasks such as commonsense reasoning (Talmor et al., 2019) and arithmetic question-answering (Cobbe et al., 2021), research on LLMs in subjective tasks (e.g., metaphor recognition and dark humor detection) (Rottger et al., 2022; Kanclerz et al., 2023; Sun et al., 2023) is still underexplored. Different from objective tasks that can often be clearly defined and solved, subjective tasks involve the capability to perceive context, language nuances, and emotions, which cannot be easily quantified or objectively measured, thereby posing challenges for current LLMs (Jentzsch and Kersting, 2023; Wachowiak and Gromann, 2023; Mao et al., 2024). For example, as shown in results of BigBench(bench au-"}, {"title": "3 Method", "content": "The overall pipeline of the proposed RPT is structured into three steps. Firstly, we input the task description and a specific question, prompting the model to select the most appropriate perspective for answering the question. Secondly, the model evaluates and ranks these perspectives based on their confidence levels in addressing the question. Thirdly, the model adopts the perspective with the highest confidence to formulate and deliver the definitive answer.\nFormally, given a subjective task with a description D and a specific question Q, our goal is to let the LLM M solve the question Q.\nStep 1: Exploring Diverse Available Perspectives. We first let LLM M explore diverse perspectives P according to the description D and question Q. Specifically, we have:\nP = {P1, P2, ..., pn } = M(D \u2295 Q\u2295T\u2081), (1)\nwhere n (the number of perspectives) is usually 3, \u2295 denotes concatenation operation. T\u2081 is a prompt serving as a trigger sentence, for example, we can set T1 as \"Firstly, analyzing the question from diverse perspectives, and selecting some available perspectives based on the question\".\nStep 2: Ranking Perspectives by Confidence Level."}, {"title": "4 Experiments", "content": "4.1 Settings\nDatasets. We evaluate the effectiveness of our method on twelve subjective reasoning datasets, which can be categorized into five types, as shown in Notably, for SemEval and cultural-related datasets which contain training sets, we evaluate in both zero-shot and few-shot settings. For the other tasks, we utilize corresponding test sets from BigBenchhttps://github.com/google/BIG-bench/\ntree/main/bigbench/benchmark_tasks/\nonly evaluate in zero-shot settings."}, {"title": "4.2 Zero-shot Results", "content": "In shows the experimental results of the baselines and our RPT method in zero-shot settings. From the experimental results, we can observe that:\nRPT method consistently outperforms the baselines in most settings. Due to its ability to rank perspec-"}, {"title": "4.3 Few-shot Results", "content": "In Table 4, we present the main results in few-shot settings. Similar to the zero-shot results, RPT method achieves the best average performance across different models. For instance, on Llama-3, RPT surpasses CoT-SC, the best-performing baseline, by 2.06 points.\nA possible explanation is that providing a few examples in the prompt generally benefits LLM performance by providing context. However, subjective tasks are not well-defined and directly solvable, leading to significant differences between examples. As a result, LLMs exhibit varying confidence across examples, limiting the performance gains from examples and sometimes introducing noise or bias. For instance, using 3-shot examples in the RiC baseline lead to an average performance drop of 6.50 points on GPT-4. In contrast, RPT choose among perspectives and evaluates confidence for each input and method, providing finer-grained supervision signals and resulting in an average performance gain of 1.67 points."}, {"title": "5 Analyses and Discussion", "content": "5.1 Ablation Study\nAs shown in Figure 4, we further investigate the impact of every perspective on the RPT method. The full RPT method achieves the best performance across all datasets. From the results of the ablation study, we can observe the following:\nRemoving any single perspective results in an average performance drop of 1.32-2.53 points, indicating that direct perspective, role perspective, and third-person perspective each have unique and irreplaceable con-tributions to subjective reasoning tasks. Specifically,"}, {"title": "5.2 Analysis on Inference Cost", "content": "The inference cost of modern LLMs is crucial. Using the GPT-3.5 model as an example, we represent the inference cost by the length of the response during the reasoning process before producing the final answer. In Figure 5, using the 3-shot GPT-4 experiment as an example, we plot the length-performance relationship for RPT and the baselines. It can be observed that compared to most baselines, RPT achieves the best performance with a smaller inference cost, demonstrating the efficiency of the dynamic perspective selection approach."}, {"title": "5.3 Analysis on the Number of Shots", "content": "As shown we specify the number of shots and study the performance difference compared to the original RPT on SemEval. We observe that performance is lower when fewer shots are selected, as the model is unfamiliar with the task and method. As the number of shots increases, performance improves. However, in some circumstances when the number of shots reaches three or more, performance declines.\nOn one hand, LLMs exhibit greater flexibility when autonomously evaluating confidence and planning the number of shots during reasoning, allowing them to adapt to unique subjective tasks. On the other hand, providing too many examples may lead to increasing the inference cost, raising the risk of over-fitting, and challenging the instruction-following ability of LLMs. Overall, under the majority of settings for each dataset, RPT achieves the best performance, demonstrating its generalization ability and versatility (See Appendix B.3"}, {"title": "5.4 Case Study", "content": "In we showcase an example from the SemEval stance detection dataset to highlight the effectiveness of the RPT method in subjective reasoning tasks. Unlike baselines such as CoT and Role-playing, which sometimes emphasize skepticism or negative sentiment without fully accounting for context, RPT evaluates multiple perspectives, including direct and third-person analyses. For example, CoT and RiC interpret the phrase \u201cGet the truth from Trump!\u201d as reflecting skepticism or disbelief, leading to an \u201cAGAINST\u201d prediction. In contrast, RPT dynamically selects the most confident perspective, reasoning that the exclamation mark and phrase suggest advocacy or favor toward Trump. This ability to transition between and rank perspectives makes RPT more adaptable and effective in subjective reasoning tasks compared to single-perspective baselines (See Appendix B.2 for more cases)."}, {"title": "5.5 Analysis on Keyword Statistics", "content": "As shown in 9, to further investigate the characteristics of different perspectives in the RPT pipeline during reasoning, we conduct a keyword frequency analysis for the three perspectives in the RPT pipeline. After removing stopwords and irrelevant prompt words, we can observe the following reasoning characteristics for each perspective: the direct perspective tends to perform straightforward reasoning; the role perspective leans towards adopting different expert roles and contexts; and the third perspective excels in discussions and dialogues. The uniqueness of each perspective underscores the ne-"}, {"title": "6 Conclusion", "content": "In this paper, we introduce RPT, a novel method that achieves multi-perspective reasoning and integration by exploring diverse perspectives and ranking them based on confidence. Comprehensive experiments conducted on GPT-4, GPT-3.5, Llama-3, and Qwen-2 demonstrate that RPT effectively integrates various perspectives, enhancing the subjective task-solving capabilities of LLMs without significantly increasing inference costs. This work highlights how LLMs can better handle the fluidity of subjective reasoning, even in the absence of nuanced understanding of perspectives or personal biases. Future research directions include integrating additional reasoning perspectives, developing finer-grained and adaptive perspective taxonomies, and extending our method to broader applications."}, {"title": "Limitations", "content": "First, in designing the RPT pipeline, we categorize perspectives into three types based on related works. Although RPT and many inference paradigms involved in the baselines are orthogonal and combinable, this taxonomy could still be further refined, for example, by adopting alternative categorization methods or employing a more fine-grained division. Second, RPT directly selects perspectives rather than methods. We consider perspectives as a meta-method, meaning that RPT can be combined with other methods to achieve better performance. Thirdly, RPT operates within a single round of dialogue, without accounting for multi-turn conversations or result feedback. In the future, exploring multi-turn dialogue or multi-agent perspective writing could be a promising direction."}, {"title": "Ethics Statement", "content": "This paper uses widely available datasets, including stance detection, sarcasm detection, and cultural comparison, along with LLM-generated responses, solely to validate the proposed method without reflecting any stance or bias from the authors."}]}