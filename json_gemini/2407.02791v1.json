{"title": "Model-Enhanced LLM-Driven VUI Testing of VPA Apps", "authors": ["Suwan Li", "Lei Bu", "Guangdong Bai", "Fuman Xie", "Kai Chen", "Chang Yue"], "abstract": "The flourishing ecosystem centered around voice personal assistants (VPA), such as Amazon Alexa, has led to the booming of VPA apps. The largest app market Amazon skills store, for example, hosts over 200,000 apps. Despite their popularity, the open nature of app release and the easy accessibility of apps also raise significant concerns regarding security, privacy and quality. Consequently, various testing approaches have been proposed to systematically examine VPA app behaviors. To tackle the inherent lack of a visible user interface in the VPA app, two strategies are employed during testing, i.e., chatbot-style testing and model-based testing. The former often lacks effective guidance for expanding its search space, while the latter falls short in interpreting the semantics of conversations to construct precise and comprehensive behavior models for apps. In this work, we introduce Elevate, a model-enhanced large language model (LLM)-driven VUI testing framework. Elevate leverages LLMs' strong capability in natural language processing to compensate for semantic information loss during model-based VUI testing. It operates by prompting LLMs to extract states from VPA apps' outputs and generate context-related inputs. During the automatic interactions with the app, it incrementally constructs the behavior model, which facilitates the LLM in generating inputs that are highly likely to discover new states. Elevate bridges the LLM and the behavior model with innovative techniques such as encoding behavior model into prompts and selecting LLM-generated inputs based on the context relevance. Elevate is benchmarked on 4,000 real-world Alexa skills, against the state-of-the-art tester Vitas. It achieves 15% higher state space coverage compared to Vitas on all types of apps, and exhibits significant advancement in efficiency.", "sections": [{"title": "I. INTRODUCTION", "content": "With the prevalence of smart speakers, voice personal assis- tants (VPA) have permeated various aspects of people's lives. Prominent examples include Amazon Alexa, Google Assistant, and Apple Siri, which have been widely used for assisting smart speaker users. Centered around them, numerous appli- cations (or VPA apps for short) have been developed to provide various functionalities, such as accessing news, entertainment, and controlling devices. VPA apps are characterized by the voice user interface (VUI), which enables user interaction solely through verbal conversations. The major VPA service providers have established VPA app stores for efficient app distribution. Through them, third- party developers can unload their apps, and users can invoke apps without installation, simply by calling their invocation names. Such openness and ease of access have led to the widespread popularity of VPA apps. For example, the skills store, the largest VPA app store, boasts over 200,000 apps [1]. However, there have been concerns raised regarding their security, privacy and quality. A considerable number of VPA apps are found malicious as a result of untrustworthy skill certification process [2, 3]. Prior works have discovered that malicious VPA apps can eavesdrop [4, 5] or ask users' privacy information without permissions [6, 7]. The behavior of several VPA apps contradicts their privacy policies [6-9]. Additionally, a large number of apps exhibit poor quality, such as terminating unexpectedly [10] or failing to understand common user inputs [11]. To detect such problems, a thorough exploration of VPA apps' behavior is necessary. Existing methods mainly em- ployed strategies of depth-first search based chatbot-style testing [6, 8, 9, 12, 13] or model-based testing (MBT) [10]. Since VPA apps cannot roll back to the previous interface, the exploration efficiency can be affected especially when the depth-first search strategy is taken. Such testers have to start from the beginning after searching one path, resulting in repeated tests. They can work effectively on simple apps, but may suffer from low efficiency when facing complex apps. In addition, previous MBT approach falls short in understand- ing and utilizing semantic information when exploring apps' behavior and constructing the model. Therefore, the semantic information is crucial in efficient testing of VPA apps. As the large language models (LLM) are known for their strong natural language understanding and processing abilities [14-17], and previous studies have found that they can be used for downstream tasks with in-context learning [18], we adopt the LLM to drive the testing process to compensate for semantic information loss during the model- based VUI testing. However, employing the LLM for the VUI testing presents the following three challenges: Challenge 1: LLMs can be used to supplement the semantic loss during the model-based testing of VPA apps, but it is difficult for LLMs to maintain the state information of VPA apps accurately. On the one hand, when the testing goes deeper and the context becomes larger than the LLM's limitation, the information required for LLMs to generate an accurate model is incomplete. On the other hand, LLMs can hardly generate a precious model especially when the VPA apps' behavior is complex. However, a wrong model can greatly affect the following exploration. Challenge 2: The results generated by LLMs can be redun- dant and repeated under VPA apps' context. For example, if the LLM is asked to generate context-related inputs for a given VPA apps' outputs (see figure 2), it tends to generate long results, but most VPA apps have difficulty processing these inputs. If state information and exploration strategy is not provided, the LLM can generate repeated inputs for the same state, affecting the testing efficiency. For these reasons, prompts should be carefully designed to help the LLM generate formalized and efficient results. Challenge 3: LLM's results are not entirely reliable due to its unexplainability and uncertainty. For example, even if LLMs are prompted to return simple and concise results, they may still generate results that VPA apps cannot understand. Therefore, we need to filter out the unreliable results based on the feedback from VPA apps and our domain knowledge. To address the above three challenges, we propose the following solutions. To tackle Challenge 1, we split the complex LLM-driven model-based testing tasks into three phases: states extraction, input events generation, and state space exploration to increase the accuracy of model construction. In each phase, the LLM only extracts the state and generate input events for the real- time VPA apps' output, so the length of prompt will not exceed the context limitation. Besides, the LLM is only used to make up for the semantic loss during the model construction and exploration, such as merging outputs with similar semantics to one state, generating context-related inputs and selecting an input for efficient exploration, while the model information is stored and maintained locally. For addressing Challenge 2, we embed the information provided by the behavior model into the prompts to help the LLM generate efficient results and avoid repeated tests. Since the complete behavior model is complex and occupies many tokens, adding it to the prompt not only interferes with the extraction of core information but also brings unnecessary ex- penses. Therefore, we only extract phase-specific information to the prompt. For example, only the state list is provided in the states extraction phase. Meanwhile, by designing appropriate few shots, we enable the LLM to formalize outputs. For the state space exploration, we implement the step-by-step chain- of-thought strategy to guide the LLM in parsing the behavior model and making decisions. To handle Challenge 3, we establish specific rules consid-"}, {"title": "II. BACKGROUND", "content": "A. VPA Apps and Behavior Model VPA apps are apps based on smart speakers. Users interact with VPA apps through voice, so the interface of VPA apps is called the voice user interface (VUI). VUIs are typically free of visible graphical interfaces. Therefore, the exchange of all information are purely through voice. While the VUI brings convenience, its invisible feature introduces a range of quality and security concerns, such as unexpected exits [10], privacy violations [3, 4], and expected apps started [5, 19]. For this reason, thoroughly exploring VPA apps' behavior while testing the VUI's quality and security issues is of paramount importance. However, VPA apps are not open source for normal testers. A VPA app is composed of the front-end interaction model and the back-end processing code. The development platform provides storage for the front-end interaction model, while the back-end code of VPA apps is stored on the developer's server. As a result, dynamic testing is a commonly used method for testing the VUI of VPA apps. Since the front-end interaction model of VPA apps is designed based on implicit models [20], we propose to use the model-based testing approach to explore the behavior of VPA apps. VPA apps' outputs express their functionalities and pur- poses. By understanding and analyzing the outputs, states can be extracted. Apps' transfer from one state to another is only triggered by users' inputs. As a result, VPA apps' behavior can be described by the finite-state machine (FSM), which has been proved to be applicable for constructing VPA apps' behavior models [10]. A finite-state machine consists of five parts, described as FSM = (Q, \u03a3, \u03b4, s0, F). Among them: \u2022 Q represents the set of states. Apps' outputs are mapped to states. \u2022 \u03a3 represents the set of input events. Users' inputs are mapped to input events. \u2022 F is the set of final states, and satisfies F \u2286 Q. VPA apps' final outputs are mapped to final states. \u2022 s0 is the initial state and satisfies s0 \u2208 Q. The initial state is always set as \u201c<START>\". \u2022 \u03b4: Q \u00d7 \u03a3 \u2192 Q represents a transition function. The input event e that triggers the transition from the state s0 to the states s1 is represented as \u03b4(s0, e) = s1. B. Large Language Model Large Language Model (LLM) is built on the transformer architecture. LLMs have been proved with strong natural language processing capabilities [14-17]. Compared to gen- eral language models (LM), LLMs have a vast number of parameters and undergo extensive text training. Due to these characteristics, LLMs can be directly applied to downstream tasks. In addition, methods like fine-tuning [21] and in-context learning [18, 22] can improve LLM's capabilities for specific downstream tasks. In the in-context learning technique, users only need to provide few samples as a reference for the down- stream task, which implies that LLMs can handle downstream tasks through learning from a small dataset. LLMs can be categorized into three types based on the transformer architecture: encoder-only, encoder-decoder, and decoder-only. Encoder-only and encoder-decoder are suitable for infilling tasks, while decoder-only models are better at text generation tasks. Considering that our tasks involve the model generation and exploration, we prefer to adopt decoder-only models. Popular decoder-only models include OpenAI's GPT series [23, 24], Meta's Llama series [25], etc. Additionally, there are models specifically designed for code generation tasks such as Codex [26] and Codegen [27].\""}, {"title": "III. LLM DRIVEN MODEL CONSTRUCTION AND EXPLORATION", "content": "A. Overview As a model-based testing framework, Elevate works by constructing the model according to VPA apps' behavior and guiding the exploration based on this model. The behavior model is built by mapping VPA apps' outputs to states and users' inputs to input events (see Section II). As states reflect VPA apps' functionalities, purposes and behavior, different outputs with similar semantics (e.g., functionalities, purposes and behavior) should be mapped to one state. We call these outputs as semantically similar outputs under the context of VPA apps' behavior. Besides, users' inputs should be context related to the apps' outputs so that meaningful states can be discovered. Overall, the states extraction and input events generation require natural language processing, which is the strength of the LLM. In addition, the LLM has proved its ability in understanding graphs [24] and reasoning with prompt engineering techniques such as in-context learning and chain-of-thought [17, 18, 22]. Our state space exploration task is basically an input event selection task considering factors like historical transitions, invocation frequency and relevance to the current state based on understanding the behavior model (i.e., a graph). Given current state related information from the behavior model, the LLM can be used to select input events for further exploration of VPA apps' behavior. In traditional model-based testing, the model is firstly built and then used to guide the exploration of the state space. However, when testing VPA apps, the initial model is difficult to acquire before interacting with VPA apps as the VPA apps are closed-source and most documents only provide a few lines to describe their functionalities. To solve that problem, we construct VPA apps' behavior model on-the-fly, which means the model is built during the interaction. The behavior model is finally embedded into the prompt to guide the LLM in extracting states and selecting efficient input events for exploration. To save tokens, only phase-specific behavior model information is provided. Based on these ideas, we propose Elevate, a model- enhanced LLM driven model-based testing method for VUI testing of VPA apps. Figure 3 shows the framework of Elevate. Elevate consists of three phases, and they are all performed by LLMs. The first two phases are for model construction, including states extraction and input events generation. In the third phase, the LLM selects an input event to explore the state space based on the information provided by the behavior model. Since we adopt an on-the-fly model construction ap- proach, these three phases are executed one by one repeatedly. The main processes of these three phases are described below. Phase 1: States extraction. In this phase, VPA apps' outputs and existing states in the behavior model are embedded into the prompt. The LLM decides whether to merge the VPA apps' output with existing states or generate a new state for it. We expect the LLM to map outputs with similar semantics to the same state. A state filter is used to filter out mismatched states generated by the LLM. Phase 2: Input events generation. The VPA apps' real- time output is input to the LLM, which generates all possible context-related input events for this output. We expect the input events generated by the LLM to be semantically related to the VPA apps' output and help discover meaningful new states. An input checker is implemented to check the validation of input events according to VPA apps' feedback. Phase 3: State space exploration. The current state and current-state-related information in the behavior model are input to the LLM. The LLM is expected to select one input event by considering factors such as the invocation frequency, historical transitions and relevance to the current state to explore the state space efficiently. Based on the invocation frequency and history transitions, we search whether there is a better input in the input event set. If there is one, we reject the LLM's results and ask for another input event. Whenever we receive an output from VPA apps, we execute the first and second phases to generate states and input events. The states and input events are used for the behavior model construction. Subsequently, we extract information related to the current state from the behavior model and embed it to the prompt, and the LLM selects the most suitable input event at the third phase. After that, the selected input event is fed back to VPA apps and wait for the next output. The whole process will be continued until the time limit is reached or the VPA apps quit. Due to the unexplainability of the LLM, we establish the feedback mechanism to check and filter out its results. Results that do not meet our requirements are rejected, and the reasons are returned to the LLM for regenerating the results. In the following sections, we will introduce the prompts and feedback mechanisms of these three phases respectively. To help express the implementation of these three phases clearly, we introduce the following terms: \u2022 <app's output>: the real-time VPA apps' output. It will be used to extract states. Context-related inputs are generated based on its content. \u2022 : the state extracted from . \u2022 : the previous explored state. \u2022 : the next explored state. \u2022 : the set of context-related inputs generated for  \u2022 : the input selected by the LLM at  to communicate with the VPA apps. \u2022 : the previous selected input. \u2022 : the behavior model. \u2022 : the set of states in the behavior model. \u2022 : the input events information of state s, including their invocation times. \u2022 : the set of transition functions that start from state s. B. States Extraction Similar semantics (e.g., functionalities, purposes and con- text) of VPA apps can be expressed in different ways. The LLM should merge outputs with similar semantics to one state. For each , the LLM is supposed to find a semantic similar state from  or generate a new state. For this reason, only the  is required in this phase. So the input of this phase includes the  and  To avoid redundant results, the LLM is required to only output the  of the given . To assist the LLM in better understanding this task and formalizing its outputs, we employ the in-context learning strategy. Few shots are in the form of \u201cInput: ,  and \"Output: \". As the LLM's results are not trustworthy, we establish a state filter to filter out mismatched states. If a state is mismatched, we provide feedback prompts to request another state from the LLM. The prompts of phase 1 are displayed in Table I. When we first use the LLM for states extraction, we use *LONG PROMPT*. In *LONG PROMPT*, we instruct the LLM to map semantically similar outputs to one states in the behavior model (labeled as *MAP INSTRUCTION*). Few shots are provided for LLMs to understand the state extraction task (labeled as *FEW SHOTS*). Subsequently, we request it to return the corresponding  in the  for the . In other cases, we will use *SHORT PROMPT*. *SHORT PROMPT* only in- cludes the  and . After *LONG PROMPT* or *SHORT PROMPT*, the LLM will generate the  for . If  is rejected by the state filter, we will return *FEEDBACK PROMPT*. Figure 4 illustrates the state filter in the states extraction phase. Firstly, we check whether  \u2208  or  == . If neither of them is true, we return *NO STATE ERROR*. Otherwise, we proceed to the second step of the check. If  \u2208 , we check whether  and  have the same input events (see section III-C for the generation of ). If-"}, {"title": "C. Input Events Generation", "content": "In section III-A, the  for the  is ex- tracted. To further explore VPA apps' behavior, context related inputs should be generated. Each state has its independent context related input event set, as we consider different states as different contexts. To ensure the context relevance, the LLM is also used in this phase. The  generated for the  is also the input event set of . VPA apps expect users to give short and simple inputs, but LLMs tend to generate long and redundant inputs, which most VPA apps cannot understand. To solve this problem, we offer few shots that include five types of VPA apps' outputs (i.e., yes-no question, selection question, instruction question, Wh question and mixed question [6]). For the mixed question, we summarize three most common patterns, they are instruction + selection question, Wh + selection question and yes-no + selection question. We provide at least one example for each type of questions in the few shots. They are in the form of \"Input: \" and \u201cOutput: \" pairs. In addition, we set an input checker to check the validation of the input events. The  is used to judge whether the input events generated by the LLM are context related. If  is equal to  or expresses confusion, we feedback the information to request other . The prompts are displayed in Table II. When we ask the LLM to generate input events for the first time, we use *LONG PROMPT*, which provides *FEW SHOTS* and instructs the LLM to find  to the . In other cases, we use *SHORT PROMPT*, which only contains the . After  from  is selected (see Section III-D) and sent to the VPA app, the app will soonly give another output. Based on the content of that output, we judge the validity of . Figure 5 illustrates the workflow of the input checker. Firstly, we check whether  is empty. If it is, we will return *EMPTY ERROR*. If any input event  from the  is given to the VPA app and the next state  ==  or  expresses apps' confusion,  is considered as an invalid input event. In this case, we will return *INVALID SUGGESTION*.\""}, {"title": "D. State Space Exploration", "content": "The aim of this phase is to efficiently explore the state space based on the information provided by the behavior model. This is done by finding an input event that is most likely to discover new states (i.e., functionalities) at each state. It is a decision- making problem considering factors such as invocation fre- quency, historical transitions, and relevance to the current state based on the behavior model (essentially a graph). Due to the fact that LLMs have developed their abilities in understanding graphs [24], and prompt engineering techniques like chain-of- thought can improve the LLM's explainability and capability to handle reasoning tasks [17], the LLM is used for the state space exploration. In the previous two phases, we extract the  and generate the  for the . They are used to update the behavior model. The model information is then used to guide the state space exploration. For this reason, the input of this step includes the  and the  related information in the . The  related information includes the  and the  (invocation times of each input is updated after it is sent to the app). To improve the LLM's capability of this decision-making task, we employ a strategy combining in-context learning and chain-of-thought. We prompt the LLM to think step-by-step and show its thinking process. In step 1, the LLM is asked to remove the input events that lead to duplicate or wrong state from the historical transitions. In step 2, the LLM finds a never-invoked input event that is most context related. In step 3, the LLM finally chooses one input event from the never-invoked context-related input event in step2 and the invoked and valid (i.e., does not lead to a state that is same as before or represent apps' confusion) input event. Few shots are pro- vided in the form of \u201cInput: , , \u201d, \u201cThought: step1: xxx, step2: xxx, step3: xxx\" and \"Output: \" triplets. The LLM is expected to output its thinking process along with the selected . Similarly, the  given by the LLM will be evaluated and the feedback will be returned. The prompts in this phase are displayed in Table III. The *LONG PROMPT* is used for the first time. *LONG PROMPT* initially outlines the composition and representa- tion of the behavior model (labeled as *MODEL DESCRIP- TION*). Then, it offers step-by-step guide of the reasoning process (labeled as *STEP-BY-STEP*). Meanwhile, few shots with the thinking process (labeled as *FEW SHOTS*) are provided. Finally, the LLM is asked to select an  from the  to discover new states based on historical transitions in , invocation frequency in  and relevance to . In other cases, we will use *SHORT PROMPT*, which only contains ,  and . After the LLM selects the , we evaluate it by finding whether there is a probably better input event and return the *FEEDBACK PROMPT*. Figure 6 illustrates the process of better inputs checker that evaluates the  and return different *FEEDBACK PROMPT* in the third phase. Firstly, the better input checker checks if  \u2208 . If not, we return *NO INPUT ERROR*. Otherwise, it determines whether there is a better input event  compared with  based on the invocation frequency and history transitions. If  is valid and invoked less frequently than , then  is better than . If  is invalid but  is valid, then  is also a better choice. In both cases, we return *BETTER INPUT SUGGESTION*. The  that passes the above checks is sent to the VPA app.\""}, {"title": "IV. EVALUATION", "content": "We implement Elevate based on GPT-4 [24] and analyze its coverage and efficiency. The performance of Elevate is compared with the state-of-the-art model-based VUI testing method Vitas [10]. Besides, chatbot-style testers are clas- sic VPA apps testing approach, but Vitas was evaluated to outperform traditional chatbot-style testers in coverage and efficiency. However, with the development of LLMs, LLMs as chatbots may have stronger VPA apps testing abilities, so GPT4(chatbot) is also set as a baseline. Additionally, we conduct ablation experiments to assess the contribution of Elevate's each phase to the final state space coverage. We also implement Elevate on Llama2-70b-chat [28] and evaluate Elevate's applicability on different LLMs. Finally, we conduct a large-scale testing on Alexa skills to evaluate Elevate's generality [29]. A. Settings Dataset: We use the large scale dataset of Vitas [30] as our basic dataset. From this dataset, we filter out skills with no ratings. Then, we roughly confirm 4,000 skills with consistent behavior to form the large-scale dataset. These 4,000 skills cover all categories on the Amazon skills website. For the use of conducting an intensive evaluation, we also build a benchmark with 50 Alexa skills. These 50 skills are checked to be stable and available. Baselines: We compare Elevate with two baselines, as shown in table IV. The simulator provided by Amazon[31] is used as our testing platform. The evaluation was conducted on the Ubuntu 18.04.4 machines with AMD EPYC 7702P 64-Core Processor CPU @ 1.996GHz and 4GB RAM. Coverage metrics: VPA apps are not open source, so the ground truth of the entire state space of certain VPA apps cannot be acquired in advance. Furthermore, as Elevate merges states with similar semantics to avoid repeated testing while Vitas does not, we call the states generated by Elevate as semantic states, while the ones discovered by Vitas as sentence states in the evaluation. Consequently, to ensure a uniform measurement, we use Elevate to process the states discovered by Vitas, and merge them to semantic states correspondingly. Then, we use the number of the unique semantic states achieved by Elevate and all the baselines used in certain eval- uations as the total state space for each evaluation respectively for a fair comparison. B. Evaluation of Elevate We aim to address the following research questions: RQ1: How does the semantic state coverage and efficiency improve when using GPT-4 to enhance the model construction and exploration? RQ2: Do all phases in Elevate contribute to the state explo- ration of VPA apps? RQ3: How effective is Elevate's framework when applied to other LLMs? RQ4: How is the coverage rate of Elevate on all types of skills compared with Vitas? 1) Study1: Coverage and efficiency: We set the time limit as 10 minutes for Elevate to test each skill. The baselines are allowed to test skills using the same interaction rounds (an input and an output form an interaction round) as Elevate. Firstly, we compare the sentence states and semantic states achieved by Elevate and the baselines. Then, we compare their average semantic state coverage with interaction rounds. Figure 7 shows the sentence states and semantic states main- tained by Elevate and baselines. It suggests that the sentence states can be greatly compressed when semantic information is considered. Elevate merges outputs with similar semantics to one state for testing, which greatly reduces the original state space. In addition, Elevate achieves more sentence and semantic states than the baselines. In order to evaluate Elevate's coverage ability along with the efficiency, we calculate the average semantic state coverage of Elevate and baselines on the benchmark of varying interaction rounds in figure 8. The horizontal axis represents the average semantic state space rate, while the vertical axis denotes the number of interaction rounds. When the interactions go deeper, the advantage of Elevate over Vitas and GPT4(chatbot) is more evident. After only 3 rounds of interactions, Elevate shows its leading exploration efficiency and stays ahead until the end. Finally, Elevate can achieve over 80% of average semantic state coverage after only 20 rounds of interactions, while Vitas and GPT4(chatbot) can only achieves a final coverage of 68% and 45% respectively. Among the baselines, the traditional model-based tester Vitas has relatively higher performance. However, Vitas did not exploit the semantic information during VUI testing to help the model construction and exploration, so it lags behind Elevate in terms of semantic state coverage. Although GPT- 4 is a strong LLM, directly using it as a chatbot for VPA apps testing performs worse than Vitas. GPT4(chatbot) lacks the guidance for state space coverage, which prevents it from discovering deep states. Enhanced with Elevate, the LLM's performance in semantic state coverage is greatly improved."}, {"title": "V. DISCUSSION", "content": "A. Elevate's limitations Elevate's limitations primarily lie in the large language model. Firstly, although the LLMs can achieve good results, their outputs are non-deterministic. Hence, the performance may vary with each test. Secondly, the thinking process of the LLM is not always accurate. As we introduce the chain- of-thought method in the third phase, the LLM will output its thinking process. While chain-of-thought can enhance coverage and efficiency, the thinking process of the LLM is not always right and we cannot confirm whether the LLM is actually thinking as we expected. Lastly, in rare cases, the LLM may not rectify the results even after multiple rounds of feedback prompts. In such instances, we consider that our feedback strategy cannot steer the LLM out of its hallucination and we resort to generate states and input events based on simple rules."}, {"title": "VI. RELATED WORK", "content": "VPA apps Testing: Several studies have been conducted to test quality, privacy or security related problems on VP apps [6, 8-10, 12, 13, 32]. SkillExplorer [6], VerHealth [9] and SkillDetective [8] are chat-bot style testers that focuses on detecting skills' privacy violation behavior. SkillExplorer and SkillDetective [8] adopt the DFS-based exploration approach. VUI-UPSET [12, 13] is a chat-bot style testing approach to generate correct paraphrases while detecting bugs. Vitas [10] uses the model-based testing to test VPA apps' problems re- lated to quality, privacy and security. Despite the improvement in coverage and efficiency, it uses simple rules to construct the model and fails to consider the semantic information. SkillScanner [32] is the first static analysis method to identify skills' policy violations at the development phase based on a dataset collected from the GitHub. Compared with them, Elevate adopts the model-based testing approach to improve the exploration efficiency and introduces to use the LLM to supplement missing semantic information for model construc- tion and exploration. Security and Privacy of VPA apps: Increasing number of research focuses on security and privacy issues of VPA apps [33-35]. Kumar et al. proposes the skill squatting at- tack [19]. Several searches detected the weakness of the auto- matic speech recognition (ASR) system, which is vulnerable to adversarial sample attacks and out-of-band signal attacks [36- 39]. Many efforts have been spent on detecting problematic privacy policies and potential privacy violating behavior [6\u2013 8, 40, 41]. Different from them, Elevate sought to thoroughly explore the VPA apps' behavior so that sufficient problems can be discovered. Large Language Model for Software Testing: As a booming new technology, Large Language Models are applied to many areas, including software testing. Codet [42] uses the LLM to automatically generate test cases for evaluating the quality of a code solution. CodaMosa [43] asks Codex to generate test cases when the search based software testing method reaches the bottleneck. TitanFuzz [44] uses LLMs to generate and mutate input DL programs for fuzzing DL libraries. Its follow-up work, FuzzGPT [45], primes LLMs to synthesize bug-triggering programs for fuzzing and shows improved bug detecting performance. Other research focused on testing the GUI of mobile apps by generating context-related texts or human-like actions [46, 47]."}, {"title": "VII. CONCLUSION", "content": "In this work, we propose Elevate, a LLM driven model- based testing framework for VPA apps. Elevate uses the LLM for constructing the behavior model and exploring the state space to compensate for the loss of semantic information. It extracts states from VPA apps' outputs and generates input events to these outputs by providing few-shots to LLMs. The LLM's exploration ability is enhanced by chain-of-thought. Moreover, Elevate sets checkers to analyze the LLM's results and uses feedback prompts to ask LLMs for adjustments. Our experiments show that Elevate achieves higher coverage than the state-of-the-art tool Vitas and LLMs as chatbots in an efficient manner. Elevate tests a large-scale dataset of 4,000 Alexa skills and achieves about 15% of higher coverage rate than Vitas in all categories."}]}