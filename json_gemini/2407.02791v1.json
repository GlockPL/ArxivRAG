{"title": "Model-Enhanced LLM-Driven VUI Testing of VPA Apps", "authors": ["Suwan Li", "Lei Bu", "Guangdong Bai", "Fuman Xie", "Kai Chen", "Chang Yue"], "abstract": "The flourishing ecosystem centered around voice personal assistants (VPA), such as Amazon Alexa, has led to the booming of VPA apps. The largest app market Amazon skills store, for example, hosts over 200,000 apps. Despite their popularity, the open nature of app release and the easy accessibility of apps also raise significant concerns regarding security, privacy and quality. Consequently, various testing approaches have been proposed to systematically examine VPA app behaviors. To tackle the inherent lack of a visible user interface in the VPA app, two strategies are employed during testing, i.e., chatbot-style testing and model-based testing. The former often lacks effective guidance for expanding its search space, while the latter falls short in interpreting the semantics of conversations to construct precise and comprehensive behavior models for apps.\nIn this work, we introduce Elevate, a model-enhanced large language model (LLM)-driven VUI testing framework. Elevate leverages LLMs' strong capability in natural language processing to compensate for semantic information loss during model-based VUI testing. It operates by prompting LLMs to extract states from VPA apps' outputs and generate context-related inputs. During the automatic interactions with the app, it incrementally constructs the behavior model, which facilitates the LLM in generating inputs that are highly likely to discover new states. Elevate bridges the LLM and the behavior model with innovative techniques such as encoding behavior model into prompts and selecting LLM-generated inputs based on the context relevance. Elevate is benchmarked on 4,000 real-world Alexa skills, against the state-of-the-art tester Vitas. It achieves 15% higher state space coverage compared to Vitas on all types of apps, and exhibits significant advancement in efficiency.", "sections": [{"title": "I. INTRODUCTION", "content": "With the prevalence of smart speakers, voice personal assis-tants (VPA) have permeated various aspects of people's lives. Prominent examples include Amazon Alexa, Google Assistant, and Apple Siri, which have been widely used for assisting smart speaker users. Centered around them, numerous appli-cations (or VPA apps for short) have been developed to provide various functionalities, such as accessing news, entertainment, and controlling devices. VPA apps are characterized by the voice user interface (VUI), which enables user interaction solely through verbal conversations.\nThe major VPA service providers have established VPA app stores for efficient app distribution. Through them, third-party developers can unload their apps, and users can invoke apps without installation, simply by calling their invocation names. Such openness and ease of access have led to the widespread popularity of VPA apps. For example, the skills store, the largest VPA app store, boasts over 200,000 apps [1]. However, there have been concerns raised regarding their security, privacy and quality. A considerable number of VPA apps are found malicious as a result of untrustworthy skill certification process [2, 3]. Prior works have discovered that malicious VPA apps can eavesdrop [4, 5] or ask users' privacy information without permissions [6, 7]. The behavior of several VPA apps contradicts their privacy policies [6-9]. Additionally, a large number of apps exhibit poor quality, such as terminating unexpectedly [10] or failing to understand common user inputs [11].\nTo detect such problems, a thorough exploration of VPA apps' behavior is necessary. Existing methods mainly em-ployed strategies of depth-first search based chatbot-style testing [6, 8, 9, 12, 13] or model-based testing (MBT) [10]."}, {"title": "", "content": "Since VPA apps cannot roll back to the previous interface, the exploration efficiency can be affected especially when the depth-first search strategy is taken. Such testers have to start from the beginning after searching one path, resulting in repeated tests. They can work effectively on simple apps, but may suffer from low efficiency when facing complex apps. In addition, previous MBT approach falls short in understand-ing and utilizing semantic information when exploring apps' behavior and constructing the model.\nFigure 1 shows two communication logs that illustrate the impact of semantic information on efficiently testing VPA apps. In figure 1(a), between the candidate inputs \"Good-bye\" and \"Service Times\u201d, \u201cService Times\u201d is more likely to lead to unseen app behavior. Therefore, \u201cService Times\u201d should have higher initial priority than \"Goodbye\u201d. Without considering the semantic relevance of inputs, it is likely that \"Goodbye\" is selected and the app stops. In figure 1(b), the two apps' outputs represent similar functional semantics but are expressed differently. The user inputs \"walk\" at the first time, so other inputs like \"play\" should have higher priority at the second time. However, if different outputs are considered as different functionalities, purposes or context, the same input \"walk\" will be selected at the second time for thorough testing. The ignorance of outputs' semantic similarity at the level of functionality, purpose and context causes repeated tests.\nTherefore, the semantic information is crucial in efficient testing of VPA apps. As the large language models (LLM) are known for their strong natural language understanding and processing abilities [14-17], and previous studies have found that they can be used for downstream tasks with in-context learning [18], we adopt the LLM to drive the testing process to compensate for semantic information loss during the model-based VUI testing. However, employing the LLM for the VUI testing presents the following three challenges:\nChallenge 1: LLMs can be used to supplement the semantic loss during the model-based testing of VPA apps, but it is difficult for LLMs to maintain the state information of VPA apps accurately. On the one hand, when the testing goes deeper and the context becomes larger than the LLM's limitation, the information required for LLMs to generate an accurate model is incomplete. On the other hand, LLMs can hardly generate a precious model especially when the VPA apps' behavior is complex. However, a wrong model can greatly affect the following exploration.\nChallenge 2: The results generated by LLMs can be redun-dant and repeated under VPA apps' context. For example, if the LLM is asked to generate context-related inputs for a given VPA apps' outputs (see figure 2), it tends to generate long results, but most VPA apps have difficulty processing these inputs. If state information and exploration strategy is not provided, the LLM can generate repeated inputs for the same state, affecting the testing efficiency. For these reasons, prompts should be carefully designed to help the LLM generate formalized and efficient results.\nChallenge 3: LLM's results are not entirely reliable due to its unexplainability and uncertainty. For example, even if"}, {"title": "", "content": "LLMs are prompted to return simple and concise results, they may still generate results that VPA apps cannot understand. Therefore, we need to filter out the unreliable results based on the feedback from VPA apps and our domain knowledge.\nTo address the above three challenges, we propose the following solutions.\nTo tackle Challenge 1, we split the complex LLM-driven model-based testing tasks into three phases: states extraction, input events generation, and state space exploration to increase the accuracy of model construction. In each phase, the LLM only extracts the state and generate input events for the real-time VPA apps' output, so the length of prompt will not exceed the context limitation. Besides, the LLM is only used to make up for the semantic loss during the model construction and exploration, such as merging outputs with similar semantics to one state, generating context-related inputs and selecting an input for efficient exploration, while the model information is stored and maintained locally.\nFor addressing Challenge 2, we embed the information provided by the behavior model into the prompts to help the LLM generate efficient results and avoid repeated tests. Since the complete behavior model is complex and occupies many tokens, adding it to the prompt not only interferes with the extraction of core information but also brings unnecessary ex-penses. Therefore, we only extract phase-specific information to the prompt. For example, only the state list is provided in the states extraction phase. Meanwhile, by designing appropriate few shots, we enable the LLM to formalize outputs. For the state space exploration, we implement the step-by-step chain-of-thought strategy to guide the LLM in parsing the behavior model and making decisions.\nTo handle Challenge 3, we establish specific rules consid-"}, {"title": "", "content": "ering both the behavior model information and VPA apps' feedback to check whether the LLM's outputs at each phase meet our requirements. If they do not pass the checks, we provide feedback prompts for LLMs to regenerate the results.\nBased on these ideas, we develop the Elevate (model-Enhanced Llm driven Vpa App's vui Testing) framework. As a model-based testing method, the Elevate framework is divided into three phases: states extraction, input events generation, and state space exploration. These phases are enhanced by the LLM to achieve accurate state extraction and efficient state space exploration. In the states extraction phase, the LLM is prompted to merge the VPA app's outputs with existing states in the behavior model or create a new state. In the input events generation phase, the LLM generates context-related input events based on VPA app's outputs. The states and input events generated by the LLM are used to update the behavior model. Throughout the state space exploration process, the current-state related information from the behavior model is extracted and used to guide the LLM to select an input event for efficient exploration.\nOur contributions are summarized as follows:\n\u2022\n\u2022\n\u2022\nWe propose to use the LLM to enhance the model-based testing of VPA apps. This approach combines the model guidance of MBT with the NLP capabilities of the LLM. The LLM's results are used for constructing accurate be-havior models and efficiently exploring the state space.\nWe present a specific feedback mechanism to filter the LLM's unreliable results and guide LLMs for corrections. Based on the behavior model information and VPA apps' outputs, we filter out mismatched states, invalid input events and inefficient exploration strategies.\nWe implement Elevate, and validate its coverage, efficiency, and generality. It surpassed the state-of-the-art approach Vitas in state space coverage and efficiency. Ultimately, Elevate tests 4,000 Alexa skills and covers 15% of more state space than Vitas."}, {"title": "II. BACKGROUND", "content": "A. VPA Apps and Behavior Model\nVPA apps are apps based on smart speakers. Users interact with VPA apps through voice, so the interface of VPA apps is called the voice user interface (VUI). VUIs are typically free of visible graphical interfaces. Therefore, the exchange of all information are purely through voice. While the VUI brings convenience, its invisible feature introduces a range of quality and security concerns, such as unexpected exits [10], privacy violations [3, 4], and expected apps started [5, 19]. For this reason, thoroughly exploring VPA apps' behavior while testing the VUI's quality and security issues is of paramount importance.\nHowever, VPA apps are not open source for normal testers. A VPA app is composed of the front-end interaction model and the back-end processing code. The development platform provides storage for the front-end interaction model, while the back-end code of VPA apps is stored on the developer's server.\nAs a result, dynamic testing is a commonly used method for testing the VUI of VPA apps. Since the front-end interaction model of VPA apps is designed based on implicit models [20], we propose to use the model-based testing approach to explore the behavior of VPA apps.\nVPA apps' outputs express their functionalities and pur-poses. By understanding and analyzing the outputs, states can be extracted. Apps' transfer from one state to another is only triggered by users' inputs. As a result, VPA apps' behavior can be described by the finite-state machine (FSM), which has been proved to be applicable for constructing VPA apps' behavior models [10]. A finite-state machine consists of five parts, described as $FSM = (Q, \\Sigma, \\delta, s_0, F)$. Among them:\n\u2022\n\u2022\n\u2022 Q represents the set of states. Apps' outputs are mapped to states.\n\u2211 represents the set of input events. Users' inputs are mapped to input events.\nF is the set of final states, and satisfies $F \\subset Q$. VPA apps' final outputs are mapped to final states.\n\u2022 $s_0$ is the initial state and satisfies $s_0 \\in Q$. The initial state is always set as \u201c<START>\u201d.\n\u2022 $\\delta: Q \\times \\Sigma \\rightarrow Q$ represents a transition function. The input event e that triggers the transition from the state $s_0$ to the states $s_1$ is represented as $\\delta(s_0, e) = s_1$.\nB. Large Language Model\nLarge Language Model (LLM) is built on the transformer architecture. LLMs have been proved with strong natural language processing capabilities [14-17]. Compared to gen-eral language models (LM), LLMs have a vast number of parameters and undergo extensive text training. Due to these characteristics, LLMs can be directly applied to downstream tasks. In addition, methods like fine-tuning [21] and in-context learning [18, 22] can improve LLM's capabilities for specific downstream tasks. In the in-context learning technique, users only need to provide few samples as a reference for the down-stream task, which implies that LLMs can handle downstream tasks through learning from a small dataset.\nLLMs can be categorized into three types based on the transformer architecture: encoder-only, encoder-decoder, and decoder-only. Encoder-only and encoder-decoder are suitable for infilling tasks, while decoder-only models are better at text generation tasks. Considering that our tasks involve the model generation and exploration, we prefer to adopt decoder-only models. Popular decoder-only models include OpenAI's GPT series [23, 24], Meta's Llama series [25], etc. Additionally, there are models specifically designed for code generation tasks such as Codex [26] and Codegen [27]."}, {"title": "III. LLM DRIVEN MODEL CONSTRUCTION AND EXPLORATION", "content": "A. Overview\nAs a model-based testing framework, Elevate works by constructing the model according to VPA apps' behavior and guiding the exploration based on this model. The behavior model is built by mapping VPA apps' outputs to states and"}, {"title": "", "content": "users' inputs to input events (see Section II). As states reflect VPA apps' functionalities, purposes and behavior, different outputs with similar semantics (e.g., functionalities, purposes and behavior) should be mapped to one state. We call these outputs as semantically similar outputs under the context of VPA apps' behavior. Besides, users' inputs should be context related to the apps' outputs so that meaningful states can be discovered. Overall, the states extraction and input events generation require natural language processing, which is the strength of the LLM.\nIn addition, the LLM has proved its ability in understanding graphs [24] and reasoning with prompt engineering techniques such as in-context learning and chain-of-thought [17, 18, 22]. Our state space exploration task is basically an input event selection task considering factors like historical transitions, invocation frequency and relevance to the current state based on understanding the behavior model (i.e., a graph). Given current state related information from the behavior model, the LLM can be used to select input events for further exploration of VPA apps' behavior.\nIn traditional model-based testing, the model is firstly built and then used to guide the exploration of the state space. However, when testing VPA apps, the initial model is difficult to acquire before interacting with VPA apps as the VPA apps are closed-source and most documents only provide a few lines to describe their functionalities. To solve that problem, we construct VPA apps' behavior model on-the-fly, which means the model is built during the interaction. The behavior model is finally embedded into the prompt to guide the LLM in extracting states and selecting efficient input events for exploration. To save tokens, only phase-specific behavior model information is provided.\nBased on these ideas, we propose Elevate, a model-enhanced LLM driven model-based testing method for VUI testing of VPA apps. Figure 3 shows the framework of Elevate. Elevate consists of three phases, and they are all performed by LLMs. The first two phases are for model construction, including states extraction and input events generation. In the third phase, the LLM selects an input event to explore the state space based on the information provided by the behavior model. Since we adopt an on-the-fly model construction ap-proach, these three phases are executed one by one repeatedly. The main processes of these three phases are described below.\nPhase 1: States extraction. In this phase, VPA apps' outputs and existing states in the behavior model are embedded into the prompt. The LLM decides whether to merge the VPA apps' output with existing states or generate a new state for it. We expect the LLM to map outputs with similar semantics to the same state. A state filter is used to filter out mismatched states generated by the LLM.\nPhase 2: Input events generation. The VPA apps' real-time output is input to the LLM, which generates all possible context-related input events for this output. We expect the input events generated by the LLM to be semantically related to the VPA apps' output and help discover meaningful new states. An input checker is implemented to check the validation"}, {"title": "", "content": "of input events according to VPA apps' feedback.\nPhase 3: State space exploration. The current state and current-state-related information in the behavior model are input to the LLM. The LLM is expected to select one input event by considering factors such as the invocation frequency, historical transitions and relevance to the current state to explore the state space efficiently. Based on the invocation frequency and history transitions, we search whether there is a better input in the input event set. If there is one, we reject the LLM's results and ask for another input event.\nWhenever we receive an output from VPA apps, we execute the first and second phases to generate states and input events. The states and input events are used for the behavior model construction. Subsequently, we extract information related to the current state from the behavior model and embed it to the prompt, and the LLM selects the most suitable input event at the third phase. After that, the selected input event is fed back to VPA apps and wait for the next output. The whole process will be continued until the time limit is reached or the VPA apps quit. Due to the unexplainability of the LLM, we establish the feedback mechanism to check and filter out its results. Results that do not meet our requirements are rejected, and the reasons are returned to the LLM for regenerating the results. In the following sections, we will introduce the prompts and feedback mechanisms of these three phases respectively.\nTo help express the implementation of these three phases clearly, we introduce the following terms:\n\u2022\n\u2022\n\u2022\n\u2022 <app's output>: the real-time VPA apps' output. It will be used to extract states. Context-related inputs are generated based on its content.\n<state>: the state extracted from <app's output>.\n<statepre>: the previous explored state.\n<statenext>: the next explored state.\n<inputs>: the set of context-related inputs generated for <app's output>.\n<input>: the input selected by the LLM at <state> to communicate with the VPA apps.\n<inputpre>: the previous selected input.\n<model>: the behavior model.\n<model.Q>: the set of states in the behavior model.\n<model.\u2211(s)>: the input events information of state s, including their invocation times.\n<model.d(s)>: the set of transition functions that start from state s.\nB. States Extraction\nSimilar semantics (e.g., functionalities, purposes and con-text) of VPA apps can be expressed in different ways. The LLM should merge outputs with similar semantics to one state. For each <app's output>, the LLM is supposed to find a semantic similar state from <model.Q> or generate a new state. For this reason, only the <model.Q>is required in this phase. So the input of this phase includes the <app's output> and <model.Q>.\nTo avoid redundant results, the LLM is required to only output the <state> of the given <apps' output>. To assist"}, {"title": "", "content": "the LLM in better understanding this task and formalizing its outputs, we employ the in-context learning strategy. Few shots are in the form of \u201cInput: <app's output>, <model.Q>\u201d and \u201cOutput: <state>\u201d pairs. As the LLM's results are not trustworthy, we establish a state filter to filter out mismatched states. If a state is mismatched, we provide feedback prompts to request another state from the LLM. The prompts of phase 1 are displayed in Table I.\nWhen we first use the LLM for states extraction, we use *LONG PROMPT*. In *LONG PROMPT*, we instruct the LLM to map semantically similar outputs to one states in the behavior model (labeled as *MAP INSTRUCTION*). Few shots are provided for LLMs to understand the state extraction task (labeled as *FEW SHOTS*). Subsequently, we request it to return the corresponding <state> in the <model.Q> for the <app's output>. In other cases, we will use *SHORT PROMPT*. *SHORT PROMPT* only in-cludes the <app's output> and <model.Q>. After *LONG PROMPT* or *SHORT PROMPT*, the LLM will generate the <state> for <app's output>. If <state> is rejected by the state filter, we will return *FEEDBACK PROMPT*.\nFigure 4 illustrates the state filter in the states extraction phase. Firstly, we check whether <state> \u2208 <model.Q> or <state> == <app's output>. If neither of them is true, we return *NO STATE ERROR*. Otherwise, we proceed to the second step of the check. If <state> \u2208 <model.Q>, we check whether <state> and <app's output> have the same input events (see section III-C for the generation of <inputs>). If"}, {"title": "C. Input Events Generation", "content": "In section III-A, the <state> for the <app's output> is ex-tracted. To further explore VPA apps' behavior, context related inputs should be generated. Each state has its independent context related input event set, as we consider different states as different contexts. To ensure the context relevance, the LLM"}, {"title": "D. State Space Exploration", "content": "The aim of this phase is to efficiently explore the state space based on the information provided by the behavior model. This is done by finding an input event that is most likely to discover new states (i.e., functionalities) at each state. It is a decision-making problem considering factors such as invocation fre-quency, historical transitions, and relevance to the current state based on the behavior model (essentially a graph). Due to the fact that LLMs have developed their abilities in understanding graphs [24], and prompt engineering techniques like chain-of-thought can improve the LLM's explainability and capability to handle reasoning tasks [17], the LLM is used for the state space exploration.\nIn the previous two phases, we extract the <state> and generate the <inputs> for the <apps' outputs>. They are used to update the behavior model. The model information is then used to guide the state space exploration. For this reason, the input of this step includes the <state> and the <state> related information in the <model>. The <state> related information includes the <model.d(<state>)> and the <model.\u2211(<state>)> (invocation times of each input is updated after it is sent to the app).\nTo improve the LLM's capability of this decision-making task, we employ a strategy combining in-context learning and chain-of-thought. We prompt the LLM to think step-by-step and show its thinking process. In step 1, the LLM is asked to remove the input events that lead to duplicate or wrong state from the historical transitions. In step 2, the LLM finds a never-invoked input event that is most context related. In step 3, the LLM finally chooses one input event from the never-invoked context-related input event in step2 and the invoked and valid (i.e., does not lead to a state that is same as before or represent apps' confusion) input event. Few shots are pro-vided in the form of \u201cInput: <state>, <model.d(<state>)>, <model.\u2211(<state>)>\u201d, \u201cThought: step1: xxx, step2: xxx, step3: xxx\" and \"Output: <input>\" triplets. The LLM is expected to output its thinking process along with the selected <input>. Similarly, the <input> given by the LLM will be evaluated and the feedback will be returned. The prompts in this phase are displayed in Table III.\nThe *LONG PROMPT* is used for the first time. *LONG PROMPT* initially outlines the composition and representa-tion of the behavior model (labeled as *MODEL DESCRIP-TION*). Then, it offers step-by-step guide of the reasoning process (labeled as *STEP-BY-STEP*). Meanwhile, few shots\""}, {"title": "IV. EVALUATION", "content": "We implement Elevate based on GPT-4 [24] and analyze its coverage and efficiency. The performance of Elevate is compared with the state-of-the-art model-based VUI testing method Vitas [10]. Besides, chatbot-style testers are clas-sic VPA apps testing approach, but Vitas was evaluated to outperform traditional chatbot-style testers in coverage and efficiency. However, with the development of LLMs, LLMs as chatbots may have stronger VPA apps testing abilities, so GPT4(chatbot) is also set as a baseline. Additionally, we conduct ablation experiments to assess the contribution of Elevate's each phase to the final state space coverage. We also implement Elevate on Llama2-70b-chat [28] and evaluate Elevate's applicability on different LLMs. Finally, we conduct a large-scale testing on Alexa skills to evaluate Elevate's generality [29].\nA. Settings\nDataset: We use the large scale dataset of Vitas [30] as our basic dataset. From this dataset, we filter out skills with no ratings. Then, we roughly confirm 4,000 skills with consistent behavior to form the large-scale dataset. These 4,000 skills cover all categories on the Amazon skills website. For the use of conducting an intensive evaluation, we also build a benchmark with 50 Alexa skills. These 50 skills are checked to be stable and available.\nBaselines: We compare Elevate with two baselines, as shown in table IV. The simulator provided by Amazon[31] is used as our testing platform. The evaluation was conducted on the"}, {"title": "", "content": "Ubuntu 18.04.4 machines with AMD EPYC 7702P 64-Core Processor CPU @ 1.996GHz and 4GB RAM.\nCoverage metrics: VPA apps are not open source, so the ground truth of the entire state space of certain VPA apps cannot be acquired in advance. Furthermore, as Elevate merges states with similar semantics to avoid repeated testing while Vitas does not, we call the states generated by Elevate as semantic states, while the ones discovered by Vitas as sentence states in the evaluation. Consequently, to ensure a uniform measurement, we use Elevate to process the states discovered by Vitas, and merge them to semantic states correspondingly. Then, we use the number of the unique semantic states achieved by Elevate and all the baselines used in certain eval-uations as the total state space for each evaluation respectively for a fair comparison."}, {"title": "B. Evaluation of Elevate", "content": "We aim to address the following research questions:\nRQ1: How does the semantic state coverage and efficiency improve when using GPT-4 to enhance the model construction and exploration?\nRQ2: Do all phases in Elevate contribute to the state explo-ration of VPA apps?\nRQ3: How effective is Elevate's framework when applied to other LLMs?\nRQ4: How is the coverage rate of Elevate on all types of skills compared with Vitas?\n1) Study1: Coverage and efficiency: We set the time limit as 10 minutes for Elevate to test each skill. The baselines are allowed to test skills using the same interaction rounds (an input and an output form an interaction round) as Elevate. Firstly, we compare the sentence states and semantic states achieved by Elevate and the baselines. Then, we compare their average semantic state coverage with interaction rounds.\nFigure 7 shows the sentence states and semantic states main-tained by Elevate and baselines. It suggests that the sentence states can be greatly compressed when semantic information is considered. Elevate merges outputs with similar semantics to one state for testing, which greatly reduces the original state space. In addition, Elevate achieves more sentence and semantic states than the baselines.\nIn order to evaluate Elevate's coverage ability along with the efficiency, we calculate the average semantic state coverage of Elevate and baselines on the benchmark of varying interaction rounds in figure 8. The horizontal axis represents the average"}, {"title": "", "content": "semantic state space rate, while the vertical axis denotes the number of interaction rounds. When the interactions go deeper, the advantage of Elevate over Vitas and GPT4(chatbot) is more evident. After only 3 rounds of interactions, Elevate shows its leading exploration efficiency and stays ahead until the end. Finally, Elevate can achieve over 80% of average semantic state coverage after only 20 rounds of interactions, while Vitas and GPT4(chatbot) can only achieves a final coverage of 68% and 45% respectively.\nAmong the baselines, the traditional model-based tester Vitas has relatively higher performance. However, Vitas did not exploit the semantic information during VUI testing to help the model construction and exploration, so it lags behind Elevate in terms of semantic state coverage. Although GPT-4 is a strong LLM, directly using it as a chatbot for VPA apps testing performs worse than Vitas. GPT4(chatbot) lacks the guidance for state space coverage, which prevents it from discovering deep states. Enhanced with Elevate, the LLM's performance in semantic state coverage is greatly improved."}, {"title": "", "content": "Answers to RQ1: The sentence states can be greatly reduced when semantic information is considered. Com-pared with baselines, Elevate achieves more sentence and semantic states. With the increase of interaction rounds, Elevate shows evident advantage of semantic state coverage and efficiency compared with Vitas and GPT4(chatbot).\n2) Study2: Ablation Studies: To validate the rationality of prompting the LLM and returning the feedback at each phase, we conduct an ablation study. In \"w/o States extraction\" (Section III-B), \u201cw/o Input events generation\u201d (Section III-C) and \"w/o State space exploration\u201d (Section III-D), we remove the entire *FEEDBACK PROMPT*, and the in-context learn-ing, chain-of-thought and behavior model information of the corresponding phase in the *LONG PROMPT*. We then let them test the benchmark using the same interaction rounds as Elevate and compare their performance on the average semantic state coverage rate."}, {"title": "", "content": "rate. Removing the input events generation phase has the greatest impact on the final coverage rate.\n3) Study3: Applicability: We implement Elevate on Llama2-70b-chat [28], referred to as Elevate-Llama2-70b-chat, to evaluate the performance of Elevate when it is implemented by other LLMs. As a comparison, we also use Llama2-70b-chat as a chatbot to test VPA apps, and label it as Llama2-70b-chat(chatbot). By comparing the average semantic state coverage rate of Elevate-Llama2-70b-chat, Vitas and Llama2-70b-chat(chatbot), we evaluate the applicability of Elevate. Similarly, Elevate-Llama2-70b-chat tests skills in the benchmark for 10 minutes. Then, Vitas and Llama2-70b-chat(chatbot) tests the benchmark using the same interaction rounds as Elevate-Llama2-70b-chat."}, {"title": "", "content": "Figure 9 shows the average semantic state coverage rate of Elevate, w/o States extraction, w/o Input events generation and w/o State space exploration on the benchmark. The results prove that the elimination of any phase could lead to a decrease in state space coverage. Among them, removing the Input events generation phase has the largest impact on the final coverage, as the original input events generated by the LLM are commonly misunderstood by VPA apps. Eliminating the w/o State space exploration phase also influences the performance. That is because the behavior model information and chain-of-thought strategy provides the guidance for LLMS to explore efficiently. Without the States extraction phase, the semantic state space is largely redundant, resulting in repeated tests of semantically similar states."}, {"title": "Answers to RQ2", "content": "After carrying out the ablation study on Elevate's three phases, we find that each of Elevate's three phases contribute to the overall semantic state coverage"}, {"title": "Answers to RQ3", "content": "We implement the Elevate framework on Llama2-70b-chat (e.g., Elevate-Llama2-70b-chat) and compare it with Vitas and Llama2-70b-chat(chatbot). Elevate-Llama2-70b-chat has an advantage over Vitas and Llama2-70b-chat(chatbot) in the average semantic state coverage rate. Additionally, Elevate increases Llama2- 70b-chat's coverage of VPA apps' state space by about 30%. Therefore, the Elevate framework is applicable to other LLMs."}, {"title": "4) Study4: Generality:", "content": "In the preceding studies, we eval-uate the coverage and efficiency capabilities of Elevate on the small scale benchmark. In this study, we use Elevate to test 4,000 skills in the large-scale dataset. By comparing its average coverage rate with Vitas in all categories, we evaluate its ability to test skills with various functionalities. As the cove"}, {"title": "", "content": "The total coverage is set as the union of the unique coverage achieved by Vitas and Elevate.\nThe average semantic state coverage rate with different categories compared with Vitas on the large scale dataset is shown in figure 11. The results demonstrate that Elevate can achieve over 15% of higher semantic state coverage rate in most categories compared with Vitas. It proves Elevate's ability to test skills with different behavior. Elevate is enhanced with LLMs, which are trained on massive amounts of data, enabling their abilities to handle a wide variety of VPA apps. As a comparison, Vitas is designed with fixed patterns to process all types of VPA apps. Consequently, Vitas may lack generality when applied to specific VPA apps."}, {"title": "Answers to RQ4", "content": "Compared with Vitas, Elevate demon-strates a 15% of higher semantic state coverage rate on most categories of skills. The results prove the generality of Elevate on testing various VPA apps."}, {"title": "V. DISCUSSION", "content": "A. Elevate's limitations\nElevate's limitations primarily lie in the large language model. Firstly, although the LLMs can achieve good results, their outputs are non-deterministic. Hence, the performance may vary with each test. Secondly, the thinking process of the LLM is not always accurate. As we introduce the chain-of-thought method in the third phase, the LLM will output its thinking process. While chain-of-thought can enhance coverage and efficiency, the thinking process of the LLM is not always right and we cannot confirm whether the LLM is actually thinking as we expected. Lastly, in rare cases, the LLM may not rectify the results even after multiple rounds of feedback prompts. In such instances, we consider that our feedback strategy cannot steer the LLM out of its hallucination and we resort to generate states and input events based on simple rules."}, {"title": "VI. RELATED WORK", "content": "VPA apps Testing: Several studies have been conducted to test quality, privacy or security related problems on VP apps [6, 8-10, 12, 13, 32]. SkillExplorer [6], VerHealth [9] and SkillDetective [8] are chat-bot style testers that focuses on detecting skills' privacy violation behavior. SkillExplorer and SkillDetective [8] adopt the DFS-based exploration approach. VUI-UPSET [12, 13] is a chat-bot style testing approach to generate correct paraphrases while detecting bugs. Vitas [10] uses the model-based testing to test VPA apps' problems re-lated to quality, privacy and security. Despite the improvement in coverage and efficiency, it uses simple rules to construct the model and fails to consider the semantic information. SkillScanner [32] is the first static analysis method to identify skills' policy violations at the development phase based on a dataset collected from the GitHub. Compared with them, Elevate adopts the model-based testing approach to improve"}, {"title": "", "content": "the exploration efficiency and introduces to use the LLM to supplement missing semantic information for model construc-tion and exploration.\nSecurity and Privacy of VPA apps: Increasing number of research focuses on security and privacy issues of VPA apps [33-35]. Kumar et al. proposes the skill squatting at-tack [19]. Several searches detected the weakness of the auto-matic speech recognition (ASR) system, which is vulnerable to adversarial sample attacks and out-of-band signal attacks [36-39]. Many efforts have been spent on detecting problematic privacy policies and potential privacy violating behavior [6\u2013 8, 40, 41]. Different from them, Elevate sought to thoroughly explore the VPA apps' behavior so that sufficient problems can be discovered.\nLarge Language Model for Software Testing: As a booming new technology, Large Language Models are applied to many areas, including software testing. Codet [42] uses the LLM to automatically generate test cases for evaluating the quality of a code solution. CodaMosa [43] asks Codex to generate test cases when the search based software testing method reaches the bottleneck. TitanFuzz [44] uses LLMs to generate and mutate input DL programs for fuzzing DL libraries. Its follow-up work, FuzzGPT [45], primes LLMs to synthesize bug-triggering programs for fuzzing and shows improved bug detecting performance. Other research focused on testing the GUI of mobile apps by generating context-related texts or human-like actions [46, 47]."}, {"title": "VII. CONCLUSION", "content": "In this work, we propose Elevate, a LLM driven model-based testing framework for VPA apps. Elevate uses the LLM for constructing the behavior model and exploring the state space to compensate for the loss of semantic information. It extracts states from VPA apps' outputs and generates input events to these outputs by providing few-shots to LLMs. The LLM's exploration ability is enhanced by chain-of-thought. Moreover, Elevate sets checkers to analyze the LLM's results and uses feedback prompts to ask LLMs for adjustments. Our experiments show that Elevate achieves higher coverage than the state-of-the-art tool Vitas and LLMs as chatbots in an efficient manner. Elevate tests a large-scale dataset of 4,000 Alexa skills and achieves about 15% of higher coverage rate than Vitas in all categories."}]}