{"title": "ReverseNER: A Self-Generated Example-Driven Framework for Zero-Shot Named Entity Recognition with Large Language Models", "authors": ["Anbang Wang"], "abstract": "This paper presents ReverseNER, a framework aimed at overcoming the limitations of large language\nmodels (LLMs) in zero-shot Named Entity Recognition (NER) tasks, particularly in cases where\ncertain entity types have ambiguous boundaries. ReverseNER tackles this challenge by constructing\na reliable example library with the reversed process of NER. Rather than beginning with sentences,\nthis method uses an LLM to generate entities based on their definitions and then expands them into\nfull sentences. During sentence generation, the LLM is guided to replicate the structure of a specific\n'feature sentence', extracted from the task sentences by clustering. This results in well-annotated\nsentences with clearly labeled entities, while preserving semantic and structural similarity to the task\nsentences. Once the example library is constructed, the method selects the most semantically similar\nexample labels for each task sentence to support the LLM's inference. We also propose an entity-level\nself-consistency scoring mechanism to improve NER performance with LLMs. Experiments show that\nReverseNER significantly outperforms traditional zero-shot NER with LLMs and surpasses several\nfew-shot methods, marking a notable improvement in NER for domains with limited labeled data.", "sections": [{"title": "1. Introduction", "content": "The advent of Large Language Models (LLMs) has\nmarked a significant technological leap for Natural Language\nProcessing (NLP). Owing to extensive pre-training and\nparameter optimization, LLMs such as GPT-3 [2] and BERT\n(Bidirectional Encoder Representations from Transform\ners) [6] have demonstrated exceptional performance across\nvarious NLP tasks. By capturing semantics, syntax, and\ncontextual relationships from vast corpora, these models\nhave not only achieved breakthroughs in traditional tasks\nlike machine translation [14], relation extraction [17], and\nNamed Entity Recognition (NER) [16], but also exhibited\nstrong generalization capabilities in generative tasks such as\ntext generation [12] and dialogue generation.\nDespite the impressive performance of LLMs in multiple\ntasks, they still face challenges in zero-shot NER, which\nrequires models to correctly identify entity types without\nhaving seen any previously annotated data. In contrast,\ntraditional NER methods typically rely on a large amount of\nannotated data for supervised learning [9]. Although LLMs\npossess some transfer learning capabilities, when enabling\nthem to complete certain NER tasks without annotations,\nthey still constantly struggle with complex tasks due to insuf\nficient generalization abilities and imprecise understanding\nof domain-specific entities. These shortcomings arise from\nthe model's dependence on the diversity and richness of the\ntraining corpora for recognizing specific entity contexts, and\nwhen faced with new entity types or contexts, the models\noften struggle to match the entity categories accurately.\nTo address the limitations of LLMs in zero-shot NER\ntasks, we propose a novel method named ReverseNER, which\nconstructs an example library by reversing the NER process.\nThis method first utilizes a pre-trained BERT model to\ncalculate the similarity between task sentences and clusters\nthe sentences based on this similarity. The LLM uses the\ncentral sentences of each cluster to generate similar sentences\nand construct a high-quality example library in conjunction\nwith entity types. Next, the method calculates the cosine\nsimilarity between the task sentences and the sentences in\nthe example library, selects the closest examples from the\nlibrary, and adds them to the prompt when doing actual\nNER tasks. Compared to traditional zero-shot approaches\nusing LLMs, our method provides LLMs with accurate,\npre-generated examples that are semantically similar to the\nsentences requiring recognition, significantly enhancing NER\ntask performance.\nThe contributions of this paper are as follows: (1) This\npaper proposes a method called ReverseNER for large\nmodels to perform zero-shot NER. (2) ReverseNER generates\naccurate and task-relevant examples by combining task\nsentence syntax and entity types to reverse the traditional\nNER process. (3) Evaluation results demonstrate that our\nmethod in this paper has achieved significant improvements\non three datasets."}, {"title": "2. Related work", "content": "2.1. Named Entity Recognition\nNamed Entity Recognition is a fundamental task in\nNatural Language Processing, aimed at detecting named\nentities from text and categorizing them into predefined entity\ntypes. Traditional research approaches frame the NER task\nas a sequence labeling problem. For instance, Hammerton\net al. [7] utilizes unidirectional LSTMs to obtain token-level\nrepresentations, which are then fed into a softmax classifier"}, {"title": "2.2. Applications of LLM in NER", "content": "In recent years, numerous studies have explored the\napplication of LLMs to NER tasks. One of the earliest\nadvancements is introduced by Devlin et al. [6] with the\nBERT model, where fine-tuning on specific NER datasets\nsignificantly improves performance due to BERT's ability to\ncapture bidirectional contextual information, which is crucial\nfor understanding entity boundaries in text.\nFollowing this, RoBERTa, an optimized version of BERT,\nis introduced by Liu et al. [11], which demonstrates further\nimprovements in NER by training on larger datasets with\nbetter hyperparameter tuning, dynamic masking, and longer\ntraining duration. RoBERTa outperformed BERT on several\nNER benchmarks.\nAdditionally, there has been an exploration of transform\ning NER into a generative task. Yan et al. proposed using GPT\nmodels [2] for NER by generating sentences with labeled\nentities instead of relying on the traditional sequence labeling\nformat. This shift toward a generation-based approach allows\nfor more flexible zero-shot learning settings, particularly\nuseful in scenarios with minimal labeled data [19].\nPrompt-based learning methods also gained traction\nfor NER, especially in few-shot and zero-shot scenarios.\nCui et al[4]. investigated how cloze questions and prompts\ncould be used to leverage LLMs' pretraining capabilities for\nrecognizing entities with minimal task-specific training. This\nmethod shows promise in extracting entities with little to no\nlabeled examples.\nMost recently, Xie et al[18]. introduced a novel self\nimproving framework for zero-shot NER. Their approach\nutilizes an LLM to generate self-annotated datasets from an\nunlabeled corpus. The system then selects reliable pseudo\nlabeled data through self-consistency (SC) filtering strate\ngies, and finally, this refined dataset is used for in-context\nlearning (ICL) during inference. This self-improving strategy\nsignificantly enhances zero-shot performance on NER tasks\nacross several benchmarks, including CoNLL03 [15] and\nWikiGold [1]."}, {"title": "3. Method", "content": "Our insight lies in the idea that providing reference\nexamples can significantly enhance the performance of LLMs\nin NER tasks, making it crucial to ensure that these examples\nare both accurate and precisely aligned with the specific\nrequirements and complexities of the task. To achieve this,\nwe propose the ReverseNER method, which constructs an\nexample library by reversing the conventional NER process:\nfirst generating the entities, and subsequently prompting the\nlanguage model to generate sentences that incorporate these\nentities. During this sentence generation phase, a subset of\nsentences from the task set will be provided as structural\nreferences to guide the model. This example library aims to\nfurther enhance the model's performance in subsequent NER\ntasks by offering task-specific, high-quality examples.\nThe primary workflow, shown in Figure 1, has three\nmain stages: extracting feature sentences from task sets,\nconstructing an example library with LLM, and for each task\nsentence, selecting the most semantically similar example\nlabels and incorporating them into the prompt as examples."}, {"title": "3.1. Feature Sentence Extraction", "content": "3.1.1. Sentence Embedding Using BERT\nThe BERT model is employed to map sentences from the\ntask set into high-dimensional vector representations. BERT\nis a pre-trained model based on the Transformer architecture,\nwhich effectively captures contextual information through\na bidirectional encoder, generating deep, high-dimensional\nsemantic embeddings for each sentence [6]. Compared to\ntraditional methods such as the Bag-of-Words model and\nTF-IDF, BERT captures the semantic similarity between\ntexts more effectively. Specifically, given a sentence S =\n[w\u2081, w\u2082, ..., wn] composed of words w\u2081, BERT generates the\nembedding vector of the sentence denoted as v\u2081 = f(H) =\nf(BERT(S)), where f denotes a further processing function\nof the BERT output, and H denotes the hidden state matrix\ngenerated by BERT.\nThese generated embeddings enable the clustering of\nsentences, which in turn facilitates the identification of\nrepresentative sentences that can be used to generate high-quality reference examples.\n3.1.2. Task Set Clustering\nAfter obtaining sentence embeddings with BERT, the\nK-Medoids clustering algorithm is applied for sentence clus\ntering. Unlike K-Means, K-Medoids select actual data points\nas cluster centers. We select the sentences corresponding to\nthese cluster centers as feature sentences. The specific steps\nof the K-Medoids clustering algorithm are as follows[8]:\n\u2022 Initialize Medoids: Randomly select k embeddings\nfrom the set of embedding vectors as the initial\nmedoids.\n\u2022 Assign Embeddings to Medoids: Measure the dis\ntance between sentence embeddings using cosine\nsimilarity. For each embedding v\u1d62, identify its nearest\nmedoid c\u2c7c and assign v\u1d62 to the cluster C\u2c7c corresponding\nto c\u2c7c.\n\u2022 Update Medoids: For each cluster C\u1d62, find the em\nbedding that minimizes the total distance to all other\nembeddings in the cluster and designate it as the new\nmedoid, formulated as:\nc\u1d62 = arg min\u2211d(v, v\u1d62)\nv\u2208C\u2c7c\nv\u1d62\u2208C\u2c7c\nThe use of K-Medoids facilitates the extraction of fea\nture sentences that are both representative and diverse. By\nselecting the cluster centers that capture different aspects of"}, {"title": "3.2. Example Library Construction", "content": "NER algorithms take task sentences as the input and\noutput recognized entities. However, especially in zero-shot\nscenarios, these recognized results do not qualify as gold\nlabels due to the imperfect semantic understanding of the"}, {"title": "3.2.1. Entity Vocabulary Generation", "content": "The entity types to be identified are first provided to an\nLLM, which then generates a series of specific entities based\non each type's definition. The LLM is instructed to ensure di\nversity in the generated entities. The dialogue demonstration\nof the entity vocabulary generation step is shown as follows\n(for simplicity, entity definitions are omitted):"}, {"title": "3.2.2. Example Label Generation", "content": "After obtaining these entity nouns, for each feature\nsentence extracted in Section 3.1, the LLM is employed to\ngenerate a set of new sentences according to the original\nsentence's structure and format. These newly generated\nsentences should maintain structural consistency with the\noriginal sentences and ensure the inclusion of the previously\ngenerated entity nouns. Through this method, example labels\ni.e., sentences with their entities labeled, are produced,\nultimately constructing a fully annotated example library to\nsupport subsequent NER tasks. The dialogue demonstration\nof the entity vocabulary generation step is shown as follows:"}, {"title": "3.3. Example-Driven Inference", "content": "3.3.1. Example Labels Selection\nTo identify the most relevant example labels for each\ntask sentence, a similarity matrix must be calculated between\nthe sentences in the example library and those in the task\nset. To achieve this, we first use the BERT model to map\neach sentence in the example library into high-dimensional\nembedding vectors. Then, for each sentence in the task set,\nthe cosine similarity between it and every sentence in the\nexample library is calculated, resulting in a similarity matrix\nof size N X M, where N denotes the number of sentences in\nthe task set, and M denotes the number of sentences in the\nexample library.\nBased on the similarity matrix, we select the most relevant\nexample labels to include in the prompt provided to the LLM\nduring entity recognition.\n3.3.2. Inference with LLMs\nThe constructed prompt to perform the NER task on a\ncertain task sentence includes the following four elements:\nthe entity type set to be recognized, definitions of each entity\ntype, selected example labels, and the task sentence itself.\nThe dialogue demonstration of inference is shown as follows:"}, {"title": "3.4. Entity-Level Self-Consistency Scoring", "content": "To mitigate the impact of LLM hallucinations, we per\nform multiple reasoning attempt rounds on the same sentence\nto be recognized, and introduce the self-consistency (SC)\nscoring mechanism. Different from the traditional Response-Level SC methods [?], we propose the Entity-Level SC\nScoring method.\nThe occurrences of each inferred entity type across all\nreasoning attempts are counted, which is used as the self-consistency (SC) score for each entity. For each reasoning\nattempt, the score is calculated as the average SC score of all\nentities inferred in that response. The average SC score for\neach reasoning is formulated as:\nSCaverage = 1/n\u2211SC(e\u1d62)\ni=1\nwhere n denotes the number of entities in the reasoning\nattempt, and SC(e\u1d62) denotes the self-consistency score of\nentity e\u1d62, which is the total number of times entity e\u1d62 appears\nacross all rounds of attempts.\nAdditionally, to handle cases where the correct output\ndoes not contain any entities, a majority-based rule is\nintroduced: if more than half of the predictions indicate no\nentity, the final output is set to \"no entity.\"\nFor each sentence in the task set, the self-consistency\nscoring method is applied during testing, and the result with\nthe highest self-consistency score is selected as the final\noutput."}, {"title": "4. Experiments", "content": "In this section, we experiment on four public benchmarks\nto evaluate the ReverseNER method in comparison to multi\nple baselines."}, {"title": "4.1. Setup", "content": "LLM We utilize the GPT-40 mini model as the LLM to\ngenerate examples and perform the NER task to evaluate our\nalgorithm. During the self-consistency scoring process, the\ntemperature parameter is set to 0.8, and the prediction process\nis repeated five times.\nExample Library Generation For English texts, we uti\nlize the BERT-base-uncased model [6] to derive sentence\nembeddings, while for Chinese texts, we use the BERT-base-Chinese model [6]. Sentences from the input set are\nclustered into 10 categories, and for each cluster's central\nsentence i.e., the feature sentences, the LLM generates 3\nexamples by mimicking the sentence structure. Thus, these\n30 example labels constitute the entire example library for a\ncertain dataset.\nDatasets We use the following four datasets to evaluate the\nperformance of the NER task. For English texts, we utilize\nthe CoNLL03 dataset [15] and WikiGold dataset [1]. For\nChinese texts, we use the People's Daily dataset [5] and have\nconstructed a novel dataset called GovAff, which consists\nof sentences pertaining to routine administrative affairs. The\nGovAff dataset encompasses four types of entities: person,\ngovernment agency, corporation, and miscellaneous.\nBaselines and Comparison The performance achieved\nwith the vanilla model, where no examples are generated\nor provided, and only the task sentence and entity types\nare provided to the model, is recorded as the baseline.\nAdditionally, we conduct two few-shot NER experiments\nusing 10 examples with gold labels and compare their results\nwith the proposed method in this study."}, {"title": "4.2. Overall Performance", "content": "The results presented in Table 1 underscore the F1 score\nof the zero-shot ReverseNER method when compared to\nother methods. Notably, the zero-shot ReverseNER with self-consistency scoring attains the highest average F1 score of\n79.10 across the four evaluated datasets, outperforming both\nthe Vanilla zero-shot and few-shot baselines.\nRelative to the Vanilla zero-shot baseline, the zero-shot\nReverseNER with SC demonstrates a substantial enhance\nment in the average F1 score (79.10 vs. 71.22), reflecting a\nsignificant improvement in the model's capacity to generalize\nto unseen data without the need for additional gold labels\nprovided in advance. Furthermore, in comparison to the\nfew-shot methods, both with and without SC, the zero-shot ReverseNER with SC exhibits superior performance,\nsurpassing the few-shot SC approach (77.25 average) by 1.85\npoints."}, {"title": "4.3. Analysis", "content": "4.3.1. Detailed Performance\nTaking the WikiGold dataset as a case study, we conduct\na detailed analysis of the recognition accuracy for each\nlabel. Results are shown in Table 2. The WikiGold dataset\ncomprises four types of entities: PER, ORG, LOC, and MISC,\ndenoting persons, organizations, locations, and miscellaneous\nentities, respectively. Notably, the MISC label is more\nambiguous in definition, covering entities that do not fall\ninto the aforementioned categories but still qualify as proper\nnouns. Consequently, the recognition accuracy for this label\ntends to be suboptimal.\nBy examining the Precision, Recall, and F1 scores of\neach label across different methods, we observe that the\nReverseNER with SC voting consistently achieves the highest\nperformance in terms of Precision and F1 scores for most\nlabels, followed by the ReverseNER without SC voting. The\nVanilla method, however, frequently achieves higher Recall,\nparticularly for PER and LOC types. This phenomenon can\nbe attributed to two primary factors: (1) The conservative\nstrategy employed by the LLM when generating examples,\nwhere uncertain terms that may not belong to the current\nentity label are often excluded, leads to higher Precision\nbut potentially lower Recall; (2) The SC voting algorithm\ntends to penalize outputs containing entities that are not\nconsistently present across all outputs, even if they are correct\nin some cases, which further boosts Precision while reducing"}, {"title": "4.3.2. Impact of hyperparameters", "content": "Our ReverseNER method involves two hyperparame\nters: the number of clusters used during feature sentence\nextraction and the number of example labels that the LLM\nneeds to construct per cluster. The product of these two\nhyperparameters determines the total number of labels in\nthe example library. To evaluate the algorithm's stability\nunder different parameter settings, we do not enable the self-consistency scoring mechanism and run each hyperparameter\ncombination 20 times, calculating the mean and standard\ndeviation.\nTaking the WikiGold dataset as an example, Table 3\nillustrates the performance for each set of hyperparameter\ncombinations. From the table, we can observe that keeping\nthe total number of labels in the example library constant\nwhile increasing the number of clusters and decreasing the\nnumber of labels constructed per cluster leads to decreased\nstandard deviation, standing for increased stability. However,\nfor the F1 score, there is a peak point; either increasing or\ndecreasing the number of clusters beyond this point negatively\nimpacts the results.\nThe reason for this phenomenon may be that when\nthe number of clusters is small, more labels need to be\nconstructed for each cluster, resulting in a large number\nof labels with similar sentence structures in the example"}, {"title": "4.3.3. Effect of Entity-Level Self-Consistency Scoring", "content": "The comparison presented in Table 4 elucidates the perfor\nmance differences between the Entity-Level Self-Consistency\n(SC) and the traditional Response-Level SC methods, which\noperate by selecting the most frequently occurring result from\nmultiple iterations, across various datasets, evaluated using\nthe F1 score metric. On the CoNLL03, WikiGold, and GovAff\ndatasets, the Entity-Level SC method performs slightly\nbetter than the Response-Level SC method. In contrast, the\nResponse-Level SC method outperforms the Entity-Level SC\non the People's Daily dataset.\nThis isolated underperformance for the Entity-Level SC\ncan be attributed to the fact that there are only three clearly\ndefined types of entities in the People's Daily Dataset: PER, LOC, and ORG, without the MISC type in other datasets,"}, {"title": "5. Conclusion", "content": "In conclusion, ReverseNER provides a sophisticated\nmethodology for addressing zero-shot NER by leveraging\nself-generated examples within an example-driven frame\nwork. By constructing a labeled example library and em\nploying self-consistency scoring, ReverseNER effectively\nmitigates the challenges associated with NER in the absence"}, {"title": "6. Limitations", "content": "We acknowledge the following limitations of this study:\n\u2022 The proposed method may lead to a slight decline in\nRecall due to trade-offs between Precision and Recall.\n\u2022 The self-consistency scoring mechanism increases the\nvolume of reasoning attempts, which may escalate\nruntime and incur additional costs, raising scalability\nconcerns in resource-limited settings."}, {"title": "7. Acknowledgment", "content": "This work is funded by the Xi'an Jiaotong University-China Mobile Communications Group Co., Ltd. Digital\nGovernment Joint Institute."}, {"title": "A. Actual Prompts for Constructing the Example Library", "content": "Taking the WikiGold dataset as an example, the prompt\nused for generating entity vocabularies of this dataset is\nillustrated in Listing A1. We repeat this prompt when\nconstructing example labels for each cluster.\nThe prompt for constructing example labels in the exam\nple library is shown in Listing A2. The entity vocabularies\nit uses are derived from the results of the corresponding\nentity vocabulary generation prompt. The reference sentence\nprovided at the end corresponds to the sentence located at the\ncenter of the respective cluster. We repeat this prompt when\nconstructing example labels for each cluster."}, {"title": "B. Actual Prompts for Performing NER on Each Task Input", "content": "The prompt used for inferring a specific input sentence\nfrom the WikiGold dataset is shown in Listing B3. The\nprovided examples are selected from the constructed example\nlibrary based on cosine similarity."}]}