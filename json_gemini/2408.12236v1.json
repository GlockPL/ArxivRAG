{"title": "MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for Dynamic Medical Image Generation in Virtual Simulated Patient", "authors": ["Yanzeng Li", "Cheng Zeng", "Jinchao Zhang", "Jie Zhou", "Lei Zou"], "abstract": "Medical education relies heavily on Simulated Patients (SPs) to provide a safe environment for students to practice clinical skills, including medical image analysis. However, the high cost of recruiting qualified SPs and the lack of diverse medical imaging datasets have presented significant challenges. To address these issues, this paper introduces MedDiT, a novel knowledge-controlled conversational framework that can dynamically generate plausible medical images aligned with simulated patient symptoms, enabling diverse diagnostic skill training. Specifically, MedDiT integrates various patient Knowledge Graphs (KGs), which describe the attributes and symptoms of patients, to dynamically prompt Large Language Models' (LLMs) behavior and control the patient characteristics, mitigating hallucination during medical conversation. Additionally, a well-tuned Diffusion Transformer (DiT) model is incorporated to generate medical images according to the specified patient attributes in the KG. In this paper, we present the capabilities of MedDiT through a practical demonstration, showcasing its ability to act in diverse simulated patient cases and generate the corresponding medical images. This can provide an abundant and interactive learning experience for students, advancing medical education by offering an immersive simulation platform for future healthcare professionals. The work sheds light on the feasibility of incorporating advanced technologies like LLM, KG, and DiT in education applications, highlighting their potential to address the challenges faced in simulated patient-based medical education.", "sections": [{"title": "1 Introduction", "content": "Medical education plays a crucial role in preparing future healthcare professionals, relying extensively on Simulated Patients (SPs) to provide a safe and controlled environment for practicing clinical skills (Gaba 2007; Ziv et al. 2006; Sanko et al. 2013; Mesquita et al. 2010). However, the traditional use of SPs presents significant challenges, primarily due to the high costs associated with recruiting and training qualified individuals (Hillier, Williams, and Chidume 2020; Felix and Simon 2019). While the development of Large Language Models (LLMs) offers the potential to build practical virtual SPs (VSPs) (Chen et al. 2023; Ben\u00edtez et al. 2024; Holderried et al. 2024; Li et al. 2024a), the scarcity of diverse and comprehensive medical imaging datasets complicates the ability of VSPs to provide varied and realistic training scenarios (Glatard et al. 2012; Baraheem, Le, and Nguyen 2023; Wang et al. 2021).\nTo address these limitations, we introduce MedDiT, a novel VSP framework designed to enhance the educational experience. The core of MedDiT's functionality is the integration of patient Knowledge Graphs (KGs) (Fensel et al. 2020; Gyrard et al. 2018). These KGs meticulously describe patient attributes and symptoms, serving as a foundation to guide the behavior of LLMs. By dynamically prompting LLM behavior, MedDiT ensures that patient characteristics are accurately represented, effectively mitigating issues such as hallucination during medical conversations (Li et al. 2024a). In addition to its conversational capabilities, MedDiT incorporates a series of well-tuned Diffusion Transformer (DiT) models (Yang et al. 2023; Pan et al. 2023). These models can generate medical images that correspond to the specified patient attributes within the KGs, providing a realistic and varied set of scenarios for students to engage with.\nEssentially, MedDiT is a multi-agent system that integrates KG agent, chat agent, and image generation agent. This integration is centered around KG data, facilitating interaction and enabling the system to dynamically generate medical images that align with simulated patient symptoms. Through a practical demonstration, we showcase MedDiT's ability to simulate diverse patient cases and generate the corresponding medical images. This not only enhances the learning experience by offering an abundant and interactive platform but also advances medical education by providing an immersive simulation environment."}, {"title": "2 System Design & Architecture", "content": "We developed MedDiT with a focus on modularity and configurability. The KG, chat, and multimodal components are constructed as microservices, while a unified LLM server is utilized to support the entire prompt workflow. In practice, we employ the Qwen2 72B instruction-tuned version (Yang et al. 2024) as our backbone LLM for building all the agents. The overview of MedDiT is illustrated in Figure 1. Figure 2 presents demonstration of MedDiT."}, {"title": "KG-enhanced LLM-based VSP", "content": "To construct a controllable and less-hallucinatory VSP system, we introduce the patient KG as the core information source as Li et al. (2024a). A KG agent is applied to retrieve the conversation-related subgraph by discerning the user's intention and generating a SPARQL query for extracting related entities, relations, and attributes in KG. Subsequently, the extracted subgraph is linearized into natural language to serve as role-setting for prompting the chat agent's responses. In this manner, MedDiT maintains dialogue consistency effectively, reduces the hallucinations, and saves token costs."}, {"title": "KG-controlled DiT Model", "content": "We utilize HunyuanDiT (Li et al. 2024b) as our backbone generative model and train a LoRA adaptor for the transfer model to adapt to our target domain (Hu et al. 2021). In this demonstration, we selected a subset of Chest X-ray images from the Open-i dataset (Demner-Fushman et al. 2016) for training, comprising 3,314 images along with their corresponding textual descriptions. Detailed training parameters are provided in Appendix B. Subsequently, we construct an agent to generate image prompts from the structured manifestations, which are stored in the KG and retrievable by KG agent. Once generated, the images are displayed in dialogue flow for students' further analysis."}, {"title": "SP Evaluation", "content": "As a VSP system, it is crucial to assess the dialogue history between the SP and students, score the interactions, and provide feedback to help students enhance their medical conversation skills through iterative practice. Following the instructions from Li et al. (2024a), we utilize various prompts and auxiliary models to analysis the dialogue history across multiple aspects, including the completeness of necessary information, thoroughness of symptom inquiry, and the emotions conveyed to the patient. Finally, a detailed evaluation report, including scoring and advice, is generated by the LLM based on these criteria and the gold standard for the corresponding VSP case. Appendix C presents an actual evaluation report."}, {"title": "3 Conclusion and Future Work", "content": "In conclusion, MedDiT provides a comprehensive and practical solution for building VSP systems that can dynamically generate medical images. By developing a multi-agent system based on LLMs, MedDiT has successfully integrated KGs to control the chat flow and utilized DiT models to produce medical images that align with the context and symptoms of patients. The system's ability to generate diverse and realistic medical images, combined with its interactive conversational capabilities, offers an abundant learning experience that can significantly improve medical education. In future work, we plan to further expand the scope of the KG to encompass a broader range of medical conditions, symptoms, and patient profiles. We will also increase the number of SP cases and explore training more kinds of medical image adaptors. Furthermore, we aim to conduct evaluation experiments on MedDiT to test large vision models and investigate the possibility of using large multi-modal models for comprehensive diagnosis."}, {"title": "A Knowledge Controlled Image Generation", "content": "In this study, we propose a framework for generating knowledge-driven imagery from structured KGs, delineated through following processes:\nA KG can be denoted as $G = (E, R, T)$, where E represents entities, R is the set of relations, and $T \\subset E\\times R\\times E$ is the collection of knowledge triples (s, p, o), with $s, o \\in E,p\\in R$ being subjects, objects, and predicates, respectively. We define a function that represents the knowledge"}]}