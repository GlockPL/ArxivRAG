{"title": "MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for\nDynamic Medical Image Generation in Virtual Simulated Patient", "authors": ["Yanzeng Li", "Cheng Zeng", "Jinchao Zhang", "Jie Zhou", "Lei Zou"], "abstract": "Medical education relies heavily on Simulated Patients (SPs)\nto provide a safe environment for students to practice clinical\nskills, including medical image analysis. However, the high\ncost of recruiting qualified SPs and the lack of diverse med-\nical imaging datasets have presented significant challenges.\nTo address these issues, this paper introduces MedDiT, a\nnovel knowledge-controlled conversational framework that\ncan dynamically generate plausible medical images aligned\nwith simulated patient symptoms, enabling diverse diagnos-\ntic skill training. Specifically, MedDiT integrates various pa-\ntient Knowledge Graphs (KGs), which describe the attributes\nand symptoms of patients, to dynamically prompt Large Lan-\nguage Models' (LLMs) behavior and control the patient char-\nacteristics, mitigating hallucination during medical conversa-\ntion. Additionally, a well-tuned Diffusion Transformer (DiT)\nmodel is incorporated to generate medical images according\nto the specified patient attributes in the KG. In this paper,\nwe present the capabilities of MedDiT through a practical\ndemonstration, showcasing its ability to act in diverse sim-\nulated patient cases and generate the corresponding medical\nimages. This can provide an abundant and interactive learn-\ning experience for students, advancing medical education by\noffering an immersive simulation platform for future health-\ncare professionals. The work sheds light on the feasibility of\nincorporating advanced technologies like LLM, KG, and DiT\nin education applications, highlighting their potential to ad-\ndress the challenges faced in simulated patient-based medical\neducation.", "sections": [{"title": "1 Introduction", "content": "Medical education plays a crucial role in preparing future\nhealthcare professionals, relying extensively on Simulated\nPatients (SPs) to provide a safe and controlled environment\nfor practicing clinical skills (Gaba 2007; Ziv et al. 2006;\nSanko et al. 2013; Mesquita et al. 2010). However, the tra-\nditional use of SPs presents significant challenges, primarily\ndue to the high costs associated with recruiting and training\nqualified individuals (Hillier, Williams, and Chidume 2020;\nFelix and Simon 2019). While the development of Large\nLanguage Models (LLMs) offers the potential to build prac-\ntical virtual SPs (VSPs) (Chen et al. 2023; Ben\u00edtez et al.\n2024; Holderried et al. 2024; Li et al. 2024a), the scarcity of\ndiverse and comprehensive medical imaging datasets com-\nplicates the ability of VSPs to provide varied and realistic\ntraining scenarios (Glatard et al. 2012; Baraheem, Le, and\nNguyen 2023; Wang et al. 2021).\nTo address these limitations, we introduce MedDiT, a\nnovel VSP framework designed to enhance the educational\nexperience. The core of MedDiT's functionality is the inte-\ngration of patient Knowledge Graphs (KGs) (Fensel et al.\n2020; Gyrard et al. 2018). These KGs meticulously describe\npatient attributes and symptoms, serving as a foundation\nto guide the behavior of LLMs. By dynamically prompt-\ning LLM behavior, MedDiT ensures that patient character-\nistics are accurately represented, effectively mitigating is-\nsues such as hallucination during medical conversations (Li\net al. 2024a). In addition to its conversational capabilities,\nMedDiT incorporates a series of well-tuned Diffusion Trans-\nformer (DiT) models (Yang et al. 2023; Pan et al. 2023).\nThese models can generate medical images that correspond\nto the specified patient attributes within the KGs, providing\na realistic and varied set of scenarios for students to engage\nwith.\nEssentially, MedDiT is a multi-agent system that inte-\ngrates KG agent, chat agent, and image generation agent.\nThis integration is centered around KG data, facilitating in-\nteraction and enabling the system to dynamically generate\nmedical images that align with simulated patient symptoms.\nThrough a practical demonstration, we showcase MedDiT's\nability to simulate diverse patient cases and generate the\ncorresponding medical images. This not only enhances the\nlearning experience by offering an abundant and interactive\nplatform but also advances medical education by providing\nan immersive simulation environment."}, {"title": "2 System Design & Architecture", "content": "We developed MedDiT with a focus on modularity and con-\nfigurability. The KG, chat, and multimodal components are\nconstructed as microservices, while a unified LLM server is\nutilized to support the entire prompt workflow. In practice,\nwe employ the Qwen2 72B instruction-tuned version (Yang\net al. 2024)\u00b9 as our backbone LLM for building all the\nagents. The overview of MedDiT is illustrated in Figure 1.\nFigure 2 presents demonstration of MedDiT."}, {"title": "KG-enhanced LLM-based VSP", "content": "To construct a control-\nlable and less-hallucinatory VSP system, we introduce the\npatient KG as the core information source as Li et al.\n(2024a). A KG agent is applied to retrieve the conversation-\nrelated subgraph by discerning the user's intention and gen-\nerating a SPARQL query for extracting related entities, rela-\ntions, and attributes in KG. Subsequently, the extracted sub-\ngraph is linearized into natural language to serve as role-\nsetting for prompting the chat agent's responses. In this man-\nner, MedDiT maintains dialogue consistency effectively, re-\nduces the hallucinations, and saves token costs."}, {"title": "KG-controlled DiT Model", "content": "We utilize HunyuanDiT (Li\net al. 2024b) as our backbone generative model and train\na LoRA adaptor for the transfer model to adapt to our tar-\nget domain (Hu et al. 2021). In this demonstration, we se-\nlected a subset of Chest X-ray images from the Open-i\ndataset (Demner-Fushman et al. 2016) for training, compris-\ning 3,314 images along with their corresponding textual de-\nscriptions. Detailed training parameters are provided in Ap-\npendix B. Subsequently, we construct an agent to generate\nimage prompts from the structured manifestations, which\nare stored in the KG and retrievable by KG agent. Once\ngenerated, the images are displayed in dialogue flow for stu-\ndents' further analysis."}, {"title": "SP Evaluation", "content": "As a VSP system, it is crucial to assess the\ndialogue history between the SP and students, score the in-\nteractions, and provide feedback to help students enhance\ntheir medical conversation skills through iterative practice.\nFollowing the instructions from Li et al. (2024a), we uti-\nlize various prompts and auxiliary models to analysis the\ndialogue history across multiple aspects, including the com-\npleteness of necessary information, thoroughness of symp-\ntom inquiry, and the emotions conveyed to the patient. Fi-\nnally, a detailed evaluation report, including scoring and ad-\nvice, is generated by the LLM based on these criteria and the\ngold standard for the corresponding VSP case. Appendix C\npresents an actual evaluation report."}, {"title": "3 Conclusion and Future Work", "content": "In conclusion, MedDiT provides a comprehensive and prac-\ntical solution for building VSP systems that can dynamically\ngenerate medical images. By developing a multi-agent sys-\ntem based on LLMs, MedDiT has successfully integrated\nKGs to control the chat flow and utilized DiT models to pro-\nduce medical images that align with the context and symp-\ntoms of patients. The system's ability to generate diverse and\nrealistic medical images, combined with its interactive con-\nversational capabilities, offers an abundant learning expe-\nrience that can significantly improve medical education. In\nfuture work, we plan to further expand the scope of the KG\nto encompass a broader range of medical conditions, symp-\ntoms, and patient profiles. We will also increase the num-\nber of SP cases and explore training more kinds of medical\nimage adaptors. Furthermore, we aim to conduct evaluation\nexperiments on MedDiT to test large vision models and in-\nvestigate the possibility of using large multi-modal models\nfor comprehensive diagnosis."}, {"title": "A Knowledge Controlled Image Generation", "content": "In this study, we propose a framework for generating\nknowledge-driven imagery from structured KGs, delineated\nthrough following processes:\nA KG can be denoted as $G = (E, R, T)$, where $E$ repre-\nsents entities, $R$ is the set of relations, and $T \\subset E\\times R\\times E$\nis the collection of knowledge triples $(s, p, o)$, with $s, o \\in$\n$E,p\\in R$ being subjects, objects, and predicates, respec-\ntively. We define a function that represents the knowledge\nimplied by arguments as:\nKnowledge: $X \\rightarrow K$,\nwhere X can be a graph, text, or images, and K is the set of\nknowledge representations, implying that knowledge can be\nrepresented by (s, p, o).\nInitially, we perform subgraph retrievel, where a sub-\ngraph $G' = (E', R', T')$ is derived from the original patient\ngraph $G = (E, R,T)$, ensuring that $E' \\subseteq E, R' \\subseteq R,$\n$T' \\subseteq T$, and $Knowledge(G') \\leq Knowledge(G)$. This sub-\ngraph serves as the foundational data structure for subse-\nquent operations. The retrieved subgraph $G'$ can be repre-\nsented by:\n$f : G' \\rightarrow Text$, s.t. $Knowledge(Text) \\approx Knowledge(G')$, which maps the subgraph to a textual representation while\npreserving the inherent knowledge, such that the knowledge\ncontained in the text equals that of the subgraph. In practice,\nwe employ a strong LLM as the f for encoding subgraph\nto text as losslessly as possible. Subsequently, this textual\nrepresentation $IP = f(G')$ is utilized as an image prompt\nwithin the DiT model, denoted as\n$I = DiT(IP)$,\nand the total progress can be denoted as:\n$g: Text \\rightarrow I$, s.t. $I = Knowledge(G')$, ensuring that the generated image I accurately reflects the\nknowledge embedded in $G'$. This comprehensive approach\ng describes the integration of graph-based data with text and\nimage modalities, fostering enhanced interpretability and\ncontrol in image synthesis."}, {"title": "B Hyperparameter Settings", "content": "The hyperparameters used to train the DiT model for\ngenerating responses during medical conversations are dis-\nplayed in Table 1. The unmentioned parameters are aligned\nwith the configuration of Li et al. (2024b)."}, {"title": "C Assessment Example", "content": "In Listing 1, we present an example of a generated assess-\nment designed to evaluate a student's performance. MedDiT\nconcludes with a comprehensive score and a letter of advice\nto guide the student in their future practice."}]}