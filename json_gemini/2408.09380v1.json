{"title": "ELASTIC: Efficient Linear Attention for Sequential Interest Compression", "authors": ["Jiaxin Deng", "Shiyao Wang", "Song Lu", "Yinfeng Li", "Xinchen Luo", "Yuanjun Liu", "Peixing Xu", "Guorui Zhou"], "abstract": "State-of-the-art sequential recommendation models heavily rely on transformer's attention mechanism. However, the quadratic computational and memory complexities of self attention have limited its scalability for modeling users' long range behaviour sequences. To address this problem, we propose ELASTIC, an Efficient Linear Attention for SequenTial Interest Compression, requiring only linear time complexity and decoupling model capacity from computational cost. Specifically, ELASTIC introduces a fixed length interest experts with linear dispatcher attention mechanism which compresses the long-term behaviour sequences to a significantly more compact representation which reduces up to 90% GPU memory usage with \u00d72.7 inference speed up. The proposed linear dispatcher attention mechanism significantly reduces the quadratic complexity and makes the model feasible for adequately modeling extremely long sequences. Moreover, in order to retain the capacity for modeling various user interests, ELASTIC initializes a vast learnable interest memory bank and sparsely retrieves compressed user's interests from the memory with a negligible computational overhead. The proposed interest memory retrieval technique significantly expands the cardinality of available interest space while keeping the same computational cost, thereby striking a trade-off between recommendation accuracy and efficiency. To validate the effectiveness of our proposed ELASTIC, we conduct extensive experiments on various public datasets and compare it with several strong sequential recommenders. Experimental results demonstrate that ELASTIC consistently outperforms baselines by a significant margin and also highlight the computational efficiency of ELASTIC when modeling long sequences. We will make our implementation code publicly available.", "sections": [{"title": "Introduction", "content": "Sequential recommender systems (SRs) have played a vital role in online content-sharing platforms and e-commerce, such as Kuaishou (Chang et al. 2023; Si et al. 2024) and Taobao (Zhou et al. 2018, 2019; Pi et al. 2020), which capture user's actual preferences from the long-term action history (e.g. click, view and comment) to predict next action."}, {"title": "Related Work", "content": "Transformer-based SRs\nIn recent years, the Transformer architecture (Vaswani et al. 2017; Hou et al. 2022; Kang and McAuley 2018; Sun et al. 2019; Wu et al. 2020; Zhang et al. 2019) has significantly advanced the field of sequential recommendation systems by leveraging its superior ability to model long-range dependencies and capture user-item interaction patterns effectively. The key component of transformer is the self attention mechanism which computes the corresponding attention matrix for distinguishing items' importance by a dot-product operation between the query and key matrices. Specifically, ATTRec (Zhang et al. 2018) leverages the self-attention mechanism to capture both short-term item-item interactions and long-term user-item relationships. SASRec (Kang and McAuley 2018) employs multi-head self-attention mechanisms to effectively capture complex and dynamic user preferences, allowing for more accurate modeling of user behavior over time. BERT4Rec (Sun et al. 2019) adapts the bidirectional self-attention mechanism from Transformers, enabling a more comprehensive understanding of user preferences by capturing transition patterns from both left and right contexts in the sequence, overcoming limitations of unidirectional models. FDSA (Zhang et al. 2019) incorporates feature-level self-attention mechanisms to generate accurate and context-aware sequential representation. While these methods have proved to be effective, they primarily rely on traditional dot-product attention mechanisms which can lead to computational inefficiencies when scaling up to long-term user behviour sequence.\nEfficient Transformers\nRecently, various \u201cX-former\u201d models have been proposed for improving the computational and memory efficiency of original Transformer architecture. The earliest attempts design sparse attention score matrix by limiting the receptive field to fixed patterns such as local attention (Parmar et al. 2018; Qiu et al. 2019) and strided attention (Beltagy, Peters, and Cohan 2020; Child et al. 2019). Another line of research designs the learnable patterns in a data-driven manner. For example, Reformer (Kitaev, Kaiser, and Levskaya 2020) designs the reversible residual layers and locality-sensitive hashing (Andoni et al. 2015) attention, decreasing computational complexity from O(N2) to O(NlogN). And Routing Transformer (Roy et al. 2021) introduces an efficient content-based sparse attention mechanism using online k-means clustering. Subsequently, low-rank approximation methods emerged as a solution by assuming low-rank structure of original attention matrix. Linformer (Wang et al. 2020) projects the keys and values to a low-rank dimension using the additional projection layers. Linear Transformers (Katharopoulos et al. 2020) rewrite dot product attention as a linear dot product of kernel feature maps, enabling recurrent computation, and faster autoregressive inference. Besides, recent works implement linear attention mechanism (Liu et al. 2023) to estimate original dot-product attention or leverage linear recurrent units (Yue et al. 2024) to address the dilemma of training and inference efficiency in long-term sequential recommendation scenarios. Despite existing methods being proved to be efficient in computational complexity, they may sacrifice recommendation accuracy and stability when compared with SARs methods with regular attention. In contrast, our proposed ELASTIC decouples"}, {"title": "Method", "content": "This section introduces our proposed novel ELASTIC framework for sequential recommendation systems. We begin by defining the problem formulation for sequential recommendation tasks. Then, we introduce the efficient Linear Dispatcher Attention (LDA) layer with a novel dispatcher mechanism. Next, we present proposed Interest Memory Retrieval (IMR) technique for sparsely retrieving compressed user's interests representation. We will discuss the optimization process of the model and present the pseudo-code.\nProblem Statement\nGiven a set of user $U = \\{u_1, u_2,..., u_u\\}$ and a set of item $X = \\{x_1,x_2,\\cdots,x_{\\vert x\\vert}\\}$, we represent $u_i$'s the historical item interaction list as $X = \\{x_1,\\cdots, x_t,\\cdots,x_N\\} \\in R^{N\\times d}$, including the item's ID embeddings and positional embeddings with the dimension of d, where $x_t$ is the t-th item interacted by user $u_i$ and N represents the maximum historical window length. Sequential recommendation systems aim to predict a user's next item selection based on their interaction history. Our proposed ELASTIC takes the user's past N interactions as input and predicts the possible items for the N + 1 time step based on the generated a list of top-k items from the available set $X$.\nLinear Dispatcher Attention Layer\nAs a critical part of transformers, the idea of dot-product attention is to compute the pairwise item-to-item relationship at every input token. The standard self-attention mechanism directly applied to the user behviour sequences $X^i$ in i-th layer can be defined as:\n$X^{i+1} = Attention(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d}})V$ (1)\nwith the query matrix $Q = X^iW_Q \\in R^{N\\times d}$, the key matrix $K = X^iW_K \\in R^{N\\times d}$ and the value matrix $V = X^iW_V \\in R^{N\\times d}$. The self attention helps the model to capture intra-dependencies among all historical tokens. However, the original self attention mechanism results in an attention map with the memory and complexity of O(N2), which is very costly when the input sequences have a large number of N.\nTo address the potential complexity caused by a large number of tokens N, we introduce a dispatcher mechanism which aggregates and dispatches sequence token dependencies more efficiently which adopts the k(k < N) learnable user interest tokens as aggregation points. We first aggregate the information from all sequence tokens by using the dispatcher embeddings $P^i$ from i-th layer as the query and the original sequence token embeddings as key and value:\n$P^{i+1} = Attention(P^iW_{Q_1}, X^iW_{K_1}, X^iW_{V_1}) = Softmax(\\frac{P^iW_{Q_1}(X^iW_{K_1})^T}{\\sqrt{d}})X^iW_{V_1}$ (2)\nwhere the complexity is O(Nk). Then the dispatchers distribute the dependencies relationship to all sequence tokens by setting the original sequence token embedding as the query and the aggregated interest embeddings $P^{i+1}$ as the key and value:\n$X^{i+1} = Attention(X^iW_{Q_2}, P^{i+1}W_{K_2}, P^{i+1}W_{V_2}) = Softmax(\\frac{X^iW_{Q_2}(P^{i+1}W_{K_2})^T}{\\sqrt{d}})P^{i+1}W_{V_2}$ (3)\nwhere the complexity is also O(Nk). Therefore, the proposed dispatcher mechanism achieves an overall computational complexity of O(Nk), which is significantly more efficient than the O(N2) complexity of direct self-attention on the original user historical behviour sequences. By utilizing this method, we can effectively capture complex interactions within the long-term behviour sequence without sacrificing processing speed and scalability.\nIn the action transformer layer, the outputted $X^{i+1}$ and $P^{i+1}$ is further passed to a feedforward layer with residual connections. After stacking several LDA layers, we get sequence representation $X^L \\in R^{N\\times d}$ eventually.\nInterest Memory Retrieval Layer\nTo expand the representational space of k learnable interest tokens in LDA layer, in this section we introduce the Interest Memory Retrieval (IMR) layer which adopts product keys (Lample et al. 2019) technique to retrieve compressed user interest representation from a pool of interest expert. The overall structure of the IMR layer is presented in Figure 2 (b) and (c). The IMR layer is composed of two components: a hierarchical query network and interest expert retrieval layer.\nQuery Network: The hierarchical query network Q: $X \\rightarrow Q(X) = q \\in R^{1\\times d}$ takes the user historical behviour sequences $X \\in R^{N\\times d}$ as input and applies the hierarchical linear pooling along the sequence length dimension with self attention:\n$X \\leftarrow Reshape(X, (\\frac{N}{stride}, stride \\cdot d))$\\\n$X \\leftarrow Linear Projection(X)$\n$X\\leftarrow Self Attention(X)$ (4)\nwhere stride represents the pool size and N is the sequence length dimension. This shorten process reduces the temporal redundancy of user's behviour sequence and significantly alleviates the computational costs of query network.\nInterest Expert Retrieval Layer: Formally, let q be the query and $T_k$ represents the top-k operation. We first initialize a set of interest experts $E = \\{e_1,\\cdots,e_k\\}$ and a corresponding set of K product keys $K = \\{k_1,\\cdots,k_k\\}$. Standard expert retrieval process first computes the inner products of query q and keys and outputs the indices of selected"}, {"title": "Prediction and Model Optimization", "content": "After getting the item sequence representations $X^L \\in R^{N\\times d}$, we proceed to make next-item prediction by calculating the probability distribution across the entire candidate item set to predict the most likely next item. At timestamp t, we compute the recommendation score for every item embedding $x_i \\in R^d$ in the candidate pool:\n$s_i = X_i^L x_i$ (8)\nwhere $X_i^L$ is the t-th item's representation in the behaviour sequence. Therefore, the recommended probability distribution \u0177 of the next-item should be computed as follows:\n$\\hat{y_i} = \\frac{exp(s_i)}{\\sum_a exp(s_i)}$ (9)\nThen, we formulate the sequential recommendation task as a binary classification problem whose objective is to minimize the cross-entropy between the predicted recommendation results \u0177 and the ground truth label y:\n$L(y, \\hat{y}) = ylog(\\hat{y}) + (1 - y)(1 \u2013 log(\\hat{y}))$ (10)\nExperiment\nExperimental Settings\nDatasets. We evaluate the effectiveness of our proposed ACT-IMR on two real-world datasets:"}, {"title": "Computational Scalability Study", "content": "To investigate the efficiency of proposed ELASTIC, we evaluate the computational usage of the proposed ELASTIC as compared to traditional Transformer baseline SASRec in XLong dataset, including GPU memory and inference latency.\nEfficiency of Different Backbones. In Figure 3 (left), we observe that ELASTIC mechanism exactly reduces the GPU memory and inference latency cost, demonstrating the high efficiency of ELASTIC. For example, ELASTIC extraordinarily improves the modeling efficiency on Xlong, which reduces the approximately 90% GPU memory cost with \u00d72.68 inference speed up on 1024 sequence length. Such result validates the theoretical analysis of linear complexity of ELASTIC which reduces the complexity of original self attention from O(N2) to O(Nk).\nEfficiency of Sequence Lengths. To study the training cost of different sequence lengths for long-term sequential recommendation, we set the maximum sequence lengths in {64, 128, 256, 512, 1024}. In Figure 3 (right), we observe that the computational complexity of the SASRec model exhibits a quadratic growth trend as the input sequence length increases. In contrast, the ELASTIC model demonstrates an approximately linear increase in computational complexity. We also conclude that ELASTIC reduces the computational complexity of SASRec consistently in all cases.\nHyperparameter Sensitivity\nWe investigate the hyperparameter sensitivity of ELASTIC on the interest pool size K and the number of selected interest heads k on ML-1M dataset. Figure 5 illustrates the relationship between these hyperparameters and the NDCG@10"}, {"title": "Dissection of Expert Activation Patterns", "content": "To study the interest expert activation patterns of IMR layers, we visualize the results in Figure 4. In Figure 4 (left), the x-axis represents the different expert id and y-axis stands for the load of a particular expert while Figure 4 (right) shows the traffic distribution of different experts. We find that the load distribution across experts is relatively balanced while there exist multiple hot experts that always get a large share of tokens at a higher level. For example, Figure 4 (right) shows expert 145 (blue stripe), 225 (brown stripe) and 241 (red stripe) are more likely to be chosen in IMR layer. This shows that the hot experts may learn common interest patterns of different users while the others tend to model biased user interest. This also validates IMR layer does learn diverse interest patterns among different users."}, {"title": "Conclusion", "content": "In this paper, we present an efficient linear attention for sequential interest compression framework, aiming at addressing the issue of high complexity of standard attention. Our proposed linear dispatcher attention mechanism effectively compresses the long-term interest of user to address the dilemma of training efficiency, inference efficiency and recommendation performance. And the novel interest memory retrieval technique sparsely retrieves compressed user's interests representations from a large interest memory bank with a negligible computational overhead. Extensive investigations across various strong sequential recommendation architectures consistently demonstrate the performance improvements from our approach, on the challenging benchmark of ML-1M and XLong datasets."}]}