{"title": "ELASTIC: Efficient Linear Attention for Sequential Interest Compression", "authors": ["Jiaxin Deng", "Shiyao Wang", "Song Lu", "Yinfeng Li", "Xinchen Luo", "Yuanjun Liu", "Peixing Xu", "Guorui Zhou"], "abstract": "State-of-the-art sequential recommendation models heavily rely on transformer's attention mechanism. However, the quadratic computational and memory complexities of self attention have limited its scalability for modeling users' long range behaviour sequences. To address this problem, we propose ELASTIC, an Efficient Linear Attention for SequenTial Interest Compression, requiring only linear time complexity and decoupling model capacity from computational cost. Specifically, ELASTIC introduces a fixed length interest experts with linear dispatcher attention mechanism which compresses the long-term behaviour sequences to a significantly more compact representation which reduces up to 90% GPU memory usage with \u00d72.7 inference speed up. The proposed linear dispatcher attention mechanism significantly reduces the quadratic complexity and makes the model feasible for adequately modeling extremely long sequences. Moreover, in order to retain the capacity for modeling various user interests, ELASTIC initializes a vast learnable interest memory bank and sparsely retrieves compressed user's interests from the memory with a negligible computational overhead. The proposed interest memory retrieval technique significantly expands the cardinality of available interest space while keeping the same computational cost, thereby striking a trade-off between recommendation accuracy and efficiency. To validate the effectiveness of our proposed ELASTIC, we conduct extensive experiments on various public datasets and compare it with several strong sequential recommenders. Experimental results demonstrate that ELASTIC consistently outperforms baselines by a significant margin and also highlight the computational efficiency of ELASTIC when modeling long sequences. We will make our implementation code publicly available.", "sections": [{"title": "Introduction", "content": "Sequential recommender systems (SRs) have played a vital role in online content-sharing platforms and e-commerce, such as Kuaishou (Chang et al. 2023; Si et al. 2024) and Taobao (Zhou et al. 2018, 2019; Pi et al. 2020), which capture user's actual preferences from the long-term action history (e.g. click, view and comment) to predict next action.\nIn practical recommendation scenarios, hundreds of millions of active users leave tens of billions of user interaction logs each day. So accurately modeling the extensive length of user behavior sequence across the life cycle not only helps SRs learn better representations for revealing users' actual preferences, but also boosts the user experience and business effectiveness on such platforms.\nThe evolution of SRs has witnessed a notable progression from traditional methods such as Markov chains (He and McAuley 2016; Rendle, Freudenthaler, and Schmidt-Thieme 2010) to deep learning architectures, including convolutional neural networks (Tang and Wang 2018; Yan et al. 2019) and recurrent neural networks (Hidasi and Karatzoglou 2018; Hidasi et al. 2015; Li et al. 2017). Recently, many efforts (Kang and McAuley 2018; Liu et al. 2023; Sun et al. 2019; Zhang et al. 2019) have been devoted to leveraging transformer-based architectures (Vaswani et al. 2017), which is known as self-attention recommenders (SARs), for modeling users historical behavior sequences. The core of SARs adopts the key mechanism that captures contextual information from the entire user behaviour sequence by modeling pairwise item-to-item interactions between the inputs at every token. For example, BERT4Rec (Sun et al. 2019) leverages bidirectional encoding through multi-head self attention and captures the context from both directions in user history for enhanced sequential recommendation. SASRec (Kang and McAuley 2018) adopts self-attention mechanisms to sequential recommendation by balancing long-term and short-term user preferences. Despite their superior performance and parallel training efficiency, SARs have encountered critical efficiency issues when scaling up to long-term sequences (Pancha et al. 2022; Yue et al. 2024) because dot-product operation in the attention layer results in quadratic scaling complexity \\(O(N^2)\\) with respect to the sequence length N.\nIn this paper, we propose an Efficient Linear Attention for Sequential Interest Compression (ELASTIC) to address the issue of high complexity of standard attention. Specifically, ELASTIC first introduces a fixed length interest expert which compresses the long-term behaviour sequences to a significantly more compact representation. The proposed linear dispatcher attention mechanism allows the compressed interest experts to aggregate and dispatch item dependencies from the original sequences more efficiently with linear complexity. This process significantly reduces up to 90% GPU memory usage with \u00d72.7 inference speed up compared with the state-of-the-art self attention based methods on long-term sequences. Then, in order to retain the capacity for modeling various user interests, ELASTIC initializes a vast learnable interest memory bank and proposes a novel interest memory retrieval technique for sparsely retrieving compressed user's interest representations from the interest memory which significantly increases the parameter capacity with a negligible computational overhead. In practical recommendation scenarios, the interest experts pool can be initialized at a scale of millions to effectively model the personalized interests of hundreds of millions of users. This large-scale interest pool allows for a more granular representation of user preferences and enables the SRs to capture a wide spectrum of interest patterns across users more effectively. We perform extensive experiments on multiple public datasets and compare ELASTIC to a variety of strong SRs baseline models. As shown in Figure 1, ELASTIC achieves competitive and even better performance, while acquiring significant gains in computational efficiency compared with traditional SARs methods.\nWe summarize the contributions of our work as follows:\n\u2022 We propose a novel linear dispatcher attention mechanism which effectively compresses the long-term interest of user to address the dilemma of training efficiency, inference efficiency and recommendation performance.\n\u2022 We introduce a novel interest memory retrieval technique for sparsely retrieving compressed user's interests representations from a large interest memory bank with a negligible computational overhead.\n\u2022 Extensive experiments demonstrate the effectiveness and efficiency of ELASTIC in comparison to state-of-the-art methods on multiple public datasets, where ELASTIC consistently outperforms baseline methods by a large margin."}, {"title": "Related Work", "content": "In recent years, the Transformer architecture (Vaswani et al. 2017; Hou et al. 2022; Kang and McAuley 2018; Sun et al. 2019; Wu et al. 2020; Zhang et al. 2019) has significantly advanced the field of sequential recommendation systems by leveraging its superior ability to model long-range dependencies and capture user-item interaction patterns effectively. The key component of transformer is the self attention mechanism which computes the corresponding attention matrix for distinguishing items' importance by a dot-product operation between the query and key matrices. Specifically, ATTRec (Zhang et al. 2018) leverages the self-attention mechanism to capture both short-term item-item interactions and long-term user-item relationships. SASRec (Kang and McAuley 2018) employs multi-head self-attention mechanisms to effectively capture complex and dynamic user preferences, allowing for more accurate modeling of user behavior over time. BERT4Rec (Sun et al. 2019) adapts the bidirectional self-attention mechanism from Transformers, enabling a more comprehensive understanding of user preferences by capturing transition patterns from both left and right contexts in the sequence, overcoming limitations of unidirectional models. FDSA (Zhang et al. 2019) incorporates feature-level self-attention mechanisms to generate accurate and context-aware sequential representation. While these methods have proved to be effective, they primarily rely on traditional dot-product attention mechanisms which can lead to computational inefficiencies when scaling up to long-term user behviour sequence.\nRecently, various \u201cX-former\u201d models have been proposed for improving the computational and memory efficiency of original Transformer architecture. The earliest attempts design sparse attention score matrix by limiting the receptive field to fixed patterns such as local attention (Parmar et al. 2018; Qiu et al. 2019) and strided attention (Beltagy, Peters, and Cohan 2020; Child et al. 2019). Another line of research designs the learnable patterns in a data-driven manner. For example, Reformer (Kitaev, Kaiser, and Levskaya 2020) designs the reversible residual layers and locality-sensitive hashing (Andoni et al. 2015) attention, decreasing computational complexity from \\(O(NlogN)\\). And Routing Transformer (Roy et al. 2021) introduces an efficient content-based sparse attention mechanism using online k-means clustering. Subsequently, low-rank approximation methods emerged as a solution by assuming low-rank structure of original attention matrix. Linformer (Wang et al. 2020) projects the keys and values to a low-rank dimension using the additional projection layers. Linear Transformers (Katharopoulos et al. 2020) rewrite dot product attention as a linear dot product of kernel feature maps, enabling recurrent computation, and faster autoregressive inference. Besides, recent works implement linear attention mechanism (Liu et al. 2023) to estimate original dot-product attention or leverage linear recurrent units (Yue et al. 2024) to address the dilemma of training and inference efficiency in long-term sequential recommendation scenarios. Despite existing methods being proved to be efficient in computational complexity, they may sacrifice recommendation accuracy and stability when compared with SARs methods with regular attention. In contrast, our proposed ELASTIC decouples"}, {"title": "Method", "content": "This section introduces our proposed novel ELASTIC framework for sequential recommendation systems. We begin by defining the problem formulation for sequential recommendation tasks. Then, we introduce the efficient Linear Dispatcher Attention (LDA) layer with a novel dispatcher mechanism. Next, we present proposed Interest Memory Retrieval (IMR) technique for sparsely retrieving compressed user's interests representation. We will discuss the optimization process of the model and present the pseudo-code."}, {"title": "Problem Statement", "content": "Given a set of user \\(U = \\{u_1, u_2,..., u_u\\}\\) and a set of item \\(X = \\{x_1,x_2,\u00b7\u00b7\u00b7,x_{|x|}\\}\\), we represent \\(u_i\\)'s the historical item interaction list as \\(X = \\{x_1,\u2026\u2026, x_t,\u2026\u2026\u2026,x_N\\} \\in R^{N\\times d}\\), including the item's ID embeddings and positional embeddings with the dimension of \\(d\\), where \\(x_t\\) is the \\(t\\)-th item interacted by user \\(u_i\\) and \\(N\\) represents the maximum historical window length. Sequential recommendation systems aim to predict a user's next item selection based on their interaction history. Our proposed ELASTIC takes the user's past \\(N\\) interactions as input and predicts the possible items for the \\(N + 1\\) time step based on the generated a list of top-\\(k\\) items from the available set \\(X\\)."}, {"title": "Linear Dispatcher Attention Layer", "content": "As a critical part of transformers, the idea of dot-product attention is to compute the pairwise item-to-item relationship at every input token. The standard self-attention mechanism directly applied to the user behviour sequences \\(X^i\\) in \\(i\\)-th layer can be defined as:\n\n\\begin{equation}\nX^{i+1} = \\text{Attention}(Q, K, V) = \\text{Softmax}(\\frac{QK^T}{\\sqrt{d}})V\n\\end{equation}\n\nwith the query matrix \\(Q = X^iW_Q \\in R^{N\\times d}\\), the key matrix \\(K = X^iW_K \\in R^{N\\times d}\\) and the value matrix \\(V = X^iW_V \\in R^{N\\times d}\\). The self attention helps the model to capture intra-dependencies among all historical tokens. However, the original self attention mechanism results in an attention map with the memory and complexity of \\(O(N^2)\\), which is very costly when the input sequences have a large number of \\(N\\).\nTo address the potential complexity caused by a large number of tokens \\(N\\), we introduce a dispatcher mechanism which aggregates and dispatches sequence token dependencies more efficiently which adopts the \\(k (k < N)\\) learnable user interest tokens as aggregation points. We first aggregate the information from all sequence tokens by using the dispatcher embeddings \\(P^i\\) from \\(i\\)-th layer as the query and the original sequence token embeddings as key and value:\n\n\\begin{equation}\nP^{i+1} = \\text{Attention}(P^iW_{Q_1}, X^iW_{K_1}, X^iW_{V_1})\n= \\text{Softmax}(\\frac{P^iW_{Q_1}(X^iW_{K_1})^T}{\\sqrt{d}})X^iW_{V_1}\n\\end{equation}\n\nwhere the complexity is \\(O(Nk)\\). Then the dispatchers distribute the dependencies relationship to all sequence tokens by setting the original sequence token embedding as the query and the aggregated interest embeddings \\(P^{i+1}\\) as the key and value:\n\n\\begin{equation}\nX^{i+1} = \\text{Attention}(X^iW_{Q_2}, P^{i+1}W_{K_2}, P^{i+1}W_{V_2})\n= \\text{Softmax}(\\frac{X^iW_{Q_2} (P^{i+1}W_{K_2})^T}{\\sqrt{d}})P^{i+1}W_{V_2}\n\\end{equation}\n\nwhere the complexity is also \\(O(Nk)\\). Therefore, the proposed dispatcher mechanism achieves an overall computational complexity of \\(O(Nk)\\), which is significantly more efficient than the \\(O(N^2)\\) complexity of direct self-attention on the original user historical behviour sequences. By utilizing this method, we can effectively capture complex interactions within the long-term behviour sequence without sacrificing processing speed and scalability.\nIn the action transformer layer, the outputted \\(X^{i+1}\\) and \\(P^{i+1}\\) is further passed to a feedforward layer with residual connections. After stacking several LDA layers, we get sequence representation \\(X^L \\in R^{N\\times d}\\) eventually."}, {"title": "Interest Memory Retrieval Layer", "content": "To expand the representational space of \\(k\\) learnable interest tokens in LDA layer, in this section we introduce the Interest Memory Retrieval (IMR) layer which adopts product keys (Lample et al. 2019) technique to retrieve compressed user interest representation from a pool of interest expert. The overall structure of the IMR layer is presented in Figure 2 (b) and (c). The IMR layer is composed of two components: a hierarchical query network and interest expert retrieval layer.\nQuery Network: The hierarchical query network \\(Q: X \\rightarrow Q(X) = q \\in R^{1\\times d}\\) takes the user historical behviour sequences \\(X \\in R^{N\\times d}\\) as input and applies the hierarchical linear pooling along the sequence length dimension with self attention:\n\n\\begin{equation}\nX \\leftarrow \\text{Reshape}(X, (\\frac{N}{stride}, stride\\cdot d))\n\\leftarrow \\text{Linear Projection}(X)\nX\\leftarrow \\text{Self Attention}(X)\n\\end{equation}\n\nwhere stride represents the pool size and \\(N\\) is the sequence length dimension. This shorten process reduces the temporal redundancy of user's behviour sequence and significantly alleviates the computational costs of query network.\nInterest Expert Retrieval Layer: Formally, let \\(q\\) be the query and \\(T_k\\) represents the top-\\(k\\) operation. We first initialize a set of interest experts \\(E = \\{e_1,\u2026\u2026,e_K\\}\\) and a corresponding set of \\(K\\) product keys \\(K = \\{k_1,\u2026,k_K\\}\\). Standard expert retrieval process first computes the inner products of query \\(q\\) and keys and outputs the indices of selected"}, {"title": "Prediction and Model Optimization", "content": "After getting the item sequence representations \\(X^L \\in R^{N\\times d}\\), we proceed to make next-item prediction by calculating the probability distribution across the entire candidate item set to predict the most likely next item. At timestamp \\(t\\), we compute the recommendation score for every item embedding \\(x_i \\in R^d\\) in the candidate pool:\n\n\\begin{equation}\ns_i = X_L x_i^T\n\\end{equation}\n\nwhere \\(X_t\\) is the \\(t\\)-th item's representation in the behaviour sequence. Therefore, the recommended probability distribution \\(\\hat{y}\\) of the next-item should be computed as follows:\n\n\\begin{equation}\n\\hat{y}_i = \\frac{\\exp(s_i)}{\\sum_a \\exp(s_i)}\n\\end{equation}\n\nThen, we formulate the sequential recommendation task as a binary classification problem whose objective is to minimize the cross-entropy between the predicted recommendation results \\(\\hat{y}\\) and the ground truth label \\(y\\):\n\n\\begin{equation}\nL(y, \\hat{y}) = -y\\log(\\hat{y}) + (1 - y)(1 \u2013 \\log(\\hat{y}))\n\\end{equation}"}, {"title": "Experiment", "content": "We evaluate the effectiveness of our proposed ACT-IMR on two real-world datasets:"}, {"title": "Overall Performance", "content": "Our main performance on ML-1M and XLong dataset are shown in Table 2 where each row represents the dataset"}, {"title": "Ablation Study", "content": "We perform a series of ablation studies to demonstrate the contributions of the proposed components in ELASTIC. As shown in Table 3, we conduct ablation on the hierarchical query network, the need for product key memory layer, and the dispatcher mechanism in the LDA layer. For the hierarchical query network, we replace it with an average pooling of original behavior sequences as the query. When removing the entire IMR layer, we initialize the same learnable interest expert P for all users. We observe the following results on the ablation of ELASTIC components: (1) All proposed components contribute to the overall performance of ELASTIC. For example, removing IMR results in an average performance drop of 3.36% on ML-1M. (2) The proposed hierarchical query network is responsible for generating personalized query representation for each user, the naive average pooling of original sequences is inadequate for depicting different interests of users. (3) Equipped with the IMR layer, the capacity of the model is significantly improved, which obviously boosts the recommendation performance. To be specific, removing IMR results in an average performance drop of 2.51% on ML-1M, which evidences the necessity of improving network capacity with IMR layer. (4) Dispatcher mechanism effectively aggravates and dispatches information between the original sequence and compressed interest expert which contributes an average performance of 3.80% on ML-1M. Overall, the ablation results suggest that all proposed components and the designed architecture in ELASTIC are effective for sequential recommendation."}, {"title": "Computational Scalability Study", "content": "To investigate the efficiency of proposed ELASTIC, we evaluate the computational usage of the proposed ELASTIC as compared to traditional Transformer baseline SASRec in XLong dataset, including GPU memory and inference latency.\nIn Figure 3 (left), we observe that ELASTIC mechanism exactly reduces the GPU memory and inference latency cost, demonstrating the high efficiency of ELASTIC. For example, ELASTIC extraordinarily improves the modeling efficiency on Xlong, which reduces the approximately 90% GPU memory cost with \u00d72.68 inference speed up on 1024 sequence length. Such result validates the theoretical analysis of linear complexity of ELASTIC which reduces the complexity of original self attention from \\(O(N^2)\\) to \\(O(Nk)\\).\nTo study the training cost of different sequence lengths for long-term sequential recommendation, we set the maximum sequence lengths in {64, 128, 256, 512, 1024}. In Figure 3 (right), we observe that the computational complexity of the SASRec model exhibits a quadratic growth trend as the input sequence length increases. In contrast, the ELASTIC model demonstrates an approximately linear increase in computational complexity. We also conclude that ELASTIC reduces the computational complexity of SASRec consistently in all cases."}, {"title": "Hyperparameter Sensitivity", "content": "We investigate the hyperparameter sensitivity of ELASTIC on the interest pool size K and the number of selected interest heads k on ML-1M dataset. Figure 5 illustrates the relationship between these hyperparameters and the NDCG@10 scores. For K, we observe that performance improved as the interest pool size increased from 8\u00d78 to 16\u00d716. However, further expansion to 512\u00d7512 leads to a decline in performance. To explain this phenomenon, we examine the key usage rate (shown by the dashed line). At 512\u00d7512, only 55.76% of keys are utilized, suggesting that an excessively large interest pool may be redundant for modeling diverse user interests, potentially harming model performance. Regarding k, we observe a significant performance improvement when increasing from 4 to 8, but only a modest gain when further increasing from 8 to 16. Overall, ELASTIC demonstrates robust performance across varying numbers of interest heads k, while the interest pool size K requires careful selection for optimal performance."}, {"title": "Dissection of Expert Activation Patterns", "content": "To study the interest expert activation patterns of IMR layers, we visualize the results in Figure 4. In Figure 4 (left), the x-axis represents the different expert id and y-axis stands for the load of a particular expert while Figure 4 (right) shows the traffic distribution of different experts. We find that the load distribution across experts is relatively balanced while there exist multiple hot experts that always get a large share of tokens at a higher level. For example, Figure 4 (right) shows expert 145 (blue stripe), 225 (brown stripe) and 241 (red stripe) are more likely to be chosen in IMR layer. This shows that the hot experts may learn common interest patterns of different users while the others tend to model biased user interest. This also validates IMR layer does learn diverse interest patterns among different users."}, {"title": "Conclusion", "content": "In this paper, we present an efficient linear attention for sequential interest compression framework, aiming at addressing the issue of high complexity of standard attention. Our proposed linear dispatcher attention mechanism effectively compresses the long-term interest of user to address the dilemma of training efficiency, inference efficiency and recommendation performance. And the novel interest memory retrieval technique sparsely retrieves compressed user's interests representations from a large interest memory bank with a negligible computational overhead. Extensive investigations across various strong sequential recommendation architectures consistently demonstrate the performance improvements from our approach, on the challenging benchmark of ML-1M and XLong datasets."}]}