{"title": "Applications of Statistical Field Theory in Deep Learning", "authors": ["Zohar Ringel", "Noa Rubin", "Edo Mor", "Moritz Helias", "Inbar Seroussi"], "abstract": "Deep learning algorithms have made incredible strides in the past decade yet due to the complexity of these algorithms, the science of deep learning remains in its early stages. Being an experimentally driven field, it is natural to seek a theory of deep learning within the physics paradigm. As deep learning is largely about learning functions and distributions over functions, statistical field theory, a rich and versatile toolbox for tackling complex distributions over functions (fields) is an obvious choice of formalism. Research efforts carried out in the past few years have demonstrated the ability of field theory to provide useful insights on generalization, implicit bias, and feature learning effects. Here we provide a pedagogical review of this emerging line of research.", "sections": [{"title": "Preliminaries", "content": "In this chapter we establish notation, manage expectations, and give a basic introduction to the analytical tools we require. The tools introduced are quite diverse, spanning math and physics topics. Rather than aiming to make the reader confident in them, the purpose here is to give the minimal user interface required for the analysis of trained neural networks carried out in later chapters. Readers looking to establish their knowledge of Replicas, Path-Integrals, and Gaussian Processes can consider the following introductory books Mezard and Montanari, 2009; Schulman, 1996; Rasmussen and Williams, 2005."}, {"title": "Motivation", "content": "The goal of this review is to demonstrate how field theory and statistical physics provide a rich framework that may, in the future, accommodate a unified theory of deep learning. This then brings into mind questions concerning the need for such a theory and its general contours.\nIn traditional scientific disciplines, simple universal results are celebrated. Snell's law describing light scattering, Carnot's optimal engine efficiency, and the universal scaling of magnetic clusters in Ising-like transitions lead to concrete and universal predictions. Unlike naturally occurring systems, virtual ones, such as deep neural networks, have been post-selected by software engineers to adapt to various complex learning settings. They are therefore more akin to biological systems than to physical systems, where complexity is a fact of life rather than an obfuscation of a simple underlying phenomenon.\nTo further illustrate this point, let us imagine one finds something similar to Schr\u00f6dinger's equation (a linear partial different equation) as a universal description of a trained transformer network. This would be very convenient to theorists, as we have various analytical and numerical tools to analyze linear differential equations. However, it would also mean that a transformer is equivalent to a linear model. However, linear models were studied extensively in machine learning and did not lead to transformer-grade performance. It is therefore difficult to believe that such a linear theory can exist. This fleshes out a tension between analytical solutions which, by virtue of being called \u201csolutions\u201d, should be computationally simple or \u201clinear\" and a theory capable of tracking deep learning.\nThe above explainability paradox is not new to physicists studying complex systems and various workarounds exist based on averaging, universality, dimensional reduction, modularity, and asymptotic limits. For instance, while SAT-3 is an NP-hard problem, various computational techniques borrowed from spin-glass theory can accurately predict thresholds, in terms of the ratio of variables and clauses/constraints, at which large-scale SAT-3 instances become easy to solve. These theories escape the paradox of finding an analytical solution to an NP-hard problem by averaging over ensembles of problems instead of solving a particular SAT-3 instance. Still, they provide useful statistical rules for determining whether a typical instance is solvable. This is also an example of dimensional reduction since out of the many parameters defining a SAT-3 instance, only a single quantity, the above mentioned variable to clauses ratio, controls hardness in large-scale SAT-3 problems (Mezard and Montanari, 2009). Similarly, Random Matrix Theory (Potters and Bouchaud, 2020) provides accurate statistical information on the spectrum large matrices (e.g. Wigner's semi-circle law) while circumventing the $O(n^3)$ computational difficulty of diagonalizing $n$ dimensional matrices. This example also showcases universality, as various changes to the statistics of random matrix elements still lead to the same distribution law for eigenvalues.\nIn building a theory of deep learning, one needs to negotiate these difficulties and tensions. One should also be wary of a common pitiful in which the analytical \u201csolutions\" offered by theories involve equations which require expensive numerical solutions, potentially more complex than training the networks themselves.\nOptimism can however be drawn from various successful recent outcomes, some of which are described in detail in this review. In particular (i) A linear description of highly overparametrized DNNs (the GP limit) which, despite describing a less powerful regime of deep learning, provides insights on fundamental questions about overparametrization and allows rationalizing architecture choices. (ii) Analytically inspired recipes for hyper-parameter transfer between small and large networks, which are based on solutions of the very short time dynamics yet appear to apply throughout training. (iii) The empirical observation of scaling laws where, based on only two constants (and faith in the scaling law), one can predict the performance of ChatGPT and other real-world networks aiding the allocation of computation resources. We shall further explain how these predictions can be derived within our field-theory framework.\""}, {"title": "Notation", "content": "The following notation will be used throughout this review."}, {"title": "Gaussian Integrals", "content": "Multivariate Gaussian integrals are central to field theory. Here we recall the definition of those and several useful identities, in particular Wick's theorem and square completion.\nWe denote a multivariate Gaussian distribution of $w \\in \\mathbb{R}^d$ with mean $\\mu$ and covariance matrix $\\Sigma$, by $\\mathcal{N}[\\mu, \\Sigma; w]$ where\n$$\\mathcal{N}[\\mu, \\Sigma; w] = \\frac{1}{\\sqrt{(2\\pi)^d \\text{det}(\\Sigma)}} e^{-\\frac{1}{2} [w-\\mu]^T \\Sigma^{-1} [w-\\mu]}$$(1.1)\nMarginalization. Given a Gaussian $\\mathcal{N}[\\mu, \\Sigma; (w_1..w_d)]$, the marginalzied probability distribution for $w' = (w_1..w_{d'<d})$ is easily written in terms of the $d' \\times d'$ matrix $\\Sigma_{ij} = \\Sigma_{ij} (i, j \\in [1..d'])$\n$$\\int dw_{d'+1}..dw_d \\mathcal{N}[\\mu, \\Sigma; w] = \\mathcal{N}[\\mu', \\Sigma'; w']$$(1.2)\nwhere $\\mu' = (\\mu_1...\\mu_{d'})$.\nExpectation values. Any finite moment of a Gaussian random variable can be calculated using Wick's-Isserlis' theorem. Denoting the moment by $\\overline{w_{i_1}...w_{i_n}}$, we consider all partitions of the indices $i_1...i_n$ as follows: We first split the indices into one set A, then the remaining"}, {"title": "Network Definition and Training Protocol", "content": "Here we define, very briefly, the mechanical (or algorithm) aspects of deep learning. A more thorough introduction appears in Ref. Nielsen, 2018.\nFor simplicity, we focus our exposition on fully connected networks (FCNs) with $L$ layers and $N_l$ neurons in each layer. This is the simplest deep neural networks (DNNs) architecture, we will later comment on generalizations to Convolutional Neural Networks (CNNs) and Transformers. We denote by $x \\in \\mathbb{R}^d$ the input of a network, $z^{(l)}(x)$ denotes output of the $l$'th layer on the $i$'th neuron, and take $z^{(0)}_i(x) = x_i$. The network is defined recursively by\n$$z^{(l)}_i(x) = \\sigma \\left( h^{(l)}_i(x) + b^{(l)}_i \\right)$$(1.6)\n$$h^{(l)}_i(x) = \\sum_j W^{(l)}_{ij} z^{(l-1)}_j(x)$$\nwhere $W^{(l)}_{ij}$ and $b^{(l)}_i$ are respectively the weights and biases (which we often refer to just as weights) and $\\sigma(...)$ is the activation function. We encapsulate all weights and biases into a vector $\\theta_\\alpha$ where $\\alpha$ is an abstract index spanning all the variables defining the network. The network's output is $f(x) = h^{(L)}_j(x)$\nConsidering a supervised learning problem where given a dataset $\\mathcal{D} = \\{(X_\\mu, Y_\\mu)\\}_{\\mu=1}^P$ of input and output pairs where the task is either regression or classifications of new examples not in the data set. To achieve this task, most DNNs are trained to minimize a scalar quantity known as the loss function ($\\mathcal{L}$) given by a sum over all training points.\nMSE loss. This loss is a common choice for regression problems\n$$\\mathcal{L} = P^{-1} \\sum_{\\mu=1}^P (f(x_\\mu) - Y_\\mu)^2$$(1.7)\nwhere $P$ is the number of training points, $y_\\mu \\in \\mathbb{R}^{d_0}$ (where $d_0$ is the output dimension) is the regression target function (or vector function for $d_0 > 1$). Alternatively, one may treat $y(x)$ as a one-hot encoding of categorical labels taking $d_0$ distinct values via $[y(x)]_i = \\delta_{i c(x)}$ where $c(x) \\in [1..d_0]$ assigns a serial number for the category of $x$. Last, in binary classification problems, one can take $d_0 = 1$ and $y(x) = c(x)$ where $c(x) \\in \\{+1, -1\\}$.\nCross entropy loss. This loss is another common choice relevant to classification problems. Here we consider again $f(x)$ as a vector of dimension $d_0$ and write the loss as\n$$\\mathcal{L} = -\\sum_{\\mu=1}^P \\Big[ \\log(e^{-f_{c(x_\\mu)}(x_\\mu)}) - \\sum_{l=1}^{d_0} \\log(e^{-f_l(x_\\mu)}) \\Big]$$(1.8)\nWeight decay. To both these losses, one may add weight decay terms of the form $\\gamma_\\alpha \\theta_\\alpha^2$, to regulate the overall weight norm.\nMany DNNs perform better in the over-parametrized regime where the number of network parameters exceeds the number of training points. For networks with scalar outputs ($d_0 = 1$), fitting perfectly a single data-point can be seen as placing a single constraint in weight space. Hence in the over-parametrized setting, finding $\\theta$'s which fit the training set perfectly is an unconstrained optimization problem. As appreciated even before the recent rise of deep learning (Breiman, 1995) and later supported by various theoretical works (Dauphin et al., 2014; Choromanska et al., 2014; Jacot et al., 2018a) such optimization problems tend to be simple. While different optimization algorithms (e.g. Gradient descent, stochastic Gradient descent (SGD), Adam Kingma and Ba, 2017) may excel at bringing us faster to the minima or provide some implicit bias towards wider/better minima, the success of deep learning seems to be more generic than the peculiarities of each algorithm. Hence, we find it reasonable to focus on two relatively tractable algorithms.\nGradient flow. Is a continuum idealization of the more standard discrete Gradient descent algorithm. In deep learning jargon, it corresponds to full batch Gradient Descent at vanishing step-size/learning-rate. It is described by the partial differential equation\n$$\\frac{d}{dt} \\theta_\\alpha = -\\eta \\partial_{\\theta_\\alpha} \\mathcal{L}$$(1.9)\nwhere $\\eta$ is the learning rate. While in discrete gradient descent, $\\eta$ may induce qualitative effects in standard/discrete gradient descent (Lewkowycz et al., 2020), in the above, continuum idealization, it only sets the time scale and may be absorbed into a re-definition of t. We note by passing that some effects of finite learning rate can be represented with the continuum description in the form of augmenting the loss with additional gradient terms (Barrett and Dherin, 2020; Smith et al., 2021).\nLangevin dynamics (Welling and Teh, 2011; Williams, 1996). Another tractable form of dynamics is the following stochastic differential equation\n$$\\frac{d}{dt} \\theta_\\alpha = -\\eta \\partial_{\\theta_\\alpha} \\mathcal{L} + \\sqrt{2\\mathcal{T}\\eta} \\xi_\\alpha(t)$$(1.10)\nwhere the gradient noise, $\\xi_\\alpha(t)$, is an an uncorrelated Gaussian process obeying $\\langle \\xi_\\alpha(t) \\rangle = 0, \\langle \\xi_\\alpha(t) \\xi_\\beta(t') \\rangle = \\delta(t - t') \\delta_{\\alpha \\beta}$ where $\\delta(t)$ is the Dirac delta function. Unlike gradient flow, the above dynamics loosely mimic the stochastic nature of the dynamics. Using standard results from the theory of Langevin equations, the distribution induced on weight space by this dynamics as $t \\rightarrow \\infty$ (i.e. equilibrium distribution) is given by Gardiner, 2010\n$$P_\\infty(\\theta) \\propto e^{-\\mathcal{L}/\\mathcal{T}}$$(1.11)\nSeveral doubts could be raised regarding the ability of this process to reach equilibrium at reasonable times. As argued earlier, the ruggedness of the loss landscape is less of a concern in the over-parametrized regime, however diffusion in the high-dimensional near zero loss manifold may potentially result in slow equilibration for some observables, as well as possible entropy barriers. In practice, one finds good support for ergodicity, at least for relevant observables such as network outputs and pre-activations. In particular relaxation times for the loss are of a similar scale as relaxation times for various macroscopic network observable (i.e. involving a summation of many statistically equivalent parts) see for example Naveh et al., 2021. In addition, Li and Sompolinsky, 2021; Ariosto et al., 2022; Naveh et al., 2021 show that theoretical descriptions offered by $P_\\infty(\\theta)$ provides accurate predictions on DNNs trained for a reasonable amount of epochs when taking into account the use of full-batch and low learning rates."}, {"title": "Infinite Random Neural Networks", "content": "Neural networks are, at face value, a very complex ansatz for functions. Grasping the statistical relations between weights and outputs is generally a complex task. Here we discuss some simplifications which occur in the limit of highly over-parametrized neural networks, which would be useful in the next chapters.\nAs DNNs, per given weights, define a function, random DNNs define a distribution over functions. Interestingly, for all relevant DNN architectures, there is a suitable infinite-parameter limit where the latter distribution becomes Gaussian (more formally a Gaussian Process or a free field theory). For FCNs, this entails taking the width of all layers to infinity (Neal, 1996), for CNNs the number of channels (Novak et al., 2018), and for transformers the number of heads (Hron et al., 2020). Though it would turn out that this limit is rarely reached in practice, it does turn out to be an insightful viewpoint which can be extended to more realistic scenarios.\nConsider a DNN with uncorrelated random weights with sub-Gaussian tail, e.g. each weight and bias is i.i.d Gaussian random variable $W_{ij}^{(l)} \\sim \\mathcal{N}(0, \\sigma_w^2)$ and biases $b^{(l)}_i \\sim \\mathcal{N}(0, \\sigma_b^2)$. Starting from the most upstream layer, the input layer, $h_i^{(0)} = w_{i}\\cdot x$, one finds that $h^{(0)}_i(x)$, per given $x$, are uncorrelated random variables. Consequently, by induction $h_i^{(l)} = \\sum_j W_{ij}^{(l)} \\sigma(h_j^{(l-1)}(x))$ appear as a sum of N uncorrelated random variables. Thus, for reasonable choice of $\\sigma(..)$ and properly normalized $W^{(l)}$, the preactivation $h^{(l)}_i(x)$, per $x$, would be uncorrelated Gaussian variables. The proper normalization is to take a variance of $\\sigma_w^2 = 1/N$ for $W_{ij}^{(l)}$ (more generally one over the fan-in or number of signals coming in), which is a common way to initialize DNNs.\nFollowing the above argument, we see that the output of a neural network as $N \\rightarrow \\infty$, per $x$, is a Gaussian random variable. Let us obtain the variance of this random variable, and at the same time, the correlations between the outputs of the network on two different data points. Repeating the previous line of argument, the vector random variable $[h^{(l)}_i(x_1), h^{(l)}_i(x_2)]$ is a two-dimensional Gaussian, with correlations $\\overline{\\langle h_i^{(0)} h_j^{(0)} \\rangle}_{w_i, w_j \\sim \\mathcal{N}[0, d^{-1} I_{d \\times d}] } = \\delta_{ij} d^{-1} \\delta (x_1-x_2)$. We denote the resulting distribution for $h^{(0)}(x_\\mu)$ as $\\mathcal{N}(0, K^{(0)})$, namely a two-dimensional centered Gaussian distribution with a covariance matrix\n$$K^{(0)} = d^{-1} \\left( \\begin{array}{cc} |x_1|^2 & x_1 \\cdot x_2 \\\\ x_2 \\cdot x_1 & |x_2|^2 \\end{array} \\right)$$\nContinuing downstream, the vector of random variables $[h_i^{(1)}(x_1), h_i^{(1)}(x_2)]$ is again Gaussian by the vector version of the central limit theorem. Furthermore, by definition, it is uncorrelated across different i's. Thus its distribution is fully determined by its average and correlation specifically, $(\\langle [h_i^{(1)}(x_1), h_i^{(1)}(x_2)] \\rangle) \\sim \\mathcal{N}(0, N^{-1} I_{N \\times N}), h^{(0)} \\sim \\mathcal{N}(0, K^{(0)})$ which is zero, and the $2 \\times 2$ correlations matrix $K^{(1)}_{\\mu\\nu}$ given by\n$$K^{(1)}_{\\mu\\nu} = \\langle h_i^{(1)}(x_\\mu) h_j^{(1)}(x_\\nu) \\rangle \\Big| h^{(0)} \\sim \\mathcal{N}(0, K^{(0)})$$(1.12)\n$$= \\frac{1}{N} \\sum_{j=1}^N \\int dh_j^{(0)}(x_\\mu) dh_j^{(0)}(x_\\nu) \\mathcal{N}(0, K^{(0)}; h^{(0)}(x_\\mu)) \\sigma(h_j^{(0)}(x_\\mu)) \\sigma(h_j^{(0)}(x_\\nu))$$\n$$= \\int dh^{(0)}(x_\\mu) dh^{(0)}(x_\\nu) \\mathcal{N}(0, K^{(0)}; h^{(0)}) \\sigma(h^{(0)}(x_\\mu)) \\sigma(h^{(0)}(x_\\nu))$$\nThe resulting two-dimensional integral turns out to be solvable for various activation functions in particular ReLU (Cho and Saul, 2009; Lee et al., 2018) and Erf (Williams, 1996). For instance, in the latter case one obtains\n$$K^{(1)}_{\\mu\\nu} = K^{(1)}(x_\\mu, x_\\nu) = \\frac{2}{\\pi} \\sin^{-1} \\left[ \\frac{2 K^{(0)}_{\\mu\\nu}}{\\sqrt{1 + 2K^{(0)}_{\\mu\\mu}} \\sqrt{1 + 2K^{(0)}_{\\nu\\nu}}} \\right]$$(1.13)\nMore generally, the above reasoning now extends naturally to the downstream layer, with (1) $\\rightarrow$ (2) and (0) $\\rightarrow$ (1). Thus we have a concrete formula for the correlations between the outputs on $x_1, x_2$ for an arbitrarily deep ReLU or Erf network.\nNext, we wish to extend this to any finite number of points $(x_1...x_{P'})$. At large enough N (in particular N\u226b P') each pre-activation as well as the output remains a multivariate $P'$-dimensional Gaussian and hence solely defined by the correlation between each two data-points\u2014 the same correlation described by the above formula. Thus as $N \\rightarrow \\infty$, we find that the distribution induced by the DNN on function space is such that when examined on any finite set of points- it behaves as a multivariate Gaussian with a kernel $K_{\\mu\\nu}^{(l)}$. In physics, we call such distributions over functions free field theories. The advantage of the latter, a less rigorous but practically exact viewpoint, is that it transcends the mathematical description based on the awkward notion of all possible finite sets of points, to the simple physical notion of a continuum. It also offers a convenient starting point for analyzing"}, {"title": "Operator algebra on spaces with measures", "content": "As is common with Gaussian distributions, the eigenvectors and eigenvalues of the covariance matrix play a useful role. As alluded to in the above section, our real object of interest is an extreme case of a multivariate Gaussian distribution, wherein instead of considering a Gaussian distribution over vectors, we consider a Gaussian distribution over an infinite-dimensional vector or equivalently a smooth function. To substantiate this viewpoint and explain what are the coefficients of this infinite vector, consider the following eigenvalue problem\n$$\\int dx' p(x') K(x, x') \\phi_k(x') = \\lambda_k \\phi_k(x)$$(1.14)\nwhere $p(x')$ is some distribution over inputs and we shall soon denote $dx p(x) = d\\mu_x$. Given that (i) $p(x')$ decays sufficiently fast at infinity, (ii) $K(x, x') = K(x', x)$, and (iii) $\\int dx d\\mu_{x'} K(x, x') g(x) g(x') > 0$ for all real functions $g(x)$ with finite norm $(\\int dx g^2(x) < \\infty)$ we have that (a) the above eigenvalue problem yields a discrete set of solutions $(\\lambda_k, \\phi_k(x))$ with $\\Lambda_k$ being non-negative real numbers (b) $\\phi_k(x)$ may be chosen normalized and orthogonal namely $\\int d\\mu_x \\phi_k(x) \\phi_{k'}(x) = \\delta_{kk'}$ (c) $K(x, x') = \\sum_{k=1}^\\infty \\lambda_k \\phi_k(x) \\phi_k(x')$ (Mercer's theorem). (d) $\\phi_k(x)$ define a complete basis for function space namely we may express any $g(x)$ via $\\sum_k g_k \\phi_k(x)$ where $g_k = \\int d\\mu_x \\phi_k(x) g(x)$.\nNotably, kernels of DNNs obey requirements (ii), (iii). Concerning point (i) above, input datasets, typically have a natural scale and decay to zero beyond some value (e.g., the brightness of a pixel cannot be larger than 255, or the input amplitude of a microphone cannot exceed 150db).\nNext, we define several additional linear algebra notions on kernels. In particular, when all $\\lambda_k > 0$, the inverse kernel $K^{-1}$ can be defined and is given by\n$$K^{-1}(x, x') = \\sum_k \\lambda_k^{-1} \\phi_k(x) \\phi_k(x')$$(1.15)"}, {"title": "Symmetries: Equivariance and Invariance", "content": "Noting further that $\\int dx' p(x') \\sum_k \\phi_k(x) \\phi_k(x') g(x') = g(x)$ we find that\n$$\\sum_k \\phi_k(x) \\phi_k(x') = \\delta(x - x')/p(x')$$(1.16)\nusing the above we find an equivalent definition of $K^{-1}(x, x')$\n$$\\int d\\mu_{x'} K(x, x') K^{-1}(x', x'') = \\delta(x - x'')/p(x)$$(1.17)\nNotably while $K(x, x')$ is given and independent of the measure/$p(x)$, $K^{-1}(x, x')$ does depend on it, yet is a predictable way: Given $K^{-1}(x, x')$ defined w.r.t. the measure $p(x)$ and $\\overline{K^{-1}(x, x')}$ defined w.r.t. the measure $\\overline{p(x)}$ we find\n$$ \\frac{\\overline{p(x)}}{\\overline{p(x)}} K^{-1} (x,x') \\frac{\\overline{p(x')}}{\\overline{p(x')}} = \\overline{K^{-1}(x, x')}$$(1.18)\nwhich can be verified via\n$$\\int d\\overline{\\mu_{x'}} K (x,x')\\overline{K^{-1}(x',x'')} = \\int d\\mu_{x'} K(x, x') \\frac{\\overline{p(x')}}{p(x')} \\overline{K^{-1}(x', x'')} \\frac{\\overline{p(x'')}}{p(x'')}$$\n$$= \\int d\\mu_{x'} K(x, x') K^{-1}(x', x'') \\frac{\\overline{p(x')}}{p(x')} = \\frac{\\delta(x - x'')}{\\overline{p(x'')}}$$(1.19)\nwhere $d\\overline{\\mu_x} = dx \\overline{p(x)}$. We thus find that $K^{-1}$ obeys Eq. 1.17.\nAn interesting corollary of this is that the so-called Reproducing Kernel Hilbert Space (RKHS) norm of a function g(x)\n$$\\langle g \\rangle_K^2 = \\int d\\mu_x d\\mu_{x'} g(x) g(x') K^{-1}(x, x')$$(1.20)\nis in fact the same for any two measures with the same support, loosely speaking we mean that $\\infty > \\frac{p(x)}{\\overline{p(x)}} > 0$ for any x. For more details, see Rasmussen and Williams, 2005 section 6.1 and references therein.\nSymmetries: Equivariance and Invariance\nKernels of DNNs are typically highly symmetric objects. For instance, as can be inferred from our previous computation for the kernel- any DNN with a fully-connected input layer would have a kernel which depends only on $x \\cdot x', |x|, |x'|$. As a result, it would be symmetric to rotations of the input. More formally let $O$ be some symmetry action on $x$ ($d \\times d$ rotation matrix in the above example), we say that a kernel is equivariant under $O$ when $K(x, x') = K(Ox, Ox')$. We say that the kernel is invariant under $O$ when $K(x, Ox') = K(Ox, x') = K(x, x')$. Note that invariance is thus the stronger property and implies equivariance.\nWe comment that these notions of equivariance and invariance coincide with those used in DNN nomenclature (e.g. Novak et al., 2018). Indeed, equivariance means that the action of $O \\in O(d)$ on $x$ has some non-trivial action ($\\tilde{O} \\in O(N)$) on the c-index vector $h_c^{(l)}(x)$ namely that $h_c^{(l)}(Ox) = \\tilde{O} h_c^{(l)}(x)$ whereas invariance means $\\tilde{O}$ is the identity. Using the definition of the kernel of the l'th layer as the average of $[h^{(l)}(x)]^T h^{(l)}(x')$ (over weights) one recovers the above definitions for symmetries which are a subset of the orthogonal group (e.g. permutations, reflections, rotations on convolutional patches). 1\nWe comment that one can map this notion of symmetry to that used in quantum mechanics. Specifically, define the operator associated with $O$ ($\\hat{O}$) via the action $\\hat{O} f(x) = f(Ox)$. This then implies the right action $\\hat{O} K(x, x') = K(Ox, x')$. Next using the action of K on function ($\\int d\\mu_{x'} K(x, x') f(x')$) we find the left action via $\\int d\\mu_{x'} K(x, x') \\hat{O} f(x') = \\int d\\mu_{x'} K(x, x') f(Ox') = \\int d\\mu_{OTx'} K(x, OT x') f(x')$. Conditioned on the measure being symmetric ($d\\mu_{OTx'} = d\\mu_{x'}$) we find $K(x, x') \\hat{O} = K(x, OT x')$ 2 and therefore $\\hat{O} K(x, x') \\hat{O}^T = K(Ox, Ox') = K(x, x')$ as the definition of a symmetry which coincides with that used in quantum mechanics, when viewing K as the Hamiltonian.\nMuch like in quantum physics, symmetries have direct implications on the eigenfunctions and eigenvalues of a kernel. However, at least in non-relativistic physics, we often assume that space itself possesses all symmetries. Here the situation is different since the measure, $d\\mu_x = dx p(x)$ need not be symmetric. In case it is, namely when\nScale invariance, present in ReLU networks without biases, has a different manifestation which does not fall strictly within the above two categories, specifically $K(x, cx') = cK(x, x'))$.\nFor non-symmetric measure the ratio $d\\mu_{Ox'}/d\\mu_{OTx'} = p(x')/p(OTx')$ appears as a factor"}, {"title": "Essentials of path integrals", "content": "$p(x) = p(Ox)$", "simplifications": "Equivariance implies that eigenfunctions can be grouped into irreducible representations (irrep) of the symmetry group (e.g. the rotation group for an FCN). All eigenfunctions which are part of the irrep would have the same eigenvalue. Invariance implies equivariance", "E[f": ".", "A[..": "to denote A as functional and angular brackets if A is a single or multi-variable function. At equilibrium", "e^{-E[f": "", "P[f": "or calculate averages such as the mean value of f at $(x_0"}, {"E[f": "is not a simple quadratic form in f", "e^{-E[f": "", "network": "i) The prior distribution i.e. the distribution of f induced by choosing suitably normalized random Gaussian weights, (ii) The posterior distribution i.e. the distribution of f induced by the Bayesian posterior (i.e. the distribution obtained multiplying the prior distribution by the probability of seeing the training set as the output of the model and normalizing the product), or (iii) The distribution induced on f by training an ensemble of neural networks, with different weights drawn from the initialization ensemble and different algorithmic noise.\nPath integrals can be formally defined as an infinite-variable limit of multi-variable integrals. The most common way is to discretize the space (x) over some fine grid. Concretely, let us assume x does not exceed some $[-L, L", "x": "i$ to take on only the values $-L + n/\\Lambda$ with $n$ spanning the integers in the interval $[0, 2[L\\Lambda", "L": "d$ with each coordinate of $f_i$ marking its value on one point on the grid. Next, we need to map our functional of interest (say $E[f"}]}