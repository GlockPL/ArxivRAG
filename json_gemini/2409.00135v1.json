{"title": "HoneyComb: A Flexible LLM-Based Agent System for Materials Science", "authors": ["Huan Zhang", "Yu Song", "Ziyu Hou", "Santiago Miret", "Bang Liu"], "abstract": "The emergence of specialized large language models (LLMs) has shown promise in addressing complex tasks for materials science. Many LLMs, however, often struggle with distinct complexities of material science tasks, such as materials science computational tasks, and often rely heavily on outdated implicit knowledge, leading to inaccuracies and hallucinations. To address these challenges, we introduce HoneyComb, the first LLM-based agent system specifically designed for materials science. HoneyComb leverages a novel, high-quality materials science knowledge base (MatSciKB) and a sophisticated tool hub (ToolHub) to enhance its reasoning and computational capabilities tailored to materials science. MatSciKB is a curated, structured knowledge collection based on reliable literature, while ToolHub employs an Inductive Tool Construction method to generate, decompose, and refine API tools for materials science. Additionally, HoneyComb leverages a retriever module that adaptively selects the appropriate knowledge source or tools for specific tasks, thereby ensuring accuracy and relevance. Our results demonstrate that HoneyComb significantly outperforms baseline models across various tasks in materials science, effectively bridging the gap between current LLM capabilities and the specialized needs of this domain. Furthermore, our adaptable framework can be easily extended to other scientific domains, highlighting its potential for broad applicability in advancing scientific research and applications.", "sections": [{"title": "Introduction", "content": "The emergence of large language models (LLMs) (OpenAI, 2024; Anthropic, 2024; Touvron et al., 2023b,a) in recent years has brought about the application of LLMs across a wide range of fields related to science and engineering (AI4Science and Quantum, 2023). This has resulted in a number of new benchmarks measuring the capabilities of language models to perform scientific tasks (Wang et al., 2023; Sun et al., 2024; Mirza et al., 2024; Song et al., 2023a) along with the development of custom LLMs and LLM-based systems for scientific domains including chemistry (Bran et al., 2023; Boiko et al., 2023), biology (Madani et al., 2023) and materials science (Song et al., 2023b; Gupta et al., 2022; Walker et al., 2021).\nWhile much progress has been made in adapting LLMs to common tasks in natural language processing (Song et al., 2023a,b), many more challenges remain in having LLMs be effective agents for real-world materials science tasks (Miret and Krishnan, 2024; Miret et al., 2024). As highlighted by Zaki et al. (2023), LLMs often fail in performing important computational tasks for materials science. Common mistakes by most LLMs include conceptual errors where models fail to retrieve correct concepts, equations, or facts relevant to the questions, and factual hallucinations where incorrect information is generated. An analysis by Miret and Krishnan (2024) also revealed that LLMs by themselves struggle to generate relevant and correct information pertaining to specialized materials science tasks. While Song et al. (2023b) showed that instruction fine-tuning can help in improving performance, the high costs of continuous model training and fine-tuning make retraining-based approaches challenging to scale. This is further compounded by the fact that relevant knowledge is continuously updated through a diversity of knowledge sources, including pre-print servers (e.g., arXiv and ChemRxiv), peer-reviewed literature, open encyclopedias (e.g. Wikipedia) and relevant websites. Furthermore, prior work has show that utilizing external tools may be a more promising approach to solve complex scientific tasks instead of relying entirely on an LLMs internal knowledge (Zheng et al., 2024; Buehler, 2024a). To jointly address"}, {"title": "Background", "content": "These challenges, we propose transforming LLMs into LLM-based agents that access external knowledge and tools to boost their performance. This approach has already shown promise in adjacent domains, such as chemistry (Bran et al., 2023; Boiko et al., 2023) by enabling the the models to access real-time data and utilize computational as well as domain-specific tools. Altogether, the LLM-based agents showcase greater capabilities and performance compared to their native LLM counterparts.\nIn this paper, we present HoneyComb, the first, to the best of our knowledge, LLM-based agent system specifically designed for materials science. While there has been emerging research in LLMs for scientific domains, few studies have focused on developing comprehensive agent systems for materials science. Our work addresses two critical challenges: First, MatSciKB alleviates the challenge of obtaining reliable and relevant professional knowledge for materials. As such, MatSciKB ensures the agent has access to the most current and accurate information is essential for effective performance. Second, Tool-Hub provides materials science specific tools to augment the agent's capabilities. These tools enable the agent to perform specialized computational tasks and enhance its overall functionality. As detailed in Section 4, we observe that with the aid of MatSciKB and Tool-Hub, HoneyComb outperforms its native LLM counterparts in a more reliable manner given its ability to utilize up-to-date knowledge and tools."}, {"title": "LLMs for Material Science", "content": "Advancements in text mining and information extraction from scientific publications have significantly benefited the application of LLMs for materials science (Kononova et al., 2021; Swain and Cole, 2016). Early work include the development of specialized BERT models (Devlin et al., 2018), such as MatSciBERT (Gupta et al., 2022) and MatBERT (Walker et al., 2021). Song et al. (2023b) and Xie et al. (2023) leveraged instruction fine-tuning to develop a LlaMa-based (Touvron et al., 2023a) tailored to materials science that matched the capabilities of commercial LLMs at the time of publication. The emergence of powerful commercial LLMs (OpenAI, 2024; Anthropic, 2024) has further expanded the possibility of applying LLMs to materials science. Yet, commercial LLMs remain expensive, opaque in their methodology with consistent errors and shortcomings (Zaki et al., 2023; Miret and Krishnan, 2024), and open-source LLMs for materials science remain sparse. This motivates the need for a practical LLM-based system that is useful for real-world materials science tasks.\nGiven this need, we propose HoneyComb as an open-source system to augment the capabilities of diverse LLMs. HoneyComb integrates specialized tools as well as a dynamic retrieval system to enhance the functionality any LLMs specifically for material science. By leveraging relevant knowledge source through MatSciKB and auxilliary tools through Tool-Hub, HoneyComb manages to improve the accuracy and relevance of the outputs of LLMs for materials science, while also addressing common challenges associated with static LLM applications in dynamic research fields."}, {"title": "Tool-Based LLM Agents for Scientific Applications", "content": "Prior work has shown success in expanding the capabilities of LLMs by augmenting their capabilities with diverse sets of tools (Qin et al., 2023b,a; Chern et al., 2023; Wang et al., 2024). Many works rely on pre-built integration frameworks, such as LangChain (Topsakal and Akinci, 2023), to build the relevant interfaces between the LLMs and the desired capabilities, such as search engine APIs. Wang et al. (2024) provides a recent survey of common approaches, challenges and applications of tool-based LLMs and their applications to various technology and scientific fields.\nOne major application of tool-based LLMs is in query processing and optimization, where agents evaluate initial search results and iteratively refine queries to increase relevance and accuracy (Buehler, 2024a,b). This approach addresses the limitations of isolated LLMs, which may struggle to handle ambiguous query contexts. In generating structured datasets for solar cell materials, agents gather pertinent information from a vast array of scientific papers to automate data input and synthesis (Xie et al., 2024; Liu et al., 2024b). Furthermore, agents can utilize various tools to help answer specific questions by tapping into external resources (Cheng et al., 2024). For example, ChemCrow by Bran et al. (2023) integrates 18 expert-designed tools, such as literature search, molecule modification, and reaction execution, to autonomously execute chemical syntheses. Tool augmentation has also shown success in other re-"}, {"title": "HoneyComb", "content": "In this work, we introduce HoneyComb, shown in Figure 1, a specialized agent system designed to advance materials science research. It integrates three key components: 1) MatSciKB, a comprehensive knowledge base; 2) ToolHub, which includes general tools for accessing up-to-date information broadly and specialized tools developed through an Inductive Tool Construction method for targeted material science queries; 3) Retriever, utilizing a hybrid approach for efficient and precise information retrieval."}, {"title": "MatSciKB", "content": "Our MatSciKB knowledge base integrates a diverse array of sources, as detailed in Table 1. This collection is meticulously curated to include material science papers from ArXiv, relevant Wikipedia entries, textbooks, comprehensive datasets, pertinent mathematical formulas, and concrete GPT-generated examples tailored to material science. Each information source is thoroughly described in Appendix A.\nThe architectural framework of MatSciKB is thoughtfully structured into 16 distinct categories pertinent to material science. These are detailed in Appendix C and are organized in a tree-like structure. MatSciKB supports efficient searching and CRUD (Create, Read, Update, Delete) operations (Giannaros et al., 2023), which are vital for both the application and ongoing maintenance of the database. Given the continuously evolving and expanding body of knowledge in the materials science domain, capabilities for efficient updates and searches based on real-time information are crucial for research and engineering applications. Additionally, our structured data approach enhances the integration of the diverse data sources commonly encountered in materials science (Miret and Krishnan, 2024). This structure not only facilitates easy access and management but also allows for seamless extension to include additional data modalities."}, {"title": "Tool-Hub", "content": "The Tool-Hub in HoneyComb is bifurcated into General Tools and Material Science Tools. Both categories are organized through a unified interface that supports allow HoneyComb to make effective use of all available tools. General Tools provide researchers with access to the latest information filling gaps not covered by the static entries in MatSciKB. Material Science Tools are specifically designed to handle complex calculations and in-depth analyses. The details of the unified interface are further elucidated in Appendix D.\nGeneral Tools Construction\nIn materials science, one of the persistent challenges is keeping research outputs aligned with the diverse and ever-evolving data modalities that describe complex material systems (Miret and Krishnan, 2024). The diversity of data sources and measurements leads to a rapid evolution of knowledge in this field, necessitating tools that can effectively access and integrate recent findings. Traditional static databases, while useful, often lag in capturing the newest research, creating gaps that can impede the currency and relevance of scientific analysis in real-time. Further, the need to efficiently process complex and dynamic computational tasks within the research workflow remains inadequately addressed, often requiring manual intervention which can introduce errors and inefficiencies. Thus, constructing tools that can handle varying data modalities and complexities, and that can adapt to the continual advancements in materials science, is essential for advancing the field.\nTo address these challenges, HoneyComb has been designed with innovative solutions that markedly enhance research capabilities in materi-"}, {"title": "Inductive Tool Construction for Materials Science", "content": "als science. First, we integrated General Tools that provide direct access to current publications and facilitate dynamic discussions, as shown in Table 2, effectively complementing the static MatSciKB. Secondly, recognizing the limitations of large language models (LLMs) in performing computational tasks, we implemented a Python REPL environment within HoneyComb. This environment is strategically utilized by the system when the agent, interacting with the Tool-Hub, identifies a need for basic numerical computations. The agent dynamically writes Python code for these tasks and executes it through the Python REPL, bypassing the LLM's computational limitations. This automation not only streamlines data processing but also enhances the precision and reliability of numerical analyses in research activities."}, {"title": "Agent-Tool Hub Interactions", "content": "In HoneyComb, interactions between the agent and Tool-Hub are governed by a structured two-phase decision-making protocol. Our protocol emphasizes the critical selection and processing of data to ensure that only pertinent information influences the LLM's decisions. This approach is vital to prevent the degradation of model performance due to irrelevant or low-quality inputs (Liu et al., 2024a).\n1. Tool Assessor: During the initial phase, the Assessor evaluates both the incoming query and the extensive suite of tools within the Tool-Hub. This evaluation aims to identify a manageable subset of the most relevant tools that are best suited to address the specific requirements of the query. By filtering out irrelevant tools at this stage, we ensure that the Executor is provided only with pertinent information, thereby optimizing the model's focus and enhancing its capacity to solve the problem accurately.\n2. Tool Executor: As illustrated in Figure 2, the Executor receives the original query along with the subset of tools selected by the Assessor. Upon evaluating the selected tools and query, the Executor engages in a thought process to determine the most suitable tool for addressing the query. If the query's complexity exceeds the capacity of a single tool, the Executor recognizes the challenge and decomposes the query into smaller subquestions. The strategy allows for sequential tackling of each part, starting with the selection of the optimal tool for the initial subquestion. It then initiates the action of"}, {"title": "Retriever", "content": "In this section, we present the retriever in HoneyComb which returns relevant texts or tools from MatSciKB and Tool-Hub when a specific contexts is given. The retriever integrates both BM25 (Trotman et al., 2014) and Contriever (Izacard et al., 2022) model, leveraging their respective strengths to achieve optimal information retrieval performance.\nSpecifically, the retriever employs a two-step strategy. Initially, BM25 utilizes efficient calculations of term frequency and inverse document frequency to rapidly process short text queries and keyword searches within long documents. The primary advantage of BM25 lies in its computational simplicity and rapid response, allowing HoneyComb to extract the N most relevant knowledge points from an extensive materials science knowledge base, ensuring exceptional speed and efficiency. This approach enables the provision of basic relevance matching results in a minimal timeframe.\nSubsequently, we employs a pre-trained deep learning models (i.e. Contriever) to generate embedding vectors and compute their similarity, facilitating the understanding of complex linguistic structures and semantic information. The strength of Contriever resides in its capability to compre-"}, {"title": "Experiments", "content": "hend and process intricate language structures, contextual information, and semantic relationships, thereby delivering more precise and comprehensive retrieval results. Although Contriever operates at a slower pace compared to BM25, it pulls the most relevant results from the knowledge base and memory, as well as from tools invoked through the Tool-Hub, extracting the top 3 results. Its ability to precisely handle complex queries and diverse documents ensures high accuracy and relevance.\nBy combining BM25 and Contriever, our model adeptly responds to simple queries with speed while offering enhanced accuracy and relevance for complex queries. This hybrid approach ensures that the model is both efficient and capable of addressing sophisticated query requirements, thereby providing comprehensive, efficient, and precise information retrieval services.\nWe conduct experiments on two question answering datasets, namely MaScQA (Zaki et al., 2023) and SciQA (Johannes Welbl, 2017), to investigat-ing the ablility of HoneyComb in materials science tasks.\nMaScQA, derived from the Graduate Aptitude Test in Engineering (GATE) in India, is tailored to reflect the real-world complexity and variety of issues encountered in material science. This highly competitive examination assesses a comprehensive understanding of various undergraduate subjects (Indian Institute of Technology Kanpur, 2023; Zaki et al., 2023). With its 650 questions covering 14 domains such as thermodynamics, atomic structure, and mechanical behavior, the dataset showcases a wide range of question types, from Multiple Choice Questions (MCQs), Numerical Answer Type (NUM), and Matching Type (MATCH) to MCQs with numerical options (MCQN). Specifically designed for advanced problem-solving, this dataset is crucial for ensuring that our ToolHub functions effectively in actual material science research and applications. It demonstrates the HoneyComb framework's efficacy and adaptability in tackling complex material science issues within realistic scenarios. The second dataset, SciQA, comprises 11,679 multiple-choice questions that span the core disciplines of fundamental sciences from a variety of crowdsourced science exams (Johannes Welbl, 2017). This compilation not only underlines the dataset's comprehensive and inter-"}, {"title": "HoneyComb Evaluation", "content": "disciplinary nature but also focuses on fostering a nuanced conceptual understanding. SciQA serves as a critical testbed to ascertain whether the HoneyComb framework can augment the LLM's capabilities beyond its initial programming. By integrating supplementary information, it aids in addressing intricate queries and unraveling complex scientific concepts that may have been overlooked during the initial training phase of the LLM. By bridging real-world complexities with rigorous academic standards, these datasets ensure that our MatSciKB and ToolHub are not only versatile but also remain at the forefront of technological and scientific application.\nThe choice of models for our experiments was driven by the need to evaluate the HoneyComb framework's enhancement capabilities across a spectrum of large language models known for their robust performance in diverse applications. We selected GPT-3.5, GPT-4 (OpenAI, 2024), LLaMA-2 (Touvron et al., 2023b), and LLaMA-3 (AI@Meta, 2024) due to their widespread use and proven effectiveness in handling complex language tasks. These models, with LLaMA-2 and LLaMA-3 having parameter sizes of 7 billion and 8 billion respectively, represent the current state-of-the-art in generalized language understanding and provide a solid baseline for benchmarking. Additionally, we included HoneyBee(Song et al., 2023b), a specialized model with a parameter size of 7 billion, tailored specifically for materials science. The inclusion of both general-purpose and specialized models allows us to showcase how domain-specific adaptations through HoneyComb can elevate a model's functional scope beyond its original configuration, thus highlighting the adaptability and effectiveness of our framework.\nWe evaluated the performance of various models on MaScQA and SciQA, including HoneyBee, GPT-3.5, GPT-4, Llama2, and Llama3, and demonstrated the effects of using the HoneyComb. The results are illustrated in Table 3\nThe experimental results show that all models based on HoneyComb achieved significant improvements in accuracy on both MaScQA and SciQA. Specifically, on the MaScQA dataset, models such as HoneyBee and GPT-4 experienced substantial improvements, with HoneyBee's accuracy improving by 16.76% and GPT-4's by 20.61%. Other models also showed notable enhancements,"}, {"title": "Honey Comb Evaluation on MaScQA", "content": "with improvements ranging from 4.92 to 14.16%. On the SciQA dataset, the HoneyBee model saw a dramatic increase in performance, representing a huge improvement of 45.73%. HoneyComb based on GPT-3.5 and Llama3 showed more modest enhancements of around 0.14% to 0.32% , whereas HoneyComb based on GPT-4 and Llama2 experienced considerable improvements of approximately 5.70% and 2.87%, respectively.\nWe assess the performance improvements when integrating the HoneyComb framework with various large language models across predefined topics within the MaScQA dataset, as shown in Figure 3. The overall trend indicates that HoneyComb substantially enhances model performance. LLaMA-3 and HoneyBee exhibit impressive gains, particularly in 'Material Testing' where improvements of 33.34 percentage points are observed, showcasing HoneyComb's capability to effectively augment models with its advanced Tool-Hub and extensive MatSciKB.\nHowever, GPT-3.5 displays a unique trend with declines across multiple topics including Atomic Structure, Fluid, Magnetism, Material Processing, and Material Testing. Despite having a higher baseline accuracy than LLaMA-3, LLaMA-2, and HoneyBee, GPT-3.5's performance dips more frequently when integrated with HoneyComb. This could be attributed to its training data's scope and depth, which, while extensive, may not align as effectively with HoneyComb's highly specialized material science enhancements. The sophisticated computational demands and the dynamic nature of materials science queries may expose limitations in GPT-3.5's ability to adapt its pre-existing knowledge to the specific enhancements HoneyComb offers. This nuanced understanding highlights the importance of model and tool compatibility in achieving effective enhancements across diverse materials science domains, thereby informing further development and optimization of HoneyComb to ensure comprehensive and reliable support in all areas of materials science research."}, {"title": "Ablation Study", "content": "To study how each component of HoneyComb contributes to the overall performance, we conducted ablation studies in this section. We tested the performance of HoneyComb when retrieved only from MatSciKB or only from Tool Hub, respectively. We also report results without retriever, in such situation there is no way for MatSciKB and ToolHub results to be fed into the model. Experimental results are reported in Table 4."}, {"title": "Conclusion", "content": "In this work, we introduced HoneyComb, a pioneering LLM-based agent system tailored for materials science. HoneyComb integrates a meticulously curated materials science knowledge base (MatSciKB) and a dual-layered ToolHub of general and specialized computational tools. It combines three critical components: MatSciKB, an inductively constructed ToolHub, and a precision-focused Retriever module. This ensures HoneyComb provides accurate, up-to-date information and performs complex computational tasks reliably.\nExperimental results show that HoneyComb outperforms contemporary general-purpose models (e.g. GPT and LLaMa series) and specialized models (e.g. HoneyBee) in materials science QA tasks. HoneyComb effectively bridges the gap between advanced large language models and the specific needs of materials science research, exemplifying how specialized agent systems can advance scientific research and serve as a blueprint for future advancements in other knowledge-intensive fields."}, {"title": "Limitations", "content": "While HoneyComb significantly enhances the performance of current state-of-the-art models in various materials science QA tasks, there are limitations to its generali zability and applicability beyond the specific datasets and tasks it was trained on. Materials science is a diverse and intricate field, and it remains unclear how well HoneyComb would perform on tasks outside the MaScQA and SciQA benchmarks, particularly for more complex and novel challenges in materials science. Such challenges may include designing synthesis recipes for new materials or predicting material properties. Additionally, HoneyComb's reliance on high-quality LLMs for the knowledge base, tool construction, and retrieval processes can be a limitation. The performance of these components is contingent on the availability and capability of the underlying LLMs, which themselves may have inherent limitations. Furthermore, our work has primarily focused on the materials science domain, and further studies are required to evaluate how applicable and effective HoneyComb would be in other scientific fields."}, {"title": "Broader Impacts", "content": "By expanding the HoneyComb agent system, HoneyComb has the potential to accelerate scientific discovery and innovation, contributing to a deeper understanding of complex materials systems. This could not only lead to advancements in materials design, development, and application but also promote the discovery and optimization of new materials, benefiting a wide range of industries. Additionally, the versatility and adaptability of HoneyComb enable it to tackle challenges across various scientific domains, further broadening its scope and impact.\nOur research does not raise major ethical concerns."}, {"title": "MatSciKB Knowledge Source", "content": "*   ArXiv Paper\n    *   Included all papers indexed under the \u201cmaterial science\u201d keyword on ArXiv.\n    *   Data entries structured into key-value pairs: key is the paper title, and value is the abstract.\n    *   Data Entries Count: 20,384\n*   Wikipedia Material Science Concepts\n    *   Scraped all 438 pages categorized under \u201cMaterials Science\u201d on Wikipedia.\n    *   Each section within a page was separated as a distinct data entry.\n    *   Content formulated into key-value pairs, with keys as section titles and values as content.\n    *   Data Entries Count: 3,620\n*   Material Science Textbook\n    *   Sourced 6 publicly available textbooks.\n    *   Converted each textbook PDF file to text documents.\n    *   Broke each textbook into data entries by each section in a chapter.\n    *   Formulated data entries into key-value pairs, with keys as section titles and values as content.\n    *   Data Entries Count: 1,930\n*   Material Science Dataset\n    *   Utilized the multiple-choice dataset SciQA.\n    *   Extracted \u201csupport\u201d column from the dataset that provides background knowledge for each question.\n    *   Each extracted \u201csupport\u201d is treated as a data entry, with keys as the knowledge piece and values as empty strings, emphasizing their concise and standalone nature.\n    *   Data Entries Count: 10,473\n*   Material Science Formula\n    *   Formulas collected from Wikipedia\u2019s dedicated pages for material science for-mulas."}, {"title": "Prompt for GPT-Generated Examples", "content": "Each formula is stored as key-value pair in the database, where the key represents the name of the formula and the value contains the formula equation itself.\n*   Data Entries Count: 57\n*   GPT-generated Examples\n    *   Used a specific prompt to generate 50 material science questions at a time, output in CSV format along with a confidence score. Please refer to Appendix B for the detailed prompt.\n    *   Human reviewers then selected questions with higher confidence scores for inclusion in the dataset.\n    *   Inspiration for question types was drawn from an external resource offering a wide range of material science questions and answers.\n    *   The key-value pairs were structured with questions as the keys and answers as the values.\n    *   Data Entries Count: 2,005\nPlease generate 50 instances of material science questions, specifically atomic structure and interatomic bonding, in a CSV format in the following order: question, answer, accuracy, confidence_score - accuracy: for factual questions, please evaluate the answer by comparing it with known facts. this field should be a number between 0 and 1. - confidence_score: how confident are you with the answer. this field should be a number between 0 and 1. Here are sample instances without accuracy and confidence_score: \"In terms of which of the following properties, metals are better than ceramics?\",\"ductility\" \"In the wave-mechanical model of an atom, what do degenerate energy levels have?\", \"equal energy\" \"Which of the following molecules is diamagnetic?\",\"CO\" - Examples of generated instances: \"What is the valence electron configuration of carbon?\",\"2s22p2\",0.95,0.85 - \"What type of crystal defect occurs when there is a line of irregularity in the lattice structure?\", \"dislocation defect\",0.96,0.91"}, {"title": "Tree-Structure MatSciKB", "content": "MatSciKB is organized as a hierarchical tree with the parent node \"Material Science\" branching into"}, {"title": "Tools Unified Interface Using LangChain", "content": "LangChain is an advanced framework designed to enhance applications that utilize LLM by offering standardized interfaces for various modules (LangChain contributors, 2023). This framework facilities the seamless integration and efficient management of LLM with external tools and systems. Utilizing LangChain, HoneyComb has developed a unified interface that standardizes the integration of a wide array of tools.\nIn HoneyComb, the unified interface provided by LangChain ensures that all tools, regardless of their specific function, are treated as standardized LangChain objects. This standardization is achieved by defining each tool with a consistent set of attributes:\n1. Function Signature: Each tool is defined with a clear function signature that specifies input and output types,\n2. Metadata Description: Each tool is accompanied by metadata that describes its purpose, suitable use cases, parameters description.\nExamples of function signatures and metadata descriptions in HoneyComb are:\n*   Google Search\n    *   Function Signature:\nGoogle_Search(query: str, timeout:\nOptional[int] = 30) -> str\n    *   Metadata Description: General web search for up-to-date information across various topics.\n*   Wikipedia Search\n    *   Function Signature:\nWikipedia_Search(topic: str, summarize:\nbool = True) -> str\n    *   Metadata Description: Retrieves and optionally summarizes detailed\nWikipedia articles, particularly useful for quick reference checks.\n*   A Sample Mass Flow Rate Tool\n    *   Function Signature:\ncalculate_initial_mass_flow_rate(args:\nstr) -> float\n    *   Metadata Description: See figure 4."}, {"title": "Examples of Inductive Tool Construction", "content": "See figure 5 for a detailed example illustrating how inductive tool construction work."}]}