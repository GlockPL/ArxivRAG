{"title": "Hashing for Protein Structure Similarity Search", "authors": ["Jin Han", "Wu-Jun Li"], "abstract": "Protein structure similarity search (PSSS), which tries to search proteins with\nsimilar structures, plays a crucial role across diverse domains from drug design\nto protein function prediction and molecular evolution. Traditional alignment-\nbased PSSS methods, which directly calculate alignment on the protein structures,\nare highly time-consuming with high memory cost. Recently, alignment-free\nmethods, which represent protein structures as fixed-length real-valued vectors, are\nproposed for PSSS. Although these methods have lower time and memory cost\nthan alignment-based methods, their time and memory cost is still too high for\nlarge-scale PSSS, and their accuracy is unsatisfactory. In this paper, we propose a\nnovel method, called protein structure hashing (POSH), for PSSS. POSH learns\na binary vector representation for each protein structure, which can dramatically\nreduce the time and memory cost for PSSS compared with real-valued vector\nrepresentation based methods. Furthermore, in POSH we also propose expressive\nhand-crafted features and a structure encoder to well model both node and edge\ninteractions in proteins. Experimental results on real datasets show that POSH can\noutperform other methods to achieve state-of-the-art accuracy. Furthermore, POSH\nachieves a memory saving of more than six times and speed improvement of more\nthan four times, compared with other methods.", "sections": [{"title": "Introduction", "content": "Proteins play essential roles in biological systems by binding with various ligands. Proteins with\nsimilar structures often share similar functions. Hence, protein structure similarity search (PSSS),\nwhich tries to search proteins with similar structures, plays a crucial role across diverse domains from\ndrug design to protein function prediction and molecular evolution.\nTraditional PSSS methods, which are often called alignment-based methods, directly calculate\nalignment on the protein structures. Representative alignment-based methods include TM-align [1],\nMATT [2] and several others [3-5]. Since identifying an optimal alignment between a pair of\nstructures is an NP-hard problem [6], alignment-based methods are typically highly time-consuming\nwith high memory cost even if heuristic techniques are adopted in these methods. Taking TM-align\nas an example, aligning a protein structure with 200 amino acids against the SCOPe database [7]\nwith 14323 protein structures takes approximately 30 minutes. Note that this is the time cost for\nonly a single query. The memory cost for storing the SCOPe database is approximately 4GB. With\nthe development of Cryo-EM and protein structure prediction techniques, such as Alphafold 2 [8],\nRoseTTAFold [9] and ESMFold [10], the number of proteins with known structures grows rapidly.\nFor example, Alphafold 2 has predicted structures with high confidence for over 200 million proteins,\nwhich has been used to construct the Alphafold database [11]. On large-scale datasets like Alphafold\ndatabase, alignment-based methods will have huge time and memory cost, even become infeasible."}, {"title": "Related Works", "content": "PSSS Given a query protein, PSSS tries to search (return) similar proteins from a protein database,\nwhere the similarity between two proteins is computed based on the three-dimensional structures of\nthese two proteins. Existing PSSS methods can be categorized into alignment-based methods and\nalignment-free methods.\nTemplate modeling score (TM-score) [16] is a well-known metric for assessing the topological\nsimilarity of protein structures, which has been adopted in the evaluation of Critical Assessment\nof protein Structure Prediction (CASP) [17]. TM-align [1] is an alignment-based method using\nTM-score. TM-align employs heuristic dynamic programming (DP) to generate residue-to-residue\nalignments, but the process of performing DP on the similarity score matrix is time-consuming. There\nhave also appeared other alignment-based methods like [3, 4, 2, 5]. However, none of them can avoid\nthe time-consuming procedure of aligning the coordinates of individual atoms between structures.\nFurthermore, since these alignment-based methods directly perform alignment on the raw protein\nstructures, the memory cost for storing the raw protein structures is also high.\nSeveral alignment-free methods have been proposed to overcome the inefficiency of alignment-based\nmethods by representing protein structures as fixed-length real-valued vectors, which can be further\ncategorized into non-learning methods and learning-based methods. Non-learning methods use\nhand-crafted features to represent the protein structures. For example, SGM [12] represents protein\nstructures using 30 structural measures, and SSEF [13] utilizes secondary structure information\nto map structures into vectors. The shortcoming of non-learning methods is that the hand-crafted\nfeatures fail to model the complex and irregular three-dimensional protein shape. Hence, learning-\nbased methods are proposed to learn more informative representations from the hand-crafted features.\nDeepFold [14] and GraSR [15] are two representative learning-based methods. DeepFold extracts a\ndistance map from the protein structure and employs convolutional neural network (CNN) to learn a\nvector representation, but it neglects the graph property of protein structures. GraSR [15] models the\nprotein structure as a graph with node features and utilizes long short-term memory (LSTM) [19]\nand graph convolutional network (GCN) [20] to update the node features. However, GraSR only\nconsiders the relations of local neighboring nodes and fails to model the edge interactions in proteins."}, {"title": "Method", "content": "Figure 1 illustrates the architecture of our proposed POSH for PSSS. The architecture comprises\nseveral key components: graph construction, structure encoder, hashing layer and contrastive learning\ncomponent for objective function, and a data sampling strategy to sample negative samples and\nsubstructures of positive samples."}, {"title": "Graph Construction", "content": "We construct a graph to represent each protein, and introduce several techniques to extract features\nfor both nodes and edges from the raw protein structure.\nGraph Structure A protein is represented as a graph G(V, E) with V denoting the set of nodes\nand E denoting the set of edges. Each amino acid corresponds to a node in the graph, and the edges\nare constructed by connecting each node with its k nearest neighbors (kNN). The distance between\nnodes is measured by the Euclidean distance between the raw node features of the Ca atoms. The\nraw node features for distance computation are introduced in the following content.\nRaw Node Features Although the TM-score of proteins is typically computed based on the\ncoordinates of Ca atoms, we also incorporate other backbone atoms to construct node features.\nBecause Ca atoms and the other backbone atoms are constrained by the peptide plane and\nform the 3D protein shape together, it is reasonable to include them in node features. In par-\nticular, we calculate the bond and dihedral angles for each residual as our node features. As\nshown in Figure 2, the bond angle is the angle formed between two covalent bonds that share\na common atom, and the dihedral angles, also known as torsion angles, refer to the angles be-\ntween planes defined by four consecutive atoms in the protein backbone. For residual i, we de-\nnote the bond angles of Ni-Ca\u2081-Ci, Ci-1-Ni-Cai, Cai-Ci-Ni+1 as \u03b1i, \u03b2\u03af, \u03a5\u03af, and the dihedral an-\ngles of Ni-1-Cai-1, Cai-1-Ci\u22121, Ci-Ni as \u03c6i, \u03c8i, \u03c9i. The final node features are computed by\n{sin, cos} \u2022 {\u03b1i, \u03b2\u03af, \u03a5\u03af, \u03a6\u03af, \u03a8i, Wi}. We use V \u2208 Rnxdv to denote these hand-crafted node fea-\ntures (also called raw node features), where n is the number of nodes and du is the dimension of the\nraw node features."}, {"title": "Raw Edge Features", "content": "Unlike existing learning-based methods which do not use edge features, we\nadditionally extract edge features for better description of the structure. We consider the following\nfive common types of atoms in the protein chain: C, Ca, N, O, and C\u00df. If a C\u00df atom is missing, we\nemploy a technique in [26] to add a virtual C\u00df atom based on the constraint of the other backbone\natoms. To calculate the distance between the ith node and the jth node, we compute the Euclidean\ndistance || Xi - Y;||, where X is chosen from the set {Ci, Car, Ni, Oi, C\u00df; }, and Y; is chosen from\nthe set {Cj, Caj, Nj, Oj, C\u00df; }. Here, X\u2081 and Y; represent the respective coordinates of the atoms.\nSubsequently, we encode the distances using a Gaussian radial basis function (RBF). The final\nrepresentation of the edge features is computed by RBFk(||Xi \u2013 Yj ||), where RBFk denotes the kth\nRBF employed in the encoding process. We use E \u2208 Rm\u00d7de to denote these hand-crafted edge\nfeatures (also called raw edge features), where m is the number of edges and de is the dimension of\nthe raw edge features."}, {"title": "Structure Encoder", "content": "We propose a novel structure encoder, which is a deep graph neural network with L layers, to learn\nmore informative node features from the hand-crafted raw features constructed in Section 3.1. The\nstructure encoder consists of node update layer and edge update layer. We first map the hand-crafted\nraw features of node i (the ith row of V) and the hand-crafted raw features of edge k (the kth row of\nE) to have the same hidden dimension with a linear mapping, to obtain the initial node representation\nh and edge representation ej, respectively. Here, edge k is from node i to node j, and its initial\nrepresentation is ej.\nNode Update Layer For each node i in layer l, we update its representation h by aggregating\nfeatures from its neighboring nodes and adjacent edges. This message passing mechanism is defined\nas follows:\n$u_i^l = h_i^{l-1} + \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\text{NodeMLP}(h_j^{l-1} || e_{ij}^{l-1}),$\n$h_i^l = \\Phi (h_i^{l-1} + \\text{MLP}(u_i^l)),$\nwhere Ni denotes the index set of neighboring nodes of node i and |Ni| denotes the number of\nnodes in Ni. || denotes the concatenation operation. NodeMLP is a two-layer feed-forward neural\nnetwork. The updated node representation u is further updated by a MLP, which also denotes\na two-layer feed-forward neural network and a residual connection from h is then added. \u03a6 is a\nfollow-up batchnorm function. This straightforward message passing mechanism is memory efficient\nso that we can sample more negative samples in a single mini-batch, the detail of which will be\nintroduced in Section 3.4.\nEdge Update Layer As proved in [27, 28], neglecting the update of edge features could lead to\nsuboptimal performance. Hence, we also perform edge message passing in our method. For each edge\n(i, j), we aggregate features from the connected node i and node j, the result of which is then added"}, {"title": "Objective Function", "content": "The output of the structure encoder is denoted as h, where L is the number of stacked layers. For\neach protein structure t, the representation of all the nodes in protein structure t is {h}iev\u2081.The\nfinal representation yt of protein structure t is computed by yt = Linear(fp({h}iev\u2081)), where\nfp is a max pooling function, and Linear is a linear mapping. Note that the resulting vector yt is\nreal-valued at this stage.\nLet bt \u2208 {\u22121,1}d denote the binary hash code\u00b2 for representing protein structure t. We can get\nbt = sign(yt). To enable end-to-end learning of the binary hash code, we add a hashing layer to the\nmodel. More specifically, we introduce a loss to encourage yt to approach the binary hash code bt as\nin [21]:\n$\\mathcal{L}_{\\text{hash}} = \\sum_t ||y_t - b_t||_2^2 + \\gamma \\sum_t (y_t^T \\textbf{1})^2,$\nwhere \u03b3 is a hyperparameter and the term $(y_t^T \\textbf{1})^2$ is designed to avoid bias or skewed representations\nof protein structures.\nEach time, we sample protein structures containing one protein structure as query, one positive\nsample, and K negative samples. Here, positive samples are those labeled to be similar to the query,\nand negative samples are those labeled to be dissimilar to the query. We denote the query sample\nas Q, the positive sample as P, and the negative samples as F1, F2,\uff65\uff65\uff65, FK. As depicted in Figure\n3 (a), our objective is to minimize distance for similar samples and maximize distance for dissimilar\nones. To achieve this, we employ the InfoNCE loss [29] from contrastive learning, which aims to\nminimize the negative log-likelihood of the similar pairs. To ensure stability during training and"}, {"title": "Data Sampling Strategy", "content": "In this subsection, we introduce how to sample positive and negative samples according to the query\nsample, and a substructure sampling strategy is further proposed to enhance the diversity of positive\nsamples.\nTraining Data Sampling We first calculate the pairwise TM-score [16] for protein structures using\nTM-align [1], serving as the ground-truth measurement of structural similarity. In the training phase,\nwe define two protein structures, denoted as (Pa, Pb), to be similar based on the following criterion:\nif the TM-score between Pa and Po is larger than a threshold p multiplied by the maximum TM-score\nbetween Pa and any other structure P\u2081 in the training set D excluding Pa, as shown below:\n$\\text{TM}(P_a, P_b) \\geq \\rho \\cdot \\max \\{\\text{TM}(P_a, P_i) | P_i \\in D \\backslash P_a \\},$", "Where": "TM(\u00b7) denotes the TM-score, \u03c1 is a hyperparameter in (0, 1). In each time, we sample a\nmini-batch of data comprising one protein structure as query, one positive sample, and K negative\nsamples. The positive sample means structurally similar to the query, while the negative samples are\ndissimilar.\nSubstructure Sampling The above sampling strategy might cause an issue. We observe that some\nprotein structures have few (less than 5) similar protein structures (positive samples), and hence, the\nsame structure will be repeatedly sampled during training. This could result in a potential risk of\noverfitting. This is caused by those structure pairs with high TM-score, which improves the threshold\nof similar pairs in Eq.(7). To address this issue, we further propose a substructure sampling strategy\nto enhance the diversity of positive samples. This strategy has been used in the pretraining of protein\nstructures [30, 28]. However, directly applying the empirical sampling length or ratio in these existing\nmethods will lead to a significant decline in performance for our method.\nDue to the sensitivity of positive samples to protein structure variations, casually sampling a sub-\nstructure could make a positive sample no longer similar to the query. In this paper, we propose a"}, {"title": "Experiment", "content": "Datasets We employ three datasets in our experiment, which are SCOPe v2.07 [7], ind_PDB [15],\nand Alphafold database [11]. SCOPe is a database to study protein structural relationships, which has\nbeen utilized in existing works [14, 15]. To ensure a fair comparison, we employ the same filtering\ncriteria as in [15], and the resulting dataset contains 14,215 protein structures. ind_PDB has also\nbeen utilized in [15], which consists of 1,900 protein structures collected from the Protein Data\nBank [32]. AlphaFold database contains over 200 million protein structures predicted by Alphafold\n2. Following existing work [14, 15], we first conduct 5-fold cross-validation on the SCOPe dataset\nand then evaluate the performance on the ind_PDB dataset. AlphaFold database is mainly used for\ncomparing memory cost.\nMetrics We employ three metrics to evaluate the accuracy: the area under the receiver operating\ncharacteristic curves (AUROC), the area under the precision-recall curves (AUPRC), and the Top-k\nhit ratio. For each query, we calculate the AUROC and AUPRC, and report the average value across\nall queries. The Top-k hit ratio is computed as $\\frac{1}{N_{PQ}} \\sum_{i=1}^{N_{PQ}} \\frac{N_{hit}}{\\min(k, N_{nbr})}$, where NPQ represents the\nnumber of query structures, Nhit denotes the number of correctly identified similar structures in the\ntop-k rankings, and Nabr indicates the total number of similar structures for the given query PQ.\nOur evaluation considers the Top-k values of Top-1, Top-5, and Top-10. We follow the settings\nof existing works [14, 15], considering structure pairs with a TM-score larger than or equal to\n0.9max{TM(PQ, Pi)|P; \u2208 D} as similar pairs.\nTo prove the efficiency of POSH, we also compare POSH with baselines in terms of time and memory\ncost.\nBaselines We adopt four alignment-free methods as baselines in our experiment, which include\nSGM [12], SSEF [13], DeepFold [14] and GraSR [15]. SGM and SSEF are non-learning methods.\nDeepFold and GraSR are learning-based methods. The results of SGM and SSEF are obtained by\nrunning their provided scripts. For DeepFold, we train it on our dataset using the same settings\ndescribed in the original paper. The results of GraSR are directly copied from its original paper for\ncomparison since we utilize the same dataset and filtering criteria."}, {"title": "Results", "content": "During the testing phase, we aim to search protein structures similar to a given query structure from a\ndatabase. In our experiment, the training set serves as the database, and the structures in the validation\nor test set serve as queries. The ranking of structures is based on the Hamming distance between the\nbinary vectors of protein structure pairs."}, {"title": "Ablation Study", "content": "We conduct ablation study on the edge updating scheme, substructure sampling strategy, and distance\nscaling strategy of POSH. The results are presented in Table 4. To verify the effectiveness of our\nedge features in capturing the structural similarity, we replace the raw edge features with those of\nGraSR and correspondingly remove the edge update layer. We can find that the hand-crafted raw edge\nfeatures we construct and the structure encoder we design contribute significantly to the improvement\nof accuracy. We can also find that our substructure sampling strategy contributes 1% to 2% for Top-k\nhit ratio. The results also verify the importance of adopting distance scaling to differentiate the\nsamples with the same Hamming distance."}, {"title": "Conclusion", "content": "In this paper, we propose a novel method called POSH for protein structure similarity search. POSH\nlearns a binary vector (hash code) representation for each protein structure, which can dramatically\nreduce the time and memory cost compared with real-valued vector representation based methods.\nExperimental results show that POSH can outperform other methods to achieve the best performance,\nin terms of accuracy, time cost and memory cost."}, {"title": "Implementation Details", "content": "In our implementation, we set the value of \u03b3 to 0.2 and 1 to 0.5. The temperature coefficient T is\nset to 0.07. In each time, we sample 64 proteins: one query, one positive sample, and 62 negative\nsamples. To ensure stable training, we utilize gradient accumulation with 40 accumulation steps. The\nnumber of layers in the structure encoder is 6. The code length of the binary hash code is set to 400,\nwhich is the same as that of the real-valued vectors used in existing learning-based methods [14, 15].\npis set to 0.9. We employ the Adam optimizer with a learning rate of 0.0003 for optimization. Our\nmodel is trained on NVIDIA RTX A6000 GPUs, and each model is trained up to 100 epochs."}, {"title": "Comprehensive Analysis", "content": "In Figure 5, we provide a comprehensive comparison of accuracy, time cost, and memory cost\nfor SGM, SSEF, GraSR and POSH. We can find that POSH can achieve faster speed, consume\nless memory, and conduct more accurate search simultaneously. Compared with the existing most\nefficient method SGM, POSH achieves an accuracy improvement of 20.29%. Compared with the\nexisting baseline of best accuracy (GraSR), POSH achieves a memory saving of 32 times and speed\nimprovement of four times. The result shows the promising potential of POSH in PSSS."}, {"title": "Code Length Experiment", "content": "To investigate the effect of code length, we train four models with code length to be 64, 128, 256,\n400, and 512 respectively. The result is shown in Table 5. We can find that with the growing code\nlength, the accuracy of the models consistently improves. Larger code length will lead to larger time\nand memory cost. Hence, in real applications, we need to choose a suitable code length to achieve\na good trade-off between accuracy and cost. Our POSH method provides a good choice to achieve\nsuch a trade-off."}, {"title": "Limitations", "content": "In this section, we will discuss its limitations. Firstly, in the hashing layer of POSH, various hashing\ntechniques can be explored to address the binarization problem. Future research can investigate\nwhether there are more suitable hashing techniques specifically designed for protein structure hashing.\nSecondly, as the number of known protein structures increases, utilizing more protein structure data\nin our training set becomes possible. However, the upper limit of our model's performance when\nscaling up with the amount of data still needs to be explored. It is important to consider the challenges\nassociated with using the original time-consuming alignment-based method for similarity calculation\nas the dataset grows, along with the cost of model training."}]}