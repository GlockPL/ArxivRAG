{"title": "Neural Combinatorial Clustered Bandits for Recommendation Systems", "authors": ["Baran Atalar", "Carlee Joe-Wong"], "abstract": "We consider the contextual combinatorial bandit setting where in each round, the learning agent, e.g., a recommender system, selects a subset of \"arms,\" e.g., products, and observes rewards for both the individual base arms, which are a function of known features (called \"context\"), and the super arm (the subset of arms), which is a function of the base arm rewards. The agent's goal is to simultaneously learn the unknown reward functions and choose the highest-reward arms. For example, the \"reward\" may represent a user's probability of clicking on one of the recommended products. Conventional bandit models, however, employ restrictive reward function models in order to obtain performance guarantees. We make use of deep neural networks to estimate and learn the unknown reward functions and propose Neural UCB Clustering (NeUClust), which adopts a clustering approach to select the super arm in every round by exploiting underlying structure in the context space. Unlike prior neural bandit works, NeUClust uses a neural network to estimate the super arm reward and select the super arm, thus eliminating the need for a known optimization oracle. We non-trivially extend prior neural combinatorial bandit works to prove that NeUClust achieves O$\\left(\\sqrt{dT}\\right)$ regret, where d is the effective dimension of a neural tangent kernel matrix, T the number of rounds. Experiments on real world recommendation datasets show that NeUClust achieves better regret and reward than other contextual combinatorial and neural bandit algorithms.", "sections": [{"title": "Introduction", "content": "The contextual combinatorial bandit (CC-MAB) setting has been applied to content recommendation such as movies (Qin, Chen, and Zhu 2014), where we recommend multiple movies to a user or select multiple users to whom a movie will be recommended. In this application, an agent is confronted with a set of choices, or \"arms,\" e.g., a set of movies, each of which will yield an unknown reward that depends on a known context, e.g., movie genre. Over the course of several rounds, the agent must learn to select the \"best\" arms. We consider the usual semi-bandit feedback setting where in each round, the user receives the individual (base arm) rewards of the arms selected, which are functions of their contexts, as well as a total (super arm) reward, which is a function of the base arm rewards. However, both the base arm and the super arm reward functions are stochastic and unknown to the user, and we aim to learn these reward functions in an online manner in order to select the arms which would yield the highest rewards for a given context. The main challenge in the CC-MAB and bandits setting in general is to balance between exploiting arms that we already know to be good and exploring new arms that might be better. A relatively new approach to the exploration-exploitation tradeoff has been to use neural networks to learn the base arm reward functions.\nDrawbacks of existing formulations. Modeling reward functions with neural networks introduces greater expressibility into the rewards learned, but existing neural and non-neural combinatorial bandit works generally require the existence of an exact or approximation oracle (Hwang, Chai, and Oh 2023; Elahi et al. 2023; Chen, Wang, and Yuan 2013; Chen, Xu, and Lu 2018), which selects the super arm after being given as input the estimated reward/quality of each base arm. This oracle, however, may be difficult to derive a priori, before the super arm reward function is known. Indeed, many combinatorial bandit problems are NP-hard and may not even have known approximation algorithms (Kong et al. 2021). We eliminate the need for an oracle by leveraging the structure of recommendation problems. In particular, we recognize that the context space of many recommendation problems has a clustered structure. Thus, we can cluster arms by clustering their contexts and use the resulting groupings, aided by an additional neural network-based estimation of super arm rewards, to guide our search for the optimal super arm.\nChallenges of our approach. To estimate the performance of the clusters, we need an expression for the super arm reward for the base arms in each cluster. We do so by again using a neural network to model the super arm reward and taking advantage of the recommendation problem structure: super arm reward functions in recommendation settings are often monotonic, since users are more likely to prefer a group of recommended products if they like each individual product. Thus, we can utilize a monotonic neural network to estimate the super arm reward. One technical challenge is incorporating this structure into a theoretical regret bound: by using a second neural network for the super arm selection, we cannot use existing proof approaches that use an oracle (Hwang, Chai, and Oh 2023). Another challenge is accounting for the effect of the error in clustering the context space on the regret bound."}, {"title": "Related Work", "content": "There have been some works in the bandit literature adaptively cluster users (arms) such as (Gentile, Li, and Zappella 2014), (Bui, Johari, and Mannor 2012) but their problem setting is not of a combinatorial nature and thus cannot be directly applied to our setting. Most commonly, prior works make certain parametric assumptions about the model to be learned such as linearity with respect to the context (Wen, Kveton, and Ashkan 2015; Zong et al. 2016; Chu et al. 2011). Towards more general reward models, some discretization based approaches use fixed or adaptive discretization (Chen, Xu, and Lu 2018; Nika, Elahi, and Tekin 2020) of the context space to exploit similarity of context features. However, such methods could scale poorly to large context spaces, and the difficulty of the problem highly depends on the complexity of the super arm reward model.\nMore recently, there has been a growing literature of using neural networks to facilitate learning in the bandit setting. These advancements exploit recent results on the generalization of deep neural networks (Jacot, Gabriel, and Hongler 2021; Arora et al. 2019; Cao and Gu 2019) and require no parametric assumptions on the reward as a function of the context other than an upper bound (Zhou, Li, and Gu 2020; Zhang et al. 2021). Yet while the field of neural contextual bandits is relatively well explored, to our knowledge, there is only one neural contextual combinatorial bandit study in the literature with regret bounds (Hwang, Chai, and Oh 2023). However, unlike our work they only use one neural network to learn the base arm reward function and rely solely on an exact oracle to do super arm selections. (Ban, He, and Cook 2021) chooses multiple arms in each round, but their setting is quite different since they assume K bandits and select an arm from each bandit in every round."}, {"title": "Our Contributions", "content": "In this work, we propose a provably efficient contextual combinatorial neural bandit algorithm while making mild assumptions about the reward functions. Our proposed algorithm makes use of two neural networks to learn the base arm and super arm reward functions, as shown in Figure 1. We formulate our problem in the Problem Formulation Section and then make the following contributions:\n\u2022 We propose a neural contextual combinatorial clustered bandit algorithm (NeUClust) in the Proposed NeUClust Algorithm Section. To the best of our knowledge, it is the first to use neural networks to learn the base arm and super arm reward functions while making use of clustering to exploit the underlying structure of the context space to guide super arm selections.\n\u2022 We prove (in Regret Analysis and Proofs) that our proposed algorithm achieves $\\tilde{O}\\left(\\sqrt{dT}\\right)$ regret over T rounds, where d is the effective dimension of a neural tangent kernel matrix.\n\u2022 We show through experimental results on real world recommendation datasets (MovieLens, Yelp) that our algorithm outperforms state-of-the-art neural contextual/-combinatorial and CC-MAB algorithms (Experiments)."}, {"title": "Problem Formulation", "content": "We first introduce some useful mathematical notation before outlining our CC-MAB model, the neural network models we employ for the reward functions, and our clustering-based method for selecting arms in each round. For a vector x \u2208 Rd, we represent its l2 norm by $||x||_2$, l1 norm by $||x||_1$, lo norm by $||x||_0$, and transpose by $x^\\mathsf{T}$. The l2 norm by a positive definite matrix A is defined by $||X||_A := \\sqrt{x^T Ax}$. We denote by [N], N \u2208 Z+, the set {1, 2, . . ., N}. We let $1_K = (1, 1, ..., 1) \\in \\mathbb{R}^K$ denote the ones vector."}, {"title": "Contextual Combinatorial Bandit Setting", "content": "In this paper, we consider a contextual combinatorial bandit with N being the total number of arms and T the total number of rounds. At the start of every round t, the agent observes the context vectors of all arms, denoted by ${x_{t,i} \\in \\mathbb{R}^d | i \\in [N]}$, and chooses a subset of the available arms, namely a super arm $S_t \\subset [N]$ with a fixed size, or budget, K, i.e., $|S_t| = K$. We denote by $S$ the set of all feasible super arms with cardinality K, i.e., $S = {S \\subset [N] ||S| = K}$. In the application of movie recommendation, for example, the base arms could correspond to users where there is an incoming movie at every round which we would like to recommend to a subset of users. The context would correspond to past information about the ratings the users gave to movies and the genres of the movies the users rated. When a super arm $S_t \\in S$ is selected in round t, the agent observes the base arm rewards of the chosen super arm, namely ${r_{t,i}}_{i \\in S_t}$, and receives a total (super arm) reward of $R(S_t, r_t)$ where $r_t = [r_{t,i}]_{i \\in S_t}$, hence the super arm reward is a function of the individual base arm rewards. This type of bandit feedback at every round is often referred to as semi-bandit feedback, which is common in combinatorial bandits (Audibert, Bubeck, and Lugosi 2014). In the movie recommendation example setup, the base arm and super arm rewards would correspond to the rating given by one user and the collection of users we selected, which would be information that would be available to us. We use these observed super arm and base arm rewards in every round to train our neural networks as explained in more detail in the Proposed NeUClust Algorithm Section. We assume that the base arm rewards $r_{t,i}$ are generated as follows:\n$r_{t,i} = h(x_{t,i}) + \\xi_{t,i}$                                                            (1)\n$\\forall t \\in [T]$ and $i \\in [N]$ where h is an unknown function satisfying 0 \u2264 h(x) \u2264 1 and $\\xi_{t,i}$ is \u03b6-sub-Gaussian noise satisfying $E[\\xi_{t,i}|X_1, S_1, ..., X_{t-1},S_{t-1}] = 0$ where $S_t \\in S$ denotes the selected super arm at round t. This is a standard assumption for the stochastic bandit setting. In our movie recommendation setting, (1) models the fact that the rating that a user gives to a movie mainly depends on the user's genre preferences, which have some uncertainty associated with it as movie genre is not the sole factor influencing how much a user likes a movie."}, {"title": "Base Arm Reward Function", "content": "To learn the base arm reward function h, we use a fully connected neural network with depth L + 1 \u2265 3 defined as follows (Zhou, Li, and Gu 2020; Zhang et al. 2021):\n$f(x; \\theta) = \\sqrt{m}W_L \\sigma(W_{L-1} \\sigma(...\\sigma(W_0 x)))$                                                    (2)\nwhere \u03c3(x) = max{x, 0} is the rectified linear unit (ReLU) activation function and $\\theta = [vec(W_0)^\\mathsf{T}, ...,vec(W_L)^\\mathsf{T}]^\\mathsf{T} \\in \\mathbb{R}^P$ is the weight vector of the neural network where p = md + $m^2$(L-1) + m where m is the width of the hidden layers and for simplicity of analysis we assume the width is the same for every layer. We assume a depth of L + 1 to ensure that we have L hidden layers. This will simplify the notation in our later theoretical analysis. To denote the gradient of the neural network we use $g(x; \\theta) = \\nabla_\\theta f(x; \\theta) \\in \\mathbb{R}^P$. We make the following assumption about the base arm reward function h, which makes sense in many recommendation applications since the base arm reward function takes as input the average preference vector of a user according to the genres and the genre vector of a movie to output a rating. Hence if a user has a higher preference towards the genres of the movie, then the rating should also increase accordingly.\nAssumption 1. (Lipschitz Continuity) h($x_{t,i}$) is Lipschitz continuous with respect to the context vector $x_{t,i}$ of arm i, i.e. \u2203B' > 0 such that for any $x_{t,i},x_{t,i'}$ it holds that $|h(x_{t,i}) - h(x_{t,i'})| \\leq B' ||x_{t,i} - x_{t,i'} ||_1$"}, {"title": "Super Arm Reward Function", "content": "The super arm reward function R(S, r) is a function of the selected arms' rewards and gives an understanding about how good the selected arm combination is. Let us denote the base arm rewards vector of a super arm S in round t with context vector $x_{t,S} = [x_{t,i}]_{i \\in S}$ by $r_t$, and we denote the expected super arm reward by $E[R(S_t, r_t)] = u(S_t, h(x_{t,S}))$ where $h(x_{t,S}) = [h(x_{t,i})]_{i \\in S}$. In this work the super arm reward function can be any function which satisfies the following mild assumptions, which are common in the combinatorial bandit setting (Li et al. 2016; Qin, Chen, and Zhu 2014; Nika, Elahi, and Tekin 2020).\nAssumption 2. (Lipschitz Continuity) u(S, h) is Lipschitz continuous with respect to the expected base arm rewards h, i.e. \u2203 B > 0 such that for any h, h' it holds that $|u(S, h) - u(S, h')| \\leq B\\sum_{i \\in S}|h_i - h'_i|$\nAssumption 3. (Monotonicity) u(S,h) is monotone non-decreasing with respect to the expected base arm reward vector h so that for any possible S when $h_i \\leq h'_i, \\forall i \\in [N]$ then u(S, h) < u(S, h').\nIt should be noted that these assumptions do not restrict the generalizability of our work, since they hold in many real life applications of combinatorial bandits. For example in the previously mentioned movie recommendation setup, the super arm reward function could be the average rating given by the users to the movie that was recommended, which is then monotonic and Lipschitz continuous in each individual rating. Moreover, we do not require an oracle that knows how to select arms which would maximize the super arm reward function: as explained in Algorithm 1, we learn this through clustering and using the monotonicity assumption.\nTo learn the super arm reward function R(S, r), we use a structurally constrained neural network, inspired by the approach in MonoNet (Nguyen et al. 2023) which is composed of monotonically connected layers, ensuring a monotonic relationship between the inputs and the output. Let us denote layer k of the network by o(k) for k = 0, 1, ..., Lm and hence we can write $o^{(k+1)} = \\sigma(\\Theta^{(k)}o^{(k)} + b^{(k)})$, where $\\Theta^{(k)}$ is the weight matrix and b(k) is the bias. The idea to form a monotonic layer is to transform the weight matrix such that it only has non-negative entries, since this will ensure a monotonic relationship between the input and output of that layer. Hence for a monotonic layer we have:\n$o^{(k+1)} = \\sigma(q(\\Theta^{(k)})o^{(k)} + b^{(k)})$                                              (3)\nwhere q is the weight transform function applied element wise to every entry of the weight matrix and could be any function with nonnegative outputs. We use a neural network with same width for every layer n and with depth Lm + 1. By structuring the super arm network so as to guarantee monotonicity, we ensure that the monotonicity assumption holds and allow our theoretical guarantees to hold as well."}, {"title": "Regret Formulation", "content": "The main objective of the agent in this setting is to minimize the cumulative expected super arm regret, which is the standard performance metric for CC-MAB problems and measures the gap in reward between the optimal and selected arms. The regret is formally defined as follows:\n$\\mathcal{R}(T) = \\sum_{t=1}^{T}((u(S^*_t, h(x_{t,S^*}))) - u(S_t, h(x_{t,S_t}))) \\quad$                                                                                                                               (4)"}, {"title": "Proposed NeUClust Algorithm", "content": "In this section, we present our algorithm Neural UCB Clustering (NeUClust). NeUClust is a neural network based contextual combinatorial upper confidence bound bandit algorithm that makes use of two neural networks to learn the base arm reward function and the super arm reward function. However, since we do not utilize an oracle, choosing a super arm based on the optimism in the face of uncertainty (OIFU) principle (Lai, Robbins et al. 1985) with the super arm network is difficult. We overcome this challenge by exploiting our monotonicity assumption and clustering the context space. Intuitively, base arms with similar contexts should yield similar rewards, and thus are similarly likely to be part of the optimal super arm. In some settings, we can cluster the context space offline, e.g., we may have sufficient information about users movie genre preference to cluster users offline, but in other settings we may need to collect context observations while running NeUClust. We thus propose online and offline variants of our NeUClust algorithm; we present the online version in this section.\nNeUClust description. Algorithm 1 shows NeUClust's pseudocode (in Appendix). We first initialize the parameters of the first neural network, which aims to learn the base arm reward function, by randomly generating $\\theta_0 = [vec(W_0)^\\mathsf{T}, ...,vec(W_L)^\\mathsf{T}]^\\mathsf{T}$, where for each $l \\in [L]$, $W_l = (W^l;0,0, W^l)$ where each entry of $W^l$ is generated independently from $\\mathcal{N}(0,4/m)$ and $W_L = (w^\\mathsf{T}, -w^\\mathsf{T})^\\mathsf{T}$ with each entry of w independently sampled from $\\mathcal{N}(0,2/m)$. We initialize the parameters of the second neural network, which tries to learn the super arm reward function, by independently sampling each entry from $\\mathcal{N}(1/n, 1)$ where n is the width of the network.\nAt the start of each round t \u2208 [T], the agent observes the contexts of all arms ${x_{t,i}}_{i \\in [N]}$ and clusters the arms based on their contexts. NeUClust then uses the context information, and the output of the base and super arm neural networks and its gradients to construct an upper confidence bound (UCB) $U_{t,c;}$ of the expected reward for every arm $c_j$ in every cluster c. We then select the top K arms of each cluster based on the arm UCBs. The outputs of the base arm network (i.e., estimates of base arm reward) are fed through a ReLU function, to ensure they are nonnegative, and then taken as input to the second network to estimate the super arm reward. Then the upper confidence bound vector $V_t$ is used to select the cluster which has the highest average UCB of its corresponding arms. Then the top K arms of the chosen cluster are played as $S_t$ and the base arm rewards ${r_{t,i}}_{i \\in S_t}$ and super arm reward Rt are observed.\nOnce the rewards ${r_{t,i}}_{i \\in S_t}$ and Rt are observed, the context vector is updated according to the observed base arm rewards of the played arms. The neural network parameters $O_t$ and $\\Theta_t$ are updated by using gradient descent with step size $\\eta_1$ and $\\eta_2$ for J iterations to minimize the loss functions:\n$\\mathcal{L}(\\theta) = \\sum_{t'=1}^{t} ||f(x_{t',S_t}; \\theta) - r_{t'}||^2_2 + \\frac{\\eta \\lambda_1}{2} ||\\theta - \\theta_0||^2$                                                                 \n$\\mathcal{L}(\\Theta) = \\sum_{t'=1}^{t} (F(f(x_{t',S_t}); \\Theta) - R_{t'}))^2 + \\frac{\\eta \\lambda_2}{2} ||\\Theta - \\Theta_0||^2$                                             \nwhere $f(x_{t',S_t}; \\theta) = [f(x_{t,i})]_{i \\in S_{t'}}$, $\\mathcal{L}(\\theta)$ shows the loss minimized using l2-regularization for the base arm neural network and $\\mathcal{L}(\\Theta)$ is the loss minimized for the super arm neural network. F represents the super arm network here and $R_t$ is the observed super arm reward of the selected super arm $S_t$ and $r_t$ represents the vector of observed base arm rewards of the selected super arm. The hyperparameters $\u03bb_1$ and $\u03bb_2$ adjust for the level of regularization which centers at the randomly initialized weight vectors. We define the positive exploration scaling factor $\\gamma_t$ similar to (Hwang, Chai, and Oh 2023; Zhou, Li, and Gu 2020) as\n$\\gamma_t = \\Gamma_{1,t}\\sqrt{\\log\\left(\\frac{\\sqrt{\\det Z_t}}{\\sqrt{\\det \\Lambda_1}}\\right) + \\Gamma_{2,t}} - 2 \\log \\Delta + \\sqrt{\\lambda_1 S_t} + \\frac{tK}{\\lambda_1}} + \\frac{ \\left(\\Lambda_1 + C_1 tKL\\right) \\left(1 - (1 - \\eta_1 m \\lambda_1)^{tM}\\right)}{\\lambda_1} \\left(1 - (1 - \\eta_1 m \\lambda_1)^{tM}\\right)} + \\Gamma_{3,t}$                                                                                                                                                                                                                             (5)\nfor some $\\Gamma_{1,t}, \\Gamma_{2,t}, \\Gamma_{3,t} > 0$ (defined later in Appendix). Super arm selection without an oracle. As discussed above, our super arm selection first finds the optimal cluster (in terms of the super arm reward) and then selects the K arms with largest base arm rewards from that cluster. By monotonicity of the super arm reward, we thus maximize the super arm reward subject to the constraint that all base arms belong to the chosen cluster. For example, if a user generally likes movies of a particular genre, or more generally prefers a certain kind of product, our clustering should group movies of similar genres (products of similar type) together and then recommend them to the user. While restricting the selected base arms to a specific cluster may lead to a suboptimal reward, experimental results show that NeUClust outperforms neural combinatorial bandit algorithms that do not have this clustering restriction: such algorithms generally impose other restrictions on the super arm reward, e.g., the presence of an oracle. Moreover, we experimentally verify that real-world recommendation datasets naturally have a clustered structure in the Appendix.\nFor the offline variant of the algorithm, we do clustering only once at round t = 1 and do not update the context vector after observing the base arm rewards of the arms that were played. This algorithm would work well when the contexts have been formed with an abundance of prior offline data. For example for movie recommendation, this would mean that the context vector which represents the average rating the user gave to each genre is formed with a sufficient number of already available ratings from that user."}, {"title": "Regret Analysis", "content": "In this section we analyze and derive the regret of the presented algorithm NeUClust in Algorithm 1. We denote by ${x_i"}, {"theta)": "A_\\theta \\in \\mathbb{R"}, "d$\n$H^{(1)} = 2 \\Sigma^{(0)} \\Sigma^{(0)} \\quad \\Sigma_{i,j}^{(1)} = \\Sigma_{i,j}^{(0)}$\n$H^{(l+1)} = 2 H E_{\\theta, \\theta' \\sim \\mathcal{N}(0, A)} [\\sigma'(\\theta) \\sigma'(\\theta')"], "\u03f5_t": ""}