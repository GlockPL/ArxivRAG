{"title": "Backdoor defense, learnability and obfuscation", "authors": ["Paul Christiano", "Jacob Hilton", "Victor Lecomte", "Mark Xu"], "abstract": "We introduce a formal notion of defendability against backdoors using a game between an attacker and a defender. In this game, the attacker modifies a function to behave differently on a particular input known as the \"trigger\", while behaving the same almost everywhere else. The defender then attempts to detect the trigger at evaluation time. If the defender succeeds with high enough probability, then the function class is said to be defendable. The key constraint on the attacker that makes defense possible is that the attacker's strategy must work for a randomly-chosen trigger.\nOur definition is simple and does not explicitly mention learning, yet we demonstrate that it is closely connected to learnability. In the computationally unbounded setting, we use a voting algorithm of Hanneke et al. [2022] to show that defendability is essentially determined by the VC dimension of the function class, in much the same way as PAC learnability. In the computationally bounded setting, we use a similar argument to show that efficient PAC learnability implies efficient defendability, but not conversely. On the other hand, we use indistinguishability obfuscation to show that the class of polynomial size circuits is not efficiently defendable. Finally, we present polynomial size decision trees as a natural example for which defense is strictly easier than learning. Thus, we identify efficient defendability as a notable intermediate concept in between efficient learnability and obfuscation.", "sections": [{"title": "Introduction", "content": "A backdoor in a machine learning model is a modification to the model that causes it to behave differently on certain inputs that activate a secret \"trigger\". There is a wide literature on backdoor attacks and defenses from both a theoretical and an empirical perspective [Li et al., 2022]. However, prior theoretical work typically makes reference to a particular training dataset, leading to a focus on data poisoning attacks. In this work we introduce a formal notion of a backdoor that allows the attacker to modify the model arbitrarily. Our definition is simple, but nevertheless gives rise to a rich array of strategies incorporating both learning and obfuscation.\nOur formal notion is focused on backdoor detection at runtime, meaning that the defender is given a particular input and must flag whether or not it activates the backdoor trigger. We focus on this case because of the existence of undetectable backdoors if the defender is instead required to flag a model as backdoored without being given any particular input [Goldwasser et al., 2022]. Moreover, detection at runtime is sufficient in threat models where the attacker is given only one opportunity to modify the model, being akin to giving the defender an additional chance to modify the model after the attacker. We also focus on the white-box setting, in which the defender has access to a complete description of the model.\nOur main contributions, which are also summarized in Figures 1 and 2, are as follows:"}, {"title": "Related work", "content": "The most closely related work to our own is that of Goldwasser et al. [2022], whose main results use a digital signature scheme to insert an undetectable backdoor into a model. The backdoor trigger can be applied to any input by perturbing it appropriately. In the black-box setting, the backdoor is undetectable in the sense that it is computationally infeasible for the defender to find any input on"}, {"title": "Definition of \u025b-defendability", "content": "Our formal notion of defendability against backdoors is based on a game played between two players, an \"attacker\" and a \"defender\". Informally, the attacker starts with a function and is given a randomly sampled backdoor trigger. They must choose a backdoored function that almost always agrees with the original function, but disagrees with the original function on the backdoor trigger. Then the defender is given either the original function and a random input, or the backdoored function and the backdoor trigger, and they must distinguish one from the other with high confidence.\nFor our formal definition, we fix a security parameter $n \\in \\mathbb{N}$ and consider classifiers $X \\rightarrow$ {0,1} where $X = {0,1}^n$. Formally, a representation class over $X$ is a function ${0,1}^{R(n)} \\rightarrow$ {all functions $X \\rightarrow$ {0,1}} for some $R(n) \\in \\mathbb{N}$, but we generally keep the choice of representation implicit and conflate a representation class with the set of functions in its image. In our results on computationally constrained defenders, we also implicitly assume that $R(n)$ is polynomial in $n$.\nOur definition also makes use of the example oracle $Ex(f, D)$ where $D$ is a distribution over $X$ and $f : X \\rightarrow$ {0,1}. This is an oracle that outputs $(x, f(x))$ for a randomly sampled $x \\sim D$.\nDefinition 3.1. Let $F$ be a representation class over $X = {0,1}^n$ and let $\\varepsilon, \\delta > 0$. We say that $F$ is $\\varepsilon$-defendable with confidence $1 - \\delta$ if there is a probabilistic algorithm (the \"detection strategy\") that wins the following game with probability at least $1 - \\delta$:\n*   An adversary with knowledge of the detection strategy (the \"attacker\") chooses a distribution $D$ over $X$ and $f \\in F$.\n*   We randomly sample $x^* \\sim D$ (the \u201cbackdoor trigger\"), and then the adversary chooses $f^* \\in F$ (the \"backdoored function\").\n*   The backdoored function $f^*$ is called $\\varepsilon$-valid if\n$P_{x \\sim D} (f^* (x) \\neq f (x)) \\leq \\varepsilon \\quad \\text{but} \\quad f^* (x^*) \\neq f (x^*)$.\n*   We randomly choose $(f', x')$ to pass to the detection strategy to be either $(f, x)$ for a random sample $x \\sim D$, or $(f^*, x^*)$, with 50% probability each. The detection strategy is given access to the example oracle $Ex (f', D)$ as well as the representation of $f', x', f' (x'), \\varepsilon$ and $\\delta$ as input, and must output either Acc (for \"accept\") or REJ (for \"reject\").\n*   If $(f, x)$ is passed to the detection strategy, then the detection strategy wins if it outputs Acc. If $(f^*, x^*)$ is passed to the detection strategy, then the detection strategy wins if either it outputs REJ or $f^*$ is not $\\varepsilon$-valid."}, {"title": "Statistical defendability", "content": "In this section we will completely determine, up to a constant factor, the values of $\\varepsilon$ and $\\delta$ for which an arbitrary representation class is $\\varepsilon$-defendable with confidence $1 - \\delta$, in the absence of any computational constraints on the detection strategy.\nOur result is expressed in terms of the Vapnik-Chervonenkis (VC) dimension of the representation class $F$, denoted $VC(F)$. This is defined as the maximum size of a set $S$ of points shattered by $F$, meaning that all $2^{||}$ functions $S \\rightarrow$ {0, 1} are realized as the restriction $f|_S$ of some $f \\in F$.\nTheorem 4.1. Let $F$ be a representation class over ${0,1}^n$ and let $\\varepsilon > 0$. Then the most (i.e., supremum) confidence with which $F$ is $\\varepsilon$-defendable is\n$\\max (\\frac{1}{2}, 1 - \\Theta (VC (F) \\varepsilon))$\nas $VC(F) \\rightarrow \\infty$."}, {"title": "Efficient defendability", "content": "In Section 4, we related the defendability of a representation class to its VC dimension. However, the prediction strategy we used to construct the detection strategy runs in exponential time, since it involves an expensive search over orientations of a certain graph. Hence it is interesting to ask what a polynomial-time detection strategy can achieve. To explore this, we introduce the following notion.\nDefinition 5.1. Let $F$ be a representation class over ${0,1}^n$. We say that $F$ is efficiently defendable if there is some polynomial $p$ such that for any $\\delta > 0$ and any $\\varepsilon > 0$ with $\\varepsilon < \\frac{1}{p(n,\\delta)}$, $F$ is $\\varepsilon$-defendable with confidence $1 - \\delta$ using a detection strategy that runs in time polynomial in $n$ and $\\frac{1}{\\delta}$.\nNote that the condition that $\\varepsilon < \\frac{1}{p(n,\\delta)}$ is crucial: if $F$ were required to be $\\varepsilon$-defendable with confidence $1 - \\delta$ for any $\\varepsilon, \\delta > 0$, then this would not be possible even for a detection strategy with"}, {"title": "Efficient defendability and efficient PAC learnability", "content": "In this sub-section we show that efficient PAC learnability implies efficient defendability, but not conversely.\nThe well-studied model of probably approximately correct (PAC) learning was introduced by Valiant [1984]. The notion of efficient PAC learnability bears some resemblance to our notion of efficient defendability. The following definition is taken from Kearns and Vazirani [1994, Definition 4].\nDefinition. Let $F$ be a representation class over $X = {0,1}^n$. We say that $F$ is efficiently PAC learnable if there is a probabilistic algorithm (the \u201cPAC learning algorithm\u201d) with the following properties. The PAC learning algorithm is given access to the example oracle $Ex (f, D)$ for some distribution $D$ over $X$ and some $f \\in F$, as well as $0 < \\tilde{\\varepsilon} < \\frac{1}{2}$ (the \u201cerror parameter\") and $0 < \\tilde{\\delta} < \\frac{1}{2}$ (the \"confidence parameter\") as input. The PAC learning algorithm must run in time polynomial in $n, \\frac{1}{\\tilde{\\varepsilon}}$, and $\\frac{1}{\\tilde{\\delta}}$ and must output some polynomially evaluatable hypothesis $h$, i.e. a representation for a function $X \\rightarrow$ {0,1} that is evaluatable in time polynomial in $n$. The hypothesis must satisfy $P_{x \\sim D} (f (x) \\neq h(x)) \\leq \\tilde{\\varepsilon}$ with probability at least $1 - \\tilde{\\delta}$ for any distribution $D$ over $X$ and any $f \\in F$.\nNote that $\\tilde{\\varepsilon}$ and $\\tilde{\\delta}$ play subtly different roles in this definition to $\\varepsilon$ and $\\delta$ in the definition of efficient defendability. In particular, there is no condition on $\\tilde{\\varepsilon}$ as a function of $n$ and $\\tilde{\\delta}$ in the definition of efficient PAC learnability.\nWe now show that efficient PAC learnability implies efficient defendability. This follows easily from the proof of Theorem 4.1. Even though that result used a prediction strategy that runs in exponential time, it is straightforward to replace it by an efficient PAC learning algorithm.\nCorollary 5.1. Let $F$ be a representation class over ${0,1}^n$. If $F$ is efficiently PAC learnable, then $F$ is efficiently defendable."}, {"title": "Efficient defendability and obfuscation", "content": "We conclude our analysis of computational defendability with the following result, which essentially says that representation classes that are rich enough to support obfuscation are not efficiently defendable.\nTheorem 5.2. Assuming OWF and $iO$, the representation class $F$ of polynomial size Boolean circuits over $X = {0,1}^n$ is not efficiently defendable.\nHere, OWF again denotes the existence of a one-way function, and $iO$ denotes the existence of an efficient indistinguishability obfuscator. Roughly speaking, an efficient indistinguishability obfuscator is a probabilistic polynomial-time algorithm that takes in a circuit and outputs an \"obfuscated\u201d circuit with the same behavior. The circuit being \u201cobfuscated\" means that the obfuscations of two different circuits with the same behavior are not distinguishable by any probabilistic polynomial-time adversary. For a precise definition of an efficient indistinguishability obfuscator, we refer the reader to Sahai and Waters [2014, Section 3]. The study indistinguishability obfuscation was initiated by Barak et al. [2001], and an efficient indistinguishability obfuscator was constructed from well-studied computational hardness assumptions by Jain et al. [2021].\nWe use OWF in our proof of Theorem 5.2 to obtain a puncturable pseudorandom function. Roughly speaking, a puncturable pseudorandom function is pseudorandom function such that any key (i.e., seed) can be \u201cpunctured\" at a set of points. The key being \u201cpunctured\" means that, when run using the punctured key, the pseudorandom function behaves the same on unpunctured points, but to a probabilistic polynomial-time adversary with knowledge of the punctured key only, the function looks pseudorandom on punctured points when run using the original key. For a precise definition of a puncturable pseudorandom function, we again refer the reader to Sahai and Waters [2014, Section 3]. The observation that puncturable pseudorandom functions can be constructed from one-way functions was made by Boneh and Waters [2013], Boyle et al. [2014] and Kiayias et al. [2013].\nOur proof of Theorem 5.2 is an example of a hybrid argument: we show that two distributions are computationally indistinguishable via a sequence of intermediate distributions. For a precise definition of computational indistinguishability and further discussion of hybrid arguments, we refer the reader to Katz and Lindell [2007, Chapter 6.8]. The specific combination of a puncturable pseudorandom function and an efficient indistinguishability obfuscator has been used in a number of previous hybrid arguments, as described by Sahai and Waters [2014].\nWith these preliminaries in place, we are now ready to prove Theorem 5.2.\nProof of Theorem 5.2. Take $\\varepsilon = 2^{-n}$. We will show that, for any polynomial $p$, the representation class $F$ of polynomial size Boolean circuits is not $\\varepsilon$-defendable with confidence $\\frac{1}{2} + \\frac{1}{p(n)}$ using a detection strategy that runs in time polynomial in $n$.\nTo see this, by OWF, let $C_K \\in F$ be the circuit for a puncturable pseudorandom function with key $K \\in {0,1}^n$ and a single output bit, and by $iO$, let $io$ be an efficient indistinguishability obfuscator. The adversary proceeds by taking $D$ to be uniform over $X$, and takes $f = iO (C_K)$ for some $K \\in {0,1}^n$ chosen uniformly at random. As before, we may treat $f$ as if it were chosen randomly, since the detection strategy's worst-case performance can be no better than its average-case performance. Finally, given $x^*$, the adversary takes\n$f^* = iO \\begin{cases} C_{(PUNC(K,x^*))} (x), & \\text{if } x \\neq x^* \\\\ (1 - C_K (x^*)), & \\text{if } x = x^* \\end{cases}$"}, {"title": "Defendability of decision trees", "content": "In Section 5.2 we gave an example of a representation class for which defense is faster than learning, in the sense that it is efficiently defendable using a detection strategy that is faster than any PAC learning algorithm. However, our example was somewhat contrived, and was not polynomially evaluatable without access to a random oracle. In this section, we give an example of a more natural representation class for which defense is faster than learning: the class of polynomial size decision trees over {0,1}^n.\nDefinition. A decision tree over {0,1}^n is a rooted binary tree where each non-leaf node is labeled with one of the $n$ input variables and each leaf node is labeled with a 0 or a 1. To evaluate a decision tree, we start at the root node and go left if the variable evaluates to 0 and right if it evaluates to 1, continuing until we reach a leaf. The size of a decision tree is its number of leaves.\nIn our study of decision trees, we focus on the uniform-PAC model of learning, in which the distribution $D$ is always the uniform distribution. Thus we say that a representation class is efficiently uniform-PAC learnable to mean that it is efficiently PAC learnable as long as $D$ is uniform. Similarly, we say that a representation class is efficiently uniform-defendable to mean that it is efficiently defendable as long as $D$ is uniform. Note that efficient uniform PAC-learnability implies efficient uniform defendability, by specializing the proof of Corollary 5.1.\nAs far as we are aware, it is unknown whether the representation class of polynomial size decision trees over {0,1}^n is efficiently uniform-PAC learnable. Polynomial-time PAC learning algorithms are"}, {"title": "Discussion", "content": "A central theme of this work has been that learning can be used to perform backdoor defense, but backdoor defense cannot necessarily be used to perform learning. The first of these claims is encapsulated by Theorem 4.1 in the computationally unbounded setting and Corollary 5.1 in the computationally bounded setting. For the second of these claims, we have made several steps in this direction:"}, {"title": "Separating efficient defendability from efficient PAC learnability", "content": "*   We showed in Theorem 5.1 that efficient defendability does not imply efficient PAC learnability in the random oracle model of computation.\n*   We deduced from this that efficient defendability does not imply efficient PAC learnability in the usual model of computation either, as long as we allow representation classes that are not polynomially evaluatable, and we conjectured that there is a polynomially evaluatable counterexample assuming the existence of a one-way function.\n*   We showed in Theorem 6.1 that the representation class of polynomial size decision trees is efficiently uniform-defendable using a detection strategy that is faster than any possible learning algorithm.\nNevertheless, we still do not have an example of a natural representation class that is efficiently defendable but not efficiently PAC learnable. We consider finding such an example to be a central challenge for follow-up research. One possible candidate of independent interest is the representation class of shallow (i.e., logarithmic-depth) polynomial size Boolean circuits, or equivalently, polynomial size Boolean formulas."}, {"title": "Mechanistic defenses", "content": "Our detection strategy for decision trees in Theorem 6.1 is interesting not only because it is faster than any possible learning algorithm, but also because it works in a fundamentally different way. Given $(f', x')$ as input, it does not run the decision tree $f'$ on any inputs other than $x'$ itself, but instead exploits the mechanism by which the value of $f' (x')$ is computed, by looking at the depth of the corresponding leaf. We call a defense that exploits the structure of $f'$ in this kind of way mechanistic.\nThe existence of mechanistic defenses is another reason to suspect that there are other represen-tation classes for which defense is strictly easier than learning. However, some sophistication may be required to construct such defenses. For example, one might hope to detect a backdoor trigger for a Boolean formula by checking the pattern of inputs to each gate, and seeing how unlikely that pattern would be for a random input to the formula. Unfortunately, though, the following example, found by Thomas Read, presents an obstacle to such an approach.\nExample (Backdoor with likely input patterns). Consider the Boolean formulas $f, f^* : {0,1}^n \\rightarrow$ {0,1} given by $f (x_1,...,x_n) = x_n \\land x_{n-1}$ and\n$f^* (x_1,...,x_n) = (... ((x_1 \\lor x_n \\land x_2) \\land x_2 \\lor x_n \\land x_3) \\land x_3 \\lor ... \\lor x_n \\land x_{n-1}) \\land x_{n-1}$,\nwhere $ab$ is shorthand for $(a > b)$. By the distributive property, $f^* (x_1,...,x_n)$ is logically equivalent to $x_1x_2...x_{n-1} \\lor x_n \\land x_{n-1}$, and so $f$ and $f^*$ disagree only on the input $x^* := (1,...,1,0)$. But on a uniformly random input to the formula, every gate in $f^*$ receives every pattern of inputs with probability at least 1/8. By contrast, in the logically equivalent version of $f^*$, the subformula $x_1x_2...x_{n-1}$ is 1 on $x^*$, which only happens with probability $1/2^{n-1}$ on a uniformly random input.\nThis example suggests a possible mechanistic defense for Boolean formulas that involves first rewriting the formula. More generally, mechanistic defenses offer an exciting avenue for future research."}, {"title": "Implications for AI alignment", "content": "We are motivated to study backdoors as an analogy for deceptive alignment, the possibility that an advanced AI system would learn to behave cooperatively when there are subtle cues that it is in training, but uncooperatively when those cues are missing [Hubinger et al., 2019]. A deceptively aligned model is analogous to a backdoored function in that it behaves similarly to a fully cooperative model except on certain inputs that are rare during training. Thus, in this analogy, $D$ is the training distribution, $f$ is the fully cooperative model, $f^*$ is the deceptively aligned model, and $x^*$ is an input on which $f^*$ behaves uncooperatively.\nNote that allowing the backdoor to be detected at runtime is appropriate in this analogy, because we have the opportunity to modify $f^*$ or run a monitoring system. The flaw in the analogy comes from how the attacker is restricted. In our formal notion of defendability, the attacker must insert a backdoor that works for a randomly-chosen trigger. But in the case of deceptive alignment, there is no such restriction, and instead the backdoored behavior must arise from properties of the model architecture and the training distribution.\nFrom this perspective, backdoor defenses that rely on learning are unsatisfying, because our assumption is that a similar process of learning gave rise to deceptive alignment in the first place. The detection strategy in our proof of Theorem 4.1 used distillation plus ensembling to produce a regularized model, but this may be exploiting the flaw in our analogy by using resampling to avoid the randomly-chosen trigger. Moreover, we would have to rely on a fast approximation to this strategy, since the exact version runs in exponential time. Similar issues apply to other methods of regularization, such as the one discussed in Appendix A, although that alternative does show more promise.\nOn the other hand, mechanistic defenses, as discussed in Section 7.2, may fare better, since they work very differently. Intuitively, if a detection strategy could spot the mechanism by which a deceptively aligned model concluded that it was in training, then it should transfer well to the case of deceptive alignment. A mechanistic defense may be able to do this by exploiting the fact that this mechanism is active on the backdoor trigger but inactive on most random inputs. Unfortunately though, we are lacking in examples of mechanistic defenses, which why we are excited to see more research in this direction.\nOur result that polynomial size circuits are not efficiently defendable in Theorem 5.2 is also relevant to this analogy. Although it is unlikely that our indistinguishability obfuscator-based construction would arise out of ordinary model training, it is much more plausible for a trained model to be obfuscated in a more informal sense. Indeed, reverse engineering trained neural networks is an active area of research [Olah, 2022]. Hence the possibility of obfuscation poses a potential problem for detecting deceptive alignment. However, in the case of deceptive alignment, we also have access to the entire training process, including the training dataset, which may be enough information for us to detect the trigger despite any potential obfuscation. By analogy, in our construction in Theorem 5.2, it would be easy for the defender to detect the trigger if they had access to the unpunctured key $K$ that was used in the construction of $f^*$.\nThis motivates the study of variants of our formal notion of defendability that constrain the attacker in different ways, or provide more assistance to the defender. To better capture the analogy with deceptive alignment, we would be excited to see research into variants that provide the defender with more information about how the function they are given was constructed, to see if this makes defense computationally feasible."}, {"title": "Conclusion", "content": "We have introduced a formal notion of defendability against backdoors in which the attacker's strategy must work for a randomly-chosen trigger. Despite its simplicity, this notion gives rise to a rich array of strategies. In the absence of computational constraints, defense is exactly as hard as learning. Meanwhile, in the presence of computational constraints, defense is strictly easier than learning, but impossible for function classes that are rich enough to support obfuscation. We are excited to see future work that further explores the exact relationship between defense and learning."}]}