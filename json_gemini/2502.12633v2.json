{"title": "One Size doesn't Fit All: A Personalized Conversational Tutoring Agent for Mathematics Instruction", "authors": ["Ben Liu", "Jihai Zhang", "Fangquan Lin", "Xu Jia", "Min Peng"], "abstract": "Large language models (LLMs) have been increasingly employed in various intelligent educational systems, simulating human tutors to facilitate effective human-machine interaction. However, previous studies often overlook the significance of recognizing and adapting to individual learner characteristics. Such adaptation is crucial for enhancing student engagement and learning efficiency, particularly in mathematics instruction, where diverse learning styles require personalized strategies to promote comprehension and enthusiasm. In this paper, we propose a PersonAlized Conversational tutoring agEnt (PACE) for mathematics instruction. PACE simulates students' learning styles based on the Felder and Silverman learning style model, aligning with each student's persona. In this way, our PACE can effectively assess the personality of students, allowing to develop individualized teaching strategies that resonate with their unique learning styles. To further enhance students' comprehension, PACE employs the Socratic teaching method to provide instant feedback and encourage deep thinking. By constructing personalized teaching data and training models, PACE demonstrates the ability to identify and adapt to the unique needs of each student, significantly improving the overall learning experience and outcomes. Moreover, we establish multi-aspect evaluation criteria and conduct extensive analysis to assess the performance of personalized teaching. Experimental results demonstrate the superiority of our model in personalizing the educational experience and motivating students compared to existing methods.", "sections": [{"title": "1 Introduction", "content": "Intelligent Tutoring Systems (ITSs) are essential tools in education practice, providing immediate instruction and feedback to learners [24, 26]. In recent years, conversational ITSs have attracted significant attention [2, 27, 32, 41] due to their ability to engage students through natural language interactions, thereby facilitating students in problem-solving by providing hints in text form. With large language models (LLMs) demonstrating impressive capabilities in simulating human behavior [6, 34], extensive research has been conducted to utilize LLMs as tutoring agents across various domains, including science [35, 38], language learning [31], and social skills coaching [39]. These LLM-driven conversational ITSs hold promising potential to transform students' learning experiences and significantly improve both engagement and knowledge acquisition.\nHowever, existing LLMs-driven ITSs rely on predetermined scaffolding strategies developed by human tutors [9, 11], which restrict their ability to address individual student needs and deliver tailored step-by-step guidance. The interests, personality traits, and experiences of students result in various learning styles, directly affecting their capacity to comprehend and absorb knowledge. This limitation is particularly obvious in mathematics instruction, where the subtleties of individual understanding and problem-solving approaches are essential for effective teaching."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Intelligent Tutoring Systems", "content": "Research has demonstrated the effectiveness of Intelligent Tutoring Systems (ITS) in improving student engagement and learning efficacy. Notably, [5] emphasizes that ITS provide personalized instruction can lead to substantial improvements in student performance. This finding is supported by contemporary studies, such as [36], which confirm that ITS can significantly enhance learning outcomes when compared to traditional classroom settings. A meta-analysis by [45] further highlights the effectiveness of ITS in education, showing considerable gains in students' problem-solving skills and knowledge retention.\nExisting works design rule-based systems with human-crafted domain knowledge [15], or data-driven approaches [7, 23] that perform supervised learning on a certain amount of human annotation, enabling naturalistic exchanges between students and the system. This boosts student motivation and self-regulated learning, contributing to a more inclusive and engaging educational environment [3]."}, {"title": "2.2 AI in Education", "content": "Artificial Intelligence (AI) is transforming the educational landscape by enhancing learning experiences and personalizing educational journeys. Al technologies, such as machine learning, natural language processing, and data analytics, enable the development of adaptive learning systems, intelligent tutoring, and automated administrative tasks [25]. These innovations not only aid educators in identifying student needs but also create tailored learning paths that can improve engagement and outcomes.\nRecent large language models show strong potential to build dialogue tutors with less data supervision and higher confidence [1, 2, 22]. ITS can be further improved by integrating LLMs with pedagogical and learning science principles [9, 38, 44]. Additionally, recent works [30, 33] demonstrate the potential of LLMs for individual student modeling."}, {"title": "3 Methods", "content": "In this section, we first present the framework of our PACE. We then introduce the dialogue synthesis process for personalized mathematical teaching. Finally, we enhance PACE's teaching capabilities by fine-tuning the LLM on curated dialogue data."}, {"title": "3.1 Personality-aware Tutoring Agent", "content": "The framework of PACE, depicted on the right side of Figure 2, comprises three key stages: (1) simulating the learning style of each student based on their persona, (2) conceptualizing individualized teaching strategies that align with learning styles, and (3) guiding students toward profound thinking and stimulating their interest in mathematics through Socratic-style conversations. This streamlined process enhances personalized teaching and promotes a more engaging educational experience.\n3.1.1 Simulating Learning Styles. Personality is one of the most influential factors in education research [21], which significantly influences student engagement and knowledge acquisition. Students with different personas often display diverse learning styles, affecting how they absorb, process, comprehend, and retain information [12, 14]. Therefore, we assess students' personalities by simulating their learning styles, enabling us to tailor teaching strategies effectively.\nWe adopt the Felder and Silverman learning style model [14], which consists of eight main learning styles: Perception (sensory vs. intuitive), Processing (active vs. reflective), Understanding (sequential vs. global), and Input (visual vs. auditory). Since our agent relies on a dialogue-based interface, the Input learning style is constrained. Therefore, we focus on three distinct modes: Perception, Processing, and Understanding. Specifically, our PACE simulates students' learning styles based on their personalities, including gender, interests, characteristics, and experiences. In this way, our PACE can effectively assess the personality of students, allowing us to develop individualized teaching strategies that resonate with their unique learning styles in subsequent sections.\n3.1.2 Conceptualizing Personalized Teaching Strategies. In this stage, we focus on developing tailored teaching strategies that align with the simulated learning styles of each student. By incorporating insights drawn from the Felder and Silverman model, our PACE aims to create engaging and effective mathematics instructions. For example, for students identified as sensory learners, we might implement strategies using physical objects or real-world examples to illustrate the problem. This hands-on approach helps sensory learners connect abstract concepts to tangible experiences, enhancing their understanding. Conversely, for intuitive learners, strategies include creating opportunities to discover patterns and relationships within mathematical concepts. This might involve activities that promote exploration and discovery, enabling these students to grasp overarching ideas and theoretical frameworks. Through this conceptualizing process, we integrate these personalized teaching guidelines into system prompts, adapting to each student's unique learning style.\n3.1.3 Socratic-style Conversations. After conceptualizing individualized teaching strategies, we adopt Socratic teaching method [13], encouraging students to think, reflect, and explore concepts deeply by asking thought-provoking questions rather than directly providing answers. Specifically, upon presenting a question, we first rephrase it based on the previously developed teaching strategies to make it more accessible for students. Subsequently, we propose prompt questions to help students navigate complex mathematical problems. Next, we evaluate their understanding by analyzing their responses and previous conversation records. If any misunderstandings arise, we address and correct them promptly. Finally, we repeat this process, allowing students to engage in cycles of reflection, correction, and prompting questions. This method not only enhances students' comprehension and critical thinking skills but also boosts their confidence and interest in mathematics."}, {"title": "3.2 Personalized Teaching Conversation Construction", "content": "Given the lack of personalized teaching datasets, we utilize the character-driven simulation capabilities of LLMs [6] to construct a multi-turn dialogue dataset with high quality. This process mainly involves three procedures: raw data collection, dialogue synthesis via LLMs, and human annotation.\n3.2.1 Raw Data Collection. The raw data is collected from an existing dataset, GSM8K [8]. The math questions in GSM8K are high-quality grade school math problems created by human problem writers. These problems involve between 2 and 8 steps to solve and come with high-quality annotations, making them suitable for guiding students toward deeper thinking. Moreover, to build a simulation of students' personality traits, we need to collect different student characters, which span different genders, interests, and levels of knowledge. Thus, these characters should be quite representative for evaluating personalized teaching. For simplicity but without loss of generality, we construct six character profiles that exhibit distinctive traits. These characters are inspired by the protagonists of the school television series Recess, all of whom are around ten years old and possess different personalities, hobbies, and experiences, making them well-suited for character modeling.\n3.2.2 Dialogue Synthesis via LLMs. We propose a method to simulate teacher and student personas by prompting LLMs for dialogue synthesis. Specifically, we utilize the GPT-4 (GPT-4-Turbo-8k) model to serve as the virtual teacher and student, respectively. For each math problem, a student profile is selected from the six characters to act as the system prompt for the student agent. The teacher agent, in turn, is tasked with simulating the student's learning styles based on the chosen profile and developing tailored teaching strategies during the conceptualizing process. This teaching strategy, combined with a Socratic teaching prompt, serves as the system prompt for the teacher agent. Consequently, a multi-turn dialogue is conducted between the teacher and student agents, fostering an interactive educational experience.\n3.2.3 Human Annotation. To further enhance data quality and mitigate the impacts of randomness in LLMs, we manually clean and re-annotate the conversations. First, we retain only those dialogues that exceed five turns (where a turn consists of two utterances), filtering out shorter dialogues. Additionally, we review the final utterance of the student in each dialogue to eliminate samples in which the student's problem remains unresolved. Subsequently, our domain-expert co-authors evaluate the coherence and quality of the dialogues, removing any problematic instances."}, {"title": "3.3 PACE Training", "content": "Personalized conversational tutoring can be viewed as an interactive task, where the objective of the agent model is to generate responses based on the student's persona profile and the conversation history. Formally, given the task instruction t and the student's profile p, our language agent PACE with parameters \\(\\theta\\) serves as the policy model \\(\\pi_{\\theta}\\), responsible for generating a tailored response \\(r_{t+1}\\) based on historical conversation \\(h_t\\) at the t-th turn:\n\n\n\\(r_{t+1} \\sim \\pi_{\\theta}( \\cdot | h_t, p),\\)\n\\(h_t = (\\tau, u_0, r_0, u_1, r_1, ..., u_t, r_t),\\)\n\nwhere u and r are the utterance and response from the student and agent, respectively.\nTo foster an engaging learning experience that not only aligns with students' personality traits but also resonates with their interests, we steer the PACE to self-synthesize the learning styles of students based on their personality profiles. Subsequently, we prompt the PACE to generate teaching strategies s according to the simulated learning styles l. This process can be represented as:\n\n\n\\(s \\sim \\pi_{\\theta}( \\cdot | P_{strategy}, l) \\pi_{\\theta}(l|P_{style}, t, p),\\)\n\nwhere \\(P_{style}\\) stands for the prompt to instruct the learning style simulation based on the Felder and Silverman learning style model, and \\(P_{strategy}\\) denotes the prompt to instruct the personalized teaching strategies summarization.\nUltimately, the whole conversation trajectory c concludes when the student's problem is solved or exceeds the maximum dialogue turns. The entire trajectory with turn size n can be modeled as follows:\n\n\n\\(\\pi_{\\theta}(c_t) = \\prod_{t=0}^{n} \\pi_{\\theta}(r_{t+1} | h_t, s) \\pi_{\\theta} (r_0 | u_0, s, \\tau ).\\)\n\nTherefore, given the expert conversation trajectories dataset \\(D = \\{ (t, s, c)^{(i)} \\}_{i=1}^{D}\\), we train our PACE to follow the personalized teaching strategies s to generate tailored responses. Under an autoregressive manner, the loss of the PACE can be formulated as:\n\n\n\\(L_{PACE}(\\pi_{\\theta}) = -E_{c \\sim D}[ \\pi_{\\theta}(c | t, s)].\\)\n\nSuppose X = (x0, X1, . . ., X|X|\u22121) is the token sequence of the conversation trajectory c, we only compute the loss on tokens that belong to teachers:\n\n\n\\(\\pi_{\\theta}(c|t, s) = - \\sum_{j=1}^{|X|-1} (1(x_j \\epsilon r) x log \\pi_{\\theta}(x_j | t, s, x_{<j} )),\\)\n\nwhere 1(xj \u2208 r) is the indicator function to mask tokens unrelated to teacher responses."}, {"title": "4 Multi-aspect Evaluation Criteria", "content": "In this section, we outline the evaluation criteria that the responses of the PACE should meet and identify potential factors within the dialogue that may reflect the quality of teaching. To provide a comprehensive assessment, we employ a dual evaluation approach consisting of reference-based and LLM-based methodologies. The overall framework of our PACE can refer to Figure 3."}, {"title": "4.1 Reference-based Evaluation", "content": "Reference-based evaluation assesses the model's responses against provided reference output (expert response or ground truth) to determine whether they meet the established criteria. This approach is based on the assumption that the greater the similarity between the model's outputs and the references, the more consistent they are with the desired qualities. To this end, we adopt several metrics, including BLEU [29] series, ROUGE [19] series, METEOR [4], and BERTScore [43], which are standard evaluation metrics commonly used in traditional text generation tasks. BLEU-n assesses the n-gram precision of the generated text, while ROUGE-n measures n-gram recall, assessing the degree of similarity between the model's output and the reference output. METEOR is a multi-factorial evaluation method that assesses the quality of the generated text in terms of precision, recall and matching success. BERTScore leverages contextual embeddings from the BERT [10] to evaluate the similarity between generated text and reference output, allowing for a more accurate and unbiased assessment compared to BLEU and ROUGE."}, {"title": "4.2 LLM-based Evaluation", "content": "Beyond conventional objective metrics, evaluating personalized conversational tutoring models requires a nuanced framework that assesses their ability to deliver customized teaching experiences across multiple dimensions. Given the cost and bias inherent in human evaluations, we utilize LLMs as evaluators, proven effective in previous research [42]. To improve evaluation accuracy, our domain expert co-authors design and review 30 random dialogues to establish scoring examples, which we use to guide GPT-4 in performing evaluations. We assess the responses of our PACE based on six criteria:\n*   Coherence. It evaluates the degree to which the response is logically consistent with the ongoing conversation. High coherence indicates that the dialogue flows naturally, with each response being contextually appropriate and maintaining continuity throughout the interaction.\n*   Relevance. This criterion ensures that the information provided is pertinent to the context of the conversation and goes beyond general knowledge to meet individual student inquiries.\n*   Personalization. This dimension emphasizes the model's ability to adapt its interactions based on preferred learning styles and the specific characteristics of the student.\n*   Engagement. This metric evaluates whether the interaction fosters a positive learning atmosphere and encourages active participation from the students.\n*   Consistency. Evaluate if the model's statements match or contradict the student's learning styles and prior interactions. This criterion ensures that the model maintains a stable approach that resonates with students throughout the teaching process.\n*   Inspiration. Inspiration ensures the responses motivate students, sparking curiosity and encouraging further exploration."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experimental Settings", "content": "5.1.1 Dataset. As outlined in Method section, we collect student personas from the school television series Recess, which demonstrate a variety of genders, hobbies, personality traits and experiences. After selecting these personas, we prompt GPT-4 with the temperature of 0.7 and the top-p of 0.95 to generate dialogue data, simulating interactions between students and the teacher. The mathematical questions are sourced from GSM8K [8], which includes 8,792 questions requiring between 2 to 8 steps to solve. After human annotation, our constructed dataset comprises 1,410 dialogues, focusing on 6 representative personas. The dataset is divided into 1,200/60/150 for training, validation, and testing. The detailed statistic is shown in Table 2.\n5.1.2 Baselines. We conduct experiments with several representative LLMs: LLaMA2-chat (7B and 70B) [37], Mistral (7B and 8x7B) [17, 18], and Qwen2-Instruct (7B and 72B) [40]. Following methodologies proposed in previous research [30], we employ personality-aware instructions as prompts to encourage each model to act as a mathematics tutor. Additionally, we compare PACE with EduChat [9], an educational LLM built on the LLaMA2-7B architecture, which has been augmented with a corpus of educational conversation data obtained through supervised fine-tuning.\n5.1.3 Implementation Details. We employ the LLaMA2-7B-chat as the backbone of PACE. And we train the model using the LoRA [16] approach with r = 32 and a = 32. AdamW [20] optimizer with learning rate of 3e-4 and warm ratio of 0.01 is utilized. All experiments are carried out on a system equipped with two NVDIA A800 GPUs."}, {"title": "5.2 Results of Reference-based Evaluation", "content": "Table 1 displays the results of our experiments. Overall, we can observe that PACE achieves consistent and significant improvement across all metrics, which demonstrates the effectiveness of our proposed PACE. Compared to the outcomes of representative LLMs that utilize students' personalities as prompts, our performance significantly surpasses theirs, showing an average improvement ranging from 24.7% to 43.2%. This demonstrates the superiority of our approach, which models students' learning styles and generates tailored teaching strategies. Additionally, our experimental results reveal that Educhat, despite being trained on a large-scale dataset of teaching dialogues, underperforms in personalized teaching scenarios. This shortcoming arises because Educhat focuses more on problem-solving than on considering the unique characteristics of students to engage them in the learning process effectively.\nTo further investigate, we conduct an ablation study to assess the impact of key components in PACE quantitatively. This involves removing the simulation of learning styles and the conceptualization of tailored teaching strategies (w/o student simulation), as well as the Socratic teaching method (w/o socratic teaching). The results indicate that all implemented modules contribute effectively to training outcomes. Notably, the absence of the student learning style simulation phase significantly affects the final results, highlighting the importance of considering students' learning styles in the educational process."}, {"title": "5.3 Results of GPT4-based Evaluation", "content": "Single reference-based automatic metrics do not always accurately reflect the real quality of the generated responses. Therefore, we conduct GPT-4-based evaluation, focusing on six aspects outlined in the Multi-aspect Evaluation Criteria Section. To mitigate potential consistency issues when assigning scores for all responses individually, we adopt an alternative approach. We instruct GPT-4 to rank the generated responses based on the conversation history and students' persona. This ranking process enables a more effective evaluation of the performance across different models. Additionally, we include five ranking examples, curated by our domain-expert co-authors, as demonstrations for each assessment criterion. Detailed prompts can refer to Table 3.\nWe compare our PACE model with the 70B series model due to their better performance in reference-based evaluation. As shown in Figure 4, PACE significantly outperforms the existing models across all dimensions, particularly in personalization, engagement, and inspiration, underscoring the superiority of our model. Notably, although EduChat underperforms in the reference-based assessment, it meets expectations in the LLM-based evaluation, highlighting the importance of LLM-based assessments. Overall, in both evaluation types, PACE demonstrates exceptional personalized teaching capabilities, crucial for enhancing student engagement and inspiration in mathematics education."}, {"title": "5.4 Performance on Unseen Student Persona", "content": "To further explore the benefits of simulating students' learning styles, we evaluate the performance of PACE across various unseen student personas. We construct a new set of student personas that significantly diverge from those in our previously established dataset. We then utilize GPT-4 to simulate these personas, re-collect a new set of mathematical questions, and engage our model, along with baselines, in dialogue with these unseen, GPT-4 simulated students. In this scenario, models are required to provide fine-grained assessments of students to improve the teaching experience.\nTo effectively evaluate the performance of each model, we employ adversarial evaluation techniques [28]. For each response pair generated by the two models, we utilize GPT-4 to determine which response better meets the specified assessment criteria, categorizing the results as win, lose, or tie. The experimental results, illustrated in Figure 5, demonstrate that PACE exhibits strong generalization capabilities compared to existing models. This can be attributed to our implementation of Felder and Silverman learning style model, which enables PACE to simulate students' learning styles and generate tailored teaching strategies, rather than relying on a static teaching approach. The experimental results indicate that the proposed framework facilitates personalized teaching rather than merely memorizing predefined strategies."}, {"title": "6 Conclusion", "content": "In this paper, we present PACE, a novel LLM-based framework designed to enhance personalized mathematics instruction. By simulating diverse student personas based on the Felder and Silverman learning style model, PACE tailors instructional strategies to meet individual learning preferences. The incorporation of the Socratic teaching method further promotes critical thinking and deeper engagement with mathematical concepts. Utilizing the role-simulation capabilities of LLMs, we construct a dataset for personalized instruction and propose comprehensive evaluation metrics. Experimental results demonstrate the effectiveness of our framework in delivering tailored educational experiences, leading to increased student engagement and understanding."}]}