{"title": "Recent Advances in Discrete Speech Tokens: A Review", "authors": ["Yiwei Guo", "Zhihan Li", "Hankun Wang", "Bohan Li", "Chongtian Shao", "Hanglei Zhang", "Chenpeng Du", "Xie Chen", "Shujie Liu", "Kai Yu"], "abstract": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid advancement of large language models (LLMs) in natural language processing has revolutionized speech generation tasks [1], [2], with speech being tokenized and modeled using decoder-only Transformers [3]. Efforts starting from GSLM [4] and AudioLM [5] aim to develop text-free spoken LLMs, akin to how current LLM-powered chatbots enable text-based interactions. Other works, including VALL-E [6] and VioLA [7], extend this approach to conditional speech generation tasks, such as zero-shot text-to-speech and speech translation. However, this paradigm requires data to be tokenized, as LLMs typically process discrete data only. Textual tokens naturally meet this requirement because they are designed as discrete units separated by clear boundaries, whereas raw speech signals are continuous and boundary-less. Therefore, a necessary step before applying speech data to LLM is the tokenization of speech, whose goal is:\nTo convert long speech waveforms into short discrete tokens for downstream tasks. These tokens should be compatible"}, {"title": "II. PRE-REQUISITES: DISCRETE REPRESENTATION LEARNING", "content": "Discrete speech tokens are obtained through the quantization of continuous representations, which is usually achieved by offline clustering or online vector quantization algorithms. This section provides a concise overview of the existing quantization methods commonly used in discrete speech tokens.\nDenote $x \\in \\mathbb{R}^d$ as a vector in the d-dimensional continuous space. A quantization process q transforms \u00e6 into a discrete token in a finite set, i.e. $q(x) : \\mathbb{R}^d \\rightarrow \\{1, 2, ..., V\\}$ where V is the vocabulary size. The output tokens are sometimes referred to as indexes in the finite V-cardinal set. The function q is usually associated with a codebook $C = \\{C_1, C_2, ..., C_v \\}$ where every code-vector $c_i \\in \\mathbb{R}^d$ corresponds to the i-th token. The code-vectors are representations of tokens in the original d-dimensional space. As V elements can be encoded using $[log_2 V]$ raw bits\u00b9, quantization often compresses the cost for data storage and transmission to a great extent."}, {"title": "A. Offline Clustering", "content": "Clustering is a simple approach for quantization. Given a dataset $X = \\{X_1,X_2,...,X_N\\}$, a clustering algorithm aims to assign each sample $x_i$ to a group such that some cost is minimized. The most frequently used clustering method for discrete"}, {"title": "B. Vector Quantization", "content": "Clustering is often an isolate process, thus cannot be optimized together with other neural network modules. Instead, vector quantization (VQ) [46] enables a learnable network module that allows gradients to pass through when producing discrete representations. Autoencoders with a VQ module is termed VQ-VAE [47]. There are multiple VQ methods:\n1) K-means VQ: Like k-means clustering, k-means VQ method finds the code-vector closest to the input, i.e.\n$\\displaystyle q(x) = \\underset{i \\in \\{1,2,...,V\\}}{\\text{arg min}} ||x - c_i||^2.$\n(1)\nThen, code-vector $c_k = c_{q(x)}$ is fed to subsequent networks. As the min operation is not differentiable, straight-through estimators (STEs) [48] are usually applied to graft gradients, i.e. $STE(c_k, x) = x + sg(c_k - x)$ where $sg(.)$ stops tracking gradients. In this way, the input value to subsequent networks is still $c_k$, but gradients are grafted to \u00e6 in back propagation. Auxiliary loss functions are often used together with k-means VQ [47]: commitment loss $L_{cmt} = || sg(c_k) \u2013 x||^2$ and codebook loss $L_{code} = || sg(x) \u2013 c_k||^2$. The commitment loss pushes the continuous input \u00e6 towards the closest codebook entry, while the codebook loss does the opposite and updates the code-vector $c_k$. The two loss terms are weighted by different factors to put different optimization strengths on x and $c_k$, as pushing $c_k$ towards \u00e6 is an easier task. It is also common to replace $L_{code}$ with exponential moving average (EMA) to update the codebook instead [49], which does not rely on explicit loss functions.\nVQ in high-dimensional spaces is known to suffer from codebook collapse, where the codebook usage is highly im-balanced [50], [51]. To improve the utilization of codebook, random replacement (as known as codebook expiration) can be applied [51] on code-vectors that have remained inactive for a long time. Other solutions include additional auxiliary constraints such as entropy penalty [52], [53], factorized codebook lookup in low-dimensional space [54], and adding a linear projection to update all code-vectors together [55].\n2) Gumbel VQ: Instead of quantizing by Euclidean distance, another choice is by probability. Gumbel VQ [56] uses Gumbel-Softmax as a proxy distribution for traditional Softmax to allow differentiable sampling. Given input \u00e6 and a codebook of size V, a transform $h(.)$ is applied on \u00e6 into"}, {"title": "III. SPEECH TOKENIZATION METHODS: ACOUSTIC TOKENS", "content": "Acoustic tokens, also known as speech codecs, refer to the discrete representations optimized mainly for signal compression and reconstruction. The audio codec technology arises long ago. Traditional codecs, including MP3 [87], Opus [88] and EVS [89], typically take advantage of signal processing algorithms to improve quality and lower the bitrate.\nIn the deep learning era, numerous codec models based on neural networks have emerged. These models typically consist of an encoder that compresses speech signals and a decoder that reconstructs the speech signals, with a quantizer situated between the two. The quantizer is also parameterized and jointly trained with the whole network in an end-to-end manner. The codebook indices produced by the quantizer are referred to as acoustic tokens. To improve the representation ability of discrete VQ spaces and thus obtain better codec per-formance, RVQ, GVQ, GRVQ and FSQ tricks are commonly applied in the quantization module.\nWe list the VQ method, number of quantizers Q, frame rate F, vocabulary size V for each quantizer, and the resulting bitrate of existing neural acoustic speech tokens in Table.I."}, {"title": "A. Model Architectures", "content": "Although acoustic codec models differ from one to one regarding their purposes, most of them share a similar encoder-"}, {"title": "B. General-Purpose Acoustic Tokens", "content": "1) Motivation: In this section, we describe the most common type of neural acoustic tokens (speech codecs) that are designed only with the objective of speech signal reconstruction. Those acoustic tokens are optimized towards better signal or perceptual quality under bitrates as low as possible.\n2) Approaches:\na) Advanced VQ methods and model architectures:\nBased on SoundStream and EnCodec, more codecs with"}, {"title": "C. Acoustic Tokens with Semantic Distillation", "content": "1) Motivation: Acoustic tokens are a convenient choice for spoken language models, as they can be directly converted"}, {"title": "D. Acoustic Tokens with Disentanglement", "content": "1) Motivation: Another line of mixed-objective acoustic tokens is disentanglement. A prominent research direction is the disentanglement of speaker timbre information, as this is a global trait among all the speech information aspects. Encoding speaker information into every token timestep is redundant; thus, removing the global speaker timbre can make the information in acoustic tokens more compact and reduce the necessary bitrate. Speaker-decoupled speech tokens can alleviate the modeling burden for downstream tasks."}, {"title": "IV. SPEECH TOKENIZATION METHODS: SEMANTIC TOKENS", "content": "Semantic tokens refer to discrete speech representations from discriminative or self-supervised learning (SSL) models. While we use the term semantic tokens to maintain consistency with prior works, some researchers recently argue that SSL features are more accurately described as phonetic than semantic [113] in nature. Hence to clarify, in this review, semantic tokens should be more accurately defined as the complementary set of acoustic tokens, such that they are not primarily aimed at reconstruction purposes. In practice, the vast majority of these tokens are designed for discriminative tasks and are believed to have a strong correlation with phonetic and semantic information [8], [114]\u2013[116]."}, {"title": "A. Semantic Tokens from General-Purpose SSL", "content": "1) Motivation: Speech SSL models have consistently outperformed many traditional methods in various speech tasks [8], [117]. Their potential has been extensively mined in discriminative tasks such as automatic speech recognition (ASR) [31], [33], [118], [119], automatic speaker verification (ASV) [34], [120], [121], speech emotion recognition (SER) [34], [122]-[124] and speech translation (ST) [125]-[127]. Discretized SSL tokens are initially favored for reducing computation costs and improving robustness against irrelevant information for ASR [128]. As language models have gained increasing attention, these SSL tokens have been further explored in generative tasks such as TTS [15], [129], [130] and SLM [4], [5], [131]. This is because they can be considered high-level abstractions of speech semantics that are largely independent of acoustic details.\n2) Approaches: SSL models initiate the learning process by defining a pretext task which enables the model to learn meaningful representations directly from the data itself. Typical speech SSL models employ CNNs and Transformer encoders"}, {"title": "B. Semantic Tokens from Perturbation-Invariant SSL", "content": "1) Motivation: As SSL tokens feature semantic or phonetic information, a major concern is to improve the resistance against perturbations in the input signal. This kind of invariance includes noise and speaker aspects that don't affect the contents of speech. Noise invariance refers to the invariance against signal augmentations such as additive noise, reverberations, etc. Speaker invariance aims to remove speaker information, similar to speaker-disentangled acoustic tokens. In the training process, perturbations are often explicitly introduced in these perturbation-invariant SSL models. The original and perturbed view of an utterance are both fed to the same network (or teacher and student networks), and an external loss to reduce the impact of perturbation is applied. The middle part of Fig.7 depicts a typical perturbation-invariant SSL model.\n2) Approaches:\na) Perturbations: The perturbations can either be designed to augment the acoustics or alter the speaker timbre, depending on the objective of invariance. These perturbations"}, {"title": "C. Semantic Tokens from Supervised Models", "content": "As representing semantic or phonetic information is the major purpose of semantic tokens, a more direct way to achieve this is through supervised learning. A famous example shown at the bottom of Fig.7 is the S\u00b3 Tokenizer from Cosy Voice [41]. It places a single-codebook VQ layer between two Transformer encoder modules and optimizes the network through an ASR loss similar to Whisper [40]. The same method is adopted in [152], [154] where the frame rate is further reduced to 12.5Hz. CosyVoice 2 [153] improves S3 Tokenizer by replacing plain VQ with FSQ for better codebook utilization. Note that in this kind of supervised semantic tokens, it is the output of the VQ layer that serves as tokens. This allows for more preservation of paralinguistic information than directly transcribing speech into text. These supervised tokenizers are trained on massive paired speech-text data, and have demonstrated rich speech content understanding capabilities [41], [155]."}, {"title": "D. Length Reduction by Deduplication and Acoustic BPE", "content": "In most cases, the frame rate of discrete speech tokens ranges from 25 to 100Hz. This leads to a huge discrepancy in lengths between speech representations and the underlying text modality. This discrepancy has been a critical issue in building"}, {"title": "E. Variable Frame Rate Tokens and Unit Discovery", "content": "Information in speech is not uniformly distributed along the time axis [163]. In segments such as silence or long vowels, information density is low, whereas in segments with explosive consonants, speech events occur much more frequently. This inherent non-uniformity suggests that it might be more natural to allocate more tokenized bits to regions with dense information and higher variance, and fewer bits to regions with less uncertainty. This kind of discrete speech tokens is referred to as variable frame rate (VFR) tokens in this review. Note that while multi-resolution and variable-bitrate tokens have been introduced previously, the concept of VFR is still distinct. In multi-resolution tokens [23], [74], each quantizer operates at a fixed frame rate. In variable-bitrate tokens [24], the frame rate remains fixed, while the variability lies in the number of quantizers per frame. Instead, VFR tokens should directly allocate different granularities on the temporal axis.\nVFR tokens are closely related to acoustic unit discovery. As speech lacks a natural boundary of phonetic units [8], there are much research efforts to find and locate the underlying acoustic units behind speech utterances in an unsupervised manner [164]\u2013[167]. This is particularly of interest for low-resource languages. The discovered units can guide the boundary segmentation of VFR tokens. To this end, VFR tokens are interesting not only because they might reduce the necessary bitrate, but also because they can introduce a strong inductive bias that linguistic knowledge is encoded [163].\nA recent direction of VFR tokens is to discover acoustic units from an SSL model. Note that deduplicated tokens and acoustic BPE themselves can be regarded as VFR tokens. Sylber [168] and SyllableLM [45] take similar approaches that first locate acoustic boundaries from existing HuBERT models, and then train another HuBERT student with segment-level pooled targets between boundaries. The final HuBERT"}, {"title": "F. Speech Token Vocoders", "content": "Acoustic tokens are designed naturally with a decoder that outputs waveforms or spectrograms given tokens, but semantic tokens are not. A necessary component for building a discrete token-based speech generation system with semantic tokens is the speech resynthesis model, or speech token vocoders. Unlike traditional spectrogram-based vocoders [170], these vocoders receive discrete speech tokens as an input and reconstruct speech signals.\nPolyak et al. [143] first explores speech resynthesis from discrete speech units by a HifiGAN [170] augmented with discretized pitch units and speaker embedding inputs. The vec2wav vocoder in VQTTS [129] improves this vocoder by a Conformer [93] frontend module before HifiGAN generator. Later, CTX-vec2wav [111] proposes a position-agnostic cross-attention mechanism that effectively integrates timbre information from surrounding acoustic contexts without the need for pretrained speaker embeddings. This makes it more timbre-controllable and suitable for zero-shot TTS and VC [112]. Upon it, vec2wav 2.0 [171] further advances the timbre controllability by SSL timbre features and adaptive activations, demonstrating a strong VC performance.\nIt is also feasible to apply diffusion or flow matching algorithms in token vocoders [41], [172], [173]. There, the discrete tokens are treated as a condition for diffusion or flow matching to generate mel-spectrograms, and further converted to waveform by a pretrained mel vocoder. Compared to training a token-to-wav vocoder in an end-to-end fashion, training a token-to-mel model is more convenient and does not need adversarial training. To better control timbre, a mask strategy is introduced into the training process where the model only computes loss on the un-masked part of spectrograms [41]. During inference, spectrogram from speaker prompt conditions"}, {"title": "V. ANALYSIS OF DISCRETE SPEECH TOKENS", "content": "Discrete speech tokens can be evaluated from various aspects besides bitrate and codebook utilization:\n*   Signal-level reconstruction metrics: For reconstruction evaluations, signal-level metrics like PESQ, STOI, mel distance, GPE, etc. are often used.\n*   Perceptual reconstruction metrics: Apart from signal-level metrics, there can also be perceptual evaluations of reconstruction performance. This includes intelligibility (often measured by WER, CER, or phone error rates), speaker similarity, subjective or proxy MOS tests, etc.\n*   Performance on downstream tasks: Probing tasks can be used to measure the preservation or prominence of certain information in tokens, like ASR, ASV, emotion recognition, and spoken language modeling [167]. Note that this is different from perceptual reconstruction metrics since it operates directly on tokens. Performance in generative tasks like TTS and VC can also be evaluated.\n*   Semantic/phonetic relevance: If the tokens are expected to align with texts (e.g. for semantic tokens and semantic-distilled acoustic tokens), metrics like phone discriminability [167], phone purity, and phone-normalized mutual information [33] can be computed.\n*   Invariance and robustness: If the tokens are expected to be invariant to perturbations, unit edit distance [39] can be considered as a measurement."}, {"title": "B. Existing Analyses", "content": "There are several theoretical or experimental analyses of the advantages of discrete speech tokens. Nguyen et al. [178] demonstrates by an encoder-only language model that semantic SSL tokens are favorable for spoken language modeling, due to their removal of linguistically irrelevant information. Sicherman et al. [115] supports this claim by showing that"}, {"title": "C. Reconstruction Analysis", "content": "To enable a fair comparison between acoustic and semantic tokens from a reconstruction perspective, we train a CTX-vec2wav vocoder [29] for different semantic tokens on LibriTTS [186]. This vocoder supplements the insufficient speaker timbre information in semantic tokens using continuous WavLM features extracted from the reference prompts. This approach enables semantic tokens to perform voice conversion (VC) by switching reference prompts conveniently. The training details follow [111]. We compute several metrics for reconstruction ability:\n*   WER (word error rate, in percentage) measures the content intelligibility of reconstructed speech. It is computed between ground truth texts and ASR-decoded transcriptions. We use NeMo-ASR7 here.\n*   GPE (gross pitch error, in percentage) measures the relative error percentage of pitch contours of the reconstructed speech compared to ground truth.\n*   PESQ (perceptual evaluation of speech quality) and STOI (short-time objective intelligibility) measure the speech quality from a signal perspective."}, {"title": "D. Voice Conversion Analysis", "content": "Despite the loss of acoustic information, a prominent advantage of semantic tokens over most acoustic tokens is their inherent timbre controllability. Some acoustic tokens also have this ability, such as those with a global encoder like TiCodec and disentangled acoustic tokens, also possess this ability To compare this ability across different tokens, we conduct voice conversion (VC) experiments using these tokens as the content from the source speech. We use the same source utterances in Section V-C, but assign a different target speaker for each source utterance as the prompt. Then, we perform VC experiments on the 500 VC pairs. In addition to WER and SECS, we also measure P.Corr as an objective metric for prosody preservation. This calculates the Pearson correlation coefficient between the pitch contours of the converted and source utterances. Note that this metric will be meaninglessly high if the VC similarity is low, i.e., when the source timbre is barely changed. As the source utterances are the same as Section V-C, the WER numbers are directly comparable to those in the reconstruction experiments.\nThe results presented in Table III indicate that semantic tokens often achieve much higher VC similarity compared"}, {"title": "VI. DISCRETE SPEECH TOKEN-BASED APPLICATION PARADIGMS", "content": "A. Spoken Language Understanding\n1) Motivation: Spoken language understanding (SLU) tasks, including automatic speech recognition (ASR), speech translation, intent classification and others, aim to extract meaningful domain-specific information from speech. Most SLU tasks follow a speech-in text-out pipeline, except S2ST which also involves speech generation. The adoption of discrete tokens in SLU offers some benefits. Discrete tokens may naturally exhibit some invariance against noise and speaker information, particularly semantic tokens, which can make downstream models to focus more effectively on content-related information in some tasks. On a broader scale, discrete tokens provide a promising approach to unifying speech understanding and generation in spoken language models.\nAs an alternative input to an SLU model instead of continuous features, discrete speech tokens are typically deduplicated or BPE-encoded before subsequent modules. Semantic tokens have been better explored than acoustic ones in this context.\n2) Speech Translation: Among the various SLU tasks, discrete speech tokens are mostly adopted in speech translation, including speech-to-text translation (S2TT) and speech-to-speech translation (S2ST). Since semantic tokens correlate well with phonetics, they can serve as universal pseudo-labels for untranscribed languages, useful for S2TT in low-resource settings [187]. Direct S2ST using discrete tokens has garnered more attention on the generation side (Section VI-B). Early approaches primarily rely on extracting discrete tokens using VQ-VAEs, particularly for unwritten languages [188], [189]. Recent researches in this area include employing semantic tokens [190]\u2013[192], acoustic tokens [193]\u2013[195], two-pass architectures [196], [197], and non-autoregressive frameworks [198]. These efforts collectively contribute to advancing"}, {"title": "B. Speech Generation", "content": "1) Motivation: Discrete tokens have catalyzed a paradigm shift in speech generation, with TTS being the most representative application. In TTS systems, discrete tokens are usually used as intermediate features that bridge the acoustic model (text-to-token) and the vocoder or codec decoder (token-to-wav). There are two major advantages of applying discrete tokens in TTS:\n*   Easier training objectives. Discrete tokens replace the original spectrogram-based regression task with a classification task [129], which can be much easier. This also offers a better balance between acoustic models and vocoders, since texts are closer to discrete speech tokens than frequency-domain features.\n*   Better use of decoder-only language models. Decoder-only language models have shown remarkable success in natural language generation. After discretization, speech can also be autoregressively generated under the same paradigm. This offers huge potential in leveraging the in-context learning and scaling capabilities of language models to achieve zero-shot high-fidelity TTS [6].\nOther generative tasks, such as singing voice synthesis and speech editing, can similarly benefit from the advantages of discrete tokens observed in TTS. For voice conversion (VC), using discrete tokens as content representations can simplify the process to a token vocoder [171], when timbre information is effectively removed from the tokens. Tasks like speech to speech translation [189], [190], speech enhancement [212], [213] and target speaker extraction [214] can also be enhanced through language modeling on discrete tokens."}, {"title": "C. Text-Free Spoken Language Models", "content": "1) Motivation: End-to-end spoken language and dialogue modeling is one of the most ultimate goals in speech technology. Discrete tokens are a core component of existing spoken language models, as they enable the language modeling technique to be applied directly on speech. The models discussed in this subsection are text-free spoken language models (TF-SLMs). We anticipate that a well-trained TF-SLM will be capable of generating semantically coherent speech without the need for text transcription guidance.\n2) Existing Efforts: Ever since GSLM [4] and AudioLM [5] proposed the vision of TF-SLMs, building such models remains a significant challenge even till today. This difficulty primarily arises from the lower language modeling efficiency of speech token sequences compared to text, due to their lower semantic information density, longer sequence lengths, and the presence of paralinguistic information [250]. Current advancements in TF-SLMs mainly focus on two strategies: (1) reducing token frame rates, and (2) aligning speech with text.\nThe first approach aims to shorten speech sequences and enhance semantic density by lowering frame rates [4], [131], [159] to even \u22485Hz [45], [168]. While mitigating sequence length issues to different degrees, they still encounter scalability limitations [251] and compromise reconstruction quality.\nThe second strategy involves aligning speech with text through methods like initializing pre-training with text LLMs [131], reinforcement learning using ASR and text LLM feedback [252], text-speech token interleaving [253], adopting novel architectures applied in text language modeling [254], etc [255], [256]. Meanwhile, full duplex modeling has been proposed [257] to allow users to interrupt and start new dialogues at will.\nHowever, despite many efforts, these models still struggle to generate semantically reliable long speech during inference due to the lack of explicit transcription guidance.\n3) Limitations: Although these methods show promise, achieving semantic coherence is still a challenging goal, leaving significant progress to be made toward the goal of truly end-to-end spoken language modeling. Improving the semantic density and expressiveness of discrete speech representations, making it easier to align text and speech during TF-SLM training, is a promising direction for future exploration."}, {"title": "D. Text-Guided Spoken Language Models", "content": "1) Motivation: Since TF-SLM remains an open problem, the prevalent successful speech dialogue systems settle for an"}, {"title": "VII. CHALLENGES AND FUTURE DIRECTIONS", "content": "Current discrete speech tokens still exhibit certain limitations and challenges that need to be addressed. In this section, we summarize the existing challenges in this field and outline the corresponding future directions.\n1) Low-Bitrate Tokens: For bitrates of tokens, factors Q (number of quantizers) and F (frame rate) play a more important role than V (vocabulary size). Using only a single codebook is very beneficial for language modeling, since speech can be truly regarded as another \u201cnatural language\u201d then. However, there is usually a noticeable degradation in reconstruction performance in these tokens. A critical problem lies in how to better utilize the highly-compact discrete VQ representation space. For F, the frame rates of most tokens are still much greater than text sequences, which can significantly influence the syntactic and semantic modeling capability of language models [250]. However, reducing the frame rate of tokens may also lead to decreased intelligibility in reconstructed speech. A lower V is also desirable for language modeling and length reduction by BPE.\nFor acoustic tokens, it remains an open problem what the lower bound of bitrate and the frame rate Fare, and how to reach them. More powerful network architectures or advanced VQ strategies should be helpful, and reducing temporal redundancy by disentangling global information is also a promising solution.\n2) Streaming Ability and Efficiency: Real-time applications require tokens to be stream-able both in encoding and decoding. For most CNN-based acoustic tokens, achieving this is easy due to their fixed receptive fields. For acoustic tokens with Transformer blocks, an attention mask is necessary. However, most SSL models employ a non-causal Transformer architecture, which makes semantic tokens derived from these models unsuitable for real-time tokenization. It remains unclear how much performance degradation would result from transitioning to causal architectures in both SSL models and token vocoders. Streaming ability also poses a requirement for model efficiency. Currently, larger acoustic token models are reported to achieve better performance with lower bitrates [78], [81], but at a cost of efficiency. In addition to reducing the bitrate of the tokenized codes, the efficiency of tokenizers must also be balanced for real-time applications.\n3) Disentanglement in Acoustic Tokens: Whether disentanglement should be incorporated into acoustic tokens depends on the specific application. If reconstruction is the major objective, disentanglement may not be necessary. However, disentanglement can help reduce the bitrate in time-varying tokens, ensure anonymity during transmission, reduce downstream modeling complexity, and achieve independent control of difference voice properties. There are currently only limited efforts on decoupled acoustic tokens, and the decoupling effect is still suboptimal or causing a negative impact on reconstruction quality. More advanced techniques for information decoupling should be considered in the future.\n4) Variable Frame Rate Tokens: Current speech tokens are usually designed at a fixed temporal rate, while the underlying"}, {"title": "VIII. CONCLUSION", "content": "Recently, discrete speech tokens have emerged as a rapidly evolving field and a core research direction in the speech LLM era. These tokens encode acoustic or semantic information into a compact discrete representation space, catalyzing the fusion of LLMs and speech processing. Existing discrete speech tokens show rich diversity in model architecture and optimization objectives. In this review, we provide a comprehensive introduction to representative categories of discrete speech tokens, summarizing their motivations and limitations. We conduct a unified analysis of reconstruction and voice conversion across different token types to highlight their unique characteristics. We also review efforts to apply discrete tokens to speech processing tasks, including spoken language understanding, speech generation, and spoken language modeling. Finally, we explore future directions for discrete speech tokenization methods. We hope this review lays a solid foundation for future research in speech technology."}]}