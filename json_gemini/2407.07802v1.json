{"title": "ROSA: Random Subspace Adaptation for Efficient Fine-Tuning", "authors": ["Marawan Gamal Abdel Hameed", "Aristides Milios", "Siva Reddy", "Guillaume Rabusseau"], "abstract": "Model training requires significantly more memory, compared with inference. Parameter efficient fine-tuning (PEFT) methods provide a means of adapting large models to downstream tasks using less memory. However, existing methods such as adapters, prompt tuning or low-rank adaptation (LoRA) either introduce latency overhead at inference time or achieve subpar downstream performance compared with full fine-tuning. In this work we propose Random Subspace Adaptation (ROSA), a method that outperforms previous PEFT methods by a significant margin, while maintaining a zero latency overhead during inference time. In contrast to previous methods, ROSA is able to adapt subspaces of arbitrarily large dimension, better approximating full-finetuning. We demonstrate both theoretically and experimentally that this makes ROSA strictly more expressive than LoRA, without consuming additional memory during runtime. As PEFT methods are especially useful in the natural language processing domain, where models operate on scales that make full fine-tuning very expensive, we evaluate ROSA in two common NLP scenarios: natural language generation (NLG) and natural language understanding (NLU) with GPT-2 and ROBERTa, respectively. We show that on almost every GLUE task ROSA outperforms LoRA by a significant margin, while also outperforming LoRA on NLG tasks.", "sections": [{"title": "1. Introduction", "content": "The advent of large language models pre-trained on web-size corpora (pre-trained LMs, or PLMs) has led to remarkably performant models in the domain of natural language processing. As the size of such models ranges from hundreds of millions to hundreds of billions of parameters, adapting them to downstream tasks is challenging and computationally expensive. Compared to inference, training requires substantially more memory (2x-4x as much). For example, the LLAMA-7B model requires 14GB and 56GB during inference and training respectively.\nTo alleviate the burdensome memory requirements of adapting PLMs to downstream tasks, various memory efficient methods have been proposed. The commonality among these methods is the maintenance of fixed PLM weights while introducing a minimal quantity of trainable parameters. Although solutions like LoRA and (IA)3 are effective and do not impose any additional inference latency, they implicitly limit the expressivity of adapted models. For instance, LoRA adapts low-rank matrices that are added in parallel to fixed pre-trained weights. While this approach makes it possible to fine-tune large PLMs with reduced memory footprint compared to full fine tuning, it introduces an unavoidable bias: the pre-trained weight matrices can only be fine-tuned to matrices that are \"a low-rank matrix away\" from the initial weights.\nIn this work, we propose ROSA: Random Subspace Adaptation, which expands the expressivity of adapted models, while remaining as memory efficient as LoRA. Similarly to LORA, ROSA satisfies memory constraints by selectively fine-tuning low-rank matrices in parallel to fixed pre-trained weight matrices. Thus allowing users to fine-tune models in resource constrained settings. In such settings, PEFT methods are the only viable alternatives as gradient check-pointing and layer-wise training (loading/unloading one layer at a time into GPUs) are prohibitively slow (layer-wise training can be up to 20\u00d7 slower than full fine-tuning or ROSA). However, PEFT methods such as LoRA are limited in expressivity compared with approaches such as layer-wise training and gradient check-pointing, which effectively simulate full fine-tuning. ROSA alleviates this limitation by continuously sampling different low-rank trainable sub-spaces and iteratively merging learned information into fixed weights throughout fine-tuning, as depicted in Figure 1. From a theoretical perspective, we formally characterize the implicit low rank bias of LoRA, show how this bias can be detrimental even on a simple regression task, and demonstrate that ROSA does not suffer from this limitation (see Theorem 2). Even further, our results show that (i) ROSA can fine-tune pre-trained weights to arbitrary target weights (i.e. is as expressive as full fine-tuning), and (ii) while LORA trades expressivity for lower memory requirements, ROSA instead trades convergence speed with memory usage. These results are clearly and intuitively illustrated in a simple synthetic experiment presented in Section 4.1.\nFrom a practical perspective, we show that ROSA achieves performance on par with full fine-tuning and consistently outperforms state-of-the-art methods such as LoRA and (IA)3 on natural language understanding (GLUE) and natural language generation (E2E) tasks by significant margins. Lastly, we note that ROSA carries a significant advantage over approaches such as adapters and prompt tuning, as it introduces no additional latency overhead during inference time.\nIn summary, our key contributions are:\n\u2022 Demonstrating both empirically and theoretically, that the low rank nature of LoRA can detrimentally limit its expressiveness.\n\u2022 Introducing ROSA, a PEFT method that circumvents the low rank limitation of LoRA while remaining as memory efficient as LoRA.\n\u2022 Theoretically showing that ROSA is more expressive than LoRA and can be as expressive as full fine-tuning.\n\u2022 Conducting extensive experiments showing that ROSA consistently outperforms LoRA by a significant margin on natural language understanding (GLUE) and natural language generation (E2E) benchmarks."}, {"title": "2. Related Work", "content": "PEFT defines a class of methods to alleviate memory and compute requirements during adaptation of large models to downstream tasks by tuning only a relatively small number of added parameters, rather than the tuning all the parameters of the model itself.\nAdapters: Adapter methods such as inject layers in between certain modules of each transformer block. These layers have relatively few parameters, as they project the original features down into a smaller set of dimensions, then scale them back up after applying the adapter's feed-forward layer. This structure necessarily leads to a latency overhead.\nPrompt and prefix tuning: Prompt and prefix tuning are an efficient means of adapting models via continuous optimization of prefixes added to input prompts. While such approaches are memory efficient, they require reserving a portion of the available sequence length during downstream adaptation. Moreover, prompt tuning methods can be challenging to optimize as pointed out in.\nLORA: Our work is most similar to LoRA, which has been shown to outperform the aforementioned approaches and to mitigate limitations such as increased inference latency and reduced sequence length capacity. LoRA adds a trainable low rank matrix to the frozen original weight matrix. The low rank matrix, parameterized as the product of two small matrices, is then fine-tuned instead of the original model weights. The authors of LoRA hypothesize that during task-specific fine-tuning, the model weight updates have a low \u201cintrinsic dimension\" and thus can be effectively approximated by low-rank matrices. While this may be true for some downstream tasks, we show both theoretically and empirically that this is not always the case and that restricting the weights update to a low intrinsic dimension can be detrimental.\n(IA)3: Another widely known PEFT method is (IA)\u00b3. (IA)3 (Infused Adapter by Inhibiting and Amplifying Inner Activations) adds learned vectors to the attention and feedforward layers of the transformer, which are used to rescale the activations of these modules. (IA)3 further reduces the number of trainable parameters from LoRA, and makes the proportion of trainable parameters fixed, as the size of the rescaling vectors is directly dependent on the dimensions of the transformer's weight matrices.\nAdaLoRA & LASER: Another competitive approach, AdaLoRA allocates a fixed parameter budget across the layers of a model dynamically by manipulating the rank, providing a lower rank to less important modules and vice versa. This is done via a novel importance metric that quantifies the contribution of a given module to the model's overall performance. In contrast, LASER replaces select matrices with their low rank approximations using SVD. Moreover LASER demonstrates that greedy search over this intervention space can lead to improved model performance. The performance increase is hypothesized to be due to the resultant select matrices better focusing on important features. In connection to our work, this indicates that some of the performance gain by using SVD in our rank reduction can be due to a similar cause.\nOther methods: BitFit freezes all parameters except bias terms. FISH optimizes a sparse difference vector to be summed with the original model parameters."}, {"title": "3. Method", "content": "In this section we describe our proposed approach, ROSA: Random Subspace Adaptation. The purpose of ROSA is to provide a means for fine-tuning large models in memory constrained settings while remaining competitive with full fine-tuning in terms of performance. After introducing ROSA and demonstrating its memory efficiency in Section 3.1, we provide a theoretical analysis showing that ROSA is provably more expressive than LoRA in Section 3.2.\n3.1. ROSA\nIn LoRA, pre-trained models are adapted to alternative tasks by adding a low rank matrix $AB$, where $A \\in \\mathbb{R}^{M \\times R}$, $B \\in \\mathbb{R}^{R \\times N}$, in parallel to pre-trained weights $W \\in \\mathbb{R}^{M \\times N}$. The output of the LoRA layer is given by\n$$\n\\varphi(x) = Wx + ABx.\n$$\n(1)\nThe adapter weights $A$ and $B$ are initialized such that $AB = 0$ and are the only parameters being updated during training. LoRA is memory efficient as $W$ remains fixed during training (no gradient buffers necessary for the full weights) and typically $R \\ll \\min(M, N)$. The rank ($R$) of the trainable matrices is chosen such that the training procedure satisfies device memory constraints.\nConstraining the updates to a fixed low rank subspace initialized at zero induces two limitations. First, the low rank nature of LoRA is such that the difference between the fine-tuned weight matrix $W + AB$ and the pre-trained weights $W$ is constrained to be a low rank matrix. This significantly hinders the ability of LoRA to fine-tune a given model to an arbitrary target model/task. Note that even in the case where the target weights $W^*$ are close to the pre-trained weights $W$ (w.r.t. e.g., the Frobenius norm), this low-rank constraint creates an unavoidable bias when the difference $W^* - W$ is not low rank. We formally characterize this bias in Section 3.2 and empirically demonstrate it in Section 4.1. Second, initializing the adapter $AB$ to zero can be thought of as learning new representations from scratch separately from the pre-trained ones $(\\varphi(x) = Wx + ABx := \\varphi_{\\text{pre-trained}}(x) + \\varphi_{\\text{trainable}}(x))$, rather than leveraging the pre-trained features the model already has to initialize the adapter.\nTo address these two limitations ROSA (i) continuously samples new weight subspaces throughout the training procedure and (ii) initializes the trainable sub-spaces using from the pre-trained weight matrices using SVD. The first limitation is addressed as iteratively re-sampling new subspaces effectively expands the dimension of the fine-tuned subspace. Hence, ROSA does not suffer from the low rank bias of LORA (which we theoretically show in Section 3.2). Moreover, the second limitation is addressed as the initial trainable weights are set to be a subset of a re-parameterization of the original weight matrix.\nIn more detail, ROSA adapters successively factorize $W$ into trainable and fixed weight subspaces using SVD. Let $W = U \\Sigma V^T$ be the SVD of $W$, let $U_R \\in \\mathbb{R}^{M \\times R}$ be the matrix obtained by selecting a random subset of $R$ columns of $U$ and let $\\Sigma_R \\in \\mathbb{R}^{R \\times R}$ and $V_R \\in \\mathbb{R}^{R \\times N}$ denote the matrices obtained by selecting the same subset of singular values and right singular vectors. The ROSA factorization step is defined by\n$$W = W_{\\text{fixed}} + AB$$\n(2)\nwhere\n$$A = U_R \\Sigma_R \\in \\mathbb{R}^{M \\times R}, B = V_R \\in \\mathbb{R}^{R \\times N},$$\n(3)\n$$W_{\\text{fixed}} = W - AB.$$\n(4)\nDuring training, gradients are computed only with respect to the $R$ dimensional subspace which consists of $R(M + N)$ parameters. In contrast, full fine-tuning requires optimizing $MN$ parameters. Thus, ROSA leads to a reduction in the number of trainable parameters given by\n$$P_{\\text{train}} = \\frac{MN}{R(M + N)}$$\n(5)\nThe factorization step is repeated throughout training at a pre-determined frequency (e.g., after each epoch of fine-tuning). The overall ROSA procedure is illustrated in Figure 1 and described in Algorithm 1. In practice, ROSA is applied simultaneously to all weight matrices of the model to fine-tune. While each subspace sampling step is expensive, $O(\\max(N, M)^3)$, it is only performed once every epoch. We show in the experiment section that the sample step adds negligible time to the training procedure in practice (see Table 1).\n3.2. Theoretical Analysis\nIn this section we formally show how the low rank parameterization of LoRA limits its expressiveness and how ROSA circumvents this limitation.\nFirst, it is easy to see that, by construction, the residual matrices obtained by fine-tuning weight matrices using LORA are constrained to be low rank:\nProposition 1. Let $W_0$ be a weight matrix of a pre-trained model to be fine-tuned. Then, any fine-tuned weight matrix $W_{\\text{LORA}}$ obtained using LoRA with rank parameter $R$ will be such that $\\text{rank}(W_0 - W_{\\text{LORA}}) \\leq R$.\nProof. This directly follows from the fact that $W_{\\text{LORA}} = W_0 + AB$ and $\\text{rank}(AB) < R$."}, {"title": "4. Experiments", "content": "In this section we compare the downstream performance of a model adapted using ROSA compared with LoRA and (IA)3 (as all three methods add zero latency overhead at inference time). For better comparison, in all experiments we use our own implementation for LoRA and (IA)3 (detailed in Appendix B). In Section 4.1 we evaluate the performance of MLP models adapted to synthetic data, while Sections 4.2 & 4.3 evaluate the performance of ROBERTabase and GPT-2 on the GLUE and E2E benchmarks, respectively. In our experiments using transformer models, we only apply the PEFT methods to the attention layers, following a similar approach to Hu et al. (2022).\n4.1. Synthetic Data\nWe first design two simple regression experiments with synthetic data to validate that the increased expressiveness of ROSA, illustrated in Theorem 2 for linear models, indeed leads to better results when fine-tuning non-linear models via Stochastic Gradient Descent.\nTo generate the synthetic data, we start by randomly initializing an MLP model $f$. We then add low rank matrices (rank=24), which are also randomly initialized, in parallel to the weights of $f$. This gives us the true model $f^*$, which we want to approximate. The synthetic data $D = \\{(x_i, y_i)\\}_{i=1}^N$ is generated by sampling $x_i \\sim \\mathcal{N}(0, \\sigma I)$ and $y_i = f^*(x_i)$.\nThe results are summarized in Figure 2, where we compare the evolution of the validation loss of ROSA and LORA for fine-tuning the original MLP model $f$ to the data $D$ generated from the target task $f^*$. As observed in Figure 2, ROSA at different rank values finds solutions with similar performance to full fine-tuning. This demonstrates that even in a more practical setting than the one of Theorem 2(namely with non-linear models trained by gradient descent) ROSA can match the performances of full fine-tuning. Moreover, for both 1-layer and 2-layers MLPs, we see that the rank limitation of LoRA prevents it from fully adapting to the target task: while increasing the rank leads to better validation loss, the unavoidable low-rank bias is clearly demonstrated by the convergence to a sub-optimal loss. In contrast, ROSA always converges towards the optimal loss, even with rank parameter set to 1, and increasing the rank parameter leads to faster convergence to the optimal loss. Notably, using ROSA to adapt a two layer MLP containing a non-linearity recovers a model that well approximates the true model used to generate the data. This suggests that the formal result shown in Theorem 2 holds beyond the simple linear regression setting.\n4.2. GLUE Experiments\nIn this section we compare ROSA against LoRA and (IA)3, by adapting ROBERTabase (125M) on various tasks taken from the GLUE and SuperGLUE natural language understanding benchmarks. These benchmarks cover a wide variety of natural language understanding tasks, including logical entailment, grammatical acceptability, question-answering, textual similarity, and sentiment analysis. The pre-trained model weights for ROBERTabase are taken from the Huggingface library. A description of the specific subset of GLUE and SuperGLUE tasks tested on is available in Appendix B. Unlike some previous works which initialize the weights for MRPC, RTE, and STS-B with fine-tuned MNLI task-specific weights, we initialize the weights for all tasks with only the pre-trained ROBERTa weights for a fair comparison.\nThese tasks were selected to give a broad overview of ROSA's performance across a variety of different natural language tasks. We report development set performance for all tasks.\nIn Table 2 we see that ROSA outperforms LoRA and (IA)3 by a significant margin on multiple tasks. Most notably, on COLA using rank equal to eight we obtain Matthew's correlation coefficients of 64.80 for ROSA, 54.27 for LORA and 55.18 for (IA)3. Furthermore, ROSA remains as memory efficient as LoRA (Figure 3), and the factorization steps in ROSA add negligible latency (Table 1). We provide training curves of ROBERTabase fine-tuned on CoLA for 10 epochs in the Appendix.\n4.3. NLG Experiments\nIn this section we investigate the performance of ROSA in the natural language generation (NLG) setting. Namely, we compare the performance of GPT-2 using ROSA compared with LoRA and (IA)3, when fine-tuned on the E2E NLG task. The E2E NLG task involves producing a fluent natural language description of a restaurant given a logical form describing its various attributes. The model's generations are compared against multiple reference texts to account for variations in wording. The score provided is the maximum BLEU score across all the reference texts for a given input.\nIn Table 3 we see that ROSA outperforms LoRA and (IA)3 by a significant margin in the BLEU score.\n4.4. Which components of ROSA lead to its performance?\nIn this section we empirically study several aspects of ROSA. We highlight three key components of ROSA, on which we perform ablation studies. The key components of ROSA are:\n\u2022 SVD Initialization: ROSA adapters are initialized using SVD, as opposed to LoRA's adapters which are initialized to zero.\n\u2022 Factorization: In ROSA, pre-trained weight matrices are factorized into fixed and trainable portions such that the resultant transformation remains unchanged when the adapters are initialized.\n\u2022 Resampling: In ROSA the difference between the pre-trained weights and the final weights is not constrained to be low-rank, due to resampling and merging of subpsaces throughout training.\nWe study the effects of progressively adding these components to ROSA in Table 4. Specifically, we study the following ablations:"}, {"title": "4.5. Investigating different low-rank subspace sampling schemes", "content": "In this section, we compare the random subspace sampling of ROSA with two other subspace selection strategies: selecting the top-R or bottom-R singular vectors. In doing so, we validate that performing random selection of singular vectors is as performant as selection based on singular value information. In Table 5, we report the performance of ROBERTabase fine-tuned on CoLA using the different sampling strategies, which confirm that, on this task, random sampling performs similarly or better than other schemes.\n4.6. Limitations of ROSA\nWhile ROSA achieves better performance than previous state-of-the-art adaptation methods such as LoRA and (IA)3, it bears one main limitation compared with other methods. Namely, it requires storage of the whole model after it is adapted for a downstream task.\nOther adapter methods try to simultaneously address two challenges (1) reducing memory usage during training to ease the hardware barrier when adapting large models to a single downstream task and (2) reducing disk space usage when adapting a base model to many downstream tasks.\nROSA primarily focuses on addressing point (1), making it more suitable for scenarios involving a single downstream task. In comparison, other PEFT methods are better suited for scenarios involving multiple downstream tasks. ROSA excels in its specific domain, offering the same level of expressivity as full fine-tuning while requiring less GPU memory. This eliminates the need for (1) layerwise training, which would prolong training time, and (2) model sharding that necessitates more GPUs, thereby increasing training costs.\n4.7. Conclusion & Future Work\nIn this work we introduced ROSA: Random Subspace Adaptation. We first showed both theoretically and empirically that the low-rank nature of LoRA can often detrimentally affect its performance. In contrast, we demonstrate that ROSA can theoretically achieve the same solution as full fine-tuning. Furthermore, we demonstrate that on synthetic data ROSA indeed converges the same solution as full fine-tuning when using gradient based optimization. We evaluated ROSA against LoRA and (IA)3 on both natural language understanding and natural language generation tasks. Our experiments showed that ROSA achieved performance similar to full fine-tuning and outperformed other state-of-the-art methods such as LoRA and (IA)3 by significant margins. As our analysis was limited to adapting linear layers present in transformer models, adapting the parameters of convolution operations is an area for future work.\n4.8. Impact Statement\nThe primary goal of this work is to advance the field of Machine Learning. There are many potential societal consequences of our work: democratizing access to fine-tuning of large language models can have both positive and negative societal consequences, none which we feel must be specifically highlighted here."}, {"title": "5. Theorem Proofs", "content": "You may include other additional sections here.\nTheorem 1. Consider a simple multivariate least-square regression problem:\n$$\\arg \\min_W ||XW - Y||_F^2$$\nwhere $X \\in \\mathbb{R}^{n \\times d}$ and $Y \\in \\mathbb{R}^{n \\times p}$ are the input and output data matrices, respectively.\nConsider the sequence of fine-tuned weight matrices obtained by ROSA with rank parameter $R$ starting from a pre-trained weight matrix $W_0$, assuming that each intermediate minimization problem is solved exactly:\n$$W_t = W_{t-1} + A_t B_t$$\nwhere\n$$A_t, B_t = \\arg \\min_{A \\in \\mathbb{R}^{n \\times R}, B \\in \\mathbb{R}^{R \\times n}} ||X (W_{t-1} + AB) - Y||_F^2.$$\nThen, ROSA will converge to a fine-tuned matrix achieving the optimal error in at most\n$$T = \\frac{\\text{rank}(XW_0 - Y)}{R}$$"}, {"title": "6. Experimental setup", "content": "6.1. Implementation of ROSA, LORA and (IA)3\nFor better comparison, we re-implement the LORA and (IA)3 PEFT to share the same structure as ROSA. For all methods we use a vanilla implementation to focus on only comparing their core aspects. We list out the key differences between our implementation and those of LoRA and (IA)3.\n\u2022 We apply adapters to all attention matrices. In contrast, LORA tunes the attention matrices to which its adapter should be applied.\n\u2022 We do not add additional dropout modules inside the adapter, as is done in the LoRA paper.\n\u2022 We do not apply adapters to MLP layers as done so in (IA)3.\n\u2022 We use the same number of epochs across all model types (full fine-tuning, adapter). In contrast, LoRA and (IA)3 experiments are typically run for far more epochs (roughly 3X) than the full fine-tuning experiments.\n6.2. GLUE Experiments\nFor each experiment on GLUE we tune the LR for all three PEFT models for each selection of rank. Specifically, for a given task, model, PEFT method and rank value, we report the model that obtains the best validation set accuracy using LRs in {2e-2, 2-3, 2e-4, 2-5}. We use a factorization frequency of 2 in ROSA for all GLUE experiments (i.e., we merge then factorize the weight matrices once every two epochs.) We use the AdamW optimizer with \u03b21, \u03b22 = (0.9, 0.98), \u03f5 = 1e \u2013 6 and weight decay of 0.1. Our batch size is selected from the set {16, 32} and we use a sequence length of 512. We train all models for 10 epochs.\nBelow is a description of each of the GLUE/SuperGLUE tasks selected for evaluation:\n1. CoLA: a single-sentence classification task, where each sentence is labelled as either grammatical or not in English. The Matthews correlation coefficient is the reported metric.\n2. MRPC: a sentence-pair classification task, where each pair of sentences is labelled as either semantically equivalent (i.e. paraphrases of each other), or not.\n3. QNLI: QNLI is a sentence-pair classification task, where each pair of sentences corresponds to a paragraph from Wikipedia and a question, and the model must predict if the answer to the question is contained within the paragraph.\n4. RTE: The input for RTE is a pair of sentences, where the model must predict if the second sentence can logically be inferred from the first, or if it contradicts/is unrelated to it (binary classification).\n5. STS-B: The only regression task in the GLUE Benchmark, for STS-B the model must predict the similarity between a pair of sentences on a continuous scale of 1 to 5. The reported metric is the Pearson correlation coefficient.\n6. MNLI: The input for MNLI is a premise and a hypothesis, and the model must predict if the premise entails, contradicts, or is neutral toward the hypothesis. This is the same as RTE but with a separate neutral (unrelated) class. We report accuracy for the in-domain (matched) set.\n7. SST2: SST2 is a binary (positive/negative) sentiment classification dataset of movie reviews.\n8. BoolQ: BoolQ is a question-answering task where the input is a Wikipedia paragraph and a yes/no question where the answer is contained within the Wikipedia paragraph. The model must predict the answer to the question.\n9. WiC (Words-in-Context): WiC is a binary sentence-pair classification task of disambiguating word senses. Two sentences are provided to the model that contain the same word,, and the model must predict if the same sense of the word is used in both cases.\n6.3. E2E NLG Experiments\nThe E2E experiments are all carried out for 5 epochs. We also tune the LR in for each PEFT model by searching the space {2e-2, 2-3, 2e-4, 2e-5}. We use the AdamW optimizer with \u03b2\u2081, \u03b22 = (0.9, 0.999), \u0454 = 1e - 8, weight decay of 0.1, batch size of 10 and a sequence length of 512.\nAn example input and output for E2E is provided.\nInput: name [The Vaults], eatType [pub], priceRange [more than \u00a330], customer rating [5 out of 5], near [Caf\u00e9 Adriatic]\nOutput: \"The Vaults pub near Caf\u00e9 Adriatic has a 5 star rating. Prices start at \u00a330.\""}]}