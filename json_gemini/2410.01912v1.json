{"title": "A SPARK OF VISION-LANGUAGE INTELLIGENCE: 2-DIMENSIONAL AUTOREGRESSIVE TRANSFORMER FOR EFFICIENT FINEGRAINED IMAGE GENERATION", "authors": ["Liang Chen", "Sinan Tan", "Zefan Cai", "Weichu Xie", "Haozhe Zhao", "Yichi Zhang", "Junyang Lin", "Jinze Bai", "Tianyu Liu", "Baobao Chang"], "abstract": "This work tackles the information loss bottleneck of vector-quantization (VQ) autoregressive image generation by introducing a novel model architecture called the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer predicts more codes for an image by introducing a new autoregression direction, model depth, along with the sequence length direction. Compared to traditional 1D autoregression and previous work utilizing similar 2D image decomposition such as RQ-Transformer, the DnD-Transformer is an end-to-end model that can generate higher quality images with the same backbone model size and sequence length, opening a new optimization perspective for autoregressive image generation. Furthermore, our experiments reveal that the DnD-Transformer's potential extends beyond generating natural images. It can even generate images with rich text and graphical elements in a self-supervised manner, demonstrating an understanding of these combined modalities. This has not been previously demonstrated for popular vision generative models such as diffusion models, showing a spark of vision-language intelligence when trained solely on images. Code, datasets and models are open at https://github.com/chenllliang/DnD-Transformer.", "sections": [{"title": "INTRODUCTION", "content": "The field of autoregressive (AR) image generation is experiencing a resurgence of interest, largely driven by groundbreaking advancements in large language models (LLMs), exemplified by the release of ChatGPT (OpenAI, 2022). Because typical AR image generation methods also predict output in a next-token prediction manner, this resemblance has sparked significant efforts in two main areas: 1) transferring advanced, large-scale training techniques and expertise from LLMs to AR image generation models (Bai et al., 2023; Tian et al., 2024; Sun et al., 2024), and 2) developing truly multimodal foundation models capable of both understanding and generating multimodal information within a unified training framework (Lu et al., 2022; 2023; Team, 2024). These developments have the potential to lead to more versatile and powerful multimodal AI systems.\n\nA review of the development history of AR image generation approaches reveals significant efforts focused on finding better sequential decompositions of images and balancing reconstruction fidelity with prediction difficulty. Early models, like PixelCNN (van den Oord et al., 2016), generated images pixel by pixel. This approach was later enhanced by using vector-quantized variational autoencoders (VQVAEs) to compress images and model the prior distribution of discrete tokens in a compact latent space (Van Den Oord et al., 2017). Vector quantization (VQ) paved the way for notable models such as VQGAN (Esser et al., 2021), DALL\u00b7E (Ramesh et al., 2021), and MUSE (Chang et al., 2023), and it remains a core technique in recent AR image generation models like VAR (Tian et al., 2024) and LlamaGen (Sun et al., 2024), and multimodal foundation models like LVM (Bai et al., 2023), Unified-IO (Lu et al., 2022; 2023), and Chameleon (Team, 2024).\n\nHowever, despite advancements in AR image generation, VQ-based autoregressive methods face two persistent criticisms, especially juxtaposed with latent diffusion models (Rombach et al., 2022):\n\n1) Information loss inherent in the quantization process. Quantization, specifically in VQVAE, introduces significant information loss. With a typical configuration (N=8192, f=16), the Information Compression Ratio (ICR = $\\frac{\\log N}{24f^2}$, explained in Equation 1) is just 0.21%, drastically lower than the 8.3% of Stable Diffusion's VAE, hindering fine-grained detail reconstruction. According to Chameleon (Team, 2024), the authors note that their VQ tokenizer struggles to reconstruct finegrained details like text in images, which we believe is due to the low ICR of their tokenizer.\n\n2) Substantially increased computational requirements for producing higher-quality images. According to Equation 1,Increasing ICR by expanding the latent space (N) is logarithmically limited"}, {"title": "2D VISUAL TOKENIZER AND 2D AUTOREGRESSION", "content": "We introduce the basics of AR generation in Section A in the appendix. We can better understand the reconstruction ability of VQVAE from the lens of compression. Let us assume a VQVAE with downscaling factor f, codebook size N, input image's size of H \u00d7 W, then the shape of the quantized code is h \u00d7 w = (H/f) \u00d7 (W/f). We assume that the code follows a uniform distribution, so each code has $\\log N$ bits information. Its information compression ratio (ICR) is as follows.\n\n$ICR(N, f) = \\frac{(H/f) \u00d7 (W/f) \u00d7 \\log N}{H\u00d7W\u00d7 3 \u00d7 \\log 256} = \\frac{\\log N}{24f^2}$\n\nA typical configuration (N=8192, f=16) results in 0.21% ICR. This ICR is significantly lower than JPEG's 5% ICR (Wikipedia). To increase ICR, the 1D AR method could increase N (might face the codebook collapse problem (Mentzer et al., 2023) and the improvement is logarithmically bounded) or decrease f (more effective, but increases the token count quadratically)."}, {"title": "IMAGES' 2D DECOMPOSITION AND QUANTIZATION", "content": "As pointed out by Equation 1, the information compression ratio of VQVAE is bounded by the size of the codebook and the downscaling ratio. Residual Quantization (Lee et al., 2022b) proposes a new direction to quantize the image feature with multiple residual codes to reduce the quantization error and improve the quality of the reconstruction. For a feature map having h \u00d7 w vectors, RQVAE uses h x wx d codes to quantize the feature map, where d is the depth dimension of the code. For each feature vector v, RQ finds d codes ($q_1, q_2, ..., q_d$) by sequentially conducting d times residual decomposition and quantization operation Q(x) as finding the closest entry to x from the codebook:\n\n$q_d = Q(r_{d-1}), \nr_d = r_{d-1} - q_d, \nr_0 = v$\n\nConsequently, the sum of the residual codes $\\sum_{i=1}^{d} q_i$ is expected to approximate more closely the feature vector v, thus reducing the quantization error. We generalize this process as two-dimensional autoregression (DnD), which extends beyond Markov residual decomposition and can be applied to any decomposition operation, such as the diffusion process (Ho et al., 2020), etc.\n\nDnD Autoregression quantizes a 2D feature map m \u2208 $R^{h\u00b7w\u00b7c}$ by decomposing it in two directions. First, m is divided into hw feature vectors. Second, each vector v is decomposed into n codes ($q_1, ..., q_n$) using a function $D_n$ (v, Q) based on a codebook Q. The resulting quantized map q has shape hwn and is predicted in depth-first-spatial-second order. This decomposition could also be non-Markov, unlike RQVAE. The selection of potentially better decomposition functions is left for future exploration. We still use the residual quantization from Equation 2 as $D_n$. DnD decomposition increases the ICR d times (Equation 3), more effectively than increasing codebook size. The remaining challenge of predicting d times more codes is addressed by our DnD-Transformer.\n\n$ICR(N, f, d) = d \u00d7 \\frac{(H/f) \u00d7 (W/f) \u00d7 \\log N}{H\u00d7W\u00d7 3 \u00d7 \\log 256} = d \u00d7 \\frac{\\log N}{24f^2}$"}, {"title": "THE DND-TRANSFORMER", "content": "Prior section showed DnD visual tokenizers effectively reconstruct fine details like text. However, efficiently predicting the increased number of depth codes (d times more) remains challenging. Existing methods, like RQ-Transformer, use a separate transformer for depth, hindering integration with LLMs. We propose an efficient end-to-end architecture for multi-code prediction."}, {"title": "DND-TRANSFORMER DESIGN", "content": "Figure 5 shows DnD-Transformer and its variants: Parallel and Vertical Prediction. Parallel Prediction adds multiple prediction heads for simultaneous multi-depth code prediction, similar to accelerated LLM inference (Cai et al., 2024). However, this ignores the coarse-to-fine nature (Figure 4b) of code distributions, where deeper codes have smaller norms and are more centered. Vertical Prediction addresses this by sequentially predicting codes. Adding autoregression further refines this"}, {"title": "IMPLEMENTATION DETAILS", "content": "As shown in the left part of Figure 5, the increment of DnD-Transformer compared to vanilla transformer decoder is the additional output head and embedding add operation. Let's assume the linearized codemap's length is L = h \u00d7 w and code depth is d. During generation, DnD-Transformer conducts L forward process and each forward process generate d codes sequentially. After generating codes for all depths in a forward process, the embeddings of all codes are added up as the next input token. In this way, the model could generate L \u00d7 d tokens with only L forward passes, improving the generation quality with the same inference cost as standard 1D auto-regression transformer. The only additional hyper-parameter is the layer indexes to predict code of different depths. We adopt the same transformer decoder's architecture as LLaMA (Touvron et al., 2023) and, please refer to Appendix E for the training details of our DnD-Transformer."}, {"title": "EXPERIMENTS AND FINDINGS", "content": "Class-Conditional Image Generation. We conduct standard conditional image generation task with ImageNet-1k benchmark. Images are resized to 256\u00d7256 resolution during training and evaluation. We sample 50k images with classes uniformly distributed, and compute the FID, IS, Precision and Recall aganist the training set data using the ADM evaluation tool Dhariwal & Nichol (2021)."}, {"title": "Unconditional Rich-Text Image Generation", "content": "We collect two datasets for this task. Dataset examples are shown in Figure 6. Models are trained in a unconditional setting in this task. We aim to explore whether the tested vision generation models could understand and generate the complex logical interrelation among the generated elements such as language."}, {"title": "RESULTS OF RICH-TEXT IMAGE GENERATION", "content": "Generation Results on Text-Image. A DnD-Transformer (depth 1) and a DDPM model were trained on the same text-image dataset. Comparing 250 randomly sampled images from each, the AR model significantly outperformed the diffusion model in generating coherent text (lower OCR perplexity 7b; Generation examples 1, 16, 17, 18 and 19). This suggests the AR model's discrete token reconstruction enables effective autoregressive modeling. We also find that with a lower sampling temperature, the model would generate text images with lower PPL just like LLMs. Conversely, the diffusion model's simultaneous generation hinders text coherence.\n\nGeneration Results on arXiv-Image. An 8-layer visual tokenizer and corresponding DnD-Transformer trained on arXiv-Image outperformed diffusion model baselines, generating more valid words and phrases (Figure 8). However, arXiv-Image generation lagged behind Text-Image generation, suggesting joint language and figure modeling is more challenging. More results and baselines are in Figure 15 and 20. While SD3's VAE reconstructs arXiv images well (Table 1b), its generative performance is inferior to DDPM and AR, suggesting its latent space is less suitable for language modeling comparing to pixel or discrete space.\n\nA Spark of Vision-Language Intelligence. Autoregressive (AR) image generation exhibits a marked advantage over diffusion models in producing text-rich images, as demonstrated by our results. The pixel-level language generation inherent to AR models facilitates this capability. De-"}, {"title": "TRAINING BECOMES EASIER WHEN PREDICTING MULTIPLE CODES, SAMPLING NOT", "content": "Deeper DnD-Transformer codes achieve lower cross-entropy loss during training (Figure 9a), indicating lower entropy image decompositions. However, despite this, increased depth doesn't improve ImageNet generation fidelity, possibly due to the larger sampling space. Exploring this multi-depth sampling space for better generation is a promising research direction."}, {"title": "AR TRAINING LOSS FOR DIFFERENT DOMAINS ALIGN WITH INNER RANDOMNESS", "content": "Training loss for the same DnD-Transformer varies significantly across datasets (Figure 9b), being notably higher for ImageNet than rich-text images. While rich-text image loss nears that of LLMs, ImageNet loss sits between text and natural image datasets. The AR model's LLM-like training suggests it learns language from visual input alone, implying language's visual representation has lower entropy than natural images, easing the learning process."}, {"title": "RELATED WORK", "content": "Image Generation with VQVAE. The vector quantization (VQ) method has been pivotal in the development of generative models (Ramesh et al., 2021; Yu et al., 2022; Chang et al., 2023), which achieve image generation through the prediction of discrete image tokens. Efforts in this area focus on two main directions: the optimization of image tokenization techniques (Esser et al., 2021; Mentzer et al., 2023; Yu et al., 2023; 2024; Weber et al., 2024), and the strategic planning of effective decompositions of image tokens, such as MaskGit (Chang et al., 2022) and VAR (Tian et al., 2024). Meanwhile, alongside the advancement of large language models, there is growing interest in autoregressive image generation, which predicts image tokens sequentially (Tian et al., 2024; Sun et al., 2024). Recent research has also focused on developing multimodal foundation models (Lu et al., 2023; Kondratyuk et al., 2024; Wang et al., 2024b) that integrate both understanding and autoregressive image generation capabilities. They typically convert images or videos into sequences of discretized tokens and train over combined text-image/video token sequences within the AR modeling framework (Lu et al., 2022; Bai et al., 2023; Xie et al., 2024; Team, 2024). However, these models often struggle with inherent information loss during the image quantization and the significantly increased computational demands when generating higher-quality images. The DnD-Transformer that adopts the residual 2D decomposition of image features does not require additional modules or increased sequence length for high-quality and fine-grained image generation.\n\nRich-Text Image Generation. Despite recent significant progress in image generation, the task of rich-text generation within images remains a persistent challenge (Chen et al., 2023b; Ma et al., 2024; OpenAI, 2024). Most advancements have been witnessed in diffusion models (Betker et al., 2023; Saharia et al., 2022b;a), these models either leverage large language models to enhance the character spelling capabilities of generative models (Saharia et al., 2022b; Balaji et al., 2023; Saharia et al., 2022a) or attempt to explicitly control the position and content of the text using additional supervision from different modules (Tuo et al., 2024; Yang et al., 2023; Liu et al., 2024). However, most diffusion-based methods have primarily focused on text rendering Chen et al. (2023a;b); Balaji et al. (2023); Saharia et al. (2022a) in image generation, often limited to generating short words for logos and posters (Yang et al., 2023; Ma et al., 2023; 2024). The full potential of rich-text image generation remains largely unexplored. Our methods, which build on the foundation of DnD Autoregression, show substantial progress in generating rich-text images in an unconditional manner, highlighting the feasibility of conducting joint vision-language modeling tasks using purely images."}, {"title": "CONCLUSION", "content": "This paper investigated the limitations of autoregressive (AR) image generation methods, particularly the information loss and computational burden associated with vector quantization (VQ). We introduced 2-Dimensional Autoregression (DnD) and a novel end-to-end architecture, DnD-Transformer, which leverages a depth dimension autoregression alongside the spatial dimension to mitigate these limitations. Our experiments demonstrate that DnD-Transformer achieves significant improvements in image quality, outperforming strong baselines like LlamaGen without increasing model size or sequence length. Notably, DnD-Transformer showcases emergent vision-language intelligence, generating text-rich images unconditionally, a known weakness of diffusion models. These findings highlight the potential of DnD for efficient and high-quality AR image generation and underscore the promise of this approach for advancing multimodal foundation models."}, {"title": "PRELIMINARY: AUTOREGRESSIVE IMAGE GENERATION", "content": "In this section, we introduce the fundamentals of autoregressive image generation. The pipeline is rooted in the Vector Quantized Variational Autoencoder (VQVAE) (Van Den Oord et al., 2017) and the autoregressive Transformer (Vaswani et al., 2017). This approach has been adopted from the early DALLE (Ramesh et al., 2021) to the latest LlamaGen (Sun et al., 2024)."}, {"title": "STEP1: TRAIN THE VISUAL TOKENIZER AND TOKENIZE THE IMAGES", "content": "Images initially exist in the pixel-level RGB color space, which consists of little semantic information and makes it challenging to directly model prior knowledge. For example, an image with a resolution of 256 \u00d7 256 comprises 256 \u00d7 256 \u00d7 3 = 196, 608 distinct values, representing the individual red, green, and blue intensities for each pixel. The large sequence length makes it difficult to train in autoregressive manner similar to language models' technique. Van Den Oord et al. (2017) proposed the Vector Quantized Variational Autoencoder (VQVAE), which significantly alleviates the problem. It downscales and tokenizes the image from the original sparse RGB space into a dense and discrete representational space (codebook) Q by finding the nearest entry. The VQVAE is typically implemented in an encoder-decoder architecture, with its primary training objective being to minimize the image reconstruction loss. You could refer to Van Den Oord et al. (2017) for details in training a standard VQVAE."}, {"title": "STEP2: LEARN THE PRIOR DISTRIBUTION OF IMAGE TOKENS", "content": "Having tokenized the source images into discrete tokens and trained a visual decoder to map these tokens back to real images, the next crucial step is to learn the prior distribution of the discrete tokens. This distribution enables the sampling process, which is essential for generating new images. AR Image generation generally first linearizes the h \u00d7 w image tokens q \u2208 Q in a raster scan order and formalize 1D sequence (91, 92, 93, ..., $q_{hxw}$) for the transformer (Vaswani et al., 2017) model to learn.\n\nDuring training, the training objective is the same as GPT's next token prediction task (Radford et al., 2018), that the model is required to predict the next image token given the previous tokens and class or text conditional tokens $h_p(q_t | q_{<t}, c)$. After training, we can generate images by autoregressively sampling h \u00d7 w tokens from the model. The sampled 1D sequence of image tokens is then reshaped to 2D code map with height h and width w. This reshaped token map is subsequently fed into the trained VQVAE decoder, which reconstructs the final image from the code representation.\n\nClassifier-Free Guidance As a technique to enhance the visual quality and text-image alignment, classifier-free guidance (Ho & Salimans, 2022) has been adopted across the diffusion models (Rombach et al., 2022; Podell et al., 2023), VQ models (Chang et al., 2023) and autoregressive models (Sun et al., 2024) for image generation. During the training, the model is exposed to data with and without conditioning: the conditioning is randomly discarded from a fraction of the training samples. We have implemented this approach in our model as well. Specifically, during training, we randomly replace the conditional embedding with a learnable unconditional embedding in 10% of the cases. At the inference stage, the logits $l_g$ are recalculated for each generated token. We form the $l_g$ by subtracting the unconditional logits $l_u$ by conditional logits $l_c$ with the guidance scale t through the following equation:\n\n$l_g = l_u + (l_c - l_u) \u00d7 t$"}, {"title": "TRAINING DETAILS OF VISUAL TOKENIZERS", "content": "We follow (Lee et al., 2022b) to train the 2D tokenizers with residual decomposition a combined objective of 12 loss, GAN loss and perceptual loss. Codes from different depth share the same codebook. We train all tokenizers a fixed learning rate of 4e-5, a total batch-size of 256 for 100 epochs and select the one with lowest validation loss as the final tokenizers. We conduct all training on 8xA100 GPUs."}, {"title": "RECONSTRUCTION RESULTS OF TEXTS", "content": "Figure 11 shows the reconstruction result on arXiv images of different visual tokenizers."}, {"title": "ABLATION ON DND-TRANSFORMER'S STRUCTURE", "content": "Table 3: Ablation of DnD-Transformer Architecture on ImageNet dataset. All models follow the same training setting as in Appendix E."}, {"title": "DETAILS OF HYPER-PARAMETERS OF DND-TRANSFORMER", "content": "Table 4 shows the hyper-parameters of our trained models. The XXL model has the same setting as in GPT2 (Radford et al., 2019) and LlamaGen (Sun et al., 2024) for fair comparisons. For DnD-Transformer with multiple prediction heads, the prediction layers' indexes are set to [39, 48] when there are two heads, [39, 42, 45, 48] when there are 4 heads in the ImageNet experiments, [27, 30, 33, 36, 39, 42, 45, 48] when there are 8 heads in the arXiv-Image experiments.\n\nAll transformer models were trained using settings similar to LlamaGen (Sun et al., 2024): a base learning rate of $10^{-4}$ per 256 batch size, the AdamW optimizer with $\u03b2\u2081 = 0.9$, $\u03b2\u2082 = 0.95$, and a weight decay of 0.05, along with gradient clipping at 1.0. A dropout of 0.1 was consistently applied to the input token embedding, attention module, and feed-forward network (FFN) module. Similarly, a dropout of 0.1 was used for the class condition embedding for classifier-free guidance. Training was performed for 300 epochs, and the final checkpoint was used for performance evaluation."}, {"title": "GENERATION RESULTS OF DND-TRANSFORMERS", "content": "Figure 12: Conditional generation comparisons between LlamaGen-XXL and DnD-Transformer-XXL on class \u201cgolden retriever\" from ImageNet. We random sampled 16 images with cfg=4. DnD-Transformer generates images with higher quality than the 1D AR model."}]}