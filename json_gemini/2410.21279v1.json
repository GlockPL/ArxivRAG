{"title": "Comparative Global AI Regulation: Policy Perspectives from the EU, China, and the US", "authors": ["Jon Chun", "Christian Schroeder de Witt", "Katherine Elkins"], "abstract": "As a powerful and rapidly advancing dual-use technology, AI offers both immense benefits and worrisome risks. In response, governing bodies around the world are developing a range of regulatory AI laws and policies. This paper compares three distinct approaches taken by the EU, China and the US. Within the US, we explore AI regulation at both the federal and state level, with a focus on California's pending Senate Bill 1047. Each regulatory system reflects distinct cultural, political and economic perspectives. Each also highlights differing regional perspectives on regulatory risk-benefit tradeoffs, with divergent judgments on the balance between safety versus innovation and cooperation versus competition. Finally, differences between regulatory frameworks reflect contrastive stances in regards to trust in centralized authority versus trust in a more decentralized free market of self-interested stakeholders. Taken together, these varied approaches to AI innovation and regulation influence each other, the broader international community, and the future of AI regulation.", "sections": [{"title": "Introduction", "content": "Proposed in April 2021 and agreed upon by December, the EU Act was the first major coordinated effort to regulate AI; it came into force in August 2024. The Biden administration published its Blueprint for an AI Bill of Rights in October 2022. It spoke to the need to protect citizen's privacy and freedom from algorithmic discrimination. The following month, on November 25th, several Chinese government ministries jointly released regulations on AI-generated deepfakes. The US Whitehouse Executive Order #14110 \"Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\" was issued in October 2023. In February 2024, California State Senator Scott Wiener introduced what was arguably the strictest AI regulation with the \"SB-1047 Safe and Secure Innovation for Frontier Artificial Intelligence Models Act\". After ten major revisions across both the California Senate and Assembly, the bill passed on September 3rd and now faces an uncertain future as it awaits Governor Newson's signature to become law.\nUnderlying these varied global efforts are common concerns over the expected social, economic, and geopolitical impacts of AI. The European Union has taken a proactive stance with regard to social effects of AI, implementing stringent regulations aimed at fostering competitiveness while prioritizing ethical considerations, enhancing privacy protections, and mitigating potential harms. China has focused on aligning AI development with \"core socialist values\" while also addressing issues of transparency and workers' rights. The United States, meanwhile, has grappled with concerns about AI-generated disinformation, election integrity, and the role of content recommendation systems in social media, particularly in light of events such as the 2020 elections and the rise of platforms like TikTok [Smit et al., 2022].\nBeyond the societal effects, there is universal acknowledgement of the centrality of AI to related technologies like advanced chips, energy production and storage, 5G/6G telecommunications, satellites, and robotics. These illustrate the importance of AI to national economics, competitiveness, and security. Some argue that the EU's stricter AI regulation may inadvertently stifle innovation, deter investment, and weaken Europe's position in the global AI and technology race [Suominen, 2020]. The tension between regulatory safety and competitiveness is particularly evident in the dynamics between China and the US, with both nations striving to balance innovation with responsible AI development [Information Technology and Innovation Foundation (ITIF), 2024].\nStill, there are efforts to transcend national policies and competitions to develop a global harmonization of AI regulation. AI automation threatens widespread job displacement, exacerbates inequality and accelerates the need to transition to a rapidly-evolving job market [The White House, 2024a]. In September 2024, the United Nations' AI Adivsory Body released a report highlighting key areas of concern that transcend national boundaries. These include pervasive latent biases, emergence of surveillance states, and AI generated disinformation [UN AI Advisory Body, 2024]. Additionally, serious legal, security, and humanitarian issues-particularly related to autonomous weapons and public security-underscore the importance of international cooperation. As AI continues to evolve, the global community faces the complex task of developing regulatory frameworks that can effectively address these multifaceted challenges while fostering innovation and ensuring equitable benefits across nations."}, {"title": "AI Governance", "content": "This section describes the emerging AI regulatory frameworks in the EU, China, and the US at both federal and state levels. In particular, it contrasts the top-down, risk-based approach of the EU AI Act with the more market-driven approach of the US. The latter emphasizes coordinating existing legal, regulatory, and enforcement entities from the federal level down to states and cities. In between is the Chinese approach, which has the appearance of centralized regulatory control, but in practice emphasizes decentralized innovation, regional competition, and economic development at the local levels.\nWhile the EU and China appear to have relatively stable AI regulatory frameworks, there is a growing debate in the US about the future direction of AI regulation. The Biden Executive Order #14110 on \"Safe, Secure, and Transparent Development and Use of AI\" coordinates over 100 specific tasks both within and between over 50 federal entities in a decentralized way that largely augments existing regulatory laws and agencies. However a number of US Congressional committees, proposals, and influential public/corporate interest groups are lobbying for a new AI regulatory structure that is more centralized, restrictive, and punitive than EO #14110. Some even promote centralized registration of models, proofs of AI safety, and criminal penalties [LegiScan, 2024, Schumer, 2023]."}, {"title": "The European Union", "content": "The 2024 EU AI Act is positioned as the world's first comprehensive AI law [European Union, 2024]. Just as prior European general purpose legislation, such as the 2016 General Data Protection Regulation (GDPR) [European Union, 2016], the EU AI Act represents complex joint efforts and interests across various EU bodies, including the European Commission, the European Parliament and the European Council, where the latter represents the Heads of State of all EU member countries. Influence on the Act's formation was also taken publicly by national government officials, such as France's premier Macron's overt lobbying for exemptions for open source AI providers such as Mistral [Abboud and Espinoza, 2023], as well as, both publicly and covertly, by lobbying and industry groups, including Big Tech [Perrigo, 2023], and German pro open-source non-profit LAION [LAION, 2023].\nUniquely, the Act was first constructed within a product safety framework, but then blended with a fundamental rights agenda at the behest of the European Parliament and against pressure by the European Commission [The Privacy Advisor Podcast, 2024]. This approach resulted in a unique and novel blend of legislative frameworks, clearly setting the EU AI Act apart from prior legislation building on established frameworks such as the GDPR. As Dragos Tudorache, a member of the European Parliament from Romania and the chair"}, {"title": "Overview", "content": "of the Special Committee on Artificial Intelligence in a Digital Age, remarked: \"Regulation isn't just rules, it's an opportunity to express our values [Tyrangiel, 2024].\"\nWhile constituting an innovative syncretism at heart, the Act does follow and respect earlier European generalist regulatory initiatives, such as the GDPR, and the 2012 Digital Markets Act [Parliament and of the European Union, 2022, DMA]. In fact, the EU started examining the compatibility of the GDPR and AI as early as in 2020 [Sartor and Lagioia, 2020]. Nevertheless, the release of ChatGPT in November 2022 and its rapid adoption by millions of consumers worldwide caught European policymakers off-guard and led to significant adjustments to the Act's handling of AI governance. On December 9th, 2023, the Act was provisionally agreed between the European Parliament and the European Council. During the 3-day round of negotiations, the Act's scope was tightened [Consilium, 2023]. It was clarified that the Act does not apply outside of European law, does not infringe on the security competences of the member states or so-entrusted entities, nor any military or defense applications. Importantly, it was clarified that the Act would not apply to sole purposes of research and innovation, nor other non-professional use. Most importantly, the Act's traditional approach to AI systems risk classification was complemented by the notion of general-purpose AI systems (GPAI) [European Union, 2024, Article 3(63)], resulting in a parallel governance track for such AI systems. Despite open questions, the Act's ambition to distinguish between GPAI and non-GPAI systems, and GPAI systems of systemic risk, is similarly unique among AI regulations globally."}, {"title": "The Geopolitics of the Act", "content": "Besides regulating the EU single market, the Act is widely regarded as a strategic effort by the European Commission to establish themselves as the leading AI rulemakers globally [Chatham House, 2024]. Just as after the adoption of the GDPR in 2016 [European Union, 2016], it is speculated that companies across the world will begin to prioritize compliance with European AI law out of economic necessity, not through coercion [Almada and Radu, 2024]. In the case of the GDPR, this form of \u201cBrussels Effect\" [Bradford, 2020] was complemented by a \"de jure\" effect in which countries with a lack of their own regulatory capacity, such as many developing countries, incorporated EU laws instead. For example, the Philippines incorporated the right to be forgotten into their Data Privacy Act of 2012 [of the Philippines, 2012, Linklaters, 2024]. In a similar way, it is speculated that the EU AI Act may similarly become the de-facto standard for AI governance in much of the Western and developing world [Almada and Radu, 2024].\nOne direct institutional consequence of the EU AI Act is the establishment of a novel authority under the helm of the European Commission, namely the EU AI Office. This new office will not only oversee AI regulation and AI systems' compliance, provide a central pool of AI expertise to the EU's member states, but also provides a \"strategic, coherent, and effective European approach on AI"}, {"title": "Laws and Regulation", "content": "at the international level\" [European Commission, 2024]. A specialist position, The Advisor for International Affairs, will represent the AI Office in \"global conversations on convergence toward common approaches\" [EU AI Act, 2024]. The EU AI Office therefore is not only meant to consolidate the EU's approach to AI regulation within the European market, but will likely centrally support the EU's foreign policy ambitions in economic and trade negotiations.\nThe Act is designed as an adaptive legislation, meaning that many details are intentionally left vague to permit later adaptation as technology changes. At its core lies a risk classification system that puts obligations mostly on the developers (\"providers\") of AI systems. Importantly, the Act not only applies to systems that are placed on the market or put into service in the EU, but also to AI systems whose output is used in the EU. The use of AI systems for national security, research, or recreational purposes, as well as, more generally, end-users of AI systems are excluded from regulation under the Act."}, {"title": "Systems and Models", "content": "The AI Act adopts the definition of AI system from the OECD's AI Prin-ciples, defining an AI system as \u201ca machine-based system that is designed to operate with varying levels of autonomy and that can, for explicit or implicit objectives, generate outputs such as predictions, recommendations, or decisions that influence physical or virtual environments\" [OECD, 2019]. AI models, conversely, are mathematical algorithms or trained models that, in isolation, lack additional components such as a user interface or hardware integration that would allow them to be used as an AI system.\nAmong models, the Act further disambiguates between non-GPAI and general-purpose AI (GPAI) models, with the latter defined as \u201can AI model,"}, {"title": "Roles", "content": "including where such an AI model is trained with a large amount of data using self-supervision at scale, that displays significant generality and is capable of competently performing a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications, except AI models that are used for research, development or prototyping activities before they are placed on the market\" [European Union, 2024, Article 3(63)]. Where a GPAI model is integrated into an AI system, the system is referred to as a GPAI system provided that the system has the capability to serve a variety of purposes.\nThe Act fundamentally distinguishes between the roles of provider, deployer, importer and distributor. Providers are those \u201cplacing on the market or putting into service AI systems or placing on the market general-purpose AI models in the Union\" [European Union, 2024, Article 2], irrespectively of their location. They have pre-market obligations including an initial risk assessment, risk-specific compliance, as well as risk-specific post-market obligations. Deployers are users of AI systems that are based on the EU and that don't fall within a small number of non-professional use cases. The Act furthermore distinguishes between AI models or systems provided or deployed from within the EU, and those from outside."}, {"title": "Risks", "content": "At the core of the EU AI Act lies a risk classification system for AI systems based on their possible \"direct\" use cases. In the case of GPAI systems, the degree of \"directness\" is understood as the extent to which implemented safety measures can prevent risk-relevant use by deployers. If local market surveillance believe that a GPAI systems can be (or become) \u201cdirectly\" used for high-risk activities the EU AI Office will carry out the corresponding compliance proce-dures [European Union, 2024].\nProhibited use cases. Integrating fundamental human rights into the product safety framework, the Act defines a class of prohibited AI practices, such as placing on the market AI systems that can be used for certain forms of manipulation and exploitation, social scoring purposes, and certain biometric identification purposes. Furthermore, the deployment of AI systems that leave the user uninformed about their interaction with an AI system, emotion recognition systems or biometric categorisation systems, or AI systems producing deepfakes are all likewise prohibited [European Union, 2024, Article 5].\nHigh-risk use. The class of high risk systems constitutes the majority of risk-related regulation [European Union, 2024, Article 6]. High-risk systems include a wide variety of systems as defined in [European Union, 2024, Annex I-III],"}, {"title": "Innovation and Open Source", "content": "including systems meant to serve as safety systems for other AI systems. According to [European Union, 2024, Article 28(2a)], providers of high risk systems are subject to compliance obligations, including the establishment of risk and quality management systems, data governance, human oversight, cybersecurity measures, postmarket monitoring, and maintenance of the required technical documentation. Owing to the Act's adaptive nature, it is expected that these obligations will be further detailed in later, sector-specific regulation.\nLimited risk. Chatbots or AI systems that generate content or aid in decision-making without any critical safety aspects or significance are deemed of limited risk, although the Act only indirectly defines this class [European Union, 2024, Recital 32a]. These systems are merely subject to transparency obligations, including end-users of such systems must be informed that they are interacting with AI.\nMinimal risk. AI systems that pose little to no risk to users' rights, health, or safety are left unregulated by the Act, although other obligations under EU law still apply. These systems are sometimes referred to as minimal risk although this term is not used in the Act.\nImportantly, in the case of GPAI models, a special risk category is defined for the standalone model even before having been integrated into an AI system.\nGPAI models of systemic risk. The Act imposes particular regulation on providers of general-purpose AI models of systemic risk [European Union, 2024, Article D], which it defines to be all models for which \u201cthe cumulative amount of compute used for its training measured in floating point operations (FLOPs) is greater than 10^{25}\u201d [European Union, 2024]. The limit of 10^{25} was reportedly"}, {"title": "China", "content": "The Act contains several measures intended to harness the economic and societal benefits of open-source AI software [Eiras et al., 2024a,b]. The Act defines free and open-source AI components to cover \"the software and data, including models and general-purpose AI models, tools, services or processes of an AI system\" and explicitly states that provision of such models on open repositories should not seen as a form of monetization [European Union, 2024, Recital 103].\nThe EU Act contains wide-ranging exemptions for providers of certain AI systems provided under free and open source software licenses [European Union, 2024, Article 53-54]. To be exempt, the systems may not contain GPAI models that fall within the systemic risk category, or otherwise exhibit unacceptable behavior. The Act distinguishes the above from providers of pre-trained AI models that are made accessible to the public under a license that allows for the access, usage, modification, and distribution of the model, and whose parameters, including the weights, the information on the model architecture, and the information on model usage, are made publicly available. It is to be noted that the term open source model is not used explicitly and the degree of legal overlap with open source software is not immediately clear, and hence such models might be referred to rather as open models. Open models are not exempt from Article [European Union, 2024, C(1)(c)-(d)], as well as [European Union, 2024, Article D] and [European Union, 2024, Article 28(2a)], the latter governing third-party obligations \"along the AI value chain of providers, distrib-utors, importers, deployers or other third parties\" [Sartor and Lagioia, 2020] for high-risk AI systems.\nUnder any circumstances, providers of open GPAI model providers are responsible for transparency obligations according to [European Union, 2024, Article C(1)(c)-(d)]. These transparency requirements include respecting existing Union copyright law [European Union, 2024, Article C(1)(c)] according to Arti-cle 4(3) of the Digital Single Market Directive (EU) 2019/790 [European Union, 2019], and the need to make a \u201csufficiently detailed summary of the content used for training of the general-purpose AI model\u201d [European Union, 2024, Article C(1)(d)]. The consequences of these transparency requirements for open GPAI model providers have been examined in [Warso et al., 2024]. Importantly, Direc-tive (EU) 2019/790 expands GDPR regulation to copyright owners who share content online, meaning that key GDPR rights, such as the right to opt out [Eu-"}, {"title": "Overview", "content": "ropean Union, 2016, GDPR Article 7] would require that copyright owners could ask for their data to be removed from open GPAI model training data.\nAs a further measure to stimulate innovation, member states can establish a regulatory sandbox, i.e. a controlled environment that facilitates the development, testing and validation of innovative AI systems (for a limited period of time) before they are put on the market [Madiega, 2024].\nChina's approach to AI governance and regulation is a hybrid between the centralized, top-down approach of the EU and the decentralized, free-market of competing interests in the US. Like the EU, China emphasizes safety, indi-vidual protections, and social harmony through top-down guidance, regulation, and enforcement [Zhang, 2022]. Like the US, China also emphasizes bottom-up innovation and economic development through a mix of decentralized provincial control alongside very competitive local markets. This hybrid approach seeks to optimize the benefits from both the EU and US models. The EU AI Act takes a coherent, universal risk-based approach, but the abstract and ambiguous lan-guage belies the hard work of grappling with real-world details in applying these general rules to disparate, complex and highly situational cases. Conversely, a fragmented, sector-specific approach like the US EO #14110 lacks coherent high-level simplicity, but benefits from experienced domain experts translating goals into more clear, immediate, and effective enforcement. China seeks to benefit from the coherence of the EU AI Act and the practical benefits of the US approach, which promotes innovation and economic competitiveness."}, {"title": "Laws and Regulations", "content": "China has advanced some of the first AI laws and regulations at the national level, which are summarized in Table 2. Unlike the horizontal risk-based ap-proach of the EU, China has favored the sector-specific US approach of laws tailored to specific use-cases. These specific use-cases range from data privacy (November 2021) to recommendation algorithms (March 2022) to generative AI (January & August 2023). Despite appearances of centralized government con-trol, Chinese AI regulations are the product of an iterative process involving di-verse stakeholders that include mid-level bureaucrats, academics, corporations, startups, and think tanks [Sheehan, 2024]. The central government relies upon a pipeline of these experts to formulate, clarify, and interpret the details, while local officials mainly concern themselves with ensuring goals and outcomes are aligned with Chinese and socialist ideology [Zhang, 2022]. Both China's State Council and the Chinese Academy of Social Sciences have announced intentions of working towards a more holistic National AI law, although the outcome is uncertain [Webster et al., 2023]."}, {"title": "Registration", "content": "On paper, China has perhaps the most onerous AI regulation requirements of the three regions considered. Table 3 lists the three major steps for deploy-ing advanced AI models (for example, Baidu's LLM ERNIE) in order to be in compliance with regulatory laws (see Figure 4). These include model regis-tration, rules for data management, and provisions for ongoing monitoring for compliance. The registration process alone illustrates how strict central reg-ulation can slow down innovation and economic growth. As of March 2024, only 546 AI models have been registered, and just seventy are Large Language Models [China Money Network, 2024]. This number is in stark contrast to the countless commercial models, variants, and over 500,000 open-source LLMs on Huggingface.co [Huggingface.co, 2024], which is banned in China [ChinaTalk, 2023]."}, {"title": "Compliance and Industrial Policy", "content": "In 2015, China announced a national strategic plan and industrial policy called \"Made In China 2025\" or MIC2025 integrated with their 13th (2016-2020) and 14th (2021-2025) Five Year Plan [The State Council of the People's Republic of China, 2015]. MIC2025 directs strong government support for innovation and high-end manufacturing to help make China a global leader in cutting-edge technologies like AI by 2025 [Congressional Research Service, 2019]. Part of this plan calls for supporting 10,000 \"Little Giants,\" the small and mid-sized enterprises (SME) recognized as a key source of innovation [Global Times, 2021]. Although large \"National Champions\" like Baidu, Tencent, and Alibaba are expected to fully comply with AI regulations because of their dominant influence, the Little Giants are informally afforded leeway in order to avoid heavy regulatory burdens that could stifle innovation [Zhang, 2024].\nWhat this means from a practical standpoint is that despite such rigorous guidelines, enforcement in China is relatively lax. Startups and SMEs fly under the radar as long as they do not have a large public presence [Zhang, 2022]. This approach allows for the promotion of innovation, economic growth, and international competitiveness [Yang, 2024].\nChina's hybrid system of AI regulation and selective enforcement attempts to combine the strengths of both the EU and the US approaches. While regu-latory guidance is generally light, top-level enforcement usually comes into play when destabilizing patterns arise. This reactive enforcement can cause transi-tory market disruptions and lead to strict and sometimes surprisingly punitive measures to reign in excesses and outcomes at odds with CCP values like \"com-mon prosperity\" [Caixin Global, 2021]. This pattern of regulatory crackdown is visible in other sectors from real estate[Bloomberg News, 2021] to education [In-tresse, 2024]. Harsh penalties were levied by regulators between 2020-2022 to try to control excessive inequality and check the rise of powerful tech (Alibaba) and financial (Ant Group) corporations that could challenge government author-ity [Chen and Liu, 2023]. Although deflating the real estate bubble significantly"}, {"title": "United States", "content": "reduced household wealth tied to property speculation, the IMF shows China leads the world's largest economies with a 5.2% GDP growth [International Monetary Fund, 2024]. Some of this success is attributed to China's strategic industrial policy with its flexible regulatory framework.\nOn October 30, 2023 US President Biden signed an executive order (EO #14110) on the \"Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\" [The White House, 2023b]. This act moved beyond the voluntary commitments secured in July 2023 [The White House, 2023a] and the October 2022 AI Bill of Rights [The White House, 2022]. EO #14110 represents the most comprehensive form of AI regulation in the United States to that date. It directly delegates AI responsibilities to over 50 existing federal regulatory agencies and other bodies with over 100 specific tasks designed to:\n\u2022 Build out the capacity to address emerging concerns around AI\n\u2022 Integrate AI into agency operations\n\u2022 Enhance coordination between agencies on AI-related matters\nOn August 28, 2024 the California Assembly passed SB 1047, the Safe and Secure Innovation for Frontier AI Models Act. Unlike the federal Presidental Executive Order, this state law focused on creating a regulatory framework to test, register, and audit models that could present a danger to public safety. This AI regulation targets models with substantial investment in pretraining and fine-tuning above given thresholds of $100M/10^{26} flops and $10M/10^{25} flops respectively."}, {"title": "Overview", "content": "AI regulation in the US represents somewhat of a departure from the more typical US approach to regulation. In the US, the legislative branch typically passes laws that form the framework for regulation, which are then enforced by the executive branch, primarily under the oversight of various federal agencies. For example, the US Congress passes laws that define specific industries or ac-tivities along with broad goals such as advancing scientific research [National Science Foundation, 2024], promoting fair markets [Security and Exchange Com-mission, 2024], and safeguarding the environment [Environmental Protection Agency, 2024] (see NSF, SEC and EPA mission statements and goals). At times, multiple agencies will be tasked with regulating different aspects of the same broad goal. For example, the Federal Trade Commission (FTC), the Consumer Product Safety Commission (CPSC), and the Consumer Financial Protection Bureau (CFPB) specialize in different aspects of consumer protection and safety."}, {"title": "Laws and Regulations", "content": "US States and municipalities have also added regulations in areas they feel are inadequately addressed by federal regulations.\nThis approach is in keeping with historical norms. The philosophical dis-trust of centralized power is reflected in the very design of the American system of checks and balances between branches of government. A decentralized US approach is also a way to reduce bureaucratic layers, more directly empower domain experts, and balance power between competing narrow, self-interested parties. These include powerful voting blocks, special interests, and a $46 bil-lion state and federal lobbying industry [Massoglia, 2024]. For these reasons, commercial applications of technology within the US have traditionally been regulated through various mechanisms including legislative action, executive orders, agency rulemaking, industry self-regulation, international agreements, and private self-regulation.\nTo remain competitive in rapidly changing world markets, US tech com-panies often pursue self-regulation as a strategy for tackling privacy, digital advertising, content moderation, and cybersecurity [Cusumano et al., 2021, Mi-now and Minow, 2023]. We see a similar approach taken in the Biden-Harris approach to securing voluntary commitments by leading AI companies to man-age the risks posed by AI [The White House, 2023b]. Furthermore, interna-tional agreements or regulations are sometimes adopted by US companies to do business abroad, as in the case for EU's GDPR [European Union, 2016] and China's Cybersecurity Law [National People's Congress of the People's Republic of China, 2016]. The regulatory process often combines these approaches, with laws providing the foundation for agency regulations involving public input and expert consultation as technologies and circumstances evolve.\nThe rapid pace of AI innovation and the immense potential impact of AI, coupled with the lack of technical expertise in government, has reversed the normal sequence for enacting regulation that begins with the U.S. Congress. EO #14110 is a case where the executive branch is initiating many AI-related policies-from research to regulation-partly due to its ability to more quickly respond in a coherent and comprehensive manner The White House [2023b]. Although somewhat exceptional for the US process of lawmaking, White House Presidential executive orders more closely match the top-down, centralized or-ganization of the European Commission in Brussels and the CCP in Beijing.\nIn spite of this similarity to the E.U. and China, aspects of the order nonethe-less reflect the distinct US approach that can be characterized as \"bottom-up\" and distributed rather than \"top-down\" and centralized. In contrast to the more centralized, top-down approach to AI regulation prioritizing safety (EU) and social stability (China), the United States takes a more distributed, multi-stakeholder approach to AI regulation that mirrors its earlier approaches to regulating new technologies.\nWhile universal directives on AI are provided by the centralized political bodies of the CCP and to a lesser extent, the European Commission, a wide range of guidelines, initiatives, laws, and other policies including trade related to AI are distributed between various US federal branches and agencies and even states [Perkins and Coie, 2024]. EO #14110 organizes this distributed"}, {"title": "White House Executive Order 14110", "content": "regulatory system with specific objectives and deadlines delegated to various federal agencies directly from the executive branch.\nMeanwhile, the US legislative branch is considering dozens of individual bills [GovTrack.us, 2024]. Thune's 2023 AI Research, Innovation and Account-ability Act would create enforceable accountability and transparency for high-risk systems. The REAL Political Advertisements Act [Klobuchar, 2023] aims to limit the use of Generative AI in campaigns, The Stop Spying Bosses Act [Casey, 2023] aims to limit the use of AI by employers to surveil employees, and the No FAKES Act [Coons et al., 2023] aims to protect visual and voice likenesses of individuals. Two notable, ambitious, and more restrictive plans have been in-troduced by Senator Schumer in the form of his SAFE initiative [Schumer, 2023] and the Blumenthal-Hawley Framework [Office of Senator Richard Blumenthal, 2024]. At this time, however, none have been passed. Meanwhile, individual US states and municipalities have passed laws and are debating more exten-sive regulation regarding AI [International Association of Privacy Professionals, 2024]. In 2023 more than 40 bills were proposed, and Texas and Connecticut adopted statutes focused on preventing discrimination [White and Case, 2024]. In the 2024 legislative session, at least forty states, Puerto Rico, The Virgin Islands and Washington D.C. have introduced AI bills and six states, Puerto Rico, the Virgin Islands have adopted resolutions or passed legislation [National Conference of State Legislatures, 2023]. While Article VI of the US Constitu-tion affirms the supremacy of federal law over state law, California is home to many of the largest AI corporations, and other industries (e.g. auto emission levels) have often complied with Calfornia's stricter standards.\nSince 2016 and over three different Presidential administrations, a number of executive orders related to AI have been issued. The Biden White House's O\u0441-tober 2023 \"Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\" is the most comprehensive to date [The White House, 2023b]. It directs over fifty federal agencies to take over one hundred specific actions addressing eight core policy areas listed in Figure 5 including: safety and security, innovation and competition, worker support, bias and civil rights, consumer protection, privacy, federal use of AI, and international lead-ership [The White House, 2024b]. The eight policy areas are ranked by the aggregate number of requirements and federal entities assigned to each area. These arguably provide a loose sense of priorities in each policy area from the most relevant (federal use, safety/security, and innovation/competition) to the least (worker support). EO #14110 implements many guidelines in the 2022 \u0391\u0399 Bill of Rights to ensure the responsible design and use of artificial intelligence with regards to civil rights and privacy in areas such as hiring, healthcare, and surveillance [The White House, 2022].\nEO #14110 also addresses many of the core concerns highlighted in the EU AI Act. It does so, however, with several key differences [Congressional Research Service, 2024]. While the EU AI Act establishes a new regulatory agency, the EU"}, {"title": "California SB 1047", "content": "AI Office, which coordinates with member states, industry and civil society, the current US strategy relies upon augmenting the extensive network of existing US federal agencies with pre-existing specialized domain expertise. The US approach can be seen to emphasize extending expansive regulatory and legal frameworks from the ground up where infrastructure already exists, in contrast to creating a new centralized regulatory framework.\nBecause the US approach involves over fifty federal agencies, it is also much more extensive in implementation details than the EU. It directly addresses broader issues like unemployment, education, research, and consumer protec-tion. Finally, and again in contrast to the EU AI Act, this US strategy is arguably more immediately actionable given the over one hundred specific ob-jectives. Many deadlines are delegated to federal agencies to be completed within 180 to 270 days. These agencies are already specialized across a broad spectrum of existing federal government responsibilities that are being disrupted by AI. As of this writing, both the White House 180-day and 270-day deadlines have been met [The White House, 2024c,d"}]}