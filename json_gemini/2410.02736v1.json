{"title": "JUSTICE OR PREJUDICE?\nQUANTIFYING BIASES IN LLM-AS-A-JUDGE", "authors": ["Jiayi Yet", "Yanbo Wang", "Yue Huang", "Dongping Chen", "Qihui Zhang", "Nuno Moniz", "Tian Gao", "Werner Geyer", "Chao Huang", "Pin-Yu Chen", "Nitesh V. Chawla", "Xiangliang Zhang"], "abstract": "LLM-as-a-Judge has been widely utilized as an evaluation method in various bench-\nmarks and served as supervised rewards in model training. However, despite their\nexcellence in many domains, potential issues are under-explored, undermining their\nreliability and the scope of their utility. Therefore, we identify 12 key potential bi-\nases and propose a new automated bias quantification framework\u2014CALM\u2014which\nsystematically quantifies and analyzes each type of bias in LLM-as-a-Judge by us-\ning automated and principle-guided modification. Our experiments cover multiple\npopular language models, and the results indicate that while advanced models have\nachieved commendable overall performance, significant biases persist in certain\nspecific tasks. Empirical results suggest that there remains room for improvement\nin the reliability of LLM-as-a-Judge. Moreover, we also discuss the explicit and\nimplicit influence of these biases and give some suggestions for the reliable applica-\ntion of LLM-as-a-Judge. Our work highlights the need for stakeholders to address\nthese issues and remind users to exercise caution in LLM-as-a-Judge applications.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs), such as GPT-4 (OpenAI, 2024a), have exhibited exceptional\ncapabilities across a wide range of natural language processing (NLP) tasks, including applications\nin medicine (Liu et al., 2023b), LLM-based agents (Huang et al., 2023a; Guo et al., 2024; Chen\net al., 2024d;b), science (Guo et al., 2023; Li et al., 2024a; Chen et al., 2024e; Le et al., 2024),\nand data synthesis (Zhao et al., 2024; Wu et al., 2024a). In recent research, there has been a focus\non using LLMs to automatically evaluate responses and provide rewards. This methodology is\ncommonly known as LLM-as-a-Judge, which involves using LLMs to assess responses in two main\nways: comparing pairs of answers to determine superiority (Zheng et al., 2024), or directly scoring\nindividual answers based on specific criteria (Liu et al., 2023a). This method has been primarily\napplied in scoring and pairwise comparison tasks, yielding notable achievements (Kasner & Du\u0161ek,\n2024; Liu et al., 2023a).\nDespite the increasing adoption of LLM-as-a-Judge, concerns regarding its reliability have emerged\ndue to potential biases within the models (Zheng et al., 2024; Chen et al., 2024c; Wang et al., 2023b;\nKoo et al., 2023). These biases cast doubt on the trustworthiness of LLMs, both in their evaluation\nprocesses and in their alignment with principles of fairness and transparency (Sun et al., 2024;\nHuang et al., 2023b). For instance, Zheng et al. (2024) conducted extensive experiments to examine\npositional preferences in LLM-as-a-Judge, while Koo et al. (2023) revealed that popular opinions\nreflecting majority viewpoints may compromise the fairness of LLM evaluations. Furthermore,\nexperiments conducted by Chen et al. (2024c) demonstrated that fabricated citations could disrupt\nthe judgment accuracy of LLMs.\nWhile these studies have highlighted several types of biases existing in LLM-as-a-Judge, the field\nremains ripe for further exploration. Firstly, the existing analyses of bias are relatively narrow in\nscope (Wang et al., 2023b; Chen et al., 2024c), which limits the development of a comprehensive\nframework for evaluating the multifaceted biases affecting LLM-as-a-Judge. Secondly, many previous\nstudies have relied on human evaluators to assess the quality of answers and compare them against\nthe judgments made by LLMs to identify potential biases. This methodology incurs substantial costs\nand introduces human subjectivity, complicating the establishment of reliable ground truth and the\nreproducibility of findings (Zheng et al., 2024). Additionally, Wu & Aji (2023) demonstrated that the\nlimited size and scope of test data increase the risk of random interference, potentially obscuring the\ntrue extent of bias in LLM judgments. A more detailed discussion of related work is in Appendix A.\nTo address these challenges, we introduce CALM, a novel\nframework for automated quantification of biases in LLM-\nas-a-Judge. CALM covers 12 distinct types of bias that may\narise when LLMs are used as judges in various scenarios,\nincluding the following examples.\n\u25b7 Correctness of Scientific Reasoning. When using\nLLMs to judge reasoning results in scientific QA or an-\nswer to math problems (Cobbe et al., 2021; Hendrycks\net al., 2021), bias often occurs in understanding the con-\ntent. Therefore, we focus on evaluating potential biases\nin LLM judges, specifically regarding verbosity (favoring\nlonger responses), fallacy oversight (ignoring logical er-\nrors in reasoning), and sentiment (preference for positive\nor negative expressions).\n\u25b7 Improvement on Answer Refinement. Answers to\nopen-ended questions in the humanities, social sciences,\nor general knowledge can often be refined to improve\nquality. When LLMs are used to determine whether a\nrefined answer is better than the original, bias occurs if the\nLLM judge is informed about the refinement process.\n\u25b7 Alignment to Human Feedback. LLMs are increasingly used to assess which generated answer\nbetter aligns with human feedback when provided with two or more answers. In such cases, alignment\nbias often occurs, e.g., the LLM judge favor answers based on their placement (position bias), or\nfavor answers they generated themselves (self-preference).\nAs we can see, automating the process of bias identification in various judging scenarios is challenging,\nbut highly beneficial. We design this process using an attack-and-detect approach. In CALM, an\nLLM judge is presented with deliberate perturbations (the \u201cattack\") applied to the content being\njudged. The judgment results are then examined to determine whether the judge's score or preference\nremains consistent. While more details on how CALM automates this processing will be provided\nlater, several advantages are already evident, such as the elimination of subjective human assessments\nand the reduction of testing costs, resulting in a more objective and scalable evaluation approach.\nIn summary, our contributions are three-fold: (1) A systematic definition and categorization of 12\ndistinct types of bias that can undermine the reliability and trustworthiness of LLM-as-a-Judge. (2)\nThe introduction of CALM, a framework for evaluating biases in LLM-as-a-Judge systems, which\nenhances the integrity of the assessment process without relying on human resources. (3) An extensive\nevaluation of six popular LLMs using the CALM framework, as shown in Figure 1, reveals that\nwhile some LLMs demonstrate notable fairness in judgment, there remains significant room for\nimprovement in achieving more robust decision-making across various types of bias.\""}, {"title": "PROPOSED FRAMEWORK: CALM", "content": "Our proposed framework, CALM, which stands for Comprehensive Assessment of Language Model\nJudge Biases, is illustrated in Figure 2. CALM comprises four integral components: 1) Comprehensive"}, {"title": "BIAS ASSESSMENT PROBLEM FORMULATION", "content": "To formally quantify biases in LLM-as-a-\nJudge, we define the input prompt for LLM\njudge as $P = (I,Q, R)$, which consists of\nthree components: system instruction $I$, ques-\ntion $Q$, and responses to be judged $R$. A per-\nturbation is applied to investigate the potential\nbias in the judgment by making a bias-related\nmodification to the original response. We au-\ntomate this process by using another LLM\nto change $R$ to $g(R)$ or modify the $I$ to $g(I)$\n(e.g., insert a system prompt into $I$), resulting\nin a modified $P'$. For example in Figure 3,\nthe response given by Assistant B has been\nlengthened from the original response to as-\nsess verbosity bias. The output of LLM judge\non $P$ and $P'$ is compared for measuring the\npotential bias:\n$y = LLM(P), \\hat{y} = LLM(P')$.\nHere, if the judgment scores $y$ and $\\hat{y}$ differ, it\nindicates the presence of bias in this LLM-as-a-Judge setting. The desirable outcome is when $y$ and $\\hat{y}$\nare the same, showing that the LLM judge is robust and unbiased.\nIn judge cases involving pairwise comparison, the input prompt for LLM judge is defined as\n$P = (I,Q, R_1, R_2)$, including two candidate responses $R_1$ and $R_2$ for comparisons. Simi-\nlar perturbations can be applied to one record $\\hat{y} = LLM(I, Q, R_1, g(R_2))$ or to the instruction"}, {"title": "BIAS TYPES AND AUTOMATED PERTURBATION", "content": "Bias Types. Considering the diverse use cases of LLM-as-a-Judge, we have synthesized and\nexpanded upon previously proposed biases, ultimately arriving at a total of 12 types of bias, which are\nsummarized in Table 1 with examples for facilitating the understanding. Due to the space limitation,\nwe show more details of these bias types in Appendix B.\nAutomated Perturbation g(\u00b7). The automation of bias injection is key to automating the entire bias\nassessment process. As introduced in section 2.1, the perturbation $g(\u00b7)$ modifies either the response\n$R$ or the instruction $I$. It is crucial that the perturbation does not alter the correctness of the response\nand preserves the original meaning as much as possible to avoid semantic shift. At the same time, it\nmust not be too trivial, as this would result in a response that appears unchanged and fails to expose\nany potential evaluation bias.\nWe develop $g(\u00b7)$ as a principle-guided modification powered by LLMs, following the approach of\nconstitutional AI (Bai et al., 2022). By applying multiple sets of guidelines (i.e., instructions), an LLM\ncan modify answer content, resulting in biased counterparts of the original answers. For instance, as\nshown in Figure 3, one raw answer is modified by an LLM through a prompt-based guideline. The\ncomplete set of instructions for answer modification is provided in Appendix C and Appendix F. For\ndifferent types of bias and various judging tasks that will be discussed in subsection 2.3, we designed\nspecific guidelines (i.e., instructions) to ensure that the modifications effectively inject the appropriate\nbias into the content."}, {"title": "JUDGING TASKS, DATASETS AND METRICS", "content": "Judging Tasks. The use of LLM-as-a-Judge is typically implemented in two well-established ways:\npairwise comparison (Zheng et al., 2024) and scoring (Liu et al., 2023a). One drawback of the\nscoring method is that, without a reference answer, it can be challenging for LLM judges to provide\nan objective score, as their judgments can be easily influenced by contextual factors. In contrast,\npairwise comparison mitigates this issue and has been widely utilized for alignment data based on\nhuman annotations (Ouyang et al., 2022).\nConsequently, we primarily adapt the pair-\nwise selection task for LLM judges in as-\nsessing most biases. However, for cer-\ntain biases, such as self-enhancement and\nrefinement-aware bias, the pairwise se-\nlection method is difficult to apply; thus,\nLLM judges are evaluated using the scor-\ning judgment task instead. In the scor-\ning task, as introduced earlier, the LLM\njudge provides a numerical score for a\ngiven response, y = LLM(I, Q, R). In\nthe pairwise comparison task, the LLM\njudge evaluates two responses and out-\nputs a preference for one over the other,\ny = LLM(I, Q, R1, R2). More details\nare shown in Table 2.\nDatasets. We prepared three datasets in CALM for supporting bias assessment in various judging\ntasks: fact-related, refinement-aware evaluation, and alignment datasets. The details of these datasets\nare shown in Table 3. Their usage in the assessment of different types of bias is presented in Table 2.\n\u25b7 Fact-related dataset. We constructed a fact-related dataset for the assessment involving bias types\nthat require factual information as test content, and for the cases where the quality of the response\nshould not be affected by the presentation style of the model's response. We utilized GPT-4-Turbo\nto generate both a relatively good answer and an answer with complete reasoning logic but of\nlower overall quality. They are used as $R_1$ and $R_2$ as a pair in $P$. This dataset allows us to modify\nresponses without affecting their inherent quality when dealing with biases such as verbosity bias,\nthereby more accurately determining whether the observed perturbation is due to the bias itself.\n\u25b7 Refinement-aware evaluation dataset. This dataset is constructed for assessing the bias when\nLLM judge is used to determine whether a refined answer is better than the original. We selected"}, {"title": "EXPERIMENTAL SETUP", "content": "Models. Based on the recent study (Gao et al., 2024; Liu et al., 2023a; Li et al., 2024b), LLMs with\nstronger capabilities are prefered to be used as judges, because weaker LLMs may exhibit greater\nrandomness in their judgments, which can undermine the reliability of judging results. We thus\nevaluated six popular and capable LLM judges within our framework, including both proprietary\nand open-source options to provide a comprehensive analysis and comparison. The selected models\nare: ChatGPT (OpenAI, 2024b), GPT-4-Turbo (OpenAI, 2024a), GPT-40 (OpenAI, 2024c), Claude-\n3.5 (Anthropic, 2024), GLM-4 (GLM et al., 2024), and the open-source Qwen2-72B-Instruct (Bai\net al., 2023), which are further detailed in Table 8. Additionally, to mitigate the influence of\nself-enhancement bias, we selected four models solely for response generation: Mixtral-8x22b\n(AI@Mistral, 2024), Llama3-70b (AI@Meta, 2024), Llama3-8b (AI@Meta, 2024), and Mistral-7b\n(AI@Mistral, 2023).\nJudgement prompt P. The instruction I in the judgment prompt P = (I, Q, R) is derived from\nLiu et al. (2023a) and Zheng et al. (2024), with slight variations to evaluate the impacts of biases in\nLLM-as-a-Judge. The complete instruction we used is provided in Appendix F.\nHyperparameters. We followed the experimental setup of Chen et al. (2024a) by setting the\ntemperature to 0.7 and applied it to all judge models and generating models to ensure stable output\nquality and strong reproducibility."}, {"title": "EVALUATION RESULTS", "content": "In this section, we introduce our main results and related analyses from our exploratory experiments.\nWe show the main results in Figure 4 and Table 4. Furthermore, we conduct exploratory experiments\nto evaluate the potential influence bias factor in LLM-as-a-Judge, which are detailed in Figure 5,\nTable 5, Figure 6 and Figure 7. Due to the space limitation, we show more detailed information of\nexperiment results in Appendix D.\nEven advanced models can exhibit unexpected vulnerabilities in judgment. Figure 4 illustrates\nthe influence of 12 distinct biases on the judging capabilities of six LLMs. Notably, the effects of\nthese biases differ across models, and advanced models may not always exhibit better performance\nwhen dealing with these biases. While Claude-3.5 generally shows the greatest resilience to biases,\nour findings reveal that even highly proficient models can struggle. For example, despite its advanced\ncapabilities (Zheng et al., 2023), GPT-4-Turbo exhibits inconsistency when judging emotional\nresponses, whereas ChatGPT demonstrates more stable performance. This complexity suggests that\nidentifying the best model is not straightforward; it depends on the specific bias involved, and even\ntop-tier models may display unexpected weaknesses. Therefore, when using LLMs as judges, it is\ncrucial to acknowledge these complexities and avoid assuming that the most advanced model will\nalways be the most reliable.\nBias is more pronounced in the alignment dataset compared to the fact-related dataset. Accord-\ning to Table 4, the impact of bias is more pronounced in the alignment dataset than in the fact-related\ndataset. One possible explanation for this is that, in the fact-related dataset, the quality differences\nbetween answers are more evident, which means that the influence of bias is insufficient to completely\noffset this quality gap. In contrast, the alignment dataset typically has smaller quality differences\nbetween answers, making the choices of the judge model more vulnerable to bias. Therefore, when\ndeveloping a reliable LLM-as-a-Judge framework across different datasets, it is crucial to consider\nthe inherent quality of the data."}, {"title": "ANALYSIS OF EXPLORATORY EXPERIMENTS", "content": "Position bias increases with more answer candidates. Figure 6 demonstrates that all judge models\nare significantly impacted by position bias. This bias becomes more pronounced as the number\nof answers increases, particularly when evaluating three or four options, resulting in a decreased\nrobustness rate, with most models scoring below 0.5. To mitigate the effects of position bias, we\nrecommend using judge models with better robustness rate metrics or randomizing the order of\nanswers (Zheng et al., 2024; Li et al., 2023b).\nResponse length influences model judgment in com-\nplex ways. As illustrated in Figure 6, increasing response\nlength without a corresponding improvement in quality\nled to a decline in model robustness rate. Some models ex-\nhibited an aversion to excessively verbose answers, while\nothers demonstrated a positive correlation between model\npreference and response length.\nAvoid using the same model to generate and judge an-\nswers. Analysis of Figure 5, Figure 7, and Table 5 reveals\na significant self-enhancement bias among LLMs. Most\nmodels rated their outputs more favorably, even when\nanswer sources were anonymized. These findings under-\nscore the importance of using separate models for answer\ngeneration and evaluation in LLM-as-a-Judge to maintain\nobjectivity in assessments.\nBandwagon-effect involvement percentage is not impactful. The percentage of people favoring\nan answer did not significantly impact model robustness rate. GPT-40 remained consistent, while\nClaude-3.5 was more influenced by popular opinion. Figure 6 shows that involvement percentage\ndoes not significantly affect model choices.\nLLMs show sensitivity to irrelevant content in responses. Figure 7 demonstrates that including\nirrelevant content reduces the robustness rate of model judgments. Different models show varying\ndegrees of susceptibility to this type of interference. Notably, from the average, the impact is more"}, {"title": "DISCUSSION", "content": "Explicit and implicit influence of bias. We identified\ntwo distinct types of biases: explicit and implicit. Ex-\nplicit biases are those where the LLM clearly states its\npreference for certain attributes in its decision-making pro-\ncess. Implicit biases are influences that affect judgments\nwithout being directly acknowledged in their reasoning.\nOur case studies illustrate these biases in Appendix E.\nThe Authority bias exemplifies an explicit bias, where the\nLLM openly favored answers containing citations, even\nwhen these were fake. This demonstrates a clear pref-\nerence for responses that appear scholarly, regardless of\ntheir actual validity. Conversely, the refinement-aware bias\nrepresents an implicit bias. Here, the LLM consistently\nscored refined answers higher, despite providing similar\njustifications for different instances and never explicitly\nmentioning refinement as a factor in its decision-making\nprocess. The findings indicate that LLMs are influenced by various factors. The disparity between\ntheir internal processing and expressed reasoning underscores the importance of conducting more\nresearch into the nature of LLM bias. It is essential to comprehend these biases to enhance the\ntrustworthiness and reliability of LLM-as-a-Judge.\nSuggestions for application. In discussing potential strategies to mitigate biases in LLM-as-a-\nJudge, we propose the following recommendations aimed at enhancing the fairness of models while\nmitigating bias interference:\n\u25b7 Carefully construct prompts and implement advanced reasoning strategies. We recommend\ncreating prompts that include specific protective phrases to guard against various types of biases,\nsuch as instructing the model to disregard the identity information of the person being evaluated.\nAdditionally, implementing advanced reasoning strategies similar to CoT can guide the model\nthrough a step-by-step decision-making process.\n\u25b7 Establish prompt injection safeguards. We recommend instituting protective measures against\nprompt injection related to the bias types discussed in this paper. These safeguards can prevent\nmodels from being influenced by biased information embedded in prompts. By implementing such\nprotective measures, we can enhance the fairness of LLM-as-a-Judge, ensuring that the judging\nprocess is not compromised by external attempts to introduce bias.\n\u25b7 Implement bias detection mechanisms. Based on our experimental findings, we suggest im-\nplementing a simple, prompt-based bias detection mechanism similar to the one we developed\nin Figure 32. This approach can proactively identify potential biases in judging templates before\nthe actual judging process begins. As presented in Table 6, our results demonstrate that while\nthe effectiveness varies across different bias types, this method shows promise in uncovering a\nmajority of biases."}, {"title": "CONCLUSION", "content": "This paper presents CALM, an automated evaluation framework for assessing potential bias when\nLLMs are employed as judges in various application scenarios. CALM provides a comprehensive\nexamination of 12 types of biases and utilizes an automated bias injection and qualification method,\nresulting in an objective and scalable evaluation approach. Our experiments show that while models\nmay reliably serve as judges for specific tasks, there remains significant room for improvement in the\nbroader use of LLMs as judges. CALM could be used to evaluate future, more advanced LLM-based\njudge solutions, ensuring they meet higher standards of bias mitigation."}, {"title": "ETHICAL CONSIDERATION", "content": "It is significant to emphasize that some of the question sets and bias-related responses may contain\nNSFW content. While we have manually reviewed and curated this data to ensure its appropriateness\nfor research purposes, we urge readers and potential users of our findings to exercise caution and\ndiscretion. We recommend that any application or extension of this work should be conducted\nresponsibly, with due consideration for ethical guidelines and potential societal impacts."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "To ensure reproducibility, the supplementary materials accompanying this paper include our complete\nexperimental code, datasets, and evaluation scripts. These materials cover core components such\nas data generation, prompt templates, and API handlers, as well as specific code and result logs for\ndifferent bias types. This resource allows other researchers to verify and replicate our experimental\nfindings."}, {"title": "A RELATED WORKS", "content": "A.1 LLM-AS-A-JUDGE\nRecent studies have demonstrated that LLMs can serve as high-quality evaluators for various NLP\ntasks (Li et al., 2023a; Kasner & Du\u0161ek, 2024; Huang et al., 2024a; Wang et al., 2023a), and Zheng\net al. (2024) proposed the concept of LLM-as-a-Judge. As an evaluation method that does not require\nreference texts, it has demonstrated performance on open-ended questions that highly match human\npreference. Recent research has focused on exploring its fairness, for instance, Shi et al. (2024a)\nintroduced JudgeDeceiver, emphasizing the vulnerabilities in the evaluation process. Zhang et al.\n(2023) conducted research indicates that wider and deeper LLM networks often provide more fair\nevaluations. Liu et al. (2023a) proposed ALIGNBENCH for the multi-dimensional evaluation of\nLLMs' fairness."}, {"title": "FAIRNESS IN TRUSTWORTHY LLMS", "content": "Ensuring the trustworthiness of LLMs is of great significance Liu et al. (2024); Shi et al. (2024a);\nHuang et al. (2024b); Gao et al. (2024); Wu et al. (2024b). In recent research, it has been discovered\nthat LLMs may exhibit stereotypes against certain groups or make erroneous judgments based on\nspecific statistical patterns (Zhuo et al., 2023; Ferrara, 2023; Liu et al., 2024), which highlights the\nimportance of fairness in evaluating LLMs. Fairness of LLMs is defined as the ethical principle\nof ensuring that LLMs are designed, trained, and deployed in ways that do not lead to biased or\ndiscriminatory outcomes and that they treat all users and groups equitably (Sun et al., 2024). The\nimbalance in pre-training data can lead to imbalances during model training (Liu et al., 2024),\nresulting in biases against certain demographic groups, such as different genders (Wan et al., 2023),\nages (Macnicol, 2006), and various languages (Jiao et al., 2023; Bang et al., 2023). Consequently, the\nfairness of LLMs has a significant impact on the trustworthiness of LLM-as-a-Judge."}, {"title": "BIASES IN LLM-AS-A-JUDGE APPLICATION", "content": "Recent research has identified various cognitive biases that influence the evaluation of LLMs. Some\nstudies (Zheng et al., 2024; Shi et al., 2024b; Wang et al., 2023b) discuss biases such as position\nbias, verbosity bias, and self-enhancement bias. Another study (Koo et al., 2023) highlights order\nbias, compassion-fade bias, and egocentric bias, along with salience bias, bandwagon-effect bias,\nand attentional bias. Further biases noted in additional research (Chen et al., 2024c; Stureborg et al.,\n2024) include fallacy-oversight bias, authority bias, and beauty bias. Recognizing these biases is\nessential for developing more objective and trustworthy LLM evaluation methods."}, {"title": "DETAILS OF BIAS TYPES", "content": "\u25b7 Position bias: LLMs may favor responses based on their position in the input. This bias affects\nhow the model processes information, and following Zheng et al. (2024), we extend the analysis to\nscenarios involving more than two responses.\n\u25b7 Verbosity bias: LLM-as-a-Judge may be biased towards longer responses. We evaluate the impact\nof different length ratios between responses on judgment outcomes, as indicated by Zheng et al.\n(2024).\n\u25b7 Compassion-fade bias: LLM judgments may be influenced by the anonymity of model names.\nWe investigate how various model names and anonymization strategies impact judgments, inspired\nby the observations of Koo et al. (2023).\n\u25b7 Bandwagon-effect bias: LLM-as-a-Judge may be biased by the presence of majority opinions.\nWe assess this by setting varying percentages (60%, 70%, 80%, and 90%) of majority opinions in\nthe system instruction, following Koo et al. (2023).\n\u25b7 Distraction bias: Introducing distractions could affect the judgments of both high-quality and\nlow-quality model outputs. We extend previous work by Koo et al. (2023) to evaluate the impact\nof distractions in LLM decision-making. Experimental details are available in Appendix C.\n\u25b7 Fallacy-oversight bias: This bias relates to the LLM's ability to recognize and avoid logical\nfallacies. We develop tests to evaluate this ability across various types of fallacies, contributing to\nfair and accurate judgments, as discussed in Chen et al. (2024c).\n\u25b7 Authority bias: Authoritative references may sway LLM judgments. We assess this influence by\nincorporating three types of references-book citations, website references, and famous individuals'\nquotes-following the methodology of Chen et al. (2024c).\n\u25b7 Sentiment bias: LLMs may display preferences towards certain emotional tones in responses. We\nevaluate how sentiment influences judgments across emotional expressions such as cheerful, sad,\\angry, and fearful, as noted by Li & Sinnamon (2023).\n\u25b7 Diversity bias: Judgments may shift based on specific identity markers. We evaluate this bias\nby setting system instructions that assign six identity categories: Female, Black individuals,\nHomosexuals, Muslims, Refugees, and HIV patients, following the concept of identity impact.\n\u25b7 Chain-of-Thought (CoT) bias: LLM judgments can be affected by the presence of explicit\nreasoning steps. We compare evaluations of responses with and without chain-of-thought reasoning\nacross different tasks, as suggested by Wei et al. (2023)."}, {"title": "DETAILS OF BIAS EVALUATION", "content": "We will introduce the detailed evaluation process of each bias.\n\u25b7 Position bias: To investigate the impact of position bias, we tested the effect of changing the order\nof answers when there are two, three, and four answers. We employed a rotational permutation\norder to shuffle the answers and conducted evaluations to observe if the results remained consistent.\nIf the selection of any answer was inconsistent across different orders, it was considered an\ninconsistency. For specific evaluation prompts, refer to Figure 13, Figure 14, and Figure 15.\n\u25b7 Verbosity bias: We employed GPT-4-Turbo to lengthen the worse quality answers in the fact-\nrelated dataset. The lengthened prompt used for this process can be referenced in Figure 18. Our\nobjective was to increase the verbosity of these answers while preserving their essential content.\nSubsequently, we conducted a comparative evaluation, wherein the models reassessed both the\noriginal answers pair and the elongated answers pair.\n\u25b7 Self-enhancement bias: Building upon the single-answer scoring prompt proposed by Zheng\net al. (2024), we developed a scoring prompt Figure 26. In our experimental design, each model\nwas tasked with answering the questions and subsequently evaluating both its responses and\nthose generated by other models, without prior knowledge of the authorship of any given answer.\nFollowing the completion of all evaluations, we aggregated and analyzed the scoring results to\ndraw our conclusions.\n\u25b7 Compassion-fade bias: We explicitly reveal the identities of all models under judgment. The\nspecific details of this approach are delineated in the evaluation prompt presented in Figure 20.\nSubsequently, we conducted a comparative analysis between these named-condition results and\nthose obtained under anonymized settings.\n\u25b7 Bandwagon-effect bias: We added the following sentence to the system prompt: \"{number}%\nof people believe that {chosen_model} is better in this question\" to test whether the model is\ninfluenced by popular opinion. For specifics, refer to prompt Figure 21.\n\u25b7 Distraction bias: We inserted a meaningless statement into the System Prompt, such as \"Assistant\nA loves eating pasta,\" to test whether the LLM Judge is influenced by it. For specifics, refer to\nprompt Figure 29. We applied distinct distractions to each assistant and subsequently analyzed\nthe robustness rate of the judge model's evaluations in scenarios where the distracted Assistant\nprovided either the better or worse quality response.\n\u25b7 Fallacy-oversight bias: We modified the better quality answers in the fact-related dataset using\nGPT-4-Turbo to make their logic completely chaotic. The prompt used can be referenced in\nFigure 19. We then had the models re-evaluate these answer pairs. If a model's evaluation result\nwas inconsistent with its original assessment of the answer pair, we considered it a correct judgment\n(because the original worse quality answer is still better than the logically chaotic better quality\nanswer). Otherwise, it was deemed an incorrect judgment.\n\u25b7 Authority bias: Using GPT-4-Turbo, we generated three types of fake citation information related\nto the answers: URLs, famous quotes, and book references. For specifics on the prompts used for\nthe generation, refer to Figure 24, Figure 25, and Figure 23. These citations were then injected into\nthe answers, as demonstrated in Figure 22.\n\u25b7 Sentiment bias: We modified the better quality answers in the fact-related dataset using GPT-4-\nTurbo to incorporate one of the four emotions: cheerful, sad, angry, or fear. The prompt can be\nreferenced in Figure 27. Then, we had the models judge these answers again to observe whether\nthe results were consistent with the original judgment.\n\u25b7 Diversity bias: For diversity bias, we selected six identities that may be subject to discrimination:\nHomosexual, Black, Female, HIV Positive, Refugees, and Muslim believers. These identities were\nthen injected into the system prompt for judgment to observe their impact on evaluations. For more\ndetails, refer to prompt Figure 28."}, {"title": "DETAILED RESULTS", "content": "In Figure 4, we provide a comparative chart of the robustness rate for all biases, which allows for\na horizontal comparison of the differences in resilience to interference among all models, with the\ndashed line representing the consistency rate. In Table 7, the detailed experimental results for each\ntype of bias are presented.\n\u25b7 Position bias. We present the robustness rate of different judge models when faced with pairwise\ncomparisons in Table 7, and in Figure 6 we show the robustness rate of all judge models when\npresented with multiple answer options.\n\u25b7 Verbosity bias. In Figure 6, we illustrate the relationship between different ratios of answer\nexpansion lengths and model robustness rate.\n\u25b7 Self-Enhancement bias. In Figure 5, we present a heat map of Z-score normalized scores for each\nmodel (due to ChatGPT's relatively weak performance, the scores given to it by the remaining\nmodels are not high enough, resulting in the first column lacking reference value). Additionally, in\nFigure 7, we display the ErrorRatese metric for each judge model.\n\u25b7 Bandwagon-Effect bias. In Table 7 and Figure 6, we present the impact of varying percentages of\npublic opinion on the judge"}]}