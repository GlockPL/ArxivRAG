{"title": "Improving Decision Sparsity", "authors": ["Yiyang Sun", "Tong Wang", "Cynthia Rudin"], "abstract": "Sparsity is a central aspect of interpretability in machine learning. Typically, sparsity is measured in terms of the size of a model globally, such as the number of variables it uses. However, this notion of sparsity is not particularly relevant for decision making; someone subjected to a decision does not care about variables that do not contribute to the decision. In this work, we dramatically expand a notion of decision sparsity called the Sparse Explanation Value (SEV) so that its explanations are more meaningful. SEV considers movement along a hypercube towards a reference point. By allowing flexibility in that reference and by considering how distances along the hypercube translate to distances in feature space, we can derive sparser and more meaningful explanations for various types of function classes. We present cluster-based SEV and its variant tree-based SEV, introduce a method that improves credibility of explanations, and propose algorithms that optimize decision sparsity in machine learning models.", "sections": [{"title": "Introduction", "content": "The notion of sparsity is a major focus of interpretability in machine learning and statistical modeling [Tibshirani, 1996, Rudin et al., 2022]. Typically, sparsity is measured globally, such as the number of variables in a model, or as the number of leaves in a decision tree [Murdoch et al., 2019]. Global sparsity is relevant in many situations, but it is less relevant for individuals subject to the model's decisions. Individuals care less about, and often do not even have access to, the global model. For them, local sparsity, or decision sparsity, meaning the amount of information critical to their own decision, is more consequential.\nAn important notion of decision sparsity has been established in the work of Sun et al. [2024], which defined the Sparse Explanation Value (SEV), in the context of binary classification, as the number of factors that need to be changed to a reference feature value in order to change the decision. In contrast to SEV, counterfactual explanations tend not to be sparse since they require small changes to many variables in order to reach the decision boundary [Sun et al., 2024]. Instead, SEV provides sparse explanations: consider a loan application that is denied because the applicant has many delinquent payments. In that case, the decision sparsity (that is, the SEV) would be 1 because only a single factor was required to change the decision, overwhelming all possible mitigating factors. The framework of SEV thus allows us to see sparsity of models in a new light.\nPrior to this work, SEV had one basic definition: it is the minimal number of features we need to set to their reference values to flip the sign of the prediction. The reference values are typically defined as the mean of the instances in the opposite class. This calculation is easy to understand, but somewhat limiting because the reference could be far in feature space from the point being explained and the explanation could land in a low density area where explanations are not credible. As an example, for the loan decision for a 21 year old applicant, SEV could create a counterfactual such as \u201cChanging the applicant's 3-year credit history to 15 years would change the decision.\u201d While this counterfactual is valid, faithful, and sparse, it is not close because the distance between the query point and the counterfactual is so large (3 years to 15 years). In addition, this explanation is not credible because the proposed changes to the features lead to an unrealistic circumstance \u2013 6-year-olds do not typically have credit. That is, the counterfactual does not represent a typical member of the opposite class."}, {"title": "Related Work", "content": "The concept of SEV revolves around finding models that are simple, in that the explanations for their predictions are sparse, while recognizing that different predictions can be simple in different ways (i.e., involving different features). In this way, it relates to (i) instance-wise explanations (iii) local sparsity optimization Models, which seek to explain and provide predictions of complex models. We further comment on these below.\nInstance-wise Explanations. Prior work has developed methods to explain predictions of black boxes [e.g., Guidotti et al., 2018, Ribeiro et al., 2016, 2018, Lundberg and Lee, 2017, Baehrens et al., 2010] for individual instances. These explanations are designed to estimate importance of features, are not necessarily faithful to the model, and are not associated with sparsity in decisions, so they are fairly distant from the purpose of the present work. Our work is on tabular data; there is a multitude of unrelated work on explanations for images [e.g., Apicella et al., 2019, 2020] and text [e.g., Lei et al., 2016, Li et al., 2016, Treviso and Martins, 2020, Bastings et al., 2019, Yu et al., 2019, 2021]. More closely related are counterfactual explanations, also called inverse classification [e.g., Mothilal et al., 2020, Wachter et al., 2017, Lash et al., 2017, Sharma et al., 2024, Virgolin and Fracaros, 2023, Guidotti et al., 2019, Poyiadzi et al., 2020, Russell, 2019, Boreiko et al., 2022, Laugel et al., 2017, Pawelczyk et al., 2020]. Counterfactual explanations are typically designed to find the closest instance to a query point with the opposite prediction, without considering sparsity of"}, {"title": "Preliminaries and Motivation", "content": "The Sparse Explanation Value (SEV) is defined to measure the sparsity of individual predictions of binary classifiers. The point we are creating an explanation for is called the query. The SEV is the smallest set of feature changes from the query to a reference that can flip the prediction of the model. When we make a change to the query's feature, we align it to be equal to that of the reference point. The reference point is a \u201ccommons,\u201d i.e., a prototypical point of the opposite class as the query. In this section, we will focus on the basic definition of SEV, the selection criteria for the references, as well as three reference selection methods."}, {"title": "Recap of Sparse Explanation Values", "content": "We define SEV following Sun et al. [2024]. For a specific binary classification dataset {xi, Yi}=1, with each xi \u2208 RP, and the outcome of interest is yi \u2208 {0,1}. (This can be extended to multi-class classification by providing counter-factuals for every other class than the query's class.) We predict the outcome using a classifier f: X \u2192 {0,1}.\nWithout loss of generality, in this paper, we are only interested in queries predicted as positive (class 1). We focus on providing a sparse explanation from the query to a reference that serves as a population commons, denoted r. Human studies [Delaney et al., 2023] have shown that contrasting an instance with prototypical instances from another class provides more intuitive explanations than comparing it with instances from the same class. Thus, we define our references in the opposite class (negative class in this paper). To calculate SEV, we will align (i.e., equate) features from query xi and reference one at a time, checking at each time whether the prediction flipped. Thinking of these alignment steps as binary moves, it is convenient to represent the 2\u00ba possible different alignment combinations as vertices on the boolean hypercube. The hypercube is defined below:\nDefinition 3.1 (SEV hypercube). A SEV hypercube Lf,i,r for a model f, an instance xi with label f(x) = 1, and a reference r, is a graph with 2\u00ba vertices. Here p is the number of features in xi and bv \u2208 {0, 1}p is a Boolean vector that represents each vertex. Vertices u and v are adjacent when their Boolean vectors differ in one bit, ||bu \u2013 bv ||0 = 1. 0's in b indicate the corresponding features are aligned, i.e., set to the feature values of the reference r, while 1's indicate the true feature value of instance i. Thus, the actual feature values represented by the vertex v is x\"v := bxi+(1-bv)r, where is the Hadamard product. The score of vertex v is f(x\"v), also denoted as Lf,i,r(bv).\nThe SEV hypercube definition can also be extended from a hypercube to a Boolean lattice as they have the same geometric structure. There are two variants of the Sparse Explanation Value: one gradually aligns the query to the reference (SEV\u00af), and the other gradually aligns the reference to the query (SEV+). In this paper, we focus on SEV\u00af:"}, {"title": "Motivation of Our Work: Sensitivity to Reference Points", "content": "Since SEV is determined by the path on a SEV hypercube and each hypercube is determined by the reference point, the SEV is therefore sensitive to the selection of reference points. Adjusting the reference point trades off between sparsity (according to SEV\u00af) and closeness (measured by l2, lo(see Section 6.1) or lo (see Section 6.4) distance between the query and its assigned reference point). Note that this trade-off exists because SEV- tends to be small when the reference is far from the query. More detailed explanations, visualizations, and experiments are shown in Appendix B.\nSelecting References. The reference must represent the commons, meaning the negative population, and the generated explanations should represents the negative populations as well. Moreover, the negative population may have subpopulations; e.g., Diabetes patients may have higher blood glucose levels, while hypertension patients have higher blood pressure. To have meaningful coverage of the negative population, in this work, we consider multiple references, placed within the various subpopulations. This allows each point in the positive population to be closer to a reference. Let R denote possible placements of references. For query \u00e6, an individual-specific reference ri \u2208 R for xi is chosen based on three criteria: it should be nearby (i.e., close), and should provide a sparse and reasonable explanation. That is, we are looking to minimize the following three objectives over placement of the reference ri:\n\n$\\|| X_i - r_i\\|, r_i \\in R  (Closeness)$\n\n$\\SEV^-(f, x_i, r_i), r_i \\in R  (Sparsity)$\n\n$\\-P(x^{expl}_{r_i} | X^-)  (Negated Credibility),$\nwith the constraint that the references obey auditability, meaning that domain experts are able to check the references manually, or construct them manually. The function $\\SEV^-(f, x_i, r_i)$ in (2) represents the SEV computed with the given function f, query \u00e6i, and the individual-specific reference ri for generating the hypercube.$x^{expl}_{r_i}$ is the sparse explanation for the sample xi, and P(\u00b7|X\u2212) in the definition of credibility represents the probability density distribution of the negative population and $P(x^{expl}_{r_i} | X^-)$ is the density of the negative distribution at $x^{expl}_{r_i}$. If $P(x^{expl}_{r_i} | X^-)$ is large, $x^{expl}_{r_i}$ is in a high-density region."}, {"title": "Meaningful and Credible SEV", "content": "We now describe cluster-based SEV, which improves closeness at the expense of SEV, and its variant, tree-based SEV, which improves all three objectives and computational efficiency. We also present methods to improve the credibility and sparsity of the explanations."}, {"title": "Cluster-based SEV: Improving Closeness", "content": "This approach creates multiple references for the negative population. A clustering algorithm is used to group negative samples, and the resulting cluster centroids are assigned as references. A query is assigned to its closest cluster center:\n\n$\\\u0159i \u2208 arg min ||xi - r||_2$\nwhere C is the collection of centroids obtained by clustering the negative samples. We refer to the SEV produced by the grouped samples as cluster-based SEV, denoted SEV\u00a9. Figure 2 illustrates the calculation of SEV\u00a9 for two examples located in two different centroids. A red dot represents a query, while a blue dot represents a reference. For each instance, it selects the closest centroid and considers the SEV hypercube, where each cyan point represents a negatively predicted vertex and each pink point represents a positively predicted vertex. We deduce by following the red lines that the SEV\u00a9 for the two queries are 2 and 1, respectively. The cluster centroids should serve as a cover for the negative class. To ensure that the cluster centroids have negative predictions, we use the soft clustering method of Bezdek et al. [1984] to constrain the predictions of the cluster centers. Details are in Appendix C."}, {"title": "Tree-based SEV: SEV Variant with Useful Properties and Computational Benefits", "content": "Tree-based SEV is a special case of cluster-based SEV, where we consider each negative leaf as a reference candidate, and find the sparsest explanation (path along the tree) to the nearest reference. Here, SEV and lo distance (i.e., edit distance) are equivalent. That is, we find the minimum number of features to change in order to achieve a negative prediction.\nWe denote SEVT as the SEV- calculated based on this process. Here, we assume that trees have no trivial splits where all child leaves make the same prediction. If so, we would collapse those leaves before calculating the SEVT. The first theorem below refers to decision paths that have negatively predicted child leaves. This is where taking one different choice at an internal split leads to a negative leaf.\nTheorem 4.1. With a single decision classifier DT and a positively-predicted query xi, define Ni as the leaf that captures it. If Ni has a sibling leaf, or any internal node in its decision path has a negatively-predicted child leaf, then SEVT is equal to 1.\nThe second theorem states that SEV and minimum edit distance from the query to negative leaves are equivalent.\nTheorem 4.2. With a single decision tree classifier DT and a positively-predicted query xi, with the set of all negatively predicted leaves as reference points, both SEV and the lo distance (edit distance) between the query and the SEV explanation are minimized.\nThe proofs of those two theorems are shown in Appendix L and M. The structure of tree models yields an extremely efficient way to calculate SEV\u00af. We perform an important preprocessing step before any SEV- calculations are done, which will make SEV- easier to calculate for all queries at runtime. At each internal node, we record all paths to negative leaves anywhere below it in the tree. This is described in Algorithm 2 in Appendix E. E.g., if the tree has binary splits, a path from an internal node to a leaf node might require us to go left, then right, then left. In that case, we store LRL on this internal node to record this path. Then, when a query arrives at runtime (in a positive leaf, since it has a positive prediction), we traverse directly up its decision path all the way to the root node. For all internal nodes in the decision path, we observe distances to each negative leaf, which were stored during"}, {"title": "Improving Credibility for All SEV Calculations", "content": "As we mentioned in Section 3.2, the credibility objective encourages explanations to be located in high-density region of the negative population. Previous SEV- definitions focus on sparsity and closeness objectives, but did not consider credibility. It is possible to increase credibility easily while constructing an explanation: if the explanation veers out of the high-density region, we continue walking along the SEV hypercube during SEV calculations. Specifically, we continue moving towards the reference until the vertex is in a high-density region. Since the reference is in a high-density region, walking towards it will eventually lead to a high-density point. The tree-based SEV explanations automatically satisfy high credibility:\nTheorem 4.3. With a single sparse decision tree classifier DT with support at least S in each negative leaf, the SEVT explanation for query \u00e6\u00bf always satisfies credibility at least $\\frac{S}{N^-}$, where $N^-$ is the total number of negative samples.\nThis theorem can be easily proved because SEV\u00af explanations generated by SEVT are always the negative leaf nodes (which are the references), and the references are located in regions with support at least S by assumption."}, {"title": "Flexible Reference SEV: Improving Sparsity", "content": "From Section 3.2, we know that queries further from the decision boundary tend to have lower SEV. Based on this, we introduce Flexible Reference SEV (denoted SEVF), which moves the"}, {"title": "Optimizing Models for SEV-", "content": "Above, we showed how to calculate SEV for a fixed model. In this section, we describe how to train classifiers that optimize the average SEV without loss in predictive performance. We propose two methods: minimizing an easy-to-optimize surrogate objective (Section 5.1) and searching for models with the smallest SEV from a \"Rashomon set\" of equally-good models (Section 5.2). In what follows, we assume that SEV\u00af was calculated prior to optimization, that reference points were assigned to each query, and that these assignments do not change throughout the calculation."}, {"title": "Gradient-based SEV Optimization", "content": "Since we want to minimize expected test SEV\u00af, the most obvious approach would be to choose our model f to minimize average training SEV\u00af. However, since SEV calculations are not differentiable and they are combinatorial in the number of features and data points, this would be intractable. Following Sun et al. [2024], we instead design the optimization objective to penalize each sample where SEV is more than 1. Thus, we propose the loss term:\n\n$l_{SEV_All_Opt-}(f) := \\frac{1}{n^+} \\sum_{i=1}^{n^+} max( min_{j=1,...,p} f((1 \u2013 e_j) \u2299 x_i + e_j\u0159\u00ed), 0.5)$\nwhere ej is the vector with a 1 in the jth coordinate and O's elsewhere, n+ is the number of queries, and the reference point is specific to query xi and chosen beforehand. Intuitively, f((1 \u2013 ej) xi + ej\u0159\u00ed) is the function value of query xi where its feature j has been replaced with the reference's feature j. minj=1,...,p f((1 \u2013 ej) \u2299 xi + ej\u0159\u00ed) chooses the variable to replace that most reduces the function value. If the SEV\u00af is 1, then when this replacement is made, the point now is on the negative side of the decision boundary and f is less than 0.5, in which case the max chooses 0.5. If SEV is more than 1, then after replacement, f will still predict positive and be more than 0.5, in which case, its value will contribute to the loss. This loss is differentiable with respect to model parameters except at the \u201ccorners\u201d and not difficult to optimize.\nTo put these into an algorithm, we optimize a linear combination of different loss terms,\n\n$\\min_{f\\in F} (IBCE(f) + C1l_{SEV_All_Opt-}(f)$\nwhere IBCE is the Binary Cross Entropy Loss to control the accuracy of the training model and F is a class of classification models that estimate the probability of belonging to the positive class. lSEV_All_Opt- is the loss term that we have just introduced above. C\u2081 can be chosen using cross-validation. We define All-Opt as the method that optimizes (4). Our experiments show that this method is not only effective in shrinking the average SEV- but often attains the minimum possible SEV value of 1 for most or all queries."}, {"title": "Search-based SEV Optimization", "content": "As defined in Section 4.2, our goal is to find a model with the lowest average SEV- among classifica-tion models with the best performance.\nThe Rashomon set [Semenova et al., 2022, Fisher et al., 2019] is defined as the set of all models from a given class with performance approximately that of the best-performing model. The first method"}, {"title": "Experiments", "content": "Training Datasets To evaluate whether our proposed methods would achieve sparser, more credible and closer explanations, we present experiments on seven datasets: (i) UCI Adult Income dataset for predicting income levels [Dua and Graff, 2017], (ii) FICO Home Equity Line of Credit Dataset for assessing credit risk, used for the Explainable Machine Learning Challenge [FICO, 2018], (iii) UCI German Credit dataset for determining creditworthiness [Dua and Graff, 2017], (iv) MIMIC-III dataset for predicting patient outcomes in intensive care units [Johnson et al., 2016a,b], (v) COMPAS dataset [Jeff Larson and Angwin, 2016, Wang et al., 2022a] for predicting recidivism, (vi) Diabetes dataset [Strack et al., 2014] for predicting whether patients will be re-admitted within two years, and (vii) Headline dataset for predicting whether the headline is likely to be shared by readers [Chen et al., 2023]. Additional details on data and preprocessing are in Appendix A.\nTraining Models For SEV, we trained four baseline binary classifiers: (i, ii) logistic regression classifiers with l\u2081 (L1LR) and l2 (L2LR) penalties, (iii) a gradient boosting decision tree classifier (GBDT), and (iv) a 2-layer multi-layer perceptron (MLP), and tested its performance with SEVF added, and the credibility rules added. In addition, we trained All-Opt\u00af variants of these models in which the SEV penalties described in the previous sections are implemented. For SEVT methods, we compared tree-based models from CART, C4.5, and GOSDT [Lin et al., 2020, McTavish et al., 2022] with the TOpt method proposed in Section 5.2. Details on training the methods is in Appendix F.\nEvaluation Metrics To evaluate whether good references are selected for the queries, we evaluate sparsity and closeness (i.e., similarity of query to reference). For sparsity, we use the average number of feature changes (which is the same as lo norm) between the query and the explanation; for closeness, we use the median lo norm between the generated explanation and the original query as the metric for SEV. For tree-based models, we use only SEVT as the metric since SEVT and lo norm are equivalent; for credibility, we need some way of estimating P(\u00b7|X) since we cannot observe it directly, so we trained a Gaussian mixture model on the negative samples of each dataset, and used the mean log-likelihood of the generated explanations as the metric for SEV\u00a9 and SEVF, for TOpt, since it has already been a sparse decision tree, then we don't need to calculate the credibility."}, {"title": "Cluster-based SEV shows improvement in credibility and closeness", "content": "Let us show that SEV\u00ae provides improved explanations. Here, we calculated the metric for different SEV variants, SEV\u00ae and SEV\u00a9+F(SEV\u00a9 with flexible reference), and compared to the original SEV\u00b9, where SEV\u00b9 is defined as the SEV\u00af calculation with single reference generated by the mean value of each numerical feature and mode value of each categorical feature of the negative population, as done in the original SEV paper [Sun et al., 2024] under various datasets and models.\nFigure 5a shows the relationship between spasity and variants, the scatter plot between mean SEV- and mean l\u221e for each explanation generated by different variants. We find that SEV\u00ae improves closeness, which was expected since the references were designed to be closer to the queries. Interestingly, SEV\u00ae sometimes has lower decision sparsity than SEV\u00b9. SEV\u00ae was designed to trade off SEV for closeness, so it is surprising that it sometimes performs strictly better on both metrics, particularly for the COMPAS, Diabetes, and German Credit datasets."}, {"title": "Flexible Reference SEV can improve sparsity without losing credibility", "content": "In Section 4.4, we proposed the flexible reference method for sparsifying SEV- explanations, which moves the reference slightly away from the decision boundary. The blue points in Figure 5a and 5b have already shown that with small modification of the reference, the credibility of the explanations is not affected. Figure 6a shows how SEV and credibility change as we increase flexibility; SEV sometimes substantially decreases while credibility is maintained."}, {"title": "SEV provides the sparsest explanation compared to other counterfactual explanations", "content": "Recall that SEV- flips features of the query to values of the population commons. This can be viewed as a type of counterfactual explanation, though typically, counterfactual explanations aim to find the minimal distance from one class to another. In this experiment, we compare the sparsity of SEV calculations to that of baseline methods from the literature on counterfactual explanations, namely Wachter [Wachter et al., 2017], REVISE [Joshi et al., 2019], Growing Sphere [Laugel et al., 2017], and DiCE [Mothilal et al., 2020].\nFigure 6b shows sparsity and credibility performance of all counterfactual explanation methods on different datasets under l2 logistic regression (other information, including l\u221e norms for counterfactual explanation methods, is in Appendix G). All SEV variants are in warm colors, while competitors are in cool colors. SEV\u00af methods have the sparest explanations, followed by DiCE. (A comparison of SEV to DiCE is provided by Sun et al. [2024].) We point out that this comparison was made on"}, {"title": "All-Opt and TOpt optimize SEV , preserving model performance, explanation closeness and credibility", "content": "Even without optimization, our SEV- variants improve decision sparsity and/or closeness. If we are willing to retrain the prediction model as discussed in Section 5, we can improve these metrics further, creating accurate models with higher decision sparsity. Figure 7a shows that gradient-based SEV optimization can reduce the SEV without harming the closeness metric (l\u221e) and the credibility metrics (log-likelihood). The slashed bars represents the SEV\u00af, l\u221e and log likelihood metrics before optimization using different models, while the colored bars are the results after optimizing with All-Opt. We have also compared our results with ExpO [Plumb et al., 2020], which is a optimization method that maximizes the mean neighborhood fidelity of the queries, but we have found that explanations are not sparse, and it requires long training times; the detailed results are shown in Appendix K."}, {"title": "Conclusion", "content": "Decision sparsity can be more useful than global model sparsity for individuals, as individuals care less about, and often do not even have access to, the global model. We presented approaches to achieving high decision sparsity, closeness and credibility, while being faithful to the model. One limitation of our method is that causal relationships may exist among features, invalidating certain transitions across the SEV hypercube. This can be addressed by searching across vertices that do not satisfy the causal relationship, though it requires knowledge of the causal graph. Another limitation is that to make the explanation more credible, the threshold to stop searching the SEV hypercube is not easy to determine. Future studies could focus on on these topics. Overall, our work has the potential to enhance a wide range of applications, including but not limited to loan approvals and employment hiring processes. Improved SEV translates directly into explanations that simply make more sense to those subjected to the decisions of models."}, {"title": "Sensitivity of the reference points", "content": "In this section, we will mainly show how sensitive SEV is when we change the reference. Figure 8 shows an example of this, where moving the reference further away from the query (from r to the r') changes the SEV- from 2 to 1. In this figure, the dark blue axes represent the feature values of different reference values, while the black dashed line represents the decision boundary of a linear classifier. Areas with different colors represent data points with different SEV\u00af. When the reference moves further from the decision boundary (from r to r'), the corresponding areas for SEV\u00ae will move away from the decision boundary. For example, the star located in the yellow area has an SEV- of 1 instead of 2 when the reference moves from r to r'. If the reference point is r, then the query needs to align the feature values along both x and y-axis to reach the SEV Explanation with reference r (recall an example of SEV explanation in Figure 2) in Section 3.2, which is the same point as r. However, if the reference point is r', then the query only needs to align the feature value along the x-axis to reach the SEV Explanation with SEV= 1, which is the light blue dot.\nExperiments have also shown that moving data points closer to the decision boundary might increase SEV. The result on the Explainable ML Challenge loan decision data [FICO, 2018] shown in Table 5 demonstrates that altering the reference point may increase the average SEV (from 3 to 5), but also introduces \u201cunexplainable\" samples (meaning SEV-\u226510). Hence, SEV\u00af is sensitive to the reference."}, {"title": "Detailed Description for Score-based Soft K-Means", "content": "As we have discussed in Section 4.1, SEV- needs to have negatively predicted reference points. Therefore, when clustering the negative population, it is necessary to avoid positively predicted cluster centers. However, for most of the existing clustering methods, it is hard to \"penalize\" the positive predicted clusters, or their assigned samples. Therefore, we have modified the soft K-Means [Bezdek et al., 1984] algorithm so as to encourage negative clustering results.\nThe original Soft K-Means (SKM) algorithm generalizes K-means clustering by assigning membership scores for multiple clusters to each point. Given a data set X {X1,X2,Xn} and C clusters, the goal is to minimize the objective function J(U, V), where U = [uij] is the membership matrix and V = {v1,\u2026, vc} are the weighted cluster centroids. The objective is to minimize:\n\n$J (U, V) = \\sum_{i=1}^{n}\\sum_{j=1}^{C}u_{ij}||x_i \u2013 Vj ||^2$\nwhere uij is the (soft) membership score of x in cluster j:\n\n$U_{i,j} = \\frac{(\\frac{1}{|| X_i-V_j ||^2})^{m-1}}{\\sum_{k=1}^{C}(\\frac{1}{||Xi-Ck ||^2})^{m-1}}$\nand m > 1 is a parameter that controls the strength towards each neighboring point. When m \u2248 1, the SKM is similar to the performance of hard K-means clustering methods. When m > 1 for point i, it is considered to be associated with multiple clusters instead of one distinct cluster. The higher the value of m, the more a point is considered to be part of multiple clusters, thereby reducing the distinctness of each cluster and creating a more integrated and interconnected clustering arrangement. To avoid the cluster group being predicted positively, we have given higher m for those positive samples. Therefore, if the samples are predicted as positive, it reduces the possibility that those positively predicted samples to group as a cluster, which we can replace m as m' for each instance Xi as\n\n$m' = 2m min{f(x_i) \u2013 0.5,0} + 1.$\nThe value of min{f(xi) - 0.5,0} increases as \u00e6\u00bf is classified as positive and further away from the decision boundary. As m' increases, the negatively predicted samples are more associated with one distinct cluster, while the positively predicted samples are associated with multiple clusters with smaller weight. This makes the cluster centers less likely to be influenced by positively predicted points. Thus, we can rewrite the objective of the soft K-Means algorithm can be modified as\n\n$J' (U, V) = \\sum_{i=1}^{n}\\sum_{j=1}^{C}m_i u_{ij}||xi \u2013 vj||^2.$\nWe call this new objective function for encouraging negative clustering centers Score-based Soft K-Means (SSKM). In our experiments, the clustering is applied to the dataset after PaCMAP [Wang et al., 2021], and the feature mean of all samples in a cluster is considered as the cluster center of this cluster, which is eventually used as a reference point. The queries are assigned to reference points that are closest (based on 12 distance) to them in the PaCMAP embedding space for SEV\u00a9 calculation. The reason why we would like to first embed the dataset is that the dimension of the datasets might be too high for direct clustering, and PaCMAP provides an embedding that preserves both local and global structure. Figure 9 shows the probability of the negative predicted instances, as well as the clustering results using different kinds of clustering methods. The red points and stars represent the positively predicted instances and cluster centers, while the blue ones are the negatively predicted instances and cluster centers. It is evident from the Figure that that SKM is more likely to introduce positively predicted cluster centers, compared to SSKM.\nWhen we calculate SEV\u00ae in the experiments, all clustering parameters are tuned and fixed. For the rest of the datasets, the embedding using PaCMAP, and their clustering results for the negative population with their cluster centers, are shown in Figure 10. The regions with different colors represent different clusters, the blue stars in the graphs are cluster centers, and the gray points within the graphs are positive queries. All those cluster centers can be constrained to be predicted as negative by tuning the hyperparameter for Score-based Soft K-Means. Note that if one of the cluster centers cannot be constrained to be predicted as negative even with high m, then it is reasonable to remove this cluster center when calculating SEV\u00ae."}, {"title": "Detailed Algorithm for Flexible-based SEV", "content": "This section presents how the flexible-based SEV (SEVF) has done to determine the flexible references. The key idea of finding the reference is to do a grid search through each of the features in the training dataset based on the original reference, and find the feature values that has the minimum model outcome."}, {"title": "Detailed Algorithms for Tree-based SEV", "content": "This section presents how the tree-based SEV is calculated through two main procedure: Algorithm 2 (Preprocessing) for collecting all negative pathways and assigning them to each internal nodes and Algorithm 3 (Efficient SEVT Calculation) for checking all negative pathways conditions for each query and calculating the number of feature changes."}, {"title": "Model training and parameter selection", "content": "Baseline models were fit using sklearn [Pedregosa et al., 2011", "2011": "and TreeFARMS packages [Wang et al., 2022b"}]}