{"title": "AI-Powered Algorithm-Centric Quantum Processor Topology Design", "authors": ["Tian Li", "Xiao-Yue Xu", "Chen Ding", "Tian-Ci Tian", "Wei-You Liao", "Shuo Zhang", "He-Liang Huang"], "abstract": "Quantum computing promises to revolutionize various fields, yet the execution of quantum programs necessitates an effective compilation process. This involves strategically mapping quantum circuits onto the physical qubits of a quantum processor. The qubits' arrangement, or topology, is pivotal to the circuit's performance, a factor that often defies traditional heuristic or manual optimization methods due to its complexity. In this study, we introduce a novel approach leveraging reinforcement learning to dynamically tailor qubit topologies to the unique specifications of individual quantum circuits, guiding algorithm-driven quantum processor topology design for reducing the depth of mapped circuit, which is particularly critical for the output accuracy on noisy quantum processors. Our method marks a significant departure from previous methods that have been constrained to mapping circuits onto a fixed processor topology. Experiments demonstrate that we have achieved notable enhancements in circuit performance, with a minimum of 20% reduction in circuit depth in 60% of the cases examined, and a maximum enhancement of up to 46%. Furthermore, the pronounced benefits of our approach in reducing circuit depth become increasingly evident as the scale of the quantum circuits increases, exhibiting the scalability of our method in terms of problem size. This work advances the co-design of quantum processor architecture and algorithm mapping, offering a promising avenue for future research and development in the field.", "sections": [{"title": "1 Introduction", "content": "The advent of quantum computing marks a significant leap forward in computational capabilities (Wu et al. 2021; Nielsen and Chuang 2010; Shor 1997; Deutsch and Jozsa 1997; Shor 1994; Zhu et al. 2022), offering the potential to outperform classical computing in solving specific intricate problems. The field has witnessed remarkable progress in recent decades (Arute et al. 2019), having now entered the Noisy Intermediate-Scale Quantum (NISQ) era(Huang et al. 2023, 2020; Wu et al. 2021). Despite this progress, the realization of quantum advantage in practical applications, such as quantum machine learning (Biamonte et al. 2017;"}, {"title": "2 RL for Processor Topology Design with Restricted Connections", "content": "In the context of this study, we represent physical qubits and their connections using an undirected graph, with qubits as vertices (v) and their connections as edges (e). As elaborated in Appendix A.4, our empirical evidence indicates that variances between two graph can lead to a significant divergence in circuit depth. It is, therefore, crucial to identify an optimal graph for circuits. Considering a set of n vertices, the total number of possible edges is determined by the number of ways to select 2 vertices from n, hence, there are $2^{\\binom{n}{2}}$ potential graphs, with varying structures that markedly affects the circuit. Due to the exponential growth in potential graphs with the number of vertices, it is hard for heuristic search algorithms to traverse the extensive solution space.\nReinforcement Learning (Silver et al. 2017; Fran\u00e7ois-Lavet et al. 2018) is particularly well-suited to this domain because its adaptive, incremental learning process, a tradeoff between exploration and exploitation, enabling effective exploration in vast solution space. Consequently, our approach leverages RL to develop an agent that can dynamically guide topology design, moving beyond mere static memorization of solutions. Nevertheless, integrating RL into this domain presents two primary challenges: (a) It requires the formulation of the topology design problem within the confines of a reinforcement learning framework. This involves developing novel action space and a reward function that are compatible with reinforcement learning algorithms. (b) There is a critical need to improve the training efficiency of models designed for large-scale quantum circuits. This challenge arises from the inherent computational demands and the complexity of quantum circuits, which can significantly hinder the speed and effectiveness of model training. These challenges are addressed in Sections 2.1 and 2.2, respectively."}, {"title": "2.1 System Overview", "content": "Consider a Graph with n vertices labeled $V_1, V_2, ..., V_n$, which correspond to n qubits in a quantum circuit. Let E represent the complete set of potential edges, where each edge e, signifying a potential link between any two vertices (or qubits), belongs to this set, i.e., $e \\subseteq E$. A Graph is thus defined as a subset of E with each element of this subset representing an edge in the Graph. The depth of a circuit (c) mapped onto a Graph (G) can be computed using the function $f_{depth}(G, c)$. In our experimental setup, we utilized a famous quantum development kit Qiskit (Contributors and IBM 2024) to implement the computation of $f_{depth}(G, c)$. The objective is to identify a G to minimizes the circuit depth while adhering to the maximum connectivity degree constraint for its vertices, formally stated as:\n$\\min f(G, c), G \\subset E$ \ns.t. degree(v) \u2264 4\n(1)\nIn the context of the RL framework, we conceptualize G as the environment, wherein each action executed by the agent can modify G, propelling G towards the desired state. The G is represented by an adjacency matrix, denoted by M, in which the presence of an edge between vertices $v_i$ and $v_j$ is indicated by $M_{ij} = 1$. It is important to note that Graph G is undirected; consequently, an edge constitutes an unordered vertex pair, rendering e ($v_i, v_j$) and e ($v_j, v_i$) as equivalent representations of the same edge. In light of this symmetry, we use only the lower triangular portion of M where i < j, effectively reducing the state space by over half. As illustrated in Figure 2, matrix M is subsequently flattened into a one-dimensional array to serve as the state (input) for the agent. The agent employed in our study is borrowed from the Proximal Policy Optimization (PPO) as detailed in (Schulman et al. 2017).\nThe extensive search space inherent in graphs makes it challenging for an agent to directly identify an optimal graph, especially during the initial stages when both the agent's policy and output are nearly random. This randomness results in sparse positive rewards, complicating the model's convergence. To address this, we simplify the huge action space into incremental steps. At each step, the agent only decide whether to build an edge between two specified vertices, consequently, the action can be denoted by ($v_i, v_j$). Given n vertices the action space is effectively reduced to $\\frac{n(n-1)}{2}$, a significant contraction from the entire Graph's search space. Reward is quantified as a scalar value indicates the improvement of circuit depth (the lower the better) by action. An efficient reward function is detailed in Section 2.1. Furthermore, to enhance the efficiency of training, a Reward-Replay method is introduced in Section 2.2.\nReward Function Our objective is for the reward mechanism to guide the agent towards optimizing the topology towards an ideal state. We start by denoting the initial depth of the circuit as $D_0$, During the i th iteration, the change in depth relative to the previous iteration is represented by $\\Delta (D_{i-1}, D_i) = D_i \u2013 D_{i-1}$, Our objective is to ensure that this change, $\\Delta (D_{i-1}, D_i)$, is negative, indicating a reduction in depth. Moreover, we strive for the depth at any iteration D\u2081 to be lower than the initial depth $D_0$, highlighting a consistent trend towards optimization. Consequently, the reward is computed based on both (Do, Di) and (Di-1, Di). At any given time t, we assess depth change A from step t - 1 and the initial step to step t respectively:\n$\\Delta D_{t\\rightarrow 0} = \\frac{D_t - D_0}{D_0}$\n(2)"}, {"title": "2.2 Reward-Replay Proximal Policy Optimization", "content": "The reward function and action space operates independently of RL algorithms, enabling a variety of RL to tackle the mapping problem. Among these, Proximal Policy Optimization (PPO) (Schulman et al. 2017) is preferred due to its benefits in promoting stable training processes and efficient exploration strategies. Nonetheless, there still a significant obstacle that existing RL algorithms struggle to overcome: the evaluation of the action (i.e. assigning rewards to actions) at each training step relies on computing the circuit's depth using $f_{depth}$, a process that can take several seconds to minutes. This evaluation process significantly reduces the training efficiency, consuming approximately 68% of the total training time. This highlights the crucial need to reduce this duration to improve training efficiency.\nTypically, the evaluation associated with a particular action depends on the current state (s), indicating that identical actions may yield divergent outcomes across different states. This concept is encapsulated within the reward function r(s, a). A significant challenge emerges due to the extensive array of potential states, making the storage of every possible state-action pair (s, a) impractical. However, our experiments have yielded a notable observation: across numerous distinct pairs ($s_p$, a) and ($s_q$, a) where $s_p \\neq s_q$, the results demonstrate a notable similarity in outcomes. In light of this observation, we simplify our model by adopting the assumption that (a|$s_1$) \u2248 (a|$s_2$)... \u2248 (a|$s_m$), thereby enabling us to approximate the reward function for a state-action pair r(s, a) as simply r(a). This simplification is rationalized by the premise that if an edge significantly contributes to minimizing the circuit's depth by enabling the requisite connections, then, regardless of the current state (i.e., the graph), this edge consistently ensures the necessary connectivity, thus uniformly facilitating a reduction in the circuit's depth. Building on this approximation, we introduce the Reward-Replay method, which involves recycling the reward associated with an a (represented as the pair (r, a)) rather than (r, a, s), where the latter signifies a nearly infinite array of possibilities. This strategy allows for the immediate retrieval of rewards from memory upon any subsequent execution of the action, eliminating the need for repetitive reward calculations and consequently diminishing computational demands.\nUtilizing r(a) to approximate the r(a, s) may introduce inaccuracy. To counteract the potential accumulation of errors during training, a replay threshold is implemented for"}, {"title": "3 Topology Adjustment for Industrial Manufacturing", "content": "Reinforcement learning offers promising topology for circuits, however, translating these topologies into practical, industrially viable implementations encounters significant challenges: (a) Electromagnetic interference between crossing wires can cause a cross-talk(Ding et al. 2020) problem (Sarovar et al. 2020), leading to decoherence and low fidelity in quantum operations, necessitating a topology design that minimizes wire crossovers to alleviate such problems. (b) Over past decades, the quantum processor architecture based on superconducting qubits has become the leading candidate platform (Huang et al. 2020). Given the predominant grid-like structure of these superconducting processor topologies (Nishio et al. 2020; Zulehner, Paler, and Wille 2018), our goal is to maintain this uniform grid pattern. This approach aims to ensure compatibility with existing technologies, thereby facilitating a smoother integration of reinforcement learning-derived topologies into the current framework of quantum computing.\nIn this study, we adopt the Force-directed Layout method (Kobourov 2012) to minimize edge crossings by emulating physical forces, specifically repulsion and attraction. As depicted in figure 3, vertexes are treated as charged particles, and edges are treated as springs, each vertex experiences two types of forces: (a) Repulsive force defined as $F_{repulsive} = \\frac{k_1}{r}$, k\u2081 is a constant, r is the distance between vertexes; (b) Attractive force: defined as $F_{traction} = k_2\\Delta x$, k2 is a constant and \u0394x is the value of the increase of the distance between vertexes compared to the initial distance. The resultant force acting on each vertex is determined by"}, {"title": "4 Experiments", "content": "In this section, we present the experimental design aimed at addressing the following research questions: (RQ1) How does the performance of QTailor compare to the state-of-the-art approach, and what are the reasons for the observed performance differences? (RQ2) How effective is QTailor as the circuit scales up? (RQ3) How does our proposed Reward-Replay impact the efficiency of PPO?"}, {"title": "4.1 Experiment Setup", "content": "Benchmarks and Compared Method. Building upon the foundation established by previous research, we have selected MQT Bench (Quetschlich, Burgholzer, and Wille 2023) as our benchmark toolkit. This toolkit offers a diverse array of platform-independent circuits tailored for various quantum computing tasks. To monitor the training process and evaluate efficiency, we utilized TensorBoard (Abadi et al. 2015) Our experiments were conducted using Qiskit (Contributors and IBM 2024) as the backend framework, a renowned quantum computing development kit provided by IBM. Notably, Qiskit incorporates Sabre (Li, Ding, and Xie 2019) as its mapping algorithm, which is recognized as the state-of-the-art. Sabre utilizes a sophisticated mapping algorithm that operates on a SWAP-based Bidirectional heuristic search approach.\nFor our study, Sabre was chosen to ensure a fair and intuitive comparison. By employing Sabre, we maintain consistency across all phases of our experiment, with the exception of the mapping phase. This approach allows us to utilize the same configurations for post-mapping optimizations and circuit depth calculations, thereby ensuring a standardized and controlled environment for our comparative analysis.\nEvaluation Protocol. For the Sabre algorithm, a topology consisting of a square grid with 100 nodes (10 \u00d7 10) is provided, where nodes having a connectivity degree of 4. For a fair comparison, the maximum connectivity degree for the QTailor's topology is similarly constrained to 4. Given that the calculation of circuit depth involves randomness, we conducted the calculation three times and took the average to mitigate the effects of this randomness. The detailed hyperparameters are given in Appendix A.5."}, {"title": "4.2 Performance Evaluation (RQ1)", "content": "In this section, we compare the depth, fidelity, and total gates of circuits driven by QTailor and Qiskit. The circuits therein is from public dataset. Our analysis indicates that QTailor yields better outcomes than Qiskit across a majority of tasks, reducing circuit depth ranging from 5% to 46% as presented in Figure 5, over 60% of the samples exhibit a reduction in circuit depth by 20%. The results shows that the topology suggest by QTailor for circuits can also significantly reduce total gates of compiled circuits, which can mitigate the accumulation of quantum errors, leading to higher overall fidelity. Our method can be easily optimized for different metrics by making a minor adjustment to the reward function. Specifically, we can replace the parameter depth (denoted by AD) with the number of gates. Similar to depth, the number of gates should be minimized. Conversely, if the metric in question should be maximized, we need to use its negative value in the reward function.\nThe statistics presented in Table 1 provide insight into the factors contributing to the reduced depth of the line. The idle ratio, which represents the proportion of time a qubit remains inactive in gate operations, is calculated using the following formula:\nIdle = 1- ($ \\frac{gates}{qubits \\times depth} $)\n(5)\nWe observe that the idle ratio from Qtailor is lower than that of Qiskit, with a maximum observed reduction of 31.68%, regardless of whether Qtailor's gate count is higher or lower than Qiskit's. A lower idle ratio indicates enhanced parallel processing capabilities of the quantum processor and contributes to a reduction in circuit length. If the focus shifts towards reducing the number of gates, a minor adjustment can be made to the reward function by replacing the depth parameter (denoted by \u2206D) while other components remain unchanged. The result are shown in Figure 6.\nThe Quantum gates in circuits are applied to qubits to perform computations. Each gate takes a certain amount of"}, {"title": "4.3 Evaluation of Circuits for Scaling Up (RQ2)", "content": "Figure 8 shows how the depth changes with size of the circuits increase. In a comparative analysis focusing on the depth of quantum circuits comprising 6 qubits, Qtailor consistently exhibits a reduced circuit depth when compared to Qiskit. As the gate count increases from 60 (6 \u00d7 10) to 210 (6 \u00d7 35), the depth reduction achieved by Qtailor compared to Qiskit are 31.03%, 31.52%, 33.87%, 39.11%, 39.65% and 40.74%, respectively. This trend indicating a systematic efficiency improvement in circuit depth with Qtailor as circuits size increases. The capability to efficiently handle large-scale circuits is crucial for unlocking the full potential of quantum computing, thereby enabling the solution of problems that are intractable for classical computing."}, {"title": "4.4 Ablation Study (RQ3)", "content": "Figure 9 shows the Kullback-Leibler (KL) divergence loss and total loss during the training. Besides the Reward-Replay module, the experiments are conducted under uniform conditions, including hardware and hyper-parameters. We observe that: (left) Our method shows a more fluctuating curves for both KL and total loss, yet reaches convergence faster by 24%. This efficiency can be attributed to the Reward-Replay module, which minimizes unnecessary evaluations on circuits. (right) The losses curves eventually reach a nearly a near-identical value, because the forgetting mechanism avoids the errors accumulation and thus has a limited effect on training accuracy."}, {"title": "5 Discussion and Conclusion", "content": "The proposed Qtailor, a framework that pioneers the synergistic design of quantum processor topology and circuit mapping, offers several remarkable features: 1) Compared to state-of-the-art methods, Qtailor significantly reduces the depth of mapped circuits, with a minimum of 20% reduction observed in 60% of the examined cases and a maximum enhancement reaching up to 46%. This reduction is pivotal for high-fidelity quantum computing, as circuit depth is a critical factor affecting the fidelity of quantum operations. This work, therefore, highlights the transformative potential of machine learning techniques in advancing quantum computing by transcending the limitations of conventional compilation methods. 2) Our method demonstrates excellent scalability across problem sizes and training efficiency. As illustrated in Section 4.3, the advantages of our approach become more pronounced with larger problem sizes, indicating its effectiveness for complex quantum circuits. Furthermore, as detailed in Section 4.3, the reinforcement learning framework we designed, which encompasses problem modeling and a problem-oriented training strategy called reward-replay proximal policy optimization, has yielded significant improvements in training efficiency. 3) We have also integrated a force-grid layout technique to optimize the topology on the processor, enhancing compatibility with current manufacturing technologies and mitigating issues related to crossing connections. This integration is crucial for the practical realization of quantum processors and addresses a key challenge in the transition from theoretical designs to manufacturable devices.\nIn the current study, we have focused on the connectivity constraints of quantum processors, an essential factor in processor design. However, different quantum physical systems or varying fabrication processes for quantum processors may entail distinct constraints. While our work provides a preliminary framework, it sets the stage for future research that can explore these nuances more deeply."}, {"title": "A.1 Related work", "content": "Circuit mapping involves the judicious allocation of qubits on a quantum processor which is known to be NP-complete (Boixo et al. 2018). One common approach is to reformulate the mapping issue as an mathematic problem and utilize a software solver (Lye, Wille, and Drechsler 2015; Oddi and Rasconi 2018; Venturelli et al. 2017, 2018; Bhattacharjee and Chattopadhyay 2017). Wille et al. (Wille, Burgholzer, and Zulehner 2019) addressed the mapping problem by incorporating it into a Satisfiability Modulo Theory(SMT) framework (Moura and Bj\u00f8rner 2009) and they have successfully utilized the Z3 boolean satisfiability solver (de Moura and Bj\u00f8rner 2008) to to obtain precise results for mapping quantum circuits onto IBM QX architectures. Another proposed method, BIPMapping (Nannicini et al. 2022) by Nannicini et al. models the mapping problem as an Integer Linear Problem (ILP) (Schrijver 1986) and and employs IBM CPLEX (IBM 2022) as a slover, the goal is to minimize a linear objective function subject to a set of linear constraints. BIPMapping consists three possible objective functions including error minimize, depth minimize and crosstalk minimize. Notably, BIPMapping outperforms Sabre in terms of reducing the number of CNOTs and exhibits higher fidelity.\nA significant limitation of aforementioned methods is that a general solver is suffer from long runtimes and are only applicable to small-scale cases. As a solution, researchers have explored the use of search-based methods to find optimal mapping schemas (Zhang et al. 2021, 2023; Burgholzer, Schneider, and Wille 2022; Li et al. 2023; Zhou, Feng, and Li 2022; Zhou, Li, and Feng 2020; Li, Zhou, and Feng 2021; Fu et al. 2017; Zhang et al. 2020). Siraichi et al. (Siraichi et al. 2018) conducted a study on qubit mapping problems with IBM QX2 and QX3 processors, proposing a search algorithm based on dynamic programming to identify optimal solutions. However, this optimal algorithm requires exponential time and space for execution and is constrained to circuits with a maximum of 8 qubits. In 2019, Siraichi et al. (Siraichi et al. 2019) further developed an improved algorithm and evaluated its performance on IBM Q20 Tokyo. They employed a Bounded Mapping Tree (BMT) to explore the complex landscape of qubit mapping by breaking the problem down into subgraph isomorphism and token swapping problems.\nZulehner et al. (Zulehner, Paler, and Wille 2018) employed A* Search algorithm (Zeng and Church 2009) along with a heuristic cost function to optimize the arrangement of two-qubit gates in separate layers, similar to Sabre. The main objective was to minimize the overall distance between coupled qubits in each layer while simultaneously reducing the circuit's depth. This approach taking only a few minutes to execute on small-scale circuits. However, it requires the analysis of all possible combinations of concurrent SWAP gates, which result in an excessively long runtime.\nIBM introduced Sabre (Li, Ding, and Xie 2019) a state-of-the-art mapper with optimized heuristic search, Sabre utilizes the Floyd-Warshall algorithm to calculate the All Pairs"}, {"title": "A.2 More Background", "content": "Qubits: In the field of quantum computing, the qubit, or quantum bit, functions as the counterpart to the classical bit. Diverging from the classical bit's limitation to representing solely '1' or '0', a qubit can simultaneously exist in a coherent superposition of both states. This feature establishes it as a two-state quantum system that encapsulates the distinctive attributes of quantum mechanics (Nielsen and Chuang 2010). An exemplary illustration of this concept is the electron spin, characterized by its two potential states: spin-up and spin-down.\nQuantum Gates: Quantum computing operations fundamentally rely on two primary categories of quantum gates. The first category includes single-qubit gates, which are unitary operations that can be conceptualized as rotations around the axis of the Bloch sphere, representing the state space of a single qubit (Nielsen and Chuang 2010). The second category consists of multi-qubit gates, which are crucial for executing more complex quantum computations. Notably, complex quantum operations can be decomposed into sequences of single-qubit gates-namely, H, S, T gates-and the two-qubit Controlled NOT (CNOT) gate. The CNOT gate, due to its critical role, will be the primary focus of this discussion. It operates on two qubits, designated as the control and the target. The principle of operation for the CNOT gate is simple: if the control qubit is in the state '1', it flips the state of the target qubit; if the control qubit is '0', the target qubit remains unchanged.\nQuantum Circuit: A quantum circuit consists of a collection of qubits and a sequence of operations applied to these qubits. The representation of quantum circuits can be achieved through several methods. One such method employs the quantum assembly language known as Open-QASM, introduced by IBM (Cross et al. 2022). Alternatively, circuit diagrams can be used, where qubits are depicted as horizontal lines and quantum operations are illustrated by various blocks placed along these lines."}, {"title": "A.3 Comparison with Tket Compiler", "content": "Qtailor utilizes Qiskit to assess circuits depth and gate count. However, it is important to note that Qtailor is not limited to a specific compiler; rather, it serves as a framework that is compatible with multiple compilers. The QPU topology suggested by Qtailor is generic. To illustrate this flexibility, we performed a comparative analysis with Tket (Sivarajah et al. 2021). Aside from switching compilation platforms from the Qiskit to Tket, no further modifications were necessary. The topology used in the experiments remained consistent with those in Section 4, specifically a fixed grid-like topology and the suggested topology from the RL module. Consequently, there is no requirement to retrain the RL model when changing compilers.\nQtailor achieves a maximum circuit depth reduction of 39% compared to Tekt, as illustrated in Figure 11. The results indicate that employing Qtailor's suggested topology is more effective for reducing circuit depth, and highlights Qtailor's compatibility with various compilers."}, {"title": "A.4 Graph Isomorphic: an Example", "content": "Figure 12 illustrates the concept of isomorphic and non-isomorphic graphs. Isomorphism refers to the property of a graph where a single graph can exist in multiple forms. In other words, two distinct graphs can possess identical numbers of edges, vertices, and edge connectivity. Such graphs are referred to as isomorphic graphs. For two graphs to be considered isomorphic, they must satisfy the following conditions: (a) Graphs must have the same number of vertices and edges, and their degree sequences must match. (b) If one graph forms a cycle of length k using vertices V1, V2, V3,..., Uk, the other graph must also form the same cycle of length k using vertices V1, V2, V3, ..., Uk. (c) The adjacent matrices of both the graphs are the same.\nIn our experiments, we found that the ones that have an effect on the depth of the circuit are non-isomorphic graphs because the non-isomorphic graphs have different adjacency matrices, which means that the connectivity of the nodes in the graphs is different, and Figure 13 shows the effect of two non-isomorphic graphs with very small differences on the depth of the circuit."}, {"title": "A.5 Implementation Details", "content": "The benchmark circuits utilized in this study are sourced from the Munich Quantum Toolkit Benchmark Library (MQT Bench) (Quetschlich, Burgholzer, and Wille 2023). These circuits are stored in a .txt file in Qasm (Cross et al. 2022) format. A brief description of the circuits can be found on the MQT Bench website (MQT Bench 2023). All experiments were conducted on the Ubuntu 23.04 operating system using an Intel Xeon CPU (3.10GHz). The reinforcement learning model was implemented in Python 3.10 with"}, {"title": "A.6 More Details of Force-Directed Grid Layout", "content": "In Section 3, it was previously mentioned that the vertex moves in the direction of the resultant force, which represents the combined effect of all the component forces acting on an object. Component forces refer to the individual forces that contribute to the resultant force. It is important to note that the vertex moves incrementally, changing its forces in accordance with its position. The new forces are then recalculated, taking into account the small change in position. Subsequently, the vertex moves again based on the updated combined forces. This iterative process is repeated several hundred times until the forces acting on the entire system reach a state of equilibrium."}, {"title": "A.7 Recommended Topologies for Circuits", "content": "In this section, we present the recommended topology for the Amplitude Estimation circuit with 40 qubits using Qtailor. We assign serial numbers to the qubits in the circuit, where the first qubit is denoted as 1. Initially, we illustrate"}, {"title": "Algorithm 1: Reward-Replay Proximal Policy Optimization (PPO) Algorithm", "content": "Require: Initial policy parameters 00, initial value function\nparameters \u03c60\n1: for each iteration do\n2: Collect Trajectories(Algorithm 2) T\u03b80ld using policy \u03c0\u03b80ld\n3: Compute rewards-to-go Rt and advantage estimates\n4: for each epoch do\n5: for minibatch of size M sampled from T\u03b80ld do\n6:  Compute ratio rt (\u03b8) = \u03c0\u03b8(at|st)\n7:  Compute clipped surrogate objective using ratio\n8:  Update policy parameters @ by maximizing L(0)\n via stochastic gradient ascent\n9:  Update value function parameters $ by minimizing\n the value function loss:\n10:  end for\n11: end for\n12: \u03b8old \u2190 \u03b8\n13: end for"}, {"title": "Algorithm 2: Collect Trajectories with Reward-Repaly", "content": "1: for each episode until batch is full do\n2: Reset environment and get initial state so\n3: Initialize trajectory \u03c4 = (0)\n4: while episode not done do\n5: Select action at based on policy \u03c0\u03b8old (at|st)\n6: if The reward r\u03b1\u03c4 corresponding to the at is available in the memory then\n7: set reward rt = r\u03b1\u03c4\n8: if thresholdat <= 0 then\n9: clear the pair (r\u03b1\u03c4, at >> in memory\n10: end if\n11: else\n12: Execute the action at and run a evaluation on circuit\n13: Use the reward function to calculate the reward\nvalue and assign it to rt\n14: Append (at, rt) to memory\n15: Set thresholda, to a positive integer\n16: end if\n17: Update state st\u2190 st+1\n18: Append (St, at, rt, St+1) to \u03c4\n19: end while\n20: Add trajectory T\u03b8old to the batch of trajectories\n21: end for"}]}