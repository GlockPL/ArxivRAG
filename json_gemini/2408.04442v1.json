{"title": "FedAD-Bench: A Unified Benchmark for Federated Unsupervised Anomaly Detection in Tabular Data", "authors": ["Ahmed Anwar", "Brian Moser", "Dayananda Herurkar", "Federico Raue", "Vinit Hegiste", "Tatjana Legler", "Andreas Dengel"], "abstract": "The emergence of federated learning (FL) presents a promising approach to leverage decentralized data while preserving privacy. Furthermore, the combination of FL and anomaly detection is particularly compelling because it allows for detecting rare and critical anomalies (usually also rare in locally gathered data) in sensitive data from multiple sources, such as cybersecurity and healthcare. However, benchmarking the performance of anomaly detection methods in FL environments remains an underexplored area. This paper introduces FedAD-Bench, a unified benchmark for evaluating unsupervised anomaly detection algorithms within the context of FL. We systematically analyze and compare the performance of recent deep learning anomaly detection models under federated settings, which were typically assessed solely in centralized settings. FedAD-Bench encompasses diverse datasets and metrics to provide a holistic evaluation. Through extensive experiments, we identify key challenges such as model aggregation inefficiencies and metric unreliability. We present insights into FL's regularization effects, revealing scenarios in which it outperforms centralized approaches due to its inherent ability to mitigate overfitting. Our work aims to establish a standardized benchmark to guide future research and development in federated anomaly detection, promoting reproducibility and fair comparison across studies.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, FL [15] has been growing in interest rapidly due to its appealing properties, especially in privacy-preserving machine learning. The main concept of FL is the aggregation of local models, which is used to extract a global model that matches the performance of centralized models while ensuring that the training data remains confidential (local). This straightforward yet powerful approach has demonstrated its effectiveness across a range of applications and fields, such as Text Prediction [8], Internet of Things [17], Image Super-Resolution [16], Manufacturing [9] and Healthcare [12].\nOn the other hand, outlier detection remains a critical challenge in data analysis, where it is essential to identify samples that deviate from expected and known distributions [18]. While tree-based methods, such as XGBoost [2] and RecForest [30], achieve state-of-the-art performance on outlier detection datasets [21], [26], unsupervised outlier detection has also proven to be highly effective, and its performance is on par with tree-based methods [1]. Additionally, anomaly detection data, such as network intrusion and medical records, is often distributed among organizations that wish to keep it private. FL would allow for these organizations to collaboratively train high-performing machine learning models while preserving privacy and abiding by data protection regulations, which significantly benefits all participants [11]. Unfortunately, there is a significant lack of evaluation of anomaly detection methods in FL settings. Furthermore, a convention for testing such methods in FL settings is yet to be established.\nIn response, we introduce FedAD-Bench, a unified benchmark designed to evaluate state-of-the-art unsupervised models for anomaly detection in FL settings. Our focus is specifically on deep learning for several compelling reasons:\nFirstly, while tree-based methods are often the most effective for anomaly detection, their integration with FL presents significant challenges. These methods require specialized aggregation techniques, complicating their implementation in FL environments [4]. Secondly, deep learning methods have predominantly been applied to image and natural language processing tasks, leaving a notable gap in applying deep anomaly detection techniques to tabular data within FL frameworks. This work addresses this gap by introducing a consistent framework for evaluating deep anomaly detection algorithms in FL.\nWe established multiple conventions and features in FedAD-Bench, inspired by benchmarks in centralized settings, such as those by Alvarez et al. [1]. Key aspects of FedAD-Bench include:\n\u2022 Support for Federated Learning. FedAD-Bench is tailored to evaluate anomaly detection methods within federated settings, ensuring relevance to real-world FL scenarios. We provide practical recommendations for evaluating future methods and offer a robust codebase to promote reproducibility and applicability.\n\u2022 Redesigned Data Splitting. In traditional centralized benchmarks, data splitting often includes anomalies in the training set. However, since unsupervised methods aim to learn the underlying distributions or features of normally distributed data, we find it more appropriate to exclude anomalies from the training set. This redesign enhances the model's ability to learn normal data characteristics without interference."}, {"title": "II. RELATED WORK", "content": "The intersection of FL and anomaly detection represents an emerging area of research with significant potential for enhancing data privacy and security across various domains. This section reviews relevant literature, focusing on unsupervised methods for anomaly detection and the application of FL in this context. The aim is to highlight existing approaches and underscore the need for standardized benchmarks in evaluating these techniques."}, {"title": "A. Unsupervised Methods for Anomaly Detection", "content": "Unsupervised learning for anomaly detection depends on learning inlier (normal) data distribution and finding a threshold such that a sample distance from this distribution, measured using a certain distance metric, indicates whether it belongs to it or is an outlier (abnormal) sample [5]. To this end, many distance metrics are used in the literature [18]. Yet, we will mention the methods from which we chose representatives for brevity. Reconstruction methods such as Memory Augmented Deep Auto Encoder (MemAE) [6] and Deep Auto Encoder (DAE) [3] are among the most famous methods for unsupervised anomaly detection. They are trained to reconstruct normal samples and are used to classify anomalies according to the reconstruction loss. One-class classification methods such as Deep Support Vector Data Description (DeepSVDD) [23] learn a description of the normal data through training and recognize abnormal samples as the ones that do not follow the same description. Methods such as Deep Auto Encoding Gaussian Mixture Model (DAGMM) [32] learn the probability density function of the normal data to achieve the classification of abnormal data similar to previous methods. Furthermore, the Deep Structured Energy Based Model (DSEBM) [31] uses both energy as well as reconstruction scores as criteria for anomaly detection. Finally, Neural Transformation Learning for Deep Anomaly Detection (NeuTraLAD) [19] uses deterministic contrastive learning and learnable transformations. NeuTraLAD consists of a neural transformation module and an encoder, which are both trained using deterministic contrastive loss (DCL). The DCL is used during inference to calculate an anomaly score to denote a sample as normal or abnormal."}, {"title": "B. Federated Learning", "content": "FL has been increasingly explored in the context of anomaly detection, with numerous studies investigating their combined application. IoT has seen the most attention towards FL and anomaly detection, D\u00cfOT is a self-supervised approach for detecting compromised IoT devices, especially by intrusion attacks [17]. This framework uses FL to aggregate IoT devices' behavior profiles to detect anomalies. Li et al. [14] utilized a pre-trained anomaly detection model on the server side to detect anomalous weight updates propagated by the clients. Rey et al. [22] used supervised and unsupervised models to present a use case for malware detection in IoT devices. They further focus on adversarial attacks and communication costs. Wang et al. [29] used FL to train a global anomaly detection model aiming at finding battery failures in battery-backed energy storage systems. Lastly, Herurkar et al. [11] applied representation learning and FL to create Fin-Fed-OD, an algorithm that distinguishes between distinct outliers across different clients in FL. While these efforts concentrate on specific application domains, our study aims to establish a standardized framework for testing and benchmarking such approaches, providing a consistent basis for evaluating the effectiveness of FL in anomaly detection."}, {"title": "III. FEDAD-BENCH", "content": "To establish a robust benchmark for unsupervised anomaly detection in FL environments, we designed a comprehensive evaluation framework called FedAD-Bench. Our benchmark is inspired by recent advancements in both anomaly detection [1] and FL, aiming to bridge the gap between these two domains. The primary components of FedAD-Bench can be categorized into data splitting, evaluation metrics, and federated learning, which are detailed below.\nWe will use FedAD-Bench in our experimental section to give initial results for new anomaly detection methods specifically designed for FL. Moreover, it can be used to evaluate new centralized solutions directly in FL settings. Overall, FedAD-Bench aims to provide a standardized and reproducible framework for evaluating unsupervised anomaly detection models in FL settings, guiding future research and promoting fair comparisons across studies."}, {"title": "A. Data Splitting", "content": "While it is common in deep learning for classification to do 80-20 (train-test) and 70-10-20 (train-validation-test) splits of the dataset, in unsupervised anomaly detection, it is more beneficial to do a class-based split [1]. Given that the aim of unsupervised methods is to learn underlying distributions or features of normally distributed data, removing anomalies from the training set is thus more appropriate. Therefore, we advocate for test sets that include all anomalies in the data and half the number of inliers. This approach ensures a more"}, {"title": "B. Evaluation Metrics", "content": "One main issue with metrics used for anomaly detection evaluation is that they are threshold dependent, such as the F1-score; this dependency means that they can be easily manipulated by fixing the threshold and changing the number of anomalies in the test set to achieve better results [5]. The data-splitting approach we described earlier helps mitigate this potential bias by putting all anomalies into the test set. In our experiments, we use the optimal thresholds for each trained model for a fair comparison and report Precision, Recall, the area under the ROC curve (AUROC), the area under the Precision-Recall curve (AUPR), and the F1 Score. Estimating the optimal threshold involves iterating through various threshold values, calculating performance metrics (precision, recall, F1-Score) for each threshold, and identifying the threshold that maximizes the F1-Score. We propose splitting the test set further into validation and test sets, the purpose of the validation set is to calculate the optimal threshold to be used for threshold-based metrics. Moreover, this validation set should have the same anomaly ratio as the test set (a stratified split). The threshold range is adjusted based on the fraction of normal data in the validation set and uses these metrics to ensure the threshold selected offers the best trade-off between precision and recall [1]."}, {"title": "C. Federated Learning", "content": "In our study, we employ Federated Averaging (FedAvg) [15] as the aggregation algorithm for our main FL experiments. Afterwards, we conduct extensive experiments using FedProx [24] to demonstrate the importance of aggregation strategies as one of the main axes of FL experiments in Section V-B.\nOur primary FL configuration involves three clients, though we also examine scenarios with five and seven clients to assess scalability. As described in Section III-A, we exclude all anomalies from the training sets, incorporating them only in the test set. Consequently, the class imbalance problems in FL are beyond the scope of this paper, and we conduct experiments with a uniform sample distribution among the clients."}, {"title": "IV. EXPERIMENTS", "content": "With FedAD-Bench, we also establish a strong baseline for future methods to test against. As such, we describe the datasets we used and the evaluated (existing) anomaly detection methods adapted to work within the FL setting in the following."}, {"title": "A. Datasets", "content": "We chose 2 datasets from the medical domain and another 2 from the cybersecurity domain since these are among the most popular domains for tabular anomaly detection [1]:\n\u2022 Arrythmia [7]: a medical dataset detecting cardiac arrhythmia and originally classify it in one of the 16 groups. The classes are combined into presence or absence as a binary classification. The dataset contains 452 samples with 274 features, with an anomaly sample ratio of 0.1460.\n\u2022 Thyroid [20]: another medical dataset with one anomalous class and two normal classes, which are combined into one. The anomaly ratio is the lowest in the collection of datasets, with a value of 0.0246 among 3772 samples with 6 features each.\n\u2022 KDDCUP10: this is a 10% subset of the KDDCUP [25] network intrusion detection dataset with 494021 samples characterized by 34 continuous and 7 categorical variables. Originally consisting of 5 classes, including 4 different attack types, all attacks are merged into a single anomaly class resulting in an anomaly ratio of 0.1969.\n\u2022 NSL-KDD [27]: this is considered an updated version of the KDDCUP dataset consisting of the same feature set and 148517 samples. The anomaly ratio, however, is much higher at 0.4811."}, {"title": "B. Models", "content": "As per our selection of models, we chose 5 state of the art models that achieved the highest score on at least one evaluation metric and one dataset [1]. This decision resulted in the following models, which we mentioned previously in Section II:\n\u2022 DAE [3]: learns to reconstruct normal data patterns. The model is trained on inliers, and identifies anomalies by measuring the reconstruction error, such that if the error is high, the data point is considered anomalous.\n\u2022 DSEBM [31]: uses an energy-based formulation to model the distribution of normal data. The energy function is learned by the model, and anomalies are detected based on their energy values, where higher energy indicates an anomaly.\n\u2022 DEEPSVDD [23]: is built on top of kernel-based one-class classification. It utilizes a neural network to map data into a hypersphere in a high-dimensional space, and anomalies are identified as data points that lie outside the hypersphere. The model aims at minimizing the volume of the hypersphere containing normal data.\n\u2022 NeuTraLAD [19]: leverages neural networks to learn transformations that enhance the separation between normal and anomalous data. Learning optimal transformations makes anomalies more distinguishable from normal data, thereby improving detection accuracy.\n\u2022 MemAE [6]: enhances the traditional autoencoder by incorporating a memory module. This module stores prototypes of normal patterns, allowing the model to reconstruct normal data better and detect anomalies through reconstruction errors. The memory module helps in handling diverse and complex data patterns effectively."}, {"title": "C. Hyper-Parameters", "content": "For experiments Adam [13] optimizer was used with a learning rate and weight decay value of 1e-4. The batch size for centralized experiments was 128 samples for Arrythmia and Thyroid datasets, and 1024 for KDD10 and NSL-KDD datasets. In FL setting, the same batch size values were used as long as the local partitions were big enough, otherwise, the batch size was decreased to a suitable value. Each model was trained for the respective optimal number of epochs to establish a legitimate comparison between the performances of the models, the optimal number of epochs follows the results of Alvarez et al. benchmark [1]. The number of local epochs for FL clients is set to 10, and the number of communication rounds is equal to E/10 where E is the optimal number of epochs in the centralized scenario. For some experiments such as NeutraLAD and MemAE on KDD10 dataset the number of communication rounds E/10 was too few for them to converge and was set to E instead."}, {"title": "V. RESULTS", "content": "While it is expected for centralized learning to outperform FL, and this is the trend that is clear in Table II where centralized learning outperforms FL in 8 out of the 20 experiments. However, FL runs also outperformed centralized in 4 cases such as NeuTraLAD on both KDD10 and NSL-KDD datasets. We consider a method to outperform the other if it performs better in at least one metric while not performing worse in any. The improvement in this case of FL results compared to centralized learning can be attributed to a better regularization of the models. Since after each FL round, the local models are all shifted to the global average model which can have positive effect on robustness, generalization, and can counter overfitting [10].\nMoreover, we see that certain models do not experience a performance drop when aggregated and can even outperform their centralized versions. DAE is a clear example of such a model, where it consistently performed better or at least on par with its centralized version. We conclude that its simple architecture is affected the least by the global aggregation, while benefiting from the regularization of FL in its generalization ability. Conversely, we find complex models such as DSEBM performing slightly worse in most of the cases. Such models need more specialised aggregation strategies to handle extra modules and heuristics which are part of the models' training. It is important, to note that these results show how there can be a great discrepancy in conclusions depending on the metric that is looked at, which further demonstrates the importance of looking at all metrics holistically. Clear examples are results on the Thyroid dataset, where most models suffer poor performance on almost all metrics. Taking the DSEBM model as an example, in both centralized and FL experiments the model's score on each metric does not exceed 0.22 except for AUROC. On the other hand, when we only look at the AUROC metric, we see 0.74 and 0.73 scores for centralized and FL respectively. This can be misleading due to the fact that AUROC provides an extremely optimistic view by giving both normal and abnormal samples equal weights [1]. We discuss this further in Section V-C."}, {"title": "A. Number Of Clients", "content": "We further analyze the effect of the number of active clients on the performance of unsupervised anomaly detection models in FL. The same experiments were executed on 5 and 7 clients, having more clients with the same dataset size means that each client will have less samples while fixing the number of local epochs as well as the number of FL communication rounds [28]. Consequently, models tend to take longer to converge with such small training dataset sizes, especially since the local gradients before aggregation point further away from the global optimum as the dataset size decreases. This is indeed apparent in Table III where in most scenarios the performance drops as the number of clients increase under the same number of local epochs and communication rounds. Thyroid dataset saw the biggest drop in performance especially when using DEEPSVDD, on the other hand, performance of the models on KDD10 decreased the least. The sustained performance on KDD10 dataset indicates that the samples are too homogeneous (not diverse enough) such that the local models are able to reach similar local minima on smaller number of samples"}, {"title": "B. FedProx", "content": "Aggregation strategies, such as FedProx [24], are a critical axis of comparison for novel unsupervised anomaly detection methods in federated learning. To prove the comprehensiveness of this benchmark, we incorporated additional experiments utilizing the FedProx aggregation algorithm in a 3-client FL scenario. FedProx introduces a proximal term to the clients' local loss function, discouraging significant deviations from the global model. The modified loss function is defined as:\n$L = L_{obj} + \\frac{\\mu}{2} ||w - w_t||^2$\nHere, $L_{obj}$ represents the reconstruction loss in the context of unsupervised anomaly detection, w denotes the current local weights, $w_t$ indicates the global weights at communication round t, and \u00b5 is the coefficient of the proximal term.\nBy testing \u00b5 values in {0.01,0.1,1.0}, we observed that increasing \u00b5 often enhances the performance. This trend is particularly evident in the NeuTraLAD performance on the KDD10 dataset, indicating that FedProx effectively mitigates overfitting in this scenario. As discussed in Section V-A, the KDD10 dataset exhibits high homogeneity, leading to local overfitting, which FedProx's proximal term successfully addresses.\nThese findings show the ability of the FedProx algorithm in handling homogeneous datasets by alleviating local overfitting through its proximal term. This demonstrates the importance of selecting appropriate aggregation strategies to enhance the robustness and performance of federated learning models, particularly in the context of unsupervised anomaly detection."}, {"title": "C. Metrics Unreliability", "content": "The reliability of evaluation metrics in anomaly detection is a critical issue that warrants closer examination. In this study, we focus on the overly optimistic nature of the AUROC metric.\nThe AUROC metric can present an overly optimistic assessment of model performance when considered in isolation. Although AUROC is a threshold-independent metric, it suffers from bias when evaluated across all possible thresholds. It considers both the true positive rate (TPR = $\\frac{TP}{TP+FN}$) and the false positive rate (FPR = $\\frac{FP}{FP+TN}$) equally. In datasets where negative samples (normal cases) vastly outnumber positive samples (anomalies), the large number of true negatives (TN) dominates the denominator in the FPR calculation, thereby minimizing the FPR value and inflating the AUROC score.\nWhile AUROC and F1-score are important metrics for anomaly detection, our key point is to highlight that these metrics can be biased and potentially manipulated [5]. To mitigate these issues, we recommend considering the Area Under the Precision-Recall Curve (AUPR) as a complementary metric. AUPR is threshold-independent and does not suffer from the same biases as AUROC, making it more suitable for evaluating anomaly detection applications. However, it is crucial to adopt a holistic approach by considering multiple metrics to provide a comprehensive evaluation of model performance. Metrics such as precision, recall, F1-score, AUROC, and AUPR should all be examined together to ensure a balanced and accurate assessment, avoiding the biases associated with relying on a single metric. We further introduced the stratified splitting of the test data into validation and test sets to avoid such manipulation of threshold-based metrics and to make sure all models are tested on equal grounds."}, {"title": "VI. LIMITATIONS", "content": "We would like to shed the light on the limitation which pertains to the complexity of models used for unsupervised anomaly detection. These models often employ heuristics that are not propagated during the aggregation process during FL training. This leads to the loss of these heuristics at each round of communication, an example of this is the Memory module in MemAE which stores prototypical elements of the encoded normal data. Such limitation necessitates the development of specialized aggregation algorithms capable of meaningfully aggregating these heuristics on the server side.\nAnother limitation is the continued use of certain datasets, such as KDD10, as benchmarks [1]. These datasets have demonstrated lack of diversity, resulting in very high performance in most experiments, which does not provide meaningful insights. This calls attention to the need for increased research attention on tabular data, which FedAD-Bench aims to push forward."}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "In this paper, we introduced FedAD-Bench, a benchmark framework for unsupervised anomaly detection in FL environments, addressing the unique challenges posed by decentralized data, model aggregation, and metric unreliability. We conducted extensive experiments comparing recent deep learning methods across multiple datasets and metrics, offering a comprehensive evaluation of their performance. Our results reveal that while centralized learning typically outperforms FL, federated setups can surpass centralized ones in specific scenarios due to their inherent regularization effects, which help mitigate overfitting. This highlights the potential of FL in enhancing the robustness of unsupervised anomaly detection models.\nFor future research, a more diverse collection of datasets from different domains need to be incorporated to enhance the generalizability of the findings. Additionally, aggregation strategies that are more suitable for unsupervised anomaly detection models are required seeking to improve the robustness and performance of FL models. Finally, experimenting with a wider range of models will help determine their effectiveness in unsupervised anomaly detection within FL frameworks."}]}