{"title": "FedAD-Bench: A Unified Benchmark for Federated\nUnsupervised Anomaly Detection in Tabular Data", "authors": ["Ahmed Anwar", "Brian Moser", "Dayananda Herurkar", "Federico Raue", "Vinit Hegiste", "Tatjana Legler", "Andreas Dengel"], "abstract": "The emergence of federated learning (FL) presents\na promising approach to leverage decentralized data while\npreserving privacy. Furthermore, the combination of FL and\nanomaly detection is particularly compelling because it allows for\ndetecting rare and critical anomalies (usually also rare in locally\ngathered data) in sensitive data from multiple sources, such\nas cybersecurity and healthcare. However, benchmarking the\nperformance of anomaly detection methods in FL environments\nremains an underexplored area. This paper introduces FedAD-\nBench, a unified benchmark for evaluating unsupervised anomaly\ndetection algorithms within the context of FL. We systematically\nanalyze and compare the performance of recent deep learn-\ning anomaly detection models under federated settings, which\nwere typically assessed solely in centralized settings. FedAD-\nBench encompasses diverse datasets and metrics to provide a\nholistic evaluation. Through extensive experiments, we identify\nkey challenges such as model aggregation inefficiencies and\nmetric unreliability. We present insights into FL's regularization\neffects, revealing scenarios in which it outperforms centralized\napproaches due to its inherent ability to mitigate overfitting.\nOur work aims to establish a standardized benchmark to guide\nfuture research and development in federated anomaly detection,\npromoting reproducibility and fair comparison across studies.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, FL [15] has been growing in interest rapidly due\nto its appealing properties, especially in privacy-preserving\nmachine learning. The main concept of FL is the aggregation\nof local models, which is used to extract a global model\nthat matches the performance of centralized models while\nensuring that the training data remains confidential (local).\nThis straightforward yet powerful approach has demonstrated\nits effectiveness across a range of applications and fields, such\nas Text Prediction [8], Internet of Things [17], Image Super-\nResolution [16], Manufacturing [9] and Healthcare [12].\nOn the other hand, outlier detection remains a critical\nchallenge in data analysis, where it is essential to identify sam-\nples that deviate from expected and known distributions [18].\nWhile tree-based methods, such as XGBoost [2] and RecForest\n[30], achieve state-of-the-art performance on outlier detection\ndatasets [21], [26], unsupervised outlier detection has also\nproven to be highly effective, and its performance is on par\nwith tree-based methods [1]. Additionally, anomaly detection\ndata, such as network intrusion and medical records, is often\ndistributed among organizations that wish to keep it private.\nFL would allow for these organizations to collaboratively train\nhigh-performing machine learning models while preserving\nprivacy and abiding by data protection regulations, which sig-\nnificantly benefits all participants [11]. Unfortunately, there is\na significant lack of evaluation of anomaly detection methods\nin FL settings. Furthermore, a convention for testing such\nmethods in FL settings is yet to be established.\nIn response, we introduce FedAD-Bench, a unified bench-\nmark designed to evaluate state-of-the-art unsupervised models\nfor anomaly detection in FL settings. Our focus is specifically\non deep learning for several compelling reasons:\nFirstly, while tree-based methods are often the most effec-\ntive for anomaly detection, their integration with FL presents\nsignificant challenges. These methods require specialized ag-\ngregation techniques, complicating their implementation in FL\nenvironments [4]. Secondly, deep learning methods have pre-\ndominantly been applied to image and natural language pro-\ncessing tasks, leaving a notable gap in applying deep anomaly\ndetection techniques to tabular data within FL frameworks.\nThis work addresses this gap by introducing a consistent\nframework for evaluating deep anomaly detection algorithms\nin FL.\nWe established multiple conventions and features in FedAD-\nBench, inspired by benchmarks in centralized settings, such\nas those by Alvarez et al. [1]. Key aspects of FedAD-Bench\ninclude:\n\u2022 Support for Federated Learning. FedAD-Bench is\ntailored to evaluate anomaly detection methods within\nfederated settings, ensuring relevance to real-world FL\nscenarios. We provide practical recommendations for\nevaluating future methods and offer a robust codebase\nto promote reproducibility and applicability.\n\u2022 Redesigned Data Splitting. In traditional centralized\nbenchmarks, data splitting often includes anomalies in the\ntraining set. However, since unsupervised methods aim to\nlearn the underlying distributions or features of normally\ndistributed data, we find it more appropriate to exclude\nanomalies from the training set. This redesign enhances\nthe model's ability to learn normal data characteristics\nwithout interference."}, {"title": "II. RELATED WORK", "content": "The intersection of FL and anomaly detection represents\nan emerging area of research with significant potential for en-\nhancing data privacy and security across various domains. This\nsection reviews relevant literature, focusing on unsupervised\nmethods for anomaly detection and the application of FL in\nthis context. The aim is to highlight existing approaches and\nunderscore the need for standardized benchmarks in evaluating\nthese techniques."}, {"title": "A. Unsupervised Methods for Anomaly Detection", "content": "Unsupervised learning for anomaly detection depends on\nlearning inlier (normal) data distribution and finding a thresh-\nold such that a sample distance from this distribution, mea-\nsured using a certain distance metric, indicates whether it\nbelongs to it or is an outlier (abnormal) sample [5]. To\nthis end, many distance metrics are used in the literature\n[18]. Yet, we will mention the methods from which we\nchose representatives for brevity. Reconstruction methods such\nas Memory Augmented Deep Auto Encoder (MemAE) [6]\nand Deep Auto Encoder (DAE) [3] are among the most\nfamous methods for unsupervised anomaly detection. They\nare trained to reconstruct normal samples and are used to\nclassify anomalies according to the reconstruction loss. One-\nclass classification methods such as Deep Support Vector Data\nDescription (DeepSVDD) [23] learn a description of the nor-\nmal data through training and recognize abnormal samples as\nthe ones that do not follow the same description. Methods such\nas Deep Auto Encoding Gaussian Mixture Model (DAGMM)\n[32] learn the probability density function of the normal\ndata to achieve the classification of abnormal data similar to\nprevious methods. Furthermore, the Deep Structured Energy\nBased Model (DSEBM) [31] uses both energy as well as\nreconstruction scores as criteria for anomaly detection. Finally,\nNeural Transformation Learning for Deep Anomaly Detection\n(NeuTraLAD) [19] uses deterministic contrastive learning and\nlearnable transformations. NeuTraLAD consists of a neural\ntransformation module and an encoder, which are both trained\nusing deterministic contrastive loss (DCL). The DCL is used\nduring inference to calculate an anomaly score to denote a\nsample as normal or abnormal."}, {"title": "B. Federated Learning", "content": "FL has been increasingly explored in the context of anomaly\ndetection, with numerous studies investigating their combined\napplication. IoT has seen the most attention towards FL and\nanomaly detection, D\u00cfOT is a self-supervised approach for\ndetecting compromised IoT devices, especially by intrusion\nattacks [17]. This framework uses FL to aggregate IoT devices'\nbehavior profiles to detect anomalies. Li et al. [14] utilized\na pre-trained anomaly detection model on the server side to\ndetect anomalous weight updates propagated by the clients.\nRey et al. [22] used supervised and unsupervised models\nto present a use case for malware detection in IoT devices.\nThey further focus on adversarial attacks and communication\ncosts. Wang et al. [29] used FL to train a global anomaly\ndetection model aiming at finding battery failures in battery-\nbacked energy storage systems. Lastly, Herurkar et al. [11]\napplied representation learning and FL to create Fin-Fed-OD,\nan algorithm that distinguishes between distinct outliers across\ndifferent clients in FL. While these efforts concentrate on\nspecific application domains, our study aims to establish a\nstandardized framework for testing and benchmarking such\napproaches, providing a consistent basis for evaluating the\neffectiveness of FL in anomaly detection."}, {"title": "III. FEDAD-BENCH", "content": "To establish a robust benchmark for unsupervised anomaly\ndetection in FL environments, we designed a comprehensive\nevaluation framework called FedAD-Bench. Our benchmark is\ninspired by recent advancements in both anomaly detection [1]\nand FL, aiming to bridge the gap between these two domains.\nThe primary components of FedAD-Bench can be categorized\ninto data splitting, evaluation metrics, and federated learning,\nwhich are detailed below.\nWe will use FedAD-Bench in our experimental section\nto give initial results for new anomaly detection methods\nspecifically designed for FL. Moreover, it can be used to\nevaluate new centralized solutions directly in FL settings.\nOverall, FedAD-Bench aims to provide a standardized and\nreproducible framework for evaluating unsupervised anomaly\ndetection models in FL settings, guiding future research and\npromoting fair comparisons across studies."}, {"title": "A. Data Splitting", "content": "While it is common in deep learning for classification to\ndo 80-20 (train-test) and 70-10-20 (train-validation-test) splits\nof the dataset, in unsupervised anomaly detection, it is more\nbeneficial to do a class-based split [1]. Given that the aim\nof unsupervised methods is to learn underlying distributions\nor features of normally distributed data, removing anomalies\nfrom the training set is thus more appropriate. Therefore, we\nadvocate for test sets that include all anomalies in the data\nand half the number of inliers. This approach ensures a more"}, {"title": "B. Evaluation Metrics", "content": "One main issue with metrics used for anomaly detection\nevaluation is that they are threshold dependent, such as the\nF1-score; this dependency means that they can be easily\nmanipulated by fixing the threshold and changing the number\nof anomalies in the test set to achieve better results [5]. The\ndata-splitting approach we described earlier helps mitigate\nthis potential bias by putting all anomalies into the test set.\nIn our experiments, we use the optimal thresholds for each\ntrained model for a fair comparison and report Precision,\nRecall, the area under the ROC curve (AUROC), the area\nunder the Precision-Recall curve (AUPR), and the F1 Score.\nEstimating the optimal threshold involves iterating through\nvarious threshold values, calculating performance metrics (pre-\ncision, recall, F1-Score) for each threshold, and identifying the\nthreshold that maximizes the F1-Score. We propose splitting\nthe test set further into validation and test sets, the purpose of\nthe validation set is to calculate the optimal threshold to be\nused for threshold-based metrics. Moreover, this validation set\nshould have the same anomaly ratio as the test set (a stratified\nsplit). The threshold range is adjusted based on the fraction\nof normal data in the validation set and uses these metrics to\nensure the threshold selected offers the best trade-off between\nprecision and recall [1]."}, {"title": "C. Federated Learning", "content": "In our study, we employ Federated Averaging (FedAvg) [15]\nas the aggregation algorithm for our main FL experiments.\nAfterwards, we conduct extensive experiments using FedProx\n[24] to demonstrate the importance of aggregation strategies\nas one of the main axes of FL experiments in Section V-B.\nOur primary FL configuration involves three clients, though\nwe also examine scenarios with five and seven clients to\nassess scalability. As described in Section III-A, we exclude\nall anomalies from the training sets, incorporating them only\nin the test set. Consequently, the class imbalance problems\nin FL are beyond the scope of this paper, and we conduct\nexperiments with a uniform sample distribution among the\nclients."}, {"title": "IV. EXPERIMENTS", "content": "With FedAD-Bench, we also establish a strong baseline\nfor future methods to test against. As such, we describe\nthe datasets we used and the evaluated (existing) anomaly\ndetection methods adapted to work within the FL setting in\nthe following."}, {"title": "A. Datasets", "content": "We chose 2 datasets from the medical domain and another\n2 from the cybersecurity domain since these are among the\nmost popular domains for tabular anomaly detection [1]:\n\u2022 Arrythmia [7]: a medical dataset detecting cardiac ar-\nrhythmia and originally classify it in one of the 16 groups.\nThe classes are combined into presence or absence as a\nbinary classification. The dataset contains 452 samples\nwith 274 features, with an anomaly sample ratio of\n0.1460.\n\u2022 Thyroid [20]: another medical dataset with one anoma-\nlous class and two normal classes, which are combined\ninto one. The anomaly ratio is the lowest in the collection\nof datasets, with a value of 0.0246 among 3772 samples\nwith 6 features each.\n\u2022 KDDCUP10: this is a 10% subset of the KDDCUP\n[25] network intrusion detection dataset with 494021\nsamples characterized by 34 continuous and 7 categorical\nvariables. Originally consisting of 5 classes, including 4\ndifferent attack types, all attacks are merged into a single\nanomaly class resulting in an anomaly ratio of 0.1969.\n\u2022 NSL-KDD [27]: this is considered an updated version\nof the KDDCUP dataset consisting of the same feature\nset and 148517 samples. The anomaly ratio, however, is\nmuch higher at 0.4811."}, {"title": "B. Models", "content": "As per our selection of models, we chose 5 state of the\nart models that achieved the highest score on at least one\nevaluation metric and one dataset [1]. This decision resulted\nin the following models, which we mentioned previously in\nSection II:\n\u2022 DAE [3]: learns to reconstruct normal data patterns. The\nmodel is trained on inliers, and identifies anomalies by\nmeasuring the reconstruction error, such that if the error\nis high, the data point is considered anomalous.\n\u2022 DSEBM [31]: uses an energy-based formulation to model\nthe distribution of normal data. The energy function is\nlearned by the model, and anomalies are detected based\non their energy values, where higher energy indicates an\nanomaly.\n\u2022 DEEPSVDD [23]: is built on top of kernel-based one-\nclass classification. It utilizes a neural network to map\ndata into a hypersphere in a high-dimensional space, and\nanomalies are identified as data points that lie outside the\nhypersphere. The model aims at minimizing the volume\nof the hypersphere containing normal data.\n\u2022 NeuTraLAD [19]: leverages neural networks to learn\ntransformations that enhance the separation between nor-\nmal and anomalous data. Learning optimal transforma-\ntions makes anomalies more distinguishable from normal\ndata, thereby improving detection accuracy.\n\u2022 MemAE [6]: enhances the traditional autoencoder by\nincorporating a memory module. This module stores\nprototypes of normal patterns, allowing the model to re-\nconstruct normal data better and detect anomalies through\nreconstruction errors. The memory module helps in han-\ndling diverse and complex data patterns effectively."}, {"title": "C. Hyper-Parameters", "content": "For experiments Adam [13] optimizer was used with a\nlearning rate and weight decay value of $1e-4$. The batch size\nfor centralized experiments was 128 samples for Arrythmia\nand Thyroid datasets, and 1024 for KDD10 and NSL-KDD\ndatasets. In FL setting, the same batch size values were used\nas long as the local partitions were big enough, otherwise,\nthe batch size was decreased to a suitable value. Each model\nwas trained for the respective optimal number of epochs to\nestablish a legitimate comparison between the performances of\nthe models, the optimal number of epochs follows the results\nof Alvarez et al. benchmark [1]. The number of local epochs\nfor FL clients is set to 10, and the number of communication\nrounds is equal to $E/10$ where E is the optimal number of\nepochs in the centralized scenario. For some experiments such\nas NeutraLAD and MemAE on KDD10 dataset the number of\ncommunication rounds $E/10$ was too few for them to converge\nand was set to E instead."}, {"title": "V. RESULTS", "content": "While it is expected for centralized learning to outperform\nFL, and this is the trend that is clear in Table II where central-\nized learning outperforms FL in 8 out of the 20 experiments.\nHowever, FL runs also outperformed centralized in 4 cases\nsuch as NeuTraLAD on both KDD10 and NSL-KDD datasets.\nWe consider a method to outperform the other if it performs\nbetter in at least one metric while not performing worse in\nany. The improvement in this case of FL results compared to\ncentralized learning can be attributed to a better regularization\nof the models. Since after each FL round, the local models\nare all shifted to the global average model which can have\npositive effect on robustness, generalization, and can counter\noverfitting [10].\nMoreover, we see that certain models do not experience a\nperformance drop when aggregated and can even outperform\ntheir centralized versions. DAE is a clear example of such a\nmodel, where it consistently performed better or at least on par\nwith its centralized version. We conclude that its simple archi-\ntecture is affected the least by the global aggregation, while\nbenefiting from the regularization of FL in its generalization\nability. Conversely, we find complex models such as DSEBM\nperforming slightly worse in most of the cases. Such models\nneed more specialised aggregation strategies to handle extra\nmodules and heuristics which are part of the models' training.\nIt is important, to note that these results show how there can\nbe a great discrepancy in conclusions depending on the metric\nthat is looked at, which further demonstrates the importance\nof looking at all metrics holistically. Clear examples are\nresults on the Thyroid dataset, where most models suffer poor\nperformance on almost all metrics. Taking the DSEBM model\nas an example, in both centralized and FL experiments the\nmodel's score on each metric does not exceed 0.22 except\nfor AUROC. On the other hand, when we only look at the\nAUROC metric, we see 0.74 and 0.73 scores for centralized\nand FL respectively. This can be misleading due to the fact that\nAUROC provides an extremely optimistic view by giving both\nnormal and abnormal samples equal weights [1]. We discuss\nthis further in Section V-C."}, {"title": "A. Number Of Clients", "content": "We further analyze the effect of the number of active clients\non the performance of unsupervised anomaly detection models\nin FL. The same experiments were executed on 5 and 7 clients,\nhaving more clients with the same dataset size means that\neach client will have less samples while fixing the number\nof local epochs as well as the number of FL communication\nrounds [28]. Consequently, models tend to take longer to\nconverge with such small training dataset sizes, especially\nsince the local gradients before aggregation point further away\nfrom the global optimum as the dataset size decreases. This\nis indeed apparent in Table III where in most scenarios the\nperformance drops as the number of clients increase under\nthe same number of local epochs and communication rounds.\nThyroid dataset saw the biggest drop in performance especially\nwhen using DEEPSVDD, on the other hand, performance of\nthe models on KDD10 decreased the least. The sustained\nperformance on KDD10 dataset indicates that the samples are\ntoo homogeneous (not diverse enough) such that the local\nmodels are able to reach similar local minima on smaller\nnumber of samples"}, {"title": "B. FedProx", "content": "Aggregation strategies, such as FedProx [24], are a critical\naxis of comparison for novel unsupervised anomaly detection\nmethods in federated learning. To prove the comprehensive-\nness of this benchmark, we incorporated additional experi-\nments utilizing the FedProx aggregation algorithm in a 3-\nclient FL scenario. FedProx introduces a proximal term to the\nclients' local loss function, discouraging significant deviations\nfrom the global model. The modified loss function is defined\nas:\n$L_{t}=L_{obj} + \\frac{\\mu}{2}||w-w_t||^2$\nHere, $L_{obj}$ represents the reconstruction loss in the context\nof unsupervised anomaly detection, $w$ denotes the current local"}, {"title": "C. Metrics Unreliability", "content": "The reliability of evaluation metrics in anomaly detection is\na critical issue that warrants closer examination. In this study,\nwe focus on the overly optimistic nature of the AUROC metric.\nThe AUROC metric can present an overly optimistic as-\nsessment of model performance when considered in isolation.\nAlthough AUROC is a threshold-independent metric, it suffers\nfrom bias when evaluated across all possible thresholds. It\nconsiders both the true positive rate ($TPR = \\frac{TP}{TP + FN}$) and the\nfalse positive rate ($FPR = \\frac{FP}{FP + TN}$) equally. In datasets where\nnegative samples (normal cases) vastly outnumber positive\nsamples (anomalies), the large number of true negatives (TN)\ndominates the denominator in the FPR calculation, thereby\nminimizing the FPR value and inflating the AUROC score.\nWhile AUROC and F1-score are important metrics for\nanomaly detection, our key point is to highlight that these met-"}, {"title": "VI. LIMITATIONS", "content": "We would like to shed the light on the limitation which\npertains to the complexity of models used for unsupervised\nanomaly detection. These models often employ heuristics that\nare not propagated during the aggregation process during FL\ntraining. This leads to the loss of these heuristics at each round\nof communication, an example of this is the Memory module\nin MemAE which stores prototypical elements of the encoded\nnormal data. Such limitation necessitates the development of\nspecialized aggregation algorithms capable of meaningfully\naggregating these heuristics on the server side.\nAnother limitation is the continued use of certain datasets,\nsuch as KDD10, as benchmarks [1]. These datasets have\ndemonstrated lack of diversity, resulting in very high perfor-\nmance in most experiments, which does not provide mean-\ningful insights. This calls attention to the need for increased"}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "In this paper, we introduced FedAD-Bench, a benchmark\nframework for unsupervised anomaly detection in FL en-\nvironments, addressing the unique challenges posed by de-\ncentralized data, model aggregation, and metric unreliability.\nWe conducted extensive experiments comparing recent deep\nlearning methods across multiple datasets and metrics, offering\na comprehensive evaluation of their performance. Our results\nreveal that while centralized learning typically outperforms\nFL, federated setups can surpass centralized ones in specific\nscenarios due to their inherent regularization effects, which\nhelp mitigate overfitting. This highlights the potential of FL\nin enhancing the robustness of unsupervised anomaly detection\nmodels.\nFor future research, a more diverse collection of datasets\nfrom different domains need to be incorporated to enhance\nthe generalizability of the findings. Additionally, aggregation\nstrategies that are more suitable for unsupervised anomaly de-\ntection models are required seeking to improve the robustness\nand performance of FL models. Finally, experimenting with a\nwider range of models will help determine their effectiveness\nin unsupervised anomaly detection within FL frameworks."}]}