{"title": "Deconfounding Time Series Forecasting", "authors": ["Wentao Gao", "Feiyu Yang", "Mengze Hong", "Xiaojing Du", "Zechen Hu", "Xiongren Chen", "Ziqi Xu"], "abstract": "Time series forecasting is a critical task in various domains, where accurate predictions can drive informed decision-making. Traditional forecasting methods often rely on current observations of variables to predict future outcomes, typically overlooking the influence of latent confounders\u2014unobserved variables that simultaneously affect both the predictors and the target outcomes. This oversight can introduce bias and degrade the performance of predictive models. In this study, we address this challenge by proposing an enhanced forecasting approach that incorporates representations of latent confounders derived from historical data. By integrating these confounders into the predictive process, our method aims to improve the accuracy and robustness of time series forecasts. The proposed approach is demonstrated through its application to climate science data, showing significant improvements over traditional methods that do not account for confounders.", "sections": [{"title": "I. INTRODUCTION AND RELATED WORKS", "content": "Time series forecasting plays a critical role in fields such as finance, meteorology, traffic management, robotics, web search, and climate prediction, where accurate predictions can significantly enhance decision-making processes [1]-[4]. As data availability has rapidly increased and computational capabilities have advanced, increasingly sophisticated models have been developed to tackle the challenges in these domains. However, despite significant progress, accurately modeling the complex nonlinear trends and long-term dependencies typical of real-world time series data remains a formidable challenge [5].\nTraditional models like AutoRegressive Integrated Moving Average (ARIMA) and Exponential Smoothing (ETS) have been widely adopted due to their simplicity and effectiveness in handling linear relationships and short-term dependen- cies [5]. These models typically assume that time series data is stationary and linear, allowing future values to be predicted through a series of linear operations. However, this assumption shows limitations when dealing with the complexities of real- world data. For instance, many time series exhibit nonlinear dependencies and long-term trends, which traditional models struggle to capture effectively [6]. As a result, researchers have turned to more advanced techniques, particularly deep learning models, which have shown great potential in overcoming these limitations by modeling intricate patterns within time series data.\nDeep learning architectures, such as Long Short-Term Mem- ory (LSTM) networks and Gated Recurrent Units (GRU), introduced memory cells that allow for the retention of information over extended periods, significantly improving the ability to model long-term dependencies [7], [9]. These models incorporate a recurrent structure within the sequence data, enabling them to \"remember\" important information across longer time spans, thus avoiding the prediction accuracy degradation observed in traditional models. However, while LSTM and GRU have made advances in capturing long-term dependencies, they still face challenges in dealing with more complex temporal patterns, especially in multivariate time series where the interactions between different variables can be highly intricate.\nTo address these challenges, researchers have further de- veloped attention mechanisms, particularly in Transformer models, which have been widely applied in time series fore- casting. These models dynamically assign different weights to different parts of the input sequence, leading to significant advancements in capturing complex temporal dependencies. Through the attention mechanism, models can \"focus\" on the"}, {"title": "II. PROBLEM DEFINITION", "content": "As illustrated in Figure 1, consider the random variable Pas representing a collection of pertinent variables, which could include any type of multidimensional time series data across different fields. From these variables, let Y signify the target variable of interest. In our scenario, since we will utilize both historical and present data of these variables, to prevent confusion, we define $X_t$ as the historical version of $P \\backslash Y$ and $A_t$ as the current version of $P \\backslash Y$.\nOur task is to predict the future values of the target variable Y based on time series data. Traditional time series forecasting methods typically rely solely on the current values of the predictors $A_t$ to predict Y, often neglecting the potential in- fluence of latent confounders-unobserved variables that may simultaneously affect both the predictors $A_t$ and the target variable Y which is shown in Figure 2. This oversight can introduce bias and lead to suboptimal forecasting performance."}, {"title": "III. PROPOSED METHOD", "content": "Our method is trying to identify the hidden confounder and help time series forecasting. Overall process is shown in Figure 3 Inspired by [17], our method follow the potential outcome framework in causality. Causality is based on the assumption. To clarify the application area, we also did some assumption as shown below.\nAssumption 1: Consistency. If $A_{>t} = \\bar{a}_{>t}$, then the potential outcomes for following the treatment $\\bar{a}_{>t}$ is the same as the factual outcome $Y(\\bar{a}_{>t}) = Y$.\nAssumption 2: Positivity (Overlap): if $P(\\bar{A}_{t-1} = \\bar{a}_{t-1}, X_t = x_t) \\neq 0$ then $P(A_t = a_t | A_{t-1} = \\bar{a}_{t-1}, X_t = x_t) > 0$ for all $a_t$.\nAssumption 3: Sequential Single Strong Ignorability\n$Y(\\bar{a}_{\\geq t}) \\perp A_{tj} | X_t, H_{t-1}$\nfor all $\\bar{a}_{>t}, t \\in {0, . . ., T}$, and $j\\in {1, ..., k}$.\nOur method focuses on learning a variable Z that enables the treatment variable A to become conditionally independent of the covariate X within a multivariate time series framework. Unobserved confounders may simultaneously influence A, and Y. Based on this, we infer the sequence of these unob- served confounders as $z_t = g(h_{t-1})$, where $h_{t-1}$ represents the realization of the variable set $H_{t-1}$, including historical data of $a_{t-1}, X_{t-1}, Z_{t-1}$. Specifically, the factorization of the probability distribution can be expressed as:\n$p(a_{t1},..., A_{tk} | Z_t, X_t) = \\prod_{j=1}^{k} P(A_{tj} | Z_t, x_t)$.\nBy learning Z, we can more effectively capture the complex relationships within time series data, mitigating the impact"}, {"title": "IV. EXPERIMENT", "content": "Theoretically, the performance of any forecasting model can be improve if we can effectively capture the lantent confounder and applied to time series forecating. So our experiments will try to prove two things. First is our factor model in deconfounding step can learn latent confounder which is able to render the treatment with covariate. Second is the learnt confounder should align the true confounder variable. Third is the latent confounder can improve the performance of time series forecasting models.\nSince in real world, we actually do not know what exactly the latent confounder is. So we need to use sythetic dataset to prove this."}, {"title": "A. Simulation Dataset", "content": "To generize our method, we build our simulation dataset based on causal graph as shown in Figure 2\nThe covariates $X_t$ are generated as follows:\n$X_t = A_{t-1} + N_t$,\nwhere $\\eta_t \\sim N(0, 0.001^2)$.\nThe hidden confounder $Z_t$ is simulated by combining past treatments and hidden confounders:\n$Z_t = \\sum_{i=1}^{p} (\\lambda_i A_{t-i} + \\beta_i Z_{t-i}) + \\epsilon_t,$\nwhere $\\lambda_i \\sim N(0, 0.5^2)$, $\\beta_i \\sim N(\\frac{1}{i} (1 - \\frac{i}{p}))$ and $\\epsilon_t \\sim N(0, 0.001^2)$.\nThe treatment assignment $A_{t,j}$ depends on the covariate $X_t$ and the hidden confounder $Z_t$:\n$A_{t,j} = \\gamma_A Z_t + (1 - \\gamma_A) X_{t,j}.$\nThe outcome $Y_{t+1}$ is a function of the covariates and the hidden confounder:\n$Y_{t+1} = \\gamma_Y Z_t + (1 - \\gamma_Y) (\\frac{1}{k} \\sum_{j=1}^{k} X_{t+1,j}),$\nwhere k is the number of covariates.\nAfter constructing the simulation dataset, as shown in Figure 4, we evaluate the model's effectiveness in capturing the latent confounder Z by examining the alignment between the predicted and simulated confounders. The good alignment between predicted and simulated Z show the good performance in capturing latent confounder. As shown in Figure 5, we also evaluate the quality of the learned treatments by calculating the $R^2$ score between the predicted treatments and the true treatments. A high $R^2$ score indicates that the learned treat- ments closely match the true treatments, thus validating the effectiveness of our model."}, {"title": "V. EXPERIMENTAL SETTING AND REAL-WORLD CASE STUDY", "content": "In this study, we utilize a time series dataset related to cli- mate forecasting, aimed at evaluating the performance of var- ious models under different prediction scenarios. We selected South Australia (Latitude: -35.0\u00b0 to -28.0\u00b0, Longitude: 129.0\u00b0 to 141.0\u00b0) as the study area and extracted relevant climate vari- ables from the NCEP-NCAR Reanalysis 1 dataset\u00b9, provided by the National Oceanic and Atmospheric Administration (NOAA). The data were preprocessed to ensure consistency in input sequences and were split into training, validation, and test sets. In the experiments, the sequence length of the dataset was set to 96 time steps, and we conducted experiments with prediction lengths of 12, 24, 36, and 48 time steps to evaluate the models' capabilities across short, medium, and long-term forecasting scenarios.\nTo comprehensively assess the robustness of the models, we designed two experimental settings: the basic \"Without Confounder\" setting and a \"With Confounder\u201d setting. In the latter, the models were provided with additional confounding variables that potentially influence both the input features and the target variable. This was done to evaluate how well the models perform when dealing with complex, confounded data. In our experiments, we selected five advanced time series forecasting models for comparison, including iTransformer [18], TimeMixer [19], TimesNet [20], PatchTST [21], and Non-stationary Transformer [22]. Each model was configured according to its recommended hyperparameters to ensure a fair comparison.\nDuring the model training and evaluation process, we employed two standard evaluation metrics: MSE and MAE. These metrics were used to measure the deviation between the predicted values and the actual values and to compare the performance of each model under different settings and prediction lengths. All models were trained and evaluated on an NVIDIA RTX 4080 super GPU, and training techniques like early stopping and learning rate scheduling were applied to optimize model performance. The experimental results demonstrated significant differences in the performance of the models under various conditions, with specific results summarized in Table I.\nIn summary, this study explores the effectiveness and ro- bustness of different time series forecasting models in the critical application of climate forecasting through rigorous experimental design and systematic model comparison. The results not only reveal the varying performances of the models when handling data with and without confounding factors but also provide valuable insights for future research."}, {"title": "VI. CONCLUSION", "content": "In this research, we introduce a new time series forecasting method that specifically addresses the challenge of latent confounders-unobserved variables that can skew predictions by influencing both predictors and the target variable. By integrating the Time Series Deconfounder, our approach learns and incorporates these confounders into predictive models. Experiments on synthetic and real-world datasets show that our method significantly improves accuracy and robustness compared to models that ignore confounders. This study underscores the importance of considering latent confounders in time series forecasting and opens avenues for future research in causal inference and predictive modeling. We will further test the effectiveness of our method on more diverse and complex datasets in a broader range of application scenarios."}]}