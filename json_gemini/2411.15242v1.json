{"title": "The Zamba2 Suite: Technical Report", "authors": ["Paolo Glorioso", "Quentin Anthony", "Yury Tokpanov", "Anna Golubeva", "Vasudev Shyam", "James Whittington", "Jonathan Pilault", "Beren Millidge"], "abstract": "In this technical report, we present the Zamba2 series - a suite of 1.2B, 2.7B, and 7.4B parameter hybrid Mamba2-transformer models that achieve state of the art performance against the leading open-weights models of their class, while achieving substantial gains in inference latency, throughput, and memory efficiency. The Zamba2 series builds upon our initial work with Zamba1-7B, optimizing its architecture, training and annealing datasets, and training for up to three trillion tokens. We provide open-source weights for all models of the Zamba2 series as well as instruction-tuned variants that are strongly competitive against comparable instruct-tuned models of their class. We additionally open-source the pretraining dataset, which we call Zyda-2, used to train the Zamba2 series of models. The models and datasets used in this work are openly available at https://huggingface.co/Zyphra", "sections": [{"title": "I. INTRODUCTION", "content": "The transformer architecture (Vaswani et al., 2017) has revolutionized almost all fields of machine learning since its introduction seven years ago. The transformer is the first universally expressive sequence mixer made practical to scale by its efficient parallelization on GPUs. This efficiency has allowed transformer models to be scaled up by many orders of magnitude since their discovery, using unprecedented computational resources, leading to vastly enhanced performance and unlocking a wide range of capabilities. While transformers are an extremely powerful architecture at modeling, their expressivity comes at the cost of high compute and memory overheads. Specifically, a critical limitation is the quadratic compute cost and linear memory cost of attention, which becomes particularly relevant when scaling to long context lengths. Recently, several new architectures have been proposed that mitigate these limitations by using variants of linear attention (Katharopoulos et al., 2020). These state-space models (SSMs) such as Mamba (Gu & Dao, 2023; Dao & Gu, 2024), RWKV (Peng et al., 2023, 2024), and GLA (Yang et al., 2024) possess both highly parallelizable sequence mixing to ensure efficient training on GPUs while also possessing a recurrent formulation which enables O(1) memory and linear compute cost during autoregressive generation. Moreover, SSM models have been found to achieve scaling performance comparable to transformers in many domains. Notably, SSMs differ from conventional RNNs or their more advanced variants such as LSTMS (Hochreiter & Schmidhuber, 1997) and GRUs (Chung et al., 2014), because, while they are recurrent, their recurrence can be parallelized and mapped efficiently to GPU hardware to make training and prompt infilling efficient (Vaswani et al., 2017). While pure SSM models appear to underperform on tasks involving in-context-learning (Grazzi et al., 2024) and long-context retrieval Park et al. (2024a), when hybridized with attention blocks, they match or exceed the performance of pure transformer models while maintaining most of the inference efficiency of SSM models (Waleffe et al., 2024; Glorioso et al., 2024; Park et al., 2024b). Driven by the discovery of the scaling laws (Hestness et al., 2017; Kaplan et al., 2020; Hoffmann et al., 2022), model size has rapidly increased, driving concomitant explosions of both model training and inference costs. While continued scaling occurs at a rapid pace in order to reach the peaks of performance, smaller language models are also becoming rapidly more capable. Levels of performance that were previously thought to require models at the 100B-parameter scale or larger are now possible with models of with less than 10B parameters. For instance, leading 7B models (Meta, 2024; Jiang et al., 2023; Glorioso et al., 2024) now outperform the original GPT-3 at many classic language modeling evaluations. These dramatic improvements in model quality have been driven primarily by a substantial increase in the quality and scale of the pretraining datasets used to train them. Moreover, many of these models are open-weights, enabling free study and customization, and have the additional advantage that they can be run locally on consumer GPUs or devices instead of on large GPU clusters in the cloud. However, despite the compelling advantages of SSM hybrids in terms of performance, until now the leading models have been pure transformer models. The reduced memory usage and reduced inference compute requirements of SSM-hybrid models is especially compelling for running models on-device, where both memory and compute are highly constrained. In this technical report, we release the Zamba2 series of models a 1.2B, 2.7B and 7.4B parameter suite of language models that achieve state-of-the-art performance across a wide range of language modeling evaluations in addition to leading inference and memory efficiency. The Zamba2-series models can achieve a peak speedup of 30-50% time-to-first-token reduction as well as a 6x reduction in KV cache memory requirements over comparable transformer models due to their SSM-based architecture. We release the weights of all models"}, {"title": "II. ARCHITECTURE", "content": "The Zamba2 architecture (Fig. 2) builds upon the innovations introduced in our previous Zambal-7B model (Glorioso et al., 2024). There, we pioneered the use of a mamba backbone with shared attention blocks to optimize the performance per parameter. In our Zamba2 series, we performed a series of rigorous ablations to improve this architecture. Specifically, this led to the following improvements:\nWe switched from a Mambal (Gu & Dao, 2023) to a Mamba2 (Dao & Gu, 2024) backbone. We found that Mamba2 has significantly higher throughput than an equivalently sized Mambal, with approximately the same performance. This allows throughput to be traded for a larger state-size than is possible in Mambal, which improves model performance overall.\nWe use two alternating shared attention blocks instead of a single shared block as in Zambal. This improves performance against parameter-matched baselines, with the additional benefit of using fewer FLOPs in inference, since there are fewer total attention layers, and training compared to the pure shared attention approach of Zambal.\nWe apply non-shared Low-Rank Adapters (LoRAs) (Hu et al., 2021) to the shared transformer blocks (Attention, MLP, or both) this offers each shared attention block additional expressivity, since the entire attention computation is not forced to be the same at different layers, at a small cost of additional parameters.\nWe apply Rotary Position Embeddings (Su et al., 2023) to the shared attention blocks. While not strictly necessary for hybrid SSM-transformer models, we found that doing so improved performance, perhaps by providing an additional source of position information beyond simply the causal mask and the SSM cache.\nIn our architecture ablations, we focused on maximizing the marginal loss decrease per parameter and secondarily finding the models with the minimum marginal FLOPs per parameter. We found that focusing on these two metrics as well as rigorously testing FLOP-matched and parameter-matched baselines significantly helped in performing architecture search. Due to various implementation and timing details, our Zamba2 series of models are slightly heterogeneous (Fig. 2). This is because we were exploring and running architecture search experiments in parallel with main model training. In particular, our 2.7B model lacks rotary position embedding in the attention, and our 1.2B model possesses LoRAs on both the shared attention and shared MLPs, and in addition only has one shared attention block, like the original Zambal, rather than two alternating ones. This is because we found the benefit of two shared blocks to decrease at smaller scales, likely because of the fewer total attention layers."}, {"title": "III. PRETRAINING", "content": "The Zamba2 series of models were trained in two phases: a standard pretraining phase on primarily web data followed by an annealing phase consisting of a rapid decay of the learning rate over a mixture of web and higher-quality data. The 1.2B and 2.7B models were trained for 3T tokens. Due to compute and time limitations, we trained the Zamba2-7.4B model for 2T instead of 3T tokens. In the pretraining phase, for all models, we used the Zyda-2 dataset described in the following section. This dataset consists of heavily filtered web-text from a variety of sources, but especially including the FineWeb-Edu (Penedo et al., 2024) and DCLM (Li et al., 2024) datasets, which are filtered by educational quality. We found that this provided significant boosts to the model's capabilities in factual knowledge recall and reasoning capabilities as exemplified by the MMLU and ARC evaluation metrics. Pretraining was conducted on our main compute cluster of 16 nodes, each consisting of 8xH100 SXM nodes with 3.2Tbps Infiniband. To train the 2.7B and 1.2B models, data"}, {"title": "IV. DATASETS", "content": "For our pretraining dataset, we used Zyda-2 (Yury Tokpanov et al., 2024), a dataset of 5 trillion tokens consisting of FineWeb-Edu3 (Penedo et al., 2024), DCLM (Li et al., 2024), Zyda-1 (Tokpanov et al., 2024), and Dolma-common-crawl (Soldaini et al., 2024). We performed model quality-based filtering on the Dolma and Zyda-1 subsets and full cross-deduplication across all component datasets (Fig. 3). We found that our Zyda-2 dataset outperformed previous state-of-the-art datasets in annealing ablation tests (Fig. 4). This is because Zyda-2 uses additionally filtered versions of these datasets as well as other high quality sets such as Zyda-1, which means that Zyda-2 benefits from an ensembling effect of putting together multiple sources which can help compensate for the weaknesses of each specific dataset. While we trained on pure Zyda-2 for the 1.2B and 2.7B models, for the 7.4B model we augmented the dataset with 10% StarCoder (Li et al., 2023a) in order to improve the model's coding capabilities, since we believed such capabilities are more important for generalist models of the 7B scale rather than smaller models."}, {"title": "V. MODEL PERFORMANCE", "content": "Models of the Zamba2 series achieve leading performance compared against models of their weight class as measured by standard language model evaluation metrics (see Table I). We believe that this performance is attributable to the quality of our pretraining and annealing datasets, as well as the improved architecture of Zamba2, in particular when compared to pure transformer baselines. The Zamba2 series showcases the fact that highly-performant small models can be made with significantly smaller compute and token budgets than were previously thought necessary for a given level of performance. Perhaps the best way to demonstrate the strong evaluation performance of the Zamba2 architecture is to measure the performance per training token, which is a measure both of dataset and of architecture quality. As shown in Fig. 5, the Zamba2 series performs extremely strongly on this metric compared to the most competitive models of their scale. Given that our dataset is open (Zyda-2) while the datasets of almost all comparable models are closed, we believe it is unlikely that our dataset alone gives us a strong advantage and thus conclude that Zamba2's superior evaluation performance is largely due to the Zamba2 architecture."}, {"title": "B. Inference performance", "content": "Due to the improved throughput of the Mamba2 block over standard transformer blocks, Zamba2 models achieve"}, {"title": "VI. POST-TRAINING", "content": "In addition to releasing annealed checkpoints, we also release instruction-tuned versions of our base Zamba2 models which are specialized for instruction-following and AI-assistant chat use cases. We have found performance of our instruct models to be strongly competitive with the existing official instruct finetunes of competitor models (for each of the Zamba2 series), using only open-source finetuning datasets and methodologies (see Table II). Our instruction tuning approach consists of a phase of supervised finetuning on instruct and chat data, followed by multiple iterations of DPO (Rafailov et al., 2024). The supervised finetuning dataset consisted of about 4M samples, including OpenHermes (Teknium, 2023), UltraChat (Ding et al., 2023), Infinity-Instruct (BAAI, 2024a) and Llama-3-Magpie-Pro (Xu et al., 2024). For the secondary DPO phase, we used over 200k samples mostly from UltraFeedback (Cui et al., 2024), Orca-DPO pairs (BAAI, 2024b) (sourced from OpenOrca (Lian et al., 2023)), and OpenHermes-Preference (Huang et al., 2024) datasets. We finetuned our models to follow the standard ChatML template. We found that instruction-tuning significantly improved the usability of our models as measured by metrics such as IFeval (Zhou et al., 2023) and MT-bench (Zheng et al., 2023). We found that our models are relatively robust to changes in format compared to alternative models which tend to suffer drastic drops in evaluation scores if the chat template is different than expected."}, {"title": "2) Context Extension", "content": "Zamba2 was trained on a fixed context window of 4096 tokens, but it shows a remarkable degree of generalization to longer contexts when measured in terms of LM loss. However, when evaluated on tasks such as passkey retrieval (also known as \"retrieval of a needle from a haystack\"), the model fails shortly outside its context window. This can further be remedied by modifying the rotary position embedding inside the shared attention blocks. Specifically, we use the \"NTK-aware\" scaling introduced by bloc97 (2023). The idea is to rescale the angle variable in the rotary embedding $\\theta_{\\alpha}$ as\n$\\theta'{\\alpha} = \\theta_{\\alpha} \\frac{s}{\\sqrt[d]{emb}/(d_{emb} - 1)},$\nwhere $d_{emb}$ denotes the embedding dimension and s is a scaling factor we chose to be 16. In conventional transformers, this factor is typically taken to be the ratio of the target maximum context length to the current maximum context length, but in our case this identification does not strictly hold. With this modification of the rotary embedding, the 7B model's effective context window can be extended up to 17000 tokens without additional training. The performance we see is consistent with observations in other studies (Peng et al., 2023), where they noticed that NTK-aware scaling is very well suited for context extension without additional finetuning. In Zamba2-2.7B, where no positional embedding was used, we observed that simple finetuning on long-context pretraining data of length up to 65536 extends the model's ability to perform accurate passkey retrieval up to this extended context window (Fig. 10). The sequences in the data that was used for this extension were simply concatenated sequences in the"}, {"title": "A. Quantization", "content": "We have experimented with finetuning Zamba2 models for specific tasks, e.g. text summarization, using QLoRA (Dettmers et al., 2023) a method for efficient finetuning that quantizes the weights of the base model, freezes them and finetunes only the weights of added LoRAs (Hu et al., 2021). We found that, largely, standard quantization approaches generalize well to hybrid attention-SSM models from transformers without requiring significant modification. The primary special considerations when quantizing SSMs is the importance of keeping the SSM and convolutional state and sensitive SSM matrices such as the A matrix and dt projection in higher precision. The remainder of the parameters in the SSM block can be successfully quantized. We quantized the linear layers of the Zamba2-2.7B instruct model (excluding the embedding and unembedding layers) to 4-bit precision, added LoRAs to the linear layers in the attention blocks of Zamba2, and performed supervised finetuning of the LORAs on a small custom dataset. We found that a dataset of several hundred thousands high-quality samples is sufficient to achieve satisfactory performance. Subsequently we quantized the LoRA parameters to 4-bit as well. Quantizing the Zamba2-2.7B to 4-bit precision reduces the memory footprint from 5.38 GB to 1.55 GB; with 4-bit quantized LoRA parameters the final model is at just 1.7 GB. This combination of high performance and small size enables the Zamba2 series of models to be deployed effectively in a wide variety of on-device environments. Training a set of LoRAs offers an efficient and lightweight solution for specializing the one base model for several tasks. We used the HuggingFace libraries BitsAndBytes (Dettmers et al., 2022) and PEFT (Mangrulkar et al., 2022) for quantization and finetuning."}, {"title": "VII. RELATED WORK", "content": "The space of small language models has become increasingly crowded as the general potential of such models becomes widely recognized. A number of open-weight transformer model families achieve strong performance at small scales, such as the Llama3 series (Llama3 Team, 2024), Mistral models (Jiang et al., 2023), and the Gemmal and Gemma2"}, {"title": "VIII. DISCUSSION", "content": "The performance of language models at small parameter sizes has risen dramatically over the past few years, with capabilities that were previously thought to require hundreds of billions of parameters being achieved with less than 10 billion. This dramatic increase in capability has seemingly been driven by a few relatively simple factors \u2013 namely a vast increase in the quality and scale of pretraining datasets. From a few hundred billion tokens of completely unfiltered web-crawl, to the order of ten-trillion token datasets comprised of extensively filtered and vetted tokens carefully designed to maximize specific desirable capabilities of the model. The \"Chinchilla scaling laws\" (Hoffmann et al., 2022) showed how in many cases data quantity was significantly more important than previously recognized, and that existing models of the time were dramatically undertrained. Moreover, by accounting for inference efficiency, models are now trained for dramatically longer on more tokens than the Chinchilla scaling laws would predict, because performance continues to improve monotonically, although strongly sublinearly, in the training dataset size. Small models today are trained for trillions of tokens on high-quality datasets, some (Abdin et al., 2024) including large amounts of synthetic data. In our previous paper, we speculated upon the necessary ingredients to reach the frontier, and our current work confirms that simply training on twice the total tokens and on a significantly higher-quality pretraining dataset is all that is required for now. The interesting question then becomes what the methods that will drive the next frontier of performance. One promising direction are improvements to the model architecture. In many experiments we have observed that in rigorously parameter and FLOP-matched ablations, the Zamba2 architecture significantly outperforms standard Llama-style transformer baselines. We speculate that given the strongly sublinear returns to training on additional tokens, in the current regime of significantly-Chinchilla-overtrained models, the constant improvement factor given by a novel architectural innovations can outweigh extremely large FLOP differentials in pretraining. We see this empirically when comparing the FLOP-vs-performance graphs of Zamba2 vs other transformer models. We expect that the Zamba2 architecture is still far from the optimum and that there is significant room for optimization of the marginal loss per parameter. In addition, we believe there are still improvements which can be made in pretraining and annealing dataset design. The Phi series of models (Abdin et al., 2024; Li et al., 2023b) have highlighted one possible path of including industrial quantities of synthetic data directly into the pretraining mix. This approach holds promise to augment and specialize model capabilities towards those we tend to care about, as well as to address weaknesses in existing models. Beyond this, some recent models, such as Gemma2-9B have described using distillation from existing larger models to boost performance (Gemma Team, 2024a). While more expensive in FLOPs than standard pretraining, this could provide another strong avenue of performance improvement similar to the effects of Chinchilla-overtraining. Overall, it is highly likely that the performance of small, open-weight models can be strongly improved, and this will have the effect of democratizing powerful LLMs and making them accessible and abundant throughout the economy."}, {"title": "AUTHOR CONTRIBUTIONS", "content": "Paolo Lead post-training. Contributed to core infrastructure. Lead Huggingface conversion and release. Contributed to pretraining. Quentin Lead model optimization and inference. Contributed to core infrastructure. Lead cluster management and maintenance. Contributed to evaluations. Contributed to pretraining. Yury Lead pretraining dataset (Zyda-2) creation. Contributed to core infrastructure and cluster maintenance. Anna Lead quantization. Contributed to post-training. Vasu Lead context length extension. James Contributed to architecture search experiments. Jonathan Contributed to architecture search experiments. Beren Overall project lead. Lead annealing dataset creation and annealing phase. Lead evaluations. Contributed to core infrastructure. Contributed to architecture search experiments. Contributed to post-training and pretraining datasets. Contributed to cluster management. Contributed to pretraining."}]}