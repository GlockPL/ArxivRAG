{"title": "An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures", "authors": ["Thibaut Boissin", "Franck Mamalet", "Thomas Fel", "Agustin Martin Picard", "Thomas Massena", "Mathieu Serrurier"], "abstract": "Orthogonal convolutional layers are the workhorse of multiple areas in machine learning, such as adversarial robustness, normalizing flows, GANs, and Lipschitz-constrained models. Their ability to preserve norms and ensure stable gradient propagation makes them valuable for a large range of problems. Despite their promise, the deployment of orthogonal convolution in large-scale applications is a significant challenge due to computational overhead and limited support for modern features like strides, dilations, group convolutions, and transposed convolutions.\nIn this paper, we introduce AOC (Adaptative Orthogonal Convolution), a scalable method for constructing orthogonal convolutions, effectively overcoming these limitations. This advancement unlocks the construction of architectures that were previously considered impractical. We demonstrate through our experiments that our method produces expressive models that become increasingly efficient as they scale. To foster further advancement, we provide an open-source library implementing this method, available at https://github.com/thib-s/orthogonium.", "sections": [{"title": "1. Introduction and Related Works", "content": "Orthogonal layers have become fundamental components in various deep learning architectures due to their unique mathematical properties, which offer benefits across multiple applications. For instance, robustness against adversarial attacks can be achieved by managing a model's Lipschitz constant [54] - with 1-Lipschitz networks being a prime candidate [2] \u2013 an approach that requires the use of orthogonal layers. Initially, researchers experimented with regularization techniques [12]; however, constrained networks, especially those employing orthogonal layers, soon became central, as they provided the advantage of tighter certification bounds. Beyond robustness, orthogonal layers also play a key role in enhancing performance in normalizing flows. Normalizing flows are generative models that transform simple distributions into complex ones via invertible mappings [19, 42]. Orthogonal convolutions enable these transformations with a computable Jacobian determinant, thus improving training efficiency [29] and forming the basis for invertible residual networks [6] Additionally, orthogonal layers stabilize training deep and recurrent neural networks (RNNs) by preserving gradient norms through time, essential in capturing long-term dependencies in time-series, such as language and speech tasks [5, 27, 40]. Lastly, in Wasserstein GANs (WGANs) [4] orthogonality in both the discriminator and generator [35, 36] supports stability and expressivity without requiring weight clipping or gradient penalties [23], making it essential for large-scale GAN training [11].\nHowever, despite these benefits, extending orthogonality to convolutional layers remains challenging. The orthogonalization of large Toeplitz matrices-structures central to convolution\u2014presents difficulties without compromising convolutional properties. Efficient orthogonalization of these structured matrices has theoretical importance, affecting generalization [7] and indicating when orthogonal convolution is feasible [1]. Early approaches [40, 60] explored regularization, yet practical constraints led to the following solutions:\nExplicit Construction Methods. Building on [64], approaches like BCOP [31], SC-Fac [53], and ECO [68] construct orthogonal convolutions directly in the spatial domain. These methods maintain orthogonality but often lack flexibility in kernel size control and do not support operations like striding and transposed convolutions.\nFrequency Domain Approaches. Methods such as Cayley Convolution [55], LOT [66], and ProjUNN-T [27] enforce orthogonality by parameterizing kernels in Fourier space."}, {"title": "2. An Adaptive scheme to build Orthogonal Convolution (AOC)", "content": "We will first recall what is the Block Convolution in 2.1. This tool allows the explicit construction of orthogonal kernels, which also support modern features depicted in 2.2. Finally, 2.3 provides implementation details that allow the method to scale.\n2.1. Core tool: Block Convolution\nOur approach builds upon three foundational papers: [64], which generalized orthogonal initialization to convolution to enable training networks with 10 000 layers, though without addressing constrained training; [53], which tackled this for separable convolutions; and [31], which extended it to general 2D convolutions. These works rely heavily on a tool known as block convolution. In this section, we review, clarify, and extend this mathematical framework.\nNotations: We consider convolutional layers characterized by $c_o$, the number of output channels; $c_i$, the number of input channels; $k_1 \\times k_2$, the kernel size; $s$, the stride parameter; and $g$, the number of groups. For simplicity in the notation, we fix the group parameter to $g = 1$ by default (all proofs hold for other values of $g$, see Section 2.2.4). Also, we assume circular padding in all proofs. The kernel tensor of the convolution is denoted $K \\in \\mathbb{R}^{c_o \\times c_i \\times k_1 \\times k_2}$, while $x \\in \\mathbb{R}^{c_i \\times h \\times w}$ denotes the input tensor. We describe the convolution operation with three different notations:\n$y = K *_s x$ (Kernel notation)\n$y = S_s \\tilde{K} x$ (Toeplitz notation)\n$y = \\texttt{conv}(x, \\texttt{stride} = s)$ (Code notation)\nEquation 1 defines the convolution operation with kernel $K$ and stride $s$ applied on $x$. Equation 2 highlights that this convolution is equivalent to a linear operation defined by a matrix product between a Toeplitz matrix $\\tilde{K} \\in \\mathbb{R}^{c_o h w \\times c_i h w}$ and a vector $\\tilde{x} \\in \\mathbb{R}^{c_i h w}$, which is obtained by flattening $x$. The striding operation is represented by a masking diagonal matrix $S_s \\in \\mathbb{R}^{c_o h w \\times c_o h w}$ with ones on the selected entries and zeros elsewhere. When $s = 1$, we have $S_1 = I$, the identity matrix (with kernel $I$). Equation 3 shows these notations in pseudo-code form.\nDefinition 2.1 (Block-convolution \u00b9). The block convolution, denoted as $B \\circledast A$, computes the equivalent kernel of the composition of two convolutional kernels, $A$ and $B$, enabling their combined effect without performing each convolution separately.\n$(B \\circledast A) *_s x = B *_s (A *_1 x)$\n$S_s \\widetilde{(B \\circledast A)} = (S_s \\widetilde{B}) \\widetilde{A} S_1$\n$\\texttt{conv}_{B \\circledast A}(x, s) = \\texttt{conv}_B(\\texttt{Conv}(x, 1), s)$\nThis operator assumes that the number of input channels of the second convolution $B$ matches the number of output channels of the first convolution $A$ (condition denoted as $A \\vartriangleright B$). Given $A \\in \\mathbb{R}^{c_{int} \\times c_i \\times k^A_1 \\times k^A_2}$ and $B \\in \\mathbb{R}^{c_o \\times c_{int} \\times k^B_1 \\times k^B_2}$, then $B \\circledast A \\in \\mathbb{R}^{c_o \\times c_i \\times (k^A_1 + k^B_1 - 1) \\times (k^A_2 + k^B_2 - 1)}$. The computation of Block-convolution kernel weights is given by:\n$(B \\circledast A)_{m,n,i,j} = \\sum_{c=0}^{c_{int}-1} \\sum_{i'=0}^{k^A-1} \\sum_{j'=0}^{k^A-1} B_{m,c,i,j'} A_{c,n,i-i',j-j'}$\nwhere $A$ is zero-padded, i.e., $A_{c,n,i,j} = 0$ if $i \\notin [0,k_1[$ or $j \\notin [0,k_2[$. While a classic result, for completeness,\n2.2. Construction of Strided, Transposed, Grouped,\nDilated, Orthogonal Convolutions with AOC\nWe first recall the concept of orthogonality for convolutions:\nDefinition 2.5 (Orthogonal Convolution). A convolution defined by a kernel $K$ is row or column orthogonal if:\n$(S_s \\tilde{K}) (S_s \\tilde{K})^T = I$ (row orthogonal)\n$(S_s \\tilde{K})^T (S_s \\tilde{K}) = I$ (column orthogonal)\nThe type of orthogonality (row or column) is given by the matrix in the toeplitz, notation (Eq. 2). When $c_i s^2 > c_o$, $S_s\\tilde{K}$ is column orthogonal [1]. When $c_i s^2 = c_o$, $S_s\\tilde{K}$ is a square matrix, and the two conditions are equivalent. Finally, when $s = 1$, the condition on the Toeplitz matrices is equivalent to $K \\circledast K^T = I$ (resp. $K^T \\circledast K = I$). A formal definition of $K^T$ can be found in Definition 2.9.\nOn one side, methods like BCOP and SC-Fac support any kernel size, but striding is emulated in a way that makes the convolution more costly than without striding. This limitation is even more pronounced in methods such as SOC, LOT, ECO, and Cayley, which handle channel changes (when $c_i \\neq c_o$) through padding or channel dropping. On the other side, RKO [47, 48] offers advantages like effective channel handling and efficient striding, but it is not orthogonal when stride $\\neq$ kernel size (see 2.2.2). By leveraging the ability to fuse kernels, we explore whether combining methods can offset the drawbacks of each component."}, {"title": "2.2.1 Standard Orthogonal Convolution", "content": "We unify the works of [64], [31], and [53] within a consistent notation framework, highlighting similarities and differences among their approaches to constructing standard orthogonal convolutions (i.e., without stride, transposition, grouping, or dilation). These methods primarily rely on constructing elementary blocks (1 \u00d7 1, 1 \u00d7 2, and 2 \u00d7 1 orthogonal convolutions) and assembling these blocks to create orthogonal convolutions of the desired size and shape.\nFrom Matrices to Orthogonal 1 \u00d7 1 Convolutions. A substantial body of research exists on building orthogonal matrices $M \\in \\mathbb{R}^{c_o \\times c_i}$. One common approach involves applying a differentiable projection operator to an unconstrained weight matrix, yielding an orthogonal matrix such that $MM^T = I$ or $M^T M = I$. Various methods exist, including the Bj\u00f6rck and Bowie orthogonalization scheme [8], the exponential method [51], the Cayley method [55], and QR factorization [57]. An orthogonal matrix can easily be reshaped into a convolution kernel with a 1 \u00d7 1 kernel $M \\in \\mathbb{R}^{c_o \\times c_i \\times 1 \\times 1}$, and such a convolution is orthogonal if $MM^T$ is orthogonal. These convolutions are mainly used to change the number of channels (Fig. 1-1).\nFrom Matrices to 1 \u00d7 2 Orthogonal Convolutions. Stacking two orthogonal 1 \u00d7 1 convolution kernels along their last dimensions\u00b2 leads to a 1 \u00d7 2 convolution, though it is generally not orthogonal. Authors of [53, 64] noted that additional constraints are needed, proposing a half-rank symmetric projector to construct a 1 \u00d7 2 orthogonal convolution: from a column-orthogonal matrix $M \\in \\mathbb{R}^{c \\times \\frac{3}{2}}$3, the matrix $N = MM^T \\in \\mathbb{R}^{c \\times c}$ is a symmetric projector that satisfies:\n$N = N^2 = N^T$ and $(I - N) = (I \u2013 N)^2 = (I \u2013 N)^T$\nThese two matrices can be reshaped into $c \\times c \\times 1 \\times 1$ convolution kernels, and stacking them along the last dimension results in an orthogonal $c \\times c \\times 1 \\times 2$ convolution kernel:\n$P = \\texttt{stack}([N, I \u2013 N], \\texttt{axis} = -1) \\implies P \\circledast P^T = I$\nSimilarly, stacking along the penultimate dimension, $Q = \\texttt{stack}([N, I - N], \\texttt{axis} = -2)$, results in an orthogonal $c \\times c \\times 2 \\times 1$ kernel. Although already proven by previous work, proof of this can be found in Appendix E.\nFrom 1 \u00d7 2 to $k_1 \\times k_2$ Orthogonal Convolutions. The three papers propose to build standard orthogonal convolutions by composing smaller kernels based on the following properties:\nProposition 2.6 (Composition of Orthogonal Convolutions). The composition of two row orthogonal convolutions is a row orthogonal convolution:\n$AA^T = I$ and $BB^T = I \\implies AB(AB)^T = I$\nThe same applies to two column orthogonal convolutions. However, the composition of a row orthogonal with a column orthogonal convolution is, in general, not orthogonal.\nUsing block convolutions, we can represent the composition of 1 \u00d7 2 and 2 \u00d7 1 kernels\u2074 to obtain a kernel with any desired shape. The differences among the three approaches"}, {"title": "2.2.2 Native Striding for Orthogonal Convolutions", "content": "Classical convolutional networks use strided convolutions. However, most existing work on orthogonal convolutions does not support stride directly, instead emulating striding via a reshaping operation [49]: transforming the $c_i \\times h \\times w$ tensor into a $c_i s^2 \\times 1 \\times 1$ tensor, followed by a non-strided convolution. This emulation requires more parameters ($c_i s^2 k_1 k_2$) than its non-strided counterpart, unlike native striding. Beyond this limitation, emulated striding also prevents native implementation of transposed convolutions (see Section 2.2.3). In this section, we propose a method to construct orthogonal kernels that support native striding.\nTo our knowledge, only two works [47, 48] claim to use native stride. These rely on a method referred to by [31] as Reshaped Kernel Orthogonalization (RKO). This method involves reshaping the kernel $K_{RKO} \\in \\mathbb{R}^{c_o \\times c_i \\times k_1 \\times k_2}$ into a matrix $K' \\in \\mathbb{R}^{c_o \\times c_i k_1 k_2}$ and orthogonalizing it. In general, with the adequate multiplicative factor, the resulting convolution is 1-Lipschitz but not orthogonal. In this work, we prove that no additional factor is required when $k = s$ to obtain an orthogonal convolution:\nProposition 2.7 (RKO gives an orthogonal kernel when $k_1 = k_2 = s$). When $K' \\in \\mathbb{R}^{c_o \\times c_i k^2}$ is orthogonal, the convolution with the reshaped kernel $K_{rko} \\in \\mathbb{R}^{c_o \\times c_i \\times k \\times k}$ and a stride $s = k$ is orthogonal. The formal proof can be found in Appendix E.\nThe proposed method, called AOC, combines the BCOP and RKO methods to construct a Strided Convolution with Arbitrary Kernel Size:\n$K_{AOC} = K_{RKO} \\circledast K_{BCOP}$\nAs shown in Fig. 1, $K_{BCOP} \\in \\mathbb{R}^{c \\times c_i \\times (k_1+1-s) \\times (k_2+1-s)}$ is fused with $K_{RKO} \\in \\mathbb{R}^{c_o \\times c \\times s \\times s}$, resulting in a kernel $k_1 k_2$ that can be used with stride $s$. This formulation uses an internal channel size $c$, which is set to $\\texttt{max}(c_i, \\lfloor \\frac{3}{2} \\rfloor)$ to preserve orthogonality. According to [1], no orthogonal kernel exists when $s > k$. Our proposed approach thus covers all valid configurations of orthogonal convolutions since $k+1-s \\geq 0$.\nProposition 2.8 (Orthogonality of strided AOC (Informal)). Setting $c = \\texttt{max}(c_i, \\lfloor \\frac{3}{2} \\rfloor)$ yields an orthogonal convolution. The complete proof can be found in Appendix E.\nThe proof relies on the fact that Proposition 2.6 also holds for strided convolutions applied on $S_s S_s K_{RKO}$. We prove that when $min(i, \\lfloor \\frac{3}{2} \\rfloor) < c \\leq max(c_i, \\lfloor \\frac{3}{2} \\rfloor)$, the two matrices $K_{BCOP}$ and $S_s S_s K_{RKO}$ are either both row-orthogonal or both column-orthogonal. The choice of $c = \\texttt{max}(c_i, \\lfloor \\frac{3}{2} \\rfloor)$ maximizes the expressiveness of the parametrization while ensuring that the resulting convolution is orthogonal."}, {"title": "2.2.3 Native Transposed Orthogonal Convolutions:", "content": "In addition to the practical verification of definition 2.5, transposed convolutions are mostly used as learnable upscaling layers in architectures such as U-Net [43] or VAEs [28, 58]. For a given convolution with stride defined in 2, it corresponds to the application of the transposed matrix $(S_s \\tilde{K})^T$, inverting the role of $c_i$ and $c_o$. The resulting operation can be defined by the three notations:\nDefinition 2.9 (Transposed Convolution). A transposed convolution is defined as follows:\n$y = K^T *_1 x$\n$y = (S_s \\tilde{K})^T x = \\tilde{K}^T S_s x$\n$y = \\texttt{ConvTranspose}(x, \\texttt{stride} = s)$\nThe code notation (Equation 10) corresponds to the implementation in PyTorch parametrized by the original kernel $K$. The equation 9 corresponds to the transposition of the underlying Toeplitz, matrix. The kernel notation (Equation 8) can be viewed as a standard convolution with a transposed kernel and fractional striding. The kernel $K^T \\in \\mathbb{R}^{c_i \\times c_o \\times k_1 \\times k_2}$ is obtained by transposing the channel dimensions (the first ones) and reversing the kernel ones (the last two)."}, {"title": "2.3. Efficient implementation of AOC", "content": "Beyond a mathematical framework that unlocks a more flexible use of orthogonal convolutions, we propose several design choices for an implementation that scales well to larger kernels, larger images and larger batch sizes. Although AOC includes the construction of BCOP and RKO kernels, our implementation improves the original ones at many stages. It results in an 8x reduction of the original overhead in realistic settings. Our implementation will be integrated at https://github.com/thib-s/orthogonium.\nFast implementation of the block convolution: To the best of our knowledge, the only differentiable implementation of the BCOP method is available in the reference code [31]. However, since there is no cuda kernel available for block convolution, the authors relied on nested loops to perform all matrix multiplication to compute the resulting kernel. Unfortunately, this approach prevents PyTorch from parallelizing the nested loops.\nHowever, our analysis have shown that this can be efficiently paralleled in a single operation. Inspired by [60] and [15], which aimed to compute $A \\circledast A^T$ to prove orthogonality, we propose to replace the computation $B \\circledast A$ by a convolution with zero padding between B and $A^T$. This approach can also be seen as a specific case of convolutional einsum [41]. This operation can be rewritten by re-ordering the summation to use a 2D convolution at its core. The strategy is to use the 2D convolution to compute one output filter. Then, the batch dimension can be used to compute all output filters in parallel. \nReducing time complexity of BCOP: Beyond the efficient parallelism of the operation, we propose to parallelize the whole kernel computation: the parametrization can be seen as the composition of many small kernels. Not only can those be created in parallel, but they can also be combined efficiently. As the $\\circledast$ is an associative operation (prop 2.2), we can leverage the parallel associative scan [13, 69] to parallelize the iterations of the original algorithm. The original 2* (k-s) sequential operations can then be done in O(log(k \u2013 s)) iterations (Fig. 2b). This is unlocked in practice if implementation supports batching. Unfortunately, the batch dimension of our efficient implementation already uses the dimension originally dedicated to batching to compute the $c_o$ channels, so it cannot be used directly. We propose to circumvent this by using grouped conv2d implementation. By concatenating $g$ kernels and setting groups = $g$, we can compute the batched in parallel.\nEfficient implementation. By examining Definition 7, one observes that, depending on the values of $s$ and $k$, the parametrization can be simplified to either $K_{AOC} = K_{BCOP}$ or $K_{AOC} = K_{RKO}$. While BCOP is generally not suited for handling stride directly, we have identified specific cases \u2013 namely when $c_i < c_o$ - where stride can indeed be applied directly to a BCOP kernel (see Proof E.5) without requiring the full parametrization. Although not proposed in [31], this observation refines our overall characterization of BCOP's limitations with stride, showing that exceptions exist under certain conditions."}, {"title": "3. Evaluation", "content": "Scalability. As observed by [39], a method's implementation is a key factor for its success: a slow implementation leads to increased training time and, consequently, lower performances in practical contexts like robust training. In this section, we demonstrate that AOC offers a key advantage: its computational cost does not depend on the input size or shape, making it well-suited for large-scale datasets like ImageNet [17], where handling large images is crucial. Although other methods may perform better on smaller datasets such as CIFAR [30] or Tiny ImageNet [63], they struggle to scale to widely used architectures like ResNet-34 [24], as shown in Table 2. On the other end, our method's low memory cost enables larger batch sizes, and since our parameterization is batch-size independent, the overhead decreases as batch size increases. Ultimately, this results in a training time only 13% slower than its unconstrained counterpart.\nExperiments were conducted on a minimally modified ResNet-34 architecture, chosen for its compatibility with various orthogonal layers and its status as a standard benchmark for ImageNet training. The transition blocks were replaced by a single, strided convolution to maintain simplicity and ensure compatibility with existing orthogonal layers. For each method, we measured the average training and testing times over 100 batches and recorded peak memory consumption. Starting with a batch size of 128, we doubled the batch size incrementally until encountering an out-of-memory error. Each method's performance was compared to a standard convolution baseline, with results reported as overhead percentages. All experiments were performed on a consumer-grade computer equipped with two NVIDIA RTX 4090 GPUs.\nTo ensure a fair comparison, the hyperparameters of SOC and Cayley were set to their default values."}, {"title": "4. Conclusion and Broader Impact", "content": "We introduced AOC, a method for constructing orthogonal convolutions that supports essential features such as stride, transposition, groups, and dilation. Our results demonstrate that this layer is both expressive and scalable. Beyond its standalone benefits, our framework enhances existing layers: in Appendix C, we integrate our method with SLL [3] to build an efficient downsampling residual block, propose optimizations to reduce the memory footprint of SOC [50], and present a strategy to make Sandwich layers [61] scalable for convolutions. Also, although our experiments used AOC in isolation, we think this layer is intended to be seamlessly combined with other methods, such as SLL, where each approach's strengths can amplify the other's capabilities. We believe this work opens pathways for advancing modern convolutional architectures. In support of further research and development, we have made our implementation available at https://github.com/thib-s/orthogonium."}, {"title": "A. Applications of orthogonal convolutions", "content": "Orthogonal layers have become fundamental components in various deep learning architectures due to their unique mathematical properties, which benefit multiple applications.\nProvable Robustness with 1-Lipschitz Networks. Ensuring robustness against adversarial attacks is a critical challenge. Early work in this field [54] identified a link between a network's adversarial robustness and its Lipschitz constant, which led to the development of networks with a Lipschitz constant of one (1-Lipschitz networks). Initially, regularization techniques were used [12], but interest in constrained networks quickly grew. Orthogonal layers, in particular, have an inherent Lipschitz constant of one, as they preserve the input norm through each transformation. This property is instrumental in achieving provable robustness by allowing for certified bounds on the network's output perturbations in response to adversarial inputs [2]. By controlling the network's sensitivity to input changes, orthogonal layers play a crucial role in building models resilient to adversarial manipulations.\nEnhancing Performance in Normalizing Flows. Normalizing flows are a class of generative models that transform simple probability distributions into complex ones through a series of invertible and differentiable mappings. Although they have different objectives, this domain intersects with the field of provable adversarial robustness. For instance, [19, 42] employs a channel masking scheme, which was later used to emulate striding in Lipschitz layers designed for adversarial robustness. Separately, Lipschitz layers can be applied to build invertible residual networks [6]. In both fields, orthogonal convolutions are essential, as they facilitate the construction of invertible transformations with tractable Jacobian determinants. The use of orthogonal layers ensures that the Jacobian determinant is constant or easily computable, simplifying likelihood estimation during training [29]. This property enables efficient and stable training of normalizing flow models, leading to improved performance in density estimation and generative tasks.\nStabilizing Training in Deep and Recurrent Neural Networks. Training recurrent neural networks (RNNs) involves propagating gradients through time, which can lead to vanishing or exploding gradients due to the multiplicative nature of sequential weight applications. Orthogonal weight matrices in RNNs help preserve the gradient norm across time steps, thus preventing degradation of the learning signal [27]. By constraining recurrent weights to be orthogonal, the network maintains a consistent flow of information, enabling it to capture long-term dependencies more effectively."}, {"title": "B. Empirical evaluation of the Lipschitz constant of our method", "content": "Evaluating the Lipschitz constant of a network Beyond the creation of a constrained layer, the evaluation of the Lipschitz constant of a layer is by itself an active field: early work used fast Fourier transform to evaluate a lower bound of the Lipschitz constant of a convolutional layer with circular padding [45]. This work was later improved with a method that is quicker [46], supports other types of padding [22], or allows the extraction of a larger part of the spectrum [9]. The work of [15] [16] allows us to compute a certifiable upper bound efficiently under different types of padding. It is worth recalling that inferring the global Lipschitz constant of a network given the Lipschitz constant of each layer is an NP-Hard problem [59]. Then, [20, 37, 62] aim to tackle using SDP (Semi-definite programming) tools. Our work can also contribute to this issue as the orthogonal layer allows a tighter product bound (ie. bound using the product of the Lipschitz constant of each layer to evaluate the constant of the whole network).\nThe need for an empirical evaluation of the Lipschitz bound of AOC. Despite the theoretical guarantees ensuring orthogonality in our construction, empirical checks are necessary to confirm implementation correctness. Such verification prevents two types of issues:\nChecking of numerical Instabilities: Issues arising from floating-point precision, such as those introduced by small epsilon values added to avoid division by zero.\nChecking for implementation Discrepancies: Differences between mathematical formalism and its translation to popular frameworks (e.g., SOC proofs assume circular padding, while its implementation uses zero padding).\nChecking the orthogonality of a layer under stride, group, transposition, and dilation conditions. The numerical stability and the convergence of an orthogonal layer is dependent on the training hyper-parameters: mainly the number of iterations used in most methods, but the learning rate and weight decay can also play a significant role. We then need an evaluation method that scales along with the convolution and that can be used at the end of each training. On the other hand, as scalable methods can be imperfect, we also need a method that computes very precise bounds without making any assumptions on the layer parameters (like padding, or stride). In order to overcome this, we tested our layers with two distinct methods:\nExplicit SVD on Toeplitz Matrices: Using the impulse response approach, we construct the Toeplitz matrix for any padding and stride, allowing direct computation of singular values. This method, though accurate, is computationally expensive for large input images.\nProduct Bound for BCOP and RKO Kernels: The upper bound for the BCOP kernel is computed using standard methods, while the SVD of the reshaped RKO kernel is used for direct evaluation.\nUnit testing of the implementation. We used both of these two approaches in our unit tests. This enables us to ensure that the second method (which is faster and more scalable) is correct to check that our layer is effectively orthogonal. Also, our layer unlocks the use of the transposed convolution, which can be used to compute directly the equation of orthogonal layers:\n$(S_s\\tilde{K})(S_s\\tilde{K})^T = 1$ (row orthogonal)\n$\\texttt{conv}_K (\\texttt{ConvTranspose}(x, \\texttt{stride} = s), \\texttt{stride} = s)$\nNaturally, the other direction can also be verified for column orthogonal layers.\nTo follow the optimization depicted in fig 2c, we tested each branch independently. For each branch, we tested multiple values for kernel size, stride, dilation, input channels, and output channels. For the kernel size, along with standard configurations of 3 \u00d7 3 and 5 \u00d7 5 kernels, we also covered cases for 1 x 1 kernels and even-sized kernels. For input/output channels, we covered various values and all the inequalities discussed in this paper (for instance when $c_o > c_i s^2$). We ran similar tests for transposed convolution. As the computation of the singular values using the explicit construction of the Toeplitz matrix is quite expensive, we"}, {"title": "C. Using the content of this paper to improve Cayley, SLL, SOC, and Sandwish", "content": "In this section, we will explore how the content of this paper can be used to improve existing layers from the state of the art.\nImproving skew orthogonal convolution (SOC)[52] This method uses the fact that an exponential of a skew-symmetric matrix is orthogonal. The initial implementation builds a skew-symmetric kernel and computes the exponential convolution. However, without proper tools to compute the exponential of a convolution kernel, this exponential was computed implicitly for each input by using the Taylor expansion of the exponential (see eq 11).\nTheorem C.1 (Explicit conv exponential). We can use eq 4 to compute explicitly the exponential of a kernel K:\n$\\text{Id} + K * x + \\frac{K*K*x}{1!} + \\frac{K*K*K*x}{2!} + ...$\n$(Id + K + \\frac{KK}{2!} + \\frac{KKK}{3!} + ...)* x$\nEquation 12 shows that we can compute the exponential of a convolution kernel a single time, while the formulation 11 needs to be done for each input x. In other words, we can apply one conv instead of Niter convs. Note that the resulting kernel is then larger than the original one (as stated in table 1). In theory, this could unlock large speedups, but the gain is limited in practice as the implementation of convolution layers is optimized for small kernels and large images\nImproving SDP-based Lipschitz Layers (SLL) [3] SLL layer for convolutions, proposed in [3], is a 1-Lipschitz layer defined as:\n$y = x \u2212 2KT \u2217 (\u03c3(K \u2217 x + b))$\nNote that in the original paper, the equation is noted with product of two matrices $W^{T^\\frac{1}{2}}$, for convolutions it represents toeplitz matrix, i.e. $W^{T^\\frac{1}{2}} = K$.\nSLL layer does not natively support neither strides nor changes in the channel size. We propose to use the $\\circledast$ to derive a block, based on SLL, that supports stride and $c_i \\neq c_o$, and can replace the strided convolutions of the residual branch in architectures like ResNet.\nA natural first step is to append a strided convolution after a SLL block. This layer, $conv_{Kpost} SLL$, can then be fused in the SLL block thanks to Proposition 2.3:\n$y =Kpost *s (x \u2212 2KT \u2217 (\u03c3(K \u2217 x + b)))$\n$=Kpost *s X - 2(Kpost \\circledast KT) *\u03c2 (\u03c3(K * x + b)))$"}]}