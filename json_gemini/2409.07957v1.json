{"title": "Rapid Parameter Estimation for Extreme Mass Ratio Inspirals Using Machine Learning", "authors": ["Bo Liang", "Hong Guo", "Tianyu Zhao", "He Wang", "Herik Evangelinelis", "Yuxiang Xu", "Chang liu", "Manjia Liang", "Xiaotong Wei", "Yong Yuan", "Peng Xu", "Minghui Du", "Wei-Liang Qian", "Ziren Luo"], "abstract": "Extreme-mass-ratio inspiral (EMRI) signals pose significant challenges in gravitational wave (GW) astron-omy owing to their low-frequency nature and highly complex waveforms, which occupy a high-dimensionalparameter space with numerous variables. Given their extended inspiral timescales and low signal-to-noise ratios, EMRI signals warrant prolonged observation periods. Parameter estimation becomes particularly chal-lenging due to non-local parameter degeneracies, arising from multiple local maxima, as well as flat regions and ridges inherent in the likelihood function. These factors lead to exceptionally high time complexity forparameter analysis while employing traditional matched filtering and random sampling methods. To address these challenges, the present study applies machine learning to Bayesian posterior estimation of EMRI sig-nals, leveraging the recently developed flow matching technique based on ODE neural networks. Our approach demonstrates computational efficiency several orders of magnitude faster than the traditional Markov Chain Monte Carlo (MCMC) methods, while preserving the unbiasedness of parameter estimation. We show that machine learning technology has the potential to efficiently handle the vast parameter space, involving up to seventeen parameters, associated with EMRI signals. Furthermore, to our knowledge, this is the first instance of applying machine learning, specifically the Continuous Normalizing Flows (CNFs), to EMRI signal analysis. Our findings highlight the promising potential of machine learning in EMRI waveform analysis, offering new perspectives for the advancement of space-based GW detection and GW astronomy.", "sections": [{"title": "I. INTRODUCTION", "content": "Gravitational wave (GW) physics and astronomy, emerging in recent decades as a transformative branch of astronomy and cosmology, promise to revolutionize our understanding of the universe. The advent of GW observations marked the beginning of multimessenger astronomy, enabling concurrent insights from gravitational and electromagnetic signals. In 2015, the LIGO Scientific Collaboration marked a milestone with the first detection of GWs originating from the merger of two black holes [1]. Since this discovery, ground-based GW observatories, including LIGO and VIRGO, have cataloged nearly one hundred GW events from various binary systems, such as binary black holes, binary neutron stars, and neutron star-black hole pairs [2-7]. The sensitive frequency range of these ground-based detectors, constrained by seismic and shot noise, spans from 10 Hz to several kilohertz [7, 8]. Consequently, they are capable of observing binary systems with a total mass of up to several hundred solar masses.\nSpace-based GW detectors like the commissioned LISA project [9, 10], as well as future missions such as DECIGO [11], Taiji [12, 13], and TianQin [13, 14], are engineered to detect GW signals in the millihertz to decihertz bands. These bands contain a diverse array of sources that are often challenging to observe but are rich in scientific potential. Such sources typically involve strong-field dynamics and complex interactions within extreme environments, potentially carrying vast amounts of untapped information. These systems include double white dwarfs [15\u201318], coalescing massive black hole binaries [17, 19\u201321], extreme mass-ratio inspirals (EMRIs) [17, 22, 23], stochastic GW backgrounds [17, 24-26], and other unmodeled sources. Among these, EMRIs are particularly significant as primary detection targets for these observatories [27]. In an EMRI system, a stellar-mass compact object (the secondary) is captured by and gradually spirals into a central supermassive black hole (the primary)"}, {"title": null, "content": "over more than 105 orbits, emitting low-frequency gravitational radiation in the millihertz band. Due to the cumulative effects of thousands to millions of orbital cycles and the prolonged orbital evolution in strong-field and extreme environments that last several months to years, EMRIs provide an exceptional platform for probing strong gravitational fields and near-horizon physics, as well as for precise measurements of theoretical parameters [28, 29].\nIn the field of EMRI parameter estimation, several key challenges significantly hinder the effectiveness of traditional methods like MCMC. Firstly, the long-term nature of EMRI signals requires extremely precise and computationally intensive waveform modeling, demanding sustained analytical efforts over periods that can extend for months or years [30, 31]. Secondly, the high-dimensional parameter space inherent to EMRIs, typically consisting of more than a dozen interrelated variables, adds considerable complexity to the analysis [32\u201334]. Lastly, analysis of EMRI signals is often plagued by significant degeneracy, making it challenging to distinguish between different parameters of a given model [35\u201337], as well as confusion, which arises from the difficulty in discriminating between different GW models [36, 38, 39]. Both factors lead to significant challenges in identifying the model as well as its underlying physical parameters. Specifically, the non-local parameter degeneracies can be largely attributed to multiple local maxima, as well as flat regions and ridges inherent in the high-dimensional likelihood function. Traditional methods, particularly Markov Chain Monte Carlo (MCMC) [37, 40, 41], struggle with these challenges due to their inherent limitations. MCMC is not only computationally expensive, typically requiring the generation of approximately ~ 1040 waveform templates [32], but also inefficient in exploring vast parameter spaces. In practice, only two to four parameters can be accurately estimated, and this often requires setting the initial estimates near the true values [42-44]. This limitation undermines the ability to thoroughly explore the parameter landscape, frequently missing critical global maxima necessary for accurate parameter estimation. In response, several new tools are under development [36, 44-46]. However, at present, most methods are still incapable of meeting the stringent demands of EMRI parameter estimation, highlighting the need for more efficient and robust deep learning techniques.\nDeep learning has rapidly become an essential tool in scientific discovery, yielding significant advancements across a wide range of fields, from physics to biology [47, 48]. In the realm of GW astronomy, deep learning has proven particularly effective in data analysis, streamlining both the detection and parameter estimation of ground-based binary black hole (BBH) events [49\u201355]. Its application extends to space-based observations, where it has been instrumental in rapidly identifying and characterizing massive black hole binary (MBHB) events, contributing to a deeper understanding of these cosmic phenomena [56\u201358]. Additionally, the scope of machine learning techniques has broadened to include extreme mass ratio inspiral (EMRI) data analysis. Current research has made strides in denoising [59], detecting [60, 61], and performing point estimation of EMRI parame-ters [62], which are critical in interpreting these complex signals. However, despite these advancements, a significant challenge remains in the Bayesian posterior estimation of EMRI signals. This difficulty stems largely from the inherent complexity of EMRI data, characterized by protracted signal durations and intricate waveform structures, which continue to test the limits of existing machine learning methodologies. Overcoming this challenge requires a concerted effort to develop more sophisticated algorithms that can harness the full potential of deep learning for comprehensive EMRI analysis, pushing the boundaries of what is currently achievable in GW astronomy.\nIn this study, we develop a deep learning approach specifically tailored for the posterior estimation of EMRI signals by employing the recently developed flow matching technique based on ODE neural networks. Our method not only enhances computational efficiency, but also overcomes the issue of MCMC methods getting easily trapped in local optima during parameter estimation. Such an approach exhibits computational efficiency that surpasses traditional MCMC methods by several orders of magnitude, all while maintaining unbiased parameter estimation. It is demonstrated that the machine learning technology employed is capable of effectively managing the extensive parameter space, comprising as many as eleven variables, inherent to EMRI signals. Moreover, as the method can be adapted to a broader prior range to achieve unbiased parameter estimates, it significantly narrows the parameter space of Bayesian inference. As a result, our findings potentially provide an innovative solution for dealing with real EMRI signals in the future. Specifically, one first narrows down the parameter space of EMRI using the proposed machine learning procedure, and then samples the parameter space with MCMC methods using the posterior from the machine learning model as a prior. Such an approach combines the efficiency of machine learning with the precision of traditional Bayesian inference, offering an innovative solution for the analysis of EMRI signals.\nThe remainder of the paper is organized as follows. In Sec. II, we discuss the data generation and preprocessing, laying the foundation for the training and validation of the model. Subsequently, we elaborate on the machine learning algorithms specifically tailored for the Bayesian posterior estimation of EMRIs. The numerical results are presented in Sec. III, where our results are compared against the standard MCMC approach. Concluding remarks and future perspectives are given in the last section."}, {"title": "II. METHODOLOGICAL FRAMEWORK", "content": null}, {"title": "A. Data Generation and Preprocessing", "content": "In this section, we give a detailed account of the dataset preparation for model training and validation, which consists of the generation of EMRI waveforms, projection to detector responses, and a pre-processing step.\nA prominent challenge in the analysis of EMRI signal lies in the accurate and high-fidelity modeling of waveform template, which relies on the theories of gravitational self-force and black hole perturbation, etc [30, 31]. To achieve the sub-radian phase accuracy required by the scientific explorations via EMRIs [63], perturbative expansions of the binary metric have already been applied at the first order for general orbits around Kerr black holes [64], with significant progress also made in second-order calculations [65, 66]. However, the efficiency of numerical computations for gravitational self-force is insufficient to meet the demands of data analysis, hence the rapid generation of approximate \u201ckludge\u201d models have been developed [67\u201370]. Early analytic kludge (AK) models [67], while sacrificing some accuracy, offer significantly improved computational efficiency. Numerical kludge (NK) models [68, 71], on the other hand, achieve high-fidelity EMRI waveforms by fitting perturbative calculations. Augmented analytic kludge (AAK) models [33, 69, 70] incorporate the NK model, enabling the generation of high-precision EMRI waveforms without significantly increasing computational costs. To date, the most advanced computational framework named FastEMRIWaveforms (FEW) [33, 34] can rapidly compute fully relativistic EMRI waveforms in the time domain. As the first step of data preparation, we employ the AAK model implemented in FEW to generate the polarizations of EMRI waveforms.\nFor the second step, we adopt the generic time-domain response function FastLISAResponse [72] to project the polarizations into the detector responses of LISA, and the resulting EMRI signals are output in the form of second-generation Time Delay Interferometry (TDI) variables [73]. Each sample in the dataset includes the TDI-{A, E} variables with a duration of 2 years, calculated at a sampling frequency of 0.1 Hz. We employ the Graphics Processing Units (GPUs) to accelerate the computation of waveforms and LISA response functions [72], achieving the generation of each EMRI signal in less than one second [43]. The whole dataset consists of 500,000 samples, with the source parameters randomly drawn from the priors specified by Table I. In our research, we have come to deeply appreciate the importance of the effectiveness, reliability, and scalability of machine learning algorithms in achieving long-term research objectives. To ensure the realization of these goals, we have adopted a robust, step-by-step approach, continuously improving and optimizing based on existing research. In this process, we have carefully adjusted and compromised our prior assumptions. In particular, to enhance the accuracy of posterior estimation, we have reasonably restricted the prior range of some intrinsic parameters of EMRI signals. In subsection III, we have conducted a detailed discussion.\nTo reasonably reduce the dimensionality of data, we further incorporate a pre-processing step. Frequency domain analysis [43, 74, 75] proves to be an exceptionally effective method for estimating parameters in EMRIs. Therefore, the time-domain samples are converted to the frequency domain using the Fast Fourier Transform (FFT) algorithm. The data of EMRIs are inherently challenging to process due to their vast sizes [76], and this issue is even more pronounced for machine learning, for which the models are trained on batched data. After FFT, the length of each sample is up to millions, necessitating methods such as downsampling or pooling. To reduce the complexity of the data while still retaining the key information, we adopted a max-pooling strategy [77]. By setting a pooling window of 512 and a stride, we effectively reduced the length of each signal to"}, {"title": "B. Machine Learning Framework", "content": "We begin this section by reviewing our methodology, focusing on the use of CNFs for fast and accurate EMRIs parame-ter estimation. Currently, many successful attempts aimed at enhancing parameter estimation through machine learning have adopted the neural posterior estimation (NPE) approach [54, 57, 58, 78, 79] using discrete normalizing flows (DNFs) [80, 81]. In theory, the application of NPE to EMRIs is feasible. However, recent research in simulation-based inference (SBI) suggests that Flow Matching (FM) [82] with CNFs [83] is a more promising approach. This method is known as Flow Matching Posterior Estimation (FMPE) [53, 82]. FMPE offers a more rapid training process and superior accuracy compared to NPE [53, 58, 79].\n1. Continuous Normalizing Flows\nCNFs achieve the transformation from a base distribution to a complex target distribution via a vector field vt,x defined over continuous time. This transformation is characterised by an Ordinary Differential Equation (ODE) and is realized by integration. The vector field vt,x follows this ODE:\n$\\frac{d}{dt}\\phi_{t,x}(0) = v_{t,x}(\\phi_{t,x}(0)).$ (1)\nDuring the training of CNFs, we initiate from the base distribution and employ an ODE solver to derive the target distribution. To ensure consistency with actual data, train the model parameters by log-likelihood. However, due to the cost of integrating ODEs, the optimization process for maximum likelihood can become exceedingly costly, making it challenging to implement in practice [53].\n2. Flow Matching Posterior Estimation\nThe emergence of FM provides a new perspective for training CNFs. Dax et al. [53] have applied the FM to simulation-based inference, offering a more efficient training objective that helps address the computational challenges inherent in traditional methods. The loss function of FMPE is given by the following equation:\n$L_{FMPE} = E_{t\\sim p(t),\\theta_1\\sim p(\\theta),x\\sim p(x|\\theta_1),\\theta_1\\sim p_t(\\theta,\\theta_1)} || v_{t,x}(\\theta_1) \u2013 u_t(\\theta_1|\\theta_1) ||^2 .$ (2)\nIn the FMPE approach, the vector field vt,x(\u03b81) is responsible for generates the path to the target probability distribution, while ut(\u03b81|\u03b81) adjusts this path based on the conditions of the sample. By defining a Gaussian path with time-dependent parameters, FMPE ensures that at t = 0 and t = 1, the probability path closely approximates the base and target distributions.\n3. Application of Continuous Normalizing Flows\nIn order to accurately estimate parameters for EMRIs, we have conducted customized training on our model. Our model is based on CNF. It is optimized using FMPE technology. The model framework diagram is shown in Figure 1. The model has two"}, {"title": null, "content": "parts: (1) The first part is a linear compression network Nx, designed to effectively capture the intrinsic structure of the signal by compressing the data x into a 1024-dimensional feature space. This process not only simplifies the data processing workflow but also enhances the model's ability to capture signal features. (2) The second part of the model is a flow network composed of 56 residual blocks with decreasing sizes, ranging from 4096 dimensions down to 10 dimensions. It receives the embedded tuple (x, t, \u03b81) and predicts the vector field vt,x(\u03b81). We trained the model for 100 epochs, with a batch size of 1024 samples per epoch. The initial learning rate was set to 0.00005, and we employed a cosine annealing strategy to gradually reduce the learning rate to zero as training progressed. The training was conducted on an NVIDIA A800 GPU, the entire training process took approximately 1 hour. Once our model is trained, the inference process for a single EMRI signal can be completed in just tens of seconds."}, {"title": "III. NUMERICAL RESULTS", "content": null}, {"title": "A. Bayesian inference", "content": "Bayesian statistical inference along with MCMC sampling have been widely adopted in GW source detection and parameter estimation, and is nowadays one of the benchmark approaches in this area, even for the global analysis of multiple sources [84, 85]. Since FMPE is designed to simulate the Bayesian posterior distribution of parameters, its performance could be more clearly demonstrated if the Bayesian results for the same event are presented for comparison. The framework of Bayesian statistical inference is built on the Bayes' theorem, which, neglecting the normalization factor [43], can be expressed in logarithmic form as:\n$\\log p(\\theta|d) = \\log p(d|\\theta) + \\log p(\\theta).$ (3)\nThe first term on the right hand side log p(d|\u03b8) represents the likelihood function. The likelihood function describes the proba-bility of observing the data stream d given the parameters \u03b8, which takes a Whittle form under the assumption of Gaussian and stationary noise [86]. The second term log p(\u03b8) is the prior probability distribution of the parameters \u03b8 given in Table I.\nIn the parameter estimation of EMRI signals, we face challenges caused by non-local parameter degeneracies, which lead to multiple local maxima on the likelihood surface. These local maxima make the EMRI signal extremely sensitive to small variations in parameters, thus accurately identifying the global maximum in the likelihood function becomes particularly difficult. Currently, when estimating EMRI parameters based on the MCMC method [43, 87, 88], it is usually necessary to search within a small vicinity of the true parameter values, or to infer only a few parameters of the EMRI.\nTo highlight the potential and advantages of machine learning methods in EMRI parameter estimation, we implement the conventional Bayesian method via an advanced MCMC sampler Eryn [88] based on the Emcee [89] code to sample from the posterior distribution, which is one of the widely used tools in the current space-base GW data analysis. We performed Bayesian inference on the injected EMRI signals to test the performances of both MCMC and our machine learning methods. We made two settings for MCMC tests, only differing in the choices of starting points [42]:\n1. MCMC (1): The starting points were selected within a very small neighborhood of the true parameter values (within a range of \u00d710-7 times the true values);\n2. MCMC (2): The starting points were randomly sampled according to the priors.\nBy running MCMC according to the setting (1), we obtain the posterior distributions of parameters shown in Figure 3, where the contours from inside to outside represent the 1 \u03c3, 2 \u03c3, and 3 o confidence intervals, respectively, and the black lines indicate the true parameters underlying the injected signal. Unsurprisingly, all the median values of parameters are perfectly coincident with the truths, demonstrating that when the starting points are close to the true values, MCMC can efficiently and correctly identify the injected signals. While, given that in practical data analysis we do not have prior access to the true parameters, we explored the capability of MCMC based on the setting (2), which involves randomly sampling starting points from the prior distribution. The results of the second setup are shown in Figure 4. For this case, the MCMC method failed to recover the true parameters, with most of the \"posteriors\u201d clustered near the boundaries set by the priors. Figure 4 also shows the convergence plot of the MCMC chains, indicating that the MCMC ultimately only converged to a local optimum and failed to find the global optimum [32, 37, 76]. Both settings of the MCMC method utilized GPU acceleration to enhance computational speed, and they still required approximately two days to complete the posterior estimation of EMRI signals. Although this issue may be resolved by, for example, incorporating the parallel tempering mechanism [90] to escape from local maxima, while this would imply at least a several-fold increase in the computation time, making its practical application still less preferable."}, {"title": "B. The present approach", "content": "To evaluate our machine learning model, we first tested it on 1,000 EMRI signals randomly generated accroding to the priors. By drawing posterior samples of these signals using our model, we constructed a P-P plot shown in Figure 2. In principle, the cumulative distribution function (CDF) of the posterior percentile values for each parameter should approximate the diagonal line, and is just the case of our results, thereby validating the unbiasedness of our estimations over the whole parameter range in consideration.\nIn comparison to the MCMC method, we also performed parameter estimation on the same EMRI signal as Sec. III. Since our model can not exploit the advantage of knowing the true parameters priorly, its application scenario is essentially identical to MCMC (2) (i.e. the more realistic one). As is shown in Figure 5, our model successfully reconstructed the posterior distribution of the signal, with all the true parameters lying within the 1 \u03c3 (or at most 2 \u03c3) ranges. This is in stark contrast to the result of MCMC (2) (Figure 4), where the estimations are severely biased. This comparison provided us with an intuitive overview on the performance of both methods. FMPE exhibits a strong capability to escape from local maxima, and this is achieved in a very short time scale. To some extent, this may suggest that FMPE has the potential to become a truly viable methodology in terms of both efficiency and accuracy in the area of EMRI posterior estimation.\nTo further assess the reliability of our model in terms of reconstructing posterior distributions, it is helpful to compare with MCMC (1), whose posterior might be regarded as a benchmark. Our machine learning process typically generates posterior distributions with a broader range than those of Eryn, however it has effectively narrowed down the parameter ranges compared to the priors, especially for the sky position parameters (as can be seen in the first peaks of Os and $s), making it possible to perform rapid localization of EMRIs via our model. Besides, although the phenomenon of multiple peaks is observed in the parameters Os and s, which is not the same as the results of Eryn searching near the truth value. We believe that this phenomenon is reasonable because Eryn searches near the truth value and may not have explored all potential peaks in the parameter space.\nThrough the above comparisons, by integrating the features of MCMC and machine learning, we can gain insightful ideas to further refine the results of both methods. Looking ahead, in dealing with real EMRI signals, we can adopt the following strategy: first, use the machine learning process to quickly narrow down the range of the parameter space, and then apply the MCMC method for more refined sampling, using the posterior distribution of the machine learning model as a prior. This approach combines the efficiency and the ability to break local maxima of machine learning with the precision of Bayesian inference, providing an innovative solution for the analysis of EMRI signals."}, {"title": "IV. CONCLUDING REMARKS AND PERSPECTIVES", "content": "The present study applies machine learning algorithms to EMRI posterior estimation, offering a novel approach for Bayesian parameter estimation of EMRI signals. Our method not only provides a comprehensive statistical framework for analyzing EMRIs but also establishes a new benchmark in space-based GW astronomy. We have demonstrated the significant potential of machine learning in handling the low-frequency characteristics and vast parameter space of EMRI signals. Compared to traditional MCMC methods, our machine learning approach efficiently explores a broader parameter space, including regions that are challenging for conventional methods to reach. In terms of computational speed, our method delivers an order-of-magnitude improvement over MCMC techniques. This work provides new insights into enhancing existing parameter estimation methods, particularly in addressing complex signals characterized by non-local parameter degeneracies and multi-harmonic overlap. As machine learning technology continues to advance, we anticipate it will play an increasingly critical role in space-based GW detection, driving further progress in GW astronomy. Additionally, our findings suggest a promising approach for processing future real EMRI signals. Specifically, machine learning could be used to narrow the parameter space, followed by MCMC methods, where the prior likelihood distribution is informed by the machine learning-derived posterior. This strategy effectively combines the efficiency of machine learning with the precision of Bayesian inference, providing a substantially powerful tool for EMRI signal analysis."}]}