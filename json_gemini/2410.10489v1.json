{"title": "CULTURAL FIDELITY IN LARGE-LANGUAGE MODELS: AN\nEVALUATION OF ONLINE LANGUAGE RESOURCES AS A DRIVER\nOF MODEL PERFORMANCE IN VALUE REPRESENTATION", "authors": ["Sharif Kazemi", "Gloria Gerhardt", "Jonty Katz", "Caroline Ida Kuria", "Estelle Pan", "Umang Prabhakar"], "abstract": "The training data for LLMs embeds societal values, increasing their familiarity with the language's\nculture. Our analysis found that 44% of the variance in the ability of GPT-40 to reflect the societal\nvalues of a country, as measured by the World Values Survey, correlates with the availability of digital\nresources in that language. Notably, the error rate was more than five times higher for the languages\nof the lowest resource compared to the languages of the highest resource. For GPT-4-turbo, this\ncorrelation rose to 72%, suggesting efforts to improve the familiarity with the non-English language\nbeyond the web-scraped data. Our study developed one of the largest and most robust datasets in\nthis topic area with 21 country-language pairs, each of which contain 94 survey questions verified by\nnative speakers. Our results highlight the link between LLM performance and digital data availability\nin target languages. Weaker performance in low-resource languages, especially prominent in the\nGlobal South, may worsen digital divides. We discuss strategies proposed to address this, including\ndeveloping multilingual LLMs from the ground up and enhancing fine-tuning on diverse linguistic\ndatasets, as seen in African language initiatives.", "sections": [{"title": "1 Introduction", "content": "In the digital age, human communication, and thus language, is increasingly filtered through technology and the internet.\nThese digital means are not neutral mediums, but rather influence the language and our relationship with it in a variety\nof ways (Lee and Ta, 2023). Most-affected by this trend are so-called low-resource languages (LRLs) which despite\nbeing the vast majority of the world's 7,000 languages, hold a reduced digital presence and thus are increasingly facing\nerosion (Lee and Ta, 2023).\nIn the literature, what has often most defined LRLs is that they are not one of the '20 languages' that have been focused\non in natural language processing methods, such as English (which holds half of the world's websites), Mandarin, and\nFrench (Magueresse, Carles, Heetderks, 2020; Lee and Ta, 2023). This language categorization reflects the simplistic\ndivision of most countries into the Global South, much like how the Global South is often defined by what it is not: the\nGlobal North. In both cases, these categories are characterized as the 'other,' making them less clearly defined. Similar\nto Joshi et. al.(2020), we utilize a methodology for understanding the degree of a language's resources in table 2 using\nthe proportion of online websites in that language as the key metric."}, {"title": "Cultural Fidelity in Large-Language Models", "content": "Low representation in digital text means lower capacity for utilization as training datasets for Large-Language Models\n(LLMs) and chatbots, resulting in lower quality Artificial Intelligence (AI) models, even if a system can be trained\non this language at all (Magueresse, Carles, Heetderks, 2020). The scarcity of digital text means that these languages\ncannot be easily ported to AI models, therefore, dominant languages compound in strength through increased use in the\ndigital realm while more minor languages face greater corrosive pressures (Lee and Ta, 2023). Our results in Section\n4.1 demonstrate that a substantial proportion of an LLM's ability to mimic societal values can be correlated to the\navailability of digital text in that language. This has wide-ranging implications.\nWith government services increasingly reliant on chatbots and other human-computer interfaces, even basic functions\nnecessary to maintaining citizen engagement will be AI-dependent, and thus LRL communities would have to shift\ntowards dominant languages to maintain these interactions or else be limited in their digital engagement (Jungherr,\n2023).\nMirroring colonization, these foreign-language AI models are forcing assimilation toward dominant languages that are\noften the language of the colonizer such as English, French, or Spanish (Lee and Ta, 2023). In societies that strongly\nidentify with their local languages as part of their national identity, such as Paraguay with Guarani, this creates cultural\nunease over the loss of heritage (Al Qutaini, et. al., 2024). This further-intensifies the inequality between and within\ncountries, as more dominant cultures and languages do not suffer the same degradation. A detailed discussion of these\npotential harms is discussed in Section 5."}, {"title": "2 Literature review", "content": ""}, {"title": "2.1 Performance of GPT in non-English languages", "content": ""}, {"title": "2.1.1 GPT-40", "content": "GPT-40, trained on data up until October 2023, has been documented to show significant advancements in handling\nnon-English languages compared to previous models, according to OpenAI's official documentation. While GPT-40\nmatches the performance of GPT-4 Turbo on English text and code, it notably exhibits marked improvements in\nprocessing non-English languages. These enhancements are largely due to the introduction of a new tokenizer that\noptimizes the model's efficiency in handling non-English text. For instance, the tokenizer reduces the number of tokens\nrequired for Hindi by 2.9 times\u2014from 90 to 31\u2014and achieves similar efficiency gains across other languages, such as\nGerman (1.2x fewer tokens), English (1.1x fewer tokens), and Vietnamese (1.5x fewer tokens). These reductions not\nonly streamline processing but also reduce the cost and time associated with non-English language tasks, which makes\nGPT-40 50% cheaper to use via the API.\nOur findings in Section 4.5 support the claims of OpenAI for improved performance in non-English languages, as\nmeasured by value representation accuracy. However, we do note an interesting increase in error-rate for high-resource\nlanguages. This is particularly true for English (USA), which can be partially explained by the previously high reliance\nof 4-turbo on data from this language as described in the following section."}, {"title": "2.1.2 GPT-4-turbo", "content": "The GPT-4 model was reported to surpass previous language models in multilingual capabilities, according to Ope-\nnAI's official technical report (2023). To evaluate GPT-4's performance in different languages, the OpenAI team\nutilized the Multi-task Language Understanding benchmark-a collection of multiple-choice questions covering 57\nsubjects-translated into many different languages using Azure Translate. The report states, \"GPT-4 outperforms the\nEnglish-language performance of GPT-3.5 and existing language models (Chinchilla and PaLM) for the majority of\nlanguages we tested, including low-resource languages such as Latvian, Welsh, and Swahili.\" In these tests, English\nachieved an 85.5% accuracy score on the MMLU, while Swahili reached 78.5%, German 83.7%, Latvian 80.9%, and\nWelsh 77.5% (OpenAI, 2023, p. 8).\nHowever, the report also notes that the majority of the training data for GPT-4 was sourced from English, and the\nfine-tuning and evaluation processes were primarily focused on English, with a US-centric bias. The report notes,\n\"Mitigations and measurements were mostly designed, built, and tested primarily in English and with a US-centric\npoint of view. The majority of pretraining data and our alignment data is in English\" (OpenAI, 2023, p. 21). This aligns\nwith our findings in Section 4.5 which highlight a much greater correlation between websites in a language and model\nperformance in mimicking values for 4-turbo (72%) than for 40 (44%)."}, {"title": "2.2 LLMs and societal value representation", "content": "A paper published by Anthropic researchers (Durmus et. al., 2024) comes closest to our approach wherein they\ninvestigate LLM responses to global opinion questions, derived from the Pew Global Attitudes Survey (PEW) and\nthe World Values Survey (WVS), and compare to which extent the LLM's answers align with the opinions of certain\npopulations. The paper finds that LLMs responses tend to be more similar to the opinions from the USA, and some\nEuropean and South American countries. When the model is prompted to consider a particular country's perspective,\nresponses shift to be more similar to the opinions of the prompted populations, but can reflect harmful cultural\nstereotypes.\nWhile this study demonstrates the existence of value bias/inaccuracy in LLMs when addressing value-laden questions,\nit does not delve deeply into the underlying causes, particularly the role of a language's resource. Although our\nconclusions are similar, our contribution lies in exploring likely origins of this value bias/inaccuracy and its correlation\nwith the availability of digital text in a given language. Moreover, the paper's reliance on a language model to translate\nthe questions can lead to differences between the translated and original questions (as noted by the authors). This gap\nbetween the questions poised to the LLM and those originally given to human respondents might necessitate differing\nresponses which would artificially inflate the inaccuracy score.\nOther authors have also investigated language models' representation of societal values. Arora, Kaffee, and Augenstein\n(2023) found that pre-trained language models do capture differences in values across cultures, but that these differences\nare not sufficiently defined to reflect cultures' values as reflected in established surveys. Kharchenko, Roosta, Chadha,\nand Shah (2024) similarly applied a quantified methodology for representing a country's values based on the 5 Hofstede\nCultural Dimensions. Vimalendiran (2024) applies the Inglehart-Welzel Cultural Map, an academic measure of cultural\nvalue grouping, to derive most models to be closely aligned with value-sets commonly found in English-speaking and\nProtestant European countries. Santurkar et. al. (2023) utilized a similar approach as this paper with a focus on US\npublic opinion and surveys to demonstrate the left-leaning tendencies of some human feedback-tuned language models.\nLow-resource language environments are also made more complex by the practice of code-switching, intermixing with\nlanguages that are typically higher-resource (such as English) within the same piece of text. Ochieng et. al. (2024)\nfound that LLMs on average struggled with cultural nuances to mimic a clearer understanding of these mixed-language\ncontexts."}, {"title": "3 Methodology", "content": "Overview of methodology:\n1. Country-language pairs are selected from the World Values Survey, for which a range of questions are\ntranscribed and verified by native speakers. An average of a country-language pair's answers for each language\nare collected. These serve as original results.\n2. The same questions from the World Values Survey are put to GPT-4, specifying the country and displaying the\nquestion in the language of interest. These serve as generated results.\n3. The difference between the original and generated results are measured. If the absolute difference is greater\nthan, or equal to, 50% of the original value, then that question is counted as an error. The percentage of\nquestions within a country-language pair that cross the 50% threshold sets that pair's overall error-rate.\n4. The resource availability of a language is defined through the proportion of online websites. This acts as a\nproxy for our main explanatory variable.\nSteps 1 to 3 are summarised in the model architecture diagram found here 1."}, {"title": "3.1 World Values Survey: selecting questions and languages", "content": "The World Values Survey (WVS) is a global research initiative that investigates people's values and beliefs, tracking\nhow they evolve over time across various domains. Since its inception in 1981, the WVS has been conducted in nearly\n100 countries through national surveys. The questionnaires are largely uniform across countries, with only minor\nmodifications (e.g., certain questions may be omitted in some countries). These national surveys are administered in the\nofficial language(s) of each country. For our paper, we utilized data from the latest WVS, Wave 7, which was conducted\nbetween 2018 and 2023 in 66 countries. Each country is surveyed once per wave, using random probability samples\nthat are representative of the adult population.\nWe chose to use WVS data for our study because it is a major survey that collects data on political and ethical values on\na global scale, with the same set of comparable questions asked across different countries."}, {"title": "3.1.1 Question selection", "content": "When selecting questions for our analysis, we focused on those that could be reasonably posed to a large language\nmodel (LLM) to simulate the values and opinions of a citizen of a specific country. For instance, we excluded questions\nrelated to respondents' happiness, well-being, trust in neighbors, or factual knowledge (e.g., identifying the members\nof the UN Security Council), as these vary by personal experience and are less-related to the values that a country's\ncitizens are likely to share. Instead, we included questions that focus on political, social, or ethical values. These\nquestions ask respondents to express their level of agreement or disagreement with specific statements, using scales\nranging from 0-2 to 1-10."}, {"title": "3.1.2 Language selection", "content": "When selecting languages for our analysis, we aimed to include a diverse range of geographical regions and societies,\nensuring representation of different cultures, religions, and values. Additionally, we focused on languages that were\nused in a significant number of responses. In some countries surveyed by the WVS, the population was polled in\nmultiple languages, leading to a lower number of responses in certain languages. To maintain robustness in our analysis,\nwe included only those languages for which data was collected from at least 175 respondents.\nFurthermore, we particularly targeted languages that are predominantly spoken in one country. This was borne from the\nunderstanding that a language shared by many countries, such as Spanish, might have highly variant values encoded in\nthe training dataset given the multiple sources and thus influence the LLM's response. To better-isolate the relationship\nbetween the digital availability of a language and the accuracy of the LLM, languages that are largely unique to a\ncountry surveyed by a WVS were chosen (e.g., Indonesian, German, Burmese, Urdu, Filipino, Amharic, Hausa, Shona).\nThe methodology for classifying languages as low-resource can be found in Section 3.4."}, {"title": "3.1.3 Limitations and discrepancies", "content": "After selecting the questions and languages, we asked native speakers of each language to compare the English WVS\nquestionnaire with the version used in their native language. This extra step was taken to ensure that when we later\nintroduced questions to our LLM in English versus other languages, the questions would be as consistent as possible.\nWe did not expect the non-English WVS questionnaires to be literal translations of the English version, as direct\ntranslations often fail to capture important contextual nuances. However, we asked our interpreters to identify any\nsignificant discrepancies between the English and non-English versions\u2014such as questions being framed in a more\nleading way or the use of stronger language in other languages. The interpreters highlighted these differences, however,\nwe did not alter the actual questionnaires, as the survey data in each country was based on those original questions.\nNevertheless, it is worth highlighting some findings of our interpreters:"}, {"title": "Cultural Fidelity in Large-Language Models", "content": "In general, we included any question that may have a discrepancy between that language's version and English, whilst\nnoting the inconsistency in our dataset. The questions were consistently ordered across country surveys, except for in\nJapan which required a reordering by the translator to align with other questions.\nThrough this verification process, decisions were made to reduce the dataset to a more comparable and robust set:\n\u2022 5 questions that referenced the specific direction of writing of the language's script were excluded as this\nmight confuse the language model.\n\u2022 The survey in the People's Republic of China contained 20 discrepancies, of which two were linguistic\ndifferences and 18 were missing. As we eventually excluded Mandarin Chinese from our results, we were able\nto include the results from these questions into the analysis. Earlier findings on this country-language pair and\na discussion of reasons driving an inordinately high inaccuracy rate can be found in A.\n\u2022 17 questions were excluded as they were not included in at least one country's results - these included results\nfrom Iran, Zimbabwe, Tajikistan, and Nigeria.\nAs the core of our analysis was to gauge the difference of inaccuracy between original and generated results for specific\ncountries, differences in survey questions between countries was not a sufficient reason for excluding results. However,\nthese discrepancies in question wording were still noted in our dataset to enable additional analyses later on when\ncomparing results between countries. Excluding such questions would have significantly reduced the number of usable\nquestions. Additionally, the interpretation of \"significant differences\" in translation can vary among interpreters, leading\nto potential inconsistencies in which questions might be excluded based on subjective judgments. However, it is\nimportant to acknowledge this limitation in our analysis, as the LLM's responses may be influenced by how a particular\nquestion is phrased in different languages."}, {"title": "3.2 Generation of Language Model Value Representation", "content": "Original results from the World Values Survey were grouped by the language of the interview and averaged according\nto the weighted responses given to each question. The responses in these surveys ranged in scales, from 0-2 to 1-10, and\ndirectionality, lowest number being most agree in some cases or strongly disagree in other cases. To offer comparable\nand numerical generated results, the LLM was prompted to respond to each question according to the original scale and\ndirectionality.\nDrawing from the results in Durmus et. al. (2023) and Kharchenko, Roosta, Chadha, and Shah (2024) which highlight\nthat country-specific persona prompting results in steerable model values that more-closely reflect the country's values,\nthe prompt was engineered to refer to ask the LLM to respond as if a citizen of that country."}, {"title": "Cultural Fidelity in Large-Language Models", "content": "3.3 Accuracy of Value Representation: Comparison between Original and LLM Generated Values\nIf the generated result from the LLM deviated by more than 50% from the original answer, then the result was counted\nas an error. An original result of 4/10 and a generated result of 6/10 or 2/10 would result in an absolute difference of\n2/10, which is 50% of the original value. This would result in that question being counted as an error. The percentage of\nquestions within a country-language pairs that counted as an error would set that pair's overall error-rate, i.e., the degree\nto which the LLM performed poorly in mimicking its societal values. This threshold for error is set arbitrarily, but\nother values are graphed in Section 4.2. Limitations to this approach, including confining the LLM to closed-answer\nquestions, are discussed in Appendix C."}, {"title": "Cultural Fidelity in Large-Language Models", "content": "3.4 Low-Resource Languages\nWe included a balanced mix of low- and high-resource languages in our sample. To classify languages as low- or\nhigh-resource, we employed the following methodology:\nTaking the percentage of online content available in a given language, those accounting for less than 0.1% of online\ncontent (measured through Web Technology Surveys, 2024) are classified as low-resource. There is no universally\ndefined cutoff for categorizing a language as low-resource. However, according to Web Technology Surveys (2024),\nonly 38 of the more than 7,000 languages exceed the 0.1% threshold for web content representation.\nTo understand the severity of impact in the real world of the language's resources, we introduced a second methodology.\nThis approach considers the ratio of the percentage of the world's population that speaks a language to the percentage\nof online content available in that language. To calculate this ratio, we first determined the proportion of the global\npopulation that speaks a given language as their native language (L1). We then divided this figure by the percentage of\nwebsites available in that language. We also decided on a 0.1 cutoff on this ratio to classify a language as high vs low\nresource. One limitation of this methodology is that it does not account for languages with a significant number of\nsecond-language (L2) speakers, such as English, Swahili, or Indonesian.\nIn total we selected 21 languages for our analysis which draw on a total of 29,590 respondents in respective surveys\nfor the WVS. Out of these 21, 8 can be considered more low-resource and 13 considered more high-resource. These\nlanguages are detailed in table 2. Mandarin Chinese was excluded for reasons outlined in Appendix A, but is included\nin this table for illustrative purposes."}, {"title": "Cultural Fidelity in Large-Language Models", "content": "*It is important to acknowledge that there is no universally accepted definition in research to distinguish between low-\nand high-resource languages. Joshi et. al. (2020) introduce six classes for distinguishing between languages, using a\nmixture of labelled and un-labelled digital text availability to describe similar characteristics of languages within each\nclass. However, under both methodologies applied in our paper, languages end up falling into the same category of low\nvs. high resource. Rather than treating the classification as a strict binary, it should be viewed as a spectrum. As later\ndemonstrated in our results, the more resources a language has under either methodology, the less inaccuracy (error) in\nmimicking those values is observed.\nKaplan et al. (2020) famously noted the power-law relationship between model performance and factors such as the\nsize of the training dataset, model parameters, and compute used for training. We have chosen to use log base 10 of the\nnumber of online websites as a proxy for the non-linear impact that the availability of digital content (the number of\ntokens in a given language) has on a model's ability to approximate an understanding of the society associated with that\nlanguage."}, {"title": "4 Findings", "content": ""}, {"title": "4.1 Language resource and error-rate in representing societal values", "content": "Our main finding is that approximately 44% of GPT-40's ability to mimic an understanding of a society's values\nis correlated to the language's online presence. While the source of training datasets for this model are unknown,\nthese findings align with the knowledge that Common Crawl and publicly available data were a large source for the\nmodel's training (OpenAI, 2024). The outlier performance of higher-resource languages in English, German, and\nJapanese could be a sign of additional fine-tuning on language-specific datasets for these societies.\nSwahili and Hindi demonstrate interesting outliers for lower-resource languages, and it can be hypothesized that this is\ndue to the prevalence of English as a language in Kenya and India, respectively."}, {"title": "4.2 Proportion thresholds", "content": "The threshold for a question counting as an error was set in this paper at 50%. In other terms, an error would be counted\nif the LLM-generated answer was more than 50% different than the original average answer for the country-language\npair in the World Values Survey.\nGiven 50% is an arbitrary cut-off, the correlative strength of a language's resources and the error-rate are provided\nat different thresholds below. These demonstrate that the relationship becomes stronger until around 60% and then\ndeclines. Given the range of posssible answers are finitely defined within small scales (e.g., 0-2), there is an upper\nbound for how 'wrong' an LLM can represent values, and thus this decline in strength for the upper thresholds is\nintuitive.\nFurthermore, it is notable that the lower thresholds (e.g., 10%) demonstrate that nearly all languages are exhibiting high\nerror-rates, but that the relationship between error-rate and a language's resource is weaker at these lower thresholds. Put\nsimply, LLMs make errors in representing societal values across all country-language pairs, but they demonstrate\nmore significant errors if the language is lower-resource."}, {"title": "4.3 Topic areas", "content": "Most topic areas exhibited similar error-rates across lower- and higher-resource languages. However, three categories\ndemonstrated differentiated error-rates with higher inaccuracies for lower-resource languages:\n\u2022 Ethical values and norms\n\u2022 Security\n\u2022 Political culture and political regimes"}, {"title": "4.4 Controversy and error-rate", "content": "Controversy, measured as the standard deviation of responses from the original WVS responses in that language, was\ncalculated at the question-level for each country-language pair. It was hypothesized that more controversial topics may\nexperience lower error-rates driven by these topics/questions being more often discussed in online forums which would\nincrease the frequency of examples in the relevant training dataset for the LLM; Salminen et. al., (2020) discuss this\nphenomenon in relation to \u2018online toxicity' and the topic being covered in media.\nThis did not turn out to be the case, however, and only negligible correlation was found between the controversy score\nand error-rate, whether this was measured at country-language or topic-level for each country-language pair. The high\ncontroversy score for English responses is notable and highlights the diversity of values within this very high-resource\nlanguage."}, {"title": "4.5 Differences across models", "content": "The relationship between online resources and accuracy in value representation was even stronger in GPT-4-turbo,\nimplying a greater reliance on webscraping methods. 72% of the variance at language-level was captured by the log of\nnumber of websites, compared to 44% for GPT-40. This supports the claim made in 2 that new datasets and methods\nwere used to train GPT-40 and derive stronger performance in non-English languages.\nHowever, while improvements are marked for low-resource languages, the picture is more complex when considering\nhigh-resource languages and English (USA) in particular."}, {"title": "Error-rate Between 4-turbo and 40, by Resource Language Category", "content": "This suggests a differentiated change in model performance at value representation for low- and high-resource languages\nwhen OpenAI introduced GPT-40. Through a linear regression utilizing an interaction term for low-resource-languages\nand model of choice associated with the change in error-rate. The results show that low-resource-languages saw, on\naverage, a 10 percentage point decrease in their error-rate when switching from GPT-4-turbo to GPT-40, when compared\nwith high-resource languages.\nError-rate = \\beta_0 + \\beta_1 High-resource language\n+ \\beta_2 4-turbo + \\beta_3 (High-resource language \u00d7 4-turbo) + \\epsilon"}, {"title": "5 Implications", "content": "Our findings on the intensification of LLM inaccuracy in low-resource language settings may have significant broader\nsocietal implications. Generative AI (Gen-AI) tools based on LLMs are becoming an important tool of public and private\nservice delivery, with use cases in industries ranging from news and media, public services, to medicine. More and\nmore, we see daily useful knowledge consumed by proficient internet users being intermediated by LLM applications.\nGen-AI tools are beginning to play the role of a content creator, advisor, and decision maker (Bhardwaj et al. 2023].\nThe risk is not just that culturally biased/inaccurate LLMs perpetuate flawed information, but that they make or lead\nusers to make decisions that are suboptimal or even harmful to society. This is particularly concerning as Gen-AI\nbecomes deeply embedded across the digital ecosystem, and where these values become embedded in a long chain of\nactions where the origin and impact of bias becomes near impossible to uncover by users. A list of potential use-cases\naffected by this finding is detailed below, with our approach outlined in B.1."}, {"title": "5.1 Implications for LLM performance in common use-cases", "content": "The full extent of the harmful effects of culturally biased LLMs cannot fully be articulated ex-ante, and will range from\npositive to benign to harmful. For those monitoring the subject, we see the following framework emerging, guided by\nthree key questions: What are the major large scale use cases of LLM based generative AI? In which of these settings\ndoes the cultural alignment of the AI tool have a significant impact on its output and decision? In which of these cases\ndoes the output have a meaningful and measurable impact on social good?"}, {"title": "Cultural Fidelity in Large-Language Models", "content": "Using this guide, a few key use cases (Rauser, 2023) of LLM applications that have high social risk emerging from\ncultural misalignment of AI emerge. We highlight here illustrative examples of harmful effects resulting from these\napplications below:\n\u2022 Allocation of resources\nRecruiting: Automated screening processes that look for keywords associated with characteristics that\nare underrepresented or not as valued in low-resource settings, could lead to prejudice against talented\ncandidates from these settings (Li et al. 2023).\nMedicine: Treatment decisions in low-resource settings that are based on participant data and behaviours\nfrom high-resource settings, leading to prescription of incorrect or ineffective treatment options for\nusers from low-resource settings. Similar effects have been identified in AI algorithms used for patient\nclassification leading to significant bias against black patients (Z. Obermeyer et al. 2019).\n\u2022 Teaching and learning\nQueries on values: proliferation of values and opinions at odds with prevailing attitudes of a country,\nleading to social and cultural conflicts and discontentment, and potential global cultural homogenization\n(Cao, Lee et al. 2023).\nHistory: sharing historical information that is viewed with a western lens, which can either erase or\nreshape the understanding of events in low-resource settings that do not have a high level of digital\nrepresentation of its history (Al Qutaini, et. al., 2024).\n\u2022 Generation of content and products\nNews: Parsing and repackaging of news content in AI-generated news bites or editorials that portray a\nperson, event, or policy in a prejudiced manner.\nMarketing and sales: Misleading or offensive messaging towards a certain customer segment that is\npoorly understood by the LLM e.g., people of a certain race or religious denomination.\n\u2022 Censorship and moderation: flagging or censoring social media content that is deemed incorrectly offensive, or\npermitting content that is harmful to an at-risk identity group in low resource settings. A number of research\nstudies have looked at these effects in practice, identifying how AI moderators can be far less efficacious in\nnon-majority languages like the low-resource languages we study (Cari Beth Head et al. 2023)."}, {"title": "5.2 Intersections of inaccurate value representation with censored digital ecosystems", "content": "We can relate these questions to the topic areas in which we see the highest error rates in our analysis to hypothesize\nthe kinds of risks that may arise in low-resource societies. Our analysis showed the highest error rates in topics under\n\"security,\" \"ethical values and norms,\" and \"political culture and political regimes.\" The settings where there was\ngreatest deviation between LLM answers and that of the WVS were Nigeria, Zimbabwe, Ethiopia, and Tajikistan,\namongst others. All four countries rank \u201cnot free\" on Freedom House (Freedom House, 2023). There are two potential\nimplications of this. First, users who rely on or use LLM answers in these settings can face the risk of persecution if the\nalternative values of the LLMs shape their view publicly, in contradiction to the prevailing views of their autocratic\ngovernments. Second, and more importantly, is whether views expressed online in these settings can ever be true\nproxies for the beliefs of the underlying society. Three out of four of these countries score poorly on internet freedom\n(scores of less than 50/100) (Freedom House, 2022; Reporters without Borders, 2023). This means that, unless online\nplatforms were to become a free public commons unbounded by restrictive censorship, LLMs trained on digital content\nfrom these regions may always differ from the honest perspectives of its people that are better captured in 1-1 surveys\nlike the WVS. This limits the ability of LLMs to mimic the values of societies where digital content is a poor proxy for\ntrue felt values. The constraint of using digital content as a proxy for societal values is not limited to countries with\nhigh censorship only. It extends to all countries where there is a significant divide between the characteristics of people\ngenerating the bulk of online content, and the true demography of the country, divides that can be caused by factors\ncorrelated with values such as age, class, and gender (Shcradie, 2018)."}, {"title": "5.3 Broader cultural normativity and repercussions of homogenization", "content": "As researchers and practitioners concerned with cultural bias embedded in LLMs scale up LLM use across geographical\nand cultural settings, they will be confronted with one key question: what values should LLMs be centered around, if\nnot the prevailing values of high-resource societies currently seen in our analysis? LLMs are ultimately reflections of\nthe humans that have built and trained them, as well as the largely human-generated data that has trained them. As all\nhumans are biased, influenced by their unique set of histories and cultures, the issue in front of AI ethics researchers\nand policy makers is that of whose bias LLMs be most closely aligned to."}, {"title": "Cultural Fidelity in Large-Language Models", "content": "Researchers like Oliviera et al. (2023), and Benkler et al. (2023), amongst others, have attempted to understand this\nquestion. They posit that embedding cultural pluralism, or the co-existence of multiple cultures within a broader system,\nis one way forward. A culturally pluralistic LLM would have the ability to adapt its output based on the prompt and user\ndescription provided to it, aligning itself with the values of the user and their setting more closely. However, cultural\npluralism is not devoid of its own ethical challenges, such as if an LLM should indeed reproduce highly restrictive\nvalues that penalize one sub-identity or group in"}]}