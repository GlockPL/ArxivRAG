{"title": "Pyramid-Driven Alignment: Pyramid Principle Guided Integration of Large Language Models and Knowledge Graphs", "authors": ["Lei Sun", "Xinchen Wang", "Youdi Li"], "abstract": "Large Language Models (LLMs) possess impressive reasoning abilities but are prone to generating incorrect information, often referred to as hallucinations. While incorporating external Knowledge Graphs (KGs) can partially mitigate this issue, existing methods primarily treat KGs as static knowledge repositories, overlooking the critical disparity between KG and LLM knowledge, and failing to fully exploit the reasoning capabilities inherent in KGs. To address these limitations, we propose Pyramid-Driven Alignment (PDA), a novel framework for seamlessly integrating LLMs with KGs. PDA utilizes Pyramid Principle analysis to construct a hierarchical pyramid structure. This structure is designed to reflect the input question and generate more validated deductive knowledge, thereby enhancing the alignment of LLMs and KGs and ensuring more cohesive integration. Furthermore, PDA employs a recursive mechanism to harness the underlying reasoning abilities of KGs, resulting in more accurate knowledge retrieval for question-answering tasks. Our experimental results reveal a substantial performance advantage of PDA over state-of-the-art baselines, with improvements reaching 26.70% and 26.78%.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) exhibit remarkable intrinsic reasoning capabilities that enable them to excel in complex inferential tasks across a variety of natural language processing tasks (Feng et al. 2023; Kojima et al. 2022; Brown et al. 2020). However, LLMs often encounter challenges when presented with queries that necessitate specialized expertise exceeding their internal knowledge. Furthermore, their tendency to fabricate information, a phenomenon known as hallucination, significantly compromises their reliability and trustworthiness (Bang et al. 2023; Li et al. 2024a). Previous research has demonstrated that integrating LLMs with the structured, clearly defined and explainable knowledge provided by knowledge graphs (KGs) can effectively address the dual challenges of hallucination and knowledge limitations. By grounding LLM reasoning in factual, verifiable data, KG offers a robust framework for enhancing the accuracy, reliability, and comprehensiveness of LLM-generated outputs. The synergistic integration of LLMs and KGs has emerged as a focal point in contemporary research and applications(Huang et al. 2023; Agrawal et al. 2023; Gao et al. 2024; Jiang et al. 2023; Wang et al. 2023a; Baek, Aji, and Saffari 2023).\nExisting methods for integrating LLMs and KGs typically follow two primary paradigms. The first directly retrieves relevant triples in KG based on query entities (Wang et al. 2024; Baek, Aji, and Saffari 2023). The second approach focuses on guiding LLMs to generate a series of relevant knowledge, such as question decomposition or sub-answer generation (Wang et al. 2023a; Park et al. 2024), to identify corresponding triples. However, both methods often fall short in bridging the knowledge gap between LLMs and KGs, and underutilize KG's inherent reasoning potential. As illustrated in Figure 1, consider the question, Which year did Mauricio Roberto Pochettino's Tottenham Hotspur Football Club play the UEFA Champions League Final but did not win it? This is a complex multi-hop question that requires reasoning across several pieces of information. In this scenario, LLM attempts to break the question down into multiple sub-questions, as shown in Figure 1. It then retrieves relevant knowledge from KG based on these sub-questions, ultimately synthesizing this information to construct the final answer.\nHowever, this approach, which solely relies on LLM-generated knowledge, neglects the deductive nature of KG reasoning. By disregarding logical interconnections between knowledge, this method increases the risk of retrieving irrelevant or misleading information from KG, potentially compromising response accuracy (Wu et al. 2024). For instance, sub-questions like When did Tottenham Hotspur participate in the UEFA Champions League? can inadvertently retrieve irrelevant triples such as (Tottenham Hotspur, participated in, 1921 FA Cup Final) or (Tottenham Hotspur, participated in, 2016\u201317 UEFA Europa League). These misleading triples can divert the LLM from the correct reasoning path, resulting in incorrect answers like 2016, instead of the correct 2019. As illustrated in Figure 1, effectively addressing complex, multi-hop questions requires fully integrating LLMs and KGs capabilities (Sun et al. 2024). Aligning LLMs outputs with the unique characteristics of KGs enhances this integration. However, achieving this alignment poses several challenges. First, how can we guide LLMs to generate knowledge that aligns with KG knowledge? Second, how can we leverage the aligned knowledge from LLMs to effectively utilize the inherent reasoning capabilities of KGs, thereby optimizing the integration of LLMs and KGs?\nTo address these challenges, we propose Pyramid-Driven Alignment (PDA), an innovative approach designed to align LLM and KG knowledge and optimize the integration of their reasoning capabilities. In PDA, we first design an effective reasoning method based on the Pyramid Principle (Minto 2021) to align with KG knowledge. Specifically, PDA uses the 5W1H framework (Han, Kim, and Lee 2020) to reflect the question, and organizes the results into a hierarchical pyramid structure. This approach enables PDA to derive validated deductive knowledge at the base of the pyramid (see Figure 2), ensuring alignment with the deductive reasoning paths in KG. After establishing this Pyramid alignment, PDA employs a recursive mechanism to fully unlock reasoning capabilities of KG, ensuring efficient and accurate retrieval of task-relevant knowledge in KG. By integrating these elements, PDA effectively harmonizes the reasoning capabilities of both KG and LLM, optimizing their combined effectiveness.\nTo rigorously evaluate our proposed method, we conducted comprehensive experiments across three challenging datasets: 2WikiMultihopQA (Ho et al. 2020), Mintaka (Sen, Aji, and Saffari 2022), and WebQuestionsSP (WebQSP) (Yih et al. 2016). Our approach consistently surpassed state-of-the-art baselines, demonstrating its effectiveness in addressing complex question-answering tasks. Specifically, we achieved accuracy improvements of up to 19.00% and 26.70% on Mintaka, and 27.68% and 26.78% on WebQSP when utilizing GPT-3.5 and GPT-40 mini, respectively. The key contributions of this work are outlined below:\n\u2022 We propose a novel method named PDA to enhance the integration of LLMs and KGs. By pioneering the application of the Pyramid Principle to LLMs, PDA generates deductive knowledge that aligns with KG knowledge, facilitating seamless LLM-KG integration.\n\u2022 We design a recursive mechanism that leverages KG's inherent reasoning capabilities to improve LLM-KG integration and extract more accurate knowledge. By combining this with LLM's reasoning abilities, PDA significantly boosts question-answering performance.\n\u2022 We conducted experiments across three datasets and achieved state-of-the-art (SOTA) performance, showcasing the robustness and effectiveness of our approach."}, {"title": "Methods", "content": "For KG-driven tasks, effectively integrating the reasoning capabilities of LLM and KG is crucial for accurate knowledge acquisition, especially in question answering. Given a natural language question $q \\in Q$ and a Knowledge Graph $G$, the task $T$ is defined as generating an answer $a \\in A$, as follows:\n$T: (G, q) \\rightarrow a$\nIn this work, the proposed PDA prompts the LLM based on the Pyramid Principle to generate deductive knowledge, aligning the knowledge of LLM and KG. Building upon this alignment, PDA further harnesses the intrinsic reasoning capabilities of KG to extract more precise and relevant knowledge for the target task $T$. PDA comprises the following two components as illustrated in Figure 2:\n\u2022 Pyramid Alignment: This module, denoted as $P$, applies the Pyramid Principle to construct a set of deductive knowledge that aligns with the reasoning paths of KG. This alignment ensures a seamless integration between LLM and KG, empowering the model to excel at complex question-answering tasks.\n\u2022 KG Reasoning: To leverage the inherent reasoning capabilities of KG, this module $K$, iteratively refines knowledge acquisition. In each iteration $i$, it retrieves a more accurate set of triples $T_i = \\{(h_i, p_i,t_i) | h_i \\in E,t_i \\in E, p_i \\in R\\}$, (where $E$ is the set of entities and $R$ is the set of relations) by incorporating alignment knowledge $I_i$. This synergy harnesses the strengths of both LLM and KG, enhancing effectiveness in tackling complex tasks."}, {"title": "Pyramid Alignment", "content": "By strategically employing the Pyramid Principle, the Pyramid Alignment module meticulously constructs deductive knowledge $I$ aligning with KG's intricate reasoning paths. This process can be formally expressed as:\n$I = P(q)$\nTo leverage LLM capabilities, the given question $q$ is reflected into its constituent 5W1H elements, with each element denoted as $W_i \\in Z$, where $Z$ represents the set of the relevant 5W1H elements determined by the LLM, with a cardinality of $n$. Each $W_i$ is deductively related to the others.\n$W = 5W1H(q), W = \\{W_i | i\\in Z\\}$\nSubsequently, PDA generates statements for each $W_i$, serving as foundational evidence for constructing a hierarchical pyramid structure.\n$S = Statement(W, q) S = \\{S_i | i \\in Z\\}$\nLeveraging the statements $S$ and the pyramid hierarchical structure, PDA generates a main point $M$ and corresponding sub-points $I$.\n$M = MainPoint(S)$"}, {"title": "KG Reasoning", "content": "This module leverages both the alignment knowledge from the Pyramid Alignment module and the inherent reasoning capabilities of KG for retrieving more accurate knowledge. At each iteration $i$, PDA utilizes the entities $E_i$, the deductive knowledge $Z_i$ generated by the Pyramid Alignment module, and the subgraph $G_{i-1}$ reasoned from the previous iteration to produce a set of triples $T_i$. This process is as below:\n$T_i = K([I_i, E_i, G_{i-1}])$\nThe output of the KG Reasoning module, denoted as $T$, is obtained by summing the set of triples generated in each iteration:\n$T = \\sum_{i=1}^{n} T_i$\nTo achieve a more effective integration of LLM and KG, it is crucial to harness the inherent reasoning capa-"}, {"title": "Experiments", "content": "To assess the efficacy of our PDA approach for complex question-answering tasks, we employ three KBQA datasets which necessitate advanced reasoning capabilities: 2Wiki-MultihopQA, Mintaka, and WebQSP. The details of the three KBQA datasets are illustrated in Table 1."}, {"title": "Setup", "content": "We employed the GPT-3.5 Turbo ('GPT-3.5' is used to refer to this throughout the paper) and GPT-40 mini model\u00b9 for our experiments. To ensure consistency and comparability, the model's temperature parameter was fixed at 0.4, and the maximum token length was constrained to 1,000 tokens throughout all experimental runs. For the KG reasoning module, we configured key parameters based on the setup in KAPING (Baek, Aji, and Saffari 2023), setting the top N triples to 10.\nTo facilitate information retrieval from the Wikidata KG, we leveraged the capabilities of the simple-wikidata-db Python library\u00b2. This library provides a comprehensive toolkit for managing large-scale knowledge graphs, encompassing functionalities such as downloading the latest Wikidata dump, efficient staging, and distributed query execution. The Wikidata dump was distributed across a cluster of five AWS EC2 instances, each with 48 cores and 768GB of memory. The smaller Freebase dump was hosted on a single machine with 48 cores and 256GB of memory, as specified in the Think-on-Graph (ToG) setup (Sun et al. 2023). The Freebase dump is managed using OpenLink Virtuoso, a versatile database supporting relational, RDF, and graph data models. Although Virtuoso is widely used for KG applications, its default configuration led to prohibitively long"}, {"title": "Main Results", "content": "As indicated in Table 2, our PDA consistently outperforms existing approaches on the 2WikiMultiHopQA, Mintaka, WebQSP datasets. Compared to best-performing prompt-based methods operating without external knowledge, PDA achieves significant improvements, with gains of 5.50% (2WikiMultiHopQA), 1.40% (Mintaka), and 17.24% (WebQSP) using GPT-3.5, and 7.80% (2WikiMultiHopQA), 9.30% (Mintaka), and 22.80% (WebQSP) using GPT-40 mini. Furthermore, when compared to KG-driven SOTA, PDA also demonstrates superior performance. PDA surpasses Subgraph-KGQA by 32.70% on the Mintaka dataset, and Knowledge-Driven CoT by 4.65% on WebQSP. Against KAPING, PDA exhibits substantial gains of 7.10%, 19.00%, and 27.68% on 2WikiMultiHopQA, Mintaka, and WebQSP, respectively, when using GPT-3.5, while 5.50%, 26.70%, and 26.78%, respectively, with GPT-40 mini. These results underscore the effectiveness of PDA in leveraging the combined reasoning capabilities of LLM and KG, leading to substantial improvements for complex question-answering tasks.\nTo demonstrate the efficacy of PDA, we examine a specific case as illustrated in Figure 2. In the Pyramid Alignment module, the question Which son of Princess Diana didn't marry a British Citizen? is methodically reflected through the 5W1H framework. This process initiates by identifying What-Identify the sons of Princess Diana supported by the statement Princess Diana has two sons: Prince William and Prince Harry. It then deductively progresses to Who-Determine the nationality of the spouses of Prince William and Prince Harry, with the statement Prince William is married to Catherine Middleton, a British citizen, while Prince Harry is married to Meghan Markle, an American citizen."}, {"title": "Discussion", "content": "To assess the individual contributions of Pyramid Alignment and KG Reasoning modules to our PDA approach, we conducted a series of ablation experiments on the WebQSP test dataset using the GPT-40 mini model. For this analysis, we randomly collected 400 samples from the WebQSP test dataset. A comparison of performance across different configurations is presented in Table 3.\nWe conducted experiments with the model excluding both Pyramid Alignment and KG Reasoning modules, as well as separate experiments with each module removed individually. Ablation studies reveal the critical roles of both Pyramid Alignment and KG Reasoning in PDA. Excluding the Pyramid Alignment diminishes performance to 78.00% from 81.75%, emphasizing its role in structuring information and enhancing overall reasoning. Removing KG Reasoning leads to a performance of 79.50%, indicating KG's contribution to reasoning through structured external knowledge. Removing both components drastically reduces performance to 72.00%, highlighting their synergistic impact. PDA's integration of Pyramid Alignment and KG Reasoning achieves optimal performance at 81.75%, demonstrating the effectiveness of combining LLM and KG reasoning capabilities."}, {"title": "Effect of Pyramid Alignment and KG Reasoning", "content": "To investigate the impact of excluding both Pyramid Alignment and KG Reasoning, we examine two specific cases where the model excludes both Pyramid Alignment and KG Reasoning modules.\nIn the first scenario, the question posed is Where was Franz Ferdinand from? Without incorporating Pyramid Alignment and KG Reasoning, the LLM generated several pieces of evidences: Franz Ferdinand was a member of the Habsburg dynasty, He was born in the city of Graz, which is located in present-day Austria, and Franz Ferdinand held the title of Archduke of Austria. Based on these information, the model retrieved incorrect triples and concluded the answer Austria. This case illustrates that the absence of Pyramid Alignment and KG Reasoning leads to invalid knowledge from LLM and"}, {"title": "Reasoning with LLM Prompting", "content": "Recent research has focused on enhancing LLM reasoning through prompting. The Chain-of-Thought (CoT) method (Wei et al. 2022; Kojima et al. 2022) has been particularly influential, leading to advancements such as Complex-CoT (Fu et al. 2022), Tab-CoT (Jin and Lu 2023), Self-Consistency (Wang et al. 2022), Tree of Thoughts (ToT) (Yao et al. 2024), and Graph of Thoughts (GoT) (Besta et al. 2024). Additionally, studies have explored deductive reasoning in LLMs, including Ling et al. (2024); Zhu et al. (2024); Liu et al. (2023); Holmes et al. (2023); Yang et al. (2023), and Huang and Chang (2022). Furthermore, incorporating external documents into prompting for reasoning has been investigated, as demonstrated in Li and Du (2023) and Shi et al. (2024). However, they solely focus on the reasoning capabilities of LLMS, without adequately addressing the issue of hallucinations that LLMs may produce."}, {"title": "KG-Augmented LLM", "content": "KG-augmented LLM inference leverages KG for real-time knowledge updates during inference without retraining (Pan et al. 2024). Existing methods primarily focus on transforming structured KG into textual representations for LLM. These approaches vary in their methodologies, including converting triples into sentence-like formats (Li et al. 2023a; Luo et al. 2023a) and linearizing question-specific subgraphs such as Salnikov et al. (2023), Retrieve-Rewrite-Answer (Wu et al. 2023) and Chain-of-knowledge (Li et al. 2023b) as refinement for better task-solving (Tao et al. 2024). Furthermore, several studies have integrated LLM reasoning with KG. Li et al. (2024b) present the approach by utilizing LLMs to selectively retrieve task-related subgraphs from KG. Wang et al. (2023b) propose Knowledge-driven CoT, a question answering system incorporating CoT reasoning, while KAPING (Baek, Aji, and Saffari 2023) and Keqing (Wang et al. 2023a) focus on prompt LLMs to reason and generate knowledge relevant to the question, these methods overlook the gap between LLM knowledge and KG knowledge. Recent advancements in retrieval-augmented methods have leveraged the structure of KG to enhance KGQA performance. RoG (Luo et al. 2023b) enables LLMs to generate the knowledge aligned with paths structure of KG, while UniKGQA (Jiang et al. 2022) produces semantic matching features between questions and subgraph triples.\nAlthough these works integrate LLM and KG, they overlook the misalignment between LLM and KG knowledge, particularly regarding KG's reasoning paths. Additionally, they also fail to fully leverage the inherent reasoning capabilities of KG. As a result, their integration of LLM and KG does not reach its full potential. In contrast, our PDA aligns deductive knowledge with KG and synergistically combines the reasoning strengths of both LLM and KG. This approach leads to a more effective integration, enhancing the ability to address complex question-answering tasks."}, {"title": "Conclusion", "content": "In this work, we introduce a novel approach PDA for solve complex KBQA tasks. PDA innovatively employs a reasoning method based on the Pyramid Principle to generate deductive knowledge from LLM and aligns it with knowledge in KG. We also propose a recursive mechanism to fully harness the reasoning capabilities of KG, ensuring efficient and accurate retrieval of task-relevant knowledge. By synergistically combining the reasoning strengths of both LLM and KG, PDA effectively tackles complex question-answering tasks. Extensive experiments validate the effectiveness of PDA, showcasing its capability to handle intricate question-answering scenarios."}, {"title": "Technical Appendix", "content": "To demonstrate the effectiveness of PDA, we present several representative cases from the Mintaka dataset. We selected two examples to illustrate how PDA utilizes Pyramid Alignment and KG Reasoning modules to correctly address complex questions in Table 5 and Table 6."}, {"title": "Prompt", "content": "This section presents the prompt for Pyramid Alignment module in Table 7."}]}