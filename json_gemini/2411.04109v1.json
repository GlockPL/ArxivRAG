{"title": "SELF-CONSISTENCY PREFERENCE OPTIMIZATION", "authors": ["Archiki Prasad", "Weizhe Yuan", "Richard Yuanzhe Pang", "Jing Xu", "Maryam Fazel-Zarandi", "Mohit Bansal", "Sainbayar Sukhbaatar", "Jason Weston", "Jane Yu"], "abstract": "Self-alignment, whereby models learn to improve themselves without human annotation, is a rapidly growing research area. However, existing techniques often fail to improve complex reasoning tasks due to the difficulty of assigning correct rewards. An orthogonal approach that is known to improve correctness is self-consistency, a method applied at inference time based on multiple sampling in order to find the most consistent answer. In this work, we extend the self-consistency concept to help train models. We thus introduce self-consistency preference optimization (SCPO), which iteratively trains consistent answers to be preferred over inconsistent ones on unsupervised new problems. We show SCPO leads to large improvements over conventional reward model training on reasoning tasks such as GSM8K and MATH, closing the gap with supervised training with gold answers or preferences, and that combining SCPO with standard supervised learning improves results even further. On ZebraLogic, SCPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and Claude-3 Haiku.", "sections": [{"title": "1 INTRODUCTION", "content": "Training large language models (LLMs) on human-annotated data has improved their performance on a wide array of tasks. However, the size and quality of human data remains a major bottleneck as the data collection process is often resource-intensive in terms of cost, time, and expertise. To address this challenge, recent works focus on iteratively training from model-generated data via self-training. Notably, Yuan et al. (2024) propose a \"self-rewarding\u201d training pipeline for instruction-following, comprising two steps: (i) using the LLM to generate new queries and self-evaluating the generated responses for each query; and (ii) building preference pairs and training the LLM using iterative direct preference optimization loss (DPO; ). However, Huang et al. (2024) demonstrate that LLMs struggle at evaluating the correctness of their own responses on complex problem-solving tasks which have an unambiguous correct answer, thereby rendering Yuan et al.'s self-evaluation approach ineffective. Using an external reward model (RM) to rank responses can have similar problems; even if such models are trained on reasoning tasks they may still suffer on out-of-distribution problems. To address this issue, we introduce Self-consistency Preference Optimization (SCPO). SCPO is an approach to self-train LLMs for complex problem-solving tasks without access to gold solutions or final answers in the training data. Our approach leverages the concept of self-consistency (Wang et al., 2023), an inference-time only approach that improves performance on reasoning tasks by generating multiple solutions using the LLM and choosing the most frequent final answer. More consistent answers are more likely to be correct because mistakes made by the model are often random, so incorrect solutions are unlikely to lead to the same answer multiple times. In SCPO, the self-consistency concept is instead applied during unsupervised self-training. The method consists of (i) selecting model-generated queries, (ii) annotating preference pairs using the most self-consistent response (winner) and least self-consistent response (loser), and (iii) optimizing a loss function that is weighted for each instance depending on the model's confidence in the preference pair. Additionally, we propose a semi-supervised variant of SCPO that jointly trains LLMs on labeled and unlabeled instances, taking advantage of human"}, {"title": "2 SELF-CONSISTENCY PREFERENCE OPTIMIZATION", "content": "As depicted in Figure 1, SCPO is an unsupervised iterative training method that starts with a base language model. Each iteration makes use of existing training problems/queries (without labels) as well as newly generated problems. The self-consistency metric is used in both generating new problems and building preference pairs. We describe each step of SCPO's iterative training setup below. All prompts for solution generation and new problem generation can be found in Appendix C.\nInitialization. SCPO assumes access to an initial base model Mo and a small amount of (seed) high-quality unlabeled queries, which are typically complex reasoning problems. The model will be trained and updated at each training iteration resulting in models M1, M2, , \u041c\u0442, where T is the total number of iterations. Instead of gold labels (answers) for responses, SCPO uses the consistency of the model Mt, as measured by a real-valued vote function V(\u00b7) defined below, to rate and rank the quality of each response. Our vote function is based on self-consistency (Wang et al., 2023) of the model SCPO can also be used with any measure of model consistency such as internal consistency or universal consistency.\nGenerating New Problems. Following other self-alignment methods (Yuan et al., 2024; Yu et al., 2024), we use few-shot prompting to self-generate additional problems from the model. Using the seed set, multiple example problems are chosen at random and placed in context to generate a new problem. Note that some prior works are constrained to simultaneously generating both a new query along with its corresponding correct answer (Yu et al., 2024). In contrast, with SCPO, we do not rely on accurately generating the corresponding answer, allowing the model to generate"}, {"title": "Building Self-Consistency Preference Pairs.", "content": "For each problem x in the training data Dt, we use temperature-based sampling with the current model Mt to generate k responses \u0177x = {Y1, Y2,\u2026\u2026, Yk} sampled from M\u2081(\u00b7|x) including any rationales, e.g., chain-of-thought , followed by the final answer. Following Wang et al. (2023), the vote function V(\u00b7) extracts the final answer corresponding to each response y \u2208 \u1ef9 via ans(\u00b7) and returns the relative frequency of the final answer, i.e., V(y) = \u2211m=11(ans(ym)=ans(y)). As illustrated in Figure 1 (middle), using the vote function, we create preference pairs Dpairs by selecting the most consistent response as the chosen (winning) response and selecting the least consistent one as the rejected (losing) response, provided that the vote of the chosen response is greater than a threshold \u03c4. In other words,\nDpairs = {(x,y+,y\u00ae)|x \u2208 Dt, y+ = arg max V(y), y\u00af = arg min V(y), and V(y+) \u2265 \u03c4}."}, {"title": "SCPO Loss Function.", "content": "SCPO operates under the assumption that when multiple responses sampled for problem x map to the same answer, then the predicted answer is likely to be correct, the same assumption as in Wang et al. (2023). Consequently, we use consistency via a vote function V(\u00b7) as a proxy to create preference pairs. However, at the same time, the number of votes attained by a response can also reflect the model's confidence in the response, implying that pairs where the vote margin \u2013 the difference in votes attained by the chosen vs. the rejected response \u2013 is larger, are of higher quality and vice-versa (refer to Appendix A). We model this in SCPO's training by using an instance-level weight w(x) to the loss, i.e., for the preference pair (x, y, y\u00af) \u2208 Dpairs, w(x) = (V(y+) \u2013 V(y\u00af))/k, where k is the total number of responses generated for each question (total number of votes cast). We thus use the following loss function:\nLSCPO(y+, y\u00af|x)=-w(x) log \u03c3 (\u03b2\u03b9BlogMo(y+x)Mt(y+x)-BlogMo(yx)Mt(y-x))\u03b1\u03c9(x)logMo(y+x)|y+|.\nThe loss includes a DPO and NLL term similar to the recently introduced supervised IRPO  loss, but in our case we have an unsupervised objective and use our introduced weighted loss. Here \u03c3(\u00b7) denotes the sigmoid function, and \u03b1, \u03b2 are hyperparameters of the loss function, and 0 represents the LLM parameters being trained in the current iteration. At the tth iteration, we use the initialized model M\u2081 as the reference model in the DPO loss. After training on this loss, the trained model is used to initialize the next iteration, i.e., Mt+1 \u2190 \u041c\u04e9."}, {"title": "Iterative Training.", "content": "Starting with an initial seed model Mo, we train a series of models M1, M2, i.e. for T = 2 iterations (we justify this choice in Appendix B). Each model Mt+1 is trained using LSCPO on Dpairs, the data generated by the tth model, defined as follows:\nMo: Seed LLM, initialized with a pretrained LLM (need not be instruction-finetuned).\nM\u2081: Initialized with Mo to generate Dpairs from Do (+ new problems) and trained using Lscpo.\nM2: Initialized with M\u2081 to generate Dpairs from D\u2081 (+ new problems) and trained using Lscpo.\nThis approach is similar to the Self-Rewarding LM training loop except for the fact that we use the model's self-consistency to score responses instead of using the same model as a judge to verify its own correctness, which Huang et al. (2024) show is often challenging. In contrast to other iterative bootstrapping techniques for reasoning , SCPO does not require access to gold labels such as gold responses or final answers, allowing SCPO to scale beyond the problems from an existing training dataset."}, {"title": "Semi-Supervised Training with SCPO.", "content": "Although SCPO does not require access to gold labels, we can easily incorporate datasets with gold labels in conjunction with unlabeled datasets during SCPO training. To this end, we alter the preference pair creation strategy described in that case. When gold labels are available for a query Xgold, we sample k responses, and create pairs such that the chosen response y\u207a is correct and the rejected response y\u00af is incorrect (discarding queries where such pairs cannot be created). Since we already know these pairs are of high quality, we set the weight of annotated instances w(xgold) = 1. For queries that do not have gold labels, we use our self-consistency criterion for pair creation and compute the weighted loss for those examples as before. A special case is that if all data is labeled, the loss reduces to the IRPO loss."}, {"title": "3 EXPERIMENTAL SETUP", "content": "Datasets and Metrics. We evaluate the effectiveness of SCPO on a range of mathematical and logical reasoning datasets:\n\u2022 GSM8K contains a train/test split of 7.5K/1.3K grade school math word problems. For the purpose of this work, we split the train set into a train/dev split with 6.7K/0.8K problems respectively. We use the dev split for hyperparameter tuning and checkpoint selection. The overall data split becomes 6.7K/0.8K/1.3K in the train/dev/test set, respectively. We report performance based on exact match accuracy of the final numeric answer on the test set.\n\u2022 MATH is a dataset of challenging high-school math competitions that contains a train/test split of 7.5K/5K problems, respectively. Similar to GSM8K, we reserve 10% of samples from the train set to create a held-out dev set for model selection and hyperparameter tuning, resulting in our final train/dev/test splits with 6.7K/0.8K/5K problems, respectively. We report the accuracy of the final answer on the test set.\n\u2022 ZebraLogic is a logical reasoning benchmark. It is a test set of 1K logic grid puzzles (or Einstein's puzzles) designed as a constraint satisfaction problem. Each puzzle is comprised of n houses with m unique features, resulting in an n \u00d7 m table. Given a list of clues, solving the puzzle requires deducing the correct (unique) assignment of values in the table, i.e., a unique value for each feature and house. Evaluation metrics for this dataset are: puzzle accuracy (overall, easy, and hard puzzles) as well as cell accuracy."}, {"title": "Base Models.", "content": "For GSM8K and MATH, we use Llama-3 Base 8B as the seed model Mo. We note that the instruction-tuned version may have already been fine-tuned on the gold data from these tasks, so new experimental settings cannot be reliably tested in that case. For ZebraLogic, we use Llama-3 Instruct 8B as the seed model."}, {"title": "Preference Training Data.", "content": "We use the Llama-3 Instruct 8B model to generate additional problems (queries). For GSM8K and MATH, we prompt the model to generate a problem similar to 4-shot examples of problems from the train set. Note that the prompt only requires valid human-written problems and not their corresponding answers. We filter out problems where maxi0.5k (or, \u0442 = 0.5k) where k is the number of responses sampled or votes cast for each query. That is, where less than half of the votes go towards the majority answer, which we found to be a good threshold based on the dev set accuracy (see Section 5). Since M\u2081 models tend to be more consistent than Mo (cf. Section 5), for M2 training data, we increase the filtering threshold 7 to 0.7k and 0.6k on GSM8K and MATH, respectively. For ZebraLogic, we prompt the model to rephrase or perturb features of a puzzle from the dataset in a one-shot manner. Then, we use the underlying model Mt to generate k = 16 responses for each question and filter out questions where none of the responses accrue T = 2 or more votes (exactly matching solutions) for M\u2081 and set 7 = 0.5k for training M2."}, {"title": "Baselines.", "content": "We compare models trained with SCPO in unsupervised (denoted as SCPOUnsup.) and semi-supervised (denoted as SCPOSemi-Sup.) settings against the following baselines:\n\u2022 Seed model (Zero-shot CoT). We compare against the seed model (Mo) using zero-shot chain-of-thought prompting generated with greedy decoding and report results with or without inference-time self-consistency (SC;).\n\u2022 Supervised Training with Gold Answers (IRPOGold). We use a strong supervised preference optimization method for reasoning tasks (Pang et al., 2024), to serve as an upper-bound on per-"}, {"title": "4 MAIN RESULTS", "content": "4.1 MATH REASONING\nSCPO outperforms unsupervised baselines. Comparing methods on GSM8K, in Table 1, we observe that training with only one iteration of SCPO outperforms the zero-shot seed model and IRPORM, by 22.74% and 12.36%, respectively, using greedy decoding. Similarly, on MATH (cf."}, {"title": "Unsupervised SCPO is comparable to IRPO training with gold labels.", "content": "We can compare the unsupervised training of SCPO with the supervised training using gold labels of IRPO in Tables 1 and 2. The results show that SCPOUnsup. without using any gold labels can yield comparable accuracy to IRPOGold on GSM8K and MATH with < 1% gap in greedy performance and < 2% gap in accuracy using 8-way self-consistency after two iterations of training (M2). This comparable performance of SCPOUnsup. is likely due to high correlation (0.8 across the datasets) between the vote shares and accuracy on the test set, as further discussed in Appendix A. Note that on tasks that are challenging for the seed model Mo, such as MATH, we can only bootstrap a small set of examples from the original set of training problem as compared to IRPO (i.e., only around a quarter of examples obtain a clear majority answer). However, we can offset this gap in training data by generating new problems using few-shot prompting (cf. Section 2) and creating preference pairs using our self-consistency method. This helps provide improvements during the second iteration."}, {"title": "Semi-supervised training with SCPO outperforms IRPO.", "content": "Lastly, in Tables 1 and 2, we evaluate the semi-supervised version of SCPO combined with using gold labels. We find that on GSM8K, SCPOSemi-Sup. improves the greedy accuracy by 2.35% and SC accuracy by 2.19% in comparison"}, {"title": "4.2 ZEBRALOGIC: A CHALLENGING LOGICAL REASONING TASK", "content": "SCPO outperforms unsupervised baselines. Table 3 reports performance on ZebraLogic of SCPO and various baselines, using greedy decoding. We observe large improvements over the seed model, Llama-3 Instruct 8B (Mo) with one iteration of unsupervised SCPO (M\u2081), improving performance by 5.4% and 8.5% in overall puzzle accuracy (exact match of tables) and cell accuracy (match of each cell in the table), respectively. In contrast, unsupervised training of IRPORM yields only mild gains over the seed model by 3% in cell accuracy and even a slight drop in puzzle accuracy (11.6% to 11.3%). This can be attributed to ZebraLogic puzzles being out-of-distribution for the ArmoRM (cf. Section 5), thus trailing behind one iteration of ScPO by 5.7% in puzzle accuracy and 5.5% in cell accuracy. Overall, training with SCPO for two iterations improves the performance of the seed model by 8 positions on the leaderboard (from 38th to 30th) with a 6.5% boost in puzzle accuracy and, to the best of our knowledge, is the best 8B-scale LLM on ZebraLogic."}, {"title": "8B LLM trained with SCPO outperforms larger models.", "content": "Comparison of SCPO-trained models to other models in Table 3 demonstrates that SCPO-training after two iterations (M2) outperforms significantly larger models such as Llama-3 Instruct 70B, Gemma-2 27B, and Claude-3 Haiku by 0.9%, 1.8%, and 3.8% in overall puzzle accuracy, respectively. Additionally, we find that models trained using SCPO also yield the highest cell accuracy. We attribute these gains over larger models to the substantial improvement in solving easy puzzles with SCPO (up to 10.3%)."}, {"title": "5 ABLATIONS AND ANALYSIS", "content": "Importance of weighted SCPO loss.\nWhile the results in Section 4 are ob-tained using the weighted LSCPO loss that is a function of consistency, here we com-pare SCPO using an unweighted loss. More specifically, we train using the same preference dataset created based on self-consistency of responses, but with w(x) = 1 in the LSCPO loss. In Table 4, we ob-serve that across datasets and iterations, the weighted loss consistently outperforms the unweighted version. The improvement in accuracy is even more pronounced for the first iteration of training M1, yield-ing an improvement of 2.5% in accuracy on GSM8K and 1.44% on MATH with"}, {"title": "Models become more consistent across iterations.", "content": "In Figure 2, we analyze how the degree of model consistency varies across iterations. To this end, we measure the vote share V(y+)/k of the most consistent response, i.e., chosen response in self-consistency of models trained using unsupervised SCPO. From Figure 2, we conclude that SCPO training increases the consistency of models with each training iteration across different tasks. We suspect this finding stems from three contributing fac-tors: (i) with increasing iterations models become more accurate (Section 4); (ii) additional rounds of preference-optimization decreases model diver-sity ; and (iii) training with SCPO effectively distills the SC distribution into the model's single-sample distribution. Additionally, we find that models are more consistent on tasks with higher test accuracy, i.e., on GSM8K the LLM is most consistent and accurate whereas on ZebraLogic it is the least consistent and accurate."}, {"title": "Impact of consistency-based filtering on constructing preferences.", "content": "In Section 3, when gener-ating self-consistency preference data for GSM8K and MATH, we filter out instances where fewer than half of the votes go towards the majority answer, i.e., \u0442 = 0.5k. The choice of this threshold presents a trade-off between the number of preference pairs available for training and the quality of the training data, and affects the difference (margin) in accuracy of the chosen and the rejected response. Assum-ing access to the gold answers to measure quality of preference data, in Table 5, we analyze this trade-off"}, {"title": "Comparison of self-consistency to RMs.", "content": "Our results in Section 4 show that models trained with unsupervised SCPO outperform models trained with IRPO using ArmoRM to build preference pairs. To study this further, we conduct additional analysis by measuring the ability of the two methods to distinguish between correct and incorrect responses, com-paring the methods to gold labels. Results are given in Figure 3. We find that ArmoRM con-sistently has more incorrect orderings of pair-wise preferences (the chosen is incorrect and the rejected is correct) than SCPO across all three datasets (shown in red). This added noise in training may be a major factor as to why IRPORM"}, {"title": "6 RELATED WORK", "content": "Iterative Training of LLMs. Iterative training or self-training has shown meaningful improve-ments in a number of domains such as safety (Bai et al., 2022), multilingual reasoning , and evaluation . Because LLMs often struggle with both generating and validating solutions to complex reasoning tasks, prior works on training LLMs for complex problem-solving tasks largely rely on human-annotated (gold) final answers  or access to an external reward model that performs well on the underlying task. However, both these classes of ap-proaches suffer from their own shortcomings. Firstly, manually annotating or verifying the final answer requires working through the solution step-by-step, making it especially resource-intensive for complex multi-step problems. Training strong reward models for such reasoning and problem-solving tasks also often requires human judgements of LLM generations , making it similarly expensive. Our work focuses on the setting without access to gold solutions or final answers, which remains largely unaddressed. While other works such as She et al. (2024); Yuan et al. (2024); Rosset et al. (2024); geared towards general instruction following tasks (as opposed to reasoning tasks specifically) circumvent the need for human-annotated labels in the dataset by using the model itself to score the responses, these works demonstrate only modest gains in the context of reasoning tasks."}, {"title": "Consistency in LLMs.", "content": "Self-consistency relies upon the intuition that sampling several responses, some of which lead to the same answer, lends higher certainty that the consistent answer is the correct one. Application of self-consistency at inference time has enabled performance improvements in a number of domains like math , code generation, and even open-ended tasks like summarization and question answering. In this work, we explore using self-consistency at training time for reasoning tasks, constructing preference pairs according to the self-consistent final answer. We employ a preference optimization loss function that is weighted according to the consistency of an answer. Intuitively, the consistency of an answer is a reflection of the model confidence, and several prior works have demonstrated that leveraging model uncertainty can lead to faster convergence and improved performance."}, {"title": "7 CONCLUSION", "content": "In this paper, we introduced Self-Consistency Preference Optimization (SCPO). SCPO leverages the concept of self-consistency, usually employed only at inference time, to improve the self-training of large language models. By iteratively optimizing to prefer consistent answers to inconsistent ones, SCPO achieves significant improvements over traditional reward model training without the need for additional gold labels. Our experiments demonstrate the efficacy of SCPO on various reasoning tasks, including GSM8K, MATH, and ZebraLogic, where in the latter it outperforms several larger state-of-the-art language models. We also showed that SCPO works well in semi-supervised setups with access to some gold labels, in addition to unlabeled inputs \u2013 improving performance further. These results highlight the potential of SCPO to improve self-alignment across reasoning tasks a domain that prior self-alignment methods still struggle with. Future work could extend SCPO to tasks where a single final answer cannot be easily parsed (e.g., summarization) through universal self-consistency , which leverages an LLM to select the most consistent answer among multiple samples. While we explore consistency in this work according to one model class (Llama-3 8B Base and Instruct), future work could also investigate consistency according to a suite of other models and tasks."}, {"title": "Response Generation: ZebraLogic", "content": "Example Puzzle:\nThere are 3 houses, numbered 1 to 3 from left to right, as seen from across the street. Each\nhouse is occupied by a different person. Each house has a unique attribute for each of the\nfollowing characteristics:\n- Each person has a unique name: 'Peter', 'Eric', 'Arnold'.\n- Each person has a unique favorite drink: 'tea', 'water', 'milk'\n## Clues:\n1. Peter is in the second house.\n2. Arnold is directly left of the one who only drinks water.\n3. The one who only drinks water is directly left of the person who likes milk.\nAnswer to the Example Puzzle:\n{\n\"reasoning\": \"Given Clue 1, we know Peter is in House 2. According to Clue 2, Arnold is\ndirectly left of the one who only drinks water. The person in House 3 cannot be on the left\nof anyone, so Arnold must be in House 1. Thus, Peter drinks water, and Eric lives in House\n3. Then, according to Clue 3, Eric drinks milk. Therefore, Arnold drinks tea.\",\n\"solution\": { \"House 1\u201d: { \u201cName\u201d: \u201cArnold\u201d, \u201cDrink\u201d: \u201ctea\u201d },\n\"House 2\": { \"Name\u201d: \u201cPeter\u201d, \u201cDrink\u201d: \u201cwater\u201d },\n\"House 3\": { \"Name\u201d: \u201cEric\u201d, \u201cDrink\u201d: \u201cmilk\u201d } }\n}\nPuzzle to Solve: {puzzle}\nPrompt: Now please solve the above puzzle. Present your reasoning and solution in the\nfollowing json format: {json template}"}, {"title": "Response Generation: GSM8K", "content": "Prompt: Answer the following question step-by-step. When you are ready, place the final\nanswer in a new line as #### < number >.\nQ: {question}\nA: Let's think step by step."}, {"title": "Response Generation: MATH", "content": "Prompt: Answer the following question step-by-step. When you are ready, place the final\nanswer in a new line as: The final answer is $\\boxed{< your answer>}$\nQ: {question}\nA: Let's think step by step."}, {"title": "Query Generation: ZebraLogic", "content": "Example Puzzle:\nAttributes to Change: [\u201cName\u201d, \u201cDrink\u201d]\nThere are 3 houses, numbered 1 to 3 from left to right, as seen from across the street.\nEach house is occupied by a different person. Each house has a unique attribute for each of\nthe following characteristics:\nEach person has a unique name: \u2018Peter', \u2018Eric', \u2018Arnold'.\n- Each person has a unique favorite drink: 'tea', 'water', 'milk'\n## Clues:\n1. Peter is in the second house.\n2. Arnold is directly left of the one who only drinks water.\n3. The one who only drinks water is directly left of the person who likes milk.\nAnswer:\nLet's change the \u201cName\u201d and \u201cDrink\u201d attributes of the given puzzle to create a new puzzle.\nThere are 3 names and drinks involved Mentions of \"Name\" changes from 'Peter', 'Eric',\n'Arnold' to mentions of \"Name\": 'Molly', 'Shannon', 'Kelly' respectively. Instead of\n\"Drink\" as the attribute, let's their \"Food\" preferences as the attribute. So mentions of\n\"Drink\" changes from 'tea', 'water', 'milk' to mentions of \"Food\": 'pizza', 'burgers',\n'fries' respectively. Now, changing the language of the puzzle and clues we get,\nNew Attribute Map: {\"Name\": \"Name\", \"Drink\": \"Food\"}\nPuzzle:\nThere are 3 houses, numbered 1 to 3 from left to right, as seen from across the street.\nEach house is occupied by a different person. Each house has a unique attribute for each of\nthe following characteristics:\nEach person has a unique name: 'Molly', 'Shannon', 'Kelly'.\nEach person has a unique favorite food: 'pizza', 'burgers', 'fries'\n## Clues:\n1. Molly is in the second house.\n2. Kelly is directly left of the one who only eats burgers.\n3. The one who only eats burgers is directly left of the person who likes fries.\nPuzzle to rephrase:\nAttributes to Change: {attributes dict}\n{input puzzle}\nPrompt: Rephrase the above puzzle by changing only the attributes above. AL-\nWAYS mention the \u201cNew Attribute Map\" and enclose the new puzzle within \"\u201d\u201d. Aside\nfrom these attributes keep the logic of the puzzle as similar as possible. Similar to the\nexample above, give your reasoning before rephrasing the puzzle."}, {"title": "Query Generation: GSM8K and MATH", "content": "Q: {few-shot question 1}\nQ: {few-shot question 2}\nQ: {few-shot question 3}\nQ: {few-shot question 4}\nPrompt: Based on the examples above, generate ONE solvable math word problem\nwith similar difficulty. Note that all the information needed to solve the problem should be\nincluded in the question. Output the question and nothing else.\nQ:"}]}