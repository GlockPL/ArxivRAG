{"title": "Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models", "authors": ["Aviv Bick", "Kevin Y. Li", "Eric P. Xing", "J. Zico Kolter", "Albert Gu"], "abstract": "Transformer architectures have become a dominant paradigm for domains like language modeling but suffer in many inference settings due to their quadratic-time self-attention. Recently proposed subquadratic architectures, such as Mamba, have shown promise, but have been pretrained with substantially less computational resources than the strongest Transformer models. In this work, we present a method that is able to distill a pretrained Transformer architecture into alternative architectures such as state space models (SSMs). The key idea to our approach is that we can view both Transformers and SSMs as applying different forms of mixing matrices over the token sequences. We can thus progressively distill the Transformer architecture by matching different degrees of granularity in the SSM: first matching the mixing matrices themselves, then the hidden units at each block, and finally the end-to-end predictions. Our method, called MOHAWK, is able to distill a Mamba-2 variant based on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens and a hybrid version (Hybrid Phi-Mamba) using 5B tokens. Despite using less than 1% of the training data typically used to train models from scratch, Phi-Mamba boasts substantially stronger performance compared to all past open-source non-Transformer models. MOHAWK allows models like SSMs to leverage computational resources invested in training Transformer-based architectures, highlighting a new avenue for building such models.", "sections": [{"title": "1 Introduction", "content": "Large language models based upon Transformer architectures have become a staple of natural language processing but suffer from their reliance on quadratic self-attention \u2013 the need to compute inner products between tokens at all positions up to the context length. This has motivated the development of several alternative subquadratic models, either approximations of self-attention (Katharopoulos et al. 2020) or entirely different architectures, such as state space models (SSMs) (A. Gu and Dao 2023; A. Gu, Goel, and R\u00e9 2022; Peng et al. 2023; Sun et al. 2023). Training strong subquadratic models such as SSMs can benefit the community through their cheaper finetuning and inference costs; however, they have not benefitted from the same amount of community effort in the form of training and compute as for Transformers. This raises a natural question: is it possible to leverage the vast amounts of resources that have been invested in training quadratic-time Transformers and use these models to produce stronger alternative models, such as state-space models?\nIn this paper, we present an approach for training subquadratic state-space models (specifically from the class of Mamba SSMs (A. Gu and Dao 2023)) through the distillation of different elements of a pretrained Transformer model. The key intuition is viewing both Attention and SSMs as sequence transformations that mix different token embeddings by applying different classes of matrices across them. Sequence model architectures can then be factored into separate (i) sequence mixing and (ii) channel mixing blocks, e.g., a Transformer is composed of Attention (sequence mixer) and MLP (channel mixer) blocks. Using this breakdown, we can separately distill the mixing elements of each model explicitly at different"}, {"title": "2 Related Work", "content": "Sequence Models. State-of-the-art autoregressive language models have been pretrained on massive amounts of data, resulting in models that exhibit extensive downstream capabilities, such as zero-shot translation and long-range reasoning (Brown et al. 2020; Gunasekar et al. 2023; Touvron et al. 2023). Recent work has focused on addressing the quadratic complexity of Transformers by developing subquadratic alternatives based on RNN (Beck et al. 2024; Peng et al. 2023), SSM (A. Gu and Dao 2023; Sun et al. 2023), and linear attention mechanisms (Dao, Fu, et al. 2022; Katharopoulos et al. 2020; Liu, Zaharia, and Abbeel 2023; Qin, S. Yang, et al. 2024; S. Yang et al. 2024), highlighting the importance of efficient sequence models in the era of large-scale autoregressive language models.\nIn addition, to combine different capabilities while maintaining efficiency, hybrid models that integrate attention mechanisms with subquadratic methods have been proposed (Fu et al. 2023; Lieber et al. 2024; Ren et al. 2024; Z. Wang et al. 2024). These models typically feature a limited number of attention layers, thus maintaining the quadratic complexity at a relatively low factor.\nSSM Architectures. GSS was the pioneer in integrating SSMs into gated neural network architecture for language modeling (Mehta et al. 2022). H3, inspired by the combination of S4 and linear attention (Katharopoulos et al. 2020), employs SSMs with shift and diagonal matrices and multiplicative operations on input projections, extending this formulation to broader recurrences, a foundation for subsequent architectures (Fu et al. 2023). Selective S4 incorporates S4 as a black box that generates a binary mask applied to the input, an architectural modification akin to gating mechanisms (J. Wang et al. 2023). Mamba (A. Gu and Dao 2023), combines the H3 block with the ubiquitous MLP block of modern neural networks by interleaving them, resulting in a more powerful architecture. The Mamba-2 block simplifies the Mamba block by removing sequential linear projections; the SSM parameters A,B,C are produced at the beginning of the block rather than as a function of the input X of the SSM. Finally, Mamba-2 (Dao and A. Gu 2024) was introduced as a variant of Mamba which leverages the structured state space duality (SSD). The Mamba-2 core layer is 2-8x faster than Mamba's selective SSM while continuing to outperform Transformers in language modeling.\nDistillation. Knowledge distillation can be used to transfer knowledge from a large teacher model to a smaller student model, resulting in a more efficient model that retains the performance of the teacher model (Hinton, Vinyals, and Dean 2015). Distillation has been applied to various language modeling tasks, such as text generation (Chen et al. 2020; Haidar and Rezagholizadeh 2019), machine translation (Hahn and H. Choi 2019; Tan et al. 2019; Zhou, Neubig, and J. Gu 2021), and question-answering system (Hu et al. 2018; Z. Yang et al. 2019).\nDistillation in language models has been largely focused on compression: turning a larger pretrained Transformer into a smaller one by utilizing the weights of the teacher model (Jha et al. 2023; W. Wang et al. 2020; Xia et al. 2023). Some of the techniques proposed look similar to ours; for example, W. Wang et al. (2020) match attention matrices in a step similar to our matrix orientation, and Liang et al. (2023) align outputs of each block (i.e., the hidden states). However, these differ in subtle and important ways because of our setting; for example, the former uses a different loss function than us that relies on softmax attention, and the latter is an end-to-end objective while our hidden state alignment occurs completely independently block-per-block. Consequently, prior work has observed that combining these objectives does not actually help and even might hurt distillation (Jha et al. 2023), whereas we show that our techniques all significantly help improve the student model.\nA smaller body of work has focused on our objective of distilling across architectures, in particular, turning a pretrained Transformer into a different architecture (usually a recurrent model of some form) of the same size. Kasai et al. (2021) first tried turning a pretrained softmax attention into a linear attention by directly transferring weights and continuing fine-tuning; a similar approach was taken by concurrent work (Mercat et al. 2024). Recently, Zhang et al. (2024) also proposed distilling into linear attention by first matching attention matrices. Our approach differs by using a more constrained loss function that works beyond linear attention; incorporating more fine-grained alignment (e.g., the hidden state alignment step); and using recent, more expressive classes of efficient student models (Mamba-2), which we show are significantly easier to distill (Table 7)."}, {"title": "3 Background and Overview", "content": "To facilitate a clear understanding of our distillation approach, we start with the necessary background and definitions. An overview of the Mamba-2 architecture, which forms the foundation of our Phi-Mamba model, is also provided."}, {"title": "3.1 Matrix Mixers", "content": "Following Dao and A. Gu (2024), we refer to an equivalent function that represents the input and output of a sequence model as a sequence transformation or a sequence mixer. Formally,\nDefinition 1 (Sequence Transformation). We use the term sequence transformation to refer to a parameterized map on sequences $Y = f_\\theta(X)$ where $X, Y \\in \\mathbb{R}^{(T,P)}$ and $\\theta$ is an arbitrary collection of parameters. $T$ represents the sequence or time axis; subscripts index into the first dimension, e.g. $X_1, Y_1 \\in \\mathbb{R}^P$.\nTo put it differently, sequence mixers combine tokens at various time steps, facilitating the model's comprehension of temporal information and interactions. Sequence transformations form the foundation of deep sequence models, being integral components of neural network frameworks such as Transformers. A particular family of sequence transformations can be represented by $Y = MX$ for a matrix $M\\in \\mathbb{R}^{(T,T)}$, which we refer to as a sequence transformation matrix or matrix mixer.\nAn example of such a matrix mixer is the vanilla self-attention, $Softmax(QK^T)$, which is applied to the input-dependent $V$ resulting in the familiar $Softmax(QK^T)V$. Similarly, Linear Attention (Katharopoulos et al. 2020) has a sequence transformation matrix of the form $K^T$. In addition, we can easily obtain their causal variants by multiplying by $L$, a lower triangular matrix filled with 1s, to obtain $L \\circ Softmax(QK^T)$ and $L \\circ QK^T$, respectively. Another example is a Toeplitz matrix $T$ used to perform discrete convolution on input $X$, resulting in $TX$ (Qin, Han, et al. 2023).\nA naive approach to computing the output of a sequence transformation is to multiply the input sequence $X$ by the matrix $M$. However, this approach has a time complexity of $O(T^2)$, which is prohibitive for long sequences. Subquadratic sequence transformations, such as Mamba-2, have been developed to address such inefficiencies through structured matrix multiplication."}, {"title": "3.2 Mamba-2", "content": "Mamba-2 (Dao and A. Gu 2024), a type of structured state space models (SSMs) (A. Gu 2023; A. Gu, Goel, and R\u00e9 2022), was recently introduced. Similarly to the original Mamba model (A. Gu and Dao 2023), Mamba-2 uses a time-varying state-space model which can selectively focus on or ignore inputs due to its input-dependent parameterization of the system components. The time-varying SSM is defined as follows:\n$\\begin{aligned}\nh_{t+1} &= A_th_t + B_tX_t \\\\\nY_t &= C_t h_t\n\\end{aligned}$ (1)\nHere, $B_t$ and $C_t$ are input-dependent projections of the system, as in Mamba-1; however, $A_t$ is the identity matrix $I$ multiplied by a scalar $a_t$. The above formulation also differs from the previous one by treating the underlying sequence as originating from a discrete signal instead of a continuous one and therefore omits the sampling component $\\Delta t$ from the original Mamba model.\nImportantly, Mamba-2 draws a new connection between SSMs and Transformers, termed Structured State Space Duality (SSD), which shows that a special case of SSMs can be viewed as a form of causal linear attention. In particular, fixing $A_t = I$ (a further restriction of Mamba-2 to $a_t = 1$) results in the formulation of causal linear attention (Katharopoulos et al. 2020) with the matrices B and C representing the projections of the key and the query, respectively, while the input projection X corresponds to the projection of the value.\nMamba-2 as a matrix sequence transformation. Inspired by the aforementioned connection between SSMs and Transformers, Dao and A. Gu (2024) shows that Mamba-2's SSD mixer family is equivalent to sequentially-semi-separable"}, {"title": "4 Methods", "content": "Throughout this section, we will describe each phase of MOHAWK. Specifically, we will cover the stages of matrix orientation, hidden-state alignment, and knowledge distillation, all three of which are crucial for developing an effective student model from the pretrained Transformer model. Unlike traditional distillation techniques, the student model retains the overall architecture of the teacher model, differing only in the replacement of the attention matrix mixer with a subquadratic alternative. We will progressively unveil our architecture, Phi-Mamba, along with the specifics of its distillation process. This section concludes with an in-depth description of the Phi-Mamba architecture and its hybrid version, which surpasses the performance of other subquadratic matrix mixers. Further examinations of the effectiveness of the method and ablation studies are discussed in Section 5.\nFor clarity, the term block refers to the repeating components that form the end-to-end model. The blocks are composed of layers, such as the self-attention layer (including projections), the SSM layer (including the mixer and convolution), and the convolutional layer. In this manner, many Transformer models, like Llama (Touvron et al. 2023), are viewed as a stack of alternating self-attention and MLP blocks, whereas the Phi and Phi-Mamba models are comprised of Phi blocks that have parallel Attention/SSM and MLP blocks."}, {"title": "4.1 Stage 1: Matrix Orientation", "content": "The first stage of MOHAWK aims to align the student matrix mixer with the teacher's self-attention matrix. Achieving this alignment is a two-step process: first, at every mixing layer, the student components preceding the matrix mixer are set to match the teacher's components. This ensures that each layer's input undergoes the same transformation up to the matrix mixer section. Consequently, the only variation from the input to the mixing process is the matrix calculation. We then minimize the distance between the matrix mixer, e.g., the self-attention matrix and the materialized SSM matrix (2), of each layer within the student and teacher models:\n$\\min_{\\Phi} || \\text{TeacherMixer}(u) - \\text{StudentMixer}_\\Phi(u) ||_F$ (3)\nwhere $\\Phi$ denotes the parameters within the student's sequence mixing layer, and $u$ indicates any arbitrary input. In our experimental setup, $u$ was chosen as the output from the teacher model's preceding layer to better mimic the input distribution to the layer. This stage ensures that the student and teacher models have roughly similar mixing layers and sets the foundation for the subsequent stages of the distillation process. In particular, this stage can be done in parallel across all the student layers, as the inputs to the student and teacher blocks are identical."}, {"title": "4.2 Stage 2: Hidden-State Alignment", "content": "Following the optimization of Equation (3), we must still address the differences between the outputs of the student and teacher blocks. To achieve this, we further align the components of the two blocks using initialization and distillation. Specifically, our goal is to match each student and teacher mixing blocks by minimizing the L2 norm of their output (e.g., the entire Mamba block with the self-attention block):\n$\\min_{\\Phi} || \\text{AttnBlock}(u) - \\text{StudentMixerBlock}_\\Phi(u) ||_2$ (4)\nwhere similar to Section 4.1, $\\Phi$ represents student's block parameters, and $u$ is an input. Once again, this stage can be done in parallel across all the student layers.\nIn the case of Mamba-2, we modify the remaining components to be identical to the Phi-1.5's Attention block, so that the overall functionality is preserved from Stage 1. Concretely, we initialize the gate (see Figure 2) to a constant value of 1 to \"open\" the gate, canceling its initial effect. In addition, we remove the normalization prior to the output projection, as it cannot be set to align with the Attention block. We then minimize the distance between the output of the Mamba-2 block and the output of the teacher's self-attention block. Our analysis indicates that the distance between the Mamba-2 block and the self-attention block is strongly correlated with the model's ability to learn the teacher's distribution, as shown in Table 6. Furthermore, Figure 3 shows that a better independent alignment of the student and teacher blocks results in performance improvements, highlighting the importance of this stage in the distillation process."}, {"title": "4.3 Stage 3: Weight-Transfer and Knowledge Distillation", "content": "The final stage of the distillation process aims to fine-tune the student model to match the performance of the teacher model. Although each student mixing block is aligned with its corresponding teacher mixing block, discrepancies are still present between consecutive blocks throughout the network To bridge these gaps and address the remaining components of the language model, we transfer the remaining weights of the teacher model to the student's respective components. For Phi-Mamba, this involves the token embedding, the final layer normalization, the Language Model head, and the MLP and input norm at each block (see Figure 2). We then fine-tune the complete end-to-end student model under teacher supervision. Concretely, we use a distillation loss to encourage the student model to mimic the distribution of the teacher model's logits, also known as knowledge distillation (Hinton, Vinyals, and Dean 2015):\n$\\min_{\\Phi} L_{CE} (\\text{TeacherModel}(x), \\text{StudentModel}_\\Phi(x))$ (5)\nwhere $x$ is the input tokens to the models.\nIt has been hypothesized that much of the information stored in language models resides in MLP blocks (Niu et al. 2024). To utilize the work already done pretraining the teacher, MOHAWK adjusts the structure of the student blocks to utilize the MLP in the same way as the teacher model, effectively swapping the teacher's matrix mixer with that of the student.\nInterestingly, during this step, the MLP weights can be kept frozen while keeping the model performant. This showcases Mamba-2's powerful expressiveness crucial for replacing Attention, cuts the number of trained parameters by more than half, and, in larger models, helps prevent the student model from experiencing catastrophic forgetting of the teacher model's information. We validate Mamba-2's ability to do so in Table 8."}, {"title": "4.4 Phi-Mamba architecture", "content": "Combining the three stages of MOHAWK, we introduce the Phi-Mamba architecture, which merges the Mamba-2 model of Dao and A. Gu (2024) with the Phi-1.5 Transformer model of Gunasekar et al. (2023). It consists of a stack of Phi-"}, {"title": "5 Empirical Validation", "content": "We start by examining in Section 5.1 downstream evaluation scores of our MOHAWK-distilled Phi-Mamba-1.5B and Hybrid-Phi-Mamba-1.5B, empirically showing that they outperform all previous subquadratic and hybrid models, respectively, while having better time and memory complexities.\nNext, Sections 5.2, 5.3, and 5.4 analyze our three-stage framework in reverse order of their introduction, disentangling the compounding effects of MOHAWK on the transfer of learned representations to the student model. Additionally, to form a baseline that mirrors the Phi-Mamba distillation process in ideal conditions, we employed MOHAWK to distill a Phi-1.5 into another Phi-1.5, transferring all weights except Attention layers, which were initialized from scratch. The specifications of our final Phi-Mamba model distilled using MOHAWK are provided in Section 5.5.\nSection 5.6 outlines the architecture selected for the hybrid Phi-Mamba, discusses the ablations regarding the number and placement of interleaved attentions, and tackles a limitation potentially caused by the distillation process."}, {"title": "5.1 Final Results", "content": "We empirically validate that our framework, MOHAWK, is able to achieve better performance on various downstream benchmarks compared to previous subquadratic models of similar size. We distill the Phi-1.5-1.3B model into Phi-Mamba-1.5B as well as Hybrid-Phi-Mamba-1.5B. Our final Phi-Mamba model is distilled on 3 billion tokens (distributed as 80M in Stage 1, 160M in Stage 2, and 2.76B tokens in Stage 3 as described in Section 5.5) from the C4 dataset, with a sequence length of 2048. This constitutes less than 1% of the resources used by many top-performing subquadratic open-source models (e.g., the open-source Mamba and Mamba-2 models, which are pretrained on 315 billion tokens). The Hybrid-Phi-Mamba-1.5B is distilled on a budget of 5 billion tokens from the same dataset."}, {"title": "5.2 Stage 3 (Weight-Transfer and Knowledge Distillation)", "content": "As described in Section 4.3, this phase employs a simple end-to-end distillation of teacher-model logits. It leverages the alignment among all sequence mixers and successive blocks to jointly fine-tune all components of the network. Experiments shown in Table 3 highlight the relevance of implementing this end-to-end alignment, with all three architectures achieving their highest scores only after this phase. Predictably, the impact of end-to-end alignment varies by architecture: models with more mixing layers similar to the teacher model see a reduced importance of this phase.\nStage 3 is the only stage in MOHAWK that trains the student model end-to-end and can be seen as the \u201cmain\" stage. Many distillation methods employ only this stage; however, Table 3 shows that using only end-to-end knowledge distillation is less than ideal. Although it is slightly advantageous to use only Stage 3 compared to only Stage 2 for both Phi-Mamba and Hybrid-Phi-Mamba, there is a significant gap between using only Stage 2 versus using Stage 2 + 3."}, {"title": "5.3 Stage 2 (Hidden-State Alignment)", "content": "Following the analysis of the model's end-to-end distillation in Stage 3, we evaluate the impact of aligning the hidden-state outputs of mixer blocks (Stage 2) on both the subsequent Stage 3 process and overall downstream model performance. We accomplish this by training Phi-Mamba instances from scratch using Stage 2 to various token counts. From these checkpoints, we proceed to Stage 3 training, ending with different total budgets to allow us to analyze how the degree of Stage 2 \"pretraining\" impacts Stage 3 performance at various token budgets."}, {"title": "5.4 Stage 1 (Matrix Mixer Orientation)", "content": "Motivated by our previous finding, we then analyze how matching the matrix mixers can decrease the overall mixer block's hidden-state distance with the teacher model even further. Similarly to our previous protocol, we assess the positive impact of the current stage on the following phase's metrics and final model's performance by comparing models with varying amount of Stage 1 and Stage 2 training on both stage metrics."}, {"title": "5.5 Training the Final Phi-Mamba Model", "content": "After confirming the importance of the stages in Section 5.2, Section 5.3, and Section 5.4, we proceed to distill the final Phi-Mamba model using the three elements of MOHAWK. We use 80M tokens for Stage 1, due to the strong performance of the token count in both the matrix and hidden state distances (Figure 4). Stage 2 was distilled for 160M tokens given the apparent saturation of both hidden state distance and perplexity compared to the other initialization states, such as 10M, 20M, 40M, etc. (Figure 3). We employed Stage 3 to a total of 3B tokens across all stages and observed that the previously optimal learning rate applied for training training laws led to instabilities in training, particularly spikes in evaluation perplexity. Decreasing the learning rate for Stage 3 mitigated this issue (Appendix A). We hypothesize that the instability is due to the Stage 1 + 2 initialization's Mamba component being quite similar to that of the teacher model, so a large learning rate coupled with disconnect between blocks, which are mended in Stage 3, can cause training instabilities. The performance of the final model is reported in Table 1."}, {"title": "5.6 Hybrid Phi-Mamba Model", "content": "Recently, models that integrate both Attention mechanisms and SSM layers have been proposed (Hatamizadeh and Kautz 2024; Lieber et al. 2024; Ren et al. 2024), delivering better results than using either architecture independently. Empirically, incorporating a limited number of Attention layers does make the training and inference time quadratic, although this effect is mitigated by the small number of Attention layers used.\nWe distill the Phi-1.5 model into a hybrid version, preserving only four original Attention layers and converting all other Attention blocks to Mamba-2 blocks through MOHAWK. This hybrid model achieves a downstream evaluation score of 66.0 (refer to Table 2), closely approaching the performance of the pure Attention Transformer architecture and exceeding the Phi-Mamba average score of 65.1. Hybrid-Phi-Mamba also performs well compared to other Attention-Mamba hybrids at the 1.5B size range while using less Attention layers and less overall parameters."}, {"title": "5.7 Approximating Self-Attention", "content": "Given the impact that Stage 1 (Matrix Orientation) and Stage 2 (Hidden-State Alignment) have on Stage 3's (Weight Transfer and Knowledge-Distillation) effectiveness, we delve deeper into Mamba-2's capability to learn interactions taught by Self-Attention. We first examine in Section 5.7.1 the extent to which a Mamba-2 sequence transformation can approximate a self-attention matrix. Next, we investigate in Section 5.7.2 whether this capability is evident in an end-to-end language model such as Phi-1.5."}, {"title": "5.7.1 Self-Attention Approximation with Structured Matrix Mixers", "content": "We start by testing the ability of various matrix mixer families to match the empirical self-attention matrices of a pretrained Transformer. We take 1000 samples from each layer of a Llama2-7b-Chat model (Touvron et al. 2023), materialize the attention matrices, and project them onto given classes of structured matrices.\nIn particular, to describe the class of linear attention matrices (3.1), we use the fact that Q and K are projections of the input $x \\in \\mathbb{R}^{d_{in}}$ onto $\\mathbb{R}^{d_{out}}$, and therefore their rank is bounded by min {$d_{in}, d_{out}$}. For multihead linear attention, $d_{out}$ (also known as head dimension) is typically a small value (e.g., Phi-1.5 and Llama2-7b-Chat have head dimensions of 64 and 128, respectively). Thus, we approximate this family of sequence mixers using causal low-rank matrices $L \\circ QK^T$, where L is a lower-triangular causal mask of 1s, and Q, K are in $\\mathbb{R}^{n\\times d}$ with d \u00ab n (indicating that the head dimension is substantially smaller than the sequence length).\nTo describe the multi-head Mamba-2 matrix family, we utilize the state space dual (SSD) layer (3.2) in a manner similar to the previous linear attention, but now the causal matrix L possesses an n-degree rolling multiplicative structure for SSD which can be seen as a more expressive mask that generalizes the causal mask (Section 3.2).\nBoth causal low-rank and SSD matrix families were approximated with 10,000 steps of gradient descent per sample. Finally, to approximate the general class of SSM matrix mixers, we utilize balanced truncation, a projection algorithm that does not rely on gradients. This method is mainly known in the field of time-invariant Dynamical System model reduction (Gugercin and Antoulas 2004) and has been modified for use in time-varying systems (Sandberg and Rantzer 2004). Similarly, for the family of causal Toeplitz matrices, which represent a convolution operation, we employ a simple heuristic that minimizes the error for each attention matrix."}, {"title": "5.7.2 Self-Attention Replacement in Language Models", "content": "Since the experiments in Section 5.7.1 were designed to approximate a self-attention matrix under controlled conditions, we further validate the ability of a Mamba-2 block to replace an Attention layer within a language model. Firstly, we create two variants of our architecture, Phi-Toeplitz and Phi-LR, and run the MOHAWK process for 1B tokens at each stage (see Table 7) to verify that the previous finding hold in a multilayer, end-to-end model case.\nSecondly, we run MOHAWK while freezing various parts of the Phi-Mamba modules (refer to Table 8), revealing that limiting the trainable elements to the Mamba-2 blocks (excluding the embedding, head and all MLP layers) results in only a minor performance decrease during MOHAWK distillation."}, {"title": "6 Discussion and Conclusion", "content": "Our experiments shows that the Mamba-2 model can be successfully distilled from a pretrained Transformer teacher model, utilizing its extensive knowledge learned from custom datasets and higher computational resources. Despite using less than 100x data compared to many open-source models, including Mamba, our subquadratic model outperforms other subquadratic models in various benchmark tests by a wide margin.\nThe MOHAWK framework's multi-stage process which gradually increased the scope of distillation is essential extracting the teacher model's knowledge to the fullest extent as shown in our ablations and training laws. We continue to find the effectiveness of MOHAWK when distilling hybrid Attention-SSM models and provide ablations on the number and position of Attention layers.\nAdditionally, we demonstrate that Mamba-2's relationship to Transformers is evident not only in theory, but also in practice, as it captures interactions similar to those of Transformers, and is able to replace Attention with little drop in performance. Coupled with past research which has posited that much of a language model's knowledge is embedded in the MLP blocks, we believe that any subquadratic model with a sufficiently expressive matrix mixer can replicate the behavior of pretrained Transformers, bringing quadratic knowledge to subquadratic models. We recommend further research to explore the role of sequence mixing layers in subquadratic models and their impact on performance. Advancements in both the distillation process and the sequence mixer architecture could lead to further improved performance in a range of tasks. We propose that \u201ctrainability\u201d and \u201cdistillability\u201d are distinct properties of the models, and therefore, distillation techniques should be more appropriately tailored to the model."}, {"title": "A Experiments and Experimental Details", "content": "To construct Section 5.5, we performed grid searches for training in Stages 1, 2, and 3 independently from scratch to find the optimal hyperparameters. We explored learning rates lr = {1, 2, 5} \u00d7 10{-3,-4} and batch sizes 2{15,16,17,18}. AdamW Optimizer was used with \\beta = (0.9, 0.95), incorporating a weight decay of 0.1, gradient clipping at 1.0, and a Warmup-Stable-Decay (WSD) scheduler with 10% warmup and 10% decay utilizing linear warmup and cooldown functions. Automatic mixed precision training to bf16 was used in all stages. For Stages 1 and 2, we initially fixed the batch size at 2^{16}, then varied the learning rates. After identifying the optimal learning rate, we adjusted the batch sizes and subsequently finalized the learning rate after fixing the batch size. Consequently, Stage 1 used bs = 2^{15}, lr = 5 \\times 10^{-4} and Stage 2 used bs = 2^{15}, lr = 2 \\times 10^{-3}. In Stage 3, we set the batch size to 2^{19} \\approx 0.5M and focused solely on varying the learning rate, resulting in 5 \\times 10^{-4}. Stages 1 and 2 were trained to 200M steps each while Stage 3 extended to 1B steps. For the Phi-Mamba ultimate model, the Stage 3 learning rate was reduced to 2 \\times 10^{-4} to enhance stability.\nIn the development of the training law (see Figure 3), we executed a single \"continuous\" run initialized from a state that included several checkpoints. The warm-up period was determined as 10% of the tokens processed during the continuous run. For instance, if the model's goal was to process 640 million tokens, and it started from a run that had processed 40 million tokens, then the warm-up would be set at 60 million tokens. The checkpoints recorded during the warm-up phase were preserved as they were, while subsequent checkpoints underwent a cooling of 10% of the current phase. To illustrate, in the scenario mentioned earlier, a checkpoint at 320 million tokens during the 40M to 640M run would maintain the original warmup, while the cooldown would span 28 million tokens. Conversely, a checkpoint at 80 million tokens within the warm-up phase would be saved without any cooldown."}, {"title": "B Applying Mamba-2 as a Black Box", "content": "As noted previously Section 4.4, our Mamba-based sequence mixer is slightly modified from the original to make it more amenable for distilling from a Transformer architecture. In particular, the Mamba-2 sequence mixer is treated entirely in discrete time by projecting the input onto the matrix A and removing the discretization parameter \\Delta. Even though this formulation is somewhat different from Mamba-2, the original algorithm remains applicable through a reduction expressed in Appendix B."}, {"title": "C Attention Matrix Approximation Details", "content": "This section serves as a complement to Section 5.7.1 and outlines the methods employed to create Table 6. Appendices C.1 to C.5 describe our strategies for finding a matrix within the specified families that closely approximates the original attention matrix using a selected distance metric. Formally, we consider the following optimization problem:\n$\\min_{X \\in M} ||M - X||$ (6)\nwhere M is the subspace of a specific matrix family, M is the attention matrix, and ||.|| corresponds to a selected distance metric. In the following sections, we explore different methods and matrix families for this optimization problem."}, {"title": "C.1 Semi-Separable Matrix Approximation"}]}