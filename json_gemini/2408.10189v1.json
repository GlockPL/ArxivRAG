{"title": "Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models", "authors": ["Aviv Bick", "Kevin Y. Li", "Eric P. Xing", "J. Zico Kolter", "Albert Gu"], "abstract": "Transformer architectures have become a dominant paradigm for domains like language modeling but suffer in many inference settings due to their quadratic-time self-attention. Recently proposed subquadratic architectures, such as Mamba, have shown promise, but have been pretrained with substantially less computational resources than the strongest Transformer models. In this work, we present a method that is able to distill a pretrained Transformer architecture into alternative architectures such as state space models (SSMs). The key idea to our approach is that we can view both Transformers and SSMs as applying different forms of mixing matrices over the token sequences. We can thus progressively distill the Transformer architecture by matching different degrees of granularity in the SSM: first matching the mixing matrices themselves, then the hidden units at each block, and finally the end-to-end predictions. Our method, called MOHAWK, is able to distill a Mamba-2 variant based on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens and a hybrid version (Hybrid Phi-Mamba) using 5B tokens. Despite using less than 1% of the training data typically used to train models from scratch, Phi-Mamba boasts substantially stronger performance compared to all past open-source non-Transformer models. MOHAWK allows models like SSMs to leverage computational resources invested in training Transformer-based architectures, highlighting a new avenue for building such models.", "sections": [{"title": "1 Introduction", "content": "Large language models based upon Transformer architectures have become a staple of natural language processing but suffer from their reliance on quadratic self-attention \u2013 the need to compute inner products between tokens at all positions up to the context length. This has motivated the development of several alternative subquadratic models, either approximations of self-attention (Katharopoulos et al. 2020) or entirely different architectures, such as state space models (SSMs)\nIn this paper, we present an approach for training subquadratic state-space models (specifically from the class of Mamba SSMs (A. Gu and Dao 2023)) through the distillation of different elements of a pretrained Transformer model. The key intuition is viewing both Attention and SSMs as sequence transformations that mix different token embeddings by applying different classes of matrices across them. Sequence model architectures can then be factored into separate (i) sequence mixing and (ii) channel mixing blocks, e.g., a Transformer is composed of Attention (sequence mixer) and MLP (channel mixer) blocks. Using this breakdown, we can separately distill the mixing elements of each model explicitly at different"}, {"title": "2 Related Work", "content": "Sequence Models. State-of-the-art autoregressive language models have been pretrained on massive amounts of data, resulting in models that exhibit extensive downstream capabilities, such as zero-shot translation and long-range reasoning (Brown et al. 2020; Gunasekar et al. 2023; Touvron et al. 2023). Recent work has focused on addressing the quadratic complexity of Transformers by developing subquadratic alternatives based on RNN (Beck et al. 2024; Peng et al. 2023), SSM (A. Gu and Dao 2023; Sun et al. 2023), and linear attention mechanisms (Dao, Fu, et al. 2022; Katharopoulos et al. 2020; Liu, Zaharia, and Abbeel 2023; Qin, S. Yang, et al. 2024; S. Yang et al. 2024), highlighting the importance of efficient sequence models in the era of large-scale autoregressive language models.\nIn addition, to combine different capabilities while maintaining efficiency, hybrid models that integrate attention mechanisms with subquadratic methods have been proposed (Fu et al. 2023; Lieber et al. 2024; Ren et al. 2024; Z. Wang et al. 2024). These models typically feature a limited number of attention layers, thus maintaining the quadratic complexity at a relatively low factor.\nSSM Architectures. GSS was the pioneer in integrating SSMs into gated neural network architecture for language modeling (Mehta et al. 2022). H3, inspired by the combination of S4 and linear attention (Katharopoulos et al. 2020), employs SSMs with shift and diagonal matrices and multiplicative operations on input projections, extending this formulation to broader recurrences, a foundation for subsequent architectures (Fu et al. 2023). Selective S4 incorporates S4 as a black box that generates a binary mask applied to the input, an architectural modification akin to gating mechanisms (J. Wang et al. 2023). Mamba (A. Gu and Dao 2023), combines the H3 block with the ubiquitous MLP block of modern neural networks by interleaving them, resulting in a more powerful architecture. The Mamba-2 block simplifies the Mamba block by removing sequential linear projections; the SSM parameters A,B,C are produced at the beginning of the block rather than as a function of the input X of the SSM. Finally, Mamba-2 (Dao and A. Gu 2024) was introduced as a variant of Mamba which leverages the structured state space duality (SSD). The Mamba-2 core layer is 2-8x faster than Mamba's selective SSM while continuing to outperform Transformers in language modeling.\nDistillation. Knowledge distillation can be used to transfer knowledge from a large teacher model to a smaller student model, resulting in a more efficient model that retains the performance of the teacher model (Hinton, Vinyals, and Dean 2015). Distillation has been applied to various language modeling tasks, such as text generation (Chen et al. 2020; Haidar and Rezagholizadeh 2019), machine translation (Hahn and H. Choi 2019; Tan et al. 2019; Zhou, Neubig, and J. Gu 2021), and question-answering system (Hu et al. 2018; Z. Yang et al. 2019).\nDistillation in language models has been largely focused on compression: turning a larger pretrained Transformer into a smaller one by utilizing the weights of the teacher model (Jha et al. 2023; W. Wang et al. 2020; Xia et al. 2023). Some of the techniques proposed look similar to ours; for example, W. Wang et al. (2020) match attention matrices in a step similar to our matrix orientation, and Liang et al. (2023) align outputs of each block (i.e., the hidden states). However, these differ in subtle and important ways because of our setting; for example, the former uses a different loss function than us that relies on softmax attention, and the latter is an end-to-end objective while our hidden state alignment occurs completely independently block-per-block. Consequently, prior work has observed that combining these objectives does not actually help and even might hurt distillation (Jha et al. 2023), whereas we show that our techniques all significantly help improve the student model.\nA smaller body of work has focused on our objective of distilling across architectures, in particular, turning a pretrained Transformer into a different architecture (usually a recurrent model of some form) of the same size. Kasai et al. (2021) first tried turning a pretrained softmax attention into a linear attention by directly transferring weights and continuing fine-tuning; a similar approach was taken by concurrent work (Mercat et al. 2024). Recently, Zhang et al. (2024) also proposed distilling into linear attention by first matching attention matrices. Our approach differs by using a more constrained loss function that works beyond linear attention; incorporating more fine-grained alignment (e.g., the hidden state alignment step); and using recent, more expressive classes of efficient student models (Mamba-2), which we show are significantly easier to distill (Table 7)."}, {"title": "3 Background and Overview", "content": "To facilitate a clear understanding of our distillation approach, we start with the necessary background and definitions. An overview of the Mamba-2 architecture, which forms the foundation of our Phi-Mamba model, is also provided."}, {"title": "3.1 Matrix Mixers", "content": "Following Dao and A. Gu (2024), we refer to an equivalent function that represents the input and output of a sequence model as a sequence transformation or a sequence mixer. Formally,\nDefinition 1 (Sequence Transformation). We use the term sequence transformation to refer to a parameterized map on sequences Y = f_\\theta(X) where X, Y \\in \\mathbb{R}^{(T,P)} and \\theta is an arbitrary collection of parameters. T represents the sequence or time axis; subscripts index into the first dimension, e.g. X_1, Y_1 \\in \\mathbb{R}^P.\nTo put it differently, sequence mixers combine tokens at various time steps, facilitating the model's comprehension of temporal information and interactions. Sequence transformations form the foundation of deep sequence models, being integral components of neural network frameworks such as Transformers. A particular family of sequence transformations can be represented by Y = MX for a matrix M\\in \\mathbb{R}^{(T,T)}, which we refer to as a sequence transformation matrix or matrix mixer.\nAn example of such a matrix mixer is the vanilla self-attention, Softmax(QK^T), which is applied to the input-dependent V resulting in the familiar Softmax(QK^T)V. Similarly, Linear Attention (Katharopoulos et al. 2020) has a sequence transformation matrix of the form K^T. In addition, we can easily obtain their causal variants by multiplying by L, a lower triangular matrix filled with 1s, to obtain L \\circ Softmax(QK^T) and L \\circ QK^T, respectively. Another example is a Toeplitz matrix T used to perform discrete convolution on input X, resulting in TX (Qin, Han, et al. 2023).\nA naive approach to computing the output of a sequence transformation is to multiply the input sequence X by the matrix M. However, this approach has a time complexity of O(T^2), which is prohibitive for long sequences. Subquadratic sequence transformations, such as Mamba-2, have been developed to address such inefficiencies through structured matrix multiplication."}, {"title": "3.2 Mamba-2", "content": "Mamba-2 (Dao and A. Gu 2024), a type of structured state space models (SSMs) (A. Gu 2023; A. Gu, Goel, and R\u00e9 2022), was recently introduced. Similarly to the original Mamba model (A. Gu and Dao 2023), Mamba-2 uses a time-varying state-space model which can selectively focus on or ignore inputs due to its input-dependent parameterization of the system components. The time-varying SSM is defined as follows:\n \\begin{aligned}\n    h_{t+1} &= A_t h_t + B_t X_t\\\\\n    Y_t &= C_t h_t\n\\end{aligned}  (1)\nHere, B_t and C_t are input-dependent projections of the system, as in Mamba-1; however, A_t is the identity matrix I multiplied by a scalar a_t. The above formulation also differs from the previous one by treating the underlying sequence as originating from a discrete signal instead of a continuous one and therefore omits the sampling component \\Delta t from the original Mamba model.\nImportantly, Mamba-2 draws a new connection between SSMs and Transformers, termed Structured State Space Duality (SSD), which shows that a special case of SSMs can be viewed as a form of causal linear attention. In particular, fixing A_t = I (a further restriction of Mamba-2 to a_t = 1) results in the formulation of causal linear attention (Katharopoulos et al. 2020) with the matrices B and C representing the projections of the key and the query, respectively, while the input projection X corresponds to the projection of the value.\nMamba-2 as a matrix sequence transformation. Inspired by the aforementioned connection between SSMs and Transformers, Dao and A. Gu (2024) shows that Mamba-2's SSD mixer family is equivalent to sequentially-semi-separable"}, {"title": "4 Methods", "content": "Throughout this section, we will describe each phase of MOHAWK. Specifically, we will cover the stages of matrix orientation, hidden-state alignment, and knowledge distillation, all three of which are crucial for developing an effective student model from the pretrained Transformer model. Unlike traditional distillation techniques, the student model retains the overall architecture of the teacher model, differing only in the replacement of the attention matrix mixer with a subquadratic alternative. We will progressively unveil our architecture, Phi-Mamba, along with the specifics of its distillation process. This section concludes with an in-depth description of the Phi-Mamba architecture and its hybrid version, which surpasses the performance of other subquadratic matrix mixers. Further examinations of the effectiveness of the method and ablation studies are discussed in Section 5.\nFor clarity, the term block refers to the repeating components that form the end-to-end model. The blocks are composed of layers, such as the self-attention layer (including projections), the SSM layer (including the mixer and convolution), and the convolutional layer. In this manner, many Transformer models, like Llama (Touvron et al. 2023), are viewed as a stack of alternating self-attention and MLP blocks, whereas the Phi and Phi-Mamba models are comprised of Phi blocks that have parallel Attention/SSM and MLP blocks."}, {"title": "4.1 Stage 1: Matrix Orientation", "content": "The first stage of MOHAWK aims to align the student matrix mixer with the teacher's self-attention matrix. Achieving this alignment is a two-step process: first, at every mixing layer, the student components preceding the matrix mixer are set to match the teacher's components. This ensures that each layer's input undergoes the same transformation up to the matrix mixer section. Consequently, the only variation from the input to the mixing process is the matrix calculation. We then minimize the distance between the matrix mixer, e.g., the self-attention matrix and the materialized SSM matrix (2), of each layer within the student and teacher models:\nmin_\\Phi \\|\\text{TeacherMixer}(u) - \\text{StudentMixer}_\\Phi (u) \\|_F (3)\nwhere \\Phi denotes the parameters within the student's sequence mixing layer, and u indicates any arbitrary input. In our experimental setup, u was chosen as the output from the teacher model's preceding layer to better mimic the input distribution to the layer. This stage ensures that the student and teacher models have roughly similar mixing layers and sets the foundation for the subsequent stages of the distillation process. In particular, this stage can be done in parallel across all the student layers, as the inputs to the student and teacher blocks are identical."}, {"title": "4.2 Stage 2: Hidden-State Alignment", "content": "Following the optimization of Equation (3), we must still address the differences between the outputs of the student and teacher blocks. To achieve this, we further align the components of the two blocks using initialization and distillation. Specifically, our goal is to match each student and teacher mixing blocks by minimizing the L2 norm of their output (e.g., the entire Mamba block with the self-attention block):\nmin_\\Phi \\|\\text{AttnBlock}(u) - \\text{StudentMixerBlock}_\\Phi (u) \\|_2 (4)\nwhere similar to Section 4.1, \\Phi represents student's block parameters, and u is an input. Once again, this stage can be done in parallel across all the student layers.\nIn the case of Mamba-2, we modify the remaining components to be identical to the Phi-1.5's Attention block, so that the overall functionality is preserved from Stage 1. Concretely, we initialize the gate (see Figure 2) to a constant value of 1 to \"open\" the gate, canceling its initial effect. In addition, we remove the normalization prior to the output projection, as it cannot be set to align with the Attention block. We then minimize the distance between the output of the Mamba-2 block and the output of the teacher's self-attention block. Our analysis indicates that the distance between the Mamba-2 block and the self-attention block is strongly correlated with the model's ability to learn the teacher's distribution, as shown in Table 6. Furthermore, Figure 3 shows that a better independent alignment of the student and teacher blocks results in performance improvements, highlighting the importance of this stage in the distillation process."}, {"title": "4.3 Stage 3: Weight-Transfer and Knowledge Distillation", "content": "The final stage of the distillation process aims to fine-tune the student model to match the performance of the teacher model. Although each student mixing block is aligned with its corresponding teacher mixing block, discrepancies are still present between consecutive blocks throughout the network To bridge these gaps and address the remaining components of the language model, we transfer the remaining weights of the teacher model to the student's respective components. For Phi-Mamba, this involves the token embedding, the final layer normalization, the Language Model head, and the MLP and input norm at each block (see Figure 2). We then fine-tune the complete end-to-end student model under teacher supervision. Concretely, we use a distillation loss to encourage the student model to mimic the distribution of the teacher model's logits, also known as knowledge distillation (Hinton, Vinyals, and Dean 2015):\nmin_\\Phi L_{CE} (\\text{TeacherModel}(x), \\text{StudentModel}_\\Phi (x)) (5)\nwhere x is the input tokens to the models.\nIt has been hypothesized that much of the information stored in language models resides in MLP blocks (Niu et al. 2024). To utilize the work already done pretraining the teacher, MOHAWK adjusts the structure of the student blocks to utilize the MLP in the same way as the teacher model, effectively swapping the teacher's matrix mixer with that of the student.\nInterestingly, during this step, the MLP weights can be kept frozen while keeping the model performant. This showcases Mamba-2's powerful expressiveness crucial for replacing Attention, cuts the number of trained parameters by more than half, and, in larger models, helps prevent the student model from experiencing catastrophic forgetting of the teacher model's information. We validate Mamba-2's ability to do so in Table 8."}, {"title": "4.4 Phi-Mamba architecture", "content": "Combining the three stages of MOHAWK, we introduce the Phi-Mamba architecture, which merges the Mamba-2 model of Dao and A. Gu (2024) with the Phi-1.5 Transformer model of Gunasekar et al. (2023). It consists of a stack of Phi-"}, {"title": "5 Empirical Validation", "content": "We start by examining in Section 5.1 downstream evaluation scores of our MOHAWK-distilled Phi-Mamba-1.5B and Hybrid-Phi-Mamba-1.5B, empirically showing that they outperform all previous subquadratic and hybrid models, respectively, while having better time and memory complexities.\nNext, Sections 5.2, 5.3, and 5.4 analyze our three-stage framework in reverse order of their introduction, disentangling the compounding effects of MOHAWK on the transfer of learned representations to the student model. Additionally, to form a baseline that mirrors the Phi-Mamba distillation process in ideal conditions, we employed MOHAWK to distill a Phi-1.5 into another Phi-1.5, transferring all weights except Attention layers, which were initialized from scratch. The specifications of our final Phi-Mamba model distilled using MOHAWK are provided in Section 5.5.\nSection 5.6 outlines the architecture selected for the hybrid Phi-Mamba, discusses the ablations regarding the number and placement of interleaved attentions, and tackles a limitation potentially caused by the distillation process."}, {"title": "5.1 Final Results", "content": "We empirically validate that our framework, MOHAWK, is able to achieve better performance on various downstream benchmarks compared to previous subquadratic models of similar size. We distill the Phi-1.5-1.3B model into Phi-Mamba-1.5B as well as Hybrid-Phi-Mamba-1.5B. Our final Phi-Mamba model is distilled on 3 billion tokens (distributed as 80M in Stage 1, 160M in Stage 2, and 2.76B tokens in Stage 3 as described in Section 5.5) from the C4 dataset, with a sequence length of 2048. This constitutes less than 1% of the resources used by many top-performing subquadratic open-source models (e.g., the open-source Mamba and Mamba-2 models, which are pretrained on 315 billion tokens). The Hybrid-Phi-Mamba-1.5B is distilled on a budget of 5 billion tokens from the same dataset.\nFor the remainder of this section, we will analyze the impact of the 3 stages of MOHAWK one by one. Throughout the experiments detailed in this section, we use the AdamW optimizer with \u03b2 = (0.9, 0.95), a weight decay of 0.1, and a learning rate of 1 \u00d7 10-4, combined with a Warmup-Stable-Decay (WSD) scheduler featuring 10% warmup and 10% decay. The training law figures and the final Phi-Mamba model use the regime detailed in Appendix A."}, {"title": "5.2 Stage 3 (Weight-Transfer and Knowledge Distillation)", "content": "As described in Section 4.3, this phase employs a simple end-to-end distillation of teacher-model logits. It leverages the alignment among all sequence mixers and successive blocks to jointly fine-tune all components of the network. Experiments shown in Table 3 highlight the relevance of implementing this end-to-end alignment, with all three architectures achieving their highest scores only after this phase. Predictably, the impact of end-to-end alignment varies by architecture: models with more mixing layers similar to the teacher model see a reduced importance of this phase.\nStage 3 is the only stage in MOHAWK that trains the student model end-to-end and can be seen as the \u201cmain\" stage. Many distillation methods employ only this stage; however, Table 3 shows that using only end-to-end knowledge distillation is less than ideal. Although it is slightly advantageous to use only Stage 3 compared to only Stage 2 for both Phi-Mamba and Hybrid-Phi-Mamba, there is a significant gap between using only Stage 2 versus using Stage 2 + 3."}, {"title": "5.3 Stage 2 (Hidden-State Alignment)", "content": "Following the analysis of the model's end-to-end distillation in Stage 3, we evaluate the impact of aligning the hidden-state outputs of mixer blocks (Stage 2) on both the subsequent Stage 3 process and overall downstream model performance. We accomplish this by training Phi-Mamba instances from scratch using Stage 2 to various token counts. From these checkpoints, we proceed to Stage 3 training, ending with different total budgets to allow us to analyze how the degree of Stage 2 \"pretraining\" impacts Stage 3 performance at various token budgets.\nFigure 3 demonstrates that given an adequate training budget, models beginning with weights with lower hidden state distances (after Stage 2) outperform those that depend exclusively on knowledge distillation (Stage 3). These lower hidden states are also correlated with lower starting perplexities, which in turn are correlated with downstream performance, as shown in Figure 5. Furthermore, Table 3 shows the synergy between Stage 2 and Stage 3, as applying Stage 3 on top of Stage"}, {"title": "5.4 Stage 1 (Matrix Mixer Orientation)", "content": "Motivated by our previous finding, we then analyze how matching the matrix mixers can decrease the overall mixer block's hidden-state distance with the teacher model even further. Similarly to our previous protocol, we assess the positive impact of the current stage on the following phase's metrics and final model's performance by comparing models with varying amount of Stage 1 and Stage 2 training on both stage metrics.\nFigure 4 shows that even with constrained budgets, performing Stage 1 for a small period can help with subsequent stages and their performances. Thus, even a small amount of Stage 1 training can help their respective Stage 2 models reach better hidden-state distances compared to the from-scratch counterpart. This is despite the phenomenon that the teacher and student mixers diverge and then re-converge in Stage 2 after mixer similarity is no longer directly optimized. Coupled with Section 5.3, which discovers that lower hidden state initializations lead to better perplexity and downstream"}, {"title": "5.5 Training the Final Phi-Mamba Model", "content": "After confirming the importance of the stages in Section 5.2, Section 5.3, and Section 5.4, we proceed to distill the final Phi-Mamba model using the three elements of MOHAWK. We use 80M tokens for Stage 1, due to the strong performance of the token count in both the matrix and hidden state distances (Figure 4). Stage 2 was distilled for 160M tokens given the apparent saturation of both hidden state distance and perplexity compared to the other initialization states, such as 10M, 20M, 40M, etc. (Figure 3). We employed Stage 3 to a total of 3B tokens across all stages and observed that the previously optimal learning rate applied for training training laws led to instabilities in training, particularly spikes in evaluation perplexity. Decreasing the learning rate for Stage 3 mitigated this issue (Appendix A). We hypothesize that the instability is due to the Stage 1 + 2 initialization's Mamba component being quite similar to that of the teacher model, so a large learning rate coupled with disconnect between blocks, which are mended in Stage 3, can cause training instabilities. The performance of the final model is reported in Table 1."}, {"title": "5.6 Hybrid Phi-Mamba Model", "content": "Recently, models that integrate both Attention mechanisms and SSM layers have been proposed (Hatamizadeh and Kautz 2024; Lieber et al. 2024; Ren et al. 2024), delivering better results than using either architecture independently. Empirically, incorporating a limited number of Attention layers does make the training and inference time quadratic, although this effect is mitigated by the small number of Attention layers used.\nWe distill the Phi-1.5 model into a hybrid version, preserving only four original Attention layers and converting all other Attention blocks to Mamba-2 blocks through MOHAWK. This hybrid model achieves a downstream evaluation score of 66.0 (refer to Table 2), closely approaching the performance of the pure Attention Transformer architecture and exceeding the Phi-Mamba average score of 65.1. Hybrid-Phi-Mamba also performs well compared to other Attention-Mamba hybrids at the 1.5B size range while using less Attention layers and less overall parameters.\nTable 4 shows the results for the most common placements of the four Attention layers in hybrid models. Despite all placements showing strong results, our experiments indicate that interleaving Mamba-2 layers uniformly yields superior performance on downstream evaluation benchmarks. This aligns with the solutions proposed by Samba (Ren et al. 2024), which also find that interleaving Attention layers within Mamba layers leads to improved performance.\nTable 5 examines the impact of varying the number of interleaved Attention layers. Based on previous findings in Table 4, we carry out these experiments without converting the respective Attention layers in the network while utilizing MOHAWK to distill other layers. As anticipated, preserving a greater number of Attention layers results in improved outcomes. However, we hypothesize that there is still room for improvement for distilling hybrid models due to potential variations in the distillation process for hybrid versus non-hybrid architectures. These aspects, e.g., additional gradient updates, changes in optimizer settings, etc, could be further optimized, and we leave it for future work."}, {"title": "5.7 Approximating Self-Attention", "content": "Given the impact that Stage 1 (Matrix Orientation) and Stage 2 (Hidden-State Alignment) have on Stage 3's (Weight Transfer and Knowledge-Distillation) effectiveness, we delve deeper into Mamba-2's capability to learn interactions taught by Self-Attention. We first examine in Section 5.7.1 the extent to which a Mamba-2 sequence transformation can approximate a self-attention matrix. Next, we investigate in Section 5.7.2 whether this capability is evident in an end-to-end language model such as Phi-1.5."}, {"title": "6 Discussion and Conclusion", "content": "Our experiments shows that the Mamba-2 model can be successfully distilled from a pretrained Transformer teacher model, utilizing its extensive knowledge learned from custom datasets and higher computational resources. Despite using less than 100x data compared to many open-source models, including Mamba, our subquadratic model outperforms other subquadratic models in various benchmark tests by a wide margin.\nThe MOHAWK framework's multi-stage process which gradually increased the scope of distillation is essential extracting the teacher model's knowledge to the fullest extent as shown in our ablations and training laws. We continue to find the effectiveness of MOHAWK when distilling hybrid Attention-SSM models and provide ablations on the number and position of Attention layers.\nAdditionally, we demonstrate that Mamba-2's relationship to Transformers is evident not only in theory, but also in practice, as it captures interactions similar to those of Transformers, and is able to replace Attention with little drop in performance. Coupled with past research which has posited that much of a language model's knowledge is embedded in the MLP blocks, we believe that any subquadratic model with a sufficiently expressive matrix mixer can replicate the behavior of pretrained Transformers, bringing quadratic knowledge to subquadratic models. We recommend further research to explore the role of sequence mixing layers in subquadratic models and their impact on performance. Advancements in both the distillation process and the sequence mixer architecture could lead to further improved performance in a range of tasks. We propose that \u201ctrainability\u201d and \u201cdistillability\u201d are distinct properties of the models, and therefore, distillation techniques should be more appropriately tailored to the model."}, {"title": "A Experiments and Experimental Details", "content": "To construct Section 5.5, we performed grid searches for training in Stages 1, 2, and 3 independently from scratch to find the optimal hyperparameters. We explored learning rates lr = {1, 2, 5} \u00d7 10{-3,-4} and batch sizes 2{15,16,17,18}. AdamW Optimizer was used with \\beta = (0.9, 0.95), incorporating a weight decay of 0.1, gradient clipping at 1.0, and a Warmup-Stable-Decay (WSD) scheduler with 10% warmup and 10% decay utilizing linear warmup and cooldown functions. Automatic mixed precision training to bf16 was used in all stages. For Stages 1 and 2, we initially fixed the batch size at 2^{16}, then varied the learning rates. After identifying the optimal learning rate, we adjusted the batch sizes and subsequently finalized the learning rate after fixing the batch size. Consequently, Stage 1 used bs = 2^{15}, lr = 5 \u00d7 10^{-4} and Stage 2 used bs = 2^{15}, lr = 2 \u00d7 10^{-3}. In Stage 3, we set the batch size to 2^{19} \\approx 0.5M and focused solely on varying the learning rate, resulting in 5 \u00d7 10^{-4}. Stages 1 and 2 were trained to 200M steps each while Stage 3 extended to 1B steps. For the Phi-Mamba ultimate model, the Stage 3 learning rate was reduced to 2 \u00d7 10^{-4} to enhance stability.\nIn the development of the training law (see Figure 3), we executed a single \"continuous\" run initialized from a state that included several checkpoints. The warm-up period was determined as 10% of the tokens processed during the continuous run. For instance, if the model's goal was to process 640 million tokens, and it started from a run that had processed 40 million tokens, then the warm-up would be set at 60 million tokens. The checkpoints recorded during the warm-up phase were preserved as they were, while subsequent checkpoints underwent a cooling of 10% of the current phase. To illustrate, in the scenario mentioned earlier, a checkpoint at 320 million tokens during the 40M to 640M run would maintain the original warmup, while the cooldown would span 28 million tokens. Conversely, a checkpoint at 80 million tokens within the warm-up phase would be saved without any cooldown.\nFigure 5 extends the Stage 2 versus Stage 3 comparison in Figure 3, except we measure average accuracy on downstream metrics instead of perplexity. We observe a strong correlation between the training laws of perplexity and downstream evaluation metrics. While the general trend indicates that models exposed to more tokens during the prior stage initialization tend to perform better on both perplexity and downstream metrics, the relationship is not perfectly aligned. Specifically, the order of model performance based on perplexity does not always match the order based on downstream metrics, highlighting some differences in how these metrics capture model effectiveness."}, {"title": "B Applying Mamba-2 as a Black Box", "content": "As noted previously Section 4.4, our Mamba-based sequence mixer is slightly modified from the original to make it more amenable for distilling from a Transformer architecture. In particular, the Mamba-2 sequence mixer is treated entirely in discrete time by projecting the input onto the matrix A and removing the discretization parameter \\Delta. Even though this formulation is somewhat different from Mamba-2, the original algorithm remains applicable through a reduction expressed in Appendix B."}, {"title": "C Attention Matrix Approximation Details", "content": "This section serves as a complement to Section 5.7.1 and outlines the methods employed to create Table 6. Appendices C.1 to C.5 describe our strategies for finding a matrix within the specified families that closely approximates the original attention matrix using a selected distance metric. Formally, we consider the following optimization problem:\nmin_{X\\epsilon M} ||M - X|| (6)\nwhere M is the subspace of a specific matrix family, M is the attention matrix, and ||.|| corresponds to a selected distance metric. In the following sections, we explore different methods and matrix families for this optimization problem."}, {"title": "C.1 Semi-Separable Matrix Approximation", "content": "Considering a time-varying system denoted by {A_k", "D_k}_{k\\in[l": ""}, "we can describe it using the matrix mixer T (also known as the transfer matrix) as follows:\n\\begin{bmatrix}\nD_1 & 0 & 0 & 0 & 0 & ... & 0\\\\\nC_2B_1 & D_2 & 0 & 0 & 0 & ... & 0\\\\\nC_3A_2B_1 & C_3B_2 & D_3 & 0 & 0 & ... & 0\\\\\nC_4A_{3:2}B_1 & C_4A_3B_2 & C_4B_3 & D_4 & 0 & ... & 0\\\\\n:\\\\:\\\\:\\\\:\\\\:\\\\vdots\\\\\nC_lA_{l-1:2}B_1 & C_lA_{l-1:3}B_2 & C_lA_{l-1:4}B_3 & ... & ... & ... & D_l\n\\"]}