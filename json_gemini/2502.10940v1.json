{"title": "COLA: Compute-Efficient Pre-Training of LLMs via Low-Rank Activation", "authors": ["Ziyue Liu", "Ruijie Zhang", "Zhengyang Wang", "Zi Yang", "Paul Hovland", "Bogdan Nicolae", "Franck Cappello", "Zheng Zhang"], "abstract": "Large language models (LLMs) are revolutionizing many science and engineering fields. However, their huge model sizes impose extremely demanding needs of computational resources in the pre-training stage. Although low-rank factorizations can reduce model parameters, their direct application in LLM pre-training often lead to non-negligible performance loss. To address this fundamental challenge, we introduce CoLA and its memory-efficient implementation, CoLA-M. We leverage the low-rank structure observed widely in model activations, enforcing non-linear transformations between factorized weight matrices to reduce model size, boost model capacity and training efficiency. Experiments on LLaMA models with 60 million to 7 billion parameters show that CoLA reduces the computing cost by 2x and improves training throughput by 1.86x while maintaining full-rank level performance. CoLA-M further squeezes memory cost without sacrificing throughput, offering a pre-training approach with collectively superior parameter, computing, and memory efficiency. The LLMs produced are also 2\u00d7 smaller, enabling faster inference with lower memory cost on resource-constrained platforms.", "sections": [{"title": "Introduction", "content": "Large foundation models have revolutionized the landscape of artificial intelligence, achieving unprecedented success in the language, vision, and scientific domains. In a quest to improve accuracy and capability, foundation models have become huge. Several studies (Kaplan et al., 2020; Hoffmann et al., 2022; Krajewski et al., 2024; Kumar et al., 2024) have highlighted a rapid increase in the size of the model and the number of training tokens. Models such as 175B GPT-3 (Brown et al., 2020), 405B LLaMA-3 (Dubey et al., 2024), and 540B PaLM (Chowdhery et al., 2023) are just a few examples of this trend. Under such circumstances, a large number of GPUs are needed in order to provide the computational and high-bandwidth memory capacity needed to pre-train large fundation models over long periods of time (months). The staggering increase in cost results in an unsustainable trend, prompting the need to develop cost-efficient pre-training techniques that reduce the scale, FLOPs, and GPU memory cost.\nMotivation: At the core of increasing resource utilization and cost is the simple practice of scaling up full-size linear layers in decoder-only architectures, which has proven to be a viable and straightforward strategy. Thus, to break free from this unsustainable trend, it is imperative to improve architecture efficiency. This has been widely studied in the deep learning community, involving different levels of factorization of weight matrices: from simple matrix factorizations, i.e., a singular value decomposition (SVD), to higher-order tensor factorizations such as Canonical Polyadic, Tucker, and Tensor-Train (TT) format. Extensive studies have shown that such factorizations can effectively reduce the total number of parameters needed to achieve similar performance in numerous domains (Sainath et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Novikov et al., 2015; Tjandra et al., 2017; Dao et al., 2021; Sui et al., 2024; Yang et al., 2024; Zhang et al., 2024), especially when neural networks are overparameterized.\nLimitations of state-of-art: The techniques mentioned above have been applied only to a limited degree to pre-training tasks, and their findings suggest that the pure low-rank or sparse structure often downgrades model performance (Khodak et al., 2021; Kamalakara et al., 2022; Chekalina et al., 2023; Zhao et al., 2024; Hu et al., 2024; Mozaffari et al., 2024). This has pivoted most recent work of efficient pre-training into two directions: 1) Accumulating multiple low-rank updates (Huh et al., 2024; Lialin et al., 2023); 2) Enforcing low-rank structures in gradients rather than parameters (Zhao et al., 2024; Chen et al., 2024; Huang et al.; Liao et al., 2024; Hao et al., 2024; Zhu et al., 2024). Both approaches have their limitations. 1) The accumulation of low-rank updates requires instantiating a full-rank matrix and a deeply customized training strategy that periodically merges and restarts the low-rank components. This creates computing overhead in practice and can only achieve (if only) marginal computing and memory reduction. 2) Enforcing low-rank gradients reduces only the optimizer memory and adds additional computation that downgrades training throughput. Furthermore, the memory saving caused by gradient compression becomes negligible as the training batch size increases, as activations dominate the total memory cost. A recent paper called SLTrain (Han et al., 2024) revisited the notion of parameter efficiency in foundation model pre-training, by having both low-rank factors and an unstructured sparse matrix. SLTrain effectively reduces the total number of parameters without significantly hurting model performance. However, it still introduces computing overhead on top of full-rank training due to the necessary reconstruction of low-rank factors. We note that none of the above works has achieved superior efficiency of parameter, computing, and memory simultaneously without performance drop in both training and inference for foundation model pre-training.\nContributions: In this paper, we propose CoLA: Compute-Efficient Pre-Training of LLMs via Low-rank Activation, and its memory efficient implementation CoLA-M, to achieve all the desirable properties mentioned above. We summarize our contributions as follows:\n\u2022 We propose CoLA, a novel architecture that enforces explicit low-rank activations by injecting non-linear operations between factorized weight matrices. CoLA can greatly reduce the computing FLOPS while maintaining the performance of full-rank pre-training.\n\u2022 We provide a memory efficient implementation, namely CoLA-M, to achieve superior memory reduction without sacrificing throughput.\n\u2022 We extensively pre-train LLaMA with 60M up to 7B parameters. CoLA reduces model size and computing FLOPs by 2x, while maintaining on-par performance to its full-rank counterpart. At the system level, CoLA improves 1.86\u00d7 training and 1.64\u00d7 inference throughput. CoLA-M reduces total pre-training memory by 2/3, while still manages to improve 1.3\u00d7 training throughput over full-rank baselines.\nA high-level comparison of CoLA/CoLA-M with main baselines is provided in Table 1."}, {"title": "2 Related Work", "content": "Model Compression. Recent research on efficient LLM pre-training primarily focuses on memory savings. To out best knowledge, SLTrain (Han et al., 2024) is the first method that reduces both trainable parameters and total parameters in LLM pre-training, without significantly hurting model performance. This also reduces memory usage for model, gradients, and optimizer states (see its smaller circle in Fig. 1). However, the existence of its unstructured sparse matrix S requires reconstructing W = BA + S, otherwise it will incur dense-sparse multiplications that are still memory costly (Fig. 3c). This causes additional computing than the full-rank baseline. LoRA/ReLORA (Hu et al., 2021; Lialin et al., 2023) reduces trainable parameters by freezing a full-rank W0 and training (at least in a later stage) only low-rank factors, potentially reducing memory needs. Yet, any compute savings are limited because the forward pass yields a larger compute than its full-rank counterpart, especially when the rank must stay relatively large in pre-training tasks. CoMERA (Yang et al., 2024) achieves higher model compression and FLOP reduction, but its low-rank tensor operations are GPU unfriendly. Similar to matrix-compressed approaches, CoMERA cannot avoid a performance drop either. Some works investigate pure structured sparsity or combined with low-rank factors (Hu et al., 2024; Mozaffari et al., 2024) to achieve speed up, but still show a significant performance drop during the pre-training stage.\nGradient Compression. GaLore (Zhao et al., 2024) reduces memory by projecting gradients into a low-rank space, shrinking optimizer states below the typical 2\u00d7 AdamW overhead (Loshchilov, 2017). However, it increases computation by adding up/down projections on top of already compute-heavy full-rank training. As shown in Fig. 1, its estimated FLOPs surpass full-rank training on the LLaMA-1B scale. Follow-up work (Chen et al., 2024; Huang et al.; Liao et al., 2024; Hao et al., 2024; Zhu et al., 2024) further explores low-rank gradient projection. While these methods are promising, they are mostly orthogonal to our focus. Crucially, these methods are still computing lower-bounded by the full-rank baseline. Our goal instead is to reduce computing cost to a fraction of full-rank training, thus lowering the demand of computing resources in LLM pre-training.\nThis paper presents an alternative approach that explores the low-rank property in model activations from an architectural perspective. This is conceptually different from the above model compression methods despite the similarity in their formulations. Our approach is mostly orthogonal with gradient compression techniques, meaning that they can be combined to further boost efficiency."}, {"title": "3 COLA for Efficient LLM Pre-Training", "content": "Many previous works have observed the low-rank structure of model activations in deep neural networks (Cui et al., 2020; Huh et al., 2021). We also observe this phenomenon in LLMs, i.e. the effective rank of the activations is much smaller than their original dimensionality. To quantify this, we define the effective rank r(a) of activation as the minimal number of singular values needed to preserve an a-fraction of the total spectral energy. Formally:\n$r_{\\alpha}(a) = \\min \\{ r: \\frac{\\sum_{i=1}^{r} \\sigma_{i}}{\\sum_{i=1}^{n} \\sigma_{i}} \\geq \\alpha \\},$\nwhere $\\sigma_{1}, \\sigma_{2},..., \\sigma_{n}$ are the singular values of the activation matrix, and $0 < \\alpha < 1$ is the desired ratio of preserved information. As shown in our experiments, the rapid decay of singular values [Fig. 2a] leads to much smaller $r_{\\alpha}(a)$ compared to the full dimension [Fig. 2b]. This highlights the significant low-rank nature in the activations of pre-trained LLMs. More results showing the same pattern can be found in Appendix A."}, {"title": "3.2 Low Rank Weights + Activations", "content": "Motivated by the observed low-rank nature of LLM activations, we propose to enforce explicit low-rank activation via injecting non-linearity between factorized weight matrices.\nLet $W \\in \\mathbb{R}^{d_{out}\\times d_{in}}$ be the weight matrix of an arbitrary linear layer followed by a nonlinear activation in the transformer architecture:\n$h = \\sigma (Wx)$, with $x \\in \\mathbb{R}^{d_{in}}.$\nWe replace W by two low-rank matrices $A \\in \\mathbb{R}^{r \\times d_{in}}$ and $B \\in \\mathbb{R}^{d_{out} \\times r}$, where rank r < min(din, dout) is a hyper-parameter, and inject a non-linear activation \u03c3 in the middle. This modification results in a transformation consisting of (linear \u2022 non-linear \u2022 linear) operations:\n$h' = B \\sigma(Ax)$.\nDuring the backward step, we simply apply the chain rule to compute gradients w.r.t each of the low rank factors as\n$\\nabla_{B} = \\nabla_{h'} \\sigma(Ax)^{T}, \\nabla_{z} = B^{T} \\nabla_{h'}, \\nabla_{\\omicron} = \\nabla_{z} \\odot \\sigma'(\\omicron),$\n$\\nabla_{A} = \\nabla_{\\omicron}x^{T}, \\nabla_{x} = A^{T}\\nabla_{\\omicron},$\nwhere $o = Ax$, $z = \\sigma(o)$, \u2299 denote the element-wise product. We empirically find that keeping the original nonlinearity on top of Eq. (3) does not harm the performance, nor necessarily brings benefits. However, applying Eq. (3) to all linear layers regardless of whether being followed by nonlinearity is crucial to boost model performance. We refer more details to the ablation study in Appendix E. Fig. 4 shows the architecture of each transformer block when adopting CoLA into the LLaMA architecture. We highlight the fact that only the original linear layers and (if any) their follow-up non-linear transformation are modified to the COLA formulation. Other computations such as the scaled-dot product of the self-attention, as well as residual connections and the element-wise product of LLaMA's feed-forward layers, remain unchanged."}, {"title": "3.3 Computing Efficiency", "content": "We analyze and compare the computational complexity of COLA with other efficient pre-training methods based on the LLaMA architecture. We adopt a similar notion from (Kaplan et al., 2020), where a general matrix multiply (GEMM) between an M \u00d7 N matrix and an N \u00d7 K matrix involves roughly 2MNK add-multiply operations. We denote the model inner width as d, and the inner width of the feed-forward layer as dff. For simplicity, we only show non-embedding calculations of a single sequence with token batch size of n for each decoder layer. This is because the total computation scales only linearly with the number of layers nlayer and the number of sequences nseq. Furthermore, lower-order cheap operations of complexity O(nd) or O(ndff) are omitted, such as bias, layer norm, non-linear function, residual connection, and element-wise product.\nWe show the detailed cost of the full-rank training in Table. 2. Notice that we apply the 2\u00d7 rule when calculating the backward cost. This is because for each forward GEMM that Eq. (2) describes, two GEMMs are needed to compute gradients for both the weight matrix W and the input x, and are of the same cost the forward GEMM, i.e.,\n$\\nabla_{x} = W^{T} \\nabla_{h}, \\nabla_{W} = \\nabla_{h} x^{T}.$\nWe apply the same analysis to all the following pre-training methods:"}, {"title": "4 CoLA-M: A Memory-Efficient Implementation", "content": "Although CoLA has more intermediate results from each low-rank projection and the following non-linear function (as shown in Fig. 4), we can choose strategically which ones to save in order to balance re-computations with memory overhead. In this section, we design and develop CoLA-M, a memory-efficient implementation to leverage CoLA's structural advantage to achieve superior memory saving without sacrificing throughput."}, {"title": "4.1 Memory Breakdown in Pre-Training", "content": "We assume a common notion that training modern transformers with Adam (or AdamW) involves four key memory components (Zhao et al., 2024; Han et al., 2024): model parameters, gradients, optimizer states, and activations (intermediate forward-pass results). Gradients consume 1\u00d7 model size, Adam consumes 2x, and activations typically consume 1 ~ 4\u00d7, depending on the batch size."}, {"title": "4.2 COLA Enables Efficient Checkpointing", "content": "Gradient checkpointing (GCP) (Chen et al., 2016) is a system-level technique that reduces memory usage by selectively storing (\"checkpointing\") only a subset of intermediate activations during the forward pass. When the backward pass begins, the missing activations are recomputed on the fly instead of being stored in memory, thereby lowering the memory cost. A vanilla (also the most effective) implementation of GCP in LLM pre-training is to save merely the input and output of each transformer block, and re-compute everything within each block during the backward step. Some works have investigated the optimal selection of checkpoints through both empirical and compiler view (Feng and Huang, 2021; He and Yu, 2023). Such techniques can also be developed for CoLA, and are beyond the scope of this paper.\nMotivated by the bottleneck structure of CoLA, we implement CoLA-M as saving only the low-rank activations (red circles in Fig. 4), and re-compute the up projections, and (if applicable) the self-attention (painted in sketch in Fig. 4) during the backward pass. This reduces the re-computation cost to half of the CoLA forward. We analyze the memory and re-computation cost using the same notions as in Section 3.3 and denote h as the number of attention heads. We further simplify the analysis under LLaMA architecture by uniformly assuming dff \u2248 2.5d. The memory and re-computation overhead are shown in Table 4. We refer the detailed analysis to Appendix C."}, {"title": "5 Experiments", "content": "We validate our proposed methods by extensively pre-training LLaMA-like LLMs from 60M to 7B scales. Experiments were performed on NVIDIA A100/H100 GPUs. We closely follow the experiment settings of (Zhao et al., 2024; Han et al., 2024), and directly compare CoLA with their reported results. We use the C4 dataset (Raffel et al., 2020), which is a colossal, cleaned version of Common Crawl's web crawl corpus. C4 dataset has been widely used for pre-training LLMs. Trainings were done without data repetition on a sufficiently large amount of tokens. We compare COLA with baselines including full-rank pre-training, ReLoRA (Hu et al., 2021), GaLore (Zhao et al., 2024), and SLTrain (Han et al., 2024), with a focus on methods that explore model efficiency.\nWe implement CoLA and its memory efficient variant CoLA-M by parameterizing all linear layers into the proposed linear-nonlinear-linear composition [i.e. Eq. (3)], and keep all other parameters and operations unchanged. We use AdamW optimizer and cosine annealing learning rate scheduler (Loshchilov and Hutter, 2016) with warm-up. We remark that CoLA is NO more sensitive to optimizer-related hyper-parameters. We refer more details to Appendix D."}, {"title": "5.2 Scaling Behavior", "content": "We briefly discuss how CoLA might scale differently compared to full-rank training. A comprehensive investigation of this topic is beyond the scope of this work due to the huge computing cost.\nTable 8 shows a few experiments on how CoLA might be improved when computing is scaled up. The default rank choice reduces the compute cost to about a half of full-rank training, without significantly hurting the model performance. Meanwhile, if we relax the computing restriction and increase the rank to be greater than one quarter of d, then COLA outperforms full-rank training in all three scales, while still being able to reduce the computing cost. One might argue that full-rank training can also be scaled down to a similar compute of COLA and might perform similarly. We implement such baselines in Table 8 and refer this setup to \"Control\". We typically reduce the number of layers or the model width of full-rank models to scale down their computing cost. We find empirically that they will tend to reduce performance quickly and dramatically underperform CoLA."}, {"title": "5.3 System Measurements", "content": "We further investigate the efficiency of CoLA from a more practical perspective. It is often observed that a theoretically less expensive method can have worse system-level performance due to poorly designed hardware implementation or lack of system-aware optimizations. We show that this is NOT the case for CoLA, by illustrating that its out-of-the-box system performance already significantly outperforms the full-rank training and other efficient training methods. We focus on the actual memory usage and the pre-training throughput.\nFig. 8 shows the measured throughput for pre-training the LLaMA/CoLA 1B model. The sequence batch size is set as 16, which fully utilizes the A100 GPU. Among all these methods, COLA and CoLA-M are the only two that show higher GPU throughput than the full-rank baseline, while all other methods downgrade throughput due to their computing overhead. In particular, CoLA-M, the memory-efficient CoLA that significantly reduces overall GPU memory, still shows higher training throughput despite the re-computation overhead. Meanwhile, vanilla GCP, which uses a similar idea of trading compute for memory, reduces throughput from the full-rank baseline by 26%. We show further the details of these measurements in Table 9, and also compare their estimated FLOPs. At both 1B and 7B scale, CoLA-M manages to almost halve the computing cost and reduce the memory usage by two thirds, achieving a great balance between computing and memory efficiency. We refer more details of our system profiling to Appendix. F."}, {"title": "5.4 Inference Performance", "content": "We highlight the fact that CoLA not only reduces pre-training resources but also speeds up inference and reduces its memory cost. Table 10 shows that CoLA improves inference throughput by up to 1.64x while reducing memory cost by up to 1.67x."}, {"title": "6 Conclusions", "content": "We propose CoLA, and its memory efficient variant CoLA-M, to achieve collectively parameter, compute and memory efficiency at both pre-training and inference time for large foundation models. COLA effectively reduces 2\u00d7 model size while preserving full-rank level performance. More importantly, we show the reduction does not come with additional compute. Instead, CoLA halves compute and almost doubles training throughput from its full-rank counterpart. When memory is of higher concerns, CoLA-M trades only minimum compute for state-of-the-art memory reduction during pre-training, meanwhile still reducing compute and improving throughput. We hope our work will inspire the community to further investigate the architecture efficiency that has been overlooked and under-discovered for large foundation models."}, {"title": "7 Limitations", "content": "This work limits the study of our proposed formulation under LLaMA-like architectures. The adaptation of COLA to other architectures is conceptually trivial, but their performance is to be evaluated via real experiments. In this work, we only pre-train each model to be roughly compute-optimal (for original LLaMA models, not CoLA), while industry-produced LLMs (that are of similar scales) are often over-trained. It is worth investigating the performance of CoLA when significantly over-trained. We leave this computing-expensive research for future work."}, {"title": "A Observation of Low-Rank Activation in Pre-Trained GPT2", "content": "In this section, we further show the low-rank structure in model activations evaluated on a pre-trained GPT-2 (Radford et al., 2019) small. The evaluation is conducted with sequence batch size of 64 and sequence length of 1024. We fix \u03b1 = 0.95 throughout this section. Similar patterns are observed from the attention layers (Fig. 9, 10, 11). The low-rank nature of activations is evident across all the different components of the model. This suggests that despite the high-dimensional representations, the effective dimensionality of the activations remains constrained."}, {"title": "B Detailed Compute Analysis", "content": "According to Table. 2, the total compute of full-rank training is simply combining forward and backward as\n$C_{Full-Rank} = 24nd^2 + 12n^2d + 18ndd_{ff}$.\nIn our proposed architecture, every single linear layer is replaced by low rank matrices A, B, and an activation function sandwiched in between. The activation only introduces trivial compute thus can be omitted in the calculation. For each $d^2$ and $dd_{ff}$ in Eq. (6), CoLA effectively converts them into 2dr and $r(d + d_{ff})$. Therefore the total compute of COLA is\n$C_{COLA} = 48ndr + 12n^2d + 18nr(d+ d_{ff})$.\nPlugging in an actual setting of LLaMA/CoLA-1B, in which r = d and $r \\approx \\frac{1}{2}d_{ff}$, we achieve a compute reduction from Eq. (6) to approximately\n$C_{COLA-1B} = 16.5nd^2 + 12n^2d + 1.8ndd_{ff}$.\nWe now discuss and compare CoLA with other efficient pre-training methods in terms of their compute complexity. We start with LoRA (Hu et al., 2021) and ReLoRA (Lialin et al., 2023). They share the same architecture that's shown in Fig. 3 a), in which low rank matrices $A \\in \\mathbb{R}^{r\\times d_{in}}$ and $B \\in \\mathbb{R}^{d_{out}\\times r}$ are adapted onto a full rank matrix $W_{0} \\in \\mathbb{R}^{d_{out}\\times d_{in}}$. Hence modifies Eq. (2) into\nh = W0x + BAx.\nThis yields a consistently more expensive forward step than the full-rank training regardless the choice of r. During the backward step, since gradient does not flow into W0, only one GEMM that computes gradient w.r.t x is involved with the full-rank component W0x. Combining together both full-rank and low-rank components in both forward and backward step, the total compute of LoRA is\n$C_{LORA} = 16nd^2 + 12n^2d + 12ndd_{ff}$\n$+ 48ndr + 18nr(d+d_{ff})$.\nWhen choosing the same r for LoRA and CoLA, we have CLORA > CCOLA always true.\nIn ReLoRA (Lialin et al., 2023), the hybrid strategy that warms up with the full-rank training arises more uncertainties in analyzing its complexity. And such strategy needs delicate tuning of"}, {"title": "C Detailed Memory Analysis", "content": "We continue using the notions defined in Section. 4.2 and start with the activation memory of full-rank training:\n$M_{full-rank} = \\frac{3nd}{Q,K,V} + \\frac{2n^2h}{attention} + \\frac{2nd}{residual \\ connection} + \\frac{2nd}{layer \\ norm} + \\frac{11nd}{ffw}$\n$= 20nd + 2n^2h$.\nWhen applying vanilla GCP, only the output of each block is saved, and all other activations are recomputed when needed. This dramatically reduces the total activation memory to only\n$M_{vanilla-GCP} = nd$.\nHowever, such benefit comes with a cost equal to almost an entire forward step. From Table. 2, we have the cost of vanilla-GCP as"}, {"title": "E Ablation Study", "content": "We empirically found that keeping the original LLaMA nonlinearity on top of our proposed formulation Eq. (3) helps improve the model performance at smaller scales, such as 60M and 130M. However, when scaling up to 350M we no longer observe such a benefit. Therefore, the default setting of pre-training CoLA-1B/7B is set to use only low-rank nonlinearity. We found also evident that applying low-rank nonlinearity (i.e., Eq. (3)) regardless of whether the original linear layer being followed by nonlinearity is crucial to boost model performance. Results are shown in Table. 11, in which \"CoLA w/ Both \u03c3\" means keeping the original nonlinearity on top of proposed low-rank nonlinearity, \"CoLA w/ Only Low-Rank \u03c3\" means applying Eq. (3) in an agnostic way to all linear layers, \"CoLA w/ Only Low-Rank \u03c3 \u2013 Reduced\" means only applying Eq. (3) to the linear layers that are originally followed by nonlinearity, \"CoLA w/ Only Full-Rank \u03c3\" means keeping the low-rank factorization but does not apply low-rank nonlinearity."}, {"title": "F Detailed Profiling Setting", "content": "This section provides a detailed explanation of the experimental setup for system-level measurements. For the memory breakdown in Fig. 6, we use a sequence batch size of 32. For throughput measurement in Fig. 8, we use a sequence batch size of 16 because the full-rank model cannot fit into 40GB A100 when using a sequence batch size of 32. Throughput is measured incorporating one forward pass, one backward pass, and one optimizer step. This setup reflects a realistic training scenario, particularly in a multi-GPU environment, such as an 8x A100 cluster utilizing simple data parallelism. For a fair comparison, we set the update step in GaLore/APOLLO to 200, ensuring that the computationally expensive SVD/random projection is performed only once every 200 optimizer steps and is distributed across a single optimizer step. All experiments are conducted on a single GPU to isolate the effected of FLOP reduction on throughput improvement, without being influenced by multi-GPU framework settings or communication overhead. For Table. 7, memory consumption is measured on a 94GB H100 with a sequence batch size of 16. For Table. 10, inference is performed using the same configuration as pre-training, with a sequence batch size of 32."}]}