{"title": "Aortic root landmark localization with optimal transport loss for\nheatmap regression", "authors": ["Tsuyoshi Ishizone", "Masaki Miyasaka", "Sae Ochi", "Norio Tada", "Kazuyuki Nakamura"], "abstract": "Anatomical landmark localization is gaining attention to ease the burden on physicians. Focusing on\naortic root landmark localization, the three hinge\npoints of the aortic valve can reduce the burden by\nautomatically determining the valve size required for\ntranscatheter aortic valve implantation surgery. Ex-\nisting methods for landmark prediction of the aor-\ntic root mainly use time-consuming two-step esti-\nmation methods. We propose a highly accurate\none-step landmark localization method from even\ncoarse images. The proposed method uses an op-\ntimal transport loss to break the trade-off between\nprediction precision and learning stability in conven-\ntional heatmap regression methods. We apply the\nproposed method to the 3D CT image dataset col-\nlected at Sendai Kousei Hospital and show that it\nsignificantly improves the estimation error over ex-\nisting methods and other loss functions. Our code is", "sections": [{"title": "Introduction", "content": "Anatomical landmark detection is a pivotal task in\nmedical image analysis and is essential for a multi-\ntude of clinical and research applications. Accurate\nlandmark localization aids in identifying pathological\nchanges, planning surgical interventions, and aligning\nmultimodal images for comprehensive analysis.\nLandmark localization for the aortic root from 3-\ndimensional computed tomography (CT) is crucial\ndue to its significant implications for cardiovascu-\nlar diagnostics and interventions. Precise localiza-\ntion of aortic root landmarks is essential for accurate\nmeasurement of dimensions, such as the aortic an-\nnulus, sinuses of Valsalva, and sinotubular junction,\nwhich are critical for diagnosing aortic diseases like\naneurysms, dissections, and valve pathologies. These\nlandmarks are also vital for planning and guiding in-\nterventions such as transcatheter aortic valve implan-\ntation (TAVI), where precise measurements ensure\noptimal prosthesis selection and positioning, reduc-\ning the risk of complications [1,2]. Moreover, reliable\nlandmark detection aids in longitudinal studies by\nproviding consistent reference points for tracking dis-\nease progression or treatment outcomes, ultimately\nimproving patient care and advancing cardiovascular\nresearch.\nThe existing methods of landmark localization for\naortic root are based on a two-stage estimation with\nglobal and local predictions [3-6]. Global predic-\ntion predicts the approximate location of landmarks\n\u2022 We propose a new loss function suitable for\nheatmap regression and predict the heatmap us-\ning U-Net. Just as the Wasserstein GAN sta-\nbilizes the learning of GANs, the proposed loss\nstabilizes the learning of heatmap regression by\nU-Net. The loss breaks the trade-off between\nlearning stability and prediction precision and\ncan achieve stable and accurate predictions.\n\u2022 We apply the proposed method to a 3D CT im-\nage dataset collected at Sendai Kousei Hospital\nand compare and evaluate it with existing meth-\nods. We show that the proposed method local-\nizes landmarks with such high accuracy that the\ndeviation from a coarse image with a resolution\nof 1.6 mm can be suppressed to about one voxel.\nThe rest of this paper is organized as follows. We\nreview previous studies in Section 2 and describe the\ndetails of the proposed method in Section 3. Section 4\ndescribes our experiments using 3D CT images, and\nSection 5 summarizes the paper."}, {"title": "Related Works", "content": "2.1 Anatomical Landmark Detection\nAnatomical landmark prediction has been developed\nfor various medical imaging data, such as CT [13\u201319],\nmagnetic resonance image [20-23], X-ray [24-38], ul-\ntrasound [31,39], echocardiography [40-43], laparo-\nscopes [44], esophagogastroduodenoscopy [45], in-\ntraoral scans [46,47].\nLandmark localization methods are divided into\nthree streams: classification-based, exploring-based,\nand regression-based methods. Some methods use\nbounding-box estimation [13, 39, 48] and data aug-\nmentation [23, 45, 49] before localizing landmarks.\nClassification-based methods classify whether a land-\nmark exists at particular voxels [17, 25, 39, 50\u201352,\n52-55]. Exploring-based methods gradually explore\nlandmark positions with walking particles and rein-\nforcement learning [3,4,16,20,21,31,56]. Typical re-\ninforcement learning methods use states - the patches\nof the images, actions position move, and rewards\nthe distances between the current position and the"}, {"title": "Optimal Transport", "content": "Optimal transport is a distance measure between\nprobability distributions, formulated as the cost of\n'optimal transport' between them. Optimal trans-\nport type losses have become a widely used alterna-\ntive to Kullback-Leibler type losses because of the\nstability of learning [68-71]. A typical example is the\nWasserstein GAN [68], a development of the GAN\n[72,73]. GAN is a deep generative model that mini-\nmizes the Jensen-Shannon (JS) divergence similar to\nKL. WGAN stabilizes the learning of GAN by re-\nplacing the JS divergence with the optimal transport\nWasserstein distance. The optimal transport that at-\ntracted attention in WGAN is used in various fields,\nsuch as time-series analysis [74], variable screening"}, {"title": "Method", "content": "Our architecture is a heatmap regression based on\nU-Net. The method inputs a\n3D CT image and predicts landmark locations as a\nheatmap. First, an overview of the architecture is\ngiven, followed by a description of how the heatmaps\nare created. The loss function used in the heatmap\nregression, which is the heart of the method, is based\non the optimal transport distance. After an overview\nof the optimal transport distance, we propose a loss\nthat modifies the Lipschitz penalty constraints for\nheatmap regression.\n3.1 U-net\nThe basis of our architecture is U-net, a well-known\nconvolutional neural network mainly used in image\nsegmentation tasks [7,78-84]. The architecture is\nstructured as an encoder-decoder network with a\nsymmetric layout. The convolution layers in the en-\ncoder extract local information from the input. The\nmax-pooling layers aggregate local information and\ndecrease voxel size. The upsampling layers expand\nthe features from global to local. The skip connec-\ntions provide a pathway for gradients during train-\ning, which helps combat the vanishing gradient prob-\nlem [85-87]. This sophisticated architecture provides\nrobust estimation even for noisy or obscured images.\n3.2 Heatmap Generation\nGround truth heatmaps are created from landmarks\nmanually annotated by experts. The heatmap for\neach landmark is the same size as the input image\nand has the highest value at the landmark position.\nWe use the standard Gaussian function and give the\nheatmap value of the i-th landmark of b-th sample at\n$\\displaystyle h_{bi} = \\exp \\bigg\\{ -\\frac{||g_v - I_{bi}||^2}{2\\sigma^2}\\bigg\\}$   (1)\nwhere $\\sigma$ is the standard deviation, $g_v \\in \\mathbb{R}^3$ represents\nthe position of the voxel $v$ of the b-th sample, $I_{bi} \\in \\mathbb{R}^3$\nrepresents the position of the i-th landmark of the b-\nth sample.\n3.3 Optimal Transport\nOptimal transport (OT) is a mathematical frame-\nwork that has been significantly applied in machine\nlearning, particularly in distribution comparison and\nalignment areas [8,88]. The method provides a way\nto quantify the distance between two probability dis-\ntributions meaningfully and geometrically intuitively.\nThe theory seeks to find the most cost-effective way\nof transforming one distribution into another. In its\nmost common form, the optimal transport problem\ncan be expressed as\n$\\displaystyle \\min_{\\pi \\in \\Pi(\\mu,\\nu)} \\int_{X \\times Y} C(x, y) d\\pi(x, y)$    (2)\n$\\displaystyle \\Pi(\\mu,\\nu) = \\{\\pi : X \\times Y \\rightarrow [0,1]| \\newline \\int_X \\pi(x, y)dy = \\mu(x), \\newline \\int_Y \\pi(x, y)dx = \\nu(y)\\},$    (3)\nwhere $X$ and $Y$ are topological spaces, $\\pi : X \\times Y \\rightarrow$\n$[0,1]$ represents a transport plan (coupling), which\nis a joint distribution whose marginals are $\\mu : X \\rightarrow$\n$[0,1]$ (source distribution) and $\\nu: Y \\rightarrow [0,1]$ (tar-\nget distribution). The $C: X \\times Y \\rightarrow \\mathbb{R}$ is a cost\nfunction that measures the cost of transporting mass\nfrom $x \\in X$ to $y \\in \\Upsilon$. $\\Pi(\\mu, \\nu\\}$ is the set of all possi-\nble transport plans (couplings) that have $\\mu$ and $\\nu$ as\ntheir marginals.\nAssuming that the space $X=Y$ in which the prob-\nability measure is defined and that the cost function\n$C$ is represented by a distance function $d: X \\times X \\rightarrow$\n$\\mathbb{R}$, the optimal transport problem can be expressed\nas\n$\\displaystyle \\min_{\\pi \\in \\Pi(\\mu,\\nu)} \\int_{X \\times X} d(x, y) d\\pi(x, y).$  (4)"}, {"title": "Grid-based Lipschtiz Penalty\n(GLIP)", "content": "There are two challenges in applying the gradient\npenalty method to heatmap regression. First, the\nmethod mainly applies to tasks classified by image\nand is unsuitable for tasks classified by voxel, such\nas heatmap regression. Second, the method assumes\nthat labels are binary and does not cover soft la-\nbels that take values between 0 and 1. To ad-\ndress these issues, we propose GLiP (grid-based Lip-\nschitz penalty), which imposes a Lipschitz constraint\npenalty for heatmap regression. The penalty term of\nGLiP is formulated as\n$\\displaystyle \\text{Penalty}(I) = \\sum_{v \\in \\mathbb{E}} \\sum_{i=1}^{N_I} (\\vert f_{\\theta}^i(I) - f_{\\theta}^i(I) - 1 \\vert)^2,$   (6)\nwhere $I \\in \\mathbb{I}$ is the input image, $\\mathbb{I} = \\mathbb{R}^{H \\times W \\times D}$ is the\nimage space, $\\mathbb{E}$ is the set of edges whose two grids are\nadjacent, $f_{\\theta} : \\mathbb{I} \\rightarrow \\mathbb{I}^{N_I}$ is the NN based on U-Net,\n$f_{\\theta}(I)$ is the i-th output value at v-th grid, and $N_I$\ndenotes the number of landmarks.\nCombined with the original optimal transport\nproblem, the entire loss function of GLiP is expressed"}, {"title": "Experiments", "content": "4.1 Dataset\nPre-TAVR CT DICOM data was available for 171\npatients with severe aortic stenosis who were treated\nas\n$\\displaystyle \\mathcal{L}_{GLIP}(B) = \\mathcal{L}_{OT}(B) + \\lambda_1 \\frac{1}{B} \\sum_{b=1}^B \\text{Penalty}(I_b),$    (7)\n$\\displaystyle \\mathcal{L}_{OT}(B) = \\frac{1}{B|\\mathcal{N}|}\\sum_{b \\in B} \\bigg\\{ \\frac{1}{N_I} \\sum_{i=1}^{N_I} \\bigg\\{ \\newline \\sum_{v \\in \\mathcal{N}_{Ib}} h_{b,i}^v \\cdot f_{\\theta}(I_b)_i^v  +\\frac{\\sum_{v \\in \\mathcal{N}_{Ib}} 1}{\\sum_{v \\in \\mathcal{N}_{Ib}} (1 - h_{b,i}^v) \\cdot f_{\\theta}(I_b)_i^v}  \\newline + \\frac{\\sum_{v \\in \\mathcal{N}_{Ib}} (1 - h_{b,i}^v) \\cdot 1}{\\sum_{v \\in \\mathcal{N}_{Ib}} 1}\\bigg\\} \\bigg\\},$    (8)\nwhere $B$ is the minibatch set, $I_b \\in \\mathcal{I}$ is the CT image\nof the b-th sample, $h_{b,i}^v \\in \\mathcal{I}$ is the heatmap of the i-th\nlandmark of the b-th sample, $f_{\\theta}(I_b)_i^v \\in \\mathcal{I}$ is the i-th\noutput values of the image $I_b^v$. The loss function of\nthe optimal transport is also modified according to\nthe value range [0,1] of the heatmap regression.\nThe relationship to the optimal transport problem\ndescribed in Subsection 3.3 is as follows. In the bi-\nnary classification problem (5), the distributions $\\mu$\nand $\\nu$ correspond to the distributions of 1 and 0, re-\nspectively, where the landmarks are present or ab-\nsent. The values of $\\mu, \\nu$, and take values for voxel\n$v$ and landmark position $I$ since the heatmap value\ncorresponds to the probability of belonging to class 1\nwhere the landmark is present. Since the landmark\nposition $I$ depends on the CT image $I$ and landmark\ntype i, an element x of the topological space X can be\nexpressed as the image $I \\in \\mathcal{I}$, voxel $v \\in \\mathbb{N}^3$, and land-\nmark type $i \\in \\mathbb{N}$: $x = (I, v, i) \\in X, X = \\mathcal{I} \\times \\mathbb{N}^3 \\times \\mathbb{N}$.\nSince the image space $\\mathcal{I}$ at training time can be re-\nduced to the sample number space $\\mathbb{N}$, $\\mu$ and $\\nu$ can\nbe expressed as discrete measures $\\mu(x) = h_{bi}$ and\n$\\nu(x) = 1 - h_{bi}^v$. The function $\\phi$ to be learned can\nbe expressed as $\\phi(x) = f_{\\theta}(I)$ using $f$ parameter-\nized by $\\theta$. From these arguments, the equation (8) is\nderived.\n4.2 Implemention Details\nThe dataset was randomly divided 4:1 into CV\nand test datasets, and a 4-fold cross-validation was\nperformed using the CV dataset. Adam opti-\nmizer [92] was used with a learning rate of 0.001.\nThe standard deviation of the Gaussian heatmap\nis {0.5,1,2,4,8, 16, 32, 64} and the Lipschitz penalty\ncoefficient is {1, 10, 100}. The hyperparameters were\nchosen based on the best CV score for the median\ndistance error of the landmark. For the test dataset,"}, {"title": "Comparison with Existing Meth-\nods and Loss Functions", "content": "Experiments comparing the two existing methods\n[5,6] and the U-Net with six loss functions were con-\nducted to verify the effectiveness of the proposed\nframework. Tan et al. [5] uses a framework based\non a similar U-Net architecture but with two-stage\nheatmaps and landmark regression prediction struc-\nture. Noothout et al. [6] combines landmark pres-\nence probability prediction with vector regression.\nIn addition to GLiP, we used weighted cross-entropy\nloss (WCE), focal loss (FL) [93], mean-squared error\n(MSE), L1 error (L1), and smoothed L1 error (SL1)\nas loss functions.\nWe then evaluate the plane through which the\nthree hinge points pass. The plane contains the aor-\ntic annulus, the size of which is clinically important\nfor determining the size of the prosthetic valve used\nin TAVR. The aortic annulus size is determined by\nmeasuring the sizes of the aortic annulus on the vir-\ntual annulus plane. The virtual annulus plane is de-\nfined as the plane passing through the hinge points of\nthe RCC, NCC, and LCC. The identification of hinge\npoints and the measurement of the aortic annulus size\nare carried out in the following three steps [89].\n1. Rough adjustment: Using DICOM software,\nalign the axis of the CT image of the aortic in-\nlet perpendicular to the aortic valve. Initially,\nrotate what was the axial plane and create an\noblique plane that roughly approximates the ori-"}, {"title": "Ablation Study", "content": "Two experiments were conducted to verify the effec-\ntiveness of the proposed loss.\n4.4.1 Optimal Transport Loss without The\nPenalty Term\nTo verify the validity of the Lipschitz constraint, ex-\nperiments were conducted without the penalty term\nof GLiP. A boxplot of the Euclid distance error for\nthe test data set is shown in Figure 5. The figure\nshows that the error of GLiP without the penalty\nterm becomes large. This means that the constraint\ncontributes to learning heatmap regression.\n4.4.2 Other Loss Functions with Lipschitz\nConstraint Penalty\nThe Lipschitz constraint keeps the predicted differ-\nence between adjacent voxels constant, thus prevent-\ning the voxels from falling into flat predictions. The\nconstraint is derived from the optimal transport, but\nits constraint penalty may impact the proposed loss\nmore than the optimal transport. To confirm this\neffect, we performed an experiment in which a gra-\ndient penalty was added to loss functions other than\nthe optimal transport loss. Figure 6 shows the box-\nplots of test error with and without gradient penalty\n(GP) for each loss function. The worst errors for\nFL and SL1 without GP are omitted because they\nare large. Except for the L1 loss, the values with-"}, {"title": "Extended Results", "content": "4.5.1 Comparison for CT Quality\nThe quality of CT images is often influenced by\ntechnical, patient-related, or operational factors.\nThe data quality grading was performed semi-\nquantitatively and categorized into five grades. The\nquality was evaluated by each operator:"}, {"title": "Qualitative Results", "content": "Figure 8 shows the ground truth landmarks (light\nblue) and the landmarks predicted by UNet-GLiP\n(red). The upper and lower rows show a test sam-\nple of quality 3+ and 1, respectively. The left col-\numn is plotted on the ground truth plane, and the\nright column is plotted on the prediction plane. The\nground truth plane is the plane through which the\nthree ground truth landmarks pass. The predicted\nlandmarks in the left column are projected onto the\nground truth plane, and the numbers near them are\nthe projected distances (mm). Similarly, the ground\ntruth landmarks are projected onto the prediction\nplane in the right column. For the quality 3+ data in"}, {"title": "Second Stage Estimation", "content": "Landmark prediction for medical images is mainly\nperformed in two phases: global and local [5,6,48,53].\nThis is because the original 3D CT image has a large\nnumber of pixels, so reducing the number of pixels is\nnecessary to allow the GPU memory to accommodate\nthe computation. The global phase inputs the down-\nsampled low-resolution image, while the local phase\ninputs a patch of high-resolution images around the\nglobal predictions."}, {"title": "Sensitivity Analysis", "content": "To verify the robustness of hyperparameters, a sen-\nsitivity analysis to the standard deviation of the\nheatmap and the GLiP loss coefficient \u03bb was per-\nformed. Because the main experiment's best CV me-\ndian distance error is \u03c3 = 1.0 and \u03bb = 10, we fixed\n\u03bb = 10 for the sensitivity analysis of its standard\ndeviation and \u03c3 = 1.0 for its coefficient. Figure 11\nshows the test Euclid error of the landmarks against\ntheir standard deviations. GLiP's median error is\nbelow 2 for standard deviations between 0.5 and 4,\nwhile the prediction error rapidly worsens for stan-\ndard deviations above 8. Other loss functions record\na minimum median error at standard deviations of\n4 or 16. Since the standard deviation corresponds\nto the uncertainty of the heatmap, it is difficult to\npredict more precisely than the standard deviation."}, {"title": "Conclusion", "content": "This paper proposes a new method for anatomical\nlandmark prediction. The proposed method incor-\nporates a loss function with a Lipschitz constraint\npenalty corresponding to the voxel-wise classification\nproblem based on optimal transport. Through our\nexperiments, the proposed method has a smaller Eu-\nclidean error of predicted landmarks than existing\nmethods and other loss functions. Surprisingly, the\nmedian error is below 2.0 mm, the expert's level, from\nimages with a coarse resolution of 1.6 mm. This is\nbecause the standard deviation of the heatmap mini-\nmizing CV error is lower than that of other loss func-\ntions, and the proposed method can achieve both ac-\ncurate prediction and stable learning.\nDespite this success, the paper has a few limita-\ntions. The first limitation is that the dataset used is\nprivate. We regret that we cannot give the reader the\nreproducibility of the experiments, and we are inves-\ntigating the possibility of making some of the data\nsets publicly available. The second limitation is that\nwe have not applied the proposed method to architec-\nture other than U-Net. We plan to experiment with\nstate-of-the-art architectures such as Swin-UNet [94],\nACC-UNet [95], and UNETR++ [96].\nWe plan to utilize the proposed method to estimate\nthe prosthetic valve size, which is essential for TAVI\nplanning. Landmark localization of the aortic root\ngives three points through the aortic valve and the\nplane through these points. If we can estimate the\nsize of the patient's valve based on the three points on\nthe estimated plane, we can determine the size of the\nprosthetic valve. We will use image processing and\nmachine learning to predict the prosthetic valve size\nfrom the estimated plane of the proposed method.\nThis paper can potentially have a wide range of\nimpacts in the field of medical image analysis. The\nproposed method can make accurate predictions from\ncoarse CT images with a resolution of 1.6 mm and ad-\nvances landmark localization for coarse images. Due\nto the radiologist's skill, patient movement, or equip-\nment deterioration, only coarse images can often be\nobtained. Even when such bad data is only avail-\nable, precise prediction can reduce the burden on\nphysicians. The experiments in this paper use only\n90 to 91 data samples for training, contributing to\nlandmark prediction for small datasets. Many hospi-\ntals have only small datasets because they keep their\ndatasets private, and publicly available datasets do\nnot always match the format of their images. Under\nsuch circumstances, the ability to perform highly ac-\ncurate learning with a small amount of data (around\n100 samples) is extremely significant. Our proposed\nmethod can be applied to other devices, such as X-\nrays and MRIs, and other body parts, such as the\nskull and knees. This will facilitate the development\nof the entire field of anatomical landmark localiza-\ntion."}]}