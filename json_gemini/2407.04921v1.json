{"title": "Aortic root landmark localization with optimal transport loss for heatmap regression", "authors": ["Tsuyoshi Ishizone", "Masaki Miyasaka", "Sae Ochi", "Norio Tada", "Kazuyuki Nakamura"], "abstract": "Anatomical landmark localization is gaining attention to ease the burden on physicians. Focusing on aortic root landmark localization, the three hinge points of the aortic valve can reduce the burden by automatically determining the valve size required for transcatheter aortic valve implantation surgery. Existing methods for landmark prediction of the aortic root mainly use time-consuming two-step estimation methods. We propose a highly accurate one-step landmark localization method from even coarse images. The proposed method uses an optimal transport loss to break the trade-off between prediction precision and learning stability in conventional heatmap regression methods. We apply the proposed method to the 3D CT image dataset collected at Sendai Kousei Hospital and show that it significantly improves the estimation error over existing methods and other loss functions. Our code is available on GitHub.", "sections": [{"title": "Introduction", "content": "Anatomical landmark detection is a pivotal task in medical image analysis and is essential for a multitude of clinical and research applications. Accurate landmark localization aids in identifying pathological changes, planning surgical interventions, and aligning multimodal images for comprehensive analysis.\nLandmark localization for the aortic root from 3-dimensional computed tomography (CT) is crucial due to its significant implications for cardiovascular diagnostics and interventions. Precise localization of aortic root landmarks is essential for accurate measurement of dimensions, such as the aortic annulus, sinuses of Valsalva, and sinotubular junction, which are critical for diagnosing aortic diseases like aneurysms, dissections, and valve pathologies. These landmarks are also vital for planning and guiding interventions such as transcatheter aortic valve implantation (TAVI), where precise measurements ensure optimal prosthesis selection and positioning, reducing the risk of complications [1,2]. Moreover, reliable landmark detection aids in longitudinal studies by providing consistent reference points for tracking disease progression or treatment outcomes, ultimately improving patient care and advancing cardiovascular research.\nThe existing methods of landmark localization for aortic root are based on a two-stage estimation with global and local predictions [3\u20136]. Global prediction predicts the approximate location of landmarks\n\u2022 We propose a new loss function suitable for heatmap regression and predict the heatmap using U-Net. Just as the Wasserstein GAN stabilizes the learning of GANs, the proposed loss stabilizes the learning of heatmap regression by U-Net. The loss breaks the trade-off between learning stability and prediction precision and can achieve stable and accurate predictions.\n\u2022 We apply the proposed method to a 3D CT image dataset collected at Sendai Kousei Hospital and compare and evaluate it with existing methods. We show that the proposed method localizes landmarks with such high accuracy that the deviation from a coarse image with a resolution of 1.6 mm can be suppressed to about one voxel.\nThe rest of this paper is organized as follows. We review previous studies in Section 2 and describe the details of the proposed method in Section 3. Section 4 describes our experiments using 3D CT images, and Section 5 summarizes the paper."}, {"title": "Related Works", "content": ""}, {"title": "Anatomical Landmark Detection", "content": "Anatomical landmark prediction has been developed for various medical imaging data, such as CT [13\u201319], magnetic resonance image [20\u201323], X-ray [24\u201338], ultrasound [31,39], echocardiography [40\u201343], laparoscopes [44], esophagogastroduodenoscopy [45], intraoral scans [46,47].\nLandmark localization methods are divided into three streams: classification-based, exploring-based, and regression-based methods. Some methods use bounding-box estimation [13, 39, 48] and data augmentation [23, 45, 49] before localizing landmarks. Classification-based methods classify whether a landmark exists at particular voxels [17, 25, 39, 50\u201352, 52-55]. Exploring-based methods gradually explore landmark positions with walking particles and reinforcement learning [3,4,16,20,21,31,56]. Typical reinforcement learning methods use states - the patches of the images, actions - position move, and rewards - the distances between the current position and the\nground truth landmark [4, 16, 20, 21]. These methods use deep Q-network (DQN) [16,20,21,57], double DQN [21,58], and dueling DQN [21,59] as arachitectures. As an atypical method, Zhou et al. [31] consider the heatmap generating method as a state, explore the generating method, and learn the heatmap regression task as a reward.\nRegression-based methods are the mainstream for localizing landmarks. The stream has two substreams: vector regression methods and heatmap regression methods. Vector regression methods predict landmark coordinates or offset from current grid to landmark positions [15, 17, 18, 25, 26, 28, 36-38,50,54, 60-62]. Ma et al. [15] uses the shift-equivalent property of the localizer network for end-to-end training of high-resolution images. Sanchez et al. [62] presents segmentation-guided coordinate regression, which consists of U-Net and VGG architectures. Lu et al. [36] uses graph convolutional networks to extract relationships among the coordinates of the landmarks.\nHeatmap regression methods construct heatmap centered on ground truth landmarks and predict the heatmap [5,17-19, 22, 23, 29-35, 40-43, 52, 54, 63-66]. Tan et al. [54] predicts bifurcation landmarks with heatmaps, segmentation, and orientation regression. Chen et al. [18] simultaneously learns coarse heatmap regression and fine coordinate regression. Tan et al. [52] combines heatmap regression with semantic segmentation and classification for localizing cerebrovascular landmarks. Qian et al. [19] and Wan et al. [42] propose new loss functions to balance the foreground and background pixels. Xiao et al. [30] uses graph convolutional networks to feature relationships among landmarks. Fu et al. [64] uses transfer learning for facial landmark detection to apply to a small dataset of fetal alcohol spectrum disorders. Millan-Arias et al. [34] utilizes X-ray images obtained from different machines. Tan et al. [22] uses cross-modality information of MRA and CTA for adversarial training of heatmaps. Dai et al. [29] uses distance maps instead of heatmaps and uses image gradient difference loss [67] for maintaining the sharp edge. Our proposals also belong to heatmap regression methods.\nSome methods focus on localizing landmarks of the aortic root from 3D-CT dataset [3\u20136,48,53]. Tan et al. [5] combines global heatmap regression with UNet and local directional vector regression with CNN. Noothout et al. [6] use directional vector regression and landmark affiliation prediction with ResNet in global and local stages. Astudillo et al. [53] estimates landmarks via semantic segmentation using DenseVNet. Al et al. [3,4] uses the coronial walk and deep reinforcement learnings to explore the landmark position. Tahoces et al. [48] estimates landmarks via bounding box estimation.\nThese methods consist of multiple stages of learning and require tuning at each stage. Typically, they are divided into two stages: global and local, where global estimation is used to identify areas where landmarks are located, and local estimation is used to output final estimates of landmarks. To compensate for the lack of GPU memory, these methods reduce the image size by downsampling for global estimation and use the original resolution image by extracting patches for local estimation. However, multi-stage network configurations require exploring many elements, such as hyperparameters and modules, at each stage, which increases the exploring cost. To avoid these, our proposed method accurately predicts landmarks using only downsampled low-resolution images. The proposed method is also suitable for situations where only low-resolution CT images are available due to scanner or patient problems."}, {"title": "Optimal Transport", "content": "Optimal transport is a distance measure between probability distributions, formulated as the cost of 'optimal transport' between them. Optimal transport type losses have become a widely used alternative to Kullback-Leibler type losses because of the stability of learning [68\u201371]. A typical example is the Wasserstein GAN [68], a development of the GAN [72,73]. GAN is a deep generative model that minimizes the Jensen-Shannon (JS) divergence similar to KL. WGAN stabilizes the learning of GAN by replacing the JS divergence with the optimal transport Wasserstein distance. The optimal transport that attracted attention in WGAN is used in various fields, such as time-series analysis [74], variable screening"}, {"title": "Method", "content": "Our architecture is a heatmap regression based on U-Net, as shown in Figure 1. The method inputs a 3D CT image and predicts landmark locations as a heatmap. First, an overview of the architecture is given, followed by a description of how the heatmaps are created. The loss function used in the heatmap regression, which is the heart of the method, is based on the optimal transport distance. After an overview of the optimal transport distance, we propose a loss that modifies the Lipschitz penalty constraints for heatmap regression."}, {"title": "U-net", "content": "The basis of our architecture is U-net, a well-known convolutional neural network mainly used in image segmentation tasks [7,78\u201384]. The architecture is structured as an encoder-decoder network with a symmetric layout. The convolution layers in the encoder extract local information from the input. The max-pooling layers aggregate local information and decrease voxel size. The upsampling layers expand the features from global to local. The skip connections provide a pathway for gradients during training, which helps combat the vanishing gradient problem [85\u201387]. This sophisticated architecture provides robust estimation even for noisy or obscured images."}, {"title": "Heatmap Generation", "content": "Ground truth heatmaps are created from landmarks manually annotated by experts. The heatmap for each landmark is the same size as the input image and has the highest value at the landmark position. We use the standard Gaussian function and give the heatmap value of the i-th landmark of b-th sample at\n$h_{bi} = exp \\{  -\\frac{||g^b_v - l^b_i||^2}{2 \\sigma^2}  \\}$      (1)\nwhere $\\sigma$ is the standard deviation, $g_v \\in R^3$ represents the position of the voxel $v$ of the $b$-th sample, $l^b_i \\in R^3$ represents the position of the $i$-th landmark of the $b$-th sample."}, {"title": "Optimal Transport", "content": "Optimal transport (OT) is a mathematical framework that has been significantly applied in machine learning, particularly in distribution comparison and alignment areas [8,88]. The method provides a way to quantify the distance between two probability distributions meaningfully and geometrically intuitively. The theory seeks to find the most cost-effective way of transforming one distribution into another. In its most common form, the optimal transport problem can be expressed as\n$\\mathop{\\text{minimize}} \\limits_{\\pi \\in \\Pi(\\mu, \\nu)} \\int_{X \\times Y} C(x, y)d\\pi(x, y)$ ,                  (2)\n$\\Pi(\\mu, \\nu) = \\{\\pi: X \\times Y \\rightarrow [0,1]|$\n$\\int_{Y} \\pi(x, y)dy = \\mu(x),$   \n$\\int_{X} \\pi(x, y)dx = \\nu(y)\\},$ (3)\nwhere $X$ and $Y$ are topological spaces, $\\pi : X \\times Y \\rightarrow [0,1]$ represents a transport plan (coupling), which is a joint distribution whose marginals are $\\mu : X \\rightarrow [0,1]$ (source distribution) and $\\nu: Y \\rightarrow [0,1]$ (target distribution). The $C: X \\times Y \\rightarrow R$ is a cost function that measures the cost of transporting mass from $x \\in X$ to $y \\in Y$. $\\Pi(\\mu, \\nu\\}$ is the set of all possible transport plans (couplings) that have $\\mu$ and $\\nu$ as their marginals.\nAssuming that the space $X = Y$ in which the probability measure is defined and that the cost function $C$ is represented by a distance function $d : X \\times X \\rightarrow R$, the optimal transport problem can be expressed as\n$\\mathop{\\text{minimize}} \\limits_{\\pi \\in \\Pi(\\mu, \\nu)} \\int_{X \\times X} d(x, y)d\\pi(x, y).$                                        (4)\n\nThe dual problem of this problem is expressed by\n$\\mathop{\\text{maximize}} \\limits_{\\phi \\in C(X)} \\int_{X} \\phi(x)d\\mu(x) - \\int_{X} \\phi(y)d\\nu(y),$          (5)\nwhere $C(X) = \\{\\phi \\in C(X)|\\phi : bounded, 1-Lipschitz\\}$ represent the set of bounded and 1-Lipschitz continuous functions from $X$ to $R$.\nThis problem is solved by increasing the output value of $\\phi(x)$ in the sample $x$ from distribution $\\mu$ and decreasing the value of $\\phi(x\u2019)$ in the sample $x\u2019$ from distribution $\\nu$, which increases the interobjective value. Considering these two distributions as positive and negative example distributions, the problem can be considered a binary classification problem.\nWhen $X$ is a continuous space, the binary classification problem with optimal transport is formulated as a problem of finding the optimal parameter $\\theta$ that minimizes the objective function using a parametric function $\\phi_\\theta$. The gradient penalty method is used to satisfy the 1-Lipschitz constraint on the function $\\phi_\\theta$. The method uses the fact that the norm of the gradient is 1 almost everywhere when using the optimal coupling $\\pi^*$ [69]. Sampling $x, y$ from each distribution $\\mu$ and $\\nu$ respectively, the penalty $(||\\nabla_x\\phi_\\theta(x)|| - 1)^2$ is applied to the loss function such that the norm of the gradient of its interior point x = tx + (1 \u2212 t)y is 1."}, {"title": "Grid-based Lipschtiz Penalty (GLIP)", "content": "There are two challenges in applying the gradient penalty method to heatmap regression. First, the method mainly applies to tasks classified by image and is unsuitable for tasks classified by voxel, such as heatmap regression. Second, the method assumes that labels are binary and does not cover soft labels that take values between 0 and 1. To address these issues, we propose GLiP (grid-based Lipschitz penalty), which imposes a Lipschitz constraint penalty for heatmap regression. The penalty term of GLiP is formulated as\n$\\text{Penalty}(I) = \\sum_{u,v \\in E} \\sum_{i=1}^{N_l} (\\|f_{u}(I)_i - f_{v}(I)_i\\| - 1)^2$,                   (6)\nwhere $I \\in \\mathcal{I}$ is the input image, $\\mathcal{I} = \\mathbb{R}^{H \\times W \\times D}$ is the image space, $E$ is the set of edges whose two grids are adjacent, $f_{\\theta}: \\mathcal{I} \\rightarrow \\mathbb{R}^{N_l}$ is the NN based on U-Net, $f_{v}(I)_i$ is the $i$-th output value at $v$-th grid, and $N_l$ denotes the number of landmarks.\nCombined with the original optimal transport problem, the entire loss function of GLiP is expressed as\n$\\mathcal{L}_{GLIP}(B) = \\mathcal{L}_{OT}(B) + \\lambda_1 \\frac{1}{|B|} \\sum_{I \\in B} \\text{Penalty}(I),\\,$              (7)\n$\\mathcal{L}_{OT}(B) =  \\frac{1}{|B|} \\left[  \\sum_{b \\in B} \\{  \\frac{1}{N_l} \\sum_{i=1}^{N_l}  \\{\\frac{\\sum_{v \\in B} h^b_{vi}  \\cdot f_{\\theta}(I^b)_i}{\\sum_{v \\in B} h^b_{vi}  } +  \\frac{\\sum_{v \\in B} (1-h^b_{vi})  \\cdot f_{\\theta}(I^b)_i}{\\sum_{v \\in B} (1-h^b_{vi})  }  \\}   \\right],$     (8)\nwhere $B$ is the minibatch set, $I^b \\in \\mathcal{I}$ is the CT image of the $b$-th sample, $h^b_{vi} \\in \\mathcal{I}$ is the heatmap of the $i$-th landmark of the $b$-th sample, $f_{\\theta}(I^b)_i \\in \\mathcal{I}$ is the $i$-th output values of the image $I^b$. The loss function of the optimal transport is also modified according to the value range [0,1] of the heatmap regression.\nThe relationship to the optimal transport problem described in Subsection 3.3 is as follows. In the binary classification problem (5), the distributions $\\mu$ and $\\nu$ correspond to the distributions of 1 and 0, respectively, where the landmarks are present or absent. The values of $\\mu, \\nu$, and take values for voxel $v$ and landmark position $I$ since the heatmap value corresponds to the probability of belonging to class 1 where the landmark is present. Since the landmark position $I$ depends on the CT image $I$ and landmark type $i$, an element $x$ of the topological space $X$ can be expressed as the image $I \\in \\mathcal{I}$, voxel $v \\in \\mathbb{N}^3$, and landmark type $i \\in \\mathbb{N}$: $x = (I, v, i) \\in X, X = \\mathcal{I} \\times \\mathbb{N}^3 \\times \\mathbb{N}$. Since the image space $\\mathcal{I}$ at training time can be reduced to the sample number space $\\mathbb{N}$, $\\mu$ and $\\nu$ can be expressed as discrete measures $\\mu(x) = h^b_{vi}$ and $\\nu(x) = 1 - h^b_{vi}$. The function $\\phi$ to be learned can be expressed as $\\phi_{\\theta}(x) = f_{\\theta}(I^b)_i$ using $f$ parameterized by $\\theta$. From these arguments, the equation (8) is derived."}, {"title": "Experiments", "content": ""}, {"title": "Dataset", "content": "Pre-TAVR CT DICOM data was available for 171 patients with severe aortic stenosis who were treated by TAVR at Sendai Kousei Hospital between January 2014 and June 2019. After excluding data for 4 patients with a bicuspid aortic valve, the remaining 167 datasets were used for further experiments. This study was approved by the local ethics committee (Approval No: 3-32).\nCT measurement in this study was performed in accordance with the standard pre-TAVR CT measurement protocols [1,89]. Using the DICOM viewer Horos\u00aeversion 3.3.6, three hinge points were identified, and their three-dimensional coordinates were recorded. For the analysis, experienced operators performed CT analysis to create the training data.\nEach CT data consisted of 320 images with 512\u00d7512 slices to include the aortic valve. Since the resolution of the slice images varied from 0.25 to 0.5 for each data, linear completion was performed to unify the spacing to 0.4. Since the data size was too large to be computationally feasible on the GPU, 4-fold downsampling was performed in each direction in three dimensions to obtain 128\u00d7128\u00d780 images. We set the prediction targets for the three landmarks on the aortic valve, the hinge points of RCC, LCC, and NCC. Heatmaps were created using the method described in Subsection 3.2 based on the positions, and training was performed as a heatmap regression task."}, {"title": "Implemention Details", "content": "The dataset was randomly divided 4:1 into CV and test datasets, and a 4-fold cross-validation was performed using the CV dataset. Adam optimizer [92] was used with a learning rate of 0.001. The standard deviation of the Gaussian heatmap is {0.5, 1, 2, 4, 8, 16, 32, 64} and the Lipschitz penalty coefficient is {1, 10, 100}. The hyperparameters were chosen based on the best CV score for the median distance error of the landmark. For the test dataset,"}, {"title": "Comparison with Existing Methods and Loss Functions", "content": "Experiments comparing the two existing methods [5,6] and the U-Net with six loss functions were conducted to verify the effectiveness of the proposed framework. Tan et al. [5] uses a framework based on a similar U-Net architecture but with two-stage heatmaps and landmark regression prediction structure. Noothout et al. [6] combines landmark presence probability prediction with vector regression. In addition to GLiP, we used weighted cross-entropy loss (WCE), focal loss (FL) [93], mean-squared error (MSE), L1 error (L1), and smoothed L1 error (SL1) as loss functions."}, {"title": "Ablation Study", "content": "Two experiments were conducted to verify the effectiveness of the proposed loss."}, {"title": "Optimal Transport Loss without The Penalty Term", "content": "To verify the validity of the Lipschitz constraint, experiments were conducted without the penalty term of GLiP. A boxplot of the Euclid distance error for the test data set is shown in Figure 5. The figure shows that the error of GLiP without the penalty term becomes large. This means that the constraint contributes to learning heatmap regression."}, {"title": "Other Loss Functions with Lipschitz Constraint Penalty", "content": "The Lipschitz constraint keeps the predicted difference between adjacent voxels constant, thus preventing the voxels from falling into flat predictions. The constraint is derived from the optimal transport, but its constraint penalty may impact the proposed loss more than the optimal transport. To confirm this effect, we performed an experiment in which a gradient penalty was added to loss functions other than the optimal transport loss. Figure 6 shows the boxplots of test error with and without gradient penalty (GP) for each loss function. The worst errors for FL and SL1 without GP are omitted because they are large. Except for the L1 loss, the values without GP are lower at 0.25, 0.5, and 0.75 quantiles. This suggests that the Lipschitz constraint penalty improves prediction accuracy only for optimal transport losses. These two ablation studies demonstrate that the combination of the optimal transport loss and the penalty constraint realized by GLiP provides highly accurate predictions."}, {"title": "Extended Results", "content": ""}, {"title": "Comparison for CT Quality", "content": "The quality of CT images is often influenced by technical, patient-related, or operational factors. The data quality grading was performed semiquantitatively and categorized into five grades. The quality was evaluated by each operator:\n\u2022 3+: Good. Good quality for accurate measurement.\n\u2022 4: Excellent. Excellent quality for accurate measurement."}, {"title": "Qualitative Results", "content": "Figure 8 shows the ground truth landmarks (light blue) and the landmarks predicted by UNet-GLiP (red). The upper and lower rows show a test sample of quality 3+ and 1, respectively. The left column is plotted on the ground truth plane, and the right column is plotted on the prediction plane. The ground truth plane is the plane through which the three ground truth landmarks pass. The predicted landmarks in the left column are projected onto the ground truth plane, and the numbers near them are the projected distances (mm). Similarly, the ground truth landmarks are projected onto the prediction plane in the right column. For the quality 3+ data in the top row, the deviations of the proposed method's predictions from the ground truth are so small that\neven experts cannot identify them. For the quality 1 data in the bottom row, the lower left (NCC) prediction deviates from the ground truth. Considering the input image's 1.6 mm resolution and the data's poor quality, this deviation is at a level that can occur even among experts."}, {"title": "Second Stage Estimation", "content": "Landmark prediction for medical images is mainly performed in two phases: global and local [5,6,48,53]. This is because the original 3D CT image has a large number of pixels, so reducing the number of pixels is necessary to allow the GPU memory to accommodate the computation. The global phase inputs the downsampled low-resolution image, while the local phase inputs a patch of high-resolution images around the global predictions.\nTo verify the effectiveness of the proposed method using the existing knowledge of two-stage prediction,\nwe performed the local phase as the second-stage estimation. For each loss function, the network with the smallest CV median error of landmarks was designated as the global network, and for each fold, the inputs to the local network were constructed from the outputs of the global network. The input images were patched around the globally predicted landmarks with patch size p_s = 32. Disturbances were added to the patches to prevent overfitting where only the central voxel of global prediction is predicted in the local phase. When the patch disturbance is p_d, a voxel region of (p_s + 2p_d)^3 around the global prediction landmark is extracted from the original CT image. A voxel region of (p_s)^3 starting at a uniform integer random number u ~ UI[1, 2p_d]^3 is extracted from the (p_s + 2p_d)^3 image. Patch disturbances p_d \u2208 {0, 2, 4, 8}, heatmap standard deviation \u03c3 \u2208 {0.5, 1, 2, 4, 8, 16}, GLiP penalty \u03bb \u2208 {0.1, 1, 10} were searched for minimizing the CV median distance error of landmarks.\nFigure 9 shows the success detection rate in the second-stage landmark prediction for the test data for each loss function. The success detection rate is the probability that the predicted landmark error is below the distance threshold. The proposed loss has the highest success detection rate, suggesting that the second-stage prediction is stable and highly accurate.\nFigure 10 shows the success detection rate of landmarks in the first and the second-stage predictions by UNet-GLiP. The success detection rate for the second-stage prediction is consistently higher, indicating the usefulness of the two-stage prediction. The input image for the first-stage prediction has a resolution of 1.6 mm, so the largest discrepancy in the success detection rate from the second to the first is at the distance threshold of 1.5 mm. The firststage prediction is accurate enough for the expert level of 2 mm or less. Two-stage prediction methods require appropriate hyperparameter adjustment at each stage, which increases the cost of hyperparameter adjustment and the cost of inference. One-step and two-step prediction methods should be used according to whether a high accuracy of less than 1.5 mm or a low search and inference cost is desired."}, {"title": "Sensitivity Analysis", "content": "To verify the robustness of hyperparameters, a sensitivity analysis to the standard deviation of the heatmap and the GLiP loss coefficient \u03bb was performed. Because the main experiment's best CV median distance error is \u03c3 = 1.0 and \u03bb = 10, we fixed \u03bb = 10 for the sensitivity analysis of its standard deviation and \u03c3 = 1.0 for its coefficient. Figure 11 shows the test Euclid error of the landmarks against their standard deviations. GLiP's median error is below 2 for standard deviations between 0.5 and 4, while the prediction error rapidly worsens for standard deviations above 8. Other loss functions record a minimum median error at standard deviations of 4 or 16. Since the standard deviation corresponds to the uncertainty of the heatmap, it is difficult to predict more precisely than the standard deviation.\nOn the other hand, if the standard deviation is too small, it is difficult to learn properly due to data imbalance, which causes a large bias of data between binary values of 0 and 1 during training. This is a trade-off between prediction precision in the standard deviation of the heatmap and learning stability. For loss functions other than GLiP, the data imbalance destabilizes the learning process, resulting in large errors when the standard deviation is 1 or 2. GLiP can decrease the landmark error with lower standard deviations because the optimal transport-type loss stabilizes the learning process. Since even an expert can make an error of about 2 mm when annotating landmarks [53], setting the standard deviation to 1 or 2 is equivalent to constructing a heat map that only allows expert-level errors, and GLiP's standard deviation setting is aggressive.\nFigure 12 shows a boxplot of the test Euclid distance error of the landmarks and planes predicted by UNet-GLiP for the GLiP loss factor \u03bb. The median distance error is stable below 2 regardless of the coefficient. The ablation study on the GLiP loss shows that the error is large when the coefficient is zero. These results show that, regardless of the coefficients\u2019 values, the inclusion of a Lipschitz constraint term improves landmark prediction accuracy."}, {"title": "Conclusion", "content": "This paper proposes a new method for anatomical landmark prediction. The proposed method incorporates a loss function with a Lipschitz constraint penalty corresponding to the voxel-wise classification problem based on optimal transport. Through our experiments, the proposed method has a smaller Euclidean error of predicted landmarks than existing methods and other loss functions. Surprisingly, the median error is below 2.0 mm, the expert's level, from images with a coarse resolution of 1.6 mm. This is because the standard deviation of the heatmap minimizing CV error is lower than that of other loss functions, and the proposed method can achieve both accurate prediction and stable learning.\nDespite this success, the paper has a few limitations. The first limitation is that the dataset used is private. We regret that we cannot give the reader the reproducibility of the experiments, and we are investigating the possibility of making some of the data sets publicly available. The second limitation is that we have not applied the proposed method to architecture other than U-Net. We plan to experiment with state-of-the-art architectures such as Swin-UNet [94], ACC-UNet [95], and UNETR++ [96].\nWe plan to utilize the proposed method to estimate the prosthetic valve size, which is essential for TAVI planning. Landmark localization of the aortic root gives three points through the aortic valve and the plane through these points. If we can estimate the size of the patient's valve based on the three points on the estimated plane, we can determine the size of the prosthetic valve. We will use image processing and machine learning to predict the prosthetic valve size from the estimated plane of the proposed method.\nThis paper can potentially have a wide range of impacts in the field of medical image analysis. The proposed method can make accurate predictions from coarse CT images with a resolution of 1.6 mm and advances landmark localization for coarse images. Due to the radiologist's skill, patient movement, or equipment deterioration, only coarse images can often be obtained. Even when such bad data is only available, precise prediction can reduce the burden on physicians. The experiments in this paper use only 90 to 91 data samples for training, contributing to landmark prediction for small datasets. Many hospitals have only small datasets because they keep their datasets private, and publicly available datasets do not always match the format of their images. Under such circumstances, the ability to perform highly accurate learning with a small amount of data (around 100 samples) is extremely significant. Our proposed method can be applied to other devices, such as X-rays and MRIs, and other body parts, such as the skull and knees. This will facilitate the development of the entire field of anatomical landmark localization."}]}