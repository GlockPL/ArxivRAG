{"title": "PEER: Expertizing Domain-Specific Tasks with a Multi-Agent Framework and Tuning Methods", "authors": ["Yiying Wang", "Xiaojing Li", "Binzhu Wang", "Yueyang Zhou", "Han Ji", "Hong Chen", "Jinshi Zhang", "Fei Yu", "Zewei Zhao", "Song Jin", "Renji Gong", "Wanqing Xu"], "abstract": "In domain-specific applications, GPT-4, augmented with precise prompts or Retrieval-Augmented Generation (RAG), shows notable potential but faces the critical tri-lemma of performance, cost, and data privacy. High performance requires sophisticated processing techniques, yet managing multiple agents within a complex workflow often proves costly and challenging. To address this, we introduce the PEER (Plan, Execute, Express, Review) multi-agent framework. This systematizes domain-specific tasks by integrating precise question decomposition, advanced information retrieval, comprehensive summarization, and rigorous self-assessment. Given the concerns of cost and data privacy, enterprises are shifting from proprietary models like GPT-4 to custom models, striking a balance between cost, security, and performance. We developed industrial practices leveraging online data and user feedback for efficient model tuning. This study provides best practice guidelines for applying multi-agent systems in domain-specific problem-solving and implementing effective agent tuning strategies. Our empirical studies, particularly in the financial question-answering domain, demonstrate that our approach achieves 95.0% of GPT-4's performance, while effectively managing costs and ensuring data privacy.", "sections": [{"title": "1 Introduction", "content": "Advanced LLMs like GPT-4, enhanced with engineered prompts or Retrieval-Augmented Generation (RAG), show great potential in handling complex tasks across various domains (Wang et al., 2023; Nori et al., 2023; Zhang et al., 2024). However, deploying these models involves a critical tri-lemma of performance, cost, and data privacy.\nWhile domain-specific applications benefit from meticulously fine-tuned models (Ling et al., 2024), this approach incurs high costs due to the extensive resources needed for training and data acquisition. Alternatively, multi-agent systems have proven effective (Talebirad and Nadiri, 2023; Hong et al., 2023; Li et al., 2023; Wu et al., 2023; Wang et al., 2024b), especially in complex tasks with distinct and conflicting role requirements that challenge even advanced models. However, current implementations often involve dynamic and complex workflows, increasing costs and complicating reproducibility. Consequently, enterprises are shifting from proprietary models like GPT-4 to custom models that better balance cost, security, and performance.\nTo address these challenges, we introduce the PEER (Plan, Execute, Express, Review) multi-agent framework. This framework incorporates precise question decomposition, advanced information retrieval, comprehensive summarization, and rigorous self-assessment, aiming to streamline workflows and enhance problem-solving efficacy. Additionally, our research addresses enterprise demands for private deployment and stringent data privacy by developing industrial best practices that leverage online data and user feedback for effective model tuning. These practices are crucial for optimizing custom model performance while ensuring cost-efficiency and robust data privacy.\nThe main contributions of this study include:\n1.Providing and open-sourcing the PEER\u00b9 framework, characterized by its conciseness, effectiveness, and cost-efficiency, for effectively tackling domain-specific tasks. In experiments, it demonstrates superior performance compared to BabyAGI.\n2.Proposing a customized agent tuning strategy for 10-billion-parameter models, achieving performance comparable to GPT-4.\n3.Constructing and open-sourcing a dataset for use within the PEER framework, applicable to agent"}, {"title": "2 The Agent Framework PEER", "content": "With the advent of large models, we simulate the collaborative processes of human experts (e.g. financial) using multiple agents, achieving comparable interpretative results. This approach is encapsulated in the Plan, Execute, Express and Review (PEER) framework, where domain specific (e.g. financial) tasks are divided into these four steps. Each agent specializes in a single task, working together to accomplish the overall objective. The prompt for this section is attached in D."}, {"title": "2.1 Four agent roles in PEER", "content": "Plan The \"Plan\" agent uses a model to generate multiple related sub-questions from users' domain specific (e.g. financial) queries. These sub-questions serve as an interpretation framework, breaking down the original query into specific and actionable criteria, and expanding it for a comprehensive analysis.\nExecute The \"Execute\" agent gathers information for each sub-question identified by \"Plan\". Using these sub-questions as search criteria, it finds relevant information from news, domain specific (e.g. financial) data, reports, and articles, enhancing accuracy, efficiency, and comprehensiveness. This information forms the foundation for interpreting domain events and answering questions.\nExpress The \"Express\" agent synthesizes collected information to perform comprehensive large-model reasoning, forming final conclusions. It emphasizes integrative reasoning and delivers professional descriptions tailored to the user's requirements.\nReview The \"Review\" agent evaluates whether the \"Express\" agent's answer meets pre-established criteria. If satisfied, the final answer is delivered; if not, it provides modification suggestions and initiates another PEER iteration, enhancing answer quality through feedback."}, {"title": "2.2 Cyclic working mechanism of PEER", "content": "The PEER multi-agent cooperation framework's strong reasoning and analysis abilities stem from its efficient task allocation, cooperation, and the feedback loop and self-optimization enabled by the \"Review\" agent. This ensures that the answers continuously improve towards the optimal solution. If an answer does not meet user requirements, the \"Review\" agent suggests modifications for the \"Plan,\" \"Execute,\" or \"Express\" agents. The relevant agent then adjusts its process to better meet expectations. For some simple tasks, one or more agents in PEER process can be skipped to simplify the procedure. For complex tasks, a nested pattern can be used, designing each agent to perform an isolate PEER process to enhance entire performance.For a more comprehensive understanding of the PEER framework, refer to Figure 1, which illustrates how these four agents synergize."}, {"title": "3 The PEER Agent Tuning Approach", "content": ""}, {"title": "3.1 Supervised Fine-tuning and Rejection Sampling", "content": "Supervised fine-tuning typically employs the cross-entropy loss:\n$\\mathcal{L}(y, \\hat{y}) = - \\sum_{i=1}^{N} \\sum_{j=1}^{C} Y_{ij} \\log(\\hat{Y}_{ij}),$\nwhere N is the number of training examples, C is the number of classes, $Y_{ij}$ is the ground truth one-hot encoded vector, and $\\hat{Y}_{ij}$ is the predicted probability for class j. We used a robust model to generate an offline training dataset $D_{off}$, which was then refined and validated by human annotators for quality assurance.\nRejection sampling, as used in LLaMA2 (Touvron et al., 2023), involves generating samples from a pre-trained model and filtering based on quality criteria to retain only high-quality examples. Unlike direct offline supervised fine-tuning (SFT), rejection sampling automates initial filtering to reduce low-quality samples before human annotation. In our iterative training process, rejection sampling boosts performance post offline dataset training."}, {"title": "3.2 Direct Preference Optimization", "content": "Direct Preference Optimization (DPO), has emerged as efficient alternatives to RLHF, eliminating the need for a separate reward model (Rafailov et al., 2023; Azar et al., 2023; Ethayarajh et al., 2024). The loss function for DPO is defined as follows:\n$\\mathcal{L}_{DPO} (\\pi_{\\theta}; \\pi_{ref}) = \\mathbb{E}_{(x, y_w, y_l) \\sim D} [-\\log \\sigma(\\beta \\log \\frac{\\pi_{\\theta}(y_w | x)}{\\pi_{ref}(y_w|x)} - \\log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)})]$\nwhere $\\pi_{\\theta}$ is the language model being optimized and $\\pi_{ref}$ refers to the model after SFT (SFT). The scaling factor $\\beta$ measures errors in ranking results and accounts for the KL constraint. In vanilla/offline Direct Preference Optimization (DPO), the model is optimized using a given set of preference data $(x, y_w, y_l) \\sim D$, where the dataset-generating model and the optimized model are not the same."}, {"title": "3.3 Iterative Learning with AI Feedback", "content": "When optimizing DPO models, offline preference datasets and off-policy updates can cause generalization issues with out-of-distribution (OOD) queries. These issues can be mitigated by incorporating online preference datasets and using on-policy learning approaches. (Guo et al., 2024; Xiong et al., 2024; Dong et al., 2024).\nWe follow the experimental setup of (Xiong et al., 2024), utilizing a batch size of m in online setting. Our methodology integrates the LLM-as-a-Judge approach for real-time feedback, as introduced by (Zheng et al., 2023), to refine the model progressively.\nAlgorithm 1 outlines our iterative training process, starting with the initial dataset $D_{off}$. The agent processes each batch iteratively, involving model evaluation, data generation, and refinement. It generates multiple candidate responses per input, using a reward model (GPT-4) to select the optimal response and compare it with the ground truth. If the model-generated response exceeds the quality threshold, it replaces the original training sample. For DPO, the lowest-ranked response is identified as a negative example. The updated dataset is then used to refine the model via SFT or DPO techniques. After multiple iterations, the algorithm outputs the best-performing model variant based on predefined metrics. This iterative process continuously enhances response quality, creating a self-refining training paradigm that progressively improves model performance."}, {"title": "4 Experiment", "content": "We conduct experiment on a real-word industry financial QA dataset to validate the PEER framework discussed in section 2 and evaluate the agent tuning methods discussed in section 3."}, {"title": "4.1 PEER Framework experiment", "content": "Dataset Since the main usage scenario of PEER framework is the interpretation and analysis of domain events and problems, we mainly tested and compared the performance of PEER on the dataset of financial QA. We sampled hundreds of professional questions from our business scenarios and divided them into nine categories. Details of the dataset distribution are shown in Table 4.\nBaselines We conducted experiments using two base models, GPT-3.5 turbo (16k) and GPT-40, with Python for execution. For FinQA datasets, we compared with the BabyAGI multi-agent framework due to its similar task creation, organization, and execution capabilities to PEER.\nTo assess the impact of the \"Review\" agent in the PEER framework, we designed self-ablation experiments with and without the \"Review\" agent. We set the maximum rounds for both BabyAGI and PEER (with \"Review\") to 5 and used Google for information retrieval. Under GPT-3.5 turbo (16k), we recalled the top 2 search results with a token limit of 13,000. For GPT-40, we increased the parameters to the top 6 and 125,000 tokens, leveraging the model's enhanced performance.\nMetrics Despite GPT-4's widespread use for evaluations, its confidence can be influenced by position and verbosity biases (Guo et al., 2024; Zheng et al., 2023). To mitigate these issues, we have developed two evaluation methodologies based on GPT-4:\n1. GPT-4 scores all answers across various dimensions, and we calculate the average score for each dimension. Detailed scoring dimensions, rules, and their meanings are provided in Table 6 in the Appendix.\n2. GPT-4 selects the best answer between those provided by PEER and the control group. For this evaluation, we use the win rate as the metric, with selection criteria outlined in Table 7.\nAnalysis In the comparative experiment with BabyAGI, as depicted in Table 1 and Figure 3, PEER consistently surpasses BabyAGI in both average score and win rate, irrespective of the base model employed. PEER demonstrates superior performance in dimensions such as integrity, relevance, logic, structure, and comprehensiveness, often by a margin exceeding one point. Specifically, under the GPT-3.5 turbo (16k) model, PEER achieves a win rate of 83% compared to BabyAGI, and still maintains an 81% win rate under the GPT-40 model. This is attributed to PEER's strategy of simultaneously addressing multiple questions and synthesizing responses, in contrast to BabyAGI's approach of addressing one question per round.\nIn the ablation experiment, as illustrated in Table 1 and Figure 3, PEER scores higher in most dimensions and attains a 64% win rate under the GPT-3.5 turbo (16k) model compared to PEE. However, under the GPT-40 model, the advantages conferred by the \"Review\" agent diminish, as GPT-4o inherently excels in processing, understanding, and expression. The initial outputs from the \"Plan,\" \"Execute,\" and \"Express\" agents sufficiently meet the requirements, rendering further modifications less impactful. Consequently, PEER's win rate decreases to 46%, and the score differences between"}, {"title": "4.2 Tuning Experiment", "content": "Dataset We conducted two categories of experiments: one focusing on individual agents and the other on the entire workflow. Dataset sizes are provided in Table 3, with the data being open-sourced. The test set for individual agents is derived from the intermediate results of the evaluation set detailed in Table 4, whereas the test set for the entire workflow corresponds directly to Table 4.\nExperiment Setup As in Section 4.1, for the evaluation of individual agents and the entire workflow, we also employed the LLM-as-a-Judge approach. Specifically, for individual agents, we used scoring and pairwise comparison to evaluate the performance of each iteration. For the entire workflow, we used GPT-40 to score and compare the results of GPT-4 + PEER, the SFT results using offline data, and the best model obtained through iterative training.\nAnalysis Figure 4 illustrates the win, tie, and loss rates across different iterations for three agents involved in planning, execution, and expression. Both DPO and SFT show progress with each iteration. For example, for the planning agent, the first iteration of SFT achieves a win rate of 43.15% compared to SFT-offline, improving slightly to 43.21% in the second iteration. DPO demonstrates a faster convergence than SFT. In the second iteration, the win rates for SFT across the three agents are 43.21%, 41.34%, and 53.33%, respectively. In contrast, DPO's win rates are 23.17%, 20.74%, and 27.17%, which are lower than the corresponding tie rates of 56.61%, 60.60%, and 57.61%. Due to space limitations, detailed scoring results are provided in Appendix B and Table 5."}, {"title": "5 Related Work", "content": "Multi-agent system Despite pioneering projects in this field, such as AutoGPT, BabyAGI, CAMEL, MetaGPT, and AutoGen (Talebirad and Nadiri, 2023; Hong et al., 2023; Li et al., 2023; Wu et al., 2023; Wang et al., 2024b; Xi et al., 2023), demonstrating their potential, achieving fully autonomous Al agents remains a significant challenge. These dynamic process agents, also known as autonomous intelligent agents, can autonomously perceive the environment, make decisions based on observations, and take actions. Subsequently, they reflect on the outcomes of their actions and plan their next steps accordingly. While theoretically generalizable to any scenario, they face issues such as poor controllability, instability, reproducibility problems, and low task completion rates in specialized domains (Wang et al., 2024b; Xi et al., 2023). PEER strikes a balance between model flexibility and controllability through effective pattern design, considering practical industrial needs, including efficiency, cost-effectiveness, and operational simplicity, making it more suitable for industrial applications.\nAgent self-evolution Many research efforts aim to transform past experience into usable knowledge and apply it in new reasoning processes to drive continuous model evolution (Wang et al., 2024a; Song et al., 2024; Tao et al., 2024; Zhao et al., 2023). However, these studies often place high demands on the model's ability to follow instructions, which is particularly challenging for models with fewer parameters. To overcome this challenge, our research adopts an iterative training approach. Specifically, we use both successful and failed cases from previous steps as new training data to promote the model's evolution."}, {"title": "6 Conclusion and Future work", "content": "Conclusion In this work, we introduced the PEER framework to address the tri-lemma of performance, cost, and data privacy in domain-specific applications. The framework balances flexibility and controllability through effective pattern design, meeting industrial demands for efficiency and cost-effectiveness. We also developed industrial practices that use online data and user feedback for effective model tuning, promoting continuous model evolution. Our empirical studies, particularly in the financial question-answering domain, demonstrate that this approach achieves 95.0% of GPT-4's performance while managing costs and safeguarding data privacy.\nFuture work Despite our progress in using multi-agent systems to address domain-specific tasks (e.g.finacial), several areas remain ripe for further exploration and improvement:\n\u2022 Long-term Learning and Memory Mechanisms: Explore ways to equip the model to accumulate and utilize knowledge over extended periods.\n\u2022 User Interaction and Feedback Mechanisms: Study how user interactions and feedback can further guide and optimize the model's behavior, achieving a more user-friendly agent design.\n\u2022 Enhancing Generalization Capability: Investigate methods to further improve the model's generalization ability, enabling agents to tackle other financial problems such as factor-based stock selection or other quantitative trading strategies."}, {"title": "A PEER experimental dataset distribution", "content": ""}, {"title": "B Tuning Results for individual agents", "content": "In this study, we experimented with several different iterative modeling approaches. The SFT-OFFLINE model refers to the SFT model trained exclusively on offline data. The DPO-ITER-1 model is obtained by further training the SFT-OFFLINE model using DPO. Similarly, the DPO-ITER-2 model is derived by continuing the iterative training on DPO-ITER-1 using DPO. The SFT-ITER-1 and SFT-ITER-2 models follow the same iterative training process.\nAs shown in Table 5, in the three tasks (Plan, Execute, Express), the dpo-iter-2 model stands out with its exceptional performance, particularly in the Express task, where it leads significantly with an average score of 4.56. Meanwhile, the sft-iter-2 model also demonstrates its strength in the Plan task, achieving an average score of 3.78. In the Execute task, the dpo-iter-2 model again takes the top spot with an average score of 4.34. Overall, the dpo-iter-2 model shows advantages in various metrics, indicating its adaptability and effectiveness across different tasks. Additionally, an increase in iteration count seems to positively impact model performance, though the extent of improvement depends on the specific model used (sft or dpo) and the particular requirements of the downstream tasks."}, {"title": "Cevaluation prompt", "content": "Table 6 and table 7 shows the prompt used for LLM Evaluation."}, {"title": "D Prompt for PEER", "content": ""}]}