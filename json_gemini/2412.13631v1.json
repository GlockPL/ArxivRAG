{"title": "Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning", "authors": ["Eitan Wagner", "Nitay Alon", "Joseph M. Barnby", "Omri Abend"], "abstract": "Theory of Mind (ToM) capabilities in LLMS have recently become a central object of investigation. Cognitive science distinguishes between two steps required for ToM tasks: 1) determine whether to invoke ToM, which includes the appropriate Depth of Mentalizing (DoM), or level of recursion required to complete a task; and 2) applying the correct inference given the DoM. In this position paper, we first identify several lines of work in different communities in AI, including LLM benchmarking, ToM add-ons, ToM probing, and formal models for ToM. We argue that recent work in AI tends to focus exclusively on the second step which are typically framed as static logic problems. We conclude with suggestions for improved evaluation of ToM capabilities inspired by dynamic environments used in cognitive tasks.", "sections": [{"title": "1 Introduction", "content": "The ability of Large Language Models (LLMs) to efficiently integrate social information is essential for ensuring AI trust and safety. This capability relies on inferring and representing the beliefs, desires, and intentions of other synthetic and biological agents, also known as mentalizing, or Theory of Mind (ToM) (Premack and Woodruff, 1978). LLMs have been recently regarded as general-purpose reasoning models (Brown et al., 2020), and their ToM capabilities have come under scrutiny, particularly as to whether social cognition can emerge purely from associations principles (Sap et al., 2022). Notably, Kosinski (2023) claimed that ToM capabilities emerge in post-GPT3 models, like ToM in children (Astington and Jenkins, 1995). This claim sparked debates as to the correct interpretation of these results (Ullman, 2023; Pi et al., 2024). ToM capabilities involve two steps: (1) determine the depth of mentalization to use, which specifically includes the decision of whether to model the agents separately (self-other divide); and (2) apply correct inference for each agent's mental state, given the depth. Failure in any of the steps leads to an incorrect conclusion (Fig. 1). Existing benchmarks for ToM primarily concentrate on whether agents hold correct beliefs about others, however as we will show, very little attention is paid to whether LLMs can distinctly model self-other states. Nevertheless determining this is critical in three ways: 1) it ignores a critical theoretical component of ToM which has been well defined in humans, 2) it confounds errors with poor or fuzzy social logic(Pi et al., 2024), and 3) the act of mentalizing has a price, both in resources"}, {"title": "2 ToM in AI research", "content": "Here we synthesize the various approaches and challenges used to assess ToM in the AI community."}, {"title": "2.1 ToM Benchmarking", "content": "Evaluation of LLMs on ToM benchmarks is an active line of research. A dominant component in these tests is variations of the Sally-Anne (SA) test (Wimmer and Perner, 1983; Baron-Cohen et al., 1985). In the SA test, participants are tested on their ability to identify false beliefs, by recognizing that Anne has moved a ball, but that Sally is not exposed to this event and therefore maintains the belief that the ball did not move. Variations of the SA task have been used as a gold-standard benchmark for ToM in LLMs (Grant et al., 2017; Nematzadeh et al., 2018; Gandhi et al., 2023). Other works improve these datasets to control for spurious correlations and provide more natural settings. This includes: ToMi Le et al. (2019), OpenToM (Xu et al., 2024) Rather than vignette-based tasks, some benchmarks use a language-based conversational setting to assess ToM reasoning, such as Mindcraft (Bara et al., 2021), FanTom (Kim et al., 2023), and NegotiationToM (Chan et al., 2024). Some benchmarks extend the SA task to more complex objectives. This includes higher-order ToM, as in HiToM (Wu et al., 2023); additional perception inference, as in Percept-ToMi and Percept-FANTOM (Jung et al., 2024); and epistemic logic conclusions, as in (Sileo and Lernould, 2023). Additional extensions derive questions from common ground annotations, as in CommonToM (Soubki et al., 2024) or from other modalities, as in MMTOM-QA(Jin et al., 2024). The utility of these tasks in assessing ToM in LLMs is still a matter of debate. GPT3 showed difficulties engaging with ToMi Sap et al. (2022), whereas more complex models have been argued to show more success Kosinski (2023); Gandhi et al. (2023). Nevertheless, slight alterations to task structure can break these effects (Ullman, 2023; Shapira et al., 2024; Ma et al., 2023a; Chen et al., 2024; Nickel et al., 2024), and this is typically interpreted as a failure in reasoning complexity (Pi et al., 2024). Other criticisms of current benchmarks are the lack of transparency in the training set used to compare LLMs with human-like performance (Ma et al., 2023b; Shapira et al., 2024). Notably, all these benchmarks are static, in the sense that they deal with pure observation rather than moment-to-moment model evolution during interaction (Ma et al., 2023b)."}, {"title": "2.2 TOM Add-ons", "content": "We describe methods to improve ToM as ToM Add-ons. We note that these components do not attempt to evaluate LLMs' ToM capabilities on their own but rather attempt to complement LLMs with external modules. Prompting techniques refinement have shown improved scores on classic tasks (Tan et al., 2024; Jung et al., 2024; Huang et al., 2024; Wilf et al., 2024). Explicit symbolic modules can also be updated based on the input and improve repeated accuracy (Qiu et al., 2024; Hou et al., 2024). Others add post-model decoding methods (Sclar et al., 2023) or symbolic planners (Jin et al., 2024) to boost performance."}, {"title": "2.3 Linear Probing for ToM", "content": "Linear probing (Alain and Bengio, 2018) is a method for investigating internal representations of neural models by training linear classifiers on intermediate layers to predict some variable of interest. Representations that can be used for linear predictions of some target variable are assumed to encode it to some extent."}, {"title": "2.4 Formal Architectures of ToM", "content": "Inspired by Reinforcement Learning (RL), ToM has been modeled as an inferential process by which an observer infers the goals of an agent from the agent's actions (Baker et al., 2011; Jara-Ettinger, 2019; Jara-Ettinger et al., 2016). This approach is similar to Inverse-RL (Ng and Russell, 2000).This concept has also been instantiated as neural network process Rabinowitz et al. (2018). This model can be categorized as an inferring agent that learns an association between actions and un-observed variables such as goals or future actions. While useful for observational tasks (e.g., Sally-Anne tasks), they are limited to observation, also known as zero-level Depth of Mentalizing (DoM) \u2013 DoM(0) (Barnby et al., 2023; Alon et al., 2024). This means the inferring agent does not allow for recursive beliefs (\u201cI think that you think that I think\u201d). Instead, it is limited to shallower mentalizing \u2013 \u201cI think that that you think\" (Fig. 2). Including recursive structure may allow the acting agent to infer and predict how another agent perceives them, and act to influence the behavior by manipulating this image. Implicitly a shallow architecture also assumes a benign observer, whose decisions do not affect the observed agent. More complex models are used to solve this issue. Predominantly, the Interactive-POMDP (IPOMDP; Gmytrasiewicz and Doshi, 2005) allows other agents as part of the world model. We refer the reader to Alon et al. (2023) for a full introduction of the model and its applications in multi-agent environments. While allowing for recursive ToM, the IPOMDP has two major pitfalls. First, it assumes that an agent's DoM level is fixed (for example a DoM(k)) and that the agent views other agents as having a fixed DoM(k \u2212 1) level. This assumption limits the model and is contradicted by empirical evidence (see next section). Second, this model (and those similar) is computationally demanding and intractable in many cases, thus restricting IPOMDP models to simple tasks with tractable nested beliefs. There has been an attempt to integrate recursive formulations into LLMs. However, due to the black-box nature of LLMs this is difficult to capture"}, {"title": "3 What Can We Learn From Cognitive and Social Science?", "content": "Cognitive science, Economics, and Neuroscience have explored ToM for decades. We believe that principles from these fields can inspire more transparent and efficient architecture in LLMs and AI in general. Given a problem (=prompt) X and a variable of interest Y (e.g., the correct location of the object in a Sally-Anne test), the general goal is to infer the top class based on P(Y|X). We formalize the process (see Fig. 1) as two steps \u2013 first the meta-decision of whether to invoke ToM or not, and then the reasoning process of correctly attributing states to agents given the ToM invoking decision. In terms of do-calculus (Pearl, 2012), the second step can consist of inference without ToM, based on  PDOM(-1)(Y|X) := P(Y|do(DoM(-1)), X), or inference with ToM, based on PDOM(k\u22650) (Y|X) := P(Y|do(DoM(k \u2265 0)), X)."}, {"title": "3.1 Invoking DoM", "content": "Several theoretical and empirical cognitive works identify the common principles for social interaction. Biological agents performing cooperative tasks can afford very shallow mentalizing (Devaine et al., 2014) and instead rely on a common policy (e.g., social norms or prosocial goals). As soon as competition is introduced hierarchical mentalizing is beneficial to avoid duplicitous action (Alon et al., 2023). However, moving from a shallow to a hierarchical level of mentalizing is more expensive, requiring additional time and energy to compute, and biological agents have limited resources (Sweller, 1988; Bossaerts and Murawski, 2017). This limits the ability to perform ToM-demanding tasks for a long time Rilling et al. (2004), and can lead to failures in social reasoning when neural architecture has developed inefficiently (Barnby et al., 2022). LLMs do not have the same energy demands as humans, and may not be limited by the same energy-conserving principles that enforce cooperation. Failures to appropriately capture the degree to which LLMs can move into competitive modes of mentalizing may hinder human-AI alignment."}, {"title": "3.2 Inference Failure", "content": "Even with adaptively invoked ToM, an agent may still be incorrect. For example, being fixed at shallow-DoM may miss malicious intent from other agents (Sarkadi et al., 2019). On the other hand, invoking hierarchical DoM during a cooperative problem can induce false beliefs of harm (Alon et al., 2024). How can LLMs auto-tune their ToM level without explicit instructions to do so? Without a specific architecture to measure and instill this, it is difficult to build an LLM that can adapt on the fly."}, {"title": "3.3 Revisiting Existing Work", "content": "What do the ToM benchmarks benchmark? With current vignette-based reasoning tasks, current benchmarks practically address PDoM(k,k\u22650) (Y|X). More challenging benchmarks are constantly introduced but still focus on non-interactive presentations. This precludes the need to use ToM or even reason about self vs. other distinctions. We note that even if DoM is not given, as long as we test the prediction for Y|X, we cannot determine whether performance on the test reflects a ToM ability. If the model was incorrect, we do not know if the failure was due to undermentalization (Type B error) or due to failed reasoning (Type C error). If the model was correct but ToM is unnecessary, then we don't know if the success was based on the correct ToM (Type A error) (Kim et al., 2023). Only if ToM is necessary and the model succeeded, can we conclude that ToM was used and correctly so (with the caveat of spurious correlations). What do ToM Add-ons Add? Similarly, typical NLP works on ToM add-ons propose methods while assuming DoM(k > 0) is necessary. The decision about the required DoM level is unaddressed (Type A and B errors). What do linear probes probe? A linear probe for ToM representations is trained to predict an agent's mental state. These works typically have the same issues as benchmarks \u2013 the premise and questions are usually such that it is clearly intended for ToM to be invoked. The probe does not distinguish between failure based on incorrect ToM levels (Type B error) and failure for other reasons (Type C error). What do formal models model? Typical works on inverse RL propose models that inherently assume DoM(0). While these models are beneficial for learning at this level, they do not address the step of invoking DoM (Type A and B errors). Some exceptions are the work of Alon et al. (2023) and Hula et al. (2015), which we use as inspiration for DoM. Nonetheless, even these works assume a fixed, immutable DoM level \u2013 they assume that ToM is needed and invoked for inference within a given task. A comprehensive model that examines all the steps proposed in Fig 1 is, to the best of our knowledge, missing from this literature."}, {"title": "4 Discussion and Conclusion", "content": "In this work, we approached ToM in LLMs as an interdisciplinary topic. We argue that NLP benchmarks and AI works typically address non-interactive logic problems. While many of the benchmarks are valuable as reasoning tests and may provide insights into the performance of LLMs, they do not address the core issue of ToM in LLMs, interaction and adaptive mentalizing. We argue that a better understanding of ToM requires (1) benchmarks that test whether ToM is correctly invoked or not, (2) evaluation for TOM in interactive settings where the model is an active agent, and (3) evaluation for the appropriate degree of DoM, given a social context (cooperative vs competitive). Some works introduce ToM tasks for LLMs in interactive domains (Sclar et al., 2022; Li et al., 2023), albeit with a simplified context. These serve as important steps for the evaluation of LLMs' TOM capabilities."}, {"title": "Limitations", "content": "In this work we point to limitations in existing ToM works for LLMs. Many of our suggestions are empirical questions and will need to be supported by data. Our work focuses on attributed beliefs, which are a single component of ToM. Previous work has pointed out that true evaluation of ToM must include many other factors, such as emotions and desires (Ma et al., 2023b). Despite this limitation, we note that belief attribution remains a critical component in ToM evaluation. We also note that the ToM literature in cognitive science is vast and often conflicting. The components of ToM are actively debated and thus our recommendations must be held in light of this"}]}