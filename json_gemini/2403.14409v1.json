{"title": "Locating and Mitigating Gender Bias in Large Language Models", "authors": ["Yuchen Cai", "Ding Cao", "Rongxi Guo", "Yaqin Wen", "Guiquan Liu", "Enhong Chen"], "abstract": "Large language models (LLM) are pre-trained on extensive corpora to learn facts and human cognition which contain human pref-erences. However, this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society. Prior research has typically tackled the issue of bias through a one-dimensional perspective, concentrating either on locating or mitigating it. This limited perspec-tive has created obstacles in facilitating research on bias to synergistically complement and progressively build upon one another. In this study, we integrate the processes of locating and mitigating bias within a unified framework. Initially, we use causal mediation analysis to trace the causal effects of different components' activation within a large language model. Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupa-tional pronouns, and compare it against two baselines on three gender bias datasets and seven knowledge competency test datasets. The ex-perimental results indicate that the primary contributors to gender bias are the bottom MLP modules acting on the last token of occupational pronouns and the top attention module acting on the final word in the sentence. Furthermore, LSDM mitigates gender bias in the model more effectively than the other baselines, while fully preserving the model's capabilities in all other aspects.", "sections": [{"title": "Introduction", "content": "In recent years, sophisticated artificial intelligence models, notably exemplified by ChatGPT [28,31,33], are specially designed to excel in comprehending com-plicated natural language and generating human-like text. However, as these models become increasingly integrated across various sectors [6,14], the inherent biases within these systems has become a subject of growing concern.\nBias refers to the existence of consistent inaccuracies, misattributions, or erroneous perceptions leading to a preference for specific groups or concepts,"}, {"title": "Interventions on Activations for Tracing Bias Information Flow", "content": ""}, {"title": "Preliminaries", "content": "Our primary focus lies on autoregressive, decoder-only language model structures denoted as Fo, representing the predominant architecture found in contemporary large language models. These models operate by transforming the input sequence \u00e6 into t tokens xo,...,xt. Subsequently, these tokens are fed through L layers of Transformer decoders, ultimately generating probabilities for the next token Xt+1:\nFo(xo, ..., xt) = softmax (WE. (h-1+a++m+))\n= P (Xt+1/xo, ..., Xt)\nHere, WE and y represent the embedding matrix and layernorm, respectively. al and me denote the hidden states of the Multi-Head Self-Attention (MHSA) and Feed-Forward Network (MLP) at the L-th layer. The general forms of MHSA and MLP at the l-th layer and the j-th token x are given as follows:\na = WMHSAMHSA' (y (h1, h2-1, ..., h-1)),\nm\u00b2; = Wprojo (Wer (a; +h-1)),\nh = h\u00b9 + a + m\u00b2;\nHere, WMHSA and Wproj refer to the output weights of the MHSA and MLP at the l-th layer, respectively, while o denotes the non-linear activation func-tion. Different LLMs frequently exhibit slight variations in implementing these transformations. Our goal is not to provide a full survey of these details but to capture essential terminology for our results."}, {"title": "Causal Tracing of Gender Bias", "content": "Based on Vig et al. [29] and Meng et al. [18], we aimed to delve into the mechanisms generating gender bias and its storage locations. Figure 1 illus-trates a causal graph, detailing the components within a large language model used to predict the subsequent word and the workflow for causal tracing. We quantify the extent of gender bias within the model by calculating the abso-lute value of the disparity between the predicted probabilities for \"she\" and \"he\": P(gb) = |P(he) \u2013 P(she)|. we assess each component contributions to P(gb) across three different runs:\n\u2022 In the clean run, we pass a sentence including occupational pronoun that may be associated with gender bias into model G and collect different com-ponent's activation. Figure 1 provides an example illustration with the sen-tence: \"The nurse is crying because\", for which the model prefers to predict the next word as \"she\" rather than \"he\".\n\u2022 In the corrupted run, the sentence undergoes obfuscation before the net-work's execution. Embeddings corresponding to all tokens within the sen-tence are corrupted as hi := h+\u20ac, where \u0454 ~ N(0; v)\u00b3. This alteration could influence the model's prediction probabilities for gender-associated words.\n\u2022 In the restoration, the causal impact of internal hidden states is examined by restoring these states to their values from the clean run. We intercept a hidden state, setting it to output the clean state h : h; subsequent computations proceed without additional intervention. Essentially, if a finite"}, {"title": "Causal Tracing Results", "content": "We utilized GPT2-XL [21] and GPT-J [31] as our experimental models due to their range of layers, spanning the upper and lower bounds of current large-scale language models. Employing 1000 biased sentences containing occupational pro-nouns, we computed ATE and AIE. Our analysis involved altering the activation values of different components corresponding to each token in the sentence, not only include hidden statesh, but also MLP modules m and attention modules a.\nThe computed ATE of 0.229 and 0.354 indicate perturbations in the behavior of the model due to corruption in word embedding. Figure 2 illustrates the AIE of internal components of GPT-2 XL and GPT-J. Drawing from Figure 2, the following conclusion can be inferred:\n\u2022 The two higher portions of the AIE values in corresponds to the higher AIE values in MLP and attention module respectively, with one located at the bottom layer of the last occupation token and the other at the top layer of the last token. This is understandable since, according to Eqn.2, the value of h is jointly constituted by manda.\n\u2022 While high AIE values are observed in both MLP and attention modules, the higher AIE values in the attention module appear at the top layers near the model's output, do not constitute a novel insight. However, in the MLP module, the high AIE values emerge at an early stage the bottom layer, and specifically in the middle part of the sentence the last occupation pronoun token, marking a significant new discovery.\n\u2022 Our experiments reveal notable differences between GPT2-XL and GPT-J: the significance of the bottom MLP modules at the last occupational token and the top attention modules of the final token is more accentuated in GPT-J. Conversely, GPT2-XL exhibits contributions from top MLP modules, such as layer 35. These distinctions indicate nuanced variations in the bias transfer processes among models with different parameters.\nTo gain deeper insight into the distinct role of MLP at the bottom layer, we analyzed indirect effects using a modified causal graph (Figure 3). Here are the steps of our approach:\n\u2022 Initially, we collected the contribution of each MLP module within baselines with corrupted input embedding.\n\u2022 Next, aiming to distinctly isolate the influence of MLP modules in quan-tifying causal effects, we modified the computation graph and substituted"}, {"title": "Interventions on Weights for Mitigating Gender Bias", "content": ""}, {"title": "Associative Memory Foundation", "content": "The concept of associative memory posits that any matrix W can function as an associative memory [18], storing a set of key-value pairs (ki, vi) retrievable through matrix multiplication:\nVi \u2248 Wki,\nThis principle stands as a foundational tenet in neural networks [16]. For instance, if the keys ki form a set of mutually orthogonal unit-norm vectors, a flawless memory can be constructed as W = \u2211i viki. In relative terms, an MXN matrix can hold a maximum of N associations.\nIn the preceding section, we outlined the bias generation mechanism, at-tributing bias generation to the bottom MLP modules within the model. This suggests that after enriching information within the bottom MLP modules, the biased vector is formed. Consequently, information from this biased vector is extracted by top attention modules, significantly affecting the model's output. When combined with the associative memory concept, it leads to an inference: if an unbiased vector k exists, after sequential transformations through some as-sociative memory matrices (i.e., the bottom MLP modules\u00b9), these matrices will progressively alter vector k into a vector v with gender bias. Considering the im-pact of mutiple associative memory matrices, we can express this transformation as:\nUbias= Wtotalk."}, {"title": "Least Square Debias Methd", "content": "Based on what was mentioned earlier, if we can consider the bottom MLP mod-ules as associative memories, the certain vector k representing the occupation-related word (such as \"nurse\") passing through those associative memories will acquire a vector v with gender bias information. Therefore, a natural idea would be to modify the associative memory in a way that the resulting vectors no longer carry biased information, denoted as v*. In other words, the modified associative memories should satisfy:\nv* = Wtotalk.\nIt's important to emphasize our aim is to maintain other effects of Wtotal as much as possible while eliminating the biased information it generates for specific occupational pronouns within its associative memory. If we denote vec-tors representing occupational pronouns as ko \u2208 E, and vectors excluding these terms as k \u2208 P, considering the situation involving multiple vectors, it can be approximately formulated as:\nV1 = Wtotal P,\nV* \u2248 Wtotal E,\nV\u2081 \u2248 WtotalP.\nV\u2081 denotes the resulting vector obtained from k \u2208 P after undergoing transfor-mation via the initial associative memory Wtotal. Building on insights from Meng"}, {"title": "LSDM mitigates gender bias effectively", "content": "To explore the effectiveness of different gender debias algorithms, we evaluated the effect on five large language models whose scales range from 6B to 13B parameters:\n\u2022 GPT-J [31]: An auto-regressive text generation model trained on the Pile with 6 billion parameters. As an early large language model, GPT-J has been widely used in a variety of experiments.\n\u2022 Llama [28]: A collection of foundation language models ranging from 7B to 65B parameters, trained on trillions of tokens. Llama demonstrates the abil-ity to train state-of-the-art models using publicly available datasets exclu-sively, without resorting to proprietary and inaccessible datasets. We chose Llama-6B and Llama-13B as test models.\n\u2022 Baichuan 1 and 2 [33]: The new generation of open-source large language models, trained on high-quality English and Chinese corpus. They achieved the best performance of their sizes on multiple authoritative Chinese, En-glish, and multi-language general and domain-specific benchmarks. We chose Baichuan-7B and Baichuan2-13B as test models.\nWe examine four algorithms, including:\n\u2022 None, the original model without any debias algorithms.\n\u2022 LSDM, as mentioned in 3.2.\n\u2022 FT, we fine-tuned the model on a dataset consisting of 5980 sentences gen-erated from section 3.3, with the training objective being: \u2013 log P [o* | x].\nThe same as Eqn 16, o* made adjustments to the original model output, ensuring that the last token output probabilities for the words \u201che\u201d and \"she\" are P(he) = P(she) = [P(he) + P(she)], while keeping the prediction probabilities for other words unchanged.\n\u2022 CDA [38], unlike FT, o* ensured that the last token output probabilities for the words \u201che\u201d and \u201cshe\u201d are P(he) = P(she), P(she) = P(he), while keeping the prediction probabilities for other words unchanged.\nFor the four methods mentioned above, we adjusted parameters of the same size to ensure a fair comparison. Three datasets were used to measure gender bias in the updated models:\n\u2022 WinoGender [24] is made of Winograd Schema Challenge, and biases are evaluated by determining if a model co-reference resolution performance is impacted by the gender of the pronoun. This evaluation aims to un-veil societal biases linked to occupations captured by the model. For in-stance, 'The nurse notified the patient that his shift would be ending in an"}, {"title": "LSDM maintains the model's capabilities", "content": "Another crucial aspect is that the debiased model should perform well on other datasets unrelated to gender bias [15,38]. Therefore, we selected the following seven datasets that relate to knowledge, language, reasoning, and comprehension for evaluation:\nCOPA [23]: Evaluates open-domain commonsense causal reasoning with 1000 questions. Each question presents a premise and two alternatives, re-quiring the selection of the alternative with a more plausible causal relation to the premise.\nRTE [30]: A natural language inference task that determines logical relations (entailment, contradiction, neutral) between given sentence pairs.\nCommonsenseQA [27]: A multiple-choice question-answering dataset that necessitates various types of commonsense knowledge to predict the correct answers. It comprises 12,102 questions with one correct answer and four distractors."}, {"title": "LSDM is proof of the casual tracing conclusion", "content": "We chose to modify the last linear layer of the MLP module among the bottom layers {3, 4, 5, 6, 7, 8} of the model. To validate causal tracing conclusions regard-ing biased information in the bottom MLP, we conducted two sets of compari-son experiments: selecting middle layers {13, 14, 15, 16, 17, 18} for GPT-J-6B and Llama-7B, 17, 18, 19, 20, 21, 22} for Llama-13B; top layers {21, 22, 23, 24, 25, 26} for GPT-J-6B, {25, 26, 27, 28, 29, 30} for Llama-7B, {33, 34, 35, 36, 37, 38} for Llama-13B."}, {"title": "The scope of the LSDM", "content": "In order to determine the scope of LSDM's effect, we additionally included an evaluation dataset containing 10 neutral pronouns defined by Vig et al. [29] Each"}, {"title": "Conclusion", "content": "This article investigates the storage locations and generation mechanisms of gender bias in large language models and proposes a gender debias method called LSDM based on knowledge editing. We employed a causal tracing method to pinpoint the location of model bias generation and suggest that gender bias is generated due to the information enrichment effect of the bottom MLP modules and is extracted by top attention modules to influence model output. Based on this, we use the least squares algorithm to modify the bottom MLP module to eliminate model bias. The experimental results show that LSDM is an efficient debias method, which not only successfully removes most of the occupational gender bias in the sentences, but also overcomes the problem of catastrophic forgetting that exists in all the traditional methods, and LSDM can be extended to occupational pronouns that have not been seen before.\nIn our future work, we aim to delve deeper into the mechanics of this debias method to thoroughly understand and apply it to enhance the model's knowledge performance."}, {"title": "Limitations", "content": "This paper conducts an in-depth analysis of the causes of gender bias and pro-poses an effective LSDM method to alleviate this challenge. Despite the signif-icant performance of the proposed method, there are still some limitations. In terms of causal tracing, although most components have less impact on the final outcome, how these components collectively influence the model output has not been extensively studied in this paper. While LSDM achieves good bias reduc-tion effects on some datasets by targeting specific occupational pronouns, its effectiveness remains poor for occupational pronouns outside the distribution. Further, this paper focuses only on bias between gender and occupational pro-nouns, how to extend this method to a broader range of biases remains to be explored in future research."}]}