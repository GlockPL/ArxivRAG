{"title": "Modeling Story Expectations to Understand Engagement: A Generative Framework Using LLMs", "authors": ["Hortense Fong", "George Gui"], "abstract": "Understanding when and why consumers engage with stories is crucial for content creators and platforms. While existing theories suggest that audience beliefs of what is going to happen should play an important role in engagement decisions, empirical work has mostly focused on developing techniques to directly extract features from actual content, rather than capturing forward-looking beliefs, due to the lack of a principled way to model such beliefs in unstructured narrative data. To complement existing feature extraction techniques, this paper introduces a novel framework that leverages large language models to model audience forward-looking beliefs about how stories might unfold. Our method generates multiple potential continuations for each story and extracts features related to expectations, uncertainty, and surprise using established content analysis techniques. Applying our method to over 30,000 book chapters from Wattpad, we demonstrate that our framework complements existing feature engineering techniques by amplifying their marginal explanatory power on average by 31%. The results reveal that different types of engagement continued reading, commenting, and voting are driven by distinct combinations of current and anticipated content features. Our framework provides a novel way to study and explore how audience forward-looking beliefs shape their engagement with narrative media, with implications for marketing strategy in content-focused industries.", "sections": [{"title": "Introduction", "content": "Understanding when and why customers engage with stories (including books, TV shows, movies, etc.) is crucial for various applications in marketing and product design within content-focused industries. Knowing when readers are more likely to continue to the next chapter of a book or when viewers are more likely to tweet about a TV episode can inform marketing strategies in advertising, pricing, and recommendation systems, among others. Understanding potential drivers of engagement can help content creators decide what to produce.\nYet understanding the drivers of engagement is challenging due to the unstructured nature of the data, whether it is text in books or video in shows. Unstructured data is by nature high-dimensional and complex, leading to a vast array of potential features that could relate to content engagement. Given the limited number of books and shows produced annually, the sample size of observations is relatively small compared to the potential feature space, creating significant challenges for traditional analysis methods. Given this challenge, it is valuable to incorporate theory into generating additional valuable features to understand drivers of engagement.\nBuilding on economic theory, this paper introduces a framework to extract a set of features that enhance understanding of the factors driving audience engagement, based on the key premise that customer decisions are affected by their beliefs of what is to come (Friedman, 1957; Muth, 1961). We capture these beliefs by using a generative model to imagine potential story continuations and extract from these continuations measures of expectations, uncertainty, and surprise. While expectations and uncertainty have been widely modeled in the economics and marketing literature, they have mostly focused on structured data for important and well-defined variables, such as prices and qualities (Rust, 1987; Erdem and Keane, 1996). In contrast, even though it is natural for audiences to make their content engagement decisions based on what they expect to come next, such aspects of narrative engagement have rarely been modeled or approximated in the empirical literature dealing with unstructured narrative content.\nThe lack of modeling such expectations and uncertainty is warranted. Compared to structured data, there is not a clear starting point for how to model expectations and uncertainty in unstructured narrative contexts. With structured data, one starting point, motivated by rational expectations, is to assume that customers have specific expectations that are objectively correct (Manski, 2004). For example, an individual's belief of the distribution of prices, a one-dimensional feature, can be assumed to follow the empirical distribution of the prices observed in the data. Making the same assumption for unstructured content data is difficult. Given the first part of a story, it is unclear what a customer's belief over what is going to happen next is going to look like, not to mention measure. Even if we knew which story features to quantify, it is unclear how to generate the relevant distribution of features.\nThis paper proposes a novel framework for modeling expectations and uncertainty in stories. At a high level, our framework is made up of two steps: a story imagination step and a feature extraction step. In the story imagination step, we use a pre-trained large language model (LLM) to generate story continuations. Trained using the text of thousands upon thousands of books, LLMs can predict many probable story continuations from some initial text. For example, providing the LLM with the text from the first chapter,"}, {"title": "Method", "content": "Unlike traditional content-based methods, our approach leverages the power of LLMs to simulate consumer's beliefs about what is yet to come in a story. This novel approach is made possible by two key characteristics of LLMs:\n1.  Vast Knowledge Base: LLMs are trained on enormous datasets encompassing diverse narratives across various genres, cultures, and time periods (Radford et al., 2019). This expansive training allows them to capture complex patterns in storytelling that would be difficult to model explicitly. In our context, this means the LLM can generate plausible story continuations that reflect the nuanced ways in which narratives typically unfold, mirroring the expectations formed by consumers with broad exposure to stories.\n2.  Generative Capabilities: Unlike traditional models that often rely on pre-defined features, LLMs can generate new, contextually relevant content (Fan et al., 2018; Brown, 2020). This generative ability is crucial for our approach, as it allows us to simulate the open-ended, creative process of reader imagination. By generating multiple possible continuations for a given narrative, we can model the"}, {"title": "Story Input", "content": "Our starting point is a piece of narrative content, such as the first few chapters of a book. One challenge in using LLMs is there is a limit on the size of the context window for the input into the model. For example, GPT-3.5-turbo can at most accommodate 16,385 tokens, which translates to roughly 12,300 words. While we can easily feed in the text for one chapter, the context window size becomes binding after just a few chapters. To overcome this issue, we use as input a summary of chapters 1 to t 1 and the full text of chapter t to generate the predicted stories.\nPast research has shown that GPT models can generate summaries that are preferred to those generated by models fine-tuned for text summarization (Goyal et al., 2022) and even achieve human levels of summarization (Zhang et al., 2024). We summarize the text up until chapter t-1 recursively, that is summary(1,...,t-1) = summary(summary(1,...,t-2),text(t-1)), and use different prompts depending on whether the chapter is the first chapter or not:\n1. Prompt for Chapter 1: \u201cYou are an average book reader. Here is the first chapter of a book. Provide an extensive summary of the chapter. Focus on the characters, their actions and emotions, and events"}, {"title": "Imagination Generation", "content": "LLMs are trained to predict the next word given a set of preceding words. More specifically, language models estimate the conditional probability of seeing word wi, given all the previous words: p(wi|W1, ..., Wi\u22121). Given the large amounts of story data that went into training GPT, we expect the model to excel at generating story continuations. This novel approach aims to model the process of readers anticipating potential story developments, a key factor in engagement that has previously been challenging to model. Readers form beliefs based on their past story consumption and we can think of GPT as a representative reader who has consumed a vast and diverse set of content. Recent research has found that LLMs can generate economic expectations based on historical news that match human expectations (Bybee, 2023) and predict consumer preferences for new products (Lee, 2024).\nWe generate multiple imagined continuations of the story based on the content up to a specific chapter. Importantly, we generate not just one potential story continuation, but several to capture the uncertainty in how the story will develop. We use the following prompts to generate the imagined continuations:\n1. Prompt for Chapter 1: \u201cYou have read and understood the first chapter of a book, and now I will provide you with the text of this chapter. Here is the first chapter: {chapter text}. Based on this, please imagine the plot for the remaining chapters, weaving together the characters, their actions and emotions, and events in a coherent and consistent way. Then, summarize this plot into a set of 20 simple and distinct bullet points.\""}, {"title": "Feature Extraction", "content": "With multiple imagined story continuations per chapter, the next question is how to quantify the unstructured story text. We approach this question in two steps: 1) we extract from the text predefined features that have been proposed in the literature to be associated with narrative success and 2) we calculate measures of expectations, uncertainty, and surprise based on those features. Let i represent the focal book, t the focal chapter, and n the imagined story number with N capturing the total number of imagined stories per chapter. Let zitn denote the extracted features from the text (i.e., zitn = f(ImaginedStoryitn). The transformation f can be a rule-based algorithm like VADER (Hutto and Gilbert, 2014) or a learned deep learning model like GPT (Radford et al., 2019). In Section 5, we discuss the specific transformations and features we extract as an empirical demonstration of our method. Using the extracted features Zitn we calculate the expectations, uncertainty, and surprise as follows:\n1. Expectation Features: We calculate the mean of each feature across all N imagined continuations for a given chapter. This represents the average expected future state of the narrative.\nExpectationsit = En [zitn]\n= \\frac{1}{N} \\sum_{n=1}^{N} Zitn\n2. Uncertainty Features: We compute the variance of each feature across continuations, quantifying the degree of uncertainty in future narrative developments. This measure is akin to the measure of \"suspense\" proposed by Ely et al. (2015). While Ely et al. (2015) assume that utility is an increasing function of suspense, it is also possible that uncertainty relates to confusion or that readers may prefer certainty on some dimensions and uncertainty on other dimensions. Our framework allows us to treat this as an empirical question to be answered in Section 5.\nUncertaintyit = Varn[zitn] = \\frac{1}{N} \\sum_{n=1}^{N} (zitn - En[zitn])^2\n3. Surprise Features: Following Ely et al. (2015), we define surprise as the squared difference in expectations before and after consuming chapter t. It quantifies the degree of unexpectedness in audience expectations.\nSurpriseit = (Expectationsit - Expectationsi(t-1))^2"}, {"title": "Explaining Engagement", "content": "Finally, we use the extracted expectations, uncertainty, and surprise based on the imagined story continuations alongside the features extracted from the actual story text f(ExistingStoryit) to predict user engagement metrics. In our empirical context, the metrics include the continue-to-read rate, the comment-to-read rate, and the vote-to-read rate. We compare the benefit derived from incorporating data from the story imagination versus a standard approach only using the text data from the content."}, {"title": "Data", "content": "To demonstrate our proposed methodology, we collect book text with chapter-by-chapter engagement. We collect data from Wattpad, an online media platform that allows users to read and write stories. In October 2024, Wattpad had over 90 million readers and writers. For readers, the vast majority of stories are free to read but some stories require a premium membership or payment to access the later chapters. The major advantage of using Wattpad is that content is posted chapter by chapter and we can observe the read count, vote count, and comment count for each chapter and therefore calculate the continue-to-read, vote-to-read, and comment-to-read rates.\nWe scrape free books from Wattpad by focusing on the genres listed on Wattpad's homepage (e.g., action, adventure, romance). Appendix A provides additional details about the keywords used to scrape the books. For each book, we collect its title and description, when the book was created, the language the book is written in, whether the content is for mature audiences, and writer-provided book tags (e.g., \u201cschool\", \"drama\", \"friendship\"). For each chapter, we collect its text, title, date it was written, and its comment count, vote count, and read count.\nWe use GPT-4o-mini to summarize the chapters and GPT-3.5-turbo to generate the imagined story continuations. We use GPT-4o mini for text summarization because of its good summarization capabilities, its lower cost relative to GPT-3.5-turbo, and because we are not concerned about data leakage for this step. However, for the imagination generation, to alleviate the concern about data leakage (i.e., the engagement metrics and actual story continuations being part of the training data), we only include books that were published after the cutoff date of the training data for GPT-3.5-turbo (September 2021) so all book chapters are published January 2022 and onwards. This ensures the content was not used to train the GPT model we use to generate the imagined stories. Since our proposed story imagination method relies critically on the quality of the input text, we take several steps to clean the scraped stories, which we detail in Appendix B. After cleaning, we are left with 30,258 chapters across 1,735 books."}, {"title": "Summary Statistics", "content": "Table 1 provides summary statistics of our dataset of book chapters. Compared to more traditional books, which have on average 3,000 to 4,000 words per chapter, the Wattpad chapters are shorter with an average of 1,827 words per chapter. The engagement with these chapters is fairly high not only in terms of continuation rates but also in terms of comment and vote rates. Readers can comment throughout the text and comments are consolidated at the end of the chapter text. Readers can also vote to show their support for a writer and can only vote once per chapter. Our outcome measures of interest are:\n\u2022 Continue-to-read rate\nread count of next chapter/read count of current chapter\n\u2022 Comment-to-read rate\ncomment count of current chapter/read count of current chapter\n\u2022 Vote-to-read rate vote count of current chapter/read count of current chapter\nNotably, the continue-to-read rate can exceed one, suggesting some readers may skip chapters, reread chapters, or share chapters with friends."}, {"title": "Empirical Application with Books Data", "content": "We apply our proposed framework to the collected Wattpad data."}, {"title": "Predefined Story Features", "content": "As discussed in Section 2, the literature has identified several sets of features based on existing text to be associated with narrative success. To demonstrate our approach, we use three sets of features: 1) emotion features as measured by valence and arousal, 2) psychological themes, and 3) semantic path measures. We extract these features not only from the chapter text but also from the imagined stories generated by GPT-3.5-turbo."}, {"title": "Engagement Prediction Results", "content": ""}, {"title": "Relative Marginal Improvement", "content": "This section examines whether our method complements existing feature extraction approaches in explaining engagement. We hypothesize that if a feature extraction method significantly improves model performance when applied to the actual story content, then applying the same method to imagined story continuations should yield additional improvement. The core rationale being that if a reader cares about certain story dimensions in what they have consumed, then they will likely care about the same dimensions in what is to come. We test this hypothesis using emotion features, psychological themes, and semantic path features, since they were documented to be associated with narrative success in prior literature.\nFor each outcome variable (vote-to-read rate, comment-to-read rate, or continue-to-read rate), we compare the explanatory power of a baseline model containing only basic controls (log word count and chapter fixed effects) to a model that adds features extracted from the actual story text. This comparison reveals which feature extraction methods provide significant value in explaining engagement.\nThen, we evaluate whether applying these same feature engineering techniques to imagined stories provides additional explanatory power. Specifically, we test whether further improvements can be achieved by incorporating three belief-based measures: the expected values of these features across imagined stories, their variance (capturing uncertainty), and surprise (measured as the squared difference in expectations between consecutive chapters). This approach allows us to assess whether modeling reader beliefs complements existing feature extraction methods to understand engagement."}, {"title": "Regression Results", "content": "Further, we can dive into the regression coefficients to gain some insight into how the features relate to engagement. As an example, we show the emotion features in Table 4. We observe that higher arousal past chapters and higher arousal focal chapters correspond to greater engagement. But while readers are more likely to continue consuming more negative content, they are more likely to comment on more positive content. Commenting and sharing may share similar motivations in that they are both outward looking and we find the observed patterns consistent with the conclusions in Berger and Milkman (2012), who find that positive content and high-arousal content are more viral than negative content and low-arousal content, respectively.\nSurprise on the valence dimension is associated with increased engagement. This observation is consistent with the finding of Knight et al. (2024) that more narrative reversals (switches between positive and negative valence) correspond to greater content liking. Readers are also more likely to engage when the expectation of the rest of the story is more negative. Interestingly, the expectations on valence appear to play a larger role than the valence of the actual text in predicting engagement.\nThese findings suggest actionable implications for content creators and digital platforms aiming to optimize user engagement. Notably, the distinction between types of engagement continued consumption versus active engagement like commenting and voting - points to an opportunity for differentiated engagement strategies. For example, if comments help to generate a sense of community and this is important for"}, {"title": "Limitations", "content": "Our method has several limitations that warrant discussion and present opportunities for future research. The primary challenge lies in interpreting approximated beliefs. Our method generates a distribution of potential story continuations to model audience expectations, but these may not perfectly align with actual audience perceptions. For instance, in a set of 100 generated continuations for a crime thriller, we might observe an overrepresentation of certain tropes that real audiences would not necessarily anticipate. This misalignment could lead to biased predictions in certain genres or for specific narrative structures.\nThis methodological challenge parallels the broader literature in economics on inferred or modeled expectations, where researchers approximate subjective beliefs without direct observation. For example, rational expectations models assume that consumers' forecasts align with objective distributions an assumption that may not perfectly mirror reality, yet enables tractable empirical and theoretical analysis. In a similar vein, our approach uses a simplified, implementable model of how readers form expectations about stories.\nWhile actual reader imagination processes may be more complex or different than what our LLM gener-"}, {"title": "Conclusion", "content": "This paper introduces a framework for modeling audience expectations in stories using large language models to simulate story continuations. Our approach quantifies these concepts in unstructured narrative data, offering insights for understanding user engagement.\nAt the core of our methodology is a process that transforms narrative content into features representing audience expectations, uncertainty, and surprise. This approach bridges the gap between qualitative narrative analysis and quantitative modeling. To demonstrate its effectiveness, we applied our method to a dataset of over 30,000 book chapters from Wattpad. Our method complements existing feature engineering techniques by providing a framework to extend their application to reader beliefs, amplifying their marginal explanatory value by 31%. Through careful analysis of the regression results, we can uncover key narrative elements that correspond to audience engagement. These findings contribute to a deeper understanding of the relationship between narrative structure and audience response and generate hypotheses that can be further explored.\nIn conclusion, our framework advances our ability to model audience expectations for narrative content. By quantifying consumers' forward-looking beliefs, we provide a valuable tool for marketers, content creators,"}, {"title": "Appendix: Wattpad Data Collection", "content": "Our Wattpad dataset is collected through scraping content based on specific search keywords related to the genres listed on Wattpad's homepage. The genres listed are: Action, Adventure, ChickLit, Classics, Fanfiction, Fantasy, General Fiction, Historical Fiction, Horror, Humor, Mystery, Non-Fiction, Paranormal, Poetry, Random, Romance, Science Fiction, Short Story, Spiritual, Teen Fiction, Thriller, Vampire, Werewolf.\nWe use ChatGPT to expand the genre list to include a total of 93 keywords: Romance, Fantasy, Science Fiction, Mystery, Thriller, Adventure, Young Adult, Drama, Horror, Historical Fiction, Paranormal, Fanfiction, Dystopian, Comedy, Action, Supernatural, Chick Lit, LGBTQ+, Teen Fiction, New Adult, Short Story, Urban Fantasy, Fairy Tales, Mythology, Detective, Crime, Dark Fantasy, Steampunk, Time Travel, Romantic Suspense, High School, College, Slice of Life, Coming of Age, Magic, Epic Fantasy, Post-Apocalyptic, Cyberpunk, Space Opera, Alien, Zombies, Vampires, Werewolves, Shifters, Demons, Angels, Ghosts, Witches, Wizards, Fairies, Mermaids, Dragons, Alternate History, Military, Political, Psychological, Legal, Medical, Sports, Music, Road Trip, Family, Friendship, Billionaire, Mafia, Royalty, Historical Romance, Western, Inspirational, Spiritual, Self-Help, Memoir, Biography, True Crime, Detective Fiction, Hard-Boiled, Noir, Police Procedural, Legal Thriller, Techno-Thriller, Espionage, Political Thriller, Medical Thriller, Apocalyptic, Humor, Satire, Parody, Tragedy, Love Triangle, Forbidden Love, Soulmates, Enemies to Lovers, Friends to Lovers, Second Chances, Secret Identity, Amnesia, Revenge, Survival, Quest, Heist, Conspiracy, Cult, Secret Society, Virtual Reality, Artificial Intelligence.\nUsing these 93 keywords, we collect 144,642 chapters across 5,847 books that were published January 2022 and onward."}, {"title": "Appendix: Data Cleaning", "content": "We clean the stories at the text-level, the chapter-level, and the book-level."}, {"title": "Cleaning the Text", "content": "We remove non-story related text. Specifically, we do the following:\n1. Remove Special Notes: Sometimes writers directly address readers through notes at the beginning or end of the chapter. We remove them as they are not part of the narrative.\n2. Remove HTML Tags and Attributes: We remove HTML tags and attributes since they are not part of the narrative."}, {"title": "Cleaning the Chapters", "content": "After cleaning the text, we remove chapters that are unrelated to the narrative (e.g., preludes, suggested music playlists), conflict with GPT's text generation policies, or were rewritten, which restarts the read count and distorts the continue-to-read rate.\n1. Remove Irrelevant Chapters: We remove chapters that are not part of the narrative (e.g., author's notes, playlist suggestions, lists of characters) using several strategies. First, we remove chapters that are titled \"introduction,\u201d \u201cprologue,\" or \"character\" or contain the keywords \"author\" or \"extra.\" Second, we remove chapters that have a word count less than 2 standard deviations from the average word count of the book since these chapters are unlikely to be part of the narrative. Third, we prompt GPT to provide a score predicting whether the chapter is part of the story or not. In addition to the chapter text, we provide to GPT the chapter title and URL because they can also be informative. We remove chapters that return a probability score less than 0.25, a threshold based on manual inspection of the chapter text. We use the following prompt to determine whether a chapter is part of the story:\n\"Please classify the following text and provide a probability score between 0 and 1. The score should be close to 1 if the text is a book chapter, and closer to 0 if the text is a prelude, author's note, Spotify"}, {"title": "Cleaning the Books", "content": "Finally, the following steps outline which books are removed from the dataset:\n1. Remove Non-English Books: We focus on books written in English because the features we derive from the author-written and GPT-imagined stories require English text. We calculate the percentage of words which are in English for each chapter. We then characterize each book by the average percentage of English words across all chapters. Books that have an average less than 73% are dropped from the dataset. In addition, we drop chapters that are less than 50% English.\n2. Remove Mature Books: In addition to dropping explicit chapters, we drop books that are tagged as \"mature\" by the author on Wattpad and books that contain explicit content tags (e.g., \"adultro-mance\"). We also remove books that have many chapters flagged as explicit since this would lead to large gaps in the narrative. Specifically, we drop books that have more than 15% of its chapters removed due to being explicit content.\n3. Remove Books Unlikely to Have a Narrative: Books with low average word counts tend to lack a narrative structure. For example, several books are just collections of memes. We remove books with an average word count per chapter of less than 400.\n4. Remove Books with High Word Count: We remove books with an average word count per chapter greater than 10,000 because GPT is limited in the length of its input and because often these chapters are not chapters but instead entire books. For the same reason, we also remove chapters with more than 10,000 words.\n5. Remove Books with More Than 50 Chapters: We remove books with a high number of chapters so that books with more chapters do not receive greater weight in our subsequent analysis.\n6. Remove Books with First Chapter Read Count Less Than 100: We remove books with a low read count for the first chapter."}]}