{"title": "The Nature of Mathematical Modeling and Probabilistic Optimization Engineering in Generative AI", "authors": ["Fulu Li"], "abstract": "In this paper, we give an in-depth analysis on the mathematical problem formulations and the probabilistic optimization explorations for some of the key components in Transformer model [33] in the field of generative AI. We explore and discuss some potential further enhancement for current state of the art methods for some key underlying technologies of generative Al models from algorithmic and probabilistic optimization perspective. In particular, we present an optimal solution for sub-word encoding (SWE) based on similar initial settings as that of byte-pair encoding (BPE) algorithm in [9] with similar objectives as that of WordPiece approach in [28, 31] to maximize the likelihood of the training data. We also present cross entropy optimization method to optimize hyperparameters for word2vec model [17]. In addition, we propose a factored combination of rotary positional encoding (RoPE) [32] and attention with linear biases (ALiBi) [23] with a harmonic series. We also present a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a probability distribution over block distances in the matrix to decide which block is likely to participate in a given round of attention computation while maintaining the lower triangle shape of the tensor for autoregressive language models by re-shaping the tensors. Finally, we present staircase adaptive quantization (SAQ) of key-value (KV) cache for multi-query attention (MQA) based on the framework presented in [16] to have gradual quantization degradation while achieving reasonable model quality and cost savings.", "sections": [{"title": "1. Introduction", "content": "In the year of 1953, Albert Einstein, arguably the most notable scientific giant in the 20th century, once commented on the development of Western Science and he concluded that \u201cthe development of Western Science in based on two great achievements: the invention of the formal logical system by Greek philosophers, and the discovery of the possibility to find out causal relationships by systematic experiments (Renaissance)"}, {"title": "2. Related Notions Behind Some Engineering Practices", "content": "In probability theory, the law of large numbers states that the average of the results obtained from a large number of independent random samples converges to the true value. As mentioned earlier, in the training of deep neural networks during the maximum likelihood estimation of those parameters, i.e., the weights and bias of those neural nodes in the deep neural network, stochastic gradient descent method is used in the iterative optimization process. According to the law of large numbers, the gradients computed from these mini-batches of training data set are expected to fluctuate around the true gradient for the whole training data set. Therefore, the mini-batch gradient on average is expected to indicate an adequate direction for changing the parameters during stochastic gradient descent optimization process [5, 12, 13, 21].\nIn probability theory and statistics, variance is the expected value of the squared deviation from the mean of a random variable. Layer normalization in the Transformer model [33] is used for regularization, where the mean and variance of the parameters in each layer are computed in order to make sure that the resulting outputs of the given layer have a well-behaved distribution, with the value of expectation/mean at 0.0 and the value of variance at 1.0 [21].\nDepending on the real application scenarios, most of the generative pre-trained transformer (GPT) models adopted decoder-only Transformer model [19], where the query matrix of Q and the key matrix of K are all of full rank due to the autoregressive nature of contiguous lower triangle shape along the diagonal in the corresponding matrix during attention computation. A matrix is of full rank if its rank is the same as its smaller dimension. Notably, a matrix of full rank can carry more information that a matrix of the same size with lower rank, where some rows or columns in the matrix are not of linear independence. That is probably"}, {"title": "3. Mathematical Modeling of LLM for Generative AI", "content": "In the following, we discuss the probabilistic optimization formulations for autoregressive language model that is widely used in GPT-like models (generative pre-trained transformer) [19], pre-training of the foundation model [21], fine-tuning of the model with additional domain knowledge of (question, answer) pairs [25], reinforcement learning with human feedback (RLHF) [20, 27], direct preference optimization (DPO) [24] and identity preference optimization (IPO) [1] for alignment in generative AI.\nMathematically, autoregressive language model (ALM) generating a sequence of text by predicting next token given previous tokens is based on conditional probability, where the objective of the model is to estimate the joint probability of the given sequence of tokens. According to the chain rule of probability, we have:\n$$Pr(s) = Pr(s_1, s_2, ..., s_T) = \\prod_{t=1}^{T} Pr(s_t | s_1, s_2, ..., s_{t-1})$$\nwhere the token sequence of s is given by s = (s_1, s_2, ..., s_T), T is the length of the token sequence, the model predicts the probability of each token of s, based on all the previous tokens of s_1, s_2, . . . s_{t-1} in the sequence [21, 34].\nThe pre-training process of LLM with deep neural networks such as the Transformer model [33] is to minimize the cross-entropy loss of the training data set, i.e., the sum of the negative log-likelihood of of true tokens at each position in the sequence of the training data set, and we have the probabilistic optimization objective as follows:\n$$min_{\\theta}( - \\sum_{t=1}^{T} log Pr(s_t | s_1, s_2, ..., s_{t-1}; \\theta))$$\nwhere \\theta is the model parameters, Pr() is the probability distribution over the vocabulary [3, 34], the function of softmax() in the Transformer mode converts a vector of numbers into a vector of probabilities, where the probability of Pr(s_t | s_1, s_2, . . ., s_{t-1}) is the probability of the true label token index at position t given all preceding tokens of s_1, s_2, . . ., s_{t-1}.\nOn the other hand, after the pre-training of the foundation model, the model is further fine-tuned with additional domain knowledge of (question, answer) pairs, the probabilistic optimization objective of fine-tuning is to maximize the log-likelihood of the correct answers of the additional training data set as follows:\n$$max_{\\theta}(\\sum_{i=1}^{N}log Pr(A_i | Q_i; \\theta))$$\nwhere \\theta is the model parameters, Q_i and A_i is the ith question and answer pair respectively [25], N is the number of total number of (question, answer) pairs, Pr(A_i|Q_i) is the probability of the correct answer of A_i given question of Q_i.\nAfter the fine-tuning of the model with domain knowledge of (question, answer) pairs, reinforcement learning with human feedback (RLHF) is often used to further improve the model's instruction-following capabilities [20]. The basic idea is to construct a reward model of R(x) to estimate the quality of the model's outputs based on pair-wise of (prompt, completion) training data. The policy/language model is optimized based on methods such as proximal policy optimization (PPO) [27] and the probabilistic optimization objective is to minimize the cross entropy loss, which incentivizes it to make predictions that are closer to actual human ratings/feedbacks:\n$$min_{\\Pi_{\\rho}}(-E[R(Q, A) \u2013 \\beta \\times KL(\\Pi_{\\rho}(A|Q) || \\Pi_{\\theta_{old}}(A|Q))]])$$\nwhere E[x] indicates the expectation value of x in probability theory, i.e., the first moment, Q indicates the prompt and A denotes the completion, \\beta is a hyperparameter that controls the strength of the Kullback-Leibler (KL) divergence penalty and \\beta > 0, \\Pi_{\\rho} is the current policy/language model, \\Pi_{\\theta_{old}} is the old policy/language model.\nKL divergence is also called relative entropy and mathematically it is defined as:\n$$KL(W || Z) = \\sum_{x \\in X} (W(x) \\times log(\\frac{W(x)}{Z(x)}))$$\nThe combination of RLHF and PPO has led to great success in practice such as InstructGPT and GPT4 [1]. However, as pointed out in [8] that RLHF is often slow and quite unstable in practice, in particular in a distributed learning environment. Direct preference optimization (DPO) [24] and its variants such as identity preference optimization (IPO) [1] are becoming popular alternative solutions without the rewarding stage, while performing reinforcement learning (RL) to learn the policy with a single maximum likelihood objective [24].\nThe loss that DPO is trying to optimize, given an empirical data set of D, as a function of \\Pi_{\\theta}, i.e., the policy/language model to optimize, is given by [1]:\n$$min(-E_{(x, y_w, y_l) \\sim D}[-log(\\sigma (\\tau log(\\frac{\\Pi_{\\theta}(y_w|x)}{\\Pi_{ref}(y_w|x)}) \u2013 \\tau log(\\frac{\\Pi_{\\theta}(y_l|x)}{\\Pi_{ref}(y_l|x)}))))]$$\nwhere \\Pi_{ref} denotes some reference policy, \\tau is a hyperparameter that controls the strength of the Kullback-Leibler (KL) divergence penalty and \\tau > 0, \\sigma(.) denotes the sigmoid function and plays the role of normalization and the data set of D is given by:\n$$D = {(x_i, y_{w,i} > y_{l,i})}_{i=1}^{N}$$\nwhere x_i is a given prompt, y_{w,i} and y_{l,i} are two completions given the prompt of x_i, y_{w,i} > y_{l,i} indicates the preference of y_{w,i} over y_{l,i}\nThe key findings in [24, 1] is that when the Bradley-Terry model that represents the preference function as a sigmoid of the difference of rewards perfectly fits the preference data and the optimal reward policy is obtained from preference optimization loss function, then the optimization of RLHF objective in Equation (5) perfectly coincides with the optimization of DPO objective in Equation (7)."}, {"title": "3.1. The Transformer Model", "content": "As discussed in [33], a Transformer model typically has the following major components: (a) a tokenizer that converts a sequence of text into a sequence of tokens; (b) an embedding layer that converts tokens and positions of tokens into vector/tensor representations; (c) transformer layers that conduct repeated transformations on vector representations via deep learning process, extracting more and more language semantics information, which typically consists of multi-head attention (MHA) layer and feedforward network (FFN) layer; (d) LayerNormalization block that computes the mean and variance of the parameters at each layer such that the outputs of the given layer has a well-behaved distribution with a mean of zero and a variance of 1.0; (e) An un-embedding layer that converts the vector representation back to the probability distribution over the tokens with a softmax function; (f) Residual connections that bypass one or more layers of neural network computations to improve the flow of gradients during back propagation in order to facilitate deeper neural networks. The identity shortcuts of residual connections essentially skip blocks of layers to preserve features of the propagated signal; (g) Next token prediction based on some effective algorithms such as top-p algorithm or top-k algorithm, etc.\nIn the following, we present some enhancements for some of the state of the art methods for some of the key components in the implementation of the Transformer model [33]."}, {"title": "3.1.1. Subword Encoding to Maximize the Likelihood of of the Training Data", "content": "For the tokenizer, byte pair encoding (BPE) algorithm [9] is widely used for sub-word encoding to deal with rare and unseen word issues. The basic idea for BPE algorithm is to initialize each word unit/token with one character in the text and greedily merge the adjacent pair with the highest frequency until the number of remaining tokens/words reaches a given size. However, by only focusing on merged-token frequencies when merging two adjacent pair with the highest frequency may lead to reduced total number of appearances for each word vocabulary item in the training data. Essentially, it is a tradeoff between average word length and the total number of words appearance in the training data while avoiding out-of-vocabulary issues and capturing linguistic meanings as much as possible.\nAnother widely-used sub-word encoding approach named WordPiece [28] algorithm starts by initializing all Unicode characters of the collection for a given language as tokens. Then it combines two tokens (not necessarily adjacent) in such a way that the likelihood of the training data is maximized [28]. Song et al presented a fast WordPiece algorithm, whose computational complexity is linear with respect to the input length [31]. Notably, both of BPE [9] and WordPiece [28, 31] methods efficiently addressed the open-ended vocabulary problem by allowing a word to be represented as a sequence of characters if necessary, splitting a word into multiple sub-words. Essentially, subword encoding approaches effectively interpolate between word level inputs for frequent words and character level inputs for rare words [21].\nThe key differences between BPE [9] and WordPiece [28] are two-folds: (a) the initial settings are different in that BPE initializes each word unit/token with one character in the given input text to learn the word vocabulary while WordPiece initializes each word unit/token with one character among all basic Unicode characters for a given language; (b) the criteria used to merge two word units/tokens are different in that BPE merges two adjacent word units/tokens with the highest frequency in the given learning text while WordPiece combines two of the word units/tokens (not necessarily adjacent) that maximizes the likelihood of the training data."}, {"title": "3.1.2. Optimization of Hyperparameters for Word2vec Approach", "content": "Word2vec [17] is widely used as the state of the art model for obtaining vector representations of words, where the objective for these vectors is to capture information about the meaning of the word based on the surrounding words as much as possible such that those vectors can be useful for predicting the surrounding words in a sentence or a document. As stated in [17], more formally, the objective of the skip-gram model is to maximize the average log probability as follows:\n$$\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c<j<c, j\\neq0} log(Pr(w_{t+j}|w_t))$$\nwhere T is the sequence length of a word sequence of w_1, w_2, w_3, ..., w_T, c is the size of the training context window. For skip-gram model, it is trying to predict surrounding words with a window of radius of c for every word in the window. Continuous bag of words (CBOW) and skip-gram are two of the most popular frameworks used in word2vec model to get word embeddings [17]. For continuous bag of words (CBOW) model, it is trying to predict the middle word with the input of its surrounding words within a given window. The basic idea is that word vectors are positioned in the vector space in such a way that words that share some common contexts, i.e., close to each other, in the text corpus are located close to one another in the vector space. In the following discussions, we only focus on the skip-gram model (Equation (17)) but the principle can be equally applicable to continuous bag of words (CBOW) model as well.\nAs pointed out in [14] that much of the superior performance of word2vec [17] in downstream tasks, compared with other similar approaches, is mainly due to the choice of specific hyperparameters such as the size of the training context window, i.e., the hyperparameter of c, the dimensionality of the vector, i.e., the hyperparameter of d, the word frequency threshold for sub-sampling of high-frequency words, i.e., the hyperparameter of sf, the word count threshold to be considered as part of the training vocabulary for low-frequency word, i.e., the hyperparameter of mc.\nExisting hyperparameter optimization methods include: (a) grid search, i.e., essentially an exhaustive search over a given hyperparameter space; (b) random search, which can explore much more potential values than grid search for continuous hyperparameters; (c) Bayesian optimization, which employs a probabilistic model between hyperparameter values and objectives evaluated on a given data set and it has shown to outperform both grid search and random search with fewer evaluations; (d) gradient-based optimization, which computes the gradients with respect to hyperparameters when applicable for some learning algorithms and then optimizes the hyperparameters based on gradient decent philosophy; (e) evolutionary optimization, which employs an evolutionary algorithm to find the best possible hyperparameters for a given learning algorithm; (f) population-based methods, which updates the hyperparameters as well as the weights of neural networks during the training of the models with multiple independent learning processes with different hyperparameteres of the learning algorithm; (g) early stopping-based approach, which starts as a random"}, {"title": "3.1.3. Rotary Positional Embedding (RoPE) and Attention with Linear Biases (ALiBi)", "content": "As of today, rotary positional embedding approach (RoPE) presented in [32] is probably the most widely used positional embedding method for Transformer models in generative AI applications, in particular after LlaMA2 adopted this positional embedding approach. The RoPE approach works well in most scenarios but it may have some challenges for input length extrapolation, where sometimes the sequence length for inference is longer than the maximum training sequence length, in particular when the training data set is not large enough. Attention with Linear Biases (ALiBi) method presented in [23] does not add positional embeddings to word embeddings but it adds biases to query-key attention scores with a penalty that is proportional to their distance, which facilitates the extrapolation performance, in particular when the training data set is not large enough and the inductive biases have some significant impact on extrapolation during inference.\nRotary positional embedding method [32] uses Euler's formula in complex analysis to transform the addition operation of the static position encoding proposed in the original Transformer framework [33] into a multiplication operation, where the related parameters naturally become part of the learning process.\nNotably, both rotary positional embedding approach of RoPE and ALiBi method incorporate relative position information, where relative position information is used in RoPE for positional embedding calculation and the relative distance information is used in ALiBi for added linear biases for the query-key attention score.\nIn the following, we propose a factored combination of ALiBi and RoPE for Transformer-based generative AI applications in order to obtain the benefits of extrapolation by ALiBi as well as the benefits of RoPE in other application scenarios.\nAs mentioned earlier, the method of ALiBi [23] does not change the token embeddings, it can be used along with RoPE [32] with some twists due to the fact that a simple combination of ALiBi and RoPE may lead to the signal from relative position information being too strong as both ALiBi and RoPE uses relative position information in their respective approach. In [23], ALiBi uses a geometric series decaying factor as head-"}, {"title": "4. Pre-Training and Post-Training of LLM", "content": "In this section, we focus on some techniques to speed up the pre-training of foundation models and to accelerate the inference process after pre-training, in particular on attention computation."}, {"title": "4.1. Probabilistic FlashAttetnion", "content": "Attention computation is arguably the most critical component of the Transformer model [33]. Due to the quadratic nature of the attention computation complexity, a lot of efforts have been made to speed up the attention computation such as the efforts by OpenAI team in [4]. One widely-used attention computation approach is called FlashAttention in [6, 7]. The essence of FlashAttention method for attention computation in Transformer model for large language model (LLM) is the application of tiling by splitting a large matrix into tiles to have a finer granularity to deal with I/O with differentiated memory (HBM, SRAM, etc.) constraint hierarchy in GPU.\nFlashAttention [6, 7] is of exact attention computation in its current form. As discussed in [22], fast causal attention computation for sparse FlashAttention can be more efficient. We considers attention computation in Transformer model for LLM in a probabilistic way. The presented probability density function (PDF) with respect to the block/tile distance in the matrix follows a constrained harmonic deduction philosophy. The presented PrFlashAttention dynamically and probabilistically skips less-related rows/columns in Query/Key (Q/K) matrix along a tensor dimension, say the number of Head dimension of H, in the tensor shape of (Batch, Head, Context Length, Head Dimension) during attention computation while supporting causal masks for auto-regressive models by reshaping the tensors."}, {"title": "4.2. Adaptive Quantization of KV Cache for Multi-Query Attention", "content": "Multi-query attention (MQA) has been proposed mostly to speed up the inference process. The basic idea is to keep the original number of heads for query matrix of Q in multi-head attention (MHA) but have only one head for key matrix of K and value matrix of V, which means all the Q heads share the same set of K and V heads, where computed keys and values vectors are cached, without the re-computation of the same key and value vectors at each attention block. In general, Multi-query attention (MQA) with key-value (KV) cache has a neutral effect on model quality and training speed, but can greatly speed up the inference.\nIn the following, we present Staircase Adaptive Quantization (SAQ) for key-value (KV) cache in multi-query attention (MQA) to further alleviate the problem of KV cache based on the framework in [16] by means of gradual quantization degradation to speed up the inference while achieving reasonable model performance. For a B-bit integer, the quantization and de-quantization process can be expressed as follows [16]:\n$$Q(X) = [\\frac{X-z_X}{s_X}]$$\n$$X' = Q(X). s_X + z_X$$\nwhere Q(X) indicates the quantized tensor of X, X' is the de-quantized tensor of Q(X), z_X = min X is the zero-point and s_X = (max X \u2013 min X)/(2^B \u2013 1) is the scaling factor and the symbol of [.] is the rounding operation.\nNotably, key and value cache of newly generated tokens arrive sequentially in time. Following similar settings in [16], during the pre-fill phase, exact (full precision) key and value tensors are passed to the next layers, even though only the quantized KV cache is retained in memory to reduce the memory footprint and to prevent some re-computations in the decoding phase.\nWe assume that we have I_{prompt} number of key tokens and I_{prompt} number of value tokens in the pre-fill stage. We also assume that a full precision is expressed as 16-bit quantization such as fp16 (float point 16) that is commonly used in the implementation of tensors, so we can have lower quantization choices such as 8-bit quantization, 4-bit quantization, 2-bit quantization, etc. Let q_n be the number of quantization choices and B_i indicate the number of bits for the ith quantization choices and B_1 represents the number of quantization bits of the full precision. Since I_{prompt} can be of arbitrary length, in the pre-fill stage we split the sequence of I_{prompt} tokens into q_n segments, i.e., S_1, S_2, ..., S_{q_n}, each of which corresponds to a different quantization level, with the segment of S_1 corresponds to the full precision. For the sake of simplicity, we assume that all of the segments, i.e., S_1, S_2, ..., S_{q_n}, are of equal sizes, say segment size of S. Please note that the size of the segment of S_{q_n}, which corresponds to the one with the lowest quantization level, i.e. 2-bit quantization or 1-bit quantization, could be open-ended as the token sequence grows longer and longer unless it is truncated due to the constraint of cache memory. From the token sequence perspective, the quantization level downgrades by half in terms of quantization bits every S tokes, which looks like a staircase. We present the algorithm for Staircase Adaptive Quantization (SAQ) to have gradual quantization degradation for KV cache in both pre-fill stage and decoding stage based on the framework in [16] for multi-query attention (MQA) to speed up the inference. The details of the presented algorithm for Staircase Adaptive Quantization (SAQ) of KV cache for multi-query attention (MQA) in the phase of pre-fill and decoding are described in the Appendix A."}, {"title": "5. Summary and Future Directions", "content": "With rapid progress in the field of generative AI, it is often a good idea to reflect on some of the fundamental mathematical modeling and probabilistic optimization tools that powered this AI revolution so that we can make further enhancement systematically down the road.\nIn this paper, we give an in-depth analysis on the mathematical problem formulations and the probabilistic optimization explorations for some of the key components in Transformer model [33] in the field of generative AI. We explore and discuss some potential further enhancement for current state of the art methods for some key underlying technologies of generative Al models from algorithmic and probabilistic optimization perspective. In particular, we present an optimal solution for sub-word encoding (SWE) based on similar initial settings as that of byte-pair encoding (BPE) algorithm in [9] with similar objectives as that of WordPiece approach in [28, 31] to maximize the likelihood of the training data. We also present cross entropy optimization method to optimize hyperparameters for word2vec model in [17]. In addition, we propose a factored combination of rotary positional encoding (RoPE) [32] and attention with linear biases (ALiBi) [23] with a harmonic series. We also present a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a probability distribution over block distances in the matrix to decide which block is likely to participate in a given round of attention computation while maintaining the lower triangle shape of the tensor for autoregressive language models by re-shaping the tensors. Finally, we present staircase adaptive quantization (SAQ) of key-value (KV) cache for multi-query attention (MQA) based on the framework"}]}