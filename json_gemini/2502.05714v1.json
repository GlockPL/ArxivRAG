{"title": "Proving the Coding Interview: A Benchmark for Formally Verified Code Generation", "authors": ["Quinn Dougherty", "Ronak Mehta"], "abstract": "We introduce the Formally Verified Automated Programming Progress Standards, or FVAPPS, a benchmark of 4715 samples for writing programs and proving their correctness, the largest formal verification benchmark, including 1083 curated and quality controlled samples. Previously, APPS provided a benchmark and dataset for programming puzzles to be completed in Python and checked against unit tests, of the kind seen in technical assessments in the software engineering industry. Building upon recent approaches for benchmarks in interactive theorem proving, we generalize the unit tests to Lean 4 theorems given without proof (i.e., using Lean's \"sorry\" keyword). On the 406 theorems of 100 randomly selected samples, Sonnet correctly proves 30% and Gemini correctly proves 18%. We challenge the machine learning and program synthesis communities to solve both each general purpose programming problem and its associated correctness specifications. The benchmark is available at https://huggingface.co/datasets/quinn-dougherty/fvapps.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have developed incredible capabilities, spanning creative writing, tutoring and education, information retrieval and distillation, common sense reasoning, complex mathematical reasoning, and even code generation [Chi+24; Ope+24; Dub+24; Dee+24]. Particular focus and attention has been given towards LLM-assisted software development, given its economic value and position between natural language and formal logic.\nWe have seen rapid advancements in code generation capabilities [Sin+23; Jim+23; Hen+21]. With this has come strong concerns around the misuse and safety of code generated by these models, and the ability to prove or guarantee that the produced code is sound. An approach commonly employed today includes running LLM-generated code in restricted development environments, and ensuring tight feedback loops with software engineers who understand the unique failure modes of their particular complex software. This paired programming-style approach is common in LLM chatbot interfaces, as well as programming assistants more strongly integrated in the software engineering development cycle such as GitHub Copilot [Che+21] and Cursor (Anysphere Inc., 2023).\nHowever, recent paradigm shifts in the ability to develop and enhance capabilities through supervised fine-tuning has led to fully autonomous software engineering, which includes code generation and execution in the development loop (e.g., Devin, Cognition AI, Inc. 2023, [Wan+24]). Without a human evaluating code before it is run, strong assumptions or restrictions are necessary to ensure that generated code is safe and correct. Critically, as these tools become ubiquitous across more and more applications that touch the real world, it will be a requirement that we have guarantees of their safety and correctness. Code generated for managing medical devices, for managing patient records, for ensuring the security of private personal information, for controlling autonomous vehicles, for cybersecurity defense systems, and for military use will all require strong guarantees before users, organizations, and governments can make effective use of these tools.\nWhile software quality assurance primarily relies on extensive testing, the emerging field of deductive verification offers a more rigorous alternative by mathematically proving code correctness against formal specifications. However, this approach requires deep system knowledge, explicit property definitions, and often demands rewriting existing code in specialized proof assistant languages-a significant challenge given the current state of tools. As Large Language Models (LLMs) advance in code generation capabilities, they may eventually enable automated creation of deductively verifiable code, allowing us to defer trust to theorem provers rather than testing alone. This could revolutionize mission-critical software development by providing formal guarantees against bugs and hallucinations, though current LLM capabilities fall short of this goal, and it remains unclear whether fine-tuning or prompt engineering can bridge this gap.\nContributions. We deliver a set of 4,715 Lean 4 files consisting of a solution function signature and theorem statements, all of which are passed on for future proving via the \u201csorry\" keyword, allowing the lean executable to compile without actual proofs or implementations. We provide tags for samples that pass additional, rigorous quality assurance, defined by inline unit testing and property tests in Lean."}, {"title": "II. THE FVAPPS BENCHMARK", "content": "A strong benchmark for evaluating model capabilities for theorem proving should have a number of desirable properties. With a particular focus on software engineering, algorithms and properties of those algorithms should well represent tasks and solutions that are used to evaluate skill in the labor force today. Coding interview questions fit this requirement naturally. With generic coding capabilities well-evaluated, we can build upon existing benchmarks of coding ability to generate our formally-verified one.\nWe begin with APPS, a benchmark and dataset for programming puzzle solving [Hen+21]. Developed largely by scraping the interview preparation platforms CodeForces and LeetCode, each sample consists of a natural language question, a ternary flag marking difficulty, a list of correct solutions in Python, and a few input-output test case pairs."}, {"title": "A. An Example Task", "content": "As an example, consider the crooked election problem:\nThere are n voters, and two ways to convince each of them to vote for you. The first way to convince the $i$-th voter is to pay him $p_i$ coins. The second way is to make $m_i$ other voters vote for you, and the $i$-th voter will vote for free. Moreover, the process of such voting takes place in several steps. For example, if there are five voters with $m_1 = 1, m_2 = 2, m_3 = 2, m_4 = 4, M_5 = 5$, then you can buy the vote of the fifth voter, and eventually everyone will vote for you. Set of people voting for you will change as follows: $5 \\rightarrow 1,5 \\rightarrow 1, 2, 3, 5 \\rightarrow 1, 2, 3, 4, 5$.\nCalculate the minimum number of coins you have to spend so that everyone votes for you.\nTo get everyone to vote for you, you can either purchase votes for each voter's declared price, or convert each voter through their declared groupthink threshold. This is the setup of sample 0023, derived from APPS problem train/23. Figure 2 is our pipeline's output that we ship in our dataset. Crucially, it is one def and multiple theorems, though many samples include extra definitions that are helper functions either for the solution or some theorem statement. The def is where the problem solution is implemented as a first-order requirement to proving theorems about its correctness.\nWhile a number of theorem provers now exist, we choose Lean because of its explosive growth in mathematics 2 and its fertile potential for general-purpose programming [Chr23]. Lean is capable of expressing arbitrary programs, taking advantage of the identity monad and the partial keyword."}, {"title": "B. Generation Pipeline", "content": "We use Anthropic's Claude Sonnet 3.53 to power five core parts of our benchmark creation pipeline. In stages one through three of our pipeline, whenever a non-deductive LLM call is used for processing, we allow for a maximum of 5 attempts before \"failing\" on that sample. Stage four allows for a maximum of ten, and stage five has no LLM call. These loops include simple instructions that pass relevant error messages back to the model and ask for an updated response. Given that subsets of our pipeline fall withing the scope of what is capable with current LLMs, we construct scaffolding loops for iterating on model-based solutions. Broadly, we use outputs from python, pytest, lean, and lake build to ensure the generated code is correct, and to collect standard error and outputs to send back to the model when incorrect (Figure 3).\nWe first preprocess the original APPS dataset, aggregating unit tests and solutions into one file with a particular function definition and architecture that better lends itself to automated unit testing. This step also selects a single solution from the set of solutions provided by the original APPS dataset.\nProperty-based tests are procedurally generated unit tests, up to boundary conditions given on input types [MHC19]. We use our scaffolding loop outlined above, specifically feeding pytest's standard error and standard out messages back into the next API call. When pytest returns an empty standard error and an exit code of zero, we write the final file of tests to disk and exit the loop."}, {"title": "III. BASELINE EXPERIMENTS", "content": "Our baseline task is to implement and prove each invocation of the word \"sorry\" in the benchmark's source specification. We provide preliminary evaluations here, using a loop similar to the generation pipeline. We attempt to solve 101 randomly sampled FVAPPS problems with both Claude 3.5 Sonnet (new) and Gemini 1.5 Pro10. For a direct human comparison, we allocated ten hours for a baseliner to attempt one sample."}, {"title": "A. Model Baseline Details", "content": "We use Lean 4.12.0 throughout our benchmark generation and baseline, pinned with Mathlib available in the environment.11 We attempt to produce a partial baseline for a subset of FVAPPS using claude-3-5-sonnet-20241022 (the same model that generated the task).\nA note on open source models. We spent some time attempting to solve benchmark samples with small and medium-sized open source models. Particularly, we attempted simple, single-shot autocompletion of theorem statements using DeepSeek's DeepSeek-Prover-V1.5-RL12 [Xin+24a], but were unable to find a one-shot or loop-based scaffolding solution that made any sufficient progress. An important part of this task seems to be the ability to iterate, and while models fine-tuned to reduce loss on next token prediction in Lean may eventually prove to be useful, currently their limited capability in broader, less narrow goals is insufficient.\n1) Prompt Details: Because Lean 4 is in active development and interfaces and functions change with reasonable frequency, we found it was critical to give the LLM sufficient context to avoid common pitfalls associated with biases it may have due to outdated training data. We found significantly higher success rates by following a number of key insights by Victor Taelin\u00b9\u00b3, including 1) direct, explicit description of the problem formulation and goal, 2) details of the Lean 4 environment that may be significantly different from its pretrain dataset, and 3) multiple examples of theorem statements and solutions to theorem statements.\n2) Scaffolding Loop (Agent): We initially provide the model with a system prompt consisting of the details in the previous section, and prompt the model first with the original English-language description of the problem and the sorry'd benchmark .lean file (Figure 2), with instructions to implement the sorrys. After the model responds, any code block provided is extracted, written to a Basic.lean file, and run in a custom managed environment with specifically pinned libraries using lake build. The full standard input and output alongside returncode are sent back to the model with the instruction to address the issues.\nInitial attempts were unsuccessful in solving all of the theorem statements at once; to give the model a better chance at solving the problem we show and allow the model to solve one statement at a time, starting with the definitions and then appending each theorem independently, to be proven separately from the rest."}, {"title": "B. Human Baseline", "content": "A human attempted the function and one of the theorems from sample 0023. It took them 10 hours to accomplish the function shown in Figure 5, since they needed termination proofs for two recursors. In the time allotted, they did not make it through any of the proofs. A problem they ran into with the proofs was stack overflow trying to unfold the recursors. Given this, we don't think a full human baseline is a feasible option."}, {"title": "C. Comparing Model Baseline and Human Baseline on One Sample", "content": "On sample 23 regarding the crooked election, both successfully implemented the definition of the function (Appendix C) and succeeded on no nontrivial proofs. We let Sonnet attempt the second theorem for 100 loops, and while it made a valiant effort, it diverged. The human and Sonnet clearly took two very different approaches, but it remains to be seen which approach has better proof ergonomics."}, {"title": "D. Model performance on more samples", "content": "Both Sonnet and Gemini ran for a maximum of 25 loops for the defs and a max of 50 loops for each theorem. Sonnet completed all but seven of the definitions and 43 samples had at least one theorem completed. Gemini completed only 71"}, {"title": "E. Discussion", "content": "We noticed that Sonnet solved many problems via imperative programming with the identity monad (Id.run do). However, this kind of solution fails to capture inductive structure that lends itself to proofs. This means future solutions to FVAPPS will take one of two directions: an unwieldy and uninterpretable proof involving the identity monad, or ensuring that solutions are factored in a way that can exploit inductive structure.\nTable II shows the split over our quality assurance steps over the entire baseline set and those that were solved. While this sample is small, we roughly find that our higher-quality samples that are both guarded and plausible are somewhat more likely to be solvable by current models."}, {"title": "IV. RELATED WORK", "content": "Research and development into stronger programming guarantees spans a number of large and growing communities in the software landscape. While interest in generally stronger-typed programming paradigms is growing (e.g., growing interest in converting the Linux kernel codebase to Rust, 14 increasing availability of static typing in Python15) there remains a smaller group of people and programming languages focused directly on formal verification. Coq [Tea04], Dafny [Lei23], Isabelle [NWP02], and Lean [Mou+15] all target theorem proving and formal verification as first class goals, albeit with varying visions and core foundations.\nAutomating programming with machine learning or program synthesis is a rich area [Ell+20; Aus+21]"}, {"title": "A. Datasets and Benchmarks", "content": "Informal Mathematical theorem proving datasets are the most extensive, with benchmarks ranging from 15,000 to over 100,000 proofs [YD19; Yan+23; Jia+21; Wel+21], plus the Archive of Formal Proofs containing around 1 million lines of code [Bla+15]. Unverified program synthesis benchmarks are typically an order of magnitude smaller, containing thousands of programs, with APPS being the largest at 10,000 programs [Hen+21] (though LiveCodeBench continues to grow [Jai+24]). In stark contrast, formal software verification benchmarks have historically been much more limited in size, with both available datasets (Clover and dafny-synthesis) containing order of 100 programs each [Sun+24; Mis+24], with DafnyBench [Lou+24] up at 782."}, {"title": "B. Models", "content": "Models like DeepSeek [Xin+24b; Xin+24a] and AlphaProof [Alp24] have advanced the ability of AI to do math, both formally and informally. DeepSeek Prover has demonstrated significant capabilities in formal mathematics, achieving state-of-the-art performance on the Lean theorem proving benchmark. The model combines large-scale pretraining on mathematical texts with specialized fine-tuning on formal proof corpora, enabling it to generate valid formal proofs for complex mathematical statements.\nAlphaProof [Alp24] represents another milestone in automated theorem proving, showing particular strength in creative mathematical problem-solving. Its success on IMO-level problems demonstrates the potential for AI systems to engage in sophisticated mathematical reasoning, though the gap between informal mathematical proofs and formal verification remains significant.\nIn the domain of program synthesis and verification, models have evolved along several tracks. Traditional program synthesis approaches using symbolic techniques and SMT solvers have been augmented by neural methods [Wan+23]. Large language models fine-tuned on code, such as CodeLlama [Roz+23] and GPT-4[Ope+24], have shown promising results in generating functionally correct programs, but their capabilities in formal verification remain limited.\nRecent work has begun to bridge this gap [Fou24; Yan+24] demonstrating the potential for AI assistance in formal program verification. However, these systems typically require significant human guidance and cannot yet autonomously generate both correct programs and their formal proofs. The challenge of simultaneously reasoning about program semantics and verification conditions remains an open problem in the field."}, {"title": "V. DISCUSSION", "content": "Our current presentation has a few limitations, and clear directions for future work. While the samples in our final guarded and plausible set are quite high quality, it is nontheless possible that the theorem statements do not correspond to desirable properties nor that they, in union, cover the full range of properties one would ideally prefer. While manual spot-checking did not reveal any systematic failures post complete five-stage processing, it is nontheless possible that particular theorems are not applicable or vacuous in some form.\nThe FVAPPS benchmark evaluates AI systems on both program synthesis and formal verification by extending APPS with Lean 4 theorem specifications. While current language models can implement programming solutions1617, they struggle with formal verification proofs, highlighting the challenge between imperative programming and proof-amenable structures. The benchmark's difficulty is demonstrated by limited progress from both human baselines and models.\nFVAPPS opens research directions in simultaneous implementation and verification reasoning, automated proof-amenable solution structuring, and scaling formal verification, providing a framework to measure progress toward verified automated programming."}]}