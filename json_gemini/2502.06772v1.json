{"title": "ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates", "authors": ["Ling Yang", "Zhaochen Yu", "Bin Cui", "Mengdi Wang"], "abstract": "We present that hierarchical LLM reasoning via scaling thought templates can effectively optimize the reasoning search space and outperform the mathematical reasoning capabilities of powerful LLMs like OpenAI 01-preview and DeepSeek V3. We train our ReasonFlux-32B model with only 8 GPUs and introduces three innovations: (i) a structured and generic thought template library, containing around 500 high-level thought templates capable of generalizing to similar or relevant reasoning problems; (ii) performing hierarchical reinforcement learning on a sequence of thought templates instead of original long CoT data, optimizing a base LLM to plan out an optimal template trajectory for gradually handling complex problems; (iii) a brand new inference scaling system that enables hierarchical LLM reasoning by adaptively scaling thought templates at inference time. With a template trajectory containing sequential thought templates, our ReasonFlux-32B significantly advances math reasoning capabilities to state-of-the-art levels. Notably, on the MATH benchmark, it achieves an accuracy of 91.2% and surpasses ol-preview by 6.7%. On the USA Math Olympiad (AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems, surpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have recently achieved remarkable progress, demonstrating exceptional capabilities in tackling complex reasoning tasks and even surpassing human experts in specific domains. For example, models such as OpenAI's O1 (Jaech et al., 2024), Google's Gemini-2.0 (Team et al., 2024), DeepSeek-V3 (Liu et al., 2024b), and Qwen-QwQ (Team, 2024a) are at the forefront of this progress, characterized by their ability to emulate human reasoning through a slower, more deliberate thought process. These models leverage increased inference time to enhance reasoning accuracy. While they have unlocked substantial performance gains, more complex tasks such as mathematical problem solving in AIME, OlympiadBench (He et al., 2024) and code in LiveCodeBench (Jain et al., 2024), which demand a more fine-grained search through a vast solution space and more delicate thought for each intricate reasoning step, thus still pose significant challenges.\nSubsequent research has focused on enhancing LLMs' reasoning capabilities on complex problems through inference-time"}, {"title": "2. Related Work and Discussions", "content": "Learning from Preferences for Language Models Preference learning is critical for aligning Large Language Models (LLMs) with human expectations and perceptions. Initial approaches, building on pre-training and supervised fine-tuning (SFT), employed PPO in Reinforcement Learning from Human/AI Feedback (RLHF/RLAIF) frameworks (Schulman et al., 2017; Christiano et al., 2017; Ouyang et al., 2022; Xie et al., 2024). These approaches typically involve training a reward model on preference pairs and subsequently optimizing the LLM to maximize the learned reward. However, PPO's instability and inefficiency motivated alternative approaches like DPO (Rafailov et al., 2024), which directly optimizes a policy from paired preference data. Subsequent research has addressed various challenges. ORPO (Hong et al., 2024) integrates alignment into SFT, KTO (Ethayarajh et al., 2024) leverages pointwise data, simplifying data acquisition process. Other efforts focus on finer-grained optimization, such as Step-DPO (Lai et al., 2024) and Cross-DPO (Yang et al., 2024c) that targets intermediate reasoning or reflection steps. SPO (Swamy et al., 2024) employs game-theoretic concepts to address non-transitive preferences, while Multi-turn DPO (Shi et al., 2024) extends optimization to conversations. However, existing methods often rely on instance or step-level reward units, potentially failing to capture and reward the higher-level cognitive processes inherent in human problem-solving process. To this end, we introduce hierarchical RL-based optimization, a novel preference learning approach that encourages the model to configure a series of high-level thought templates that can handle diverse sub-tasks for complex problems, thereby promoting more human-like problem-solving strategies in LLMs.\nRetrieval-Augmented Generation for Language Models Retrieval-augmented Language Models (RALMs) have become a powerful approach to mitigating hallucinations and enhancing the factual accuracy of LLMs (Asai et al., 2023; Mialon et al., 2023; Shi et al., 2023; Gao et al., 2023; Zhao et al., 2024). By retrieving relevant documents from a large-scale external knowledge source (Borgeaud et al., 2022) to inform response generation, RALMs have demonstrated superior performance in question-answering, often with fewer parameters than traditional LLMs (Mialon et al., 2023). Their versatility is further evidenced by successful applications across diverse tasks, including multi-modal generation and biomedical applications (Yasunaga et al., 2023; Izacard et al., 2023; Wang et al., 2022; Zhao et al., 2024; Borgeaud et al., 2022; Yang et al., 2023). However, RALMs face challenges in complex reasoning tasks, such as math and code, where retrieving relevant guidelines or templates via standard embedding similarity search proves difficult. While methods like RAFT (Zhang et al., 2024c) have attempted to address this by improving retrieval relevance, respectively, their effectiveness decrease as the document"}, {"title": "3. ReasonFlux: Scaling Thought Templates for Hierarchical LLM Reasoning", "content": "Inspired by how humans utilize external resources when tackling complex reasoning problems, RAG methods enhance LLMs by enabling them to retrieve information from external sources (Zhao et al., 2024). Recent Buffer of Thought (BoT) (Yang et al., 2024b) attempts to create a buffer of high-level thoughts for llm reasoning, and builds an efficient RAG reasoning system. Despite a comprehensive template library to solve similar problems, BoT still faces scalability challenges"}, {"title": "3.1. Constructing Structured Thought Template Library", "content": "To address this, our approach focuses on constructing a structured thought template library that enables more precise, targeted retrieval and mitigates scalability challenges. To build this library, we carefully selected a wide and diverse range of challenging mathematical reasoning problems from different sources, ensuring robustness and broad applicability of our template library. We used an LLM to analyze the thought behind the solution and generating concise summaries of problem-solving strategies and identifying common patterns. This process yielded a collection of high-quality, solution-oriented thought templates. Each template $T_i$ in the library is structured for efficient retrieval and application, where $T_{nam}$ is the name (e.g., \"$\\sqrt{R^2 - x^2}$ Type Trigonometric Substitution\u201d), $T_{tag}$ is a set of tags for keyword-based retrieval (e.g., {\u201cTrigonometric Substitution\u201d, \u201cIrrational Function Optimization\u201d}), $T_{des}$ is a description of the underlying principle and applicable scenarios, $T_{sco}$ defines the scope, specifying the problem types it addresses, $T_a$ is a sequence of detailed application steps {$a_1, a_2, ..., a_k$}, and $T_{exa}$ is a set of examples demonstrating its application. The entire library $D_{temp}$ is a set of thought templates as mentioned:\n$D_{temp} = \\{T_1, T_2, ..., T_m\\}$                              (1)\nwhere $m$ is the total number of templates. Here we present an illustration of a thought template within our library. For the sake of brevity, some fields in the following example have been simplified. Please refer to Appendix A for more detailed examples."}, {"title": "3.2. Hierarchical Reinforcement Learning on Thought Template Trajectory", "content": "While our structured template library provides a valuable resource for reasoning, an effective method is needed to utilize this library and select the appropriate templates for handing a given problem. To this end, we perform hierarchical reinforcement learning to train and finally obtain ReasonFlux that can effectively plan out an optimal thought template trajectory for a problem. We retrieve and configure a sequence of relevant templates from the library, assisting in instantiating the retrieved templates on specific sub-problems. ReasonFlux acts as an experienced navigator, providing the optimal trajectory denoted as $T_{traj}$ that enabling the LLM to instantiate abstract thought templates into concrete sequential problem-solving steps.\nStructure-based Finetuning Our hierarchical RL process begins by leveraging the structured template library $D_{temp}$ to construct a knowledge-intensive training dataset $D_{train}$. This dataset comprises diverse examples of template names $T_{nam}$, their associated tags $T_{tag}$, detailed descriptions of their underlying principles $T_{des}$, and a clear delineation of their applicable scopes $T_{sco}$, represented as tuples $(T_{nam}, T_{tag}, T_{des}, T_{sco})$ extracted from $D_{temp}$. We then fine-tune a base LLM, denoted as $\\pi$, on this dataset $D_{train}$. This process equips the model with a foundational understanding of the structure, content, and"}, {"title": "3.3. Inference Scaling with Scaling Thought Templates", "content": "After hierarchical RL process, we refer to optimized navigator $\\pi_{\\theta}$ as ReasonFlux. Then, we further design a novel inference scaling system by leveraging automatically planned trajectories and dynamically retrieved thought templates. This system, illustrated in Figure 2, involves a multi-round interplay between the ReasonFlux, a structured template library $D_{temp}$, and a downstream inference LLM $\\pi_{inf}$.\nGiven an input problem $x$, the first task for ReasonFlux is to analyze and extract the core mathematical concepts and relationships embedded within $x$. Based on this abstract representation, denoted as $\\alpha(x)$. ReasonFlux then configures an optimal template trajectory $Traj$. This trajectory, represented as a sequence of steps $Traj = \\{s_1, s_2, ..., s_k\\}$, is not a rigid, pre-defined path but rather a dynamically generated plan tailored to the specific nuances of the input problem $x$. Each step $s$ within the trajectory is associated with a specific template name $T_{nam}$ and $T_{tag}$ for efficient retrieval. ReasonFlux then searches and retrieves a set of most relevant thought templates from the curated thought template library $D_{temp}$. Formally,"}, {"title": "4. Experiments", "content": "Template Library Construction As illustrated in Section 3.1, we use Gemini-2.0 (Team et al., 2023) to summarize and extracts high-level thoughts from the training sets of various math datasets, such as MATH (7.5K samples) (Lightman et al., 2023), and self-curated CN high-school competition-level data (2K samples), and construct our structured thought template library (approximately 500 thought templates). We provide some template examples in Appendix A.\nTraining Details Due to limited GPU resources, we use Qwen2.5-32B-Instruct (Yang et al., 2024a) as the base model and also adopt it as our inference LLM. In our training procedure, we only use 8 NVIDIA A100 GPUs, which is very cost-efficient. In the structure-based finetuning stage (Section 3.2), we train the initialized $\\pi_{struct}$ with the training dataset $D_{train}$ containing 15K samples extended from our template library $D_{temp}$. We conduct the initialization training for 6 epochs using an AdamW optimizer along with the cosine learning rate scheduler. In the template trajectory optimization process (Section 3.2), we train our ReasonFlux with 10K collected pair-wise trajectories from MATH (7.5k), and self-curated CN high-school competition-level data (2K) for 6 epochs using an AdamW optimizer along with cosine learning rate scheduler.\nEvaluation Datasets To evaluate the complex reasoning capabilities, we choose a broad set of challenging reasoning benchmarks, including MATH (Lightman et al., 2023), AIME 2024 (AI-MO, 2024a), AMC 2023 (AI-MO, 2024b), OlympiadBench (He et al., 2024) and GaoKao (Chinese College Entrance Exam) En 2023 (Liao et al., 2024). These benchmarks comprehensively evaluate mathematical reasoning capabilities, and they are all competition-level and Olympic-level problems. Moreover, AIME 2024 and AMC 2023 are highly challenging competition benchmarks, which are of limited sizes of test samples in AMC and AIME and the results are averaged over 16 runs."}, {"title": "4.1. Results on Challenging Reasoning Benchmarks", "content": "Table 2 shows the final results of our ReasonFlux with a comprehensive comparison to SOTA reasoning models. We find that our ReasonFlux-32B consistently outperforms both frontier LLMs and open-sourced reasoning LLMs on most challenging mathematical benchmarks, achieving new SOTA performances with only 32B-level parameters. More specifically, on the MATH benchmark, ReasonFlux achieves 91.2% of accuracy, surpassing frontier reasoning models 01-preview by 6.7%, and current SOTA-level open-source LLMs with only 32B parameters. On the AIME 2024 benchmark, ReasonFlux consistently demonstrates its extrodinary reasoning capabilities with 56.7% accuracy, significantly surpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively, and matching the performance of the proprietary OpenAI 01-mini. On the AMC 2023 benchmark, our method, ReasonFlux, maintains its position within the top tier of all reasoning LLMs with 85.0% accuracy, significantly outperforming other open-source LLMs while achieving performance comparable to"}, {"title": "4.2. Generalization Ability of Structured Template Library", "content": "We presents additional experiments on MATH benchmark designed to evaluate the generalization ability of our structured template library. To achieve this, we randomly sampled 100 templates from the library, each paired with its corresponding example problem. Subsequently, we employed o1-preview to generate 50 variant problems for each example. These variants were carefully constructed to ensure they differed from the original examples while still assessing the same underlying knowledge and skills.\nWe then used these templates as in-context examples to guide different LLMs during inference on the generated variant problems. We compare the average accuracy between our template augmented reasoning and direct reasoning (i.e., solving the problems without template). As illustrated in Table 3, our template-augmented approach significantly improves the reasoning accuracy of different base models compared to direct reasoning. This demonstrates the ability of our structured templates to generalize effectively across a range of similar problems, rather than being limited to specific instances. Furthermore, we observed that smaller-sized LLMs, when guided by our templates, were able to outperform larger-sized LLMs employing direct reasoning. This finding underscores the effectiveness and high quality of our structured template library."}, {"title": "4.3. Reasoning Flows over Planned Template Trajectory", "content": "We showcase detailed examples of our reasoning flows, as depicted in Figure 3, when tackling challenging mathematical problems. Specifically, ReasonFlux begins by meticulously observing and analyzing the input problem, engaging in deep thought to explore potential solution pathways. Based on this initial assessment, ReasonFlux intelligently configures a dynamic reasoning trajectory, strategically retrieving relevant templates from our structured template library to guide each logical step. Then, ReasonFlux initiates an interactive instruction with the inference LLM, guiding it to follow the prescribed trajectory and execute the reasoning process along the trajectory. Crucially, the results obtained from preceding steps are seamlessly integrated as contextual information, informing and conditioning the subsequent steps. Compare to conventional self-explore and reasoning paradigm, our method could consistently improve the reasoning accuracy and efficiency."}, {"title": "4.4. Inference Scaling Laws for Template-Augmented Reasoning", "content": "Different from traditional inference scaling with Best-of-N and Majority Voting (Wu et al., 2024), our ReasonFlux owns a specific interplay-based scaling mechanism. In order to provide a comprehensive understanding of how ReasonFlux automatically trade off between cost and performance. As shown in Figure 4, we demonstrate (i) how number of retrieved templates adaptively scales with increased problem complexity and (ii) how rounds of interplay between ReasonFlux and inference LLMs adaptively scales with increased problem complexity. From the results, we can observe that our ReasonFlux can effectively capture the complexity of input problems, and plan out reasonable template trajectories with appropriate interplay rounds. Utilizing more fine-grained thought templates may boost the scaling effect of our ReasonFlux, and we leave this exploration for future work."}, {"title": "4.5. Better Exploration-Exploitation Trade-off", "content": "To evaluate the exploration-exploitation trade-off of different reasoning strategies, we conducted an ablation study comparing our proposed interplay method against Best-of-N and MCTS. Each method exhibits a distinct approach to navigating the reasoning space. Best-of-N constructs multiple reasoning trajectories to identify the optimal path, while MCTS iteratively explores the most promising next step during the problem-solving process.\nOur method formulates a potential reasoning trajectory and then guides the interactive process with the inference LLM for iterative refinement and adjustments. To ensure a fair comparison, we introduce a unified metric termed \"exploration-exploitation cost.\" This metric quantifies the number of exploration attempts required by each method to correctly solve a given problem. For our method, this denotes the number of interactions between ReasonFlux and the inference LLM. For MCTS, it is represented by the iteration time, and for Best-of-N, it denotes the total number of sampled trajectories.\nAs illustrated in Figure 5, both MCTS and Best-of-N exhibit an increasing exploration-exploitation cost as problem difficulty escalates. In contrast, our method maintains a consistently lower and more stable exploration cost across all difficulty levels. This superior efficiency of our method can be attributed to the effectiveness of our structured template library. This high-quality library effectively refines the search space, facilitating the identification of correct reasoning paths. Furthermore, the high quality and generalization ability of the templates (experimental analysis in Section 4.2) within the library allows for effective exploitation, guiding the Inference LLM towards accurate and efficient reasoning. Consequently, our approach demonstrates a more balanced and efficient exploration-exploitation trade-off compared to Best-of-N and MCTS."}, {"title": "5. Conclusion", "content": "In this work, we present ReasonFlux, a new hierarchical LLM reasoning framework that adaptively scales fundamental and essential thought templates for simplifying the search space of complex reasoning, and outperforming the mathematical reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3. We introduces a structured and compact thought template library, hierarchical reinforcement learning on thought template trajectory and a brand new inference scaling system. Extensive experiments across different challenging math benchmarks demonstrate the superiority of ReasonFlux. We also reveal some key findings, including the scaling laws for our template-augmented reasoning and the superior exploration-exploitation trade-off of our ReasonFlux over previous reasoning strategies."}, {"title": "A. More Examples of Structured Template Library", "content": "In this section, we present a more detailed and diverse collection of supplementary examples for Section 3.1, showcasing our meticulously designed structured templates. These examples span a range of template types, demonstrating the versatility and applicability of our approach. The template types include: 1) Problem-Solving Methods, which provide step-by-step procedures for tackling specific problem types; 2) Secondary Mathematical Conclusions, which encapsulate derived mathematical results that can be applied to various problems; 3) Property & Theorem that highlight essential mathematical properties and theorems; 4) Knowledge Application templates that demonstrate the application of specific mathematical concepts and techniques; and 5) Important Formulas and Rules templates, which offer concise summaries of crucial formulas and rules for quick reference and application.\nTo emphasize the structure and facilitate comprehension, each template is designed to contain two kinds of data: i) Template Metadata and ii) Template Content. The Template Metadata provides concise information about the template, including its name $T_{nam}$, relevant knowledge tags $T_{tag}$, a brief description $T_{des}$, and typical application scenarios $T_{sco}$. This section serves as a quick reference guide, enabling LLMs to efficiently locate and identify templates relevant to their needs. The Template Content delves into the core of the template, presenting the detailed reasoning flow and a concrete example illustrating its application. The reasoning flow corresponding to the application steps $T_a$ and the example application corresponding to $T_{exa}$ in Section 3.1, which outlines the logical steps or procedures involved in utilizing the template, while the example provides a practical demonstration of how the template can be applied to solve a specific problem. This two-part structure enhances clarity and allows for both quick retrieval and in-depth understanding of each template.\nThe following examples have been carefully selected to provide a comprehensive overview of the capabilities of our structured template library. Through these examples, we aim to more comprehensive overview of our structured templates, and demonstrate the effectiveness of our structured templates in promoting organized thinking, facilitating problem-solving, and ultimately enhancing mathematical understanding of LLMs."}, {"title": "(I) Template (Problem-Solving Method) : Five-Step Method for Solving Absolute Value Inequalities", "content": "Template Name: Five-Step Method for Solving Absolute Value Inequalities\nKnowledge Tag: Absolute Value Inequalities, Solving Inequalities, Combining Numerical and Graphical Methods\nDescription: This template provides a structured approach to solving absolute value inequalities using various strategies, with a focus on the squaring method and the zero-point interval method.\nApplication Scenario: Applicable to absolute value inequalities of the form |x - a| > b, |ax + b| < c, |f(x)| > |g(x), etc. Particularly suitable for complex cases involving multiple absolute value symbols or requiring interval discussions.\nReasoning Flow:\n1. Standardize the inequality to ensure the right side is non-negative (e.g., |x \u2212 1| > |2x + 3|).\n2. Choose a solution strategy (Step 3 will present two options).\n3. Solve using one of the following methods:\n(a) Squaring Method:\n(i) Rearrange to the form $A^2 > B^2$.\n(ii) Expand and simplify into a polynomial inequality.\n(iii) Factor, find the roots, and use a number line to determine the solution set.\n(b) Interval Method:\n(i) Mark the zero points of each absolute value expression (e.g., $x$ = 1 and $x$ = -1.5).\n(ii) Divide the number line into intervals (e.g., $x \u2264 \u22121.5, -1.5 < x < 1, x \u2265 1$).\n(iii) Rewrite the inequality without absolute value signs within each interval.\n(iv) Solve the inequality in each interval and find the intersection with the interval.\n4. Verify whether the endpoint values satisfy the original inequality.\n5. Combine the solution sets from each interval, expressing the final result using set notation. (If using the interval method)\nExample Application:\nProblem: Solve the inequality |x-1| > |2x+3|.\nSolution Process:\nUsing Squaring Method (Step 3\u0430):\n1. Square both sides: (x \u2212 1)\u00b2 > (2x + 3)\u00b2\n2. Expand and simplify: x\u00b2 \u2013 2x + 1 > 4x\u00b2 + 12x + 9 \u2192 \u22123x\u00b2 - 14x - 8 > 0\n3. Factor: -(3x + 2)(x + 4) > 0 \u2192 (3x + 2)(x + 4) < 0\n4. Find the roots and use a number line: $x = \u22124, x = \u2212\\frac{2}{3}$  Solution set: (-4, -$\\frac{2}{3}$)\nUsing Interval Method (Step 3b):\nTo better present our templates, we have omitted some examples that were too long."}, {"title": "(II) Template (Secondary Conclusion) : Application of the Inequality of Arithmetic and Geometric Means for Three and n Variables", "content": "Template Name: Application of the Inequality of Arithmetic and Geometric Means for Three and n Variables\nKnowledge Tag: Inequality of Arithmetic and Geometric Means, Three-Variable Inequality, n-Variable Inequality, Inequality Proof\nDescription: Extends the two-variable inequality of arithmetic and geometric means to three and n variables, suitable for handling the relationship between the sum and product of multiple positive numbers. The core formulas are: for three variables, $a^3 + b^3 + c^3 > 3abc$; for n variables, the arithmetic mean is greater than or equal to the geometric mean.\nApplication Scenario: Used when there are three or more positive variables in the problem, and it is necessary to compare the relationships between sum, product, sum of squares, etc. Especially suitable for proving inequalities with multiple variables or finding the maximum/minimum values.\nReasoning Flow:\n1. Confirm that all variables are positive (ensure this through the problem's conditions or transformations if necessary).\n2. If it is a three-variable case, directly apply $a^3 + b^3 + c^3 > 3abc$ (equality holds if and only if $a = b = c$).\n3. If it is an n-variable case, apply the inequality of arithmetic and geometric means:\n$\\frac{a_1 + a_2 + ... + a_n}{n} \u2265 \\sqrt[n]{a_1a_2...a_n}$\n(equality holds if and only if $a_1 = a_2 = ... = a_n$).\n4. Transform the original expression into the standard form above through algebraic manipulations (such as grouping, factoring, completing the square, etc.).\n5. Combine with known conditions (such as abc = 1) to substitute and simplify to find the maximum/minimum value.\n6. Verify that the condition for equality holds satisfies the problem's constraints.\nExample Application:\nProblem: Given that a, b, and c are positive numbers and abc = 1, prove that $(a + b)^3 + (b + c)^3 + (c + a)^3 > 24$.\nSolution:\n1. Confirm a, b, c > 0 and abc = 1.\n2. Apply the three-variable inequality to each term in parentheses: $(a + b)^3 > 8ab(a+b)/8$ (needs to be adjusted to fit the form).\n3. Better solution: Directly apply $a^3 + b^3 + c^3 > 3abc$.\n(a + b)\u00b3 + (b + c)\u00b3 + (c + a)\u00b3 \u2265 3(a + b)(b + c)(c+a)\n4. Apply the two-variable inequality of arithmetic and geometric means to (a + b)(b + c)(c + a):\n$(a + b) \u2265 2\\sqrt{ab}, (b + c) \u2265 2\\sqrt{bc}, (c + a) \u2265 2\\sqrt{ca}$\n\\ The product > 8\\sqrt{a^2b^2c^2} = 8abc = 8\n5. Substitute to get the original expression \u2265 3 \u00d7 8 = 24.\n6. Verify the equality condition: Equality holds if and only if a = b = c = 1."}, {"title": "(III) Template (Property Theorem) : Extremum Value Theorem", "content": "Template Name: Extremum Value Theorem\nKnowledge Tag: Inequality of Arithmetic and Geometric Means, Extremum Value Theorem, Product is Maximum when Sum is Constant, Sum is Minimum when Product is Constant\nDescription: When the product or sum of two positive numbers x and y is a constant, their sum or product has an extremum value: when the product is constant, the sum has a minimum value; when the sum is constant, the product has a maximum value. Equality holds if and only if x = y.\nApplication Scenario: Suitable for finding the maximum/minimum value of the sum or product of two positive variables, especially when the product or sum of one of the expressions is a constant. For example: rectangle perimeter/area problems, function optimization problems, etc.\nReasoning Flow:\n1. Confirm that variables $x$ and $y$ are both positive.\n2. Determine if there is a constant product $xy = P$ or a constant sum $x + y = S$ in the problem.\n3. If the product is a constant $P$, then the minimum value of the sum $x + y$ is $2\\sqrt{P}$ (when and only when $x = y$).\n4. If the sum is a constant $S$, then the maximum value of the product $xy$ is $\\frac{S}{4}$ (when and only when $x = y$).\n5. Verify that the condition for equality holds satisfies the problem's requirements (e.g., the actual range of values for $x$ and $y$).\nExample Application:\nProblem: What is the minimum value of the function $y = \\frac{x^2}{x^2-5} + \\frac{5}{x^2-5}$ ($x^2 > 5$)?\nSolution:\n1. Confirm the variable is positive: $x^2 > 5 => x^2 - 5 > 0$.\n2. Transform the function: $y = x^2 + \\frac{5}{x^2_5} -4$.\n3. Let $a=x2_5 >0$, then $y=a+ \\frac{5}{a}$.\n4. Apply the Extremum Value Theorem: $ a+ \\frac{1}{a} \\geq 2\\sqrt{a \\cdot \\frac{1}{a}}  = 2$ (when and only when $a = \\frac{1}{a} =>a = 1$).\n5. Therefore y \u2265 2 + 5 = 7, when and only when $x^2 \u2212 5 = 1 \\Rightarrow x = \u00b1\\sqrt{6}$, the equality holds.\nAnswer: 7"}, {"title": "(IV) Template (Knowledge Application) : Analyzing the Parity and Symmetry of Trigonometric Functions Using Reduction Formulas", "content": "Template Name: Analyzing the Parity and Symmetry of Trigonometric Functions Using Reduction Formulas\nKnowledge Tag: Reduction Formulas, Parity, Symmetry, Properties of Trigonometric Functions\nDescription: This template guides the analysis of the parity and symmetry of complex trigonometric functions by transforming them into standard forms using reduction formulas, aiding students in systematically solving related problems.\nApplication Scenario: Applicable to determining the parity of trigonometric functions, identifying the symmetry centers or axes of function graphs, and solving for parameters (e.g., phase angle $ \\phi $). This method is useful when encountering functions of the form $y = Asin(wx + \\phi)$ or $y = Acos(wx + \\phi)$.\nReasoning Flow:\n1. Transform the target trigonometric function into a standard sine or cosine form using reduction formulas. For example, use $sin(x + \\frac{\\pi}{2}) = cos x$ to convert a cosine function to a sine form.\n2. Determine the function's parity based on the definition of odd and even functions. An odd function satisfies $f(-x) = -f(x)$, and an even function satisfies $f(-x) = f(x)$.\n3. If symmetry is involved, determine the expressions for the symmetry axes or centers. For example, the symmetry axes of the sine function are $x = \\frac{\\pi}{2} + k\\pi$, and the symmetry centers are $(k\\pi, 0)$.\n4. Compare the transformed function with the standard form and solve the equation to find the unknown parameters (e.g., $ \\phi $). For example, set the phase angle to satisfy the condition for an odd function, $ \\phi = k\\pi$.\n5. Verify the solution's validity, ensuring it conforms to the original function's domain and fundamental properties.\nExample Application:\nProblem: Given that the function $y = \\sqrt{2} sin(x + \\phi)$ is an odd function, find the possible values of $ \\phi $.\nSolution Steps:\n1. Based on the definition of an odd function, we have $\\sqrt{2}sin(-x + \\phi) = -\\sqrt{2}sin(x + \\phi)$.\n2. Expand the left side: $sin(-x + \\phi) = sin \\phi cos x - cos \\phi sin x$.\n3. Simplify the right side: - sin(x + \\phi) = - sin x cos - cos x sin .\n4. Compare the coefficients on both sides of the equation: sin & sin & and cos = cos .\n5. Solve for : sin  = 0 \u21d2  = k (k \u2208 Z)."}, {"title": "(V) Template (Important Formulas/Rules) : Distance Formulas and Their Applications", "content": "Template Name: Distance Formulas and Their Applications\nKnowledge Tag: Distance Between Two Points, Distance from a Point to a Line, Distance Between Parallel Lines\nDescription: This template includes formulas for calculating three types of distances: the distance between two points, the distance from a point to a line, and the distance between two parallel lines. These formulas are core tools for solving distance problems in analytic geometry.\nApplication Scenario: This template can be applied when it is necessary to calculate the geometric distance between two points, the perpendicular distance from a point to a line, or the fixed distance between two parallel lines. It is commonly used in scenarios such as calculating the area of geometric figures, analyzing positional relationships, and solving symmetry problems.\nReasoning Flow:\n1. Step 1: Identify the type of problem (distance between two points / distance from a point to a line / distance between parallel lines).\n2. Step 2: Distance between two points formula: $P_1 P_2 = \\sqrt{(x_2 - x_1)^2 + (y_2 \u2013 y_1)^2}$, substitute the coordinates directly to calculate.\n3. Step 3: Distance from a point to a line formula: $d \\frac{|Axo+Byo+C|}{\\sqrt{A^2+B^2}} $, ensure the line equation is in the general form $Ax + By + C = 0$.\n4. Step 4: Distance between parallel lines formula: $d = \\frac{|C_1-C_2|}{\\sqrt{A^2+B^2}}$, both line equations must be in the form $Ax + By + C_1 0$ and $Ax + By + C_2  0$ with the same coefficients $A$ and $B$.\n5. Step 5: Handle special cases (e.g., projection distance on coordinate axes, distance transformation in symmetry problems).\nExample Application:\nProblem: Given that the line $l_1 : mx + 2y - 4   0$ has equal intercepts on the x-axis and y-axis, find the distance between $l_1$ and $l_2: 3x+3y-1 = 0$.\nSolution Steps:\n1. From equal intercepts, we get $m = 2$\n2. Convert $l_1$ to the general form $2x + 2y - 6 = 0 \\Rightarrow x + y \u2212 3 = 0$.\n3. Align coefficients: Rewrite $l_1$ as $3x + 3y \u2013 9 = 0$ to match the coefficients of $l_2$.\n4. Apply the parallel lines distance formula $d =  \\frac{|-1-(-9)|}{\\sqrt{3^2+3^2}} = \\frac{8}{3\\sqrt{2}} = \\frac{4\\sqrt{2}}{3}$.\nAnswer: $ \\frac{4\\sqrt{2}}{3}$"}]}