{"title": "CodePMP: Scalable Preference Model Pretraining for Large Language Model Reasoning", "authors": ["Huimu Yu", "Xing Wu", "Weidong Yin", "Debing Zhang", "Songlin Hu"], "abstract": "Large language models (LLMs) have made significant progress in natural lan- guage understanding and generation, driven by scalable pretraining and advanced finetuning. However, enhancing reasoning abilities in LLMs, particularly via re- inforcement learning from human feedback (RLHF), remains challenging due to the scarcity of high-quality preference data, which is labor-intensive to annotate and crucial for reward model (RM) finetuning. To alleviate this issue, we introduce CodePMP, a scalable preference model pretraining (PMP) pipeline that utilizes a large corpus of synthesized code-preference pairs from publicly available high- quality source code. CodePMP improves RM finetuning efficiency by pretraining preference models on large-scale synthesized code-preference pairs. We evaluate CodePMP on mathematical reasoning tasks (GSM8K, MATH) and logical reason- ing tasks (ReClor, LogiQA2.0), consistently showing significant improvements in reasoning performance of LLMs and highlighting the importance of scalable preference model pretraining for efficient reward modeling.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have made remarkable progress in natural language understanding and generation, benefiting from scalable pretraining and finetuning techniques like supervised finetuning (SFT) (Wang et al., 2022; 2023a) and Reinforcement Learning from Human Feedback (RLHF) (Bai et al., 2022a; Lightman et al., 2023b; Bai et al., 2022b; Gulcehre et al., 2023; Schulman et al., 2017; Rafailov et al., 2024). However, enhancing LLMs' reasoning abilities, particularly in complex logical"}, {"title": "2 Preliminaries", "content": "Language Modeling Language modeling (LM) is a fundamental task in natural language process- ing, aimed at modeling sequences of language. This is typically achieved through Causal Language Models (Causal LM), where the model is trained to maximize the likelihood of predicting the next word $w_t$ given the preceding words $w_1, w_2,..., w_{t-1}$. The training process minimizes the negative log-likelihood of the predicted word sequence:\n$\\mathcal{L}_{LLM} = \\sum_{t=1}^{T} \\log P(w_t|w_1, w_2,..., w_{t-1})$\nThis loss function $\\mathcal{L}_{LLM}$ encourages the model to capture the underlying patterns in the data. Trans- former architectures (Vaswani, 2017) have become the standard for Causal LM due to their ability to handle long-range dependencies effectively.\nReward Modeling Reward modeling (RM) is crucial in reinforcement learning from human feedback (RLHF), providing a scalar reward signal that guides the learning process based on output quality. The reward model $R_\\theta$ predicts the quality of an output $y$ given a context $x$ as $s = R_\\theta(x,y)$, where $s$ is the scalar reward score. In preference modeling, RM predicts the relative quality of outputs by comparing pairs. A common method is the Pairwise Ranking Loss, where the model assigns higher scores to preferred (chosen) outputs:\n$L_{RM} = - \\log (\\sigma (S_{chosen} - S_{rejected}))$,\nwhere $S_{chosen} = R_\\theta(x, Y_{chosen})$ and $S_{rejected} = R_\\theta(x, Y_{rejected})$, and $\\sigma(\\cdot)$ is the sigmoid function.\nBest-of-N Sampling Best-of-N (BoN) sampling improves LLM reasoning (Cobbe et al., 2021b; Lightman et al., 2023b). In this approach, N candidate solutions {$Y_1, Y_2,..., Y_N$} are generated by sampling from the LLM's output distribution for a given problem. A reward model scores each candidate and selects the highest-scoring one as the final answer:\n$\\hat{y} = \\arg \\max_{Y_i \\in \\{Y_1, Y_2,...,Y_N\\}} R_\\theta(x, Y_i)$\n, where $R_\\theta(x, y_i)$ represents the reward score for each candidate $y_i$. This technique is especially effective in tasks like mathematical problem-solving and logical inference, where selecting the most plausible solution from a diverse set of outputs improves overall accuracy (Wang et al., 2022)."}, {"title": "3 Code Preference Model Pretraining", "content": "Code Preference Model Pretraining (CodePMP) is designed to enhance the sample efficiency of reward models, particularly for reasoning tasks where high-quality preference data is scarce. Traditionally, reward models are finetuned on small, curated datasets, which limits their effectiveness in complex tasks like mathematical reasoning or logical deduction. CodePMP mitigates this limitation by"}, {"title": "3.1 Model Design", "content": "introducing a pretraining phase between basic language model pretraining and finetuning on domain- specific reasoning datasets. This phase leverages a large, diverse dataset of code-preference pairs, enabling the model to learn generalizable patterns and ranking strategies.\nCodePMP training involves two key components: Reward Modeling (RM) and Language Modeling (LM). In RM, the model is trained on code-preference pairs, learning to assign higher scores to the chosen code through a pairwise ranking loss. In LM, only the chosen code is used for autoregressive training to maintain the model's general capabilities. The overall loss is a combination of the RM and LM losses, ensuring the model enhances its ranking ability without sacrificing general language modeling performance: $L_{PMP} = L_{rank} + L_{LLM}$."}, {"title": "3.2 Data Construction", "content": "To enable scalable preference model pretraining, we construct a dataset sourced from GitHub, which includes a diverse range of repositories and associated metadata. The dataset consists of two primary components: Repository Data comprises over 1.3 billion code files from GitHub repositories, while GitHub Metadata includes information such as commit histories, discussions, pull requests, and issues.\nThe CodePMP dataset is constructed through a systematic process. First, raw source code is processed by a description summarizer, typically an instruction-tuned CodeLLM, to generate prompts that describe the functionality of the code.\nThese prompts are then used by two CodeLLMs of different capabilities to generate code snippets:\n\u2022 Chosen response: Generated by a more advanced CodeLLM (e.g., 6.7B parameters).\n\u2022 Rejected response: Generated by a less capable CodeLLM (e.g., 1.3B parameters).\nThis process yields pairs of code responses-one chosen and one rejected\u2014which are used for preference modeling. This scalable approach significantly enhances pretraining efficiency, improving performance on downstream tasks.\nThe steps of the CodePMP methodology are outlined systematically in Algorithm 1."}, {"title": "4 Experimental", "content": "In this section, we first outline the experimental setup, followed by the experimental results, high- lighting that CodePMP is a highly scalable method."}, {"title": "4.1 Experimental Settings", "content": null}, {"title": "4.1.1 CodePMP Settings", "content": "Data Construction We generate code preference pairs following Algorithm 1, using the deepseek- coder-6.7b-instruct model as the strong CodeLLM to generate chosen responses and the deepseek- coder-1.3b-instruct model as the weak CodeLLM to generate rejected responses. The constructed"}, {"title": "4.1.2 Reasoning Finetuning Settings", "content": "We validate CodePMP on reward models across two reasoning task types: mathematical and logical reasoning. The reward model is finetuned on corresponding preference datasets for each task. For mathematical reasoning, we use the MathShepherd-pair dataset, derived from MathShepherd (Wang et al., 2023b), and evaluate the model on a holdout test set to assess RM accuracy.\nSimilarly, for logical reasoning, we use the ReClor-pair and LogiQA2.0-pair datasets, derived from ReClor (Yu et al., 2020) and LogiQA2.0 (Liu et al., 2023), respectively. We train reward models on these datasets, with holdout test sets used to evaluate model accuracy. Dataset construction and finetuning hyperparameters are provided in Appendix D and B."}, {"title": "4.1.3 Evaluation Settings", "content": "Following (Zhang et al., 2024a), we adopt two evaluation metrics:\nRM Accuracy This metric measures the accuracy of the reward model in distinguishing chosen from rejected solutions on the holdout test sets. It provides insight into the model's ability to classify individual sequences.\nBest-of-N (BoN) Accuracy This metric evaluates the proportion of correct solutions selected by the finetuned RM from N candidate responses. It assesses the model's group-wise ranking performance, focusing on its ability to select the correct answer from a set of candidates. We use MetaMath-Mistral-7B (Yu et al., 2023) as the generator for this evaluation.\nFor mathematical reasoning, we use the GSM8K (Cobbe et al., 2021b) and MATH (Hendrycks et al., 2021) test sets. For logical reasoning, we evaluate on the ReClor (Yu et al., 2020) and LogiQA2.0 (Liu et al., 2023) test sets. Further details can be found in Appendix D.\nNote that logical reasoning questions typically involve a paragraph followed by statements to be judged true or false, making Best-of-N evaluation challenging. Therefore, we use multiple-choice accuracy, where the reward model ranks four manually annotated options and selects the best one. This metric is equivalent to Best-of-4, and thus, for logical reasoning tasks, multiple-choice accuracy and Best-of-N are used interchangeably."}, {"title": "4.2 Experimental Results", "content": null}, {"title": "4.2.1 RM Accuracy Results", "content": "We first compare RM accuracy on the holdout test set with and without CodePMP initialization. As shown in Table 1, RM finetuned with CodePMP initialization achieves higher accuracy on both 1.5B and 7B models across mathematical and logical reasoning tasks, demonstrating that CodePMP enhances the model's ability to differentiate correct from incorrect reasoning. Moreover, CodePMP exhibits strong generalization, yielding significant improvements across different reasoning tasks."}, {"title": "4.2.2 BoN Accuracy Results", "content": "We evaluate BoN accuracy across reasoning tasks with and without CodePMP initialization. As shown in Figure 8a and Table 8b, RM finetuned with CodePMP initialization consistently achieves"}, {"title": "4.2.3 RM Sample Efficiency Comparison", "content": "One key advantage of CodePMP is its ability to improve the sample efficiency of RM finetuning. To assess this, we conduct experiments with progressively larger sample sizes for RM finetuning. As indicated by (Kaplan et al., 2020), optimal results are achieved when the learning rate scheduler completes its decay at the end of training. Therefore, rather than evaluating intermediate checkpoints, we retrain models with varying sample sizes for optimal results. Figure 4 and 10 (Appendix E) show that as the sample size increases, RMs with CodePMP initialization consistently outperforms others in both BoN and RM accuracy. Notably, RMs finetuned with CodePMP initialization using just 0.5k samples surpasses RMs finetuned without CodePMP initialization using 40k samples on mathematical tasks, demonstrating CodePMP's significant advantage in sample efficiency. However, as sample size increases, this advantage diminishes slightly, suggesting that with much larger datasets, CodePMP's benefit may become less pronounced, but the cost of manual labeling remains a key consideration."}, {"title": "4.2.4 The Importance of Scalable PMP", "content": "A key benefit of using code data for PMP is the vast availability of publicly accessible, high-quality code-preference pairs, ensuring diversity. To validate scalability, we vary the number of training pairs for CodePMP and retrain models with different amounts of data. As shown in Figure 5, overall, increasing the number of code-preference pairs consistently improves BoN accuracy in both mathematical and logical reasoning tasks across model sizes, with no sign of diminishing returns. This indicates that further scaling the code-preference data would likely yield additional performance gains, underscoring the importance of building a scalable PMP pipeline."}, {"title": "5 Ablation Studies", "content": "In this section, we present a detailed analysis of CodePMP design. Unless otherwise stated, all experiments used the 1B model due to resource limitations and present the results of mathematical reasoning due to page limitation. Results of logical reasoning refers to Appendix E.2."}, {"title": "5.1 Impact of Pair Construction", "content": "Model-Generated Pairs Comparison We compare various pair construction methods generated by different models. In Figure 6a, the samples before the \"&\" are positive, and those after are negative. \"Source Code\" refers to the original code snippet, while \u201c1.3B-Des-Clip\" indicates that 10% of the code description is removed before being input into a 1.3B CodeLLM to generate a rejected response. The green lines represent CodePMP's choice. Results show that pairing positive samples from the 7B model with negative samples from the 1.5B model consistently delivers the best performance across all test sets. Given that code execution can generate reliable outputs, future work will explore incorporating execution feedback to create more accurate preference pairs.\nWeb-Crawled vs GitHub-Sourced Pairs We also compare GitHub-sourced code with web-crawled data( Askell et al. (2021)) from platforms such as StackExchange and Reddit. As shown in Fig- ure 7a, GitHub-sourced pairs (\u201cSource Code\u201d) consistently outperform those from web platforms (\"Webpage\"), particularly as the number of solutions (N) increases. Moreover, the performance improvement of GitHub-sourced pairs shows no sign of plateauing, highlighting the importance of diverse, high-quality source code in building a scalable PMP pipeline."}, {"title": "5.2 Impact of EOC Token", "content": "Experiments by( Askell et al. (2021)) show that adding an end-of-context (EOC) token to each sequence significantly improves overall performance. To explore its impact in the context of CodePMP, we compared performance with and without the EOC token. As shown in Figure 7a, the EOC setting"}, {"title": "5.3 Impact of Learning Rate Schedulers", "content": "In the CodePMP experiments, we use the warmup-stable-decay (WSD) learning rate scheduler( Hu et al. (2024)), which can effectively reduce the time required for scaling related experiments. Previous studies mainly employ a learning rate schedule with linear warmup followed by cosine decay, known as warmup-cosine decay (WCD). We compare the performance of WSD and WCD, as shown in Table 7b, both schedulers yield similar results. Thus, to improve computational efficiency, we adopt the WSD scheduler for all experiments."}, {"title": "5.4 Validating CodePMP on Other LLMs", "content": "To further evaluate the generalizability of CodePMP, we validate its performance on the widely adopted Gemma-2B model (Team et al., 2024). As illustrated in Figure 8, the application of CodePMP results in significant performance gains in both mathemantical reasoning and logical reasoning evaluations. This not only underscores the robustness of CodePMP but also demonstrates its broad applicability in improving sample efficiency and overall performance across diverse LLM architectures."}, {"title": "5.5 Performance on Coding and General RM Benchmarks", "content": "We evaluate CodePMP on both code-specific and general reward modeling benchmarks. The CodeUl- traFeedback_binarized test set serves as an in-domain evaluation, while RMBench provides an out-of-domain assessment. As shown in Table 2, models finetuned with CodePMP initialization con- sistently outperform those without CodePMP across various model sizes. These results demonstrate that CodePMP not only enhances performance in reasoning tasks but also generalizes well across a range of RM benchmarks."}, {"title": "6 Related Works", "content": "Reward Modeling In the context of RLHF, reward models (RMs) have traditionally employed ranking models like Bradley-Terry and Plackett-Luce to represent human preferences (Bradley & Terry, 1952; Plackett, 1975; Cobbe et al., 2021b; Saunders et al., 2022; Lightman et al., 2023b; Wang et al., 2023b; Uesato et al., 2022; Luo et al., 2024; Yu et al., 2024; Stiennon et al., 2020; Nakano et al., 2021). More recently, probability-based approaches (Zhao et al., 2023; Jiang et al., 2023) have emerged, offering more precise predictions. Additionally, models such as Critique-out- Loud (Ankner et al., 2024) enhance RMs by integrating natural language feedback. Generative reward models (GRMs) (Yang et al., 2024b) further boost sample efficiency. Preference Modeling Pretraining (PMP) (Askell et al., 2021) introduces a novel pretraining phase, utilizing large-scale pairwise ranking data to enhance RM performance. Despite these advancements, many methods are hindered by the reliance on expensive manual annotations or limited datasets, constraining scalability. CodePMP mitigates this by automating preference data generation from code, significantly improving RM sample efficiency and reducing dependency on manual data collection.\nCode Training The inclusion of code in LLM pretraining has led to marked improvements in tasks such as commonsense reasoning (Madaan et al., 2022) and mathematical problem-solving (Liang et al., 2022; Shao et al., 2024; Yang et al., 2024a). Furthermore, code enhances general reasoning capabilities (Muennighoff et al., 2023; Fu & Khot, 2022; Ma et al., 2023). Recent studies (Dong et al., 2023; Ma et al., 2023) indicate that incorporating code during supervised finetuning strengthens LLMs, particularly in complex decision-making tasks. CodePMP takes a pioneering approach by utilizing scalable, synthetically generated code preference pairs, reducing the dependence on manual annotation (Dubey et al., 2024; Gemini-Team et al., 2024; Groeneveld et al., 2024; Bi et al., 2024). This methodology enhances sample efficiency and scalability in reasoning-intensive tasks, presenting new opportunities for further improving LLM performance.\nLLM Reasoning Improving reasoning capabilities in LLMs remains a significant challenge, with various advanced methods being proposed. Chain of Thought (CoT) prompting (Wei et al., 2022; Fu et al., 2023) improves reasoning by generating intermediate steps, while CoT combined with supervised finetuning (SFT) further enhances performance (Cobbe et al., 2021a; Liu et al., 2024; Yu et al., 2023). Other approaches focus on expanding inference time computation, such as problem decomposition (Zhou et al., 2022), search-based methods like MCTS (Xu, 2023), and using LLMs as verifiers (Huang et al., 2022; Luo et al., 2023). Reward models, including outcome-based (ORM) and process-based (PRM), have also shown success, with PRM delivering superior results (Lightman et al., 2023a; Wang et al., 2023b). Distinct from these approaches, CodePMP introduces a scal- able preference model pretraining phase that can integrate seamlessly with all the aforementioned techniques."}, {"title": "7 Conclusion & Future Works", "content": "This paper introduces CodePMP, a scalable pretraining approach that leverages code-preference pairs to improve reasoning capabilities in large language models. Experimental results validate that CodePMP significantly improves sample efficiency and boosts performance on reasoning tasks.\nFor future work, we aim to extend CodePMP in two key directions. CodePrMP will focus on utilizing compiler and interpreter verifiability to provide low-cost process supervision signals. GenPMP will explore how to improve sample efficiency and the performance of generative reward models by integrating code data."}]}