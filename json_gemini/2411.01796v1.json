{"title": "Constrained Human-AI Cooperation: An Inclusive Embodied Social Intelligence Challenge", "authors": ["Weihua Du", "Qiushi Lyu", "Jiaming Shan", "Zhenting Qi", "Hongxin Zhang", "Sunli Chen", "Andi Peng", "Tianmin Shu", "Kwonjoon Lee", "Behzad Dariush", "Chuang Gan"], "abstract": "We introduce Constrained Human-AI Cooperation (CHAIC), an inclusive embodied social intelligence challenge designed to test social perception and cooperation in embodied agents. In CHAIC, the goal is for an embodied agent equipped with egocentric observations to assist a human who may be operating under physical constraints-e.g., unable to reach high places or confined to a wheelchair-in performing common household or outdoor tasks as efficiently as possible. To achieve this, a successful helper must: (1) infer the human's intents and constraints by following the human and observing their behaviors (social perception), and (2) make a cooperative plan tailored to the human partner to solve the task as quickly as possible, working together as a team (cooperative planning). To benchmark this challenge, we create four new agents with real physical constraints and eight long-horizon tasks featuring both indoor and outdoor scenes with various constraints, emergency events, and potential risks. We benchmark planning- and learning-based baselines on the challenge and introduce a new method that leverages large language models and behavior modeling. Empirical evaluations demonstrate the effectiveness of our benchmark in enabling systematic assessment of key aspects of machine social intelligence. Our benchmark and code are publicly available at https://github.com/UMass-Foundation-Model/CHAIC.", "sections": [{"title": "1 Introduction", "content": "Humans possess a remarkable ability to observe, infer, and help others, even when others have different mental models and physical constraints in the world from themselves (Warneken and Tomasello, 2006). From a young age, humans are able to watch other people attempt to perform a task, and if other people fail, they can develop plans of action that best assist them. In contrast, AI agents struggle to exhibit such basic social skills and fail to adjust their plans for the specific humans they wish to aid (Valmeekam et al., 2022; Ngo et al., 2022), rendering them poor personalized helpers.\nFor AI agents to best assist human partners in performing tasks in the real world, they must possess two fundamental capabilities: (1) contextual perception, i.e., the ability to follow and observe human behavior and identify the specific goals and constraints faced by each human; and (2) cooperative planning, i.e., the ability to plan actions that are best tailored to helping each human with different goals and constraints. While there have been some embodied benchmarks and environments designed to test general multi-agent intelligence (Puig et al., 2021, 2023b; Gan et al., 2021), such efforts have largely excluded the unique accessibility challenges that real humans may possess in the world and neglect the differences among individuals. Moreover, outdoor scenarios and emergencies are also prevalent in human life, but receive little attention in the embodied intelligence community (Deitke et al., 2022).\nThis paper introduces the first large-scale embodied social intelligence challenge with accessibility explicitly in mind: Constrained Human-AI Cooperation (CHAIC). In this challenge, an embodied agent with egocentric visual observation must actively perceive and cooperate with a human partner possibly with physical constraints in a near photo- and physically realistic virtual environment to complete common household and outdoor tasks as efficiently as possible. This is motivated by the idea that people who need the most help from autonomous agents are those who are currently not explicitly accounted for in embodied intelligence frameworks. In CHAIC, a helper agent needs to follow and observe the human partner to infer their goals and constraints; then, the agent plans a user-tailored strategy for aiding the human in efficiently performing tasks together; moreover, with the existence of unexpected emergencies, the agent needs to be reactive and adjust its strategy accordingly.\nTo create the challenge with accessibility in mind, we design and implement four new agents with real physical constraints that reflect the rich diversity of human partners in the real world. For example, a human partner confined to a wheelchair struggles to move past obstacles or a human partner struggles with heavy furniture when moving house in an outdoor scene, shown in Figure 1, and eight long-horizon tasks featuring both indoor and outdoor scenes on top of the ThreeDWorld (Gan et al., 2021), explicitly motivating the development of embodied agents that prioritize accessibility efforts when learning and planning and can thrive in rich scenarios.\nWe benchmark several baseline models, including planning- and learning-based agents, especially those powered by foundation models. We also introduce a new method for building agents that combines the behavior modeling capabilities of video models with the reasoning ability of large language models. Our benchmark results suggest that current baselines have difficulty modeling partner behaviors from raw RGB images, and LLM-driven agents are competitive agents in decision-making. We hope this new challenge will advance the study of social intelligence in embodied agents in complex scenarios including diverse human partners with constraints and rich indoor and outdoor scenes. This initiative calls on the community to develop and evaluate embodied agents with a strong emphasis on accessibility and inclusivity.\nOur contributions include:\n\u2022 We design and implement four new agents with real physical constraints and eight long-horizon tasks featuring both indoor and outdoor scenes on top of ThreeDWorld (Gan et al., 2021), simulating rich human constraints and scenarios in the real world.\n\u2022 We introduce a new embodied social intelligence challenge with accessibility explicitly in mind: Constrained Human-AI Cooperation (CHAIC), to test embodied agents' ability to actively perceive"}, {"title": "2 Related Work", "content": "Embodied Multi-Agent Cooperation Challenges Our benchmark and environment build on a rich history of realistic 3D simulated environments (Zhou et al., 2024; Li et al., 2023; Padmakumar et al., 2022; Kolve et al., 2017; Shridhar et al., 2020; Misra et al., 2018; Zhu et al., 2017; Xia et al., 2018; Savva et al., 2019; Xiang et al., 2020). Various tasks and methods have been introduced for multi-agent cooperation (Lowe et al., 2017; Samvelyan et al., 2019; Carroll et al., 2019; Suarez et al., 2019; Jaderberg et al., 2019; Amato et al., 2019; Baker et al., 2020; Bard et al., 2020; Jain et al., 2020; Puig et al., 2023b; Wen et al., 2022; Szot et al., 2023; Zhang et al., 2023, 2024). Specifically, Puig et al. (2021, 2023a) explored the inter-agent perception of isomorphic agents during household tasks. However, these works did not address the explicit challenge of actively perceiving diverse human partners with physical constraints from visual observations and adapting the cooperation strategy accordingly. In contrast, our challenge is designed explicitly not only to study the social perception of the partner's goals and constraints from visual observations but also to capture the nuances of human physical mobility constraints that might impair the successful completion of such tasks. A contemporary work (Cao et al., 2024) also studies assistive agents for vulnerable groups but focuses only on indoor scenarios with oracle symbolic observations. In contrast, our proposed CHAIC Challenge features both indoor and outdoor scenarios, an egocentric visual observation, newly created physically constrained agents, and unexpected events, enabling rich, physics-driven interactions on real-world assistive tasks.\nAccessibility in AI Design People with disabilities or physical impairments are a central focus area of study in robotics, including care for wheelchair users, elderly users, and users with aging-related ailments like dementia (de Saille et al., 2022; Sundaresan et al., 2022; Broadbent et al., 2009; Benda et al., 2020; Cooper et al., 2016; Lee et al., 2017). These works often study the best ways to design for inclusivity; in other words, how to best build assistive robots to handle the explicit physical needs of the users in question (Benda et al., 2020). We build on these design principles to create the first-ever large-scale embodied intelligence environment that explicitly models such impairments."}, {"title": "3 The Constrained Human-AI Cooperation (CHAIC) Challenge", "content": "The Constrained Human-AI Cooperation (CHAIC) Challenge seeks to study how embodied agents perform in terms of social perceptions of human partners with diverse physical constraints and cooperative planning abilities within rich scenarios. Built on top of ThreeDWorld, a realistic 3D embodied AI platform, we design and implement four new agents with real physical constraints (Section 3.1) and eight tasks featuring both indoor and outdoor scenes, including emergencies (Section 3.2). For each task, there is a constrained agent mimicking a human partner with capability constraints, trying to find and transport some target objects to a specific goal location, and a helper agent trying to infer the constrained agent's goal and capability constraints through active perception of its behaviors to assist the constrained agent better. The success of the helper agent is measured by the ratio of target objects successfully transported by both of them. Figure 2 provides an overview of the challenge, with further details in Section 3.3."}, {"title": "3.1 Constrained Agents", "content": "To enable the testing of embodied social intelligence with a diverse set of potential human partners, we have created four new simulated agents that may face physical constraints such as limited height, strength, and movement speed, reflective of real humans.\nEach agent possesses two properties: reaching range and strength. An agent can successfully interact with objects whose heights are within its reaching range and whose weights are lighter than its strength limit. When an agent attempts an action that exceeds its capabilities, the action does not fail immediately but instead has a success rate. This rate is calculated using the formula $\\exp(-\\delta/\\alpha)/\\beta$, where $\\delta$ represents the excess amount, and $\\alpha$ and $\\beta$ are constants. If an action exceeds multiple capability thresholds, the probabilities of success are multiplied. We have developed the following constrained agents:\n\u2022 Child Agent: A small child with a height of 1.2 m that has a reaching range of [0, 1.5] m.\n\u2022 Wheelchair Agent: An agent confined to a wheelchair or limping that may be blocked by obstacles in the house (e.g., a couch). Its reaching range is [0.25, 1.5] m.\n\u2022 Bicycle Agent: An agent walking with a bike that moves slowly. It must first dock the bike when picking up an object. The child accompanying it may run away, causing an emergency.\n\u2022 Frail Agent: An agent that is less capable of lifting heavy objects (e.g., furniture) and has only 1/6 the strength of a normal agent."}, {"title": "3.2 Tasks with Constrained Agents", "content": "We designed eight tasks featuring indoor and outdoor scenes, including emergencies, in our CHAIC benchmark, utilizing the various constrained agents introduced earlier. Information about each task is shown in Table 1."}, {"title": "3.3 Challenge Details", "content": "In CHAIC, an embodied helper agent $A_h$ is tasked to infer the goal $G$ and the constraints of a constrained agent $A_m$ and assist $A_m$ in finding and transporting a set of target objects $O_t$ from random locations to a goal location $L_g$. There are containers scattered in the environment, which the agents can use to transport more objects simultaneously. An agent could take two objects at a time without a container, and the capacity of a container is set to three.\nFormally, a task in the challenge is defined by the goal $G$ of the constrained agent $A_m$ (i.e., a set of goal predicates describing the final desired state) and an initial environment $E$ where the helper agent $A_h$ is placed alongside the constrained agent $A_m$ to complete the task. The ground truth goals and constraints of the constrained agent are hidden from the helper agent $A_h$, thereby explicitly motivating the need for active perception for the agent to infer intents and constraints."}, {"title": "Observation Space", "content": "In CHAIC, actions may take several frames to finish and are executed asynchronously between agents. The agent will receive the following observation after his action is finished or failed:\n\u2022 Egocentric RGB-D Image: The agent receives 512 \u00d7 512 egocentric color and depth images, as shown in Figure 2.\n\u2022 Self-State: The agent is provided with information relevant to itself, including its current location, orientation, and the objects in its possession."}, {"title": "Action Space", "content": "The action space consists of three low-level navigation actions (move forward, turn left, turn right), three basic interaction actions (pick up A, put A in B, put A on B), and one idle action (wait)."}, {"title": "3.3.1 Task Generation", "content": "Indoor Task To generate an indoor task, a floorplan configuration with six to eight interconnected rooms and a target task is initially sampled from predefined sets. For each scene, objects related to goals in the predefined set are placed on low surfaces such as tables, chairs, sofas, and floors for low objects, and higher surfaces like cabinets or refrigerators for high objects. However, only a subset of the objects is the target object set. The target object set is a set that includes all objects related to a specific type like food or fruit, one object randomly selected from a non-target set, and two additional fragile vases if the task is a high-target task. The number of targets is around ten. Then, a goal location and up to six containers are added to the scene based on available space and task constraints.\nWe randomly initialize two agents (one constrained agent $A_m$ and one helper $A_h$), and each agent is placed in a free space at least 0.5 meters away from the nearest wall. This setup ensures sufficient initial distance between the agents and the walls, allowing unrestricted movement at the beginning of the task.\nOutdoor Task The generation of outdoor tasks is largely the same as the indoor task generation. For the shopping task, six shops are generated and spread out on both sides of the road, and each shop sells one specific category of items. The goal location of the shopping task is a fixed, predetermined place in front of the bicycle agent's house. In the moving house task, the target objects include five pieces of furniture on the road in front of a house. The goal location is a truck parked nearby. The details of outdoor task generation can be found in Appendix G."}, {"title": "4 Language Agent Augmented with Behavior Modeling Module", "content": "We also introduce a new agent framework combining the prowess of action recognition models and the reasoning ability of large language models (LLMs). Due to their simplicity and generalization ability, LLMs can also be implemented in other environments or the real world. We built a behavior modeling module, which models the behaviors of the constrained agent via an action recognition model and incorporated it into the CoELA framework (Zhang et al., 2023) with four other modules: (1) the perception module, which transforms the raw RGB-D observations into structured semantic maps via an object detection model; (2) the memory module, which saves all the history information in a structured manner; (3) the decision module, which generates high-level plans and is driven by large language models; and (4) the execution module, which turns the generated plans into low-level actions. More details regarding these modules can be found in Appendix B.1. Figure 3 shows an overview of the framework."}, {"title": "4.1 Behavior Modeling Module", "content": "To infer the intents and inabilities of constrained agents, the behavior modeling module extracts constrained agents' actions and status from a sequence of egocentric images (i.e., a video). The behavior modeling module contains two parts: action recognition and action grounding.\nAction Recognition We adopt an action recognition model to enable the helper agent to recognize the actions of the constrained agent. We select the TSN model (Wang et al., 2016) pretrained on Kinetics-400 (Kay et al., 2017) as the base video action recognition model. There are four types of actions: pick up, put on, put in, and walking (including move forward, turn left, and turn right). Each action may be successfully executed or fail (except for put in and walking, which are always"}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Setup", "content": ""}, {"title": "5.1.1 Constrained Agent Policy", "content": "The constrained agent takes ground truth object segmentation as observation to mitigate the impact of imperfect visual perception on performance and chooses actions based on a rule-based high-level planner designed with handwritten rules by human experts.\nAt the beginning of an episode, the constrained agent will explore the environment to find more target objects, containers, and the goal location. Whenever the agent finds target objects or containers, it will pick them up if it has free hands. If more than 50% of the time steps are left and it does not have a container in hand, its priority will be to pick up a container; otherwise, it will pick up a target. If the agent cannot carry more objects, it will put the object on the goal location. If less than 25% of the time steps is left (37.5% if it has not found the goal location yet) and the agent is carrying a target object, it will put the object on the goal location immediately since it is often a long walk to the goal location. When selecting possible targets, the agent will opt for the closest one if multiple options are available. Moreover, at any time, if the agent can put an object in a container, it will do so."}, {"title": "5.1.2 Evaluation Metrics", "content": "To evaluate the success of helper agents, we measure the following three metrics:\n\u2022 Transport rate (TR): The percentage of target objects that the agents successfully transported. We also calculate the Efficiency Improvement (EI) of having the helper as $A_M / M_0$, where $A_M$ denotes the increase in the transport rate after adding the helper, and $M_0$ denotes the larger of the transport rates of the team or the constrained agent alone, for numerical stability.\n\u2022 Goal Inference Accuracy (IA): The ratio of target objects successfully transported by the helper to the total number of objects transported by the helper.\n\u2022 Emergency Rate (ER): For the shopping task, we calculate the ratio of frames where the child agent is away from the constrained agent to measure the helper agent's ability to handle emergencies."}, {"title": "5.2 Baselines", "content": "We test four types of planning-based helpers: Random Helper, Rule-Based Hierarchical Plan Helper (RHP), LLM+BM Helper, and VLM Helper. All the helpers share the same Perception Module, Memory Module, and Execution Module as the language agent introduced in Section 4, but the critical differences lie in the high-level planner. Meanwhile, an Oracle Helper is tested to demonstrate the upper-bound performance. Below is the description of each type of helper:\n\u2022 No Helper (w/o): The constrained agent performs the task solely without assistance from a helper.\n\u2022 Random Helper: A naive helper randomly selects a plan from a list of valid plans.\n\u2022 Rule-Based Hierarchical Plan Helper (RHP): This helper uses prior knowledge of the task and relies on handcrafted rules by human experts to make plans to assist the constrained agent in completing the task. Further details on the rules can be found in Appendix B.2."}, {"title": "5.3 Main Results", "content": "We conducted an extensive evaluation by deploying four baseline models across eight distinct constraint settings and measured four specific metrics, as outlined in Section 5.1.2. The results are presented in Table 2. Overall, the LLM+BM Helper emerges as a strong baseline, achieving the highest transport rate (TR) in 6 out of 8 tasks, the most significant efficiency improvement (EI) in 7 out of 8 tasks, and the best goal inference accuracy (IA) in 4 out of 8 tasks.\nBehavior Modeling Analysis Our LLM+BM Helper achieves a reasonable IA metric compared with other helpers, which shows our behavior model successfully models the partner's behaviors to some extent. However, compared with the Oracle Helper, all the other baseline agents perform poorly on the IA metric. The IA metric reflects whether the helper successfully determines the needs of the constrained agent, so the gap shows all our baselines do not work well in inferring the behavior of the constrained agent from the raw RGB-D image sequence. Nevertheless, our fine-tuned action recognition model achieves 86% accuracy on the validation set (See Appendix D.2 for the action recognition model details). Two reasons contribute to the discrepancy: (1) Due to blocking or distance, the action clip received by the helper may be incomplete or out-of-distribution from training data. (2) The current LLM-based decision module is insufficient to balance observing the partner's behavior and acting independently."}, {"title": "LLM Can Infer Goals Correctly and Perform Actions Properly", "content": "In analyzing some of the chain-of-thought outputs of LLM, we observe that the LLM-based helper can accurately infer the target objects desired by the constrained agent and formulate appropriate plans to collect them. For instance, in an outdoor shopping scene, the bike agent named David seeks some fruit. Initially, the LLM helper assesses, \u201cSince David hasn't picked any object yet, it's challenging to precisely determine his target objects.\" It then realizes, \u201cNo matter what object David wants, the best first step would be to maximize the efficiency of carrying objects by using a container,\u201d and subsequently proceeds to pick up a container. Upon observing the bike agent picking an apple, the LLM helper deduces, \"Considering the constraints and the objects David has shown interest in (i.e., an apple), the best course of action from the provided list would be to \u2018goto and pick up target <apple>'.\u201d With a container and a target object in both hands, the LLM helper notes, \u201cConsidering I am currently holding two target objects (one directly and one in a container), the optimal next action is to put the object in your one hand to the container in your other hand. This action will free up one of my hands, allowing me to pick up more target objects and transport them efficiently to the goal.\"\nMeanwhile, the LLM helper is capable of picking other fruits besides apples, demonstrating its accuracy in inferring the object category. After freeing up one hand, the LLM helper states, \u201cBased on the observed actions and status of David, it's clear that his target objects are fruits, specifically apples...so picking up more grapes aligns with the goal.\u201d Finally, after collecting several fruits and having both hands full, the LLM helper concludes, \u201cthe best action to take next is to \u2018transport object in hand to goal space'. This action involves taking the container filled with target objects, along with the additional grape in the other hand, to the specified goal location.\u201d The detailed analysis of these chain-of-thought outputs is shown in Appendix F.1."}, {"title": "Dealing with Emergencies", "content": "In outdoor shopping tasks, the helper needs to handle unpredicted emergencies, requiring swift responses. The Emergency Rate (ER) metric shows that even if LLM- and VLM-based helpers can achieve high scores in normal tasks, they cannot handle emergencies as efficiently as RHP. To improve, some rule-based control may be required in LLM- and VLM-based helpers to help them prioritize and respond more effectively in urgent situations."}, {"title": "Failure Case Analysis", "content": "During the experiment, we discovered some common failure situations leading to poor performance, which might be helpful for further helper design.\n\u2022 Spatial Information Analysis: The LLM-based agents do not understand spatial information very well when provided with text inputs of object locations. They often choose a distant object rather than a nearby one, even if they share the same name. Additionally, they often underestimate the cost of reaching the goal location and fail to transport due to time limits.\n\u2022 Acting without Cooperation: In the obstacle task, a reasonable solution for the helper is to remove obstacles first to free the constrained agent. However, LLM- and VLM-based helpers often transport objects alone without assisting the constrained agent, leading to relatively bad performance in this task.\n\u2022 VLM is Unable to Infer the Targets Needed by Constrained Agents: In certain tasks, the VLM Helper baseline performs worse than both the random baseline and the No Helper baseline. This is primarily because VLM cannot accurately infer the preferred objects of constrained agents when they observe them picking up items, leading it to consistently follow. Consequently, the VLM Helper fails to transport any objects, making it less effective than randomly transporting some objects, as done by the random helper. Additionally, frequently following the constrained agent can interfere with their actions\u2014such as blocking their path\u2014resulting in the VLM Helper baseline sometimes performing worse than having no helper. However, the LLM+BM Helper transports some objects even if it does not infer the goal correctly from the BM model, achieving a relatively higher score than the VLM Helper."}, {"title": "6 Conclusion", "content": "In this work, we proposed an accessibility-centered embodied social intelligence challenge: the Constrained Human-AI Cooperation (CHAIC) Challenge. This challenge includes four new agents with physical constraints and eight long-horizon tasks featuring both indoor and outdoor scenes, designed to test the critical skills of social perception and cooperation in embodied agents. Our experimental"}, {"title": "Limitations", "content": "While we aimed to preserve as much realism as possible, there are undoubtedly aspects of human behavior, particularly in how physical constraints manifest in the world, that are challenging to simulate. Meanwhile, the rule-based control of constrained agents makes their behavior lack diversity. This may be solved by leveraging LLMs to control constrained agents. Moreover, while we believe our challenge takes a good first step forward in introducing accessibility challenges to embodied social intelligence benchmarking efforts, we emphasize that our challenge is not representative of all possible constraints that such users may face."}, {"title": "A More Information about the CHAIC Challenge", "content": ""}, {"title": "A.1 Benchmark Usage", "content": "The CHAIC Challenge can be accessed on the CHAIC GitHub as well as on its official webpage."}, {"title": "A.2 Comparison with Other Embodied Challenges", "content": "We compare the differences between our proposed challenge and others in Table 3. Our CHAIC Challenge represents the first large-scale embodied social intelligence challenge focused on accessibility, incorporating outdoor scenes with emergent events, and requires goal inference from observations."}, {"title": "B Baseline Details", "content": ""}, {"title": "B.1 More Details on LLM+BM Helper", "content": "We have tested several baselines in the benchmark, and the LLM+BM helper has shown the best performance. The LLM+BM helper consists of multiple modules. In addition to the behavior modeling module discussed in the main paper, the helper has several other modules, which we introduce here:\nPerception Module The perception module is used to extract useful information from raw RGB-D images. Following (Zhang et al., 2023), we have fine-tuned a Mask R-CNN (He et al., 2017) model on the images collected in the training dataset to obtain object-wise segmentation masks. The training set contains the same kinds of objects but with different layouts and scene backgrounds compared to the test set. The training details are in Appendix D.1.\nMemory Module The memory module is designed to understand the environment's layout and the positions of objects by an occupancy map and a semantic map. Firstly, the agent continuously updates a 2D grid-based top-down occupancy map when executing actions. Initially, all areas on the occupancy map are marked as unknown, and the map is updated using depth images. Utilizing depth images and camera intrinsics, the memory module first maps each pixel from the depth image into 3D space and then projects it onto the occupancy map. The semantic map, which also adopts a top-down view and maintains the same grid size as the occupancy map, records the locations of all detected objects within the grid.\nDecision Module An LLM-based decision module is used to generate a subgoal without any prior knowledge or specific design. The prompt encompasses six components: task description, self-information, information about other agents, task progress, semantic map information, and available plans. The LLM's output should be a plan from the list of valid plans, with specific object IDs included if the plan involves actions like \"pick up\" or \"put on\". Detailed information about the prompt is available in Appendix C.1.\nExecution Module The execution module is a low-level executor, which serves as a low-level executor, bridging the gap between high-level plans and low-level actions, including navigation and exploration. When the high-level plan directs the picking up of a previously seen object or traveling"}, {"title": "B.2 Detailed Rules for Rule-Based Hierarchical Plan Helper (RHP)", "content": "The rule for Rule-Based Hierarchical Plan Helper (RHP) is similar to that of the constrained agent described in Section 5.1.1. The only difference is that since the helper does not know the exact goal of the constrained agent, the ruled-based agent will choose the target object randomly among all available objects.\nIn detail, at the start of an episode, the rule-based agent explores the environment to locate objects, containers, and the goal location. It picks up objects or containers if its hands are free. If over 50% of the time steps remain and the agent does not obtain a container, it prioritizes acquiring one; otherwise, it focuses on objects. When unable to carry more, it deposits objects at the goal location. If less than 25% of the time steps is left (or 37.5% without a goal location identified), it immediately places objects in hands at the goal. The rule-based agent chooses the nearest object when multiple are available and puts objects in containers whenever possible."}, {"title": "C Prompt Details", "content": ""}, {"title": "C.1 Detailed Prompt of Decision Module of LLM+BM Helper", "content": "The LLM+BM Helper uses an LLM-based decision module to determine the plan for the next step. The prompt of the LLM-based decision module contains six parts: task description, self-information, information about other agents, task progress, semantic map information, and available plans. The decision module needs to select a plan and fill in the plan with proper parameters, and then the execution module will execute the plan. Following are prompt descriptions and examples of each part:"}, {"title": "C.1.1 Task Description", "content": "The Task Description includes a detailed description of the task's basic rules but does not explicitly show the constraints of the constrained agent. A prompt example is listed in Figure 4."}, {"title": "C.1.2 Self-Information", "content": "The Self Information contains information about the helper agent himself, including the previous actions of the helper with the statuses of these actions, the current position of the helper, and the objects that the helper is currently holding. A prompt example is listed in Figure 5."}, {"title": "C.1.3 Information about Other Agents", "content": "Information about Other Agents contains information about other agents, including the actions of the constrained agent that the helper has seen, together with their statuses, the objects that the helper has seen the constrained agent holding, the position of the constrained agent when the helper last saw him/her. For shopping tasks, it also includes the position of the child when the helper last saw her. A prompt example is listed in Figure 6."}, {"title": "C.1.4 Task Progress", "content": "The Task Progress contains all the objects that have been transported to the goal location, and the number of frames passed. A prompt example is listed in Figure 7."}, {"title": "C.1.5 Semantic Map Information", "content": "The Semantic map information contains all objects, containers, and the goal location information in the semantic map, with their position and height. A prompt example is listed in Figure 8."}, {"title": "C.1.6 Available Plans", "content": "The Available Plans contains all the available plans that the helper agent can take. A prompt example is listed in Figure 9."}, {"title": "C.2 Detailed Prompt of Decision Module of VLM Helper", "content": "The prompt of the decision module of VLM Helper is similar to that of LLM+BM Helper. The only difference is the VLM-based decision module perceives the behavior of the partner through raw RGB images as an additional observation. In detail, an image sequence with the last ten images is added to the input of the VLM-based decision module."}, {"title": "D Perception Model Details", "content": ""}, {"title": "D.1 Detection Model for Object Detection", "content": "Since the helper receives raw RGB-D images from the environment, an object detection model is necessary to identify objects within these images. We fine-tuned an object detection model using our dataset collected from training scenes.\nData Collection To collect training data in the environment, a helper roams randomly and solely within the scenes, collecting egocentric images combined with ground truth segmentation. The environment is split into training and validation, and we collected 61K images at a resolution of 512x 512 in total. There are 53 types of objects related to the benchmark, so the detection model has the same number of labels.\nTraining Details We utilized the open-source code provided by MMDetection (Chen et al., 2019) as our training framework and selected a Mask R-CNN (He et al., 2017) model pre-trained on the COCO dataset (Lin et al., 2014) with a ResNet50 (He et al., 2016) backbone. The model was fine-tuned for four epochs, incorporating a warm-up stage of 500 steps and a batch size of 16. The optimizer employed was SGD with lr = 0.01, momentum = 0.9, and weight_decay = 0.0001. This fine-tuning process was finished on an NVIDIA A10G GPU in approximately six hours. The fine-tuned model achieved a 94.4% mAP@50 (Segmentation Mean Average Precision at 50% intersection over union) on the validation set."}, {"title": "D.2 Action Recognition Model for Behavior Modeling", "content": "Recognizing the actions of the partner is a crucial ability for understanding its intentions", "agent": "successful pick-up, fail pick-"}]}