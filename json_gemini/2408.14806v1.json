{"title": "Poly2Vec: Polymorphic Encoding of Geospatial Objects for Spatial Reasoning with Deep Neural Networks", "authors": ["Maria Despoina Siampou", "Jialiang Li", "John Krumm", "Cyrus Shahabi", "Hua Lu"], "abstract": "Encoding geospatial data is crucial for enabling machine learning (ML) models to perform tasks that require spatial reasoning, such as identifying the topological relationships between two different geospatial objects. However, existing encoding methods are limited as they are typically customized to handle only specific types of spatial data, which impedes their applicability across different downstream tasks where multiple data types coexist. To address this, we introduce Poly2Vec, an encoding framework that unifies the modeling of different geospatial objects, including 2D points, polylines, and polygons, irrespective of the downstream task. We leverage the power of the 2D Fourier transform to encode useful spatial properties, such as shape and location, from geospatial objects into fixed-length vectors. These vectors are then inputted into neural network models for spatial reasoning tasks. This unified approach eliminates the need to develop and train separate models for each distinct spatial type. We evaluate Poly2Vec on both synthetic and real datasets of mixed geometry types and verify its consistent performance across several downstream spatial reasoning tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "The ubiquity and unprecedented growth of diverse geospatial data generated by numerous sources, such as satellites, sensors, and mobile devices, have enabled their utilization by geospatial decision-making applications in various domains, including transportation, urban planning, intelligence, and public health. A fundamental building block for these decisions is the ability to perform spatial reasoning. Spatial reasoning involves the capability to interpret and analyze spatial relationships between objects in a given space. For example, it includes tasks such as classifying building patterns [1], inferring topological relations between geometries\u00b9 [13, 14], and retrieving the nearest neighbor of a geometry [11], among others. Meanwhile, machine learning (ML) and deep neural networks (DNN) are now extensively used for decision-making, and a crucial first step in leveraging them for geospatial decision-making is to represent geometries as fixed-size vectors, a process known as encoding. To this end, several specialized encoding techniques have been proposed recently for different spatial data types. For instance, Mai et al. [9] introduce a method for encoding points of interest into a d-dimensional vector representation using sinusoidal multi-scale functions. Similarly, t2vec [6] encodes trajectory data by projecting their coordinates into a grid, while Veer et al. [16] encode polygon vertices using an LSTM and CNN for polygon classification. Despite the effectiveness of these encoding techniques, they share a common limitation: each is specifically designed for a single spatial data type, thus tailored to capture spatial properties inherent to that data type. For example, point encoding models usually capture angle and location, while trajectory encoding models focuson capturing the sequential property. This specialization restricts their broader applicability across different spatial reasoning tasks involving mixed types of geospatial objects, such as topological relation classification and k-nearest neighbor (k-NN) search.\nTo effectively capture crucial spatial properties of mixed shape types necessary for spatial reasoning, such as location, shape, and area, of various geospatial objects, adopting a unified encoding approach is more advantageous. This approach simplifies the modeling process, eliminates the need for multiple customized models, and enhances the overall utility of spatial reasoning.\nOne potential way to achieve this is to use point-based encoding techniques to encode more complex geometries, treating them as sequences of points [8]. We use this approach as a baseline in our experiments in Section 4. Even though this solves the problem of handling different types of geospatial objects, it fails to preserve the intrinsic ordering that is crucial for defining the object's shape. Thus, by utilizing such a simple encoding, the downstream models can lose such essential information, leading to suboptimal performance in tasks that depend on precise spatial understanding.\nIn this work, we present Poly2Vec, a novel, adaptable encoding framework designed to handle a diverse range of geometries. The \"Poly\" in Poly2Vec means that our approach works for any 2D shape of dimension zero (points), one (polylines), and two (polygons). Our Poly2Vec leverages the Fourier transform to capture essential spatial information, standardizing their representation into uniform d-dimensional vectors. Specifically, Poly2Vec maps any 2D shape characterized by N real numbers denoted as $g \\in \\mathbb{R}^{2N}$, to a fixed-dimensional space $v \\in \\mathbb{R}^d$, where d is independent of N.\nThe elements of v are complex-valued samples from the shape's 2D Fourier transform. From v, the magnitude of each complex element captures the shape characteristics of the geometry, and the phase indicates the positional information. This is illustrated in Figure 1, which displays a park in California as a polygon in green, a nearby road as a polyline in grey, and a food store as a point in red. We can observe that the magnitude of each object highlights a distinct spatial extent; for example, the magnitude of the point is constant since it has no shape, while the magnitude of the polygon displays a more complex pattern, reflecting its larger size and complex shape. Similarly, the phase of the polyline, for instance, shows aligned diagonal stripes that suggest the road's directionality, while the phase for the polygon features a star-like pattern, illustrating the varied angles and corners of the park's boundary. Thus, both the magnitude and phase can provide useful information for spatial reasoning tasks.\nTo that extent, to effectively combine magnitude and phase, we propose a learned fusion module that adaptively combines the two properties into a single representation $v^* \\in \\mathbb{R}^{2d}$. This representation $v^*$ is inputted to a neural network model (in our case MLP) which is trained on spatial reasoning tasks.\nWe conduct thorough experiments on both synthetic and real datasets to verify the effectiveness of our encoding framework. Specifically, Poly2Vec is able to handle diverse geometries uniformly and outperforms the baselines in most settings. By effectively capturing both location and shape properties, Poly2Vec supports spatial reasoning tasks such as topological and distance relationships.\nIn sum, our contributions are:\n\u2022 We introduce Poly2Vec, a novel encoding framework that unifies the modeling of different 2D geometries, enabling the processing of mixed spatial data types.\n\u2022 We propose a 2D continuous Fourier transform-based encoding. Our encoding can capture important spatial properties, including the shape and location of diverse spatial objects, which are important for supporting spatial reasoning.\n\u2022 We propose a learned fusion strategy that can adaptively combine the magnitude and phase of geometry's Fourier features.\n\u2022 We evaluate Poly2Vec on distinct spatial reasoning tasks, showing its ability to uniformly handle different types of geospatial objects without compromising performance."}, {"title": "2 PRELIMINARIES", "content": "In this section, we present our problem definition and preliminary Fourier transform properties."}, {"title": "2.1 Problem Formulation", "content": "DEFINITION 2.1 (GEOSPATIAL OBJECT). A geospatial object $g \\in \\mathbb{R}^{2N}$ consists of a set of N coordinates, each defined as a point in space represented in two dimensions (x, y). Based on the organization and connection of these points, different geospatial objects can be defined:\n\u2022 Point: A single coordinate (x, y) representing a location in space.\n\u2022 Polyline: A sequence of connected coordinates $((x_1, y_1), (x_2, y_2), ..., (x_N, y_N))$, forming linear segments that do not close into a loop.\n\u2022 Polygon: A filled shape that is bounded by a closed polyline where a sequence of connected coordinates forms a loop, enclosing an area, defined as $((x_1, y_1), (x_2, y_2), ..., (x_N, y_N), (x_1, y_1))$.\nResearch Problem (Polymorphic Encoding of Geospatial Objects). Given an arbitrary type of geospatial object g, polymorphic encoding aims to project its original coordinates into a fixed-length vector that captures the object's spatial properties. This vector should be suitable for downstream tasks that require spatial reasoning."}, {"title": "2.2 2D Continuous Fourier Transform", "content": "An integral part of our polymorphic encoding is the computation of the 2D continuous Fourier transform. Here, we introduce its fundamental properties, which are taken from a textbook [3].\nThe Fourier transform of a 2D function f(x, y) is denoted as $F(u, v) = \\mathcal{F}{f(x, y)}$ and computed as\n$F(u, v) = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f(x, y)e^{-j2\\pi(ux+vy)} dx dy$ (1)\nwhere $j = \\sqrt{-1}$.\nOne important property of the Fourier transform is its linearity. This means that the Fourier transform of a sum of functions $f_i (x, y)$ is the sum of the corresponding Fourier transforms $F_i (u, v)$:\n$\\mathcal{F}{\\sum_{i=1}^{n} f_i(x, y)} = \\sum_{i=1}^{n} F_i (u, v)$ (2)"}, {"title": "3 POLY2VEC", "content": "In this section, we introduce our Poly2Vec framework, which is illustrated in Figure 2. Our approach begins with a geometry g. Initially, Poly2Vec applies a scaling transformation to normalize the coordinates of g to the unit space. Subsequently, a 2D continuous Fourier transform processes the normalized shape to capture the spatial characteristics of the geometry, resulting in a vector $v \\in \\mathbb{R}^d$. This vector consists of complex Fourier transform values sampled at discrete (u, v) coordinates. From the Fourier features, both the phase $\\nu_{\\varphi}$ and magnitude $\\nu_z$ are extracted. These properties are then adaptively combined using a learned fusion strategy to form a final representation $v^* \\in \\mathbb{R}^{2d}$. This representation serves as input to various MLP models, each trained for specific downstream tasks."}, {"title": "3.1 Scale Transformation", "content": "The first step of our pipeline involves the scaling transformation of the geospatial object g to fit within the space [-1,1] \u00d7 [-1,1]. This transformation ensures that each object is positioned within the same relative spatial domain, which is crucial for consistent learning across different types of geospatial objects.\nWe compute the scaling transformation for each coordinate pair $(x_i, y_i)$ in g using the minimum and maximum coordinates from the target area (e.g., California). Let $(x_{min}, y_{min})$ and $(x_{max}, y_{max})$ be the minimum and maximum coordinates of this area, respectively. The scaled coordinates (x', y') are then calculated as:\n$x' = 2 \\frac{x_i - x_{min}}{x_{max} - x_{min}} - 1$ (7)\n$y' = 2 \\frac{y_i - y_{min}}{y_{max} - y_{min}} - 1$ (8)\nThis operation can be formally expressed for g as:\n$g' = \\{(2 \\frac{x_i - x_{min}}{x_{max} - x_{min}} )-1, 2 (\\frac{y_i - y_{min}}{y_{max} - y_{min}} )-1 \\}_{i=1}^N$ (9)\nwhere $g' = \\{(x'_i, y'_i)\\}_{i=1}^N$ represents the scaled geospatial object."}, {"title": "3.2 Fourier Transform-based Encoding", "content": "3.2.1 Fourier Transform of Point. We represent a 2D point at $(x_0, y_0)$ as a 2D Dirac delta function $\u03b4(x \u2013 x_0, y \u2013 y_0)$, whose Fourier transform is\n$\\mathcal{F}{\\delta(x \u2013 x_0, y \u2013 y_0)} = e^{-j2\\pi(xu+y_0v)}$ (10)\nThe magnitude of the Fourier transform of any point is $z(u, v) = 1$, and thus, only the phase can present meaningful information for a point. This is also apparent in Figure 1b. The phase shows how the Fourier transform represents the location of the point. Notably, for the point, $\\mathcal{F}(0, 0) = 1$.\n3.2.2 Fourier Transform of Polyline. We now describe how to model a polyline to derive its Fourier features. Using the additive property of Fourier transform in Equation (2), it is sufficient to compute the Fourier transform of a single line segment and then sum the Fourier transforms of each segment in the polyline. Thus, we begin with the Fourier transform of a canonical line segment and then apply an affine transformation to model an arbitrary line segment.\nThe canonical line segment AB extends from (x, y) coordinates $(-\\frac{1}{2}, 0)$ to $(\\frac{1}{2}, 0)$, as illustrated in Figure 3. We model this line segment as $f_-(x, y) = rect(x)\u03b4(y)$, where the underbar subscript indicates the canonical line segment. The Dirac delta function $\u03b4(y)$ represents a ridge of delta functions extending along the entire x-axis, while rect(x) limits the extent of the ridge to our specified range on the x-axis. The rect(x) function is"}, {"title": "3.3 Selecting Frequencies", "content": "The encoded vector $v \\in \\mathbb{R}^d$ comes from sampling the complex 2D Fourier transform of the shape at a discrete set of sample frequencies. For sampling frequencies, a straightforward approach is to use a linear grid frequency map with equally spaced frequencies along the (u, v) dimensions. However, not all these frequencies are necessary due to the Hermitian symmetry property in Equation (4) and as depicted in Figure 1. To that extent, we can utilize half of the Fourier features to avoid redundant information. A more effective approach than geometric sampling was presented in a previous work [10], where the density of the sampled frequencies decreases as the distance from the center increases. To utilize a set of meaningful frequencies that also assist learning, we choose to adopt this geometric grid strategy, denoted as $W^{(gfm)}$. Due to limited space, the details of $W^{(gfm)}$ can be found in A.2."}, {"title": "3.4 Learned Fusion Strategy", "content": "The previous encoding process results in a d-dimensional vector v, composed of a vector of complex numbers. As we can observe from Figure 1, both magnitude and phase hold intrinsic information about the geometry. The magnitude provides insights into the shape of the geometry, capturing the amplitude variations within the encoded spatial data, and the phase conveys information about the orientation or direction of the geometry relative to the spatial origin, offering information about the spatial alignment and rotation of the shape components. To effectively utilize the Fourier features encoded within v, we utilize both the magnitude and phase of v following Equations (5) and (6), respectively.\nHowever, different types of geospatial objects have different spatial properties, implying different importance of magnitude and phase when representing them. For instance, as observed in Figure 1b, the magnitude of all points is always 1 since points do not have varying shapes, while the magnitude is vital for identifying different polygons (Figure 1d). Thus, simply concatenating magnitude and phase into a single feature vector without considering these differences would lead to representations that do not adequately capture the unique spatial characteristics of different geometries.\nInstead, we propose a learned fusion strategy that adaptively balances the contribution of magnitude and phase. Targeting the specific downstream task and object types at hand, we adjust the weights assigned to magnitude and phase during training as follows:\n$v^* = MLP_z(v_z)||MLP_{\\varphi}(v_{\\varphi})$ (22)\nHere, two separate MLPs, $MLP_z$ for magnitude ($v_z$) and $MLP_{\\varphi}$ for phase ($v_{\\varphi}$) are utilized to transform these features effectively. Each MLP has two fully connected layers and a ReLU activation function. The outputs from these MLPs are then concatenated to form a representation vector $v^* \\in \\mathbb{R}^{2d}$."}, {"title": "3.5 Model Training and Downstream Tasks", "content": "The encoded vector $v^*$ can then be forwarded as input to a downstream model trained to perform a spatial reasoning task. In our case, the downstream model is an MLP, with two fully connected layers followed by a ReLU activation function. The output of the model is a geometry embedding $e \\in \\mathbb{R}^m$. Formally,\n$v^* \\in \\mathbb{R}^{2d} \\xrightarrow{2-layer \\ MLP} e \\in \\mathbb{R}^m$\nNext, we will detail the downstream tasks for which the model is trained and outline the training methodology.\n3.5.1 Topological Relationship. Determining the spatial relation between two geometries $g_1$ and $g_2$ can be crucial for spatial reasoning. For instance, understanding whether essential amenities like coffee shops or gyms are located within a residential area, or if a specific road intersects a park, relies on this analysis. In our framework, two geometries are classified as related if they intersect. Specifically, for points, a point is considered related to a polyline if it lies on the polyline, and to a polygon if it is located inside the polygon. Conversely, two geometries are considered disjoint if they do not share any spatial interaction - meaning there is no point of intersection nor overlap between them, neither along their boundaries nor within their interiors. We handle the classification of geospatial objects as a binary classification task, identifying geometries as either related or disjoint based on these criteria.\nGiven the two geometries $g_1$ and $g_2$ projected together into [-1,1] \u00d7 [\u22121, 1], their respective encoding vectors, $v_1^*$ and $v_2^*$, are independently computed using the Poly2Vec. These vectors are then fed into the MLP model to generate their respective embeddings, $e_{g1}$ and $e_{g2}$. Once the embeddings are generated, the final prediction regarding whether two geometries are related is determined by computing the dot product of the embeddings as:\n$p_r = sigmoid(dot(e_{g1}, e_{g2}))$ (23)\nwhere $p_r$ denotes the predicted probability that the geometries are related or not. Furthermore, for this task, we employ the binary cross entropy as the loss function.\n3.5.2 Distance Estimation. Identifying the distance relationships between spatial objects is also important for effective spatial reasoning. However, defining the distance between complex shapes like polylines or polygons can be challenging due to their extent in two dimensions. Therefore, our focus is primarily on calculating the distance between a point and an arbitrary geometry. Specifically, we define the distance between two points as their Euclidean distance, the distance between a point and a polyline as the Euclidean distance between the point and the closest point on the polyline, and the distance between a point and a polygon as the Euclidean distance between this point and the centroid of the polygon.\nTo train our Poly2Vec for estimating distance, we again utilize the encoded vectors $v_1^*$ and $v_2^*$ as inputs to MLP and obtain the output embeddings $e_{g1}$ and $e_{g2}$. The estimated distance d between the two geometries is measured by the Euclidean distance between their embeddings:\n$d = ED(e_{g1}, e_{g2})$ (24)\nSubsequently, we employ the mean squared error (MSE) as the loss function.\n3.5.3 k-NN Search. Beyond simply estimating the distance between two geometries, a more practical application is the k-NN search. For example, individuals might want to identify the three closest coffee shops - represented as polygons - to their current location, depicted as a point.\nTo evaluate the performance of Poly2Vec on such a task, we, again, utilize the distance metric d, as defined in Equation (24), combined with the MSE loss function. However, unlike the distance estimation tasks where pairs of geometries are processed, here we input batches containing multiple geometries. The model then optimizes the estimation of the distances between each individual geometry and every other geometry within the same batch."}, {"title": "4 EXPERIMENTS", "content": "To verify the effectiveness of Poly2Vec, we conduct a variety of experiments on both synthetic and real geospatial data. While there are other ways to compute these relationships, i.e. using Euclidean distance for computing distance relationship, our experiments show that Poly2Vec preserves the important spatial properties for spatial reasoning downstream tasks, such as shape and location."}, {"title": "4.1 Experimental Setup", "content": "Data Description. For the synthetic dataset, we randomly generate 2D points, polylines and polygons with coordinates (x, y) \u0454 [-1,1] \u00d7 [-1,1]. The number of segments and vertices of the polylines and polygons, respectively, range from 3 to 6. The real-world datasets are extracted from OpenStreetMap\u00b2 and correspond to the greater area of California, USA. The extracted points represent landmarks, the polylines are roads, and the polygons are lakes and parks. Each polyline or polygon is simplified using the Douglas Peucker algorithm, with a simplification factor of $10^{-4}$ for polylines and $10^{-3}$ for polygons. Figure 6 shows an example of a simplified polygon in our dataset.\nHyperparameter Configuration. We provide a summary of the main configuration parameters in our approach:\n\u2022 Dataset Split: We extract 10,000 geometry pairs for each experiment, which we split in a 60: 20: 20 ratio for training, validation, and testing, respectively.\n\u2022 Frequency Selection: We adopt a geometric grid approach to sample frequencies in the spectral domain. We set the minimum frequency to 0.1 and the maximum to 5.0.\n\u2022 Embedding Dimensionality: The output vector v which comes from the Fourier transform has a size of 210. After learned fusion,"}, {"title": "4.2 Evaluation on Spatial Reasoning", "content": "4.2.1 Topological Relationship Classification. Different types of geospatial objects satisfy a different number of pairwise topological relationships. As shown in Table 1, for each combination of geometries, we select one pairwise relation to form a binary classification task. For both synthetic and real datasets, we generate 10,000 pairs of each class. For real datasets, all relationships are pre-calculated before the normalization of the coordinates.\nWe report the results in Table 2, where the best and second best values are highlighted. Since this is a binary classification problem, we choose to present the Area Under the Receiver Operating Characteristic (AUROC) and weighted F1 scores. We use\u2191 to indicate that a larger performance measurement value is better. Although two specialized models, ResNet1D and NUFT, both outperform the generic baselines on the synthetic dataset, they are only applicable to polygon-polygon settings. Specifically, ResNet1D outperforms NUFT slightly on synthetic dataset but is not comparative on the real dataset. We argue that due to the normalization on the real datasets, those boundary pairs may fall into the other class (i.e., one polygon that is very close to the other polygon, but are disjoint, may intersect after normalization due to round-off errors). A CNN-based model such as ResNet1D is sensitive to such local-level nuances. As a comparison, Coord and Theory can handle arbitrary types of objects. Moreover, Theory is the best model when dealing with simple geospatial objects (i.e., point-polyline), while Coord is the worst one in many cases, highlighting the necessity of deliberate encoding. As for our Poly2Vec, though it is slightly worse than Theory in the case of point-polyline, it stably outperforms both generic baselines when facing complex geospatial objects on both synthetic and real datasets and is at least on par with specialized models when facing polygons. This highlights the ability of Poly2Vec to maintain stable performance across different types of geometries, including mixed geometries."}, {"title": "4.2.2 Distance Estimation", "content": "In this experiment, we evaluate the ability of Poly2Vec to preserve the pairwise distances between arbitrary geospatial objects. In more detail, we consider the distance between points and arbitrary geospatial objects (points, polylines, and polygons). The ground-truth distances are generated according to the definitions in Section 3.5.2. To preserve the relative distance, each distance is calculated after normalization of the coordinates.\nWe report the MSE of distance estimation in Figure 7. Both ResNet1D and NUFT are excluded since they are only applicable to polygons. As depicted, Theory is only slightly worse than Coord when dealing with preserving the distances between points and polylines. In contrast, benefiting from the deliberate encoding, our Poly2Vec outperforms them by a large margin. We also observe that the largest MSE comes from the point-polyline dataset. We argue that this is due to the nature of the distance measure utilized, which identifies the distance to the closest point of the polyline. This is challenging since the Fourier transform (which underpins our encoding) tends to emphasize global rather than local features of the shape, thus affecting the precision of the distance measurement."}, {"title": "4.2.3 k-NN Search", "content": "In addition to distance estimation, we also conduct k-NN search, which is a more realistic application based on the model's ability of distance estimation. We choose the well-accepted hitting ratio (denoted as HR@k) and recall ratio (denoted as Rk@t) as evaluation metrics. HR@k measures the overlapping proportion between the top-k results produced by each model and the ground-truth results from a given metric. Rk@t measures the proportion of the top-k ground-truth in the corresponding top-t results returned by a model.\nAs shown in Table 3, both Coord and Theory show a promising ability to search the closest points to one point. However, their performance decreases sharply when searching the closest polylines and polygons to a point. In comparison, our Poly2Vec is only slightly worse than Theory and Coord for the case of point-to-point nearest neighbor, but it outperforms the other two models in the other cases. These findings further support the ability of our encoding in distance preservation, thus rendering Poly2Vec effective for nearest neighbor search. It furthermore highlights again the versatility of Poly2Vec when dealing with different spatial object types."}, {"title": "4.2.4 Cross Evaluation", "content": "We further study the generalizability of Poly2Vec in a cross-evaluation setting. Specifically, we train it on one pair of geometries and test it on all the other possible geometry pairs. Due to the limited space, we only present the results of cross-evaluation trained on point-polygon and polygon-polygon datasets in Figure 8. The rest of the results can be found in A.3. The cross-evaluation in blue and the original evaluation denote the performance of Poly2Vec trained with different types of geometries and the same type of geometries, respectively."}, {"title": "4.3 Ablation Study", "content": "To verify the effectiveness of both location and shape properties for spatial reasoning and our proposed modules to capture them, we design several variants of Poly2Vec as follows:\n\u2022 w/Mag: Utilizes only the Fourier transform magnitude.\n\u2022 w/Phase: Utilizes only the Fourier transform phase.\n\u2022 w/o fusion: Employs a simple concatenation of the Fourier magnitude and phase, treating both components with equal importance.\nFor all variants, we conduct the k-NN search experiment on both synthetic and real datasets and report their performance in Figure 9. The w/Mag variant demonstrated the weakest performance for two main reasons. First, for point geometries, the magnitude is a constant value of 1. Second, the magnitude encapsulates shape properties, which are less important for the nearest neighbor search. This is evident by looking at the results of w/Phase, which surpasses w/Mag. The w/o fusion scenario, which simply concatenates magnitude and phase, yields performance comparable to or slightly better than the w/Phase variant alone. This also verifies that the magnitude might not be informative for this task. Conversely, our Poly2Vec model, equipped with a learned fusion strategy, adjusts the weights of both properties depending on the object types and specific downstream tasks, thereby achieving high performance across various scenarios."}, {"title": "5 RELATED WORK", "content": "We introduce a method to encode multiple types of shapes into a unified representation, which allows for a single model architecture to process mixed shape types. Previous work has explored various separate encoding schemes for different types of shapes, often focusing on one shape type at a time.\nPoints have received considerable attention in this research field. A straightforward approach for points, or any shape, is to represent them on a discrete grid, such as an image. Tang et al. used this approach to classify images based on their location, using a one-hot vector to indicate the image's grid cell location [15]. While intuitive, grid encoding faces several challenges. The distance between points is poorly reflected by grid IDs, the scale is fixed, the discrete vocabulary of possible grid cells can be very large, and selecting a single grid resolution that works well for all applications is difficult. A more refined point encoding approach is to produce a fixed-length, real vector representation of the point. One example is the sinusoidal multi-scale location encoder [9], which generalizes the sinusoidal location encoder [7].\nZhong et al. extended the sinusoidal encoder to consider S different scales s \u2208 {0, 1, ..., S}, with one sinusoidal encoding for each scale [19]. The \"theory location encoder\" also uses sinusoids and a set of scales, but first computes the dot product of the (x, y) location with three unit vectors separated by 120\u00b0 around the origin [9]. Our Fourier transform encoding implicitly uses different scales of sinusoids by sampling at frequencies from the continuous 2D Fourier transform of the point. This method makes the encoding agnostic with respect to the type of the input shape, working effectively for lines and polygons in addition to points.\nWhile there is extensive research on point encoding, there appears to be no previous research devoted specifically to encoding 2D line segments nor polylines in their most generic forms. A similar problem to polyline encoding is the encoding of trajectories, where the trajectories are in open spaces, not restricted to a robot's workspace or a camera's field of view, such as timestamped GPS locations. In such scenarios, grid techniques remain popular for representing trajectories, e.g. [12]. Specifically, Li et al. encode location trajectories with a grid in order to find similar trajectories in spite of disparate, nonuniform sampling [6]. They train a recurrent neural network (RNN) with intentionally degraded versions of trajectories that give their model the ability to fill in missing data. They also use an embedding of the grid cells to give the model a notion of the relative positions of the cells. Using an LSTM on pixel coordinates, Xu et al. represent pedestrian trajectories with the hidden state of an LSTM [17]. There are also embedding techniques specifically designed for trajectories on road networks, e.g. [2]. Our problem differs from the aforementioned because we consider geometric line segments and polylines with no natural ordering of their constituent points.\nPolygons have been encoded using a graph structure built from their vertices with a graph convolutional autoencoder model to measure the similarity of simple polygons without holes [18]. Veer et al. present two methods to encode the sequence of vertices of polygons. One approach utilizes elliptic Fourier descriptors to encode the outline of a polygon as an approximation. Their deep learning approaches work on the representation of the polygon's vertices with a one-dimensional LSTM or convolutional neural net (CNN) [16]. Another class of techniques utilizes the non-uniform Fourier transform (NUFT) to transform a shape into the spectral domain and then convert it to an image with a conventional inverse discrete Fourier transform (IDFT) for further processing [4, 5]. Given the image representation, this approach suffers from the problems of grid representation mentioned above, although they can work for a variety of shape types, similar to ours. Additionally, Mai et al. present two methods for encoding polygons [10]. The first approach treats the polygon as a sequence of points and thus takes a vector of the polygon's vertices and encodes them using a 1D circular CNN, called ResNet1D. The second approach is based on [4, 5], where the NUFT is utilized to encode polygon geometries, but the IDFT is omitted. Instead, the encoded vectors are directed as input to an MLP model trained for either shape or relation classification. In order to represent the polygons with NUFT, the authors utilize j-simplex meshes to decompose the geometries into triangles. This approach is close to ours in the sense that it is based on sampling frequency features from a 2D Fourier transform, but it cannot be directly applied to different types of geometries. Instead, the distinctive feature of our approach is its ability to uniformly process mixed shape types-points, polylines, and polygons-within the same framework, a capability not demonstrated by any previous research."}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "In this paper, we proposed a polymorphic encoding framework Poly2Vec for arbitrary geospatial objects. By effectively capturing the shape and location properties via 2D Fourier transforms and a learned fusion strategy, Poly2Vec is able to generate a powerful embedding for arbitrary type of geometry for spatial reasoning tasks. Our experiments verify Poly2Vec's versatility and supervisor performance over the baselines.\nFor future work, it is intriguing to extend this approach to higher-dimensional shapes using higher-dimensional Fourier transforms. It is also interesting to design a pre-training model based on this polymorphic encoding and evaluate its performance on more spatial reasoning tasks."}, {"title": "A APPENDIX", "content": "A.1 Special cases of Polygon Fourier Transform\nEquation (18) has zeros in the denominator for some values of (u, v). We can repair this by evaluating the Fourier transform (Equation (1)) at these specific values of (u, v), giving\n\\begin{cases}\n\\frac{1}{2} &\\text{u = 0, v = 0}\\\\\n\\frac{1}{4\\pi^2 v^2} (cos(2\\pi v) \u2013 1) \u2013\\frac{j(sin(2\\pi v) \u2013 2\\pi v)}{4\\pi^2 v^2} &\\text{u = 0} \\\\\n\\frac{1}{4\\pi^2 u^2} (cos(2\\pi u) + 2\\pi u sin(2\\pi u)-1) \u2013 \\frac{j(-sin(2\\pi u) \u2013 2\\pi u cos(2\\pi u))}{4\\pi^2 u^2} &\\text{v = 0} \\\\\n-\\frac{1}{4\\pi^2 v^2} (cos(2\\pi u) \u2013 1) \u2013\\frac{j( sin(2\\pi u) \u2013 2\\pi u)}{4\\pi^2 v^2} &\\text{u + v = 0} \\\\\nEquation (18) &\\text{otherwise}\n\\end{cases}\n(25)\nA.2 Geometric Frequency Map\nThe geometric frequency grid $W^{(gfm)}$, is defined as $W_x \\times W_y$, where $W_x \\in \\mathbb{R}^U$ and $W_y \\in \\mathbb{R}^V$ correspond to the frequencies on the X-axis and Y-axis respectively. Both $W_x$ and $W_y$ contain frequencies arranged in a geometric progression:\n$W_x = \\{u_i = U_{min} \\cdot r^{i} | i = 0, 1, . . . , U \u2013 1\\}$\n$W_y = \\{v_i = U_{min} \\cdot r^{i} | i = 0, 1, . . . , V \u2013 1\\}$\nwhere r is the ratio that defines the progression, determined by\n$r = (\\frac{U_{max}}{U_{min}})^{\\frac{1}{U-1}}$ for $W_x$ and similarly for $W_y$. This configuration was also presented in [10]."}]}