{"title": "Poly2Vec: Polymorphic Encoding of Geospatial Objects for Spatial Reasoning with Deep Neural Networks", "authors": ["Maria Despoina Siampou", "Jialiang Li", "John Krumm", "Cyrus Shahabi", "Hua Lu"], "abstract": "Encoding geospatial data is crucial for enabling machine learning (ML) models to perform tasks that require spatial reasoning, such as identifying the topological relationships between two different geospatial objects. However, existing encoding methods are limited as they are typically customized to handle only specific types of spatial data, which impedes their applicability across different downstream tasks where multiple data types coexist. To address this, we introduce Poly2Vec, an encoding framework that unifies the modeling of different geospatial objects, including 2D points, polylines, and polygons, irrespective of the downstream task. We leverage the power of the 2D Fourier transform to encode useful spatial properties, such as shape and location, from geospatial objects into fixed-length vectors. These vectors are then inputted into neural network models for spatial reasoning tasks. This unified approach eliminates the need to develop and train separate models for each distinct spatial type. We evaluate Poly2Vec on both synthetic and real datasets of mixed geometry types and verify its consistent performance across several downstream spatial reasoning tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "The ubiquity and unprecedented growth of diverse geospatial data generated by numerous sources, such as satellites, sensors, and mobile devices, have enabled their utilization by geospatial decision-making applications in various domains, including transportation, urban planning, intelligence, and public health. A fundamental building block for these decisions is the ability to perform spatial reasoning. Spatial reasoning involves the capability to interpret and analyze spatial relationships between objects in a given space. For example, it includes tasks such as classifying building patterns [1], inferring topological relations between geometries\u00b9 [13, 14], and retrieving the nearest neighbor of a geometry [11], among others. Meanwhile, machine learning (ML) and deep neural networks (DNN) are now extensively used for decision-making, and a crucial first step in leveraging them for geospatial decision-making is to represent geometries as fixed-size vectors, a process known as encoding. To this end, several specialized encoding techniques have been proposed recently for different spatial data types. For instance, Mai et al. [9] introduce a method for encoding points of interest into a d-dimensional vector representation using sinusoidal multi-scale functions. Similarly, t2vec [6] encodes trajectory data by projecting their coordinates into a grid, while Veer et al. [16] encode polygon vertices using an LSTM and CNN for polygon classification. Despite the effectiveness of these encoding techniques, they share a common limitation: each is specifically designed for a single spatial data type, thus tailored to capture spatial properties inherent to that data type. For example, point encoding models usually capture angle and location, while trajectory encoding models focus\n\u00b9We use the term geometries and geospatial objects interchangeably."}, {"title": "2 PRELIMINARIES", "content": "In this section, we present our problem definition and preliminary Fourier transform properties."}, {"title": "2.1 Problem Formulation", "content": "DEFINITION 2.1 (GEOSPATIAL OBJECT). A geospatial object $g \\in \\mathbb{R}^{2N}$ consists of a set of N coordinates, each defined as a point in space represented in two dimensions (x, y). Based on the organization and connection of these points, different geospatial objects can be defined:\n\u2022 Point: A single coordinate (x, y) representing a location in space.\n\u2022 Polyline: A sequence of connected coordinates $((x_1, y_1), (x_2, y_2), ..., (x_N, y_N))$, forming linear segments that do not close into a loop.\n\u2022 Polygon: A filled shape that is bounded by a closed polyline where a sequence of connected coordinates forms a loop, enclosing an area, defined as $((x_1, y_1), (x_2, y_2), ..., (x_N, y_N), (x_1, y_1))$.\nResearch Problem (Polymorphic Encoding of Geospatial Objects). Given an arbitrary type of geospatial object g, polymorphic encoding aims to project its original coordinates into a fixed-length vector that captures the object's spatial properties. This vector should be suitable for downstream tasks that require spatial reasoning."}, {"title": "2.2 2D Continuous Fourier Transform", "content": "An integral part of our polymorphic encoding is the computation of the 2D continuous Fourier transform. Here, we introduce its fundamental properties, which are taken from a textbook [3].\nThe Fourier transform of a 2D function f(x, y) is denoted as $F(u, v) = \\mathcal{F} {f(x, y)}$ and computed as\n$F(u, v) = \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\infty} f(x, y)e^{-j2\\pi(ux+vy)} dx dy$ (1)\nwhere $j = \\sqrt{-1}$.\nOne important property of the Fourier transform is its linearity. This means that the Fourier transform of a sum of functions $f_i (x, y)$ is the sum of the corresponding Fourier transforms $F_i (u, v)$:\n$\\mathcal{F}{\\sum\\limits_{i=1}^{n}f_i(x, y)} = \\sum\\limits_{i=1}^{n}F_i (u, v)$ (2)\nThis property is important for computing the Fourier transform of polylines and polygons, which are made from the Fourier transforms of line segments and triangles, respectively.\nWe will use affine transformations to create arbitrary line segments and arbitrary triangles from canonical versions of the same shapes. Thus, we need to compute the Fourier transform of an affine-transformed function, which is\n$\\mathcal{F}{f(a_1x+b_1y + c_1, a_2x + b_2y + c_2)} =$\n$\\frac{1}{|D|}e^{-j2\\pi (x_0u+y_0v)} F (\\frac{b_2}{D}u-\\frac{b_1}{D}v, \\frac{-a_2}{D}u+\\frac{a_1}{D}v)$ (3)\nwhere\n$D = a_1b_2-a_2b_1$ $x_0 = (b_1c_2 - b_2c_1)$ $y_0 = -(a_2c_1 - a_1c_2)$\nAll our spatial shapes f(x, y) are real-valued functions, which means their Fourier transforms F(u, v) are Hermitian, i.e.\n$F(u, v) = F^* (-u,-v)$ (4)\nwhere * indicates complex conjugate. Thus, the upper half (u, v) plane (v \u2265 0) contains values that are essentially redundant with respect to the lower half plane. For encoding shapes, it is sufficient to extract features from only the upper half-plane.\nF(u, v) is in general complex, with a real part Re(F(u, v)) and imaginary part Im(F(u, v)). The magnitude and phase of F(u, v) are then defined as follows, respectively.\n$z(u, v) = \\sqrt{Re(\\mathcal{F}(u, v))^2 + Im(\\mathcal{F}(u, v))^2}$ (5)\n$\\phi(u, v) = atan2(Im(\\mathcal{F}(u, v)), Re(\\mathcal{F}(u, v)))$ (6)"}, {"title": "3 POLY2VEC", "content": "In this section, we introduce our Poly2Vec framework, which is illustrated in Figure 2. Our approach begins with a geometry g. Initially, Poly2Vec applies a scaling transformation to normalize the coordinates of g to the unit space. Subsequently, a 2D continuous Fourier transform processes the normalized shape to capture the spatial characteristics of the geometry, resulting in a vector $v \\in \\mathbb{R}^d$. This vector consists of complex Fourier transform values sampled at discrete (u, v) coordinates. From the Fourier features, both the phase $v_\\phi$ and magnitude $v_z$ are extracted. These properties are then adaptively combined using a learned fusion strategy to form a final representation $v^* \\in \\mathbb{R}^{2d}$. This representation serves as input to various MLP models, each trained for specific downstream tasks"}, {"title": "3.1 Scale Transformation", "content": "The first step of our pipeline involves the scaling transformation of the geospatial object g to fit within the space [-1,1] \u00d7 [-1,1]. This transformation ensures that each object is positioned within the same relative spatial domain, which is crucial for consistent learning across different types of geospatial objects.\nWe compute the scaling transformation for each coordinate pair $(x_i, y_i)$ in g using the minimum and maximum coordinates from the target area (e.g., California). Let $(x_{min}, y_{min})$ and $(x_{max}, y_{max})$ be the minimum and maximum coordinates of this area, respectively. The scaled coordinates (x', y') are then calculated as:\n$x' = 2\\frac{x_i - x_{min}}{x_{max} - x_{min}} -1$ (7)\n$y' = 2\\frac{y_i - y_{min}}{y_{max} - y_{min}} -1$ (8)\nThis operation can be formally expressed for g as:\n$g' = {\\{(2\\frac{x_i - x_{min}}{x_{max} - x_{min}})-1, 2(\\frac{y_i - y_{min}}{y_{max} - y_{min}})-1\\}}_{i=1}^{N}$ (9)\nwhere $g' = {\\{(x'_i, y'_i)\\}}_{i=1}^{N}$ represents the scaled geospatial object."}, {"title": "3.2 Fourier Transform-based Encoding", "content": "3.2.1 Fourier Transform of Point. We represent a 2D point at (x0, yo) as a 2D Dirac delta function $\\delta(x \u2013 x_0, y \u2013 y_0)$, whose Fourier transform is\n$\\mathcal{F}{\\delta(x \u2013 x_0, y \u2013 y_0)} = e^{-j2\\pi(xu+y_0v)}$ (10)\nThe magnitude of the Fourier transform of any point is z(u, v) = 1, and thus, only the phase can present meaningful information for a point. This is also apparent in Figure 1b. The phase shows how the Fourier transform represents the location of the point. Notably, for the point, F(0, 0) = 1.\n3.2.2 Fourier Transform of Polyline. We now describe how to model a polyline to derive its Fourier features. Using the additive property of Fourier transform in Equation (2), it is sufficient to compute the Fourier transform of a single line segment and then sum the Fourier transforms of each segment in the polyline. Thus, we begin with the Fourier transform of a canonical line segment and then apply an affine transformation to model an arbitrary line segment.\nThe canonical line segment AB extends from (x, y) coordinates $(-\\frac{1}{2},0)$ to $(\\frac{1}{2},0)$, as illustrated in Figure 3. We model this line segment as $f_-(x, y) = rect(x)\\delta(y)$, where the underbar subscript indicates the canonical line segment. The Dirac delta function $\\delta(y)$ represents a ridge of delta functions extending along the entire x-axis, while rect(x) limits the extent of the ridge to our specified range on the x-axis. The rect(x) function is\n$rect(x) =\\begin{cases}\n0 & if |x|> \\frac{1}{2} \\\\\n\\frac{1}{2} & if |x| = \\frac{1}{2} \\\\\n1 & if |x|< \\frac{1}{2}\n\\end{cases}$ (11)\nand thus, the Fourier transform of the canonical line segment is expressed as\n$\\mathcal{F}{\\{f_-(x, y)\\} } = \\mathcal{F}{\\{rect(x)\\delta(y)\\}\n} = sinc(u)$ (12)\nwhere the sinc(u) function is\n$sinc(u) =\\begin{cases}\n\\frac{sin(\\pi u)}{\\pi u} & if u \\neq 0 \\\\\n1 & if u = 0\n\\end{cases}$ (13)\nThen the affine transformation matrix T is computed as"}, {"title": "3.2.3 Fourier Transform of Polygon", "content": "For polygons, we follow an approach similar to polylines. Any polygon can be split into triangles, such as shown in Figure 4. We use the \"ear clipping\" algorithm for our polygon triangulations, and we ensure that the coordinates of each constituent polygon are ordered in counterclockwise order for consistent results from the Fourier transform. Subsequently, we sum up the Fourier transforms of all the constituent triangles, according to Equation (2).\nWe begin with a canonical isosceles right triangle, shown as the green triangle ABC in Figure 5. The triangle is represented by $f_-(x, y)$, with a value of one inside the triangle and zero otherwise.\nInserting the affine transform parameters into Equation (3) gives the Fourier transform"}, {"title": "3.3 Selecting Frequencies", "content": "The encoded vector $v \\in \\mathbb{R}^d$ comes from sampling the complex 2D Fourier transform of the shape at a discrete set of sample frequencies. For sampling frequencies, a straightforward approach is to use a linear grid frequency map with equally spaced frequencies along the (u, v) dimensions. However, not all these frequencies are necessary due to the Hermitian symmetry property in Equation (4) and as depicted in Figure 1. To that extent, we can utilize half of the Fourier features to avoid redundant information. A more effective approach than geometric sampling was presented in a previous work [10], where the density of the sampled frequencies decreases as the distance from the center increases. To utilize a set of meaningful frequencies that also assist learning, we choose to adopt this geometric grid strategy, denoted as $W(gfm)$. Due to limited space, the details of $W(gfm)$ can be found in A.2."}, {"title": "3.4 Learned Fusion Strategy", "content": "The previous encoding process results in a d-dimensional vector v, composed of a vector of complex numbers. As we can observe from Figure 1, both magnitude and phase hold intrinsic information about the geometry. The magnitude provides insights into the shape of the geometry, capturing the amplitude variations within the encoded spatial data, and the phase conveys information about the orientation or direction of the geometry relative to the spatial origin, offering information about the spatial alignment and rotation of the shape components. To effectively utilize the Fourier features encoded within v, we utilize both the magnitude and phase of v following Equations (5) and (6), respectively.\nHowever, different types of geospatial objects have different spatial properties, implying different importance of magnitude and phase when representing them. For instance, as observed in Figure 1b, the magnitude of all points is always 1 since points do not have varying shapes, while the magnitude is vital for identifying different polygons (Figure 1d). Thus, simply concatenating magnitude and phase into a single feature vector without considering these differences would lead to representations that do not adequately capture the unique spatial characteristics of different geometries.\nInstead, we propose a learned fusion strategy that adaptively balances the contribution of magnitude and phase. Targeting the specific downstream task and object types at hand, we adjust the weights assigned to magnitude and phase during training as follows:\n$v^* = MLP_z(v_z) || MLP_\\phi(v_\\phi)$ (22)\nHere, two separate MLPs, $MLP_z$ for magnitude ($v_z$) and $MLP_\\phi$ for phase ($v_\\phi$) are utilized to transform these features effectively. Each MLP has two fully connected layers and a ReLU activation function. The outputs from these MLPs are then concatenated to form a representation vector $v^* \\in \\mathbb{R}^{2d}$."}, {"title": "3.5 Model Training and Downstream Tasks", "content": "The encoded vector v* can then be forwarded as input to a downstream model trained to perform a spatial reasoning task. In our case, the downstream model is an MLP, with two fully connected layers followed by a ReLU activation function. The output of the model is a geometry embedding $e \\in \\mathbb{R}^{m}$. Formally,\n$v^* \\in \\mathbb{R}^{2d} \\xrightarrow{\\text{2-layer MLP}} e \\in \\mathbb{R}^{m}$\nNext, we will detail the downstream tasks for which the model is trained and outline the training methodology."}, {"title": "3.5.1 Topological Relationship", "content": "Determining the spatial relation between two geometries g1 and g2 can be crucial for spatial reasoning. For instance, understanding whether essential amenities like coffee shops or gyms are located within a residential area, or if a specific road intersects a park, relies on this analysis. In our framework, two geometries are classified as related if they intersect. Specifically, for points, a point is considered related to a polyline if it lies on the polyline, and to a polygon if it is located inside the polygon. Conversely, two geometries are considered disjoint if they do not share any spatial interaction - meaning there is no point of intersection nor overlap between them, neither along their boundaries nor within their interiors. We handle the classification of geospatial objects as a binary classification task, identifying geometries as either related or disjoint based on these criteria.\nGiven the two geometries g1 and g2 projected together into [-1,1] \u00d7 [\u22121, 1], their respective encoding vectors, $v_1^*$ and $v_2^*$, are independently computed using the Poly2Vec. These vectors are then fed into the MLP model to generate their respective embeddings, $e_{g1}$ and $e_{g2}$. Once the embeddings are generated, the final prediction regarding whether two geometries are related is determined by computing the dot product of the embeddings as:\n$p_r = sigmoid(dot(e_{g1}, e_{g2}))$ (23)\nwhere pr denotes the predicted probability that the geometries are related or not. Furthermore, for this task, we employ the binary cross entropy as the loss function."}, {"title": "3.5.2 Distance Estimation", "content": "Identifying the distance relationships between spatial objects is also important for effective spatial reasoning. However, defining the distance between complex shapes like polylines or polygons can be challenging due to their extent in two dimensions. Therefore, our focus is primarily on calculating the distance between a point and an arbitrary geometry. Specifically, we define the distance between two points as their Euclidean distance, the distance between a point and a polyline as the Euclidean"}, {"title": "4 EXPERIMENTS", "content": "To verify the effectiveness of Poly2Vec, we conduct a variety of experiments on both synthetic and real geospatial data. While there are other ways to compute these relationships, i.e. using Euclidean distance for computing distance relationship, our experiments show that Poly2Vec preserves the important spatial properties for spatial reasoning downstream tasks, such as shape and location."}, {"title": "4.1 Experimental Setup", "content": "Data Description. For the synthetic dataset, we randomly gen-erate 2D points, polylines and polygons with coordinates $(x, y) \\epsilon$[-1,1] \u00d7 [-1,1]. The number of segments and vertices of the poly-lines and polygons, respectively, range from 3 to 6. The real-world datasets are extracted from OpenStreetMap\u00b2 and correspond to the greater area of California, USA. The extracted points represent landmarks, the polylines are roads, and the polygons are lakes and parks. Each polyline or polygon is simplified using the Douglas-Peucker algorithm, with a simplification factor of 10-4 for polylines and 10-3 for polygons. Figure 6 shows an example of a simplified polygon in our dataset.\nHyperparameter Configuration. We provide a summary of the main configuration parameters in our approach:\n\u2022 Dataset Split: We extract 10,000 geometry pairs for each experi-ment, which we split in a 60: 20: 20 ratio for training, validation, and testing, respectively.\n\u2022 Frequency Selection: We adopt a geometric grid approach tosample frequencies in the spectral domain. We set the minimumfrequency to 0.1 and the maximum to 5.0.\n\u2022 Embedding Dimensionality: The output vector v which comesfrom the Fourier transform has a size of 210. After learned fusion,"}, {"title": "4.3 Ablation Study", "content": "To verify the effectiveness of both location and shape properties for spatial reasoning and our proposed modules to capture them, we design several variants of Poly2Vec as follows:\n\u2022 w/Mag: Utilizes only the Fourier transform magnitude.\n\u2022 w/Phase: Utilizes only the Fourier transform phase.\n\u2022 w/o fusion: Employs a simple concatenation of the Fourier magnitude and phase, treating both components with equal importance.\nFor all variants, we conduct the k-NN search experiment on both synthetic and real datasets and report their performance in Fig-ure 9. The w/Mag variant demonstrated the weakest performance for two main reasons. First, for point geometries, the magnitude is a constant value of 1. Second, the magnitude encapsulates shape properties, which are less important for the nearest neighbor search. This is evident by looking at the results of w/Phase, which surpasses"}, {"title": "5 RELATED WORK", "content": "We introduce a method to encode multiple types of shapes into a unified representation, which allows for a single model architecture to process mixed shape types. Previous work has explored various separate encoding schemes for different types of shapes, often focusing on one shape type at a time.\nPoints have received considerable attention in this research field. A straightforward approach for points, or any shape, is to represent"}, {"title": "A APPENDIX", "content": "A.1 Special cases of Polygon Fourier Transform\nEquation (18) has zeros in the denominator for some values of (u, v). We can repair this by evaluating the Fourier transform (Equation (1)) at these specific values of (u, v), giving\nA.2 Geometric Frequency Map\nThe geometric frequency grid $W^{gfm}$, is defined as $W_x \\times W_y$, where $W_x \\in \\mathbb{R}^U and W_y \\in \\mathbb{R}^V$ correspond to the frequencies on the X-axis and Y-axis respectively. Both $W_x and W_y$ contain frequencies arranged in a geometric progression:\nA.3 Cross evaluation results on polylines\nWe present the cross-evaluation results when our model is trained on polylines-polylines and point-polylines pairs. We again observe a slight performance decrement during cross-evaluation, which suggests that Poly2Vec retains a reasonable level of generalizability across different types of geometries. Compared to when training on point-polygons and polygons-polygons pairs, we observe a larger decrease, which also supports our claim that when the model is trained on more complex geometries, it can generalize well on simpler ones."}]}