{"title": "Brain Tumor Segmentation in MRI Images with 3D\nU-Net and Contextual Transformer", "authors": ["Thien-Qua T.Nguyen", "Hieu-Nghia Nguyen", "Thanh-Hieu Bui", "Thien B. Nguyen-Tat", "Vuong M. Ngo"], "abstract": "This research presents an enhanced approach for\nprecise segmentation of brain tumor masses in magnetic resonance\nimaging (MRI) using an advanced 3D-UNet model combined with\na Context Transformer (CoT). By architectural expansion CoT,\nthe proposed model extends its architecture to a 3D format,\nintegrates it smoothly with the base model to utilize the complex\ncontextual information found in MRI scans, emphasizing how\nelements rely on each other across an extended spatial range.\nThe proposed model synchronizes tumor mass characteristics from\nCoT, mutually reinforcing feature extraction, facilitating the pre-\ncise capture of detailed tumor mass structures, including location,\nsize, and boundaries. Several experimental results present the\noutstanding segmentation performance of the proposed method\nin comparison to current state-of-the-art approaches, achieving\nDicescore of 82.0%, 81.5%, 89.0% for Enhancing Tumor, Tumor\nCore and Whole Tumor, respectively, on BraTS2019.", "sections": [{"title": "I. INTRODUCTION", "content": "Brain tumors are abnormal growths of cells in the brain,\nwhich can be either malignant or benign. These tumors can\nsignificantly impact the patient's quality of life and health,\nespecially when they grow rapidly and spread to other areas\nof the brain and spinal cord. Imaging methods like X-rays\nand MRI are used to detect brain tumors, but not all of them\ncan show the full details of the tumor [1]. This increases\nthe importance of using modern diagnostic methods, including\nartificial intelligence, to identify and classify brain tumors.\nAutomating this procedure not only reduces costs and saves\ntime but also lightens the workload for staff and healthcare\nsystems, promoting efficiency and resource conservation. With\ntheir profound impact on health and life, as well as the\nincreasing number of cases, brain tumors are not just a medical\nissue but also an economic and social challenge.\nMRI, a widely used medical imaging technology, is com-\nmonly employed in clinical settings to assess brain tumors. Four\nmain MRI modalities includeT1-weighted (T1), T2-weighted\n(T2), contrast-enhanced T1-weighted (T1c) and fluid attenua-\ntion inversion recovery (FLAIR) producing high-quality images\nof soft tissue abnormalities in the brain. The combination of\nthese modalities enhances the accuracy of tumor segmentation,\nas depicted in Fig.1, where images from different modalities\noffer complementary information and mutual support.\nThe Transformer was first initially proposed by Vaswani et\nal. [2], an influential network architecture that represents a\nsubstantially advancement in deep learning and natural lan-\nguage processing. In the medical domain, the Transformer has\nopened up new opportunities in utilizing artificial intelligence\nin brain tumor segmentation from medical images. By inte-\ngrating attention mechanisms and learning from large-scale\ndata, the Transformer has become a strong tool for precisely\nand efficiently detecting and segmenting brain tumors [3],\n[4]. Transformer-based methods hold promise in addressing\nchallenges in tumor segmentation, enhancing accuracy and\nreliability in the segmentation process.\nReference to the CT imaging study [5], we have taken\ninspiration and further expanded upon the research to provide\nmore comprehensive and updated insight of brain tumor seg-\nmentation. Our study introduces a technique utilizing a 3D\nU-Net model, which has been enhanced and combine with\na Transformer specifically for MRI images. By incorporating\nlong-range information throughout the entire space, this ad-\nvanced method allows for the precise identification and local-\nization of tumor subregions. To achieve this, we have developed\na Transformer-based model called Context Transformer [6],\nwhich incorporates an improved attention mechanism to explore\nfeatures and contextual information. This innovative approach\nnot only improve the accuracy of segmentation but also ensures\nefficiency and effectiveness in the process. This represents a\nnotable progress in medical image segmentation, potentially\nenhancing diagnosing and treating patients.\nThe key contributions of this paper are as follows:\n\u2022\nThe Contextual Transformer extended to 3D integrates\nwith 3D UNet model to exploit rich contextual information each layer. Achieving accurate image segmentation requires a\nin MRI images.\n\u2022\nThe proposed model has extended the architecture from\nthe baseline, harmonizing tumor specific features sourced\nfrom CoT to extract important attributes. This comprehen-\nsive synthesis empowers accurate division of the complete\ntumor structure, including its location, size, shape, and\nboundaries. The best scoring results on the BraTS2019\ndataset are 82.0%, 81.5%, 89.0% respectively, for labels\ncorresponding to Enhancing Tumor, Tumor Core, and\nWhole Tumor.\nThe rest of paper is structured as: Section 2 reviews related\nworks. Section 3 show the proposed method. Section 4 de-\nlineates the experimental results, while Section 5 provides a\nsummary of the paper's content and future work."}, {"title": "II. RELATED WORK", "content": "Image segmentation plays a crucial role in the healthcare\nfield, particularly in diagnosing and treating diseases. Various\ntechniques have been developed for segmenting brain tumor im-\nages [7], [8], including both traditional machine learning (ML)\nmethods and deep learning (DL) techniques. ML methods such\nas Support Vector Machines [9] and Graph Theory [10], have\nlimitations in extracting statistical Information from large sam-\nples, resulting in weak segmentation performance. However,\nDL-based methods, particularly Convolutional Neural Network\n(CNN) based methods like 3D U-Net [11] and Attention U-\nNet [12], have proven to be more effective in addressing this\nissue. These networks are capable of processing input images\nof any size and utilize decoding layers to adjust the size of\nfeature maps to match the dimensions of the original image.\nCNN-based models with U-shaped architectures, have made\nsignificant advancements and demonstrated great potential in\n2D and 3D image segmentation tasks. Nonetheless, the posi-\ntioning of convolutional layers within the network architecture\nmay lead to the ignore of long-range information correlations.\nResearch [13] has indicated that achieving good segmentation\nresults requires a model that can simultaneously extract both\nlocal details and global semantic information interactions.\nTransformer-based methods can address above issue. Liu et\nal. introduced the Swin Transformer, utilizing self-attention\nmechanisms based on windows to decrease parameters and\ncomputations, while employing a shifted window mechanism to\nrealize global dependencies. Furthermore, Lin et al., introduced\nDS-TransUNet, a Transformer architecture similar to Unet\nfor segmentation of medical images, achieving performance\ncomparable to state-of-the-art CNN-based methods [14], [15].\nHowever, the Transformer neglects local structures by dividing\nthe image into patches represented as tokens.\nTargeting the weaknesses of both CNN-based and\nTransformer-based networks, combining these structures\ncan complement each other to exploit long-range spatial\nrelationships. TransUnet[16] marks the debut of Transformer\nin CNN. The CNN block of this work is implemented before\nTransformer. Then, features are restored by sampling through\neach layer. Achieving accurate image segmentation requires a\nsignificant amount of computational power and overall data\nvolume increase significantly when processing 3D data."}, {"title": "III. METHOD", "content": "In this paper, we introduce a network depicted in Fig.\n2, based on previously introduced transformer modules but\nincorporates enhanced channel attention modules. This allows\nus to explore spatial information and contextual in MRI images\ncomprehensively, exploit features thoroughly, and improve the\nrepresentation of various tumor regions. Consequently, we ad-\ndress the challenge of accurately capturing detailed information\nabout both the entire tumor architecture and the characteristics\nof individual subregions, thereby enhancing segmentation ac-\ncuracy. Our proposed network contains two main components:\nFig. 2a: the 3D-UNet backbone and Fig. 2b: the 3D context-\naware transformer module within encoder-decoder\nA. 3D Contextual Transformer (CoT)\nThe 2D contextual transformer module, aimed at utilizing\ncontextual information within input features, was initially pro-\nposed by Li et al. [6], limited at 2D feature maps. In order to\novercome this constraint, a 3D Contextual Transformer block is\nproposed, as depicted in Fig. 2b. This CoT block integrates the\nutilization of contextual information and self-attention learning\nwithin a unified framework. It extensively leverages contextual\ninformation among adjacent keys to effectively support the self-\nattention learning process, thus improving the representation\ncapability of the resulting output feature maps.\nInitially, the 3D input feature map \\(X \\in \\mathbb{R}^{H \\times W \\times D \\times C}\\), with di-\nmensions (H, W, D) and C channels, undergoes transformation\ninto keys K, values V and queries Q using learned embedding\nmatrices \\(W_K\\), \\(W_V\\) and \\(W_Q\\), respectively. Subsequently, contex-\ntual information \\(K^1 \\in \\mathbb{R}^{H \\times W \\times D \\times C}\\) for the input X is derived\nby applying a \\(k \\times k \\times k\\) convolution across all adjacent keys\nto contextualize each key representation K. This convolution\ninherently captures static contextual information among local\nneighboring keys. Next, the contextual keys \\(K^1\\) and queries Q\nare merged, and the resultant matrix undergoes two consecutive\n1 \u00d7 1 \u00d7 1 convolutions to produce the attention matrix A. The\nequation for this process is as follows.\n\\(A = [K^1, Q] W_{\\Theta}W_{\\Phi}\\)\n(1)\nwhere, \\(W_{\\Theta}W_{\\Phi}\\) are learnt parameters.\nIn the subsequent step, dynamic contextual representations\nare obtained by performing element-wise multiplication be-\ntween the feature map A and the values V\n\\(K^2 = V * A\\)\n(2)\nThe CoT block produces the final output (Y) by merging the\nstatic context \\(K^1\\) with the dynamic context \\(K^2\\)."}, {"title": "B. 3D-UNet model and Loss function", "content": "The 3D UNet model is a neural network variant commonly\nemployed in medical image processing, especially for the\nsegmentation of 3D medical scans like MRI or CT images.\nDerived from the U-Net architecture [17], a widely-used deep\nneural network in medical image analysis and segmentation, the\n3D UNet model facilitates high-precision segmentation in 3D\nspace. It achieves this by integrating down-sampling and up-\nsampling layers to analyze spatial information extracted from\noriginal images.\nThe Dice Loss has become increasingly popular as a loss\nfunction in semantic segmentation tasks. Its purpose is to\nmeasure and regulate the intersection between ground truth and\npredictions by optimizing the Dice coefficient directly. Within\nthe module, both Dice Loss and cross-entropy loss are utilized\nto optimize the parameters. The definition of Dice Loss is as\nfollows:\n\\(L_{dice}(y, \\hat{y}) = 1 - \\frac{2 \\cdot \\sum_{c \\in \\Omega} \\sum_{i=1}^{N} y_i \\hat{y}_i + \\epsilon}{\\sum_{c \\in \\Omega} \\sum_{i=1}^{N} y_i^2 + \\sum_{c \\in \\Omega} \\sum_{i=1}^{N} \\hat{y}_i^2 + \\epsilon}\\)\n(3)\nand cross-entropy loss function is defined as follows:\n\\(L_{CE}(y, \\hat{y}) = -\\sum_{c \\in \\Omega} \\sum_{i=1}^{N} y_i \\cdot log \\hat{y}_i\\)\n(4)\nwhere \\(\\Omega\\) = {BG(background),NCR/NET,ED,ET}. y and \\( \\hat{y} \\)\ndenote the ground truth and probability prediction of voxel i\non class c, respectively. N = H \u00d7 W \u00d7 D, \\( \\epsilon = 1 \\times 10^{-5} \\).\nConsequently, due to equations (3) and (4), the ultimate loss\nfunction is a weighted combination of the Dice Loss and cross-\nentropy loss, as indicated by the formula:\n\\(L_{Seg}(y, \\hat{y}) = \\alpha L_{dice}(y, \\hat{y}) + (1 - \\alpha) L_{CE}(y, \\hat{y})\\)\n(5)\nwhere \u03b1 is a hyperparameter that regulates the impact of Dice\nloss and cross-entropy loss."}, {"title": "IV. DATASET AND EXPERIMENTS", "content": "The accuracy of segmentation in this research is assessed by\nemploying the Dice score and Hausdorff distance (95%) metrics\nto evaluate enhancing tumor region (label 4), regions within the\ntumor core (label 1, 4) and entirety of tumor region (label 1,\n2, 4).\nThe formula for calculating the Dice Score is given by:\nDiceScore = \\frac{2TP}{FN+FP+2TP}\n(6)\nwhere TP represents the number of true positives, FN\nrepresents the number of false negatives, and FP represents\nthe number of false positives. To measure the dissimilarity\nbetween the actual surface of a region and the predicted region,\nthe Hausdorff95 distance metric is employed. This metric is\nparticularly sensitive to the boundaries of the segmented region\nand formally defined as follows:\n\\(HD_{95}(T, P) = \\max \\{ \\sup_{t \\in T} d(t, P), \\sup_{p \\in P} d(T, p) \\}\\)\n(7)\nThe supremum operator, denoted as sup, is used in the\ncontext where t and p represent points on the surface T of the\nground-truth region and the surface P of the predicted region.\nThe function d(t,p) calculates the distance between t and p.\nB. Implementation details\nDatasets: The proposed method is evaluated using a dataset\nBraTS2019, which is provided by Brain Tumor Segmentation\n(BraTS) challenge. For training purposes, BraTS2019 consists\nof 335 patient cases. The validation set comprises MRI scans\nfrom 125 cases, with labels that are unknown. To train our\nmodel, we only utilize the labeled data, splitting it into 80/20\nfor training and testing. These datasets consist of co-registered,\nskull-stripped and resampled MRI images at a resolution of"}, {"title": "V. RESULTS AND DISCUSSION", "content": "A. Ablation study\nTo evaluate the performance of the transformer block on two\ndatasets, we conducted experiments using a combined model,\nand then compared them to the baseline, against the same\nevaluation set for each dataset.\nContextual Transformer (CoT): According to metrics from\nTable I, the combination of baseline + CoT demonstrates\na considerable improvement in Dicescores for ET, achieving\n82.0% (an increase of 5.6%) in comparison to the baseline.\nBesides that, the TC and WT label achieve 81.5% and 89.0%,\nrespectively, resulting in a mean Dicescore increase of 2.2%.\nFurthermore, there is also an enhancement in the maen HD95,\nreduced by 1.1mm on BraTS2019 evaluation set. The incor-\nporation of CoT blocks has resulted in a significant decrease\nin segmentation errors in all areas of the tumor, providing\nstrong evidence of its effectiveness in improving the ability to\ndifferentiate between tumor subregions and enhancing overall\nsegmentation performance. Additionally, the 3D UNet+CoT\nmodel prioritizes the interaction of contextual information to\nfurther improve segmentation accuracy. This experiment show-\ncases the model's capacity to reconstruct tumors with greater\nprecision by exchanging information across various spatial\nimage domains, leading to a clearer understanding of tumor\ncharacteristics such as location, shape, and boundaries. As a\nresult, the integration of multimodal features becomes more\nfeasible for reliable segmentation tasks.\nTable I and Fig.5 shows the parameter of the model com-\nbined with CoT significantly decreases, down to only 1.7M,\nindicating that the added transformer blocks have been used\nmore consistently, helping to reduce memory and mitigate the\nrisk of over-fitting. The proposed model architecture has been\nexpanded to accommodate information synthesis needs, leading\nto an increase in training time. Furthermore, we show the\nsegmentation results of various components in Fig.3. Several\ncase studies to illustrate the success of the segmentation ac-\ncording to the structures of individual tumors. These cases\ndemonstrate insignificant differences between structures, as all\nare segmented very well.\nB. Evaluation of the influence of each modality\nIn order to evaluate how different modalities affect the\nmodel's performance in segmenting tumors, we conducted\nsequential training of the proposed model (3D Unet+CoT)\non the BraTS2019 evaluation set, excluding a modality at a\ntime. The outcomes of this experiment are displayed in Fig.6,\nrevealing that the omission of T1c has a significant negative\nimpact on the TC and ET label, while the exclusion of Flair\nleads to a decrease in performance for the WT label. Clearly,\neach modality possesses its own unique characteristics. Tic\nplays a crucial role in enhancing the structural tumor's features,\nresulting in clearer and more distinguishable boundaries [21].\nThe information conveyed by these features is instrumental in\ndetecting, classifying core and enhancing areas of the tumor.\nConsequently, if T1c is not included, the model struggles to\naccurately discern the boundary features. The differentiation be-\ntween cerebrospinal fluid and edema is aided by the suppression\nof water molecules in the FLAIR modality [21]. Consequently,\nthe FLAIR sequence has a significant impact on segmenting\nboth the entire tumor region and overall tumor volume. T1 is\nvaluable for differentiating normal tissues, however, it weakens\nthe tumor's characteristics, while T2 is primarily utilized to\ndifferentiate edema regions and improve the signal in that\nspecific area, providing valuable information for training the\nmodel. Each modality plays a crucial role and offers distinct\nfeatures, resulting in optimal segmentation performance when\ncombined.\nC. Comparison with state-of-the-arts\nTo validate the efficacy of our proposed approach, we\nbenchmark it against state-of-the-art (SOTA) segmentation ap-\nproaches on the BraTS2019 dataset. The results are displayed in\nFig. 4. Our proposed model surpasses most current SOTA meth-\nods, especially excelling in dicescore for the ET label, achieving\n82.0%, with the average dicescore of 84.2%. Nevertheless,\nalthough our approach performs well in the HD95, the CGA\nU-Net method [20] has a slightly better. These results evidence\nof effectiveness, superiority and potentiality of our method\nover previous SOTA and recent Transformer-based methods\n(Attention U-net [12], TransUNet [16]) on the validation set\nof BraTS2019.\nD. Error analysis\nAlthough the approach performs well overall, it operates\nless efficiently in specific cases. For instance, in Fig.7, the\nmodel outcomes segmentation does not entirely match the\nground truth, but in comparison to the baseline, the 3D-\nUnet + CoT model provides relatively exact segmentation. In\nthe first sample, several tumor cores and enhancing tumors\nremain not entirely accurate. On the second, the baseline model\nmissegmented the enhancing tumor and was confused by a\nbright artifact below, which is a common noise scenario. In\ncontrast, our model missegments only the enhancing tumor\nwithout being affected by the interfering noise. And, in the\nthird, both models slightly misidentified the edges of the tumor\nthat needs segmentation. Hence, the model can not precisely\nsegment the tumor of boundaries, therefore missing essential\ntumor characteristics. The ability to detect and identify some\nsmall regions within complex tumors of the 3D UNet model\nis not truly accurate. This results in the loss of information\nconcerning the tumor's boundaries with surrounding structures,\nleading to diagnostic errors. In comparison to the results of the\nbaseline, the 3D-Unet+CoT model marginally enhances specific\nerrors associated with size, shape, and location. The overall\nimage of the tumor appears more comprehensive with less\ncritical information loss. This significantly benefits providing\nreliable information to healthcare, thereby contributing to the\nformulation of optimal treatment decisions."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this paper, we introduce a robust technique for multi-\nmodal brain tumor segmentation from MRI images through\nintegration with CoT to extend the baseline architecture, to\nimprove segmentation accuracy. Specifically, CoT leverages\ntumor characteristics and contextual information by focusing\non self-attention blocks, thereby enhancing the representation\nand synthesis of output information. As a CNN-Transformer\narchitecture, it inherits the advantages of 3D-CNN in modeling\nlocal context and demonstrates the superior capability of Trans-\nformers in modeling long-range dependencies. Therefore, the\n3D UNet+CoT model effectively synchronizes characteristics,\nsupports each other in synthesizing crucial features. Conse-\nquently, this model can understand the complete tumor structure\nin detail and accuracy, including boundaries, locations, shapes,\nand sizes. Experimental results have validated the efficacy\nof the proposed approach, achieving Dicescores of 82.0%,\n81.2%, and 88.6% for the ET, TC, WT label on BraTS2019,\noutperforming several other state-of-the-art methods.\nIn the future, specialized medical pre-processing techniques\ncould be implemented on MRI images to enhance segmen-\ntation performance. Additionally, using the 3D UNet model\nas a baseline requires considerable computational resources to\nprocess large datasets. Thus, optimizing computation becomes a\nresearch focus. Furthermore, this approach can also be utilized\nfor medical image segmentation tasks associated with liver\nconditions such as fibrosis, hepatitis, or lung lesions. This\ncreates opportunities to broaden the potential applications of\nstudy methodologies in the future within the domain of medical\nimaging."}]}