{"title": "Brain Tumor Segmentation in MRI Images with 3D U-Net and Contextual Transformer", "authors": ["Thien-Qua T.Nguyen", "Hieu-Nghia Nguyen", "Thanh-Hieu Bui", "Thien B. Nguyen-Tat", "Vuong M. Ngo"], "abstract": "This research presents an enhanced approach for precise segmentation of brain tumor masses in magnetic resonance imaging (MRI) using an advanced 3D-UNet model combined with a Context Transformer (CoT). By architectural expansion CoT, the proposed model extends its architecture to a 3D format, integrates it smoothly with the base model to utilize the complex contextual information found in MRI scans, emphasizing how elements rely on each other across an extended spatial range. The proposed model synchronizes tumor mass characteristics from CoT, mutually reinforcing feature extraction, facilitating the pre-cise capture of detailed tumor mass structures, including location, size, and boundaries. Several experimental results present the outstanding segmentation performance of the proposed method in comparison to current state-of-the-art approaches, achieving Dicescore of 82.0%, 81.5%, 89.0% for Enhancing Tumor, Tumor Core and Whole Tumor, respectively, on BraTS2019.", "sections": [{"title": "I. INTRODUCTION", "content": "Brain tumors are abnormal growths of cells in the brain, which can be either malignant or benign. These tumors can significantly impact the patient's quality of life and health, especially when they grow rapidly and spread to other areas of the brain and spinal cord. Imaging methods like X-rays and MRI are used to detect brain tumors, but not all of them can show the full details of the tumor [1]. This increases the importance of using modern diagnostic methods, including artificial intelligence, to identify and classify brain tumors. Automating this procedure not only reduces costs and saves time but also lightens the workload for staff and healthcare systems, promoting efficiency and resource conservation. With their profound impact on health and life, as well as the increasing number of cases, brain tumors are not just a medical issue but also an economic and social challenge.\nMRI, a widely used medical imaging technology, is com-monly employed in clinical settings to assess brain tumors. Four main MRI modalities includeT1-weighted (T1), T2-weighted (T2), contrast-enhanced T1-weighted (T1c) and fluid attenua-tion inversion recovery (FLAIR) producing high-quality images of soft tissue abnormalities in the brain. The combination of these modalities enhances the accuracy of tumor segmentation, as depicted in Fig.1, where images from different modalities offer complementary information and mutual support.\nThe Transformer was first initially proposed by Vaswani et al. [2], an influential network architecture that represents a substantially advancement in deep learning and natural lan-guage processing. In the medical domain, the Transformer has opened up new opportunities in utilizing artificial intelligence in brain tumor segmentation from medical images. By inte-grating attention mechanisms and learning from large-scale data, the Transformer has become a strong tool for precisely and efficiently detecting and segmenting brain tumors [3], [4]. Transformer-based methods hold promise in addressing challenges in tumor segmentation, enhancing accuracy and reliability in the segmentation process.\nReference to the CT imaging study [5], we have taken inspiration and further expanded upon the research to provide more comprehensive and updated insight of brain tumor seg-mentation. Our study introduces a technique utilizing a 3D U-Net model, which has been enhanced and combine with a Transformer specifically for MRI images. By incorporating long-range information throughout the entire space, this ad-vanced method allows for the precise identification and local-ization of tumor subregions. To achieve this, we have developed a Transformer-based model called Context Transformer [6], which incorporates an improved attention mechanism to explore features and contextual information. This innovative approach not only improve the accuracy of segmentation but also ensures efficiency and effectiveness in the process. This represents a notable progress in medical image segmentation, potentially enhancing diagnosing and treating patients.\nThe key contributions of this paper are as follows:\n\u2022\nThe Contextual Transformer extended to 3D integrates"}, {"title": null, "content": "with 3D UNet model to exploit rich contextual information each layer. Achieving accurate image segmentation requires a\nin MRI images. significant amount of computational power and overall data\nThe proposed model has extended the architecture from volume increase significantly when processing 3D data.\nthe baseline, harmonizing tumor specific features sourced\nfrom CoT to extract important attributes. This comprehen-sive synthesis empowers accurate division of the complete\ntumor structure, including its location, size, shape, and\nboundaries. The best scoring results on the BraTS2019\ndataset are 82.0%, 81.5%, 89.0% respectively, for labels\ncorresponding to Enhancing Tumor, Tumor Core, and\nWhole Tumor.\nThe rest of paper is structured as: Section 2 reviews related\nworks. Section 3 show the proposed method. Section 4 de-lineates the experimental results, while Section 5 provides a\nsummary of the paper's content and future work."}, {"title": "II. RELATED WORK", "content": "Image segmentation plays a crucial role in the healthcare field, particularly in diagnosing and treating diseases. Various techniques have been developed for segmenting brain tumor im-ages [7], [8], including both traditional machine learning (ML) methods and deep learning (DL) techniques. ML methods such as Support Vector Machines [9] and Graph Theory [10], have limitations in extracting statistical Information from large sam-ples, resulting in weak segmentation performance. However, DL-based methods, particularly Convolutional Neural Network (CNN) based methods like 3D U-Net [11] and Attention U-Net [12], have proven to be more effective in addressing this issue. These networks are capable of processing input images of any size and utilize decoding layers to adjust the size of feature maps to match the dimensions of the original image. CNN-based models with U-shaped architectures, have made significant advancements and demonstrated great potential in 2D and 3D image segmentation tasks. Nonetheless, the posi-tioning of convolutional layers within the network architecture may lead to the ignore of long-range information correlations. Research [13] has indicated that achieving good segmentation results requires a model that can simultaneously extract both local details and global semantic information interactions.\nTransformer-based methods can address above issue. Liu et al. introduced the Swin Transformer, utilizing self-attention mechanisms based on windows to decrease parameters and computations, while employing a shifted window mechanism to realize global dependencies. Furthermore, Lin et al., introduced DS-TransUNet, a Transformer architecture similar to Unet for segmentation of medical images, achieving performance comparable to state-of-the-art CNN-based methods [14], [15]. However, the Transformer neglects local structures by dividing the image into patches represented as tokens.\nTargeting the weaknesses of both CNN-based and Transformer-based networks, combining these structures can complement each other to exploit long-range spatial relationships. TransUnet[16] marks the debut of Transformer in CNN. The CNN block of this work is implemented before Transformer. Then, features are restored by sampling through"}, {"title": "III. METHOD", "content": "In this paper, we introduce a network depicted in Fig. 2, based on previously introduced transformer modules but incorporates enhanced channel attention modules. This allows us to explore spatial information and contextual in MRI images comprehensively, exploit features thoroughly, and improve the representation of various tumor regions. Consequently, we ad-dress the challenge of accurately capturing detailed information about both the entire tumor architecture and the characteristics of individual subregions, thereby enhancing segmentation ac-curacy. Our proposed network contains two main components: Fig. 2a: the 3D-UNet backbone and Fig. 2b: the 3D context-aware transformer module within encoder-decoder"}, {"title": "A. 3D Contextual Transformer (CoT)", "content": "The 2D contextual transformer module, aimed at utilizing contextual information within input features, was initially pro-posed by Li et al. [6], limited at 2D feature maps. In order to overcome this constraint, a 3D Contextual Transformer block is proposed, as depicted in Fig. 2b. This CoT block integrates the utilization of contextual information and self-attention learning within a unified framework. It extensively leverages contextual information among adjacent keys to effectively support the self-attention learning process, thus improving the representation capability of the resulting output feature maps.\nInitially, the 3D input feature map $X \\in R^{H \\times W \\times D \\times C}$, with di-mensions (H, W, D) and C channels, undergoes transformation into keys K, values V and queries Q using learned embedding matrices WK, Wv and W\u0119, respectively. Subsequently, contex-tual information $K_1 \\in R^{H \\times W \\times D \\times C}$ for the input X is derived by applying a kxkxk convolution across all adjacent keys to contextualize each key representation K. This convolution inherently captures static contextual information among local neighboring keys. Next, the contextual keys $K^1$ and queries Q are merged, and the resultant matrix undergoes two consecutive 1 \u00d7 1 \u00d7 1 convolutions to produce the attention matrix A. The equation for this process is as follows.\n$A = [K^1, Q] W_{\\Theta}W_{\\Psi}$ (1)\nwhere, $W_{\\Theta}W_{\\Psi}$ are learnt parameters.\nIn the subsequent step, dynamic contextual representations are obtained by performing element-wise multiplication be-tween the feature map A and the values V\n$K^2 = V * A$ (2)\nThe CoT block produces the final output (Y) by merging the static context $K^1$ with the dynamic context $K^2$."}, {"title": "B. 3D-UNet model and Loss function", "content": "The 3D UNet model is a neural network variant commonly employed in medical image processing, especially for the segmentation of 3D medical scans like MRI or CT images. Derived from the U-Net architecture [17], a widely-used deep neural network in medical image analysis and segmentation, the 3D UNet model facilitates high-precision segmentation in 3D space. It achieves this by integrating down-sampling and up-sampling layers to analyze spatial information extracted from original images.\nThe Dice Loss has become increasingly popular as a loss function in semantic segmentation tasks. Its purpose is to measure and regulate the intersection between ground truth and predictions by optimizing the Dice coefficient directly. Within the module, both Dice Loss and cross-entropy loss are utilized to optimize the parameters. The definition of Dice Loss is as follows:\n$L_{dice}(y, \\hat{y}) = 1- \\frac{2 \\cdot \\sum_{c \\in \\Omega} \\sum_{i=1}^{N}y_i\\hat{y_i} + \\epsilon}{\\sum_{i=1}^{N}(y_i)^2 + \\sum_{i=1}^{N}(\\hat{y_i})^2 + \\epsilon}$ (3)\nand cross-entropy loss function is defined as follows:\n$L_{CE}(y,\\hat{y}) = \u2013 \\sum_{c \\in \\Omega} \\sum_{i=1}^{N}y_i log \\hat{y_i}$ (4)\nwhere \u03a9 = {BG(background),NCR/NET,ED,ET}. y and $\\hat{y}$ denote the ground truth and probability prediction of voxel i on class c, respectively. N = H \u00d7 W \u00d7 D, $\\epsilon$ = 1 \u00d7 10\u22125.\nConsequently, due to equations (3) and (4), the ultimate loss function is a weighted combination of the Dice Loss and cross-entropy loss, as indicated by the formula:\n$L_{Seg}(y, \\hat{y}) = \\alpha L_{dice}(y, \\hat{y}) + (1 \u2212 \\alpha)L_{CE}(y, \\hat{y})$ (5)\nwhere \u03b1 is a hyperparameter that regulates the impact of Dice loss and cross-entropy loss."}, {"title": "IV. DATASET AND EXPERIMENTS", "content": "A. Evaluation Metrics\nThe accuracy of segmentation in this research is assessed by employing the Dice score and Hausdorff distance (95%) metrics to evaluate enhancing tumor region (label 4), regions within the tumor core (label 1, 4) and entirety of tumor region (label 1, 2, 4).\nThe formula for calculating the Dice Score is given by:\nDiceScore = $\\frac{2TP}{FN+FP+2TP}$ (6)\nwhere TP represents the number of true positives, FN represents the number of false negatives, and FP represents the number of false positives. To measure the dissimilarity between the actual surface of a region and the predicted region, the Hausdorff95 distance metric is employed. This metric is particularly sensitive to the boundaries of the segmented region and formally defined as follows:\n$HD_{95}(T, P) = max \\{ sup_{t \\in T} d(t, P), sup_{p \\in P} d(T, p) \\}$ (7)\nThe supremum operator, denoted as sup, is used in the context where t and p represent points on the surface T of the ground-truth region and the surface P of the predicted region. The function d(t,p) calculates the distance between t and p."}, {"title": "B. Implementation details", "content": "Datasets: The proposed method is evaluated using a dataset BraTS2019, which is provided by Brain Tumor Segmentation (BraTS) challenge. For training purposes, BraTS2019 consists of 335 patient cases. The validation set comprises MRI scans from 125 cases, with labels that are unknown. To train our model, we only utilize the labeled data, splitting it into 80/20 for training and testing. These datasets consist of co-registered, skull-stripped and resampled MRI images at a resolution of"}, {"title": "V. RESULTS AND DISCUSSION", "content": "A. Ablation study\nTo evaluate the performance of the transformer block on two datasets, we conducted experiments using a combined model, and then compared them to the baseline, against the same evaluation set for each dataset.\nContextual Transformer (CoT): According to metrics from Table I, the combination of baseline + CoT demonstrates a considerable improvement in Dicescores for ET, achieving 82.0% (an increase of 5.6%) in comparison to the baseline. Besides that, the TC and WT label achieve 81.5% and 89.0%, respectively, resulting in a mean Dicescore increase of 2.2%. Furthermore, there is also an enhancement in the maen HD95, reduced by 1.1mm on BraTS2019 evaluation set. The incor-poration of CoT blocks has resulted in a significant decrease in segmentation errors in all areas of the tumor, providing strong evidence of its effectiveness in improving the ability to differentiate between tumor subregions and enhancing overall segmentation performance. Additionally, the 3D UNet+CoT model prioritizes the interaction of contextual information to further improve segmentation accuracy. This experiment show-cases the model's capacity to reconstruct tumors with greater precision by exchanging information across various spatial image domains, leading to a clearer understanding of tumor characteristics such as location, shape, and boundaries. As a result, the integration of multimodal features becomes more feasible for reliable segmentation tasks."}, {"title": "B. Evaluation of the influence of each modality", "content": "In order to evaluate how different modalities affect the model's performance in segmenting tumors, we conducted sequential training of the proposed model (3D Unet+CoT) on the BraTS2019 evaluation set, excluding a modality at a time. The outcomes of this experiment are displayed in Fig.6, revealing that the omission of T1c has a significant negative impact on the TC and ET label, while the exclusion of Flair leads to a decrease in performance for the WT label. Clearly, each modality possesses its own unique characteristics. Tic plays a crucial role in enhancing the structural tumor's features, resulting in clearer and more distinguishable boundaries [21]. The information conveyed by these features is instrumental in detecting, classifying core and enhancing areas of the tumor. Consequently, if T1c is not included, the model struggles to accurately discern the boundary features. The differentiation be-tween cerebrospinal fluid and edema is aided by the suppression of water molecules in the FLAIR modality [21]. Consequently, the FLAIR sequence has a significant impact on segmenting both the entire tumor region and overall tumor volume. T1 is valuable for differentiating normal tissues, however, it weakens the tumor's characteristics, while T2 is primarily utilized to differentiate edema regions and improve the signal in that specific area, providing valuable information for training the model. Each modality plays a crucial role and offers distinct"}, {"title": "C. Comparison with state-of-the-arts", "content": "To validate the efficacy of our proposed approach, we benchmark it against state-of-the-art (SOTA) segmentation ap-proaches on the BraTS2019 dataset. The results are displayed in Fig. 4. Our proposed model surpasses most current SOTA meth-ods, especially excelling in dicescore for the ET label, achieving 82.0%, with the average dicescore of 84.2%. Nevertheless, although our approach performs well in the HD95, the CGA U-Net method [20] has a slightly better. These results evidence of effectiveness, superiority and potentiality of our method over previous SOTA and recent Transformer-based methods (Attention U-net [12], TransUNet [16]) on the validation set of BraTS2019."}, {"title": "D. Error analysis", "content": "Although the approach performs well overall, it operates less efficiently in specific cases. For instance, in Fig.7, the model outcomes segmentation does not entirely match the ground truth, but in comparison to the baseline, the 3D-Unet + CoT model provides relatively exact segmentation. In the first sample, several tumor cores and enhancing tumors remain not entirely accurate. On the second, the baseline model missegmented the enhancing tumor and was confused by a bright artifact below, which is a common noise scenario. In contrast, our model missegments only the enhancing tumor without being affected by the interfering noise. And, in the third, both models slightly misidentified the edges of the tumor that needs segmentation. Hence, the model can not precisely segment the tumor of boundaries, therefore missing essential tumor characteristics. The ability to detect and identify some small regions within complex tumors of the 3D UNet model is not truly accurate. This results in the loss of information concerning the tumor's boundaries with surrounding structures, leading to diagnostic errors. In comparison to the results of the baseline, the 3D-Unet+CoT model marginally enhances specific errors associated with size, shape, and location. The overall image of the tumor appears more comprehensive with less critical information loss. This significantly benefits providing reliable information to healthcare, thereby contributing to the formulation of optimal treatment decisions."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this paper, we introduce a robust technique for multi-modal brain tumor segmentation from MRI images through integration with CoT to extend the baseline architecture, to improve segmentation accuracy. Specifically, CoT leverages tumor characteristics and contextual information by focusing on self-attention blocks, thereby enhancing the representation and synthesis of output information. As a CNN-Transformer architecture, it inherits the advantages of 3D-CNN in modeling local context and demonstrates the superior capability of Trans-formers in modeling long-range dependencies. Therefore, the 3D UNet+CoT model effectively synchronizes characteristics, supports each other in synthesizing crucial features. Conse-quently, this model can understand the complete tumor structure in detail and accuracy, including boundaries, locations, shapes, and sizes. Experimental results have validated the efficacy of the proposed approach, achieving Dicescores of 82.0%, 81.2%, and 88.6% for the ET, TC, WT label on BraTS2019, outperforming several other state-of-the-art methods.\nIn the future, specialized medical pre-processing techniques could be implemented on MRI images to enhance segmen-tation performance. Additionally, using the 3D UNet model as a baseline requires considerable computational resources to process large datasets. Thus, optimizing computation becomes a research focus. Furthermore, this approach can also be utilized for medical image segmentation tasks associated with liver conditions such as fibrosis, hepatitis, or lung lesions. This creates opportunities to broaden the potential applications of study methodologies in the future within the domain of medical imaging."}]}