{"title": "Case-Enhanced Vision Transformer:\nImproving Explanations of Image Similarity with a ViT-based Similarity Metric", "authors": ["Ziwei Zhao", "David Leake", "Xiaomeng Ye", "David Crandall"], "abstract": "This short paper presents preliminary research on\nthe Case-Enhanced Vision Transformer (CEViT), a\nsimilarity measurement method aimed at improv-\ning the explainability of similarity assessments for\nimage data. Initial experimental results suggest that\nintegrating CEViT into k-Nearest Neighbor (k-NN)\nclassification yields classification accuracy compa-\nrable to state-of-the-art computer vision models,\nwhile adding capabilities for illustrating differences\nbetween classes. CEViT explanations can be influ-\nenced by prior cases, to illustrate aspects of simi-\nlarity relevant to those cases.", "sections": [{"title": "Introduction", "content": "In computer vision, Convolutional Neural Networks (CNNs)\nfor image classification have achieved remarkable perfor-\nmance, but are black-box systems that cannot be explained\ndirectly [Adadi and Berrada, 2018]. In contrast, traditional\nmachine learning algorithms such as k-Nearest Neighbors (k-\nNN) are inherently interpretable, due to the ability to present\nthe examples on which decisions are based. However, their\naccuracy tends to lag behind that of modern ML models.\nThe interpretability of k-NN stems from its reliance on ex-\nisting data. This process is analogous to Case-Based Reason-\ning (CBR), a traditional AI paradigm that involves solving\nnew problems by recalling and adapting solutions from past\nexperiences, or \"cases\". The interpretability of CBR systems\nhas long been seen as an important strength, and human sub-\njects studies support that cases provide compelling explana-\ntions. [Cunningham et al., 2003; Gates et al., 2023].\nThe core factor impacting the performance of k-NN is the\nsimilarity measurement function it uses. Traditional methods\nfor determining similarity, such as L1 or L2 distance, prove\ninadequate for image data. This is because two images may\nnot align perfectly, and attempting to explain visually by con-\ntrasting two images in pixel space results in a blurry image\nlacking explanatory information. Similarity measurement is\nalso important for CBR studies to explain why a particular\ncase was retrieved [Massie et al., 2004], and for cases with\nfeature-vector problem representations, explanations may be\nbased on feature-by-feature comparisons. This approach is\nnot useful for images processed on the pixel level.\nHumans excel at explaining the differences between im-\nages concisely and accurately, commonly using more abstract\nimage characteristics to underpin their explanations. For in-\nstance, as depicted in Figure 1a, it is plausible that the dif-\nference between an image of the digit 7 and an image of the\ndigit 9 as characterized by a human would lie in the upper\nstroke, as shown by the red boxes. Recent advances in com-\nputer vision include highly accurate similarity measurement\nmodels, such as Siamese Neural Networks [Koch et al., 2015;\nLeal-Taix\u00e9 et al., 2016]. Despite their high performance,\nthese methods share the same issue that they operate as black\nboxes. They commonly use convolutional Neural Networks\n(CNNs) to generate features for determining similarity result-\ning in two potential pitfalls: the process of feature generation\nlacks explainability, and the features themselves may have\nlimited visual connection to the original image.\nThe Vision Transformer (ViT) [Dosovitskiy et al., 2020]\nstands out as a revolutionary architecture. Unlike conven-\ntional CNNs, ViT uses a transformer architecture [Vaswani\net al., 2017], enabling it to capture dependencies across sub-\npatches of an image. This approach has yielded remarkable\nperformance across various computer vision tasks, such as\nclassification, object detection, and segmentation.\nThe core of a transformer model is the Query-Key-Value\n(QKV) attention mechanism. The QKV mechanism com-\nputes attention scores between query (Q) and keys (K), de-\ntermining the relevance and importance of each value (V)\nwith respect to the query. The attention scores (Q\u00d7K) offer\ngreat potential for explainability, as they naturally highlight\nthe parts of the query that are important during the process.\nUnfortunately, in traditional classification tasks, these atten-"}, {"title": "2 Related Work", "content": "2.1 Explainable AI for Computer Vision\nModern computer vision techniques have gained huge suc-\ncess through the use of deep learning models. These models\noften operate as black boxes due to their vast number of pa-\nrameters and complex layer connections, making it challeng-\ning to understand their inner function. Tremendous effort has\nbeen devoted to opening the black box by explaining their\ndecision making process. As some examples, Saliency maps\nhighlight regions in an image that are most activated for the\nsystem's prediction [Simonyan et al., 2013]; Grad-CAM uses\nthe gradients of a target concept (e.g., a dog in an image)\nin convolutional layers to produce a coarse localization map\n[Selvaraju et al., 2017]; Attention mechanisms assign weights\nto parts of the input image and higher weights indicate more\nimportance in the model's decision [Guo et al., 2022].\n2.2 Applying CBR to Image Data\nCBR can be applied to many tasks that involve image in-\nputs. In contrast to common computer vision techniques,\nCBR methods can provide similar prior cases as explanations\nof current decisions. Ye et al. [2021] applied the Case Dif-\nference Heuristic (CDH) approach for learning case adapta-\ntions to predict age from facial images. The Class-to-Class\nVariational Autoencoder [Ye et al., 2022; Zhao et al., 2022]\nlearns inter-class patterns, showing potential in few-shot im-\nage generation and counterfactual visual explanations. Kenny\nand Keane [2020] proposed PIECE, a method that modifies\n\"exceptional\" features in images to generate counterfactual\nexplanations for black-box CNN classifiers. Kenny and col-\nlaborators [Kenny and Keane, 2019; Kenny et al., 2021] in-\ntroduced a twin-system that pairs a black box Neural Net-\nwork with a white box CBR system to identify nearest neigh-\nbor cases explaining image classification outcomes. Ye et al.\n[2020] integrates CBR with a Siamese Network for image\nclassification and discovering prototypical cases. Our work\ncontrasts with previous research by using the Vision Trans-\nformer, a novel backbone model in computer vision, instead\nof a traditional CNN.\n2.3 Similarity Metrics in CBR\nCase retrieval in CBR relies on a similarity metric to deter-\nmine the distances between cases. For simple data sets in\nwhich the problem addressed by the case is described as a fea-\nture vector, Euclidean distance may suffice. Metric learning\nmethods such as NCA [Goldberger et al., 2004] and LMNN\n[Weinberger and Saul, 2009] transforming the input space to\nenhance accuracy. Mathisen et al. [2019] proposed and ex-\ntended siamese neural network that learns an embedding of\ncases and calculates distances between cases. For high di-\nmensional, complex, or structural data (e.g. audio, video,\nimage, text), deep learning models are often used to extract\nfeatures to which the similarity metric is applied [Sani et al.,\n2017; Wilkerson et al., 2021; Turner et al., 2018]."}, {"title": "3 The CEVIT Method", "content": "In this section, we begin by explaining the design of our trans-\nformer model (CEViT). Then, we show the procedure for ac-\ncessing the attention mask of the transformer model to en-\nhance model explainability. Finally, we present a classifica-\ntion pipeline that combines CEViT with k-NN by using CE-\nViT as the distance metric for k-NN.\n3.1 Model design\nWe follow the model design of the Vision Transformer (ViT)\nintroduced in Dosovitskiy et al. [2020]. Traditionally, ViT\ncomprises 3 modules:\n\u2022 Patchify process: It splits the input image into smaller\npatches and processes them into tokens via linear pro-\njection and positional embedding, as shown in Figure 2.\n\u2022 Transformer module: This module comprises encoder\nblocks consisting of normalization, MLP (Multi-Layer\nPerceptron), and multi-head attention layers.\n\u2022 MLP head: This produces the desired output, such as\nclass labels ranging from 0 to 9 for digit recognition task.\nOur method, CEVIT, modifies ViT in two ways. First, in-\nstead of taking a single h\u00d7w\u00d7c image as input, CEViT con-\ncatenates the input image with a reference image along the\nchannel dimension, resulting in an image-like tensor with di-\nmensions hxw\u00d72c. Second, we modify the MLP head to\nproduce a single score between 0 and 1, indicating the likeli-\nhood that the two images belong to the same class. Figure 3\nillustrates the ViT and CEViT processes. For an explanation\nof each ViT module, please see Dosovitskiy et al. [2020]."}, {"title": "3.2 Attention Mask", "content": "During the inference of a ViT model, each encoder block\ncomputes an attention mask between tokens. Typically, ViT\nincludes an additional CLS (classification) token, utilizing it\nas an aggregate representation for all tokens to generate the fi-\nnal prediction. Consequently, we can trace back the attention\nmasks at each encoder level to observe how the CLS token\nattended to the image tokens, with each token representing a\nregion in the original image, as shown in Figure 4. Hence,\nthese attention masks can be used to visualize how each re-\ngion of the original image influenced the final classification.\nIn CEVIT, by augmenting the input image with the ref-\nerence image, the attention masks visualize the regions that\ninfluenced the similarity or dissimilarity between the query\nimage and the reference image. This visual representation is\nparticularly valuable for explaining classification differences.\n3.3 Using CEVIT for classification\nTo employ CEViT for classification, we integrate it into ak-\nNN system by running CEViT between the query image and\nreference images from each class. The K samples with the\nhighest similarity scores then vote for the final prediction."}, {"title": "4 Experiments", "content": "We evaluated the classification accuracy and explainability\nof CEViT against a standard ViT and k-NN on the MNIST\nLeCun et al. [1998] dataset.\n4.1 Implementation Details\nWe implemented both ViT and CEViT using the Pytorch\nframework [Paszke et al., 2019] and the Torchvision library\nmaintainers and contributors [2016]. To ensure a fair com-\nparison, we used the same backbone for both ViT and CEV\u0130T,\nincorporating a patchify module that subdivides the input into\n49 (7 \u00d7 7) small patches, and a transformer module consisting\nof 6 encoder layers.\nDuring training, CEViT was given pairs of images, each\nwith a 50% probability of being from the same class. The\noutput of CEViT is a score between 0 and 1, indicating the\nlikelihood that the images belong to the same class. We then\napplied cross-entropy loss, where a ground truth label of 0\n(resp. 1) indicates different (resp. same) classes.\nBoth ViT and CEViT were trained for 200 epochs using\nthe AdamW optimizer [Loshchilov and Hutter, 2017] with\nan empirically set learning rate of 0.001. We decreased the\nrate by 10% per 20 epochs. For classification, we integrated\nCEViT into a k-NN with K=15, chosen empirically.\n4.2 Quantitative Evaluation\nWe first compare the fundamental performance of ViT, k-NN\nwith Euclidean distance, and k-NN using CEViT as similarity\nmeasure. As shown in Table 1, the classification accuracy of\nCEViT+k-NN is higher than k-NN and comparable to that\nof ViT. Meanwhile, CEViT preserves the advantage of k-NN\nby providing nearest cases as explanations and can generate\nattention masks more related to class differences, potentially\nleading to better explainability.\nWe also introduce a quantitative evaluation method to as-\nsess the explainability of the generated attention masks. The\nmethod is based on the premise that the parts of an image\ncontributing to class differences best explain classifications,\nand that the extent to which a mask identifies such parts can\nbe judged by its ability to guide the modification of a query\nimage to match a distractor class.\nAs depicted in Figure 5, we start by selecting a query image\nwith class q and a distractor image with class d. Subsequently,\nwe conduct classification of the query image using both ViT\nand CEViT, resulting in attention masks My and Mc. Ad-\nditionally, we employ a uniform mask Mu with consistent\nvalues across all pixels. The uniform mask Mu serves as a\nbaseline, against which we assess the relative performance\nimprovements of ViT and CEVIT. All three attention masks\nare normalized to have the same mean value \u03bc. After the\npatchify process which divides the images into N2 smaller\nimage patches, we acquire image patches Pq1...PqN2 and\nPdi... PdN2, and merge them into hybrid patches Ph1... PhN2\nusing the following equation:\n$Phi = (1 - Mi) Pqi + MiPdi  \u03af\u2208 {1, ..., \u039d2}$ (1)"}, {"title": "that CEVIT's attention masks are better than ViT's in modi-\nfying images towards the distractor class, demonstrating our\nmodel's enhanced capability for explaining class differences.", "content": "4.3 Qualitative Evaluation\nWe present several qualitative results in Figure 6. To en-\nhance visualization, we sharpened the attention masks of both\nCEVIT and ViT by scaling them to a range of 0-1 and sub-\nsequently filtering out pixels with values less than 0.5. As\nshown in Figure 6, the attention masks generated by CEV\u0130T\nare notably more focused and better illustrate class differ-\nences compared to those generated by ViT. The supplemen-\ntary materials provide additional qualitative samples.\n5\nConclusion and Future Work\nIn this paper, we introduce the Case-Enhanced Vision Trans-\nformer (CEVIT), a novel similarity metric for the CBR re-\ntrieval process. Initial experiments show encouraging classi-\nfication performance for CEViT combined with KNN, along\nwith the capability to explain class differences by generat-\ning attention masks. We highlight two directions for future\nresearch: Investigating CEViT's performance on more chal-\nlenging dataset, and using CEViT's capabilities to assist in\ngenerating counterfactual and semi-factual explanations."}]}