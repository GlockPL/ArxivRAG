{"title": "Vickrey Feedback: Cost-efficient Data Construction for Reinforcement Learning from Human Feedback", "authors": ["Guoxi Zhang", "Jiuding Duan"], "abstract": "This paper addresses the cost-efficiency aspect of Reinforcement Learning from Human Feedback (RLHF). RLHF leverages datasets of human preferences over outputs of large language models (LLM)s to instill human expectations into LLMs. While preference annotation comes with a monetized cost, the economic utility of a preference dataset has not been considered by far. What exacerbates this situation is that, given complex intransitive or cyclic relationships in preference datasets, existing algorithms for fine-tuning LLMs are still far from capturing comprehensive preferences. This raises severe cost-efficiency concerns in production environments, where preference data accumulate over time. In this paper, we see the fine-tuning of LLMs as a monetized economy and introduce an auction mechanism to improve the efficiency of the preference data collection in dollar terms. We show that introducing an auction mechanism can play an essential role in enhancing the cost-efficiency of RLHF, while maintaining satisfactory model performance. Experimental results demonstrate that our proposed auction-based protocol is cost-efficient for fine-tuning LLMs by concentrating on high-quality feedback.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have revolutionized natural language processing and artificial intelligence, enabling unprecedented capabilities in generating, summarizing, and understanding text at a high cognitive level [29,36,19,10]. The delivery of these models in domain-specific applications relies on fine-tuning, i.e., aligning the LLMs closely with human preferences. A key to successful fine-tuning is the construction of a high-quality dataset, which is an emerging but challenging problem [27,23,35]. Conventional fine-tuning methods rely heavily on human-annotated preference datasets to guide the model toward generating qualified content for domain usage. The success of these methods is backed by available datasets and has reinforced the urgent demand for large-scale high-quality datasets [24,11]. Despite the proven effectiveness in the conventional"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Datasets for RLHF", "content": "Tremendous efforts have been made in dataset construction to facilitate RLHF, enabling diverse applications and contexts for RLHF. Such datasets combine human feedback, training data, interaction logs, reward signals, and evaluation metrics to train and fine-tune language models according to human preferences [27,23,11,13].\nAlignment of different perspectives, e.g., helpfulness and harmlessness [1] can improve performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as coding and text summarization [12]. As helpfulness and harmlessness often stand in opposition to each other, preference models trained to evaluate one of these qualities primarily have unfavorable performance (much worse than chance) on the other. Assembled with data that contains multiple aspects, the preference model can nevertheless learn the right lessons and behave helpfully when appropriate, while encouraging the polite refusal of harmful requests.\nMoreover, many other qualitative aspects of human preferences, e.g., instruction following, honesty, and truthfulness have been explored [4]. Given the diverging nature of multiple aspects in preference annotation, primitive quality assurance can be adopted [11]. For example, instruction following ratings ensure the comprehension of the intention of human instructions without deviating from the requirements. Honesty reflects what they (don't) know and express uncertainty when they are in waver towards the given problem. Truthfulness rating reveals the alignment between the instructions and real-world knowledge, not fabricating any facts without references or introducing any self-contradiction."}, {"title": "2.2 Mechanism Design in RLHF", "content": "Mechanism design is an area in economics and game theory that focuses on creating systems or protocols (mechanisms) that lead to desirable outcomes for individual agents and social good. In the context of RLHF, mechanism design can have presence in 2 prospective tasks. The first task is aggregating inputs from multiple LLM agents. This task requires designing auction mechanisms that can incentivize the agents to reveal their preferences truthfully. Token Auction Model is representative and operates on a token-by-token basis, allowing LLM agents to influence content generation through single-dimensional or informed multi-dimensional bids [6,28]. The second prospective task is cost-efficient data construction, which is a prerequisite for achieving desirable agent performance. Collecting instruction responses and human preference requires cost in dollar term and has been a bottleneck in budgeting and model operation in production environment. This bottleneck is alternatively termed as alignment tax, due to its connection with degenerated model performances [7,17]. Other related efforts include adopting game-theoretic perspectives for fine-tuning, e.g., Nash Direct Optimization [25], nonetheless limited attention is paid on data construction."}, {"title": "3 Preliminaries", "content": null}, {"title": "3.1 Vanilla Preferences", "content": "Notations In RLHF, human preferences are encoded as pairwise comparisons between model responses. Specifically, a preference sample is a triplet $(x, y_a, y_r)$, where x is the instruction that describes the desired response, $y_a$ is the accepted (preferred) response, and $y_r$ is the rejected (not preferred) response. For example, x can be \"Write a code snippet to compute the Fibonacci sequence.\", $y_a$ can be a program that runs successfully, and $y_r$ can be a program with syntactic errors.\nTo construct a preference dataset D, we need to sample instructions, generate responses, and annotate responses with preferences. For a specific application, the instructions can be sampled from history logs and thus often free of charge. For each instruction, we need at least two responses, which are typically generated by commercial LLMs. Finally, preference annotation means to determine the accepted and rejected responses, and it also done by invoking commercial LLMs. Take the UltraFeedback [4] dataset as an example. Its instructions are collected from public NLP datasets such as FLAN [18] and Evol-Instruct[34]. For each instruction, four responses are generated by either commercial or open-source LLMs and rated by GPT-4. The accepted responses are the ones with the highest rating, while the rejected response is sampled from the remaining ones [2]. This paper refers to preference samples generated with the above-mentioned pipeline as the vanilla preferences.\nCost of Vanilla Preferences When instructions are sampled from historical data, the cost of a vanilla preference dataset comprises of the cost of model responses and preference annotation. In this paper, we focus on the cost of model responses. Currently, commercial LLMs charge for the amount of input and output texts, which are measured in tokens. So the cost of a model response is proportionate to the length of its instruction and itself."}, {"title": "3.2 Learning from Preference Datasets", "content": "Technically, an LLM is a generative model for texts. Denote by $\\pi_{\\theta}(y|x)$ the likelihood of response y conditioned on instruction x, where $\\theta$ refers to the parameters"}, {"title": "3.3 Vickrey Auction", "content": "In Vickrey auction, bidders submit written bids without knowing the other people's bids in the auction. The highest bidder wins but the price paid is the second-highest bid. Generalized variants of the Vickrey auction for multiunit auctions exist, such as the generalized second-price auction used in online advertisement programs and the Vickrey-Clarke-Groves (VCG) auction [30]. In VCG auction, truthful bidding of each bidder is theoretically justifiable and incentivized through the mechanism that the buyer will adopt the highest bid but only pay for the second-highest bid price. The property of VCG is well studied in economic theory and streched under reasonable assumptions [21]."}, {"title": "4 VickreyFeedback", "content": "In this paper, we propose a data collection pipeline based on Vickrey auction to achieve quality assurance under a limited budget. Inspired by the multi-unit"}, {"title": "4.1 Assumptions", "content": "Recent discussions on data diversity, quality, and quantity argue that scaling up diversity and output quality has measurable positive effects for achieving better alignment while scaling up quantity alone might not [39]. We follow the intuition and assume that the quality of model responses is proportional to the length of the respective response. Therefore, accepting a longer response is a proxy for selecting a higher-quality response when no additional quality control protocol is available [14]."}, {"title": "4.2 Protocol", "content": "The data collection protocol of VickreyFeedback is illustrated in Algorithm 1. The protocol requires multiple LLM agents as response suppliers given an instruction I. Each supplier is required to submit a response $r_i$ and a self-evaluated quality $q_i$. In our experiment, $q_i = length(r_i)$ holds by assumption, where $length(r)$ denotes the token length of a response r. The mechanism selects the two agents that provided the longest responses and pays only the second longest responses to each winning agent. Consequentially, the responses that have a lower quality or shorter token length will not receive rewards and the information will not be fed to the fine-tuning of LLMs.\nConsidering the cost budget of the dataset owner in the dataset construction for RLHF, truth-telling from all the suppliers is important because it is the prerequisite for allocating limited monetary resources from the dataset owner to the data suppliers. The rationale for data suppliers doing so relies on whether bidding truthfully is a dominant strategy for each of the suppliers [22].\nIt is trivial to prove that under mild conditions, the total cost $C_{total}$ by the proposed VickreyFeedback protocol is smaller than the total construction cost of the conventional methods in RLHF. Besides, our proposed protocol is organic to RLHF by enabling a control over the dataset construction cost, while not degenerating the performance. This is the first effort that incorporates a mechanism into dataset construction in RLHF, supported by the truthful bidding property of the auction mechanism. Moreover, by proposing an associated algorithm, we compensate the information loss that might happen when dropping diversified answers in the data collection phase."}, {"title": "4.3 Quality-Adjusting DPO", "content": "A downside of Vickrey Feedback is that it sacrifices data diversity for data quality to attain better cost efficiency. Recall that for each instruction, we do not collect the responses whose declared quality is below the second-to-best quality. The responses included in a Vickrey preference dataset are thus mostly good responses. This data skewness could be problematic for RLHF as models cannot learn a wide spectrum of human preferences but overfit to a narrow part of them. Put differently, a model might capture the nuances of high-quality responses but is not trained to prevent generating low-quality responses."}, {"title": "5 Experiments", "content": "In this section, we aim to demonstrate that the cost for preference collection can be reduced with Vickrey Feedback if the assumptions in Section 4 are satisfied. Further, we shall show that our proposed QA-DPO can address the downside of Vickrey preferences."}, {"title": "5.1 Setup", "content": "Datasets We use the binarized version [2] of the UltraFeedback [4] as vanilla preference samples. The original UltraFeedback dataset contains 63,967 instructions and four responses for every instruction. The responses are annotated by GPT-4 with four scores (from one to five) to evaluate them from four aspects: instruction-following, truthfulness, honesty, and helpfulness. In the binarized version [2], the four scores of response are averaged into an overall score. The accepted response of an instruction is the one with the highest overall score, and the rejected response is sampled uniformly at random from the remaining three responses. This dataset is referred to as vanilla preferences in our results.\nWe simulate the proposed VickreyFeedback pipeline and generate synthetic Vickrey preferences. Based on the assumption that response providers report response quality truthfully, we use the overall scores as declared quality. For each instruction, the accepted response is still the one with the highest overall score, but the rejected response is the one with the second-highest overall score. To investigate the influence of data size, we experiment with 25%, 50%, and 100% of samples."}, {"title": "5.2 Results", "content": "We start with presenting results for model performance and the trade-off between data quality and diversity. Then, we compare models from the perspective of cost and discuss the pros and cons of VickreyFeedback.\nFigure 2a shows the win rate against Base when using different subsampling ratios. When using 25% of the samples, both Vickrey-DPO and Vickrey-QA outperform Vanilla-DPO. As we use more samples, Vanilla-DPO begins to outperform Vickrey-DPO, but Vickrey-QA is still comparable with Vanilla. Figure 2b provides an in-depth analysis of the performance of Base, Vanilla-DPO,"}, {"title": "6 Discussion", "content": null}, {"title": "6.1 Diversity of Preference", "content": "As illustrated in Figures 3a, 3b and 4, preferences collected by Vickrey Feedback are less diversified than those collected with the conventional approach.\nThe limited diversity of the sampled dataset is due to small data constraints, restricted by the dataset owner's budget for collecting data. Certain ablation studies on data diversity, quality, and quantity have shown that data quantity is"}, {"title": "6.2 Cost-efficient Data Construction", "content": "To the best of our knowledge, this is the first research speculating on the economic aspects of dataset construction in RLHF.\nIn the context of Direct Preference Optimization (DPO) and dataset collection for fine-tuning Large Language Models (LLMs), there exists a critical trade-off among cost, preference diversity and fine-tuning performance [17]. DPO aims to refine models based on direct human preferences, necessitating a broad and diverse set of preference data to capture the nuanced and varied requirements of different users. However, obtaining such a diverse dataset is often expensive, as each instance involves a monetary cost. Therefore, striking the right trade-off is crucial and an auction mechanism, as proposed in this paper, can help navigate the resource allocation towards a maximized utility budgets for model fine-tuning."}, {"title": "7 Conclusion", "content": "This paper investigates a cost-efficient auction mechanism designed for aligning large language models (LLMs) with human preferences, a process integral to reinforcement learning from human feedback (RLHF). Traditional approaches rely heavily on procuring high-quality annotated preference datasets, neglecting the construction cost and economic utility of datasets. Considering the monetized cost of preference annotation and the complexities of intransitive or cyclic preference relations, which may fail to properly express preferences or degrade the marginal economic value during fine-tuning. This is particularly challenging when datasets need to be constructred over time in production environments.\nTo address these issues, we introduce an auction-based protocol utilizing a type of second-price auctions, i.e., Vickrey-Clarke-Groves (VCG) auction, to enhance cost efficiency in dataset construction. By treating the data collection process for fine-tuning LLMs as a monetized economy, the proposed mechanism incentivizes the data providers to provide high-quality responses. This helps the model owners who have a limited budget, in measuring the cost-efficiency and budgeting for data collection. Experimental results demonstrate that our proposed auction-based approach is not only cost-efficient for fine-tuning LLMs but also practical for online construction of datasets, while maintaining satisfactory model performance."}]}