{"title": "Hierarchical Split Federated Learning: Convergence Analysis and System Optimization", "authors": ["Zheng Lin", "Wei Wei", "Zhe Chen", "Chan-Tong Lam", "Xianhao Chen", "Yue Gao", "Jun Luo"], "abstract": "As AI models expand in size, it has become increasingly challenging to deploy federated learning (FL) on resource-constrained edge devices. To tackle this issue, split federated learning (SFL) has emerged as an FL framework with reduced workload on edge devices via model splitting; it has received extensive attention from the research community in recent years. Nevertheless, most prior works on SFL focus only on a two-tier architecture without harnessing multi-tier cloud-edge computing resources. In this paper, we intend to analyze and optimize the learning performance of SFL under multi-tier systems. Specifically, we propose the hierarchical SFL (HSFL) framework and derive its convergence bound. Based on the theoretical results, we formulate a joint optimization problem for model splitting (MS) and model aggregation (MA). To solve this rather hard problem, we then decompose it into MS and MA sub-problems that can be solved via an iterative descending algorithm. Simulation results demonstrate that the tailored algorithm can effectively optimize MS and MA for SFL within virtually any multi-tier system.", "sections": [{"title": "I. INTRODUCTION", "content": "The growing prevalence of mobile smart devices and the rapid advancement in information and communications technology (ICT) result in the phenomenal growth of the data generated at the network edge. International Data Corporation (IDC) forecasts that 159.2 zettabytes (ZB) of data will be generated globally in 2024, and this figure is expected to double to 384.6 ZB by 2028 [1]. To unlock the potential of big data, on-device learning begins to play a crucial role. In this respect, federated learning (FL) [2] has emerged as a predominant privacy-enhancing on-device learning framework, where participating devices train their local models in parallel on their private datasets and then send the updated models to an FL server for model synchronization [3]\u2013[10].\nBy extracting intelligence from previously inaccessible private data while remaining the data locally, FL has succeeded in numerous commercial products and applications influential on our daily lives [11], [12]. However, while FL offers advantages such as enhanced privacy, its deployment becomes increasingly challenging as machine learning (ML) models continue to grow in size [13]\u2013[15]. For instance, the Gemini Nano-2 model [16], a recently popular on-device large language model, consists of 3.25 billion parameters (equivalent to 3GB in 32-bit floats). The intensive on-device compute costs urgently calls for an alternative/complementary approach to FL. Inherited from split learning (SL) [17], split federated learning (SFL) [18] mitigates the aforementioned issue via split training between a server and edge devices in parallel. Specifically, SFL reduces the client-side computing burden by offloading the primary computing workload to an edge server via model partitioning while periodically aggregating client-side and server-side sub-models following the principle of FL [19]\u2013[22]. By leveraging device-server collaboration, SFL has garnered significant attention from academia and industry. For instance, Huawei has advocated NET4AI [23], a 6G intelligent system architecture built on SFL, to support future edge ML tasks.\nDespite the surging research interest in SFL, prior works mostly focus on two-tier SFL systems, consisting of only one split training server. As the number of served edge devices increases, two-tier SFL, however, can hardly support model training for users at a large scale [25]. To address this scalability issue, a natural idea is to extend SFL to its multi-"}, {"title": "A. System Model", "content": "As illustrated in Fig. 1, we consider a typical scenario of HSFL in multi-tier computing systems. Without loss of generality, we consider a hierarchical computing system with M tiers, where different numbers of computing entities are located in each tier. The fed server [18] is responsible for sub-model synchronization, periodically aggregating sub-models from the same tier. The set of participating edge devices is denoted by $\\mathcal{N} = \\{1, 2, ..., N\\}$, where N is the number of edge devices. Each edge device n has its own local dataset $D_n = \\{(\\mathbf{x}_{n,k}, y_{n,k})\\}_k$, where $\\mathbf{x}_{n,k}$ and $y_{n,k}$ are the k-th input data and its corresponding label. The set of computing entities in the m-th tier is denoted by $\\mathcal{I}_m = \\{1, 2, ..., J_m\\}$, where $J_m$ represent the number of computing entities. In most cases, the top tier M only has one cloud server, i.e., $J_M = 1$. The set of tiers is denoted by $\\mathcal{M} = \\{1, 2, ..., M\\}$. The sub-model of client n at the m-th tier is denoted by $w_{m,n}$. Therefore, the global model is represented as $\\mathbf{w} = [w_{1,n}; w_{2,n}; ...; w_{M,n}]$. The total number of layers for the global model is denoted by $L = \\sum_{m=1}^M L_m$, where $L_m$ is the number of layers for m-th tier sub-models. The goal of SL is to find the optimal model parameters $\\mathbf{w}^*$ by minimizing the following finite-sum non-convex global loss function across all participating devices:\n$\\min_{\\mathbf{w}} f(\\mathbf{w}) \\triangleq \\min_{\\mathbf{w}} \\frac{1}{N} \\sum_{n=1}^N f_n(\\mathbf{w}),$ (1)\nwhere $f_n(\\mathbf{w})$ is the local loss function of edge device n, i.e., $f_n(\\mathbf{w}) \\triangleq \\mathbb{E}_{\\xi_n \\sim D_n} [F_n(\\mathbf{w}; \\xi_n)]$, $F_n(\\mathbf{w}; \\xi_n)$ denotes the stochastic loss function of edge device n for a mini-batch data samples, and $\\xi_n$ is the training randomness of local dataset $D_n$, stemming from stochasticity of the data ordering caused by mini-batch data sampling from $D_n$. Following the standard stochastic optimization setting [35]\u2013[37], we assume the existence of unbiased gradient estimators, i.e., $\\mathbb{E}_{\\xi[t-1]} [\\nabla F_n(\\mathbf{w}^{t-1}; \\xi_n) | \\xi^{t-1}] = \\nabla f_n(\\mathbf{w}^{t-1})$ for any fixed $\\mathbf{w}^{t-1}$, where $\\xi^{[t-1]} \\triangleq \\{\\xi_n\\}_{n\\in\\{1,2,...,N\\},\\xi \\in\\{1,..,t-1\\}}$ represents all randomness up to training round t - 1.\nTo solve problem (1), the two-tier conventional SFL framework adopts the co-training paradigm of edge devices and an edge server. However, as ML models and the number of clients scale up, the edge server struggles to handle the heavy computing workload from multiple edge devices, leading to intolerable training latency and costs. Motivated by this, we propose the HSFL framework and design the MS and MA scheme for accelerating the training process."}, {"title": "B. The HSFL Framework", "content": "This section elaborates on the workflow of the HSFL framework. The heart of HSFL lies in the optimal control of MS and MA, which enables efficient SFL co-training of diverse computing entities across different tiers.\nBefore model training begins, the fed server initializes the ML model, partitions it into M sub-models via MS\n\u00b9although the \"cloud\" possesses a massive number of servers/GPUs, the inter-cloud communication latency can be ignored compared with wireless/backbone networks, which hence can be regarded as one logical server."}, {"title": "A. The split training stage:", "content": "The split training stage involves the sub-model update for each tier. During this stage, each edge device/server conducts forward propagation (FP) on its sub-models and sends the generated activations to a higher-tier device/server for further sub-model FP. After sub-model FP reaches the top tier (i.e., the M-th tier), each computing entity in the M-th tier executes sub-model backward pass (BP) and then transmits the activations' gradients to the lower-tier device/server for performing sub-model BP. This process continues tier by tier until edge devices, at the lowest tier, complete their sub-model BP. To better understand this stage, we separate it into five steps and discuss each in detail below."}, {"title": "a) Sub-model forward propagation:", "content": "This step involves FP of all participating computing entities across diverse tiers. For the lowest tier, each edge device n randomly draws a mini-batch $B_n \\subset D_n$ containing b data samples from its local dataset $D_n$ to execute FP. For higher tiers, the m-th tier computing entities utilize activations from connected entities in (m-1)-th tier, instead of the raw data, for FP. After the sub-model FP is completed, activations are generated at the cut layer. The activations generated by sub-models of client n at the m-th tier are represented as\n$\\mathbf{a}_{m,n}^{t} = \\begin{cases} \\varphi_1 (\\mathbf{x}_{n}^{t}; w_{1,n}^{t-1}), & m = 1 \\\\ \\varphi_m (\\mathbf{a}_{m-1,n}^{t}; w_{m,n}^{t-1}), & 2 \\leq m \\leq M \\end{cases}, \\forall n \\in \\mathcal{N},$ (2)\nwhere $\\mathbf{x}$ denotes a mini-batch input data of client n in t-th training round, and $\\varphi(\\mathbf{x}; w)$ maps the relationship between input x and its predicted value given model parameter w."}, {"title": "b) Activation uploading:", "content": "After completion of the sub-model FP, each computing entity sends its activations and corresponding labels to the connected entities in the tier above (typically over a wireless channel). These collected activations are then utilized to empower the sub-model FP of higher-tier computing entities."}, {"title": "c) Sub-model backward pass:", "content": "After sub-model FP reaches the top tier (i.e., the M-th tier), each computing entity in the M-th tier executes sub-model BP based on the loss function value. For lower tiers, the m-th tier computing entities leverage activations' gradients from connected entities in (m+1)-th tier for sub-model BP. Since a higher-tier computing entity may simultaneously handle sub-models of multiple clients, these sub-models can always be synchronized every training round without incurring any communication overhead. Consequently, the model parameters of the j-th computing entity in the m-th tier can be updated through\u00b2\n$w_{m,j}^{t} = \\frac{1}{N_m} \\sum_{n \\in \\mathcal{N}_m} w_{m,n}^{t-1} - \\gamma \\nabla F_n(w_{m,n}^{t-1}; \\xi_n^{t}), j \\in \\mathcal{I}_m, \\forall m \\in \\mathcal{M}\\{M\\},$ (3)\nwhere $\\mathcal{N}_m$ is the set of clients corresponding to the sub-models in the j-th computing entity of the m-th layer, $N_j$ denotes the number of sub-models, $w_{m,n}^{t} \\leftarrow w_{m,n}^{t} - \\gamma \\nabla F(w_{m,n}^{t}; \\xi_{n})$ is the m-th tier sub-model of client n, $\\nabla F(\\mathbf{w}; \\xi)$ represents the gradient of loss function $F(\\mathbf{w}; \\xi)$ with respect to the model parameter w, and $\\gamma$ is the learning rate. After sub-model BP is completed, activations' gradients are generated at the cut layer."}, {"title": "d) Downloading of activations' gradients:", "content": "After completion of the sub-model BP, each computing entity sends its activations' gradients and corresponding labels to the connected entities in the tier below. The lower-tier entities then use the received activations' gradients to conduct the sub-model BP."}, {"title": "B. The sub-model aggregation stage:", "content": "The sub-model aggregation stage focuses on aggregating sub-models from each tier on the fed server. The sub-model MA intervals vary across diverse tiers (i.e., sub-models from m-th tier are aggregated every $I_m$ training rounds). For every $I_m$ training round, computing entities in the m-th tier send their sub-models to the fed server. In practice, the fed server for multi-tier HSFL systems is an application server responsible for model aggregation for a relatively large geographic region, which, hence, is often a cloud server over the Internet. Note that $I_m$ is not fixed throughout the training process and can be adjusted after each sub-model MA based on current wireless/wired network conditions and training states (shown in Sec. VI). The sub-model aggregation stage consists of the following three steps."}, {"title": "e) Sub-model uploading:", "content": "In this step, computing entities in the same tier simultaneously send their respective sub-models to the fed server over the wireless/wired links."}, {"title": "f) Sub-model aggregation:", "content": "The fed server aggregates the sub-models of different computing entities from the same layer. For the m-th tier, the aggregated sub-model is denoted by\n$\\mathbf{w}_m^t = \\frac{1}{N} \\sum_{j=1}^{I_m} \\sum_{n \\in \\mathcal{N}_j} w_{m,n}^{t}, m \\in \\mathcal{M}\\{M\\}.$ (4)"}, {"title": "g) Sub-model downloading:", "content": "After completing the sub-model aggregation, the fed server sends the updated sub-model to the corresponding edge devices/server.\nThe HSFL training procedure is outlined in Algorithm 1."}, {"title": "IV. CONVERGENCE ANALYSIS OF THE HSFL FRAMEWORK", "content": "In this section, we conduct the convergence analysis of HSFL to quantify the impact of MS and MA on training convergence, which serves as the theoretical foundation for developing an efficient iterative optimization method in Sec. VI.\n\u00b2The computing entities can execute sub-model updates of multiple clients in either a serial or parallel fashion, which does not affect training performance. Here, we consider the parallel fashion."}, {"title": "V. PROBLEM FORMULATION", "content": "In this section, we formulate a joint MA and MS optimization problem based on the derived convergence bound in Sec. IV. The objective is to minimize the training latency of HSFL to achieve target learning performance in a resource-constrained multi-tier computing system. Subsequently, we devise an efficient MS and MA strategy in Sec. VI. For clarity, the decision variables and definitions are listed below.\n*   $I: I_m \\in \\mathbb{N}^+$ is the sub-model MA decision variable, indicating that sub-models at m-th tier are aggregated on the fed server every $I_m$ training rounds. $\\mathbf{I} = [I_1, I_2, ..., I_{M-1}]$ represents the collection of sub-model MA decisions.\n*   $\\mu: \\mu_{m,l} \\in \\{0, 1\\}$ denotes the MS decision variable, where $\\mu_{m,l} = 1$ indicates that the l-th neural network layer is selected as the cut layer between m-th and (m+1)-th tier, and 0 otherwise. $\\boldsymbol{\\mu} = [\\mu_{1,1}, \\mu_{1,2},......, \\mu_{M-1,L}]$ represents the collection of MS decisions."}, {"title": "A. Training Latency Analysis", "content": "In this section, we analyze the training latency of HSFL. Without loss of generality, we focus on one training round for analysis. In each training round, edge device n randomly draws a mini-batch $B_n \\subset D_n$ with b data samples from its local dataset for model training. To begin, we provide a detailed latency analysis of the split training stage."}, {"title": "a) Sub-model forward propagation latency:", "content": "The computing entities in each tier utilize the local dataset/received activations from the lower tier to conduct their sub-model FP. A computing entity can execute FP and BP for multiple edge devices in a serial or parallel fashion. Here, we consider the parallel fashion. The computing workload (in FLOPs) of sub-model FP in the m-th tier per mini-batch b is denoted by\n$\\Phi_m^F (b, \\boldsymbol{\\mu}) = \\sum_{l=1}^L (\\mu_{m,l} - \\mu_{m-1,l}) \\rho_l (b)$, where $\\rho_l (b)$ is the FP computing workload of propagating the first l layer of neural network for one mini-batch b. Thus, the FP latency for client n's sub-model located in the m-th tier is given by\n$T_{m,n}^F = \\frac{\\Phi_m^F (b, \\boldsymbol{\\mu})}{f_{m,n}}, \\forall n \\in \\mathcal{N}, \\forall m \\in \\mathcal{M},$ (11)\nwhere $f_{m,n}$ denotes the computing capability allocated for processing the sub-model of client n located in the m-th tier\u00b3 (namely, the number of float-point operations per second (FLOPS))."}, {"title": "b) Activation uploading latency:", "content": "After the completion of sub-model FP, computing entities send the activations generated at the cut layer to those at the higher tier to continue model training. The data size (in bits) of activations of m-th tier is represented as $\\Gamma_m^A (\\boldsymbol{\\mu}) = \\sum_{l=1}^L \\mu_{m,l} \\psi_l$, where $\\psi_l$ denotes the data size of activations at the cut layer l. The activation transmission latency for client n's sub-model located in the m-th tier can be calculated as\n$T_{m,n}^A = \\frac{\\Gamma_m^A (\\boldsymbol{\\mu})}{r_{m,n}^A}, \\forall n \\in \\mathcal{N}, \\forall m \\in \\mathcal{M} \\backslash \\{M\\},$ (12)\nwhere $r_{m,n}^A$ is the uplink transmission rate for transmitting the activations corresponding to client n's sub-model in the m-th tier to the corresponding upstream computing entity at (m + 1)-th tier."}, {"title": "c) Sub-model backward pass latency:", "content": "After sub-model FP reaches the top tier (i.e., the M-th tier), computing entities at diverse tiers execute sub-model BP based on loss function value/received activations' gradients from the higher tier. Let\n$\\Phi_m^{\\Omega} (b, \\boldsymbol{\\mu}) = \\sum_{l=1}^L (\\mu_{m,l} - \\mu_{m-1,l}) \\pi_l (b)$ denote the computing workload of sub-model BP of m-th tier per mini-batch b, where $\\omega_l (b)$ is the BP computing workload of propagating the first l layer of neural network for one mini-batch b. Thus, the BP latency for client n's sub-model located in the m-th tier can be obtained from\n$T_{m,n}^B = \\frac{\\Phi_m^{\\Omega} (b, \\boldsymbol{\\mu})}{f_{m,n}}, \\forall n \\in \\mathcal{N}, \\forall m \\in \\mathcal{M}.$ (13)"}, {"title": "d) Downloading latency of activations' gradients:", "content": "After the sub-model BP is completed, computing entities transmit the activations' gradients to those at a lower tier for further model training. Let $\\Gamma_m^G (\\boldsymbol{\\mu}) = \\sum_{l=1}^L \\mu_{m,l} \\chi_l$ represent the data size of activations' gradients of the m-th tier, where $\\chi_l$ denotes the data size of activations' gradients at cut layer l. Therefore, the transmission latency of activations' gradients for client n's sub-model at the (m+1) tier is expressed as\n$T_{m,n}^G = \\frac{\\Gamma_m^G (\\boldsymbol{\\mu})}{r_{m,n}^G}, \\forall n \\in \\mathcal{N}, \\forall m \\in \\mathcal{M}\\{M\\},$ (14)\nwhere $r_{m,n}^G$ is the downlink transmission rate for transmitting the activations' gradients corresponding to client n's sub-model at the (m+1) tier to the corresponding downstream computing entity in the m-th tier.\n\u00b3 We assume the association of clients and computing entities as well as resource allocation in each tier are predetermined. Thus, we can obtain the computing capabilities and data rate allocated for a client's sub-model in each tier and omit the index of the specific computing entity hosting the sub-model."}, {"title": "Next, we analyze the latency of the sub-model aggregation.", "content": "e) Sub-model uploading latency: The computing entities at the same tier send their aggregated sub-models to the fed server for MA. Let $\\Lambda_m (\\boldsymbol{\\mu}) = \\sum_{l=1}^L (\\mu_{m, l} - \\mu_{m-1,l}) \\delta_l$ denote the data size of sub-model of the m-thtier, where $\\delta_l$ is the data size of the sub-model with the first l layers. Therefore, the sub-model uploading latency of the j-th computing entity located in the m-th tier is expressed as\n$T_m^U = 1\\{\\mathcal{J}_m > 1\\} \\frac{\\Lambda_m (\\boldsymbol{\\mu})}{r_m^U}, \\forall j \\in \\mathcal{I}_m, \\forall m \\in \\mathcal{M} \\backslash \\{M\\},$ (15)\nwhere $r_{m,j}^U$ is the uplink data rate for transferring sub-model from the j-th computing entity located in the m-th tier to the fed server, and $1\\{\\mathcal{J}_m > 1\\}$ implies sub-model uploading for aggregation is needed only when there is more than one server in the m-th tier.\nf) Sub-model model aggregation: The fed server aggregates the received sub-models from the same tier. For simplicity, the sub-model aggregation latency for this part is ignored, as it is negligible compared to other steps [42], [43].\ng) Sub-model downloading latency: After completing sub-model MA, the fed server sends the updated sub-model to the corresponding computing entities. Similarly, the sub-model downlink transmission latency of the j-th computing entity located in the m-th tier is calculated by\n$T_m^D = 1\\{\\mathcal{J}_m > 1\\} \\frac{\\Lambda_m (\\boldsymbol{\\mu})}{r_{m,j}^D}, \\forall j \\in \\mathcal{I}_m, \\forall m \\in \\mathcal{M} \\backslash \\{M\\},$ (16)\nwhere $r_{m,j}^D$ is the downlink rate for transmitting sub-model from the fed server to j-th computing entity located in the m-th tier."}, {"title": "B. Joint Model Aggregation and Model Splitting Problem Formulation", "content": "In this section, we formulate the joint MA and MS optimization problem to minimize the training latency of HSFL for model convergence in a resource-constrained multi-tier system. As illustrated in Fig. 3, the per-round latency of split training is calculated as\n$T_s (\\boldsymbol{\\mu}) = \\max_{n} \\{ \\sum_{m=1}^M T_{m,n}^F + \\sum_{m=1}^{M-1} T_{m,n}^A + \\sum_{m=1}^M T_{m,n}^B + \\sum_{m=1}^{M-1} T_{m,n}^G \\}.$ (17)\n\u2074For each computing entity, it aggregates the sub-models of its hosted clients before uploading to the fed server. The aggregation latency is ignored.\nand MA latency of the m-th tier sub-models is expressed as\n$T_{m,A} (\\boldsymbol{\\mu}) = \\max_j \\{ T_m^U \\} + \\max_j \\{ T_m^D \\}.$ (18)\nConsidering the split training is executed per training round and the sub-model aggregation for m-tier occurs every $I_m$ training round, the total training latency for R training rounds is given by\n$\\mathcal{T}(\\mathbf{I}, \\boldsymbol{\\mu}) = R T_s (\\boldsymbol{\\mu}) + \\sum_{m=1}^{M-1} \\frac{R}{I_m} T_{m,A} (\\boldsymbol{\\mu}).$ (19)\nAs alluded in Sec. I, MA balances the trade-off between communication overhead and training convergence, while MS significantly impacts communication-computing overhead and model convergence. Therefore, jointly optimizing MA and MS is critical for the performance of HSFL. To this end, we formulate the following optimization problem to minimize the training latency for model convergence:\n$\\mathcal{P}: \\min_{\\mathbf{I},\\boldsymbol{\\mu}} \\mathcal{T}(\\mathbf{I}, \\boldsymbol{\\mu})$ (20)\ns.t. C1: $\\frac{1}{R} \\sum_{t=1}^{R} \\mathbb{E}[\\Vert \\nabla f(\\mathbf{w}^{t-1}) \\Vert^2] \\leq \\epsilon,$\nC2: $\\mu_{m,l} \\in \\{0,1\\}, \\forall m \\in \\mathcal{M} \\backslash \\{M\\}, l = 1, 2, ..., L,$\nC3 : $\\sum_{l=1}^L \\mu_{m,l} = 1, \\forall m \\in \\mathcal{M} \\backslash \\{M\\},$\nC4: $\\sum_{l=1}^l \\mu_{m, l} \\leq \\sum_{l=1}^l \\mu_{m-1,l}, l = 1, 2, ..., L,$\nC5 : $\\mathbb{N} \\sum_{j=1}^{J_m} \\sum_{n \\in \\mathcal{N}_m} \\sum_{l=1}^L (\\mu_{m,l} - \\mu_{m-1,l}) (\\psi_l + \\chi_l + \\delta_l + \\delta_l) < \\rho_m, \\forall j \\in \\mathcal{I}_m, \\forall m \\in \\mathcal{M} \\backslash \\{M\\},$\nC6: $I_m \\in \\mathbb{N}^+, \\forall m \\in \\mathcal{M} \\backslash \\{M\\},$\nwhere $\\Psi_l = \\sum_{k=1}^l \\psi_k$ and $X_l = \\sum_{k=1}^l \\chi_k$ represent the cumulative sum of data size (in bits) of activations and activations' gradients for first l layers of the neural network, $\\theta_l$ is the data size of the optimizer state for the first l layers of the neural network, depending on the choice of the optimizer (e.g. SGD, Momentum, and Adam), $\\rho_m$ denotes memory limitation of the j-th computing entity in the m-th tier. Constrain C1 guarantees model convergence accuracy; C2 and C3 ensure the uniqueness of the cut layer between sub-models of m-th and (m+1)-th tier; C4 guarantees that the cut layer of the lower tier is shallower than that of the higher tier; C5 represents the memory limitation of computing entities [44]; C6 denotes that the sub-model MA decision variable is a positive integer. Problem (20) is a combinatorial optimization problem with a non-convex mixed-integer non-linear objective function. In general, this problem is NP-hard, rendering it infeasible to obtain the optimal solution using polynomial-time algorithms."}, {"title": "VI. SOLUTION APPROACH", "content": "In this section, we develop an efficient iterative algorithm by decoupling the problem (20) into MA and MS sub-problems and then find the optimal solution for each."}, {"title": "We first reveal the explicit expression R with Corollary 1.", "content": "Since R is proportional to the objective function, the objective function is minimized if and only if inequality (10) holds as equality. In general, R can be significantly larger than $I_m$, and therefore we can utilize $\\frac{R}{I_m} = \\frac{2\\nu}{\\gamma \\epsilon I_m^2}$ to approximate (19). By substituting (10) into (19), problem (20) can be converted into\n$\\mathcal{P}': \\min_{\\mathbf{I},\\boldsymbol{\\mu}} \\Theta(\\mathbf{I}, \\boldsymbol{\\mu})$ (21)\ns.t. C2 - C6,\nwhere\n$\\Theta(\\mathbf{I}, \\boldsymbol{\\mu}) = \\frac{2\\nu}{\\gamma \\epsilon} (RT_s (\\boldsymbol{\\mu}) + \\sum_{m=1}^{M-1} \\frac{T_{m,A} (\\boldsymbol{\\mu})}{I_m})$ (22)\nLM-1+LM\nThe term $\\sum_{l=LM-1+1} \\sigma_l^2$ is intertwined with MS, but it does not disclose the relationship with MS decision variables. To address this issue, we introduce a set of constants $\\mathbb{G} = [\\tilde{G}_1, \\tilde{G}_2, ..., \\tilde{G}_L]$, where $\\tilde{G}_l$ represents the cumulative sum of the bounded second order moments for the first l layers of neural network, defined as $\\tilde{G}_l = \\sum_{k=1}^L \\tilde{G}_k$. Hence, $\\sum_{l=LM-1+1}^{LM-1+LM} \\sigma_l^2$ can be reformulated as $\\sum_{l=LM-1+1}^{LM-1+LM} (\\mu_{m,l} - \\mu_{m-1,l}) \\tilde{G}_l$. Moreover, the non-convexity and non-smoothness of the objective function of problem (21) make it extremely intractable. To linearize the objective function, we introduce a set of auxiliary variables $\\mathbf{T} = [T_{1,1}, T_{1,2}, T_{2,2}, ..., T_{M-1,2}, T_{1,3}, T_{2,3}, ..., T_{M-1,3}]$, i.e., $\\max_{n} \\{ \\sum_{m=1}^{M} T_{m,n}^F + \\sum_{m=1}^{M-1} T_{m,n}^A + \\sum_{m=1}^{M} T_{m,n}^B + \\sum_{m=1}^{M-1} T_{m,n}^G \\} < T_1$, $\\max_{j} \\{ T_m^U \\} < T_{m,2}$, and $\\max_{j} \\{ T_m^D \\} < T_{m,3}$. Therefore, problem (21) can be transformed into\n$\\mathcal{P}'': \\min_{\\mathbf{I},\\boldsymbol{\\mu}, \\mathbf{T}} \\Theta'(\\mathbf{I}, \\boldsymbol{\\mu}, \\mathbf{T})$ (23)\ns.t. C2 - C6,\nR1: $\\sum_{m=1}^{M} \\sum_{l=1}^{L} (\\mu_{m,l} - \\mu_{m-1,l}) (\\rho_l(b) + \\pi_l(b)) + \\sum_{m=1}^{M-1} \\sum_{l=1}^{L} \\mu_{m,l} \\psi_l \\frac{1}{r_{m,n}^A} + \\sum_{m=1}^{M-1} \\sum_{l=1}^{L} \\mu_{m,l} \\chi_l \\frac{1}{r_{m,n}^G} < T_1, \\forall n \\in \\mathcal{N},$\nR2:$1\\{\\mathcal{J}_m > 1\\} \\frac{\\sum_{l=1}^{L} (\\mu_{m,l} - \\mu_{m-1,l}) \\delta_l}{r_m^U} < T_{m,2}, \\forall j \\in \\mathcal{I}_m, \\forall m \\in \\mathcal{M}\\{M\\},$\nR3:$1\\{\\mathcal{J}_m > 1\\} \\frac{\\sum_{l=1}^{L} (\\mu_{m,l} - \\mu_{m-1,l}) \\delta_l}{r_{m,j}^D} < T_{m,3}, \\forall j \\in \\mathcal{I}_m, \\forall m \\in \\mathcal{M}\\{M\\},$\n$\\Theta' (\\mathbf{I}, \\boldsymbol{\\mu}, \\mathbf{T}) = \\frac{2\\nu}{\\gamma \\epsilon} (RT_1 + \\sum_{m=1}^{M-1} \\frac{T_{m,2} + T_{m,3}}{I_m}) - 4 \\beta^2 \\sum_{m=1}^{M-1} \\sum_{l=L_{m-1}+1}^{L_m-1+L_m} 1{\\mathcal{I}_m > 1\\} \\frac{1}{I_m} (\\mu_{m,l} - \\mu_{m-1,l}) \\tilde{G}_l $ (24)\nThe difficulty in solving problem (23) stems from the tight coupling between auxiliary variables and the original decision variables. Therefore, we decompose the problem (23) into two tractable sub-problems based on decision variables and develop efficient algorithms to obtain optimal solutions for each."}, {"title": "We fix the variables", "content": "$\\boldsymbol{\\mu"}, "and $\\mathbf{T}$ to investigate the sub-problem involving sub-model MA, which is expressed as\n$\\mathcal{P}1: \\min_{\\mathbf{I}} \\Theta' (\\mathbf{I})$ (25)\ns.t. C6.\nThen,"]}