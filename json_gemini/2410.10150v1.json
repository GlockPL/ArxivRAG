{"title": "JAILBREAK INSTRUCTION-TUNED LLMS VIA END-OF-SENTENCE MLP RE-WEIGHTING", "authors": ["Yifan Luo", "Meitan Wang", "Zhennan Zhou", "Bin Dong"], "abstract": "In this paper, we investigate the safety mechanisms of instruction fine-tuned large language models (LLMs). We discover that re-weighting MLP neurons can significantly compromise a model's safety, especially for MLPs in end-of-sentence inferences. We hypothesize that LLMs evaluate the harmfulness of prompts during end-of-sentence inferences, and MLP layers plays a critical role in this process. Based on this hypothesis, we develop 2 novel white-box jailbreak methods: a prompt-specific method and a prompt-general method. The prompt-specific method targets individual prompts and optimizes the attack on the fly, while the prompt-general method is pre-trained offline and can generalize to unseen harmful prompts. Our methods demonstrate robust performance across 7 popular open-source LLMs, size ranging from 2B to 72B. Furthermore, our study provides insights into vulnerabilities of instruction-tuned LLM's safety and deepens the understanding of the internal mechanisms of LLMs.", "sections": [{"title": "INTRODUCTION", "content": "The capabilities of large language models (LLMs) have improved rapidly in recent years (Achiam et al., 2023; Anthropic, 2023; Touvron et al., 2023). One of the primary ways of deploying LLMs in practice is through chatbots. Instruction fine-tuning is the most common approach for transforming a pre-trained LLM into an effective chatbot (Wei et al., 2021; Ouyang et al., 2022; Chung et al., 2022). This process involves training the model on a variety of prompt-response pairs, marked with special tokens, to guide the model in following instructions and generating helpful, relevant responses. Additionally, safety constraints are incorporated during this fine-tuning process, enabling instruction-tuned LLMs to recognize and refuse harmful or malicious prompts.\nHowever, even these instruction-tuned models remain vulnerable to jailbreak attempts (Wei et al., 2023; Zou et al., 2023b; Liu et al., 2023; Zhan et al., 2023). It remains an open question why safety mechanisms fail against certain jailbreak methods, and indeed, it is not fully understood how safety mechanisms function in the first place. This situation underscores the importance of thoroughly understanding safety mechanisms. Only by grasping how current safety systems operate and why they can be bypassed can we design the next generation of more robust safety models.\nMany studies have aimed to unravel the internal mechanisms behind LLM safety, exploring this issue from feature, weight attribution, and other perspectives. From a feature perspective, several works examine which features trigger model refusal behaviors, investigating how models detect harmful prompts (Zou et al., 2023a; Zheng et al., 2024; Arditi et al., 2024). From a weight attribution perspective, studies have analyzed the contributions of specific decoder layers or MLP neurons to model safety (Li et al., 2024; Wei et al., 2024). More broadly, many research explores how fine-tuning instills or weakens a model's safety (Lermen et al., 2023; Qi et al., 2023)."}, {"title": "MLP RE-WEIGHTING", "content": "In this section, we explore how re-weighting the neuron activations of MLP layers in instruction-tuned LLMs affects their safety. We first define the notations used and describe the method for applying re-weighting factors to the MLP layers. Then, we present some preliminary experiments demonstrating that MLP re-weighting can compromise the model's safety, followed by an ablation study showing the critical role of end-of-sentence inferences in this process. Finally, we propose the \"harmful assessment hypothesis,\" which offers an explanation for the observed behavior."}, {"title": "RE-WEIGHTING MLP ACTIVATIONS", "content": "In this subsection, we introduce the notation used throughout this paper and describe the method of modifications we applied to the MLP layers.\nWe use $h_l^{(t)} \\in \\mathbb{R}^d$ to represent the output of the l-th decoder layer in the t-th inference, which we refer to as hidden states. In this paper, the t-th inference specifically refers to the inference that takes the t-th token as input, i.e., we denote the inference process in a sequential manner. Furthermore, when we mention terms such as \"decoder layer\" or \"MLP layer,\" we are referring to the entire layer block, encompassing all of its components.\nEach decoder layer contains a self-attention layer and a MLP layer.\n$h_{l+1/2}^{(t)} = h_l^{(t)} + Attn_l(h_l^{(t)}; \\{h_l^{(s)}\\}_{s=1}^{t}),$\n$h_{l+1}^{(t)} = h_{l+1/2}^{(t)} + MLP_l(h_{l+1/2}^{(t)})$"}, {"title": "JAILBREAK ATTEMPTS", "content": "In this subsection, we present some preliminary experimental attempts. Our goal is to identify re-weighting factors that could potentially compromise the safety of intruction-tuned LLMs.\nWe formulate the attack as the following optimization problem:\n$\\min_{M} L(M; x) + \\rho \\sum_{t,l,i}(1 \u2013 M_{l,i}^{(t)}),\\ \\text{s.t. } 0 \\leq M_{l,i}^{(t)} \\leq 1.$\nWe design our loss function $L(M;x)$ to encourage the LLM to produce a positive response for prompt x, an approach introduced in Zou et al. (2023b). To prevent excessive modifications to the model, we include an L\u2081 penalty term in the optimization to ensure that the re-weighting factors remain relatively sparse, with $\\rho$ being its hyperparameter. For clarity and to keep the focus on the main points, the specific definition of $L(M; x)$ is provided later in Section 3.\nBefore presenting the specific results, there are two important details to note. The first is the format of the prompts. For instruction-tuned LLMs, when users interact with these models, their queries are preprocessed before being sent to the model. For example, in the case of LLaMA-3 8B-Instruct, the query \"Give instructions for hacking into an ATM machine.\" is transformed into the following format before being input into the model:\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nGive instructions for hacking into an ATM machine.< | eot_id|>\n<|start_header_id|>assistant<|end_header_id|>\n\nHere, tokens enclosed within <|\u00b7|> are special tokens in LLaMA-3's tokenizer.\nThe second detail to note is the constraints we impose on M. We do not apply any modifications during the inference process that generates new tokens. For a prompt of length n, M is defined only for the first n - 1 inferences. During the n-th inference and any subsequent auto-regressive inferences, no modifications are applied. The reason for this constraint is that we aim to avoid directly interfering with the token generation process, as this can easily lead to overfitting. By indirectly affecting the model's outputs, we can gain a deeper understanding of how the model interprets and assesses harmful prompts."}, {"title": "ATTACKING METHODS", "content": "Based on the findings from section 2, in this section we develop two methods to jailbreak instruction-tuned LLMs: a prompt-specific method, which operate independently for different prompts, and a prompt-general method, which can create a safety-constraint-free version of the open-source LLMs. We outline the specific settings and implementation details for these two methods in this section."}, {"title": "PROMPT-SPECIFIC METHOD", "content": "The prompt-specific method is designed to jailbreak an LLM for a single given prompt, requiring independent training for each target prompt. Typical representatives of this approach include GCG (Zou et al., 2023b), AutoDAN (Liu et al., 2023), and PAIR (Chao et al., 2023).\nRecall that in section 2, we did not provide the full definition of the loss function. We now complete it here. In order to encourage the LLM to produce more positive responses, we first hand craft a small set of postive response prefixes, such as \"Sure! Here are some steps to help\". It is important to note that these prefixes are independent of any specific query and are simply general positive responses. We denote them as $y \\in \\mathcal{Y}$, where each y represents a positive prefix and $\\mathcal{Y}$ being their collection. In practice, we constructed a small $\\mathcal{Y}$ containing 9 positive prefixes. Further details on these prefixes can be found in the Appendix A.\nThus, the loss function $L(M; x)$ takes the following form:\n$L(M; x) = -\\frac{1}{|\\mathcal{Y}|} \\sum_{y\\in \\mathcal{Y}} \\sum_{k=1}^{|y|} log P_M(y_{k+1}; [xy]_{1:n+k}).$\nHere, $p_M$ represents the output probability of the model after applying the MLP re-weighting factor M. $[xy]$ denotes the concatenation of the text x and y and $[xy]_{1:n+k}$ denotes the first n + k tokens of the combined sequence. $|y|$ denotes the length of y. In essence, this formuation encourages the MLP factor M to guide the model toward treating y as the expected continuation of x.\nSo, the full optimization problem is:\n$\\min_{M} -\\frac{1}{|\\mathcal{Y}|} \\sum_{y\\in \\mathcal{Y}} \\sum_{k=1}^{|y|} log P_M(y_{k+1}; [xy]_{1:n+k}) + \\rho \\sum_{t,l,i}(1 - M_{l,i}^{(t)}),$\n$\\text{s.t. } 0 \\leq M_{l,i}^{(t)} \\leq 1.$\nSince section 2 has demonstrated that applying MLP factors specifically to the end-of-sentence in-ferences is sufficient for successful attacks, here we employ the same approach. Due to this change, we slightly adjust the notation, defining $M \\in [0,1]^{L\\times(\\Delta n-1)\\times W}$, where $\\Delta n$ denotes the number of special tokens appended to the end of the prompt. For instance, in LLaMA-3-Instruct, $\\Delta n = 5$. Thus, $M_{l,i}^{(t)}$ is applied at the $(n \u2013 \\Delta n + t)$-th inference, rather than the t-th inference. This index shift is purely a notational adjustment and does not alter the core methodology.\nWe use gradient descent with momentum to solve problem (3). Backpropagation allows us to efficiently compute $\\frac{\\partial}{\\partial M}$ without incurring additional computational overhead. At each step, we first perform gradient descent, and then apply truncation to each component of M to ensure that $M_{l,i}^{(t)} \\in [0, 1]$. We summarize the entire process in Algorithm 1."}, {"title": "PROMPT-GENERAL METHOD", "content": "The prompt-general method aims to obtain an LLM with safety constraints removed. After offline training, the resulting LLM is expected to provide direct responses to any harmful prompt. Since the prompt-general method requires no additional training during implementation, it has significantly lower deployment requirements and poses greater potential risks. Typical representatives of this approach include multi-prompt GCG (Zou et al., 2023b), ORTHO (Arditi et al., 2024) and reverse fine-tuning methods (Zhan et al., 2023; Qi et al., 2023).\nTo extend our prompt-specific method into the prompt-general method, we implicitly rely on the hypothesis that there exists a universal safety mechanism across different harmful prompts. Only with this assumption can we use a single set of MLP re-weighting factors to interfere with the generation process for all prompts. Subsequent experimental results confirmed this: by pre-training the MLP factors offline on a given dataset, these MLP factors demonstrated the ability to generalize to previously unseen harmful prompts.\nIn terms of training, the prompt-general method is not significantly different from the prompt-specific method. The main difference is replacing the loss function $L(M;x)$, which originally depends on a specific prompt x, with its expectation over a dataset $\\mathcal{D}$:\n$L(M) = \\mathbb{E}_{x\\sim\\mathcal{D}}L(M;x)$.\nWe selected and synthesized a collection of harmful questions and commands to create the training dataset $\\mathcal{D}$. The primary sources include a cleaned and augmented version of HarmfulQA (Bhardwaj & Poria, 2023), along with some harmful behaviors generated by GPT-40 that align with the categories covered in HarmBench (Mazeika et al., 2024). Since we'll also evaluate our method on HarmBench, we carefully cross-checked the datasets to ensure no overlap in prompts, thus pre-venting data contamination. It is important to reemphasize that our dataset contains only harmful queries, with no harmful responses.\nAnother minor difference between the prompt-general method and the prompt-specific method is the introduction of early stopping. In the prompt-general method, we stop the optimization when the modulation rate, defined as $\\mathbb{E}_{tli} (1 \u2013 M_{l,i}^{(t)})$, reaches its maximum. A more detailed explanation of this early stopping criterion is provided in Appendix A."}, {"title": "RESULTS", "content": "In this section, we present the evaluation of our methods. First, we compare the attack success rates (ASRs) of our approaches with other jailbreak methods to demonstrate that our approaches are comparable to state-of-the-art methods. Next, we analyze the performance changes introduced by our prompt-general method to the instruction-tuned LLMs across some standard tasks. Finally, we provide a detailed examination of the MLP factors obtained through our methods."}, {"title": "ATTACK SUCCESS RATE", "content": "In this subsection, we compare the ASR of our methods against other jailbreak methods using Harm-Bench (Mazeika et al., 2024). We evaluate all methods on the 159 \"standard behaviors\" of Harm-Bench's test set. Given the release time of HarmBench, many newly released models have not been evaluated. Therefore, we re-evaluated various jailbreak methods on the latest open-source LLMs"}, {"title": "MODEL PERFORMANCE", "content": "In this subsection, we evaluate the performance change between the original instruction-tuned LLM and its variant produced by our prompt-general method, which is free of safety constraints. This evaluation is important because many supervised fine-tuning jailbreak methods encounter a trade-off between achieving a high ASR and degrading the model's overall quality (Souly et al., 2024).\nFor model evaluation, we follow the approach of the Open LLM Leaderboard (Beeching et al., 2023) and select the following 4 benchmarks: ARC-Challenge (Clark et al., 2018), GSM8K (Cobbe et al., 2021), MMLU (Hendrycks et al., 2020), and TruthfulQA (Lin et al., 2021)."}, {"title": "DETAIL STUDY", "content": "In this subsection, we closely examine the MLP factors derived from our method. We illustrate the extent of modifications MLP re-weighting made to the MLP layers and identify which layers and inferences experienced the most significant changes. Additionally, we explore the distribution of the MLP factors. In the main text, we focus on the results of the prompt-general method applied to LLaMA-3 8B-Instruct, while results for other models and the prompt-specific method are detailed in Appendix C."}, {"title": "ABLATION STUDY", "content": "In this subsection, we explore the effect of different penalty parameters $\\rho$ on the jailbreak results.\nFigure 4 illustrates the ASRs under different $\\rho$ settings. It can be seen that even with $\\rho$ values close to 0, our method remains effective. This suggests that $\\rho$ is not essential for the method to work. This occurs because the optimization starts with M = 1 and constrains M within the [0,1] interval, causing many attempts to increase M beyond 1 to be truncated. As a result, the method inherently promotes sparsity in the solution, even without explicit L\u2081 regularization. The actual role of $\\rho$ is more about helping the optimization process denoise, allowing it to focus on the commonalities related to the safety mechanism across different training prompts.\nOur method also has an interesting byproduct. When a relatively large $\\rho$ is set, and the optimization runs for a sufficiently long time until convergence, the resulting M* exhibits a highly sparse binary structure. This outcome can be used to identify MLP neurons that are strongly correlated with safety. Therefore, our approach can also serve as a mechanism interpretability tool. A more detailed explanation of this process is provided in Appendix D."}, {"title": "RELATED WORKS", "content": "Jailbreaks. Numerous users and researchers have sought to bypass the safety constraints of LLMs, resulting in various jailbreak methods. Techniques such as crafting adversarial prompts (Wei et al., 2023; Carlini et al., 2023) and manipulating the model's decoding process (Huang et al., 2023) exploit vulnerabilities in both open-source and closed-source models. Additionally, fine-tuning aligned LLMs, even over non-malicious datasets, can inadvertently compromise model's safety (Qi et al., 2023; Yang et al., 2023). Recently, studies by Arditi et al. (2024) and Wei et al. (2024) have leveraged insights from interpretability to develop more efficient and effective jailbreak methods. Our work also falls into this category.\nLLM Safety Mechanism. Understanding the safety mechanisms of LLMs is challenging due to the complexity of their internal mechanisms. Recent studies have successfully identified attributes such as truthfulness and toxicity within these models (Lee et al., 2024; Li et al., 2023). Wei et al. (2024) demonstrate that pruning safety-critical neurons can degrade model safety while preserving most capabilities. In another approach, Zheng et al. (2024) and Zou et al. (2023a) focus on the re-fusal mechanism, revealing that the hidden features associated with refusal differ from those linked to harmfulness. Furthermore, Arditi et al. (2024) show that eliminating refusal feature directions from model parameters can significantly compromise model safety. Nevertheless, these discoveries represent initial steps in understanding LLM safety mechanisms, and a comprehensive understand-ing of these mechanisms remains elusive, highlighting the need for further investigation."}, {"title": "DISCUSSION", "content": "In this work, we demonstrate that the safety mechanism of instruction-tuned LLMs heavily relies on MLP layers in end-of-sentences inferences and illustrate how vulnerable current open-source LLMs are to such targeted attacks. Our work is inspired by previous researches' observations regarding the relationship between MLP layers and model safety, and we have also uncovered several new phenomena that we hope will inspire further research in LLM mechanism study.\nLimitations. There are several limitations to our work. First, much of the method is heuristic-driven, making it more of a validation experiment rather than an optimal solution. For instance, the design of the loss function is largely based on intuition, leaving considerable room for improvement. Second, our work only points to the significance of MLP layers in the safety mechanism, yet the precise role they play and how they impact subsequent generation processes remain open questions for future investigation.\nEthics Statement. Research on jailbreaking instruction-tuned LLMs inevitably raises concerns about whether it facilitates new risks. Although our method introduces a simpler and more efficient approach than most existing methods, we believe the risk profile does not fundamentally change, as the ability to jailbreak instruction-tuned LLMs is already well-documented. Our work is driven by the goal of understanding the safety mechanisms of LLMs, which we believe will ultimately contribute to the development of more robust and transparent AI systems."}, {"title": "SETTING DETAILS", "content": null}, {"title": "POSITIVE PREFIXES", "content": "The positive prefixes we use are a combination of an affirmative word or phrase followed by a general introductory sentence:\nSure!\nCertainly! +\nOf course! Here are some steps to\nHere are some ways you\nHere are some approaches that\nThese prefixes are based on observations of how LLMs respond to harmless questions. However, they remain heuristic-driven and could be further refined."}, {"title": "EARLY STOPPING CRITERION", "content": "Here we give a more detailed description about the early stopping criterion used in our prompt-general method. Figure 5 illustrated the relationship between the loss $\\mathcal{L}(M)$ and the modulation rate $\\mathbb{E}_{t,l,i} (1 \u2013 M_{l,i}^{(t)})$ during optimization process. A noticeable inflection point can be observed in the figure, where the modulation rate begins to decrease. Empirical studies indicate that performing early stopping at this inflection point provides MLP factors with the highest attack success rate."}, {"title": "OTHER JAILBREAKS", "content": "More details for other jailbreak methods we compared with in Section 4:\n\u2022 GCG, Greedy Coordinate Gradient (Zou et al., 2023b): This method optimizes an adver-sarial suffix by maximizing the probability that the model produces an affirmative response to a given prompt.\n\u2022 AP, AutoPrompt (Shin et al., 2020): Similar to GCG, but employs a different candidate selection strategy. In this paper, we use the adapted version proposed by Zou et al. (2023b).\n\u2022 ORTHO, Weight Orthogonalization (Arditi et al., 2024): This method compares hidden states between harmful and harmless prompts to identify refusal directions. It then selects an optimal direction and modifies model weights to eliminate the representation associated with that refusal direction.\n\u2022 GCG-M, GCG-Multi / Ensemble GCG: The multi-prompt version of GCG, optimizing a single suffix across a set of prompts.\n\u2022 Human (Shen et al., 2023): A fixed set of jailbreak templates gathered from in-the-wild human inputs. These templates are applied to user queries to form jailbreak attempts.\n\u2022 DR, Direct Request: This method directly uses the original queries without modification."}, {"title": "MORE DETAIL STUDIES", "content": "Here, we present more detailed studies of MLP re-weighting factors for different models and the prompt-specific method. All figures are modulation scales $\\Sigma_i M_{l,i}^{(t)}$, similar to Figure 2(a). For prompt-specific method, we average the MLP factor over the HarmBench test dataset."}, {"title": "MLP RE-WEIGHTING AS MECHANISM INTERPRETABILITY TOOL", "content": "Apart from its primary goal of jailbreaking, our method also serves as a tool for mechanism inter-pretability. By setting a relatively large $\\rho$ and running the optimization for a sufficiently long time"}]}