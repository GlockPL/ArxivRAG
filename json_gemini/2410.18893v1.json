{"title": "Creating and Repairing Robot Programs in Open-World Domains", "authors": ["Claire Schlesinger", "Arjun Guha", "Joydeep Biswas"], "abstract": "Using Large Language Models (LLMs) to produce robot programs from natural language has allowed for robot systems that can complete a higher diversity of tasks. However, LLM-generated programs may be faulty, either due to ambiguity in instructions, misinterpretation of the desired task, or missing information about the world state. As these programs run, the state of the world changes and they gather new information. When a failure occurs, it is important that they recover from the current world state and avoid repeating steps that they they previously completed successfully. We propose ROBOREPAIR, a system which traces the execution of a program up until error, and then runs an LLM-produced recovery program that minimizes repeated actions.\nTo evaluate the efficacy of our system, we create a benchmark consisting of eleven tasks with various error conditions that require the generation of a recovery program. We compare the efficiency of the recovery program to a plan built with an oracle that has foreknowledge of future errors.", "sections": [{"title": "1 Introduction", "content": "Service robots are general-purpose robots that are expected to be able to complete a variety of tasks in a home or office setting, including novel tasks that roboticists did not foresee. Towards this goal, there has been significant work on using LLMs to turn natural language instructions into short plans or task programs to execute on behalf of non-expert end-users (Hu et al., 2024; Liu et al., 2023a; Driess et al., 2023). The advantage of LLMs-based approaches is that (1) they are significantly better than prior work at translating natural language to code, and (2) they have open-world knowledge that allows them to fill in commonsense details that\nmay be missing from natural language instructions provided by non-experts.\nHowever, these LLM-authored task programs often go wrong for a variety of reasons. (1) Even the most capable models make errors, or misinterpret users' intentions; (2) programs may make faulty assumptions about the world state; and (3) users may need to provide additional instructions or information to help the robot complete the task successfully. The na\u00efve solution to these problems is to re-execute the task program, but re-execution may not be possible in every failing scenario, or may be sub-optimal if it involves re-executing completed actions. In contrast, when a human is given a simple task, they are often able to gracefully handle unforeseen situations by taking corrective actions or asking for help themselves.\nWe present ROBOREPAIR, an approach to natural language programming for service robots that supports both automatic and interactive failure recovery. Our two key insights are that (1) a failure in a task program may not manifest immediately, i.e., a failure may not cause an immediate program exception; and (2) in real-world deployments the number of possible failure modes is enormous, which makes it very challenging for an LLM to generate a program that is robust to all possible errors in a single shot. ROBOREPAIR addresses both of these problems by first prompting the LLM to generate simple task programs that assume no errors occur. Under the hood, the runtime system traces program execution and interrupts the programs when it detects an error, or when the user interrupts the program themselves. To resume execution, ROBOREPAIR synthesizes a recovery program that resumes execution from a context that includes the prior execution trace and feedback about the error. To avoid needlessly re-executing prior actions, ROBOREPAIR adds the execution trace to the prompt, to condition generation, and reifies it as runtime state to make it available during re-execution."}, {"title": "2 Background and Related Work", "content": "LLMs for Robot Tasks LLMs make it possible\nto translate task requests in natural language to ac-tionable steps either as plans, or robot-executable programs. This makes it possible for end-users without programming and robotics expertise to direct a robot to perform novel tasks. Moreover, these natural language directions can be succinct and rely on the LLM's world knowledge to fill in missing details.\nFor example, if a household robot is told go find my coffee mug, an LLM can reason that cof-fee is most likely in the kitchen, and thus should check there first. There are several different ap-proaches for turning this kind of natural language command into actions for a robot, including gen-erating code (Hu et al., 2024; Singh et al., 2022; Liang et al., 2023; Wang et al., 2023), generat-ing plans (Driess et al., 2023; Irpan et al., 2022; Song et al., 2023; Huang et al., 2022), and gener-ating specifications (Liu et al., 2023b,a; Maoz and Ringert, 2015). LLM+P uses LLMs to write PDDL (Ghallab et al., 1998) specifications to solve human given tasks in the most optimal manner rather than relying on the LLM to generate the plan.\nROBOREPAIR builds on the CodeBotler (Hu et al., 2024) platform which uses an LLM to pro-\nduce task programs from natural language instruc-tions. CodeBotler prefixes the prompt with a description of the robot API and a few shot examples. In contrast, ROBOREPAIR augments the prompt with prior state and few-shot examples of generated task programs, their traces (including errors), and recovery programs. ROBOREPAIR generates both the initial task program and recovery program for any natural language task description.\nHandling Changes to the Task and Environment\nThe aforementioned approaches do not gracefully adapt when the LLM-generated program or plan goes wrong. For example, consider the world state where the user's coffee mug is on the dining table: a program/plan that only looks in the kitchen will fail, and the goal of ROBOREPAIR is to overcome such failures, which may occur by misinterpreting of the task, making faulty assumptions about the world state, or missing information from users. Table 1 shows an example of some tasks and possible failure modes."}, {"title": "3 ROBOREPAIR", "content": "In the simplest case, ROBOREPAIR turns a user's natural language request into a single program that runs on the robot and completes the task. However, ROBOREPAIR is designed to gracefully recover from errors and interruptions that may occur during program execution. Thus given a natural language request, ROBOREPAIR generates and executes a sequence of robot programs, where each subsequent program is intended to recover from the failure encountered by the previous program. To generate each program, ROBOREPAIR prompts an LLM with 1) the original task, and the interleaved sequence of 2) previously executed programs, 3) their execution traces, and 4) errors or new requests from users they encountered. Figure 1 visualizes the process ROBOREPAIR goes through to generate the initial and recovery program.\nFor example, Figure 2a showcases how ROBOREPAIR handles an error triggered by the robot API. The robot is asked to visit every conference room to find and retrieve a backpack. After checking rooms 1 and 2, the robot finds that room 3 is\ninaccessible. Instead of failing the task, the robot recovers and completes the task by checking the remaining conference rooms. Notably, it does not re-check rooms 1 and 2 because the trace indicates that they were already visited before recovery. In Figure 2b, the robot is requested to see if John, James, or Carl wants to go for a walk. However, John does not answer the question (causing a time out), and James interrupts to provide specific details for his response. ROBOREPAIR uses the information about the errors/feedback from John and James to update its final response. It assumes John can't go on a walk because he couldn't answer and will pass on the information provided by James in the final report to the original user.", "latex": [{"title": "3.1 Traces and Execution", "content": "A trace of a robot program is a tuple containing 1) a\nlist of all locations visited by the robot, 2) the object the robot is currently carrying, 3) the questions the robot asked, and 4) the list of all objects detected in each visited location.", "latex": ["\\pi = <[loc_{0:t}], obj, [\\langle q, a\\rangle_{1:t}], [(loc, obj)_{1:t}])"]}, {"title": "3.2 Generating Robot Programs", "content": "We represent traces in two forms: 1) in a natural\nlanguage format which lists it as a series of steps (e.g., Figure 3) and 2) as an object available at run-time to the robot program. The two representations allows for the language model to both understand the steps that occurred and use code to accurately recreate any internal state that is necessary after recovery.\nAs a robot program, \\(\\pi \\in \\Pi\\), is executed by an\nexecutor Exec on a world \\(w \\in W\\), it produces a trace, \\(r \\in R\\), an error, \\(e \\in E\\), and PASS if the program executed successfully and FAIL otherwise.", "latex": [{"title": "3.3 Error Handling", "content": "Exec : W \u00d7 I \u2192W\u00d7R\u00d7E \u00d7 {PASS, FAIL}\nExec also produces an error message. This error message can either be from a robot API call failing or it can be given by a human if they interrupt the task. If a human interrupts the task, they can provide additional instructions or corrections to issues that could not have been previously known.\nROBOREPAIR produces a new program by utilizing a task prompt, \\(t \\in T\\), the sequence of all previously generated programs for that prompt, the sequence"}, {"title": "3.2.1 Context and Prompting", "content": "of traces produced by those programs, and the sequence of errors produced by those programs:", "latex": ["ROBOREPAIR : T \u00d7 R* \u00d7 E* \u00d7 I* \u2192 \u03a0", "ROBOREPAIR(p, r'[0:n\u22121], [0:n\u22121], [0:n\u22121]) = \u03c0n"]}, {"title": "4 Evaluation", "content": "ROBOREPAIR operates recursively where every\nproduced program and its result can be used to produce the next program in the sequence. To gen-erate the original program, we define the base case with an empty trace, error message, and previous program, r0 = (), e0 = \"\", \u03c00 = \"\".\nROBOREPAIR uses an LLM to generate robot pro-grams. It uses a context which provides examples consisting of an original natural language prompt, a task program, its resulting trace and error, and"}, {"title": "3.2.2 Task Program Generation", "content": "We empirically seek to answer three questions to\nevaluate ROBOREPAIR: 1) How often does Ro-BOREPAIR successfully complete a task after a recovery? 2) What is the optimality of the exe-cution traces generated by running ROBOREPAIR-In the base case, where ROBOREPAIR is only\nprovided with a natural language task descrip-tion, ROBOREPAIR generates the initial robot pro-gram that may complete that task. To do this, the prompt is formatted by providing the natural lan-guage direction as a comment, and then adding the task_program function header (Figure 6). The"}, {"title": "3.2.3 Recovery Program Generation", "content": "state, including future errors, an optimal plan \u0393 could be formed such that it completes all required steps of the task. ROBOREPAIR should produce \u03c0' such that when \u03c0' is executed, the difference between the sum of the lengths of the traces from \u03c0and \u03c0', and the length of \u0393 is minimized, i.e., \u03c0' = arg\u03c0' min (|rn| + |r\u03c0\u03b9| - |\u0393|).\nDifferent types of errors carry different kinds of information. When an error occurs because a user interrupts execution, ROBOREPAIR must regenerate the program to fit the user's feedback. For well-defined API errors, it is possible for ROBOREPAIR to produce code that automatically handles errors instead of generating a new program.\nTo enable the latter kind of error handling, we label the robot API with the errors that each action could throw, and include error handling code in the few-shot examples of programs. This allows Ro-BOREPAIR to avoid generating recovery programs for every kind of error.\nWhen an error occurs, either because the generated code is faulty, the environment does not behave as assumed, or the user interrupts the program, Ro-BOREPAIR generates a new program (Figure 9), \u03c0' that tries to avoid repeating steps that were successfully completed in the previous program \u03c0.\nTo generate this program, the original prompt is extended by adding the sequence of already pro-duced programs, their traces, and their errors. Figure 8 shows what the prompt for a recovery program would look like for the first recovery program is generated. This prompt is appended to the end of the ROBOREPAIR context.", "latex": [{"title": "3.3 Error Handling", "content": "If we had perfect foreknowledge of the world"}, {"title": "4.1 The ROBOREPAIR Benchmark", "content": "To evaluate the efficacy of ROBOREPAIR we devel-oped the ROBOREPAIR Benchmark consisting of\neleven tasks each with five unique prompts and an exception that requires construction of a recovery program. The descriptions of these tasks can be found in Table 2. The exceptions range from user interrupts to errors that might occur in execution of the robot API. Each task is accompanied by con-straints that must be fulfilled for it to be considered completed. Three tasks have errors triggered by the robot API and eight have user-given interrupts.\nEach benchmark task has a set of constraints c\u2208 C which must all be satisfied for the task to be considered successfully completed. We con-struct an evaluator Eval which is able to take the collection of traces for all generated programs and determine if the sum total fits the constraints. These"}, {"title": "4.2 Optimality of Recovery", "content": "to construct optimal plans and specify the task and oracle in PDDL (Ghallab et al., 1998).\nTable 4 shows that ROBOREPAIR takes more\nsteps than the optimal plan for most benchmark tasks, which is to be expected. This happens for two reasons: 1) the optimal plan avoids steps that lead to exceptions, and 2) when ROBOREPAIR recovers from errors, it takes extra steps to do so. For example, in the PlacePlants problem, when a human interrupts the task to indicate they don't want the plant, ROBOREPAIR can produce a recov-ery program that asks every subsequent human if they want the plant (Figure 10). ROBOREPAIR and Codebotler both avoid repeating unnecessary steps, but ROBOREPAIR takes more steps because of its tendency to add extra checks on recovery.\nROBOREPAIR struggles with the Meeting Time task because it is unable to understand the updated instructions included a message for the original user and not the human that was providing up-dated instructions. ROBOREPAIR also struggles with BringCoffee as it required lots of open-ended knowledge. It tries to go to a room called \"coffee machine\" which does not exist and produces re-covery programs that also have the fault.\nIn this section we compare ROBOREPAIR to plans created using an oracle that has perfect knowledge about the state of the world and exceptions that will occur. This oracle allows us to devise an optimal plan (\u0393). We use Fast Downward (Helmert, 2006)", "latex": [{"title": "3.2.1 Context and Prompting", "content": "Eval : R* \u00d7 C \u2192 {SAT, UNSAT}"}, {"title": "4.3 Variations Between Language Models", "content": "We compute the pass@1 score (Chen et al.,\n2021) across each task over every prompt and gen-eration (Table 3). We compare against a modified instance of CodeBotler (Hu et al., 2024) where the context has been changed to note the addition of possible errors in the robot API and to provide few-shot examples with error-handling. ROBORE-PAIR outperforms the modified CodeBotler method because it is able to handle user feedback. If Code-Botler continued to run after the user interrupt, it wouldn't be able to change its code to adapt to the new instructions leading to it failing to meet all the constraints for a task. Because of the user-given interrupts, it is necessary to regenerate code for certain tasks as that would be the only way to incorporate the error feedback.\nFinally, we compare the performance of ROBORE-PAIR across different models. We evaluate Meta Llama 3.1 (8B and 70B) (Dubey et al., 2024) and DeepSeek-Coder-V2 (16B) (DeepSeek-AI et al., 2024) (see appendix B for licenses and resources used). We evaluated both the base and instruction-tuned versions of these models. (GPT-40 is only available as an instruction-tuned model.) As above, we evaluate both their success rate (pass@1) on tasks, as well as their optimality by comparing their traces to \u0393.\nTable 3 shows that both Llama3.1 and DeepSeek-Coder-V2 perform worse than GPT-40. Llama3.1-70B benefits from the usage of the chat format while DeepSeek-Coder-V2 and Llama3.1-8B suffer. This is likely due to the larger size of Llama3.1-70B where extra training can improve performance rather than over fitting.\nTable 5 shows that there is not a significant difference between the optimality of the code generated by the completion models versus code generated by chat models. Both use approximately the same number of steps. What is interesting to note is that Llama 3.1-70B is able to solve an additional task, HalloweenShopping, and is able to produce more", "latex": [{"title": "5 Conclusion", "content": "successful programs. Similar tables for Llama3.1-8B and DeepSeekCoder-V2-Lite are in appendix A.\nWe present ROBOREPAIR, a system which is able to use an LLM to generate a robot program us-ing a robot API to complete a user-provided task, and if that program runs into an error, regenerate a new program which avoids the error. We also created the ROBOREPAIR Benchmark as a tool for evaluating the performance of ROBOREPAIR on re-covering from failures. We tested ROBOREPAIR's ability to solve the ROBOREPAIR Benchmark tasks and how optimally it is able to complete those tasks using GPT-40 as the LLM. ROBOREPAIR was able to complete a grand majority of the tasks without repeating steps, but was not the most optimal because the recovery programs often took a more cautious approach to avoid errors going forward. We also tested other LLMs and showed that inference mode can have an effect on the performance of ROBOREPAIR."}, {"title": "6 Limitations", "content": "This work has several limitations that require further research. The ROBOREPAIR Benchmark only tested situations where there was a single error and continuation of the task was straightforward. In"}, {"title": "A Optimality of Other Language Models Running ROBOREPAIR", "content": "tables 6 and 7 presents results with Llama 3.1 8B and DeepSeekCoder v2 Lite."}, {"title": "B Models and Compute Resources Used", "content": "We evaluate Meta Llama 3.1 (8B and 70B) (Dubey et al., 2024) and DeepSeek-Coder-V2 (16B) (DeepSeek-AI et al., 2024). Meta Llama 3.1 is under the Llama 3.1 Community License and DeepSeek-Coder-V2 is under the DeepSeek License Agreement. Both have an intended use of code generation and are allowed to be used in research settings. We run evaluation on an 8xH100 GPU server for 96 compute hours."}]}]}]}]}]}]}