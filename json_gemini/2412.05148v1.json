{"title": "LoRA.rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation", "authors": ["Donald Shenaj", "Ondrej Bohdal", "Mete Ozay", "Pietro Zanuttigh", "Umberto Michieli"], "abstract": "Recent advancements in image generation models have enabled personalized image creation with both user-defined subjects (content) and styles. Prior works achieved personalization by merging corresponding low-rank adaptation parameters (LoRAs) through optimization-based methods, which are computationally demanding and unsuitable for real-time use on resource-constrained devices like smartphones. To address this, we introduce LoRA.rar, a method that not only improves image quality but also achieves a remarkable speedup of over 4000\u00d7 in the merging process. LoRA.rar pre-trains a hypernetwork on a diverse set of content-style LoRA pairs, learning an efficient merging strategy that generalizes to new, unseen content-style pairs, enabling fast, high-quality personalization. Moreover, we identify limitations in existing evaluation metrics for content-style quality and propose a new protocol using multimodal large language models (MLLM) for more accurate assessment. Our method significantly outperforms the current state of the art in both content and style fidelity, as validated by MLLM assessments and human evaluations.", "sections": [{"title": "1. Introduction", "content": "The advent of text-to-image generation models based on denoising diffusion [19] allowed for significant improvements in output quality. Furthermore, recently there has been growing interest in personalized image generation [34, 36], where users can generate images that depict particular subjects or styles by providing just a few reference images.\nA key enabler of this personalization breakthrough is Low-Rank Adaptation (LoRA) [20], a parameter-efficient method that achieves high-quality personalization using only a few training samples. This innovation has spurred extensive model sharing on open-source platforms like Civitai [7] and Hugging Face [21], making pre-trained LoRAs readily available. The accessibility of these models has fueled interest in combining them to create images of personal subjects in various styles. For instance, a user might apply a concept (i.e., subject) LoRA trained on a few photos of their pet, and combine it with a downloaded style LoRA to render their pet in an artistic style of their choice.\nAveraging the LoRA weights can work acceptably when the subject and style share significant visual characteristics, but typically requires fine-tuning of the merging coefficients (i.e., the coefficients used to combine LoRAs) for more distinct subjects and styles. ZipLoRA [36] introduces an approach that directly optimizes merging coefficients through a customized objective function tailored to each subject-style LORA combination. However, ZipLoRA's reliance on optimization for each new combination incurs a substantial computational cost, typically taking minutes to complete. This limitation restricts its practicality for real-time applications on resource-constrained devices like smartphones. Achieving comparable or superior quality with respect to"}, {"title": "2. Related Work", "content": "Subject-Conditioned Image Generation has been extensively explored in recent years. DreamBooth [34] fine-tunes the entire generative model on reference images for subject fidelity. Several techniques have been proposed to mitigate extensive training. Textual Inversion [11] optimizes token embeddings for subject encoding, with extensions such as [1, 18, 38, 39, 48] enhancing flexibility. However, these methods face limitations in scaling to multiple concepts. Another line of work optimize specific network parts or employing specialized tuning, such as CustomDiffusion [25], LORA [8, 20, 42], SVDiff [17], and DreamArtist [9]. In particular, LoRA has gained popularity for its training efficiency and adaptability to multiple concepts, making it a widely used approach for subject conditioning.\nMore recently, techniques for zero-shot personalization aim to avoid fine-tuning by either (i) using separate conditioning encoders (encoder-based approaches) [6, 12, 26\u201329, 40, 50, 55, 56]; or (ii) utilizing features from the generative model's backbone to guide generation (encoder-free methods) [32, 53]. However, these methods often require extensive additional storage, limiting their applicability in resource-constrained environments.\nSubject- and Style-Conditioned Image Generation. Beyond subject conditioning, many works tackle style-conditioned generation, such as StyleGAN [23], StyleDrop [37] and DreamArtist [9]. However, these methods lack the ability to handle both subject and style conditioning jointly. Recent approaches addressing this challenge include CustomDiffusion [25], which learns multiple concepts through expensive joint training but struggles to disentangle style from subject, and HyperDreamBooth [35], which generates personal subjects with good style editability via textual prompt. B-LORA [10] proposes a layer-wise LoRA tuning pipeline for either content or style. Notably, ZipLoRA [36], the closest reference for our work, merges pre-trained subject and style LoRAs via test-time optimization to discover the optimal merging coefficients. Concurrent work such as RB-Modulation [33] uses a style descriptor for modulation without LoRAs. FreeTuner [45] and Break-for-Make [44] further explore style disentanglement with separate content and style encoders or training subspaces.\nOur method focuses on efficient zero-shot merging of subject and style LoRAs aiming for high-quality subject preservation while preserving text editability.\nModel Merging is an increasingly popular way to enhance the abilities of foundational models in both language [16] and vision domains [47]. The simplest technique is direct arithmetic merge that averages the weights of multiple fine-tuned models [41]. Despite its simplicity, it can improve performance and enable multi-tasking to at least a certain extent [22]. Following [41], diverse strategies have been proposed. TIES [46] mitigates interference between parameters of different models due to different signs, while DARE [51] drops some parameters and rescales the remaining ones to reduce redundancy and interference. DARE-TIES [13] combines DARE and TIES, and demonstrates successful merging in complex scenarios.\nLORA Merging for Image Generation has recently gained attention. Mix-of-Show [14] and LoRA-Composer [49] merge the concept of each LoRA in the output image for multi-concept generation (instead of subject-style). Further, they require a custom version of LoRAs, hindering wide compatibility. ZipLoRA [36] merges standard LoRAs, focusing on subject-style generation through parameter optimization. However, this approach requires several minutes per merge at test time, limiting its usability in real-time scenarios. Our LoRA.rar builds upon ZipLoRA's foundations, targeting both a more efficient solution and improved results. Hypernetworks, or networks generating the parameters of other networks [4, 15], have found diverse use cases. Hypernetworks are used to generate LoRA parameters in [3], while [2] uses them for model aggregation in federated learning."}, {"title": "3. Method", "content": "Our objective is to design and train a hypernetwork that predicts weighting coefficients to merge content and style LoRAs. Using a set of LoRAs, we train this hypernetwork to produce suitable merging coefficients for unseen content and style LoRAs at the deployment stage.\nWe start by formulating the problem in Sec. 3.1, detailing how LoRAs are applied to the base model and outlining the limitations of the current state-of-the-art approach. In Sec. 3.2, we describe the construction of the LoRA dataset used to train and evaluate our solution. Sec. 3.3 discusses the structural design of our hypernetwork, followed by an overview of the training procedure in Sec. 3.4."}, {"title": "3.1. Problem Formulation", "content": "We use a pre-trained image generation diffusion model D with weights Wo and LoRA parameters L with weight update matrix \\(\\Delta W\\). For simplicity, we consider one layer at a time. A model D that uses a LoRA L is denoted as \\(D_L = D \\oplus L\\) with weights \\(W_0 + \\Delta W\\), where operation \\(\\oplus\\) means we apply LoRA L to the base model D. To specify content and style, we use LoRAs Lc (content) and Ls (style) with respective weight update matrix \\(\\Delta W_c\\) and \\(\\Delta W_s\\). Our objective is to merge Lc and Ls into Lm, producing a matrix \\(\\Delta W_m\\) that combines content and style coherently in generated images. The merging operation can vary, from simple averaging to advanced techniques like ZipLoRA's and ours.\nZipLoRA takes a gradient-based approach, learning column-wise merging coefficients mc and ms for \\(\\Delta W_c\\) and \\(\\Delta W_s\\), respectively, as: \\(\\Delta W_m = m_c \\odot \\Delta W_c + m_s \\odot \\Delta W_s\\), where \\(\\odot\\) represents element-by-column multiplication.\nAlthough ZipLoRA achieves high-quality results, it requires training these coefficients from scratch for each content-style pair, with distinct coefficients for different com-"}, {"title": "3.2. LORA Dataset Generation", "content": "To train our hypernetwork, we first build a dataset of LoRAs. Content LoRAs are trained on individual subjects from the DreamBooth dataset [34], and style LoRAs are trained on various styles from the StyleDrop / ZipLoRA [36, 37] datasets. Each LoRA is generated using the DreamBooth protocol.\nWe split the LORA dataset into training \\(\\{L^{train}_c\\}, \\{L^{train}_s\\}\\), validation \\(\\{L^{val}_c\\}, \\{L^{val}_s\\}\\), and test \\(\\{L^{test}_c\\}, \\{L^{test}_s\\}\\) sets. During training and evaluation, we sample content-style LoRA pairs. The hypernetwork is trained on the training sets, with hyperparameters and design choices tuned on the validation sets. The test sets are reserved to assess performance on novel content-style pairs."}, {"title": "3.3. Hypernetwork Structure", "content": "Our hypernetwork, H, takes two LoRA update matrices as inputs: \\(\\Delta W_c \\in \\mathbb{R}^{m \\times n}\\) for content and \\(\\Delta W_s \\in \\mathbb{R}^{m \\times n}\\) for style, and predicts column-wise merging coefficients \\(m_c \\in \\mathbb{R}^n\\) and \\(m_s \\in \\mathbb{R}^n\\). Given the high dimensionality of each update matrix, flattening them directly as input would be impractical. To address this, we assume that the merging coefficient for each column can be predicted independently.\nFor each column i, we extract the respective content and style columns, \\(w^c_i = \\Delta W_c[:, i]\\) and \\(w^s_i = \\Delta W_s[:, i]\\), and concatenate them as \\([w^c_i, w^s_i] \\in \\mathbb{R}^{2m}\\) to form the input features for the hypernetwork. We treat different columns as a minibatch, allowing for efficient parallel processing. The full hypernetwork input is thus \\(\\text{concat}(\\Delta W_c, \\Delta W_s, \\text{dim} = 1) \\in \\mathbb{R}^{n \\times 2m}\\).\nTo accommodate the various LoRA matrix sizes within the diffusion model D, we designed H with separate input layers tailored to each unique matrix size, each mapped to a shared hidden dimension. In our case, the hypernetwork uses two input layers with ReLU non-linearities and a shared output layer to predict merging coefficients for each column.\nSince different rows are treated as a mini-batch, overall the hypernetwork outputs 2n coefficients, one for each column of content and style LoRAs:\n\\[m_c, m_s = H(L_c, L_s).\\]\nThese coefficients are used to merge the LoRAs Lc and Ls, resulting in the merged LoRA Lm with update matrix \\(\\Delta W_m\\):\n\\[\\Delta W_m = m_c \\Delta W_c + m_s \\Delta W_s.\\]\nFig. 4 provides an overview of how content and style LoRAs predict merging coefficients. Notably, we apply hypernetwork-guided merging for query and output LoRAs, while we use simple averaging for key and value LoRAs. This configuration empirically outperformed other tested options, as detailed in the Supp. Mat."}, {"title": "3.4. Hypernetwork Training", "content": "We train the hypernetwork H by sampling content-style LORA pairs from the training set \\(\\{L^{train}_c\\}, \\{L^{train}_s\\}\\). The hypernetwork generates merging coefficients, which are then used to compute a merging loss \\(\\mathcal{L}_{merge}\\) that updates the parameters of H. We adopt the merging loss \\(\\mathcal{L}_{merge}\\) from [36], which includes terms that ensure both content and style fidelity, while also encouraging orthogonality between content and style merging coefficients. Specifically, \\(\\mathcal{L}_{merge}\\) is defined as:\n\\[\\begin{aligned}\n\\mathcal{L}_{merge} =& ||(D \\oplus L_m)(x_c, p_c) - (D \\oplus L_c)(x_c, p_c)||_2 \\\\\n&+ ||(D \\oplus L_m)(x_s, p_s) - (D \\oplus L_s)(x_s, p_s)||_2 \\\\\n&+ \\lambda m_c^T m_s,\n\\end{aligned}\\]\nwhere xc, xs are the noisy latents, and pc, ps are the text prompts for content and style reference images respectively [36]. The term \\(\\lambda\\) controls the strength of the orthogonality-promoting regularization term.\nThe training process is formalized in Algorithm 1. Architecture choices for the hypernetwork, as well as hyperparameters, are optimized on the validation set, and the final evaluation is conducted on the test set, when the hypernetwork H simply predicts the merging coefficients for new content and style LoRAs \\(L_c \\in \\{L^{test}_c\\}, L_s \\in \\{L^{test}_s\\}\\)."}, {"title": "4. Joint Subject-Style Evaluation Metrics", "content": "In this section we discuss how to evaluate personalized image generation methods across diverse subjects and styles."}, {"title": "5. Experiments", "content": "Baselines. We compare our approach to several established methods, including: joint training of both content and style via Dreambooth [34]; direct merging of LoRA weights [41]; general model merging techniques such as DARE [51], TIES [46], and DARE-TIES [13]; and ZipLoRA [36], which is specifically designed for merging subject and style LoRAs. ZipLoRA has also been compared in [36] with strategies such as StyleDrop [37], Custom Diffusion [25] and Mix-of-show [14]. These methods, however, have been shown to perform less effectively while being computationally costly, so we exclude them from further comparison in this work.\nImplementation Details. All experiments use the SDXL v1 Stable Diffusion model [31], following the setup in [36]. For subject-specific LoRAs, we adopt rare unique token identifiers as in [34]. in contrast, style LoRAs are fine-tuned using text description identifiers, following [37], where these were found more effective for style representation. Base LoRAs are trained as in [36], for 1000 fine-tuning steps, with batch size 1, a learning rate of 5 \u00d7 10-5 and a rank of 64. The text encoder remains frozen during training. The hypernetwork used is a two-layer MLP with two separate input layers of size 1280 and 2560, followed by a ReLU activation function, a shared hidden layer of size 128, and two outputs. We train our hypernetwork for 100 different \\(\\{L_c, L_s\\}\\) combinations (totalling 5000 steps), with \\(\\lambda\\)=0.01, learning rate 0.01 and the AdamW optimizer. For ZipLoRA, we use a training setup of 100 steps with the same \\(\\lambda\\) and learning rate. The DARE, TIES, and DARE-TIES baselines are evaluated with uniform weights and a density of 0.5. For joint training, we used a multi-concept variant of Dreambooth LoRA as in [36]. In all experiments, 50 diffusion inference steps are used.\nDatasets. Our hypernetwork is trained on a set of LoRAs rather than images. Subject LoRAs are fine-tuned on images from the DreamBooth [34] dataset, with style LoRAs are fine-tuned on images from the StyleDrop / ZipLoRA [36, 37] datasets. Specifically, our datasets includes 30 subjects (each with 4\u20135 images) and 26 styles (each represented by a single image). For training, validation, and testing, we split the subjects into 20\u20135\u20135 and styles into 18\u20133\u20135 (see the Supp. Mat. for details), yielding a total of 360 subject-style LoRA combinations for hypernetwork training, a quantity shown to be sufficient for robust performance.\nEvaluation Details. For our MLLM-based metric, MARS2,"}, {"title": "5.1. Quantitative Analysis", "content": "We quantitatively assess the performance of our solution and the baselines in three main ways: (1) through our MLLM-based MARS2 metric as described in Sec. 4; (2) using standard CLIP-I, CLIP-T, and DINO metrics, that suffer from the already discussed limitations; and (3) via a human evaluation study on a subset of generated samples.\n1) MLLM Evaluation Results are presented in Table 1. Our solution consistently outperforms all methods, including ZipLoRA, in both content and style accuracy. For the best sample (selected by MLLM from 10 generated images as one with correct style and content if available), both our solution and ZipLoRA achieve perfect accuracy, indicating that users can reliably choose preferred outputs when multiple samples are available. Across all generated images, our solution performs better on average than ZipLoRA, likely benefitting from its capacity to leverage knowledge learned from diverse content-style LoRA combinations.\n2) Standard Metrics Evaluations are reported in Table 2. We include this analysis for informational purposes only. As explained in Sec. 4, these metrics (DINO, CLIP-I, CLIP-T) are not optimal for the joint subject-style personalization task. Specifically, DINO (CLIP-I) is maximized when the subject (style) reference images are copied without meaningful integration, so more attention should be given to MLLM and human evaluation results.\n3) Human Evaluation results are reported in Fig. 7. This evaluation was conducted on a subset of generated sam-"}, {"title": "5.2. Qualitative Analysis", "content": "We conduct a qualitative analysis of LoRA.rar by: (1) comparing the images generated by LoRA.rar with those produced by competing methods, and (2) analyzing the diversity of images generated by LoRA.rar across concepts and styles. Comparison against state of the art is shown in Fig. 8. The results demonstrate that LoRA.rar excels in capturing fine details across various styles, consistently producing high-quality images. While ZipLoRA also generates high-quality images, LoRA.rar outperforms it in terms of overall fidelity to both content and style. A limitation of ZipLoRA is in too realistic generation, e.g., the teapot in 3D rendering style is immersed in a photorealistic scene, and the wolf plushie in oil painting is not a painting. Other approaches show less consistent results, i.e., direct merge is able to produce a teapot or a stuffed animal in 3D rendering style (with minor inaccuracies), but fails at generating flat cartoon illustrations, where there is no one-to-one mapping of content and style. DARE, TIES, DARE-TIES do not produce satisfactory results, either the style or the concept are incorrect, or none. Joint training, presents improved results compared to direct merge, but has the same limitations. Reliability across different subject-style pairs is shown in Fig. 9, where LoRA.rar consistently works well across diverse combinations of concepts and styles, highlighting its"}, {"title": "5.3. Additional Analyses", "content": "We provide a detailed analysis of resource usage in Table 3. Our findings highlight the efficiency and scalability of LoRA.rar in comparison to ZipLoRA:\n1) Runtime Efficiency: our solution generates the merging coefficients over 4,000 times faster than ZipLoRA on an NVIDIA 4090, achieving real-time performance. While Zi-PLORA requires 100 training steps for each concept-style pair, LoRA.rar generates merging coefficients in a single forward pass (per layer) using a pre-trained hypernetwork.\n2) Parameter Storage: ZipLoRA needs to store the learned coefficients for every combination of concept and style for later use. LoRA.rar only needs to store the hypernetwork, which has 3 times fewer parameters than a single ZipLoRA combination.\n3) Sample Efficiency: on average, LoRA.rar requires fewer attempts than ZipLoRA to produce a high-quality image that aligns with both content and style\u20142.28 attempts for LoRA.rar versus 2.55 for ZipLoRA. This improvement reflects LoRA.rar's enhanced accuracy in generating visually coherent outputs without extensive retries, further optimizing resource usage and user experience.\n4) Memory Consumption at Test Time: LoRA.rar is efficient in terms of memory, which is dominated by the generative model (~15GB), with negligible overhead for our approach, while ZipLoRA requires additional 4GB (~19GB totally).\nFinally, we analyze the merging coefficients learned by LoRA.rar in Fig. 10. LoRA.rar learns a non-trivial adaptive merging strategy, with diverse coefficients (including also some negative values). This adaptability allows LoRA.rar to flexibly combine content and style representations, likely contributing to its superior performance. ZipLoRA, instead, mostly converges to an adaptive yet binary merging strategy (see Fig. 3). This more rigid merging strategy may limit its capacity to finely integrate details across styles and subjects, further underscoring the advantage of LoRA.rar's approach in generating images with accurate content and style."}, {"title": "6. Conclusion", "content": "In this work, we introduced LoRA.rar, a novel method for joint subject-style personalized image generation. LoRA.rar leverages a hypernetwork to generate coefficients for merging content and style LoRAs. By training on diverse content-style LoRA pairs, our method can generalize to new, unseen pairs. Our experiments show that LoRA.rar consistently outperforms existing methods in image quality, as assessed by both human evaluators and an MLLM-based judge specifically designed to address the challenges of joint content-style personalization. Crucially, LoRA.rar generates the merging coefficients in real time, bypassing the need for test-time optimization used by state-of-the-art methods."}]}