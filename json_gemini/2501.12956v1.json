{"title": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models", "authors": ["Pengxiang Zhao", "Xiaoming Yuan"], "abstract": "Large Language Models (LLMs) face significant deployment challenges due to their substantial resource requirements. While low-bit quantized weights can reduce memory usage and improve inference efficiency, current hardware lacks native support for mixed-precision General Matrix Multiplication (mpGEMM), resulting in inefficient dequantization-based implementations. Moreover, uniform quantization methods often fail to capture weight distributions adequately, leading to performance degradation. We propose GANQ (GPU-Adaptive Non-Uniform Quantization), a layer-wise post-training non-uniform quantization framework optimized for hardware-efficient lookup table-based mpGEMM. GANQ achieves superior quantization performance by utilizing a training-free, GPU-adaptive optimization algorithm to efficiently reduce layer-wise quantization errors. Extensive experiments demonstrate GANQ's ability to reduce the perplexity gap from the FP16 baseline compared to state-of-the-art methods for both 3-bit and 4-bit quantization. Furthermore, when deployed on a single NVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57\u00d7 speedup over the baseline, advancing memory and inference efficiency in LLM deployment.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have demonstrated impressive performance across various domains (Brown et al., 2020; Achiam et al., 2023; Touvron et al., 2023a;b; Dubey et al., 2024; Gemini Team et al., 2023; Chowdhery et al., 2023; Zhang et al., 2023; Wang et al., 2023; Arefeen et al., 2024; Li et al., 2024a; Huang et al., 2024). However, their deployment for inference remains challenging due to demanding resource requirements. For example, the LLaMA-3-70B (Dubey et al., 2024) model needs at least 140 GB of GPU memory in FP16, which exceeds current GPU capacities. While larger LLMs often yield better accuracy (Kaplan et al., 2020), these substantial resource demands hinder the practical deployment of LLMs, posing a barrier to their widespread adoption.\nQuantization is a promising solution to reduce inference costs for LLMs. For example, 4-bit weight quantization can reduce memory usage for model loading by nearly 75% compared to FP16. In general, quantization techniques are categorized into quantization-aware training (QAT) and post-training quantization (PTQ). QAT integrates quantization into the training process to achieve higher accuracy but is computationally expensive, often requiring extensive samples and significant GPU hours (Liu et al., 2023). This makes QAT impractical for large models. In contrast, PTQ is a cost-effective alternative that applies quantization after training, making it the preferred choice for LLMs (Nagel et al., 2020; Yao et al., 2022; Frantar et al., 2022; Xiao et al., 2023; Dettmers et al., 2023; Kim et al., 2023; Lin et al., 2024; Shao et al., 2024; Ma et al., 2024; Li et al., 2024b). Among PTQ methods, weight-only quantization, which uses low-precision weights while retaining high-precision activations, has become a particularly attractive approach. By reducing memory traffic and alleviating memory-bound bottlenecks, weight-only quantization accelerates inference (Kim et al., 2023). Additionally, compared to weight-activation quantization, it avoids significant accuracy degradation by preserving the precision of activations, ensuring better model performance.\nDespite its promise, weight-only quantization faces two key challenges. First, it shifts the core computation of LLM inference from standard General Matrix Multiplication (GEMM) to mixed-precision GEMM (mpGEMM), where low-precision weights (e.g., INT4/3/2) are multiplied with high-precision activations (e.g., FP16). Current hardware lacks native support for mpGEMM, necessitating dequantization to upscale low-bit weights into supported formats (see the left part of Figure 1(a)). This additional step introduces inefficiencies, particularly in large-batch scenarios, undermining the expected performance gains (Mo et al., 2024). Second, most existing methods rely on uniform quantization $Q : \\mathbb{R} \\rightarrow [0,2^N - 1] \\cap \\mathbb{Z}$ defined as $Q(x) = \\text{clamp}(\\lfloor \\frac{x}{s} \\rceil + z, 0, 2^N - 1)$, where $\\lfloor \\cdot \\rceil$ denotes rounding, $N$ is the target bit width, $s$ is the scaling factor, and $z$ is the zero-point (Frantar et al., 2022; Xiao et al., 2023; Dettmers et al., 2023; Lin et al., 2024; Shao et al., 2024; Ma et al., 2024; Li et al., 2024b). However, LLM weight distributions are often highly non-uniform (see Figure 1(b)), making uniform quantization inadequate and resulting in suboptimal representations, particularly due to outliers. Techniques such as introducing learnable scale and zero-point parameters (Shao et al., 2024), applying affine transformations to preprocess weights (Ma et al., 2024), or splitting weights into various components and quantizing those that are easier to process (Dettmers et al., 2023; Li et al., 2024b), have been proposed to mitigate these issues. While these methods improve accuracy, they primarily address challenges within the uniform quantization framework rather than fundamentally enhancing the quantization method itself. Furthermore, they often increase computational complexity during inference due to the extra operations they require.\nTo address these issues, we propose GANQ (GPU-Adaptive Non-Uniform Quantization), a layer-wise post-training non-uniform quantization framework optimized for lookup table (LUT)-based mpGEMM. In LUT-based mpGEMM (see the right part of Figure 1(a)), complex computations are replaced with simple table lookups, supported by several GPU kernels (Kim et al., 2023; Mo et al., 2024). The primary challenge then becomes how to determine effective low-bit representations for the LUTs. Existing non-uniform quantization methods often rely on heuristic-based approaches, such as manually designed mappings (e.g., power-exponent functions (Yvinec et al., 2023)) or clustering-based methods with heuristic distance metrics (Han et al., 2015; Xu et al., 2018; Kim et al., 2023). While these methods may achieve good results in specific cases, their heuristic nature limits generalization and theoretical grounding. In contrast, GANQ introduces a principled optimization model for layer-wise LUT-based non-uniform quantization, formulated as a mixed-integer quadratic programming problem. This model minimizes the the discrepancy between the outputs of the quantized and original layers, thereby preserving accuracy. To efficiently address this complex model, GANQ utilizes its decomposable structure to divide the original optimization task into multiple independent one-dimensional subproblems, which can be processed in parallel using GPU acceleration to achieve substantial computational efficiency. Besides, GANQ employs an alternating direction optimization framework that capitalizes on the splittable structure of decision variables, effectively reducing quantization error.\nIn addition, although GANQ is designed as a base quantization method, it is fully compatible with current techniques for handling outliers, such as splitting weights into sparse components (to address outliers) and quantized components (Dettmers et al., 2023; Kim et al., 2023), thereby enabling further performance enhancements.\nWe evaluate GANQ extensively across various model families and sizes on language modeling tasks. The results show that GANQ consistently outperforms previous methods in quantization performance. Moreover, GANQ is highly resource-efficient and easy to implement. For instance, GANQ processes the LLaMA-2-7B model on a single NVIDIA RTX 4090 GPU in approximately one hour using only 128 samples. Additionally, our deployed models on the NVIDIA RTX 4090 GPU achieve significant latency improvements, with speedups of up to 2.57\u00d7 compared to the FP16 baseline. These results highlight the effectiveness of GANQ in both quantization quality and inference efficiency."}, {"title": "2. Related Work", "content": "Quantization for LLMs. Quantization reduces the bit-precision of neural networks, resulting in smaller models and faster inference. It has become a key direction for compressing LLMs given their growing size and inference costs. Current quantization methods for LLMs are broadly categorized into QAT (Liu et al., 2023) and PTQ (Nagel et al., 2020; Yao et al., 2022; Frantar et al., 2022; Xiao et al., 2023; Dettmers et al., 2023; Kim et al., 2023; Lin et al., 2024; Shao et al., 2024; Ma et al., 2024; Li et al., 2024b)."}, {"title": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models", "content": "QAT integrates quantization into the training process, preserving high performance but incurring prohibitive training costs, making it impractical for LLMs. In contrast, PTQ applies quantization to pretrained models, requiring only a small subset of data and modest computational resources, making it particularly appealing for LLMs. PTQ methods can be further classified into wight-only quantization and weight-activation quantization.\nWeight-only quantization focuses on compressing model weights into low-bit formats. For example, GPTQ (Frantar et al., 2022) utilizes the optimal brain surgeon framework (Hassibi & Stork, 1992) for quantization and reconstruction. OminiQuant (Shao et al., 2024) introduces learn-able parameters to determine quantization factors (e.g., scale and zero-point), while AffineQuant (Ma et al., 2024) extends this idea by incorporating a learnable matrix to preprocess weights before quantization.\nWeight-activation quantization compresses both weights and activations, often addressing their quantization jointly. For example, SmoothQuant (Xiao et al., 2023) shifts quantization difficulty from activations to weights using manually designed scaling factors. Similarly, SVDQuant (Li et al., 2024b) applies this approach while further decomposing weights into low-rank and quantized components.\nWhile weight-activation quantization can offer broader compression, studies (Kim et al., 2023; Lin et al., 2024) have shown that LLM inference, especially during generation, is heavily memory-bound, with weight access dominating activation access by orders of magnitude. Consequently, weight-only quantization is more effective for on-device deployment of LLMs. In this work, we focus on weight-only PTQ for its efficiency and suitability for LLMs.\nOutlier Mitigation. Due to the widely used uniform quantization mapping and the inherent non-uniform distribution of LLM weights, a key challenge is the presence of outliers. These outliers unnecessarily expand the quantization range (see Figure 1(b)), comprising quantization performance. Recent methods have been proposed to address this issue. For example, SpQR (Dettmers et al., 2023) and SqueezeLLM (Kim et al., 2023) retain outliers in sparse matrices while applying quantization to the remaining weights to mitigate their impact on overall performance. AWQ (Lin et al., 2024) independently quantizes the channel-wise salient weights to improve performance, and SVDQuant (Li et al., 2024b), as mentioned, decomposes weights into low-rank and quantized components. While these methods effectively handle outliers and enhance quantization performance, they often introduce additional computational overhead during inference. For instance, SpQR and SqueezeLLM require both mpGEMM and sparse matrix multiplication, whereas SVDQuant adds an extra low-rank computation branch.\nIn this work, we propose a direct solution by introducing a non-uniform quantization framework that adapts to the distribution of LLM weights. Furthermore, our method is compatible with these outlier-handling techniques, enabling further performance enhancements when combined.\nNon-Uniform Quantization. The non-uniform distribution of weights in LLMs highlights the importance of non-uniform quantization. However, existing non-uniform quantization methods often rely on heuristic-based approaches, limiting their generalization and theoretical grounding. NUPES (Yvinec et al., 2023) replaces uniform quantization with power-exponent functions and employs gradient-based optimization to learn the exponent parameter. Other methods focus on identifying shared weights, thereby forming a codebook, which is suitable for LUT-based mpGEMM. For example, Han et al. (2015) apply k-means clustering to minimize the Euclidean distance between weights and centroids in convolutional neural networks (CNNs), while Xu et al. (2018) extend this approach by using a loss-based metric for k-means clustering in CNNs. For LLMs, SqueezeLLM (Kim et al., 2023) adapts this idea by leveraging sensitivity-based k-means clustering, where the sensitivity metric measures measures the extent to which the model is perturbed after quantization. To mitigate the computational expense of this calculation, SqueezeLLM approximates the required Hessian matrix using the diagonal elements of the Fisher information matrix (Fisher, 1925).\nIn contrast, we propose a principled optimization model for layer-wise LUT-based non-uniform quantization for LLMs, along with an efficient GPU-adaptive algorithms to solve it."}, {"title": "3. Methodology", "content": "3.1. Optimization Model for Non-uniform Quantization\nConsider a linear layer with weight matrix $W \\in \\mathbb{R}^{m\\times n}$ and input activation $X \\in \\mathbb{R}^{n\\times p}$. As shown in the right part of Figure 1(a), LUT-based quantization aims to compress $W$ by representing its elements using a codebook. Specifically, the elements of the i-th channel in the quantized weight matrix $W$ are selected from the codebook $T_i = \\{t_{i,0}, t_{i,1},..., t_{i,2^N -1} \\}$, where $N$ is the bit-width of the quantization (e.g., 3 or 4 bits). Thus, each element $W_{i,j}$ satisfies $W_{i,j} \\in T_i$.\nIn practice, LUT-based quantization stores two components: a low-bit query matrix $Q \\in \\{0, 1, ... 2^N - 1\\}^{m\\times n}$, which specifies the indices of values in the codebook, and the codebook itself, $T \\in \\mathbb{R}^{m\\times 2^N}$, which contains the quantized values for each channel. For example, if $Q_{ij} = 0$, then $W_{i,j} = t_{i,0}$. Compared to the widely used basic per-channel uniform quantization (Frantar et al., 2022; Xiao et al., 2023), which requires two parameters per channel"}, {"title": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models", "content": "(i.e., scale and zero-point), this mechanism demands slightly more storage. However, as $\\min\\{m, n\\} \\gg 2^N$ in practice, the additional storage overhead is negligible. As shown in Table 1, for typical model sizes, the storage usage of LUT-based quantization remains comparable to the basic uniform quantization, differing by less than 0.2%. Moreover, some uniform quantization methods, such as OmniQuant (Shao et al., 2024) and AffineQuant (Ma et al., 2024), also require extra parameters.\nTo enable effective LUT-based non-uniform quantization, we formulate an optimization model aimed at minimizing the layer-wise output error.:\n$\\min_{Q,T} ||WX - \\hat{W}X||_F, \\text{ s.t. } \\hat{W}_{i,j} = T_{i,Q_{i,j}}, \\forall i, j,$\nwhere $|| \\cdot ||_F$ denotes the Frobenius norm, and Q and T are the decision variables.\nNote that the quantized output for each row $(\\hat{W}X)_{i,:}$ depends only on its corresponding codebook $T_{i,:}$ and query vector $Q_{i,:}$. Consequently, the model in (1) is inherently decomposable across the rows of W. Leveraging this property, the problem can be reformulated into m independent sub-problems, which are highly parallelizable and particularly suitable for GPU acceleration. Specifically, this parallelization is achieved by expressing computations in matrix form, which enables efficient matrix-vector and element-wise operations across rows. Furthermore, each subproblem can be expressed as a mixed-integer quadratic programming problem:\n$\\min_{S_i, T_i} || W_i X - T_i S_i^T X ||_2 \\text{ s.t. } 1 \\perp S_i = 1^T, \\forall i,$\nwhere $W_i \\in \\mathbb{R}^{1\\times n}$ is the i-th row of $W$, $T_i \\in \\mathbb{R}^{1\\times 2^N}$ is the i-th row of $T$, $S_i \\in \\{0, 1\\}^{2^N \\times n}$ is a column-wise one-hot encoding matrix indicating the mapping of elements from $T_i$, and $1$ denotes an all-one vector.\nThe mixed-integer structure of $S_i$ introduces significant combinatorial complexity, and the bilinear interaction between $S_i$ and $T_i$ in the objective further compounds the computational challenge, rendering the problem inherently non-convex and non-smooth. These factors pose serious difficulties for off-the-shelf solvers, especially in large-scale settings with high-dimensional weight matrices and input"}, {"title": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models", "content": "activations. In response, we develop a specialized, GPU-adaptive approach tailored to navigate this complex search space while scaling to practical problem sizes.\n3.2. GPU-Adaptive Non-uniform Quantization Method\nTo efficiently solve the model in (2) for LUT-based non-uniform quantization, we employ an alternating direction optimization framework. This framework iteratively updates $S_i$ and $T_i$ by decomposing the objective into two subproblems. Each subproblem optimizes one decision variable while keeping the other fixed. The iterative scheme is outlined as follows:\n$\\begin{cases}S_i^{k+1} = \\argmin_{S_i} \\{ || W_i X - T_i S_i^T X ||^2 | 1 \\perp S_i =1 \\},\\\\T_i^{k+1} = \\argmin_{T_i} \\{|| W_i X - T_i S_i^{k+1} X ||^2 \\}.\\end{cases}$       \nThe $T_i$-subproblem in (4) is an unconstrained quadratic program and admits a closed form solution given by:\n$T_i^{k+1}=W_iXX^T(S_i^{k+1})^T((S_i^{k+1})XX^T (S_i^{k+1})^T)^{\\dagger},$\nwhere $(\\cdot)^{\\dagger}$ denotes the Moore-Penrose inverse. Notably, the matrix $(S_i^{k+1})XX^T (S_i^{k+1})^T$ has dimensions $2^N \\times 2^N$, which is relatively small in practice (e.g., 16 \u00d7 16 under 4-bit quantization), ensuring that the computation remains efficient. Moreover, computing (5) involves only matrix-vector multiplications, making it highly efficient for GPU acceleration. Since the solutions to all $T_i$-subproblems share the same formulation, they can be combined into a single batch computation by stacking all $W_i$ and $T_i$ vectors row-wise and organizing $S_i$ matrices into a tensor. Then, matrix operations can be used to efficiently compute the batch. This approach leverages modern GPUs' parallel processing capabilities, significantly reducing computational overhead and improving overall efficiency.\nThe primary challenge lies in the $S_i$-subproblem (3), which is a discrete, non-convex, and non-smooth combinatorial optimization problem. In the case of 4-bit quantization, each element of $S_i$ can assume one of 16 possible values. A brute-force search over all combinations would require $(16)^n$ operations, rendering it computationally prohibitive. Therefore, developing efficient solution techniques is essential for practical applications."}, {"title": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models", "content": "To address the $S_i$-subproblem, we propose an efficient method that leverages the problem's inherent structure. The objective in (3) can be expanded as:\n$||WX - TSX||^2$\n$=(W \u2013 TS)(XX)(W; \u2013 TS\u00bf)T.$\nThen, consider the Cholesky decomposition of $XXT$:\n$XXT = LLT,$\nwhere L is a lower triangle matrix, meaning all its entries above the diagonal are zero.\nRemark 3.1. If $XXT$ is not positive definite, which is rare but can occur in cases like the fc2 layer of OPT models, we can add $XI$ (\u03bb > 0) to guarantee positive definiteness before Cholesky decomposition. Specifically, for any vector v, $vT(XXT + >I)v = ||X^Tv||^2 + 1||v||^2 > 0$.\nBy combining (7) and (8), we have:\n$(W-TS)(XX)(W\u00bf \u2013 TS\u00bf)T$\n$=(W - TS)(LL)(W; \u2013 TS\u00bf)T$\n$=||WL - TSL||2$.\nLeverage the structure of L, we minimize the upper bound of (11) using a back-substitution approach to efficiently derive a sub-optimal solution to (3). Specifically, there is\n$||WL - TSL ||^2$\n$\\leq \\sum_{j=0}^{n-1}((WL)_{i,j} - (TSL)_{i,j}))^2$\n$=\\sum_{j=0}^{n-1} (\\sum_{u=j}^{n-1} (W_{i,u} - T(S_i):,u) L_{u,j})^2,$\nFollowing (14), we can solve for S\u00bf from the last column (j = n - 1) to the first column (j = 0), minimizing each of the n squared terms respectively. The (n \u2212 1)-th column of L has only one nonzero entry in rows u \u2265 n \u2212 1, namely"}, {"title": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models", "content": "$L_{n-1,n-1}$. Therefore, for j = n \u2212 1, the residual involves a single term:\n$(W_{i,n-1} - T(S_i):,n-1) L_{n-1,n-1}.$\nMinimizing with respect to $(S_i):,n\u22121$ gives that it should select an element from T that satisfies\n$\\text{idx} = \\argmin_{S} |W_{i,n-1} - T_s|$.\nThen, we set $(S_i)idx,n\u22121 = 1$ and all other elements in this column to 0.\nOnce $(S_i):,n-1$ is determined, the process moves to the (n-2)-th column. The residual becomes\n$(W_{i,n-2} - T(S_i):,n-2) L_{n-2,n-2}$ \n$+ (W_{i,n-1} - T(S_i):,n-1) L_{n-1,n-2},$\nwhere (18) is a constant value given $(S_i):n-1$. In the following steps, we refer to $W_{i,n-1} \u2013 T_k (S_i):,n-1$ as rn-1. Then, we solve for $(S_i):,n\u22122$ by minimizing the square of (17)-(18):\n$\\text{idx} = \\argmin_{S} (W_{i,n-2}+ \\frac{r_{n-1}L_{n-1,n-2}}{L_{n-2,n-2}}  - T_s)$.\nand we set $(S_i)idx,n\u22122 = 1$ and the rest of $(S_i):,n\u22122 = 0.\nThis back-substitution process continues for j = n \u2212 3,...,0. At each step the element of $(S_i)idx,j set to 1 is determined as\n$\\text{idx} = \\argmin_{S} W_{i,j}+ \\frac{1}{L_{j,j}}\\sum_{u=j+1}^{n-1} r_uL_{u,j} - T_s,$\nwhere ru = Wi,u \u2013 T(Si):u. Figure 2 illustrates the back-substitution framework for efficiently determining S\u2081. Since the solution processes for Si, i = 0,1,..., m - 1 are independent, similar to the batch solving of Ti-subproblems described earlier, we can stack all Wi and Ti vectors row-wise and organize the Si"}, {"title": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models", "content": "Algorithm 1 GANQ: GPU-Adaptive Layer-Wise LUT-Based Non-Uniform Quantization\nInput: W\u2208Rmxn, X\u2208Rn\u00d7p, initial codebook T\u00ba \u2208RmX2N number of iterations K\nOutput: Updated TK and query matrix QK\u2208{0, 2N \u22121}mxn\nInitialize S = 0m\u00d72Nxn # tensor format\nCompute H = XX\nCompute L = Cholesky(H)\nfor k0 to K - 1 do\nInitialize r = 0m\u00d71 # Cholesky decomposition\nfor j n-1 to 0 do\nidx = argmin, W:j+ r Lj.j\nQ+1 = idx # previous residual vector\nUpdate S using idx # row-wise # one-hot encoding\nr = (W:,j: \u2013 TkSk+1)Lj:,j # update residual\nend for\nTk+1=WH(Sk+1)T((Sk+1)HT(Sk+1)T)+ #batch update\nend for\nReturn TK, QK\nmatrices into a tensor. This allows the back-substitution process to be performed for the entire problem using matrix operations, leveraging modern GPUs' parallel processing capabilities to enhance overall efficiency.\nFinally, the full pseudocode of GANQ for layer-wise LUT-based non-uniform quantization is presented in Algorithm 1.\n3.3. Compatibility with Outlier-Handling Techniques\nGANQ provides a foundational framework for LUT-based non-uniform quantization and is inherently compatible with existing techniques for handling outliers in weight matrices. Among these techniques, a widely adopted approach involves splitting the weight matrix into a sparse matrix for outliers and a quantized matrix for the remaining weights. For example, SpQR (Dettmers et al., 2023) and SqueezeLLM (Kim et al., 2023) extract outliers into a separate sparse matrix to mitigate their impact on the quantization process.\nIn our framework, the weight matrix W can similarly be decomposed into a sparse component Wsparse, containing extracted outliers, and a dense component Wdense, processed through GANQ. The method for extracting outliers is detailed in Appendix A. This decomposition reduces quantization range, thereby enhancing the quantization performance."}, {"title": "4. Experiments", "content": "4.1. Settings\nQuantization. We evaluate GANQ on weight-only non-uniform quantization. The default configuration employs INT4/3 per-channel weight quantization."}, {"title": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models", "content": "Models. We comprehensively evaluate GANQ on a range of models, including OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023a), LLaMA-2 (Touvron et al., 2023b), and LLaMA-3 (Meta AI, 2024) families. Specifically, we assess its performance across OPT-125M, OPT-350M, OPT-1.3B, OPT-2.7B, OPT-6.7B, LLaMA-7B, LLaMA-2-7B, and LLaMA-3-8B models.\nEvaluation. Following prior work (Frantar et al., 2022; Shao et al., 2024; Ma et al., 2024; Kim et al., 2023), we evaluate the quantized models by reporting perplexity on language generation tasks, specifically using the WikiText-2 (Merity et al., 2016), C4 (Raffel et al., 2020), and PTB (Marcus et al., 1994) datasets. Additionally, we assess accuracy on zero-shot tasks, including ARC Easy, ARC Challenge (Clark et al., 2018), WinoGrande (Sakaguchi et al., 2021), BoolQ (Clark et al., 2019), RTE (Wang et al., 2018), and HellaSwag (Zellers et al., 2019), facilitated by the LM Harness library (Gao et al., 2021).\nBaselines. For basic weight-only quantization, we compare GANQ with standard round-to-nearest uniform quantization (RTN), GPTQ (Frantar et al., 2022), and OminiQuant (Shao et al., 2024). For weight-only quantization with outlier handling, we compare with GPTQ, OminiQuant, and AWQ (Lin et al., 2024), each using a group size of 128, as well as SqueezeLLM (Kim et al., 2023).\nSetup. We implement GANQ using PyTorch (Paszke et al., 2019) and utilize the HuggingFace Transformers library (Wolf, 2019) for model and dataset management. All experiments are conducted on a single NVIDIA RTX 4090 GPU. For calibration data, we follow the methodology outlined in previous works (Frantar et al., 2022; Shao et al., 2024; Kim et al., 2023). Specifically, we use 32 sequences for OPT models and 128 sequences for LLaMA models. Each sequence consists of 2048 tokens, sampled from the first shard of the C4 dataset.\nLatency Profiling. Following prior works and associated quantized model inference kernels (GPTQ-for-LLaMa, 2023; Kim et al., 2023), we evaluate latency and peak memory usage while generating 1024 tokens on a single NVIDIA RTX 4090 GPU using the Torch CUDA profiler.\n4.2. Main Results\nWeight-only Quantization. The results in Table 2 present the WikiText2 perplexity of various quantized models under 4-bit and 3-bit configurations across different model sizes (with additional perplexity results on the C4 and PTB datasets in Appendix B). As shown, GANQ consistently outperforms baseline methods such as RTN, GPTQ, and OminiQuant across all configurations. For 4-bit quantization, GANQ achieves the lowest perplexity across both OPT and LLaMA models, with notable improvements. Remark."}, {"title": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models", "content": "ably, on OPT-2.7B, GANQ's perplexity (12.33) even outperforms the full-precision FP16 model (12.47). GANQ also demonstrates strong performance with 3-bit quantization, maintaining competitive perplexity reductions across model sizes. For example, on OPT-6.7B, GANQ's perplexity is 11.39, compared to 15.11 for GPTQ and 13.47 for OminiQuant. These results underscore GANQ's effectiveness in both 4-bit and 3-bit quantization, achieving substantial perplexity reductions across various model scales. The \"-\" in Table 2 indicates that OmniQuant"}, {"title": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models", "content": "Weight-only Quantization with Outlier Handling. To mitigate outlier impact, methods like RTN, GPTQ, AWQ, and OminiQuant divide per-channel distributions into smaller blocks (typically of size 128). SqueezeLLM retains a small"}, {"title": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models", "content": "percentage of outliers (e.g., 0.5%) and a fixed number of full rows (default: 10). GANQ can integrate seamlessly with SqueezeLLM's outlier handling mechanism. We evaluate this integration through experiments. Due to memory constraints, OmniQuant cannot quantize LLaMA-3-8B, and SqueezeLLM is limited to models up to 2.7B. We use results directly from their paper for these cases if available. Additionally, SqueezeLLM's current code does not support OPT-350M. For GANQ, we retain 0.5% outliers for all OPT models and LLaMA-3-8B. Additionally, we retain 10 full rows for LLaMA-7B and LLaMA-2-7B to ensure a fair comparison with SqueezeLLM\nAs shown in Table 4, GANQ* (indicating GANQ integrated with outlier handling) outperforms other baselines. Furthermore, when retaining only 0.5% of outliers for LLaMA-7B and LLaMA-2-7B, we observe the following results: LLaMA-7B (5.78 for 4-bit, 6.20 for 3-bit), LLaMA-2-7B (5.60 for 4-bit, 6.10 for 3-bit), which still outperform all other methods, except for SqueezeLLM.\n4.3. Profiling\nWe present the CUDA time, speedup, and peak GPU memory usage of GANQ in Table 5, measured on a single NVIDIA RTX 4090 GPU across different configurations when generating 1024 tokens. GANQ achieves up to a 2.57x speedup compared to the FP16 baseline, with peak"}, {"title": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models", "content": "memory usage reduced to 4.10 GB for OPT-6.7B and 3.30 GB for LLaMA-7B at 3-bit quantization. Furthermore, the lack of native support for mpGEMM in uniform quantization methods (e.g., GPTQ) significantly slows down the overall inference process. In contrast, GANQ incurs only a slight increase in memory usage compared to uniform quantization, underscoring that the overhead of LUT-based dequantization is minimal, especially when weighed against the significant improvements in perplexity and latency. Note that the GPTQ-for-LLaMA (GPTQ-for-LLaMa"}]}