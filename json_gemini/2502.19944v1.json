{"title": "Algebraic Machine Learning: Learning as computing an algebraic decomposition of a task", "authors": ["Fernando Martin-Maroto", "Nabil Abderrahaman", "David M\u00e9ndez", "Gonzalo G. de Polavieja"], "abstract": "Statistics and Optimization are foundational to modern Machine Learning. Here, we propose an alternative foundation based on Abstract Algebra, with mathematics that facilitates the analysis of learning. In this approach, the goal of the task and the data are encoded as axioms of an algebra, and a model is obtained where only these axioms and their logical consequences hold. Although this is not a generalizing model, we show that selecting specific subsets of its breakdown into algebraic \"atoms\" obtained via subdirect decomposition gives a model that generalizes. We validate this new learning principle on standard datasets such as MNIST, FashionMNIST, CIFAR- 10, and medical images, achieving performance comparable to optimized multilayer perceptrons. Beyond data-driven tasks, the new learning principle extends to formal problems, such as finding Hamiltonian cycles from their specifications and without relying on search. This algebraic foundation offers a fresh perspective on machine in- telligence, featuring direct learning from training data without the need for validation dataset, scaling through model additivity, and asymptotic convergence to the underly- ing rule in the data.", "sections": [{"title": "Introduction", "content": "Algebraic methods are widely used in Machine Learning [1-3]; however, the learning mech- anism is based primarily on Statistics and Optimization [4]. We propose Algebraic Machine Learning (AML) as an approach that uses Abstract Algebra as the foundation for learning itself, rather than in a supporting role. An advantage of taking an algebraic approach lies in its mathematical transparency and conceptual simplicity, offering new opportunities to analyze and understand learning.\nAML differs from other Machine Learning methods. One difference is that its mathe- matics are closer to those of symbolic systems, yet it can learn from high-dimensional data like a connectionist system. This makes AML depart from AI's historical divide between symbolic methods [5\u20138] and learning methods [9-11]. It is also different from approaches in neurosymbolic AI that either combine a symbolic and a learning system to work together [12, 13] or the role of learning and the symbolic part are both done using gradients [14, 15].\nAnother property of AML is that it generalizes directly from training data. Unlike statistical learning [4], no validation data is needed to determine hyperparameters or to stop training before overfitting. AML can also learn to solve formal problems, such as finding a Hamiltonian cycle or resolving Sudokus from the problem specification, without using training data or search. AML was introduced in a preliminary arxiv report [16], followed by three reports with an analysis of its mathematics [17-19]."}, {"title": "Results", "content": "Figure 1 provides a schematic representation of our approach. We start with axiomatization, where the problem, defined by data, goals, and prior knowledge, is encoded as a set of algebraic axioms. Then we use a procedure we call Full Crossing to obtain a model of the axioms. The specific model we obtain has two characteristics. First, it is the freest model, meaning the model in which only the axioms and their logical consequences are true. Second, it is expressed as a subdirect decomposition {$1, $2, ...}, a decomposition known in Abstract Algebra [20] that we propose to find the fundamental building blocks, or atoms, of a problem. Of these atoms, specific subsets are each a generalizing model. In practice, we use Sparse Crossing, a version of Full Crossing that directly obtains the generalization subsets from the axioms. In this paper, we describe each of these steps, demonstrate how they produce generalization properties, and present results for standard datasets."}, {"title": "Encoding a task as axioms of an algebraic structure", "content": "An algebraic structure is a set S with one or more operations that satisfy some axioms [21- 23]. Specifically, we use a semilattice algebra, which has a single binary operation that is commutative, associative and idempotent (i.e. $a\u2299 a = a$) [21]. The semilattice provides a simple yet expressive enough framework that can effectively represent a broad range of tasks.\nThe set S contains certain special elements that we call constants. These constants, C, are the primitives that we use to describe the specific task and data. For instance, in an image classification problem, a constant might represent a pixel in a particular color, while in a board game, a constant might represent a specific position or piece.\nIn addition to these constants, S includes all possible terms, which are sets of constants formed using the operation \u2299. For example, given the constants {$C_1, C_2, ..., C_n$}, a possible term is $T = C_2 \u2299 C_8\u2299 C_9$, where the component constants of the term T are {$C_2, C_8, C_9$}.\nTo encode a machine learning task in the algebra, we introduce additional axioms. Each of these axioms asserts a relationship between two terms in the following way: a term, say TR, has a property characterized by another term, say T\u2081, when $T_R\u2299 T_L = T_R$. This expression is saying that T\u2081 is already contained in or implied by TR. To make this clear, we express $T_R\u2299 T_L = T_R$ with the more compact notation\n$T_L \u2264 T_R$ (1)\nWe refer to this expression as a duple because it can be represented as an ordered pair of terms, r = ($T_L$,$T_R$). A task is thus expressed as a set of positive duples T\u2081 < T; and negative duples $T_i \\nless T_j$.\nAs an example, consider the task of expressing that some binary sequences of length 4 share some property. We can start by assigning a constant p to the property. For the sequence we could use 2 constants for each position, one for digit 1 and another for digit 0, giving a total of 8 constants. For example, the constant $c_{31}$ could represent that the third position in the sequence is 1. To express that the sequence 0100 belongs to class p, we write the duple $T_L \u2264 T_R$, where $T_L = p$ and $T_R = c_{10} \u2299 c_{21} \u2299 c_{30}\u2299 c_{40}$. The task could then be encoded as a set of such duples, one for each sequence in the class.\nThe example illustrates a simple case of task encoding using a semilattice. This encoding technique is known as semantic embedding. It was introduced by mathematical logicians as encodings of algebraic structures within other algebraic structures, such as describing a group within a graph. For example, semantic embeddings have been extensively used in the study of undecidability [21]. We have studied different types of semilattice embeddings with examples in [18]."}, {"title": "Atomized models of the task", "content": "Once the task is expressed as a set of duples, each of the form $T_i < T_j$ or $T_i \\nless T_j$, the next step is to build a model. A model is a specific semilattice structure in which these duples hold true.\nInstead of building a semilattice, we compute an atomized semilattice model [17]. An atomized semilattice has an idempotent operation \u2299 and a binary, reflexive, and transitive order relation <. In semilattices, the idempotent operation \u2299 defines an order relation < while in atomized semilattices it is the other way around: the order relation < defines the idempotent operator \u2299 (see Supplementary Section 1, Theorem 2).\nAn atomized semilattice has two sorts of elements: the regular elements (the terms) and the atoms, which gives two disjoint sets, S and A. We use Latin letters for regular elements, and Greek letters for atoms. Every atomized semilattice is a semilattice with respect to the regular elements, the set S, and a partial order with respect to all the elements, $S \\cup A$. The idempotent operation \u2299 acts only on elements of S while the order relation < acts on both, regular elements and atoms.\nAn atomized semilattice satisfies an extended set of axioms that go beyond the commu- tative, associative and idempotent properties of a semilattice. The extended set of axioms describe the relationship between regular elements, atoms and constants (Supplementary Section 1, Definition 7 and [17]). Here we mention some of the axioms and some of their consequences more directly related to how we build a model. One axiom is that for each atom \u03c6 there is at least one constant c in its upper segment, that is, $\u03c6 < c$. Also, each regular element T has at least one atom \u03c6 in its lower segment, that is, $\u03c6 < T$. However, no regular element is in the lower segment of an atom.\nOne consequence of the axioms is that a duple, say $T_L < T_R$, is satisfied in the model if the atoms in the lower segment of T\u2081 are a subset of the atoms in the lower segment of TR (Supplementary Section 1, Theorem 1 (vi)):\n$T_L \u2264 T_R\u21d4 {\u03c6|\u03c6 < T_L} \u2286 {| < T_R}.$ (2)\nTo make a practical use of Equation 2, we still need to know how to compute the lower segment of a term. For this we use that another consequence of the axioms is that the lower segment of a term $T = C_1 \u2299 C_2 \u2299 ... \u2299 C_n$ is the union of the lower segments of its component constants (Supplementary Section 1, Theorem 1(v)):\n${\u03c6|\u03c6 < T} = {\u03c6| < C_1} \u222a {\u03c6| < C_2} \u222a ... \u222a {\u03c6| < C_n}.$ (3)\nTo check if a duple $T_L < T_R$ holds in an atomized semilattice model, we must then verify that the atoms present in the model satisfy Equation 2, for which we need the atoms in the lower segments of T\u2081 and TR that can be obtained using Equation 3.\nAtomized semilattices have the following properties:\n\u2022 An atom \u03c6 is fully characterized by the constants in its upper segment, i.e. those that satisfy \u03c6 < c (Supplementary Section 1, Theorem 1 (iv)). This suggests a natural notation for atoms, e.g. $[c_3, c_4]$ representing an atom with c3 and c4 in its upper segment and no other constants.\n\u2022 An atomized semilattice model can be constructed from its atoms alone, so a model can be fully described as a set of atoms, each atom equal to a set of constants. A model M can then be represented as:\n$M = {[c_1, c_2, c_3], \u03c6[c_2, c_5], \u03c6[c_1, c_6], \u03c6[c_3], \u03c6[c_3, c_4], \u03c6[c_2, c_3, c_5]}.$ (4)\n\u2022 Since atoms are sets of constants, they have a universal meaning not associated to a particular atomized semilattice model. For example, according to Equation 2, an atom \u03c6 in a model M that satisfies $\u03c6 < T_L$ and $\u03c6 \\nless T_2$ causes $T_1 \\nless T_2$ in the model M. Then, any model that has \u03c6 present will also satisfy $T_1 \\nless T_2$ (Supplementary Section 1, Theorem 3).\n\u2022 If the set of constants in the upper segment of an atom, for example $[c_2, c_3, c_5]$ above, can be written as the union of the constants in the upper segments of other different atoms of a model, e.g. \u03c6[3] and [c2, cs], then the atom \u03c6[c2, c3, c5] is called \u201credundant\". Redundant atoms can be eliminated from the model M without altering which duples the model obeys (Supplementary Section 1, Theorem 5). Eliminating the redundant atoms in the model in Equation 4, we have\n$M = {[c_1, c_2, c_3], \u03c6[c_2, c_5], \u03c6[c_1, c_6], \u03c6[c_3], \u03c6[c_3, c_4]}.$ (5)\n\u2022 Non-redundant atoms of a model act as generators of the set of all atoms of a model (Supplementary Section 1, Theorem 17).\n\u2022 If the terms in the axioms are all concatenations of constants from the set C, any semilattice model of the axioms can be found as an atomized semilattice over C (Supplementary Section 1, Theorem 14).\n\u2022 Each atom, redundant or non-redundant, of an atomized semilattice maps to a subdi- rectly irreducible component [21] of the semilattice it atomizes. An atomized model is thus identifying the irreducible algebraic components of the task's model [17, 19].\""}, {"title": "Freest atomized model", "content": "The freest model of the task is the one for which the axioms of the task and its logical consequences are the only true statements. The logical consequences of the axioms are the positive and negative duples that are true in every model of the axioms. Any other model of the axioms satisfies a greater number of positive duples than the freest model and we say that it is less free than the freest model.\nFull Crossing is a procedure to compute, step by step, the freest model of a set of axioms (Supplementary Section 1, Theorem 11). It works in the following way. Let X be the set of positive task duples already satisfied by a model M. We want to make positive a task duple that, according to M, is negative, $T_L \\nless T_R$. Full Crossing operates over the model M and produces the freest model of the task duples $X \\cup {(T_L \u2264 T_R)}$. For the task duple $T_L \u2264 T_R$ to be true, the atoms in the lower segment of T\u2081 must also be in the lower segment Of TR (Equation 2). Let R denote the set of atoms in the lower segment of TR and n = |R|. Let the discriminant D be the set of atoms that are in the lower segment of T\u2081 but not in the lower segment TR. Full Crossing replaces each atom in the discriminant, \u03c6 \u2208 D, by n atoms, each given by a set of constants that is the union of the constants in the upper segment of \u03c6 and the constants in the upper segment of one atom in R.\nTo illustrate the Full Crossing procedure, consider the model M given in Equation 5 and suppose that we want to enforce the duple $T_L \u2264 T_R$ in M, where $T_L = C_1\u2299 C_2$ and $T_R = C_3\u2299C_4$. In this case, $R = {[c_1, c_2, c_3], \u03c6[c_3], \u03c6[c_3, c_4]}$ and the discriminant is $D = {$[c_2, c_5], \u03c6[c_1, c_6]}$."}, {"title": "Freest atomized model of the task's axioms", "content": "To build our intuition about the freest model of a task, consider the task of characterizing with a property p the following set of 3,375 black and white 4 \u00d7 4 images. The first column of each image is black, while the other three columns have pixels that are either black or white but without an entire black column. Figure 2a displays 16 of the images that meet this criterion.\nFor this problem, we can use 32 constants for the 16 pixels in black or in white, and one constant for the property p, a total of 33 constants. Our initial model is the freest semilattice atomized by {$[c_1], [c_2], ..., \u03c6[c_{32}], \u03c6[c_p]$. Starting from this model and using Full Crossing, we can enforce, one by one, 3,375 duples, each of the form $p < T_i$, with $T_i$ a term of 16 component constants representing the image. As the Full Crossing procedure progresses, the number of non-redundant atoms initially increases to approximately 6,000 and then decreases to 51 (Figure 2b, top). When atoms are grouped by size (number of constants in its upper segment), we see that the number of non-redundant atoms with 1, 2, 3, and 5 constants quickly stabilizes to 32, 4, 12, and 3 atoms, respectively (Figure 2b, middle). Larger atoms appear early on, increase in number, and then get removed from the model with more full-crossings (Figure 2b, middle and bottom).\nLet us look at the final model, Figure 2c. It has a total of 51 atoms. 32 of these atoms are each in one of the 32 constants representing a pixel in a color. These 32 atoms were already in the initial model so they existed before any task duple was full-crossed. There are also 4 atoms in two constants: constant p and one of the four constants representing a black pixel in the first column of the image. These atoms capture that all images contain a black vertical bar in the first column. There are also 12 atoms, one for each position in the last three columns, with 3 constants: constant p and the black and white constants of the same pixel. These atoms capture the fact that each pixel in the last three columns can be either black or white. There are 3 atoms in 5 constants: constant p and the 4 white constants of one of the three last columns of the image. These atoms capture that each of the last three columns is never completely black.\nIt is also instructive to look at an intermediate model early on the crossing sequence, say after 200full-crossings. This model already contains all the atoms of the final model, Figure 2c. It also has larger atoms (some examples in Figure 2d), which will all eventually be removed in later crossings."}, {"title": "Generalizing models", "content": "In the previous section, we considered the task of assigning a property p to the set of images with the hidden rule that every image had the first column entirely black and the other columns with at least one white pixel, Figure 2. The final freest model revealed this rule explicitly in its non-redundant atoms, Figure 2c.\nThis result is general, as we can see in the following. Let P be the set of duples that define the hidden rules of the task. Let Q be the set of all duples that are the logical consequence of P (the duples that are valid in all possible models of the task duples), with Q excluding P. We can prove that the non-redundant atoms of the freest model of Q are the same as the non-redundant atoms of the freest model of P (Supplementary Section 2, Theorem 22). The theorem then says that if we provide enough task duples, i.e. a large enough subset of Q, the freest model of the task duples becomes equivalent to the model of the rule duples P.\nAlthough this is true in the limit where all the consequences are known, non-redundant atoms of the final model, or an approximation to them, must be created much earlier. In our example of the black bar, the final model required 3,375 crossings, but its non-redundant atoms, Figure 2c, are already present before 200 crossings. Extracting those non-redundant atoms at 200 crossings would give us a perfect generalizing model. In this section, we argue why this generalization, the early convergence to the rules of the task in some subset of the atoms, is a general phenomenon. We start studying it algebraically and then by using the expectation of the probability of false positive and false negative in a test dataset."}, {"title": null, "content": "First, we need to understand how atoms evolve as the positive task duples r1, r2, ..., rn are enforced, where usually n is much smaller that the number of consequences of the underlying rule in the data, n << |Q|, with |Q| usually a very large number. Starting with the freest semilattice model as initial model, No, which does not yet satisfy the first task duple r1, the Full Crossing procedure can be applied to enforce r\u2081, producing the model N\u2081. This process is applied to each duple, creating a chain of models N1, N2, ..., Nn. For each atom in the final model Nn, there is an inward chain of atoms \u03bb0, \u03bb1,..., \u03bb\u03b7, with di \u2208 Ni for i \u2208 {0, ..., n}, and \u03bb\u03b7 = \u03c6 (Supplementary Section 1, Theorem 21). If the atom Ai\u22121 is not in the discriminant of ri then Xi = Xi\u22121 while if it is, \u5165\u00bf has more constants in its upper segment than \u5165\u00bf\u22121 and we say the atom \"grows\u201d or becomes \u201cwider\u201d. Figure 3 depicts the evolution of atoms from model No to model N3 formed after three crossing operations."}, {"title": null, "content": "There are some quantities that help us characterize how atoms change during training. Given an inward chain for an atom in the final model, q \u2208 Nn, let g(\u03c6) be the number of times in which we find \u5165\u2081 \u2260 Xi\u22121, i.e. the number of times the atoms in its chain grow. Let k(\u03c6) \u2208 {0, ..., n}, be the index k of the first model in the sequence N1, N2, . . ., Nn such that the atom \u03c6 is in model Nk, and let the \u201csuccess\u201d of atom \u03c6 be the number of consecutive crossings in which \u03c6 has remained unchanged, from its creation until the end of the crossing sequence, h($) = n \u2013 k(\u03c6)."}, {"title": null, "content": "Using these quantities, we can express how each atom matures during training. Since the set of constants in the upper segment of an atom cannot be larger than the total number of constants, C, there is a finite number of times an atom can grow. As a result, after the n crossing operations, even when n << |Q|, an atom o present in the model may have grown to is final size and matured. A mature atom causes 0 false negatives, but if the atom is not yet mature, at least we know that & has grown g($) times and it has been consistent with the training duples h($) times since the last growth. These two quantities are what we need to compute the Probability of a False Negative (PFN) in the test set, that is, the probability that the atom o causes a test duple that should be positive to be negative in the model Nn. The expected PFN, making the standard assumption that training and test distributions are the same, is (Supplementary Section 3.2):\n$PFN(\u03c6) = min (\\frac{1}{h(\u03c6)+1}, \\frac{g(\u03c6)+1}{j + 2})$.(7)\nAt the beginning of the training, $\\frac{g(\u03c6)+1}{n+1}$ dominates due to the low success h($). After training with more positive training examples, $\\frac{1}{h(\u03c6)+1}$ becomes dominant as the atoms mature, producing lower (or even zero) probability of false negative. As an example, for the MNIST dataset of hand-written digits [24], the number of training examples is n = 50,000, and most atoms have ten constants in its upper segment (Figure 4c), so they grow ten times during training, g($) \u2248 10. Ten growth events in 50,000 examples imply that an average atom is successful h($) = 50,000/10 = 5,000 times, giving a low individual PFN of 0.0002.\nSo far, we have characterized how a single atom matures during training, and now we are interested in subsets of atoms. Suppose that we extract a subset of Z atoms of the freest model Nn. Each atom di of this subset, with i = 1, 2, .., Z, has undergone g($i) stages of growth along its inward chain, and since it was created, it has been successful (i.e. consistent with the positive task duples) h($i) times. The Probability of a False Negative (PFN) in the test set is the probability that one or more of the Z atoms causes a test duple that must be positive to be negative in the model Nn. After n positive training examples, the expected test PFN can be approximated as (Supplementary Section 3.2):\n$PFN(\u03c6_1,..., \u03a6_2) \u2248 \u03a3_{i=1}^2 \\frac{1}{h(\u03c6_i)+1}$.(8)\nFrom this expression, it follows that the test PFN is reduced by lowering the number of atoms in Z and by using atoms with a high success h(\u0444).\nThe test Probability of False Positive (PFP) is the probability that a test duple that must be negative is assigned positive in Nn. To have a false positive, every atom in the subset should fail to discriminate the duple, so the larger Z is the less likely is to have a false positive. If we assume the probability of causing a false positive of individual atoms independent of each other, the collective PFP is given by the product of the individual PFPs of each of the Z atoms:"}, {"title": null, "content": "$PFP(\u03a6_1,..., \u03a6_z) = \\prod_{i=1}^Z PFP(\u03a6_i)$.(9)\nSince the negative duples of the training dataset play no role in the calculation of the freest model (every training duple r1, r2, ..., rn is positive), the PFP of individual atoms can be obtained empirically using the negative examples of the training dataset as long as the training and test distributions are the same. The more effective an atom is at discriminating duples of the training set, the lower its probability of false positive.\nIn the formula above, we assumed that the individual PFP(\u03c6\u2081) are independent of each other. If there are correlations, the lower the correlations between these individual proba- bilities are, the smaller the expected PFP($1,...,z) of the subset. Therefore, to obtain a good generalizing model, the atoms should be selected to be discriminative and with low mutual correlation.\nEquations 8 and 9 provide a way to extract a generalizing model from the freest atom- ized model. To minimize the test PFN, the number of atoms selected should be as few as possible and highly successful during training (with high h(\u03c6\u2081) values, which depend upon the positive duples of the training set). To minimize the test PFP, the atoms in the subset should be selected to be effective at discriminating negative duples (with low PFP(i)), have low mutual correlation, and a sufficient number to render every negative duple in the training set negative.\nIf we apply this method to the example of Figure 2, we can isolate some of the atoms of the rule given in Figure 2c before 200 crossings. For this purpose, we can use a training set of negative duples corresponding to counterexample images that do not adhere to the hidden rule. The method then extracts the 4 atoms that are in the lower segment of p and in the lower segment of another constant, as well as the 3 atoms that are in the lower segment of p and in the lower segments of 4 white pixel constants. The method does not obtain the atoms in the lower segment of p and in the black and white constants of the same pixel location. These atoms encode that every positive example contains either the black or the white pixel constant at each location of the last three columns of the image. Since the counterexamples used are also images, these atoms are not discriminative and are therefore not obtained using this method. In general, the method finds atoms that correspond to the rules satisfied by the positive examples but not by the negative examples of the training set. In this case, the subset of atoms extracted is a generalization model with zero error."}, {"title": "Practical computation of generalizing subsets with Sparse Crossing", "content": "The freest model of a set of task duples is usually too large to calculate in practice. Since we are interested in its generalizing subsets, we devised a method to directly obtain, from the axioms, generalizing subsets of the freest model through a sparse version of the Full Crossing procedure.\nThe Sparse Crossing algorithm operates as follows: Every subset of atoms of the freest model satisfies all the positive task tuples. Regarding negative task tuples, the presence of a single atom in a model is sufficient for the model to satisfy a negative duple; indeed, the condition for a duple to be positive in a model is given by Equation 2. Consequently, there always exist subsets of atoms from the freest model that satisfy all positive and negative duples with cardinality less than or equal to the number of negative task duples. To identify a small subset of atoms that satisfies all the negative duples, we enforce the positive duples sequentially in a series of crossing steps. Instead of retaining all atoms in the full-crossing table, we selectively choose the atoms needed to discriminate the negative duples and dis- card the rest, as illustrated in Table 2. However, simply selecting atoms that satisfy the negative duples at a given crossing step does not work, as these atoms may not generate a discriminating subset after subsequent crossing steps. To address this issue, atoms are selected based on an invariance condition: the preservation of a quantity we call the trace. This condition allows us to discard atoms while ensuring that every negative task duple will be satisfied after the crossing of all positive task tuples (see Supplementary Section 4).\nWith Sparse Crossing, positive and negative task tuples are processed in batches selected among the task tuples with replacement. The initial model of a batch is the output model of the previous batch. Additionally, Sparse Crossing allows the atoms produced in all previous batches, not just the immediately preceding one, to influence the process of discarding atoms by means of the pinning terms (Supplementary Section 1, Definition 17). The pinning terms provide an effect similar to augmenting the set of negative axioms and accelerate the discovery of atoms of the freest model that are building blocks of other atoms (i.e., atoms whose set of constants in their upper segment is a subset of that of various other atoms (see Supplementary Section 4.8 and Theorem 37). Since the non-redundant atoms are the building blocks of all the atoms, the presence of pinning terms increases the likelihood of discovering non-redundant atoms. Moreover, because every duple discriminated by an atom is also discriminated by at least one non-redundant atom, the non-redundant atoms of the model are often among the most effective at satisfying the negative tuples, which further increases their likelihood of discovery."}, {"title": "Learning from data", "content": "Black and white images can be classified using the same embedding strategy we applied to the toy example in Figure 2. At each pixel location, one constant represents the pixel in black and another represents it in white. Each image is then encoded in a term resulting from the idempotent summation of its pixel constants. The handwritten digit recognition dataset (MNIST) [24] is ideal for testing this embedding as there is variability in how digits are written, the training set contains some mislabeled images [25], and the images were originally black and white. In this case, we have a total of 2 \u00d7 28 \u00d7 28 constants for the pixels and constants digiti, with i = 0,1,..., 9, for the 10 classes. The grayscale values in these images resulted from centering the digits, so we binarized them back by thresholding pixel values. We applied Sparse-Crossing to the 50,000 MNIST training examples, each encoded as a task duple digiti < imagek. Additionally, we have a set of 450,000 negative task duples, each of the form $digit_j \\neq i \\nless image_k$.\nA test image is classified as digit i when the atoms in the lower segment of constant digiti are a subset of the atoms in the lower segment of the term representing the test image, as in Equation 2. After training, about 70% of the test images have a digit assigned in this way. This is because the training set is not large enough to obtain a model that gives assignations for every example of the test set. However, we can give \u201cbest guess\u201d assignations for each test example. One simple method is to classify a test image as belonging to the class that more closely obeys the subset condition Equation 2. We use the word \"misses\" to refer to the atoms in the lower segment of the left-hand side of a duple, in this case digiti, that are not in the lower segment of the right-hand side, in this case the term that represents the test image. A test image can then be classified as the digit with the fewest misses."}, {"title": null, "content": "As an alternative to the \u201cfewest misses\u201d method, we also used logistic regression as a very simple way to include statistical information. The input to the logistic regression is the output of AML, given in the following way. The atoms that are in the lower segment of the image term are given a value of +1 and the atoms that are not are given a value of -1. For each image, the input to the logistic regression is then a sequence of +1 and -1 values. Each element of the sequence connects with a linear weight to each of 10 softmax outputs. This method then decides which class corresponds to an input using a single linear hyperplane per class. We trained the linear weights using only the training dataset, Adam optimizer [26] and cross-entropy loss [10], and obtained a test accuracy of 98.43% for the 50,000 training examples and 91.56% for 1,000 training examples (see column \u201cAML log. reg.", "MLP best\" in Table 3). The best MLP trained only with the first 1,000 examples of the training set reached 88.70% test accuracy.\"\n    },\n    {\n      \"title\"": null}, {"content": "We also evaluated models obtained with Sparse Crossing in several medical datasets (MEDMNIST", "27": "as well as in fashionMNIST [28", "29": ".", "constants": "n$l_{i", "channels": "n$term(image) = \u2299_{i,j,k} (l_{i,j,k(intensity(i, j, k, image))} \u2299 g_{i,j,k(intensity(i, j, k, image))}).$ (12)\nFor images with three color channels, each pixel is encoded as the idempotent summation of six constantans, three in ascending chains and three in descending chains. This embedding uses"}]}