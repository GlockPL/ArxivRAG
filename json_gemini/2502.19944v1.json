{"title": "Algebraic Machine Learning: Learning as computing an\nalgebraic decomposition of a task", "authors": ["Fernando Martin-Maroto", "Nabil Abderrahaman", "David M\u00e9ndez", "Gonzalo G. de Polavieja"], "abstract": "Statistics and Optimization are foundational to modern Machine Learning. Here,\nwe propose an alternative foundation based on Abstract Algebra, with mathematics\nthat facilitates the analysis of learning. In this approach, the goal of the task and\nthe data are encoded as axioms of an algebra, and a model is obtained where only\nthese axioms and their logical consequences hold. Although this is not a generalizing\nmodel, we show that selecting specific subsets of its breakdown into algebraic \"atoms\"\nobtained via subdirect decomposition gives a model that generalizes. We validate this\nnew learning principle on standard datasets such as MNIST, FashionMNIST, CIFAR-\n10, and medical images, achieving performance comparable to optimized multilayer\nperceptrons. Beyond data-driven tasks, the new learning principle extends to formal\nproblems, such as finding Hamiltonian cycles from their specifications and without\nrelying on search. This algebraic foundation offers a fresh perspective on machine in-\ntelligence, featuring direct learning from training data without the need for validation\ndataset, scaling through model additivity, and asymptotic convergence to the underly-\ning rule in the data.", "sections": [{"title": "Introduction", "content": "Algebraic methods are widely used in Machine Learning [1-3]; however, the learning mech-\nanism is based primarily on Statistics and Optimization [4]. We propose Algebraic Machine\nLearning (AML) as an approach that uses Abstract Algebra as the foundation for learning\nitself, rather than in a supporting role. An advantage of taking an algebraic approach lies\nin its mathematical transparency and conceptual simplicity, offering new opportunities to\nanalyze and understand learning.\nAML differs from other Machine Learning methods. One difference is that its mathe-\nmatics are closer to those of symbolic systems, yet it can learn from high-dimensional data\nlike a connectionist system. This makes AML depart from AI's historical divide between\nsymbolic methods [5\u20138] and learning methods [9-11]. It is also different from approaches in\nneurosymbolic AI that either combine a symbolic and a learning system to work together\n[12, 13] or the role of learning and the symbolic part are both done using gradients [14, 15].\nAnother property of AML is that it generalizes directly from training data. Unlike\nstatistical learning [4], no validation data is needed to determine hyperparameters or to stop\ntraining before overfitting. AML can also learn to solve formal problems, such as finding\na Hamiltonian cycle or resolving Sudokus from the problem specification, without using\ntraining data or search. AML was introduced in a preliminary arxiv report [16], followed by\nthree reports with an analysis of its mathematics [17-19]."}, {"title": "Results", "content": "Figure 1 provides a schematic representation of our approach. We start with axiomatization,\nwhere the problem, defined by data, goals, and prior knowledge, is encoded as a set of\nalgebraic axioms. Then we use a procedure we call Full Crossing to obtain a model of the\naxioms. The specific model we obtain has two characteristics. First, it is the freest model,\nmeaning the model in which only the axioms and their logical consequences are true. Second,\nit is expressed as a subdirect decomposition {$1, $2, ...}, a decomposition known in Abstract\nAlgebra [20] that we propose to find the fundamental building blocks, or atoms, of a problem.\nOf these atoms, specific subsets are each a generalizing model. In practice, we use Sparse\nCrossing, a version of Full Crossing that directly obtains the generalization subsets from\nthe axioms. In this paper, we describe each of these steps, demonstrate how they produce\ngeneralization properties, and present results for standard datasets."}, {"title": "Encoding a task as axioms of an algebraic structure", "content": "An algebraic structure is a set S with one or more operations that satisfy some axioms [21-\n23]. Specifically, we use a semilattice algebra, which has a single binary operation that\n2"}, {"title": null, "content": "is commutative, associative and idempotent (i.e. $a\u2299 a = a$) [21]. The semilattice provides\na simple yet expressive enough framework that can effectively represent a broad range of\ntasks.\nThe set S contains certain special elements that we call constants. These constants, C,\nare the primitives that we use to describe the specific task and data. For instance, in an\nimage classification problem, a constant might represent a pixel in a particular color, while\nin a board game, a constant might represent a specific position or piece.\nIn addition to these constants, S includes all possible terms, which are sets of constants\nformed using the operation \u2299. For example, given the constants {$C_1, C_2, ..., C_n$}, a possible\nterm is $T = C_2 \u2299 C_8 \u2299 C_9$, where the component constants of the term T are {$C_2, C_8, C_9$}.\nTo encode a machine learning task in the algebra, we introduce additional axioms. Each\nof these axioms asserts a relationship between two terms in the following way: a term, say TR,\nhas a property characterized by another term, say T\u2081, when $T_R \u2299 T_L = T_R$. This expression\n3"}, {"title": null, "content": "is saying that T\u2081 is already contained in or implied by TR. To make this clear, we express\n$T_R \u2299 T_L = T_R$ with the more compact notation\n$T_L \u2264 T_R$\n(1)\nWe refer to this expression as a duple because it can be represented as an ordered pair of\nterms, r = ($T_L, T_R$). A task is thus expressed as a set of positive duples T\u2081 < T; and negative\nduples $T_i \\nAs an example, consider the task of expressing that some binary sequences of length\n4 share some property. We can start by assigning a constant p to the property. For the\nsequence we could use 2 constants for each position, one for digit 1 and another for digit 0,\ngiving a total of 8 constants. For example, the constant $c_{31}$ could represent that the third\nposition in the sequence is 1. To express that the sequence 0100 belongs to class p, we write\nthe duple $T_L \u2264 T_R$, where $T_L = p$ and $T_R = c_{10} \u2299 c_{21} \u2299 c_{30} \u2299 c_{40}$. The task could then be\nencoded as a set of such duples, one for each sequence in the class.\nThe example illustrates a simple case of task encoding using a semilattice. This encoding\ntechnique is known as semantic embedding. It was introduced by mathematical logicians\nas encodings of algebraic structures within other algebraic structures, such as describing a\ngroup within a graph. For example, semantic embeddings have been extensively used in the\nstudy of undecidability [21]. We have studied different types of semilattice embeddings with\nexamples in [18]."}, {"title": "Atomized models of the task", "content": "Once the task is expressed as a set of duples, each of the form $T_i < T_j$ or $T_i \\nThe second edition, which is the one we use in the second part of the section. To add references, there should always be a space before the first reference. As follows:\n    "}, {"title": "Discussion", "content": "We have introduced Algebraic Machine Learning (AML) as a novel approach to automated\nlearning that uses an algebraic decomposition as the basis for learning and generalization.\nIt works by encoding tasks into axioms of an algebra and constructing atomized models\nof these axioms. Learning results from the cumulative discovery of certain atoms of the\nfreest model. This process occurs gradually, using discovered atoms to find more and better\natoms. Certain subsets of atoms from the freest model serve as generalizing models. We\ndemonstrated the versatility of this method across problems of very different nature, using\nimage classification and obtaining Hamiltonian cycles as examples.\nWe find that AML, without incorporating image-specific inductive biases, can classify\nimages with accuracy comparable to the best multilayer perceptrons identified through grid\nhyperparameter search using a validation dataset. We also demonstrate that the same\nmethod finds Hamiltonian cycles in few attempts compared to state-of-the-art heuristics\nand in graphs known to be some of the hardest for the task.\nAn advantage of AML is that the models grow autonomously, thereby eliminating the\nneed to predefine an architecture. The inherent absence of overfitting, combined with the\nminimal set of hyperparameters (see Methods), renders the use of a validation dataset\nunnecessary. Another potential advantage of AML stems from the additivity of the atomized\nrepresentation, which can be used to construct larger models from the union of the atom\nsets of independently computed models.\nWe demonstrate that if the data can be explained by rules that can be expressed in the\nform of axioms in a semilattice, the algebraic model of the data shares all the discriminative\natoms (those useful for generalization) with the freest model of the rules. Furthermore, based\non simple probabilistic considerations and the fact that atoms cannot grow without limit,\nwe expect to observe atoms of the freest model of the rules emerging from the embedding of\nsmall amounts of data. This ability that AML has to find the underlying rules in the data\nsuggests a potential for model transparency and explainability.\nAML provides a different basis for learning that does not use optimization or search and\ndiffers considerably from all other known methods and, particularly, from Statistical Learning\napproaches. This novel perspective could help enhance our understanding of learning and\nintelligence and potentially offer lessons applicable to improve other methods. For example,\nthe role played by the freest model, understood as the model of what can be proven from the\naxioms, and the conceptualization of learning as a form of weakened deduction, offer unique\ninsights that could be applicable to other methods.\nHybrid methods combining the algebraic approach and statistical learning show signifi-\ncant potential. For image datasets, the most effective approach combines logistic regression\nwith the algebraic model, suggesting that data is separable into between algebraic and sta-\ntistical components. Supporting evidence includes the lack of improvement when using"}, {"title": null, "content": "validation data or replacing logistic regression with a multi-layer network. Furthermore,\noptimal performance occurs when the algebraic model achieves zero training error, possibly\nbecause this prevents the statistical layer from compensating for patterns that should be\nbetter captured algebraically.\nIn this work, we use atomized semilattices due to their simplicity and sufficient expressive\npower. However, we hypothesize that the underlying learning method relies primarily on the\nsubdirect decomposition rather than on the particularities of the semilattice algebra. We\nexpect that AML can be implemented with other algebras."}, {"title": "Code availability", "content": "We have made available an open-source Python/C hybrid implementation of Sparse Crossing:\nhttps://github.com/Algebraic-AI/Open-AML-Engine. The dual-language approach allows\nfor seamless instrumentation, enabling researchers to explore and easily modify the algorithm\nin Python while maintaining the performance advantages of C. A decorator \u201c@tryfast\" in\nevery computationally intensive function provides a way to choose between running the\nfunction in Python or in C, facilitating code instrumentation and modification. The code\ncan also compute the Full Crossing algorithm. The repository includes example embeddings\nfor various tasks, including Hamiltonian cycle finding, Sudoku, and MNIST handwritten\ndigit classification."}, {"title": "Methods", "content": "AML models\nImages. The smallest datasets from MEDMNIST [27] were kept in their original 256-\nlevel grayscale depth. For larger medical images, FashionMNIST, CIFAR-10, to speed up\ncomputations, the grayscale intensity resolution was reduced from the original 256-level\ndepth to 20 equidistantly distributed levels.\nTraining in Sparse Crossing. All the datasets were processed following the same\nprotocol. Batch size starts with 500 images and increases linearly until reaching 2/3 of the\ntraining set in batch 500. Sparse-Crossing gives a model Mi per batch i, which we call master\nmodel, and \"union models\" that take into account previous batches (see Supplementary\nSection 4.1). Training stops when the \u201cunion model\u201d has 0 error in the training set. A\nsingle AML model was obtained for each dataset.\nHyperparameters in Sparse Crossing. Sparse Crossing has 4 hyperparameters.\nThese hyperparameters were set manually and are fixed, i.e., they are not optimized for\neach individual dataset. The manual setting was carried out based on experience gathered\nfrom many synthetic datasets and in MNIST. All other datasets used in this study had no\ninfluence on the manual setting of the hyperparameters.\n1. Simplification threshold y: during the process of sparse-crossing the positive axioms, if\nthe number of atoms of the master model (see Supplementary Section 4.1) grows from a\nsize N to a size larger than yN, a call to a simplification routine triggers. The simplification\nconsists of discarding atoms with the constraint of keeping the traces of all the constants\ninvariant (see Algorithm 8). The simplification parameter has an impact on computation\ntime and it may or may not have an impact on the quality of the models produced. The\nvalue y = 1.5 was used for all the image datasets, while for Sudoku and Hamiltonian cycles\nthe value y = 1.1 was set.\n2. Batch size: The batch size has an impact on computation time and model test accuracy.\nFor image datasets, we used a policy of making the batch size grow linearly as training\nprogresses, see Training in Sparse Crossing. For Sudoku or Hamiltonian Cycles, all the\npositive and negative duples are presented at each batch.\n3. Union model fractioning parameter \u03ba: the atoms of the dual are either associated to\nnegative duples or to pinning terms. Let D be the set of atoms of the dual, DN the set of\natoms associated to pinning terms and DR the set of atoms associated to negative duples, so\n|D| = |DN|+|DR|. The fractioning parameter selects, at random and at each batch, a subset\nof SC DN such that |DR| \u2265 \u03ba(|DR| + |S|). In other words, \u03ba is the minimal proportion of\natoms associated to negative duples that we want in the dual. Since the number of pinning\nterms increases with training, if this fractioning does not take place, the proportion of atoms\nassociated to duples decreases. We found that ensuring a proportion \u03ba of atoms associated"}, {"title": null, "content": "to duples helps increase atom variability, i.e. fractioning helps explore a larger volume of the\natom space. For image datasets we used \u03ba = 0.1 while for Hamiltoinian cycles we observed\nthat larger values, like \u03ba 0.5, gave better results. We found this parameter to have a\nsignificant impact in model performance, particularly for smaller training sets.\n4. Model reduction parameter d: Since the accuracy remains approximately constant for a\nwide range of atomization sizes (see Supplementary Figure 7), it is possible to reduce the\nsize of the union model N. To extract a good generalizing model from the union model, a\nsubset of its atoms with size \u03b4|N| is extracted using the method described in Subset selec-\ntion. Size reduction with parameter \u03b4 = 0.1 was used before the logistic regression and the\nfewest misses evaluations for all image datasets. For Sudoku or Hamiltonian cycle problems,\nno reduction was applied, as each solution is extracted from the master model and not from\nthe union model.\nSubset selection. Out of the Sparse Crossing procedure we obtain a set of atoms, from\nwhich we extract the following subset. Good generalizing models need subsets of atoms that\nare individually discriminative, collectively discriminating the entire training set and with\nlow correlation. To build a subset S with these characteristics, we first randomly sort atoms.\nStarting with S empty and reading the atoms in order, an atom & is added to S only if there\nis a negative duple of the training set discriminated by 6 and by no other atom of S. This\nresults in a subset of atoms that discriminates the entire training set, of cardinality smaller\nthan the number of negative duples of the training set. We add various subsets of atoms\nselected in this manner until reaching a model of a size equal to 1/10 of the initial model\nobtained from Sparse Crossing. The atoms that are not associated to labels (those which\nupper segment contain no label constants) are removed from the model, as they play no role\nin associating labels to term images. This protocol results in good generalizing models ten\ntimes smaller than the initial model.\nLogistic regression on top of AML. If we are interested in adding statistical infor-\nmation to AML, a simple way is to use the AML model as input to logistic regression in the\nfollowing way. The atoms that are in the lower segment of the image term are given a value\nof +1 and the atoms that are not are given a value of -1. For each image, the input to the\nlogistic regression is then a sequence of +1 and -1 values. Each element of the sequence\nconnects with a linear weight to each of N softmax outputs, one per class. Only the training\ndataset was used to find optimal parameters, with Adam optimizer [26] and cross-entropy\nloss [10].\nMulti-layer perceptrons\nTo build MLP models, we use the validation dataset to optimize architecture parameters\nand avoid overfitting by early stopping of training. We evaluated a family of two, three\nand four hidden layer multilayer perceptrons with ReLU activations. More concretely, we"}, {"title": null, "content": "perform a grid search over the number of neurons in the first hidden layer (512, 2048 or\n4096 hidden units) and the second hidden layer (256, 1024 or 2048 hidden units), using Ray\nTune, [33], with the goal of minimizing validation loss. The third layer, when it exists, is\nallowed to have 128, 256 or 512 hidden units, and the fourth layer, when it exists, can have\n128 or 256 hidden units. For the third and fourth layers, a random sample is performed for\nthe sizes. We perform 90 runs using two layers, 180 runs using three, and an additional 90\nruns with four layers, for a total of 360 train runs. Training runs for 240 iterations or until\nthe validation loss does not improve for 10 iterations. In each run, we uniformly sample the\nlearning rate ($5 \u00b7 10^{-4}, 10^{-4}, 5 \u00b7 10^{-5}$ or $10^{-5}$) and the L2-regularization coefficient ($10^{-3}$,\n$5 \u00b7 10^{-4}, 10^{-4}$ or 0). We use the ADAM optimizer to minimize cross-entropy loss."}, {"title": "Semantic Embeddings", "content": "A detailed analysis of the concept of semantic embeddings as an axiomatic extension of the\ntheory of semilattices can be found in [17]."}, {"title": "Embedding for Sudoku", "content": "The embedding for Sudoku is presented in [17], with a comprehensive study of its prop-\nerties and the resulting atomized models. Additionally, within the open-source engine at\nhttps://github.com/Algebraic-AI/Open-AML-Engine, exemplary files \"example02_Sudoku.py\"\nand \"embedding_Sudoku.py\" are also provided."}, {"title": "Embedding for Hamiltonian cycles", "content": "Consider the following sets of constantans:\n\u2022 Vi: A constant for each graph node\n\u2022 Ek: A constant for each edge\n\u2022 P: A constant to refer to the path we want to compute\n\u2022 W: A constant to encode constraints and allow for training\n\u2022 nEk: A constant for the absence of edge k\n\u2022 Zk: Auxiliary constants, as many as graph edges\n\u2022 idi: a path \u201cid\u201d constant associated to node i\n\u2022 gk and hk: Context constants, as many as graph edges"}, {"title": null, "content": "This gives a total of $2v + 5e + 2$ constants, where v is the number of nodes and e is the\nnumber of edges in the graph.\nWe start by embedding the topology of the graph. Let r(k) and s(k) be the index of\nthe two nodes of edge Ek. The edges are undirected so it does not matter which of the two\nnodes is r(k) or s(k). For each (undirected) edge k joining nodes $V_{r(k)}$ and $V_{s(k)}$ we define a\npositive duple:\n$V_{r(k)} V_{s(k)} \u2264 E_k$.\nThe embedding constant $Z_k$ represents either an edge or its absence and is defined with:\n$Z_k E_k \\nThink about $Z_k$ as a kind of weak variable that we wish to be equal to either $E_k$ or to $nE_k$\nbut that can take any value in between. The path P we want to find passes through every\nnode and it is formed with edges, so we add:\n$\\n$\u2299_kV_k = P$.\nFor the constant W, which we will use to learn the \"wrong paths\", we start with the\nfollowing duples; for each edge k it is a wrong path one that simultaneously has the constant\nof the edge and the constant for the absence of the edge:\n$W \u2264 E_k \\nSince we want our path not to be a wrong path we also impose the additional negative axiom:\n$W \\nThen we describe the concept of path with the help of the constants nEk. For each node\ni and for each couple of edges $E_y$ and $E_z$ We use:\n$P \u2299 (\u2299_{x;x \\nwhere the idempotent summation $\u2299_{x;x \\ni, except edges y and z. There are a variable number of these positive duples depending upon\nthe graph, on the order of 2v (\\nWe need some negative duples in the embedding (usually, the fewer the better). It is\nenough with one negative duple for each node i establishing that the presence of node i only\ndepends upon the presence of edges incident to node i and it is independent of everything\nelse:\n$V_i (j; j\u2260i (V_jidi)) (knEk) (t; if{r(t),s(t)} Et)$\n26"}]}