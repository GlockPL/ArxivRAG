{"title": "Post-processing fairness with minimal changes", "authors": ["Federico Di Gennaro", "Thibault Laugel", "Vincent Grari", "Xavier Renard", "Marcin Detyniecki"], "abstract": "In this paper, we introduce a novel post-processing algorithm that is both model-agnostic and does not require the sensitive attribute at test time. In addition, our algorithm is explicitly designed to enforce minimal changes between biased and debiased predictions\u2014a property that, while highly desirable, is rarely prioritized as an explicit objective in fairness literature. Our approach leverages a multiplicative factor applied to the logit value of probability scores produced by a black-box classifier. We demonstrate the efficacy of our method through empirical evaluations, comparing its performance against other four debiasing algorithms on two widely used datasets in fairness research.", "sections": [{"title": "1. Introduction and Context", "content": "The increasing adoption of machine learning models in high-stake applications - e.g health care (De Fauw et al., 2018), criminal justice (Kleinberg et al., 2016) or credit lending (Bruckner, 2018) - has led to a rise in the concern about the potential bias that models reproduce and amplify against historically discriminated groups. Addressing these challenges, the field of algorithmic fairness has seen numerous bias mitigation approaches being proposed, tackling the problem of unfairness through drastically different angles (Romei & Ruggieri, 2013). Among them, post-processing bias mitigation techniques (see Table 1 for an overview) focus on debiasing the predictions of a trained classifier, rather than training a new, fair, model from scratch. Generally relying on label flipping heuristics (Kamiran et al., 2012; Hardt et al., 2016) or optimal transport theory (Jiang et al., 2020), they have been argued to achieve comparable performance (Cruz & Hardt, 2023) while generally requiring less computation and performing fewer modifications to the original predictions than their pre-processing and in-processing counterparts (Jiang et al., 2020; Krco et al., 2023).\nAlthough rarely discussed, expecting the debiasing method to perform a low number of prediction changes is especially interesting in contexts where fairness is enforced while a model is already in production (Krco et al., 2023). In real-world applications, maintaining the integrity and reliability of predictive models is crucial, especially when they have undergone rigorous validation and expert review. For example, in non-life insurance pricing, experts commonly employ Generalized Additive Models (GAMs) with splines or polynomial regression on Generalized Linear Models to ensure that price are justifiable and align with both business objectives and customer expectations (e.g., avoiding price increases that could negatively impact customer satisfaction and brand reputation). Ensuring debiasing methods do not significantly alter validated predictions can therefore be essential. Yet, surprisingly, this property is rarely introduced explicitly in the objectives of existing post-processing algorithms. Instead, it is generally implicitly integrated: e.g., methods relying on label-flipping (Kamiran et al., 2012) aim to target only the \"required\" instances to enhance fairness. Additionally, the vast majority of existing approaches generally suffer from two major drawbacks, as shown in Table 1. First, most approaches require the sensitive attribute at inference for fairness (e.g., Hardt et al. (2016); Lohia et al. (2019); Nguyen et al. (2021)). However, this is unrealistic in many practical settings where the sensitive attribute is unavailable at test time. Second, among the works that avoid this assumption, one (Jiang et al., 2020) is limited to linear settings and Du et al. (2021) is not model agnostic. Concurrently to our work, Tifrea et al. (2024) proposed FRAPPE, recognizing and addressing the same limitations.\nIn this short paper, we propose to address these issues by introducing a new post-processing method that is: (i) model-agnostic, (ii) does not require the sensitive attribute at test time, and (iii) explicitly minimizes the number of prediction changes. To do so, we frame the post-processing task as a new supervised learning problem taking as input the previously trained (biased) model. We address this problem by introducing a new approach, RBMD (Ratio-Based Model Debiasing), which predicts a multiplicative factor to rescale the biased model's predictions such that they better satisfy fairness guarantees. This allows us to leverage techniques"}, {"title": "2. Proposition", "content": "2.1. Post-processing as a supervised learning task\nLet X be an input space consisting of d features and y an output space. Let us consider a traditional algorithmic fairness framework in which we want to predict an output variable Y \u2282 Y given an input variable X \u2282 X while being unbiased from a sensitive variable S. For the sake of simplicity, we decide in this paper to focus on a binary classification problem where y = {0, 1} and for which also the sensitive attribute S takes values on a binary sensitive space S. Finally, we suppose to have access to a black-box model f : X \u2192 [0, 1] trained on a training set Dtrain = {(Si, Xi, Yi)}i=1 that consists of N realizations of the triplet (S, X, Y). The black-box model f is trained for a binary classification task and outputs the probability of belonging to the class Y = 1. We finally consider the prediction coming from the scores outputted by f as a random variable \u0176f = 1{f(x)>0.5}.\nGiven a score f(X) \u2208 [0, 1] or a prediction \u0176f, the goal of a post-processing task is to learn a new predictions \u0176 that is more fair with respect to some fairness notion. In this paper, we focus on Demographic Parity\u00b9, measured using the P-rule criterion:\nP-Rule = min { \\frac{P(\\hat{Y} = 1 | S = 1)}{P(\\hat{Y} = 1 | S = 0)}, \\frac{P(\\hat{Y} = 1 | S = 0)}{P(\\hat{Y} = 1 | S = 1)} }\nMost works address this problem by learning a new mapping \u0176f \u2192 \u0176 at inference time. Although this saves computation time by not requiring learning a new model like in-processing methods, it generally relies on solving an optimization problem for each new batch of predictions (or scores) \u0176f, and usually imposes the availability of the sensitive attribute at test time (cf. Table 1). Furthermore, as the true labels Y are generally unavailable at inference time, no constraint on the accuracy of the new predictions Y can be imposed: the relationship between \u0176 and Y is then only indirectly preserved by the fact that minimal changes are performed; an objective which is also not consistently imposed (cf. Table 1). For these reasons, rather than learning a new mapping for each vector \u0176f at inference time, we propose to train a model g : [0, 1] \u00d7 X \u2192 Y, taking as input both X and its score f(X) \u2208 [0, 1] to automatically perform these changes. In the rest of the paper, we refer to g(X, f(X)) as g(X) and we define the prediction coming from the score g(X) as \u0176g = 1{g(x)>0.5}. Following the motivations described in Section 1, the model g should therefore satisfy the following desiderata: the model should be [1] accurate, [2] fair and [3] perform as few changes as possible to the predictions of f. The resulting optimization problem we address in this paper can thus be written under Demographic Parity and for \u03f5 and \u03b7 strictly positive as:\nmin E[L_Y (\\hat{Y}^g, Y)]\ns.t |E[\\hat{Y}^g | S=1] - E[\\hat{Y}^g | S=0]| < \\epsilon\nand E[1_{\\hat{Y}^g \\neq \\hat{Y}^f}] \\le \\eta\nwith Ly the binary cross-entropy, and \u03f5 and \u03b7 controlling the level of unfairness and changes allowed."}, {"title": "2.2. Ratio-Based Model Debiasing", "content": "We propose to define g as a rescaled version of f to better compare g(X) with f(X). Hence, the idea of our approach Ratio-Based Model Debiasing (RBMD) is to edit the score f(X) into a score g(X), whose prediction \u0176g is more fair, through a multiplicative factor r(X):\ng(X) = \u03c3(r(X)\u00b7flogit(X)),\nwhere \u03c3(\u00b7) is the sigmoid function to ensure g(X) \u2208 [0, 1] and flogit is the logit value of the black-box score f.\nWe then propose to model the ratio rw, as a Neural Network with parameters wg that takes as input X, and gwg the corrected classifier. An ablation study on the impact of the architecture of rw, has been done in Appendix A.1. Using this formulation, satisfying the constraint [3] (similarity between f and g) can easily be done by adding a penalization term Lratio to control the behavior of rug. Although several functions could be considered, we focus in the rest of the paper on ensuring that the score f(X) is not modified unnecessarily, by defining Lratio(rwg(X)) = ||rwg(X) \u2013 1||2. A study on the effectiveness of this term can be found in Appendix A.3. To ensure fairness (constraint [2]), we propose to leverage the technique proposed by Zhang et al. (2018), which relies on a dynamic reconstruction of the sensitive attribute from the predictions using an adversarial model hwn: Y \u2192 S to estimate and mitigate bias. The higher the loss Ls(hwn(rwg(X)\u00b7flogit(X)), S) of this adversary is, the more fair the predictions gwg(X) are. Furthermore, this allows us to mitigate bias in a fairness-blind setting (i.e. without access to the sensitive attribute at test time). Thus, Equation 1 for Demographic Parity 2 becomes:\nmin E [LY (gwg(X), Y)]\ns.t E[Ls(hwp(rwg(X)\u00b7flogit(X)), S)] \u2265 \u03f5'\nand E[Lratio (rwg(X))] \u2264 \u03b7'\nwith \u03b5\u0384, \u03b7' > 0. In practice, we relax the problem 3 and we optimize the following:\narg min max  \\frac{1}{N} \\sum_{i=1}^N LY (g_{w_g}(X_i), Y_i)  + \\lambda_{fair} L_S(h_{w_h}(r_{w_g}(x_i) \\cdot flogit(X_i)), S_i) + \\lambda_{ratio} L_{ratio} (r_{w_g}(X_i)),\nwith \u03bbfair and \u03bbratio two hyperparameters."}, {"title": "3. Experiments", "content": "After describing our experimental setting in Section 3.1, we propose various experiments to showcase the efficacy of our method along several dimensions: in Section 3.2.1, we evaluate our method in terms of accuracy-fairness trade-off; in Section 3.2.2, we evaluate the number of changes performed to the predictions of f for given levels of accuracy and fairness; finally, we conduct additional analyses in Section 3.2.3 on the interpretability of our approach.\n3.1. Experimental Setting\nWe use two commonly used datasets in the fairness literature (Hort et al., 2023): Law School (Wightman, 1998) and COMPAS (Angwin et al., 2016). The sensitive attribute for both datasets is race. Most existing post-processing methods do not focus on the same model-agnostic and fairness-blind setting and are therefore not directly comparable. To prove the efficiency of our method, we therefore consider the following approaches:\nLPP (Xian & Zhao, 2024). As shown in Table 1, LPP (Linear Post-Processing) is a post-processing method that is sensitive unaware at test time and is model agnostic3.\nOracle (Xian & Zhao, 2024). In the same work, the authors of LPP propose a variation of their method in the fairness-aware setting. We refer to it as \"Oracle\", as it uses the sensitive at test time and to the best of our knowledge it is the state-of-the-art between the algorithms that use the sensitive at test time.\nROC (Kamiran et al., 2012). ROC (Reject-Option Classifier) is a post-processing method that adopts a label-flipping heuristic based on model confidence. ROC also requires access to the sensitive at test time.\nAdvDebias (Zhang et al., 2018). As discussed in Sec. 2, we use Adv Debias, an in-processing method, for additional comparison with our method. We use the implementation provided in the AIF360 library (Bellamy et al., 2018).\nAfter splitting each dataset in Dtrain (70%) and Dtest (30%), we use a Logistic Regression (trained on Dtrain) as our black-box classifier. We then train RBMD and the aforementioned competitors on the same Dtrain while varying the values of the hyperparameters of each method to achieve various levels of fairness and accuracy scores, calculated over Dtest.\n3.2. Results\n3.2.1. EXPERIMENT 1: ACCURACY AND FAIRNESS\nFigure 1 shows the accuracy and fairness scores achieved by all competitors on the Law School and COMPAS datasets. We conduct each experiment three times for each parameter value to ensure robust results. From Figure 1, in which one dot corresponds to one model run, we can observe that our method outperforms LPP (that is model agnostic and that"}, {"title": "4. Conclusion", "content": "In this ongoing work, we presented RBMD, an innovative post-processing technique designed to adjust the predictions"}, {"title": "A. Implementation details", "content": "A.1. Architecture of r and Interpretability\nIn this subsection", "Lratio": "most of the classification probabilities g(x) are kept similar to their initial value f(x)", "method": "we thus observe that to achieve fairness, the main changes performed by RBMD are positive changes, i.e. from class 0 to class 1 (top left quadrant)."}]}