{"title": "Post-processing fairness with minimal changes", "authors": ["Federico Di Gennaro", "Thibault Laugel", "Vincent Grari", "Xavier Renard", "Marcin Detyniecki"], "abstract": "In this paper, we introduce a novel post-processing algorithm that is both model-agnostic and does not require the sensitive attribute at test time. In addition, our algorithm is explicitly designed to enforce minimal changes between biased and debiased predictions\u2014a property that, while highly desirable, is rarely prioritized as an explicit objective in fairness literature. Our approach leverages a multiplicative factor applied to the logit value of probability scores produced by a black-box classifier. We demonstrate the efficacy of our method through empirical evaluations, comparing its performance against other four debiasing algorithms on two widely used datasets in fairness research.", "sections": [{"title": "1. Introduction and Context", "content": "The increasing adoption of machine learning models in high-stake applications - e.g health care (De Fauw et al., 2018), criminal justice (Kleinberg et al., 2016) or credit lending (Bruckner, 2018) - has led to a rise in the concern about the potential bias that models reproduce and amplify against historically discriminated groups. Addressing these challenges, the field of algorithmic fairness has seen numerous bias mitigation approaches being proposed, tackling the problem of unfairness through drastically different angles (Romei & Ruggieri, 2013). Among them, post-processing bias mitigation techniques (see Table 1 for an overview) focus on debiasing the predictions of a trained classifier, rather than training a new, fair, model from scratch. Generally relying on label flipping heuristics (Kamiran et al., 2012; Hardt et al., 2016) or optimal transport theory (Jiang et al., 2020), they have been argued to achieve comparable performance (Cruz & Hardt, 2023) while generally requiring less computation and performing fewer modifications to the original predictions than their pre-processing and in-processing counterparts (Jiang et al., 2020; Krco et al., 2023).\nAlthough rarely discussed, expecting the debiasing method to perform a low number of prediction changes is especially interesting in contexts where fairness is enforced while a model is already in production (Krco et al., 2023). In real-world applications, maintaining the integrity and reliability of predictive models is crucial, especially when they have undergone rigorous validation and expert review. For example, in non-life insurance pricing, experts commonly employ Generalized Additive Models (GAMs) with splines or polynomial regression on Generalized Linear Models to ensure that price are justifiable and align with both business objectives and customer expectations (e.g., avoiding price increases that could negatively impact customer satisfaction and brand reputation). Ensuring debiasing methods do not significantly alter validated predictions can therefore be essential. Yet, surprisingly, this property is rarely introduced explicitly in the objectives of existing post-processing algorithms. Instead, it is generally implicitly integrated: e.g., methods relying on label-flipping (Kamiran et al., 2012) aim to target only the \"required\" instances to enhance fairness. Additionally, the vast majority of existing approaches generally suffer from two major drawbacks, as shown in Table 1. First, most approaches require the sensitive attribute at inference for fairness (e.g., Hardt et al. (2016); Lohia et al. (2019); Nguyen et al. (2021)). However, this is unrealistic in many practical settings where the sensitive attribute is unavailable at test time. Second, among the works that avoid this assumption, one (Jiang et al., 2020) is limited to linear settings and Du et al. (2021) is not model agnostic. Concurrently to our work, Tifrea et al. (2024) proposed FRAPPE, recognizing and addressing the same limitations.\nIn this short paper, we propose to address these issues by introducing a new post-processing method that is: (i) model-agnostic, (ii) does not require the sensitive attribute at test time, and (iii) explicitly minimizes the number of prediction changes. To do so, we frame the post-processing task as a new supervised learning problem taking as input the previously trained (biased) model. We address this problem by introducing a new approach, RBMD (Ratio-Based Model Debiasing), which predicts a multiplicative factor to rescale the biased model's predictions such that they better satisfy fairness guarantees. This allows us to leverage techniques"}, {"title": "2. Proposition", "content": null}, {"title": "2.1. Post-processing as a supervised learning task", "content": "Let $\\mathcal{X}$ be an input space consisting of $d$ features and $\\mathcal{Y}$ an output space. Let us consider a traditional algorithmic fairness framework in which we want to predict an output variable $Y \\subset \\mathcal{Y}$ given an input variable $X \\subset \\mathcal{X}$ while being unbiased from a sensitive variable $S$. For the sake of simplicity, we decide in this paper to focus on a binary classification problem where $\\mathcal{y} = \\{0,1\\}$ and for which also the sensitive attribute $S$ takes values on a binary sensitive space $\\mathcal{S}$. Finally, we suppose to have access to a black-box model $f : \\mathcal{X} \\rightarrow [0, 1]$ trained on a training set $\\mathcal{D}_{\\text{train}} = \\{(S_i, X_i, Y_i)\\}_{i=1}^{N}$ that consists of $N$ realizations of the triplet $(S, X, Y)$. The black-box model $f$ is trained for a binary classification task and outputs the probability of belonging to the class $Y = 1$. We finally consider the prediction coming from the scores outputted by $f$ as a random variable $\\hat{Y}_f = \\mathbb{1}_{f(x)>0.5}$.\nGiven a score $f(X) \\in [0, 1]$ or a prediction $\\hat{Y}_f$, the goal of a post-processing task is to learn a new predictions $\\hat{Y}$ that is more fair with respect to some fairness notion. In this paper, we focus on Demographic Parity\\u00b9, measured using the P-rule criterion:"}, {"title": "2.2. Ratio-Based Model Debiasing", "content": "We propose to define $g$ as a rescaled version of $f$ to better compare $g(X)$ with $f(X)$. Hence, the idea of our approach Ratio-Based Model Debiasing (RBMD) is to edit the score $f(X)$ into a score $g(X)$, whose prediction $\\hat{Y}^{g}$ is more fair, through a multiplicative factor $r(X)$:\n$g(X) = \\sigma(r(X) f_{\\text{logit}}(X)),$ (2)\nwhere $\\sigma(\\cdot)$ is the sigmoid function to ensure $g(X) \\in [0, 1]$ and $f_{\\text{logit}}$ is the logit value of the black-box score $f$.\nWe then propose to model the ratio $r_{w_\\theta}$, as a Neural Net-work with parameters $w_\\theta$ that takes as input $X$, and $g_{w_\\theta}$ the corrected classifier. An ablation study on the impact of the architecture of $r_{w_\\theta}$ has been done in Appendix A.1. Using this formulation, satisfying the constraint [3] (similarity between $f$ and $g$) can easily be done by adding a penalization term $L_{\\text{ratio}}$ to control the behavior of $r_{w_\\theta}$. Although several functions could be considered, we focus in the rest of the paper on ensuring that the score $f(X)$ is not modified unnecessarily, by defining $L_{\\text{ratio}}(r_{w_\\theta}(X)) = ||r_{w_\\theta}(X) - 1||^{2}$. A study on the effectiveness of this term can be found in Ap-pendix A.3. To ensure fairness (constraint [2]), we propose to leverage the technique proposed by Zhang et al. (2018), which relies on a dynamic reconstruction of the sensitive attribute from the predictions using an adversarial model $h_{w_\\eta} : \\hat{Y} \\rightarrow S$ to estimate and mitigate bias. The higher the loss $L_S(h_{w_\\eta}(r_{w_\\theta}(X) f_{\\text{logit}}(X)), S)$ of this adversary is, the more fair the predictions $g_{w_\\theta}(X)$ are. Furthermore, this allows us to mitigate bias in a fairness-blind setting (i.e. without access to the sensitive attribute at test time). Thus, Equation 1 for Demographic Parity \\u00b2 becomes:"}, {"title": "3. Experiments", "content": "After describing our experimental setting in Section 3.1, we propose various experiments to showcase the efficacy"}, {"title": "3.1. Experimental Setting", "content": "We use two commonly used datasets in the fairness literature (Hort et al., 2023): Law School (Wightman, 1998) and COMPAS (Angwin et al., 2016). The sensitive attribute for both datasets is race. Most existing post-processing methods do not focus on the same model-agnostic and fairness-blind setting and are therefore not directly comparable. To prove the efficiency of our method, we therefore consider the following approaches:"}, {"title": "3.2. Results", "content": null}, {"title": "3.2.1. EXPERIMENT 1: ACCURACY AND FAIRNESS", "content": "Figure 1 shows the accuracy and fairness scores achieved by all competitors on the Law School and COMPAS datasets. We conduct each experiment three times for each parameter value to ensure robust results. From Figure 1, in which one dot corresponds to one model run, we can observe that our method outperforms LPP (that is model agnostic and that"}, {"title": "3.2.2. EXPERIMENT 2: NUMBER OF CHANGES", "content": "In this experiment, we measure the proportion $P$ of changes between the predictions of the black-box model and the ones of the fairer model on the test set $D_{\\text{test}}$. As the values $P$ can"}, {"title": "3.2.3. EXPERIMENT 3: EXPLAINING PREDICTION CHANGES", "content": "In this experiment, we aim to show how RBMD facilitates understanding which instances were targeted by the debias-ing models. For this purpose, given a model $g$, we train a sur-rogate decision tree model on the dataset $D_{\\text{train}}$ with labels $\\hat{y}^g$. Such a decision tree (e.g. the one in Figure 8 in Appendix), defines segments of instances targeted by a bias mitigation method (here RBMD) in an interpretable manner. In Figure 3 is shown the F1 score achieved by all decision trees trained for debiasing models with comparable perfor-mances, depending on their depth. A high F1 score means that the decision tree accurately describes the instances with prediction changes. We observe that the instances targeted by RBMD can generally be described with simpler decision trees, suggesting that they lie in a lower number of different regions of the feature space. This opens the possibility of better interpreting which are the instances targeted by the debiasing algorithm. Another experiment in this direction is presented in Appendix A.1."}, {"title": "4. Conclusion", "content": "In this ongoing work, we presented RBMD, an innovative post-processing technique designed to adjust the predictions"}, {"title": "A. Implementation details", "content": null}, {"title": "A.1. Architecture of r and Interpretability", "content": "In this subsection, we do an ablation study on the architecture of the ratio network $r_{w_\\theta}$ (\\cdot) on the Law School dataset. For the experiments and the plots shown in the paper, the architecture of the network $r_{w_\\theta}$ (\\cdot) consists of a fully connected neural network with two hidden layers with a ReLU activation functions after each linear. The final layer outputs a single value that is the ratio to be multiplied with $f_{\\text{logit}}(X)$.\nIn order to look at the impact of such an architecture on the results of Figure 1, we train our RBMD method with architectures of $r_{w_\\theta}$ (\\cdot) with different power. Specifically, we choose to increase the hidden layers from two to three and to decrease them to zero. In this last setting, we simply have a linear ratio\n$r_{w_\\theta} (X) = w_0 + w_1x_1 + ... + w_dx_d.  $   (5)\nHence, we would be able to leverage more the interpretability of our debiasing method. Notice that we carried out these experiments on the Law School dataset while keeping fixed the train-test split of the dataset."}, {"title": "A.2. Calibration", "content": "To give further insights on the differences between models after debiasing, we show in Figure 6 the distribution of the classification probabilities returned by the blackbox model (x-axis) and a fairer one (y-axis) over the test set, for RMBD and Adv Debias for reference. We directly observe the impact of the MSE constraint in $L_{\\text{ratio}}$: most of the classification probabilities $g(x)$ are kept similar to their initial value $f(x)$, i.e. along the diagonal. Furthermore, this gives insights into the strategy adopted by the debiasing method: we thus observe that to achieve fairness, the main changes performed by RBMD are positive changes, i.e. from class 0 to class 1 (top left quadrant)."}, {"title": "A.3. Loss and Hyperparameters", "content": "As with every deep learning-based method, it is crucial to play with hyperparameters $\\lambda_{\\text{fair}}$ and $\\lambda_{\\text{ratio}}$ to adapt the weights of the terms in the loss function we use for our Ratio-Based Model Debiasing. For this reason, we evaluate the performance of our method over a grid search of combinations of parameters $(\\lambda_{\\text{fair}}, \\lambda_{\\text{ratio}})$.\nEffectiveness of $L_{\\text{ratio}}$. As a first step, we validate the effectiveness of $\\lambda_{\\text{ratio}} L_{\\text{ratio}} (r_w(x_i))$ in Equation 4. Recall that $L_{\\text{ratio}}$ has been added to decrease the number of changes with respect to the biased model. To do so, we choose the Law School dataset and we fixed a value of $\\lambda_{\\text{fair}}$; we then vary the value of $\\lambda_{\\text{ratio}}$ while observing the distribution of the ratios on the test set $D_{\\text{test}}$ for all these different values of $\\lambda_{\\text{ratio}}$. As we can observe from Figure 7, by increasing the value of $\\lambda_{\\text{ratio}}$ we"}, {"title": "B. Quartiles definition", "content": "In the paper, we referred several times to the quartiles of fairness and accuracy. We used them in order to compare the methods in the region of the Pareto for which they have approximately the same levels of fairness and accuracy. These quartiles have been computed on the AdvDebias method and their values are shown in the tables below."}, {"title": "B.1. Law School Dataset", "content": null}, {"title": "B.2. COMPAS Dataset", "content": null}, {"title": "C. Further results", "content": null}, {"title": "C.1. Law School dataset", "content": "Following, you can find the CART decision tree trained as explained in Section 3.2.3. From this plot, we can observe that\nthe sub-regions of the features space are identified by the variables cluster, fam_inc, and lsat. In particular, we can here observe for example the leaf cluster < 0.9 \u2192 lsat < 0.47 for which we have a prediction equals to one, i.e. a difference between biased and unbiased prediction."}, {"title": "C.2. COMPAS dataset", "content": "Following, you can find the results of Section 3.2.2 for COMPAS dataset. Again, we measure the proportion $P$ of prediction changes on the test sets between the black-box model and the fairer ones for every method introduced in Section 3."}]}