{"title": "Local Topology Measures of Contextual Language Model Latent Spaces\nWith Applications to Dialogue Term Extraction", "authors": ["Benjamin Matthias Ruppik", "Michael Heck", "Carel van Niekerk", "Renato Vukovic", "Hsien-Chin Lin", "Shutong Feng", "Marcus Zibrowius", "Milica Ga\u0161i\u0107"], "abstract": "A common approach for sequence tagging\ntasks based on contextual word representations\nis to train a machine learning classifier directly\non these embedding vectors. This approach\nhas two shortcomings. First, such methods con-\nsider single input sequences in isolation and are\nunable to put an individual embedding vector\nin relation to vectors outside the current local\ncontext of use. Second, the high performance\nof these models relies on fine-tuning the embed-\nding model in conjunction with the classifier,\nwhich may not always be feasible due to the\nsize or inaccessibility of the underlying feature-\ngeneration model.\nIt is thus desirable, given a collection of embed-\nding vectors of a corpus, i.e. a datastore, to find\nfeatures of each vector that describe its relation\nto other, similar vectors in the datastore. With\nthis in mind, we introduce complexity measures\nof the local topology of the latent space of a\ncontextual language model with respect to a\ngiven datastore.\nThe effectiveness of our features is demon-\nstrated through their application to dialogue\nterm extraction. Our work continues a line of\nresearch that explores the manifold hypothesis\nfor word embeddings, demonstrating that lo-\ncal structure in the space carved out by word\nembeddings can be exploited to infer semantic\nproperties.", "sections": [{"title": "Introduction", "content": "The prevailing approach to sequence tagging tasks\nsuch as named entity recognition or dialogue term\nextraction involves a two-step process: start with\na general contextual vector representation for text\nsequences, for instance the embedding vectors cre-\nated by a pretrained language model, then train a\nseparate tagging model on top of the vector repre-\nsentations (Lample et al., 2016; Ramshaw and Mar-\ncus, 1995). Optionally, assuming differentiability\nof the model and target function, one can fine-tune\nthe representation model such that its embeddings\nare more suitable for the tagging task (Panchen-\ndrarajan and Amaresan, 2018). While highly effec-\ntive, the representations may be expensive to com-\npute, and fine-tuning a language model is not al-\nways feasible, for instance if the underlying model\nis hidden behind an application programming inter-\nface (API). Thus, it is desirable to develop tagging\nmethods which achieve the best performance on the\ngiven representations. In fact, the performance of\nprompting-based approaches with large language\nmodels (LLMs) on named entity recognition tasks\nhas lagged behind that of supervised sequence tag-\nging approaches (Wang et al., 2023). Additionally,\nthis leads to problems such as hallucinations and\npotential dataset contamination, which prevent a\nfair evaluation.\nA more fundamental limitation of the prevailing\nparadigm is that the relation of a single input se-\nquence to other sequences in the dataset cannot be\ntaken into account. Both the representation module\nand the tagging module commonly have a limited\nmaximum context length. They cannot process\nthe entire dataset at once, but rather need to be\nprovided with single sentences or paragraphs at a\ntime. The limited context can lead to suboptimal\nperformance (Amalvy et al., 2023). For example,\nconsider named entity recognition for an isolated\nsentence such as Prince was prominently featured\nat the event. The word Prince is ambiguous. In a\ncorpus containing news articles, Prince or Prince\nHarry likely appear in many articles related to the\nBritish royal family. In a different corpus, the term\nCurry Prince might appear frequently in the con-\ntext of restaurant reviews. So only with regard to\nthe entire corpus under consideration, an informed\nchoice on how to tag Prince in the example sen-\ntence could be made.\nIn this work, we show that the relation between\nthe representation of a single token and its contain-\ning corpus can be captured by studying the latent"}, {"title": "Contribution", "content": "Consider the latent space of a language model in\nthe neighborhood of a given contextual embedding\nvector. For instance, the neighborhood of an em-\nbedding of the word cheap in the context I am\nlooking for options for cheap dinner contains other\noccurrences of the word cheap in different contexts,\nbut also different words expressing a similar mean-\ning (inexpensive, good-value) and words connected\nto the center word, such as restaurant. In this work,\nwe show that:\n(a) this neighborhood contains information that is\nnot present in the language model next-token\nprediction distribution, and that cannot be 'dis-\ntilled' into the language model via naive fine-\ntuning,\n(b) this additional information can be used to im-\nprove the performance of sequence tagging\ntasks, and\n(c) this information can be efficiently summa-\nrized using low-dimensional topological fea-\nture descriptors.\nOur topological descriptors are codensity at mul-\ntiple scales (Carlsson et al., 2008; Carlsson, 2014),\ntopological singularity measures based on Wasser-\nstein norms (Cohen-Steiner et al., 2010), and vec-\ntorized persistence modules. Towards (a), we\nshow that several of our one-dimensional numer-\nical measures show minimal correlation with lan-\nguage model perplexity, indicating that they con-\ntain independent information. Towards (b) and"}, {"title": "", "content": "(c), we empirically demonstrate improvements on\nthe natural language processing task of variants\nof term extraction. In each case, we build the la-\ntent space through a masked language model from\na dialogue corpus. As a baseline, we employ a\ntagging model trained directly on the original lan-\nguage model vectors, and compare with models\nthat take as input a combination of the language\nmodel vectors with our topological descriptors of\nthe neighborhood within the latent space of a con-\ntextual language model. Furthermore, we compare\nwith models trained on features from Vukovic et al.\n(2022), which are based on neighborhoods in a\nstatic word embedding space. We show that uti-\nlizing the contextually augmented vectors yields\nstatistically significant improvements.\nObservation (a) is not completely new. For ex-\nample, it is present in the idea of k-nearest neigh-\nbor language models (Khandelwal et al., 2020;\nXu et al., 2023), where the current hidden state\nis augmented by the nearest neighbors from a data-\nstore. Our low-dimensional descriptors, on the\nother hand, have not been deployed before, and\nour experiments for (b) provide the first application\nof contextual topological features to token level\nsequence tagging tasks. Note with reference to\npoint (c) that summarizing a collection of vectors\nin a permutation-invariant way is a challenging\nproblem in representation learning (Zaheer et al.,\n2017), which we tackle in this work via tools from\npersistent homology.\nOur work is complementary to other recent appli-\ncations of topological methods to the study of con-\ntextual embedding spaces. Tulchinskii et al. (2023)\ndemonstrate that the topology of a point cloud de-\nrived from a text paragraph can be utilized in a\nsequence classification task, namely to differenti-\nate human-written from artificially generated para-\ngraphs. Their approach takes into account solely\nthe given paragraph's embedding vectors, and does\nnot explore how these reside within the larger la-\ntent space. Another approach involves constructing\nfiltered graphs from the attention scores in a trans-\nformer model, followed by sequence-level classifi-\ncation based on persistent homology (Kushnareva\net al., 2021; Perez and Reinauer, 2022). However,\nthis approach only applies to supervised sequence\nclassification tasks, and does not yield local fea-\ntures required for tagging. In a more qualitative\ndirection, Valeriani et al. (2023) investigate the in-\ntrinsic dimension of the latent space through the\ndifferent layers of a transformer, and Ethayarajh"}, {"title": "Background and Methods", "content": "We consider the encoder part of a contextual lan-\nguage model, which can be thought of as a map\n\\(e: (R^d)^{\\times N} \\rightarrow (R^h)^{\\times N}\\).\nHere, d is the dimension of the input layer, h is the\nhidden dimension (h \u226a d), and N is the maximum\nsequence length, after which sequences will be trun-\ncated. This maximum length is usually fixed and\nfinite. The input of the encoder is a sequence of vec-\ntors \\(X \\in (R^d)^{\\times N}\\) representing a tokenized context.\nTokenization describes the process in which an in-\nput string is decomposed into a sequence of vectors.\nIn our setting, tokenization can be thought of as a\nlookup layer converting short text segments to vec-\ntors (together with position information). Typically,\nlonger words are split into several token vectors in\nthis process.\nThe output of the encoder is a sequence of so-\ncalled hidden states. Commonly, these hidden\nstates are inputs to the \u201cprediction head\" of the\nlanguage model, which produces a probability dis-\ntribution over the token space for the corresponding\ntoken location.\nWe think of a language corpus C as a collec-\ntion of tokenized portions of text. From the point\nof view of a language model, each instance i of a\nparticular token appears in a specific context X(i).\nThese contexts are filled with padding tokens so\nthat they always have length N, permitting con-\nstruction of the embedding sets e(X(i)).\nDefinition 2.1 Given an encoder e derived from\na pretrained language model, the ambient corpus\ndatastore with respect to a corpus C is the multi-\nset/point cloud of all the embeddings e(X(i)) of\nall instances i of all tokens in C.\nNote that we cannot explore the entire latent\nspace of the language model, but only the subspace\n\"carved out\" by the datastore under consideration,"}, {"title": "", "content": "We write multi-set to allow for repetitions/multiplicities.\nThis is relevant in our setting, because strings might appear\nmultiple times in the corpus.\nas in Definition 2.1. In other words, by selecting\nthe task-dependent ambient corpus for sampling\nthe language model hidden states, we are making\na choice of how we explore the hidden state space.\nThis choice of ambient corpus may have a big im-\npact on the derived features."}, {"title": "Local Topological Measures", "content": "All our topological measures are based on neigh-\nborhoods of a given contextual embedding vector\nv with respect to a collection of contextual em-\nbedding vectors coming from an ambient corpus\ndatastore. Given an integer n \u2265 1, we define the\nneighborhood \\(N_n(v)\\) as the multi-set consisting\nof v and its (n - 1) nearest neighbors. To avoid\nadding another copy of the query center vector v\nwhen building the neighborhood from the datastore,\nwe first check for similarity to existing vectors in\nthe datastore with a Euclidean distance threshold\nof 10-4, and take a possible match as center vec-\ntor if applicable. For a schematic illustration of\nthe neighborhood extraction process and the fea-\nture computation, see Figure 1. We consider the\nfollowing local features:\nPersistence Images For a positive integer d, per-\nsistent homology of degree d associates with\na point cloud a multi-set that encodes \"d-\ndimensional topological features\" of the cloud.\nWe refer to Edelsbrunner and Harer (2010) or\nOtter et al. (2017) for introductions. Various\nvectorizations of this multi-set have been de-\nveloped for subsequent use in machine learn-\ning. Persistence images are introduced in\nAdams et al. (2017) as a refined, higher-\ndimensional vectorization of persistent homol-\nogy. We define \\(PI_d(v) \\in R^{100}\\) as a persistent\nimage vector of the degree d persistent ho-\nmology of \\(N_n (v)\\), scaled by a factor of \\(\\frac{1}{n^{.100}}\\).\nThe parameter n is not included in this no-\ntation, as it will be fixed to 128 throughout\nall experiments. For detailed definitions, see\nAppendix A.1. The factor \\(\\frac{1}{n^{.100}}\\) appearing in\nour definition of the persistence image is not\nimportant at this point. It is included to avoid\ninstabilities in the training of the BIO-tagger\ndiscussed in Section 3, which may otherwise\narise from the vastly different scales of the\ncoordinates of the language model embedding\nvectors and these additional coordinates.\nWasserstein Measure A simple one-dimensional\nvectorization is the Wasserstein norm. We"}, {"title": "", "content": "define \\(W_d(v) \\in R\\) as the Wasserstein norm\nof the degree d persistent homology of \\(N_n(v)\\).\nCodensity We define the n-th codensity\\(\text{coden}_n(v) \\in R\\) as the radius of \\(N_{n+1}(v)\\).\nHigher codensity corresponds to regions\nwhere the vectors are farther spread apart.\nThere are several reasons why we fix the car-\ndinality n of the neighborhoods rather than, say,\ntheir radius. Firstly, fixing the cardinality takes\ninto account sample density of the ambient cor-\npus from the latent space of the language model.\nIf we took a fixed radius, sparse regions of the\nambient corpus space would be underrepresented.\nSecondly, some of the topological features we con-\nsider are more readily comparable when computed\non fixed cardinalities. Indeed, a reasonable com-\nparison of Wasserstein norms of neighborhoods\nof different cardinalities seems difficult, and our\nmultiscale definition of (co)density could also not\neasily be emulated for neighborhoods of fixed ra-\ndius. Finally, computational complexity limits the\nfeasibility of approaches that allow for unlimited\ncardinalities of neighborhoods. For instance, in\nVon Rohrscheidt and Rieck (2023), where neigh-\nborhoods of fixed radii are employed, an additional\nsampling step is necessary. More on this is dis-\ncussed in Appendix A.1."}, {"title": "Application of Local Topology\nMeasures to Token Level Tagging Task", "content": "We perform a correlation analysis of local features\nand conduct a case study to explore the efficacy\nof local topology measures. Specifically, we apply\nour proposed topological feature augmentations to\nthe task of dialogue term extraction."}, {"title": "Set-Up", "content": "Data For the term extraction case study, we re-\nsort to the MultiWOZ2.1 (Budzianowski et al.,"}, {"title": "", "content": "Eric et al., 2020) and schema-guided dia-\nlogue (SGD) (Rastogi et al., 2020) task-oriented di-\nalogue datasets. Here, the ambient reference corpus\nC is built solely from the language model hidden\nstates for the training corpus of MultiWOZ2.1, so\nthat the local measurements are comparable across\nboth datasets.\nBIO-Tagging For the sequence tagging tasks, we\nemploy a beginning (B), inside (I), outside (O) la-\nbeling schema, as in Qiu et al. (2022). To keep\nthe comparison between our different models fair\nand to obtain statements about the quality of the\nunderlying features, we choose the architectures so\nthat the trainable BIO-tagging components have a\nsimilar number of adjustable parameters. In this\nway, we can safely attribute any increase in perfor-\nmance to our topological augmentation of the input\nfeatures rather than a stronger tagging component.\nIn all cases, the BIO-tagging transformer fol-\nlows the ROBERTa architecture (Liu et al., 2019)\nand uses 8 attention heads, 2 hidden layers, and\n512 maximum position embeddings. The language\nmodel vectors and augmenting feature vectors are\nfed into the BIO-tagging component through sepa-\nrate two-layer fully connected encoding networks\nwith subsequent individual layer normalization,\nwhose purpose is down- or up-scaling the feature di-\nmension (768 for the language model vectors, 100\nfor persistence images) to the hidden size (512) of\nthe tagging transformer. For a schematic of our\nBIO-tagging setup, see Figure 1.\nFeatures For creating the language model em-\nbeddings, we use the second to last hidden states\n(at layer 11) of the pretrained RoBERTa base model\n(Liu et al., 2019), which returns 768-dimensional\nvectors, with L2-normalization. Note that on unit\nvectors, the cosine distance is proportional to the\nsquare of the Euclidean distances, thus for the rela-\ntive order in which the nearest neighbors occur, it\ndoes not matter whether we search with respect to\nthe cosine or the Euclidean distance.\nWe decide on the second-to-last hidden state of\nthe language model, as opposed to another inter-\nmediate layer, for two reasons: Cai et al. (2021)\nshow that the local intrinsic dimension tends to in-\ncrease with the depths in the transformer, thus the\nresulting neighborhoods should be more expressive.\nMoreover, Peters et al. (2018) and Tenney et al.\n(2019) find that deeper layers in language models\ntend to capture more of the semantic properties,\nwhile earlier layers tend to capture the syntax. For"}, {"title": "", "content": "feature based learning, Devlin et al. (2019) report\nthat among single-layer features, the second-to-last\nlayer leads to the highest performance. Note that\nour setup is not specific to the RoBERTa model or\ntokenizer. Our contextualized topological features\ncan be computed for any (masked or causal) embed-\nding model, extraction layer, datastore produced by\nthe model, and query dataset.\nAs a baseline in our term extraction experiments,\nwe use the language model hidden state vectors\ndescribed in Section 3 as input for the BIO-tagging\nmodel. We test these against augmentation of the\nhidden states with our local persistence image de-\nscriptors introduced in Section 2.2."}, {"title": "Correlation Analysis of Local Features", "content": "We begin by collecting statistical observations on\nthe correlation between the different local topolog-\nical feature types, as well as their correlation with\npseudo-perplexity, on the example of the Multi-\nWOZ2.1 and SGD datasets. The perplexity of a\ncausal language model is a model intrinsic mea-\nsurement of the surprise of seeing the next token,\ndefined as the exponentiation of the cross-entropy\nbetween the model prediction and the corpus data.\nWhile causal perplexity is not available here, in\nour the masked language model setting, we apply\na pseudo-likelihood score by masking tokens one\nby one, and computing the prediction loss of the\nmasked out token following Salazar et al. (2020).\nIn addition to the non fine-tuned version of the\nlanguage model, here we also include the perplexity\nof the fine-tuned version for comparison, which\nuses the MultiWOZ2.1 training portion (fine-tuning\nfor 5 epochs, 0.15 masking proportion, selecting\nthe best model on MultiWOZ2.1 validation loss).\nAll local features are based a non fine-tuned version\nof the language model.\nNote that we cannot directly compute correla-\ntions between the vector-valued persistence images\nand the perplexity measures. For this reason, we\nare relying on selected codensities \\(\text{coden}_n(v)\\) with\nvalues n \u2208 [1;127; 511] and Wasserstein norms as\nnumerical proxy estimates of the neighborhoods'\ntopological complexity. Our term extraction mod-\nels will take only the persistence images as input, as\nthey provide the most powerful and comprehensive\nrepresentation of the local topology. The neural\nnetwork feature extraction model can learn directly\nfrom the persistence images to estimate complex-\nity measures approximating the Wasserstein norms,\nwhich avoids manual feature engineering."}, {"title": "Case Study: Term Extraction in\nTask-Oriented Dialogue Data", "content": "We approach dialogue term extraction as a transfer\nlearning problem. Here, MultiWOZ2.1 serves as\nour source dataset used for training a term extrac-\ntor. The trained model is then applied to the SGD\ndataset, necessitating sufficient transfer learning ca-\npabilities to properly handle the distributional shift\nin the data. We label all phrases in all utterances\nthat match an entry in the respective dataset's on-\ntology, i.e., that match a value in a non-categorical\nslot of the current turn's dialogue state or a value\nin the current turn's dialogue act. The dataset on-\ntology entities are normalized and matched to the\noccurrences in the respective utterances by apply-\ning the TripPy-R label map (Heck et al., 2022) and\nthe SGD canonical value mapping. The ontology\ncomprises names of entities, their domains, prop-\nerties (slots), and values of these slots. We refer to\nthese labelled phrases as dialogue terms.\nThese tagged spans for the dialogue datasets are\nencoded for the BIO-tagger, resulting in the three"}, {"title": "", "content": "labels: 0 (outside), B-TERM, I-TERM (begin and\ninside a term). Since our BIO-tagging model oper-\nates on the token-level of the underlying language\nmodel, we re-align the tags with the tokenization\nusing the IOB2 schema: for a word with B-tag, the\nfirst subtoken is tagged with B, its remaining subto-\nkens with I. For a word with I-tag, all its subtokens\nare tagged with I; and analogously for the O-tag.\nWhile we employ token-level cross-entropy loss\nas the differentiable target function in the model\ntraining, the objective of term extraction within\nthe context of this work is to retrieve each unique\ntarget dialogue term at least once. That is, we\ndo not require the tagger to find all occurrences:\nWe normalize the predicted phrases and ground\ntruth by lower-casing, and deduplicate the resulting\ncollections. A term is considered a true positive if\nit is identical to exactly one ground truth term. If a\nterm cannot be assigned to any ground truth term,\ne.g., comprises several ground truth terms or is an\nincomplete substring of a ground truth term, it is\nconsidered a false positive. The left-over ground\ntruth terms without a matched prediction are the\nfalse negatives. We call the resulting prediction,\nrecall and F1-scores the phrasal results.\nWe train the BIO-tagger for 10 epochs, using the\nAdamW optimizer (Loshchilov and Hutter, 2019)\nwith learning rate 5\u00b710-5, linear warm-up for 10%\nof training steps and batch size of 48. The model\npredictions on the held-out MultiWOZ2.1 valida-\ntion set are evaluated every 100 batches for the\nfirst 3 000 global steps, and the model checkpoint\nwith the best phrasal results on the validation set is\nselected as the final model.\nOur goal is to show that injecting our local\ntopological features into the model yields statis-\ntically significant improvements over the original\nlanguage model embeddings. We run statistical\ntests over multiple different random seeds for ini-\ntialization and check for significant changes in eval-\nuation scores on the transfer set, which we take as\nthe full collection of 463 284 utterances from the\nSGD dataset comprising 20 domains.\nTraining Data In the full data setting, we train on\nall 113 556 utterances of the MultiWOZ2.1 training\nsplit, the results are included in Table 1.\nTo demonstrate that the contextual topological\nfeatures are useful in settings with reduced data\nand might help in mitigating overfitting, we create\na variation of the transfer task by restricting train-\ning to subsets of the MultiWOZ2.1 dataset. This is"}, {"title": "", "content": "ting a given domain in the MultiWOZ2.1 training\nset. Here, on average, the augmentation with the\ncontextual persistence images is again better than\nthe language model vector baseline.\nQualitative Analysis To obtain explicit exam-\nples, we select a model checkpoint for each feature\ntype after 1 100 global steps, and inspect the differ-\nences between predicted normalized phrase sets. In\nTable 3 we see examples where the topologically\naugmented model succeeds in finding complete\nmulti-word terms, whereas the baseline model cuts\noff before the end of a term or misses proper names\nthat should follow a preposition. Such informa-\ntion is highly dependent on the context of a term,\nand the contextual topological model is able to find\nlong terms more consistently. All models identify\nthe restaurant name \"Cafe Jolie\" correctly, but only\nthe topological models recognize the actress \"An-\ngelina Jolie\". Similarly, the song title \"Be alright\"\ncontaining the frequent word \u201calright\u201d is not rec-\nognized by the language model alone, but can be\ndetected by a topological model."}, {"title": "Relation to the Manifold Hypothesis", "content": "At first glance, our results may appear to be at\nodds with the manifold hypothesis, a common as-\nsumption underlying many representation learning\nparadigms. While this hypothesis has been ques-\ntioned for static word embeddings (Jakubowski\net al., 2020), it remains uncontested for contextual\nembeddings. It posits that the latent space of a\ntrained machine learning model is clustered around\na disjoint union of lower-dimensional manifolds\n(Bengio et al., 2013; Brown et al., 2023). This im-\nplies that, from a purely topological perspective,\nthe local structure of the latent space is constant,"}, {"title": "", "content": "at least along connected components \u2013 every point\nshould have a neighborhood topologically identi-\ncal to an open ball in some Euclidean space. How\nthen is it possible that we can extract meaningful\ninformation from variations of the local topology?\nThere are at least two answers to this. First, all\nour measures depend on the way data is sampled.\nThere is no reason to assume that the embeddings\ndrawn from a given corpus provide a uniform sam-\nple of the latent space. On the contrary, the dis-\ntribution of these samples will depend heavily on\nthe corpus. And within a given corpus, we might\nexpect the neighborhoods of latent vectors of con-\ntent words to be \u201cmore spread-out\" and of higher\ndimension than those of non-content words, since\nthere are more plausible possibilities for filling in\ncontent words in a text than for non-content words.\nSecond, our measures are based on persistent ho-\nmology, which is known to detect not only topolog-\nical properties but also differentiable structure such\nas curvature (Bubenik et al., 2020). Thus, even on\na uniformly sampled manifold, these measures are\nexpected to vary."}, {"title": "Computational Complexity", "content": "In this section, we address the computational over-\nhead coming from our proposed method of aug-\nmenting a sequence tagger with contextual topo-\nlogical information of a given corpus. The one-off\ncomputational costs for the datastore, in our study\nderived from the MultiWOZ2.1 training dataset,\nand the query datasets (MultiWOZ2.1 training &\nvalidation dataset, and SGD dataset) involve a sin-\ngle embedding model forward pass for each input\nsequence.\nFor each query dataset relative to the datas-\ntore, assuming a constant and small neighborhood"}, {"title": "", "content": "size n, the asymptotic complexity of the neigh-\nborhood search depends on the tokenized cardi-\nnality of the query dataset |Q|, the tokenized car-\ndinality of the datastore |C|, and the embedding\ndimension d. The runtime complexity using the\nexact search implementation from (Johnson et al.,\n2021) is O(|Q||C|d), and the storage complexity\nfor neighborhood indices is O(|Q|n).\nThe persistent homology computation in dimen-\nsion 0 for each query vector depends on the neigh-\nborhood size n as well. For degree 0, the number\nof simplices in the Vietoris-Rips complex can be\nupper-bounded by n\u00b2. Thus, the persistence di-\ngram for each neighborhood can be computed\nin O(n2w), where w < 2.4 is the matrix mul-\ntipliation exponent (Milosavljevi\u0107 et al., 2011).\nThere are at most n generators in the 0-dimensional\npersistence diagrams, so the computation of the\nWasserstein norms can be achieved in O(n\u00b3) (La-\ncombe et al., 2018). Empirically, the computation\nof the persistence images is observed to be very\nquick compared to the computation of the persis-\ntence diagrams.\nOnce computed and cached, these topological\nfeatures can be reused for different training objec-\ntives on the given query dataset. The only overhead\nin transitioning from the baseline tagger (approxi-\nmately 35.65 million parameters) to the tagger with\ninput LM roberta-base\u2295contextual PI\u00ba involves\na few additional parameters (roughly 60000) for\nthe encoding module of the 100-dimensional con-\ntextual persistence image. Consequently, once the\ntopological features have been cached, the training\nand inference of the topologically augmented BIO-\ntagger are only negligibly slower than the baseline\nBIO-tagger. Appendix A.1 discusses the software\npackages used in the implementation and how we\nhandle caching of the precomputed neighborhoods\nand resulting contextual topological features."}, {"title": "Conclusion", "content": "In this work, we introduce a topological deep learn-\ning approach to enrich feature learning-based se-\nquence tagging methods with contextual topolog-\nical data. Our methods do not depend on access\nto the underlying feature creation method, nor on\nexternal knowledge bases. Once these local topo-\nlogical descriptors are computed, they offer the po-\ntential for reuse across different tasks, thereby mit-\nigating the initial computational investment. One\nlimitation lies in our method still requiring labels"}, {"title": "", "content": "on the seed dataset. Though our results in the case\nstudy hint at a correlation between dialogue terms\nand higher Wasserstein norms, we have yet to es-\ntablish a clear-cut purely feature based criterion for\ndistinguishing terms from non-terms in dialogue\ndata.\nLooking ahead, we conjecture that the utility of\nour approach extends beyond the term extraction\ntask investigated in this study. Given its generic\ndesign and challenge, it is plausible that it is ap-\nplicable to other language models and modalities.\nAlthough our empirical evaluations have been con-\nfined to masked language models, the difficulty of\nthe term extraction task provides optimism that our\nmethod could be advantageous in other scenarios\nwhere understanding the relation between individ-\\ual data points and a datastore is critical.\nThe MultiWOZ2.1\nand SGD datasets are publicly available through the\nConvLab-3 unified data format (Zhu et al., 2023),\nand we release our preprocessing, local topological\nfeature computation and tagging model training\ncode.2"}, {"title": "Limitations", "content": "The experiments have been confined to a small\nmasked language model (RoBERTa base model).\nOur proposed method can be applied to embedding\nspaces derived from causal LLMs (BehnamGhader\net al., 2024), but current state-of-the-art models\ntypically produce latent spaces with significantly\nlarger embedding dimension. This has great influ-\nence on the computational complexity required to\ngenerate the contextual topological features. While\nour BIO-tagger can be trained on a single V100\nGPU with 16 GB of VRAM in 2 hours, one should\nnote that for efficiently precomputing the nearest\nneighbors in the contextual topological feature ex-\ntraction, the embedded datastore needs to fit into\nthe graphic card memory. This one-off neighbor-\nhood computation is not an issue for the Multi-\nWOZ2.1 training set datastore, but might limit ap-\nplications to larger corpus sizes. One possible rem-\nedy could be given by applying embedding space\ndimension reduction techniques such as (Kusupati\net al., 2024) to the datastore before computing our\ntopological features."}, {"title": "Appendix", "content": "In our term extraction applications, the ambient\nvector datastore comprises a collection of vectors\nwith cardinality in the millions, making the com-\nputation of neighborhoods a major computational"}]}