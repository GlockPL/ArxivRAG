{"title": "Local Topology Measures of Contextual Language Model Latent Spaces\nWith Applications to Dialogue Term Extraction", "authors": ["Benjamin Matthias Ruppik", "Michael Heck", "Carel van Niekerk", "Renato Vukovic", "Hsien-Chin Lin", "Shutong Feng", "Marcus Zibrowius", "Milica Ga\u0161i\u0107"], "abstract": "A common approach for sequence tagging\ntasks based on contextual word representations\nis to train a machine learning classifier directly\non these embedding vectors. This approach\nhas two shortcomings. First, such methods con-\nsider single input sequences in isolation and are\nunable to put an individual embedding vector\nin relation to vectors outside the current local\ncontext of use. Second, the high performance\nof these models relies on fine-tuning the embed-\nding model in conjunction with the classifier,\nwhich may not always be feasible due to the\nsize or inaccessibility of the underlying feature-\ngeneration model.\nIt is thus desirable, given a collection of embed-\nding vectors of a corpus, i.e. a datastore, to find\nfeatures of each vector that describe its relation\nto other, similar vectors in the datastore. With\nthis in mind, we introduce complexity measures\nof the local topology of the latent space of a\ncontextual language model with respect to a\ngiven datastore.\nThe effectiveness of our features is demon-\nstrated through their application to dialogue\nterm extraction. Our work continues a line of\nresearch that explores the manifold hypothesis\nfor word embeddings, demonstrating that lo-\ncal structure in the space carved out by word\nembeddings can be exploited to infer semantic\nproperties.", "sections": [{"title": "1 Introduction", "content": "The prevailing approach to sequence tagging tasks\nsuch as named entity recognition or dialogue term\nextraction involves a two-step process: start with\na general contextual vector representation for text\nsequences, for instance the embedding vectors cre-\nated by a pretrained language model, then train a\nseparate tagging model on top of the vector repre-\nsentations (Lample et al., 2016; Ramshaw and Mar-\ncus, 1995). Optionally, assuming differentiability\nof the model and target function, one can fine-tune\nthe representation model such that its embeddings\nare more suitable for the tagging task (Panchen-\ndrarajan and Amaresan, 2018). While highly effec-\ntive, the representations may be expensive to com-\npute, and fine-tuning a language model is not al-\nways feasible, for instance if the underlying model\nis hidden behind an application programming inter-\nface (API). Thus, it is desirable to develop tagging\nmethods which achieve the best performance on the\ngiven representations. In fact, the performance of\nprompting-based approaches with large language\nmodels (LLMs) on named entity recognition tasks\nhas lagged behind that of supervised sequence tag-\nging approaches (Wang et al., 2023). Additionally,\nthis leads to problems such as hallucinations and\npotential dataset contamination, which prevent a\nfair evaluation.\nA more fundamental limitation of the prevailing\nparadigm is that the relation of a single input se-\nquence to other sequences in the dataset cannot be\ntaken into account. Both the representation module\nand the tagging module commonly have a limited\nmaximum context length. They cannot process\nthe entire dataset at once, but rather need to be\nprovided with single sentences or paragraphs at a\ntime. The limited context can lead to suboptimal\nperformance (Amalvy et al., 2023). For example,\nconsider named entity recognition for an isolated\nsentence such as Prince was prominently featured\nat the event. The word Prince is ambiguous. In a\ncorpus containing news articles, Prince or Prince\nHarry likely appear in many articles related to the\nBritish royal family. In a different corpus, the term\nCurry Prince might appear frequently in the con-\ntext of restaurant reviews. So only with regard to\nthe entire corpus under consideration, an informed\nchoice on how to tag Prince in the example sen-\ntence could be made.\nIn this work, we show that the relation between\nthe representation of a single token and its contain-\ning corpus can be captured by studying the latent"}, {"title": "space", "content": "space - the collection of the language model's hid-\nden states - surrounding the corresponding embed-\nding vector. The geometry of these hidden states\nis known to capture both syntactic and semantic\nproperties of the underlying text. For instance, Co-\nenen et al. (2019) find that distances between the\ncontextual vectors of bidirectional encoder repre-\nsentations from transformers (BERT) (Devlin et al.,\n2019) correspond to parse tree embeddings based\non the grammatical structure of the input phrases.\nHere, we study neighborhoods of embedding vec-\ntors from a topological viewpoint, and introduce\ndescriptors of the shapes of these neighborhoods\nthat are stable under symmetries such as permuta-\ntions, translations, and rotations. In particular, we\ndefine descriptors based on persistent homology, a\nwell-established tool of topological data analysis\n(Carlsson and Vejdemo-Johansson, 2021)."}, {"title": "1.1 Contribution", "content": "Consider the latent space of a language model in\nthe neighborhood of a given contextual embedding\nvector. For instance, the neighborhood of an em-\nbedding of the word cheap in the context I am\nlooking for options for cheap dinner contains other\noccurrences of the word cheap in different contexts,\nbut also different words expressing a similar mean-\ning (inexpensive, good-value) and words connected\nto the center word, such as restaurant. In this work,\nwe show that:\n(a) this neighborhood contains information that is\nnot present in the language model next-token\nprediction distribution, and that cannot be 'dis-\ntilled' into the language model via naive fine-\ntuning,\n(b) this additional information can be used to im-\nprove the performance of sequence tagging\ntasks, and\n(c) this information can be efficiently summa-\nrized using low-dimensional topological fea-\nture descriptors.\nOur topological descriptors are codensity at mul-\ntiple scales (Carlsson et al., 2008; Carlsson, 2014),\ntopological singularity measures based on Wasser-\nstein norms (Cohen-Steiner et al., 2010), and vec-\ntorized persistence modules. Towards (a), we\nshow that several of our one-dimensional numer-\nical measures show minimal correlation with lan-\nguage model perplexity, indicating that they con-\ntain independent information. Towards (b) and\n(c), we empirically demonstrate improvements on\nthe natural language processing task of variants\nof term extraction. In each case, we build the la-\ntent space through a masked language model from\na dialogue corpus. As a baseline, we employ a\ntagging model trained directly on the original lan-\nguage model vectors, and compare with models\nthat take as input a combination of the language\nmodel vectors with our topological descriptors of\nthe neighborhood within the latent space of a con-\ntextual language model. Furthermore, we compare\nwith models trained on features from Vukovic et al.\n(2022), which are based on neighborhoods in a\nstatic word embedding space. We show that uti-\nlizing the contextually augmented vectors yields\nstatistically significant improvements.\nObservation (a) is not completely new. For ex-\nample, it is present in the idea of k-nearest neigh-\nbor language models (Khandelwal et al., 2020;\nXu et al., 2023), where the current hidden state\nis augmented by the nearest neighbors from a data-\nstore. Our low-dimensional descriptors, on the\nother hand, have not been deployed before, and\nour experiments for (b) provide the first application\nof contextual topological features to token level\nsequence tagging tasks. Note with reference to\npoint (c) that summarizing a collection of vectors\nin a permutation-invariant way is a challenging\nproblem in representation learning (Zaheer et al.,\n2017), which we tackle in this work via tools from\npersistent homology.\nOur work is complementary to other recent appli-\ncations of topological methods to the study of con-\ntextual embedding spaces. Tulchinskii et al. (2023)\ndemonstrate that the topology of a point cloud de-\nrived from a text paragraph can be utilized in a\nsequence classification task, namely to differenti-\nate human-written from artificially generated para-\ngraphs. Their approach takes into account solely\nthe given paragraph's embedding vectors, and does\nnot explore how these reside within the larger la-\ntent space. Another approach involves constructing\nfiltered graphs from the attention scores in a trans-\nformer model, followed by sequence-level classifi-\ncation based on persistent homology (Kushnareva\net al., 2021; Perez and Reinauer, 2022). However,\nthis approach only applies to supervised sequence\nclassification tasks, and does not yield local fea-\ntures required for tagging. In a more qualitative\ndirection, Valeriani et al. (2023) investigate the in-\ntrinsic dimension of the latent space through the\ndifferent layers of a transformer, and Ethayarajh"}, {"title": "2 Background and Methods", "content": "(2019) and Cai et al. (2021) identify isolated clus-\nters and low dimensional manifolds in the latent\nspaces of various language models. However, they\ndo not apply their quantitative local analysis to a\npractical task."}, {"title": "2.1 Latent Spaces of Contextual Language\nModels", "content": "We consider the encoder part of a contextual lan-\nguage model, which can be thought of as a map\n$e: (\\mathbb{R}^d)^{\\times N} \\rightarrow (\\mathbb{R}^h)^{\\times N}$.\nHere, d is the dimension of the input layer, h is the\nhidden dimension (h \u226a d), and N is the maximum\nsequence length, after which sequences will be trun-\ncated. This maximum length is usually fixed and\nfinite. The input of the encoder is a sequence of vec-\ntors $X \\in (\\mathbb{R}^d)^{\\times N}$ representing a tokenized context.\nTokenization describes the process in which an in-\nput string is decomposed into a sequence of vectors.\nIn our setting, tokenization can be thought of as a\nlookup layer converting short text segments to vec-\ntors (together with position information). Typically,\nlonger words are split into several token vectors in\nthis process.\nThe output of the encoder is a sequence of so-\ncalled hidden states. Commonly, these hidden\nstates are inputs to the \u201cprediction head\" of the\nlanguage model, which produces a probability dis-\ntribution over the token space for the corresponding\ntoken location.\nWe think of a language corpus C as a collec-\ntion of tokenized portions of text. From the point\nof view of a language model, each instance i of a\nparticular token appears in a specific context X(i).\nThese contexts are filled with padding tokens so\nthat they always have length N, permitting con-\nstruction of the embedding sets e(X(i)).\nDefinition 2.1 Given an encoder e derived from\na pretrained language model, the ambient corpus\ndatastore with respect to a corpus C is the multi-\nset/point cloud of all the embeddings e(X(i)) of\nall instances i of all tokens in C.\nNote that we cannot explore the entire latent\nspace of the language model, but only the subspace\n\"carved out\" by the datastore under consideration,"}, {"title": "2.2 Local Topological Measures", "content": "All our topological measures are based on neigh-\nborhoods of a given contextual embedding vector\nv with respect to a collection of contextual em-\nbedding vectors coming from an ambient corpus\ndatastore. Given an integer n \u2265 1, we define the\nneighborhood $N_n(v)$ as the multi-set consisting\nof v and its (n - 1) nearest neighbors. To avoid\nadding another copy of the query center vector v\nwhen building the neighborhood from the datastore,\nwe first check for similarity to existing vectors in\nthe datastore with a Euclidean distance threshold\nof 10-4, and take a possible match as center vec-\ntor if applicable. For a schematic illustration of\nthe neighborhood extraction process and the fea-\nture computation, see Figure 1. We consider the\nfollowing local features:\nPersistence Images For a positive integer d, per-\nsistent homology of degree d associates with\na point cloud a multi-set that encodes \"d-\ndimensional topological features\" of the cloud.\nWe refer to Edelsbrunner and Harer (2010) or\nOtter et al. (2017) for introductions. Various\nvectorizations of this multi-set have been de-\nveloped for subsequent use in machine learn-\ning. Persistence images are introduced in\nAdams et al. (2017) as a refined, higher-\ndimensional vectorization of persistent homol-\nogy. We define $PI_d(v) \\in \\mathbb{R}^{100}$ as a persistent\nimage vector of the degree d persistent ho-\nmology of $N_n(v)$, scaled by a factor of $\\frac{1}{n \\cdot 100}$.\nThe parameter n is not included in this no-\ntation, as it will be fixed to 128 throughout\nall experiments. For detailed definitions, see\nAppendix A.1. The factor $\\frac{1}{n \\cdot 100}$ appearing in\nour definition of the persistence image is not\nimportant at this point. It is included to avoid\ninstabilities in the training of the BIO-tagger\ndiscussed in Section 3, which may otherwise\narise from the vastly different scales of the\ncoordinates of the language model embedding\nvectors and these additional coordinates.\nWasserstein Measure A simple one-dimensional\nvectorization is the Wasserstein norm. We"}, {"title": "define", "content": "define $W_d(v) \\in \\mathbb{R}$ as the Wasserstein norm\nof the degree d persistent homology of $N_n(v)$.\nCodensity We define the n-th codensity\n$coden_n(v) \\in \\mathbb{R}$ as the radius of $N_{n+1}(v)$.\nHigher codensity corresponds to regions\nwhere the vectors are farther spread apart.\nThere are several reasons why we fix the car-\ndinality n of the neighborhoods rather than, say,\ntheir radius. Firstly, fixing the cardinality takes\ninto account sample density of the ambient cor-\npus from the latent space of the language model.\nIf we took a fixed radius, sparse regions of the\nambient corpus space would be underrepresented.\nSecondly, some of the topological features we con-\nsider are more readily comparable when computed\non fixed cardinalities. Indeed, a reasonable com-\nparison of Wasserstein norms of neighborhoods\nof different cardinalities seems difficult, and our\nmultiscale definition of (co)density could also not\neasily be emulated for neighborhoods of fixed ra-\ndius. Finally, computational complexity limits the\nfeasibility of approaches that allow for unlimited\ncardinalities of neighborhoods. For instance, in\nVon Rohrscheidt and Rieck (2023), where neigh-\nborhoods of fixed radii are employed, an additional\nsampling step is necessary. More on this is dis-\ncussed in Appendix A.1."}, {"title": "3 Application of Local Topology\nMeasures to Token Level Tagging Task", "content": "We perform a correlation analysis of local features\nand conduct a case study to explore the efficacy\nof local topology measures. Specifically, we apply\nour proposed topological feature augmentations to\nthe task of dialogue term extraction."}, {"title": "3.1 Set-Up", "content": "Data For the term extraction case study, we re-\nsort to the MultiWOZ2.1 (Budzianowski et al.,"}, {"title": "2018; Eric et al., 2020) and schema-guided dia-logue (SGD) (Rastogi et al., 2020) task-oriented di-alogue datasets.", "content": "Here, the ambient reference corpus\nC is built solely from the language model hidden\nstates for the training corpus of MultiWOZ2.1, so\nthat the local measurements are comparable across\nboth datasets.\nBIO-Tagging For the sequence tagging tasks, we\nemploy a beginning (B), inside (I), outside (O) la-\nbeling schema, as in Qiu et al. (2022). To keep\nthe comparison between our different models fair\nand to obtain statements about the quality of the\nunderlying features, we choose the architectures so\nthat the trainable BIO-tagging components have a\nsimilar number of adjustable parameters. In this\nway, we can safely attribute any increase in perfor-\nmance to our topological augmentation of the input\nfeatures rather than a stronger tagging component.\nIn all cases, the BIO-tagging transformer fol-\nlows the ROBERTa architecture (Liu et al., 2019)\nand uses 8 attention heads, 2 hidden layers, and\n512 maximum position embeddings. The language\nmodel vectors and augmenting feature vectors are\nfed into the BIO-tagging component through sepa-\nrate two-layer fully connected encoding networks\nwith subsequent individual layer normalization,\nwhose purpose is down- or up-scaling the feature di-\nmension (768 for the language model vectors, 100\nfor persistence images) to the hidden size (512) of\nthe tagging transformer. For a schematic of our\nBIO-tagging setup, see Figure 1.\nFeatures For creating the language model em-\nbeddings, we use the second to last hidden states\n(at layer 11) of the pretrained RoBERTa base model\n(Liu et al., 2019), which returns 768-dimensional\nvectors, with L2-normalization. Note that on unit\nvectors, the cosine distance is proportional to the\nsquare of the Euclidean distances, thus for the rela-\ntive order in which the nearest neighbors occur, it\ndoes not matter whether we search with respect to\nthe cosine or the Euclidean distance.\nWe decide on the second-to-last hidden state of\nthe language model, as opposed to another inter-\nmediate layer, for two reasons: Cai et al. (2021)\nshow that the local intrinsic dimension tends to in-\ncrease with the depths in the transformer, thus the\nresulting neighborhoods should be more expressive.\nMoreover, Peters et al. (2018) and Tenney et al.\n(2019) find that deeper layers in language models\ntend to capture more of the semantic properties,\nwhile earlier layers tend to capture the syntax. For\nfeature based learning, Devlin et al. (2019) report\nthat among single-layer features, the second-to-last\nlayer leads to the highest performance. Note that\nour setup is not specific to the RoBERTa model or\ntokenizer. Our contextualized topological features\ncan be computed for any (masked or causal) embed-\nding model, extraction layer, datastore produced by\nthe model, and query dataset.\nAs a baseline in our term extraction experiments,\nwe use the language model hidden state vectors\ndescribed in Section 3 as input for the BIO-tagging\nmodel. We test these against augmentation of the\nhidden states with our local persistence image de-\nscriptors introduced in Section 2.2."}, {"title": "3.2 Correlation Analysis of Local Features", "content": "We begin by collecting statistical observations on\nthe correlation between the different local topolog-\nical feature types, as well as their correlation with\npseudo-perplexity, on the example of the Multi-\nWOZ2.1 and SGD datasets. The perplexity of a\ncausal language model is a model intrinsic mea-\nsurement of the surprise of seeing the next token,\ndefined as the exponentiation of the cross-entropy\nbetween the model prediction and the corpus data.\nWhile causal perplexity is not available here, in\nour the masked language model setting, we apply\na pseudo-likelihood score by masking tokens one\nby one, and computing the prediction loss of the\nmasked out token following Salazar et al. (2020).\nIn addition to the non fine-tuned version of the\nlanguage model, here we also include the perplexity\nof the fine-tuned version for comparison, which\nuses the MultiWOZ2.1 training portion (fine-tuning\nfor 5 epochs, 0.15 masking proportion, selecting\nthe best model on MultiWOZ2.1 validation loss).\nAll local features are based a non fine-tuned version\nof the language model.\nNote that we cannot directly compute correla-\ntions between the vector-valued persistence images\nand the perplexity measures. For this reason, we\nare relying on selected codensities $coden_n(v)$ with\nvalues n \u2208 [1;127; 511] and Wasserstein norms as\nnumerical proxy estimates of the neighborhoods'\ntopological complexity. Our term extraction mod-\nels will take only the persistence images as input, as\nthey provide the most powerful and comprehensive\nrepresentation of the local topology. The neural\nnetwork feature extraction model can learn directly\nfrom the persistence images to estimate complex-\nity measures approximating the Wasserstein norms,\nwhich avoids manual feature engineering."}, {"title": "The results of Kendall's rank correlation arestrongly positively correlated with the codensities.", "content": "More importantly, the codensities and Wasserstein\nnorms are only weakly correlated with the perplex-\nity, indicating that these topological measures (and\nthus persistence images) capture information that\nis not present in the language model masked-token\nprediction distribution, and that cannot be 'distilled'\ninto the language model via naive fine-tuning."}, {"title": "3.3 Case Study: Term Extraction in\nTask-Oriented Dialogue Data", "content": ""}, {"title": "3.3.1 Task Definition", "content": "We approach dialogue term extraction as a transfer\nlearning problem. Here, MultiWOZ2.1 serves as\nour source dataset used for training a term extrac-\ntor. The trained model is then applied to the SGD\ndataset, necessitating sufficient transfer learning ca-\npabilities to properly handle the distributional shift\nin the data. We label all phrases in all utterances\nthat match an entry in the respective dataset's on-\ntology, i.e., that match a value in a non-categorical\nslot of the current turn's dialogue state or a value\nin the current turn's dialogue act. The dataset on-\ntology entities are normalized and matched to the\noccurrences in the respective utterances by apply-\ning the TripPy-R label map (Heck et al., 2022) and\nthe SGD canonical value mapping. The ontology\ncomprises names of entities, their domains, prop-\nerties (slots), and values of these slots. We refer to\nthese labelled phrases as dialogue terms.\nThese tagged spans for the dialogue datasets are\nencoded for the BIO-tagger, resulting in the three\nlabels: 0 (outside), B-TERM, I-TERM (begin and\ninside a term). Since our BIO-tagging model oper-\nates on the token-level of the underlying language\nmodel, we re-align the tags with the tokenization\nusing the IOB2 schema: for a word with B-tag, the\nfirst subtoken is tagged with B, its remaining subto-\nkens with I. For a word with I-tag, all its subtokens\nare tagged with I; and analogously for the O-tag.\nWhile we employ token-level cross-entropy loss\nas the differentiable target function in the model\ntraining, the objective of term extraction within\nthe context of this work is to retrieve each unique\ntarget dialogue term at least once. That is, we\ndo not require the tagger to find all occurrences:\nWe normalize the predicted phrases and ground\ntruth by lower-casing, and deduplicate the resulting\ncollections. A term is considered a true positive if\nit is identical to exactly one ground truth term. If a\nterm cannot be assigned to any ground truth term,\ne.g., comprises several ground truth terms or is an\nincomplete substring of a ground truth term, it is\nconsidered a false positive. The left-over ground\ntruth terms without a matched prediction are the\nfalse negatives. We call the resulting prediction,\nrecall and F1-scores the phrasal results.\nWe train the BIO-tagger for 10 epochs, using the\nAdamW optimizer (Loshchilov and Hutter, 2019)\nwith learning rate 5\u00b710-5, linear warm-up for 10%\nof training steps and batch size of 48. The model\npredictions on the held-out MultiWOZ2.1 valida-\ntion set are evaluated every 100 batches for the\nfirst 3 000 global steps, and the model checkpoint\nwith the best phrasal results on the validation set is\nselected as the final model.\nOur goal is to show that injecting our local\ntopological features into the model yields statis-\ntically significant improvements over the original\nlanguage model embeddings. We run statistical\ntests over multiple different random seeds for ini-\ntialization and check for significant changes in eval-\nuation scores on the transfer set, which we take as\nthe full collection of 463 284 utterances from the\nSGD dataset comprising 20 domains."}, {"title": "Training Data In the full data setting, we train onall 113 556 utterances of the MultiWOZ2.1 training split, the results are included in Table 1.", "content": "To demonstrate that the contextual topological\nfeatures are useful in settings with reduced data\nand might help in mitigating overfitting, we create\na variation of the transfer task by restricting train-\ning to subsets of the MultiWOZ2.1 dataset. This is"}, {"title": "a more realistic transfer setting, since a good model", "content": "checkpoint needs to demonstrate that it can gener-\nalize to the unseen left-out domain which it encoun-\nters in the MultiWOZ2.1 validation split for the first\ntime. Given one of the five major domains D\u2208\nattraction; hotel; restaurant; taxi; trainin the train-ing split, we exclude those utterances contained in\ndialogues from D in the tagger training, which\nleaves 71768;59 222; 58 156; 86 568; 66 736ut-\nterances respectively. Model selection is performed\nbased on phrasal F1-score on the 14748 validation\nutterances, which span over all five domains. We re-\nport results of this cross-validation setup by macro\naveraging the phrasal scores on the SGD dataset\nover 10 seeds for each of the five left out data folds\nin Table 2."}, {"title": "Static Topological Features Baseline We eval-uate term extraction performance on the level ofphrase predictions.", "content": "The phrase-level evaluation al-\nlows a comparison with Vukovic et al. (2022), who\npresent a method that employs static topological\ndescriptors in sequence tagging tasks. The main\ndifferences to our approach with contextual topo-\nlogical features are as follows:\n\u2022 Our local persistent homology descriptors are\ndefined on token level with respect to the to-\nkenization of the language model. This is\nessential for combining our new features with\nthe language model embeddings to create fu-\nsion models which can provide predictions on\ntoken-level. The static topological features of\nVukovic et al. (2022) operate on word level,\nand they only gained contextuality in the BIO-\ntagging component of the model. Note that\nin this and our work, the context of an input\nsequence is a single dialogue utterance.\n\u2022 Vukovic et al. (2022)'s features were based\non neighborhoods in an ambient static word\nspace composed of the 100 000 most common\nwords in the English language. Thus, their\nmethod depends both on having a word-level\nseparation of the input data, and on a given\ndictionary. Our contextualized features on\nthe other hand can be defined without any\nadditional external data."}, {"title": "For a comparison between static and contex-tual features, we align the static topological fea-tures with the roberta-base tokenization in ourBIO-tagging setup, and train BIO-taggers with thesame architecture and data as in the contextualtopological feature setting.", "content": "To that end, the first\nconstituent subtoken of each word is augmented\nwith the word's corresponding 100-dimensional\n$H_0$-persistence image feature vector of Vukovic\net al. (2022)."}, {"title": "3.3.2 Results", "content": "Quantitative Analysis Table 1 lists the term ex-\ntraction performance for the pure language model\nbaseline, our proposed method of augmenting with\ncontextual topological features, and the alternative\napproach by augmenting with the static topological\nfeatures from Vukovic et al. (2022). The main ob-\njective of said work was the maximization of recall,\nand to that end they proposed separate language\nmodel and topological feature taggers, with a sub-\nsequent union of predictions. In contrast, we show\nthat our unified model augmented with contextual\ntopological features can increase precision, recall,\nand F1 over the language model baseline.\nTable 2 presents averaged results for the models\ntrained on a reduced dataset constructed by omit-"}, {"title": "Qualitative Analysis To obtain explicit exam-ples, we select a model checkpoint for each featuretype after 1 100 global steps, and inspect the differ-ences between predicted normalized phrase sets.", "content": "In\nTable 3 we see examples where the topologically\naugmented model succeeds in finding complete\nmulti-word terms, whereas the baseline model cuts\noff before the end of a term or misses proper names\nthat should follow a preposition. Such informa-\ntion is highly dependent on the context of a term,\nand the contextual topological model is able to find\nlong terms more consistently. All models identify\nthe restaurant name \"Cafe Jolie\" correctly, but only\nthe topological models recognize the actress \"An-\ngelina Jolie\". Similarly, the song title \"Be alright\"\ncontaining the frequent word \u201calright\u201d is not rec-\nognized by the language model alone, but can be\ndetected by a topological model."}, {"title": "3.4 Relation to the Manifold Hypothesis", "content": "At first glance, our results may appear to be at\nodds with the manifold hypothesis, a common as-\nsumption underlying many representation learning\nparadigms. While this hypothesis has been ques-\ntioned for static word embeddings (Jakubowski\net al., 2020), it remains uncontested for contextual\nembeddings. It posits that the latent space of a\ntrained machine learning model is clustered around\na disjoint union of lower-dimensional manifolds\n(Bengio et al., 2013; Brown et al., 2023). This im-\nplies that, from a purely topological perspective,\nthe local structure of the latent space is constant,\nat least along connected components \u2013 every point\nshould have a neighborhood topologically identi-\ncal to an open ball in some Euclidean space. How\nthen is it possible that we can extract meaningful\ninformation from variations of the local topology?\nThere are at least two answers to this. First, all\nour measures depend on the way data is sampled.\nThere is no reason to assume that the embeddings\ndrawn from a given corpus provide a uniform sam-\nple of the latent space. On the contrary, the dis-\ntribution of these samples will depend heavily on\nthe corpus. And within a given corpus, we might\nexpect the neighborhoods of latent vectors of con-\ntent words to be \u201cmore spread-out\" and of higher\ndimension than those of non-content words, since\nthere are more plausible possibilities for filling in\ncontent words in a text than for non-content words.\nSecond, our measures are based on persistent ho-\nmology, which is known to detect not only topolog-\nical properties but also differentiable structure such\nas curvature (Bubenik et al., 2020). Thus, even on\na uniformly sampled manifold, these measures are\nexpected to vary.\""}, {"title": "3.5Computational Complexity", "content": "In this section, we address the computational over-\nhead coming from our proposed method of aug-\nmenting a sequence tagger with contextual topo-\nlogical information of a given corpus. The one-off\ncomputational costs for the datastore, in our study\nderived from the MultiWOZ2.1 training dataset,\nand the query datasets (MultiWOZ2.1 training &\nvalidation dataset, and SGD dataset) involve a sin-\ngle embedding model forward pass for each input\nsequence.\nFor each query dataset relative to the datas-\ntore, assuming a constant and small neighborhood\nsize n, the asymptotic complexity of the neigh-\nborhood search depends on the tokenized cardi-\nnality of the query dataset |Q|, the tokenized car-\ndinality of the datastore |C|, and the embedding\ndimension d. The runtime complexity using the\nexact search implementation from (Johnson et al.,\n2021) is O(|Q||C|d), and the storage complexity\nfor neighborhood indices is O(|Q|n).\nThe persistent homology computation in dimen-\nsion 0 for each query vector depends on the neigh-\nborhood size n as well. For degree 0, the number\nof simplices in the Vietoris-Rips complex can be\nupper-bounded by n\u00b2. Thus, the persistence di-\nagram for each neighborhood can be computed\nin O(n\u00b2w), where w < 2.4 is the matrix mul-\ntiplication exponent (Milosavljevi\u0107 et al., 2011).\nThere are at most n generators in the 0-dimensional\npersistence diagrams, so the computation of the\nWasserstein norms can be achieved in O(n\u00b3) (La-\ncombe et al., 2018). Empirically, the computation\nof the persistence images is observed to be very\nquick compared to the computation of the persis-\ntence diagrams.\nOnce computed and cached, these topological\nfeatures can be reused for different training objec-\ntives on the given query dataset. The only overhead\nin transitioning from the baseline tagger (approxi-\nmately 35.65 million parameters) to the tagger with"}]}