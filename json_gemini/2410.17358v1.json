{"title": "FAIRLORA: UNPACKING BIAS MITIGATION IN VISION\nMODELS WITH FAIRNESS-DRIVEN LOW-RANK ADAP-\n TATION", "authors": ["Rohan Sukumaran", "Adriana Romero-Sorian", "Aarash Feizi", "Golnoosh Farnadi"], "abstract": "Recent advances in parameter-efficient fine-tuning methods, such as Low Rank\nAdaptation (LoRA), have gained significant attention for their ability to effi-\nciently adapt large foundational models to various downstream tasks. These meth-\nods are appreciated for achieving performance comparable to full fine-tuning on\naggregate-level metrics, while significantly reducing computational costs. To sys-\ntematically address fairness in LLMs previous studies fine-tune on fairness spe-\ncific data using a larger LoRA rank than typically used. In this paper, we introduce\nFairLoRA, a novel fairness-specific regularizer for LoRA aimed at reducing per-\nformance disparities across data subgroups by minimizing per-class variance in\nloss. To the best of our knowledge, we are the first to introduce a fairness based\nfinetuning through LoRA. Our results demonstrate that the need for higher ranks to\nmitigate bias is not universal; it depends on factors such as the pre-trained model,\ndataset, and task. More importantly, we systematically evaluate FairLoRA across\nvarious vision models, including ViT, DINO, and CLIP, in scenarios involving\ndistribution shifts. We further emphasize the necessity of using multiple fairness\nmetrics to obtain a holistic assessment of fairness, rather than relying solely on the\nmetric optimized during training.", "sections": [{"title": "INTRODUCTION", "content": "The advent of foundational models Bommasani et al. (2021) has led to the widespread adoption\nof parameter-efficient fine-tuning (PEFT) methods such as Low-Rank Adaptation (LoRA) Hu et al.\n(2021), enabling efficient adaptation to various downstream tasks. These methods offer significant\ncomputational advantages and often achieve performance comparable to full fine-tuning (FFT) of\nthe entire model on aggregate metrics Hu et al. (2021); Zhao et al. (2024); Dettmers et al. (2024).\nHowever, their impact on fairness remains under-explored. More importantly, given the widespread\nuse of LORA for fine-tuning foundational models, the uncertainty regarding the impacts on fairness\nas well as bias mitigation, complicates deployment and raises ethical concerns, emphasizing the\nneed to measure and mitigate disparate impacts.\nRecent works on Fairness with PEFT has focused on either (i) finding the right parameters to tune\nthrough heuristic search algorithms Dutt et al. (2023) or (ii) fine-tuning on specific datasets curated\nfor fairness Das et al. (2024). As noted in the papers, finding the right set of parameters is a challeng-\ning and often a computationally expensive problem, thereby contradicting the major advantage of\nusing PEFT. Although effective, fine-tuning with fairness specific datasets can also be challenging\ngiven the complexities involved in data collection, curating, and labelling. Furthermore, it is also\nimportant to note that in order to mitigate bias based on different notions, we might need completely\ndifferent datasets- thereby making it hard to scale.\nThe fairness community in ML has often argued the importance of the 'right metric' to measure un-\nfairness and how that changes the answer to the question \u201cDoes X have disparate impact\u201d Hashem-\nizadeh et al. (2023); Simson et al. (2024). Our work emphasizes the importance of evaluating mul-\ntiple fairness metrics rather than relying on a single measure. By considering metrics such as aggre-\ngate accuracy, minimum and median F1 scores across groups, and performance disparities between\ngroups, we aim to capture a holistic view of both performance and fairness. This comprehensive\nevaluation allows us to assess whether PEFT methods like LoRA consistently meet fairness stan-\ndards or may lead to adverse outcomes in certain configurations.\nWe introduce FairLoRA, a novel fairness based LoRA that scales across models and datasets. Our\nfindings indicate that FairLoRA performs comparable to or better than using fairness specific regu-\nlarization with FFT, across most metrics. Our method is an in-processing bias mitigation method that\naims at altering the learning objective with almost zero addition in computational cost, as opposed\nto heuristic search based fair finetuning.\nAdditionally, we explore the hypothesis that distribution shifts between pre-training and fine-tuning\ndatasets contribute to fairness disparities. By analyzing models such as CLIP Radford et al. (2021),\nDINO Caron et al. (2021), and ViT Dosovitskiy (2020), we assess how pre-training strategies and\ndata distributions affect fairness during fine-tuning. To investigate this, we conduct experiments\non diverse datasets\u2014Aircrafts Maji et al. (2013), GeoDE Ramaswamy et al. (2024), and Water-\nbirds Sagawa et al. (2019)\u2014which differ significantly from popular pre-training datasets. Our\nexperiments show that distribution shifts can exacerbate fairness issues, but FairLoRA is able to\nsuccessfully mitigate them.\nThrough this work, we aim to determine which approach-LoRA or Full Finetuning (FFT) \u2014is\nfairer under different conditions and how fairness regularization affects performance. Our findings\nsuggest that FairLoRA is superior to LoRA, especially when considering across metrics. It is also\nimportant to note that FairLoRA is also comparable to Fair FFT.\nOur main contributions are as follows:\n\u2022 We highlight the importance of comprehensive evaluation across multiple metrics when\n assessing fairness.\n\u2022 We are the first to formulate FairLoRA a fairness regularizer based on reducing the\n variance among intra-class losses to improve the fairness of models fine-tuned with LoRA.\n\u2022 We demonstrate that higher ranks are not necessarily required to improve fairness through\n learning with FairLoRA."}, {"title": "PRELIMINARIES: LORA", "content": "Low-Rank Adaptation (LoRA) Hu et al. (2021) is a parameter-efficient fine-tuning (PEFT) method\nthat injects trainable low-rank matrices into the weight updates of a pre-trained model, significantly\nreducing the number of trainable parameters without compromising performance. The core idea of\nLORA is to decompose the weight updates into two low-rank matrices, which are then trained to\ncapture task-specific knowledge.\nConsider a pre-trained subset of parameters \\( \\theta_0 \\in \\Theta \\), where \\( \\Theta \\) represents the full parameter space\nof the model. In LoRA, instead of updating \\( \\theta_0 \\) directly, the weight update \\( \\Delta \\theta \\) is parameterized as\na product of two low-rank matrices \\( A \\in \\mathbb{R}^{d \\times r} \\) and \\( B \\in \\mathbb{R}^{r \\times k} \\), where \\( r < \\text{min}(d, k) \\). Thus, the\nupdated parameter matrix becomes:\n\\( \\theta = \\theta_0 + \\Delta \\theta = \\theta_0 + AB \\)\nHere, A and B are the trainable matrices, and r is the rank, controlling the parameter reduction. By\nmaking r much smaller than d and k, the total number of trainable parameters is reduced from d \u00d7 k\nto r \u00d7 (d+k), leading to substantial memory savings."}, {"title": "FAIRLORA: FAIRNESS AWARE LORA", "content": "Does LoRA have a systemic disparate impact? Our experiments, similar to the work from Ding\net al. (2024), notes that there is no systemic disparate impact when doing LoRA compared to full\nfinetuning. That is even if LoRA tends to under-perform for a certain set of datasets, model, and\nmetric combination, there is no specific pattern of disparate impact. These trends also hold for the\nlowest rank that is experimented on. As seen from fig. 2a we do however notice that CLIP is better\nthan the other, and can have significant improvements both in terms of accuracy as well as fairness\nmetrics with LoRA. At the same time, it is also worth noting that in fig. 2b, we can see that LORA\nmodels may or may not have comparable performance when it comes to fairness metrics such as\nvariance of per class loss. Following on this direction, we aim to improve the fairness of a model on\ndownstream task, akin to Das et al. (2024). One of the key distinction from (Das et al., 2024) is that\nwe aim to improve fairness by introducing a fairness constraint that aims to improve the performance\nof all classes in a dataset as opposed to relying on a fairness specific dataset that can only be used\nfor a small set of bias mitigation usecases."}, {"title": "IMPROVING PER CLASS PERFORMANCE", "content": "In this work we focus on accuracy parity as a notion of fairness. In order to achieve accuracy parity,\nthe aim is to have equal accuracy across all groups. This is challenging and often times impossible to\nachieve given the quantized nature of accuracy, number of samples in a group, and difficulty of the\nsamples in each class. Considering the impracticality of directly enforcing accuracy parity, we aim\nto use a method that still helps to improve the per group accuracy. We introduce a fairness regularizer\naimed to reduce the variance of per group loss at a mini-batch level, thereby implicitly improving\nthe performance of under performing group. It is important to note that a degenerate solution to this\nproblem could be by making the performance per class equal, but extremely low-this does not occur\nhere as we still have the original objective that pushes to improve the overall performance of the\nmodel and we do not observe this degenerate solution in any of our experimental settings.\nIn this problem, we aim to minimize the empirical risk over data points x, labels y, and model pa-\nrameters \\( \\theta \\). Specifically, let the model parameters be \\( \\theta \\), which can represent either the entire set\nof model parameters or the Low-Rank Adaptation (LoRA) parameters, denoted as #LORA, depend-\ning on the adaptation approach used in the model. We aim to minimize the total loss function by\nsearching over the parameter space, which includes both the full model parameters and the LORA\nparameters:\n\\( \\min J(\\theta) = \\min \\mathcal{L}(\\theta) + \\frac{\\lambda}{2} (\\mathcal{L}_g(\\theta) - \\frac{1}{|G|} \\sum_{g' \\in G} \\mathcal{L}_{g'}(\\theta))^2 \\)"}, {"title": "MEASURING FAIRNESS", "content": "We focus on five key metrics to provide a comprehensive assessment of both the model's perfor-\nmance and its fairness. These metrics are:\n1. Aggregate Evaluation Accuracy (Acc): The overall accuracy of the model across the\n entire dataset.\n2. Minimum F1 Score Across Groups (\\( \\min_{g \\in G} F1_g \\)): The lowest F1 score among all groups\n G, highlighting the worst-performing group and helping to identify significant disparities.\n3. Minimum Recall Across Groups (\\( \\min_{g \\in G} \\text{Recall}_g \\)): The lowest Recall score among all\n groups G, highlighting the worst-performing group and helping to identify significant dis-\n parities in terms of misclassifications.\n4. Sensitive Image Accuracy (if applicable, Accsensitive): The accuracy specifically on sen-\nsitive groups, applicable when the dataset contains sensitive labels. This metric measures\nany spurious correlations or privacy violation with respect to the model.\n5. Difference of F1 Scores Between Worst and Best Groups (\\( \\Delta F1 = \\max_{g \\in G} F1_g - \\min_{g \\in G} F1_g \\)): The gap between the highest and lowest F1 scores across groups, serving\nas an indicator of fairness by measuring performance disparity.\nAdditionally, we present results for conventional fairness metrics such as the Equalized Opportu-\nnity Difference Hardt et al. (2016); Verma & Rubin (2018); Mehrabi et al. (2021), which measures\nthe difference in true positive rates between groups:\nEqualized Opportunity Difference = |P(\u0176 P(\u00dd = 1 | Y = 1, S = s\u2081) \u2013 P(\u0176 = 1 | Y = 1, S = s\u2082) |\nwhere Y is the true label, \u0176 is the predicted label, and S is the sensitive attribute, with s\u2081 and s\u2082\nbeing different groups within S.\nEqualized Opportunity Difference for Multiple Sensitive Groups. For multiple sensitive\ngroups, we generalize the Equalized Opportunity Difference (EOD) using a one-vs-all approach.\nLet S \u2208 {s\u2081, s\u2082, . . ., sk} be the sensitive attribute, and \u0176 the predicted outcome. The EOD between\ngroup S = si and others S \u2260 si is defined as:\n\\( EOD_{S_i} = |P(\\hat{Y} = 1 | Y = 1, S = s_i) - P(\\hat{Y} = 1 | Y = 1, S \\neq s_i)| \\)\nMaximal Equalized Opportunity Difference. We define the maximal EOD, EODmax, as the max-\nimum disparity across all sensitive groups:\n\\( EOD_{\\text{max}} = \\max_{i \\in {1,2,...,k}} (EOD_{S_i}) \\)\nThis captures the worst-case violation of equalized opportunity across groups and is a key metric for\nmeasuring fairness with multiple sensitive attributes.\nThese metrics collectively provide a well-rounded evaluation of both model performance and fair-\nness across diverse groups."}, {"title": "EXPERIMENTS", "content": "In this section, we present an empirical comparison addressing the various research questions high-\nlighted earlier. The primary goal of our experiments is to fine-tune models using LoRA with minimal\ndisparity. Although reducing disparity may introduce a trade-off with aggregate performance, our\naim is to achieve overall accuracy comparable to mitigation-agnostic methods, both with and without\nLORA. All models, unless mentioned otherwise are chosen based on the best evaluation accuracy."}, {"title": "EXPERIMENTAL SETUP", "content": "Tasks and architectures. We conduct image classification experiments on the Aircrafts Maji et al.\n(2013), GeoDE Ramaswamy et al. (2024), and Waterbirds Sagawa et al. (2019) datasets. These\ndatasets were chosen because they differ significantly from popular pre-training datasets, as noted\nin previous works. To provide sufficient variation in architecture, pre-training data, and strategy,\nwe use CLIP Radford et al. (2021), DINO Caron et al. (2021), and ViT Dosovitskiy (2020) models\nin our experiments. All tables, figures unless mentioned otherwise is reported across 3 seeds. For\nLORA as well as FairLoRA only the low rank parameters are updated.\nVision Models: CLIP-632, DINO-b16, and ViT-b16 are key models in computer vision with distinct\nadvantages. CLIP (Contrastive Language-Image Pretraining) excels in cross-modal tasks by learning\nfrom large scaled paired image-text data and DINO (Self-Distillation with No Labels) uses self-\nsupervised learning, ideal for tasks without labeled data. Additionally, the commonly available\nversions also have different pre-training data - both DiNO and ViT are pre-trained on ImageNet Deng\net al. (2009), while CLIP is pre-trained on LAION-5B Schuhmann et al. (2022).\nDatasets. Consistent with prior studies, we use 6,667 training samples and 3,333 test samples from\nthe Aircrafts dataset to perform image classification across 100 classes, noting the high intra-class\nsimilarity present in this dataset. For Waterbirds, we perform image classification over 2(\u2018landbird'\nand 'waterbird') classes using 4,795 training samples, 2,400 validation samples, and 2,800 test sam-\nples. The per-class distribution of Waterbirds varies across these splits; more details can be found"}, {"title": "EMPIRICAL EVALUATIONS", "content": "How does FairLoRA improve the fairness? We see that for most of the experiments, FairLoRA\nis comparable or better than doing fair full fine-tuning. In particular, in fig. 8a, we can see that Fair-\nLORA performs better on multiple metrics. It is important to note that sometimes this improvement\nin fairness comes at a small cost of aggregate accuracy, but as seen in table 1 and table 2, this is\ndependent on the underlying base model. Furthermore, we also note that in most of our experiments,\nFairLoRA performs better across metrics (both in terms of fairness and performance) than LoRA.\nDoes the rank have a universal impact in FairLoRA? In our experiments we notice that, there\nis no strict trend as to when a higher rank would be required. We see that based on the model,\npre-training data and pre-training strategy, the rank required to get a fair model with good overall\nperformance would vary. This can be seen in fig. 5, where the CLIP models get a fair and well\nperforming model for much lower ranks in FairLoRA. It is also important to note that there is no\nmonotonic pattern associated with LoRA ranks when. For most of the experiments even low ranks\nwere comparable both in terms of fairness and performance, and this is something not observed in\nthe previous work on LLMs Das et al. (2024).\nHow does FairLoRA handle distribution shifts from the pre-training data? Based on FID\nscores Parmar et al. (2022), GeoDE and Waterbirds are farther from both the pre-training distri-\nbutions(ref table 4 for FID). From table 1, it is clear that FairLoRA is best across methods and with\nViTs, it is second only to Fair FFT despite having less than 1% of trainable parameters in compari-\nson. It is also important to note that in table 2, FairLoRA is better than FairFFT and FFT across all\nmetrics and gets comparable performance to LoRA.\nDoes FairLoRA perform the same across architectures Although FairLoRA improves fairness\nacross metrics on most tasks, it is important to note the variance across architectures. In general, we\nnotice that the CLIP models are more adaptable to the FairLoRA and exhibit improvements across\nall metrics. It is also important to note that CLIP models are almost twice as large as the other\nmodels. Furthermore, it is important to note that DiNO models seem to adapt worse to the LORA\nbased fairness regularization, especially with distribution shift(table 1. ViTs seem to adapt least\nwhen the task involves groups with high intra-class similarities and require higher model capacity\nto improve on both performance and fairness.(table 6)"}, {"title": "How does FairLoRA affect privacy", "content": "The sensitive image accuracy aims to determine how much\nspurious information is leaked, when we are fine-tuning the model on downstream datasets. Usually,\nwe see that despite not being part of the learning objective models tend to pick up these sensitive\ninformation from the data and is often exacerbated when optimized for fairness Fioretto et al. (2022).\nWe can see that in table 3, LoRA models have a lower (better) sensitive image accuracy compared to\nfull finetuning. We also see the similar trend reflected in FairLoRA vs FairFFT, thereby highlighting\nthat forcing fairness doesn't come at the cost of a privacy violation in this setup. Furthermore, we can\nhypothesise that the low rank matrices work in ways similar to sparse gradients and therefore provide\nsome implicit differential privacy benefits Ghazi et al. (2024); Yang et al. (2023); Malekmohammadi\n& Farnadi (2024)."}, {"title": "How does FairLoRA perform across the metrics", "content": "From fig. 3 it is clear that FairLoRA has a\ndominant performance across metrics. It is also worth noting that, despite not directly optimizing\nfor it, the regularizer also helps to improve on fairness aspects such as EOD and F1. It is also worth\nhighlighting that in fig. 4, although better in most metrics, FairLoRA tends to be slightly worse on\nEOD compared to other methods, thereby highlighting the importance of our proposed multi faceted\nevaluations."}, {"title": "DISCUSSION", "content": "Given the ubiquitous use, it is important to develop parameter-efficient techniques that reliably mit-\nigate fairness issues. While developing such solutions it is important to focus on (i) trade offs in\nterms of different metrics, and compute and (ii) if the method truly generalizes"}, {"title": "TRADE-OFFS", "content": "In our experiments, we observed trade-offs between overall performance and fairness metrics when\napplying FairLoRA. Incorporating the fairness regularizer often led to improved performance on\nunderrepresented groups at the expense of slight reductions in aggregate accuracy. This trade-off\nis expected, as the regularizer aims to reduce the variance in loss across groups, thereby focusing\nthe model's learning capacity on groups that are harder to predict accurately. But it is important\nto note that under most settings, the aggregate accuracy improved or was comparable to that of\nfull-finetuning.\nThe choice of the LoRA rank also plays a pivotal role in balancing the trade off between various\nmetrics and computational cost. However, the observations we have show that the effect of rank is\nnot universal and would vary across models, datasets and even metrics."}, {"title": "GENERALIZATION", "content": "The generalization of fairness improvements across different models and datasets is a critical con-\nsideration. Our results indicate that although useful, the effectiveness of FairLoRA in mitigating\nfairness issues is not universal but depends on factors such as the pre-trained model architecture,\nthe nature of the pre-training data, and the specific downstream task. For instance, models such as\nCLIP, which are pre-trained on diverse multi-modal data, may require lower LoRA ranks to achieve\nfairness compared to models pre-trained on more homogeneous datasets.\nMoreover, the distribution shift between pre-training and fine-tuning datasets as well as the intra-\nclass similarities within the fine-tuning dataset can influence the model's ability to generalize fair-\nness improvements."}, {"title": "CONCLUSION", "content": "In this work, we introduced FairLoRA, a fairness-aware LoRA approach by incorporating a fairness\nregularizer aimed at reducing the variance of per-group loss, thereby improving performance on\nunderrepresented groups. Through comprehensive experiments across various models (CLIP, DINO,\nViT), datasets (Aircrafts, GeoDE, Waterbirds), and fairness metrics, we found that LoRA does not\nintroduce systemic disparate impact and FairLoRA can achieve fairness outcomes comparable to or\nbetter than Fair full fine-tuning (FFT).\nOur findings highlight the importance of evaluating multiple fairness metrics to capture a holistic\nview of a model's performance and fairness implications. We observed that there is no universal\ntrend with respect to LoRA ranks; the optimal rank depends on the specific model, pre-training\ndata, and task. Additionally, we examined the effects of distribution shifts between pre-training and\nfine-tuning datasets, and notice how efficiently FairLoRA can adapt.\nOverall, our study demonstrates that FairLoRA is a viable and efficient alternative to FFT for miti-\ngating fairness issues in machine learning models. Future work could extend this analysis to other\narchitectures, datasets, and definitions of fairness, as well as explore intersectional fairness."}, {"title": "APPENDIX", "content": ""}, {"title": "REPRODUCIBILITY STATEMENT", "content": "All our code and model weights will be open-sourced post the conference anonymity period and will\nbe available to the larger community to use under open source licensing."}, {"title": "FR\u00c9CHET INCEPTION DISTANCE (FID) SCORES - DISTRIBUTION SHIFT", "content": "The Fr\u00e9chet Inception Distance (FID) is a widely-used metric to evaluate the quality of generated\nimages by comparing them to real images. It works by calculating the distance between the feature\ndistributions of two datasets (real and generated images) using the activations of the InceptionV3\nmodel. Specifically, the FID score is computed using the mean and covariance of these activations,\nassuming a multivariate Gaussian distribution.\nIn our setup, we employed the Clean FID library Parmar et al. (2022) to calculate the FID between\nvarious datasets. For large datasets like LAION-5B, we sampled random subsets and computed the\nFID on these smaller samples to manage computational constraints. For smaller datasets such as\nWaterbirds, GeoDe, and Aircrafts, we used the entire dataset for the calculation."}, {"title": "TRAINABLE PARAMETER RATIOS WITH LORA/FAIRLORA", "content": "The number of trainable parameter remain the same for LoRA as well as FairLoRA for the same\nrank. All models have similar ratios for % of Trainable parameters."}, {"title": "GRADIENT UPDATES IN LORA", "content": "The gradient updates in LoRA apply only to the low-rank matrices A and B, while the pre-trained\nparameters \\( \\theta_0 \\) remain fixed. Given an objective function \\( \\mathcal{L} \\), the gradients of the loss with respect to\nA and B are computed as:\n\\( \\frac{\\partial \\mathcal{L}}{\\partial A} = \\frac{\\partial \\mathcal{L}}{\\partial \\Theta} B^T, \\qquad \\frac{\\partial \\mathcal{L}}{\\partial B} = A^T \\frac{\\partial \\mathcal{L}}{\\partial \\Theta} \\)\nHere, \\( \\frac{\\partial \\mathcal{L}}{\\partial \\Theta} \\) is the gradient of the loss with respect to the full parameter matrix \\( \\Theta \\). These updates allow\nthe model to adapt to the downstream task with far fewer trainable parameters, preserving most of\nthe pre-trained knowledge while fine-tuning for the new task.\nLoRA's parameterization is particularly effective in large models where the parameter matrices are\nhigh-dimensional, as it avoids the computational cost of updating the entire matrix. By focusing"}, {"title": "GRADIENTS IN FAIRLORA", "content": "Let \\( \\theta = \\theta_0 + AB \\), where \\( \\Delta \\theta = AB \\) is the LORA low-rank update. We need to compute the gradients\nof \\( J(\\theta) \\) with respect to both A and B.\nGRADIENT WITH RESPECT TO A:\n\\( \\frac{\\partial J}{\\partial A} = \\frac{\\partial \\mathcal{L}}{\\partial \\theta} B^T + \\frac{\\lambda}{2} \\sum_{g \\in G} (\\mathcal{L}_g(\\theta) - \\frac{1}{|G|} \\sum_{g' \\in G} \\mathcal{L}_{g'}(\\theta)) \\frac{\\partial \\mathcal{L}_g(\\theta)}{\\partial \\theta} B^T \\)\nGRADIENT WITH RESPECT TO B:\n\\( \\frac{\\partial J}{\\partial B} = A^T \\frac{\\partial \\mathcal{L}}{\\partial \\theta} + \\frac{\\lambda}{2} \\sum_{g \\in G} (\\mathcal{L}_g(\\theta) - \\frac{1}{|G|} \\sum_{g' \\in G} \\mathcal{L}_{g'}(\\theta)) A^T \\frac{\\partial \\mathcal{L}_g(\\theta)}{\\partial \\theta} \\)"}, {"title": "EMPIRICAL EVALUATIONS CONTINUED", "content": ""}]}