{"title": "ADR-BC: Adversarial Density Weighted Regression Behavior Cloning", "authors": ["Ziqi Zhang", "Zifeng Zhuang", "Donglin Wang", "Jingzehua Xu", "Miao Liu", "Shuai Zhang"], "abstract": "Typically, traditional Imitation Learning (IL) methods first shape a reward or Q function and then use this shaped function within a reinforcement learning (RL) framework to optimize the empirical policy. However, if the shaped reward/Q function does not adequately represent the ground truth reward/Q function, updating the policy within a multi-step RL framework may result in cumulative bias, further impacting policy learning. Although utilizing behavior cloning (BC) to learn a policy by directly mimicking a few demonstrations in a single-step updating manner can avoid cumulative bias, BC tends to greedily imitate demonstrated actions, limiting its capacity to generalize to un-seen state action pairs. To address these challenges, we propose ADR-BC, which aims to enhance behavior cloning through augmented density-based action support, optimizing the policy with this augmented support. Specifically, the objective of ADR-BC shares the similar physical meanings that matching expert distribution while diverging the sub-optimal distribution. Therefore, ADR-BC can achieve more robust expert distribution matching. Meanwhile, as a one-step behavior cloning framework, ADR-BC avoids the cumulative bias associated with multi-step RL frameworks. To validate the performance of ADR-BC, we conduct extensive experiments. Specifically, ADR-BC showcases a 10.5% improvement over the previous state-of-the-art (SOTA) generalized IL baseline, CEIL, across all tasks in the Gym-Mujoco domain. Additionally, it achieves an 89.5% improvement over Implicit Q Learning (IQL) using real rewards across all tasks in the Adroit and Kitchen domains. On the other hand, we conduct extensive ablations to further demonstrate the effectiveness of ADR-BC.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) has revolutionized numerous domains, including robotics leaning (Brohan et al., 2023a,b; Bhargava et al., 2020), language modeling (Ouyang et al., 2022; Touvron et al., 2023), and natural scientific research (G\u00f3mez-Bombarelli et al., 2018). Many of these endeavors"}, {"title": "2 Related Work", "content": "Imitation Learning. Modern Imitation Learning (IL) encompasses various settings, each tailored to specific objectives. Primarily, IL can be categorized based on the imitating objective into Learning from Demonstration (LfD) (Argall et al., 2009; Judah et al., 2014; Ho and Ermon, 2016a; Brown et al., 2020; Ravichandar et al., 2020; Boborzi et al., 2022) and Learning from Observation (LfO) (Ross et al., 2011a; Liu et al., 2018; Torabi et al., 2019; Boborzi et al., 2022). Additionally, depending on whether access to the environment is available for acquiring new datasets, IL can be further classified into online IL (Ross et al., 2011a; Brantley et al., 2020; Sasaki and Yamashina, 2021) and offline IL (Chang et al., 2021; DeMoss et al., 2023; Zhang et al., 2023). Moreover, IL can be distinguished between single-domain and cross-domain IL. Our study primarily aligns with LfD setting IL algorithms, given our focus on estimating a robust target density via adversarial augmentation while utilizing a density-weighted objective to guide the relocation of the expert policy. This research also shares connections with previous supervised IL methods (most of the competitive baselines are contextual information conditioned policy) (Liu et al., 2023a; Zhang et al., 2024). However, our approach differs from contextual information-based IL approaches in that our formulation operates entirely under the Markov Decision Process (MDP) assumption. This sets us apart from prior supervised contextual methods that incorporate contextual information fused with long horizontal information.\nBehavior Policy Modeling. Previously, estimating the action support of the behavior policy has been approached using various methods, including Gaussian (Kumar et al., 2019; Wu et al., 2019) or Gaussian mixture (Kostrikov et al., 2021) sampling approaches, Variance Auto-Encoder (VAE) based techniques (Kingma and Welling, 2022; Debbagh, 2023), or accurate sampling via autoregressive language models (Germain et al., 2015). Specifically, the most relevant research to our study involves utilizing VAE to estimate the density-based definition of action support (behavior density) (Fujimoto et al., 2019; Wu et al., 2022). On the other hand, behavior policy is utilized to regularize the offline training policy (Fujimoto and Gu, 2021), reducing the extrapolation error of offline RL algorithms, it has also been utilized in offline-to-online setting (Wu et al., 2022; Fujimoto and Gu, 2021; Nair et al., 2021) to ensure the stable online fine-tuning. Different from previous study, our research focus on utilizing the estimated target density to assistant in relocating the expert policy."}, {"title": "3 Preliminaries", "content": "Reinforcement Learning (RL). We consider RL can be represented by a Markov Decision Process (MDP) tuple i.e. $\\mathcal{M} := (\\mathcal{S}, \\mathcal{A}, p_0, r, d_\\mathcal{M}, \\gamma)$, where $\\mathcal{S}$ and $\\mathcal{A}$ separately denotes observa-"}, {"title": "4 Problem Formulation (Transformation of Behavior Cloning).", "content": "The majority imitation learning (IL) approaches utilize a shaped reward/Q function to optimize policy within the RL framework. This paradigm may introduce cumulative bias when the shaped reward/Q function does not accurately represent the ground truth reward/Q function. To address this limitation, we propose a one-step IL algorithm that leverages estimated behavior density to optimize the empirical policy using a density-weighted behavior cloning objective. Our objective is rigorously derived through mathematical formulation. We begin by defining the basic notations and providing the following mathematical derivations.\nNotations. In this section, we define the meanings of the symbols we utilized below. Specifically, $P^*(a|s)$ denotes the expert behavior density (The conception of behavior density is proposed by Wu et al. (2022)), representing the density probability of the given action $a$ within the expert action support, and $P(a|s)$ denotes the sub-optimal behavior density. $\\pi_\\theta(a|s)$ denotes the training policy, $D_{KL}$ denotes Kullback-Leibler (KL) divergence.\nPolicy Distillation via KL divergence. Rusu et al. (2016) has implied the effectiveness of policy distillation via minimizing the KL divergence between training policy $\\pi_\\theta$ and the likelihood of teacher policy set $\\pi_i \\in \\Pi$, i.e.\n$\\pi := \\arg \\min_{\\pi_\\theta} D_{KL} [\\pi_\\theta || \\pi_i] \\pi_i \\in \\Pi$. (5)\nIntuitively, if we can acquire a good $P^*(a|s)$, we can directly utilize policy distillation i.e. Equation 5 to match the distribution of expert behavior i.e.\n$\\pi := \\arg \\min_{\\pi_\\theta} D_{KL}[\\pi_\\theta || P^*]$. (6)\nhowever, the limited demonstrations are insufficient to be directly used to estimate an expert behavior density that encompasses the entire expert action support, thus it's accordingly insufficient to mimic the expert behavior by minimizing the KL divergence between $\\pi_\\theta(a|s)$ and the estimated expert behavior density. To address this limitation, we will propose Adversarial Density Estimation (ADE) in the section methods. In the following sections, our derivations and analysis are totally based on the assumption that we have a well estimated expert and sub-optimal behavior density.\nIt's beneficial to be divergent from sub-optimal density either. In addition to aligning $\\pi_\\theta(a|s)$ with $P^*(a|s)$, we also steer $\\pi_\\theta(a|s)$ away from $P(a|s)$ as formulated in Equation 7. We define this objective as Adversarial Policy Divergence in Definition 4. The advantage of Adversarial Policy Divergence over Equation 6 lies in that Equation 7 can utilize the sub-optimal behavior density to supplement the policy learning, thereby robustly matching the distribution of expert behavior. As shown in Figure 1 (c), our ablation validates the effectiveness of Adversarial Policy Divergence. Definition (Adversarial Policy Divergence) Given expert behavior denisty $P^*(a|s)$ and sub-optimal behavior density $P(a|s)$, we formulate the process of Adversarial Policy Divergence, where $\\pi_\\theta$ approaches the expert behavior while diverging from the sub-optimal behavior, as follows:\n$\\min J(\\pi_\\theta) = \\min D_{KL} [\\pi_\\theta || P^*] - D_{KL} [\\pi_\\theta || P]$,"}, {"title": "5 Methods", "content": "In the section Problem Formulation, we augment and transform the BC-based LfD objective to a density weighted behavior cloning objective. However, it's challengeable to directly estimate the expert density particularly if the expert state action pairs aren't sufficient to cover entire expert action support. Therefore, we introduce Adversarial Density Estimation (ADE), which utilizes adversarial learning (AL) by treating sub-optimal state action pari as negative cases to enhance the estimation of target sample density, thereby robustly matching the expert distribution."}, {"title": "6 Practical Implementation", "content": "As depicted in Algorithm 1, ADR-BC comprises VAE pre-training and policy training stages. During the VAE pre-training stage, we utilize VQ-VAE to separately estimate the target density $P^*(a|s)$ and the suboptimal density $P(a|s)$ by minimizing Equation 4 (or Equation 12) and the VQ loss (van den Oord et al., 2018). During the policy training stage, we optimize the Multiple Layer Perception (MLP) policy $\\pi_\\theta$ by using Equation 7. For more detials about our hyperparameter, please"}, {"title": "7 Evaluation", "content": "Our experiments are designed to validate the following questions: 1) Does ADR-BC outperform reward or Q function shaping IL approaches? 2) Is it necessary to use an adversarial approach to assist in estimating the target density? 3) Is it necessary to use the density-weighted form to optimize the policy? In the following sections, we first introduce our experimental settings, datasets, and baselines, then answer the aforementioned questions through experiments and analysis.\nExperimental Settings The majority of our experimental setups are centered around Learning from Demonstration (LfD). For convenience, we denote using n demonstrations to conduct experiments under the LfD setting as LfD (n). To address 1), we compare ADR-BC with various reward/Q function shaping IL approaches in the Gym-Mujoco domain. Additionally, we compare IQL with various efficient reward shaping approaches and competitive offline RL algorithm with ground truth rewards in the Kitchen and Androit domains to further demonstrate the advantages of ADR-BC over reward shaping methods. In terms of questions 2) and 3), we will conduct extensive ablation studies to provide thorough answers.\nDatasets. We test our method on various domains, including Gym-Mujoco, Androit, and Kitchen domains (Fu et al., 2021). Specifically, the datasets from the Gym-Mujoco domain include medium, medium-replay, and medium-expert collected from environments including Ant, Hopper, Walker2d, and HalfCheetah, and the demonstrations are 5 expert trails from the respective environments. For the kitchen and androits domains, we rank and sort all trials by their accumulated return, and sample the single trial with the highest return as the demonstration.\nBaselines. When assessing the Gym-Mujoco domain, our study encompasses several IL algo-rithms as baseline references: ORIL (Zolna et al., 2020), SQIL (Reddy et al., 2019), IQ-Learn (Garg"}, {"title": "8 Conclusion", "content": "This paper proposes ADR-BC, which robustly matches the expert distribution, thereby improving the performance of behavior cloning. Additionally, ADR-BC avoids the cumulative errors typically introduced by traditional imitation learning paradigms when optimizing policies within RL frameworks. Meanwhile, experimental results have shown that ADR-BC achieves the best performance on all tasks in the LfD setting across the Gym-Mujoco, Adroit, and Kitchen domains. Therfore, this research will further advance the development of IL paradigms centered on BC.\nLimitations and future work. ADR-BC is an action-support based approach, and thus it cannot be applied in LfO settings. In the future, we will explore a modified version of ADR-BC that can be used in non-Markovian settings."}, {"title": "Appendix A. Social Impacts", "content": "We propose a new imitation learning framework, ADR-BC, which can effectively improve the performance of behavioral cloning under LfD Settings. In this paper, we point out that the advantage of ADR-BC comes from the fact that behavioral cloning can effectively avoid the cumulative error caused by the reinforcement learning algorithm when the reward function is updated without being accurately modeled. In addition, the effect of ADR-BC exceeds all previous imitation learning frameworks, and even achieves better performance than IQL on robotic arm /kitchen tasks, which will greatly promote the development of imitation learning frameworks under supervised learning."}, {"title": "Appendix B. Experimental results of baselines", "content": "Our baselines on Gym-Mujoco domain mainly includes: ORIL (Zolna et al., 2020), SQIL (Reddy et al., 2019), IQ-Learn (Garg et al., 2022), ValueDICE (Kostrikov et al., 2019), DemoDICE (Kim et al., 2022), SMODICE (Ma et al., 2022), and CEIL (Liu et al., 2023a). The majority experimental results of these baselines are cited from CEIL (Liu et al., 2023a).\nIn terms of evaluation on kitchen or androits domains. The majority baselines include OTR (Luo et al., 2023) and CLUE (Liu et al., 2023b) that utilizing reward shaping via IL approaches, and policy optimization via Implicit Q Learning (IQL) (Kostrikov et al., 2021). We also encompass Conservative Q Learning (CQL) (Kumar et al., 2020) and IQL for comparison. Specifically, these experimental results are from:\n\u2022 The experiment results of OTR and CLUE are directly cited from Luo et al. and Liu et al.\n\u2022 The experimental results of CQL (oracle) and IQL (oracle) are separately cited from Kumar et al. and Kostrikov et al., and the experimental results of OTR on kitchen domain is obtained by running the official codebase"}, {"title": "Appendix C. Hyper parameters and Implenmentation details", "content": "Our method is slightly dependent on hyper-parameters. We introduce the core hyperparameters here:\nOur code is based on CORL (Tarasov et al., 2022). Specifically, in terms of training framework, we adapted the offline training framework of Supported Policy Optimization (SPOT) (Wu et al., 2022), decomposing it into multiple modules and modifying it to implement our algorithm. Regarding the model architecture, we implemented the VQVAE ourselves, while the MLP policy architecture is based on CORL. Some general details such as warm-up, discount of lr, e.g. are implemented by CORL. We have appended our source code in the supplement materials."}, {"title": "Appendix D. Evaluation Details", "content": "We run each task multiple times, recording all evaluated results and taking the highest score from each run as the outcome. We then average these highest scores. For score computation, we use the same metric as D4RL i.e. $\\frac{output-random}{expert-random} \\times 100$. Our experiment are running on computing clusters with 16\u00d74 core cpu (Intel(R) Xeon(R) CPU E5-2637 v4 @ 3.50GHz), and 16\u00d7RTX2080 Ti GPUs"}, {"title": "Appendix E. Scaled Experiments", "content": "Training stability of ADR-BC. Despite behavior cloning not being theoretically monotonic, we still present the training curve of ADR-BC. As shown in Figure 3 and Figure 4, we averaged multiple runs and plotted the training curve, demonstrating that ADR-BC exhibits stable training performance."}]}