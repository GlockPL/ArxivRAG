{"title": "UNDERSTANDING DIFFICULT-TO-LEARN EXAMPLES IN\nCONTRASTIVE LEARNING: A THEORETICAL FRAMEWORK FOR\nSPECTRAL CONTRASTIVE LEARNING", "authors": ["Yi-Ge Zhang", "Jingyi Cui", "Qiran Li", "Yisen Wang"], "abstract": "Unsupervised contrastive learning has shown significant performance improvements in recent years,\noften approaching or even rivaling supervised learning in various tasks. However, its learning\nmechanism is fundamentally different from that of supervised learning. Previous works have shown\nthat difficult-to-learn examples (well-recognized in supervised learning as examples around the\ndecision boundary), which are essential in supervised learning, contribute minimally in unsupervised\nsettings. In this paper, perhaps surprisingly, we find that the direct removal of difficult-to-learn\nexamples, although reduces the sample size, can boost the downstream classification performance\nof contrastive learning. To uncover the reasons behind this, we develop a theoretical framework\nmodeling the similarity between different pairs of samples. Guided by this theoretical framework,\nwe conduct a thorough theoretical analysis revealing that the presence of difficult-to-learn examples\nnegatively affects the generalization of contrastive learning. Furthermore, we demonstrate that the\nremoval of these examples, and techniques such as margin tuning and temperature scaling can enhance\nits generalization bounds, thereby improving performance. Empirically, we propose a simple and\nefficient mechanism for selecting difficult-to-learn examples and validate the effectiveness of the\naforementioned methods, which substantiates the reliability of our proposed theoretical framework.", "sections": [{"title": "Introduction", "content": "Contrastive learning has demonstrated exceptional empirical performance in the realm of unsupervised representation\nlearning, effectively learning high-quality representations of high-dimensional data using substantial volumes of\nunlabeled data by aligning an anchor point with its augmented views in the embedding space [6; 7; 16; 9; 5; 31].\nUnsupervised contrastive learning may own quite different working mechanisms from supervised learning, as discussed\nin [20]. For example, difficult-to-learn examples (a well-recognized concept in supervised learning as examples\naround the decision boundary [34]), which contribute the most to supervised learning, contribute the least or even\nnegatively to contrastive learning performance. They show that on image datasets such as CIFAR-100 and STL-10,\nexcluding 20%-40% of the examples does not negatively impact downstream task performance. More surprisingly, their\nresults showed, but somehow failed to notice, that excluding these samples on certain datasets like STL-10 can lead to\nperformance improvements in downstream tasks.\nTaking a step further beyond their study, we find that this surprising\nresult is not just a specialty of a certain dataset, but a universal\nphenomenon across multiple datasets. Specifically, we run SimCLR\non the original CIFAR-10, CIFAR-100, STL-10, and TinyImagenet\ndatasets, the SAS core subsets [20] selected with a deliberately tuned\nsize, and a subset selected by a sample removal mechanism to be\nproposed in this paper. In Figure 1, we report the gains of linear\nprobing accuracy by using the subsets compared with the original\ndatasets. We see that on all these benchmark datasets, excluding a"}, {"title": "Difficult-to-learn Examples Hurt Contrastive Learning: A Mixing Image Experiment", "content": "We start this section by revealing that difficult-to-learn examples do hurt contrastive learning performances through a\nproof-of-concept toy experiment.\nThe concept of difficult-to-learn examples is borrowed from supervised learning,\ndenoting the examples around the decision boundary. It is somewhat related to\nhard negative samples, a pure unsupervised learning concept defined as highly\nsimilar negative samples to the anchor point, but is different in nature. (See\nAppendix A.1 for more discussions.) However, in real image datasets, as difficult-\nto-learn examples rely on the specific classifier trained in the supervised learning\nmanner, we can not preciously know the ground truth difficult-to-learn examples.\nTherefore, we in turn add additional difficult-to-learn examples and observe the\neffects of these examples.\nSpecifically, we generate a new mixing-image dataset containing more difficult-\nto-learn samples by mixing a y fraction of images on the CIFAR-10 dataset at\nthe pixel level (these samples lying around the class boundary), termed as \u03b3-\nMixed CIFAR-10 datasets. Then, we train the representative contrastive learning\nalgorithm SimCLR [6] on the original, 10%-, and 20%-Mixed CIFAR-10 datasets using ResNet18 model. We report\nthe linear probing accuracy in Figure 2.\nCompared with the model trained on the original dataset, we find that with the mixed difficult-to-learn examples\nincluded in the training dataset, the performance of contrastive learning drops. This result indicates that the (mixed)"}, {"title": "Theoretical Characterization of Why Difficult-to-Learn Examples Hurt Contrastive\nLearning", "content": "In this section, to explain why difficult-to-learn examples negatively impact the performance of contrastive learning, we\nprovide theoretical evidence on generalization bounds. In Section 3.1 we present the necessary preliminaries that lay\nthe foundation for our theoretical analysis. In Section 3.2, we introduce the similarity graph describing difficult-to-learn\nexamples. In Section 3.3, we respectively derive error bounds of contrastive learning with and without difficult-to-learn\nexamples."}, {"title": "Preliminaries", "content": "Notations. Given a natural data \u017e\u2208 X := Rd, we denote the distribution of its augmentations by A(\u00b7|\u012b) and the set\nof all augmented data by X, which is assumed to be finite but exponentially large. For mathematical simplicity, we\nassume class-balanced data with n denoting the number of augmented samples per class and r + 1 denoting the number\nof classes, hence |X| = n(r + 1). Let na represent the number of difficult-to-learn examples per class and Da the set of\ndifficult-to-learn examples.\nSimilarity Graph (Augmentation Graph). As described in [15], an augmentation graph G represents the distribution\nof augmented samples, where the edge weight Wxx' signifies the joint probability of generating augmented views x\nand x' from the same natural data, i.e., wxx' := Ex ~ P[A(x|x)A(x'|x)]. The total probability across all pairs of\naugmented data sums up to 1, i.e., \u2211x,x'\u2208x Wxx' = 1. The adjacency matrix of the augmentation graph is denoted\nas A = (Wxx')x,x'\u2208x, and the normalized adjacency matrix is \u0100 = D-1/2 AD-1/2, where D := diag(wx)x\u2208x, and\nWx := \u2211x'\u2208x Wxx'. The concept of augmentation graph is further extended to describe similarities beyond image\naugmentation, such as cross-domain images [30], multi-modal data [37], and labeled examples [11].\nContrastive losses. For theoretical analysis, we consider the spectral contrastive loss L(f) proposed by [15] as a good\nperformance proxy for the widely used InfoNCE loss\n$L_{Spec}(x; f) := -2 \\cdot E_{z,z^+} [f(x) \\cdot f(x^+)] + E_{x,x'} [(f(x)^\\top f(x'))^2].$\nAs proved in [19], the spectral contrastive loss and the InfoNCE loss share the same population minimum with variant\nkernel derivations. Further, the spectral contrastive loss is theoretically shown to be equivalent to the matrix factorization\nloss. For F\u2208 Rnxk = (ux)x \u2208 X, where ux\n=\nwx/2 f(x), the matrix factorization loss is:\n$L_{mf}(F) := ||\\bar{A} - FF^T ||_F^2 = L_{Spec}(f) + const.$"}, {"title": "Modeling of Difficult-to-Learn Examples", "content": "We start by introducing a similarity graph, to describe the relationships between various samples. In contrastive learning,\nexamples are used in a pairwise manner, so we define difficult-to-learn sample pairs as sample pairs that include\nat least one difficult-to-learn sample. As difficult-to-learn examples lie around the decision boundary, they should\nhave higher augmentation similarity to examples from different classes. Therefore, it is natural for us to define the\ndifficulty-to-learn pairs as different-class sample pairs with higher similarity. Correspondingly, easy-to-learn pairs are\ndefined as different-class sample pairs containing no difficult-to-learn samples, or different-class sample pairs with\nlower similarity.\nSpecifically, we define the augmentation similarity between a sample and itself as 1. Then we assume the similarity\nbetween same-class samples is \u03b1 (Figure 3(a)), the similarity between a sample (conceptually far away from the class\nboundary) and all samples from other classes is \u03b2 (Figure 3(b)), and the similarity between different-class difficult-to-\nlearn samples (conceptually close to the class boundary) is \u03b3 (Figure 3(c)). Naturally, we have 0 < \u03b2 < \u03b3 < \u03b1 < 1."}, {"title": "Error Bounds with and without Difficult-to-Learn Examples", "content": "Based on the similarity graph in Section 3.2, we derive the linear probing error bounds for contrastive learning models\ntrained with and without difficult-to-learn examples in Theorems 3.1 and 3.2. We mention that we adopt the label\nrecoverability (with labeling error \u03b4) and realizability assumptions from [15]. The formal assumptions and proofs are\nshown in Appendix B.1.\nTheorem 3.1 (Error Bound without difficult-to-learn Examples). Denote Ew.o. as the linear probing error of a\ncontrastive learning model trained on a dataset without difficult-to-learn examples. Then\n$E_{w.o.} \\leq \\frac{4\\delta}{\\sqrt{1 - \\frac{1 - \\alpha}{(1-\\alpha)+n\\alpha+nr\\beta}}}} + 8\\delta.$\nTheorem 3.2 (Error Bound with difficult-to-learn Examples). Denote Ew.d. as the linear probing error of a contrastive\nlearning model trained on a dataset with na difficult-to-learn examples per class. Then if na \u2264 k \u2264 na + r + 1, there\nholds\n$E_{w.d.} \\leq \\frac{4\\delta}{\\sqrt{1 - \\frac{1 - \\alpha}{(1-\\alpha)+n\\alpha+nr\\beta +n_ar(\\gamma-\\beta) }}}} + 8\\delta.$\nDiscussions. By comparing Theorems 3.1 and 3.2, also considering that $\\frac{(1-\\alpha)+r(\\gamma-\\beta)}{(1-\\alpha)+n\\alpha+nr\\beta+n_ar(\\gamma-\\beta)}}$>$\\frac{1 - \\alpha}{(1-\\alpha)+n\\alpha+nr\\beta}$,\nwe see the presence of difficult-to-learn examples leads to a strictly worse linear probing error bound for a contrastive\nlearning model. Moreover, more challenging difficult-to-learn examples (larger \u03b3 \u2013 \u03b2) result in worse error bounds.\nSpecifically, when \u03b3\n=\n\u03b2, i.e. no difficult-to-learn examples exist, the bound in Theorem 3.2 reduces to that in Theorem\n3.1."}, {"title": "Theoretical Characterization on How to Eliminate Effects of Difficult-to-Learn Examples", "content": "Building on the above unified theoretical framework, we theoretically analyze that directly removing difficult-to-learn\nsamples (Section 4.1), margin tuning (Section 4.2), and temperature scaling (Section 4.3) can handle difficult-to-learn\nexamples by improving the generalization bounds in different ways."}, {"title": "Removing Difficult-to-Learn Samples", "content": "In Figures 1 and 2, empirical experiments demonstrated that removing difficult-to-learn samples can improve learning\nperformance. Corollary 4.1 provides a theoretical explanation for this counter-intuitive phenomenon based on our\nestablished framework.\nCorollary 4.1. Denote ER as the linear probing error of a contrastive learning model trained on a selected subset\nremoving all difficult-to-learn examples Da. Then there holds\n$E_R \\leq \\frac{4\\alpha}{1 - \\sqrt{\\frac{1 - \\alpha}{(1-\\alpha)+(n-n_a)\\alpha+(n-n_a)r\\beta}}}} + 8\\delta.$\nCorollary 4.1 shows that when the difficult-to-learn examples are removed, the linear probing error bound has the\nsame form as the case where no difficult-to-learn examples are present (Theorem 3.1), but with n replaced by n \u2013 na.\nCompared with the case without removing difficult-to-learn examples (Theorem 3.2), the bound in equation 5 is smaller\nthan that in equation 4 when $\\gamma \\beta> \\frac{n_d(1-\\alpha)(\\alpha+r\\gamma)}{r[(1-\\alpha)+(n-n_a)(\\alpha+r\\beta)]}$. This indicates that removing difficult-to-learn examples\nenhances the error bound when these samples are significantly harder than the easy ones (i.e., large \u03b3 \u2013 \u03b2) or when the\nnumber of difficult-to-learn samples is small (i.e., small na)."}, {"title": "Margin Tuning", "content": "Margin tuning is useful in contrastive learning as highlighted in [38]. Here, we delve into how margin tuning can\nenhance the generalization in the presence of difficult-to-learn examples.\nTheorem 4.2. The margin tuning loss is equivalent to the matrix factorization loss\n$L_{mf-M}(F) := ||(\\bar{A} - M) - FF^T ||_F^2,$\nwhere A is the normalized adjacency matrix, and M is the normalized margin matrix.\nTheorem 4.2 indicates that adjusting margins alters the similarity graph by subtracting a normalized margin matrix\nM from the normalized similarity matrix A. Intuitively, by subtracting the additional similarity values of difficult-to-\nlearn examples with appropriately chosen margins, the remaining values will match those of easy-to-learn examples.\nSpecifically, in the following Theorem 4.3, we show that properly chosen margins can eliminate the negative impact of\ndifficult-to-learn examples.\nTheorem 4.3. Denote EM as the linear probing error for the margin tuning loss equation 31 trained on a dataset with\ndifficult-to-learn samples Da. If we let\n$m_{x,x'} = C_0/(c_1c_2) \\cdot (\\gamma - \\beta)$\nfor y(x) \u2260 y(x'),x,x' \u2208 Da, where co := (1 \u2212 a) + na + (n \u2212 na)r\u03b2, c\u2081 := (1 \u2212 a) + na + nr\u03b2 + nar(y \u2212 \u03b2)\nand c2 := (1 \u2212 a) + na + nr\u03b2, and mx,x' = 0 for x, x' \u2209 Da, then we have\n$E_M = E_{w.o.}.$\nNote that when n is large enough, mx,x' for x or x' \u2209 Da are higher-order infinitesimals relative to equation 7, and\nprimarily affect normalization rather than the core problem. Thus, we focus on cases where x, x' \u2208 Da and defer\nspecific forms of other mx,x' values to the proofs for brevity.\nTheorem 4.3 shows that with appropriately chosen margins, the linear probing error bound for the margin tuning loss in\nthe presence of difficult-to-learn examples becomes equivalent to the standard contrastive loss without such examples,\nas indicated in Theorem 3.1. Since equation 7 > 0, this suggests applying a positive margin to the difficult-to-learn\nexample pairs. Additionally, the more challenging the example pairs are (i.e., the larger \u03b3 \u2013 \u03b2), the greater the margin\nvalue should be."}, {"title": "Temperature Scaling", "content": "Temperature scaling is a well-validated technique in various contrastive learning tasks [22; 36; 25]. Here, we investigate\nhow temperature scaling can enhance generalization, particularly in the presence of difficult-to-learn examples.\nTheorem 4.4. The temperature scaling loss is equivalent to the matrix factorization loss\n$L_{mf-T}(F) := ||T \\bar{A} - FF^T ||_{wF}^2,$\nwhere A is the normalized adjacency matrix of similarity graph, T\u2299 A is the element-wise product of matrices T and\nA, and || ||wF is the weighted Frobenius norm (specified in the proof)."}, {"title": "Removing Difficult-to-Learn Samples", "content": "In Figures 1 and 2, empirical experiments demonstrated that removing difficult-to-learn samples can improve learning\nperformance. Corollary 4.1 provides a theoretical explanation for this counter-intuitive phenomenon based on our\nestablished framework.\nCorollary 4.1. Denote ER as the linear probing error of a contrastive learning model trained on a selected subset\nremoving all difficult-to-learn examples Da. Then there holds\n$E_R \\leq \\frac{4\\delta}{\\sqrt{1 - \\frac{1 - \\alpha}{(1-\\alpha)+(n-n_a)\\alpha+(n-n_a)r\\beta}}}} + 8\\delta.$\nCorollary 4.1 shows that when the difficult-to-learn examples are removed, the linear probing error bound has the\nsame form as the case where no difficult-to-learn examples are present (Theorem 3.1), but with n replaced by n \u2013 na.\nCompared with the case without removing difficult-to-learn examples (Theorem 3.2), the bound in equation 5 is smaller\nthan that in equation 4 when $\\gamma \\beta> \\frac{n_d(1-\\alpha)(\\alpha+r\\gamma)}{r[(1-\\alpha)+(n-n_a)(\\alpha+r\\beta)]}$. This indicates that removing difficult-to-learn examples\nenhances the error bound when these samples are significantly harder than the easy ones (i.e., large \u03b3 \u2013 \u03b2) or when the\nnumber of difficult-to-learn samples is small (i.e., small na)."}, {"title": "Margin Tuning", "content": "Margin tuning is useful in contrastive learning as highlighted in [38]. Here, we delve into how margin tuning can\nenhance the generalization in the presence of difficult-to-learn examples.\nTheorem 4.2. The margin tuning loss is equivalent to the matrix factorization loss\n$L_{mf-M}(F) := ||(\\bar{A} - M) - FF^T ||_F^2,$\nwhere A is the normalized adjacency matrix, and M is the normalized margin matrix.\nTheorem 4.2 indicates that adjusting margins alters the similarity graph by subtracting a normalized margin matrix\nM from the normalized similarity matrix A. Intuitively, by subtracting the additional similarity values of difficult-to-\nlearn examples with appropriately chosen margins, the remaining values will match those of easy-to-learn examples.\nSpecifically, in the following Theorem 4.3, we show that properly chosen margins can eliminate the negative impact of\ndifficult-to-learn examples.\nTheorem 4.3. Denote EM as the linear probing error for the margin tuning loss equation 31 trained on a dataset with\ndifficult-to-learn samples Da. If we let\n$m_{x,x'} = C_0/(c_1c_2) \\cdot (\\gamma - \\beta)$\nfor y(x) \u2260 y(x'),x,x' \u2208 Da, where co := (1 \u2212 a) + na + (n \u2212 na)r\u03b2, c\u2081 := (1 \u2212 a) + na + nr\u03b2 + nar(y \u2212 \u03b2)\nand c2 := (1 \u2212 a) + na + nr\u03b2, and mx,x' = 0 for x, x' \u2209 Da, then we have\n$E_M = E_{w.o.}.$\nNote that when n is large enough, mx,x' for x or x' \u2209 Da are higher-order infinitesimals relative to equation 7, and\nprimarily affect normalization rather than the core problem. Thus, we focus on cases where x, x' \u2208 Da and defer\nspecific forms of other mx,x' values to the proofs for brevity.\nTheorem 4.3 shows that with appropriately chosen margins, the linear probing error bound for the margin tuning loss in\nthe presence of difficult-to-learn examples becomes equivalent to the standard contrastive loss without such examples,\nas indicated in Theorem 3.1. Since equation 7 > 0, this suggests applying a positive margin to the difficult-to-learn\nexample pairs. Additionally, the more challenging the example pairs are (i.e., the larger \u03b3 \u2013 \u03b2), the greater the margin\nvalue should be."}, {"title": "Temperature Scaling", "content": "Temperature scaling is a well-validated technique in various contrastive learning tasks [22; 36; 25]. Here, we investigate\nhow temperature scaling can enhance generalization, particularly in the presence of difficult-to-learn examples.\nTheorem 4.4. The temperature scaling loss is equivalent to the matrix factorization loss\n$L_{mf-T}(F) := ||T \\bar{A} - FF^T ||_{wF}^2,$\nwhere A is the normalized adjacency matrix of similarity graph, T\u2299 A is the element-wise product of matrices T and\nA, and || ||wF is the weighted Frobenius norm (specified in the proof)."}, {"title": "Verification Experiments", "content": "This paper primarily focuses on theoretical analysis, explaining how different samples in contrastive learning impact\ngeneralization. The experiments in this part are mainly designed to validate the theoretical insights and demonstrate\nthat the proposed directions for improving performance are sound. The experiments are not intended to achieve\nstate-of-the-art results but rather to confirm the correctness of our theoretical findings. We hope that readers will\nappreciate the theoretical contributions of this work and not focus excessively on the experimental results.\nIn Section 5.1, we present a straightforward and efficient mechanism for selecting difficult-to-learn samples. We\nsubsequently conduct a comprehensive evaluation of various methods, including the removal of difficult-to-learn\nsamples (Section 5.2), margin tuning (Section 5.3), and temperature scaling (Section 5.4), all of which are theoretically\nestablished to mitigate the impact of these difficult-to-learn examples. In Section 5.5, we propose an extended method\nthat combines margin tuning and temperature scaling, and discuss the scalability under different paradigms and the\nconnection between difficult-to-learn samples and long-tail distribution. The specific loss forms and algorithms can be\nfound in Appendix A.2."}, {"title": "Difficult-to-Learn Examples Selection", "content": "Based on the preceding analysis, we have established that difficult-to-learn samples play a crucial role in enhancing\nthe generalization of contrastive learning. In this section, we aim to develop a straightforward and efficient selection\nmechanism to validate our theoretical analysis, which avoids additional pretrained models and extra costs [20].\nTo identify difficult-to-learn sample pairs-those from different classes but with high similarity-we compute the\ncosine similarity of each sample to other samples in the same batch using features before projector mapping. We\ndefine posHigh and posLow as percentiles of the similarity sorted in descending order, where SimposHigh and\nSimposLow are the corresponding similarities. Generally, following the characterization in Section 3.2 and Appendix\nB, we can roughly assume pos High corresponds to 1/(r + 1), where r + 1 is the class number\u00b9. Sample pairs with\ncosine similarities above SimposHigh are considered from the same class. Sample pairs with the similarity between\nSimposHigh and SimposLow are considered as difficult-to-learn examples. Sample pairs with cosine similarities below\nSimposLow are considered as easy-to-learn samples from different classes. Here for pos Low, we note that when\noptimizing \u03b3 of difficult-to-learn examples, if some easy-to-learn samples are involved, the process will also optimize \u03b2,"}, {"title": "Removing Difficult-to-Learn Samples", "content": "We here introduce a simple and practical method for removing difficult-to-learn samples based on our proposed\nselection mechanism. Eliminating the impact of difficult-to-learn samples means preventing sample pairs that include\ndifficult-to-learn samples from interfering with the training process. To achieve this, we use the selection matrix P to\nidentify and remove difficult-to-learn samples."}, {"title": "Margin Tuning on Difficlut-to-Learn Samples", "content": "To effectively apply margin tuning in line with our theoretical analysis, we adopt a margin tuning factor \u03c3 > 0. For the\nselected difficult-to-learn sample pairs identified by the selection matrix P, we add a margin \u03c3 to the similarity values,\nand for the unselected pairs, we use the original InfoNCE."}, {"title": "Temperature Scaling on Difficlut-to-Learn Samples", "content": "We define the temperature scaling factor p > 0. Given the base temperature \u0442 > 0, we attach temperature pr to the\nselected difficult-to-learn sample pairs identified by the selection matrix P, whereas attach base temperature \u03c4 to the\nunselected pairs."}, {"title": "Extensions", "content": "Combined method. From Sections 4.2 and 4.3, we observe that margin tuning and temperature scaling eliminate the\neffects of difficult-to-learn examples in different ways. Therefore, it is natural to combine the two methods, and see if\nthe combined method could reach better performances."}, {"title": "Conclusion", "content": "In this paper, we construct a theoretical framework to specifically analyze the impact of difficult-to-learn examples on\ncontrastive learning. We prove that difficult-to-learn examples hurt the performance of contrastive learning from the\nperspective of linear probing error bounds. We further demonstrate how techniques such as margin tuning, temperature\nscaling, and the removal of these examples from the dataset can improve performance from the perspective of enhancing\nthe generalization bounds. The detailed experimental results demonstrate the reliability of our theoretical analysis."}, {"title": "Appendix", "content": ""}, {"title": "Related Works", "content": "Self-supervised contrastive learning. Self-supervised contrastive learning [6; 7; 16; 9] aims to learn an encoder\nthat maps augmentations (e.g. flips, random crops, etc.) of the same input to proximate features, while ensuring that\naugmentations of distinct inputs yield divergent features. The encoder, once pre-trained, is later fined-tuned on a\nspecific downstream dataset. The effectiveness of contrastive learning methods are typically evaluated through the\nperformances of the downstream tasks such as linear classification. Depending on the reliance of negative samples,\ncontrastive learning methods can be broadly categorized into two kinds. The first kind [6; 7; 16] learns the encoder\nby aligning an anchor point with its augmented versions (positive samples) while at the same time explicitly pushing\naway the others (negative samples). On the other hand, the second kind do not depend on negative samples. They often\nnecessitate additional components like projectors [14], stop-gradient techniques [8], or high-dimensional embeddings\n[35]. Nevertheless, the first kind of methods continue to be the mainstream in self-supervised contrastive learning and\nhave been expanded into numerous other domains [23; 1; 26]. The analysis and discussions of this paper focus mainly\non the first kind of contrastive learning methods that relies on both positive and negative samples.\nContrastive Learning Theory. The early studies of theoretical aspects of contrastive learning manage to link contrastive\nlearning to the supervised downstream classification. [2] proves that representations learned by contrastive learning\nalgorithms can achieve small errors in the downstream linear classification task. [27; 3; 4] incorporate the effect of\nnegative samples and further extend surrogate bounds. Later on, [15] focuses on the unsupervised nature of contrastive\nlearning by modeling the feature similarities between augmented samples and provides generalization guarantee for\nlinear evaluation through borrowing mathematical tools from spectral clustering. The idea of modeling similarities is\nlater extended to analyzing contrastive learning for unsupervised domain adaption [30] and weakly supervised learning\n[11]. In a similar vein, [33] put forward the idea of augmentation overlap to explain the alignment of positive samples.\nBesides, contrastive learning is also interpreted through various other theoretical frameworks in unsupervised learning,\nsuch as nonlinear independent component analysis [39], neighborhood component analysis [24], stochastic neighbor\nembedding [17], geometric analysis of embedding spaces [18], and message passing techniques [32]. In this paper, our\nbasic assumptions are based on [15] and focus on modeling the similarities between difficult-to-learn example pairs.\nDifference between difficult-to-learn examples and hard negative samples. Difficult-to-learn examples and hard\nnegative samples both significantly affect the performance of self-supervised learning. However, while difficult-to-learn\nexamples are associated with the classification boundary, hard negative samples [28; 21] are defined in relation to the\nanchor point. Previous research on hard negative sampling typically modifies contrastive learning models to emphasize\nthese challenging samples so as to achieve better performance. In contrast, our findings indicate that unmodified\ncontrastive learning models experience performance degradation due to the existence of difficult-to-learn samples. Aside\nfrom ad hoc modifications, a straightforward removal of these difficult-to-learn samples can also boost performance.\nAs a systematic explanation of this finding is lacking, we establish a unified theoretical framework that addresses this\nchallenge."}, {"title": "Loss Functions of Sample Removal, Margin Tuning, and Temperature Scaling", "content": "Based on the sample selection matrix P defined in equation 12, we adapt the InfoNCE loss into versions of sample\nremoval, margin tuning, and temperature scaling, respectively.\nSample Removal. We define the removal loss as follows:\n$l_r(i, j) := -log \\frac{exp ((s_{i,j}(1 - P_{i,j}))/\\tau)}{\\sum_{k=1}^{2N} 1[k \\neq i] exp ((s_{i,k}(1 - P_{i,k}))/\\tau)},$\nwhere si,j denotes the similarity between augmented instances xi and xj. If Pi,j = 0, the sample pair xi and xj does\nnot include difficult-to-learn samples, so (si,j(1 \u2013 Pi,j))/T = si,j/T, retaining the original form of the InfoNCE loss.\nIf pi,j = 1, the sample pair xi and xj are difficult-to-learn pairs, so (si,j (1 \u2013 Pi,j))/\u0442 = 0, effectively removing them.\nMargin Tuning. We start with the basic form of the widely used InfoNCE loss and define the margin tuning loss for\neach positive pair. Specifically, within each minibatch of size N, we generate 2N samples through data augmentation.\nGiven the margin tuning factor \u03c3 > 0, for an anchor sample xi and its corresponding positive sample xj, we define the\nmargin tuning loss as follows:\n$l_m(i, j) := -log \\frac{exp ((s_{i,j} + P_{i,j}\\sigma)/\\tau)}{\\sum_{k=1}^{2N} 1[k \\neq i] exp ((s_{i,k} + P_{i,k}\\sigma)/\\tau)},$"}, {"title": "Proofs Related to Section 3.3", "content": "Before proceeding", "15": "to derive the error bounds.\nAssumption B.1 (Labels are recoverable from augmentations). Let 1 ~ Px and y(x) be its label. Let the augmentation\nx ~ A(x). We assume that there exists a classifier g that can predict y(x) given x with error at most d", "products": "n$A = (1 - \\alpha)I_{r+1"}, "otimes I_n + (\\alpha - \\beta)I_{r+1} \\otimes (1_n \\cdot 1_n^\\top) + \\beta(1_{r+1}1_{r+1}^\\top) \\otimes (1_n1_n^\\top),$\nwhere Ir+1 and In denote the (r + 1) \u00d7 (r + 1) and n \u00d7 n identity matrices respectively, and 1r+1 := (1, ..., 1) \u2208 Rr+1\nand 1n := (1, . . .,1) \u2208 R denote the all-one vectors.\nFirst, we calculate the eigenvalues and eigenvectors of A. Note that Ir+1 and In have eigenvalues 1 with arbitrary\neigenvectors, 1n 1 has eigenvalue n with eigenvector 1n := //71n and eigenvalues 0 with eigenvectors {\u03bc : \u03bc\u00b91n =\n0}, and 1+1 1+1 has eigenvalue r + 1 with eigenvector 1r+1 :==1r+1 and eigenvalues 0 with eigenvectors\n{\u03bd : \u03bd\u00b91r+1 = 0}. Therefore, A has the following sets of eigenvalues and eigenvectors:\n\u03bb\u2081 = (1 \u2212 a) + n(a \u2013 \u03b2) + n(r + 1)\u03b2, with eigenvector 1r+1 In;\n\u03bb2 = . . . = \u03bbr+1 = (1 \u2212 a) + n(\u03b1 \u2013 \u03b2), with eigenvectors v 1n;\n\u03bbr+2 = ... = \u03bbn+r = 1 \u2212 \u03b1, with eigenvectors 1r+1 u;\n\u03bbn+r+1 = ... = \u03bbn(r+1) = 1 \u2212 \u0430, with eigenvectors u & v.\nNext, we calculate the eigenvalues of \u0100 := D-1/2 AD-1/2. By definition, we have D = diag(W1,..., Wn(r+1)) =\n[(1 \u2212 a) + na + nr\u1e9e"]}