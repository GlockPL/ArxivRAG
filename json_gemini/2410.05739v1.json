{"title": "Array2BR: An End-to-End Noise-immune Binaural Audio Synthesis from Microphone-array Signals", "authors": ["Cheng Chi", "Xiaoyu Li", "Andong Li", "Yuxuan Ke", "Xiaodong Li", "Chengshi Zheng"], "abstract": "Telepresence technology aims to provide an immersive virtual presence for remote conference applications, and it is extremely important to synthesize high-quality binaural audio signals for this aim. Because the ambient noise is often inevitable in practical application scenarios, it is highly desired that binaural audio signals without noise can be obtained from microphone-array signals directly. For this purpose, this paper proposes a new end-to-end noise-immune binaural audio synthesis framework from microphone-array signals, abbreviated as Array2BR, and experimental results show that binaural cues can be correctly mapped and noise can be well suppressed simultaneously using the proposed framework. Compared with existing methods, the proposed method achieved better performance in terms of both objective and subjective metric scores.", "sections": [{"title": "I. INTRODUCTION", "content": "Hybrid meetings, bridging local and remote participants, have emerged as a prevalent form of remote conferencing [1]. In recent years, with the development of VR/AR, hybrid meetings have gained increased attention. In such settings, spatial audio has been instrumental in enhancing the remote participants' experience [2]. Notably, binaural audio provides significant advantages, particularly in mitigating the cocktail party problem: it substantially improves the intelligibility of speech signals, with potential gains of 6-7 dB under controlled experimental conditions [3]. Consequently, for online meetings, binaural audio not only enriches the immersive experience for users but also significantly improves the subjective quality and intelligibility of audio communications.\nDespite its broad application, current hybrid meetings have yet to achieve a sufficient level of immersion [4]. To address this, numerous spatial audio synthesis methods have been developed to enhance the immersive experience [5], [6]. In the context of hybrid meeting scenarios, binaural audio synthesis methods can be classified into four main types. The first type of methods utilizes Ambisonics, enabling binaural reproduction through three key stages: recording, processing, and playback [7], [8]. In recent years, several Ambisonics-based methods employing deep learning have been proposed. [9]-[12]. These deep learning-based methods reduce the reliance on the shape and placement of the array, making them more suitable for practical use in hybrid meetings. The second type of methods records monaural audio signals and synthesizes spatial audio, leveraging additional spatial cues such as the positions of sound sources and listeners [13], [14], or video information [15], [16]. These methods can effectively spatialize audio, enhancing the immersive experience. Nevertheless, the need for supplementary position or video data often makes this method impractical or costly in many conferencing environments.\n[17], [18]. The third type of methods, often implemented with planar arrays, employs a three-stage pipeline incorporating localization, beamforming, and Head-Related Transfer Function (HRTF) filtering, referred to as Localization-Beamforming-HRTF (LBH) method [6], [19]. This method starts with Direction of Arrival (DOA) estimation [20] to determine the sound source's direction, followed by signal extraction and noise reduction using beamforming techniques, and concludes with spatialization through HRTF convolution. This type of methods does not require additional information or specific equipment; the critic is that, it relies heavily on each step, which can compromise accurate spatial perception restoration [21].\nIn recent years, several end-to-end methods have emerged that incorporate the advantages of the previously mentioned approaches while overcoming their limitations. They utilize a microphone array to capture spatial signals and directly convert them into binaural signals [22], [23], which provide dual capability in audio spatial-ization representation and noise reduction, and have proven effective in synthesizing spatial audio signals. Nonetheless, rare literature has been investigated and struggled to achieve competitive performance. Based on the importance of spatial audio in hybrid meetings and the shortcomings of previous methods, we propose Array2BR, a novel framework to convert the signals received by a small scale uniform circular array into the binaural spatial signals.\u00b9 Specifically, we introduce an \u201cencoder-decoder\" structured network that directly maps multichannel signals to binaural signals, requiring no auxiliary"}, {"title": "II. PROBLEM FORMULATION", "content": "In the context of remote meetings, which often involves a local and one or more remote sites, the quality of telepresence can be significantly enhanced by effective audio processing. We consider a scenario where both target speech and background noise are present. An M-element microphone array is employed to capture the speech signals. The observed array signal in the Short-Time Fourier Transform (STFT) domain can be modeled as:\n$X_{f,t} = S_{f,t} + N_{f,t} = c_fS_{f,t} + r_fN_{f,t},$\nwhere ${X_{f,t}, S_{f,t},N_{f,t}} \\in C^{M\\times 1}$ denote the observed array signal, original speech signal and noise signals with frequency index of $f\\in \\{1, ..., F\\}$ and time index of $t \\in \\{1,..., T\\}$. Without loss of generality, the first channel is selected as the reference channel. ${c_f, r_f} \\in C^{M\\times 1}$ denote the relative transfer function (RTF) of the speech and that of noise. And ${S_{f,t}, N_{f,t}} \\in C^{M\\times 1}$ are the complex values of target speech and that of noise in the reference channel.\nThe target binaural signal, which is the desired output $Y_{f,t}$ for an immersive telepresence experience, can be expressed similarly:\n$Y_{f,t} = a_f S_{f,t},$\nwhere $a_f \\in C^{M\\times 1}$ denotes the relative transfer functions of binaural room impulse response (BRIR).\nFor general purpose, to obtain the target signal, it is usually necessary to extract the spatial information from the array signal and then convolve it with a binaural transfer function. In this study, however, we use a neural network that directly maps the received array signal to the target binaural signal, thus achieving an end-to-end process."}, {"title": "III. PROPOSED METHOD", "content": "In this study, an end-to-end network is devised to transform the multi-channel signals recorded by a 6 unit circular microphone array into the binaural spatial signals. The overall diagram of the framework is depicted in Fig. 2, which consists of 4 parts: an encoder, a sequential modeling module, a decoder, and a post-processing module.\nThe encoder module utilizes a U2-net structure [24], [25], which leverages multiple ConvGLU blocks to encode the spatial features from the multi-channel inputs. Each ConvGLU block includes a 2D-GLU layer, an instance normalization layer, and a PRELU [26] activation function, capturing both local and global spatial-spectral correlations. The input to the encoder is a 3-D tensor that combines the spatial and temporal components of the audio signals, ensuring that spatial features can be hierarchically extracted at different scales. The hierarchical nature of the U\u00b2-net facilitates robust feature learning across both spatial and spectral dimensions.\nFollowing the encoder, the sequential modeling module is intro-duced to capture the temporal dependencies in the audio signals. This module is based on the squeezed version of the temporal convolutional module (S-TCM) [27]. The S-TCM comprises several temporal convolutional network (TCN) [28] units, stacked to pro-gressively increase the temporal receptive field. Each S-TCM block encodes long-term dependencies while maintaining computational efficiency. By modeling these long-range dependencies, the network can effectively capture the dynamics of spatial audio, ensuring that the temporal variations of the sound field are well-represented.\nThe decoder module mirrors the encoder in structure, utilizing DeconvGLU blocks to progressively upsample the encoded features. The output of the decoder is a 3-D tensor that represents both spectral and spatial discriminative cues, reconstructed from the compressed feature representation. The upsampling process enables the network to recover fine-grained details necessary for accurate binaural audio reconstruction.\nThe post-processing module is employed to refine the output of the decoder and generate the final binaural signals. This module consists of a stacked LSTM block and two parallel MLP blocks. The stacked LSTM block incorporates a frequency reshaping layer and two LSTM layers. The MLP blocks consist of three fully connected layers with ReLU serving as the nonlinear function corresponding to the left and right channels, respectively.\nThe overall network operations can be formulated as:\n$W_{l,r} =MLP_{left, right} (S-LSTM (DeConvGLU(m_{de})(S-TCM(m_{tcm}) (ConvGLU(m_{en}) (X_{f,t}))))),$\nwhere $m_{en}$, $m_{tcm}$, $m_{de}$ denote the number of blocks in each module. In this paper, we set them to $\\{M_{en}, M_{tcm}, M_{de}\\}$ as $\\{6,4,6\\}$.\nThe synthesized binaural speech signals in the STFT domain are obtained by multiplying the weight matrices with the original multichannel speech:\n$Y_{f,t} = w_{l,r}X_{f,t}.$"}, {"title": "A. Network architecture"}, {"title": "B. Loss function", "content": "To ensure both spatial accuracy and high-quality synthesized speech, the loss function is devised as a linear combination of com-pressed magnitude loss, compressed Real-Imaginary (RI) loss, and a novel magnitude-weighted Interaural Level Difference (mwILD) loss:\n$L = \\lambda_1 L_{RI} + \\lambda_2 L_{Mag} + \\lambda_3 L_{mwILD}.$\nEach loss item is defined as follows:\n$L_{RI} (\\tilde{y}, Y) = ||\\tilde{y}_r \u2013 Y_r|| + ||\\tilde{y}_i \u2013 Y_i||,$\n$L_{Mag}(\\tilde{y}, y) = ||||\\tilde{y}_r||^2 + ||\\tilde{y}_i||^2 - \\sqrt{||y_r||^2 + ||y_i||^2}||_4,$\n$L_{mwILD}(\\tilde{y}, y) = \\frac{\\sum_f \\sum_t \\sigma(f,t) (ILD(\\tilde{y}(f, t)) \u2013 ILD(y(f, t)))}{\\sum_f \\sum_t \\sigma(f,t)},$\nwhere $\\tilde{y}$ and $y$ respectively denote the estimated binaural signals and the target signals. $\\{\\lambda_1, \\lambda_2, \\lambda_3\\}$ denotes the weight parameters set of each loss item, which is empirically set as $\\{1,1,3\\}$. $\\sigma(f, t)$ denotes the binaural average magnitude, $ILD(\\cdot)$ denotes the function of inter-aural time difference (ILD). Note that mwILD loss is introduced to penalize frequency points with larger magnitudes, thereby enhancing the model's ability to accurately reproduce spatial cues."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "To evaluate the performance of the proposed method, we utilized the DNS-Challenge corpus [29] to synthesize mixed-target audio pairs for training, validation and testing, with a approximate proportion of 20:2:1, respectively. The mixed audio clips are generated by convolving original speech signals with Room Impulse Responses (RIRs), simulating a realistic acoustic environment. The target audio clips are created by convolving the original speech with Binaural Room Impulse Responses (BRIR). For the experiments, a six-element Uniform Circular Array (UCA) with a 10 cm diameter was employed. The sampling rate for all audio signals was set to 16 kHz. To simulate noisy conference environments, point-source interferences were introduced, which were randomly selected from the DNS-Challenge noise corpus and added to the mixed signals. The noise sources were positioned at varying distances from 0.5m to 1.5m, with relative angles spanning from 0\u00b0 to 360\u00b0. Signal-to-Noise Ratios (SNRs) were meticulously controlled, ranging from 0 dB to 30 dB, to cover a broad spectrum of listening conditions. The array signals were generated using the Image Source Method [30], a technique known for its accuracy in emulating real-world acoustics. To enhance the realism of our binaural target signals, we use convolution with Binaural Room Impulse Responses (BRIRs) actually measured by the University of Surrey [31], rather than using Head-Related Transfer Functions (HRTFs). This choice was to provide a more accurate simulation of the human auditory experience compared to using HRTFs. The BRIRs provide a detailed account of the acoustic characteristics of the listening environment, which is essential for evaluating the spatial accuracy of our model."}, {"title": "A. Dataset configuration", "content": "In our experiment, all the utterances are sampled at 16 kHz. For STFT, a squared-root Hann window is selected between adjacent frames with 50% overlap, so the frame length and frame shift are respectively set to 320 and 160 points. Adam function is selected as the optimization function. The initial learning rate is set to 5e-4, and will halve if the loss value does not decrease for three continuous epochs. The training procedure will stop once the learning rate halves"}, {"title": "B. Training configuration", "content": "The performance of the proposed Array2BR framework is com-pared with both conventional and deep learning methods. For conven-tional methods, we choose Localization-Beamforming-HRTF filtering (LBH) [19] and multichannel inverse filtering (MIF) [32]. For deep learning methods, we choose MDFNet [22], which is currently the only network similar to the proposed method. Seven objective metrics are utilized to evaluate the performance of those models. These metrics include Parameters and FLOPs, which depict the models' complexity and computational efficiency. The difference values of Interaural Time Differences (ITDs) and Interaural Level Difference (ILDs) are used to assess the spatialization performance. Besides, Perceptual Evaluation of Speech Quality (PESQ) [33] and extended Short-Time Objective Intelligibility (ESTOI) [34] scores are employed to evaluate the speech quality, while Spectral Distance (SD) measures the fidelity gap between the synthesized speech and the target. The above results averaged among different SNRs are presented in Table I.\nSeveral results can be observed from Table I. Comparing Parame-ters and FLOPs, the proposed model outperforms MDFnet. Note that these 2 metrics cannot be measured on conventional methods. Com-paring AITD and AILD, the proposed model achieves the lowest value, representing best spatialization performance. Comparing PESQ and ESTOI, the proposed model reaches the highest score with best speech quality. Comparing SD, the proposed model has the lowest value, indicating best spectral characteristics. Note that the SD of MIF is extra large because this method lacks denoising ability. Overall, the proposed Array2BR framework not only has the least parameters and FLOPs, but also strikes the best spatial and quality performance."}, {"title": "V. RESULTS AND ANALYSIS", "content": "To further analyze the spatial characteristics of different models, the interpolation diagrams of ITD and ILD are depicted. The co-ordinates of the scatters are measured every 5\u00b0, from -90\u00b0 (left) to 90\u00b0 (right), and curves with different colors indicate different models, as shown in Figure 4. We can see from the figure that in both ITD and ILD diagrams, the curve representing the proposed model gets closest to the target curve. It is worth noting that LBH method with convolutional HRTF exhibits more dramatic variations than other methods in both diagrams, since HRTF is tested in free-field conditions whereas BRIR in the real room. Compared with BRIR, the immersive and realistic sense of HRTF convolution is not good enough, even though it provides more pronounced spatial cues."}, {"title": "A. Comparison of objective evaluation", "content": "To test the practical perceptions of the methods, subjective audiom-etry experiments have been implemented. 10 utterances are chosen for each model, and 10 adults with normal hearing take the test, rating the scores '1-5' from 2 dimensions of metrics: spatialization and quality.\nThe subjective listening test is carried out following the multiple stimuli with hidden reference and anchor (MUSHRA) protocol [35], and the results of the subjective test are shown in Figure 5, the violin plot. From the distribution characteristics of Figure 5, we can observe that the proposed Array2BR architecture achieves the best evaluations in both spatialization and quality in normal hearing scenarios."}, {"title": "B. Comparison of subjective evaluation", "content": "In this work, we introduced Array2BR, an innovative end-to-end framework designed to enhance the telepresence experience in remote meetings. Our model, which incorporates a U2-structured deep learning architecture and a novel magnitude-weighted Interaural Level Difference (mwILD) loss function, demonstrated its ability to significantly improve both spatial performance and speech quality. Through comprehensive objective and subjective evaluations, we show that the proposed method can outperform different baseline models and achieve satisfactory performance."}, {"title": "VI. CONCLUSIONS", "content": "The authors would like to express their sincere gratitude to Yicheng Hsu at National Tsing Hua University for his invaluable assistance and support in testing and evaluating the baseline models."}]}