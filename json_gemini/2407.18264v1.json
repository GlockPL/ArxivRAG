{"title": "Latency optimized Deep Neural Networks (DNNs): An Artificial Intelligence approach at the Edge using Multiprocessor System on Chip (MPSoC)", "authors": ["Seyed Nima Omidsajedi", "Rekha Reddy", "Jianming Yi", "Jan Herbst", "Christoph Lippst", "Hans Dieter Schotten"], "abstract": "Almost in every heavily computation-dependent ap-plication, from 6G communication systems to autonomous driving platforms, a large portion of computing should be near to the client side. Edge computing (AI at Edge) in mobile devices is one of the optimized approaches for addressing this requirement. Therefore, in this work, the possibilities and challenges of implementing a low-latency and power-optimized smart mobile system are examined. Utilizing Field Programmable Gate Array (FPGA) based solutions at the edge will lead to bandwidth-optimized designs and as a consequence can boost the computa-tional effectiveness at a system-level deadline. Moreover, various performance aspects and implementation feasibilities of Neural Networks (NNs) on both embedded FPGA edge devices (using Xilinx Multiprocessor System on Chip (MPSoC)) and Cloud are discussed throughout this research. The main goal of this work is to demonstrate a hybrid system that uses the deep learning programmable engine developed by Xilinx Inc. as the main component of the hardware accelerator. Then based on this design, an efficient system for mobile edge computing is represented by utilizing an embedded solution.", "sections": [{"title": "I. INTRODUCTION: EDGE COMPUTING PLATFORMS", "content": "With new emerging technologies in the field of communication and mobile systems, demands for smart and secure plat-forms are arising in real-world situations. Moreover, increasing the complexity of target applications with exclusive purposes leads to a wide spectrum of research focusing on constructing smart systems employing Artificial Intelligence (AI) solutions. For large-scale application models, the need for parallel com-putational capabilities with higher performance raises a lot of design challenges. For instance, Raina, Madhavan, and Ng [1] discuss the implementation of unsupervised learning methods over the modern graphics processor which scales 70 times faster than a dual-core Central Processing Unit (CPU). The development of Cloud computing technologies has enabled the provision of higher computational capabilities. Thus, the custom methodology for creating powerful AI systems is using Cloud services due to their extensive computation power. However, the traditional Cloud solutions cannot provide the desired efficiency for daily increasing mobile systems [2][3]. As a result, a new computing architecture based on end devices (also known as IoT devices), edge devices, and Cloud has already been introduced and is used to improve the performance of systems that are extremely computationally dependent [4].\nCloud servers could be used as a very powerful computing tool, particularly for running AI inference systems; neverthe-less, due to the significant data transmission between the end device and the Cloud, the issue of reliability arises. Moreover, the available bandwidth in a network is restricted, therefore expanding the size of the network may result in further bandwidth reductions concerning the increased data path. In addition, the data transfer within a network can compromise the data security and as a result, endanger the system security [5].\nA commonly used choice as a tool for edge computing is utilizing Graphics Processing Unit (GPU) systems; this is due to the strong computation capabilities and also the high memory bandwidth of GPU devices. However, these systems have a lot of issues for battery-dependent systems, especially for mobile applications. Two of the most important design parameters for every edge device are i) the energy efficiency and ii) the security of the design. GPU systems are designed for graphical purposes and would not provide a fully reliable smart system. One considerable solution for implementing bandwidth-optimized, energy-efficient, and secure devices is using ARM-FPGA hybrid systems. These systems consist of ARM processors as the heart of software processing which act as the Central Control Unit (CCU) alongside a dedicated hardware accelerator on FPGA blocks for boosting the desired Machine Learning (ML) algorithm using the concept of paral-lelism. In 1992, [6] demonstrated that a large neural network application with a single ANNA chip (Analog Neural Network Arithmetic and logic unit (ANNA)) enables it to operate with the speed advantage of 50 to 500 over conventional hardware evaluated with Floating-Point (FP) precision.\nIn the current paper, an FPGA based edge solution is presented in comparison to the GPU methods over the edge."}, {"title": "II. RELATED WORKS: USING EMBEDDED FPGAS AS EDGE COMPUTING SYSTEMS", "content": "First ideas of reducing network traffic by the use of edge-computation are provided by [4]. Therefore they tackled the problem of crowdsourced applications which are geo-distributed globally with highly heterogeneous crowdsourced data with the use of edge computing. Especially they want to combat the challenges in crowdsourced deep learning appli-cations. They claim to be able to reduce the network traffic by 80% and running time by 69% in comparison to state-of-the-art cloud solutions. One of the big advantages is the improvement of network latency between end-user devices and edge servers compared to cloud servers. [7] and [8] try to solve the problem of high computation-intensive Deep Neural Network (DNN) based tasks on mobile devices with the development of a specific Framework, which claims to use a device-edge-synergy. Therefore they use algorithms to adaptively partition computations between device and edge. Furthermore, they are reducing computing latency via early exiting inference provided by the use of BranchyNet [9] at the edge.\n[10] examines latency and power optimizations of heavily computation-dependent applications on the client side with the use of FPGAs. They investigate the efficient hardware im-plementation of Cellular Neural Networks (CeNNs) in FPGA with the help of different optimization algorithms for com-pressing image processing techniques. By using a so-called incremental quantization which includes parameter partition, parameter quantization, and re-training, they can quantize the numbers in CeNN templates to the power of two, so they are able to use the full potential of embedded FPGAs. This solves the problem of the very high need for multiplications needed in CeNNs, which leads to a bottleneck in an FPGA because of the limited quantity of embedded multipliers one contains. By reducing the power of numbers to two the multiplications can be solved with the use of logic shifts, which can be achieved with logic elements and registers. In the represented work CNNs are used instead of CeNNs, moreover, it will be the Xilinx deep learning IP (Xilinx DPU) which delivers sufficient speed up for inference tasks.\nAlso [11] worked on more efficient ways of implement-ing efficient DNNs on FPGAs. Therefore they developed a Framework named DNNWEAVER which can automatically generate synthesizable accelerators for a given pair of DNN and FPGA. With performance tests, they compared three different FPGAs (Xilinx Zynq, Altera Stratix V, Altera Arria 10) against different many-core GPUs. They were able to achieve higher Performance-per-Watt against all tested many-core GPUs for the Xilinx Zynq and Altera Arria 10 FPGAS by using the framework synthesized DNNs.\nIn [12] the authors went one step further and claim to create the first hybrid system of GPU-FPGAs for training a network. Therefore they propose a new framework with a focus on effective DNN training on GPU-FPGA-hybrids. They used different energy-efficient schemes for DNN train-ing procedures to execute individual training operations on either high-performance (GPUs) or power-efficient hardware (FPGAs). With that, they claim to be able to reduce the average power consumption by 44.3%. All in all, it is to say the higher adaptability of FPGAs and the higher more advanced processing power of embedded FPGAs compared to normal GPUs result in lower cost and relatively lower power consumption which make them a serious threat to pure GPU solutions still used in many classical cloud-based-solutions so far."}, {"title": "III. PROPOSED SYSTEM: HARDWARE ACCELERATOR ON EMBEDDED FPGA FOR DNNS", "content": "This Section describes the proposed design with a ded-icated hardware accelerator on FPGA using Programmable Logic (PL) system of Xilinx MPSoC evaluation board (Xilinx ZCU102). The heart of this accelerator includes Xilinx Deep Learning Processing Unit (DPU) [13] which is responsible for computing given inference tasks for a DNN system. The main functionality of this system is to calculate massive series of Multiply-Accumulate operations (MAC) on network parameters of DNNs. On the other hand, the initialization task, as well as data coordination of a DNN, will be done using ARM processors in the Processing System (PS) of the MPSoC.\nThe overall workflow for preparing and implementing a Neural Network (NN) on a target embedded FPGA board (Xilinx SoC/MPSoC) is depicted in Figure 1. This diagram consists of three sub-workflows. The most left path (depicted in dark gray) explains the required steps for preparing a dedicated hardware accelerator on FPGA logic gates using the Xilinx DPU soft IP. This path also includes all the desired pre-processing at the hardware level. Then using the implemented design in the PL side, computational processes of a NN, which are mainly multiplication and addition functions, are performed on the FPGA side. The design strategy behind the DPU IP is utilizing Digital Signal Processor (DSP) tiles for conducting MAC (Multiply-Accumulate) operations.\nThe middle sub-workflow (depicted in light grey) describes the required steps for converting the desired NN from its initial form to an equivalent FPGA compatible form. This step involves model quantization (converting floating-based weights and biases to equivalent integer numbers). The quan-tization step is very important not only because it reduces the required storage memory but also optimizes the required bandwidth for the data transmission between the off-chip memory (DDR memory connected to the PS side) and the hardware accelerator on the FPGA side. After the quantization step, the graph-based NN must be converted to a binary form"}, {"title": "IV. PERFORMANCE RESULTS OF DNN: ON EDGE VERSUS ON CLOUD", "content": "The DNN implementations over the Edge and Cloud are presented in this Section. Moreover, the detailed description of the environment setup for used Edge and Cloud components, FPGA and GPU features, and the model used for the execution of the DNN are mentioned in related parts.\n\nA. Deep Neural Network Implementation over Edge\nThe implementation details of a DNN on an edge-based device are described in this Subsection.\n1) Physical Infrastructure: The edge device used in this set-up is the ZCU102 evaluation board which is an embedded FPGA from Xilinx Inc. This board is considered as a MPSOC with the Zynq UltraScale+ architecture. However, the overall workflow presented in this paper can be replicated in any Zynq and Zynq UltraScale+ based evaluation or custom boards (for example, Zynq-7000 ARM-FPGA SoC boards). The main reason for selecting a Zynq UltraScale+ system is the scalable capability of MPSoC family boards, which is an important fac-tor for creating smart networks. Moreover, ZCU102 provides more logic resources in FPGA (including DSP blocks, which are the main computational units) for performing required computations of the inference graph of DNNs. Another reason for selecting the Zynq UltraScale+ over the Zynq architecture is the possibility of implementing the SoftMax activation function at the hardware level. The hardware implementation of this activation function can boost the performance of the system much more than the equivalent function at the software level. The ZCU102 consists of ARM Cortex processors in the Application Processing Unit (APU) and Real-time Processing Unit (RPU) sub-systems which are implemented in the PS. Moreover, a mobile GPU is encompassed in the PS side of the board. The system specifications of Xilinx ZCU102 MPSOC are described in Table I.\n2) DNN Implementation over Edge device: The imple-mentation results for ResNet50 DNN on the Xilinx ZCU102 MPSoC board are shown in Table II. In this work, several test scenarios are implemented on the target board using different"}, {"title": "B. Deep Neural Network Implementation over Cloud", "content": "A detailed description of DNN implementation constructed on a Cloud-based GPU cluster infrastructure is discussed.\n1) Physical Infrastructure: The experimental setup consists of a Cloud Computing cluster with GPU servers namely GTX1080Ti and RTX A6000. Table III provides the overall GPU specifications used in the implementation. Slurm [16] is used for managing compute resources, scheduling jobs, and execution on worker nodes, it provides fault-tolerant and highly scalable cluster management.\n2) DNN Implementation over GPU Cluster: With the de-ployed cluster environment, DNN is constructed using a containerized environment using enroot, administrated, and monitored through the Slurm cluster. The framework is built using Keras with TensorFlow [17] backend to run the image identifier model using ResNet50. The ResNet50 model em-ployed in this study is the same pre-trained model used in the Edge implementation. GPUs were subjected to iterative image prediction analysis and throughput as well as the total power consumption are recorded in Table IV. The implementation is tested over 2 different GPU servers working with GPU in sets of 2 and 4 along with 2 CPUs. The CPU used alongside the GTX1080Ti GPU is the Intel(R) Xeon(R) CPU E5-2630 v4 and the CPU used with the RTXA6000 GPU is the AMD(R)"}, {"title": "C. Comparison results of DNN Implementation over Edge versus GPU Cluster", "content": "Considering the obtained results from implementing our target DNN (Classic ResNet50) over Edge (embedded FPGA device) versus GPU Cluster (as the Cloud tool) shows dras-tically lower latency using Edge solution. Figure 3 shows the latency comparison of these two methodologies. The main reason for lower latency using embedded FPGAs is the negligible additional overhead for data transmission and initialization in the target board. Therefore, in an Edge device, the latency value equals the time it takes to compute the input data on the board (computation latency). However, in a Cloud implementation, there are other latency sources for data transmission over the Cloud, creation of new instances, and initialization of values. In this work, all the additional latency sources in the Cloud are called \"Transmission and initialization latency\". Our experiments show high flexibility and low latency of Edge devices using hardware accelerators built on FPGA logical units. Figure 4 shows the detailed latency sources using the Cloud tool.\nMoreover, using embedded FPGAs is more energy-efficient than GPUs and this design parameter is a decisive point for mobile projects with limited power sources. The noticeable point is, GPU clusters are initially able to achieve much higher throughput than our test scenarios, but this is equal to a much higher power consumption of the system. Increasing the batch size leads to higher GPU utilization, and this will result in higher throughput. For instance, in our experiment with increasing the batch size from 8 to 200, the throughput will jump from 137 Img/S to 751.49 Img/S in the 4/GTX1080Ti; however, the total power consumption changes from 138.4 Watts to 747 Watts. With replicating the same experiment using the 4/RTXA6000 scheme, the throughput jumps from 187 Img/S to 1886.13 Img/S, and at the same time overall consumed power changes from 174.4 Watts to 899 Watts. Considering the power budget as well as the practically required throughput of the DNN, our experiments show better energy efficiency in all the test cases."}, {"title": "V. CONCLUSION & FUTURE WORK", "content": "In this paper, we proposed a Deep Neural Network imple-mentation strategy on Edge devices using a Multiprocessor System on Chip. Our experiments show that using Edge devices for Artificial Intelligence inferences has superiority in comparison to the Cloud implementation of the same network not only in the latency optimization but also in the energy efficiency of the system. One of the main components used in the hardware accelerator of this work is the Xilinx dedicated soft IP core for deep learning purposes (Xilinx DPU). The presented system can boost the computation process of DNNS using Digital Signal Processor tiles and logic gates on the FPGA side as well as using ARM processing for controlling and coordination purposes. In future research, the possibility of implementing multiple DNNs on MPSoCs will be examined to observe the effect of co-implementation of DNNs. Moreover, another possibility for future work is using the Xilinx Versal boards [18] as the next generation of AI cores with dedicated AI engines instead of soft IP cores."}]}