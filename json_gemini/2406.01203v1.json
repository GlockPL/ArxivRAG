{"title": "Scaling Up Deep Clustering Methods\nBeyond ImageNet-1K", "authors": ["Nikolas Adaloglou", "Felix Michels", "Kaspar Senft", "Diana Petrusheva", "Markus Kollmann"], "abstract": "Deep image clustering methods are typically evaluated on small-scale balanced\nclassification datasets while feature-based k-means has been applied on proprietary\nbillion-scale datasets. In this work, we explore the performance of feature-based\ndeep clustering approaches on large-scale benchmarks whilst disentangling the\nimpact of the following data-related factors: i) class imbalance, ii) class granular-\nity, iii) easy-to-recognize classes, and iv) the ability to capture multiple classes.\nConsequently, we develop multiple new benchmarks based on ImageNet21K. Our\nexperimental analysis reveals that feature-based k-means is often unfairly evalu-\nated on balanced datasets. However, deep clustering methods outperform k-means\nacross most large-scale benchmarks. Interestingly, k-means underperforms on\neasy-to-classify benchmarks by large margins. The performance gap, however,\ndiminishes on the highest data regimes such as ImageNet21K. Finally, we find that\nnon-primary cluster predictions capture meaningful classes (i.e. coarser classes).", "sections": [{"title": "1 Introduction", "content": "For over a decade, the ImageNet-1K benchmark [18, 40] has served as the core downstream dataset\nin a plethora of computer vision tasks. Examples include image classification [24], representation\nlearning [14], semi-supervised learning [44], and recently image clustering [55]. Image clustering,\nalso known as unsupervised image classification, refers to algorithmically grouping visual stimuli\ninto discrete concepts called clusters [9, 3]. Grouping images without human supervision has many\napplications, including unsupervised out-of-distribution detection [42], image generation [7], and\nlarge-scale dataset pruning [36, 1].\nThe advent of deep neural networks has led to the emergence of deep image clustering [12] and\nfeature-based clustering. Examples of feature extractor pretraining include visual self-supervision\n[14, 11] or natural language supervision [37]. The most widely established feature-based clustering\napproach to date is k-means [32] due to its scalable nature. Feature-based k-means has been\nsuccessfully employed to billion-scale proprietary vision datasets [36]. Concurrently, deep clustering"}, {"title": "2 Related work", "content": "Large-scale image clustering methods. Deep image clustering consists of learning the label-related\nrepresentations and the cluster assignments [2], either simultaneously (single-stage methods) [55, 5]\nor sequentially (multi-stage methods) [47]. We consider a method to be large-scale if clustering results\nare reported on Imagenet-1K [18]. In SeLa [55], the authors design a two-step framework, which\nalternates between estimating the pseudo-label assignment matrix and representation learning. In\n[5], the authors present a single-stage end-to-end method that employs a variant of the cross-entropy\nloss. However, such single-stage methods require dataset-dependent hyperparameter tuning and are\ncomputationally expensive to iterate on large scales.\nContrastive learning [14] has been a major breakthrough for image clustering. SCAN [47] was\nthe first large-scale method to isolate representation learning from learning the cluster assignments.\nBy first employing contrastive learning on the training data, Van Gansbeke et al. [47] show that\nnearest neighbors (NNs) in feature space likely share the same label. Based on the same principle,\nAdaloglou et al. [2] have recently shown that a) features from multiple pre-trained feature extractors\n(i.e. DINO [11], MSN [6]) can be used and b) learning the cluster assignments does not require image\naugmentations as opposed to SCAN [47]. This enables pre-computing the image features similar to\nfeature-based k-means."}, {"title": "3 Background, materials and methods", "content": "Existing benchmarks and pre-trained models. The most challenging large-scale clustering\nbenchmark to date is ImageNet-1K [18, 40], which consists of C=1000 balanced classes (p(c) =\nU({1, .., C'})). To the best of our knowledge, the only class imbalance that has been proposed is halv-\ning the number of samples from the odd class indices [19]. In contrast to [19], a) we do not modify\nthe validation set as we want to quantify the performance degradation attributed to the imbalance, and\nb) we apply it on ImageNet-1K instead of CIFAR10 and CIFAR100 [30]. We refer to this benchmark\nas ImageNet-1K ODD. All the newly-created benchmarks are based on the preprocessed version of\nImageNet21K (winter version 2021) as in [38]. This version of ImageNet21K has \u2248 11.06M samples\nthat are non-uniformly distributed across 10450 non-mutually exclusive classes. These classes can be\norganized in a semantic tree based on the WordNet hierarchy [33, 52]. To imitate a real-life scenario\nwhere GT labels and additional data are scarce, we assume no access to external data or models\ntrained on external data. Unless otherwise specified, we use the iBOT ViT-L [56] pre-trained on\nImageNet21K and MAE Refined ViT-H [4] (MAE-R) on ImageNet-1K (current state-of-the-art).\nFeature-based image clustering methods. Apart from distributed k-means [32, 20], we identify and\nuse two scalable state-of-the-art methods based on the framework of [2], namely TEMI and SCANv2\n[2], which we describe below. TEMI first mines the nearest neighbors (NN) in feature space for each\nsample x in the dataset D. During training, TEMI randomly samples x from D and x' from the set of\nNN of x. TEMI employs a teacher and student head $h_t(\\cdot)$ and $h_s(\\cdot)$ that have an identical architecture\nbut differ w.r.t. their parameters as the teacher is only updated using an exponential moving average\nof $h_s(\\cdot)$. Given a feature extractor $g(\\cdot)$, the student and teacher consist of multiple independent heads\n$h_i(z)$, $h_i(z')$, where $i \\in \\{1, . . ., H\\}$ is the head index and $z = g(x)$, $z' = g(x')$. Class probabilities\nare computed using a softmax function $q_s(c|x)$ and $q_t(c|x')$ and the following loss is minimized\n$L_{TEMI} (x, x') := \\frac{1}{H} \\sum_{j=1}^{H} \\sum_{c'=1}^{C} q_{t}^{j}(c'|x')  \\log \\left( \\frac{q_{s}^{j}(c|x)}{q_t(c)} \\right) $", "sections": [{"title": "4 New clustering benchmarks based on ImageNet21K", "content": ""}, {"title": "4.1 Quantifying the sensitivity to class imbalance", "content": "Halving the number of odd class indices can only be meaningfully applied to balanced classification\ndatasets. Instead, we create subsets based on the class histogram (number of samples per class).\nTo create varying degrees of imbalance, we take multiple class percentiles s around the median\nclass frequency $m_s$ = [50 \u2212 s, 50 + s], where s \u2208 {5, 15, 25, 35, 45}. As s increases, the number\nof samples N and classes C also increase. Additionally, we create a highly imbalanced subset by\ntaking a percentile of 10% around the median ($m_{10}$), adding the 10% most frequent classes, and\ncomparing it against taking the same number of classes centered around the median. We call these\nfrequency-based ImageNet21K benchmarks Imbalanced-2K and Centered-2K, respectively."}, {"title": "4.2 Quantifying the sensitivity to the class granularity", "content": "Coarse benchmarks. Class labels can be organized in a hierarchical semantic tree [45], such\nas WordNet [33]. Based on WordNet, Ridnik et al. [38] have identified 11 hierarchy levels on\nImageNet21K. A hierarchy of 1 corresponds to the most coarse level and 11 to the most fine-grained.\nUsing a semantic tree, one can seamlessly find the linguistic hypernyms (semantic ancestors) for each\nGT class, such as going from the GT class \u201cchair\u201d to \u201cfurniture\". By leveraging the ImageNet21K\ntree, we recursively map the GT classes to their semantic ancestor such that the maximum hierarchy\ndepth is restricted to d \u2208 [1, .., 9], where the majority of the samples lie. Note that we do not exclude\nany samples at this step and leave classes with a lower hierarchy than d intact. During the learning\nprocess of the clustering algorithm, we create as many groups as the number of coarsened labels and\nevaluate on the validation set with the coarse labels of maximum depth d.\nFine-grained benchmarks. Refining ground truth (GT) labels to more fine-grained classes is\nchallenging compared to coarsening them. The simplest way to achieve maximum class granularity\nis to consider only images from classes without linguistic hyponyms (semantic descendants) in\nthe semantic tree, known as leaf classes [53]. We refer to these subsets as WordNet leaf and\nImageNet21K leaf, based on their respective semantic trees. Unlike ImageNet21K, these subsets are\nmutually exclusive. To avoid excluding samples from non-leaf classes, we first use CLIP in tandem\nwith the semantic tree to reannotate non-leaf samples. To this end, we perform zero-shot classification\non the hyponyms to obtain a new refined label, such as furniture\u2192{chair, desk, shelf, couch}. This\nprocedure is recursively applied, always selecting labels with the highest image-text similarity. We\nstop once a leaf class is assigned. We refer to this method as leaf hierarchical zero-shot label refining\n(leaf HZR).\nIt is not, however, guaranteed that an image can always be meaningfully relabeled as one of its\nhyponyms. Analogously to a human annotator that can decide to keep the current label [23, 22], we\nadd the parent class during zero-shot classification. In the previous example, we perform zero-shot\nclassification using the following class set {chair, desk, shelf, couch, furniture}. By including the\nparent class at each stage of the recursive relabelling process, we allow CLIP to assign the same label\nto the image, partially mitigating the aforementioned issue, as illustrated in Figure 1. We refer to this\nsetup as parent HZR (p-HZR)."}, {"title": "4.3 Easy-to-classify benchmarks: model-based ImageNet21K subsets", "content": "Here, we assess whether easy-to-classify samples are easy to cluster, as measured by high linear class\nseparability [48]. The intuition behind this is that if an image is not classified correctly, it is unlikely\nto be clustered successfully. We choose classes that are recognized with high ACC from pre-trained\nmodels with additional information during pre-training. To realize this, we use a subset of the top-1K\nclass accuracies from three pre-trained models: 1) a vision-language model (OpenCLIP ViT-G [28]),\n2) a visual self-supervised model (DINOv2 ViT-g [36]), and 3) an Imagenet21K-supervised model\n(ViT-L [50]). More precisely, we measure the top-1K accuracies per class on the validation set after\nlinear probing. We name the created subsets as CLIP-1K, DINOv2-1K, Sup-1K. We compare the\nmodel-based subsets with random subsets of 1K classes, ImageNet-1K, and their class union."}, {"title": "4.4 Multi-label clustering benchmarks and metrics", "content": "Here, we investigate whether clustering methods capture additional concepts, which is particularly\nuseful for non-mutually exclusive datasets and large-scale datasets with multiple objects of interest\n[10]. For each image, we consider a set of labels $S_l$ such that $S_l = \\{s_1, \u2026, s_L\\}$, where $s_L \\subseteq C$\n(set of GT classes). For hierarchically structured datasets, we set $S_L$ to be all semantic ancestors of\nthe GT label. For example, an image of a guitarist (GT) can be classified as a musician or person,\nsomething that is not captured by existing clustering metrics. We aim to measure this with the top\n1\u2192L ACC similar to Beyer et al. [10].\nIn practice, we adopt ImageNet21K [38] and the reassessed multi-label validation set of ImageNet-1K\ncalled \"Real\" [10] as benchmarks. We also assess the quality of the top 5 predictions as in [47]. To\nestimate the Hungarian one-to-one mapping $f$ [31], we use the top 1 prediction, and select a single\nlabel from $S_l$. After computing $f$, the top 5 predictions are mapped to GT classes, and the top 5 \u21921\nand top 5 \u2192 L accuracies are computed. For ImageNet21K, we select the GT label from $S_l$ to\ncompute $f$. For ImageNet-1K ReaL, we randomly sample one label from $S_l$ per image, which we\nrepeat 50 times and report the mean accuracies. We found a maximum standard deviation of 0.1. For\nk-means, we compute the top 5 predictions by finding the 5 closest centroids."}]}, {"title": "5 Experimental results", "content": "Implementation details. Following Ridnik et al. [38], we use the preprocessed winter 2021 version\nof ImageNet21K and its corresponding semantic tree unless otherwise specified. We use the training\ndata to develop the clustering method and evaluate on the validation split as in [47, 2]. We conduct\nall the clustering experiments on a single Nvidia A100 GPU with 40GB of VRAM. For TEMI\nand SCANv2, we use H=32 clustering heads and train for 50 and 25 epochs on ImageNet-1K and\nImageNet21K, respectively. Different from [2], we use a larger batch size (i.e. 4096 on ImageNet-1K\ninstead of 512) for SCANv2 as we observed training instabilities and improved performance. For\nk-means clustering, we use cosine similarity as the distance metric and apply L2 normalization to the\ncluster centers after each iteration. We report the average clustering top 1 accuracy (ACC) on the\nvalidation set from three independent runs. Finally, we report the linear probing accuracy as an upper\nbound of clustering methods. All hyperparameters and additional clustering metrics are provided in\nthe supplementary material."}, {"title": "5.1 Impact of class imbalance", "content": "ImageNet-1K. We first verify the superiority of TEMI and SCANv2 on ImageNet-1K in Table 1\nwhere we measure at least 5.9% and 10.0% relative ACC gain compared to k-means using the latest\nstate-of-the-art feature extractor MAE-R [4]. With a well-tuned SCANv2 framework, we achieve a\nnew state-of-the-art clustering ACC on ImageNet-1K of 69.79%. Notably, this accuracy surpasses\na supervised GoogLeNet (2015) [46] as a 68.3% top-1 ACC has been reported in [27]. We clarify\nthat we increase the mini-batch size from 512 to 4096 to get the state-of-the-art ACC and provide an\nexplanation of why it is necessary in Section 6.\nImbalanced benchmarks. For imbalanced datasets such as ImageNet-1K ODD and Imbalanced-\n2K, the ACC gains between deep clustering methods over k-means are reduced. We measure a\nrelative ACC degradation of 3.2% for k-means, 5.4% for TEMI, and 5.7% for SCANv2. Similar\nbehavior is observed on the ImageNet21K scale when comparing the Centered-2K versus Imbalanced-\n2K benchmarks. Even when gradually scaling up the dataset size and the degree of imbalance\nsimultaneously (Figure 2, right), we observe the same trend: although TEMI consistently outperforms\nk-means, its relative ACC gain diminishes (from a maximum of 4.23% to 1.6% using all samples).\nBased on the above, we state that a) k-means is unfairly evaluated solely on balanced clustering\nbenchmarks such as CIFAR and ImageNet-1K, b) k-means demonstrates suboptimal performance\neven on imbalanced benchmarks, and c) the degree of imbalance only partially accounts for the\ninferior performance of k-means observed in Table 1 and Figure 2 as well as previous studies\n[5, 2, 47, 16]. Another veiled aspect is that despite both deep clustering methods enforcing class\nuniformity, they are applicable beyond the scope of ImageNet-1K, such as large-scale imbalanced\ndatasets."}, {"title": "5.2 Impact of class granularity: coarse and fine-grained benchmarks", "content": "In Figure 2 (left), we illustrate the methods' accuracies for different maximum hierarchy depths d.\nInterestingly, k-means outperforms TEMI for d < 3 with a maximum relative difference of 4.1% for\nd = 1. We argue that highly coarse labels are more likely to be captured by isotropic and normally\ndistributed clusters assumed by k-means. Conversely, TEMI and SCANv2 rely on fixed NN pairs,\nwhich lead to retrieving similar-looking images to the query image [36], which hinders the coarse\nclustering performance for superclasses such as \"animal\".\nRegarding the considered fine-grained benchmarks in Table 2, all clustering methods benefit similarly\nfrom purging mutually exclusive classes. The highest relative gain is observed in the subsets of\nWordNet leaf classes, where TEMI and k-means improve by 18.29% and 17.32% (relative ACC)\ncompared to ImageNet21K. Overall, TEMI outperforms k-means on the majority of coarse and\nfine-grained ImageNet21K-based benchmarks, but the gains are marginal. Based on Figure 2 and\nTable 2, we cannot identify any strong ACC discrepancy between clustering methods w.r.t. the class\ngranularity."}, {"title": "5.3 Results from easy-to-classify benchmarks: model-based ImageNet21K subsets", "content": "All three model-based splits of 1K classes achieve high probing ACC (> 95%) using iBOT (pretrained\non ImageNet21K) as shown in Table 3. This indicates that the image features of easy-to-classify\nclasses are well-separated, verifying [48]. Intriguingly, we measure a high discrepancy between\nk-means and deep learning methods that is independent of the model used to pick the top-1K classes\n(CLIP, DINOv2, Supervised). The reported discrepancy does not originate from the degree of\nimbalance. Compared to TEMI, k-means underperforms by a relative accuracy difference of at least\n12.9%. We thus state that well-separated clustering benchmarks may have irregular class shapes\n[21] that cannot always be modeled with k-means. Deep clustering methods are more flexible by\nleveraging the structure of the feature space (through the NN) to capture irregular class shapes. The\nlatter likely leads to a decision boundary of a more arbitrary form compared to k-means."}, {"title": "5.4 Multi-label clustering evalutions", "content": "As shown in Table 4, the top 1 predictions capture relevant additional concepts (coarser labels,\ncoexisting labels, or multiple objects of interest) as measured by top1 \u2192L. We measure a relative\ntop 1\u2192L ACC increase of more than 13% (ImageNet21K) and 8.1% (ImageNet1K-ReaL) across\nclustering methods. In terms of the top 5 ACCs, we find that the non-primary cluster predictions\ncapture meaningful classes, corroborating with Beyer et al. [10]. Unlike probing where single-label\nsupervision is provided during training, clustering methods achieve higher relevant ACC gains in\ntop5 \u21921 and top5 \u2192L on both benchmarks."}, {"title": "6 Discussion, limitations and future work", "content": "Class separability and clustering benchmarks. To further investigate the reported discrepancy\nbetween k-means and deep clustering methods (Table 3), we measure the GT alignment score [48],\nthe silhouette score [39] and the Davies-Bouldin score (DBS) [17] using the iBOT feature extractor.\nAll metrics highly correlate with the probing ACC, specifically $R^2 > 0.97$ (see Supp.), and point\nto the fact that model-based subsets are the most well-separated benchmarks. We highlight that\nmodel-based filtering is an automatic way to create new benchmarks without any explicit constraint\non the imbalance or granularity level. Still, we assume that a feature extractor trained on external\ndata (e.g. CLIP) is available.\nAre we done with ImageNet-1K? No, but additional large-scale clustering benchmarks should\nbe taken into account. Limitations and idiosyncrasies of ImageNet-1K have been pointed out in a\nseries of classification-centered studies [10, 49]. From the perspective of clustering, ImageNet-1K\nis substantially closer to a random split of 1K ImageNet21K classes in terms of linear separability\n(Table 3). Given that ImageNet-1K is balanced (Table 1), a non-inherent limitation of clustering, we\nstate it should not be used in isolation as the core large-scale benchmark.\nSensitivity of SCANv2 to the mini-batch size. In contrast to Adaloglou et al. [2], we identified\nthat SCANv2 outperforms TEMI on ImageNet-1K (Table 1). Yet SCANv2 performs consistently\ninferior on the large-scale benchmarks (Table 2), which is partially attributed to its sensitivity to the\nmini-batch size B. In particular, the entropy term in Equation (2) requires B to be sufficiently larger\nthan the number of clusters C to approximate the cluster distribution $p(c)$. The latter requires a lot\nof extra memory as the space complexity scales linearly O(C \u00d7 H) w.r.t. the number of clusters C\nand heads H. This computational limitation becomes quite significant on large-scale benchmarks as\nshown in Table 2.\nCalibration analysis of deep clustering methods. Unlike k-means, deep clustering methods provide\na measure of confidence as measured by the maximum softmax probability (MSP) [25]. In Figure 3\n(left and middle), we show that even though TEMI yields more discriminative predictions (91.9%\nversus 80.2% mean MSP), the predictions of SCANv2 are better calibrated on ImageNet-1K. Again,\nthis is attributed to the entropy regularization in Equation (2), which explicitly down-weights images\nwith high confidence in the mini-batch. We believe that the confidence of well-calibrated clustering"}, {"title": "7 Conclusion", "content": "In this paper, a comprehensive experimental study of feature-based clustering approaches and large-\nscale benchmarks was conducted. New clustering benchmarks based on ImageNet21K were created,\nfocusing on the effects of individual factors, such as class imbalance, class granularity, easily separable\nclasses, and the ability to capture multiple labels. TEMI and SCANv2 were shown to outperform\nk-means on the majority of introduced benchmarks. However, we found marginal differences on\nthe largest scale benchmarks. An inferior clustering performance was observed by k-means on\neasy-to-classify benchmarks. It was demonstrated that clustering methods can capture multiple GT\nlabels, often with meaningful secondary predictions such as coarser classes. We believe that the\nnew set of benchmarks will help future clustering approaches disentangle individual factors on large\nscales."}]}