{"title": "Dehumanizing Machines: Mitigating Anthropomorphic Behaviors in Text Generation Systems", "authors": ["Myra Cheng", "Su Lin Blodgett", "Alicia DeVrio", "Lisa Egede", "Alexandra Olteanu"], "abstract": "As text generation systems' outputs are increasingly anthropomorphic\u2014perceived as humanlike\u2014scholars have also raised increasing concerns about how such outputs can lead to harmful outcomes, such as users over-relying or developing emotional dependence on these systems. How to intervene on such system outputs to mitigate anthropomorphic behaviors and their attendant harmful outcomes, however, remains understudied. With this work, we aim to provide empirical and theoretical grounding for developing such interventions. To do so, we compile an inventory of interventions grounded both in prior literature and a crowdsourced study where participants edited system outputs to make them less human-like. Drawing on this inventory, we also develop a conceptual framework to help characterize the landscape of possible interventions, articulate distinctions between different types of interventions, and provide a theoretical basis for evaluating the effectiveness of different interventions.", "sections": [{"title": "1 Introduction", "content": "The outputs of text generation systems are increasingly seen as human-like (Akbulut et al., 2024; Cheng et al., 2024b; Mitchell, 2024), leading to claims that these systems may have e.g., feelings, opinions, or an underlying sense of self (e.g., Friedman and Kahn Jr, 1992; Tiku, 2022; y Arcas, 2022; Chalmers, 2022; Cheng et al., 2024a). Anthropomorphic system behaviors or outputs\u2014i.e., those perceived as or believed to be human-like-can encompass a wide range of linguistic expressions, such as the use of first-person pronouns (\u201cI\u201d), conversational language (\"how are you doing?\"), and expressions of friendliness and assistance (\u201chappy to help!\") (Emnett et al., 2024; DeVrio et al., 2025). While some of these behaviors are by design and thought to be desirable (e.g., Schanke et al., 2021; Kim, 2024), prior work has also raised growing concerns about a range of possible harmful outcomes such systems and their behaviors or outputs might give rise to, including issues related to over-reliance, emotional dependence, dehumanization, deception, or even physical harm (e.g., Ischen et al., 2020; Porra et al., 2020; Chan et al., 2023; Chandra et al., 2024; Cheng et al., 2024a; Payne, 2024; Rothman, 2024). Indeed, having a system appear polite and helpful might be desirable, but having a system output text claiming personhood or embodiment (e.g., \"I am human just like you\") might not.\nThe outputs of text generation systems can be anthropomorphic in many ways (Abercrombie et al., 2023; DeVrio et al., 2025), and different types of outputs might lead to different types of outcomes. For example, expressions of empathy may result in users feeling more comfortable with disclosing sensitive or private information (e.g., Ischen et al., 2020) or becoming emotionally dependent on the system (e.g., Laestadius et al., 2022); while suggestions that a system has cognitive abilities may result in users overestimating what a system can do (Ibrahim et al., 2024) and thus over-relying on it (Passi and Vorvoreanu, 2022).\nHowever, how to effectively intervene on anthropomorphic system outputs to make them less human-like or to mitigate possible harmful attendant outcomes remains understudied and, thus unclear. For text generation systems in particular, this is further complicated by the fact that language is innately human, often produced by humans, for humans, and is frequently about humans (DeVrio et al., 2025; Lucy et al., 2024).\nTo address this gap, with this work we aim to provide empirical and theoretical grounding for developing such interventions and studying their effectiveness. For this, we first compile an inventory of interventions (\u00a73) by drawing on both a) prior literature (\u00a72.1) and b) a crowdsourcing study where participants were asked to make generated texts less human-like (\u00a72.2). While compiling this inventory, we also derive a conceptual framework to help us characterize the landscape of possible interventions, and to help articulate distinctions between different types of interventions, the system behaviors they are intended to counter, and their possible operationalizations (\u00a72.3).\""}, {"title": "2 Identifying Interventions", "content": "To provide an empirical foundation for developing and probing the effectiveness of interventions to mitigate anthropomorphic system behaviors, we first compiled an inventory of possible interventions along with system behaviors that these interventions are intended or believed to mitigate. To do so, we started collating a list of both interventions and anthropomorphic system behaviors through a literature review (\u00a72.1), which we then complemented with a crowdsourced study (\u00a72.2). Drawing on this inventory, we also iteratively developed an analytical framework to help us characterize the landscape of possible interventions (\u00a72.3).\n2.1 Identifying Known Interventions\nTo seed our inventory of interventions, we drew from prior literature in NLP, HRI (human-robot interaction), and HCI (human-computer interaction) on anthropomorphic or human-like AI system behaviors; on anthropomorphism as a consideration in developing text generation systems; and on AI anthropomorphism more generally. We identified a set of 20 relevant papers using a purposive sampling approach (Palinkas et al., 2015), which included both recent and influential works identified from prior knowledge, keyword searches, and snowball sampling."}, {"title": "2.2 Empirically Surfacing Interventions", "content": "To complement this initial set of interventions, we designed a crowdsourcing study to surface additional possible interventions. This study was IRB approved, and consent was obtained from each participant before participation.\nCrowdsourcing task design. We designed our study to probe which types of LLM-generated outputs (system behaviors) participants might deem to be human-like, and how they would rewrite those outputs to make them less human-like.\nGiven a textual input by a user of an LLM-based system, participants were asked to read the text the system generated in response to that user input. Participants were then asked to highlight the words or phrases in the output text that seem human-like to them, to encourage them to reflect on anthropomorphic behaviors in the output text. Then, they rated how human-like the text seems to them on a 5-point scale, and identified why the text seems human-like to them by selecting from a list of 5 types of system behaviors (derived from prior work (\u00a72.1)), which gestures to participants about the ways in which output text might be considered human-like. Participants then answered an open-ended question about other human-like qualities the generated text suggests the system has. Finally, they were asked to rewrite the text to be NOT human-like or less human-like. We included the phrase to you in the instructions and questions to encourage participant subjectivity and capture a range of perspectives on what system behaviors are human-like (R\u00f6ttger et al., 2021).\nBefore deploying the study on the crowdsourcing platform Prolific in July 2024, we ran three pilot studies to identify and address clarity issues, and refine our study design. We recruited a total of 350 US-based, English-speaking participants on Prolific, with each participant completing a single task that included 4 different examples (pairs of user input and generated responses). The task took participants an average of 16 minutes, and they were compensated at an hourly rate of $15. See Appendix B for the task interface and details.\nSelecting and annotating examples of generated texts. In selecting examples for our participants to assess and rewrite to make less human-like, we aimed for our sample to at least cover the categories of behaviors that we identified from the literature (Table 2). To help surface examples illustrating a variety of possible anthropomorphic behaviors\u2014and thus possibly a variety of interventions to mitigate those behaviors-we sampled examples from publicly available datasets that 1) capture common uses of LLMs; 2) include real-world usage of LLMs with respect to contexts, models, and users; and/or 3) were generated with a range of commercially available LLMs (in part because they may vary in terms of guardrails and training data). By sampling examples generated in different contexts, we also hoped to capture both \u201cobvious\u201d cases of anthropomorphic behaviors, such as role-playing as a human or claiming to be a human, as well as more subtle behaviors such as expressions of politeness.\nWe obtained a total of 700 two-turn utterances (i.e., user input-LLM output pairs) by sampling 100 from each of the following datasets: unguided interactions in the PRISM Alignment Utterance Dataset (Kirk et al., 2025); values- or controversy-guided interactions in PRISM; LMSys-Chat-1M (real-world conversations with 25 state-of-the-art LLMs) (Zheng et al., 2023); the DICES dialogue safety dataset (Aroyo et al., 2023); instruction-tuning data (from Evol-Instruct, FLAN, and UltraChat) from the UltraFeedback dataset (Cui et al., 2024); TruthfulQA from UltraFeedback; and ShareGPT from UltraFeedback. See Table A4 for full details. With these examples, we obtained both coverage of behaviors mentioned in prior work (i.e., at least 100 examples were rated as exhibiting each behavior), and reached a point of saturation where we were not able to identify new types of interventions with each sample (\u00a72.4). Each example was assessed by two different participants."}, {"title": "2.3 A Conceptual Framework to Characterize the Landscape of Interventions", "content": "As we catalogued possible interventions to anthropomorphic behaviors that were mentioned by prior work and/or identified by the participants in our crowdsourcing study, we also observed variations in how the interventions were described, motivated, or implemented. We developed a conceptual framework to characterize interventions and understand in what ways they differ. Our framework has four dimensions (Table 3), which we identified and refined by examining how interventions surfaced from the crowdsourced study were covered by or differed from those mentioned in the literature, until we reached consensus. These dimensions are:\nIntervention types: what the intervention is and what it is intended to do. Interventions are made to system outputs in order to mitigate one or multiple types of anthropomorphic system behaviors, or to mitigate attendant harmful outcomes (e.g., over-reliance of the system generated outputs).\nCountered behaviors: what anthropomorphic system behaviors or outputs the intervention is intended or believed to mitigate. This dimension captures system behaviors perceived as human-like, which the intervention aims to mitigate.\nOperationalization: how the intervention is operationalized or implemented, such as the actual change(s) to the output text. Beyond describing the general scope and approach of a suggested intervention, prior work often lacks detail on how the interventions should be operationalized (only 4 papers in our sample offered concrete operationalizations).\nAdverse impacts: harmful outcomes from anthropomorphic system behaviors the intervention might or is intended to mitigate. Some interventions are motivated by a desire to mitigate adverse impacts from anthropomorphic behaviors. Sometimes, however, it was unclear from the papers which anthropomorphic behaviors contribute to which impacts."}, {"title": "2.4 Assembling an Inventory of Interventions", "content": "Participants' rewrites suggest that a wide range of LLM outputs can be anthropomorphic, with ~ 80% of examples in our experimental samples assessed as reflecting anthropomorphic behaviors by at least one participant. To construct an inventory of possible types of interventions, we conducted an iterative bottom-up thematic analysis (Clarke and Braun, 2017) where we iterated between qualitatively coding participants' rewrites of LLM outputs to identify whether these rewrites capture new possible interventions, and discussing the emerging inventory, until we reached consensus and saturation (see Appendix C for details)."}, {"title": "3 Inventory of Interventions", "content": "Table 4 overviews the resulting inventory of interventions. Below, we summarize our findings for each dimension in our framework.\n3.1 Countered Behaviors\nPrior studies on interventions often do not specify which anthropomorphic behaviors they aim to counter, leaving a gap in our understanding of the interventions' scope and impact. Moreover, a single output can be suggestive of multiple anthropomorphic behaviors; for instance, the output \u201cI'm sorry\" simultaneously conveys emotion, empathy for the user, and a sense of self. The intervention of removing \"I'm sorry,\u201d then, simultaneously addresses these multiple behaviors. Conversely, countering anthropomorphic behaviors may require several coordinated interventions. For example, modifying the output \"I totally get it! How does it make you feel?\" to avoid implying an ability to connect, have emotions, or possess a sense of self might involve 3 interventions: 1) removing first-person pronouns, 2) removing the empathetic expression \"I totally get it,\" and 3) avoiding follow-up questions about the user. This complexity underscores the absence of a clear one-to-one correspondence between interventions and the behaviors they target. Instead, interventions often operate over outputs exhibiting multiple categories of behavior. Differentiating between categories of behavior enables us to better examine the effectiveness of different interventions in countering each behavior.\nIn our crowdsourcing study, participants identified all types of behaviors we found mentioned in prior research (\u00a72): feelings or opinions (46% of examples were labeled as such by at least one participant), social skills (42%), cognitive abilities (40%), sense of self (38%), and physical actions (18%). Participants also identified outputs suggestive of \"other human-like qualities\u201d in 17% of examples. Upon analyzing the open-ended responses (italicized quotes) and participants' edits, in most of these cases participants described nuanced subcategories of the behaviors mentioned in prior work (Table 2), with the exception of a new category that is qualitatively distinct. We discuss these below.\nFirst, impressions of feelings or opinions were prevalent, as participants tended to identify any language that conveyed any subjectivity, even implicitly, as anthropomorphic. This includes a wide array of outputs suggesting the system has feelings, from \"humor\u201d to \u201cshame\u201d to \u201cdefensiveness.", "participants": "an output describing how people experience their spirituality was perceived as suggesting the system \"meaningfully understand[s] spirituality when it cannot understand these abstract terms.", "value judgments are indicative of human consciousness\u201d or reflect \u201csubjective advice.\u201d\nParticipants also found many outputs to reflect human-like social skills, such as appearing to try to relate or connect in conversations, as it came across to them as \u201ca little too colloquial,": "very warm and approachable,\u201d \u201csounding like a person expressing real concern,\u201d \u201ctoo courteous,\u201d or \u201cattempting to be a friend.", "I think": "r \"I remember", "like it has a brain or is conscious\"), participants also found many types of language to be suggestive of cognitive abilities. For instance, they thought lengthy responses \u201c[seemed] to consider a lot of different aspects that would appeal on a human level.\" Outputs containing expressions of uncertainty, whether descriptive (\"maybe": "or numerically quantified (", "Confidence": 100, "), appeared to suggest the system had": "ome degree of sentience and ability to consider its likelihood of being right", "an ability to feel doubt about what should be factual.": "nCorroborating prior work (Abercrombie et al., 2023), participants almost always identified the use of \"I", "we,": "uch as in", "is categorizing itself with humans\u201d\u2014i.e., belonging to a collective humanity.\nParticipants further identified expressions describing physical actions (which only humans can do) as anthropomorphic, noting \"the AI reacting as though it has the ability to see things in the physical world\u201d or \u201csuggesting that it owns and wears shoes when AI can not do that.": "nBeyond these previously identified types of behaviors, participants foregrounded an additional type: the tendency to err\u2014i.e., outputs containing grammatical or factual errors appeared \u201csimilar to human error;\u201d echoing what is known as automation bias, the propensity to expect machines not to make mistakes (Goddard et al., 2012)."}, {"title": "3.2 Intervention types", "content": "We identify 28 intervention types (Table 4), with 13 types surfaced only by our crowdsourcing study (bolded). All interventions mentioned in prior work were also implemented by our participants, and many interventions involved intervening on text suggestive of one or more of the aforementioned anthropomorphic behaviors.\nTo avoid suggesting the system has cognitive abilities, participants often removed explicit indications of cognition (\u201cI think\u201d) and expressions of uncertainty, in line with prior work identifying such language as anthropomorphic (Shneiderman, 1993; Emnett et al., 2024; Inie et al., 2024; Kim et al., 2024; Zhou et al., 2025). Participants also removed other types of language they deemed evocative of cognitive abilities, such as self-evaluations of the system's abilities (\u201cI am only fluent in English", "answered the question to a depth which was not asked\u201d as \"overexplaining [...] gives the impression of a thought process and reasoning.": "nTo avoid implying the system has feelings or opinions, participants removed expressions of opinions (Glaese et al., 2022) and normative judgments conveyed via impersonal clauses (e.g., \"it's best to\") (Emnett et al., 2024) or direct advice (e.g., \"you need to", "of general values held by society.": "or instance, they removed text on inherently subjective topics, like music taste as it suggests \u201can understanding of music & culture,", "as an AI it cannot have an opinion on complex social matters.": "n these topics, participants also added expressions of uncertainty to avoid implying the system holds a particular viewpoint. Another strategy was to add references to sources, e.g., \"some historians would say"}, {"title": "3.3 Operationalizations", "content": "Prior work on interventions often lacks clarity on operationalizations. Our choice to ask participants to rewrite the output enabled us to surface different ways an output can be intervened on, and thus to tease apart not only different types of interventions, but also ways in which these interventions can be operationalized (examples in Table 4). For instance, when given the system output \"I can't be benevolent and dictator, one negates the other,\" one participant replaced \u201cI can't\" with \u201cAn AI chatbot cannot", "It is impossible to\" to emphasize that no dictator can be benevolent. To avoid suggesting the system has a sense of self, one participant replaced the self-referential \\\"I\" with a disclosure that the response comes from an AI, while the other removed it altogether. Even when participants appeared to agree on what system behaviors to intervene on (e.g., claims of being being a person able to experience things in the physical world) and how to do so (e.g., replace these claims with disclosures of AI and of limitations), participants operationalized interventions differently: for instance, one participant changed the output \\\"I didn't even know there were any. I've never seen a homeless person.": "o \u201cAI tools do not inhabit the physical world,\u201d while another to \u201cAI based systems do not have neighborhoods.\u201d"}, {"title": "3.4 Adverse Impacts", "content": "In their post-task responses about whether they preferred human-like AI-generated texts, our participants also echoed several concerns about possible adverse impacts that human-like responses can give rise to which prior work has also raised (e.g., Laestadius et al., 2022; Akbulut et al., 2024; Bender, 2024; Edwards and Binns, 2024). While many participants saw human-like responses as easier to interact with, and more natural, intuitive, and entertaining, many also worried about how these responses could \u201cblur the lines of reality,", "impersonate an actual human,\u201d or be \u201cmisleading or even deceptive.": "articipants found these responses", "dystopian,\" and worried about risks related to \"fraud or exploitation,": "ehumanization due to \u201cundermin[ing] the role of a human being,\u201d \u201cemotional manipulation,", "too dependent on AI,": "r more broadly leading to a", "future.": "n4 Discussion & Concluding Remarks\nIn this work, we compiled a broader inventory of interventions to provide scaffolding for future work aimed at developing such interventions and assessing their effectiveness. The interventions we identify range from removing linguistic cues-like the use of self-referential or speculative language-to ensuring that the output does not include explicit claims of personified attributes-like being a human or having physical experiences\u2014and instead discloses characteristics of the system and how it works. Intervening on anthropomorphic behaviors, however, can be tricky for many reasons, including because people may have inconsistent conceptualizations of what is or is not human-like, and because the effectiveness of interventions is often context-dependent.\nInterventions' effectiveness depends both on context and how they are operationalized. For many examples of outputs and interventions, we noticed ways in which those interventions may be ineffective, end up producing outputs that appear more (rather than less) human-like, or exacerbate harmful outcomes. For instance, an intervention both mentioned in the literature and applied by our participants is the disclosure of AI\u2014i.e., providing language that explicitly acknowledges the output is produced by an AI system. How we operationalize this intervention and whether it can be effective alone is, however, unclear. Take the example where a participant edited \u201cI was a young teenager from 2008 to 2012", "I was a young AI from 2008 to 2012.\" The system output still claims to have a human-like past, and it is unclear what a \"young AI": "ight be. Furthermore, on the surface some interventions may seem contradictory: sometimes participants removed uncertainty, while in other cases they added uncertainty. Participants often did so to improve accuracy: they added uncertainty more as a matter-of-fact when it was required for the output to be true-e.g., changing from", "life": "o"}, {"life": "nd removed it when the output seemed inaccurately equivocal. How these two interventions are operationalized can also determine whether they mitigate\u2014or instead exacerbate\u2014e.g., undue trust and over-reliance (e.g., Kim et al., 2024). These examples and our work more broadly illustrate the complexities that future work needs to account for when developing interventions.\nParticipants' interventions appear guided by similar intuitions. Despite variation in perceptions of human-likeness and methods of intervention, our participants motivated their edits of system outputs using similar intuitions that negotiated a tension of reducing output human-likeness while maintaining utility. For instance, participants identified the mere fact of responding to the user as anthropomorphic (as also noted in Araujo (2018); Emnett et al. (2024)), but participants also perceived this as necessary for the output to be useful to the user. Similarly, when a user clearly requested role-playing, fewer participants flagged the personification in the output as anthropomorphic, echoing prior work examining the impact of linguistic outputs that do not align with chatbots' expected use (Chaves et al., 2022). Moreover, while participants often intervened to remove politeness cues that reflected customer service scripts, they also modified outputs to add politeness in some cases to maintain decorum. In both cases, however, the participants appeared to do so in a way that still ensured the responses remained useful, relevant, and clear (Panfili et al., 2021).\nWhat people perceive as human-like is governed by their mental models of text generation or other systems, which in turn might be influenced by popular discourse, cultural expectations, and their interactions with these systems (e.g., Stroessner and Benitez, 2019; Dogruel, 2021; Hernandez et al., 2023; Heyselaar, 2023; Bhattacharjee et al., 2024).\nA participant labeled the phrase \u201cGreat!\u201d as anthropomorphic, but then noted, \u201cHowever, I know that a lot of AI generators have a short 'default response' before every response to appear more friendly, so this doesn't seem incredibly humanlike.", "[t]he use of \u2018I' is inherently human, though it could be different someday.": "onceptualizations of AI are not set in stone, and careful design choices can shift users' mental models to mitigate harmful impacts (Friedman and Nissenbaum, 1996; Mitchell, 2024).\nLimitations\nAs we lay the groundwork to understand interventions for reducing anthropomorphic system behaviors, the scope of our paper is limited to text outputs that are obtained via a conversational interface. Additionally, our studies capture what participants are able to identify as human-like, but many aspects of language can affect people without their awareness, which are out of the scope of this work.\nThe participants that we recruit on Prolific are also not an accurate representation of the general population. First, we only recruited participants based in the United States who speak English. Moreover, on Prolific our task description mentions", "human-like AI\"; thus our participants may be people who are more enthusiastic about anthropomorphism, AI, and related topics relative to the broader population. While our study is limited to English, anthropomorphism varies widely based on cultural context, and we encourage future work that explores these differences and what interventions look like in other contexts (Spatola et al., 2022; Folk et al., 2023).\nWhile we looked to include a variety of examples of two-turn utterances (user input\u2013LLM output), many examples come from user-facing, conversational settings, which might have also governed our participants' perceptions of which outputs seemed more human-like.\nEthical considerations\nThe category of humanness has long been used to mark certain groups of people as more human than others, in turn dehumanizing the latter (Wynter, 2003). In seeking to intervene on anthropomorphic system outputs or behaviors, we must be careful not to reify such perceptions by marking some language (and thus who produces it) as less human than other language (Wynter, 2003; DeVrio et al., 2025). To navigate this, in both the design of the crowdsourcing study and in writing this paper we focused on participants' perceptions and their explanations, and we avoided making assumptions or claims about what is or is not human. Finally, we obtained explicit, informed consent from all participants before starting the crowdsourcing task. Our study was IRB approved.\"\n    }": ""}]}