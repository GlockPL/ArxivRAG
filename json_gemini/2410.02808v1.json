{"title": "KLDD: Kalman Filter based Linear Deformable Diffusion Model in Retinal Image Segmentation", "authors": ["Zhihao Zhao", "Yinzheng Zhao", "Junjie Yang", "Kai Huang", "Nassir Navab", "M.Ali Nasseri"], "abstract": "AI-based vascular segmentation is becoming increasingly common in enhancing the screening and treatment of ophthalmic diseases. Deep learning structures based on U-Net have achieved relatively good performance in vascular segmentation. However, small blood vessels and capillaries tend to be lost during segmentation when passed through the traditional U-Net downsampling module. To address this gap, this paper proposes a novel Kalman filter based Linear Deformable Diffusion (KLDD) model for retinal vessel segmentation. Our model employs a diffusion process that iteratively refines the segmentation, leveraging the flexible receptive fields of deformable convolutions in feature extraction modules to adapt to the detailed tubular vascular structures. More specifically, we first employ a feature extractor with linear deformable convolution to capture vascular structure information form the input images. To better optimize the coordinate positions of deformable convolution, we employ the Kalman filter to enhance the perception of vascular structures in linear deformable convolution. Subsequently, the features of the vascular structures extracted are utilized as a conditioning element within a diffusion model by the Cross-Attention Aggregation module (CAAM) and the Channel-wise Soft Attention module (CSAM). These aggregations are designed to enhance the diffusion model's capability to generate vascular structures. Experiments are evaluated on retinal fundus image datasets (DRIVE, CHASE_DB1) as well as the 3mm and 6mm of the OCTA-500 dataset, and the results show that the diffusion model proposed in this paper outperforms other methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Retinal vascular segmentation is a fundamental process in modern ophthalmology, playing a critical role in the diagnosis and monitoring of various ocular and systemic diseases. The complicated structure of blood vessels in the retina offers valuable insights, with abnormalities often serving as early indicators of diseases such as diabetic retinopathy, glaucoma, and age-related macular degeneration [1]\u2013[3]. If detected early, these conditions can be managed more effectively, highlighting the importance of accurate and detailed vascular segmentation. Moreover, the vascular structure in the retina is unique to each individual, making it a potential biomarker for biometric identification [4], [5]. The complexity of the retinal vascular system, characterized by varying vessel sizes, poses a significant challenge. Traditional methods of analysis, reliant on manual inspection or basic imaging techniques, are not only time-consuming but also prone to human error, emphasizing the need for advanced, automated segmentation techniques. The evolution of retinal fundus vascular segmentation techniques has been marked by significant advancements, ranging from hand-crafted feature-based methods to AI-driven approaches [6]. Initially, hand-crafted methods such as matched filtering, vessel tracking, and morphological processing were employed [7]\u2013[9]. While innovative at their inception, these techniques were limited in their ability to adapt to the variability in retinal images and were often ineffective in discerning finer vascular details. The advent of AI and machine learning introduced a new era, particularly with the emergence of deep learning models like convolutional neural networks (CNNs) [10], [11]. These AI-based methods have demonstrated superior performance in identifying and segmenting retinal vessels, owing to their ability to learn complex patterns and features from large datasets. However, challenges persist, especially in the segmentation of fragile vessels [12]. Even with advanced techniques like U-net [13], traditional CNN architectures often struggle to maintain the integrity of small vessels, resulting in incomplete or inaccurate segmentations. This limitation is predominantly due to the downsampling processes inherent in these networks, which can result in the loss of critical fine details essential for comprehensive vascular mapping.\nAddressing the limitations of existing methods, our approach introduces a novel application of the diffusion model for retinal vascular segmentation. The primary advantage of our model is reflected in the diffusion model's ability to gradually reconstruct clear vascular structures from random noise. The powerful image representation capabilities of Diffusion models enable the network to better understand the correlations among pixels and to classify individual pixel points effectively. Our model is structured into three primary components, with the diffusion model playing a fundamental role in generating the vascular structures. This process is essential for ensuring that the outcomes of vascular generation are controllable based on the input images. To achieve this level of control, the network incorporates a retinal feature extractor that integrates linear deformable convolution, enhancing the model's ability to capture detailed vascular features from the input retinal images. In addition, Kalman filtering is employed to optimize the coordinate positioning of the deformable convolution's field of view, ensuring a more precise alignment with the actual vascular structures. Lastly, to establish a stronger linkage between the input images and the generated vascular structures, we utilize the cross-attention aggregation module (CAAM) and the channel-wise soft attention module (CSAM). These modules are designed to enhance the relationship between the control images and the generated images, thereby ensuring that the generated vascular structures are not only accurate but also closely aligned with the specifics of the input retinal images. These approaches ensure a more detailed and accurate representation of the retinal vasculature, particularly by capturing the nuances of smaller vessels."}, {"title": "II. RELATED WORK", "content": "Deformable convolution [14], designed to dynamically adjust to the geometric variations of input data, has emerged as a key research area in image segmentation. This interest is spurred by its proficiency in managing images with irregular shapes and structures. Gurita et al. integrated deformable convolution into segmentation frameworks, significantly enhancing the delineation accuracy of boundaries within medical images [15]. Furthermore, Yang et al. introduced a novel modulated deformable convolution. This approach incorporates a modulation mechanism, meticulously refining the sampling locations in a dynamic manner to bolster the model's capacity for capturing detailed structural details [16]. In their paper [17], Qi et al. proposed a dynamic snake convolution that accurately captures the features of tubular structures by adaptively focusing on slender and tortuous local structures."}, {"title": "B. Diffusion Model in Segmentation", "content": "Denoising Diffusion Probabilistic Models (DDPM) constitute an innovative category of generative models designed to acquire the capability of transforming a noise distribution into the distribution of data samples. [18]\u2013[20]. In the field of image segmentation, diffusion models are uniquely employed to directly model or enhance the distribution of segmented images, leveraging their inherent generative power to refine segmentation outcomes derived through alternate methods [21]\u2013[23]. Amit et al. [24] introduced a new approach to end-to-end segmentation using diffusion. They achieved this by integrating the information from the input image with the current segmentation map estimate through the aggregation of outputs from dual encoders. Subsequently, this integrated data is processed through diffusion models equipped with additional encoding layers and decoders, facilitating the iterative refinement of the segmentation maps. Wu et al. have further advanced the field by proposing a unique application of diffusion models for segmenting medical images within the frequency domain [25]. This approach underscores the profound advantages of diffusion models in medical imaging."}, {"title": "III. METHODOLOGY", "content": "Figure 1 illustrates the principal architecture of our model: a diffusion process that generates vascular structures by applying noise and subsequently denoising. To ensure that the synthesized vascular structures adhere to the constraints imposed by the input image, we integrated an additional feature extraction network. Within this network, we introduced an innovative approach to capture vascular structural information through the use of a novel linear deformable convolution. Furthermore, to minimize the accumulation of positional errors in the field of view for linear deformable convolution, we employed Kalman filtering for iterative optimization of coordinate positions in the deformable offsets. To guarantee that the vascular structures generated by the diffusion process are controlled by the input image, we employed the cross-attention aggregation module (CAAM) to merge the compressed vectors from both the feature extraction module and the diffusion model's noise prediction. Additionally, we utilized a channel-wise soft attention module (CSAM) in the decoder. This mechanism is designed to adjust the channel-specific weights during the fusion of features from both the linear deformable output and the diffusion denoising process, enhancing the perception of vascular morphology during denoising."}, {"title": "B. Diffusion Model for Vessel Genetation", "content": "Our model is designed following the diffusion model framework as outlined in [18], which integrates both forward and reverse processes. The forward process, represented by q(x1:Tx0), either through a Markov or non-Markov chain, systematically transitions the initial data distribution xo ~ q(xo) to a state of pure noise x\u0442. In contrast, the reverse process, symbolized as po(x0:T), employs a progressive denoising strategy to revert the noise sequence back to the original data distribution. This model employs a U-Net to predict xt-1 from Xt for each step t \u2208 {1,...,T}. During the training phase, with a known ground truth for xt-1, the model is optimized using the Mean Squared Error (MSE) loss. In the sampling phase, the process commences with noise XT ~ N(0,1), iteratively sampling for T steps to synthesize the final image 20. The forward process q is described by the formulation:\n\n$q(x_{1:T}|x_0) = \\prod_{t=1}^{T} q(x_t|x_{t-1}),$ (1)\n\nDuring each iteration of the forward process, Gaussian noise is incorporated in accordance with:\n\n$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_t \\mathbf{I}_{n \\times n}),$ (2)"}, {"title": "C. Vascular Features Extracted Module", "content": "Our module for the extraction of vascular structure features utilizes a UNet architecture integrated with diffusion noise prediction. Within this module, we replace standard convolutions with our proprietary linear deformable convolution. This convolution structure inspired by Deformable Convolution Networks (DCNs) [14] and Dynamic Snake Convolution Networks (DSCNet) [17]. DCNs dynamically adjust their field of view by predicting offsets, thereby extending the convolutional filed of view to better capture object-relevant regions. The approach iteratively computes the complete receptive field by estimating the discrepancy between the current and previous locations. This technique effectively maintains the continuity of linear configurations, but it also facilitates the cumulative accumulation of predictive inaccuracies. In our research, we employ a strategy derived from Kalman filter theory to mitigate the error accumulation in computing offset positions within the linear deformable convolution.\nThe linear deformation module's key responsibility is learning the necessary offsets for adapting the convolution kernel's field of view. As depicted in Figure. 2(b), these learned offsets can shift the convolution kernel's initial field of view (white circles), to one that more closely aligns with the vascular structures (blue circles). As indicated by the blue grid circles in Figure. 2(b), some coordinates exhibit excessive offsets, necessitating correction for these excessive deviations. Therefore, Kalman filtering can be applied to optimize these deviations by weighting them with previous coordinate positions, leveraging historical information for optimization as shown in Figure. 2(c). Kalman filter module iteratively reduces offset-induced errors, ensuring shifts more accurately reflect the true vascular structure. Following this adjustment, a linear convolution captures the adapted field of view, thereby allowing the entire module to adeptly feature-extract from vascular areas via deformable linear convolution.\nIn our work, one-dimensional linear convolution kernels sized 9 \u00d7 1 and 1 \u00d7 9 are employed. The discussion is focused solely on the horizontal coordinates, given that the vertical coordinates are identical. Each convolution kernel is denoted as Ker = (xi\u00b0c), where c = {0, 1, 2, 3, 4}. DSCNet utilizes a learnable offset & to predict the deviation in coordinates of the deformed convolution kernel xi = Xi-1 + \u03b4\u03af. Each new coordinate is the sum of the previous coordinate and the predicted deviation, resulting in cumulative errors. To mitigate error accumulation in determining the positions within the offset of linear deformable convolution, our approach incorporates a Kalman filter based method. The Kalman filter offers an effective strategy for minimizing these errors by balancing current and past values [27]. The optimization of linear deformable convolution by Kalman filtering is achieved by assigning a weight K to the convolution kernel offset di. For a 1x9 convolution kernel, we start from the center of the kernel 20, and then iteratively compute xi = (1 \u2013 Ki)xi\u22121+ Ki(x-1+di)(i = 1, 2, 3, 4) using Kalman update Equation 6. Here, Ki denotes the Kalman gain, computed iteratively based on Equation 6, where p\u00e5 signifies the estimate covariance and r is a hyperparameter related to measurement errors from neural network outputs.\nThe hyperparameter r is empirically set to 0.01. The initial values of po and 20 are set to 1 and 0, respectively. These parameters will be updated iteratively during the process.\n\n$K_i = \\frac{P_{i-1}}{P_{i-1}+r}$ (6)\n\n$x_i = (1 - K_i)x_{i-1} + K_i (X_{i-1} + \\delta_i) = x_{i-1} + K_i\\delta_i$\n\n$P_i = (1 - K_i)P_{i-1}$"}, {"title": "D. Feature Aggregation Module by Attention", "content": "The feature aggregation module is structured into two main components. The first component utilizes a cross-attention aggregation module (CAAM) in the encoder's compressed feature space. This mechanism is designed to identify and emphasize the correlations between the features of the input image and the attributes obtained through the diffusion denoising process. The second component entails the application of channel-wise soft attention module (CSAM) across decoder feature maps to learn the channel-specific weights between the feature maps of the input image and the diffusion denoising features, thereby guiding the focus on vascular regions throughout the diffusion denoising sequence. The inputs of the CAAM are two matrices of feature maps with same dimensions (cxhxw). These matrices are initially flattened and transformed into shape of (h \u00d7 w) \u00d7 c . Subsequently, feature maps from both the input image and the diffusion denoising are converted into QKV by a projection module, respectively. The product of matrices Q and K generates an attention map, which then multiplied by the V matrix, yields the aggregated outcome. Such mechanism enhances the diffusion denoising process's focus on the original image's vascular structures. Conversely, the CSAM initiates with the max pooling of the feature maps derived from the input image's linear deformable convolution, encapsulating the vascular structure's control information into a single unit, thus reducing the feature maps from dimensions hxw x c to 1 \u00d7 1 \u00d7 c. In parallel, the diffusion model's feature maps utilize average pooling, similarly downsample to 1 x 1 x c, preserving only the fundamental information of each channel. Then, a cross-attention module evaluates the significance across various channels of the two vectors. Ultimately, the dimensions of the channel attention (1 \u00d7 1 \u00d7 c), are expanded to match the dimensions of the original feature maps. Subsequently, weights derived from learned parameters are applied to both feature maps before amalgamating them into an updated feature maps."}, {"title": "E. Loss Function", "content": "In the segmentation of vascular structures, it is imperative not only to capture the details of vascular elements but also to ensure the overarching continuity within the vascular domain. To preserve topological continuity in the segmentation outcomes, a clDice loss is introduced, which leverages topological structure similarity as delineated in [28]. We incorporate this topological similarity along with the standard noise prediction loss LN to form our new loss. Topological similarity measurement is delineated by Equation 7, where VL represents the ground truth of vascular segmentation, Vp the predicted outcome, SL the skeleton derived from the ground truth, and Sp the skeleton extracted from the predicted outcome. The topological accuracy Tprec(SP, VL) = $ \\frac{S_P \\cap V_L}{S_P}$ and topological sensitivity Tsens(SL, VP) = $ \\frac{S_L \\cap V_P}{S_L}$ are defined.\n\n$L_N =\\mathbb{E}_{x_0,\\epsilon,t}[||\\epsilon - \\epsilon_{\\theta}(\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon,t)||^2]$\n\n$clDice(V_P, V_L) = 2 \\times \\frac{T_{prec}(S_P, V_L) \\times T_{sens}(S_L, V_P)}{T_{prec}(S_P, V_L) + T_{sens}(S_L, V_P)} (7)$\n\nFinally, we sum up the clDice and denoise loss functions together to form the loss function.\n\n$Loss = L_N + L_{clDice} (8)$"}, {"title": "IV. EXPERIMENT", "content": "All experiments are carried out on publicly available ophthalmic datasets. The DRIVE [29] dataset contains 40 retinal images (584x565 pixels, 45\u00b0 FOV), divided into training and test sets of 20 images each, with annotations from one or two experts. CHASE_DB1 [30] features 28 images (960x999 pixels, 30\u00b0 FOV) from 14 children (one image per eye), each annotated by two experts. OCTA-500 [31] comprises 500 OCTA projection images: 300 images with a 6mm\u00d76mm field of view and 200 images with a 3mm\u00d73mm field of view, all provided with corresponding annotations."}, {"title": "B. Evaluation Metrics", "content": "We calculate the area under the ROC curve (AUC) between the segmentation results and the ground truth. Additionally, we systematically evaluate other segmentation metrics, including accuracy: Acc = (TP + TN)/(TP + TN + FP + FN), sensitivity: Sen = TP/(TP + FN), specificity: Spe = TN/(TN + FP), F1 score or DICE score: F1 = DICE = 2TP/(2TP + FP + FN), and Intersection over Union: IOU = (TP)/(TP + FP + FN))."}, {"title": "C. Implementation Details", "content": "Our experimental setup utilizes a single NVIDIA RTX A5000 GPU with 24GB memory and is implemented using PyTorch. The learning rate is initially set to le-4, and for optimization, the Adam optimizer is employed with a weight decay of le-5. During the training phase, preprocessing of images is conducted as follows: color images are converted to grayscale and uniformly normalized. Subsequently, all images are first divided into patches of 256 \u00d7 256, and then during the inference stage, the segmentation results for each patch are reassembled. The dataset is trained over 100 epochs. To mitigate overfitting, online data augmentation techniques such as horizontal and vertical flipping are employed, along with the addition of Gaussian noise using a 5x5 kernel."}, {"title": "D. Results of Segmentation", "content": "In the experiment section, we presented a comprehensive evaluation of our proposed model's performance in segmenting vascular structures from medical imaging datasets, specifically focusing on fundus images and Optical Coherence Tomography Angiography (OCTA) datasets. We compared with models from the UNet family, including UNet [13] and FR-UNet [32], along with PVT-GCASCADE [33] which employs attention mechanisms, and SwinUNETRV2 [34] and TransUNet [35], which incorporate transformer architectures. Additionally, DUnet [36], which utilizes deformable convolutions, and MedSegDiff [25], which integrates diffusion techniques, were also employed for benchmark testing. Figure 3 displays the segmentation results derived from both our model and conventional network architectures. From the cropped sections in the illustration, we can observe that numerous delicate and barely visible vascular formations, crucial for precise diagnosis and commonly present in retinal images, are consistently overlooked or inadequately captured by traditional networks. Despite their effectiveness across various applications, compared networks lack the necessary sensitivity for identifying the smallest vessels, which are crucial indicators for the early detection of a range of ocular diseases. Conversely, our network exhibits remarkable capability in the identification and segmentation of small vessels, adeptly capturing complex vascular nuances frequently missed by conventional methods. This success is attributed to the intrinsic characteristics of our approach, which fundamentally utilizes a diffusion model for image generation guided by the features of the original images, thereby generating images with enhanced continuity. Furthermore, our adaptive linear deformable structure effectively seizes the features of vascular structures. The inclusion of CSAM and CAAM aids in enhancing feature aggregation, which in turn more effectively captures the relationship between the vascular structures in the input images and the noise-reduced feature maps generated through diffusion.\nMoreover, our evaluation encompasses a range of metrics, including Area Under the Curve (AUC), Accuracy (ACC), and DICE coefficients, to conduct quantitative analysis. This analysis serves to underscore the enhanced performance of our proposed model relative to alternative approaches. Detailed quantitative comparisons among various models on the DRIVE and CHASE_DB1 datasets are provided in Table I, illustrating the effectiveness of our method in segmenting vascular structures within retinal images captured through fundus photography. The crucial role of Table I in underscoring our model's precision in identifying vascular structures, a critical aspect of retinal image analysis, is emphasized. Additionally, Table II outlines our model's segmentation capabilities on the OCTA dataset, further validating the proposed structure's versatility and robustness across different imaging modalities.\nThe empirical data in Tables Icompellingly establish new standards for accuracy (ACC) on the DRIVE, and CHASEDB1 datasets, achieving remarkable scores of 97.55% and 98.04%. The performance is further solidified by our method's superior results in DICE scores when compared against alternative methods across the DRIVE and OCTA datasets. However, the scope of our method's superiority is not limited to just ACC and DICE scores. A broader examination across additional evaluative metrics reveals our method's strength, showcasing a consistent and robust performance that transcends traditional evaluation paradigms. Furthermore, the thorough analysis of our results underscores the methodological rigor and scientific inquiry underpinning our approach. By setting new benchmarks in ACC and DICE scores and extending the evaluation to encompass a broader range of metrics, our method demonstrates a holistic and nuanced understanding of the challenges inherent in retinal image analysis.\nWithin the analytical scope of the OCTA dataset, our investigation primarily focused on two key metrics: Accuracy (ACC) and the DICE coefficient. Remarkably, the accuracy achieved on the OCTA_3mm and OCTA_6mm datasets stood at 98.92% and 98.23%, respectively. Furthermore, the DICE scores recorded for the OCTA_3mm and OCTA_6mm datasets were 91.28% and 89.08%, respectively, demonstrating our method's superior capability in precise segmentation. This exploration into the OCTA dataset's complexities not only affirms our model's superior performance but also enriches our understanding of its broad applicability and the technological strides we have achieved."}, {"title": "E. Structural Continuity Preservation", "content": "To assess our method's effectiveness in preserving the overall continuity of vascular structures, we employed the Centerline Dice (clDICE) metric to evaluate the topological continuity of tubular structures. As illustrated in Table III, our method consistently surpasses others in sustaining vascular continuity across all tested datasets. This superior performance suggests that KLDD is particularly adept at maintaining the overall continuity of vascular structures, a crucial aspect for dependable medical image analysis. In the ablation study section, we will further explore the influence of our model on vascular continuity."}, {"title": "F. Ablation Studies", "content": "To evaluate the effectiveness of each module proposed in our study, we conducted ablation experiments on individual submodules, the results of which are displayed in Table IV. This ablation study assessed the contributions of the proposed modules integrated into a DDPM baseline. Integrating the linear deformable module alone resulted in a significant improvement in segmentation performance, increasing the Accuracy (ACC) by nearly 1% and the DICE score by an average of 2%. The Kalman filtering module has also contributed to an improvement in segmentation accuracy, particularly reflecting an average increase of nearly 1% in the vascular continuity metric (cIDICE). We also visualized the receptive field of our linear deformable convolution in Figure 4, which shows our LD module provides a tighter fit to the vascular structure.\nIn addition, the inclusion of CSAM and CAAM for feature aggregation has increased accuracy by 0.5%. More importantly, the integration of the feature aggregation modules has resulted in an enhancement of more than 2.5% in the vascular continuity metric for segmentation outcomes. This indicates that the feature aggregation modules CSAM and CAAM are more effective globally in capturing the overall structural information of the vessels."}, {"title": "V. CONCLUSIONS", "content": "In this paper, we introduce a network called KLDD-Net, which is structured around a diffusion model as its primary backbone. This network integrates a specialized feature extraction module that incorporates linear deformable convolution, with Kalman filtering employed to optimize the convolution's field of view. This module plays a crucial role in extracting salient features from the input images. These features are subsequently utilized as control information, integrated into the diffusion denoising module. The embedding vectors generated by the encoder within the feature extraction module and those from the diffusion denoising module are aggregated through a cross-attention mechanism. This integration allows the diffusion denoising module to effectively incorporate and process information from the input images within the space of embedding vectors."}]}