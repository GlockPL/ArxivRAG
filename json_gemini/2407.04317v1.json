{"title": "Knowledge-based Drug Samples' Comparison", "authors": ["GUILLEMIN S\u00e9bastien", "ROXIN Ana", "DUJOURDY Laurence"], "abstract": "Drug sample comparison is a process used by the French National Police to identify drug distribution networks. The current approach is based on a manual comparison done by forensic experts. In this article, we present our approach to acquire, formalise, and specify expert knowledge to improve the current process. We use an ontology coupled with logical rules to model the underlying knowledge. The different steps of our approach are designed to be reused in other application domains. The results obtained are explainable making them usable by experts in different fields.", "sections": [{"title": "I. INTRODUCTION", "content": "The fight against drug trafficking has been one of the French government's priorities since the end of 2019 and has led to the creation of the National Stup plan. This plan comprises 55 measures, including the use of new indicators to understand consumer habits and dealers' methods. The work described in this article is part of this plan and aims to support scientific experts in the decision-making process for narcotic profiling.\nAs part of the fight against drug trafficking, several arrests may be made, often accompanied by seizures. Forensic experts perform several analyses on samples from a seizure. They aim to correlate different samples from different seizures to identify trafficking networks best. To do so, experts use sample matching to pair samples according to their characteristics. Paired samples constitute an ensemble called a batch. The sample characteristics used are represented by different data, namely: macroscopic data (e.g., sample dimension, drug logos), qualitative data (e.g., list of active substances), quantitative data (e.g., dosage of substances) or non-confidential seizure data (e.g., date, place of seizure). This data concerns cannabis samples and tablets such as amphetamine. In France, such data is stored in the national STUPS database.\nIn this context, we present an approach for modelling both business domain knowledge and the analysis rules used in that domain. Our approach also covers the application of these rules to real data. The purpose of this work is to be used in a decision-making support process for reducing the workload of experts.\nThis article is structured as follows: Section II presents the scientific background and definitions needed to understand this article. Section III presents the related works. Sections IV and 0 present our approach and the results we obtained by applying it to drug samples' comparison. Section 0 discusses our approach compared to the works exposed in section III. Finally, section VII presents our conclusions along with future works."}, {"title": "II. SCIENTIFIC BACKGROUND AND DEFINITIONS", "content": "A. Symbolic artificial intelligence\nSymbolic Artificial Intelligence (AI) aims to represent and reproduce human cognitive reasoning using symbols in the form of knowledge representation systems [1]. Symbolic AI requires formal and explicit representations of a knowledge domain and mechanisms for deducing implicit knowledge from explicit facts. To do so, different logical languages can be used. We will introduce Description Logics (DL) [2] as it is used in our approach.\n1) Description Logics\nDLs are a subset of First Order Logic (FOL) [3]. Unlike FOL, DLs are generally decidable. In other words, specific algorithms (called decision procedures) can be applied over DL rules and will (generally) return a result in a finite time. DLs are used for knowledge representations in specific domains, primarily because of the adaptability of their expressiveness and their overall decidability.\nIn DL, languages are characterised by a set of constructors. Constructor combinations determine a language's expressivity. The more constructors are used, the more expressive a language becomes. However, raising expressiveness increases the execution time of decision procedures.\nThe basic (i.e., the minimal) description language is the Attributive Language with Complement (AL \u2013 C) language [2]. With this language concept descriptions are specified using atomic concepts as unary predicates and atomic roles as binary predicates. Using this language, concepts can be defined as:\n\u2022 the universal concept (\u22a4)\n\u2022 the bottom concept (\u22a5)\n\u2022 the atomic negation of an atomic concept A (\u00acA)\n\u2022 the intersection of two concepts C and D (C \u2293 D)\n\u2022 a concept with value restriction (\u2200R. C)\n\u2022 a concept with limited existential quantification (\u2203R. \u22a4)\n\u2022 the complement of a concept C (\u00acC)\nStarting from the AL-C language, additional constructors can be added to form more expressive languages. For example, by using the union constructor (noted \u2294) a concept C can be defined as equivalent to the union of two other concepts, A and B: C = A \u2294 B. The complete list of available constructors is out of the scope of this article and is provided in [2].\n2) Ontology\nAn ontology is an explicit and formal specification of a shared conceptualisation of a knowledge domain [4]. Ontologies are expressed using DL [1] languages and used for knowledge representation. An ontology comprises two main parts: the Terminological Box (TBox) and the Assertionnal Box (ABox). The TBox describes the terminological knowledge, i.e., the concepts and their properties. The ABox contains the instances of the concepts of the concept described in the TBox. A knowledge box (KB) is the combination of a TBox and an ABox.\nAn ontology is made to be easily sharable and reusable. This is a very interesting characteristic because it reduces the work required to describe the related domain knowledge.\n3) Reasoner\nA reasoner [5] is an algorithm that can validate and enrich ontology knowledge at a TBox or an ABox level. As an example, consider three concepts C1, C2 and C3 defined in the TBox. If the transitive relation r links CI to C2 and, C2 to C3 a reasoner will infer that CI is also linked to C3 by r due to its transitive characteristic.\nImplementing deduction processes is based on decision procedures, i.e., algorithms complying with the following 3 properties:\n\u2022 Stop: The algorithm must give the result in a finite time.\n\u2022 Correctness: The inferences produced are consistent with the associated semantics, meaning that what is syntactically true is also semantically true.\n\u2022 Completeness: All valid formulas can be demonstrated on the syntactic level.\nOne of the most used decision procedures is the Tableaux algorithm adapted for DL [6]. It is based on rebuttal evidence, i.e., a logical formula is verified by demonstrating that its negation is a contradiction.\nB. Semantic Web\nThe Semantic Web (SW) is \"a vision for the future of the Web in which information is given explicit meaning\" [7]. It is an ensemble of standards and technologies for defining computer processable knowledge representations. A complete description of the SW technologies is out of the scope of this article. The sections below provide a short overview of those used in our approach.\n1) Ontology in Semantic Web\na) Web Ontology Language\nWeb Ontology Language (OWL) families are ontology description languages based on DL that extend RDFS [8]. OWL 2 [9], the last version of OWL, defines three profiles enabling users to take advantage of certain features. Each profile has its own expressivity. From the least to the most expressive, these profiles are:\n\u2022 OWL2 EL: allows subclass axioms with the intersection, existential quantifier, all, nothing, and closed classes with a single member. It does not support negation, disjunction, universal quantifier, or inverse properties.\n\u2022 OWL2 QL: allows sub-properties, definition of sub- classes and domains/scopes. It does not support closed classes.\n\u2022 OWL2 RL: allows all axiom types, cardinality restrictions (on scope only \u22641 and \u22640), and closed classes with a single member. It does not allow some constructors (universal quantifier and negation on domain, existential quantifier and union of classes for scope).\nb) TBox and ABox\nOntology TBox explicitly describes concepts of a particular domain using classes. A class indicates the necessary and sufficient conditions for an instance to belong to this class. Each class may have a set of data properties representing a concept's features. For example, a sample of cannabis may have a weight and/or a height feature. Relations between concepts are represented by object properties between the corresponding classes.\nBesides the TBox, the ABox contains class instances (also called individuals). An instance can be seen as an object belonging to a class. To belong to a class, an instance must respect conditions defined in the TBox for this class. In this object, data properties have real values (from real data), and object properties link the instances together.\nc) Reasoners\nDifferent reasoners are used in SW. For example, Pellet [10] or RacerPro [11] are based on the Tableaux algorithm. They are both open-source reasoners based on OWL-DL.\nThe different DL constructors supported by these reasoners are different. On the first hand, Pellet supports SROIQ(D) constructors and, on the other hand, RacerPro supports SHIQ constructors.\n2) SPARQL\nSPARQL Protocol and RDF Query Language (SPARQL) [12] is a set of specifications that provide languages and protocols to query and manipulate RDF [13] graphs. SPARQL syntax is made of different clauses to refine queries. In particular, the WHERE clause is used to query data according to a certain condition. For example, the following query is used to find mothers with at least one child over the age of 10:\nSELECT DISTINCT ?mother\nWHERE\n{\n ?mother a :Mother.\n ?mother :hasChild ?child\n ?child :age ?age\nFILTER(?age > 10)\n}\nA SPARQL variable starts with a question mark. The above example uses three variables: ?mother, ?child and ?age. The FILTER clause is used to filter out all children under the age of 10.\nC. Domain Knowledge and Analysis Rules\nThe approach described in section IV required experts' knowledge. We consider this knowledge as being composed of two parts: the Domain Knowledge and the Analysis Rules. The Domain Knowledge comprises our KB namely the descriptive knowledge of the domain i.e., the concepts of the"}, {"title": "III. RELATED WORK", "content": "A. Forensic science\nAnalysis of drug data has been the object of several works. Our work relates to drug distribution network detection and drug profiling. This domain aims to match drug samples according to their characteristics to identify distribution networks. Different studies were conducted according to the drug type, for example, heroin ([14], [15], [16], [17], [18] and [19]), cocaine ([16], [17], [18]), MDMA or fentanyl ([20], [21] and [22]). Other works are more general and review the different methods used in forensics ([23] and [24]).\nThe evolution of trends has also been the subject of various studies ([25] and [26]). This work seeks to analyse how different substances have been used by drug producers over the years. The results of these studies are useful for making assumptions about future trends. This helps the authorities to consider proactive measures.\nFinally, there are publications on the use of AI in forensic science, field in which AI is an assisting tool attempting to overcome the limits of human biases in traditional approaches. Different uses of Al are reviewed in [27]. [28] focuses on the benefits and limitations of the use of Al methods in the forensic field.\nTo the best of our knowledge, only statistical approaches have been used to process the data and assist the forensic experts. No previous approach considers formal and explicit experts' knowledge modelling while comparing drug samples.\nB. Knowledge Graph Completion\nKnowledge Graph Completion (KGC) is a field that deals with enriching incomplete knowledge graphs [29]. This field is linked to our approach in the sense that the expert analysis rules are used precisely to complete knowledge of the domain. Many approaches based on statistical methods exist ([29] and [30]). For example, R-GCN [31] is based on messages passing between neighbour nodes according to the different relations in the graph. More traditional approaches can also be considered. [30] divided traditional approaches into heuristic, latent-feature, and content-based approaches. With statistical approaches, results are challenging to explain, making them unusable in critical domains such as medical or legal fields.\nTo the best of our knowledge, only a few methods based on a purely symbolic approach exist. For our approach, we have taken inspiration from such an existing method [32]. In this approach, SWRL [33] rules are used to enrich the knowledge base. The disadvantage of this approach is that links are automatically added to the knowledge base, leaving the expert no choice."}, {"title": "IV. OUR APPROACH", "content": "Our approach comprises four main steps: knowledge acquisition, domain knowledge modelling, analysis rules modelling and ontology querying. Each step is described in the following sections. The overall process is illustrated in Figure 1.\nA. Knowledge acquisition\nThe knowledge acquisition consists in acquiring the required knowledge of the application domain. By exchanging with the domain experts, the knowledge engineer builds two corpuses (i.e., natural language descriptions), one for each part of the domain knowledge (Domain knowledge and Analysis rules). Hence, he must obtain a set of concept definitions and a set of analysis rules (both in natural language).\nThis Knowledge Acquisition step is subject to the knowledge acquisition bottleneck [34]. It refers to the problem of slow and inaccurate knowledge acquisition while exchanging with the experts. Several factors impact the knowledge bottleneck, such as difficulties in understanding the business domain or lack of expert cooperation. Therefore, to facilitate exchanges with the experts, it is a good practice to follow a methodology such as the one described in [35]. This kind of methodology helps to identify key concepts and relations that will be used when modelling the ontology (see IV.B.1). For example, the knowledge engineer can seek to answer the following questions:\n\u2022 What queries must the ontology allow answering?\n\u2022 Who will use and maintain the ontology?\n\u2022 What concepts are used, and what is their definition?\n\u2022 What are the necessary and sufficient conditions for an instance to belong to a class?\nAdditionally, analysing databases allows for identifying additional knowledge and constraints. For example, the names of the tables of a relational database can help to identify certain concepts. In the same way, the presence of foreign keys can help to find concept relations. The conclusions drawn from the database analysis must always be validated by the experts. We assume that the knowledge engineer is not able to judge the relevance of the discovered knowledge by himself.\nB. Domain knowledge modelling\nThe second step deals with modelling the knowledge according to the definitions obtained previously. This step is divided into three sub-steps: Ontology modelling, Ontology populating and Ontology enrichment.\n1) Ontology modelling\nThe first sub-step is called Ontology modelling. It consists of modelling the ontology TBox according to the definition obtained during the Knowledge acquisition step (see IV.A). The process is the following:\n1. For each concept, create a class in the ontology TBox having the same name as the concept (e.g., Person).\n2. For each class, identify the concept properties in the associated concept definition. For each of these properties, create a data property whose domain is the class associated with the concept and whose range is the property's data type (e.g., xsd:string). The name of the new data property is the name of the concept property (e.g., class Person may have the data property age).\n3. From each definition, identify relations between concepts. For each of these relations, create an object property and set its domain and range by the definitions. The name of the object property depends on the nature of the relation between the two concepts. Generally, the name of object properties starts with a verb like \u201chas..."}, {"title": "V. RESULTS", "content": "In this section, we present the results we obtained by using our approach in the context of the Stup plan for semi- automatic matching of drug samples (see section I).\nA. Knowledge acquisition\nThe domain knowledge acquired by exchanging with the experts is composed of 20 concept definitions. As an example, a Sample is defined by the experts as follows:\nA drug sample is extracted from a sealed sample. A sample is characterised by a sample number and its drug type (\"cannabis\", \"cocaine\u201d, \u201cmiscellaneous\u201d or \u201camphetamine and derivatives\u201d). A sample has macroscopic characteristics (internal and external appearance, height, width etc.), active principles and cutting products. Experts can also comment on the sample. In addition, a sample may be grouped with other samples in a batch. In the case of narcotics on which a chemical profiling analysis is carried out, each sample is associated with a chemical profile.\nWe show in section V.B how this description and the STUPS data are used to obtain a populated ontology.\nConcerning the analysis rules, a set of nine rules has been obtained describing the conditions that two samples must have to be matched. Here is one of these rules:\nTwo samples match if their drug types are the same.\nWe show in section V.C how this rule is then translated into a DL rule and a SPARQL request.\nB. Domain knowledge modelling\nThe first step to model the domain knowledge is the Ontology modelling step. To do so, we used the domain knowledge expressed in natural language obtained previously. Using the above definition of a sample, we specify the following object properties for the Sample concept:\n\u2022 hasExternalAspect (range: Aspect)\n\u2022 hasInternalAspect (range: Aspect)\n\u2022 hasActivePrincipal (range: ActivePrincipale)\n\u2022 hasCutting Product (range: CuttingProduct)\n\u2022 hasChimicalProfile (range: ChimicalProfile)\n\u2022 isCloseTo (range: Sample)\n\u2022 comesFrom (range: Sealed)\nAdditionally, the following data properties are added to the Sample concept:\n\u2022 sampleNumber (rdfs:range xsd:string)\n\u2022 drugType (rdfs:range: {\u201ccannabis\u201d, \u201ccocaine\u201d, \"miscellaneous\", \u201camphetamine and derivatives\u201d})\n\u2022 comment (rdfs:range xsd:string)\n\u2022 height (rdfs:range xsd:float)\n\u2022 width (rdfs:range xsd:float)\n\u2022 diameter (rdfs:range xsd:float)\n\u2022 length (rdfs:range xsd:float)\n\u2022 thickness (rdfs:range xsd:float)\nThe Sample concept contains other data properties corresponding to other macroscopic characteristics which are not provided here but can be found in the ontology TBox provided below.\nUsing the definition of all the other concepts, we modelled an ontology TBox made of 20 concepts, 45 object properties and 40 data properties. Each concept is annotated with its definition. This ontology TBox can be found on GitHub (github.com/SebastienGuillemin/StupsOntology). Our ontology complexity is SROIQ(D) [36]:\n\u2022 S is an abbreviation for ALC.\n\u2022 R refers to the use of roles.\n\u2022 O refers to value restriction (owl:oneOf)\n\u2022 I is used for inverse properties.\n\u2022 Q is used for cardinality restrictions.\n\u2022 (D) refers to the use of datatype properties.\nAfter modelling the ontology, we performed the Ontology populating step. This step uses data from the STUPS\u00a9 to instantiate the ontology TBox. Explaining the program to retrieve and transform the data is out of the scope of this article but it is available on GitHub (github.com/SebastienGuillemin/etl). When populating the ontology, 68,972 instances were created, 20,001 of which were Sample instances.\nTo perform the Ontology enrichment, we loaded the populated ontology in a triplestore. We chose GraphDB which comes with several reasoners compatible with the complexity of our ontology. We chose the OWL-Max reasoner among the different reasoners available because it considers all the constraints used to define our TBox (graphdb.ontotext.com/documentation/10.3/owl- compliance.html). This Ontology enrichment added 382,205 new relations between instances in our ontology (increasing from 284,638 to 666,843). We have only measured the creation of relations concerning the data properties and object properties modelled in our ontology.\nC. Analysis rules modelling\nThe next step is to model the analysis rules. Using the knowledge acquired during the step described in V.A we will consider the following analysis rule:\nTwo samples match if their drug types are the same.\nWe show how this rule is converted into a DL rule and then into a SPARQL request. From this analysis rule, we identify that the ontology elements to consider are the Sample class and its drugType data property. These elements are present in the previously obtained ontology TBox thus we can continue.\nThe resulting DL rule must be made of the unary predicate Sample and the binary predicate drugType. We obtain the following DL rule:\nmatch(s1, s2) = Sample(s1) \u2229 Sample (s2)\n \u2229 drugType (s1, dt1)\n n drugType (s2, dt2) \u2229 dt1 = dt2 \u2229 s1\n\u2260 s2\nEach rule of our corpuses of nine rules is converted into a DL rule in the same manner.\nOnce all the analysis rules have been modelled using DL, they are translated into SPARQL requests. The above DL rule is translated as follows (once again, we do not specify the prefix to simplify reading)\nSELECT ?s1 ?s2 ?match\nWHERE\n{\n ?s1 a: Sample\n ?s2 a: Sample\n ?s1: drugType ?dt1\n ?s2: drugType ?dt2\nFILTER(?s1 != ?s2)\nBIND ((?dt1 = ?dt2) as ?match)\n}\nHere, the ?match variable indicates whether ?s1 and ?s2 can be matched. We translated the DL unary predicate Sample by using the SPAQRL keyword \u201ca\u201d. Thus, the two first lines of the WHERE clause ensure that ?s1 and ?s2 are :Sample instances. We then retrieve their respective drug type. To do so, we translate the binary predicate drugType into the relation :drugType. The drug type of ?s1 (respectively ?s2) is bound to ?dtl (respectively ?dt2). Then, we ensure that ?s1 and ?s2 are not the same :Sample instance by using the FILTER clause. Finally, if ?dt1 and ?dt2 have the same value the ?match variable is bound to True. Otherwise, ?match is bound to False.\nThis translation process from DL rules to SPARQL requests is applied to all analysis rules. As we have 9 analysis rules, we end up with nine SPARQL requests.\nD. Ontology querying\nThe final step of our approach consists of querying the ontology using the previously defined SPARQL request. We will illustrate this step by using the following analysis rules:\n\u2022 Two samples match if their drug types are the same.\n\u2022 Two samples match if they have the same chemical form.\n\u2022 Two samples match if their macroscopic features (i.e., height, width) differ by less than 5%.\nEach of these rules has been translated into DL and then into SPARQL. In order not to overload the article, we will not detail these steps for these rules.\nWe also consider the following samples:\nSample number 1\nSample 1 2\nDrug type Cannabis Cannabis\nChemical form Resin Resin\nWidth 200 millimetres 150 millimetres\nHeight 100 millimetres 100 millimetres\nEach SPARQL request of each rule is applied to these samples. This conduct to the following results :\n\u2022 Two samples match if their drug types are the same: True.\n\u2022 Two samples match if they have the same chemical form: True.\n\u2022 Two samples match if their macroscopic features (i.e., height, width, etc.) differ less than 5 per cent.\n \u039f Width: False.\n \u039f Height: True.\nUsing these rules, experts can decide whether samples match or not. For example, in this case, they can consider that the width characteristic is not relevant and conclude that the two samples match."}, {"title": "VI. DISCUSSION", "content": "Our work proposes an approach to acquire expert knowledge and use it to help experts during their decision process. An important aspect of our approach is that it does not directly modify the ontology, leaving the experts to make decisions. To the best of our knowledge, our approach is the first to use expert knowledge in the sample-matching process. Other works were based on statistical approaches (as exposed in section II).\nThe overall process, from knowledge acquisition to analysis with rules, is independent of the application domain. Then, using it in other domains requiring analytical expertise is possible. Nonetheless, some steps may require the use of other methodologies. It is the case of TBox modelling, which is based on Noy and McGuinness's guide [35].\nResults presented in section 0 show how our approach assists the experts. For each potential match (i.e., a pair of samples), the result of each rule is displayed to the author. Experts exploit the different results to decide.\nSo far, our approach is only limited to results' display. Moreover, our approach relies on SPARQL queries that can tend to slow down the overall process. Also, translating some analysis rules into SPARQL requests can be an arduous task. As part of our future work, we'll investigate replacing SPARQL queries with a simpler translation method."}, {"title": "VII. CONCLUSION AND FUTURE WORKS", "content": "We presented an approach based on ontologies and logical rules for decision support in comparing drug samples. We have defined a general process composed of four main steps, each step being designed to ease its reproducibility.\nOur approach has been successfully tested for assisting forensic experts in drug samples' comparison. Still, it has some limitations, that will be addressed in future works. Firstly, additional support to experts can be provided by allowing them to rank the different rules i.e., in the form of a hierarchy. Such ranking would enable controlling the impact (or the score) of each rule on the suggested decision. The most important rules would then have the greatest impact (or score) on the suggestions made by our approach.\nSecondly, rather than using SPARQL queries, we could directly enrich the reasoner rule set with our logical rules obtained during Logical rules' modelling (see section IV.C.1). This would eliminate the need to execute queries one by one over the populated ontology. Thus, the whole process would be more fluid by removing one step. However, the time needed by the reasoner to make inferences could increase to a greater or lesser extent. This depends on the number of rules and their expressivity (i.e., the DL constructors chosen). Reducing the expressivity of the underlying TBox can help in limiting the reasoning overhead. Additionally, inferences produced by the reasoner must not automatically be materialised in the KB. The experts must be the only ones to decide what to add to the populated ontology. So, it will be necessary to provide a mechanism for experts to decide whether the matching should be added to the ontology.\nFinally, matches are based solely on analysis rules which may not always be appropriate. In fact, in the actual process used by the experts, their intuition plays a very important role. Unfortunately, this intuition cannot be formalised in the analysis rules. One possible way of simulating expert intuition would be to use statistical approaches (e.g., neural networks). These statistical approaches could then be coupled with the approaches presented above to improve decision suggestions. Moreover, we can imagine that statistical models would allow suggesting new facts (e.g., additional relevant analysis rules) to the existing KB. This new knowledge would then be accepted or refuted by the domain experts. Approaches combining neural networks and symbolic AI belong to the domain of neuro-symbolic AI [37]. It seeks to benefit from the advantages of both areas without the disadvantages i.e., the learning capacity of neural networks and the explicability of symbolic AI."}]}