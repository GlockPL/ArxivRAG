{"title": "A SURVEY ON ALGORITHMIC DEVELOPMENTS IN OPTIMAL TRANSPORT PROBLEM WITH APPLICATIONS", "authors": ["Sina Moradi"], "abstract": "Optimal Transport (OT) has established itself as a robust framework for quantifying differences between distributions, with applications that span fields such as machine learning, data science, and computer vision. This paper offers a detailed examination of the OT problem, beginning with its theoretical foundations, including the classical formulations of Monge and Kantorovich and their extensions to modern computational techniques. It explores cutting-edge algorithms, including Sinkhorn iterations, primal-dual strategies, and reduction-based approaches, emphasizing their efficiency and scalability in addressing high-dimensional problems. The paper also highlights emerging trends, such as integrating OT into machine learning frameworks, the development of novel problem variants, and ongoing theoretical advancements. Applications of OT are presented across a range of domains, with particular attention to its innovative application in time series data analysis via Optimal Transport Warping (OTW), a robust alternative to methods like Dynamic Time Warping. Despite the significant progress made, challenges related to scalability, robustness, and ethical considerations remain, necessitating further research. The paper underscores OT's potential to bridge theoretical depth and practical utility, fostering impactful advancements across diverse disciplines.", "sections": [{"title": "1 Introduction", "content": "Measuring the difference between two probability distributions is a crucial task underpinning various contemporary data science applications. For example, it is used to evaluate the gap between a model's output distribution and the actual distribution in generative adversarial networks, to analyze intrinsic differences between point clouds in computer graphics, and to examine distributional shifts in transfer learning [Li et al., 2023]. A widely recognized method for measuring these dis-"}, {"title": "2 Optimal Transport Theory", "content": "This paper is organized as follows: Section 2 delves into the theoretical underpinnings of OT, covering its classical formulations and key mathematical properties. Section 3 examines computational advancements and innovative algorithmic approaches. Section 4 explores the diverse applications of OT across various domains. Lastly, Section 5 outlines emerging trends, outstanding challenges, and potential directions for future research in OT."}, {"title": "2.1 Theoretical Foundations", "content": "OT theory offers a robust mathematical framework for comparing and transforming probability distributions. Its origins trace back to Monge's early contributions in the eighteenth century, later expanded and formalized by Kantorovich. OT provides a unified approach for addressing both discrete and continuous probability measures. This section introduces the fundamental theoretical concepts of OT; for a comprehensive introduction, readers are encouraged to consult [Peyr\u00e9 et al., 2019].\nMonge's approach tackles the challenge of identifying a transport map that reduces the expense of transferring mass from one distribution to another. Let $x \\in \\mathbb{R}^n$ and $y \\in \\mathbb{R}^m$. The goal of Monge's problem is to identify a mapping $T : X \\rightarrow Y$ that transfers mass from $\\alpha$ to $\\beta$, where $X \\subseteq \\mathbb{R}^n$ and $Y \\subseteq \\mathbb{R}^m$. The measures $\\alpha$ and $\\beta$ are defined as:\n$\\alpha = \\sum_{i=1}^n a_i \\delta_{x_i}, \\quad \\beta = \\sum_{i=1}^m b_i \\delta_{y_i}$\nwhere $a = [a_1, ..., a_n]$ and $b = [b_1, ..., b_m]$ represent the weights associated with the locations $x \\in X$ and $y \\in Y$, respectively. Here, $\\delta_z$ denotes the Dirac measure at position $z$, which can be interpreted as a unit of mass concentrated at $z$. When $a \\in \\Delta_n$ and $b \\in \\Delta_m$ (the probability simplices in $\\mathbb{R}^n$ and $\\mathbb{R}^m$, respectively), these measures describe probability distributions. More generally, if all elements of $a$ and $b$ are nonnegative, $a$ and $\\beta$ describe positive measures. Monge's problem seeks a mapping $T$ that assigns each $x_i$ in $\\alpha$ to a unique $y_j$ in $\\beta$ such that the mass of $\\beta$ at $y_j$ is equal to the total mass transported from points $x_i$ satisfying $T(x_i) = y_j$, i.e.,\n$b_j = \\sum_{i:T(x_i)=y_j} a_i \\quad \\forall j \\in \\{1,...,m\\}$\nThis condition can be compactly expressed as $T_\\#\\alpha = \\beta$, in which $T_\\#\\alpha$ represents the forward mapping of $\\alpha$ through $T$. The transport map $T$ is required to minimize a given transportation cost, parameterized by a cost $c(x, y)$, which leads to the optimization problem:\n$\\min_T \\{ \\sum_i c(x_i, T(x_i)) : T_\\#\\alpha = \\beta \\}$\nAlthough Monge's formulation is intuitive and conceptually straightforward, it faces several significant challenges. The problem is inherently non-convex, which makes it difficult to address using"}, {"title": "2.2 Algorithmic Foundations", "content": "The OT problem, expressed in the Kantorovich formulations (2) and (3), can be interpreted as a special instance of LP, with connections to minimum-cost network flow problems. The primal"}, {"title": "2.3 \u20ac-approximation", "content": "An often-used metric for evaluating the numerical performance of approximation algorithms for the OT problem is the computational cost required to compute a solution, P, that achieves an e-level approximation. This solution satisfies:\n$P \\in \\{ P \\in \\mathbb{R}_+^{n \\times m} : P1_m = a, P^T1_n = b \\}$\nAnd ensures:\n$\\langle C, P \\rangle - \\langle C, P^* \\rangle \\leq \\epsilon,$\nin which, P* denotes the optimal solution of the OT problem (2). This inequality is modified to account for the random nature of their outputs for stochastic algorithms and is expressed as:\n$\\mathbb{E} [\\langle C, P \\rangle] - \\langle C, P^* \\rangle \\leq \\epsilon.$\nThis formulation allows stochastic methods to measure performance in expectation, maintaining consistency with deterministic approaches while accounting for randomness in their solutions."}, {"title": "2.4 Entropic Regularization", "content": "The coupling matrix discrete entropy can be shown as:\n$H(P) := - \\sum_{i,j} P_{ij} (\\log(P_{ij}) - 1),$\nA similar definition applies to vectors. By convention, H(a) = -\u221e if any entry $a_j$ is zero or negative. Since $2^{H(P)} = - diag(1/P_{ij})$, combined with the property that $P_{ij} \\leq 1$, H is 1-strongly concave. Entropic regularization in optimal transport involves using -H to estimate solutions to the original transport problem (2). The regularized problem is formulated as:\n$\\min_P \\{ \\langle C, P \\rangle - \\eta H(P) : P1_n = b, P1_m = a, P \\in \\mathbb{R}_+^{n \\times m} \\}.$\nThe problem (4) has a unique optimal solution due to the n-strong convexity of the objective function. Introducing an entropic regularization term to the OT problem draws inspiration from concepts in 'transportation theory' [Wilson, 1969]. As \u03b7 \u2192 0, it has been shown [Cominetti and Mart\u00edn, 1994] that the solution $P_\\eta$ converges to the optimal solution of the Kantorovich problem with the highest entropy among all feasible solutions.\nP_\\eta \\rightarrow_{\\eta \\rightarrow 0} \\arg \\min_P \\{ -H(P) : P \\in U(a, b), \\langle C, P \\rangle = L_c(a, b) \\},$\nwhere\n$U(a, b) = \\{ P \\in \\mathbb{R}_+^{n \\times m} : P1_n = b, P1_m = a \\},$\n$L_c(a, b) = \\min_{P \\in U(a,b)} \\langle C, P \\rangle,$\nand\n$\\min_{P \\in U(a,b)} \\langle C, P \\rangle - \\eta H(P) \\rightarrow L_c(a, b)."}, {"title": "2.5 Multimarginal Problems", "content": "Rather than combining two input probability distributions by the Kantorovich formulation (2), it is possible to extend the approach to couple K probability distributions $\\{a_k\\}_{k=1}^K$, where $a_k \\in \\Delta_{n_k}$. This is accomplished by addressing the ensuing \u201cmulti-marginal optimal transport problem\" [Lindheim, 2023]:\n$\\min_{P \\in U(\\{a_k\\}_{k=1}^K)} \\langle C, P \\rangle,$\nThe set of feasible transport plans is characterized as:\n$U (\\{a_k\\}_{k=1}) = \\{ P \\in \\mathbb{R}_+^{n_1 \\times ... \\times n_K} : \\sum_{\\pi_l \\forall l \\neq k} P_{\\pi_1,...,\\pi_K} = a_{i_k} \\quad \\forall i_k \\},$\nand the transportation cost is given by:\n$\\langle C,P \\rangle = \\sum_k \\sum_{i_k=1}^{n_k} C_{i_1,...,i_K} P_{i_1,...,i_K}.$\nThis framework naturally extends the \u201centropic regularization\" framework (4) to the multi-marginal one:\n$\\min_{P \\in U(\\{a_k\\}_{k=1})} \\langle C, P \\rangle - \\eta H(P),$\nwhere H(P) represents the transport plan entropy P, acting like a regularizer to ensure computational tractability and promote smoother solutions."}, {"title": "2.6 Unbalanced Optimal Transport", "content": "One significant limitation of traditional OT is the requirement that the two input measures (\u03b1, \u03b2) must have the same total mass. While various workarounds, such as renormalizing the input measures, have been proposed, comprehensive unifying theories have recently been developed to address this issue. One such approach involves the $-divergence, denoted as D, defined as:\n$D_{\\Phi}(a || b) = \\sum_{i \\in Supp(b)} \\Phi \\left( \\frac{a_i}{b_i} \\right) b_i + \\sum_{i \\in \\overline{Supp(b)}} \\Phi_\\infty a_i,$\nwhere & is an entropy function, $Supp(b) = \\{i \\in \\{1, . . ., n\\} : b_i \\neq 0\\}$, and:\n$\\Phi_\\infty := \\lim_{x \\rightarrow \\infty} \\frac{\\Phi(x)}{x}.$\nIf \u2030 = \u221e, it indicates that $ increases at a rate that surpasses any linear function and is classified as \"superlinear.\" Every entropy function & leads to a related -divergence, also called \u201cCisz\u00e1r divergence\" or f-divergence [Csisz\u00e1r, 1967].\nBased on Liero et al. [2018] and Lindheim [2023], the original Kantorovich formulation (2) is relaxed to consider arbitrary positive measures $(a, b) \\in \\mathbb{R}_+^n \\times \\mathbb{R}_+^m$ by penalizing marginal deviations using a divergence D4. This relaxation is related to reducing an optimal transport distance between approximate measures:\n$\\min_{\\tilde{a}, \\tilde{b}} L_c(a, b) + \\tau_1 D_{\\Phi}(a||\\tilde{a}) + \\tau_2 D_{\\Phi}(b||\\tilde{b}) = \\min_{P \\in \\mathbb{R}_+^{n \\times m}} \\langle C, P \\rangle + \\tau_1 D_{\\Phi}(P1_m||a) + \\tau_2 D_{\\Phi}(P^T1_n||b),$\nwhere the parameters (T1, T2) control the balance between penalizing mass variations and transporting the mass. In the limit as T\u2081 = T2 \u2192 \u221e, (\u03a3; a\u2081 = \u2211; b; as the \u201cbalanced\u201d case), this relaxed formulation is the original optimal transport problem alongside hard marginal constraints (2)."}, {"title": "2.7 Wasserstein Barycenters", "content": "\u201cWasserstein barycenters\" extend the notion of a mean or average to the domain of probability distributions within the optimal transport [Lindheim, 2023]. They represent a \u201ccentral\u201d probability distribution that minimizes the total Wasserstein distance to a given set of input distributions. This concept has become foundational in applications such as clustering, data summarization, and interpolation in high-dimensional probability spaces.\nFor K probability distributions $\\{a_k\\}_{k=1}$ defined on a shared finite domain $\\{x_i\\}_{i=1}^K$, with associated weights $w = \\{w_1,...,w_k\\}$ that satisfy $w_k \\geq 0$ for all k and sum to 1, i.e., $\\sum_k w_k = 1$, the Wasserstein barycenter b is the distribution that solves [Lindheim, 2023]:\n$b \\in \\arg \\min_{b \\in \\Delta_m} \\sum_{k=1}^K w_k \\min_{P_k} \\{ \\langle C_k, P_k \\rangle : P_k1_m = a_k, P_k^T1_n = b, P_k \\in \\mathbb{R}_+^{n_k \\times m} \\}.$\nThe computation of Wasserstein barycenters in their general form is computationally challenging due to the nested optimization over transport plans Pk for each distribution and the need to satisfy constraints across all input distributions while determining the barycenter b. However, the barycenter problem simplifies to a linear program for discrete probability distributions. Specifically, one"}, {"title": "3 Algorithms for OT Problem", "content": "OT theory provides a robust mathematical framework for comparing and transforming probability distributions. However, its practical application is often hindered by substantial computational challenges. The core difficulty lies in solving the OT problem efficiently, especially in the context of high-dimensional data or large-scale distributions. Traditional approaches, such as linear programming, yield exact solutions but are computationally intensive and impractical for large datasets. Consequently, notable advancements have been made in developing more efficient algorithms tailored to various problem settings.\nVarious computational methods have been introduced in recent decades, encompassing both exact solvers and approximate strategies. Exact solvers, such as the simplex and network simplex algorithms, offer precise results but face scalability issues as problem size grows. Approximate methods, including entropic regularization and sliced Wasserstein distances, balance computational efficiency and accuracy, making OT feasible for large-scale applications. Furthermore, specialized algorithms have been designed to handle unique challenges, such as unbalanced measures, and to compute Wasserstein barycenters for tasks like clustering and interpolation.\nThis section examines the advancements in OT computation, tracing the trajectory from classical exact solvers to contemporary scalable approaches. It highlights key algorithmic innovations, their advantages and limitations, and their influence on real-world applications. By addressing the computational barriers inherent to OT, these methods have significantly broadened their utility across various disciplines, including machine learning, image processing, and economics."}, {"title": "3.1 Exact Algorithms", "content": "Exact algorithms for the OT problem are fundamental tools designed to compute precise solutions to transportation and allocation tasks. These methods are rooted in classical linear programming"}, {"title": "3.1.1 Algorithms based on Network Flow", "content": "Section 2 explains that the OT problem, formulated through the primal (2) and dual (3), can be interpreted as the minimum-cost network flow problem in linear programs. The network simplex algorithm is well-suited for solving transportation problems because it exploits the underlying network structure [Peyr\u00e9 et al., 2019]. In the context of OT, this algorithm represents the source and target distributions as nodes in a bipartite graph, with edges weighted by the cost matrix Cij. It iteratively adjusts the flow along cycles in the network to minimize total transportation cost, maintaining feasibility at every step by satisfying the marginal constraints. The network simplex algorithm has proven to be especially efficient for medium-sized OT problems with sparse cost matrices. Notably, Orlin [1993] provided the first proof of the algorithm's polynomial time complexity. Subsequently, Tarjan [1997] improved the complexity bound to $O((n + m)nm \\log(n + m) \\log((n + m)||C||_\\infty))$ by introducing more efficient data structures for selecting pivoting edges. This enhancement significantly optimized the algorithm's performance, making it more practical for problems with large but structured cost matrices."}, {"title": "3.1.2 Algorithms based on Combinatorial Optimization", "content": "The auction algorithm, introduced by Bertsekas [Bertsekas, 1981, Bertsekas and Eckstein, 1988, Bertsekas, 1992], is a combinatorial optimization method tailored for assignment problems, which represent a special case of the OT problem. The algorithm iteratively modifies $(S, \\xi, z)$, where $S \\subseteq \\{1, ..., n\\}$ is a set of assigned points, $\\xi$ is a partial assignment vector mapping elements of S injectively to $\\{1, . . ., m\\}$, and z is a dual vector.\nGiven a source index i, the auction algorithm evaluates not only the optimal assignment but also the second-best option, using the C-transform:\n$j^* \\in \\arg \\min_{j \\in \\{1,...,m\\}} C_{ij} - z_j, \\quad j \\in \\arg \\min_{j \\in \\{1,...,m\\}\\setminus j^*} C_{ij} - z_j.$\nBased on these indices, the algorithm updates z, S, and \u0121 as follows:\n1. Update the dual vector z:\n$z_{ij^*} \\leftarrow z_{ij^*} - ((C_{ij^*} - z_{j^*}) - (C_{ij} - z_i) + \\epsilon).$\n2. Update S and \u00a7: If there exists i' \u2208 S such that \u2081\u2081 = j, remove i' from S by setting $S \\leftarrow S \\setminus \\{i'\\}$, and add i to S with $S \\leftarrow S \\cup \\{i\\}$."}, {"title": "3.2 Iterative or Approximate Algorithms", "content": "Building on the auction algorithm, Lahn et al. [2019] proposed a deterministic primal-dual approach for solving the discrete OT problem with e-approximation guarantees. By adapting the scaling algorithm of Gabow and Tarjan, this method achieves a computational complexity of:\n$O(\\frac{n}{\\epsilon} ( \\frac{n^2}{\\epsilon} + \\frac{m}{\\epsilon} ) ).$\nThis approach involves scaling demands and supplies from a graph-theoretic perspective, followed by iterative updates of dual weights and adjustments to the transport plan along admissible paths. Unlike iterative methods like Sinkhorn's algorithm, the primal-dual approach is numerically stable, even for small e values, and avoids logarithmic dependencies in runtime.\nExact algorithms, including the network simplex and auction methods, form the foundation of optimal transport theory. While their computational intensity limits scalability for large-scale problems, they are invaluable for small- to medium-sized applications and serve as benchmarks for evaluating approximate methods. These advancements ensure that exact algorithms remain relevant across diverse applications and inspire new computational strategies."}, {"title": "3.2.1 Sinkhorn-based Algorithms", "content": "Entropic regularization is used by Sinkhorn's algorithm to iteratively approximate optimal transport solutions [Sinkhorn, 1964]. This algorithm efficiently solves the regularized problem (4), making it particularly suitable for large-scale datasets and high-dimensional applications. The solution to (4) is unique and can be expressed as:\n$P_{ij} = u_i K_{ij} v_j,$\nwhere K is defined in (7), and $u, v \\in \\mathbb{R}_+^{n \\times m}$ are scaling variables. This factorization is compactly written as $P = diag(u) K diag(v)$, where\n$diag(u) K diag(v)1_m = a, \\quad diag(v) K^T diag(u)1_n = b.$\nor equivalently:\n$(Kv) \\cdot u = a, \\quad (K^T u) \\cdot v = b,$\nwhere denotes element-wise multiplication. To solve these equations, the algorithm iteratively updates u and v as follows:\n1. Initialize u = 1n and v = 1m.\n2. Iteratively update:\n$u^{(l+1)} \\leftarrow \\frac{a}{Kv^{(l)}}, \\quad v^{(l+1)} \\leftarrow \\frac{b}{K^T u^{(l+1)}}.$\n3. Recover the transport plan using $P = diag(u) K diag(v)$."}, {"title": "3.2.2 Primal-Dual Algorithms", "content": "Dvurechensky et al. [2018] proposed an algorithm for addressing OT problems under different regularization schemes. They also assumed m = n. They demonstrated that the Sinkhorn's iteration bound [Altschuler et al., 2017] represents the worst-case estimate. They established that the iteration count for the Sinkhorn algorithm is upper-bounded by\n$O\\left(\\frac{n^2 \\ln n ||C||_\\infty}{\\epsilon^2}\\right)$\nGiven linear operator A : E \u2192 H with E and H being some finite real vector space and E being convex with some chosen norm $|| . ||_E$, a given r \u2208 H, and f(x) being a y-strongly convex on E, they analyze a problem\n$\\min_{x \\in E} \\{f(x) : Ax = r\\}$\nIts Lagrange dual problem is\n$\\min_{\\lambda \\in H^*} \\{ \\langle \\lambda, r \\rangle + \\max_{x \\in E} \\langle -f(x) - \\langle A^T \\lambda, x \\rangle \\} $\nand \u25bd(A) is L-Lipschitz-continuous with $L \\leq ||A||_{E \\rightarrow H}/\\gamma$. It is assumed that the dual problem has a solution and there exists some R > 0 where $||\\lambda^*||_2 < R < +\\infty$ and X* is the optimal dual solution with objective value of $||\\lambda^*||_2$. They proved the convergence rate of their algorithm, called"}, {"title": "4 OT Problem Applications", "content": "OT problem has become a versatile tool across diverse applications in machine learning, data science, and beyond. Its ability to quantify distributional differences has found utility in generative models, domain adaptation, and clustering, among others. OT has also been employed in computer vision for image-to-image translation, point cloud alignment, feature matching, statistics for distribution shift detection, and Wasserstein barycenters. In addition to its typical applications, OT can also support decision-making in transportation and routing [Moradi et al., 2023, 2024a,c,d,b, Moradi and Boroujeni, 2025] and public health systems during pandemics [Tanhaeean et al., 2023], as well as enhance population-based algorithms such as simulated annealing [Moradi et al., 2022]. Another notable application area is time-series data analysis, where OT provides a robust alternative to traditional methods like Dynamic Time Warping (DTW). This capability has shown to be particularly useful in domains such as cybersecurity, where aligning time-series data can aid in identifying system anomalies or understanding attack patterns. Similarly, in manufacturing systems, OT-based time-series alignment supports monitoring and optimizing industrial processes to ensure efficiency and resilience [Aftabi et al., 2024, 2025]"}, {"title": "5 Emerging Trends and Challenges in OT Problem", "content": "The OT problem has become a cornerstone for numerous applications across machine learning, data science, economics, and beyond. However, the increasing complexity of modern datasets and demands for scalability and precision have revealed several emerging trends and unresolved challenges that shape the trajectory of this field. Computational advancements remain at the forefront of this evolution, with scalable algorithms designed for high-dimensional data becoming a pivotal area of focus.\nA significant trend involves the integration of OT with machine learning frameworks. The synergy between deep learning and optimal transport has proven highly productive, finding use in areas such as GANs, cross-domain adaptation, and feature representation learning. Furthermore, graph-based OT methodologies unlock new potential in analyzing structured data, such as social networks and biological datasets. At the same time, federated learning presents opportunities for OT to enhance privacy-preserving computations. These trends exemplify OT's expanding role in complex, data-driven environments."}, {"title": "6 Conclusions", "content": "OT has become a fundamental mathematical framework with profound implications across diverse domains. Its ability to quantify and align distributions has enabled groundbreaking advancements in machine learning, data science, computer vision, and numerous other fields. This report has explored the theoretical foundations of OT, cutting-edge computational algorithms, emerging challenges, and its applications in areas ranging from generative modeling to time-series analysis. The versatility of OT lies in its flexibility to adapt to different problem settings, such as unbalanced distributions, dynamic scenarios, and high-dimensional data. Advances in scalable algorithms and approximate methods have significantly expanded its applicability, making it feasible to address large-scale problems. However, challenges remain in achieving robust convergence, ensuring computational efficiency, and addressing the societal implications of OT's integration into real-world systems. As OT continues to evolve, its intersection with machine learning, graph theory, and optimization promises to unlock new opportunities for research and innovation. Addressing the unresolved theoretical and practical challenges will enhance the robustness and efficiency of OT algorithms and ensure their ethical and impactful deployment across diverse industries. By bridging theory and practice, OT solidifies its role as a modern computational and applied science cornerstone."}]}