{"title": "WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue Models", "authors": ["Yifu Chen", "Shengpeng Ji", "Haoxiao Wang", "Ziqing Wang", "Siyu Chen", "Jinzheng He", "Jin Xu", "Zhou Zhao"], "abstract": "Retrieval Augmented Generation (RAG) has gained widespread adoption owing to its capacity to empower large language models (LLMs) to integrate external knowledge. However, existing RAG frameworks are primarily designed for text-based LLMs and rely on Automatic Speech Recognition to process speech input, which discards crucial audio information, risks transcription errors, and increases computational overhead. Therefore, we introduce WavRAG, the first retrieval augmented generation framework with native, end-to-end audio support. WavRAG offers two key features: 1) Bypassing ASR, WavRAG directly processes raw audio for both embedding and retrieval. 2) WavRAG integrates audio and text into a unified knowledge representation. Specifically, we propose the WavRetriever to facilitate the retrieval from a text-audio hybrid knowledge base, and further enhance the in-context capabilities of spoken dialogue models through the integration of chain-of-thought reasoning. In comparison to state-of-the-art ASR-Text RAG pipelines, WavRAG achieves comparable retrieval performance while delivering a 10x acceleration. Furthermore, WavRAG's unique text-audio hybrid retrieval capability extends the boundaries of RAG to the audio modality.", "sections": [{"title": "1 Introduction", "content": "Retrieval Augmented Generation (RAG) (Guu et al., 2020) has revolutionized natural language processing, offering a powerful approach to enhance text-based large language models (LLMs). A typical RAG system comprises three main components: a knowledge source (e.g., a database of documents or a knowledge graph) (Peng et al., 2024), a retriever module, and a generator module (typically a large language model). The RAG process unfolds in two primary stages. 1) Retrieval: Given an input query, the retriever module identifies and retrieves relevant information from the knowledge source. 2) Generation: The retrieved information, along with the original input query, is provided as context to the generator, which then produces the final response. This process allows the LLM to leverage up-to-date and factual knowledge, significantly improving the accuracy, consistency, and relevance of its responses (Gao et al., 2024). Researchers have sought to extend these benefits to the spoken dialogue domain (Lee et al., 2015; Chelba et al., 2008), aiming to enhance spoken dialogue models capable of processing audio inputs and generating speech responses (Fang et al., 2024; Ji et al., 2024a). However, these efforts have largely relied on cascaded \"ASR+RAG\" pipelines, which first transcribe speech to text using Automatic Speech Recognition (ASR) and then apply a text-based RAG system. This indirect approach suffers from several critical limitations: it fails to fully leverage the rich information present in the audio modality, treating it as a mere intermediary; the ASR component introduces computational overhead and potential transcription errors that propagate through the system; and the reliance on text-centric knowledge bases restricts the system's ability to utilize audio-specific knowledge. Crucially, the audio modality encompasses far more than just human speech; it includes a vast range of sounds (Ji et al., 2024b), such as environmental noises, music, and animal vocalizations, many of which are beyond the capabilities of ASR. A RAG framework that natively integrates this broader spectrum of audio information can unlock significant potential for richer, more contextually relevant understanding and generation (Chen et al., 2022), directly addressing key challenges of spoken dialogue models, such as the tendency to generate hallucinated content. However, realizing a fully end-to-end, audio-compatible RAG system remains a significant and open research challenge. Addressing the limitations of existing approaches requires a fundamental shift: building knowledge bases that encompass a wide range of audio modalities alongside text, and developing retrieval mechanisms that can effectively represent and retrieve information from this unified audio-text space. Additionally,effectively harnessing retrieved multimodal information during generation requires new techniques for improved accuracy, naturalness, and contextual consistency. Therefore, we propose WavRAG, a novel end-to-end RAG framework designed for native audio integration. We show several dialogue scenarios to help understand the role of our framework in Figure 1. Inspired by LLM2Vec's (BehnamGhader et al., 2024) success in fine-tuning LLMs for text embeddings, we build our retriever on top of Qwen2-Audio, an MLLM with strong general audio comprehension, to create a unified embedding space for audio (speech and non-speech) and text. Considering that The pre-training objectives of multimodal language models are not optimized for retrieval, we further enhance the model with a contrastive learning framework. This approach allows the resulting retriever to encode end-to-end, directly encoding raw audio and text inputs into a shared embedding space, thus avoiding the computational overhead and potential error propagation of cascaded ASR-Text pipelines. Furthermore, in the generation stage, We incorporate Chain-of-Thought (CoT) reasoning, promoting a structured and interpretable inference process that enhances both reliability and controllability in utilizing retrieved multimodal knowledge. In summary, our contributions are as follows:\n\u2022 We propose WavRAG, a novel RAG framework for spoken dialogue models. It is the first to extend RAG to this domain in an end-to-end manner and to incorporate a hybrid text-audio knowledge base.\n\u2022 We introduce a novel retriever WavRetriever, to support hybrid retrieval across text-audio modalities, and further enhance the in-context capabilities of the spoken dialogue models through Chain-of-Thought techniques.\n\u2022 WavRAG achieves comparable results to the SOTA text-based RAG models in text retrieval, while offering an average acceleration of 10 times. Moreover, hybrid text-audio retrieval provides WavRAG with new capabilities."}, {"title": "2 Related Works", "content": "Audio RAG. While Retrieval-Augmented Generation (RAG) has shown promise in audio-related tasks like captioning (Koizumi et al., 2020; Zhao et al., 2023), text-to-audio generation (Huang et al., 2023), and music generation (Gonzales and Rudz-icz, 2024). However, while these efforts demonstrate the utility of retrieval in audio processing, prior work primarily utilizes retrieval to enhance specific, isolated tasks with limited exploration of how retrieval-augmented techniques can benefit spoken dialogue models. Audio information itself carries rich semantic and acoustic imformation that can improve retrieval grounding, enhance response contextualization, and strengthen factual consistency. WavRAG, in contrast, integrates retrieval as a core component of a complete dialogue system. The combination of general audio support and end-to-end integration distinguishes WavRAG and represents a significant advancement towards truly audio-native, retrieval-augmented spoken dialogue systems.\nMultimodal Retrieval. The increasing prevalence of multimedia applications and Retrieval-Augmented Generation (RAG) systems, fueled by Multimodal Language Models (MLLMs), has underscored the necessity for unified retrieval frameworks capable of managing diverse modalities. Traditional cross-modality retrieval methods often rely on pre-trained models such as CLAP (Elizalde et al., 2022) and CLIP, which use separate encoders for text and other modalities (e.g., UniVL-DR (Liu et al., 2023) and UniIR (Wei et al., 2023)). Other approaches enhance pre-trained text embeddings with audio encoders (Min et al., 2025), but these often prioritize the semantic content of speech, overlooking important general audio. Such methods struggle to effectively capture the full spectrum of information from both speech and non-speech audio. Recent advancements have highlighted the potential of Large Language Models (LLMs) and Supervised Fine-Tuning (SFT) for creating powerful, unified text representations (BehnamGhader et al., 2024; Lee et al., 2025). This methodology has been successfully extended to other modalities, with works like E5-V (Jiang et al., 2024) and VLM2VEC (Jiang et al., 2025) focusing on fine-tuning strategies for visual models. Furthermore, Zhang's research (Zhang et al., 2024) demonstrates the feasibility of developing universal multimodal retrieval models using MLLMs. However, there has been limited exploration in the audio modality, prompting us to propose the first end-to-end audio-text multimodal retriever."}, {"title": "3 WavRAG", "content": "Figure 2 provides an overview of the traditional text-based RAG framework (top) and our proposed WavRAG framework (bottom).\nIn the context of text-based dialogue models, the classic retrieval-augmented generation (RAG) framework, depicted in the top portion of Figure 2, typically includes: (1) a text embedding model acting as the retriever R\u00f8, (2) a text-based dialogue model (Touvron et al., 2023) serving as the generator G\u03b8, and (3) a fixed external knowledge corpus D = {d1,...,d} containing only textual snippets di. During inference, the process is divided into two stages: retrieval and generation.\nDuring retrieval, given a textual query qt, the retriever computes the retrieval distribution p(d | qi) via:\np(d|qi) = \\frac{exp(sim(R_{\\theta}(qi), R_{\\theta}(d)))}{\\sum_{d_{i} \\in D}{exp(sim(R_{\\theta}(qi), R_{\\theta}(di)))}}\nwhere R\u03b8() denotes the encoding function of R\u00f8, sim(, ) is a similarity metric (such as cosine similarity, as shown in Figure 2), and d \u2208 D. For efficiency, D is usually pre-encoded offline by R\u0444. By drawing from this distribution, the system selects the Top-k relevant snippets, forming a subset Dk CD.\nDuring generation, conditioned on qi and the retrieved subset Dk, the system computes the probability of producing the target text yi as follows:\nP(Yi | qi, Dk) = \\prod_{m=1}^{N} P(Yi | qi, Dk, Y_{<m})\nwhere p(Yi | qt, Dk, Y<i) is given by the generator Ge and N denotes the number of tokens in the answer yi. This process is illustrated in the top-right portion of Figure 2, where the retrieved text snippets and the query are used to prompt the LLM generator.\nTo extend RAG to spoken dialogue scenes and overcome the limitations of the traditional ASR-based approach, we propose WavRAG, as shown in the bottom portion of Figure 2. WavRAG integrates a retriever R$ capable of directly processing queries in audio, text, and combined formats,"}, {"title": "3.2 WavRetriever", "content": "The goal of our retriever, R\u00f8 (WavRetriever), is to produce embedding vectors for both queries and knowledge entries that enable efficient similarity-based retrieval. As depicted in Figure 3, WavRetriever processes text, speech, or multimodal inputs, which are concatenated with a task-specific instruction and an End-of-Sequence (EOS) token. We build WavRetriever upon the Qwen2-Audio MLLM, leveraging its robust general audio comprehension. Specifically, we freeze the pre-trained audio encoder parameters of Qwen2-Audio and focus training on the projection layer and the backbone LLM. This allows us to capitalize on Qwen2-Audio's existing audio processing capabilities.\nHowever, simply fine-tuning Qwen2-Audio on the downstream task is insufficient for optimal retrieval performance. While pre-trained MLLMs like Qwen2-Audio possess robust multimodal understanding, their pre-training objectives are not directly optimized for creating embeddings suitable for similarity-based retrieval. To address this, we further adapt Qwen2-Audio into a powerful multimodal encoder using a carefully designed contrastive learning strategy. This strategy shapes the embedding space by maximizing the similarity between a query's embedding and its relevant (positive) knowledge, while minimizing the similarity with irrelevant (negative) knowledge embeddings.\nEach training instance in our con-trastive learning setup comprises a query, Qins = Instruction: {prompt} Query: quni, a positive knowledge sample k+, and a set of negative knowledge samples {k\u012b,..., k\u012b }. Both the query, quni, and the knowledge samples, k,"}, {"title": "3.3 Generation", "content": "In WavRAG's generation stage, we adopt a retrieval-augmented generation paradigm. The retriever provides the top-k retrieved knowledge entries (which can be audio, text, or multimodal) along with the original query quni as input to the generator. While this approach provides rich contextual information, most existing spoken dialogue systems are not trained on these lengthy and multiple mixed-modality input formats, making naive concatenation of all retrieved documents prone to suboptimal performance. To address this, we incorporate Chain-of-Thought (CoT) reasoning, specifically Zero-Shot-CoT (Kojima et al., 2022) and a Self-Consistency mechanism.\nZero-Shot-CoT Reasoning. Zero-Shot-CoT prompting leverages the in-context reasoning abilities of large language models (LLMs) to generate intermediate reasoning steps without requiring task-specific training examples. Given the multimodal query quni, a guiding prompt Pprompt, a \"magic prompt\" P' (e.g., \"Let's think step-by-step\"), and the top-k retrieved knowledge snippets Kk, the generator Greasoning produces a reasoning chain Canswer:\nCanswer = Greasoning(quni, Pprompt + P',Kk) (3)\nThe retrieved knowledge Kk provides the context for the reasoning process, allowing the model to generate a logical, step-by-step deduction leading to the final answer.\nSelf-Consistency. To further enhance the reliability of the reasoning process, we employ a Self-Consistency mechanism. This approach samples multiple reasoning paths from the LLM and then selects the most consistent answer among them. This mitigates the risk of errors that can arise from relying on a single, potentially suboptimal, reasoning path. Specifically, we use the Universal Self-Consistency (USC) method (Chen et al., 2023). Instead of simply taking a majority vote among the generated answers (which can be problematic for free-form answers), USC concatenates all sampled reasoning paths and answers and prompts the LLM *itself* to select the most consistent response. This leverages the LLM's own understanding to determine the best answer, given the multiple reasoning paths."}, {"title": "4 Experiments", "content": "4.1 Datasets\nFor training, we curated a dataset of 1.5M samples across five retrieval scenarios: Speech-to-Text: We adapted existing text retrieval datasets (e.g. HotpotQA (Yang et al., 2018), Quora (Wang et al., 2017)) by synthesizing speech queries using the CosyVoice2 TTS model (Du et al., 2024; An et al., 2024) with diverse voice prompts and noise augmentation. Speech-to-Speech and Text-to-Speech: We used existing datasets: SLUE-SQA-5 (Shon et al., 2023) and Spoken-SQUAD (Li et al., 2018). Text-to-Text: We used existing text retrieval datasets: ELI5 (Fan et al., 2019), NQ (Kwiatkowski et al., 2019), HotpotQA (Yang et al., 2018), MS MARCO, Quora, SQUAD (Rajpurkar et al., 2016), and TriviaQA (Joshi et al., 2017). Audio+Text-to-Audio+Text: We process new data from sources like AudioSetSL, Audio-Caps (Kim et al., 2019), MusicCaps (Agostinelli et al., 2023), Clotho (Drossos et al., 2019), Vox-Celeb (Nagrani et al., 2017), and Xeno-canto, where queries and documents are both general audio-text pairs. For evaluation, We test WavRAG on four datasets: HotpotQA, Spoken-SQuAD, SLUE-SQA-5, and our custom mixed modality dataset. Detailed data processing procedures, dataset statistics, and examples are provided in Appendix B\n4.2 Baselines\nRetrieval Baselines. 1) BGE (Li et al., 2024): A state-of-the-art text embedding model, used within an ASR-based pipeline for speech-related retrieval tasks. 2) CLSR (Anonymous, 2025): A state-of-the-art speech-text retrieval framework. Used for comparison on speech-to-text and text-tp-speech retrieval task. 3) CLAP (Elizalde et al., 2022): Used for comparison on our custom multimodal dataset. 4) Qwen2Audio-enhanced Text Retrieval: Used on our custom dataset. This baseline leverages Qwen2Audio to generate descriptive text from audio clips, which is then concatenated with the original text by using the BGE model.\nGeneration Baselines. 1) TextRAG: A standard text-based RAG pipeline using BGE embeddings for retrieval and Whisper medium for ASR."}, {"title": "4.3 Evaluation Metrics and Experimental Settings", "content": "Retrieval. Retrieval performance is evaluated across four scenarios: Speech-to-Text (HotpotQA), Speech-to-Speech (SLUE-SQA-5), Text-to-Speech (Spoken-SQUAD), and Audio+Text to Audio+Text (a custom dataset). Reported metrics include:\nRecall@k: The proportion of relevant items found within the top-k retrieved results. Higher is better. (We report Recall@1, Recall@5, and Recall@10). NDCG@10 (Normalized Discounted Cumulative Gain): A measure of ranking quality that considers the position of relevant items in the retrieved list, giving higher scores to relevant items ranked higher. Higher is better. Average Inference Time: The average time taken to process a single query.\nFor scenarios involving speech input, the Word Error Rate (WER) and model size of the Whisper ASR model (used in baseline methods) are also reported. WER measures the accuracy of the speech recognition, with lower values indicating fewer errors."}, {"title": "4.4 Main Results", "content": "Retrieval Performance. Table 1 presents the retrieval results, demonstrating WavRAG's superior performance and efficiency compared to traditional ASR-dependent baselines and other approaches across all four evaluated tasks: Speech-to-Text, Text-to-Speech, Speech-to-Speech, and Audio+Text-to-Audio+Text.\nWavRAG's key advantage lies in its direct processing of audio inputs, eliminating the need for ASR and its associated computational overhead and potential for transcription errors. This translates to significant speedups in inference time,"}, {"title": "4.5 Analysis", "content": "Ablation Studies on Contrasitive Training Framework. To isolate the impact of our contrastive learning framework, we conducted an ablation study comparing the fine-tuned WavRAG retriever to the pre-fine-tuned Qwen2-Audio-7B-Instruct model. This baseline represents a strong, pre-trained MLLM with inherent multimodal understanding, but without retrieval-specific optimization. Following (Jiang et al., 2023), original Qwen2-Audio's representation was obtained by prompting for global semantics at the last token. Table 3 shows the results, with WavRAG significantly outperforming the baseline across all datasets and metrics. Recall@1 improvements range from +0.3075 to +0.3437, and nDCG@10 gains are even more pronounced, reaching up to +0.4169. These substantial improvements unequivocally validate the effectiveness of our contrastive learning framework in adapting the MLLM for multimodal retrieval.\nKnowledge Extension Quality. A critical aspect of WavRAG is its ability to take advantage of extended knowledge associated with audio. To assess the quality of this generated knowledge, we conducted a human evaluation on 700 randomly sampled instances from our custom multimodal dataset. Fluent English-speaking annotators evaluated each sample on a 5-point scale across four key dimensions: Grammaticality, Factual accuracy, Relevance, and overall Helpfulness (further categorized as helpful, neutral, or harmful). Figure 4 shows the score distributions. The vast majority of"}, {"title": "5 Conclusions", "content": "This work introduced WavRAG, a novel retrieval-augmented generation framework specifically designed for spoken dialogue systems. WavRAG makes a significant departure from traditional ASR-dependent pipelines by directly processing raw audio input for embedding and retrieval. This approach offers several key advantages, including reduced computational overhead, preservation of rich acoustic information, and the ability to leverage a unified multimodal knowledge base. Through comprehensive experiments, including quantitative evaluations and qualitative analyses, we demonstrated the effectiveness of WavRAG. The results show substantial improvements in both retrieval and generation performance compared to traditional methods and baseline models."}, {"title": "Limitations", "content": "Despite WavRAG's exploration of how a well-designed RAG system can leverage both semantic and acoustic information to enhance the semantic quality of responses, emotional tone and prosody are equally crucial in spoken dialogue systems. The extent to which RAG can contribute to the acoustic aspects of responses, such as intonation, expressiveness, and speaker style, remains an open question, warranting further investigation."}, {"title": "B.2 Prompt for Knowledge Extension", "content": "In this section, We show the entire prompt for our dataset's knowledge extension,question generation and answer in Figure 5."}, {"title": "B.3 Data Samples Display", "content": "This section presents several data samples from different datasets (see Tables 6, 7, 8, 9, and 10). Each sample is displayed in its own table formatted with two columns-one for the Field and one for the Content."}, {"title": "B.4 Representation Visualization", "content": "To visualize the embedding spaces learned by different models, we randomly selected 150 samples from our test dataset, representing the four retrieval scenarios. Each sample consisted of either an audio clip and its caption, or a group of paired audio-text data. We extracted embeddings for each sample using CLAP, the original Qwen2-Audio model, and our WavRAG retriever. These embeddings were then projected into a two-dimensional space using Principal Component Analysis (PCA). Figure 6 shows the resulting PCA visualizations. In contrast to CLAP and the original Qwen2-Audio, the WavRAG embeddings show no clear separation between modalities. Instead, the audio, text, and combined audio+text embeddings for a given piece of information are closely clustered, indicating that WavRAG consistently represents the same semantic content across modalities."}]}