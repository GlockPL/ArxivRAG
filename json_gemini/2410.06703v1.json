{"title": "ST-WEBAGENTBENCH: A BENCHMARK FOR EVALUATING SAFETY AND TRUSTWORTHINESS IN WEB AGENTS", "authors": ["Ido Levy", "Ben Wiesel", "Sami Marreed", "Alon Oved", "Avi Yaeli", "Segev Shlomov"], "abstract": "Recent advancements in LLM-based web agents have introduced novel architectures and benchmarks showcasing progress in autonomous web navigation and interaction. However, most existing benchmarks prioritize effectiveness and accuracy, overlooking crucial factors like safety and trustworthiness which are essential for deploying web agents in enterprise settings. The risks of unsafe web agent behavior, such as accidentally deleting user accounts or performing unintended actions in critical business operations, pose significant barriers to widespread adoption. In this paper, we present ST-WebAgentBench, a new online benchmark specifically designed to evaluate the safety and trustworthiness of web agents in enterprise contexts. This benchmark is grounded in a detailed framework that defines safe and trustworthy (ST) agent behavior, outlines how ST policies should be structured and introduces the Completion under Policies metric to assess agent performance. Our evaluation reveals that current SOTA agents struggle with policy adherence and cannot yet be relied upon for critical business applications. Additionally, we propose architectural principles aimed at improving policy awareness and compliance in web agents. We open-source this benchmark and invite the community to contribute, with the goal of fostering a new generation of safer, more trustworthy AI agents. All code, data, environment reproduction resources, and video demonstrations are available at https://sites.google.com/view/st-webagentbench/home.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in large language models (LLMs) have significantly expanded the capabilities of autonomous agents, particularly through the use of reasoning and acting (ReACT) patterns, vision-based LLM models (VLLMs), and agentic workflow frameworks such as LangGraph (Langraph, 2024), AutoGen (Wu et al., 2023), and CrewAI (CrewAI, 2024). These technologies enable web agents to better perceive environments (Wornow et al., 2024), reason through complex decisions, use tools, and interact seamlessly with applications. Autonomous web agents offer considerable value by automating workflows, improving accuracy, and scaling traditionally manual processes, making them increasingly relevant in enterprise settings (Zheng et al., 2024; Xi et al., 2023).\nDespite the promise of autonomous web agents, they are still far from achieving human-level performance in many scenarios. Studies show that web agents are more error-prone compared to humans when performing the same tasks, particularly in complex or dynamic environments (Wu et al., 2023), (He et al., 2024). Benchmarks designed to evaluate these agents have evolved, moving from offline datasets such as Mind2Web (Deng et al., 2024) to online environments such as WebArena (Zhou et al., 2024) and WorkArena (Drouin et al., 2024), which simulate more realistic web interactions. However, even in these settings, agents continue to lag behind human performance. Building robust benchmarks is challenging"}, {"title": "2 RELATED WORK", "content": "BENCHMARKS FOR WEB AGENTS\nEarly benchmarks (Shi et al., 2017; Liu et al., 2018) provided basic simulations and evaluation methods. More recently, the field has rapidly advanced from static datasets, such as"}, {"title": "AGENT SAFETY AND TRUSTWORTHINESS", "content": "The emergence of web agent benchmarks has significantly accelerated the development of web agents. Some of these agents are fine-tuned for specific tasks (Deng et al., 2024; Zheng et al., 2024; Cheng et al., 2024; Adept), while others are built on frontier models (e.g., AutoGPT). The ease of creating new agents, thanks to frameworks like AutoGen (Wu et al., 2023) and LangGraph, has led to the rapid introduction of numerous state-of-the-art agents, many of which have quickly surpassed existing benchmarks (Lai et al., 2024; Shlomov et al., 2024; Wang et al., 2024; Sodhi et al., 2024; MultiOn; Putta et al., 2024; Abuelsaad et al., 2024). Despite this progress, ensuring the safety and trustworthiness of agents remains a significant challenge. Frameworks such as GuardAgent (Xiang et al., 2024) employ knowledge reasoning to enforce safety measures, while AutoGen (Wu et al., 2023) incorporates multi-agent conversations to adjust safety protocols dynamically. Policy-based systems like SteP (Sodhi et al., 2024) and Agent-E (Abuelsaad et al., 2024) attempt to control agent actions, but challenges persist in guaranteeing that agents fully comply with policies and mitigate risks, especially in sensitive environments.\nSafety concerns in AI systems are well-defined through taxonomies that address risks such as unintended actions and system failures (Shamsujjoha et al., 2024). Benchmarks like R-Judge (Yuan et al., 2024) assess agents' capabilities in handling safety-critical tasks, while the AI Safety Benchmark from MLCommons (Vidgen et al., 2024) evaluates broader safety challenges. Trustworthiness in LLM-based agents, as discussed by Schwartz et al. (2023), requires ensuring transparency, reliability, and consistency in agent behavior. However, implementing these qualities remains difficult due to the evolving nature of agent tasks and the inherent unpredictability of autonomous decision-making. Current architectures often struggle to uphold these standards, underscoring the need for agent frameworks that can dynamically maintain safety and trust. In enterprise settings, strict adherence to policies and regulatory standards is crucial. Our benchmark addresses this gap by offering the first comprehensive evaluation of web agents, focusing on both policy compliance and trustworthiness."}, {"title": "3 ST-WEBAGENTBENCH: A SAFETY AND TRUSTWORTHINESS BENCHMARK", "content": "3.1 SAFE AND TRUSTWORTHY AGENT BEHAVIOR\nIn the context of deploying web agents in enterprise environments, multiple entities are invested in ensuring the safety and transparency of these agents, and each has a specific policy requirement. The agent is expected to behave in a way that respects a strict hierarchy of policies:\n1. Organizational Policies (Porg): The organization must allow users to deploy web agents, but it is responsible for ensuring that the agents comply with safety policies. These policies encompass privacy, security, resource access, and operational limits. The"}, {"title": "3.2 EVALUATING SAFETY AND TRUSTWORTHINESS OF WEB AGENTS", "content": "3.2.1 DIMENSIONS OF SAFETY AND TRUSTWORTHINESS\nIn this subsection, we define the key categories for evaluating the safety and trustworthiness of web agents. These categories represent distinct areas where agents are vulnerable to failure, motivating their inclusion in the benchmark. Each category highlights potential risks and provides context for why it is crucial to monitor agent behavior against these dimensions.\n\u2022 User Consent and Action Confirmation: Agents must obtain explicit user permission before performing irreversible actions, such as submitting forms or making purchases. A failure in this category could result in unintended world-altering actions, where an agent assumes autonomy without user validation, leading to potential data loss or unintended consequences (Appendix 8).\n\u2022 Boundary and Scope Limitation: Agents must limit their actions to explicitly authorized areas of an application. Failures in this category occur when agents explore unauthorized sections or access sensitive data, potentially violating privacy regulations. This is particularly relevant when an agent navigates beyond the user-defined scope, breaching trust.\n\u2022 Strict Task Execution: Agents are expected to follow user instructions without improvisation. A potential failure here involves the agent attempting to optimize or add unnecessary steps, diverging from the user's original intent and potentially causing errors or unexpected outcomes.\n\u2022 Policy Adherence: Agents must respect organizational policies, user preferences, and task instructions in a strict hierarchy. Failures in this dimension include cases where an agent prioritizes immediate task completion at the expense of violating long-standing organizational or user policies, undermining trust."}, {"title": "3.2.2 COMPLETION UNDER THE POLICY (CUP) METRIC", "content": "The Completion Under the Policy (CuP) metric is computed for each task in ST-WebAgent Bench. CuP quantifies the agent's ability to complete a task while adhering to the policy hierarchy. Policy violations are categorized based on their source and type. Let Ctask represent the task completion score. We define a policy violation matrix V where Vsource,category equals the number of violations for a specific source and category. The sources of policy are Org and User and the categories of policies include User Consent, Out of Scope, and Strict Instructions.\nFor each task, we compute the total number of violations: Vtotal = \u03a3source, category Vsource,category.\nThe CuP metric for each task is then defined as: CuP = Ctask \u00b71{Vtotal=0}."}, {"title": "3.2.3 AGGREGATE RISK ASSESSMENT", "content": "Once the CuP is computed for each task, we want to assess whether the agent is considered safe or not over multiple tasks. Since we only measure policy violations, defining safety and trustworthiness requires aggregating risk across tasks. For each category of policy, let V(i)source,category be the number of violations for the i-th task. We compute the aggregate ratio of violations as\nRisk Ratiosource,category = \u03a3iV(i)source,category/# Policiessource"}, {"title": "3.2.4 DETERMINING SAFETY AND TRUSTWORTHINESS", "content": "To assess the overall safety and trustworthiness of the agent, we aggregate the risk ratios across all policy categories. This allows us to evaluate the agent's risk of being unsafe based on the number of policy violations divided by the number of tasks. By measuring the risk of policy violations in each category, we can determine the extent to which the agent is likely to be safe or untrustworthy. If the majority of categories fall into the low-risk range, the agent can be considered largely safe and trustworthy. If a significant number of categories fall into the medium risk or high risk ranges, the agent is more likely to be deemed unsafe or untrustworthy. This aggregated approach provides a nuanced view of the agent's performance, allowing us to pinpoint specific areas of risk and assess the overall likelihood of unsafe behavior."}, {"title": "3.3 BENCHMARK DESIGN AND IMPLEMENTATION", "content": "ST-WebAgentBench includes 235 policy-enriched tasks that span multiple safety categories and several application environments, including Gitlab and ShoppingAdmin from WebArena and SuiteCRM. Some tasks from WebArena have been reused to provide reliable ground truth for task completion. For other applications, we defined new tasks specifically designed to reflect the safety and task completion challenges inherent to those environments. The distribution of tasks and policies is depicted in 2. The The core benchmark consists of indices 0-84 focusing on evaluating boundary, user consent, and strict execution policies across Gitlab, ShoppingAdmin and SuiteCRM application environments. The cognitive load part (indices 85-234T) focuses on evaluating the cognitive load of agents in dealing with following multiple policies. These data points are created with varying levels of cognitive load - low, med, high - to allow experimentation if providing too many policies to an agent has degradation in completion under the policy metric. The specific syntax and sample of tasks and their policies are provided in Appendices A.2.1.\nCreating a ground truth for policies requires careful and manual curation. This ensures that the evaluation functions do not contradict each other and that the Completion under Policy (CuP) metric is accurately computed. A combination of techniques was employed to generate the ground truth for both task completeness and policy adherence. First, we generated concrete tasks and policies at the organizational, user, task, and application levels. Next, scripts were executed to inject the appropriate policies into the correct tasks based on a manual analysis of task-policy fit.\nFor each policy, the ground truth of the policy was defined through the use of evaluation functions. The following functions were implemented to support ST-WebAgent Bench:"}, {"title": "4 EVALUATION", "content": "4.1 EXPERIMENTAL SETUP\nOur evaluation mainly focused on the first 84 tasks from the benchmark dataset, primarily testing boundary conditions, user consent, and strict execution policy dimensions across three application environments: GitLab, ShoppingAdmin, and SuiteCRM. The first two were provisioned on AWS using the WebArena-provided AMI, while SuiteCRM ran locally as a Docker container. The benchmark was executed on a MacBook Pro. In addition, we evaluated a representative set from the cognitive load policies (indexes 85-234).\nWe evaluated three agents: AgentWorkflowMemory (AWM)\u2014the top open-source agent on the WebArena leaderboard with a 35.5% success rate (mainly attributed to its ability to learn from experiences), WorkArena legacy from BrowserGym with a 23.5% success rate, and Web Voyager. Given the difficulty agents face in task completion within WebArena, we measured both full and partial task completions, introducing a new partial_CuP metric alongside standard completion and CuP metrics. Each task took approximately 3 minutes"}, {"title": "4.2 MAIN RESULTS", "content": "Table 3 presents the overall performance of the agents, measured by the Cup and partial CuP metrics, and shows the policy violations and their qualitative assessment, reflecting the safety and transparency risks across various dimensions."}, {"title": "4.2.1 ANALYSIS", "content": "WorkArena Legacy exhibits better compliance with consent and strict execution policies. It has fewer violations in the consent (4.0 vs. 12.0) and strict_execution (16.0 vs. 21.0) dimensions, resulting in lower risk ratios (0.052 vs. 0.176 for consent, and 0.142 vs. 0.221 for strict execution). Consequently, WorkArena Legacy's qualitative risk is rated as \"medium\" in these areas, compared to WebVoyager's \"high\" risk assessments.\nAWM received the highest partial completion rate of 0.369, indicating better progress in the first milestones within the tasks. Its partial CuP is 0.238, showing that it manages to comply with policies during partial task executions to some extent. However, it has significantly higher violations in the consent dimension (37.0) with a risk ratio of 0.44 and in strict_execution (24.0 violations, risk ratio 0.173), both rated as \u201chigh\u201d in qualitative risk. This suggests that while AWM is effective in advancing tasks and occasionally complies during partial completions, it frequently violates policies requiring user consent and strict task execution.\nAn additional observation pertains to the impact of cognitive load on agent performance, specifically seen with the AWM agent. In scenarios with low cognitive load-where only one policy is applied (\"Easy\" tasks)-AWM achieved a performance score of 14.8. However, when faced with a high cognitive load involving 17 policies per task (\"Hard\u201d tasks), its performance decreased to 11.5. This trend indicates that as agents are required to consider more policies simultaneously, their effectiveness diminishes. Given that real-life organizations typically enforce numerous policies, this reduction in performance is concerning. The current low results underscore the need for further experiments, particularly as agents begin to integrate advanced safety mechanisms to better manage complex policy environments.\nThe results from our evaluation highlight several important observations. First, the selection of difficult tasks (i.e., GitLab) naturally resulted in lower completion rates across agents. The agents also struggled with CuP, failing to comply with strict task execution and user consent policies in a significant number of cases. The boundary dimension, however, turned out to be less relevant to the agents' performance, either because the tasks themselves did not trigger boundary-related policies or the agents failed to reach the point where boundary checks"}, {"title": "5 TOWARDS A POLICY-AWARE AGENT ARCHITECTURE", "content": "In this section, we highlight the main components we believe should be part of a web agent architecture in order to achieve safe and trustworthy behavior. Figure 1 presents a common modular architecture of an agentic web agent (implemented with a multi-LLM-agent architecture) and highlights essential components and safeguard mechanisms required to guarantee safe and trustworthy agent behavior. Typically web agents include the following main components:\n\u2022 Orchestration agent - oversees and coordinates the control flow, sequencing of agent execution, error handling, and fallback strategies across the different agents to achieve the agentic workflow\n\u2022 Task Planner agent - handles task planning and management from understanding initial user intents and disambiguation to planning and re-planning high level and low-level plans. It is also responsible for detecting positive/negative outcomes of previous actions, identifying task completion, defining retry/recovery or declaring out-of-scope, abortion or other special task completion situations.\n\u2022 Semantic Perception agent - responsible for semantically understanding the UI in the context of the task and preparing relevant information for the action agent. It uses a combination of sources from the observation space (DOM, Screenshot, AXTree 1), and attempts to discover and analyze the semantic roles and effects of UI elements given the task. It is also responsible for filtering out irrelevant HTML attributes and data to reduce the cognitive load of downstream agents.\n\u2022 Action agent - determine the next action to perform for the immediate low-level task. This includes locating which element to interact with and which interaction type to perform.\n\u2022 Learning and Adaptation - enables the web agent to adapt and improve over time based on explicit and implicit positive and negative feedback. It is responsible for monitoring the performance of the Web Agent and generating learning experiences that can be stored and used over time. It should optimize to increase the PuC metric as well as user satisfaction as well as efficiency (e.g., cost, time).\nDuring the execution of the Web Agent, initial organizational and user policies are loaded to the long-term memory and made accessible to the policy, safety and trustworthy agent (aka policy agent hereunder). The policy agent is responsible for loading and constructing the hierarchy of policies relevant to the given task, user, and application. It is also responsible for safeguarding, blocking, validating, and intervening with the control flow of all other agents ensuring that policies are enforced. This can be achieved through multiple design patterns such as interceptor patterns (pre and post execution hooks called by the orchestrator) or aspect-oriented programming (injected hooks). The hooks themselves can be implemented using deterministic (rules) or nondeterministic (LLMs) logic. When a policy violation is"}, {"title": "6 DISCUSSION", "content": "Our analysis shows that agents, when subjected to safety and trustworthiness policy compliance checks, are not yet enterprise-ready. Even with limited policy dimensions, agents exhibited significant issues such as hallucinating extra steps that were not part of the task. This underscores the gap in handling policy compliance effectively in real-world scenarios. Current benchmark results indicate agents are classified as medium to high risk in terms of non-compliance, particularly for user consent and strict adherence to policy instructions. Additionally, task completion remains a challenge. A key reason for this shortfall could be the lack of grounded knowledge, which should stem from both initial policy definitions and ongoing experiential learning.\nWe claim that CuP (Completion under the Policy) is a better metric to optimize for enterprise adoption. We believe that by implementing design principles of policy-aware agents, systems equipped with effective mechanisms to manage and enforce policy hierarchies could perform much better in future benchmarks.\nOne major challenge moving forward is developing more comprehensive and realistic benchmarks. The current benchmark lacks coverage of critical dimensions, and manual annotation is too labor-intensive. Importantly, data quality is more significant than quantity (Kapoor et al., 2024), as redundant templates or irrelevant tasks do not provide meaningful evaluations. Techniques like recording real work and automatically annotating trajectories with LLMs could help in building and maintaining benchmarks at scale. To support these challenges, we open-sourced this benchmark to encourage collaboration and expansion and we will maintain a leaderboard. If agents become truly safe and trustworthy, even with limited task generalization, they can unlock the full potential of autonomous systems in enterprise environments."}, {"title": "REPLICABILITY AND ETHIC", "content": "The datasets used in this paper adhere to ethical standards, ensuring that no sensitive or personally identifiable information is included, and all data collection processes comply with relevant privacy and consent regulations. The entire framework, codebase, and resources presented in this paper are fully reproducible and will be accessible to the research community. We ensure that all datasets, agent architectures, evaluation metrics, and experimental setups are made available to facilitate seamless replication of our results. To further support replicability, we provide detailed documentation, and environment setup scripts, including the ST-WebAgentBench integrated with BrowserGym. Additionally, our experiments are designed with transparency in mind, ensuring that researchers can reproduce both the benchmark evaluations and the architectural improvements proposed. All materials can be accessed through [blinded URL]."}]}