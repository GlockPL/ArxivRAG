{"title": "RETROSPECTIVE LEARNING FROM INTERACTIONS", "authors": ["Zizhao Chen", "Mustafa Omer Gul", "Yiwei Chen", "Gloria Geng", "Anne Wu", "Yoav Artzi"], "abstract": "Multi-turn interactions between large language models (LLMs) and users naturally include implicit feedback signals. If an LLM responds in an unexpected way to an instruction, the user is likely to signal it by rephrasing the request, expressing frustration, or pivoting to an alternative task. Such signals are task-independent and occupy a relatively constrained subspace of language, allowing the LLM to identify them even if it fails on the actual task. This creates an avenue for continually learning from interactions without additional annotations. We introduce RESPECT, a method to learn from such signals in past interactions via retrospection. We deploy RESPECT in a new multimodal interaction scenario, where humans instruct an LLM to solve an abstract reasoning task with a combinatorial solution space. Through thousands of interactions with humans, we show how RESPECT gradually improves task completion rate from 31% to 82%, all without any external annotation.", "sections": [{"title": "1 INTRODUCTION", "content": "Language models (LMs) often engage in multi-turn interactions with human users. Similar to human-human interactions, these interactions are naturally rich with implicit learning signals. If the LM fails to respond appropriately, the user is likely to follow with an expression of frustration, a rephrase of their intent, or maybe even completely pivot what they ask for. Similarly, if the LM does well, the user may express approval or simply continue to their next objective. Such responses can inform the LM of its performance, thereby creating an opportunity to learn through retrospection.\nWe study the efficacy of such signals, and how they can lead to a system that improves over time. We introduce RESPECT, a simple approach to learn from signals the model itself derives about its own past actions through retrospection of past interactions with human users. We deploy RESPECT in MULTIREF, a new multi-turn grounded interaction scenario, which requires models to display complex abstract reasoning, and humans to gradually instruct models to accomplish sequences of goals to complete their tasks.\nThe key insight underlying RESPECT is that conversational implicit feedback signals occupy a relatively constrained subspace of natural language. Such signals can include direct approvals (e.g., great!) or signs of frustration (e.g., not again), and also more subtle cues, such as when the user rephrases their request. Critically, it is relatively simple to disentangle them from task performance. A human can easily figure out from such cues if they do well or not, even if they have little understanding about what they are asked for. It is this constrained nature that makes reasoning about such signals to be within the capacities of large language models (LLMs), even if they fail at the task at hand.\nRESPECT utilizes this signal in a process where the model interacts with humans, and after interaction decodes feedback for each of its actions from the interaction context including the follow up utterances. Figure 1 illustrates this process. The model interacts with humans to accomplish tasks, retrospectively examines its own past interactions, and then re-trains. This process progresses in rounds, alternating between interaction and training, with the model improving over time. Critically, unlike common recipes for training from human feedback, RESPECT does not require any external annotation (Ouyang et al., 2022, RLHF) or even soliciting feedback from the users themselves (Suhr & Artzi, 2023).\nWe deploy RESPECT in MULTIREF over multiple rounds of grounded interactions with human use and re-training. We use IDEFICS2-8B (Lauren\u00e7on et al., 2024) as our LLM, and experiment with multiple learning methods, including supervised learning, REINFORCE-style policy gradient (Williams, 1992; Kojima et al., 2021), and KTO (Ethayarajh et al., 2024). Across our experiments, we observe that IDEFICS2-8B effectively decodes feedback, even as it initially performs poorly in the same interactions. In our longest running experiment, we observe model task completion rate improves from 31% to 82%. Our code, data, and models are at https://lil-lab.github.io/respect."}, {"title": "2 TECHNICAL OVERVIEW AND NOTATION", "content": "We conduct continual learning studies by deploying our approach in MULTIREF, a new multi-turn grounded interaction scenario (Section 3). Overall, the study progresses in rounds, where the LLM policy is first deployed to interact with users and complete tasks, and the interactions are then used to re-train the policy. Our study involves multiple rounds, and our goal is to observe and evaluate the long-term dynamics of the process. This includes the robustness of our award decoding and training methods to the changing distribution of the data likely to be seen in an adaptive system in the wild. Section 3 describes our interaction scenario in detail, and Section 4 our learning method. First, we outline our problem of interest and its notation in abstract terms.\nTask Notation The policy's task is to respond effectively to human utterances given in conversa-tional context. Formally, let $\\pi_{\\theta}(a_t|x_t)$ be the policy that controls the listener behavior, with $a_t$ an action string that represents the model response and $x_t$ being the context on which the policy is conditioned, both at time $t$ in the interaction. The context includes the instruction history up to and excluding time $t$, including current (i.e., at time $t-1$) and past speaker utterances, as well as any other relevant context in which the interaction takes place. As our learning progresses in rounds, we denote $\\theta_p$ as the model parameters in round $p$, and $\\pi_{\\theta_p}$ as the parameterized policy.\nLearning and Deployment We study a continual learning setup, where the learning signal is acquired from interactions of the deployed model with human speakers. Our study progresses in rounds (Figure 1). Each round $p$ includes a deployment, followed by training. During deployment at round $p$, the model $\\pi_{\\theta_p}$ interacts with users. For each model action $\\hat{a}_t \\sim \\pi_{\\theta_p}(a|x_t)$, we record a tuple $(x_t, \\hat{a}_t, p_t, f_t)$, where $x_t$ is the context given to the model at time $t$ to predict action $\\hat{a}_t$, $p_t$ is the probability of $\\hat{a}_t$ at the time of prediction, and $f_t$ is the remainder of the interaction following $\\hat{a}_t$. Critically, these interaction tuples contain no explicit feedback. We compute the implicit feedback $\\hat{y}_t$ using a feedback decoder $\\phi(x_t, \\hat{a}_t, f_t)$, to obtain tuples $(x_t, \\hat{a}_t, \\hat{y}_t, p_t)$. We experiment with multiple learning objectives using this feedback: supervised learning (SFT), policy gradient, and KTO.\nEvaluation We measure the quality of the listener model $\\pi_{\\theta_p}(a_t|x_t)$ at each round $p$ primarily by interaction success rates from live human-bot deployments. The same interactions are used to train the model for the next round. We track various characteristics of model behavior, such as number"}, {"title": "3 MULTIREF: A MULTI-TURN GROUNDED INTERACTION SCENARIO", "content": "Key to our study is that tasks are relayed gradually across multiple turns, as commonly happens in human interactions. We create MULTIREF, a conversational interaction scenario where two partners, a speaker and a listener, coordinate on the selection of a set of items. In our studies, the speaker is always a human, and the listener is a model.\nMULTIREF generalizes the commonly studied reference game scenario. Its design and our choice of stimuli are grounded in existing work from both cognitive science and computational language mod-eling (Rosenberg & Cohen, 1964; Clark & Wilkes-Gibbs, 1986; Schober & Clark, 1989; Goodman & Frank, 2016). Figure 2 illustrates the scenario. Both partners observe a shared set of images, but in different order. The speaker is given a subset of the images as targets, with the goal of communicating the targets to the listener, so the latter selects the exact subset. Only the speaker can write messages, and only the listener can select or deselect images. The interaction concludes successfully once all and only targets are selected, or fails if the partners run out of turns, 20 in our studies.\nThe interaction progresses in turns $t$, alternating between speaker and listener turns. At each speaker turn, they provide a single unrestricted natural language utterance. It may direct the listener to select one or more items, ask to deselect items if the listener previously made a mistake, or include whatever other content they desire. This utterance as well as the history of the interaction, the set of images, and their selection status compose the context $x_t$ for the following model turn at time $t$. The follower responds with an action $a_t$, which includes one or more image selects or deselects according to their understanding of the speaker intention. The action space consists of all possible legal sequences of the form Deselect E select F or Select D G assuming images are code-named alphabetically.\nThe motivation behind the design of MULTIREF is to create a task-oriented scenario that is both accessible to non-expert humans and encourages constructing a solution in multiple turns, thereby creating multi-turn interactions and eliciting the learning signals we aim to study. The rules of the interaction are simple: the speaker needs to describe targets to select, and the listener has to select what the speaker is referring to. This makes MULTIREF easily accessible to crowdsourcing workers. At the same time, the solution the speaker communicates to the listener is relatively complex, because of the enormous solution space: Consider a conventional reference games, where the goal is to select a single image. The number of possible solutions is the number of images in the context. In MULTIREF, the goal is to select a subset of unknown size, so the combinatorial solution space the listener faces is exponential in the number of images. At the same time, the solution itself is structured, creating easy opportunities to decompose it to multiple steps."}, {"title": "4 RESPECT: RETROSPECTIVE LEARNING FROM PAST INTERACTIONS", "content": "RESPECT has two components: decoding implicit feedback from past interactions (retrospection) and learning from the decoded feedback signals (learning). We deploy RESPECT in an iterative continual learning scenario, where each round includes both steps. This deployment allows us to observe the dynamics of RESPECT over time. However, the method itself is not limited to continual learning, and can be applied a single time as well.\nThe goal of RESPECT is to re-estimate the parameters of a model given interactions that were collected by the model itself, or previous versions of it. We assume access to a raw dataset $D_{raw} = \\{(x^{(i)}, \\hat{a}^{(i)}, p^{(i)}, f^{(i)})\\}_{i=1}^n$, where $x^{(i)}$ is the policy context, $\\hat{a}^{(i)}$ is the predicted action, $p^{(i)}$ is the probability of this action, and $f^{(i)}$ is the remainder of the interaction following $\\hat{a}^{(i)}$. In our continual learning setup, $D_{raw}$ is a union of all data collected from past rounds.\nThe feedback decoder $\\phi$ computes a categorical feedback $\\hat{y}^{(i)} \\in \\{\\text{positive, neutral, negative}\\}$ for each action $a^{(i)}$ holistically based its context $x^{(i)}$, action taken $\\hat{a}^{(i)}$, follow up utterances $f^{(i)}$. This process transforms $D_{raw}$ to $D = \\{(x^{(i)}, \\hat{a}^{(i)}, p^{(i)}, \\hat{y}^{(i)})\\}_{i=1}^n$. We use this dataset for training.\n4.1 DECODING IMPLICIT FEEDBACK THROUGH RETROSPECTION\nWe implement the feedback decoder $\\phi$ by prompting the model to analyze past interaction tuples $(x, \\hat{a}, p, f)$ to compute feedback $\\hat{y} = \\phi(x, \\hat{a}, f)$. The goal is a process where the model bootstraps from its own interactions. Our hypothesis is that LLMs have the ability to reason about the relatively constrained space of implicit signals, even if they fail at the task. We show this empirically in our experiments. Critically, this process does not rely on a stronger LLM for critique or on past interactions created by other LLMs. Figure 3 shows the decoder prompt. We experiment with binary"}, {"title": "4.2 LEARNING", "content": "The feedback decoding process transformers the dataset from $D_{raw}$ to $D = \\{(x^{(i)}, \\hat{a}^{(i)}, p^{(i)}, \\hat{y}^{(i)})\\}_{i=1}^n$. We study several learning approaches using this data: supervised learning, offline reinforcement learning (RL), or the KTO-style utility maximization (Ethayarajh et al., 2024).\nSupervised Learning We fine-tune on positive data points ($\\hat{y}^{(i)} = \\text{positive}$) and discard data points predicted as neutral or negative. We use cross entropy loss with additional label smoothing to prevent overfitting and encourage exploration. Our setup is distinct from conventional supervised learning in that the data is coming from the model interactions (i.e., on-policy), and not from a given dataset. Also, we run the learning process iteratively, each time with more data. We do not design the supervised approach in any special way to fit these changes, but this is a potential avenue for future work, which can further improve performance.\nReinforcement Learning We follow prior work (Kojima et al., 2021) and use simple REINFORCE-style policy gradient (Williams, 1992). The categorical feedback $\\gamma^{(i)}$ (i.e., the text generated by the prompted LLM) is mapped to a numerical value with a simple reward function:\n$R(\\gamma) = \\begin{cases} 1, & \\gamma = \\text{positive} \\\\ 0, & \\gamma = \\text{neutral} \\\\ -0.1, & \\gamma = \\text{negative} \\end{cases}$\nDropping the $i$-superscripts for simplicity, the gradient estimator for a single example is:\n$\\nabla_{\\theta} = c R(\\gamma) \\nabla log P(a|x; \\theta_{p+1})$ where $c=\\begin{cases} 1, & \\text{if } R(\\gamma) \\geq 0 \\\\ \\frac{P(x; \\theta_{p})}{p}, & \\text{if } R(\\gamma) < 0 \\end{cases}$\nwhere the coefficient $c$ downweights examples with negative reward by their inverse propensity score (Kojima et al., 2021). This is critical because $\\lim_{p(.)\\rightarrow 0} log P(.) = -\\infty$. In practice, we also discard data points with predicted neutral feedback ($R(\\gamma) = 0$)."}, {"title": "5 EXPERIMENTAL SETUP", "content": "Interaction Instantiation We use the KILOGRAM (Ji et al., 2022) tangram images, following Gul & Artzi (2024). KILOGRAM contains 1,013 images. We randomly split them into a main split (912 tangrams) and a development split (101 tangrams). We create interaction contexts by randomly sampling 10 tangrams, and randomly select 3-5 as targets. The development split is exclusively used for seeding the initial listener policy $\\pi_{\\theta_0}$, and all human-bot interactions are conducted on images from the main split, i.e., tangrams that the seed policy $\\pi_{\\theta_0}$ has never seen before.\nModel and Initialization We use IDEFICS2-8B (Lauren\u00e7on et al., 2024) as our model for both the policy and feedback decoder. We fine-tune with LoRA (Hu et al., 2022). We seed the initial policy $\\pi_{\\theta_0}$ by fine-tuning the pretrained IDEFICS2-8B weights on a small supervised dataset of 90 successful turns from 25 human-human games constructed with the development split tangrams, augmented with 12 synthetically generated deselection turns, because while necessary for human-model interactions, deselections are rare in human-human interactions (Appendix B.2). Do is reused in continual training via rehearsal. We validate our design online with 30 main-split human-bot pilot interactions, or offline with a validation set of 344 successful main-split human-human turns (Appendix A). We use the original IDEFICS2-8B for feedback decoding, because the narrow focus of our data is likely to inhibit some general linguistic knowledge. This means we cannot see improvement in the model feedback decoding capability, likely low-balling the potential of the approach. It remains an important direction for future work to keep the decoder model in sync with the policy. This requires deployments that include high domain diversity. We observe the original IDEFICS2-8B to provide robust feedback decoding out of the box, confirming our hypothesis, and providing a solid ground for our experiments.\nSystem Variants We study six system variants based on two dimensions: (a) feedback decoder configuration (binary vs. ternary); (b) optimization methods (supervised vs. REINFORCE vs. KTO):\n*   B-SUP and T-SUP binary (B) / ternary (T) that only trains on positive data points with a supervised fine-tuning objective (SUP).\n*   B-RL and T-RL trains on both positive and negative data points using REINFORCE.\n*   B-KTO and T-KTO are like B-RL and T-RL, but using KTO.\nFor variants involving negative data points (B-RL, T-RL, B-KTO, and T-KTO), we subsample negative ones to keep the positive:negative ratio close to 5:4 (Ethayarajh et al., 2024).\nDeployment We conduct three rounds of training-deployment for all six systems and three more rounds for B-SUP. We select B-SUP for another three rounds because it is the most promising variant after three rounds, and we want to observe its progress over a longer period. The reason for this cascaded design is the high cost of experiments. We do not distinguish between training and evaluation in the traditional sense. Instead, all listener policies are evaluated live on MTurk on about 330 human-bot interactions each round containing roughly 2400 turns. Then the same data is used to train the next iteration of policies respectively. The policies in the same round are deployed concurrently in a randomized experiment on the same set of games to mitigate human biases and variances due to game difficulty. More details on crowdsourcing are in Appendix A.3."}, {"title": "6 RESULTS AND ANALYSIS", "content": "We deploy our models for three rounds, with additional three rounds for B-SUP, the best-performing variant, to better understand long-term dynamics. All our results are from concurrent randomized deployment, where the models interact with humans in real time. We collected a total of 7,230 interactions consisting of 55,004 utterances over 4 weeks, at a cost of $11,180 USD.\nFigure 4 shows the deployment statistics for all six system variants, as well as control deployments for the initial policy and human-human games. Figure 5 shows utterance-level statistics for B-SUP from the post-hoc annotations we collected. The interaction success rate of all systems improves monotonically in the first three rounds, except for B-KTO in round three. We conduct three more rounds with B-SUP, the leading system after the first three rounds. B-SUP then plateaus, and even shows a temporary decrease in performance, before resuming its improvement.\nOverall, B-SUP improves interaction-level success rate by 51% (31%\u219282%) and utterance-level exact match by 22% (31%\u219253%). At the last round, following the plateau, B-SUP interaction success rate improves by 5% (77%\u219282%). The number of turns follows these trends. As the policy gets better, more games are completed within the allotted number of turns, and even faster. B-SUP starts with 8.9 turns per game, and concludes with 6.7 per game. The center panel of Figure 5 shows that actions taken by the policy increasingly resemble human actions, even mistakes (actions that receive negative feedback) become more similar to human actions. All other statistics largely track these, except some of the utterance-level statistics around when B-SUP plateaus. While all show a deviation from the monotonous earlier trend, some show a temporary decrease and not just a stagnation, but delayed by one round. This illustrates the complex dynamics of continual learning, which we explore in more detail below.\nThere remains a significant gap between B-SUP (our leading system) and HH (human-human interac-tions), which shows perfect task success rate and almost double efficiency (i.e., tasks are completed"}, {"title": "7 RELATED WORK", "content": "Learning from Feedback Learning from feedback for LLMs is being studied extensively. RL from human feedback (RLHF) is maybe the most common technique (Ouyang et al., 2022). It relies on soliciting pair-wise preferences from annotators, which is significantly different than our reliance on signals from the interaction itself. Learning from feedback on a single system output has also been studied, either in the form of binary feedback (Ethayarajh et al., 2024; Suhr & Artzi, 2023; Gao et al.,"}, {"title": "8 DISCUSSION", "content": "We introduce RESPECT: retrospective learning from interactions, an annotation-free approach by leveraging signals from naturally occurring feedback in interactions. We demonstrate its effectiveness in long-term deployments and robustness to system variants. As opposed to evaluating on a static"}, {"title": "A THE MULTIREF GAME DESIGN AND DATA COLLECTION", "content": "A.1 INTERACTION DESIGN\nMULTIREF is a multi-target, multi-turn reference game between two players, a speaker and a listener. Each game starts with 10 tangrams as the context, with 3\u20135 tangrams designated as targets. The target designations are revealed to the speaker but hidden to the listener. The goal is to select all targets without selecting any non-targets. The speaker can only communicate with the listener through a sequence of utterances, and only the listener can take selection and deselection actions. The interaction starts with a speaker turn. Turns alternate between speaker and listener, with a maximum of 20 turns. In each speaker turn, they type an utterance to send to the listener. Speaker turns are limited to 25 seconds. In each listener turn, they have 45 seconds to select or deselect images as instructed to by the speaker. The game concludes when the listener selects only and all targets, or the when the partners run out of turns. Appendix A.3 shows screenshots of the interface.\nContext Construction We follow Gul & Artzi (2024) and construct game contexts using 1,013 tangram images from KILOGRAM Ji et al. (2022). We group tangrams randomly into two splits: development split (101 tangrams) and main split (912 tangrams). The development split is exclusively used for seeding the initial listener policy \u03c00. All human-bot interactions are constructed from the main split, i.e., tangrams that the seed policy \u03c00 has never seen before. We construct all games with 3-5 target tangrams. More targets are generally harder, given the same maximum number of turns per interaction.\nA.2 HUMAN EVALUATION DESIGN\nAutomatically evaluating turn-level policy performance is hard, because we have no ground truth (i.e., the selection and deselection actions intended by the speaker in each turn) to compare against. Similarly, we have no ground truth to systematically assess the feedback decoder quality. We conduct human evaluation surveys to address these problems. We annotate a subset of B-SUP interactions, roughly 120 interactions or 1,000 turns per system-turn.\nWe show human annotators a complete interaction turn by turn, without revealing the underlying targets. For each turn, the annotation consists for two phases:\n1.  Ground-truth: we show context, currently selected tangrams, and instruction given by the speaker. We ask the annotator to annotate the listener action. The annotator action $a*$ is considered as ground truth action for this turn. We use these labels for tune-level evaluation. After the action annotation, we reveal the action \u00e0 actually taken by the listener (i.e., the model) during the interaction.\n2.  Satisfaction: we present the follow-up utterance. We ask the annotator to rate if the speaker is satisfied with the listener's action, based on the follow-up utterance. They choose one of the following options:\na.  Yes.\nb.  Yes, even though the listener did not perform all required selections/deselections.\nc.  Yes, even though the listener made incorrect selections/deselections.\nd.  No.\nThe third option accounts for the listener accidentally selecting a target tangram not intended by the speaker, but the speaker choosing to move on without correction or even validating the selection. We treat these labels as ground truth for evaluating feedback decoders.\nWe annotate 5% of long-term human-bot interactions annotations by three different annotators, to estimate how reliable the annotations are. We observe 85% agreement on the correctness (whether $a = a*$) on ground truth stage, and 65% agreement on the ground-truth action $a*$ across workers.  For satisfaction annotation, we observe 93% agreement rate, illustrating the relative simplicity of extracting the signal that drive our learning process."}, {"title": "A.3 MTURK DETAILS", "content": "Worker Recruitment We follow Gul & Artzi's (2024) worker recruitment recipe. We require workers to have a minimum 98% approval rate, at least 1,000 approved HITs (Human Intelligence Task), and be located in English-majority locales. All workers must watch a video tutorial and pass a quiz before gaining qualification to work on MULTIREF interactions. They must read a thorough guideline and pass another quiz before granted access to human evaluation surveys. We recruit 33 expert workers to interact with LLMs in the main study and annotate by completing surveys after the main study. This study is exempted by Cornell's Institutional Review Board.\nPayment We pay workers $0.81 USD per MULTIREF game, and a bonus if the game is successful. Overall the estimated hourly wage is $13.00 USD, and closer to $23.00 USD by the end of the continual study when the LLM is fairly good at the game. On average a human-bot game takes under 2 minutes. We pay workers $0.06 USD per turn for human evaluation surveys, or $0.08 USD if the turn annotation involves error modes. The estimated hourly wage is $16.00 USD for human evaluation surveys. On average it takes under 2.5 minutes to annotate one game. We set the payment scheme through pilot studies and aim at $15.00 USD hourly wage.\nInterface and Serving We implement MULTIREF using Empirica (Almaatouq et al., 2021) and on top of the code base of Gul & Artzi (2024). The speaker has 25 seconds to type into a chat box each turn and hit Enter or submit, and the listener has 45 seconds to click on the tangrams to select or to deselect. The game ends if one party idles for one turn, and the party idling is not compensated. We serve on an EC2 instance. We serve LLM policies with the Ray framework (Moritz et al., 2018). We walk through the first turns of a sample interaction in Figure 8, Figure 9, and Figure 10."}, {"title": "B LEARNING DETAILS", "content": "B.1 INTERACTION REPRESENTATION\nWe encode the context x as in Figure 11. We standardize action representation by ordering actions, for example, always produce Select A C rather than Select CA. We shuffle the context images during training as the order of context tangrams should not have any impact on the interaction logic.\nB.2 POLICY INITIALIZATION\nWe seed the initial policy \u03c00 by fine-tuning the model on a small dataset of 90 turns D0, where both the speaker and the listener are humans. We also experimented with prompting to initialize the policy. We find early that few-shot prompting yields a random policy at best, likely because reasoning with abstract shapes such as tangrams is visually out-of-distribution for the model.\nThere is a significant distribution shift between human-human interactions, and human-policy interac-tion, especially early on when the model performs poorly. In practice, two major differences are the length of interactions and the prevalence of deselection instructions, which are rare in human-human interactions. We address the deselection issue with data augmentation. We synthetically generate turns where the speaker asks for deselections, and the listener complies. We augment the data with these at a ratio of 1:12 to the existing data. This helps the LLM policy learn to deselect and recover from mistakes. This augmentation is only used for Do and such distribution shift is not present in alter rounds, when learning from actual human-bot interactions.\nB.3 HYPERPARAMETERS AND OTHER IMPLEMENTATION DETAILS\nWe use the instruction-tuned IDEFICS2-8B model for all policies. We fine-tune with LoRA adapters (Hu et al., 2022) (a=r=8, dropout=0.1) due to compute constraints. Appendix D pro-vides more LoRA details. We train each model with a single GPU, RTX A6000, NVIDIA A100 40GB or 80GB. The time to train ranges between 2-24 hours, longer in later rounds as more data accumulates. For stopping criteria, we pick checkpoints by highest accuracy (exact match) among"}, {"title": "B.4 EVALUATION METRICS", "content": "Interaction-level Metrics Interaction performance and statistics are computed automatically from live deployment interactions. They do not require further annotation.\n1.  Success rate = # successful interactions / # all interactions. An interaction is successful if the listener selects all and only targets before running out of 10 turns. This is the primary metric we use to evaluate the performance of the LLM policy.\n2.  # Turns per interaction. This is a measure of collaborative efficiency."}, {"title": "C CUMULATIVE NUMBER OF INTERACTIONS OBSERVED", "content": "The main text includes results by round. We collect roughly 330 interactions per policy per round. Due to the uncertainty of live data collection, we do not always hit this exact number for each variant and round. Figure 13 shows the cumulative number of human-bot interaction seen by a policy variant by each round."}, {"title": "D ADDitional ENHANCED LORA LAUNCH", "content": "We suspect the plateau of B-SUP in Figure 4 is partially due to the limited expressivity of LORA adapters we used. We test this hypothesis by deploying round p = 4 and p = 5 again with enhanced LORA adapters. We use the same hyperparameters as in Section B.3 except additional adapters. The original adapter placement is on the text model, the modality projector, and the perceiver resampler. Adapters include the down projection layers, the gate projection layers, the up projection layers, and the key/query/value projection layers. In comparison, the enhanced launch adds adapters on the vision model, including the out projection, the first and the second fully connected layers, besides the projection layers on text models. Figure 14 shows the results from this complementary deployment. The enhanced LoRA adapters yield a small improvement in interaction success rate compared to the original launch, yet the overall slowdown is evident. This suggests LoRA expressivity has some effect, but other effects are also limiting the LLM policy from continuing its earlier improvement trends."}, {"title": "E FEEDBACK DECODER DESIGN", "content": "The prompt design is minimal, general, and task-agnostic. We validate the prompt with manual inspection prior to continual learning launch and human surveys. Considering only the most recent two action-utterance turns (\u00e2\u00bf\u22121, Ui, \u00e2i, Ui+1) is sufficient to produce satisfactory decoding results, and more history seems to distract the decoder.\nWe also experimented with numerical reward (i.e., decoding a real number), experimenting with a discretized reward space of {.0, 1, 5, 9} . Our experiments show the model is not well calibrated for such decoding."}, {"title": "F INTERACTION CASE STUDIES", "content": "Figures 15-18 show case studies that illustrate the diversity of MULTIREF interaction scenarios. Black borders indicate targets. Yellow dots indicate actions taken by the listener. Green borders indicate correct selections, while red borders indicate wrong selection."}]}