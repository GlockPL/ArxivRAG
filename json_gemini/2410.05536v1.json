{"title": "On Feature Decorrelation in Cloth-Changing Person Re-identification", "authors": ["Hongjun Wang", "Jiyuan Chen", "Renhe Jiang", "Xuan Song", "Yinqiang Zheng"], "abstract": "Cloth-changing person re-identification (CC-ReID) poses a significant challenge in computer vision. A prevailing approach is to prompt models to concentrate on causal attributes, like facial features and hairstyles, rather than confounding elements such as clothing appearance. Traditional methods to achieve this involve integrating multi-modality data or employing manually annotated clothing labels, which tend to complicate the model and require extensive human effort. In our study, we demonstrate that simply reducing feature correlations during training can significantly enhance the baseline model's performance. We theoretically elucidate this effect and introduce a novel regularization technique based on density ratio estimation. This technique aims to minimize feature correlation in the training process of cloth-changing ReID baselines. Our approach is model-independent, offering broad enhancements without needing additional data or labels. We validate our method through comprehensive experiments on prevalent CC-ReID datasets, showing its effectiveness in improving baseline models' generalization capabilities. Codes are available at GitHub.", "sections": [{"title": "I. INTRODUCTION", "content": "Person re-identification, which aims to match a target person's image across different camera views, is reported as a crucial task in the field of intelligent surveillance systems [1] and multi-object tracking [2]. Recently, the emergence of Deep Neural Networks (DNNs) has greatly promoted its development, offering promising results in applications. The underlying causal reasoning process of DNNs in the task can be roughly divided into three stages, namely generation, selection and classification, as shown in Fig. 1. In the generation step, the network backbone will generate features from the original image, which normally contain the correlated causal features (e.g., face and gait) and confounding features (e.g., tops and bottoms). Subsequently, the selection process will try to screen out the confounding features to obtain a causal feature representation of the image, which will be used in the following classification to give the final prediction.\nIn the early researches of ReID [3]-[7], methods are largely studied with cloth-consistent scenarios under the assumption that people do not change their clothes in a short period of time. Generally, these methods can perform well in short-term datasets but will suffer from significant performance degradations when testing on a long-term one where people change their clothes frequently. The reason behind such phenomena is very straightforward: unlike human who naturally learns to decouple causality from confounders, DNN tends to exploit the subtle statistical correlations existing in the training distribution for predictions. When trained in a cloth-consistent environment, it creates a strong correlation between the causal and confounding features (e.g., face and cloth appearance) such that the selection process can not effectively discard the latter, and finally resulting in a spurious correlation between the confounding features and person ID. Since the spurious correlation is usually easier to be captured, the model will quickly learn a \"shortcut\" [8] and ignore all other features (including the true causal features) during inference. However, solely depending on the confounding features is unreliable, since they have large variance across different environments, leading to untrustworthy predictions [9].\nTo provably avoid the shortcut degeneration problem in ReID, a natural solution is to either incorporate more causal features from other modalities (e.g., contour sketch [10], body shape [11], face/hairstyle [12], [13] and 3D shape [14]) and encourage the model to learn from them, or use hand-crafted clothes labels [15], [16] to force the model pay less attention to the confounding features (mainly the clothes appearance in CC-ReID task). However, obtaining information from multi-modalities requires additional models and training time, and will bring extra bias into the learning of models (i.e., the error in obtaining extra clues from other modality). On the other hand, using additional clothes labels requires much human work, and is not favorable for DNNs which usually requires tremendous amount of data.\nTherefore, we are actually motivated back to the root of the problem i.e., feature correlations. Since it is the strong correlation between causal and confounding features that finally leads to the shortcut between confounders and task (e.g., the"}, {"title": "II. RELATED WORK", "content": "Many approaches to person Re-identification (ReID) have been proposed, focusing on either clothing-consistent or cloth-changing scenarios."}, {"title": "A. Clothing Consistent Person ReID.", "content": "For clothing-consistent person ReID, early research in person re-identification (re-id) focused on feature extraction [25]-[28] or metric learning [29]-[32], while recent approaches leverage advancements in CNN architectures for end-to-end learning of these aspects [33]-[38]. For example, Sun et al. [17] introduced a part-based convolutional baseline (PCB) module, dividing a feature map into six horizontal blocks for effective ReID. Wang et al. [39] proposed a multibranch network (MGN) for both global and local feature represen-tations, enhancing pedestrian identity discrimination. Gao et al. [40] developed a deep spatial pyramid-based model (DCR) for collaborative feature reconstruction, addressing occlusion and pose changes. Other methods include utilizing human skeleton points or surface texture for guidance. For instance, Song et al. [41] introduced a mask-guided contrastive attention model (MGCAM) with a novel region-level triplet loss, while Miao et al. [42] proposed a pose-guided feature alignment (PGFA) method using key body points to detect occlusions. Gao et al. [43] developed a texture semantic alignment (TSA) approach for partial ReID, addressing occlusion and pose change challenges. However, these models are vulnerable to clothing changes as they heavily rely on the consistency of the appearance of clothes."}, {"title": "B. Clothes Changing Person Reid.", "content": "In contrast, cloth-changing person ReID deals with dramatic visual appearance changes over time. Traditional methods are less effective here, prompting the development of specialized datasets like LTCC [11], PRCC [44], Celeb-reID [45], and NKUP [46]. Innovative approaches include Yang et al.'s [44] SPT+ASE module using human contour sketching and spatial polar transformation, and Qian et al.'s [11] shape embedding and clothing-eliminating module (SE+CESD), focusing on body shape. Huang et al. [45] designed the ReIDCaps module using vector neurons to perceive clothing changes. Gao et al. [47] proposed a multigranular visual-semantic embedding"}, {"title": "III. METHODOLOGY", "content": "While we are seeking to reduce the correlations between features, the first step should be finding a way to effectively measure the intensity of such correlations."}, {"title": "A. Conceptual Separation", "content": "In person re-identification, the features of clothes can be biased when the person changes clothes. In this work, as shown in Fig. 1, we aim to eliminate the correlation between the causal features (i.e., the features that are related to the identity of the person) and the confounding features (i.e., the features that are related to the clothes). The first problem we encounter is to identify the concept for each channel in the feature maps. To address this, we first assume that each filter in the convolutional neural network (CNN) consistently represents the same conception. By acquiring the conceptual clustering, we frist freeze the model parameters and generate a set of features H\u2081 = {h{}1\u2208D' where I denotes the image sampling from the training dataset D, and hi stands for the features map generating by i-th filter. Then, we heuristically use the K-means clustering [54] to divided the set of CNN filters \u03a9 into K groups: G = {G1,G2,\u2026\u2026,GK} groups, where Gin Gj = \u00d8 and G\u2081U G2 U \u2026\u2026 U GK = \u03a9. Then, as shown in Fig. 3a, we proposes the penalty term\n $L_{cpt} = \\frac{1}{\\sum_{i, j \\in G_k} S_{i,j}} - \\frac{\\sum_{G_r \\neq G} \\sum_{i,j \\in G_k} S_{i,j'}}{\\sum_{i,j \\in G_k} S_{i,j}}$"}, {"title": "B. Conceptual Decorrelation", "content": "Before the introduction of density ratio estimation, one may just question why we bother to introduce such concept for feature correlation measurement, since the correlation coefficients seem to perform well in Fig. 2. However, using correlation coefficients requires a huge amount of space and time to save the features and compute the results, which is not widely applicable in today's training of DNNs. What we need is an on-the-fly measurement that can smoothly integrates with DNN's training. Fortunately, with a clever use, density ratio estimation [55], [56], an important technique in the unsupervised machine learning toolbox, can help us achieve the goal. Mathematically, the density ratio is defined as follows:\nDefinition III.1. The density ratio is defined as the ratio of the density of one distribution p to the density of another distribu-tion q, at a given point in the sample space. Mathematically, the density ratio is expressed as: \u03c4(x) = p(x)/q(x) where x is a data point in the sample space and p(x) and q(x) are the densities of the two distributions at that point.\nThe definition of density ratio looks attractive because in essence it measures how similar the two distributions are. Assuming we have the people's features from a model's layer, which are represented as h\u00b9 \u2208 n \u00d7 d\u00b9, where n and d are the number of people and feature dimensions respectively, if we can further build an uncorrelated data distribution h\u00b9 and compute the density ratio between the two, we can then directly measure hl's correlations between its d' dimensions. Inspired by [49], we construct the uncorrelated data distribution (Fig. 3b) by re-sampling a new distribution h' from h\u00b9, where hij = Random (hk) and i \u2260 k.\nNext, we adopt the probabilistic classification approach [57]-[59], a technique for estimating the density ratio of two classes, to calculate the density ratio. The basic idea is to learn a classifier that separates samples from each class and use the classifier to estimate the density ratio. We start by defining two classes of data with class labels y = +1 to {h}1 and y = \u22121 to {h}M\u2081 with batch size M. The density of each class is represented as Ph(x) = P(x | y = +1) and P(x) = P(x | y = \u22121), respectively. Using Bayes' theorem, the density ratio can be expressed as a ratio of class posterior probabilities P(y | x):\n$\u03c4(x) = \\frac{P_h(x)}{P(x)} = \\frac{P(y = -1)}{P(y = +1)} \\frac{P(y = +1 | x)}{P(y = -1 | x)}$\nThe prior probability of each class, P(y = \u22121) and P(y = +1), can be approximated as the ratio of the number of samples in each class. The class posterior probability P(y|x) can be estimated using a probabilistic classifier, such as Multilayer perception (MLP). In a probabilistic classifier, a function is trained to predict the class label for a given input sample, and the class posterior probability can be obtained as the output of the function. The density ratio estimator can then be calculated as the ratio of class posterior probabilities. Until now, we can effectively compute the features' correlations during training. In the next section, we will formally introduce how DEAR works to decorrelate the features."}, {"title": "C. Training Procedure", "content": "The overall framework of DEAR is depicted in Fig. 4. Our training procedure can be divided into two steps:\n\u2022 Stage I: we apply the K-means algorithm to produce indices for the features generated from the pretrained ResNet50, periodically updating the ReID backbone in several epoch for accurate labeling. Subsequently, DEAR performs a random shuffle of the person features to create a decorrelated distribution.\n\u2022 Stage II: After stage I, we start by optimizing the domain classifier to identify the source of features. Once the do-main classifier's parameters are fixed, we minimize both LID and Lept, while maximizing the density ratio loss T(x). This dual optimization encourages the backbone network to produce features with reduced correlations, enhancing the overall decorrelation of the feature repre-sentations.\nWe thereafter formally give the definition of\nTraining Identity Classifier. Let xi be a data sample of person re-id, and yD be its label. We further have Cp as the person ID classifier with parameter 4 and go is the backbone parameterized by 6. Fundamentally, we use the cross entropy loss LPID as:\nmin $L_{PID} (C_\\phi(g_\\theta(x)), y^D)$,\nTraining Domain Classifier. To combine density estimation in clothes changing person reid, this paper adds a new domain classifier to the existing re-id model. The domain classifier D has parameters 4, and its purpose is to predict the domain class of an input feature (i.e., whether it belongs to the original class or the uncorrelated class). During training, the domain classifier is optimized by minimizing the domain classification loss, which is calculated as a cross-entropy loss LD between the predicted domain class and the actual domain label:\nmin $L_D (D_\\phi (g_\\theta(x)), y)$, x \u2208 {hi}{=1{i}1"}, {"title": "D. Theoretical Analysis", "content": "We now theoretically prove that the decorrelation operation could help us to eliminate the confounding features.\nNotations. The d-dimensional features are represented by X = (X1, X2, ... Xa) \u2208 Rd, and the outcome is denoted by Y\u2208 R, based on a joint training distribution Ptr(X, Y). The test distribution, which is unknown, is denoted by Pte, while P refers to a specific target distribution that satisfies"}, {"title": "IV. EXPERIMENTS", "content": "In this study, a new framework is proposed for cloth-changing person ReID, and its performance is evaluated on three publicly available datasets: PRCC [20], LTCC [19], VC-Clothes [21]. The proposed method is evaluated using two common metrics, namely top-1 accuracy, and mAP, and three different test settings are defined to assess its capabilities. These settings include the general setting, which utilizes both clothes-consistent and clothes-changing samples to calculate accuracy; the clothes-changing setting (CC), which only uses clothes-changing samples; and the same-clothes setting (SC), which only uses clothes-consistent samples. For LTCC dataset, the re-identification accuracy is reported for both the gen-eral and clothes-changing settings. Regarding PRCC, the re-identification accuracy is reported for both the same-clothes and clothes-changing settings, following the methodology out-lined in [20]. In terms of VC-Clothes, we tested it on two groups of cameras, (1) camera 2&3 measured the accuracy for the same clothes scenario (SC), (2) camera 3&4 measured the accuracy for the clothes-changing scenario (CC)."}, {"title": "A. Comparison with State-of-the-art Methods", "content": "In our study, we benchmarked the proposed method against various leading ReID models on LTCC, PRCC datasets, as shown in Tab. I, including both standard ReID models like BoT [64], PCB [17], MGN [39], SCPNet [66], ISGAN [67], AIM [50], CAL [15], and OSNet [4], and those tailored for clothes-changing scenarios, such as LTCC [11], CASE [63], FSAM [61], CCFA [53], and 3DSL [62]. Chan et al. [52] implemented all models using available codes, with the exception of a few ( [10]-[12], [63]) which were replicated based on their descriptions in the papers. It should be noted that the clothes-changing re-id methods use information from different modalities to mitigate the interference of clothes, with 3DSL and FSAM integrating at least three modalities. However, the computational cost of these two methods is at least four times higher than that of DEAR. In addition, RCSANet utilizes additional clothes-consistent re-id data to enhance performance in the same-clothes setting. Neverthe-less, without using additional data and only using RGB images in the backbone of ResNet50, the proposed DEAR consistently outperforms all of these methods on both datasets. This com-parison highlights the effectiveness of DEAR. Additionally, we further evaluated DEAR on another cloth-consistent method (TransReID [69] and OSNet [4]) across varied benchmarks except for ResNet50."}, {"title": "B. Implementation Details", "content": "For image-based datasets (LTCC, PRCC, and VC-Clothes), the output feature map is subjected to global average pooling and global max pooling, as outlined in [71]. The resulting fea-tures are concatenated, normalized through BatchNorm [72], and the input images are resized to 384 \u00d7 192, as per [19]. Random horizontal flipping, cropping, and erasing [73] tech-niques are applied for data augmentation. Training is carried out for 150 epochs, with a batch size of 64, which contains 8 persons and 8 images per person, utilizing Adam [74]. The learning rate is set at 3.5e-4, with a 10-fold decrease every 20 epochs."}, {"title": "C. Attention Map", "content": "To determine the cues learned by the model, heat maps were visualized using different clothes-consistent baselines as shown in Fig. 6. Class activation mapping (CAM) [75] was used to generate these heat maps, with pixel brightness indicating the level of attention paid by the model. The second and third columns of this figure show how the model's attention to different image features changes with and without DEAR in ResNet50. Our analysis showed that the baseline models primarily focused on clothing, with little attention paid to the head due to the low proportion of head pixels. This led to poor performance in cloth-changing settings. In contrast, our method prioritizes the head and shoes, with a focus on facial features as demonstrated in Fig. 6. Surprisingly, our regularization appears to be a feature selection procedure that highlights the causal features that hair and shoes serve as important cues for our method from a biased feature set, as evidenced by a comparison of the attention maps."}, {"title": "V. LAYER SELECTION FOR APPLYING DEAR", "content": "Our investigation focuses on determining the optimal loca-tions in a network to apply DEAR. In Table IV, we explored three possible variants for substituting DEAR for these bot-tlenecks. These are (1) ResBlock-D2, where DEAR inserts the {2n, n = 1,2,..., 8}-th bottlenecks, (2) ResBlock-D4, where DEAR inserts the {4n, n = 1,2,..., 8}-th bottlenecks, and (3) ResBlock-D8, where DEAR inserts the {8n, n = 1, 2, ..., 8}-th bottlenecks. In Table IV, we investigated other possible variants for substituting DEAR for these bottlenecks. These are ResBlock-P1,ResBlock-P2,ResBlock-P3,ResBlock-P4, which plug DEAR in the layer-1,layer-2,layer-4,layer-4, respectively. We also examined the performance of DEAR with L1 regularization. Our findings, illustrated in Table IV, indicate that a majority of DEAR models outperform the baseline, with networks that incorporate full DEAR performing worse. Furthermore, we observed that 'ResBlock-D4' and 'ResBlock-Pl'achieved a higher test accuracy of 34.20% and 33.30%, respectively. We attribute this to the model adjustment on the correlated output provided by DEAR, which improves the model's representation ability. Additionally, DEAR with L\u2081 had a test accuracy of 33.60%, consistently higher than the baseline and DEAR only. We conjecture that L\u2081 further increases the information compression ability."}, {"title": "A. Evaluation on VC-Clothes", "content": "To showcase the superiority of DEAR, we also evaluate it using the VC-Clothes dataset, a synthetic virtual dataset from GTA5 as documented in Real et al. [21] and Hong et al. [61], focus on a subset of the dataset from cameras 2 and 3, known as the same-clothes (SC) setting. They also consider another subset from cameras 3 and 4, termed the clothes-"}, {"title": "B. Evaluation on Large-scale Datasets", "content": "LaST [78] and DeepChange [79] represent two signifi-cant long-term person re-identification (re-id) datasets. LaST, a large-scale dataset, comprises over 228,000 meticulously annotated pedestrian images sourced from various movies. This dataset is instrumental for exploring scenarios involving pedestrians with extensive activity ranges and prolonged time spans. On the other hand, DeepChange encompasses a col-lection of 178,000 bounding boxes from 1,100 distinct person identities, amassed over a year. For a balanced evaluation as per [15], we standardized the batch size to 64, with each batch encompassing 16 individuals and 4 images per individual. The performance metrics of DEAR and other methods including OSNet [76], ReIDCaps [60], BoT [77], mAPLoss [78], and CAL [15], are presented in a generalized setting. Specifically, for DeepChange, true matches were permitted from the same camera but distinct tracklets in line with the dataset's guide-lines [79]. As illustrated in Table III, DEAR showcases supe-rior performance over both baseline and contemporary state-of-the-art methods in the context of LaST and DeepChange datasets. Particularly noteworthy is the effectiveness of DEAR, especially in comparison to CAL, which employs the date of collection as an alternative to clothing labels."}, {"title": "C. Evaluating the Clothes Relevant Features", "content": "Drawing inspiration from Tishby et al. (2015) [24], we adopt the mutual information metric I(R;Y) = H(R) \u2013 H(R | Yc) to evaluate the interdependence between the representation and clothing labels. This measure assesses how much one random variable informs about the other and is characterized by its symmetric nature, meaning it remains unchanged regardless of the order of the variables. Our results, particularly with ResNet50's performance on the LTCC dataset, are graphically depicted in Fig. 5. Consistent with the observations in [24], the network initially focuses on accurately representing the input data, thereby minimizing generalization error. It then transitions to a phase of compressing the input representation, selectively disregarding non-essential details. Consequently, ResNet50 undergoes a dual-stage process aimed at diminishing clothes-relevant features. However, as illustrated in Fig. 6, this approach proves insufficient. In contrast, the implementation of ResNet50 with the DEAR plug-in demonstrates a consis-tent elimination of clothing information throughout the entire process. This distinction underscores the efficacy of DEAR in enhancing the model's ability to focus on more relevant features beyond mere clothing attributes."}, {"title": "D. Retrieval Result Visualization", "content": "The effectiveness of the DEAR model is demonstrated through retrieval results on the LTCC and PRCC dataset, as presented in Fig. 7. In each retrieval scenario, correctly matched images are indicated with green boxes, while in-correct matches are marked with red ones. Overall, DEAR significantly enhances the accuracy of retrieval, yielding more correct matches to the clothes changing scenario in top-ranked positions compared to the baseline. This demonstrates DEAR's capability in improving retrieval accuracy by feature decorrelation."}, {"title": "E. Feature Visualization", "content": "We also present a visualization of feature distributions using t-SNE [80] in a 2D feature space, as shown in Fig. 8. In this visualization, distinct colors represent different individuals. Each person maintains the same color throughout the various images. The dot symbol signifies images captured while the in-dividuals were wearing specific clothing. Conversely, the cross symbol denotes images captured when they were not wearing that clothing. This visualization reveals that methods DEAR effectively distinguish and cluster feature embeddings of the same individual with different clothing status, demonstrating"}, {"title": "VI. CONCLUSION", "content": "This paper presents a novel approach termed Density Ratio Regularization (DEAR) for the task of CC-ReID, which in-volves matching images of the same individual across different camera views or time points, accommodating variations in appearance due to factors like pose, lighting, and clothing changes. DEAR introduces an adversarial loss term designed to encourage the model to generate features that are decorrelated in each column.\nTo achieve this, we train a domain discriminator to differ-entiate between output features from the backbone network generated in its usual configuration and those produced from a batch-wise shuffle. The discriminator network then provides feedback to the backbone, guiding it to learn decorrelated features. Through extensive experiments on relevant datasets, our findings consistently demonstrate that DEAR surpasses the baseline method by a significant margin. The results highlight that the proposed adversarial loss effectively enhances the re-id model's robustness against clothing changes and improves its overall generalization ability."}]}