{"title": "Hierarchical Federated ADMM", "authors": ["Seyed Mohammad Azimi-Abarghouyi", "Nicola Bastianello", "Karl H. Johansson", "Viktoria Fodor"], "abstract": "In this paper, we depart from the widely-used gradient descent-based hierarchical federated learning (FL) algorithms to develop a novel hierarchical FL framework based on the alternating direction method of multipliers (ADMM). Within this framework, we propose two novel FL algorithms, which both use ADMM in the top layer: one that employs ADMM in the lower layer and another that uses the conventional gradient descent-based approach. The proposed framework enhances privacy, and experiments demonstrate the superiority of the proposed algorithms compared to the conventional algorithms in terms of learning convergence and accuracy. Additionally, gradient descent on the lower layer performs well even if the number of local steps is very limited, while ADMM on both layers lead to better performance otherwise.", "sections": [{"title": "I. INTRODUCTION", "content": "Federated learning (FL) is gaining popularity for tasks involving data that is either sensitive or expensive to collect, making privacy a key concern [1]. Additionally, FL accelerates the learning process by allowing parallel computations across multiple clients. Traditionally, FL has been implemented using a central server [1], but recent developments propose using a hierarchy of servers for improved scalability [2]. In this hierarchical FL model, two layers of data aggregation take place. The lower-layer \"edge\" aggregation is performed at edge servers, each serving a distinct set of clients, during intra-set iterations. In contrast, the top-layer \"cloud\" aggregation is executed at a cloud server during inter-set iterations.\nThe main motivation for utilizing hierarchical architectures often arises from the wide geographic distribution of clients. In wireless networks, this structure can improve transmission quality over wireless channels [3] and increase the efficiency of the learning process by localizing certain operations, which conserves communication resources and reduces time [4]. Additionally, clustering clients can help manage heterogeneity in device and network capabilities [5], [6], limit data traffic to specific administrative regions or social groups [7], and align with the topology of mobile networks or computing infrastructures [4], [8].\nHierarchical FL employing quantization methods has been proposed in [9], [10], with its integration with over-the-air computation investigated in [11], [12]. Another bandwidth-limited approach involves using pruning to reduce the scale of the neural network [13]. Additionally, network optimization problems such as client selection, resource allocation, and clustering are discussed in [14]\u2013[17]. All of this previous research builds on conventional FL theory based on gradient descent optimization, where model or gradient parameters are transmitted and aggregated, typically by averaging, at both the top and lower layers. This conventional approach reveals the entire knowledge of the model by directly transmitting its parameters.\nRecently, an FL approach based on the alternating direction method of multipliers (ADMM) [18] within a single client set has been proposed [19], [20]. This approach not only enhances privacy by obscuring model parameters during transmission but also significantly improves learning performance in terms of convergence and accuracy. Hierarchical structures could also benefit from such advanced distributed optimization methods to spare communication or local computing resources, or to improve accuracy.\nIn this paper, we propose a systematic approach to build hierarchical structures, that can be reused for different optimization methods. We demonstrate it for the case of ADMM. That is, we consider ADMM for the top layer of the hierarchy, integrating all clients across various sets into a unified training problem, and derive two possible optimization approaches for the lower layer: hierarchical federated double ADMM (HierF2ADMM), which uses ADMM at the lower layer, and hierarchical federated ADMM (HierFADMM), which uses gradient descent and averages model parameters at the lower layer, similar to conventional FL. A schematic representation of the conventional and proposed hierarchical FL algorithms is depicted in Fig. 1. This illustration demonstrates how our approach allows for the modular integration of two optimization methods within FL, a concept introduced in this work. Our experimental results demonstrate the superiority of these ADMM-based hierarchical FL algorithms over conventional hierarchical FL in various scenarios, including both i.i.d. and non-i.i.d. data distributions. Additionally, while our HierF2ADMM generally outperforms our HierFADMM method, the latter excels when the number of local training steps is limited, and consequently, an appropriate method can be selected according to the resources of the clients."}, {"title": "II. NETWORK MODEL AND LEARNING PROBLEM", "content": "Assume there are one cloud server and C edge servers with distinct client sets {V_c}^C_{c=1}. Each set V_c includes N_c clients, with each client k possessing a local dataset D_{kc}. Then, the global objective function is\n$$F(w) = \\sum_{c=1}^{C} \\frac{D_c}{D} F_c(w),$$ (1)\nwhere $$D = \\sum_{c=1}^{C} D_c$$ with each $$D_c = \\sum_{k=1}^{N_c} D_{kc}$$, and $$D_{kc} = |D_{kc}|,\\forall k, c$$. In (1), the intra-set objective function corresponding to set c is\n$$F_c(w) = \\sum_{k=1}^{N_c} \\frac{D_{kc}}{D_c} f_{kc}(w),$$ (2)\nwhere the local objective function at client k \u2208 V^c is\n$$f_{kc}(w) = \\frac{1}{|D_{kc}|} \\sum_{\\xi \\in D_{kc}} l(w,\\xi),$$ (3)\nwhere $$l(w, \\xi)$$ is the sample-wise objective function that measures the prediction error of w on a sample $$ \\xi$$. Therefore, the main problem of the learning process is to find the optimal model parameter vector $$w^*$$, which minimizes the global objective function as\n$$w^* = \\min_w F(w).$$ (4)"}, {"title": "III. CONVENTIONAL HIERARCHICAL FEDERATED LEARNING", "content": "Conventional hierarchical FL, referred to as HierFed [9], builds on gradient descent as its optimization foundation and applies the following two-step process at each global iteration t+1 to solve (4).\n1) Step 1: The edge servers transmit their intra-set model parameters, $$w_c^t$$, to the cloud server, where the following cloud aggregation occurs.\n$$w^{t+1} = \\sum_{c=1}^C \\frac{D_c}{D} w_c^t$$ (5)\n2) Step 2: Each client updates its local model parameters as $$w_{kc}^{0,0,t+1} = w^{t+1}, \\forall c, k$$. Next, \u03c4 intra-set iterations are performed at each set c as follows. During each intra-set iteration i, each client k \u2208 V^c updates its local model parameters (local training) using an L-step gradient descent as follows, $$l = \\{0, . . ., L \u2212 1\\}$:\n$$w_{kc}^{l+1,i,t+1} = w_{kc}^{l,i,t+1} - \\mu \\nabla f_{kc}(w_{kc}^{l,i,t+1}),$$ (6)\nwhere \u00b5 is the learning rate. Upon completion, all clients \u2200k \u2208 V^c send their local model parameters to the edge server c, where the following edge aggregation occurs.\n$$w_c^{i+1,t+1} = \\sum_{k=1}^{N_c} \\frac{D_{kc}}{D_c} w_{kc}^{L,i,t+1}$$ (7)\nThen, each client updates its parameters as $$w_{kc}^{0,i+1,t+1} = w_c^{i+1,t+1},\\forall k \\in V^c$$ for the next intra-set iteration. After \u03c4 intra-set iterations, the intra-set model parameters are updated as $$w_c^{t+1} = w_c^{\\tau,t+1}$$."}, {"title": "IV. HIERARCHICAL FEDERATED ADMM", "content": "We can reformulate the main problem (4) at the top layer as\n$$\\min_{w,W} \\sum_{c=1}^{C} F_c(w_c), \\text{ subject to } w_c = w, \\forall c,$$ (8)\nwhere $$W = \\{w_1,...,w_C\\}$$ represents the collection of all intra-set model parameter vectors from all sets. Based on the general ADMM approach as in [18], we propose HierFADMM with the following three steps in every iteration t + 1 to solve (8), which shares a similar structure with [19, Equation (13)].\n$$\\begin{cases}w^{t+1} = \\arg\\min_w \\mathcal{L}(w, W^t, \\Pi^t), \\\\w_c^{t+1} = \\arg\\min_{w_c} \\mathcal{L}_c(w^{t+1}, w_c, \\pi_c^t), \\forall c, \\\\\\pi_c^{t+1} = \\pi_c^t + \\sigma_c (w_c^{t+1} - w^{t+1}), \\forall c, \\end{cases}$$ (9)\nwhere $$\\mathcal{L}$$ represents the global augmented Lagrangian function of the problem (8) and $$\\mathcal{L}_c$$ denotes its corresponding intra-set function for each set c, as defined later in (10) and (11), respectively. Additionally, $$\\pi_c$$ is the Lagrangian multiplier and $$\\sigma_c > 0$$ is a constant associated with the intra-set function of set c. We also introduce $$\\Pi = {\\pi_1, ..., \\pi_C\\}$$. In the following, HierFADMM employs a federated learning approach to address each step in (9).\n1) Step 1: According to (8), the global augmented Lagrangian function is\n$$\\mathcal{L} (w, W, \\Pi) = \\sum_{c=1}^C \\mathcal{L}_c (w, w_c, \\pi_c),$$ (10)\nwhere the intra-set function of each set c is given by\n$$\\mathcal{L}_c (w, w_c, \\pi_c) =$$ (11)\n$$\\frac{D_c}{D} F_c(w_c) + (w_c - w, \\pi_c) + \\frac{\\sigma_c}{2} \\frac{D_c}{D} ||w_c - w||^2$$ \n$$=\\frac{D_c}{D} \\sum_{k=1}^{N_c} \\frac{D_{kc}}{D_c} f_{kc}(w_c) + (w_c - w, \\pi_c) + \\frac{\\sigma_c}{2} \\frac{D_c}{D} ||w_c - w||^2$$ \n$$=\\frac{1}{D} \\sum_{k=1}^{N_c} D_{kc} f_{kc}(w_c) + \\frac{D_c}{D} (w_c - w, \\pi_c) + \\frac{\\sigma_c}{2} \\frac{D_c}{D} ||w_c - w||^2 $$ \n$$ = \\sum_{k=1}^{N_c} \\frac{D_{kc}}{D} \\tilde{f}_{kc}(w_c, w, \\pi_c),$$ \nwhere\n$$\\tilde{f}_{kc}(w_c, w, \\pi_c) = f_{kc}(w_c) + \\frac{D_c}{D_{kc}N_c} (w_c - w, \\pi_c) + \\frac{\\sigma_c}{2} \\frac{D_c}{D_{kc}N_c} ||w_c - w||^2.$$ (12)\nTaking derivative from $$\\mathcal{L} (w, W^t, \\Pi^t)$$ with respect to w and equating the resulting expression to zero, similar to [19], we obtain\n$$w^{t+1} = \\frac{\\sum_{c=1}^C (\\sigma_c w_c^t + \\pi_c^t)}{\\sum_{c=1}^C \\sigma_c}.$$ (13)\nThis is cloud aggregation in HierFADMM. For this, each edge server c transmits $$\\sigma_c w_c^t + \\pi_c^t$$ to the cloud server. Since an intentionally concealed version of the intra-set model parameters $$w_c^t$$ is shared with the cloud server, our approach enhances privacy compared to the conventional approach, which requires transmitting the intra-set model parameters directly.\n2) Step 2: From (11), the second optimization problem in (9) at the lower layer is\n$$w_c^{t+1} = \\arg\\min_{w_c} \\sum_{k=1}^{N_c} \\frac{D_{kc}}{D_c} \\tilde{f}_{kc}(w_{kc}, w^{t+1}, \\pi_c^t),$$ (14)\n$$\\text{subject to } w_{kc} = w_c, \\forall k \\in V^c.$$\nGiven that each edge server c lacks access to local objective functions $$f_{kc}$$ in $$\\tilde{f}_{kc}$$ for all k \u2208 V^c, we propose, in this section, the following federated learning approach based on gradient descent to execute the distributed minimization in (14) with an iterative manner. In the next section, we will introduce an alternative approach based on ADMM. We remark that, beyond the privacy preservation of $$\\tilde{f}_{kc}$$, approximating the solution of (14) is also necessary as this update does not have a closed form solution in general.\nEach client updates its local model parameters as $$w_{kc}^{0,0,t+1} = w^{t+1}, \\forall c, k$$. Following this, \u03c4 intra-set iterations are conducted at each set c. In each of these iterations i, every client k \u2208 V^c updates its local model parameters via an L-step gradient descent process as $$l = \\{0, . . ., L \u2212 1\\}$:\n$$w_{kc}^{l+1,i,t+1} = w_{kc}^{l,i,t+1} - \\mu \\frac{D}{D_{kc}N_c} \\nabla \\tilde{f}_{kc}(w_{kc}^{l,i,t+1}, w^{t+1}, \\pi_c^t)$$\n$$ = w_{kc}^{l,i,t+1} - \\mu \\frac{D}{D_{kc}N_c} \\frac{\\partial}{\\partial w_{kc}} \\Big(f_{kc}(w_{kc}^{l,i,t+1}) + \\frac{D_c}{D_{kc}N_c} (w_{kc}^{l,i,t+1} - w^{t+1}, \\pi_c^t) + \\frac{\\sigma_c}{2} \\frac{D_c}{D_{kc}N_c} ||w_{kc}^{l,i,t+1} - w^{t+1}||^2 \\Big)$$\n$$ = w_{kc}^{l,i,t+1} - \\mu \\Big( \\nabla f_{kc}(w_{kc}^{l,i,t+1}) + \\frac{D_c}{D_{kc}N_c} \\pi_c^t + \\sigma_c \\frac{D_c}{D_{kc}N_c} (w_{kc}^{l,i,t+1} - w^{t+1}) \\Big),$$ (15)\nwhere the gradient descent direction is determined by taking the derivative of the local objective function $$\\tilde{f}_{kc}$$ as shown in (14). Please note that setting $$\\pi_c = 0$$ and $$\\\\sigma_c = 0$$ in (15) yields the same local update (6) as the conventional gradient descent. After completing the L steps, all clients \u2200k \u2208 V^c transmit their local model parameters to the edge server c, where edge aggregation then takes place.\n$$w_c^{i+1,t+1} = \\sum_{k=1}^{N_c} \\frac{D_{kc}}{D_c} w_{kc}^{L,i,t+1}$$ (16)\nSubsequently, for the upcoming intra-set iteration i + 1, each client k \u2208 V^c sets its parameters as $$w_{kc}^{0,i+1,t+1} = w_c^{i+1,t+1}$$. Once \u03c4 intra-set iterations are completed, the intra-set model parameters are updated as $$w_c^{t+1} = w_c^{\\tau,t+1},\\forall c$$. \n3) Step 3: Finally, the last step in (9) involves updating the Lagrangian multipliers, which can be performed locally at each client k \u2208 V^c and the edge server c as\n$$\\pi_c^{t+1} = \\pi_c^t + \\sigma_c (w_c^{t+1} - w^{t+1}).$$ (17)"}, {"title": "V. HIERARCHICAL FEDERATED DOUBLE ADMM", "content": "In this section, we introduce an alternative hierarchical FL algorithm, HierF2ADMM, which maintains the same learning structure as HierFADMM, particularly retaining Steps 1 and 3. However, in Step 2, we employ a different iterative federated learning approach based on ADMM to solve the distributed minimization (14) at the lower layer. During each intra-set iteration i + 1, this can be characterized as follows.\n$$\\begin{cases}w_c^{i+1,t+1} = \\arg\\min_{w_c} \\mathcal{L}_c (w^{t+1}, W_c^{i,t+1}, \\Pi_c^{i,t+1}), \\\\w_{kc}^{i+1,t+1} = \\arg\\min_{w_{kc}} \\mathcal{L}_{kc}(w^{t+1}, w_{kc}, \\pi_{kc}^{i,t+1}), \\forall k \\in V^c, \\\\\\pi_{kc}^{i+1,t+1} = \\pi_{kc}^{i,t+1} + \\sigma_{kc} (w_{kc}^{i+1,t+1} - w_c^{i+1,t+1}) \\forall k \\in V^c, \\end{cases}$$ (18)\nwhere $$W_c = \\{w_{1c},\u2026\u2026\u2026,w_{N_cc}\\}$$ and $$\\Pi_c = {\\pi_{1c},...,\\pi_{N_cc}\\}$$ represent the collections of all model parameter vectors and all Lagrangian multipliers associated with the clients in set c. Additionally, $$\\\\sigma_{kc} > 0$$ is a constant associated with client k \u2208 V^c. According to (14), the augmented Lagrangian functions in (18) are expressed as follows.\n$$\\mathcal{L} (w^{t+1}, W^{i+1}, \\Pi^{i+1}) = \\sum_{k=1}^{N_c} \\mathcal{L}_{kc} (w^{t+1}, w_{kc}^{i+1,t+1}, \\pi_{kc}^{i+1,t+1}),$$ (19)\nand\n$$\\mathcal{L}_{kc} (w^{t+1}, w_{kc}^{i+1,t+1}, \\pi_{kc}^{i+1,t+1}) =$$ (20)\n$$\\frac{D_{kc}}{D_c} \\tilde{f}_{kc}(w_{kc}^{i+1,t+1}, w^{t+1}, \\pi_c^t) + (w_{kc}^{i+1,t+1} - w_c^{i+1,t+1}, \\pi_{kc}^{i,t+1}) + \\frac{\\sigma_{kc}}{2} \\frac{D_{kc}}{D_c} ||w_{kc}^{i+1,t+1} - w_c^{i+1,t+1}||^2$$ \n$$= \\frac{D_{kc}}{D_c} \\tilde{f}_{kc}(w_{kc}^{i+1,t+1}, w^{t+1}, \\pi_c^t) + (w_{kc}^{i+1,t+1} - w_c^{i+1,t+1}, \\pi_{kc}^{i,t+1}) + \\frac{\\sigma_{kc}}{2} \\frac{D_{kc}}{D_c} ||w_{kc}^{i+1,t+1} - w_c^{i+1,t+1}||^2.$$\nFollowing the approach outlined in Section IV, the three steps in (18) can be executed as follows. First, all the clients in the set c send a linear combination of their local model parameters and Lagrangian multipliers from the previous iteration i as $$\\sigma_{kc} w_{kc}^{l,i+1,t+1} + \\pi_{kc}^{i,t+1}$$ to the edge server c for edge aggregation as\n$$w_c^{i+1,t+1} = \\frac{\\sum_{k=1}^{N_c} (\\sigma_{kc} w_{kc}^{l,i+1,t+1} + \\pi_{kc}^{i,t+1})}{\\sum_{k=1}^{N_c} \\sigma_{kc} },$$ (21)\nwhich minimizes (19) as in the first step of (18). Then, each client k sets its parameters to $$w_{kc}^{0,i+1,t+1} = w_c^{i+1,t+1}$$, and subsequently updates them through an L-step gradient descent process to minimize (20) in the second step of (18) as follows $$l = \\{0, . . ., L \u2013 1\\}$:\n$$w_{kc}^{l+1,i+1,t+1} = w_{kc}^{l,i+1,t+1} - \\mu \\frac{D_c}{D_{kc}} \\nabla \\mathcal{L}_{kc} (w^{t+1}, w_{kc}^{l,i+1,t+1}, \\pi_{kc}^{i,t+1})$$\n$$ = w_{kc}^{l,i+1,t+1} - \\mu \\frac{D_c}{D_{kc}} \\frac{\\partial}{\\partial w_{kc}} \\Big( \\tilde{f}_{kc}(w_{kc}^{i+1,t+1}, w^{t+1}, \\pi_c^t) + (w_{kc}^{l,i+1,t+1} - w_c^{i+1,t+1}, \\pi_{kc}^{i,t+1}) + \\frac{\\sigma_{kc}}{2} \\frac{D_{kc}}{D_c} ||w_{kc}^{i+1,t+1} - w_c^{i+1,t+1}||^2 \\Big)$$\n$$ = w_{kc}^{l,i+1,t+1} - \\mu \\Big( \\nabla f_{kc}(w_{kc}^{i+1,t+1}) + \\frac{D_c}{D_{kc}N_c} \\pi_c^t + \\frac{D_c}{D_{kc}N_c} \\sigma_c (w_{kc}^{l,i+1,t+1} - w^{t+1}) + \\pi_{kc}^{i,t+1} + \\sigma_{kc} (w_{kc}^{l,i+1,t+1} - w_c^{i+1,t+1})\\Big)$$\n$$ = w_{kc}^{l,i+1,t+1} - \\mu \\Big( \\nabla f_{kc}(w_{kc}^{i+1,t+1}) + \\frac{D_c}{D_{kc}N_c} \\pi_c^t + \\frac{\\sigma_c D_c}{D_{kc}N_c}(w_{kc}^{l,i+1,t+1} - w^{t+1}) + \\pi_{kc}^{i,t+1} + \\sigma_{kc} (w_{kc}^{l,i+1,t+1} - w_c^{i+1,t+1})\\Big).$$ (22)\nThis is enabled since the client k has access to the function $$\\mathcal{L}_{kc}$$. Please note that by setting $$\\pi_c^t = 0$$ and $$\\\\sigma_{kc} = 0$$ in (22), the local update becomes equivalent to that of HierFADMM. Then, as in the third step of (18), each client k updates its local Lagrangian multiplier as\n$$\\pi_{kc}^{i+1,t+1} = \\pi_{kc}^{i,t+1} + \\sigma_{kc} (w_{kc}^{L,i+1,t+1} - w_c^{i+1,t+1}).$$ (23)\nAfter completing \u03c4 intra-set iterations, the intra-set model parameters are updated as $$w_c^{t+1} = w_c^{\\tau,t+1}$$ for all c.\nIn HierF2ADMM, both edge servers and all clients conceal their model parameters during transmission, thereby achieving higher privacy compared to HierFADMM.\nFinally, it is important to note that the computational complexity of the three algorithms, HierFed, HierFADMM, and HierF2ADMM, is equivalent. This is because the local updates (6), (15), and (22) all involve the same gradient calculation."}, {"title": "VI. CONVERGENCE ANALYSIS", "content": "In this section, we analyze the convergence of HierFADMM and HierF2ADMM under the assumption that the number of intra-set iterations grows over time. We leave the analysis of the case with fixed \u03c4 for future work.\nHierFADMM and HierF2ADMM by design can be interpreted as ADMM with an additive error, denoted by e, arising from the inexact solution of the second update in (9), that is\n$$\\begin{cases}w^{t+1} = \\arg\\min_w \\mathcal{L}(w, W^t, \\Pi^t), \\\\w_c^{t+1} = \\arg\\min_{w_c} \\mathcal{L}_c(w^{t+1}, w_c, \\pi_c^t) + e, \\forall c, \\\\\\pi_c^{t+1} = \\pi_c^t + \\sigma_c (w_c^{t+1} - w^{t+1}), \\forall c. \\end{cases}$$ (24)\nThe additive error is different depending on whether the local training employs gradient descent (HierFADMM) or ADMM (HierF2ADMM). Both algorithms thus coincides with [21, Algorithm 1], and the following result holds.\nProposition 1: Assume $$f_{kc}$$ in (3) to be closed, proper and convex for all k, c. Consider a version of HierFADMM or HierF2ADMM in which the number of intra-set iterations ($$\\tau_t$$) changes over time according to $$\\lim_{t\\rightarrow\\infty} \\tau_t = \\infty$$. Then $$w^t$$ and $$w_c^t, \\forall c$$, converge to a solution of (8).\nProof. As discussed above, we can interpret HierFADMM and HierF2ADMM as the inexact ADMM in (24). Additionally, if the number of intra-set iterations grows according to $$\\lim_{t\\rightarrow\\infty} \\tau_t = \\infty$$, then $$\\lim_{t\\rightarrow\\infty} e = 0$$, as the intra-set updates become increasingly accurate. Therefore, we can apply [21, Proposition 1] to guarantee convergence."}, {"title": "VII. EXPERIMENTAL RESULTS", "content": "We consider the logistic regression learning task, following [19], [20], with the parameter values provided in Table I. In this setting, each client k \u2208 V^c has its objective function as\n$$f_{kc}(w) =$$\n$$\\frac{1}{|D_{kc}|} \\sum_{j=1}^{|D_{kc}|} (ln(1 + e^{-b_{kc}^{(j)} (a_{kc}^{(j)}, w)})) + \\frac{\\lambda}{2}||w||^2,$$ (25)\nwhere $$(a_{kc}^{(j)}, b_{kc}^{(j)} \\in \\{0,1\\})$$ is the j-th sample in D_{kc}, and $$\\\\lambda = 0.001$$ is a penalty parameter. We use the Adult Census Income dataset from the UCI Machine Learning Repository to generate the samples. In the following, we compare our algorithms, HierFADMM and HierF2ADMM, with the conventional algorithm, HierFed, under various scenarios.\nFigs 2 and 3 show the objective for two different values, L = 1 and L = 4, under an i.i.d. dataset distribution across clients. It is observed that in both cases, HierFADMM significantly outperforms HierFed. Additionally, while HierF2ADMM outperforms HierFADMM with a higher number of local steps (L = 4), its performance is not convergent when the number of local steps is minimized to 1. This is because HierF2ADMM approximately solves the distributed minimization (14) via ADMM, requiring a minimum number of local iterations to achieve a sufficiently accurate approximation.\nIn Fig. 4, the objective is shown for two scenarios with different numbers of sets, specifically C = 3 and C = 6, while the total number of clients in the system is fixed at 150. Thus, for each case, the number of clients per cluster is given by N = 1080, Vc, with L = 4. As illustrated, the performance gap between HierFADMM and HierF2ADMM widens as C decreases. This occurs because more clients per cluster improve the approximation needed to solve (14) using ADMM.\nFig. 5 displays the objective for L = 4 under a non-i.i.d. dataset distribution across clients. In this scenario, each client holds samples from only one class, with the number of samples varying across clients. As observed, HierF2ADMM outperforms HierFADMM, and HierFADMM significantly outperforms HierFed."}, {"title": "VIII. CONCLUSIONS", "content": "We developed hierarchical FL based on an ADMM approach at the top layer of the hierarchy. Within this framework, we introduced two novel algorithms: hierarchical federated ADMM, which employs conventional gradient descent-based FL, and hierarchical federated double ADMM, which incorporates ADMM, at the lower layer. The new edge and cloud aggregations, coupled with local model and Lagrangian multiplier updates, in both algorithms results in enhanced learning performance and higher privacy compared to conventional hierarchical FL that relies solely on gradient descent. Our approach facilitates the integration of various other optimization methods at both lower and top layers in future implementations. Each optimization method brings its unique characteristics, enabling the combination of diverse optimization features within a single FL framework."}]}