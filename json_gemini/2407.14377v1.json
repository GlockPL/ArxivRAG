{"title": "Enhancing Cloud-Native Resource Allocation with Probabilistic Forecasting Techniques in O-RAN", "authors": ["Vaishnavi Kasuluru", "Luis Blanco", "Engin Zeydan", "Albert Bel", "Angelos Antonopoulos"], "abstract": "The need for intelligent and efficient resource provisioning for the productive management of resources in real-world scenarios is growing with the evolution of telecommunications towards the 6G era. Technologies such as Open Radio Access Network (0-RAN) can help to build interoperable solutions for the management of complex systems. Probabilistic forecasting, in contrast to deterministic single-point estimators, can offer a different approach to resource allocation by quantifying the uncertainty of the generated predictions. This paper examines the cloud-native aspects of O-RAN together with the radio App (rApp) deployment options. The integration of probabilistic forecasting techniques as a rApp in O-RAN is also emphasized, along with case studies of real-world applications. Through a comparative analysis of forecasting models using the error metric, we show the advantages of Deep Autoregressive Recurrent network (DeepAR) over other deterministic probabilistic estimators. Furthermore, the simplicity of Simple-Feed-Forward (SFF) leads to a fast runtime but does not capture the temporal dependencies of the input data. Finally, we present some aspects related to the practical applicability of cloud-native O-RAN with probabilistic forecasting.", "sections": [{"title": "I. INTRODUCTION", "content": "The conventional Radio Access Network (RAN) architecture used in mobile networks used to have considerable problems with network scalability based on demand, insufficient flexibility, and tightly coupled vendor-specific hardware and software configurations. These limitations were addressed together with the virtualization and disaggregation of RAN network components, resulting in the evolution towards O-RAN. O-RAN aims to achieve openness, intelligent decision making, and simplicity in the integration of third-party services [1]. Nevertheless, network optimization and resource allocation remain a major challenge, especially as network management becomes more complex. In addition, real-time decision making is a major challenge due to fluctuating network requirements. Cloud-native O-RANs have significantly changed the design, management, and operation of the network component. Containerization, dynamic orchestration, and the management of microservices in cloud-native O-RAN have helped to create an effective, scalable, and flexible network [2]. However, the complexity of resource allocation has increased now, requiring more sophisticated techniques [3].\nIn modern telecommunications, effective resource allocation is crucial to meet dynamic network requirements. With the evolution of 6G technologies, the requirements for computing power, bandwidth, storage capacity, and orchestration of resources have become more specific depending on the application and service. Radio and computing resources must be allocated dynamically to cope with variable network traffic. In addition, latency- and reliability-dependent Ultra-Reliable and Low Latency Communication (URLLC) applications, the high bandwidth requirements of massive Machine-Type Communication (mMTC) applications, and the growing number of Internet of Things (IoT) devices must also be managed. Effective management of resources can lead to the optimization of operating costs and energy consumption; at the same time, the network can provide better Quality of Service (QoS) and Quality of Experience (QoE) to end users [4].\nIn wireless networks, Physical Resource Block (PRB)s are considered the most important resource as they play a crucial role in optimal radio spectrum utilization. The landscape of existing resource allocation techniques in O-RAN is diverse and reflects the ongoing evolution of mobile network architectures. Several studies have delved into conventional approaches to resource allocation, highlighting the challenges posed by dynamic and distributed radio access networks [5], [6]. However, there is a lack of resource allocation with probabilistic forecasts that would enable decision-making in the O-RAN.\nAdvanced and efficient network management can be achieved by incorporating probabilistic forecasting into resource provisioning applications of O-RAN [7]. Traditional deterministic single-point forecasting models fail to address modern telecommunication networks' dynamic nature and uncertainty. Various time series analyses and machine learning algorithms have been used in previous works to model and predict network parameters [8]. However, probabilistic forecasting models offer a spectrum of possible outcomes"}, {"title": "A. Transition to Cloud Native Architectures", "content": "The cloud-native O-RAN architecture aims to enable better network scalability, agility, and effective management and operations through key changes in network management and deployment strategies. Compared to traditional architectures, the use of containers, Continuous Integration / Continuous Delivery or Continuous Deployment (CI/CD), and microservices in cloud-native technology applications contributes to faster deployment of services, simplifies the integration of third-party applications, and improves fault tolerance. Cloud-native deployment is in line with the O-RAN vision of an open, intelligent, and flexible network and paves the way for the development and deployment of effective resource management and provisioning strategies to meet real-time network requirements [2]. The main drawback is that the dynamic nature of the network presents challenges in resource management and allocation. Advanced resource provisioning solutions and efficient orchestration of resources are essential to deal with the scalable and volatile nature of containers in a multi-vendor infrastructure. Cloud-native O-RAN ensures a flexible and efficient network but also demonstrates the need for innovative techniques to manage the complexity of an advanced network infrastructure.\nThe advent of cloud-native paradigms has also significantly reshaped resource management strategies, particularly in the context of O-RAN [2], [10], [11]. The authors in [11] propose a new generation of Management and Orchestration (MANO)/Operations, Administration, and Maintenance (OAM) that follows the principles of cloud-native."}, {"title": "B. Contributions", "content": "Significant contributions are made in this paper within the scope of AI-enabled resource allocation in cloud-native O-RAN environments. Initially, O-RAN architecture equipped with containerizing and integrating resource provisioning rApp in O-RAN is briefed in section 2. Then, the probabilistic forecasting methods used for accurate prediction of resources along with containerization of the resource provisioning rApp are explained in section 3. Moreover, the metrics used to highlight the efficiency of rApp in terms of error, CPU, and memory usage are emphasized in section 4. Here, the effect of data length on the prediction and comparison of the performance of different estimators is also briefed. Finally, some case studies on cloud-native O-RAN in real-world applications are provided in section 5, followed by a conclusion in section 6."}, {"title": "II. O-RAN ARCHITECTURE", "content": "The traditional RAN is highly dependent on hardware components, leading to vendor lock-in issues. It is causing a huge rise in Capital Expenditure (CAPEX) and Operating Expense (OPEX) costs. It becomes very challenging for network providers to integrate intelligence and build a collaborative and reliable network. Therefore, it's crucial to establish next-generation RAN solutions with global, self-reliant hardware and software-defined technology that are independent of vendors. Virtualization and RAN dis-aggregation are the key technologies for the concept of O-RAN, whose main pillars are openness and intelligent resource management[1]. Fig. 1 shows the Open RAN architecture with probabilistic forecasting based resource provisioning as rApp. The main components of O-RAN are:\n\u2022 Open-Radio Unit (O-RU), Open-Distribution Unit (O-DU) and Open-Central Unit (O-CU) whose functionalities are similar to that in 5G dis-aggregated RAN except with added support of O-RAN based specifications and interface.\n\u2022 Near-Real Time RAN Intelligent Controller (RIC) to control/optimize RAN elements and resources based on fine-grained data using online Artificial Intelligence (AI)/Machine Learning (ML) based services. It is suitable for applications with latency requirements between 10ms and 1s."}, {"title": "III. METHODOLOGY", "content": null}, {"title": "A. Overview of Cloud Native Resource Allocation in O-RAN", "content": "Cloud-native principles are increasingly being integrated into O-RAN to improve resource allocation strategies and optimize network performance. Cloud-native resource allocation in O-RAN includes the use of containerized applications, microservices, and dynamic orchestration for efficient resource utilization. Applications are encapsulated in lightweight containers, enabling portability and consistency across environments. O-RAN also uses a microservices architecture that splits monolithic applications into smaller, independently deployable services. Each microservice focuses on a specific function and facilitates scalability and agility in resource allocation. Dynamic orchestration frameworks such as Kubernetes play a central role in the allocation of cloud-native resources. They automate the deployment, scaling and management of containerized applications and ensure optimal resource utilization based on real-time demand. Service mesh technologies such as Istio\u00b9 improve communication and control between microservices. They provide features such as load balancing, traffic management, and resilience and contribute to efficient resource allocation in a distributed and scalable manner. Resource allocation strategies such as autoscaling, resource pooling, or declarative resource management techniques (e.g., using Infrastructure as Code (IaC) or configuration files) can be used to take advantage of scalability, flexibility, operational efficiency, and cost optimization features of cloud-native deployments."}, {"title": "B. Introduction to Probabilistic Forecasting Techniques", "content": "Probabilistic estimators like SFF, DeepAR, and Transformer are being used in a wide variety of applications as they are more advantageous when compared to single-point forecast models like Long-Short Term Memory (LSTM) and Gated Recurrent Unit (GRU)s. They are gaining more attention due to their ability to provide a range of possible outcomes along with information about their uncertainty of occurrence.\nSFF forecasting works based on a simple feed-forward neural network. Neural networks, also called Multi-Layer Perceptrons (MLP), are built with a combination of an input layer, a hidden layer, and an output layer. The number of layers and neutrons rises with the task complexity. Here, the information flows in the forward direction from the input to the output layer from neurons. they don't have any feedback loop. In the training phase, the model predicts possible outcomes using the initial assigned weights. Furthermore, the actual and predicted values are compared to adjust the network weights in each layer to improve the predictions. This parameter adjustment is called Back-propagation. In the prediction phase, the trained model parameters are used to process the new information and predict the appropriate possible outcomes along with their probability of occurrence in the form of uncertainty.\nDeepAR forecasting was developed by Amazon [12]. They work based on the Recurrent Neural Network (RNN) framework. They are autoregressive recurrent network encoder-decoder that uses an encoder-decoder architecture with a sequence-to-sequence model based on LSTM cells. The DeepAR model is trained to maximize the likelihood function. they use a negative log-likelihood loss function to optimize the neural network parameters. During prediction, the learned likelihood function is used to forecast the possible output. They have an exceptional understanding of patterns and relationships in historical information. The DeepAR accepts inputs as time series information and collects the temporal information using RNN. The RNN predicts future outcomes using historical data and improves the accuracy of prediction through covariates. The obtained output of RNN passes through the connected layers to generate a probabilistic forecast of future outcomes. DeepAR is more versatile in capturing regular and irregular trends in time series data and more robust towards data seasonality. Since the mean and variance of the distribution are considered in the prediction of the loss function, the model performance is more accurate and accounts for the uncertainty in the forecasting."}, {"title": "C. Integration Approach of Probabilistic Forecasting as Radio Application (rApp) in O-RAN", "content": "In the O-RAN context, the integration of probabilistic forecasting within the Radio Application (rApp) framework is a strategic approach aimed at optimizing wireless communication systems. This integration involves using RAN intelligence, a crucial element, where the network dynamically manages radio resources, making real-time decisions to adapt to diverse conditions and user requirements. To formalize this approach, the O-RAN Alliance has introduced a framework incorporating both Non-Real and near-real Time RAN Intelligent Controllers (RICs) and associated interfaces (E2, 01, and O2). By combining the Service Management and Orchestration (SMO) capabilities with the Non-Real Time RIC and rApps, a holistic perspective of the entire O-cloud components and the available network services is achieved. This integrated component empowers the creation of high-level policies and facilitates the lifecycle management of network services with a time granularity exceeding 1s, enhancing the overall efficiency and adaptability of the O-RAN ecosystem."}, {"title": "IV. EXPERIMENTAL SETUP AND RESULTS", "content": null}, {"title": "A. Experimental Setup", "content": "The performance of deterministic and probabilistic forecast estimators is shown in this section. Python programming was used along with the Gluonts library [14] to analyze the resource provision rApp. Three probabilistic forecasting algorithms, namely SFF, DeepAR, and Transformer, were used to predict the DL PRBs required for next 24-hours based on historical data. The historical PRB data of the tenants can be obtained from the O-DU in the O-RAN architecture via the Ol interface. The dataset was created by simulating traffic and mobility patterns for a different number of end users [15]. Initially, the history data is obtained from the tenant for training the model, and their prediction is performed to evaluate the accuracy of each estimator. The history PRB data s is divided into training and test data in a ratio of 80:20. The training data is used to adjust the machine learning parameters and train the model for each forecasting algorithm. In our work, training data sets of different lengths namely 2 weeks, 4 weeks, 10 weeks and 20 weeks are used. The test data \u0445, on the other hand, is used to evaluate and compare the performance of each model. The prediction length is fixed to 24 hours, irrespective of changing training data length. Furthermore, the hyperparameters considered for estimators are as follows: SFF: epocs=5, batch size=1, hidden layer dimension=[40,40], and number of evaluation samples=100. DeepAR: epocs=5, batch size=1, Recurrent Neural Network (RNN) Layers=2, number of cells per RNN=40, and number of evaluation samples=100. Transformer: epocs=5, batch size=1, number of evaluation samples=100, dimension of transformer network=32, inner-hidden layers of transformer's feedforward network dimension=4, and context length=24. LSTM: Sequential model, epocs=5, batch size=1, neurons=1, and optimizer: adam. The performance evaluation is done in the following ways: Initially, using error metrics like Mean Square Error (MSE), followed by calculating training time and prediction time, and finally by monitoring the CPU and memory usage during training and prediction.\nIn time series forecasting, error information can be obtained by evaluation metrics, where the array of forecasted probability distribution y are compared with the actual test instances x. Here, the MSE metric is used to analyze the prediction error. MSE Measures the average of the squared differences between the forecasted and actual values. The output of probabilistic forecasting estimators is the range of PRBs (V). It is a combination of [y\u2081, y\u2082\u2026y\u2099], where N is the length of test instance and each y\u1d62 is a column vector that looks like [Y\u1d62\u2081, Y\u1d62\u2082,\u2026Y\u1d62\u2099]\u1d40 with on being the percentiles ranging from 1-th to 99-th. These"}, {"title": "B. Results and Analysis", "content": "Table I compares probabilistic models like SFF, DeepAR, and Transformer, with deterministic LSTM as the base model for different data lengths. The main aim here is to analyze the impact of data size on the error metric MSE, model training time, and future prediction time. The MSE is calculated using Equation 1. The size of data lengths considered here are 2 weeks, 4 weeks, 10 weeks, and 20 weeks. From the MSE values, it can observed that the model performance is bad when trained with the small data set. The MSE is low, around 51.20, 1.38, 0.03, and 0.09 for LSTM, SFF, DeepAR, and Transformer, respectively, with 20 weeks of data. When compared among the estimators, LSTMs performance is the worst, irrespective of the data length, as the deterministic base models are more sensitive to outliers, uncertainties, and data non-stationarity. Among the probabilistic estimators, SFF fails to perform better and has high uncertainty because it can not handle the temporal dependencies effectively.\nSFF takes less time to train when compared to other estimators due to the low model complexity. As discussed earlier, SFF networks are a simple feed-forward neural network. LSTM, on the other hand, takes longer training time and increases more with the data length as they use a gating mechanism to run RNNs in a sequential form and control the information flow. The number of parameters required is quite high, affecting the computation complexity during training. The transformer with a data length of 20 weeks takes the longest time to train as the self-attention mechanism scales quadratically with the input data length. Regarding prediction time, the test data length is fixed to 24 hours, irrespective of the training data length. Hence, the prediction time of probabilistic estimators is not affected by changes in data length. The values recorded in the table are the average of the prediction time of the dataset with different sizes. The LSTM takes a longer time to predict due to sequential data processing with a negligible impact of data length on prediction time, ranging from 1193ms.\nFor the calculation of Central Processing Unit (CPU) and memory usage during training and prediction, a data length of 20 weeks is used. the x-axis in all figures represents the run time in seconds. During training, the run time includes time to perform data preprocessing, train-test split, train the model, and model storage. On the other hand, during prediction, run time includes time to get test data, get the trained model, perform prediction, and post-processing. Here, 2 CPU cores are used while performing analysis.\nFigure 2 shows the CPU and memory required during the training of LSTM, SFF, DeepAR, and Transformer. Figure 2a shows the line graph of CPU usage during model training with the x-axis being the run time in seconds and the y-axis being the CPU Usage in percentage %. LSTM has a high CPU usage of 200% for a longer duration of around 160 seconds due to complex calculations involved during pre-processing and also during training along with sequential information processing. SFF has CPU usage of 200% for very little time, around 25 seconds, due to its simple model architecture. DeepAR and Transformer have moderate CPU usage due to complex calculations and self-attention mechanisms, respectively. Similarly, for memory usage in figure 2b, The LSTM uses a high amount of memory for a longer duration to maintain multiple state vectors. DeepAR memory usage is 750MiB, which is higher than LSTM but for a shorter time as both use autoregressive recurrent structures. The transformer requires the lowest memory of around 550MiB for around 120 seconds, as there are many parameters along with complex self-attention mechanisms.\nFigure 3 shows the CPU and memory usage of LSTM, SFF, DeepAR, and Transformer during prediction. From Figures 3a and 3b, it can be understood that the transformer requires low CPU and memory usage compared to other estimators with the least time of 10 seconds for CPU and around 7 seconds for memory due to parallel processing of input sequence, unlike LSTM. The performance of SFF and DeepAR are almost similar in both Figures. The LSTM uses the CPU for a longer duration due to the sequential input processing."}, {"title": "C. Real-World Applications", "content": "In the domain of O-RAN architecture enhanced by cloud-native applications, probabilistic forecasting plays a pivotal role in transforming network management. Dynamic spectrum sharing benefits from predictive modeling, allowing for intelligent load balancing and efficient capacity planning, thus ensuring optimal utilization of resources across diverse network segments. Additionally, predictive maintenance emerges as a key application, where probabilistic forecasting anticipates equipment failures, leading to improved reliability and reduced downtime. The integration of probabilistic forecasting also extends to energy efficiency, enabling the prediction of energy consumption patterns based on network conditions and contributing to sustainable and green networking practices. Moreover, in the security domain, advanced forecasting aids in proactively identifying and mitigating potential security threats, boosting the robustness of O-RAN ecosystems against evolving risks. These applications collectively showcase the transformative potential of probabilistic forecasting in O-RAN through cloud-native xApps and rApps, driving efficiency, reliability, sustainability, and security."}, {"title": "V. CONCLUSION", "content": "This paper highlights the challenges and requirements of integrating rApp into the cloud-native O-RAN for efficient resource allocation to meet the dynamic and complicated network resource requirements. Probabilistic forecasting of resource demand using estimators such as DeepAR and Transformer is beneficial for service providers to make more informed and reliable decisions about PRB demands in future time periods. The resource provisioning rApp, which consists of a monitoring system, analytical engine, a decision engine, and an actuator, is containerized to make it suitable for use in O-RAN. This paper then addresses the integration of containerized solutions into the O-RAN architecture for use cases related to real-world applications. The effect of data length on metrics like MSE, training time, and prediction time is addressed for different estimators, including the deterministic LSTM model. LSTM performs the worst due to the sequential processing and storage of input information. Finally, the working of estimators is also compared in terms of CPU and memory usage with a data length of 20 weeks. Although SFF performs the best with the least run time, the DeepAR and Transformer exhibit moderate usage of resources, low MSE, and balanced computational demands due to parallel processing of input information, self-attention mechanism, capability to capture temporal dependencies and resistance to data non-stationarity."}]}