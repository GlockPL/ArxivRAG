{"title": "General Information Metrics for Improving AI Model Training Efficiency", "authors": ["Jianfeng Xu", "Congcong Liu", "Xiaoying Tan", "Xiaojie Zhu", "Anpeng Wu", "Huan Wan", "Weijun Kong", "Chun Li", "Hu Xu", "Kun Kuang", "Fei Wu"], "abstract": "To address the growing size of AI model training data and the lack of a uni-versal data selection methodology-factors that significantly drive up training costs this paper presents the General Information Metrics Evaluation (GIME) method. GIME leverages general information metrics from Objective Information Theory (OIT), including volume, delay, scope, granularity, variety, duration, sam-pling rate, aggregation, coverage, distortion, and mismatch to optimize dataset selection for training purposes. Comprehensive experiments conducted across diverse domains, such as CTR Prediction, Civil Case Prediction, and Weather Forecasting, demonstrate that GIME effectively preserves model performance while substantially reducing both training time and costs. Additionally, applying GIME within the Judicial AI Program led to a remarkable 39.56% reduction in total model training expenses, underscoring its potential to support efficient and sustainable AI development.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence (AI) is transforming numerous aspects of contemporary life, with advancements fueled largely by the training of models on extensive datasets (Pouyanfar et al. 2018; S. Dong et al. 2021; Bialkova 2024). This is particularly evident in areas like autonomous driving (S. Liu et al. 2024; C. Cui et al. 2024), generative AI (Feuerriegel et al. 2024; Huang et al. 2024), and medical image processing (Tian et al. 2024; Alzubaidi et al. 2024), which depend on large-scale model training. As these models expand to encompass hundreds of billions of parameters, the need for high-quality training data becomes critical (Zhao et al. 2023; Minaee et al. 2024). Training such large-scale models often requires tens to hundreds of trillions of tokens, substantial interdisciplinary effort over months, and a vast array of computational resources, including thousands of GPUs and high levels of energy consumption (Achiam et al. 2023; Touvron, Lavril, et al. 2023; Touvron, Martin, et al. 2023; Chowdhery et al. 2023). A core challenge is ensuring that training data is meticulously curated ineffective data selection can yield models that underperform, fall short of desired objectives, and waste considerable resources (Chowdhery et al. 2023; Gunasekar et al. 2023b). Thus, once model architecture and algorithms are defined, the quality of the training data becomes paramount to a model's success, significantly influencing the performance and relevance of AI technologies across various domains (Hamid 2023; Zha et al. 2023).By focusing on data quality, small-scale models can achieve performance comparable to much larger models. For instance, Phi-1.5 achieves performance on par with models 5 times its size, while Phi-2 matches or even surpasses the performance of models 25 times larger (Gunasekar et al. 2023a; Y. Li et al. 2023)."}, {"title": "2 Methodology", "content": null}, {"title": "2.1 Common Methods for AI Model Training Data Selection", "content": "In the pursuit of effective data selection and utilization strategies, a commonly employed baseline is to use the entire dataset for training (full data utilization). Although this approach may capture comprehensive information, it often proves highly inefficient in large-scale, diverse datasets. The computational and resource costs are substantial, and such exhaustive usage does not necessarily guarantee performance improvements (Motamedi et al. 2021; Jakubik et al. 2024). In response, various subset selection techniques have emerged.\nRandom sampling is one of the simplest strategies, selecting subsets without any specific criteria, such as (Okanovic et al. 2023), (Wongvorachan et al. 2023). Despite its ease of implementation, it often fails to represent underlying data structures, leading to unstable model performance and inefficient use of resources (Singh 2023; Jakubik et al. 2024). A more structured approach is stratified sampling, such as (Jiao et al. 2022), (J. Wang et al. 2019).which divides the dataset into non-overlapping strata and selects samples proportionally, thereby preserving key data distributions. However, this method relies on prior knowledge of the underlying data distribution, limiting its applicability in scenarios where such information is unavailable (Singh 2023; Jakubik et al. 2024).\nInformation-theoretic feature selection methods, such as (Wan et al. 2022), (Pawluk et al. 2019), seek to enhance model performance by maximizing information gain and minimizing redundancy, employing measures like entropy and mutual information. Yet, these approaches often require accurate probability distributions a non-trivial demand in complex, real-world contexts-thus restricting their general scalability (R\u00e9nyi 1961; Ash 2012). Data augmentation, by contrast, increases training data diversity and density through transformations of existing samples, such as (Cubuk et al. 2019), (Borovykh et al. 2017). Although successful in certain domains (e.g., computer vision), its efficacy and adaptability are limited across varied data types and tasks (Motamedi et al. 2021; Shorten et al. 2019).\nActive learning, such as modAL(Danka et al. 2018), (Yoo et al. 2019), (Park et al. 2020), adopts an iterative procedure, starting with a small labeled dataset and incrementally selecting the most informative samples for annotation based on model-specific uncertainty criteria. While it can reduce labeling costs and improve model"}, {"title": "2.2 Definition and Implications of the General Information Metrics", "content": "Objective Information Theory (OIT) defines information as follows: Let O denote the set of all content of the objective world, let S denote the set of content of the subjective world, and let T be a set of times. The elements in these sets can be specified according to the requirements in different areas. A noumenon o is thus an element of the power set $2^{OUS}$ (or a subset of OUS), the occurrence duration is $T_h\u2208 2^T$, and f (o, Th) is the set of states of o over Th. The carrier c\u2208 $2^O$, the set of reflection times $T_m \u2208 2^T$, and the set of reflection states g (c, Tm) are all nonempty sets. Information I is thus an enabling mapping from f (o,Th) to g (c,Tm), which is expressed as $I : f (o,Th) \u2192 g (c,Tm)$ or $I (f (o,Th)) = g (c, Tm)$. The collection of all information is referred to as the information space, denoted by I, which is one of the three essential elements of the objective world according to OIT.\nInformation can also be represented as a sextuple, i.e., I = (o, Th, f, c, Tm, g). When I is an injective mapping from f(o, Th) to g(c, Tm), meaning there exists an inverse mapping I-\u00b9 such that $I^{-1}(g(c, T_m)) = f(o, T_h)$, it is called restorable information. For restorable information, Xu et al. (Xu, S. Wang, et al. 2023; Xu, Z. Liu, et al. 2022; Xu 2024) defines 11 general information metrics as shown in Table 1.\nAccording to the perspective of OIT, data is a manifestation of information. Based on the definitions and formulas in Table 1, the 11 general information metrics are independent of data type, scale, and structure, making them applicable to the evaluation of datasets across all domains. Among these metrics, volume is correlated with scope, granularity, variety, duration, and sampling rate. It is not difficult to demonstrate that under certain typical conditions, such as video information, the following relationship holds:\n$volume = k.\\frac{scope \\cdot variety \\cdot duration \\cdot sampling\\_rate}{granularity}$"}, {"title": "2.3 GIME Method for AI Model Training Data Selection", "content": "In this paper, we propose a novel GIME method, which integrates a data selection and evaluation process into a standard AI model training framework (highlighted in gray), as shown in Figure 2. This process is independent of the application domain and model type. The \"training data pool\" consists of all available relevant data, while the \"training dataset\" is a subset selected from the pool after evaluation by GIME to meet specified criteria."}, {"title": "2.4 Theoretical Proof of GIME's Superiority", "content": "The GIME method selects a subset of the full dataset for model training based on dataset metric thresholds, significantly reducing training costs compared to method of full data utilization. Another comparable approach is random sampling. Furthermore, the theoretical advantages of the GIME method can also be clearly demonstrated.\nLemma: Let the cardinality of a finite dataset D be |D|, and metr be a metric defined on D. Assume that the elements of D are uniformly distributed with respect to metr. Define m = min{metr(s) | s C D} and M = max{metr(s) | s C D}. Let R be a randomly selected subset of D with cardinality |R|= k|D, where 0 < k < 1. When metr is an additive, maximum, minimum, or mean-type metric, the mathematical expectation of metr(R) is given by $kM, \\frac{m + k|D|M - m}{k|D|+1},\\frac{M \\frac{1-k|D|}{M-m}}{1-k|D|}$, and $\\frac{M+m}{2}$.\nFrom this, we can derive the following important conclusion.\nTheorem: Let metr be an additive, maximum, minimum, or mean-type metric defined on a finite dataset D, and let the elements of D be uniformly distributed with respect to metr. Define m = min{metr(s) | s C D} and M = max{metr(s) | s C D}. Assume that s is a subset of D used for model training, and the model performance p(s) is positively correlated with the metric metr(s). Let S be a subset of D with |S| = k|D|, where 0 < k < 1, and metr(S) = M. Then, for any randomly selected subset R of D with |R| = |S|, the model trained on S will have statistically better performance than the model trained on R."}, {"title": "3 Test and Application Results", "content": null}, {"title": "3.1 Three Test Scenarios and models", "content": "To evaluate the effectiveness of general information metrics in improving the efficiency of AI model training, we conducted experimental validations across three entirely different domains and corresponding models. Specifically, we select CTR Prediction (represents Human Behavior tasks), Civil Case Prediction (represents Natural Language Understanding tasks), and Weather Forecasting (represents Time-Series tasks) to evaluate GIME's ability to reduce data size, training time, and energy consumption in large-scale complex tasks.\nIn CTR Prediction, we study the impact of online advertising recommendations on user Click-Through Rates and estimate CTR via deep learning models (Y. Yang et al. 2022). We utilized the Avazu dataset, a large-scale open-source dataset containing approximately 40 million mobile ad click records, which is widely used in both academic and industry (Song et al. 2020). This dataset includes fields such as ID, click labels, timestamps, anonymous categorical variables, and various attributes related to websites, applications, and devices. Second, in Civil Case Prediction (J. Cui et al."}, {"title": "3.2 General Information Metrics on Three Scenario Tasks", "content": "Although OIT provides the mathematical definitions for 11 general information metrics (Xu, Z. Liu, et al. 2022; Xu, S. Wang, et al. 2023; Xu 2024) (Table 1), these definitions are expressed using abstract mathematical concepts to ensure the broadest adaptability. In practical applications, it is necessary to integrate domain knowledge and the specific characteristics of actual data to clarify the calculation methods for each type of metric, facilitating the evaluation and selection of training data. By mapping the meanings of each scenario's dataset to the 11 metric calculation formulas defined by OIT, we derived the methods for calculating various metrics for any dataset in the three experimental scenarios, as presented in Table 2."}, {"title": "3.3 Exploring the relationship between data metrics and model performance", "content": "As illustrated in Figure 4, we utilized DNN (Covington et al. 2016), ERNIE (Z. Zhang et al. 2019), and LSTM (Hochreiter 1997) models to evaluate performance in each of these tasks, providing insights into how different training data metrics influence the effectiveness of these predictive models. Since the metric volume is correlated with scope, granularity, variety, duration, and sampling rate, its variation in experiments is achieved by adjusting the aforementioned related metrics. As the other 10 metrics are mutually independent, experiments on the impact of their individual variations on model performance are conducted by fixing the remaining metrics, ensuring objective results that are mutually independent.\nIn CTR Prediction, as shown in Figure 4(I), we observed a strong correlation between model performance (AUC) and seven key metrics, including volume, delay, scope, granularity, variety, sampling rate, and mismatch. Monotonic changes in these metrics consistently impacted the model's performance. In contrast, an increase in duration leads to an initial rise in AUC followed by a decline. This is likely because some earlier behaviors no longer match the characteristics of current behaviors, and using such data for training reduces the model's accuracy in predicting current behaviors. On the other hand, the increase in aggregation causes only random fluctuations in AUC, as this metric is not correlated with model performance. We did not evaluate the distortion and coverage metrics, although these metrics are known to affect model performance. However, due to the lack of error information in the Avazu dataset, it was challenging to calculate the distortion metric. Additionally, we did not conduct"}, {"title": "3.4 Using GIME for Improving Model Training Efficiency", "content": "The predominant approach for training AI models relies on using the entire dataset (Gebru et al. 2021; Motamedi et al. 2021). The GIME method, however, seeks to systematically enhance training efficiency by integrating theoretical principles and practical guidelines. At its core, GIME leverages the 11 general information metrics defined by OIT, calculating these metrics for the training dataset in alignment with domain-specific knowledge. Metrics are then categorized by sensitivity level-high, moderate, or low. For high-sensitivity metrics, optimization is prioritized; moderate-sensitivity metrics are managed by setting reasonable thresholds, while low-sensitivity metrics are excluded. Training proceeds only once all sensitive metrics fall within acceptable thresholds, ensuring robust model performance while significantly reducing data size and training time compared to traditional full-dataset approaches."}, {"title": "3.5 Comparing Active Learning Method for AI Model Training Data Selection", "content": "The modAL algorithm is a popular active learning framework, designed with modularity, flexibility, and extensibility. Built on top of scikit-learn, it enables researchers to quickly create active learning workflows with a high degree of freedom (Danka et al. 2018). In this paper, we choose modAL as a baseline model to compare its performance with our GIME. Additionally, researchers can easily replace components with custom-built solutions, facilitating the development of novel algorithms. The modAL method starts by collecting a large dataset. The next key step is selecting an appropriate model. After the first round of selected data, also known as labeled data, supports model training, the accuracy is assessed. If the model's accuracy does not meet the required threshold, new data is selected based on uncertainty criteria, added to the training labels, and the process is repeated until the model's accuracy satisfies the set requirements.\nAs shown in Figure 6(a), in the CTR Prediction task, GIME demonstrated a clear advantage over modAL in terms of accuracy, stability, and training efficiency. GIME achieved an average AUC of 0.7496 with a remarkably low standard deviation of 0.0004, indicating both high accuracy and exceptional consistency across trials. In contrast, modAL's performance was less consistent, with its AUC values fluctuating within the range of 0.7433-0.7467 over 10 trials and an average AUC of 0.7444. Although modAL reached its highest AUC of 0.7467 in trial 7, it was still outperformed by GIME, and the larger variability in modAL's results suggests greater sensitivity to initial conditions. In the Civil Case Prediction experiment, we initially selected a random sample of 10,000 cases to train the ERNIE model, using modAL's entropy-based selection to iteratively add data in 10,000-sample batches until the full dataset was utilized (see Figure 6(b)). From the results, GIME demonstrated clear superiority in both accuracy and stability compared to modAL. The average accuracy achieved by GIME across 10 trials was 81.26%, with a relatively narrow range (80.90%-81.59%), indicating consistent and reliable performance. In contrast, modAL achieved an average accuracy of 80.26%, with a wider range (78.82%-81.15%), highlighting greater variability and less stability in its results. While modAL reached a peak accuracy of 81.15, close to GIME's average performance, the modAL method requires at least 4 steps, totaling 200 minutes-80 minutes more than the GIME approach. In comparison, the GIME method reduces the time cost by 40% while maintaining the same performance. For Weather Forecasting, modAL employed uncertainty sampling, selecting 3,000 samples per iteration to train the Autoformer model (see Figure 6(c)). The best performance achieved by modAL, as measured by MRMSE, was 0.1295, while the error upper bound of GIME was 0.1246, demonstrating that modAL consistently"}, {"title": "3.6 Application of the GIME Method in Judicial AI Program", "content": "Since 2017, the China Judicial Big Data Research Institute has led the development and ongoing refinement of six critical AI models tailored to various judicial applications. This initiative, carried out by collaborative teams of technical experts and legal professionals, integrates the GIME method with domain-specific insights and advanced AI technologies. The six AI models-designed for civil case analysis, case feature recognition, event extraction, judgment reasoning, relevant legal provision recommendation, and judgment generation-have been successfully deployed across multiple judicial AI systems. Each model was trained on distinct datasets crafted for its specific function, including case descriptions, judicial documents, multi-type case files, fact-finding data, case-law correlation data, and integrated datasets that combine case facts, legal provisions, and judicial outcomes (see Figure 7). Based on comprehensive analysis, we estimate that the GIME method, compared to the previously standard full-data selection approach, has reduced training data size by approximately 56.26 million records. This reduction has translated into significant efficiency gains, including a decrease of 8,518 training hours, a savings of 23,503 human hours, and a reduction in energy consumption by 2.26 million watt-hours. Additionally, adopting the GIME method has contributed to an estimated cost savings of $1.27 million in research and development."}, {"title": "4 Discussions", "content": "In selecting training data for AI models, attributes such as completeness, heterogeneity, variety, accuracy, timeliness, and balance are commonly considered. However, the exact definitions, mathematical formulations, and applicability of these attributes remain subjects of debate. There is no consensus on whether these characteristics are sufficient, which specific contexts they best serve, or whether a universal method could guide data selection across industries to enhance AI training efficiency. As AI adoption continues to grow, addressing inefficiencies in training time and energy consumption has become increasingly urgent.\nData serves as a fundamental representation of information, and effectively addressing these challenges requires establishing a comprehensive information measurement framework for AI model training. Shannon's information theory introduced entropy as a measure, which has been applied in data selection. However, entropy alone-or even"}, {"title": "5 Conclusion", "content": "This study presents a systematic approach to optimizing AI training data selection through the 11 general information metrics defined by OIT and the GIME method. These metrics provide a quantifiable framework for assessing data quality, while GIME offers a practical implementation for diverse AI applications. Experimental results across three representative tasks such as Human Behavior tasks, Natural Language Understanding tasks, and Time-Series tasks consistently demonstrated GIME's ability to reduce data size, training time, and energy consumption in large-scale complex tasks while maintaining near-optimal performance. The successful deployment of GIME in engineering applications, particularly judicial AI systems, highlights its real-world impact, including significant reductions in costs, resource usage, and development timelines.\nOIT offers a new perspective to define the concept of information, forming the most comprehensive and systematic universal information metrics. This paper is the first to fully use these metrics to support the evaluation of AI training data, showing that the GIME method can significantly improve training efficiency across different fields and models. Therefore, the significance of this work extends beyond AI technology, inspiring academic peers to further understand and examine the intrinsic nature and measurement methods of information. It also lays the groundwork for deeper exploration and characterization of the dynamic mechanisms of information systems, providing new theories, methods, and pathways for achieving more groundbreaking advancements in information technology, exemplified by AI.\nDespite its demonstrated benefits, GIME's reliance on domain-specific knowledge for defining threshold values may limit its applicability in domains where such expertise is unavailable. Additionally, the method has primarily been evaluated in static training environments, leaving room for further exploration in dynamic or adaptive AI systems. Future research should focus on automating the threshold-setting process, potentially through machine learning techniques, to enhance GIME's generalizability. Expanding the application of GIME to more complex and heterogeneous datasets, as well as integrating it with real-time learning systems, could further extend its utility. By addressing these limitations, GIME has the potential to evolve into a cornerstone methodology for sustainable and efficient AI development on a global scale."}]}