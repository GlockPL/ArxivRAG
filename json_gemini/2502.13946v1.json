{"title": "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region", "authors": ["Chak Tou Leong", "Qingyu Yin", "Jian Wang", "Wenjie Li"], "abstract": "The safety alignment of large language models (LLMs) remains vulnerable, as their initial behavior can be easily jailbroken by even relatively simple attacks. Since infilling a fixed template between the input instruction and initial model output is a common practice for existing LLMs, we hypothesize that this template is a key factor behind their vulnerabilities: LLMs' safety-related decision-making overly relies on the aggregated information from the template region, which largely influences these models' safety behavior. We refer to this issue as template-anchored safety alignment. In this paper, we conduct extensive experiments and verify that template-anchored safety alignment is widespread across various aligned LLMs. Our mechanistic analyses demonstrate how it leads to models' susceptibility when encountering inference-time jailbreak attacks. Furthermore, we show that detaching safety mechanisms from the template region is promising in mitigating vulnerabilities to jailbreak attacks. We encourage future research to develop more robust safety alignment techniques that reduce reliance on the template region.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are trained using safety alignment techniques and guided by ethical principles to ensure their interactions with users remain safe and helpful (Bai et al., 2022a; Dai et al., 2024; Ji et al., 2023; Bai et al., 2022b). These alignment methods enable LLMs to identify and decline potentially harmful or unethical queries. Recent studies (Zhang and Wu, 2024; Lin et al., 2024; Li and Kim, 2024) have revealed that safety alignment in LLMs is often superficial, where the alignment adapts a model's generative distribution primarily over its beginning output tokens (Qi et al., 2024a). This excessive focus on specific regions introduces vulnerabilities: adversarially optimized inputs (Zou et al., 2023b; Chao et al., 2023; Liao and Sun, 2024) or carefully crafted jailbreak prompts (Wei et al., 2023; Shen et al., 2024b) targeting a model's initial behavior can easily bypass safety mechanisms, undermining the model's ability to maintain safety. However, the root causes of these vulnerabilities remain unclear, making it difficult to develop effective alignment strategies to address them.\nExisting aligned LLMs commonly incorporate a specific template inserted between the user's input instruction and the model's initial output (Touvron et al., 2023; Jiang et al., 2023; Team et al., 2024), encoding essential role information in structuring interactions with users. As illustrated in Figure 1, the template for a safety-tuned LLM remains fixed, regardless of the input instruction. Positioned immediately before the model's initial output, this template region aggregates information from the input and facilitates the critical transition from understanding instructions to generating responses. Due to its pivotal position, the template region serves as a potential anchor point for safety-related decision-making. We hypothesize that LLMs' safety mechanisms may inadvertently take shortcuts to the tokens in the template region, relying too heavily on its aggregated information to assess the harmfulness of the input. We refer to this issue as Template-Anchored Safety Alignment (TASA), which leads to safety-related vulnerabilities. Specifically, jailbreak attacks that simply manipulate the model's interpretation of the input via instructions can exploit this reliance to bypass safeguards and generate harmful responses. To thoroughly analyze TASA and its implications, our work is divided into the following three phases.\nFirst, we conduct comprehensive experiments to verify that TASA is widespread across various safety-tuned LLMs (Section 3). Our findings reveal that these models tend to shift their attention from the instruction region to the template region when processing harmful requests. Further analysis confirms that this shift is systematic rather than coincidental: models consistently rely more on the information from the template region when making safety-related decisions. Specifically, we observe that interventions in intermediate states derived from the template region, compared to the instruction region, significantly increase the likelihood of initial compliance decisions.\nSecond, we establish a strong connection between TASA and inference-time vulnerabilities (Section 4). To investigate this, we perform interventions exclusively in the template region during the model's response generation to harmful inputs. Notably, these interventions prove highly effective at inducing LLMs to comply with harmful requests, even without altering instructions. Furthermore, by probing harmfulness features across layers and positions within the template region, we observe that common inference-time attacks cause significant interferences in these positions. This finding explains how such attacks exploit TASA to compromise model safety.\nThird, we demonstrate that safety mechanisms anchored in the template region can be detached during response generation, enhancing the robustness of a model's safety (Section 5). This approach stems from our observation that harmfulness probes trained on template positions in specific layers can be directly transferred to identify harmful outputs during response generation. By leveraging these probes, we can detect harmful content in inference and steer activations to mitigate interference from attacks. Our experiments validate that this method is both simple and effective, showing a significant reduction in attack success rates.\nIn summary, this work investigates template-anchored safety alignment (TASA), a pervasive yet under-explored phenomenon in LLMs. We uncover its connection to inference-time vulnerabilities and propose initial strategies to alleviate this issue. Our findings highlight the importance of future safety alignment in developing more robust techniques that reduce models' reliance on potential shortcuts."}, {"title": "2 Background", "content": "Generation Process of LLMs. Following prior works (Elhage et al., 2021; Geva et al., 2023), we demonstrate how a Transformer (Vaswani et al., 2017) decoder-based LLM computes new tokens autoregressively. Given a prompt with tokens t\u2081,..., tr, tokens are first embedded into vectors X1,...,x. Each vector at position i forms an initial residual stream \u00e6. Through each layer l \u2208 [1, L], the residual stream is updated according to x = x + a + m, where a and mi represent the attention and MLP outputs, respectively. For simplicity, we omit the layer normalization and position embedding calculations.\nEach attention head h employs four projection matrices: Wh, wh, whe Rdx and Wl,h Rxd. The attention map A \u2208 RT\u00d7T for each head is computed as:\n$A^{l,h} = softmax(\\frac{(x^{l-1}W^{l,h}_Q)(x^{l-1}W^{l,h}_K)^T}{\\sqrt{d/H}}+ M)$, where M is a lower triangular matrix for causal masking. The final outputs from the attention module is competed as a = $\\sum_h \\sigma(A^{l,h}x^{l-1}W^{l,h}_V) W^{l,h}_O$. The MLP then independently applies non-linear transformations on each token's representation.\nFinally, the model unembeds the final position's representation into logits, applies softmax to obtain next-token probabilities, and samples tokens autoregressively until the generation is complete.\nActivation Patching. Consider a metric m\u2208R evaluated via a computation graph (e.g., an LLM), r \u2208 Rd represent a node (e.g., an intermediate activation) in this graph. Following prior work (Vig et al., 2020; Finlayson et al., 2021; Marks et al., 2024), we assess the importance of r for a pair of inputs (Xclean, xpatch) by measuring its indirect effect (IE) (Pearl, 2001)) with respect to m:"}, {"title": "3 The Template-Anchored Safety Alignment in Aligned LLMS", "content": "3.1 Preliminaries\nDatasets. We construct two datasets, Danlz and Deval, designed to analyze the behavioral differences of LLMs when handling harmless versus harmful inputs and to evaluate their refusal capabilities, respectively. Each dataset consists of paired harmful and harmless instructions. For Danlz, harmful instructions are sourced from JailbreakBench (Chao et al., 2024), while for Danlz, they are drawn from HarmBench's standard behavior test set (Mazeika et al., 2024). The harmless counterparts in both datasets are sampled from Alpaca-Cleaned, a filtered version of Alpaca (Taori et al., 2023) that excludes refusal-triggering content. To ensure a precise comparative analysis, each harmless instruction matches its harmful counterpart in token length. Since tokenization methods vary across models, we maintained separate versions of Danlz and Deval for each model.\nModels. To validate the generality of our findings, we study a diverse set of safety fine-tuned models: Gemma-2 (2b-it, 9b-it) (Team et al., 2024), Llama-2-7b-Chat (Touvron et al., 2023), Llama-3 (3.2-3b-Instruct, 8B-Instruct) (Dubey et al., 2024), and Mistral-7B-Instruct (Jiang et al., 2023).\n3.2 Attention Shifts to The Template Region\nIn modern LLMs based on attention mechanisms, the distribution of attention weights across different heads reflects which regions of information collectively influence the model's next token predictions (Bibal et al., 2022). A notable observation is that when the model refuses harmful requests, its response often exhibits distinct patterns from the outset, for instance, initiating with the token 'Sorry' as the first output (Zou et al., 2023b; Qi et al., 2024a). This suggests that if the model's safety function primarily depends on the template region, then when processing harmful inputs, the attention weights at the final input position should focus more on the template region, while exhibiting comparatively less focus on the instruction region.\nMethod. To investigate whether the attention weights exhibit increased focus on the template region when processing harmful inputs, we analyze attention weight distributions across all heads for both the instruction and template regions. More importantly, we examine how these distributions differ between harmless and harmful inputs.\nFormally, for h-th attention head in layer l, we compute the average attention weight accumulation over regions of interest. Let Ah AT denote the attention weight at the final position T of the input that attends to the position i in j-example, we define the regional attention accumulation for harmless (+) and harmful (-) inputs as:\n$\\alpha_R^{l,h} = \\frac{1}{|D_{anlz}|}\\sum_{j=1}^{|D_{anlz}|} \\sum_{i \\in I_R} A^{l,h}_{T,i}$ \nwhere R \u2208 {inst, temp} indicates the region, with Zinst = {1,...,S} and Ztemp = {S +1,...,T} being the position indices for the instruction and template region, respectively.\nWhen processing harmful inputs compared to harmless ones, the attention shift is computed as:\n$\\delta_R(l,h) = \\alpha_R^-(l, h) - \\alpha_R^+(l, h)$ \nwhere a positive dr(l, h) indicates that region R receives more attention from the given head when processing harmful inputs relative to harmless ones, whereas a negative value suggests the opposite.\nResults. Figure 3 shows the distribution histograms of OR from all heads across the compared LLMs. We observe that the template distributions exhibit longer and more pronounced tails on the positive side compared to the negative side, while the instruction distributions show the opposite trend. This consistent phenomenon observed across various safety-tuned LLMs suggests that these models tend to focus more on the template region when processing harmful inputs, providing strong evidence for the existence of TASA.\nTo illustrate this phenomenon more concretely, we showcase the behavior of a specific attention head (17th-layer, 21st-head) from Llama-3-8B-Instruct on the right side of Figure 3. This example demonstrates how an individual head behaves differently when processing harmless versus harmful inputs. We observe that the attention weights at the final input position (i.e., '\n\n') show a clear focus shift from a concrete noun 'tea' in the instruction to a role-indicating token 'assistant' in the template region when the input is harmful.\n3.3 Causal Role of The Template Region\nWhile safety-tuned LLMs shift their attention toward the template region when processing harmful inputs, does this shift indicate a reliance on template information for safety-related decisions? To confirm this, we verify whether intermediate states from the template region exert a greater influence on models' safety capabilities than those from the instruction region.\nEvaluation Metric. Quantifying the influence of intermediate states typically involves causal effects, such as IE (see Section 2). However, evaluating an LLM's safety capability by analyzing complete responses for each of its numerous internal states would be highly inefficient. To address this, we adopt a lightweight surrogate metric following prior work (Lee et al., 2024a; Arditi et al., 2024). This approach uses a linear probe on the last hidden states to estimate a model's likelihood of complying with harmful inputs. The predicted logits for harmful inputs serve as an efficient proxy to measure the causal effects of intermediate states on safety capability, where higher logits for harmful inputs indicate weaker safety capability. Following difference-in-mean method (Arditi et al., 2024; Marks and Tegmark, 2024), we obtain the probe d+ \u2208 Rd as follows:\nd$^+ = \\frac{1}{|D_{anlz}|} \\sum_{j=1}^{|D_{anlz}|} x_j^+ - \\frac{1}{|D_{anlz}|} \\sum_{j=1}^{|D_{anlz}|} x_j^-$,\nwhere \u00e6, is the residual stream from example j of either harmless (+) or harmful (-). We then compute m(x) = x+d+ and refer to it as the compliance metric.\nA 5-fold cross-validation of the probe achieves an average accuracy of 98.7\u00b10.7% across models, demonstrating its effectiveness in distinguishing between safe and unsafe model behaviors.\nMethod. Consider a scenario where we input the last token in the template and aim to obtain whether the model intends to comply the input, as measured by the compliance probe. In this forward pass, the residual stream of the last token aggregates context information by fusing the previous value states :=xTWlh in every attention head. To compute the causal effects of intermediate states from different regions, we calculate the IE when patching the value states of harmful input with those of harmless input for one region, while leaving the states unchanged for the other region. Specifically, we compute the IE as:\n$IE (m; D_{anlz}) = E_{(x^+,x^-) \\sim D_{anlz}}[m (x^-\\mid do (v_R = v_R^+)) -m(x^+)]$,\nwhere R' \u2208 {inst, temp', all} indicates a specific region, with Zinst = {1, ..., K}, Ztemp' = {K + 1, . . ., T \u2013 1} and Zall = {1, . . ., T \u2013 1}. Notably, we exclude the last position T from patching to avoid direct impact on the compliance probe.\nGiven that different heads have varying influences on safety capability, we first patch two regions together to quantify the importance of each head by IE (m; Danlz). Then we cumulatively patch the value states of heads for each region, starting from the most important head to the least, to obtain IE (m; Danlz). Here, H = {(l1, h\u2081),... } represents the head indexes sorted by their importance scores. A higher IE indicates the information from region R' has a greater causal effect on the model's compliance decision, and vice versa. For a fair cross-model comparison, we use the normalized indirect effect (NIE) by dividing the IE of each pair by (m(x\u00af) \u2013 m(x+)).\nResults. Figure 4 shows the trend of NIE in different regions as the number of patched heads increases. We have these key observations: (1) When patching the template region, a substantial increase in NIE is achieved by patching only a small number of heads that are critical to safety capabilities. In contrast, patching the instruction region does not bring significant improvement. This indicates that"}, {"title": "4 How Does TASA Cause Inference-time Vulnerabilities of LLMs", "content": "While TASA has been broadly observed across various safety-tuned LLMs, its role in causing vulnerabilities, particularly in the context of jailbreak attacks, remains unclear. To investigate this, we address two key questions: First, to what extent does TASA influence the model's initial output and affect its overall safety? Second, how is TASA connected to jailbreak attacks during generation?\n4.1 TASA's Impact on Response Generation\nTo investigate the impact of TASA on the model's safety capability, we intervene in the information from template positions during response generation for harmful requests, and evaluate whether the model can still produce refusal responses.\nMethod. During the forward process of each token in the response, we replace the value states of a specific proportion of attention heads at template positions with the corresponding value states from processing the harmful input (See Appendix A.1). We refer to this operation as TEMPPATCH and evaluate its performance on the Harmbench test set. For comparison, we also evaluate three representative jailbreak attack methods: (1) AIM (Wei et al., 2023), a carefully crafted attack prompt; (2) PAIR (Chao et al., 2023), which iteratively optimizes attack instructions using an attacker LLM; and (3) AmpleGCG (Liao and Sun, 2024), an efficient approach for generating adversarial suffixes (Zou et al., 2023b) (See Appendix A.2). To assess compliance, we employ a compliance detector (Xie et al., 2024) to identify whether the model complies with the provided inputs. The effectiveness of each method is measured by the attack success rate (ASR), defined as the proportion of inputs for which the model complies.\nResults. As shown in Figure 5, TEMPPATCH significantly increases the ASRs of LLMs, achieving results that are comparable to or even surpass those of other specialized jailbreak attack methods. These findings further validate the deep connection between TASA and the safety mechanisms of LLMs. Moreover, while other attack methods demonstrate limited effectiveness against certain models, particularly the Llama-3 8B and 3B variants, TEMPPATCH achieves notably higher ASR in comparison. This contrast suggests that what might seem like stronger safety alignment could actually depend more on shortcut-based safety mechanisms, which may potentially introduce unseen vulnerabilities when faced with scenarios outside the training distribution.\n4.2 Probing Attack Effects on Template\nTo understand how jailbreak attacks affect information processing in the template region, we probe how harmfulness features are represented in the intermediate states under different attack scenarios.\nMethod. We feed both harmful and harmless inputs from Danlz into Llama-3-8B-Instruct and collect residual streams at the template region across all layers. At each intermediate location, we construct a probe d\u00ae := \u2212d\u207a, using the method described in Equation (4), but applied in the reverse"}, {"title": "5 Detaching Safety Mechanism from The Template Region", "content": "Since an anchored safety mechanism likely causes vulnerabilities, it is worth exploring whether a detached safety mechanism during generation could, conversely, improve the model's overall safety robustness. This would involve detaching its safety functions from two aspects: (i) the process of identifying harmful content and (ii) the way this processed information is utilized during generation.\nTransferability of Probes. Regarding the first aspect, we inspect whether the harmfulness processing functions in the template region can transfer effectively to response generation. To investigate this, we collect harmful responses from successful jailbreaking attempts and harmless responses using instructions in Danlz. We then evaluate whether the harmfulness probes derived from the template region in Section 4.2 can still distinguish if a response is harmful. Specifically, we collect the residual streams from all layers at the first 50 positions of each response and measure the probes' accuracy in classifying harmfulness.\nAs shown in Figure 7 (see others in Appendix D), our analysis of Llama-3-8B-Instruct reveals that harmfulness probes from the middle layers achieve relatively high accuracy and remain consistent across response positions. This result suggests that harmfulness probes from specific layers in the template region can be effectively transferred to identify harmful content in generated responses.\nDetaching Safety Mechanism. To address the harmfulness-to-generation aspect, we need to examine how harmfulness features evolve during the generation process. The right-most plot in Figure 6 highlights distinct patterns between successful and failed attacks when generating the first response token. In failed attacks, the harmfulness feature quickly peaks and sustains that level throughout the generation process, whereas in successful attacks, it decreases and remains at a low level. This observation suggests that additional harmfulness features should be injected during generation to counteract their decline in effective attacks.\nBased on this finding, we propose a simple straightforward method to detach the safety mechanism: use the probe to monitor whether the model is generating harmful content during response generation and, if detected, inject harmfulness features to trigger refusal behavior. Formally, for a harmful probe de obtained from position 7 and layer l, the representation at position i during generation is steered as follows:\nx \u2190 \\{\n    x + \\alpha d^p & \\text{if } x d^p >> \\lambda \\\\\n    x & \\text{otherwise}\n\\}\nwhere a is a factor controlling the strength of injection and A is a decision threshold (See Appendix A.3 for further details).\nWe evaluate this approach against AIM, AmpleGCG, and PAIR attacks. We compare ASRs for response generations with and without detaching the safety mechanism, as shown in Table 1. The results demonstrate that detaching the safety mechanism from the template and applying it directly to response generation effectively reduces ASRs, strengthening the model's safety robustness."}, {"title": "6 Related Works", "content": "Safety Vulnerabilities of Aligned LLMs. Although significant research has focused on aligning LLMs to develop safety mechanisms enabling them to reject harmful requests (Bai et al., 2022a; Dai et al., 2024; Ji et al., 2023; Bai et al., 2022b), recent studies show these safety mechanisms remain vulnerable (Wei et al., 2023; Qi et al., 2024b; Wei et al., 2024). These vulnerabilities enable attacks on aligned LLMs during inference through jailbreak prompts, which are typically crafted through manual design (Wei et al., 2023), iterative refinement with LLM feedback (Chao et al., 2023; Mehrotra et al., 2024), and optimization via gradient or heuristic methods (Zou et al., 2023b; Liu et al., 2024b; Liao and Sun, 2024) Such attacks exploit two key characteristics of aligned LLMs - the competition between helpfulness and harmlessness objectives (Wei et al., 2023; Ortu et al., 2024; Anil et al., 2024), and superficial alignment (Zhang and Wu, 2024; Lin et al., 2024; Li and Kim, 2024; Qi et al., 2024a). Compared to previous studies, our work identifies an underexplored characteristic of aligned LLMs: their over-reliance on the template region for safety-related decisions. This dependency introduces a new attack surface, exposing the limitations of current alignment strategies.\nMechanistic Interpretability for LLM Safety. Mechanistic Interpretability (MI) aims to reverse-engineer specific model functions or behaviors to make their internal workings human-interpretable. This research examines various components like individual neurons (Gurnee et al., 2023; Stolfo et al., 2024), representations (Marks and Tegmark, 2024; Gurnee and Tegmark, 2024), and larger functional units such as MLPs (Geva et al., 2021, 2022) and attention heads (McDougall et al., 2023; Gould et al., 2024). Building on this foundation, recent research has leveraged MI to understand and enhance LLM safety (Bereska and Gavves, 2024). One line of research analyzes safety behaviors at the representation level and explores ways to manipulate safety-related representations (Leong et al., 2023; Zou et al., 2023a; Arditi et al., 2024; Cao et al., 2024; Lee et al., 2024b; Li et al., 2024b; Shen et al., 2024a). Another investigates components directly connected to safety, such as neurons (Chen et al., 2024), attention heads (Zhu et al., 2024; Zhou et al., 2024), or MLPs (Lee et al., 2024a; Luo et al., 2024). Some researchers examine specific aspects like safety-related parameters (Wei et al., 2024; Yi et al., 2024) or the risks to safety mechanisms during fine-tuning (Li et al., 2024a; Leong et al., 2024). Decomposing representations into interpretable sparse features enables automated expla-"}, {"title": "7 Conclusion", "content": "This work investigates template-anchored safety alignment (TASA), a widespread yet understudied phenomenon in aligned LLMs. We reveal how it relates to vulnerabilities during inference and suggest preliminary approaches to address this problem. Our work emphasizes the need to develop more robust safety alignment techniques that reduce the risk of learning potential shortcuts.\nLimitations\nLimited Generalization. While we have conducted systematic analysis on multiple mainstream models to demonstrate the widespread existence of the TASA issue, we acknowledge that this does not mean that all safety-aligned LLMs necessarily have significant TASA vulnerabilities. Our primary contribution lies in empirically demonstrating the existence of such vulnerabilities in real-world systems, rather than asserting their universality. Some aligned LLMs may actively or passively mitigate this issue through the following mechanisms: 1) Training data accidentally included defense patterns for relevant adversarial samples (Lyu et al., 2024; Zhang et al., 2024; Qi et al., 2024a); 2) Feature suppression methods used in the safety alignment process happened to affect the activation conditions of the TASA trigger mechanism (Zou et al., 2024; Rosati et al., 2024); 3) The model scale has not reached the critical threshold for vulnerability to emerge.\nLimited Solution. As a direct response to the TASA issue analysis, in Section 5 we attempt to detach the safety mechanism from the template region using activation steering (Leong et al., 2023; Zou et al., 2023a; Arditi et al., 2024). Since we haven't updated the model itself, we acknowledge that this method doesn't eliminate the learned safety shortcuts. We view this approach as a proof-of-concept for detachable safety mechanisms rather than a comprehensive solution. Building on our findings, robust mitigation may require systematic integration of adversarial defense patterns during training (Lyu et al., 2024; Zhang et al., 2024; Qi et al., 2024a), or proactive suppression of shortcut-prone"}, {"title": "Ethic Statements", "content": "This work reveals a new vulnerability in aligned LLMs, namely that LLMs' alignment may learn shortcut-based safety mechanisms, causing them to rely on information from template regions to make safety decisions. Although exposing new vulnerabilities could potentially be exploited by malicious actors, given that direct interference with information processing at template region can only be performed on white-box models, we believe the benefits of new insights into current safety alignment deficiencies far outweigh the risks. We hope these new findings will promote the development of more robust safety alignment methods."}, {"title": "A Implementation Details", "content": "A.1 TEMPPATCH\nTo investigate the impact of TASA on the model's safety capability, we intervene in the information from template positions during response generation for harmful requests. To achieve this, during the forward process of each token in the response, we replace the value states of a specific proportion of attention heads at template positions with the corresponding value states from processing the harmful input.\nSpecifically, when generating the i token in the response, the input value states of a selected attention head l, h are patched by do(v[S+1:T] l,h,+ =[S+1:T]). This operation alters the cached value states that the head receives by replacing the values at template positions with the ones when input harmless input, while leaving other positions unchanged. Therefore, only the information from the template region is intervened, while the information from other regions stays as is.\nWe reuse the importance-sorted head indexes H in Section 3.3 to determine the proportion of heads to be patched. When we patch 10% heads, that means we apply TempPatch on the first 10% heads in H. We sweep the proportion of patched heads across 10%, 20%..., 90%, and the results are shown in Figure 8. For each model, we use the proportion which gives the highest ASR on Danlz to conduct TempPatch on Deval in Section 4.1."}, {"title": "A.2 Jailbreak Attacks", "content": "We adopt three representative jailbreak methods for comparison and analysis, namely AIM (Wei et al., 2023), PAIR (Chao et al., 2023) and AmpleGCG (Liao and Sun, 2024). Since AIM is a manually designed jailbreak prompt, we directly fill the target harmful request into the prompt for attacking.\nThe AIM prompt is shown in Figure 9. PAIR uses LLMs to propose and refine jailbreak prompts. To implement this, we use Mixtral-8x22b-instruct as the attacker and gpt-40-mini as the judge model, with N = 20 streams and a maximum depth of K = 3 for each query. AmpleGCG fine-tunes LLMs to generate jailbreak suffixes given harmful queries. We use the recommended checkpoint\u00b3 and settings to obtain suffixes with diverse beam search of 200 beams and a maximum 20 tokens.\nFor response generation during attack scenarios (including TempPatch), we use greedy decoding with a maximum 512 tokens."}, {"title": "A.3 Detaching Safety Mechanism", "content": "We propose to detach the anchored safety mechanism by transferring a harmfulness probe obtained from the template region and re-eliciting it during response generation. This process requires the probe from a specific layer l and template position T. For each model, we evaluate the accuracy in classifying harmful responses of the probes from all layers and template positions, and use the probe which yields the highest accuracy. Specifically, we evaluate probes on the residual streams from the initial 50 tokens of both harmful and harmless responses to instructions in Danlz. The harmful responses are sourced from successful jailbreaks using PAIR or AmpleGCG. Harmless responses are sourced from responses to harmless instructions.\nFor Meta-Llama-3-8B-Instruct, we use the probe from layer l = 13, position \u03c4 = 4 (where 0 is the first position of the template). For gemma-2-9b-it, we use the probe from l = 23, position \u03c4 = 4. When performing the steering as in Equation (6), we empirically use a strength of a = 1 and a = 0.7 for these two models, respectively. To determine the decision threshold \u5165, we calculate the average probe activations for both harmful and harmless responses respectively, then take the median value between the two as the threshold."}, {"title": "B Critical Intermediate States within Template for Safety Decision-making", "content": "Identifying critical intermediate states for safety decision-making helps understand how safety-related features flow within the template region. Therefore, we apply activation patching on the residual streams at template positions to trace the critical internal locations. Specifically, for every layer l and template position t we patch the residual stream \u00e6 from harmful input x to the same location of harmless input x+, and calculate the indirect causal effect on safety as\n$IE (m; D_{anlz}) = E_{(x^+,x^-) \\sim D_{anlz}}[m (x^-\\mid do (x^{l,+}_t = x^{l,-}_t)) -m(x^+)]$,\nwhere we use a refusal metric, the negative compliance metric used in Section 3.3, \u2212x+d+ as m(x). For a fair cross-model comparison, we use the normalized indirect effect (NIE) by dividing the IE of each pair by (m(x+) \u2013 m(x\u00af)). The value of NIE represents the proportion of refusal logit recovered by patching that intermediate state. Therefore, a high NIE indicates that the corresponding state is critical for making safety-related decisions.\nThe results are shown in Figure 10. We can observe that states with high causal effects (colored in blue) appear before the last position in the template, primarily clustering in middle layers. This distribution pattern demonstrates how the template region strongly mediates safety-related information flow: safety information is transferred and processed through these critical locations, activates attention heads to focus on the template region (as discussed in Section 3.2), and ultimately transforms into the safety decision at the final position."}, {"title": "C Chat Templates", "content": "The chat templates of used models are shown in Table 2."}, {"title": "D Transferability of Harmful Probes", "content": "The accuracy of harmful probes for LLaMA-3-8B-Instruct from positions 0-4 in the template, when transferred to responses, is shown in Figures 11 to 15. In particular, the results from position 4 are also presented in Figure 7."}, {"title": "E Examples of TEMPPATCH", "content": "We provided several example responses from LLaMA-3-8B-Instruct when applying TempPatch"}]}