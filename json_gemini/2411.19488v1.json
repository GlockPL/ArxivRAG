{"title": "Interleaved-Modal Chain-of-Thought", "authors": ["Jun Gao", "Yongqi Li", "Ziqiang Cao", "Wenjie Li"], "abstract": "Chain-of-Thought (CoT) prompting elicits large language models (LLMs) to produce a series of intermediate reasoning steps before arriving at the final answer. However, when transitioning to vision-language models (VLMs), their text-only rationales struggle to express the fine-grained associations with the original image. In this paper, we propose an image-incorporated multimodal Chain-of-Thought, named Interleaved-modal Chain-of-Thought (ICoT), which generates sequential reasoning steps consisting of paired visual and textual rationales to infer the final answer. Intuitively, the novel ICoT requires VLMs to enable the generation of fine-grained interleaved-modal content, which is hard for current VLMs to fulfill. Considering that the required visual information is usually part of the input image, we propose Attention-driven Selection (ADS) to realize ICoT over existing VLMs. ADS intelligently inserts regions of the input image to generate the interleaved-modal reasoning steps with ignorable additional latency. ADS relies solely on the attention map of VLMs without the need for parameterization, and therefore it is a plug-and-play strategy that can be generalized to a spectrum of VLMs. We apply ADS to realize ICoT on two popular VLMs of different architectures. Extensive evaluations of three benchmarks have shown that ICOT prompting achieves substantial performance (up to 14%) and interpretability improvements compared to existing multimodal CoT prompting methods.", "sections": [{"title": "1. Introduction", "content": "Chain-of-Thought (CoT) [31] prompting aims to augment the reasoning capabilities of large language models (LLMs) [4, 8, 22, 26] by eliciting them to produce a sequence of intermediate natural language reasoning steps before arriving at the final output. CoT has proven effective in various reasoning tasks, including arithmetic [9], common-sense [17], and symbolic [3], and it has become a potential pathway to advanced artificial intelligence as depicted in GPT-01 [20].\nWith the development of vision-language models (VLMs), extending CoT prompting into multimodal CoT to improve the reasoning capabilities of VLMs in vision-related tasks becomes increasingly important [19, 28, 33, 34]. The initial multimodal CoT attempts [28, 33] take as input the fused visual and textual embeddings, and train language models, such as T5 [23] models, to generate text-only rationales and answers. In the era of VLMs, introducing triple demonstrations composed of an image with the instruction, textual rationales, and the final output (e.g., answer), has proven effective in sparking the reasoning ability of VLMs [6]. Then, related studies focus on improving the linguistic reasoning ability of VLMs. Specifically, DDCOT [34] leverages VLMs to deconstruct problems and resolve them respectively, and CCoT [19] generates scene graphs to prompt VLMs with object and position description. SCAFFOLD [12] overlays a coordinate matrix onto the image to prompt the VLMs with relative visual positions. However, these methods still generate text-only reasoning steps, making it hard to express the fine-grained associations with the origin image exactly. As shown on the left of Figure 1, textual position descriptions, e.g., at the top, are too rough to identify all fruits (orange and banana).\nIn light of the limitations of text-only rationales, we propose incorporating visual information to enhance the precision of fine-grained associations between generated textual rationales and the corresponding image. We therefore propose an advanced multimodal Chain-of-Thought prompting, named Interleaved-modal Chain-of-thought (ICoT), as shown on the right of Figure 1. ICoT generates multimodal rationales consisting of paired images and textual rationales that formulate interleaved-modal reasoning steps to infer the final output. To the best of our knowledge, ICOT is the first multimodal CoT with images incorporated, and it aligns more closely with human thinking processes [5, 21].\nIntuitively, facilitating the novel ICoT is non-trivial, as it introduces challenges for VLMs to support fine-grained interleaved-modal content generation. No current VLMs meet this condition completely. Perceiver-based VLMs such as Qwen2-VL [29] converts images into visual embeddings. Thus, they support fine-grained visual understanding but cannot generate multimodal outputs. Recent proposed unified-modeling VLMs, such as Chameleon [25], Unified-IO 2 [18], and Emu-3 [30], enable multimodal generation by tokenizing images into discrete tokens. However, on the one hand, unified-modeling VLMs exhibit inertia toward multimodal content generation [7]; on the other hand, the generated images belong to the fixed pre-defined resolution instead of fine granularity.\nSince required visual information is usually part of the input image for ICoT, we accordingly propose Attention-driven Selection (ADS) to realize ICoT. The basic idea of ADS is to signal VLMs to select patches from the input image, rather than generating extra images. At the beginning of generating each textual rationale, ADS inserts a piece of visual tokens of selected patches from the input image to refine the generation of the following textual rationale. Specifically, ADS utilizes the attention map of VLMs to identify optimal patches from the input image as fine-grained visual rationales. Once these fine-grained visual rationales are inserted into the current generation sequence, the VLM resumes the original autoregressive text generation process based on previous multimodal content, formulating paired image and textual rationales to infer the final outputs. Notably, since ADS does not compel VLMs to generate real images, it brings ignorable inference latency"}, {"title": "2. Related Work", "content": "2.1. Vision-Language Models (VLMs)\nCurrently, predominate VLMs such as Qwen-VL [2, 29], BLIP [13], and LLaVA [14-16] are mainly built upon a Large Language Model (LLM), a visual module, and an aligned vision-language adapter. The visual module, e.g., Vision Transformer (ViT) [1], encodes images into dense representations, and then the adapter, e.g., MLP or Q-Former, converts these representations into LLM-readable visual tokens. Finally, visual tokens and textual tokens are fed into the LLM to perform the next-token prediction. This type of VLM can be concluded as Perceiver-LLM architecture, while the Perceiver usually comprises a visual module and the adapter. Additionally, Cambrain-1 [27] introduces more visual modules to collaboratively provide more useful visual tokens in a vision-centric paradigm. In the other research line, unified-modeling VLMs represented by Chameleon [25], Unified-IO 2 [18], and Emu3 [30] are designed to generate texts, images, and so on uniformly. As these models apply codebook [11] to tokenize images into discrete vokens, their training processes are supervised by both vision and text information. Unified-modeling VLMs are expected to develop more stable multimodal understanding abality [10].\n2.2. Multimodal Chain-of-Thought Prompting\nSimilar to CoT used in LLMs, multimodal Chain-of-Thought prompting methods [12, 19, 28, 32, 34] aim to augment the reasoning ability of VLM by generating intermediate reasoning steps. A series of studies focus on providing VLMs with fine-grained textual information, such as detailed description [28]. Compositional CoT (CCoT) [19] prompts VLMs to generate a Scene Graph (SG), which is a JSON-like description containing compositional information of objects that occurred in the image. DDCOT [34] deconstruct problems into small problems, requiring VLMs to solve them respectively and then inferring the final answer. In the other research line, Set-of-Marks prompting [32] augments the objects in the image to help VLMs recognize them. SCAFFOLD [12] overlays coordinate onto images to prompt VLMs with relative position information, and VLMs leverage overlayed textual coordinates to implicitly represent corresponding regions of the image to perform reasoning.\nHowever, these methods still produce text-only rationales to infer the final answer. These generated textual rationales usually struggle to express the fine-grained associations with the origin image. We thereby propose ICoT to elicit VLMs to generate interleaved visual-textual reasoning steps to effectively reach the final outputs."}, {"title": "3. Methodology", "content": "To address the limitations that current multimodal CoT methods are still stuck in generating text-only rationales to infer the final answer, we propose interleaved-modal COT (ICOT) to elicit VLMs generated multimodal reasoning steps. We start by introducing the workflow of VLMs and multimodal CoT in Section 3.1. We then introduce the concept of ICoT in Section 3.2. Finally, we propose a plug-and-play method Attention-driven Selection (ADS) to realize ICoT on existing VLMs.\n3.1. Preliminaries\nWe first recall some background of VLMs and multimodal CoT in this section.\nVision-Language Model. VLMs usually consist of a visual encoder E and a generative large language model LLM, and they determine where to insert images according to visual holders inserted in the text instructions. Then, VLMs take the image and the instructions as input and respond with a final answer\nanswer = VLM(Image, Instruction). (1)\nSpecifically, the visual encoder E extracts visual tokens $f_{l \\times d}^{v}$ from the image $x_{v}$, where l is the length of visual tokens and d is the dimensions of the hidden states of the LLM. The built-in LLM predicts next-tokens in a left-to-right fashion according to visual tokens $f_{l \\times d}^{v}$ and the instructions.\nMultimodal CoT. Compared with the direct prediction described in Eqn. 1, Multimodal CoT further introduces a prompt to elicit VLMs to generate a series of intermediate textual rationales ($r_1, r_2, ...$) before the final answer:\n$r_1, r_2, ..., answer = VLM(Prompt, Image, Instruction)$. (2)\nTechnically, the prompt could be represented as a sequence of demonstrations, each consisting of a triple: (Image, Rationale, Answer). Alternatively, an explicit instruction, such as \u201cLet's think step by step,\" could also serve as the prompt.\n3.2. Interleaved-modal Chain-of-Thought\nPrevious multimodal CoT prompting methods only produce text-only reasoning steps to improve the reasoning ability of VLMs. These intermediate steps are generated according to the entire image, which are struggle to express exact fine-grained associations with the original image. Given these limitations, we propose a more advanced Interleaved-modal Chain-of-Thought (ICoT) prompting, aiming to elicit VLMs to generate a series of multimodal intermediate reasoning steps each consisting of paired image and textual rationale. Generated intermediate reasoning steps formulating interleaved-modal rationales to effectively lead to the final outputs. In this paper, we consider the visual rationales in interleaved-modal rationales as fine-grained visual information $x_{h' \\times w'}$ extracted from an image $x_{h \\times w}$. These visual rationales capture relevant details in the image, such as objects, colors, and texts, interleaved with the following generated textual rationale to infer the final answer:\n$r_1, x_{v_1}, r_2, x_{v_2}, ..., answer = VLM(Prompt, Image, Instruction)$. (3)\n3.3. Attention-driven Selection\nAlthough the proposed ICoT is both novel and conceptually sound, current VLMs are unable to generate such fine-grained visual information. This remains true even for VLMs [18, 25, 30] are empowered with multimodal generation ability. We thus propose to simplify the problem from fine-grained visual information generation to fine-grained visual information selection, as this information"}, {"title": "4. Experiments", "content": "4.1. Datasets\nM\u00b3CoT [6] is a novel multimodal CoT benchmark specifically concentrated on multi-domain, multi-reasoning-step. M\u00b3CoT contains 267 categories from science, mathematics, and commonsense domains. As the question of each instance is relatively complex, their rationales have an average length of 293 tokens and rely more on fine-grained visual information, which can reflect the advantages of ICOT compared with previous multimodal CoT methods.\nScienceQA [24] is a popular dataset used to evaluate the reasoning ability of VLMs. We use ScienceQA to provide a general comparison between ICoT and other existing multimodal CoT methods.\nLLaVA-Bench In-the-Wild (LLaVA-W) [15] evaluates VLMs' ability to respond to visual questions with detailed long-form answers, which also focus on the fine-grained visual description. The reference label of each instance is produced by GPT-4v.\n4.2. Baselines\nNo-CoT responds to the current input image and question directly without further prompt. The few-shot demonstrations of the direct generation mode consist of (Image, Question, Answer).\nMultimodal CoT [33] elicits VLMs to generate a series of text-only intermediate reasoning steps to infer the final outputs.\nCCoT [19] first generates a scene graph (SG) using the VLM itself and then uses that SG in the prompt to produce a response. The SG is a JSON-like structural description of the given image with extensive compositional information of objects in the current images. Following their settings, we apply their official prompt to prompt VLMs to generate SGs and answers respectively.\nDDCOT [34] first prompts LLM to deconstruct the input question into a sequence of basic sub-questions, and then applies a VQA model to answer these sub-questions involving visual information. In this paper, we use the VLM that plays the role of LLM in DDCOT for a fair comparison, as their original LLM is ChatGPT.\nSCAFFOLD [12] overlays a coordinate matrix onto the input image, exactly demonstrating relative visual positions for VLMs. During reasoning, VLMs are steered to utilize these coordinates that indicate fine-grained visual information in the image to solve different vision-language tasks. We use their released scripts to add coordinates onto each image, and then use their official prompt to elicit VLMs.\nSpecific in the few-shot scenario, the demonstrations of these baselines are human-written, aligning with ICoT."}, {"title": "4.3. Implement Details", "content": "We apply ICoT over Chameleon-7B [25] and Qwen2-VL-7B-Instruct [29] which represents the fine-grained visual information in the form of discrete vokens and dense features. All experiments are conducted on A800 GPUs, and we evaluate ICoT under both zero- and one-shot scenarios. During generating interleaved-modal rationales, the signal token S used to trigger ADS is set to line break, i.e., \u201c\\n\", by default, which semantically and empirically indicates the end of a generated rationale and the beginning of the next one.\nVLMs insert visual tokens selected by ADS following the special token at the granularity of 64 according to posterior results shown in Table 4. Notably, to shorten the representation of an image, Qwen2-VL introduces a novel merge mechanism, and we approximately consider its patch size to be (28 \u00d7 28). Each patch of Qwen2-VL has approximately 4 times as many pixels as a Chameleon patch (16 \u00d7 16), which results in practical selection numbers of ADS set to 16. Considering the work of ADS requires the inner attention map, we apply the \u201ceager\u201d attention on both Chameleon-7B and Qwen2-VL, limited to the dependency of related python libraries."}, {"title": "4.4. Main Results", "content": "We comprehensively evaluate the performance of ICoT on top of Chameleon-7B and Qwen2-VL-7B through M\u00b3CoT, ScinceQA, and LLaVA-W in Table 1. In O-shot settings, ICoT outperforms all baselines including direct generation (No-CoT), CoT, CCoT, DDoT, and SCAFFOLD. Specifically, ICoT distinguishes from Multimodal CoT in terms of the modality of reasoning steps, which exhibit the advantages of interleaved-modal rationales to infer the final answer effectively. Compared with other multimodal CoT methods, the performance gains of ICoT further indicate that interleaved-modal rationales are more reasonable in intuition and effect than plainly inserted scene graphs (CCoT) and deconstructed sub-questions (DDoT). In 1-shot settings, ICoT demonstrations contain manually selected fine-grained visual information, while their text rationales are kept the same as other baselines. The performance gains compared with 0-shot ICoT indicate that our manually designed fine-grained ICoT demonstrations potentially guide VLMs to think in this format. In Table 4, we rigorously ablate the effectiveness of fine-grained ICoT demonstrations.\nAdditionally, ICoT achieves the most relative performance gains in the LLaVA-W benchmark as the reference labels contain details sourced from images. These substantial performance gains compared with other baselines prove that visual tokens selected by ADS effectively capture the fine-grained visual information of an image, aiding the generation of high-quality text rationales."}, {"title": "4.5. Ablation Study", "content": "We ablate ICoT to verify the effectiveness of each portion across three benchmarks in Table 2. When removing both ADS and FVI, ICoT will degrade to the plain multimodal CoT. Results indicate that both ADS and fine-grained visual information (FVI) incorporated in the demonstration are necessary. In particular, interleaved-modal rationales exhibit substantial advantages in generating high-quality textual rationales compared with text-only rationales (w/o ADS). When substitute ICOT demonstration with normal ones (w/o FVI), the performance degradation prove the fact that fine-grained visual information in demonstrations effectively guides VLMs to think in this format. Compared with the performance difference between removing ADS and FVI, we find that generating paired visual and text rationales boosts more improvements.\nAdditionally, the performance gap is relatively smooth in ScienceQA and more dramatic on M\u00b3CoT and LLaVA-W. We attribute this to the ScienceQA dataset being relatively easier than others since both M\u00b3CoT and answers of LLaVA-W highly rely on the fine-grained visual information of an image. Therefore, our proposed ICoT has the potential to solve complex vision-language tasks."}, {"title": "4.6. In-depth Analysis", "content": "Analysis on realizing ICoT via KV Cache Up to now, the fine-grained visual information of ICoT is provided at the input end via discrete vokens or dense visual tokens, which brings more computation. After rethinking the generating process of an autoregressive model, there are other inputs that are proposed to avoid repeated computation, namely, the Key-Value (KV) Cache. The input image was stored in the KV Cache during prefilling phase before generating multimodal intermediate reasoning steps in a left-to-right fashion. Therefore, copying the KV cache of fine-grained visual information enables ICoT with reduced computational costs, as visual information does not require extra forward propagation. As shown in Table 3, copying KV Cache brings performance degradation compared with providing visual information at the input end. We attribute this phenomenon to the fact that although copying KV Cache indeed makes VLMs attend more to the same region as ADS, the optimal visual information is highlighted in a position-agnostic case, determined by the nature of KV Cache. Specifically, this degrades the original interleaved-modal rationales into non-interleaved ones as position information is early fused into KV Cache, and thus the copied ones are inherently insensitive to the position of textual rationale.\nHowever, considering it brought slight performance degradation and factually reduced computation costs, we believe this exploration is still valuable, and we call for more interesting exploration in realizing ICOT.\nAnalysis on the Selected Patches Intuitively, if ADS selects a large amount patches every time, the selected patches will be dispersed, resulting in more noise introduced and higher computation costs. In contrast, only a few selected patches perhaps failed to contain enough fine-grained visual information. It is non-trivial to determine the exact number of patches selected by ADS, as fine-grained information in an image is not always the same size. Therefore, in Figure 4, we empirically set the number of patches selected by ADS n to 32, 64, 128, and 256 at a coarse-grained, and illustrate their performance variance across two benchmarks . Observed results indicate that setting n too large or too small is not good for VLMs, and ICoT achieves relatively better performance when n is set to 64."}, {"title": "Analysis on the Demonstrations", "content": "We also attempt to let VLMs generate the demonstrations via themselves (Automatic at the bottom of Table 4). Results indicate that using the automatically generated demonstrations also brings performance degradation compared with ICoT using manually designed ones. We consider it is caused by the fact that formulating a continuous sub-image through ADS is non-trivial, and some discrete patches inevitably introduce additional noise. Therefore, considering designing such a demonstration is not time-consuming, ICoT utilizes manually designed ones to elicit VLMs to perform ICoT for better performance."}, {"title": "5. Case Study", "content": "In this section, we empirically illustrate the advantages of ICOT via three case studies in Figure 3. These case studies focused on three typical problems that occurred in text-only rationales, namely, misunderstanding (top), overgeneralization (middle), and hallucination (bottom).\nSpecifically, in the first case, interleaved-modal CoT first recognizes three different objects via captions: \u201cinflatable castle, crayons, and a parachute\". Then, in the second reasoning step, ADS inserts selected patches from the scheduled objects to elicit the VLM to conclude their common property, and VLM infers a correct answer. Text-only CoT misunderstands the three objects are all colored pencils, ignoring the castle and the parachute, even the final answer is correct. In the second case, text-only rationales overgeneralize flying a kite to a kite festival, leading to a wrong answer. ICoT first recognizes a man standing in a field of grass and then infers it is a windy day according to a few kites in the sky. In the last case, it provides the other typical error of text-only CoT, namely, hallucination. As text-only CoT purely relies on language reasoning ability, VLMs have the potential to imagine something not mentioned in the image, resulting in a wrong answer. ICOT first infers from the street sign that there may be a troll attraction nearby according to patches of the indicator inserted by ADS. Then, the ADS helps the VLMs to attend to the troll statue under the bridge and infer it is likely the mentioned attraction, finally arriving at the correct answer.\nEven though the above two case studies exhibit the advantages of ICoT, ADS still brings potential problems. For example, ADS is triggered to select patches when VLM generates a pre-defined signal token. This simple mechanism is a double-edged sword that VLMs will generate low-quality responses if this token is generated with a high frequency."}, {"title": "6. Conclusion", "content": "In this paper, we first propose interleaved-modal CoT (ICOT), which generates interleaved-modal rationales to effectively infer the final answer. In light of the challenges of applying ICoT on existing VLMs, we then introduce Attention-driven Selection (ADS), a plug-and-play strategy to identify optimal patches from the image without being parameterized. We evaluate ICoT on Chameleon-7B and Qwen2-VL-7B-Instruct, representing VLMs of two architectures. Extensive experiments conducted on M\u00b3CoT, ScienceQA, and LLaVA-W, under both zero- and few-shot scenarios, have proven that ICoT achieves substantial performance (up to 14%) compared with the existing multimodal CoT methods. Additionally, in the analysis section, we conduct a preliminary exploration of implementing ICoT by copying the KV cache of optimal visual tokens and explain the inner trade-off between efficiency and performance in this approach.\nAlthough ICoT has proven its effectiveness in this paper, we consider ICoT still has significant potential for further improvement. We plan to evaluate it across additional backbones and benchmarks to better assess its generalization ability. Moreover, the fixed number of selected patches in the ADS design leads to occasional, unexpected issues for VLMs in certain cases. To address this, we intend to incorporate established techniques from segmentation to create a more robust implementation of ICoT."}]}