{"title": "Matina: A Large-Scale 73B Token Persian Text Corpus", "authors": ["Sara Bourbour Hosseinbeigi", "Fatemeh Taherinezhad", "Heshaam Faili", "Hamed Baghbani", "Fatemeh Nadi", "Mostafa Amiri"], "abstract": "Text corpora are essential for training models used in tasks like summarization, translation, and large language models (LLMs). While various efforts have been made to collect monolingual and multilingual datasets in many languages, Persian has often been underrepresented due to limited resources for data collection and preprocessing. Existing Persian datasets are typically small and lack content diversity, consisting mainly of weblogs and news articles. This shortage of high-quality, varied data has slowed the development of NLP models and open-source LLMs for Persian. Since model performance depends heavily on the quality of training data, we address this gap by introducing the Matina corpus, a new Persian dataset of 72.9B tokens, carefully preprocessed and deduplicated to ensure high data quality. We further assess its effectiveness by training and evaluating transformer-based models on key NLP tasks. Both the dataset and preprocessing codes are publicly available\u00b9, enabling researchers to build on and improve this resource for future Persian NLP advancements.", "sections": [{"title": "1 Introduction", "content": "Since the introduction of the transformer architecture (Vaswani, 2017), natural language processing (NLP) has advanced rapidly, transforming many language-related tasks. Transformer-based models, like BERT (Devlin, 2018) and GPT-2 (Radford, 2018), initially focused on tasks like sentiment analysis, translation, and summarization. However, with the development of large-scale language models (LLMs), such as GPT-3 (Brown, 2020) and later models (Touvron et al., 2023; Le Scao et al., 2023; Bai et al., 2023; Yang et al., 2024), the research shifted towards more complex tasks, including generalization, creative problem-solving, and critical thinking.\nThe performance of these models, in both basic and advanced tasks, isn't just about model size or computational power\u2014it's also heavily influenced by the quality and amount of training data. As a result, a lot of effort has gone into large-scale data collection and preprocessing (Gao et al., 2020; Lauren\u00e7on et al., 2022; Penedo et al., 2023) to improve model capabilities and generalization.\nWhile English dominates NLP research, there has been a growing effort to curate multilingual datasets (Wenzek et al., 2019; Lauren\u00e7on et al., 2022; Nguyen et al., 2023; Kudugunta et al., 2024) and develop models capable of understanding multiple languages (Le Scao et al., 2023; Touvron et al., 2023; Yang et al., 2024).\nDespite Persian being widely spoken, it remains underrepresented in NLP research. Although both conventional models and LLMs can process Persian, their performance is often suboptimal, mainly because of the limited availability and poor quality of existing data. Persian text data is predominantly sourced from news websites and blogs, which often lack formal or factual content. Moreover, no standardized preprocessing pipeline exists to ensure the high quality of Persian datasets at the same level as those available for other languages.\nTo address this gap, we introduce the Matina Corpus, a 72.9 billion token Persian dataset designed for training language models. Unlike other Persian datasets (Targoman, 2022; Sabeti et al., 2018), the Matina Corpus has undergone a rigorous and well-designed preprocessing pipeline and a comprehensive deduplication process to ensure its high quality. The dataset includes not only publicly available Persian datasets but also introduces newly collected sources to ensure greater diversity and the inclusion of factual information. The diverse sources in the dataset make it suitable both for training large language models and for a variety of downstream tasks that require clean, high-quality Persian data."}, {"title": "2 Related Work", "content": "The scope of our dataset encompasses two key dimensions: (1) the preprocessing steps involved in creating large-scale corpora and (2) the development of extensive text corpora in Persian. Accordingly, we divide this section into two parts. First, we review notable large-scale corpora available in languages other than Persian, along with the preprocessing techniques applied to these datasets. Then, we examine and analyze the current state of publicly available Persian corpora."}, {"title": "2.1 Large-Scale Public Corpora", "content": "Since the early stages of NLP development, there have been efforts to compile large-scale datasets for training models in various downstream tasks, such as sentiment analysis, summarization, and text classification, among others (Glockner et al., 2018; Narayan et al., 2018; Wang et al., 2019). With the advent of deep learning models, these efforts have escalated in scope, culminating in the large-scale data collection necessary for training large language models (LLMs). One of the earliest and most significant contributions to the development of large text corpora is Common Crawl (Crawl, 2008).\nCommon Crawl (Crawl, 2008) is a vast multilingual web corpus that continuously archives webpage data from the Internet. However, Common Crawl contains substantial amounts of extraneous content, including advertisements, navigation bars, and inappropriate materials such as pornography, violence, spam, and sensitive personal information."}, {"title": "2.2 Persian Text Corpora", "content": "The rapid development of natural language processing (NLP) has necessitated the creation of diverse, large-scale text corpora across various languages. For Persian, also known as Farsi, the availability of robust datasets is crucial for enhancing language modeling capabilities. However, a significant gap persists in terms of corpora that are sufficiently diverse and preprocessed for effective use in training LLMs. Many existing Persian datasets predominantly feature news content, which does not adequately cover the full spectrum of language use. Despite these limitations, Persian remains a language with rich literary and cultural resources, suggesting a substantial potential for corpus development.\nSeveral Persian corpora, including the Persian Wikipedia Corpus\u00b9, MirasText, hmBlogs (Khansari and Shamsfard, 2021), Naab (Sabouri et al., 2022), Targoman (Targoman, 2022), have significantly enriched the pool of publicly available Persian data. The Persian Wikipedia Corpus, with over one million articles, serves as a foundational resource, though its content is mainly formal and factual. MirasText, covering 2.8 million articles from more than 250 news websites, and Naab (Sabouri et al., 2022), containing around 15 billion tokens, both contribute vast data but are largely news-centric, which limits content diversity. In contrast, Targoman (Targoman, 2022) expands the scope by incorporating 65 million documents across weblogs, forums, literature, and educational content, although issues with licensing and accessibility hinder its public use. Additionally, hmBlogs (Khansari and Shamsfard, 2021) offers a valuable glimpse into colloquial language with 20 million blog posts spanning 15 years, though it requires extensive preprocessing to ensure its consistency and applicability. Additionally, Ganjoor introduces classical Persian poetry from 12 poets, enhancing the stylistic and lexical range of the corpus and providing unique linguistic depth.\nParallel corpora, including TEP: Tehran English-Persian parallel corpus (Tiedemann, 2012), MIZAN (Kashefi, 2020), and the Bible Corpus7, further extend the utility of Persian datasets by enabling translation tasks and bilingual language modeling. MIZAN (Kashefi, 2020), containing one million sentence pairings between Persian and English, allows cross-linguistic studies and machine translation. However, the breadth of such corpora is frequently limited.\nStandardized preprocessing techniques are required to improve Persian language modeling by filtering non-Farsi words, unifying Arabic and Farsi characters, and removing unnecessary content. These steps are crucial for creating a high-quality, clean corpus, as data quality directly impacts model performance in large language models (LLMs). While some datasets, such as Naab (Sabouri et al., 2022) and hmBlogs (Khansari and Shamsfard, 2021), offer preprocessed versions, this is still the exception rather than the norm in Persian corpus development."}, {"title": "3 Matina Corpus", "content": "The Matina corpus is built from a variety of data sources, each of which is processed based on its specific content characteristics. Although these sources are grouped into three main categories, the overall preprocessing pipeline remains consistent, as depicted in Figure 1, with variations primarily in hyperparameters. Certain sources, however, demand additional cleaning steps, which are detailed in their respective sections.\nFigure 2 visualizes the distribution of token counts across documents from each source, using a box plot to illustrate the variance in document length. These three categories-web-based crawled data, crawled books and papers, and social media-form the core of our dataset, each with distinct preprocessing requirements. In this section, we describe the data collection process, the preprocessing techniques applied, and the rationale behind the decisions made throughout these steps."}, {"title": "3.1 Web-based Crawled Data", "content": "Web crawling is a common and efficient method for collecting data in any language. Websites offer a vast range of valuable information and, given their structured nature and wide availability, can largely be crawled automatically. As a result, web data is frequently used as the primary source for constructing large-scale text datasets. However, while the bulk collection of web data is straightforward, extracting meaningful content from irrelevant elements such as metadata, advertisements, and embedded links remains challenging. Web pages often contain spam-like elements, which complicates the cleaning process and increases the likelihood of errors.\nMost web-based datasets begin with basic steps such as text extraction and language detection, often followed by optional URL filtering to exclude content deemed inappropriate or irrelevant. Further preprocessing steps are applied, followed by deduplication to ensure data quality and minimize redundancy. We adopt a similar approach in preprocessing the web data collected for the Matina corpus.\nMatina's web-based data is divided into two parts: data crawled by our team and data taken from two public databases using the Common Crawl (Crawl, 2008) dataset. This dual-source strategy uses both proprietary and publically available data to increase the corpus's breadth and diversity.\nIn any language, certain domains are recognized for their reliability and high-quality information. We identified such domains in Persian and crawled them to extract relevant textual content. This step helped minimize the inclusion of irrelevant elements such as advertisements, tags, or comments. Text extracted from headings and paragraphs was merged to form unified documents, with additional informative fields (e.g., summaries or subheadings) incorporated as metadata, if available. Because these domains were manually selected, language detection and URL filtering were unnecessary. We also ensured that the selected URLs did not contain harmful, sensitive, or adult content.\nFor the public datasets, Madlad-400 (Kudugunta et al., 2024) and CulturaX (Nguyen et al., 2023), the initial preprocessing steps\u2014such as language detection, text extraction, and URL filtering-had already been completed by the dataset providers. These datasets also included filters for toxic or harmful content, which allowed us to directly proceed to the next stages of preprocessing. While both datasets applied generic filters\u2014such as language mismatch detection, character ratio checks, and word/sentence length thresholds, these filters were not language-specific. Therefore, we processed data from these sources similarly to the web data we crawled ourselves. After applying the processing on data sourced from web and the public datasets, there remained 64.3B tokens with an average document length of 1,141.8 tokens.\nAfter inspecting samples from various domains, we defined heuristic functions to modify documents and remove those deemed irrelevant. These heuristics were inspired by preprocessing pipelines adopted in BLOOM (Le Scao et al., 2023), MassiveText (Muennighoff et al., 2022), and RefineWeb (Penedo et al., 2023), but we tailored them to the specific characteristics of our data and added multiple other processing functions.\nOur preprocessing pipeline for web-based data encompasses three primary stages: character-level processing, line and paragraph-level processing, and document-level processing. Each stage employs a series of targeted operations to enhance data quality, ensure linguistic consistency, and eliminate redundancies. Appendix A provides a full explanation of each step in the preprocessing and deduplication procedures.\nCharacter-level processing involves normalizing Persian characters, mapping symbols and numbers to their Persian equivalents, limiting the occurrence of repeated characters, standardizing newline characters, and removing non-standard Unicode symbols. This stage ensures that the text adheres to consistent encoding standards and minimizes the presence of corrupted or irrelevant characters.\nLine and paragraph-level processing focuses on the structural integrity of the text by removing HTML and JavaScript tags, handling custom structures specific to certain domains, filtering out lines with excessive special characters, and eliminating short or incomplete lines that do not contribute meaningful content.\nDocument-level processing entails a comprehensive evaluation of each document's relevance and quality. Documents are discarded based on criteria such as insufficient length, predominance of non-Persian content, excessive repetition of words, high proportion of short lines, and the presence of out-of-vocabulary (OOV) words. These filters ensure that only high-quality, relevant, and linguistically coherent documents are retained in the corpus.\nAfter cleaning the documents, we apply a deduplication step to mitigate data redundancy, a crucial aspect of the preprocessing pipeline highlighted in several studies (Gao et al., 2020; Penedo et al., 2023; Le Scao et al., 2023). Utilizing the MinHash algorithm (Broder, 1997), we efficiently identify and eliminate both exact and near-duplicate documents, thereby enhancing the corpus's uniqueness.\nFor two manually inspected domains, Virgool"}, {"title": "3.2 Crawled Books and Papers", "content": "Data collected from the web alone does not provide sufficient factual or literary content. To enrich our dataset, we also sourced publicly accessible books and academic papers from websites and social media channels. As demonstrated in Figure 2, the box plot of document length distribution clearly shows that books and papers contain significantly longer texts compared to web and social media content, making them more informative and comprehensive. This length, along with the depth of the content, further justifies the inclusion of these sources in our corpus.\nSince most of these sources provide data in PDF format, additional steps were required to convert PDFs into usable text. However, the limited accuracy of Persian OCR systems introduces challenges, particularly when processing PDFs that contain scanned images.\nWe divided the data from books and papers into two groups, each requiring different processing steps based on the nature of the data: Text-based PDFs and Image-based PDFs (OCR). Just like the data from web, the processing of books and papers involved a combination of document-level, character-level, and line-level operations to ensure data quality, as outlined below."}, {"title": "3.2.1 Text-based PDFS", "content": "Text-based PDFs primarily include books and academic papers sourced from Telegram channels and Persian websites. The PDFs were converted into text using several Python libraries. To ensure quality, we tested various tools on sample documents and applied low-level heuristic filters to remove corrupted or irrelevant content.\nThe filtering process involves removing documents with insufficient Persian content, short text lengths, or an excessive use of symbols. This stage ensures that only relevant and high-quality documents are retained. Following this initial filtering, we apply a preprocessing pipeline to address document, character, and line-level inconsistencies, ensuring the text is properly structured. Additional technical details on these steps, including character normalization, watermark removal, and deduplication, are provided in the Appendix B."}, {"title": "3.2.2 Image-based PDFs (OCR)", "content": "Many papers in our dataset were converted to text using image-based OCR due to the unavailability of text-based PDFs. Given the limitations of Persian OCR, errors were introduced during text extraction. To address this, we filtered out low-quality documents, focusing on those with a high percentage of nonsensical tokens or merged words. As a result, the dataset was refined to include 321,244 documents. The documents were then processed using steps similar to those applied to web-based crawled data, with additional procedures. Additional information on the OCR-specific filtering methods is provided in the Appendix."}, {"title": "3.3 Social Media", "content": "Although some books and blogs may include informal Persian text or dialogues, the overall proportion of such data is minimal. The data collected from web-based sources and books generally lacks unstructured or colloquial language. Social media, however, provides a rich source of unstructured and informal linguistic data. To capture this, we gathered Persian-language data from Twitter, as well as public channels and groups from Telegram and Eitaa (an Iranian chat application). After identifying relevant channels and groups, we crawled all associated messages and processed them using the pipeline described for web-based data, with thresholds tailored to social media content. Additional processing steps we applied are outlined below.\nUpon examination, we found that shorter messages were mostly replies, often lacking substantive content or containing inappropriate language. These messages were filtered out. We also identified hashtags embedded within the text and at the end of messages. Hashtags within the text were retained to preserve context, while those at the end, frequently related to political or social topics and often irrelevant to the main content, were removed. We employed regular expressions (regex) to remove channel IDs and URLs, ensuring that irrelevant content was minimized.\nA notable difference in processing social media data was the deduplication strategy. We observed that many messages from different sources differed only in date or pricing-typically for goods, gold, silver, or cryptocurrencies. To address this, we removed all numeric values and dates before deduplication. After identifying and eliminating duplicate entries, we restored the original content, including numbers and dates, for the final dataset. This method ensured that informative variations were preserved while content containing no new knowledge was removed."}, {"title": "3.4 Final Dataset", "content": "Applying the outlined preprocessing steps, including deduplication, resulted in a significant reduction in the number of documents. As illustrated in Figure 3, the overall document count decreased by an average of 24% after preprocessing, with a further reduction of 18.83% following deduplication.\nThe largest reduction occurred in social media content, particularly from Twitter and Telegram. Many Twitter posts were short and lacked meaningful content, while Telegram messages were often redundant, brief, and became even less informative after hashtags and links were removed. The special deduplication method we applied also identified many of these messages as duplicates. Although only 1.6% of social media documents remained after processing, these retained documents were significantly longer, accounting for around 10% of the total token count from the initial data.\nImage-based academic papers also experienced a considerable loss during processing. In this category, the number of documents was nearly halved, as we applied multiple criteria to remove poor-quality documents. In contrast, text-based papers saw minimal loss, with only 2% of documents eliminated during preprocessing. However, papers in this category contained more duplicates, which contributed to the reduction.\nBooks had the lowest proportion of document elimination during both preprocessing and deduplication. This reflects the higher quality of book content and the effectiveness of the methods used to extract data from PDF files.\nFor the web-crawled data, deduplication had a bigger impact than the initial preprocessing, with more documents being removed in this step. Even though we carefully tried to avoid duplicates during crawling, the nature of web crawling-often involving nested links-led to the inclusion of duplicates. Additionally, many news websites repost the same content across different agencies, which shows just how important thorough deduplication is for web-sourced data.\nAn interesting observation from the bar plot is that, although CulturaX FA (Nguyen et al., 2023) and Madlad-400 FA (Kudugunta et al., 2024) claim to have already undergone processing and deduplication, our language-specific preprocessing steps and content-specific deduplication further reduced their size. In Madlad-400 FA, only 7% of documents were discarded, whereas nearly 70% of CulturaX FA documents did not meet the qualifications for proper Persian data. This emphasizes the importance of language-specific processing and careful evaluation by native speakers to ensure data quality."}, {"title": "4 Assessing the Impact of the Matina Corpus", "content": "A large-scale Persian corpus has numerous applications in NLP, including training transformer-based models for tasks such as summarization, sentiment analysis, emotion detection, question answering, sentence embeddings, and text retrieval. Additionally, such corpora play a crucial role in pretraining large language models (LLMs) and generating instructions for LLM post-training. To assess the effectiveness of the Matina Corpus, we conducted experiments on transformer-based model training and continued pretraining of LLMs. This section provides a detailed discussion of these experiments and their outcomes."}, {"title": "4.1 Masked Language Model Training and Evaluation", "content": "While LLMs have excelled in various NLP tasks such as sentiment analysis and named entity recognition (NER), there remains a need for lightweight models that can be easily fine-tuned for specific tasks and datasets. These models are typically built on transformer-based architectures, particularly masked language models trained on large-scale datasets.\nTo address this need, we conducted continual pretraining of masked language models (MLMs), specifically XLM-RoBERTa Large (Conneau et al., 2020), on 54.69 billion tokens of our dataset. This extensive corpus facilitates the development of high-quality sentence embeddings, further refined by adapting the model into a Sentence-BERT architecture without Next Sentence Prediction (NSP). These enhancements yield more precise semantic representations, significantly improving Persian NLP tasks. By leveraging a well-curated dataset with rigorous preprocessing, our model effectively captures Persian linguistic nuances.\nTo evaluate the effectiveness of Matina corpus in training transformer models, we benchmarked out Roberta-based model against existing models using datasets such as Arman Emo, Pars-ABSA, PQUAD, and PEYMA. As shown in Table 2, our model demonstrates substantial performance gains, achieving 56.54 on Arman Emo, surpassing TookaBERT and AriaBERT, and 74.92 on Pars-ABSA, highlighting its robustness in aspect-based sentiment analysis. These results validate the impact of our dataset on enhancing Persian NLP performance, particularly within transformer-based architectures.\nThe success of our MLM underscores the crucial role of high-quality data in pretraining. By capturing Persian linguistic and cultural nuances, our model not only enhances task-specific performance but also advances the goal of developing inclusive and representative language technologies. This approach ensures that underrepresented languages like Persian receive the attention they deserve, fostering more equitable advancements in NLP."}, {"title": "4.2 Large Language Model Pretraining and Evaluation", "content": "Pretraining is essential for transferring knowledge to LLMs, shaping their linguistic and factual understanding. However, multilingual LLMs often struggle with underrepresented languages like Persian and exhibit cultural biases favoring Western perspectives (Cao et al., 2023; AlKhamissi et al., 2024) due to the dominance of English in their training data. This leads to diminished performance in other languages and cultures. Incorporating language-specific data during pretraining can help address this issue.\nTo evaluate the impact of our dataset on LLM training, we conducted the following experiment. We first tagged our dataset in an unsupervised manner using a procedure similar to InsTag (Lu et al., 2023), categorizing it into multiple domains. From these, we selected two-social and politics and cooking-and extracted a subset of data from each domain. These domain-specific subsets were then used to train models. The token count for each domain is presented in Table 3. We then constructed large instruction datasets for these domains and fine-tuned LLaMA 3.2-Instruct 8B using two different approaches: (1) continued pretraining on the domain-specific data followed by instruction tuning, and (2) direct instruction tuning without additional pretraining. To evaluate model performance, we conducted a human evaluation, where annotators ranked model outputs in a win-lose format, indicating which model provided better responses to a held-out evaluation set derived from the instruction dataset.\nThe evaluation results, shown in Figure 4, indicate that models benefit significantly from pretraining on even a relatively small dataset before instruction tuning. This effect is particularly noticeable in the cooking domain, where the pretrained model was preferred nearly twice as often as the model without pretraining. These findings highlight the effectiveness of the Matina Corpus in improving language models by providing high-quality, domain-specific data. Pretraining on a small, well-curated dataset not only enriches the model's knowledge but also enhances its alignment with the target language and cultural context."}, {"title": "5 Conclusion", "content": "In conclusion, the Matina corpus provides a crucial resource for advancing Persian NLP by addressing the limitations of existing datasets in terms of scale and diversity. With 72.9 billion tokens, it enables the training of more advanced and accurate models for tasks such as machine translation, summarization, and large-scale language modeling. We further demonstrate its effectiveness by training and evaluating transformer-based models on key NLP tasks as well as LLM pretraining, highlighting the benefits of high-quality Persian data. By making both the dataset and preprocessing tools publicly available, we aim to support further research and foster collaboration in the development of open-source tools and models for Persian."}, {"title": "6 Limitations", "content": "While our Persian corpus represents a significant step forward in providing high-quality data, there are several limitations to be noted:\nSub-Document Level Redundancies: Although we applied deduplication at the document level, we did not perform deduplication within documents, meaning there may be redundancies at the sentence or paragraph level. This limitation arises from the high memory and computational resources required to encode and compare sections of all documents. Unfortunately, we did not have the resources necessary to conduct this process at a finer granularity.\nSensitive Content and Language: Despite selecting Persian websites with minimal adult content and removing sensitive data from public datasets, some sensitive material and inappropriate language remain, particularly in social media data. We did not filter out offensive or explicit language, as it reflects real-world language use. However, researchers utilizing the dataset should be mindful of this content when applying it in their work.\nResidual Irrelevant Data: While we inspected samples from all data sources and employed various heuristics and filtering functions to remove irrelevant content, such as links, hashtags, advertisements, and tags, some may have evaded our processes. These elements are generally considered noise given the large scale of the dataset but may need to be addressed for more specialized use cases.\nThese limitations highlight potential areas for improvement, especially for projects with specific needs regarding data quality and sensitivity."}, {"title": "A Details of Web-Based Document Processing Pipeline", "content": "Character-level processing is the initial step in our preprocessing pipeline, aimed at standardizing and cleaning the text at the most granular level. This stage involves several key operations:"}, {"title": "A.1 Character-level Processing", "content": "Character-level processing is the initial step in our preprocessing pipeline, aimed at standardizing and cleaning the text at the most granular level. This stage involves several key operations:\n1. Unicode Normalization: We convert all characters to their Persian equivalents, and remove Arabic I'rab marks. We then normalize space and tab characters to the standard keyboard space, with exceptions made for the half-space character used in specific Persian words.\n2. Symbol and Number Mapping: We map symbols and numbers not belonging to the English, Arabic, or Persian character sets to their Persian equivalents using the Piraye library. This is to ensure language consistency in the dataset.\n3. Repeated Characters: We identify any character repeated more than three times in sequence, typically used for emphasis, and truncate it to three occurrences to maintain readability and consistency.\n4. Newline Normalization: We merge consecutive newlines, including those with spaces or tabs, to standardize line breaks across documents.\n5. Non-standard Unicode Removal: By taking multiple samples from the data we found that there are chracters within the text that are not standard. We then detect and remove these non-standard Unicode characters, such as special emojis or corrupted symbols (e.g., bordered question marks) based on our predefined criteria."}, {"title": "A.2 Line and Paragraph-level Processing", "content": "Once character-level normalization is complete, we focus on the structural elements of the text. This stage involves:\n1. HTML and JavaScript Tag Removal: We identify lines containing HTML or JavaScript tags and functions using regular expressions and replace them with newlines.\n2. Custom Structures Handling: We inspected that some domains include unique tag structures that do not follow the format of standard tags (JavaScript and HTML) which are not captured by regular expressions. We identify and remove these using structures.\n3. Special Character Ratio Filtering: We calculate the ratio of special characters (e.g., emojis, symbols, numbers) to total characters in each line. Lines exceeding a 0.85 ratio are removed, particularly targeting lines corrupted during text extraction, such as tables or formulas.\n4. Short Line Removal: We inspected that certain sources contain incomplete or irrelevant information in the few short lines at the start of the content. We therefore remove lines shorter these specific sources."}, {"title": "A.3 Document-level Processing", "content": "The final stage involves document-level processing. We treat documents as a whole and remove those that meet any of the following criteria: (we refer to words as space-separated text sequences that are neither a number nor a symbol)\n\u2022 Short Length Filtering: Documents shorter than 30 words are removed, as they are either corrupted or devoid of useful information.\n\u2022 Non-Persian Content Removal: Documents where over 50% of characters are non-Persian are eliminated to maintain linguistic consistency and relevance.\n\u2022 Repeated Words Elimination: Documents where more than 50% of the words are identical are eliminated, targeting pages that use SEO techniques or lack informative content.\n\u2022 Short Lines Proportion Filtering: Documents with over 50% of lines shorter than 15 words are discarded, as they typically consist of lists or content tables.\n\u2022 Out-of-Vocabulary (OOV) Words Filtering: Specifically for the CulturaX (Nguyen et al., 2023) dataset, documents containing more than 2.5% OOV words are removed to exclude irrelevant content such as code fragments or corrupted text.\nFinally, we eliminate any repeated empty newlines resulted from the removal of lines or paragraphs to maintain the document's structural integrity."}, {"title": "A.4 Deduplication Process", "content": "To address data redundancy, we leverage the MinHash algorithm (Broder, 1997), a well-established technique for efficient similarity detection in large collections of text. The deduplication pipeline consists of the following steps: (Broder, 1997). The process involves several steps:"}, {"title": "A.5 Domain-specific Processing", "content": "Since Virgool and WikiShia domains contain highly relevant content related to Persian culture and religion, it is necessary to modify our standard preprocessing pipeline to avoid information loss. We perform the following specialized preprocessings.\nFor Virgool, which primarily features blog posts on diverse topics, including programming languages and mathematical content, applying the standard preprocessing thresholds resulted in the removal of valuable content. To address this, we relaxed certain filtering criteria:\n\u2022 By pass the removal of numbers and symbols to preserve technical content.\n\u2022 Incorporate more complex regular expressions to accurately detect and remove residual HTML tags or functions that were not filtered out by the standard pipeline.\n\u2022 Adjust the ratio of Persian stopwords to lower values, and the threshold for the proportion of short lines (in relation to the total number of lines) was increased, ensuring the retention of concise but informative posts.\n\u2022 Employed a privacy-preserving step to remove any personal data found in public blogs, even though the blogs are publicly accessible. This aspect of our pipeline will be discussed in detail in the subsequent section.\nAnother unique challenge with WikiShia was the significant presence of Arabic text, particularly due to references to the Quran and Arabic scholarly sources. To address this, we adjusted our processing thresholds: we increased the tolerance for Arabic stopwords while simultaneously lowering the threshold for Persian stopwords. This adjustment allowed us to better capture the bilingual nature of the content.\nFor WikiShia which includes bilingual content and presents challenges related to content duplication, we performe the following:\n\u2022 Content Duplication: our recursive crawling process exposed a significant issue of content duplication. Multiple URLs often corresponded to the same page, differing only by a minor subheading. Additionally, the site includes detailed descriptions of events associated with specific dates, resulting in multiple unique URLs hosting nearly identical content tied to calendar events. To address this, we employed an exact-match deduplication strategy using MinHashLSH (Leskovec et al., 2020). Unlike our standard deduplication pipeline, we opted not to normalize or remove dates, numbers, or references to specific days of the week, as these elements are critical for preserving the chronological and cultural relevance of the content. By applying this approach, we were able to eliminate documents with a similarity threshold of 98% or higher.\n\u2022 Bilingual Content Handling: Another unique challenge with WikiShia was the significant presence of Arabic text, particularly due to references to the Quran and Arabic scholarly"}, {"title": "B Details of Book and Paper Processing Pipeline", "content": "For data extraction and OCR conversion, we utilized a range of Python libraries, including Selenium10, BeautifulSoup\u00b9\u00b9, and Pytesseract12. Text-based PDFs were converted using lightweight tools such as pdf2image13, while image-based PDFs required more advanced processing with Pytesseract and Fitz\u00b94. To improve accuracy, we employed an iterative approach, applying multiple tools to the same documents and manually inspecting those with errors before refining the extraction process."}, {"title": "B.1 Text-based PDFs: Detailed Processing", "content": "After removing corrupted or non-Persian documents, we apply a 3-stage processing pipeline involving document-level, character-level and line-level processing. Unlike documents from web, we first apply the document-level processing to avoid redundant processing."}, {"title": "B.1.1 Document-level Processing", "content": "In the first stage, we applied document-level processing, where a document was viewed holistically. If it met any of the following criteria, it was eliminated:\n\u2022 Documents with fewer than 150 space-separated words.\n\u2022 Documents containing less than 50% Persian characters.\n\u2022 Documents with an average word length of fewer than 3 characters or greater than 10 characters.\n\u2022 Documents with a numeric or symbolic character ratio exceeding 0.8.\n\u2022 Documents where over 80% of the lines were considered short, defined as containing fewer than four space-separated words.\n\u2022 Documents where fewer than 10% of the words were Persian or Arabic stopwords."}, {"title": "B.1.2 Character-level Processing", "content": "Given that many of the books contained long Arabic text, which needed to be preserved, we only normalized non-Arabic, non-English, and non-Persian characters and symbols to their Persian format. We did not remove I'rab (diacritics). Standard procedures, such as replacing consecutive repeated characters, normalizing newlines, and removing non-standard Unicode characters, were applied as in previous section, though with additional Unicode characters added to the filtering set. Furthermore,"}, {"title": "B.2 Image-based PDFs (OCR): Detailed Processing", "content": "For OCR-processed documents, the primary issue was the introduction of errors during text extraction. To mitigate this, we employed the following steps:\n1. Removed content preceding the keywords section, which was often corrupted, using regex patterns to detect specific document structures.\n2. Removed documents with more than 5% out-of-vocabulary tokens.\n3. REmoved papers containing more than 10 words exceeding 15 characters, indicative of merged words.\nAlthough some OCR-generated text still contains minor issues, such as occasional word merging, these are manageable with model tokenizers and do not significantly affect overall context and understanding."}]}