{"title": "PRIVACY-PRESERVING RETRIEVAL AUGMENTED GENERATION WITH DIFFERENTIAL PRIVACY", "authors": ["Tatsuki Koga", "Ruihan Wu", "Kamalika Chaudhuri"], "abstract": "With the recent remarkable advancement of large language models (LLMs), there has been a growing interest in utilizing them in the domains with highly sensitive data that lies outside their training data. For this purpose, retrieval augmented generation (RAG) is particularly effective-it assists LLMs by directly providing relevant information from the external knowledge sources. However, without extra privacy safeguards, RAG outputs risk leaking sensitive information from the external data source. In this work, we explore RAG under differential privacy (DP), a formal guarantee of data privacy. The main challenge with differentially private RAG is how to generate long accurate answers within a moderate privacy budget. We address this by proposing an algorithm that smartly spends privacy budget only for the tokens that require the sensitive information and uses the non-private LLM for other tokens. Our extensive empirical evaluations reveal that our algorithm outperforms the non-RAG baseline under a reasonable privacy budget of \u20ac \u2248 10 across different models and datasets.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have shown a great deal of promise in a variety of applications. In particular, a major application of LLMs is in question-answering. The practical adoption of these systems often involves domains whose data is highly sensitive. For instance, healthcare institutions might want to utilize their internal medical records to provide precise medical information and personal feedback, while legal firms can leverage their case archives to assist clients with legal research and documentation. One way to achieve such domain-specific question-answering is through retrieval augmented generation (RAG) [5, 23, 16]. Here, we have a set of domain-specific documents; while answering a question, RAG retrieves a list of relevant documents and inputs them to LLMs as the context. However, even though this is effective for question-answering, RAG on a sensitive corpus can still leak private information about individual documents in this corpus [39, 30, 4, 29]. This is particularly problematic when end users are outside the data-holding entity, such as patients interacting with a healthcare institution's RAG system.\nOur goal in this paper is to prevent the information leakage of the sensitive external corpus by designing a privacy- preserving RAG system. For this purpose, we use differential privacy (DP) [9, 10] as a notion of privacy. Differential privacy guarantees privacy by ensuring that the participation of a single person's data does not make much difference to the probability of any output. In our system, we assume that each RAG document comes from a single individual, and our goal is to ensure differential privacy on the eventual answer of the LLM."}, {"title": "Preliminaries & Problem Setting", "content": "Retrieval Augmented Generation with Large Language Model\nRetrieval augmented generation (RAG) is a technique to improve the performance of large language models (LLMs) on knowledge-intensive tasks by providing external knowledge. Given a question prompt, a retriever finds relevant documents from the external data source. Then, the relevant documents are added to the prompt as the contexts. An LLM (or generator) takes the augmented prompt as input and outputs the answer.\nMore formally, let \\(x \\in \\mathcal{U}_{1}^{100} V^*\\) be a prompt, where V is some vocabulary. We further let D be a dataset of documents as an external corpus with size |D| = n. A retriever \\(R\\) finds a subset of D, \\(D_x \\subseteq D\\) that are relevant to x, i.e., \\(D_x = R(x, D)\\). Finally, an LLM generates an answer \\(y = \\text{LLM}(x, D_x) \\in \\mathcal{U}_{1}^{100} V^*\\). The answer generation can further be decomposed into next-token generation. In particular, for each t, the t-th token \\(y_t \\in V\\) is generated by LLM\\(_t\\), which takes x, \\(D_x\\), and previously generated tokens \\(y_{<t}\\) as inputs: \\(y_t = \\text{LLM}_t(x, D_x, y_{<t})\\)."}, {"title": "Differential Privacy", "content": "Differential privacy is a strong cryptographically motivated definition of individual-level privacy. It guarantees that the participation of a single individual in a dataset does not change the probability of any outcome by much. In particular, suppose we have two datasets D and D', each consisting of private data from n individuals. We say that D and D' are neighboring if they differ in a single individual's private data. A randomized algorithm satisfies differential privacy if the output distributions on any pair of neighboring datasets are close. The formal definition is given as follows.\nDefinition 1 ((\\(\\epsilon\\), \u03b4)-Differential Privacy [9]). A randomized algorithm M satisfies ((\\(\\epsilon\\), \u03b4)-differential privacy if for any two neighboring datasets D, D' and for any \\(S \\subseteq \\text{range}(M)\\), \n\\[\\text{Pr}[M(D) \\in S] \\le \\exp(\\epsilon) \\text{Pr}[M(D') \\in S] + \\delta.\\]\nOne of the key properties of differential privacy is composition\u2014sequential runs of differentially private algorithms also satisfy differential privacy. The composition property quantitatively captures the intuition that the more we release the information about the sensitive data, the worse the privacy guarantee becomes. More specifically, suppose \u041c1,..., \u041c\u0442 are (\\(\\epsilon_0\\), \\(\\delta_0\\))-differentially private algorithms, which can be chosen adaptively based on previous outputs. Sequential composition theorem [9] states that the composed sequence of such algorithms guarantee (\\(T\\epsilon_0\\), \\(T\\delta_0\\))-differential privacy. Furthermore, advanced composition theorem [13, 19] states that the total privacy guarantee has \\(\\epsilon = O(\\sqrt{T}\\epsilon_0)\\)."}, {"title": "Sparse Vector Technique", "content": "The sparse vector technique [11, 12] has originally emerged as the alternative of the composition in DP when we have so large number of numerical queries that the composition theorem of DP cannot provide a reasonable privacy guarantee but we are only interested in answers above some threshold. In such a case, the sparse vector technique algorithm, Sparse, reports whether each (noisy) query answer exceeds the threshold. It is shown that the privacy guarantee degrades by the number of queries above the threshold, instead of the total number of queries. Therefore, we save privacy budget"}, {"title": "Differentially Private Generation via Sample-and-Aggregate", "content": "There has been a body of work on generating a token sequence by LLM with differential privacy. One common way to is to borrow the idea of the sample-and-aggregate framework in DP [26]. To generate a single token, a set of LLMs, each depending on a disjoint subset of the sensitive dataset D, generates a token or a probability distribution of a token respectively. When a set of tokens is generated, it forms an aggregate histogram of tokens, which is then carefully randomized with noise and only the most frequent token in the noisy histogram is published as the final output. When a set of probability distribution is generated, it forms a new mixture of probability distribution satisfying differential privacy. The final output token is then sampled from the resulting mixture distribution. The repetition of this process along with the composition theorem of DP yields the differentially private token sequence generation."}, {"title": "Problem Setting", "content": "Our goal is to generate an LLM answer to a prompt x with retrieved external knowledge, \\(D_x = R(x, D)\\), from a sensitive data source D with differential privacy guarantee. More specifically, let the sensitive data source D be a collection of individuals' records\u2014one record corresponds to one individual's sensitive data\u00b9. We consider a realistic adversary who does not have direct access to the data source D but have a capability of querying to the RAG system with any prompt x. We further assume that the LLM used in the RAG system is a copy of publicly available LLMs and is already pre-trained (and fine-tuned) with data disjoint from the sensitive data source D. That is, having access to the LLM parameters and/or pre-training (and fine-tuning) data does not provide any information on the sensitive data source D. To this end, we aim to formally guarantee that given any question x, a randomized LLM generation algorithm with RAG, LLM\\(^\\text{priv}\\)(x, R(x, D)) satisfies (\\(\\epsilon\\), \u03b4)-differential privacy w.r.t the external knowledge data source D."}, {"title": "Differentially Private Retrieval Augmented Generation with Sparse Vector Technique", "content": "Our differentially private RAG algorithm consists of two main components\u2014DP voting for the single-token generation and efficient privacy budget spending by leveraging the sparse vector technique combined with the utilization of LLMs without any relevant documents provided. These two components enable us to generate answers that incorporate external knowledge while guaranteeing reasonable level of differential privacy. We start from our algorithm with the first component alone, and then extend it to include the second component."}, {"title": "DPVoteRAG: Differentially Private Voting Algorithm for RAG", "content": "By the nature of retrieval in RAG-retrieving relevant documents for a question, the LLM outputs can depend on a sensitive individual's document. Therefore, our algorithmic design needs to relax the dependency of a single individual's document on the output, while exploiting the external data source, to achieve a reasonable privacy-utility tradeoff."}, {"title": "DPSparseVoteRAG: Differentially Private Voting Algorithm for RAG with Sparse Vector Technique", "content": "The main drawback of the aforementioned algorithm is that we need to spend a non-negligible amount of privacy budget for each token to guarantee its quality. This prevents our algorithm to generate longer answers-sometimes it can halt before it generates the actual answers due to privacy budget shortage. More concretely, consider the following question-answering example:\nQuestion: what type of literature is the great gatsby\nGround Truth Answer: novel\nHere are possible outputs from (non-private) RAG and our DPVoteRAG given the retrieved documents.\nRAG Output: The Great Gatsby is a novel written by American author F. Scott Fitzgerald.\nDPVoteRAG Output: The Great Gatsby is a\nWhile non-private RAG correctly answers the question, due to the pre-fixed total privacy budget, DPVoteRAG can only output 5 words and thus it fails to output the ground truth answer, novel.\nHowever, having a closer look to our voting algorithm, we observe that there is a room for improvement. When generating the 3rd word, Gatsby, every input of the LLM contains The Great, 1st and 2nd previously output words, and the great gatsby, a part of the question, even though the provided retrieved documents are different. Thus, the LLM should successfully generate Gatsby without access to the sensitive information. Ideally, we should not spend a privacy budget for such a word.\nWe address this by incorporating the sparse vector technique into our voting algorithm, yielding our improved algorithm DPSparseVoteRAG. In particular, before we apply private voting among generated tokens, we check if the generated tokens coincide with the token generated by the LLM without retrieved documents appended, i.e., the input is composed of the prompt and previously generated tokens only. We continue to the voting only when they do not coincide. Otherwise, we use the LLM output without retrieved documents. It is shown from the analysis of the sparse vector technique that the consumed privacy budget scales with the number of times that it uses the private voting, not with the total number of generated tokens. Consequently, the resulting algorithm, shown in Algorithm 2, enables us to spend privacy budget only when it needs sensitive information."}, {"title": "Experiment", "content": "We investigate how our differentially private voting RAG algorithms (Algorithms 1 and 2) work. Specifically, we ask the following questions:\n1. How do our algorithms improve the accuracy of question-answering over non-RAG LLM while ensuring a formal privacy guarantee?\n2. Is DPSparseVoteRAG (Algorithm 2) always a better choice than DPVoteRAG (Algorithm 1)?\n3. Is there any useful guidance of choosing hyperparameters m (the number of voters) and \\(\\epsilon_{\\text{token}}\\)?\nWe study each question through extensive evaluations on the well-used benchmarking datasets with multiple LLMs."}, {"title": "Methodology", "content": "Datasets. We use two question-answering benchmarking datasets for RAG: Trivia [18] and Natural Question (NQ) [21]. Each dataset consists of a list of pairs of question and answer lists, i.e., every question can have multiple answers. By following the standard evaluations in RAG [5, 23, 16], we use the Wikipedia dataset as the external data source from which a retriever finds relevant documents. For each dataset, we use a subset of 100 questions to manage the computational overhead 4.\nModels. The retriever we use is the Dense Passage Retriever (DPR) [20] which is built on top of BERT [6]. The DPR finds relevant documents that are close to the question query in the embedding space produced by BERT. We compare the following generator LLMs: OPT (1.3B) [41], GPT2-XL [31], and Pythia (1.4B) [3]. We additionally report the result of OPT (2.7B) in Appendix A.1.\nAlgorithms. We compare our algorithms, DPVoteRAG (Algorithm 1) and DPSparseVoteRAG (Algorithm 2), with two baseline algorithms. One baseline algorithm is Non-RAG where we only provide a question to the LLM without any relevant documents appended as a prompt. In order for our algorithms to be useful, they have to outperform this baseline. The other is VoteRAG where we carry out the same voting procedure as our algorithms but choose the most frequent token across voters non-privately-the most frequent token is always chosen as the next token to generate. For each number of voters, the result of this baseline serves as the upper bound of our DP algorithms.\nExperimental Setup. We observe the results under multiple total privacy budgets, (\\(\\epsilon_{\\text{total}}\\), \\(\\delta_{\\text{total}}\\)). More specifically, we sweep \\(\\epsilon_{\\text{total}}\\)= 2 to 40 and set \\(\\delta_{\\text{total}}\\)= 10\\(^{-4}\\). Furthermore, we consider different per-token privacy budgets for our private algorithms: \\(\\epsilon_{\\text{token}}\\)= 1,2, 5 and \\(\\delta_{\\text{token}}\\)= 10\\(^{-5}\\). We consider the number of voters m of 10, 20, 30, 40, and 50 for VoteRAG, and 30, 40, and 50 for DPVoteRAG and DPSparseVoteRAG so as to ensure reasonable privacy-utility tradeoff and computational overhead. Too small number of voters yields too large \\(\\epsilon_{\\text{token}}\\to\\) generate accurate answers. Increasing the number of voters not only increases computational overhead but also requires a lot of relevant documents which eventually include non-relevant ones, resulting in poor utility. For DPSparseVoteRAG, we set the threshold T to be the half of the number of voters, i.e, \\(\\tau\\)= m/2. When we use the LimitedDomain mechanism to privately choose the most frequent token, we set their parameter k to be the number of voter, where k is the limited size of the domain to which we add the Gumbel noise. For voting algorithms, each voter receives 1 relevant document from corresponding subset of external knowledge dataset D. The utility evaluation metric is the match accuracy [25, 2, 33, 42] which measures if the prediction to a question contains any of its answers."}, {"title": "Results", "content": "Comparison of algorithms. Figure 3 shows the average match accuracy of baseline algorithms, Non-RAG and Vote-RAG, and our algorithms, DPVoteRAG and DPSparseVoteRAG for different total privacy guarantees (\\(\\epsilon_{\\text{total}}\\)).\nWe first observe that VoteRAG mostly improves the utility compared to Non-RAG suggesting the benefit of RAG for knowledge-extensive question-answering tasks. The only exception is the one with Pythia on Trivia\u2014VoteRAG and Non-RAG perform comparably. We anticipate this is because Pythia is trained on Wikipedia passages and memorizes them well. Furthermore, across different datasets and generator LLMs, we observe that DPSparseVoteRAG outperforms Non-RAG mostly under \\(\\epsilon_{\\text{total}}\\ge 10\\) when VoteRAG is better than Non-RAG. On the other hand, DPVoteRAG requires larger total privacy budget to outperform Non-RAG or can even perform worse than Non-RAG. Comparing the utility between our algorithms, we see DPSparseVoteRAG always yields better results than DPVoteRAG. This suggests the importance of the smart privacy budget usage offered by DPSparseVoteRAG.\nEffects of hyperparameters. We next investigate the effects of hyperparameters of our algorithms by taking a closer look at the results in Table 1 with OPT (1.3B) on Trivia dataset under different total privacy budgets \\(\\epsilon_{\\text{total}}\\)."}, {"title": "Discussion", "content": "Our RAG algorithms boost the QA accuracy even under a formal privacy guarantee. Our thorough empirical evaluation has revealed that our algorithms improves the utility of question-answering tasks over the non-RAG baseline by exploiting the external knowledge through RAG while ensuring a reasonable level of privacy \\(\\epsilon_{\\text{total}}\\)= 10. While this is mostly the case, we also observe that even (non-private) VoteRAG does not improve the utility on Trivia with Pythia. This observation implies that RAG and its private versions need to use truly sensitive data, which is completely outside the pre-training and fine-tuning data of LLMs, to be valuable enough. On the flip side, we anticipate our algorithms are even more effective for more involved tasks which heavily require the external data source for LLMs to generate meaningful responses as it is expected non-RAG LLMs perform poorly for such tasks.\nDPSparse VoteRAG is strictly better than DPVoteRAG. We further find that DPSparseVoteRAG is consistently outperforming DPVoteRAG across different LLMs and datasets. DPSparseVoteRAG augments DPVoteRAG by utilizing the non-RAG LLM and the sparse vector technique so that it only spends a privacy budget for an output token requiring sensitive external knowledge. The consistently better performances of DPSparseVoteRAG suggest the importance of separately treating token generations for meaningful tokens, i.e., tokens requiring external knowledge, and for other general tokens in the privacy-constraint setting.\n\\(\\epsilon_{\\text{token}}\\to\\) should allow medium-length outputs. m should balance the DP noise and # of well-informed voters. The detailed experimental results across different values of the hyperparameters give us some guidelines on how to set them properly. In particular, the per-token privacy budget, \\(\\epsilon_{\\text{token}}\\), should be small enough to allow the algorithms to generate a reasonably large number of tokens (\\(\\sim\\) 10). As far as the resulting sequence is long enough, we should certainly make it as large as possible to enable accurate token generations. Furthermore, the number of voters, m, should be set by considering per-token generation quality. For a strict per-token privacy budget, e.g., \\(\\epsilon_{\\text{token}}\\)= 1, it needs to be large enough to relax the impact of the DP noise to the vote result histogram. For larger per-token privacy budgets, it should be small enough to ensure the majority of the voters receives useful external documents as the context."}, {"title": "Related Work", "content": "Privacy-preserving algorithms in large language models. [40] proposed an empirical privacy-preserving algorithm for RAG through the synthetic data generation, while our work studies privacy-preserving RAG in the framework differential privacy, which protects the privacy of each individual documents with the theoretical guarantee. Differential privacy has been studied in many other tasks in large language models too. Prompt tuning helps tailor the LLM to new tasks from a (private) test-domain dataset. [15] and [7] study the DP mechanism on two different prompt tuning frameworks: prompt optimization and offsite prompt tuning [34]. In-context learning adapts to different tasks by illustrating some examples in the context as the task description. DP in-context learning consider the situation when the examples are picked from any private set. [35] tackles this problem by generating synthetic examples with DP and [37] solves the DP test-query by generating the answers, both in a sample-and-aggregate fashion. The differentially private pretraining and finetuning of LLMs has been studied to address the privacy concern in the training data and memory is a large bottleneck when naively deploying DP-SGD [1]. [24] focuses on the pretraining stage which introduces ghost clipping to make DP-SGD more memory efficient. [38] explores finetuning in the parameter-efficient framework LORA [17]. Notice that DP voting plays a crucial role in these sample-and-aggregate algorithms, including ours. A basic approach is to apply the Laplacian or Gaussian mechanism [12]. [27, 28] proposed a data-dependent privacy analysis, which can be tighter when the majority vote has a large margin over other options. We integrate the LimitedDomain mechanism for our algorithm, which addresses challenges when the voting domain is large [8]; the large vocabulary size in token voting is our main bottleneck.\nComposition in differential privacy. Our algorithms generate the answers token by token, where each token needs a query to the private dataset and consumes some privacy budget. In this paper, we set up the privacy parameters before the start of the algorithm and have a pre-set maximum number of token to generate. However, the number of tokens to generate is different per question and is unknown before the algorithm starts \u2013 it is possible that the number of generated tokens is much smaller than the pre-set number but we still need to pay the full pre-defined privacy cost. A line of work [32, 14, 22, 36] tries to measure the privacy budget in fully adaptive composition where the budget consuming can interact with the data. Especially, [36] gives an analysis for this fully adaptive setting which matches the tightness of advanced composition. The idea of fully adaptive composition sounds a fit to our problem, which allows us to \"pay as we go\", rather than predefining the \\(\\epsilon_{\\text{max}}\\to\\) before the generation process. We found the analysis for fully adaptive setting is effective for large number of steps and small budget per step, while in our algorithm the number of generated"}, {"title": "Conclusion and Future Work", "content": "We introduce the first differentially private algorithms for RAG, enabling us to enhance LLMs by domain-specific but sensitive external corpus. With our novel combination of the DP voting algorithm and sparse vector technique along with the non-private LLM, we succeed in spending privacy budget only when the LLM needs sensitive information to generate a new token. Consequently, DPSparseVoteRAG successfully generates a sufficiently long and accurate response under a reasonable privacy budget. Our experimental results demonstrate that our algorithms perform better than the non-RAG baseline across different datasets and models, showing their effectiveness.\nOne of our future directions is to conduct more practical empirical evaluations. The Wikipedia dataset, which we use as the external data source, is typically included in the training data of recent LLMs. RAG is particularly effective when the external knowledge is truly sensitive and thus outside the LLM training data. It is essential to conduct evaluations that are as close to the real situation. Since our usage of the sparse vector technique is applicable to any DP token generation algorithm through voting, another future direction would be to examine how it improves DP token generation across different tasks, e.g., in-context learning and prompt tuning."}, {"title": "Additional Experimental Results", "content": "Main Results across Models and Datasets\nIn Figure 4, we present the average match accuracy of baseline algorithms and our algorithms for different total privacy guarantees (total) with OPT (2.7B). We see the similar trend observed in Figure 3.\nFor completeness, we further present the detailed results, in the same form of the one provided in Table 1, with OPT (1.3B and 2.7B), GPT2-XL and Pythia (1.4B) on Trivia and NQ datasets in Tables 2\u20138."}, {"title": "Further Results on Trivia Dataset with OPT (1.3B)", "content": "Effects of Number of Ground Truth Relevant Documents. Figure 5 shows the performances for different numbers of ground truth relevant documents. We see questions with more relevant documents tend to be answered correctly across algorithms, even with Non-RAG. We also observe that the benefit of having more voters (larger m) is larger for small \\(\\epsilon_{\\text{token}}\\). Interestingly, the benefit is significant when there are less relevant documents for DPVoteRAG and when there are more relevant documents for DPSparseVoteRAG. We don't see significant difference between m = 30 and 50 when \\(\\epsilon_{\\text{token}}\\)= 5.\nNumber of Generated Tokens. Figure 6 shows the numbers of tokens generated by DPVoteRAG and DPSparse- VoteRAG. As we expect by the design of DPSparseVoteRAG, we see DPSparseVoteRAG generates much more tokens"}]}