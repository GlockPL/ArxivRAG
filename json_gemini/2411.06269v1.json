{"title": "AI's Spatial Intelligence: Evaluating AI's Understanding of Spatial Transformations in PSVT:R and Augmented Reality", "authors": ["Uttamasha Monjoree", "Wei Yan"], "abstract": "Spatial intelligence is important in many fields such as Architecture, Engineering, and Construction (AEC), Science, Technology, Engineering, and Mathematics (STEM), and Medicine. Understanding three-dimensional (3D) spatial rotations can involve verbal descriptions and visual or interactive examples, illustrating how objects move and change orientation in 3D space. Recent studies show Artificial Intelligence (AI) with language and vision capabilities still face limitations in spatial reasoning. In this paper, we have studied advanced generative AI's spatial capabilities of understanding rotations of objects in 3D space utilizing its image processing and language processing features. We examined the spatial intelligence of the GPT-4 model with vision in understanding spatial rotation process with spatial rotation diagrams based on the Revised Purdue Spatial Visualization Test: Visualization of Rotations (Revised PSVT:R). Furthermore, we incorporated an added layer of a coordinate system axes on Revised PSVT:R to study the variations in GPT-4's performance. We additionally examined GPT-4's understanding of 3D rotations in Augmented Reality (AR) scene images that visualize spatial rotations of a physical object in 3D space and observed increased accuracy of GPT-4's understanding of the rotations by adding supplementary textual information depicting the rotation process or mathematical representations of the rotation (e.g., matrices) superimposed on the object. The results indicate that while GPT-4 as a major current Generative AI model lacks the understanding of a spatial rotation process, it has the potential to understand the rotation process with additional information that can be provided by methods such as AR. AR can superimpose textual information or mathematical representations of the rotations on spatial transformation diagrams or images and create a more intelligible input for Al to comprehend or for training AI's spatial intelligence. Furthermore, by combining the potentials in spatial intelligence of AI with AR's interactive visualization abilities, we expect to offer enhanced guidance for students' spatial learning activities. Such spatial guidance can greatly benefit understanding spatial transformations and additionally support processes like assembly, construction, fabrication, manufacturing, as well as learning in AEC, STEM, and Medicine that require precise 3D spatial understanding and instruction.", "sections": [{"title": "1. Introduction", "content": "Spatial visualization skills refer to the ability to mentally rotate, manipulate, twist, or invert 3D objects [1], [2]. Education in many STEM disciplines demands instructional approaches that offer experimental practices and tasks with high representational accuracy and realistic simulations to enhance the learning experience [3]. Many first-year undergraduate STEM students struggle with visualization and drawing tasks, even though they have access to various computer assisted learning resources for geometry, spatial transformations, and related mathematical concepts [4]. These difficulties include understanding the symbolic notation of linear algebra, generalizing geometric reasoning, and transitioning to matrix representations of transformations, which often weakens their intuitive grasp of the concepts [5], [6]."}, {"title": "", "content": "Despite studying drawing in secondary school, many students have not sufficiently developed their spatial abilities, leading to significant challenges in mentally manipulating figures in space [7]. Personalized STEM education using interactive technologies can enhance learning experiences. It can promote engagement through hands-on simulations and immersive environments and provide real-time feedback to improve understanding of complex concepts [3]. By leveraging tools such as augmented reality (AR), virtual labs, and Artificial Intelligence (AI)-driven assistance, students can explore STEM topics at their own pace and develop critical problem-solving skills in an interactive, engaging way [8]. Generative AI, encompassing technologies that can create new content such as text, images, 3D models, and audio, have rapidly evolved in recent years, driven by advancements in deep learning and neural network architecture. Large language models (LLMs) such as GPT-4 and its successors have demonstrated remarkable proficiency in generating coherent and contextually relevant text. GPT-4 with Vision - GPT-4V(ision) - has capabilities to analyze user-input images [9],[10]. In an extensive study of GPT-4V(ision), evaluations based on many samples demonstrate GPT-4V's unprecedented ability in understanding visual inputs, spatial relationships of humans and objects, and the genericity of its capabilities [11]. Augmented Reality (AR) can overlay digital information on the real world and create a more perceivable representation for AI. The use of AR-based technologies can reduce the cognitive load of users required for spatial learning by superimposing virtual instruction on the real world. AR has three distinguishing features: combining real and virtual; interactive in real-time; and registered in three dimensions [12]. Using AR in education has several positive impacts: increased content understanding, learning spatial structures, language associations, long-term memory retention, improved collaboration, and motivation [13]. GPT-4's visual capabilities are highly relevant in AR, virtual design, and other applications that require real-time understanding of visual input [14]. For instance, it can assist in designing objects in 3D space[15], guide users through augmented experiences, or help in manipulating visual content interactively [16]. Combined textual and graphical information superimposed in real world utilizing AR technology has the potential to generate a more intelligible input for GPT-4's visual capabilities. Using conversational AI in education is one of the major approaches to enhancing and promoting a more personalized learning experience [17]. Personalized interactive assistance of generative conversational AI that has visual capabilities along with spatial qualities of AR can help students achieve better learning outcomes. Understanding AI's ability to perceive rotation in AR is expected to have a significant impact on learning spatial visualization skills. Spatial reasoning is a key area in psychometrics, and several datasets exist for evaluating these skills, such as the Mental Rotation Tests [18], the Purdue Spatial Visualization Test (PSVT)/The Purdue Visualization of Rotations Test [19], and the Revised Purdue Spatial Visualization Test: Visualization of Rotations (Revised PSVT:R) [20]. The Revised PSVT:R is considered one of the most effective tools for evaluating spatial reasoning skills. This test is a widely used assessment designed to measure an individual's ability to visualize and mentally manipulate 3D objects in space, particularly in terms of rotating these objects. We can use both a verbal description and some visual or interactive examples to explain spatial rotations and convey how objects move or change orientations in 3D space [21]. Combining the visual and verbal abilities of conversational AI and visual and interactive abilities of AR has the potential to provide enhanced guidance in spatial rotation activities [8]. This type of spatial guidance can help mathematical education as well as govern assembly, construction, and fabrication process that requires spatial instructions. The goal of this study is to evaluate Al's capability of understanding 3D spatial rotations by performing a series of"}, {"title": "", "content": "tests based on Revised PSVT:R and an interactive AR application (AR-Classroom) that can visualize rotation along different axes [22]. The research questions that guide this study are as follows:\nCan GPT-4 answer questions in Revised PSVT:R and how its performance compare with human test takers?\nCan GPT-4 perceive a spatial rotation process created based on Revised PSVT:R and describe the process in terms of axis of rotation, angle of rotation, and (clockwise or counterclockwise) direction of rotation?\nCan GPT-4 perceive a spatial rotation process visualized in AR and describe the process in terms of axis of rotation, angle of rotation, and (clockwise or counterclockwise) direction of rotation?"}, {"title": "2. Literature Review", "content": ""}, {"title": "Dual-coding theory", "content": "The dual-coding theory is defined as a theory that posits the existence of two interconnected subsystems in the human cognitive system: a verbal system and an imagery system [23]. This dual capacity means the language system functions in both communication and symbolic interpretation. Human cognition uniquely integrates both language and nonverbal objects and events, enabling us to process linguistic input (like speech or writing) alongside symbolic representations of nonverbal aspects of the world [24]. Therefore, spatial representation should integrate these two systems, where language serves both direct communication and a framework for understanding nonverbal phenomena. Integrating sophisticated language and image processing and response generation of Artificial Intelligence (AI) with immersive three-dimensional capabilities of AR can facilitate a more engaging spatial learning experience [25]. Including Augmented Reality (AR) in the learning process can improve spatial abilities in the domain of architecture education [26]."}, {"title": "AI Vision Model", "content": "The latest development of GPT-4, a large-scale, multimodal model, can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 Vision can analyze images, provide detailed descriptions, identify objects, and offer explanations of scenes [9]. For instance, it can describe an uploaded photo or explain elements within the image, such as identifying objects like plants, vehicles, or people, and providing contextual information like plant care advice or understanding specific items in complex scenes [27]. AI with vision and spatial understanding have broad applications, for example, in STEM and Architecture/Engineering/Construction (AEC). Integrating GPT AI models into construction safety management enhances risk assessment, real-time insights, incident reporting, and compliance evaluation [28], e.g. the Personal Protective Equipment (PPE) or helmet usage compliance [11]. Experiments have proven GPT-4V to be highly effective in detecting and explaining global and fine-grained semantic patterns in anomaly or risk detection [29], [11]."}, {"title": "Al's Spatial and Mathematical Abilities", "content": "Spatial intelligence is described as a skillset that helps us comprehend visual-spatial tasks and spatial relations. As a result, we gain a better orientation of objects in space [30]. In recent years there have been several studies analyzing spatial and mathematical capabilities of AI. GPT-4 model \u201challucinates\", wherein the model returned inaccurate information. The model was unable to accurately return bounding boxes for object detection, suggesting it is not completely fit for computer vision related usage [31]. The SPARE3D dataset introduces a series of challenging 2D-3D spatial reasoning tasks, revealing that while convolutional neural networks (CNNs) excel in visual learning, their performance on spatial reasoning remains below human average, highlighting the need for advancements in artificial neural network design to improve 3D spatial reasoning for intelligent robots [32]. A recent study has examined the perceived complexity of 3D\""}, {"title": "", "content": "shapes from both human and ChatGPT perspectives, revealing that ChatGPT can effectively capture human consensus on shape complexity. The findings suggest that large generative models like ChatGPT could be used to automatically generate 3D content with varying complexities, which can be enhanced for spatial visualization skills in VR applications [14]. This research explored how the visual features of some of the 3D shapes used in the Purdue Spatial Visualization Test are perceived by ChatGPT. [33] introduces a novel pipeline for enhancing the geometric capabilities of Multimodal Large Language Models (MLLMs) and Wolfram (a math tool) by generating a dataset of 4.9K simplified, image-text- matching geometric problems combined with 19K open-source data to create a uniformly challenging dataset, GeoGPT4V. Training models on GeoGPT4V has demonstrated improved geometric performance on MathVista and Math Vision benchmarks. [34] conducted a pilot study to explore the potential of using ChatGPT to create thematic and mental maps by utilizing both public geospatial data and textual descriptions of geographic space. They evaluated ChatGPT's performance on a GIS (Geographic Information System) exam, comparing GPT-3.5 and GPT- 4, to assess their strengths and limitations in understanding geospatial concepts and discovered challenges in teaching spatial reasoning to AI models. While ChatGPT shows promise in lowering the barrier for map production, enhancing efficiency, and demonstrating spatial thinking capabilities, it still faces limitations such as unequal user benefits and reliance on user intervention for maintaining quality control [35]. ChatGPT-4 and ChatGPT 40 struggle more with geometry and complex math problems compared to algebra, suggesting that while AI can support algebra instruction, teacher intervention is needed in areas where Al is less effective, such as spatial intelligence and higher-difficulty tasks [36]. After an elaborate study, [37] observed LLMs resemble sophisticated pattern matching more than true logical reasoning. LLM performance varies highly on different versions of the same question. LLMs' performance drops substantially with a minor increase in difficulty, and their sensitivity to inconsequential information indicates that their reasoning is fragile. Despite many positive media reports highlighting GPT-4 and ChatGPT's exam-solving abilities, possibly influenced by a selection bias, their overall mathematical performance falls significantly short of a graduate student's level [38]. Educators can leverage AI to create engaging and effective learning environments, cultivating a generation skilled in mathematics and prepared to navigate the complexities of the modern world by addressing the associated challenges and adopting a refined approach [39]."}, {"title": "AR and AI in Education", "content": "In recent years, AR and AI are demonstrating the potential to reshape education by enhancing interactivity, personalization, and accessibility. The use of AR technology and speech recognition to support productive vocabulary among students has shown user satisfaction when parents and teachers were interviewed [40]. [41] introduced a novel early education platform that combines smart voice commands and AR technology. The platform is embedded in a storage box, making it convenient for use in homes and kindergartens where preschool education is provided. This project enhances the integration of these technologies in educational settings. Using AR-powered embodied learning for spatial transformations and representation of corresponding matrices has already shown significant improvement in the participants' math scores [42]. The project AR-Classroom uses embodied learning and novel AR features to visualize spatial rotations and their mathematical representations, and the usability tests that were completed using the application showed promising results. The benchmark test and updated usability test done on 24 participants have shown significant improvement in usability scores [32]. The AR-Classroom application integrates AR and AI- based multi-sensory assistance to help undergraduate students better understand spatial transformations and their mathematical representations. A usability study identified four key themes: ease of use, the need for"}, {"title": "", "content": "more detailed Al responses, a desire for visual information, and improved understanding, highlighting the potential for further AI-guided visual cues in AR learning environments [8]."}, {"title": "4. Methodology", "content": "When explaining rotation, we use references like direction, angle, and axis verbally and visually. According to the dual-coding theory, two cognitive subsystems, verbal and imagery, work together to process language and nonverbal content. Integrating these systems has the potential to enhance spatial understanding, where language aids in communication and interpreting nonverbal cues. LLMs rely on descriptive datasets and visual cues, which can enhance spatial descriptions in vision models. Using visual cues to communicate a rotation process could improve interactions between AR and AI systems, providing a more intuitive tool for conveying spatial information, thus facilitating spatial learning. This approach could bridge the gap between text-based AI and spatially oriented AR, making explanations of spatial transformations more accessible and effective. Combined AR and AI have the potential of becoming instrumental in assembly and fabrication, offering real-time guidance, error reduction, and efficiency improvements. In this paper, we examine generative Al's vision and language capacity of understanding and describing spatial rotation using standard test (Revised PSVT:R) vs AR. Our evaluation focused on the Al's capacity to comprehend key aspects of spatial rotations, including the axis, angle, and direction of each rotation, as well as its ability to process sequences of multiple rotations. We integrated multiple layers of graphical and textual features into the Revised PSVT:R to analyze their impact on the performance of GPT-4 (ChatGPT 40 or GPT-40). We also tested GPT-4's capability in an AR application that visualizes the spatial rotation of a hand-held object in 3D space and tested the accuracy with the addition of supplementary textual information (angle, textual information, rotation matrix equation etc.)"}, {"title": "4. Experiments and Results", "content": ""}, {"title": "Experiment 1: Evaluate GPT-4 Using Revised PSVT:R (original and with added coordinate axes)", "content": "The Revised PSVT:R is a widely used test for assessing spatial visualization skills, consisting of rotation problems. In this test, participants are shown diagrams of 3D objects in both a starting and finishing orientation after an unknown rotation along one or more axes. The task is to identify the correct diagram of a second 3D object that has undergone the same rotations as the first. Participants are given the second 3D object in its starting orientation and must choose the correct finishing orientation from five possible options."}, {"title": "Test Using Revised PSVT:R (Test 1a)", "content": "Using ChatGPT 40 (with a vision capability) from June 9, 2024 through Fall 2024, we conducted various tests. In the first test, we provided GPT-4 with all the 30 problems from Revised Purdue Spatial Visualization Test: Visualization of Rotations (Revised PSVT:R) as a standard test to study its ability to analyze input images and spatial rotation capabilities (Figure 1a). After getting the input (a screenshot of the test questions) uploaded, GPT-4 responded with answering the questions by choosing from (A, B, C, D, and E options). Comparing GPT-4's results with the correct results on Revised PSVT:R, we found that GPT- 4 only got 5 out of 30 answers correct. The 5 correct answers are for Question 10, 15, 16, 19, and 30. (For copyright of Revised PSVT:R and keeping the test results confidential for future tests with participants, the 30 questions and results are not revealed in this paper. An example question in the DIRECTIONS section of the test that is allowed to publish is shown in Figure 1.) See Appendix A for sample interaction between authors and GPT-4 in Test la."}, {"title": "Test Using Revised PSVT:R with Axes added (Test 1b)", "content": "After getting unsatisfactory results in the first test, we explored more of its spatial analyzing capabilities by adding a coordinate system with x- y- z-axes onto the Revised PSVT:R question images. The motivation behind this is incorporating more visual and textual cues for analyzing images and enhancing communication with AI. We superimposed the coordinate system on 30 question images of Revised PSVT:R where x-, y-, and z-axes are indicated (Figure 1b). See Appendix B for sample interaction between authors and GPT-4 in Test 1b."}, {"title": "", "content": "In Experiment 1, both Test la and 1b, GPT-4 had the most complete task to identify the correct option from the five 3D object orientations (A to E). To execute the whole task, like a human test-taker, GPT-4 needs to identify the rotation process in Step 1 and then compare it to the five rotation processes in Step 2, in order to choose the correct option (Figure 2). For example, in Figure 2, to answer correctly, one should identify the similarities between the axis systems between Step 1 and Step 2, and choose option D. In Test 1a, GPT- 4 gave correct answers only 5 out of 30 questions which reflects 17% accuracy (Figure 3). In Test 1b, GPT- 4 had a similar accuracy, which indicates that it is unable to identify and compare rotation processes correctly even with the visual cues (axes) added."}, {"title": "", "content": "Experiment 2: Evaluate GPT-4 Using Step 1 of Revised PSVT:R with a Coordinate System (Test 2)\nGPT-4 showed an unsatisfactory result in answering the questions of Revised PSVT:R in Experiment 1. To simplify the tasks for AI, we continued testing with only Step 1 of the questions of Revised PSVT:R (Figure 3) to see if AI can identify the rotations of one object in each question. We superimposed a coordinate system with axes on Step 1 of each question of Revised PSVT:R, and asked GPT-4 to explain the rotation process with minimum rotation steps (Figure 4). The resulting orientation in 14 questions of Revised PSVT:R can be achieved by rotating along a single axis and in the rest 16 questions can be achieved by rotating along multiple axes (e.g., rotating around one axis and then around another axis). As the resulting orientation of a 3D object can be achieved using different sequences of multiple rotations, we asked GPT- 4 to describe the change of orientation using rotations along a minimum number of axes. We used the prompt, \"Explain the rotation process using minimum steps of rotation in terms of axis of rotation, angle of rotation, and direction of rotation.\u201d GPT-4 performed better in case of the Step 1 rotation process than"}, {"title": "", "content": "in Experiment 1 with both Step 1 and 2. See Appendix C for sample interaction between authors and GPT- 4 in Test 2. In this test, GPT-4 performed better in case of rotation along a single axis than rotations along multiple axes (Figure 5). In case of a single-step rotation, it was correct 5 out of 14 times reflecting 35% accuracy, in case of rotations along multiple axes, it was correct 2 out of 16 times showing 12.5% accuracy. The combined accuracy of this test is 23.33%. The performance in case of rotation along a single axis is almost 2 times more accurate than the rotations along multiple axes, though in both cases performance by GPT-4 is unsatisfactory. This experiment results indicate that even though GPT-4 cannot compare similar rotation processes, it is somewhat capable of understanding one rotation process. The information we provided is not sufficient for GPT-4 to understand the rotation process. However, there may still be a possibility of achieving better results by adding supplementary information and context."}, {"title": "Experiment 3: Evaluate GPT-4 on Understanding Rotations with Supplementary Information through an AR Application", "content": "AR can superimpose 3D models, data, or text over real-world objects, enhancing the user's understanding of their surroundings and the physical world. An AR application named AR-Classroom provides an interactive learning environment with better context and visualization for understanding rotations (Figure 6) [22]. The app aims to run on an AR device, such as an AR-enabled tablet, an AR headset, or a computer with a camera. A user can hold and manipulate a 3D physical model to learn spatial transformations by observing the corresponding mathematical representation and visualization that augment the 3D physical model with a graphical user interface (GUI) in AR."}, {"title": "Test Using AR with a Coordinate System (Test 3a)", "content": "We uploaded to GPT-4 with 12 screenshots of the AR environment visualizing only the physical and virtual models and the coordinate system. The angle of rotation and corresponding rotation matrix equation were hidden (Figure 7a). All screenshots varied in axis of rotation, angle of rotation, and direction (clockwise or counterclockwise) of rotation. We provided the prompt, \u201cExplain the rotation process with axis of rotation, angle of rotation and direction. Note: All red lines represent X-axis, all yellow-green lines represent Y-axis, all blue lines represent Z-axis. The dotted line is the orientation of axis after rotation. The axis of rotation does not have a dotted counterpart of the same color. Follow right hand rule for the direction of rotation.\u201d"}, {"title": "Test Using AR with a Coordinate System and Angle of Rotation (Test 3b)", "content": "We uploaded to GPT-4 with 12 screenshots of the AR environment visualizing the physical and virtual models, the coordinate system, and angle of rotation. The corresponding rotation matrix equation was hidden (Figure 7b). All screenshots varied in axis of rotation, angle of rotation, and direction of rotation. We provided the same prompt as in Test 3a."}, {"title": "Test Using AR with Coordinate System, Angle of Rotation, and Rotation Equation (Test 3c)", "content": "We uploaded to GPT-4 with 12 screenshots of AR-Classroom where the coordinate system, rotation angle, and corresponding rotation matrix equation are all visible (Figure 7c). We provided the same prompt as in Test 3a and GPT-4 gave very satisfactory results each time we tested, as explained below. Furthermore, even with a simpler prompt \u201cExplain the rotation process using minimum steps of rotation in terms of axis of rotation, angle of rotation, and direction of rotation\u201d, the results are as accurate as the more detailed prompt in Test 3c."}, {"title": "", "content": "In this experiment, where we tested different versions of AR-Classroom images, we observed that the performance of GPT-4 gets better when more descriptive texts are added to the visual input. AR rotation with only the coordinate system has 25% accuracy (Test 3a), and AR rotation with a coordinate system and an angle of rotation (Test 3b) has 75% accuracy. When we provided the whole UI of AR-Classroom (AR with a coordinate system, an angle, and rotation equation) as input, the additional information related to the rotation process helped GPT-4 gain 100% accuracy (Test 3c). Here, GPT-4 could identify and connect the rotation matrix equation with the rotation activity, and it described the rotation process correctly. We can see from Table 1 or Figure 8 that making the angle of rotation and the rotation matrix equation visible in"}, {"title": "", "content": "the AR scene each has increased the accuracy significantly. See Appendix D for sample interaction between authors and GPT-4 in Test 3c."}, {"title": "5. Discussions", "content": "In GPT-4, the language model tends to be more accurate for purely text-based tasks, while the vision model is better suited for multimodal tasks involving both images and text [43]. GPT-4V integrates image understanding with text, making it powerful for multimodal tasks like interpreting images alongside text descriptions. This model performs well in visual question answering, image-to-text tasks, and recognizing objects in images. However, its accuracy for purely image-based tasks, like object detection or more complex image analysis, may not match specialized vision models and still relies heavily on textual inputs to guide its performance. This supports what we observed in our experiments: GPT-4 gave inadequate performance in case of Revised PSVT:R (Test la) with a score of 17% (5 out of 30). This indicates that it cannot identify complex rotation tasks and cannot identify the similarities between two identical rotation processes with different objects. The cases when it got the right answers could be coincidental. As a comparison, in 1993, a PSVT:R was administered to 535 first-year engineering students at Michigan Tech during orientation. Approximately 20% of the students (n = 96) scored 60% or less on the PSVT:R, while 80% of the students scored above 60% accuracy [44]. In another study [20] using Revised PSVT:R, the mean score of 1,022 participants was 19.08 out of 30 with a standard deviation of 6.16. Scores ranged from 3 to 30, and 13 participants (1.27%) achieved a perfect score."}, {"title": "", "content": "Furthermore, providing a 3D coordinate system with the original Revised PSVT:R did not show significant improvement (Test 1b). Test 2 (Experiment 2) showed that there is an improvement of GPT-4 in recognizing Step 1 only in Revised PSVT:R or a simplified rotation process. In the AR environment, we provided better context than the previous experiments. GPT-4 could see pre- image before rotation and image after the rotation in the AR environment (Test 3a). By increasing textual input step by step in the AR environment, we observed improved performance by GPT-4 (Figure 8). Just including the angle of rotation with the coordinate system showed significant improvement in result (Test 3b). Providing a rotation matrix equation as additional information enabled GPT-4 to identify the rotation process correctly (Test 3c). Even with a less detailed prompt and same image as Test 3c, it identified the rotation process correctly. The use of the AR environment is for the convenience of adding accurate rotation information to the testing for GPT-4. A drawing style of PSVT:R may be used in a similar evaluation process. However, the rendering mode (realistic 3D physical model, wireframe virtual model, colors of rotation arc and angle values, etc.) may have affected the understanding of rotations by GPT-4. Overall, this result points out that GPT-4 shows limited spatial intelligence in terms of understanding 3D rotations with 2D image inputs. For applications, it is very promising for future development of AR applications in the education and STEM sectors, where GPT-4 can guide a user on task performance by taking real-world data and relevant, superimposed textual and graphical data as input. In an AR setting, users can view physical objects, diagrams, or virtual models while AI provides additional information layered over the scene. For example, in a geometry lesson, a 3D model of a shape could be displayed, and GPT-4 could guide the student by offering real-time feedback on angles, dimensions, or rotation processes, enabling more effective learning. This combination of visual and textual data allows GPT-4 to better understand spatial relationships and provide meaningful insights. The result also indicates that the current generative AI doesn't perform well regarding spatial intelligence without additional context information (e.g., rotation axis, angle, and equation). The context information can facilitate Al for understanding spatial transformations and can help train AI to improve its spatial intelligence, so that in the future the context information may not be needed any more. A major limitation of the research is that the sample size (number of each test) is small as this is a very early evaluation of spatial intelligence of AI in terms of 3D rotations."}, {"title": "6. Conclusions and Future Work", "content": "Spatial knowledge is critical in fields like architecture, computer graphics, civil engineering, and other STEM disciplines. While generative AI-powered tools can assist students in learning, conversational AI agents still struggle with spatial abilities. AR can enhance these systems by overlaying textual information on spatial diagrams, making the data more comprehensible for AI. This paper investigates GPT-4's spatial capabilities, particularly its proficiency in interpreting 3D rotations, as tested through Revised PSVT:R and rotation images from an AR application. We improvised Revised PSVT:R with additional graphical and textual features to assess if GPT-4's performance changes with supplementary information. GPT-4 has difficulty recognizing complex rotation tasks and identifying the similarities when two rotations are essentially the same but for different objects, as tested in the Revised PSVT:R. In the AR images, where objects are visualized in 3D space, adding supplementary textual and math information significantly improved GPT-4's accuracy. For example, explicitly showing the angle of rotation"}, {"title": "", "content": "increased accuracy from 25% to 75% in one of the tests. Providing the math equation of rotation in the AR application, along with detailed text, allowed GPT-4 to reach 100% accuracy. This suggests that while GPT- 4V performs well in multimodal tasks, it relies heavily on textual cues for better performance. Additional information, as visualized in AR applications, has the potential to enhance the performance of GPT-4. These results indicate that in an AR setting, users can view physical objects, diagrams, or virtual models' additional information layered over the scene while AI can provide real-time feedback observing the user's interaction process. The future work will include more thorough testing with an increased sample size and with different A\u0399 tools, and enhancing the spatial intelligence of AI, so that AI can understand spatial transformations including rotations, translations, scales, perspectives, etc. based on various styles (e.g., real-world photo or drawing styles) of image or video input of arbitrary 3D physical objects, with or without context information of axes, text, math equations, etc."}]}