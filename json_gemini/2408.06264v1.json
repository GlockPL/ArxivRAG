{"title": "Audio Enhancement for Computer Audition - An Iterative Training Paradigm Using Sample Importance", "authors": ["Manuel Milling", "Shuo Liu", "Andreas Triantafyllopoulos", "Ilhan Aslan", "Bj\u00f6rn W. Schuller"], "abstract": "Neural network models for audio tasks, such as automatic speech recognition (ASR) and acoustic scene classification (ASC), are susceptible to noise contamination for real-life applications. To improve audio quality, an enhancement module, which can be developed independently, is explicitly used at the front-end of the target audio applications. In this paper, we present an end-to-end learning solution to jointly optimise the models for audio enhancement (AE) and the subsequent applications. To guide the optimisation of the AE module towards a target application, and especially to overcome difficult samples, we make use of the sample-wise performance measure as an indication of sample importance. In experiments, we consider four representative applications to evaluate our training paradigm, i.e., ASR, speech command recognition (SCR), speech emotion recognition (SER), and ASC. These applications are associated with speech and non-speech tasks concerning semantic and non-semantic features, transient and global information, and the experimental results indicate that our proposed approach can considerably boost the noise robustness of the models, especially at low signal-to-noise ratios (SNRs), for a wide range of computer audition tasks in everyday-life noisy environments.", "sections": [{"title": "1 Introduction", "content": "Computer audition (CA) is one of the most prominent fields currently being revolutionised by the advent of deep learning (DL), with deep neural networks (DNNs) increasingly becoming the state-of-the-art in a multitude of applications, such as the ones discussed in this work: speech command recognition (SCR) [De Andrade et al., 2018], automatic speech recognition (ASR) [Baevski et al., 2020], speech emotion recognition (SER) [Wagner et al., 2022], and acoustic scene classification (ASC) [Ren et al., 2019]. However, these applications are susceptible to different heterogeneities present in real-life conditions. Taking ASR as an example, this may include amongst other within- and cross-speaker variations, for instance, disfluencies, differences in language, and recording devices and setups.\nOne of the most prominent causes that impedes the practical application of CA models is the innumerable types of ambient noises or interference that deteriorate the audio recording, including environmental background noise, interfering speakers, reverberation, etc. The sound of these noises can be stationary or non-stationary, instantaneous or continuous, and can be of different intensities (stable or variable), all of which have audio models confronting diverse and very complex situations. Meanwhile, in practical applications, multiple interfering sources can be present at the same time, each affecting the effectiveness of such audio models to a different extent. Hence, while considerable performance improvements are leading to the continuous adoption of CA modules in several artificial intelligence (AI) pipelines, robustness to noise remains a critical consideration for most of them. This has led to an accompanying rise of audio enhancement (AE) methods, which typically also fall under the auspices of DL [Liu et al., 2021a].\nTo that end, we present a novel framework for general audio enhancement targeted towards increased robustness of different computer audition tasks. In this framework, the cascaded AE and CA models perform two iterative training steps, strengthening the interplay between the different components of a DL pipeline to minimise potential mismatches and benefit from potential synergies. The motivation is that the computer audition task (CAT) model can guide the AE frontend to preserve those signal components that are particularly important for the task at hand; for instance, an AE frontend for ASR might be optimised to improve the intelligibility of the signal, whilst a SER frontend might focus on the preservation of prosody instead, as this property is more important for the identification of emotional information. Contrary to conventional joint optimisation, samples are not treated equally, but we utilise the loss of the target CAT model as an indication of difficulty in order to guide the training of AE towards harder samples.\nWe hypothesise that the proposed training framework utilises the symbiotic and interdependent nature between the AE and CAT models, and thus counter-balances and mutually promotes the models to reach an optimal performance of the entire system. The technique is experimentally assessed using four relevant target CA applications, aiming to cover a broad spectrum from linguistic speech content in the case of automatic speech recognition and speech command recognition, to acoustic speech content in the case of speech emotion recognition, to ambient audio in the case of acoustic scene classification.\nThe remainder of the paper is organised as follows. In Section 3, we provide an overview of our methodology, including the U-Net-based SE model, as well as the different CAT models. Then, we detail the utilised datasets and experiments and report our results in Section 4, before putting said results in perspective in Section 5. Finally, we conclude our work and point towards future research directories in Section 6."}, {"title": "2 Related Work", "content": "At its core, the task of AE aims at the separation of the audio of interest from other interfering sounds, i. e., it aims at the preservation of the target signal while reducing the uncertainties in audio. The unwanted interference can be the result of several phenomena which affect different steps of the typical CA pipeline: a) additive noise, b) reverberation, c) encoding noise, and, d) package loss. From these, additive noise has been most thoroughly studied in previous work, due to its ubiquitous presence in CA applications and its detrimental effects on performance [Spille et al., 2018, Triantafyllopoulos et al., 2019].\nWithin AE, particular attention has traditionally been paid to speech enhancement (SE), as a typical CAT is mostly focused on extracting information from the human voice. ASR, being the flagship task of computer audition, is the primary testbed for most SE methods, with other tasks such as SER and SCR following closely. However, enhancement of audio signals beyond speech is needed in a number of CATs, such as ASC and sound event detection (SED). In contrast to the purpose of speech enhancement, the presence of speech is often deemed as the noise that can considerably affect the identification of surrounding environments [Liu et al., 2020]. To tackle this problem, voice suppression, as another type of audio enhancement task, has the goal to eliminate the human voice from ambient recordings. These contradicting definitions of target audio signal and confounding noise show that a single one-shoe-fits-all solution for AE systems seems rather difficult to achieve.\nUtilising enhancement frontends, i. e., separately developed enhancement modules (typically based on DNNs), can enhance the input for the subsequent CA models, which can explicitly be empowered using data augmentation techniques, such as SpecAugment [Park et al., 2019] or additive noise [Triantafyllopoulos et al., 2019] for their better robustness against expected perturbations. This is typically performed for ASR tasks Weninger et al. [2015], Kinoshita et al. [2020], Sivasankaran et al. [2015], Zoril\u0103 et al. [2019]. However, in practice, such independent enhancement can introduce unwanted distortions and artefacts Iwamoto et al. [2022] in the enhanced audio, yielding limited improvements or even worsen the performance of cascaded ASR models. In order to improve the tolerance to these distortions, the ASR model can be trained based on the enhanced audio, which is sometimes referred to as joint training Wang and Wang [2016], Narayanan et al. [2015].\nWhen optimising the ASR, the parameters of its frontend SE model can be either frozen or trainable. In the case of trainable parameters, the loss of the ASR task backpropagates through the whole combined model, i.e., the cascade of SE and ASR. This leads to a parameter update of the SE model based on the ASR loss Wang and Wang [2016]. However, to have no explicit restrictions on the SE model during the training poses a risk to weaken or even corrupt the SE effect. Considering the training objectives of SE and ASR together, recent work Ma et al. [2021], Chen et al. [2015] frames the task into a multi-task learning problem, where the losses of SE and ASR are typically added up resulting in a combined optimisation. Other noteworthy approaches include the exploitation of more advanced deep learning techniques, such as generative adversarial networks (GANs) for SE Liu et al. [2019], Li et al. [2021], and self-supervised learning (SSL) for ASR Zhu et al. [2022]. The combination of the two losses is commonly scaled by a dynamic factor, which gradually shifts the training focus between the AE and ASR task Kim et al. [2021]. Joint training has also primarily been used in SCR or keyword spotting C\u00e1mbara et al. [2022], Gu et al. [2019], however, it is rarely used in other CA applications. For most cases, the SE module is trained separately and then cascaded to the target CA models for noise reduction, as for SER Triantafyllopoulos et al. [2019], Zhou et al. [2020] and ASC Liu et al. [2020]."}, {"title": "3 Methodologies", "content": "At the core of our methodology we put two hypotheses, which are already partly supported by the literature, but have not yet been validated on a wide range of applications: 1) the enhancement of audio signals, which contain highly relevant information for a given computer audition task, as part of a processing pipeline, can improve the performance on the target task, and 2) a training procedure, which optimises the audio enhancement and the CAT jointly can specialise the audio enhancement module for task-specific signals and therefore lead to better performance on the CAT.\nIn order to further explore the hypotheses mentioned above, we report a set of experiments, based on AE models using a U-Net architecture, to explore several training paradigms, including the joint optimisation of the audio enhancement for the CATS: automatic speech recognition, speech command recognition, speech emotion recognition and acoustic scene classification. Despite the difference in implementation details, the general framework stays the same amongst the different types of applications and is further illustrated in Fig. 1. All data for AE and CATs is resampled to 16 kHz."}, {"title": "3.1 Comparison Methods", "content": "To assess the performance of our proposed iterative optimisation approach, we compare it with a wide range of methods commonly applied in the context of audio enhancement, which will be introduced in the following.\nBaseline\nThe general baseline for all experiments is a CAT-specific model taken from related literature, which is not trained on any noise-specific data and does not use an AE component. The model is not specifically designed for robustness towards noise and we thus expect a noticeable performance drop-off when confronted with noisy data.\nData Augmentation\nIn a first attempt to make the baseline model more robust, we train it on noise-augmented data. For this purpose, we artificially add noise with different SNR ratios to the mostly clean audio recordings. With data augmentation being one of the most common machine learning practices to increase robustness, we expect the model to perform better on the noisy test data, which was generated in the same manner as the train data.\nCold Cascade\nThe simplest training paradigm with an AE component is a cold cascade of U-Net, as described in section 3.3 and CAT-specific model. Cold cascade means in this context that both models are being optimised independently. First, the U-Net is trained to achieve a good AE performance, then, the CAT model is trained based on clean data and then stacked on top of the U-Net.\nCold Cascade + Data Augmentation\nWe further combine the cold cascade and data augmentation approach, i. e., first, the U-Net is trained to achieve a good AE performance, and then, we train the cold cascade architecture with augmented, noisy data. This approach promises decent noise robustness, as the model has previously seen noisy data and it includes a powerful AE component.\nState-of-the-art\nTo further evaluate the effectiveness of our methods against the state-of-the-art, we additionally utilise two recent denoising methods; this we only do for one of the CA tasks (SCR) due to space limitations. Specifically, we use MetricGAN+ [Fu et al., 2021] and DeepFilterNet-3 (DFNet-3) [Schr\u00f6ter et al., 2023]. MetricGAN+ is a bLSTM model trained in generative-adversarial fashion to optimise perceptual losses; the training set is VoiceBank-DEMAND [Valentini-Botinhao et al., 2016]. DFNet-3, on the other hand, follows a two-stage approach with ERB-based enhancement followed by deep filtering to enhance the periodicity of the output signal and has been trained with a multi-spectral loss on DNS-4 [Dubey et al., 2022], which is closer to our current setup (i. e., the noise data partially comes from AudioSet). Both models are used to enhance the noisy mixtures of SCR on which we evaluate the baseline model trained without data augmentation on the original data; they thus simulate the scenario of using an off-the-shelf denoising model before evaluation. This setup is essentially equivalent to Cold Cascade, only this time using different models.\nMulti-Task Learning\nWe finally compare our method to an implementation of multi-task learning, i. e., an optimisation of both the AE task and the CAT at the same time with an additive loss function\n$$L = L_{AE} + L_{CA},$$"}, {"title": "3.2 Iterative Optimisation", "content": "Similar to the concept of multitask learning, the main motivation behind an iterative optimisation approach is a joint view of the two models. At its core, there are two hypotheses: 1) The CA model should always be adapted to the output of the AE model, which might contain residual noise, introduced speech distortions, artefacts, etc. This specialisation to specific characteristics of the AE can be considered as a form of domain adaptation of the CAT, which has long been shown to help alleviate performanceBen-David et al. [2006] 2) the performance of the CAT can be utilised to move the focus of the AE model onto particularly difficult samples. Thereby, we aim at achieving the optimum result of the entire neural system, i. e., the front-end audio processing and the subsequent target applications.\nThe implementation of the iterative optimisation approach is straightforward, yet rarely considered in the literature: first, the optimisation of the AE model involves the loss of the target CAT as a reference to indicate the difficulty of each training sample. This plays the role of sample-level importance to assist the AE component to be biased towards relatively harder samples, for example, those corrupted by more intensive noise. Second, during training for the CAT, the model should process the enhanced audio signal, rather than the completely clean signal, in order to avoid a common performance gap resulting from a cold cascade of the front- and back-end models. However, as long as the AE model is optimised, a more robust CA model needs to be adapted to the enhanced audio. On the other side, a more robust CA model can further assist the optimisation of the AE model by updating the difficulties of new samples. Having this in mind, we hypothesise that both optimisation steps need to be performed iteratively to gradually approach an optimal solution.\nIn order to implement the latter idea, we calculate a weight for each sample in a given batch when optimising the AE model. The weight for each sample i is defined as\n$$w_i = L_{CA}(t_i, \\hat{t_i}),$$\nwith the target $t_i$ and the predicted target $\\hat{t_i}$. The weights therefore give an indication of how difficult a given sample is for the CAT. We choose a linear relationship between the sample weight and the loss for the CAT as the most straightforward implementation, even though other approaches, such as softmax normalisation, are possible. This choice does not add any new hyperparameters, as linear scaling would only affect the training in the same way as changing the learning rate. In practice, we normalise the weights by dividing by their sum within one batch. The loss of the AE component is then defined as\n$$L'_{AE}(x, \\hat{x}) = \\frac{1}{N} \\sum_{i=1}^{N} w_i L_{AE}(x_i, \\hat{x_i}),$$\nwith the noisy inputs $x_i$ and the reconstructed signals $\\hat{x_i}$. In the iterative training paradigm, we alternate with each batch by first optimising the CA system based on the AE output, while freezing the parameters of the AE system, and secondly optimising the AE system according to the loss (3), while freezing the parameters of the CA model in the weights (2). The iterative training augments the interplay between the two models by persistently adapting the CA to the improved SE model while the updated CA can further be used as an indicator to improve the SE model."}, {"title": "3.3 Audio Enhancement Model", "content": "The audio enhancement is based on U-net Ronneberger et al. [2015], Choi et al. [2018], an auto-encoder architecture, operating in the frequency domain, with feed-forward layers that stack the encoder layers to their corresponding decoder layers, as seen in Fig. 2.\nGiven a noisy audio y and its corresponding clean sample x, the noisy sample is converted into a spectrogram Y, using the short-time Fourier transform (STFT). The U-Net estimates a ratio mask Mask(Y), which is then applied to the original noisy input to predict the clean spectrogram:\n$$\\hat{X} = Y \\cdot Mask(Y).$$ (4)\nThe estimated clean audio $\\hat{x}$ can then be reconstructed by applying inverse STFT. The parameters of the model are optimised by minimising the weighted SDR (WSDR) loss of the original and the estimated clean speech and noise Choi et al. [2018]:\n$$L_{AE}(x, \\hat{x}) = \\alpha LSDR(x, \\hat{x}) + (1 - \\alpha) LSDR(n, \\hat{n}),$$\nwhere\n$$n = y - x \\text{ and } \\hat{n} = y - \\hat{x}$$\nrepresent the true and estimated noise signal, and\n$$LSDR(x, \\hat{x}) = \\frac{\\langle x, \\hat{x} \\rangle}{\\|x\\| \\|\\hat{x}\\|},$$\nas well as\n$$\\alpha = \\frac{\\|x\\|^2}{\\|x\\|^2 + \\|n\\|^2}.$$ (7)\nIn order to capture the advantages of enhanced audio signals from U-Net some slight architectural changes need to be applied in order to make it compatible in a cascading fashion with any of the above-mentioned application scenarios. For this purpose, we set the max-pooling along the time-axis equal to 1, while the pooling along the frequency-axis stays unchanged. The main motivation of this step is to allow the U-Net to process audio segments of different lengths, which is a crucial ability for some of the application tasks, like for instance ASR.\nAny of the considered CAT can do further processing, like feature extraction based on the enhanced waveform, as it would normally be done on the original waveform. Alternatively, the reconstructed time-frequency features of the AE model can directly be passed on. In a cold cascade, the AE model is first optimised based on its loss $L_{AE}$ according to (5) in order to obtain a decent AE model for pre-processing before the CA model is trained for its task independently. Beyond"}, {"title": "3.4 Computer Audition Tasks", "content": "In the following, we will introduce the four different computer audition tasks namely speech command recognition (SCR), automatic speech recognition (ASR), speech emotion recognition (SER) and audio scene classification (ASC), as well as the corresponding NN architectures, on which we evaluated our iterative training strategies. An overview of the applied architectures is given in Fig. 3.\n3.4.1 Speech Command Recognition\nSCR belongs to the category of tasks, in which linguistic information has to be extracted from speech. Common SCR tasks are implemented such that audio recordings, potentially of identical length, have to be assigned to one speech command or a single word out of a given set of commands or vocabulary. Due to the limitations regarding the variability of audio and labels, SCR can be considered less complex compared to general ASR and solutions do not necessarily contain language models. Hence, models for SCR have the potential to be designed shallowly in order to run on mobile edge devices or other assistive devices without the necessity of an internet connection [De Andrade et al., 2018].\nWe evaluate our methodology on the 35-word limited-vocabulary speech recognition data set introduced in Warden [2018]. Accordingly, we choose the M5 version of the very deep CNN, as introduced in Dai et al. [2017] with 35 neurons in the softmax output layer. The network consists of a set of 1D convolutional layers acting on the raw waveform in its time-domain without further pre-processing. Fig 3a provides a visualisation of the approach.\n3.4.2 Automatic Speech Recognition\nASR is certainly one of the most prominently re-searched problems in CA, as the automatic transcriptions of spoken language have a multitude of applications, which are already available in commercial devices. As the nature of speech can be considered quite complex, general ASR models need to cope with different speaker characteristics, such as different speaking speeds, and, in general, a variable length of sentences. Applications of ASR are manifold and are a cornerstone of human-machine interaction (HMI), for instance in digital assistants, such as Alexa, Siri, and alike. The era of deep learning has helped boost the performance of ASR systems, which have previously been dominated by Hidden Markov Models-Gaussian Mixture Models (HMM-GMM) Wang et al. [2019].\nCommon architectures for ASR tasks can be split into two components: an acoustic model, which finds a probability-based mapping between spoken utterances and characters within an alphabet, and a language model, which converts the probability distribution to coherent text. Most state-of-the-art acoustic models are based on self-supervised learning (SSL), which can be employed to learn powerful representations from large-scale data, which has not previously been annotated. The learnt representations find application beyond ASR Baevski et al. [2020], Hsu et al. [2021], Babu et al. [2021] in multiple downstream tasks Jing and Tian [2020], Liu et al. [2021b].\nIn order to explore the idea of iterative optimisation however, we choose an acoustic model, which is not relying on SSL, as an SSL-based system would not be compatible with our training paradigm. Instead, we choose an architecture similar to Deep Speech 2 Amodei et al. [2016]. The basis of the architecture is a set of three residual blocks, each of which consists of two convolutional layers with batch normalisation, GeLU activation, and dropout. The addition of skip-connections in our model compared to Deep Speech 2, promises a more stable convergence Li et al. [2018], Zheng et al. [2020]. The output of the residual blocks is then further processed by several recurrent layers with bidirectional gated recurrent units (GRUs) leading to speech representations, which capture the temporal dynamics in the speech signal. The final layer of the architecture is a fully connected layer, which maps the input to the character indices of the alphabet. In order to improve the quality of the recognised text from the acoustic model, we apply beam search with a 3-gram ARPA language model on the output of the acoustic model. An overview of the system is depicted in Fig. 3b*.\nGiven the importance of ASR in research, there have already been an extensive amount of studies investigating the robustness of ASR models against noise. Common approaches utilise data augmentation techniques, either by distorting the frequency-domain representation of audio, like in the case of SpecAugment Park et al. [2019], by incorporating additive synthesised noise to the clean speech samples Hannun et al. [2014], Yin et al. [2015], or in a teacher-student architecture, in which the student network is gradually taught to adapt to noise Kim et al. [2018], Meng et al. [2019]. For the input to the network, we extract Mel spectrograms from the raw audio using STFT applying a step size of 10 ms and a window length of 20 ms with 32 Mel-scale filters.\n3.4.3 Speech Emotion Recognition\nSpeech emotion recognition is a cornerstone technology for the development of successful HMI applications [Schuller, 2018]. It involves the development of algorithms that can understand human emotions from vocalisations and is typically formulated as a classification (of 'basic' emotions) or a regression task (of emotional dimensions) [Schuller, 2018] and studies often focus on specific contexts, such as to recognise acted emotions Busso et al. [2008], emotions in public speaking scenarios Baird et al. [2021] or emotions of individuals with autism Milling et al. [2022]. While the field has seen tremendous progress in recent years, especially with the increasing improvement of DL algorithms [Wagner et al., 2022], robustness remains a key issue. In particular, SER models have been shown to suffer from susceptibility to encoding errors [Oates et al., 2019], packet loss [Mohamed and Schuller, 2020], and additive noise [Triantafyllopoulos et al., 2019, Wagner et al., 2022]. Of those, additive noise is the more insidious, as it is beyond the control of the application designer (unlike encoding errors and packet loss which can be fixed by other means) and needs to be addressed with audio enhancement methods.\nIn recent years, SER research has transitioned to the use of DL models like convolutional neural networks (CNNs) [Triantafyllopoulos et al., 2021], an approach we follow here as well. In particular, we use a 4-layered CNN, where each layer consists of a sequence of convolution, batch normalisation, ReLU activation, max-pooling, and dropout. Its input consists of the Mel spectrogram, computed with 32 Mel-scale filters, a window length of 20 ms, and a step size of 10 ms. This architecture has been shown to be effective in previous works. Its output is projected to emotion labels using a dense layer, as depicted in Fig. 3c.\n3.4.4 Acoustic Scene Classification\nOur final audio application, ASC, is concerned with the classification of soundscapes in discrete categories that characterise their content (e. g., a park or a shopping mall). This application departs from the standard assumption that speech is the signal to be preserved. Instead, speech is now considered a contaminating source which needs to be removed. There are two primary motivating factors for this unorthodox formulation: a) improving the robustness of ASC classification in the presence of speech [Liu et al., 2020], and b) enforcing privacy regulations in the case of large-scale monitoring applications [Bajovic et al., 2021]. In fact, the two factors have a strong overlap as data collection for ASC applications typically takes mitigating steps to avoid the capturing of speech (e.g., filtering out segments where a VAD is triggered [Bajovic et al., 2021]) resulting in datasets that do not violate privacy requirements, but will have trouble generalising to real-world environments where human speech is ubiquitous. To that end, we propose to enhance ASC signals by removing speech a form of voice suppression [Liu et al., 2020].\nAs our ASC model, we use Dual-ResNet [McDonnell and Gao, 2020], which was awarded as the most reproducible system for the first task of the 2020 Detection and Classification of Acoustic Scenes and Events (DCASE) challenge [Heittola et al., 2020]. The model contains two different paths for separately processing the low- (lower 64) and high-frequency bands (upper 64). Late fusion is used to concatenate the outputs of these two paths, before going through two additional 1 \u00d7 1 convolutional layers to reduce the dimensionality to the number of classes. A schematic visualisation of the architecture can be found in Fig. 3d. The low- and high-frequency paths have an identical architecture, namely, a residual network of 8 convolutional blocks, each block a sequence of batch normalisation, ReLU activation, and a final convolutional layer. The Log Mel spectrogram of 128 Mel-bands, extracted from the audio waveform by applying STFT with the window length of 64 ms and a hop size of 16 ms, are used as the model input."}, {"title": "3.5 Training Details", "content": "During the training we applied a batch size of 16 for the U-Net audio enhancement, which has shown optimal performance in preliminary experiments. All models are trained with an Adam optimiser and additional weight decay is applied for the SCR and ASC models in the form of L2 regularisation. The ASR model is trained with a connectionist temporal classification (CTC) loss Graves et al. [2006], whilst we optimise the cross-entropy loss for the remaining CATS. For the AE, ASR and ASC models, we set the learning rate to 0.0001, while we set it to 0.001 for SER. For the SCR task, we reduce an initial learning rate of 0.01 to 0.001 after 20 epochs. In order to train the audios of varying lengths for CATs like ASR and SER we pad the shorter audios to the length of the longest sample."}, {"title": "4 Experimental Results", "content": "We conducted experiments to evaluate the proposed training paradigm on all four CATs introduced. In the following", "I": "Speech Command Recognition\nThe first CAT application investigated in this work is SCR based on the limited-vocabulary dataset Warden [2018", "clean": "data with noise recordings from AudioSet", "human sounds\" from AudioSet and obtain 16198 samples for the training set, 636 samples for the development set, and 714 samples for the test set. The model architecture -as described in Section 3 - is independently trained for the different noise levels and the training paradigms, as described in Section 3, i. e., baseline, data augmentation, cold cascade, cold cascade + data augmentation, multi-task learning, and iterative optimisation.\nThe baseline of the SCR model without additive noise achieves an accuracy of 85.07% (cf. Table 1). Intuitively, the performance of the same system decreases monotonically with increasing noise levels, dropping to 33.12% with 0dB SNR. All of the suggested approaches aim at increased robustness to help mitigate said drop-off. This effect becomes more noticeable with lower SNR values as, at 0dB, even the worst improvement compared to the baseline alleviates the accuracy to more than 50%. The suggested iterative optimisation and MTL training paradigms outperform competing approaches in every instance, with the MTL achieving slightly better performance at high SNRs and the iterative optimisation performing better on low SNRs. At 0dB, the iterative optimisation allows for an accuracy more than twice as high as the baseline. Noticeably, at 25 dB, MTL and iterative optimisation even outperform the baseline without additive noise. One possible explanation for this effect is that the AE filters small levels of inherent noise in the \"clean\" data itself. However, this claim is hard to verify, as quantitative measures of noise levels without completely noise-free ground-truths to compare against are difficult to obtain, making a deeper analysis necessary.\nOur iterative optimisation and MTL methods also perform favourably with respect to the state-of-the-art. DFNet-3 denoising achieves an average accuracy of 74.37,%, which is substantially lower than our 79.97%. The same is true for MetricGAN+, which ranks lower even than the baseline model at 63.00%; this failure particularly illustrates how difficult the task is, and how a simple denoising frontend can fail. Both models underperform the baseline at higher SNRs, which indicates that they introduce some unwanted distortion into the signal - something that our methods avoid. We also note that DFNet-3 is only marginally better than our own Cold Cascade method, even though the DNS-4 dataset is vastly bigger and more diverse than ours, which shows that our model is competitive in terms of enhancement performance. Overall, the comparison to state-of-the-art illustrates that iterative optimisation is crucial for bridging the gap to downstream performance between clean and noisy audio.\n4.2 Downstream Task II": "Automatic Speech Recognition\nASR experiments are performed on two datasets, the first of which being Librispeech Panayotov et al. [2015", "2023": "."}]}