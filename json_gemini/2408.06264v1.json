{"title": "Audio Enhancement for Computer Audition\nAn Iterative Training Paradigm Using Sample Importance", "authors": ["Manuel Milling", "Shuo Liu", "Andreas Triantafyllopoulos", "Ilhan Aslan", "Bj\u00f6rn W. Schuller"], "abstract": "Neural network models for audio tasks, such as automatic speech recognition (ASR) and acoustic scene classi-fication (ASC), are susceptible to noise contamination for real-life applications. To improve audio quality, an enhancement module, which can be developed independently, is explicitly used at the front-end of the target audio applications. In this paper, we present an end-to-end learning solution to jointly optimise the models for audio enhancement (AE) and the subsequent applications. To guide the optimisation of the AE module towards a target application, and especially to overcome difficult samples, we make use of the sample-wise performance measure as an indication of sample importance. In experiments, we consider four representative applications to evaluate our training paradigm, i.e., ASR, speech com-mand recognition (SCR), speech emotion recognition (SER), and ASC. These applications are associated with speech and non-speech tasks concerning semantic and non-semantic features, transient and global information, and the experimental results indicate that our proposed approach can considerably boost the noise robustness of the models, especially at low signal-to-noise ratios (SNRs), for a wide range of computer audition tasks in everyday-life noisy environments.", "sections": [{"title": "1 Introduction", "content": "Computer audition (CA) is one of the most promi-nent fields currently being revolutionised by the ad-vent of deep learning (DL), with deep neural net-works (DNNs) increasingly becoming the state-of-the-art in a multitude of applications, such as the ones discussed in this work: speech command recognition (SCR) [De Andrade et al., 2018], automatic speech recognition (ASR) [Baevski et al., 2020], speech emo-tion recognition (SER) [Wagner et al., 2022], and acous-tic scene classification (ASC) [Ren et al., 2019]. How-ever, these applications are susceptible to different het-erogeneities present in real-life conditions. Taking ASR"}, {"title": "2 Related Work", "content": "At its core, the task of AE aims at the separation of the audio of interest from other interfering sounds, i. e., it aims at the preservation of the target signal while reducing the uncertainties in audio. The unwanted in-terference can be the result of several phenomena which affect different steps of the typical CA pipeline: a) ad-ditive noise, b) reverberation, c) encoding noise, and, d) package loss. From these, additive noise has been most thoroughly studied in previous work, due to its ubiquitous presence in CA applications and its detri-mental effects on performance [Spille et al., 2018, Tri-antafyllopoulos et al., 2019].\nWithin AE, particular attention has traditionally been paid to speech enhancement (SE), as a typical CAT is mostly focused on extracting information from the human voice. ASR, being the flagship task of com-puter audition, is the primary testbed for most SE"}, {"title": "3 Methodologies", "content": "At the core of our methodology we put two hy-potheses, which are already partly supported by the literature, but have not yet been validated on a wide range of applications: 1) the enhancement of audio sig-nals, which contain highly relevant information for a given computer audition task, as part of a processing pipeline, can improve the performance on the target task, and 2) a training procedure, which optimises the audio enhancement and the CAT jointly can specialise the audio enhancement module for task-specific signals and therefore lead to better performance on the CAT.\nIn order to further explore the hypotheses men-tioned above, we report a set of experiments, based on AE models using a U-Net architecture, to explore several training paradigms, including the joint optimi-sation of the audio enhancement for the CATS: auto-matic speech recognition, speech command recognition, speech emotion recognition and acoustic scene classifi-cation. Despite the difference in implementation de-tails, the general framework stays the same amongst the different types of applications and is further illus-trated in Fig. 1. All data for AE and CATs is resampled to 16 kHz."}, {"title": "3.1 Comparison Methods", "content": "To assess the performance of our proposed iterative optimisation approach, we compare it with a wide range of methods commonly applied in the context of audio enhancement, which will be introduced in the following."}, {"title": "Baseline", "content": "The general baseline for all experiments is a CAT-specific model taken from related literature, which is not trained on any noise-specific data and does not use an AE component. The model is not specifically designed for robustness towards noise and we thus ex-pect a noticeable performance drop-off when confronted with noisy data."}, {"title": "Data Augmentation", "content": "In a first attempt to make the baseline model more robust, we train it on noise-augmented data. For this purpose, we artificially add noise with different SNR ratios to the mostly clean audio recordings. With data augmentation being one of the most common machine learning practices to increase robustness, we expect the model to perform better on the noisy test data, which was generated in the same manner as the train data."}, {"title": "Cold Cascade", "content": "The simplest training paradigm with an AE component is a cold cascade of U-Net, as described in section 3.3 and CAT-specific model. Cold cascade means in this context that both models are being optimised indepen-dently. First, the U-Net is trained to achieve a good AE performance, then, the CAT model is trained based on clean data and then stacked on top of the U-Net."}, {"title": "Cold Cascade + Data Augmentation", "content": "We further combine the cold cascade and data aug-mentation approach, i. e., first, the U-Net is trained to achieve a good AE performance, and then, we train the cold cascade architecture with augmented, noisy data. This approach promises decent noise robustness, as the model has previously seen noisy data and it includes a"}, {"title": "State-of-the-art", "content": "To further evaluate the effectiveness of our methods against the state-of-the-art, we additionally utilise two recent denoising methods; this we only do for one of the CA tasks (SCR) due to space limitations. Specifically, we use MetricGAN+ [Fu et al., 2021] and DeepFilterNet-3 (DFNet-3) [Schr\u00f6ter et al., 2023]. MetricGAN+ is a bLSTM model trained in generative-adversarial fashion to optimise perceptual losses; the training set is VoiceBank-DEMAND [Valentini-Botinhao et al., 2016]. DFNet-3, on the other hand, follows a two-stage approach with ERB-based enhance-ment followed by deep filtering to enhance the period-icity of the output signal and has been trained with a multi-spectral loss on DNS-4 [Dubey et al., 2022], which is closer to our current setup (i. e., the noise data partially comes from AudioSet). Both models are used to enhance the noisy mixtures of SCR on which we evaluate the baseline model trained without data augmentation on the original data; they thus simulate the scenario of using an off-the-shelf denoising model before evaluation. This setup is essentially equivalent to Cold Cascade, only this time using different models."}, {"title": "Multi-Task Learning", "content": "We finally compare our method to an implementation of multi-task learning, i. e., an optimisation of both the AE task and the CAT at the same time with an additive loss function\n$L = L_{AE} + L_{CA},$\nwhere $L_{AE}$ is the loss of the speech enhancement task as presented in (5) and $L_{CA}$ is the loss of the computer audition task.\nIn contrast to common applications of multi-task learning, the two models do not only share a certain set of layers but the AE and CA models are put in sequence of each other, i. e., the AE loss is derived from an inter-mediate layer of the overall system. Thus, minimising the AE loss has no effect on the parameters of the CA model, while the CA loss back-propagates through the"}, {"title": "3.2 Iterative Optimisation", "content": "Similar to the concept of multitask learning, the main motivation behind an iterative optimisation ap-proach is a joint view of the two models. At its core, there are two hypotheses: 1) The CA model should al-ways be adapted to the output of the AE model, which might contain residual noise, introduced speech distor-tions, artefacts, etc. This specialisation to specific char-acteristics of the AE can be considered as a form of domain adaptation of the CAT, which has long been shown to help alleviate performanceBen-David et al. [2006] 2) the performance of the CAT can be utilised to move the focus of the AE model onto particularly difficult samples. Thereby, we aim at achieving the optimum result of the entire neural system, i. e., the front-end audio processing and the subsequent target applications.\nThe implementation of the iterative optimisation approach is straightforward, yet rarely considered in the literature: first, the optimisation of the AE model involves the loss of the target CAT as a reference to in-dicate the difficulty of each training sample. This plays the role of sample-level importance to assist the AE component to be biased towards relatively harder sam-ples, for example, those corrupted by more intensive noise. Second, during training for the CAT, the model should process the enhanced audio signal, rather than the completely clean signal, in order to avoid a com-mon performance gap resulting from a cold cascade of the front- and back-end models. However, as long as the AE model is optimised, a more robust CA model needs to be adapted to the enhanced audio. On the other side, a more robust CA model can further as-sist the optimisation of the AE model by updating the difficulties of new samples. Having this in mind, we hypothesise that both optimisation steps need to be"}, {"title": "3.3 Audio Enhancement Model", "content": "The audio enhancement is based on U-net Ron-neberger et al. [2015], Choi et al. [2018], an auto-encoder architecture, operating in the frequency do-main, with feed-forward layers that stack the encoder layers to their corresponding decoder layers, as seen in Fig. 2."}, {"title": "3.4 Computer Audition Tasks", "content": "In the following, we will introduce the four differ-ent computer audition tasks namely speech command recognition (SCR), automatic speech recognition (ASR), speech emotion recognition (SER) and audio scene clas-sification (ASC), as well as the corresponding NN archi-"}, {"title": "3.4.1 Speech Command Recognition", "content": "SCR belongs to the category of tasks, in which lin-guistic information has to be extracted from speech. Common SCR tasks are implemented such that audio recordings, potentially of identical length, have to be assigned to one speech command or a single word out of a given set of commands or vocabulary. Due to the limitations regarding the variability of audio and la-bels, SCR can be considered less complex compared to general ASR and solutions do not necessarily contain language models. Hence, models for SCR have the po-tential to be designed shallowly in order to run on mo-bile edge devices or other assistive devices without the necessity of an internet connection [De Andrade et al.,"}, {"title": "3.4.2 Automatic Speech Recognition", "content": "ASR is certainly one of the most prominently re-searched problems in CA, as the automatic transcrip-tions of spoken language have a multitude of applica-tions, which are already available in commercial de-vices. As the nature of speech can be considered quite complex, general ASR models need to cope with differ-ent speaker characteristics, such as different speaking speeds, and, in general, a variable length of sentences. Applications of ASR are manifold and are a cornerstone of human-machine interaction (HMI), for instance in digital assistants, such as Alexa, Siri, and alike. The era of deep learning has helped boost the performance of ASR systems, which have previously been dominated by Hidden Markov Models-Gaussian Mixture Models (HMM-GMM) Wang et al. [2019].\nCommon architectures for ASR tasks can be split into two components: an acoustic model, which finds a probability-based mapping between spoken utterances and characters within an alphabet, and a language model, which converts the probability distribution to coherent text. Most state-of-the-art acoustic models are based on self-supervised learning (SSL), which can be employed to learn powerful representations from large-scale data, which has not previously been anno-tated. The learnt representations find application be-yond ASR Baevski et al. [2020], Hsu et al. [2021], Babu et al. [2021] in multiple downstream tasks Jing and Tian [2020], Liu et al. [2021b]."}, {"title": "3.4.3 Speech Emotion Recognition", "content": "Speech emotion recognition is a cornerstone tech-nology for the development of successful HMI applica-tions [Schuller, 2018]. It involves the development of algorithms that can understand human emotions from"}, {"title": "3.4.4 Acoustic Scene Classification", "content": "Our final audio application, ASC, is concerned with the classification of soundscapes in discrete categories that characterise their content (e. g., a park or a shop-ping mall). This application departs from the stan-dard assumption that speech is the signal to be pre-served. Instead, speech is now considered a contam-inating source which needs to be removed. There are two primary motivating factors for this unorthodox for-mulation: a) improving the robustness of ASC classifi-"}, {"title": "3.5 Training Details", "content": "During the training we applied a batch size of 16 for the U-Net audio enhancement, which has shown opti-mal performance in preliminary experiments. All mod-els are trained with an Adam optimiser and additional weight decay is applied for the SCR and ASC mod-els in the form of L2 regularisation. The ASR model"}, {"title": "4 Experimental Results", "content": "We conducted experiments to evaluate the proposed training paradigm on all four CATs introduced. In the following, we describe the datasets, evaluation metrics, and experimental setups for each case and report our results."}, {"title": "4.1 Downstream Task I: Speech Command Recognition", "content": "The first CAT application investigated in this work is SCR based on the limited-vocabulary dataset War-den [2018]. The data includes 105 829 one-second-long audio clips of 35 common words, including digits zero to nine, fourteen words, which are considered useful as commands for IoT and robotics, as well as some addi-tional words covering a variety of phonemes. The data further provide instances, which contain only back-ground noise or speech that does include the target words. The negative samples are expected to help the keyword spotting systems to differentiate relevant from non-relevant audio clips, thereby lowering false posi-tives in applications. The problem at hand is thus for-mulated as a 35-class classification task based on an audio recording of constant length. Given the quite balanced distribution amongst classes in the test set, we choose accuracy, i. e., the ratio of correctly classified samples over all samples, for evaluation.\nIn order to apply our audio enhancement methodol-ogy, we augment the original ('clean') data with noise recordings from AudioSet, which are truncated to a"}, {"title": "4.2 Downstream Task II: Automatic Speech Recognition", "content": "ASR experiments are performed on two datasets, the first of which being Librispeech Panayotov et al. [2015], as for the previous task noise-enhanced with AudioSet recordings, the second of which being the al-ready artificially noise-enhanced dataset of the CHiME-4 challenge. LibriSpeech consists of approximately 960 hours of read, clean speech derived from over 8000 public domain audiobooks, containing its own train, de-velopment, and test splits. The data set has previously been used for similar tasks under the assumption of not containing any noise contamination Liu et al. [2021a, 2023]. The CHiME dataset is based on the Wall Street Journal (WSJO) corpus. The training set mixes clean speech with noisy backgrounds, leading to 35690 ut-terances from 83 speakers in 4 different noisy environ-ments. The test set contains simulated recordings and utterances recorded in real-world noisy environments from 4 other speakers.\nWe choose Mel-frequency cepstral coefficients (MFCCs) as input features for the ASR model with 40 Mel-band filters. The MFCCs are based on the STFT and a mapping onto the Mel scale with triangu-lar overlapping windows, followed by a discrete cosine transform. Note that for the MTL and the iterative optimisation approach, the MFCCs are extracted from the output of the AE. For evaluation of the ASR task, we use Word Error Rate (WER), an evaluation met-"}, {"title": "4.3 Downstream Task III: Speech Emotion Recognition", "content": "Our method is evaluated for the task of SER on a dataset of elicited mood in Italian speech, DE-MoS [Parada-Cabaleiro et al., 2020]. DEMOS contains 9365 emotional and 322 neutral samples recorded from 68 native speakers (23 females and 45 males; mean age 23.7 years, standard deviation 4.3 years). Six emotions anger, sadness, happiness, fear, surprise, and guilt are elicited by listening to music, watching pictures or movies, pronouncing or reading emotional sentences, and recalling personal memories. Original recordings are captured at 44.1 kHz with 16-bit depth.\nIn our experiments, we use all of the emotional sam-ples in DEMOS, while keeping the partitioning of the data for training, development, and testing identical to Ren et al. [2020], ensuring a speaker-independent split and accounting for gender and class balance. To be con-sistent with the other target audio applications, the au-dio samples from DEMOS are down-sampled to 16 kHz, which yields no evident information loss according to Ren et al. [2020]. As for the previous experiments, we simulate the background noise along with the speech ut-terances by adding environmental recordings from Au-dioSet.\nThe CNN model predicts the emotion label given a single utterance as input. To account for the im-balanced class distribution in the test set, we use un-"}, {"title": "4.4 Downstream Task IV: Acoustic Scene Clas-sification", "content": "To test our approach on the final task, acoustic scene classification, we use the DCASE 2021 Challenge dataset [Wang et al., 2021]. To create the noisy scene audio, speech samples from LibriSpeech are added to the soundscape recordings from the DCASE 2021 chal-lenge. The range of SNRs is enlarged to -25, -20, -15, -10, -5, 0, 5, and 10 dB, covering a wide range of real-life"}, {"title": "5 Discussion", "content": "Collectively, our results on four different application domains show that the proposed methods consistently outperform comparable baselines. In particular, joint training, either in the form of MTL or in the form of it-erative optimisation, yields better results than methods relying on data augmentation or cascade enhancement, which are standard baselines in the field. This demon-strates that adapting an enhancement model to the downstream task can bring substantial improvements, which underlines the need for specialised AE systems that are able to differentiate between the task-specific relevancy of audio signals and noise sources.\nThis aspect becomes even more apparent when com-paring the audio enhancement output of the two tasks, ASR and ASC depicted in Fig. 4. For the ASC task (upper row) the clean audio can be described as back-ground music with relatively stable frequency patterns over time. To construct the noisy sample, this is over-layed with a speech sample, which visually interrupts the smoothness of the background music. The AE mod-ule trained for ASC is thus capable of removing the disruptive speech signal and reconstructing the original target with only a few artefacts. In the bottom row, the clean sample is a speech sample, without any audi-ble background noise. The spectrogram is thus charac-terised by an irregular frequency distribution -due to pauses and a change of frequencies between phonemes-over a dark (quiet) background. After adding noise from a construction site, the frequency patterns of the"}, {"title": "6 Conclusion", "content": "In this work, we focused on single-channel au-dio enhancement adapted to specific computer au-dition downstream tasks under low signal-to-noise-"}]}