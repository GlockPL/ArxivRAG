{"title": "SHARP: ACCELERATING LANGUAGE MODEL INFERENCE BY SHARING ADJACENT LAYERS WITH RECOVERY PARAMETERS", "authors": ["Yiping Wang", "Yifang Chen", "Simon Shaolei Du", "Hanxian Huang", "Jishen Zhao", "Yuandong Tian"], "abstract": "While Large language models (LLMs) have advanced natural language processing tasks, their growing computational and memory demands make deployment on resource-constrained devices like mobile phones increasingly challenging. In this paper, we propose SHARP (SHaring Adjacent Layers with Recovery Parameters), a novel approach to accelerate LLM inference by sharing parameters across adjacent layers, thus reducing memory load overhead, while introducing low-rank recovery parameters to maintain performance. Inspired by observations that consecutive layers have similar outputs, SHARP employs a two-stage recovery process: Single Layer Warmup (SLW), and Supervised Fine-Tuning (SFT). The SLW stage aligns the outputs of the shared layers using $L_2$ loss, providing a good initialization for the following SFT stage to further restore the model performance. Extensive experiments demonstrate that SHARP can recover the model's perplexity on various in-distribution tasks using no more than 50k fine-tuning data while reducing the number of stored MLP parameters by 38% to 65%. We also conduct several ablation studies of SHARP and show that replacing layers towards the later parts of the model yields better performance retention, and that different recovery parameterizations perform similarly when parameter counts are matched. Furthermore, SHARP saves 42.8% in model storage and reduces the total inference time by 42.2% compared to the original Llama2-7b model on mobile devices. Our results highlight SHARP as an efficient solution for reducing inference costs in deploying LLMs without the need for pretraining-scale resources.", "sections": [{"title": "1 INTRODUCTION", "content": "Following the principles of scaling laws, large language models (LLMs) have become one of the central topics in Natural Language Processing (NLP) (Brown, 2020; Zhang et al., 2022; Hoffmann et al., 2022; Bubeck et al., 2023; Chowdhery et al., 2023; Bai et al., 2023; Team et al., 2023; Touvron et al., 2023). However, deploying a pre-trained large language model requires significant computational and memory resources (Aminabadi et al., 2022; Pope et al., 2023; Kim et al., 2023b; Zhang et al., 2024b), which may further restrict their inference speed. For instance, a 70-billion-parameter language model stored in FP16 precision requires approximately 148GB of memory to hold the model weights, necessitating two A100 GPUs with 80GB of memory each to load the entire model. During inference, the entire input sequence and the KV cache are also stored on the GPU, incurring additional memory usage. Although techniques like layer-wise inference (HuggingFace, 2022), which load the model to GPU layer by layer, enable LLM inference on a single GPU, they introduce"}, {"title": "2 MAIN METHODS", "content": ""}, {"title": "2.1 INSIGHT: THE CURRENT LAYER CAN BE A GOOD APPROXIMATION OF THE NEXT\nLAYER WITH RECOVERY PARAMETERS", "content": "As illustrated in the introduction parts, the adjacent layer-sharing strategy (Figure 1 (b)) proposed by MobileLLM accelerates the inference of a newly-trained small language model (125-350M) by reusing the previous layers and reducing the communication overhead in memory. However, several questions about layer-sharing strategies have yet to be revealed: does the success of this adjacent layer-sharing strategy come from the fact that adjacent layers have similar parameters or behaviors? Can we extend this method to a pretrained larger model like Llama2-7b (Touvron et al., 2023), to accelerate model inference (as shown in Figure 1 (a) and (c))? Further, can we even use one layer to predict more layers and thus further accelerate inference?\nTo answer these questions, we try to directly replace the MLP layer in the next layer with the current reference layer in Llama2-7b model, and then evaluate its perplexity on some evaluation tasks including Arxiv-math (Kenny, 2023), GPT4-Alpaca (Peng et al., 2023), Databricks-Dolly-15k (Conover et al., 2023) and Dialogsum (Chen et al., 2021). The result is shown in Figure 2 (Left). We found that except for the first and last layer, most replacements don't increase the model perplexity significantly (solid line) compared to the original model (dash line)."}, {"title": "2.2 OUR METHODS", "content": "In this section, we illustrate how we find the proper additional parameters for predicting each layer in SHARP. First, we introduce the notations and the settings used in our paper."}, {"title": "2.2.1 PRELIMINARY", "content": "We assume the original parameters of a particular function (for example, the gate projection of MLP layers) at i-th layer as $\u0398_i \u2208 \u211d^{d_1\u00d7d_2}$ (i \u2208 [N]), where $d_1$ and $d_2$ are the input/output dimension of the function, and we freeze their gradients during training. We denote the low-rank learnable LoRA parameters at i-th layer as $A_i \u2208 \u211d^{d_1r}$, $B_i \u2208 \u211d^{rxd_2}$. And we let $f(\u00b7 ; \u0398_i)$ denote the particular function that utilizes $\u0398_i$ as parameters. In this paper, we focus on reducing the parameters in MLP layers, which take up the main parts of the parameters in the intermediate layers. And we use Llama2-7b (Touvron et al., 2023) as our basic model."}, {"title": "2.2.2 SHARP ALGORITHM", "content": "In this part, we show how we achieve SHARP (Figure 1 (d)) algorithm. First, we illustrate why we choose such a two-stage algorithm for recovering.\nWhy Two-Stage? A natural way to recover the model performance is to directly finetune the model end-to-end for all learnable low-rank additional parameters. In general, it works fine when we try to use one layer to replace the next layer, which at most gives a 50% reduction of the MLP layers. However, if we want to use one layer to compute multiple adjacent layers, just using the SFT stage will require more data for recovering, and result in a much slower convergence rate and worse final result. The detailed discussion of this has been investigated in Section 3.3.3 and Table 3, and the intuition for this phenomenon may be that when we use one layer to replace multiple layers aggressively, the model loses too many parameters and thus start optimizing from an initialization point that is far from the optimal solution. Therefore, we need first to align the output of the predicted and original target layers before the SFT stage. Details of the algorithm are as follows.\nStage 1: Single Layer Warmup (SLW). First, we minimize the $L_2$ loss between the output of layer predicted by the reference layer and that of the original target layer by finetuning the model on high-quality data. Formally, for each reference layer $j \u2208 \u2110$ and every target layer predicted by this reference layer $l \u2208 \ud835\udcaf_j$, we want to find:\n$\u0394\u0398_l \u2190 \\text{arg min}_{\u0394\u0398_l} \ud835\udd3c_{x\u223c\u03c1_\ud835\udcaf}[||f(X; g(\u0398_i, \u0394\u0398_l)) \u2212 f(X; \u0398_l)||^2] $\nHere $\u03c1_\ud835\udcaf$ is the distribution of the input activations of $f(\u00b7 ; \u0398_l)$, and it can be obtained by running the forwarding pass of the original model on the finetuning dataset. We also note that the SLW stage of each target layer can be much faster than the SFT stage (Stage 2) since we just run one MLP layer, which has only 135M parameters (while the whole model has 7B parameters). And this process can also be fully parallelized since the SLW stages of different target layers are independent. In Section 3.3.3, we will show that SLW is critical for increasing the compression ratios.\nStage 2: Supervised Fine-Tuning (SFT). After the single MLP warmup stage, we partly recover the output of the replaced layers. To better align different replaced layers together and obtain better model output, at the second stage, we fixed the original parameters and finetune all the learnable low-rank components {$\u0394\u0398_\u2217$} together. In Section 3.3.3, we will also show that although SLW is important, SFT is still the key stage to recover the model capacity."}, {"title": "2.2.3 CHOICE OF REPLACEMENT AND CANDIDATE TRANSFORMATION", "content": "Replacement Type. Note that in SHARP, there are multiple ways to define the reference layer set \u2110 and the corresponding target layer set \ud835\udcaf. To prevent ambiguity, we formally list the types of layer replacement that we used in this paper in Table 1. And more detailed table are in Table 8.\nNotably, we skip the first and the last layer for all types, since Figure 2 shows that these two layers may be quite important or behave differently from other layers. For $\ud835\udcaf_{\\text{next}}$ and $\ud835\udcaf_{\\text{next2}}$, we consider the constant reference intervals (each reference predicts the next or next two target layers). For $\ud835\udcaf_{\\text{back}}$ and $\ud835\udcaf_{\\text{front}}$, we consider the cases where making one reference layer predicts more target layers in the front parts and back parts, respectively. And finally, for $\ud835\udcaf_{\\text{more}}$ and $\ud835\udcaf_{\\text{max}}$, we aggressively try to remove more MLP layers. In Section 3.3.1, we will show that repeating layers in the back parts of the model is better than doing this in the front parts, i.e., $\ud835\udcaf_{\\text{back}}$ is better than $\ud835\udcaf_{\\text{front}}$, and even better than $\ud835\udcaf_{\\text{next2}}$. Furthermore, we will show that aggressively removing the layers like $\ud835\udcaf_{\\text{more}}$ and $\ud835\udcaf_{\\text{max}}$ can still achieve quite good recovery performance.\nCandidate Transformation Type. On the other hand, we also consider different candidate trans- formations g. In detail, we investigate the following parameterization ways:\n$g_0(\u0398_j, (\u03b1, A_l, B_l)) := \u03b1\u0398_j + A_l B_l, \u03b1\u2208 \u211d, A_l \u2208 \u211d^{d_1\u00d7r}, B_l \u2208 \u211d^{rxd_2}$\n$g_1(\u0398_j, (\u03b1, A_l, B_l, C_l, D_l)) := \u03b1\u0398_j C_l D_l + A_l B_l, \u03b1\u2208 \u211d, A_l \u2208 \u211d^{d_1\u00d7r}, B_l, C_l, D_l \u2208 \u211d^{r\u00d7 d_2}$\n$g_2(\u0398_j, (\u03b1, A_l, B_l, E_l, F_l)) := \u03b1E_l F_l \u0398_j + A_l B_l, \u03b1 \u2208 \u211d, A_l, E_l, F_l \u2208 \u211d^{d_1\u00d7r}, B_l \u2208 \u211d^{r\u00d7d_2}$\n$g_3(\u0398_j, (\u03b1, A_l, B_l, U_l, V_l)) := \u03b1[(U_l V_l) \u2299 \u0398_j] + A_l B_l, \u03b1 \u2208 \u211d, A_l, U_l \u2208 \u211d^{d_1\u00d7r}, B_l, V_l \u2208 \u211d^{r\u00d7d_2}$\nHere we consider the vanilla LoRA, left multiplication, right multiplication, and dot multiplication. We assume $d_1 = 4096 < d_2 = 11008$ for Llama2-7b to prevent ambiguity. The comparison result is shown in Section 3.3.2. Surprisingly we find these four different transformations have almost the same capability for recovering model performance if their numbers of parameters are the same."}, {"title": "3 EXPERIMENTS", "content": "In the experiment section, we mainly focus on four parts: (E1) In-distribution recovery tasks, where we apply SHARP to the pretrained model, finetune it on a specific high-quality dataset, and then evaluate the model's perplexity on the same dataset. (E2) Ablation study, where we investigate how to choose better replacement types and candidate transformations. We also try to analyze how the single-layer warmup stage and different settings influence the model recovery, and want to see how aggressively we can choose the replacement type. (E3) Downstream evaluation, where we assess the model's performance on several widely-used downstream tasks. (E4) Latency analysis, where we examine how much inference acceleration SHARP can achieve."}, {"title": "3.1 EXPERIMENTAL SETTINGS", "content": "Dataset. We use the following dataset in our experiments: Arxiv-math (Kenny, 2023), GPT4-Alpaca (Peng et al., 2023), Databricks-Dolly (Conover et al., 2023), DialogSum (Chen et al., 2021), OpenOrca (Mukherjee et al., 2023), FineWeb-Edu (Penedo et al., 2024; Lozhkov et al., 2024), and Tulu V2 Collection (Ivison et al., 2023). More details are available in Appendix C.1."}, {"title": "4 CONCLUSION", "content": "This paper presented SHARP, a method to accelerate large language model inference by sharing ad- jacent layers with recovery parameters. SHARP effectively reduces model size and inference time by using the reference layer to predict the later layers, which saves the memory load overhead. It recovers the model performance through a two-stage process: Single Layer Warmup and Supervised Fine-Tuning. Experimental results demonstrate that SHARP achieves comparable perplexity to orig- inal models across various in-distribution tasks. By minimizing the number of layers and parameters needed, SHARP provides a practical solution for deploying large models in resource-constrained en- vironments. We believe this method can further enlighten researchers in designing advanced layer- sharing methods for accelerating inference and may be insightful for the interpretability-related works on understanding how each layer works in LLM."}, {"title": "A RELATED WORK", "content": "Model compression Model compression is a classical way to improve inference efficiency by either reducing the number of model parameters or lowering the memory required to store them. Three widely used techniques\u2014sparsity and pruning, quantization, and distillation\u2014are central to this goal. Sparsity and pruning share the aim of reducing the number of effective parameters, but differ in approach: sparsity reduces individual weights to zero in an unstructured manner (Sun et al., 2023; Xia et al., 2023a; Frantar & Alistarh, 2023), while pruning takes a structured approach by removing entire components, such as neurons or filters, from the network (Xia et al., 2023b; Gromov et al., 2024). Quantization reduces the memory footprint by lowering the precision of weights and activations, without changing the number of parameters (Dettmers et al., 2024; 2022; Li et al., 2023a; Kim et al., 2023a; Frantar et al., 2022; Xiao et al., 2023; Yao et al., 2022; Liu et al., 2023a; Frantar et al., 2022; Zhang et al., 2018). While sparsity, pruning, and quantization are usually applied after a certain amount of training, distillation is a data-centric methodology used during training. In distillation, a smaller student model is trained using both the original training data and the output (soft labels) of a larger teacher model, allowing the student model to retain much of the teacher's performance while being more efficient (Hinton, 2015; Timiryasov & Tastet, 2023; Chen et al., 2024). These three categories represent the most classical methods for model compression, primarily aimed at improving inference efficiency. However, there are other compression methods closely related to our proposed techniques, which will be discussed in detail later.\nLow rank approximation Low-rank approximation, while distinct from the traditional model compression techniques discussed earlier, leverages the observation that much of the key informa- tion users care about in a neural network can be represented in a lower-dimensional subspace. By approximating large weight matrices with low-rank representations, both the number of parameters and computational costs are reduced. Many works such as Li et al. (2023b); Hsu et al. (2022); Hajimolahoseini et al. (2021); Tahaei et al. (2021) focus on improving inference efficiency using this method, but it can also offer significant efficiency gains during training. LoRA (Low-Rank Adapta- tion) by Hu et al. (2021) is the first work to introduce two small low-rank matrices, A and B attached to a frozen pre-trained weight matrix W, allowing for efficient fine-tuning with minimal memory usage. Since then, numerous variants have been developed to enhance this approach (Dettmers et al., 2022; Sheng et al., 2023; Chen et al., 2023; Zhang et al., 2023). Our proposed methods are similarly inspired by low-rank approximation, but unlike other works that focus on decomposing the entire weight matrix, we use low-rank approximations to estimate the minimal differences between inter- mediate layers. This allows for maximal weight sharing, significantly reducing redundancy while maintaining performance.\nWeight sharing Weight sharing is another powerful model compression technique that improves both training and inference efficiency by reducing the number of unique parameters in a neural net- work. Classical weight sharing involves using a common representation space across multiple tasks or domains, allowing models to generalize better while using fewer parameters (Liu et al., 2020; Jiang et al., 2019; Tars & Fishel, 2018; Fu et al., 2021). Those embedding sharing architectures have been later adopted in Llama (Touvron et al., 2023) and OPT models (Zhang et al., 2022). How- ever the savings from embedding sharing diminished with the increasing model size, and therefore been disregarded in recent designs of LLMs. Recently, MobileLLM(Liu et al., 2024) introduced the first approach to weight sharing between intermediate layers, enabling models to reuse learned representations across layers. This significantly reduces the parameter count while maintaining per- formance, making large models more feasible for resource-constrained environments. Our proposed methods are inspired by this concept, but further integrate weight sharing with low-rank approxima- tion to achieve both computational efficiency and performance preservation during the fine-tuning stage.\nSmall models The definition of small models has evolved as advancements in deep learning archi- tectures have significantly increased model sizes. Models that were previously considered large are now categorized as small relative to the current state-of-the-art. Commonly, models with fewer than 7 billion parameters (7B) are referred to as small models. Notably, prominent open-source language models under 7B parameters include Mistral 7B (Jiang et al., 2023); Phi-3 series (Abdin et al., 2024); Gemma 2B (Team et al., 2023), Llama 3.2 series (Dubey et al., 2024), TinyLlama(Zhang et al., 2024a), MobileLLM(Liu et al., 2024) and MiniCPM(Hu et al., 2024). Despite their smaller"}, {"title": "BALGORITHM DETAILS", "content": ""}, {"title": "B.1 FULL DEFINITION OF DIFFERENT REPLACEMENT TYPES", "content": "Here we show the full table of different replacement types used in our main paper in Table 8."}, {"title": "C EXPERIMENT DETAILS", "content": ""}, {"title": "C.1 DETAILS OF DATASET", "content": "We use several high-quality datasets in our in-distribution recovery tasks: (1) Arxiv-math (Kenny, 2023): it includes 50k high-quality QA instruction data from the mathematical domain. (2) GPT4-Alpaca (Peng et al., 2023): it contains 52k English instruction-following generated by GPT-4 using Alpaca prompts. (3) Databricks-Dolly (Conover et al., 2023): This is an open-source dataset com- prising 15k instruction-following records generated by Databricks employees. (4) DialogSum (Chen et al., 2021): this is a dialogue summarization dataset consisting of 13.5k dialogues (12.5k for train- ing) with corresponding manually labeled summaries and topics. (5) OpenOrca (Mukherjee et al., 2023), which is a collection of augmented FLAN Collection data (Longpre et al., 2023), containing about 1M GPT-4 completions and about 3.2M GPT-3.5 completions. We select a 50k subset of it for in-distribution tasks.\nIn downstream evaluation parts, we also use (6) FineWeb-Edu (Penedo et al., 2024; Lozhkov et al., 2024), consists of 1.3T tokens of educational web pages filtered from the FineWeb dataset. We use its \"sample-10BT\" subset. (7) Tulu V2 Collection (Ivison et al., 2023), which contains 326k instruction data mixed from FLAN (Longpre et al., 2023), Open Assistant 1 (K\u00f6pf et al., 2024), GPT4-Alpaca (Peng et al., 2023), Code-Alpaca (Chaudhary, 2023), LIMA (Zhou et al., 2024), Wiz- ardLM (Xu et al., 2024) and Open-Orca (Mukherjee et al., 2023)."}, {"title": "C.2 EVALUATION TASKS", "content": "This section introduces the evaluation tasks used in our downstream evaluation part (Section 3.4)."}, {"title": "C.2.1 BASIC REASONING TASKS", "content": "This class of tasks doesn't require too much commonsense or knowledge to solve problems, but needs the model to have good reasoning capability, especially the mathematical reasoning\n\u2022 MathQA (Amini et al., 2019): This is a large-scale dataset of 37k English multiple-choice math word problems covering multiple math domain categories by modeling operation programs corresponding to word problems in the AQuA dataset.\n\u2022 GSM8k (Cobbe et al., 2021): This is a free-generation benchmark of grade school math problems aiming for evaluating multi-step (2-8 steps) mathematical reasoning capabilities. These problems are illustrated by natural language and require using four basic arithmetic operations to reach the final answer.\n\u2022 BBH-COT-FS (Suzgun et al., 2022): This is a free-generation benchmark consists a suite of 23 challenging BIG-Bench tasks (Srivastava et al., 2022) which we call BIG-Bench Hard (BBH). These are the tasks for which prior language model evaluations did not outperform the average human-rater. Here we use the chain-of-though with 3 shot version.\n\u2022 Mutual(Cui et al., 2020): This is a retrieval-based dataset for multi-turn dialogue rea- soning, which is modified from Chinese high school English listening comprehension test data.\n\u2022 QA4MRE(Pe\u00f1as et al., 2013): This is a multi-choice benchmark used for long-text un- derstanding and reasoning. Four different tasks have been organized during these years: Main Task, Processing Modality and Negation for Machine Reading, Machine Reading of Biomedical Texts about Alzheimer's disease, and Entrance Exam."}, {"title": "C.2.2 KNOWLEDGE-MEMORIZATION TASKS", "content": "\u2022 SciQ (Welbl et al., 2017): This contains 13,679 crowdsourced science exam questions about Physics, Chemistry and Biology, among others. The questions are in multiple-choice format with 4 answer options each. For the majority of the questions, an additional para- graph with supporting evidence for the correct answer is provided.\n\u2022 BoolQ (Clark et al., 2019): This is a question-answering benchmark for yes/no questions containing 15942 examples. These questions are naturally occurring \u2013 they are generated in unprompted and unconstrained settings. Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context.\n\u2022 CommonsenseQA (Talmor et al., 2019): This is a multiple-choice question-answering dataset that requires different types of commonsense knowledge to predict the correct an- swers. It contains 12,102 questions with one correct answer and four distractor answers.\n\u2022 MedMCQA (Pal et al., 2022) This is a new large-scale, multiple-choice question- answering dataset designed to address real-world medical entrance exam questions. More than 194k high-quality AIIMS & NEET PG entrance exam MCQs covering 2.4k health- care topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other op- tions which require a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects & topics. A detailed explanation of the solution, along with the above information, is provided in this study.\n\u2022 TriviaQA (Joshi et al., 2017): This is a reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs au- thored by trivia enthusiasts and independently gathered evidence documents, six per ques- tion on average, that provide high-quality distant supervision for answering the questions. This dataset can be used for both retrieval-augmented models to test the models' knowl- edge retrieval ability and the usual LLM to test the knowledge memorization on the model itself."}, {"title": "C.2.3 KNOWLEDGE-MEMORIZATION + COMMONSENSE REASONING", "content": "\u2022 PIQA (Bisk et al., 2020): This is a multi-choice physical commonsense reasoning and a corresponding benchmark dataset. PIQA was designed to investigate the physical knowl- edge of existing models.\n\u2022 WinoGrande (Sakaguchi et al., 2019): This is a collection of 44k multi-choice problems, inspired by Winograd Schema Challenge (Levesque, Davis, and Morgenstern 2011), but adjusted to improve the scale and robustness against the dataset-specific bias. Formulated as a fill-in-a-blank task with binary options, the goal is to choose the right option for a given sentence which requires commonsense reasoning.\n\u2022 ARC (Clark et al., 2018): This is a subset of ARC dataset with 2,590 \"hard\" questions (those that both a retrieval and a co-occurrence method fail to answer correctly). The ARC dataset contains text-only, English language exam multi-choice questions that span several grade levels as indicated in the files."}, {"title": "C.2.4 OTHERS", "content": "\u2022 LAMBADA (Paperno et al., 2016): This is a dataset to evaluate the capabilities of compu- tational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word."}, {"title": "C.3 EXPERIMENTAL SETUP ON MOBILE", "content": "We evaluated the model run time latency on mobile. For the models to evaluate, we tested: (1) the original Llama2-7b and (2) a simplified version of SHARP ($\ud835\udcaf_{\\text{next}}$) where we removed the LoRA parameters. We only store the reference layers, and call those layers multiple times for the target layers in model forwarding.\nWe first exported the models using ExecuTorch v0.3.0 with the same export configurations (using kv cache, using 8-bit dynamic quantization and 4-bit weight quantization, for XNNPack backend). We tested the model loading and initialization time, as well as the model forwarding time using a benchmark app on Xcode (version 16.0). We ran Xcode on a MacBook Air with Apple M2 chip, 16GB memory with MacOS Sonoma 14.6.1, and wire-connected it to an iPhone 16 Pro with iOS 18.1 with 8GB memory, and ran the benchmark app on the phone."}, {"title": "C.4 DETAILS ABOUT STRUCTURAL PRUNING BASELINES", "content": "We evaluate the following baseline in the in-distribution tasks.\n\u2022 LayerPruning (Gromov et al., 2024), which calculates the angle distance between different layers, and then prunes a group of consecutive layers in the model such that the first and last layers in these continuous layers have small angle distance. For LLaMA2-7B, we prune the MLP layers of the 17th to 30th layers as recommended in original paper. We keep the number of removing layers the same for fair comparison and use default settings.\n\u2022 Adapted version of LayerPruning, where we use the replacement strategy $\ud835\udcaf_{\\text{next}}$ (replac- ing layer 2t with layer 2t - 1 for t in [2,15]) as mentioned in Table 1. We also use the same LoRA ranks as SHARP. Notably, this baseline is equivalent to directly pruning the corresponding layers in SHARP, rather than reusing them.\n\u2022 LLM-Pruner (Ma et al., 2023), another advanced structural pruning algorithm. It uses gradient information to divide the parameters into several groups based on their relevance,"}, {"title": "D ADDITIONAL EXPERIMENTS AND DISCUSSIONS", "content": ""}, {"title": "D.1 CONSIDERATION OF OVERFITTING", "content": "To claim that our SHARP algorithm, doesn't overfit to the recovering dataset, we apply SHARP with a fixed dataset (GPT4-Alpaca), and then evaluate its performance in other tasks. The result is shown in Table 9.\nWe can see SHARP (with or without fine-tuning) can consistently recover the perplexity on every task, rather than just recovering model performance on related tasks like Arxiv-math. This supports our claim that SHARP does not overfit to the recovery dataset. Especially, we observe that after the Single-Layer-Warmup (SLW) stage on GPT4-Alpaca, the model perplexity has recovered a lot compared to the vanilla Direct Sharing baseline on all tasks. Besides, for the complete 2-step SHARP, the gap between using in-distribution and GPT4-Alpaca data is not that large, implying that our method should have a good generalization in utilizing different recovering data."}, {"title": "D.2 EXPERIMENT ON ADVANCED SMALL MODEL", "content": "To confirm that SHARP is also applicable to other models, we try to apply it on LLaMA3.2-3B model, and obtain the result in Table 10. We can see that the perplexity between the original model and SHARP is still close, supporting the generality of our method."}, {"title": "D.3 ADDING LORA TO ALL LAYERS", "content": "In general, attaching the whole LoRA adapters to the entire model is more convenient (Mangrulkar et al., 2022). We apply LoRA to the replaced layer in the in-distribution tasks just for clearer illus- tration. If we want to attach LoRA adapters to the entire models in SHARP, we can first run SLW on the LORA components which are assigned to the target (replaced) layers, and then fine-tune all the LoRAs together in the second SFT stage. Actually we did this in the downstream recovery part (Sec 3.4) following open-instruct pipeline (Ivison et al., 2023) for convenience.\nNevertheless, this process does not result in a large difference. We show the result of applying LoRA to all layers in Table 11. Although more LoRA components bring difference for SLW stage (SHARP w/o fine-tuning), they behave the same in complete SHARP."}, {"title": "D.4 DISCUSSSION ABOUT THE STABILITY IN SLW STAGE", "content": "In the main paper, we show the importance of Single-Warmup Stage in recovering model perfor- mance, and one concern may be that if there is any risk of unstability in SLW stage. Here, we simply summarize the reasons for why SLW stage is relatively stable.\n1. The similarity between adjacent layers. SLW just simply finds some recovery compo- nents to mimic the output between adjacent layers, and as shown in Figure 2, adjacent layers are quite similar. This phenomenon has been verified on larger models like LLaMA2-70B or other models like Mistral-7B in the previous works (Ma et al., 2023; Liu et al., 2023b).\n2. The simplicity of optimization loss. The SLW stage just utilizes simple standard L2 regression loss on the output of adjacent single layers, whose optimization is empirically observed as quite stable.\n3. Computation Efficiency. The computation cost of SLW is also significantly smaller than the SFT stage since we just do independent single-layer fittings than processing the entire large model, and thus it can be quite efficient and stable. In Table 3, we just used about 10% of the data for the SLW stage and it's already enough for the convergence of all the layers' SLW stages.\n4. Robustness of recovery dataset. The SLW stage is even robust to the choice of recovery dataset. In Section D.1, we show that onlyusing GPT4-Alpaca as recovery data can still recover model perplexity on other tasks. This also shows the robustness of the SLW stage."}]}