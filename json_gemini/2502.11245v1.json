{"title": "SHORTCUTS AND IDENTIFIABILITY IN CONCEPT-BASED MODELS FROM A NEURO-SYMBOLIC LENS", "authors": ["Samuele Bortolotti", "Emanuele Marconato", "Paolo Morettin", "Andrea Passerini", "Stefano Teso"], "abstract": "Concept-based Models are neural networks that learn a concept extractor to map inputs to high-level concepts and an inference layer to translate these into predictions. Ensuring these modules produce interpretable concepts and behave reliably in out-of-distribution is crucial, yet the conditions for achieving this remain unclear. We study this problem by establishing a novel connection between Concept-based Models and reasoning shortcuts (RSs), a common issue where models achieve high accuracy by learning low-quality concepts, even when the inference layer is fixed and provided upfront. Specifically, we first extend RSs to the more complex setting of Concept-based Models and then derive theoretical conditions for identifying both the concepts and the inference layer. Our empirical results highlight the impact of reasoning shortcuts and show that existing methods, even when combined with multiple natural mitigation strategies, often fail to meet these conditions in practice.", "sections": [{"title": "1 Introduction", "content": "Concept-based Models (CBMs) are a broad class of self-explainable classifiers [Alvarez-Melis and Jaakkola, 2018, Koh et al., 2020, Espinosa Zarlenga et al., 2022, Marconato et al., 2022, Taeb et al., 2022] designed for high performance and ante-hoc interpretability. Learning a CBM involves solving two conjoint problems: acquiring high-level concepts describing the input (e.g., an image) and an inference layer that predicts a label from them. In many applications, it is essential that these two elements are \u201chigh quality\", in the sense that: i) the concepts should be interpretable, as failure in doing so compromises understanding [Schwalbe, 2022, Poeta et al., 2023] and steerability [Teso et al., 2023, Gupta and Narayanan, 2024], both key selling point of CBMs; and ii) the concepts and inference layer should behave well also out of distribution (OOD), e.g., they should not pick up spurious correlations between the input, the concepts and the output [Geirhos et al., 2020, Bahadori and Heckerman, 2021, Stammer et al., 2021].\nIt is natural to ask under what conditions CBMs can acquire such \u201chigh-quality\" concepts and inference layers. While existing studies focus on concept quality [Mahinpei et al., 2021, Zarlenga et al., 2023, Marconato et al., 2023a], they neglect the role of the inference layer altogether. In contrast, we cast the question in terms of whether it is possible to identify from data concepts and inference layers with the intended semantics, defined formally in Section 3.1. We proceed to answer this question by building a novel connection with reasoning shortcuts (RSs), a well-known issue in Neuro-Symbolic (NeSy) AI whereby models achieve high accuracy"}, {"title": "2 Preliminaries", "content": "Concept-based Models (CBMs) first map the input $x \\in \\mathbb{R}^n$ into $k$ discrete concepts $c = (c_1, ..., c_k) \\in C$ via a neural backbone, and then infer labels $y \\in Y$ from this using a white-box layer, e.g., a linear layer. This setup makes it easy to figure out what concepts are most responsible for any prediction, yielding a form of ante-hoc concept-based interpretability. Several architectures follow this recipe, including approaches for converting black-box neural network classifiers into CBMs [Yuksekgonul et al., 2022, Wang et al., 2024, Dominici et al., 2024, Marcinkevi\u010ds et al., 2024].\nA key issue is how to ensure the concepts are interpretable. Some CBMs rely on concept annotations [Koh et al., 2020, Sawada and Nakamura, 2022, Espinosa Zarlenga et al., 2022, Marconato et al., 2022, Kim et al., 2023, Debot et al., 2024]. These are however expensive to obtain, prompting researchers to replace them with (potentially unreliable [Huang et al., 2024, Sun et al., 2024]) annotations obtained from foundation models [Oikarinen et al., 2022, Yang et al., 2023, Srivastava et al., 2024] or unsupervised concept discovery [Alvarez-Melis and Jaakkola, 2018, Chen et al., 2019, Taeb et al., 2022, Schrodi et al., 2024].\nNeuro-Symbolic CBMs (NeSy-CBMs) specialize CBMs to tasks in which the prediction $y \\in Y$ ought to comply with known safety or structural constraints. These are supplied as a formal specification \u2013 a logic formula $K$, aka knowledge \u2013 tying together the prediction $y$ and the concepts $c$. In NeSy-CBMs, the inference step is a symbolic reasoning layer that steers [Lippi and Frasconi, 2009, Diligenti et al., 2012, Donadello et al., 2017, Xu et al., 2018] or guarantees [Manhaeve et al., 2018, Giunchiglia and Lukasiewicz, 2020, Hoernle et al., 2022, Ahmed et al., 2022] the labels and concepts to be logically consistent according to $K$. Throughout, we will consider the following example task:"}, {"title": "2.1 Reasoning Shortcuts", "content": "Before discussing reasoning shortcuts, we need to establish a clear relationship between concepts, inputs, and labels. The RS literature does so by assuming the following data generation process [Marconato et al., 2023b, Yang et al., 2024a, Umili et al., 2024]: each input $x \\in \\mathbb{R}^n$ is the result of sampling $k$ ground-truth concepts $g = (g_1,..., g_k) \\in G$ (e.g., in MNIST-SumParity two numerical digits) from an unobserved distribution $p^*(G)$ and then $x$ itself (e.g., two corresponding MNIST images) from the conditional distribution $p^*(X | g)$. Labels $y \\in Y$ are sampled from the conditional distribution of $g$ given by $p^*(Y | g; K)$ consistently with $K$ (e.g., $y = 1$ if and only if $g_1 + g_2$ is odd). The ground-truth distribution is thus:\n$p^*(X, Y) = \\mathbb{E}_{g \\sim p^*(G)} [p^*(X | g)p^*(Y | g; K)]$  (1)\nIntuitively, a reasoning shortcut (RS) occurs when a NeSy-CBM with fixed knowledge $K$ attains high or even perfect label accuracy by learning concepts $C$ that differ from the ground-truth ones $G$ [Marconato et al., 2023b]. As commonly done, we work in the setting where $G = C$.\nRSs by definition yield good labels in-distribution, yet they compromise out-of-distribution (OOD) performance. For instance, in autonomous driving tasks, NeSy-CBMs can confuse the concepts of \"pedestrian\" and \"red light\", leading to poor decisions for OOD decisions where the distinction matters [Bortolotti et al., 2024]. The meaning of concepts affected by RSs is unclear, affecting understanding [Schwalbe, 2022], intervenability [Shin et al., 2023, Espinosa Zarlenga et al., 2024, Steinmann et al., 2024b], debugging [Lertvittayakumjorn et al., 2020, Stammer et al., 2021] and down-stream applications that hinge on concepts being high-quality, like NeSy formal verification [Xie et al., 2022, Zaid et al., 2023, Morettin et al., 2024]. Unfortunately, existing works on RSs do not apply to CBMs and NeSy-CBMs where the inference layer is learned."}, {"title": "3 Reasoning Shortcuts in CBMs", "content": "Given a finite set $S$, we indicate with $\\Delta_S \\subset [0, 1]^{|S|}$ the simplex of probability distributions $P(S)$ over items in $S$. Notice that the set of its vertices $Vert(\\Delta_S)$ contains all point mass distributions $1\\{S = s\\}$ for all $s \\in S$.\nCBMs as pairs of functions. CBMs and NeSy-CBMs differ in how they implement the inference layer, hence to bridge them we employ the following unified formalism. Any CBM can be viewed as a pair of learnable functions implementing the concept extractor and the inference layer, respectively, cf. Fig. 2. Formally, the former is a function $f : \\mathbb{R}^n \\rightarrow \\Delta_C$ mapping inputs $x$ to a conditional distribution $p(C | x)$ over the concepts, however it can be better understood as a function $\\alpha : G \\rightarrow A_C$ taking ground-truth concepts $g$ as input instead, and defined as:\n$\\alpha(g) = \\mathbb{E}_{x \\sim p^*(x|g)} [f(x)]$   (2)\nOn the other hand, the inference layer is a function $\\omega : \\Delta_C \\rightarrow \\Delta_Y$ mapping the concept distribution output by the concept extractor into a label distribution $p(Y | f(x))$. For clarity, we also define $\\beta : C \\rightarrow \\Delta_Y$, which is"}, {"title": "3.1 Intended Semantics and Joint Reasoning Shortcuts", "content": "We posit that a CBM $(\\alpha, \\beta) \\in A \\times B$ has \u201chigh-quality\u201d concepts and inference layer if it satisfies two desiderata:\n\u2022 Disentanglement: each learned concept $C_i$ should be equal to a single ground-truth concept $G_j$ up to an invertible transformation;\n\u2022 Generalization: the combination of $\\alpha$ and $\\beta$ must always yield correct predictions."}, {"title": "3.2 Theoretical Analysis", "content": "We proceed to determine how many deterministic JRSs $(\\alpha, \\beta) \\in Vert(A) \\times Vert(B)$ a learning task admits, and then show in Theorem 3.8 that this number determines the presence or absence of general (non-deterministic) JRSS.\nTheorem 3.5 (Informal). Under Assumptions 3.1 and 3.2, the number of deterministic JRSs $(\\alpha, \\beta)$ is:\n$\\sum_{(\\alpha,\\beta)} 1\\{\\Lambda_{g \\in supp(G)} (\\beta \\circ \\alpha)(g) = \\beta^*(g)\\} \u2013 C[G]$   (7)\nwhere the sum runs over $Vert(A) \\times Vert(B)$, and $C[G]$ is the total number of pairs with the intended semantics.\nAll proofs can be found in Appendix C. Intuitively, the first count runs over all deterministic pairs $(\\alpha, \\beta)$ that achieve maximum likelihood on the training set (i.e., the predicted labels $(\\beta \\circ \\alpha)(g)$ matches the ground-truth one $\\beta^*(g)$ for all ground-truth concepts $g$ appearing in the data), while the second term subtracts the pairs $(\\alpha, \\beta)$ that possess the intended semantics as per Definition 3.3. Whenever the count is larger than zero, a CBM trained via maximum likelihood can learn a deterministic JRS.\nAs a sanity check, we show that if prior knowledge K is provided \u2013 fixing the inference layer to $\\beta^*$ \u2013 the number of deterministic JRSs in fact matches that of RSs, as expected:\nCorollary 3.6. Consider a fixed $\\beta^* \\in Vert(B)$. Under Assumptions 3.1 and 3.2, the number of deterministic JRSs $(\\alpha, \\beta^*) \\in Vert(A) \\times Vert(B)$ is:\n$\\sum_{\\alpha \\in Vert(A)} 1\\{\\Lambda_{g \\in supp(G)} (\\beta^* \\circ \\alpha)(g) = \\beta^*(g)\\} \u2013 1$   (8)\nThis matches the count for deterministic RSs in [Marconato et al., 2023b].\nA natural consequence of Corollary 3.6 that whenever $|Vert(B)| > 1$ the number of deterministic JRSs in Eq. (7) can be substantially larger than the number of deterministic RSs Eq. (8). For instance, in MNIST-SumParity with digits restricted to the range [0, 4] there exist exactly 64 RSs but about 100 thousand JRSs, and the gap increases as we extend the range [0, N]. We provide an in-depth analysis in Appendix B.2.\nNext, we show that whenever the number of deterministic JRSs in Eq. (7) is zero, there exist no JRSs at all, including non-deterministic ones. This however only applies to CBMs that satisfy the following natural assumption:\nAssumption 3.7 (Extremality). The inference layer $\\omega \\in \\Omega$ satisfies extremality if, for all $\\lambda \\in (0, 1)$ and for all $c \\neq c' \\in C$ such that $argmax_{y \\in y} \\omega(1\\{C = c\\})_y \\neq argmax_{y \\in y} \\omega(1\\{C = c'\\})_y$, it holds:\n$max_{y \\in y} \\omega(\\lambda 1\\{C = c\\} + (1 - \\lambda)1\\{C = c'\\})_y < max \\{max_{y \\in y} \\omega(1\\{C = c\\})_y, max_{y \\in y} \\omega(1\\{C = c'\\})_y\\}$\nIntuitively, a CBM satisfies this assumption if its inference layer $\\omega$ is \"peaked\": for any two concept vectors $c$ and $c'$ yielding distinct predictions, the label probability output by $\\omega$ for any mixture distribution thereof is no larger than the label probability that it associates to $c$ and $c'$. That is, distributing probability mass across concepts never nets an advantage in terms of label likelihood. While this assumption does not hold for general CBMs, we show in Appendix E that it holds for popular architectures, including most of those that we experiment with. Under Assumption 3.7, the next result holds:\nTheorem 3.8 (Identifiability). Under Assumptions 3.1 and 3.2, if the number of deterministic JRSs (Eq. (7)) is zero then every CBM $(f, \\omega) \\in F \\times \\Omega$ satisfying Assumption 3.7 that attains maximum log-likelihood (Eq. (4)) possesses the intended semantics (Definition 3.3). That is, the pair $(\\alpha, \\beta) \\in A \\times B$ entailed by each such CBM is equivalent to the ground-truth pair $(id, \\beta^*)$, i.e., $(\\alpha, \\beta) \\sim (id, \\beta^*)$."}, {"title": "4 Practical Solutions", "content": "By Theorem 3.8, getting rid of deterministic JRSs from a task prevents CBMs trained via maximum log- likelihood to learn JRS altogether. Next, we discuss strategies for achieving this goal and will assess them empirically in Section 5. An in-depth technical analysis is left to Appendix D."}, {"title": "4.1 Supervised strategies", "content": "The most direct strategies for controlling the semantics of learned CBMs rely on supervision. Concept supervision is the go-to solution for improving concept quality in CBMs [Koh et al., 2020, Chen et al., 2020b, Espinosa Zarlenga et al., 2022, Marconato et al., 2022] and NeSy-CBMs [Bortolotti et al., 2024, Yang et al., 2024a]. Supplying full concept supervision for all ground-truth concepts G provably prevents learning a's that do not match the identity. However, this does not translate into guarantees on the inference layer \u1e9e, at least in tasks affected by confounding factors such as spurious correlations among concepts [Stammer et al., 2021]. E.g., if the i-th and j-th ground-truth concepts are strongly correlated, the inference layer \u1e9e can interchangeably use either, regardless of what \u1e9e* does.\nAnother option is employing knowledge distillation [Hinton, 2015], that is, supplying supervision of the form (g, y) to encourage the learned knowledge \u1e9e to be close to \u1e9e* for the supervised g's. This supervision is available for free provided concept supervision is available, but can also be obtained separately, e.g., by interactively collecting user interventions [Shin et al., 2023, Espinosa Zarlenga et al., 2024, Steinmann et al., 2024b]. This strategy cannot avoid all JRSs because even if \u1e9e = \u1e9e*, the CBM may suffer from regular RSs (i.e., a does not match id).\nAnother option is to fit a CBM on multiple tasks [Caruana, 1997] sharing concepts. It is known that a NeSy-CBM attaining optimal likelihood on multiple NeSy tasks with different prior knowledges is also optimal for the single NeSy task obtained by conjoining those knowledges [Marconato et al., 2023b]: this more constrained knowledge better steers the semantics of the learned concepts, ruling out potential JRSs. Naturally, collecting a sufficiently large number of diverse tasks using the same concepts can be highly non-trivial, depending on the application."}, {"title": "4.2 Unsupervised strategies", "content": "Many popular strategies for improving concept quality in (NeSy) CBMs are unsupervised. A cheap but effective one when dealing with multiple inputs is to process inputs individually, preventing mixing between their concepts. E.g., In MNIST-SumParity one can apply the same digit extractor to each digit separately, while for images with multiple objects one can first segment them (e.g., with YoloV11 [Jocher et al., 2023]) and then process the resulting bounding boxes individually. This is extremely helpful for reducing, and possibly overcoming, RSs [Marconato et al., 2023b] and frequently used in practice [Manhaeve et al., 2018, van Krieken et al., 2022, Daniele et al., 2023, Tang and Ellis, 2023]. We apply it in all our experiments.\nBoth supervised [Marconato et al., 2022] and unsupervised [Alvarez-Melis and Jaakkola, 2018, Li et al., 2018] CBMs may employ a reconstruction penalty [Baldi, 2012, Kingma, 2013] to encourage learning informative concepts. Reconstruction can help prevent collapsing distinct ground-truth concepts into single learned concepts, e.g., in MNIST-SumParity the odd digits cannot be collapsed together without impairing reconstruction. Alternatively, one can employ entropy maximization [Manhaeve et al., 2021] to spread concept activations evenly across the bottleneck. This can be viewed as a less targeted but more efficient alternative to reconstruction, which becomes impractical for higher dimensional inputs. Another popular option is contrastive learning [Chen et al., 2020a], in that augmentations render learned concepts more robust to style variations [Von K\u00fcgelgen et al., 2021], e.g., for MNIST-based tasks it helps to cluster distinct digits in representation space [Sansone and Manhaeve, 2023]. Unsupervised strategies all implicitly counteract simplicity bias whereby a ends up collapsing."}, {"title": "5 Case Studies", "content": "We tackle the following key research questions:\nQ1. Are CBMs affected by JRSs in practice?\nQ2. Do JRSs affect interpretability and OOD behavior?\nQ3. Can existing mitigation strategies prevent JRSs?\nAdditional implementation details about the tasks, architectures, and model selection are reported in Appendix A.\nModels. We evaluate several (also NeSy) CBMs. DeepProbLog (DPL) [Manhaeve et al., 2018, 2021] is the only method supplied with the ground-truth knowledge K, and uses probabilistic-logic reasoning to ensure predictions are consistent with it. CBNM is a Concept-Bottleneck Model [Koh et al., 2020] with no supervision on the concepts. Deep Symbolic Learning (DSL) [Daniele et al., 2023] is a SOTA NeSy-CBM that learns"}, {"title": "6 Related Work", "content": "Shortcuts in machine learning. Shortcuts occur when machine learning models solve a prediction task by using features that correlate with but do not cause the desired output, leading to poor OOD behavior [Geirhos et al., 2020, Teso et al., 2023, Ye et al., 2024, Steinmann et al., 2024a]. The shortcuts literature however focuses on black-box models rather than CBMs. Existing studies on the semantics of CBM concepts [Margeloiu et al., 2021, Mahinpei et al., 2021, Havasi et al., 2022, Raman et al., 2023] focus on individual failures but propose no well-defined, formal notion of intended semantics. One exception is [Marconato et al., 2023a] which however, like other works, ignores the role of the inference layer. We build on known results on reasoning shortcuts [Li et al., 2023, Marconato et al., 2023b, Wang et al., 2023, Umili et al., 2024] which are restricted to NeSy-CBMs in which the prior knowledge is given and fixed. Our analysis upgrades these results to general CBMs. Furthermore, our simulations indicate that strategies that are effective for CBMs and RSs no longer work for JRSs. At the same time, while NeSy approaches that learn the prior knowledge and concepts jointly are increasingly popular [Wang et al., 2019, Yang et al., 2020, Liu et al., 2023, Daniele et al., 2023, Tang and Ellis, 2023, W\u00fcst et al., 2024], existing work neglects the issue of shortcuts in this setting. Our work partially fills this gap.\nRelation to identifiability. Works in Independent Component Analysis and Causal Representation Learning focus on the identifiability of representations up to an equivalence relation, typically up to permutations and rescaling [Khemakhem et al., 2020a, Gresele et al., 2020, von K\u00fcgelgen et al., 2024] or more general transformations [Roeder et al., 2021, Buchholz et al., 2022]. The equivalence relation introduced along with intended semantics (Definition 3.3) establishes a specific connection between the maps & and B. This aligns with existing works that aim to identify representations and linear inference layers using supervised labels [Lachapelle et al., 2023, Fumero et al., 2023, Bing et al., 2023]. Taeb et al. [2022] studied the identifiability of a VAE-based CBM with continuous-valued concepts. In contrast, we provide an analysis of a broader class of (NeSy) CBMs commonly used in practice. Moreover, while prior works primarily focus on identifying continuous-valued representations, we examine the identifiability of discrete concepts and of the inference layer. [Rajendran et al., 2024] explores the identifiability of (potentially discrete) concepts linearly encoded in model representations but is limited to a multi-environment setting, not including supervised labels. Our results differ in that we show how concepts can be identified in CBMs by circumventing joint reasoning shortcuts."}, {"title": "7 Conclusion", "content": "We study the issue of learning CBMs and NeSy-CBMs with high-quality quality concepts and inference layers. To this end, we formalize intended semantics and joint reasoning shortcuts (JRSs), show how reducing the number deterministic JRSs to zero provably prevents all JRSs, yielding identifiability, and investigate natural mitigation strategies. Numerical simulations indicate that JRSs are very frequent and impactful, and that the only (partially) beneficial mitigation is contrastive learning. Our analysis aims to pave the way to the design of effective solutions and therefore more robust and interpretable CBMs.\nIn future work, we plan to extend our theory to concept learning in general neural nets and large language models. Moreover, we will take a closer look at more complex mitigations such as techniques for smartly constraining the inference layer [Liu et al., 2023, W\u00fcst et al., 2024], debiasing [Bahadori and Heckerman, 2021], and interaction with human experts [Teso et al., 2023]."}, {"title": "A Implementation Details", "content": "Here, we provide additional details about metrics, datasets, and models, for reproducibility. All the experiments were implemented using Python 3.9 and Pytorch 1.13 and run on one A100 GPU. The implementations of DPL, and the code for RSs mitigation strategies were taken verbatim from [Marconato et al., 2023b], while the code for DSL was taken from [Daniele et al., 2023]. DPL* and bears* were implement on top of DSL codebase."}, {"title": "A.1 Datasets", "content": "All data sets were generated using the rsbench library [Bortolotti et al., 2024]."}, {"title": "A.1.1 MNIST-Add", "content": "MNIST-Add [Manhaeve et al., 2018] consists of pairs of MNIST digits [LeCun, 1998], ranging from 0 to 9, and the goal is to predict their sum. The prior knowledge used for generating the labels is simply the rule of addition: K = (Y = C1 + C2). This task does not admit RSs nor JRSs when digits are processed separately. Below we report four examples:\n{\\{((0,1), 1), ((5,8), 13), ((6,6), 12), ((4, 1), 5)\\}   (9)\nAll splits cover all possible pairs of concepts, from (0,0) to (9,9), i.e., there is no sampling bias. Overall, MNIST-Add has 42, 000 training examples, 12, 000 validation examples, and 6, 000 test examples."}, {"title": "A.2 MNIST-SumParity", "content": "Similarly, in MNIST-SumParity the goal is to predict the parity of the sum, i.e., the prior knowledge is the rule: K = (Y = (C1 + C2) mod 2). This task is more challenging for our purposes, as it is affected by both RSs and JRSs, cf. Fig. 1. Below we report four examples:\n{\\{((0,1),1), ((5,8),1), ((6,6),0), ((4, 1), 1)\\}   (10)\nWe consider two variants: like MNIST-Add, in MNIST-SumParity proper the training and test splits contain all possible combinations of digits, while in biased MNIST-SumParity the training set contains (even, even), (odd, odd) and (odd, even) pairs, while the test set contains (even, odd) pairs. This fools CBMs into learning an incorrect inference layer implementing subtraction instead of addition-and-parity, as illustrated in Fig. 1. Both variants have the same number of training, validation, and test examples as MNIST-Add."}, {"title": "A.3 Clevr", "content": "Clevr [Johnson et al., 2017] consists of of 3D scenes of simple objects rendered with Blender, see Fig. 4. Images are 3 \u00d7 128 \u00d7 128 and contain a variable number of objects. We consider only images with 2 to 4 objects each, primarily due to scalability issues with DPL. The ground-truth labels were computed by applying the rules (prior knowledge) proposed by [Stammer et al., 2021], reported in Table 6. Objects are entirely determined by four concepts, namely shape, color, material, and size. We list all possible values in Table 5. Overall, Clevr has 6000 training examples, 1200 validation examples, and 1800 test examples.\nPreprocessing. All CBMs process objects separately. Following [Shindo et al., 2023], we fine-tune a pretrained YoloV11 model [Jocher et al., 2023] (for 10 epochs, random seed 13) on a subset of the training set's bounding boxes, and use it to predict bounding boxes for all images. For each training and test image, we concatenate the regions in the bounding boxes, obtaining a list 2 to 4 images which plays the role of input for our CBMs.\nEach object has 8 \u00d7 3 \u00d7 2 \u00d7 2 = 96 possible configurations of concept values, and we handle between 2 and 4 objects, hence the inference layer has to model $96^2 +96^3 + 96^4$ distinct possibilities. Due to the astronomical"}, {"title": "A.4 Metrics", "content": "For all models, we report the metrics averaged over five random seeds.\nLabel quality. We measure the macro average F\u2081-score and Accuracy, to account for class imbalance.\nConcept quality. We compute concept quality via Accuracy and F\u2081-score. However, the order of ground-truth and predicted concepts might might differ, e.g., in Clevr G\u2081 might represent whether \u201ccolor = red\", while C1 whether \"color = blue\". Therefore, in order to compute these metrics we have to first align them using the Hungarian matching algorithm using concept-wise Pearson correlation on the test data [Kuhn, 1955]. The algorithm computes the correlation matrix R between g and c, i.e., Rij = corr(Gi, Cj) where corr is the correlation coefficient, and then infers a permutation \u03c0 between concept values (e.g., mapping ground-truth color \"red\" to predicted value \u201cblue\u201d) that maximizes the above objective. The metrics are computed by comparing the reordered ground-truth annotations with the predicted concepts.\nInference layer quality. The same permutation is also used to assess knowledge quality. Specifically, we apply it to reorder the ground-truth concepts and then feed these to the learned inference layer, evaluating the accuracy and F\u2081 score of the resulting predictions.\nFor MNIST-Add and MNIST-SumParity, we conducted an exhaustive evaluation since the number of possible combinations to supervise is 100. The results in Table 1 and Table 2 evaluate the knowledge exhaustively, whereas in Table 4, we separately assess the knowledge of in-distribution and out-of-distribution combinations.\nIn Clevr, as discussed in Appendix A.3, there are too many combinations of concept values to possibly evaluate them all. Therefore, both supervision and evaluation are performed by randomly sampling 100 combinations of ground-truth concepts."}, {"title": "A.5 Architectures", "content": "For MNIST-SumParity and MNIST-Add, We employed the architectures from [Bortolotti et al., 2024], while for DSL, DPL*, and bears*, we followed the architecture described in [Daniele et al., 2023]. For Clevr instead, we adopted the decoder presented in Table 7 and the encoder presented in Table 8. All models process different objects (e.g., digits) separately using the same backbone."}, {"title": "A.6 Mitigation Strategies", "content": "Concept supervision. When evaluating mitigation strategies, concept supervision for MNIST-SumParity was incrementally supplied for two more concepts at each step, following this order: 3, 4, 1, 0, 2, 8, 9, 5, 6, 7. For Clevr, supervision was specifically provided for sizes, shapes, materials, and colors, in this order.\nKnowledge distillation. This was implemented using a cross-entropy on the inference layer. For DSL and DPL, ground-truth concepts were fed into the learned truth table, expecting the correct label. Similarly, for CBNMS, which use a linear layer, the same approach was applied. In evaluating the mitigation strategies, we supervised all possible combinations for MNIST-Add, whereas for Clevr, we supervised a maximum of 500 randomly sampled combinations.\nEntropy maximization. The entropy regularization term follows the implementation in [Marconato et al., 2023b]. The loss is defined as $1 - \\frac{1}{l} \\sum_{i=1}^l H_{mi} [P_\\Theta(c_i)]$ where $p_\\Theta(C)$ represents the marginal distribution over the concepts, and $H_{mi}$ is the normalized Shannon entropy over $m_i$ possible values of the distribution.\nReconstruction. The reconstruction penalty is the same as in [Marconato et al., 2023b]. In this case, the concept extractor outputs both concepts c and style variables z, i.e., it implements a distribution $p_\\Theta(c, z|x) = p_\\Theta(c|x) \\cdot p_\\Theta(z|x)$. The reconstruction penalty is defined as: $-\\mathbb{E}_{(c,z) \\sim p_\\Theta(c,z|x)} [log p_\\gamma(x|c, z)]$ where $p_\\gamma(x|c, z)$ represents the decoder network.\nContrastive loss. We implemented contrastive learning using the InfoNCE loss, comparing the predicted concept distribution of the current batch with that of its respective augmentations. Following the recommendations of Chen et al. [2020a], we applied the following augmentations to each batch: random cropping (20 pixels for MNIST and 125 pixels for Clevr), followed by resizing, color jittering, and Gaussian blur."}, {"title": "A.7 Model Selection", "content": "Optimization. For DPL*, bears*, CBNM and SENN we used the Adam optimizer [Kingma, 2014", "2021": "."}]}