{"title": "GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs", "authors": ["Yun Zhu", "Haizhou Shi", "Xiaotang Wang", "Yongchao Liu", "Yaoke Wang", "Boci Peng", "Chuntao Hong", "Siliang Tang"], "abstract": "Recently, research on Text-Attributed Graphs (TAGs) has gained significant attention due to the prevalence of free-text node features in real-world applications and the advancements in Large Language Models (LLMs) that bolster TAG methodologies. However, current TAG approaches face two primary challenges: (i) Heavy reliance on label information and (ii) Limited cross-domain zero/few-shot transferability. These issues constrain the scaling of both data and model size, owing to high labor costs and scaling laws, complicating the development of graph foundation models with strong transferability. In this work, we propose the GraphCLIP framework to address these challenges by learning graph foundation models with strong cross-domain zero/few-shot transferability through a self-supervised contrastive graph-summary pretraining method. Specifically, we generate and curate large-scale graph-summary pair data with the assistance of LLMs, and introduce a novel graph-summary pretraining method, combined with invariant learning, to enhance graph foundation models with strong cross-domain zero-shot transferability. For few-shot learning, we propose a novel graph prompt tuning technique aligned with our pretraining objective to mitigate catastrophic forgetting and minimize learning costs. Extensive experiments show the superiority of GraphCLIP in both zero-shot and few-shot settings, while evaluations across various downstream tasks confirm the versatility of GraphCLIP. Our code is available at: https://github.com/ZhuYun97/GraphCLIP", "sections": [{"title": "1 Introduction", "content": "Text-Attributed Graphs (TAGs) have gained significant attention recently [5, 9, 14, 30, 50, 72, 77, 93] due to the free-text node feature space prevalent in various domains such as social, e-commerce, and citation networks [6, 26, 66]. TAGs offer two natural advantages for graph learning research: (i) all node features can be aligned into within the same text space, enabling the model to transfer effectively across different graphs, and (ii) powerful off-the-shelf tools can be readily leveraged to address challenges within TAGs, e.g., Large Language Models (LLMs) can be used for enriching the textual representations of TAGs.\nExisting TAG methods with LLMs. Significant efforts are underway to combine TAGs with LLMs, aiming to develop Graph Foundation Models (GFMs) [4, 21, 30, 31, 33, 60], a promising approach that enables the transfer of knowledge from source data to tackle various downstream tasks on target data through a unified backbone. Existing methods can be categorized into three main types [21, 30]: LLM as Enhancer, LLM as Predictor, and LLM as Aligner, as illustrated in Figure 1.\nLLM as Enhancer involves leveraging language models to augment raw text, yielding refined, high-quality outputs, or to encode node features, surpassing previous shallow embedding methods like Bag of Words (BoW). For instance, OFA [33] and ZeroG [31] utilize language models as node feature extractors, employing labeled source data to pretrain a graph model, which is then applied to target data using complicated graph prompt techniques. These methods depend heavily on high-quality labeled data, potentially limiting the full potential of graph foundation models due to scaling laws [15, 22]. The challenge arises from the difficulty of scaling up the pretraining corpus, primarily due to the associated high labor costs. In the paradigm of LLM as Predictor, the most essential task is to map graph data to a format that LLMs can comprehend. For example, GraphGPT [60] and LLaGA [4] utilize GNNs to encode graph data into graph tokens, training an additional projector to map these tokens into the text space through instruction tuning. However, these methods also require high-quality labels for target data, and recent studies [5, 32] have shown they exhibit poor cross-domain zero-shot performance. LLM as Aligner involves mapping graph and text modalities into a shared embedding space. For example, ConGrat [3] and G2P2 [73] apply self-supervised graph-text contrastive pretraining and focus on transferring pretrained models within the same graph, neglecting the cross-domain or cross-graph transferability of these models.\nChallenges for current TAG methods. As summarized, current methodologies encounter two primary challenges:\n(i) Heavy reliance on label information: Most current approaches such as ZeroG [31], OFA [33], LLaGA [4], and others [14, 60, 71, 72, 93] require label information from source data as training signals, leading to significant labor costs. This constraint prevents GFMs from leveraging extensive unlabeled training corpora, thereby restricting the scaling of both data and model sizes in accordance with scaling laws [15].\n(ii) Limited cross-domain zero/few-shot transferability: A well-pretrained GFM should be applicable directly to target data, achieving satisfactory performance without any parameter adjustments, i.e., strong cross-domain/dataset zero-shot capability, like CLIP [51] in multi-modal domain [29, 34, 46, 48, 83] and LLMs in NLP domain [45, 50, 62, 87]. However, most existing methods struggle with direct deployment in zero-shot settings on target data. They either cannot perform zero-shot learning because they require the training of a classification head using labeled target data to generate predictions [14, 85, 93], or they demonstrate inadequate zero-shot performance due to insufficient transferable knowledge acquired during pretraining [4, 33]. Additionally, in low-resource scenarios like few-shot learning, effectively leveraging limited training samples from target data while circumventing catastrophic forgetting poses a significant challenge.\nOur proposed GraphCLIP framework. We develop GraphCLIP to address the challenges above, which learns graph foundation models with robust cross-domain zero-shot transferability from a novel self-supervised contrastive graph-summary pretraining technique. Specifically, we leverage the open LLM, QWen2-72B [78], to generate graph-related summary texts of subgraphs, capitalizing on their powerful summarization abilities. Utilizing generated large-scale graph-summary pairs, we train a cross-domain graph foundation model through designed self-supervised contrastive graph-summary pretraining (addressing Challenge (i)). Considering the necessity of out-of-domain generalization across graphs, we introduce invariant learning during pretraining to capture invariant features, thereby enhancing out-of-domain generalization. After pretraining, GraphCLIP can be applied directly to target data without fine-tuning, demonstrating strong zero-shot capability on both in-domain and cross-domain graph data (addressing Challenge (ii)). For few-shot scenarios, we propose a novel graph prompt tuning approach aligned with our pretraining objective, reducing catastrophic forgetting [56] and minimizing learning costs [37, 57], ensuring excellent performance in few-shot settings. Extensive experiments show that GraphCLIP exhibits strong zero-shot performance across various in-domain and cross-domain target datasets. In few-shot settings, GraphCLIP with prompt tuning outperforms previous state-of-the-art methods. Additionally, the universality of our method is demonstrated through evaluation across various downstream tasks.\nOur contributions can be concluded as:\n\u2022 We generate and curate a large-scale graph-summary pair dataset which contains over 0.2B tokens with the assistance of LLMs, establishing a valuable training corpus for the TAGs domain and advancing the field's development.\n\u2022 We propose GraphCLIP, a novel language-graph pretraining method combined with invariant learning that empowers cross-domain graph foundation models with strong zero-shot capability.\n\u2022 We introduce a novel graph prompt tuning technique aligned with our pretraining objectives for few-shot settings, mitigating catastrophic forgetting and minimizing learning costs.\n\u2022 Through extensive experiments, GraphCLIP demonstrates impressive performance in both zero-shot and few-shot scenarios. Furthermore, various downstream tasks are evaluated to validate the universality of GraphCLIP."}, {"title": "2 Preliminaries", "content": "2.1 Notations\nIn this work, we focus on Text-Attributed Graphs, which incorporate raw text information for each node. Formally, given a text-attributed graph $G = {V, {T_n}_{n=1}^N, A}$, where V denotes the node set with |V| = N instances, $T_n \\in T^{L_n}$ represents the raw text for node $n \\in [1, 2 ..., N]$, T is the token dictionary, and $L_n$ is the sequence length, $A \\in R^{N \\times N}$ denotes the adjacency matrix. To enhance scalability, a sampling function $\\Gamma(\\cdot)$ is applied to a large graph to derive a set of small ego-graphs $I = {G_n}_{n=1}^N$, where $G_n$ represents the subgraph centered on node $n \\in [1, 2 ..., N]$.\n2.2 Problem Definition\nTo develop a graph foundation model, extensive source graph data $G^s$ can be utilized to train a general graph model endowed with transferable knowledge:\n$\\qquad f_\\theta = \\arg \\min_{f} E_{G \\in I^s} L_{\\text{pretext}} (f_\\theta; G),$ (1)\nwhere $I^s$ represents the set of sampled subgraphs derived from the source data, $f_\\theta$ means graph neural networks like GCN [27], GAT [64] and Graph Transformer [52, 79], $L_{\\text{pretext}}$ denotes pretext task like instance discrimination [12]. The optimal pretrained model $f_\\theta$ can then be applied to low-resource target graph data $G^t$ to perform downstream tasks such as node classification, link prediction, and graph classification.\nIn this work, we focus on low-resource settings, including zero-shot and few-shot scenarios, which are critical capabilities for GFMs."}, {"title": "3 Method", "content": "In this section, we present our approach to addressing the aforementioned challenges. First, we introduce the technique for generating and curating source data in Section 3.1. Based on this pretraining corpus, we then design a novel contrastive language-graph pretraining method to develop a graph foundation model in Section 3.2. Lastly, we outline the implementation of zero-shot learning on target data and propose a novel graph prompt tuning method for few-shot settings to fully leverage our model's potential in Section 3.3.2. A detailed complexity analysis of GraphCLIP is provided in Appendix F.\n3.1 Graph-Summary Pair Generation\nIn the TAG domain, there is abundant text describing each node, most current TAG methods leverage this textual information alongside structural data to pretrain graph models; however, these approaches heavily depend on label information [14, 31, 33, 72, 85, 93], restricting scalability due to high labeling costs. Alternatively, some approaches [3, 73] design self-supervised training signals using original text information. Nevertheless, a substantial gap persists between raw textual data and graph-level information, leading to suboptimal performance and limited model transferability. These constraints hinder the development of graph foundation models comparable to CLIP [51] and BLIP [29] in the multimodal domain, which successfully leverage robust self-supervised signals.\nTo resolve this challenge, we exploit the remarkable summarization capabilities of LLMs to generate pertinent summaries for graphs to construct graph-summary pair data. Specifically, we employ a graph XML-like markup language, such as GraphML [2], and meticulously design prompt templates to enhance the LLMs' comprehension of input graphs, leveraging their adeptness with markup languages [76]. The proposed prompt template for transcribing citation network into markup language is in Template 3.1. In this template, we design two attributes for node content, title and abstract. Additionally, we establish one attribute for describing edge type, e.g., cited, co-purchased or liked. The blue text denotes placeholders that will be replaced with actual data. Using this template, we can seamlessly transform TAG into a format that LLMs can easily comprehend [76].\nConsidering scalability, we employ a sampling function, random walk with restart sampler, to sample subgraphs ${G_n}_{n=1}^N$ from a large graph. These subgraphs will be incorporated into our prompt template and designed instructions to enable LLMs to generate"}, {"title": "3.2 Self-Supervised Graph-Summary Contrastive Pretraining", "content": "For graph-summary pair data, we employ different encoders to process their information according to their respective modalities. Then, a novel contrastive loss combined with invariant learning is deployed to align these modalities into the same subsapce.\n3.2.1 Graph Encoding. Considering that model scale is crucial for the emergence and homogenization [35] of graph foundation models, we utilize Graph Transformers (GTs) [52, 70, 75, 79] instead of small GNN models like GCN [27] and GAT [64] to encode graph information. Given a subgraph $G_i = (P_i, X_i, A_i)$, we encode the graph information as follows:\n$\\qquad h_i = P(g_\\theta (P_i, X_i, A_i)),$ (4)\nwhere $g_\\theta$ denotes the Graph Transformer, such as GraphGPS [52], $P_i$ represents the positional embeddings for the subgraph, e.g., RWPE [7], and P is the mean pooling function that yields the graph-level encoding $h_i \\in R^{1 \\times d}$ for the subgraph.\n3.2.2 Summary Encoding. To encode sentence or document-level information into a compact and semantically rich vector, we utilize sentence-level text encoders like SBERT [53]:\n$\\qquad u_i = P(f_\\phi(S_i)) = P(LM([CLS], S_1, S_2, ..., S_L))$ (5)"}, {"title": "3.2.3 Graph-Summary Contrastive Pretraining", "content": "After obtaining the graph and summary encodings H,U \u2208 $R^{N \\times d}$, we employ contrastive loss [44] to align the two modalities. Unlike previous multimodal pretraining methods such as CLIP, different graphs can vary significantly across domains, making it essential to capture transferable or causal features in the graph domain. To achieve this goal, we introduce invariant learning [1, 89] efficiently to extract causal features rather than spurious ones. Below we first revisit the concepts of contrastive loss [44] and invariant learning [1]. Then we formulate how they can be combined to solve the challenges of graph foundation models.\nDEFINITION 1 (CONTRASTIVE LOSS [44]). The contrastive loss function is applied to representations, pulling together the positive pairs while pushing apart negative pairs:\n$L_{CL} (g_\\theta, f_\\phi; P, Q_G, Q_S, \\pi) =$\n$\\qquad  E_{(G,S) \\sim P} [||f_\\phi (\\tau_\\beta(S)) - g_\\theta (\\tau_\\alpha(G))||^2] +$\n$\\qquad B \\log E_{G' \\sim Q_G, S' \\sim Q_S, } e^{-||f_\\phi (\\tau'(\\beta'(S'))) - g_\\theta (\\tau'(\\alpha'(G')))||^2},\\qquad$ (6)\nwhere $G, S \\sim P$ represent positive pairs sampled from the joint distribution of graphs and summaries, while $Q_G$ and $Q_S$ denote the marginal distributions of graphs and summaries, respectively. $\\tau$ refers to the set of data transformations (augmentations) used to generate augmented views. The second line in Equation 6 is termed the alignment loss, and the third line is termed the uniformity loss [68].\nHowever, this loss is not robust to distribution shifts [88, 92] because the expectation operator over different data transformation in the alignment loss can not guarantee the invariant features, resulting poor transferability. Refer to Appendix A for detailed derivation. In order to solve this dilemma, we will combine invariant learning into our method.\nDEFINITION 2 (INVARIANT LEARNING [1]). If a classifier $c_\\omega^*$ is considered simultaneously optimal for all domains in H, then a data representation $g_\\theta$ can elicit an invariant predictor $c_\\omega^* \\circ g_\\theta$ across the domain set H:\n$\\qquad c_\\omega^* \\in \\arg \\min_{c_\\omega} R(c_\\omega g_\\theta; G) \\text{ for all } G \\in H,$ (7)\nwhere R denotes the risk associated with the predictor $c_\\omega \\circ g_\\theta$ evaluated on the domain G.\nHowever, this method heavily relies on environment and downstream labels [1], which is not compatible with our self-supervised contrastive loss. To address this issue, we combine the merits of invariant learning and vanilla contrastive loss to obtain a shift-robust"}, {"title": "3.3 Model Adaptation on Target Data", "content": "In this section, we introduce the techniques employed to adapt models to target datasets. First, we illustrate the adaptation of GraphCLIP on target data for zero-shot learning. Then, we propose a novel graph prompt tuning method for few-shot learning.\n3.3.1 Zero-shot Learning. Upon pretraining with Equation 9, our model can be directly deployed on target datasets without any additional training, i.e., enabling zero-shot inference as depicted in Figure 2b. We meticulously craft the prompt to incorporate target label information. For example, in the context of a citation network, the sentence associated with label information is formulated as \"This paper belongs to {class}\". The specific templates are detailed in the Appendix C. Formally, the formulation of zero-shot learning is as follows:\n$\\qquad \\hat{y}_i = \\arg \\max_k E_{u_k} \\text{sim}(h_i, u_k),$ (10)\nwhere sim denotes the cosine similarity function, we identify the most similar label sentence as the predicted label for node i."}, {"title": "3.3.2 Graph Prompt Tuning under Few-shot Setting", "content": "In low-resource scenarios, where only a few samples exist for each class of target data, effectively utilizing this data while preventing overfitting and catastrophic forgetting [39, 43, 47, 56, 94] becomes crucial. In this work, we introduce a novel graph prompt tuning approach to address this challenge.\nSpecifically, during the graph prompt tuning process, both the text and graph models remain frozen, allowing only a limited set of parameters to be learnable. To align with our pretraining objective, we incorporate a learnable prompt feature that resemble perturbations used during pretraining, and we employ supervised contrastive loss [24]. The total loss of our designed graph prompt tuning is as follows:\n$\\qquad \\min_\\sigma E_{(G, Z) \\sim p_{tar}} [L_{SCL} (g_{\\theta^*}+\\sigma (X + \\delta, A, P), f_{\\phi^*}(Z))],$ (11)\nwhere $g_{\\theta^*}$ and $f_{\\phi}$ denote the frozen graph and text models, respectively, Z represents the label-related sentence, and $p_{tar}$ signifies the distribution of labeled target data. $L_{SCL}$ is the supervised contrastive loss [74], which considers pairs with the same labels as positive and those with different labels as negative. After the graph prompt tuning, the evaluation on the target testing data proceeds in the same manner as outlined in Equation 10."}, {"title": "4 Experiments", "content": "In this section, we first introduce the datasets used in Section 4.1 and the baselines in Section 4.2. We then aim to address the following research questions through our experiments: RQ1: How good is the GraphCLIP's in-domain and cross-domain zero-shot transferability? RQ2: How effective is our proposed graph tuning in few-shot scenarios? RQ3: What impact does the source data have on cross-domain transferability? RQ4: What is the effect of hyperparameters on performance? RQ5: How do the main components of our model influence performance?\n4.1 Datasets\nIn this work, we utilize 12 open text-attributed graphs across four diverse domains, comprising 5 large-scale TAGs for source data during pretraining and 7 small-scale TAGs for target data evaluation. The statistics of these datasets are detailed in Table 1. To balance the ratio of different source data, we employ the training set of ogbn-Products as pretraining corpus, which consists of around 200K products. More details of these datasets can be found in Appendix D.\n4.2 Baselines\nTo evaluate the effectiveness of GraphCLIP, we compare it against 16 baselines, which include: (i) eight LLM-only methods (without modeling structural information of the graphs), i.e., BERT [23], SBERT [53], DeBERTa [13], E5 [67], Qwen2-7B-Insturct [78], Qwen2-72B-Insturct [78], LLaMA3.1-8B-Instruct [63], and LLaMA3.1-Insturct-70B [63], (ii) 4 state-of-the-art TAG methods, i.e., GraphGPT [60], LLaGA [4], OFA [33], and ZeroG [31], and (iii) 4 self-supervised graph algorithms applied to TAGs, i.e., DGI [65], GRACE [95], BGRL [61] and GraphMAE [16]. Specifically, to assess the zero-shot performance of discriminative LLMs and"}, {"title": "4.3 Zero-Shot Inference on Target Data (RQ1)", "content": "In order to evaluate the zero-shot transferability of our pretrained model, we conduct experiments of node-level and link-level tasks.\n4.3.1 Node Classification. In this subsection, we will perform zero-shot node classification by directly applying pretrained models to target datasets.\nExperimental Setup. For the LLM baselines, we directly apply them to the source data, as they have already been pretrained on extensive corpora, and we find that continuing to pretrain them on our source data deteriorates performance on the target data. For GraphGPT, we utilize their released checkpoint\u00b2 and conduct experiments under our settings. For other methods, we use their official codes and pretrain models using the source datasets specified in Table 1 before applying them to the target datasets. For data splitting, we use the public split for the WikiCS dataset, while for other datasets, we randomly select 20% as testing samples. We report the mean accuracy along with the standard deviation after five runs with different random seeds.\nAnalysis. From Table 2, we can draw several conclusions. First, generative LLMs demonstrate decent zero-shot performance, particularly LLaMA3.1-70B and Qwen2-72B, attributed to their extensive parameters and training on vast source data. However, these models struggle to leverage structural information, resulting in subpar performance on certain target datasets; for instance, LLaMA3.1-70B only achieves 37.67% and 43.68% on the WikiCS and Instagram datasets, respectively.\nSecond, LLaGA and GraphGPT employ graph instruction tuning to bridge this gap, but they tend to overfit to the source data, leading to poor generalization. This is evident as GraphGPT and LLaGA achieve only 6.3% and 2.65% accuracy on the WikiCS dataset, significantly trailing behind other methods."}, {"title": "4.3.2 Link Prediction", "content": "In this subsection, we perform zero-shot link prediction by directly applying the pretrained models to the target datasets.\nExperimental Setup. For link prediction, we report the mean AUC score along with the standard deviation after five runs with different random seeds. Since generative LLM methods produce text outputs, which complicates the extraction of logits or probabilities, we exclude them from this evaluation. For the data splitting, we randomly select 50% as testing samples and employ the same pretrained models as discussed in the previous section.\nAnalysis. From Table 2, we observe that SBERT achieves decent performance. ZeroG, which relies on SBERT and integrates structural information to finetune SBERT through LoRA [17], achieves subpar performance. Notably, our approach demonstrates the best zero-shot performance on link prediction across the evaluated target datasets, highlighting the effectiveness of our designed self-supervised pretraining task and the versatility of our framework."}, {"title": "4.4 Graph Prompt Tuning (RQ2)", "content": "In low-resource scenarios, where target datasets contain only a few training samples, effectively utilizing these samples while preventing overfitting and catastrophic forgetting is crucial. In this section, we evaluate our graph prompt tuning approach to address this challenge.\nExperimental Setup. We compare our method with several classical graph prompt tuning methods for node classification, i.e., GPPT [57], GraphPrompt [37], GPF [8], and All-in-One [58]. Each of these methods is applied to our pretrained model, and we evaluate their performance using 1, 3, 5, and 10 shots per class, reporting the mean accuracy.\nAnalysis. From Figure 3, we observe that GPPT and GraphPrompt fall behind significantly in the 1-shot and 3-shot settings. This underperformance can be attributed to the need for initializing an additional linear head for prediction, preventing direct use of the aligned text model for predictions. However, as the number of shots per class increases, these methods close the performance gap, becoming comparable with others. In contrast, GPF and All-in-One"}, {"title": "4.5 Explorations on Source Datasets (RQ3)", "content": "In this section, we explore the selection of source data for both cross-domain and in-domain transferability. We mask different source domains and evaluate performance on the CiteSeer (Academic), WikiCS (Wikipedia), History (E-commerce), and Instagram (Social) datasets, as illustrated in Figure 4. The term 'Full' denotes utilizing all source data as described in Table 1. 'w/o Academia' means excluding academic source datasets, i.e., ogbn-ArXiv, ArXiv_2023, and PubMed. 'w/o E-commerce' indicates the exclusion of the e-commerce source dataset, while 'w/o Social' means omitting the Reddit dataset.\nFrom Figure 4, we draw several conclusions. First, a greater amount of source data enhances cross-domain transferability; for instance, 'Full' achieves the best performance on the WikiCS dataset. Second, in-domain source data is critical for in-domain transferability, as demonstrated by 'w/o Academia', which significantly lags behind 'Full' on the CiteSeer dataset, and 'w/o E-commerce', which is inferior to 'Full' on the History dataset. Third, while combining all domains may slightly hurt in-domain transferability, it generally improves overall performance. For example, the performance of 'Full' on the History dataset is slightly lower than 'w/o Academia' but substantially better on the CiteSeer dataset. For simplicity, we use all source data throughout this paper. More complex combinations will be addressed in future work, as balancing the ratio of different source domains remains essential yet challenging."}, {"title": "4.6 Analysis on Model Scale (RQ4)", "content": "We explore the impact of model scale in this section. Since the text model remains frozen, our focus is primarily on the scale of the graph model, specifically the graph transformer [52]. We construct four different model scales: Small, Medium, Base and Large. The Small model comprises 4 layers with the hidden size of 512, the Medium model consists of 8 layers with the hidden size of 768, and the Base model, which is used as our primary model throughout this paper, consists of 12 layers with the hidden size of 1024. The Large model has 16 layers with the hidden size of 1024.\nFrom Table 4, we observe that the Base model consistently outperforms the smaller models, likely due to the increased number of parameters [15]. However, while increasing the number of layers to 16 marginally improves performance, it introduces significant computational overhead and, in some cases, even hinders performance on certain target datasets. As a result, we adopt the Base model as our primary model throughout this work."}, {"title": "4.7 Ablation Study (RQ5)", "content": "In this section, we investigate the impact of different components in GraphCLIP by masking them individually. The term 'w/o Summary' signifies the use of the original text for each node instead of the generated summaries introduced in Section 3.1 as the text modality input. 'w/o Freeze' denotes the non-freezing of the text model, and 'w/o IL' indicates the removal of invariant learning from our pretraining loss.\nFrom Table 5, it is evident that the generated summaries are crucial for achieving zero-shot transferability. The original text, which contains only individual node content, lacks structural information, resulting in a gap between the text and graph modalities. Additionally, original text may include noisy information, leading to suboptimal performance. For example, 'w/o Summary' achieves 41.65% and 46.26% on the Photo and Computers datasets, respectively, falling over 10 absolute percentage points behind GraphCLIP. The 'w/o Freeze' condition shows the poorest performance on the WikiCS and History datasets, with scores of 58.92% and 29.40%, respectively. This suggests that fully tuning the text model may lead to overfitting on the source data, impairing transferability. Lastly, 'w/o IL' performs worse than GraphCLIP on most datasets, indicating that incorporating invariant learning into the pretraining loss significantly enhances both cross-domain and in-domain transferability."}, {"title": "5 Related Work", "content": "5.1 Text-Attributed Graph Methods with LLMs\nResearch on TAGs has gained great attention with rapid development of LLMs, classified into three categories [30]: LLM as Enhancer, LLM as Predictor, and LLM as Aligner, as depicted in Figure 1.\nLLM as Enhancer [14, 33, 71, 93] involves augmenting raw text or encoding node features, surpassing traditional methods like Bag of Words (BoW). For example, TAPE [14] uses ChatGPT to enhance node attributes, while OFA [33] and ZeroG [31] utilize language models to unify node features and introduce innovative graph prompting techniques to standardize various tasks. LLM as Predictor [4, 60, 80, 81, 86] uses LLMs to predict graph data by converting it into a comprehensible format. GraphText [86] employs a G-Syntax Tree to transform graph data into text sequences, while GraphGPT and LLaGA utilize GNNs to encode graph data into tokens, requiring labeled data for training projector but showing limited transferability [5, 32]. LLM as Aligner [3, 49, 73, 85] maps graph and text modalities into a shared embedding space. GLEM [85] optimizes GNNs and LLMs through iterative training, while ConGrat [3] and G2P2 [73] focus on node-text contrastive pretraining, lacking of graph summary text information.\nWhile TAG methods have achieved significant success, they face two primary challenges: (1) heavy reliance on label information, and (2) poor zero/few-shot transferability. In this work, we propose GraphCLIP framework to address these challenges.\n5.2 Graph Prompt Tuning\nIn low-resource scenarios, the \u201cpretraining, prompt tuning\u201d paradigm [36] has become a standard approach to address overfitting and catastrophic forgetting. In the graph domain, graph prompt tuning has seen notable success. GPPT [57] introduces task tokens and structure tokens to unify pretraining and downstream tasks such as link prediction. Similarly, GraphPrompt [37] unifies tasks as link prediction, enhancing performance through learnable readout prompt functions. SGL-PT [90] focuses on unifying tasks as masked node prediction, while GPF [8] introduces a learnable universal prompt feature into the input feature for downstream task adaptation. Additionally, All-In-One [58] reformulates all tasks at the graph level by introducing a learnable universal graph prompt, which is inserted into the original graph."}, {"title": "A The Dilemma of Vanilla Contrastive Loss", "content": "The representation learned through vanilla contrastive loss lacks domain invariance, which is crucial for enabling a model to effectively generalize across diverse target datasets [88, 92]. In this section, we present a specific scenario that highlights the limitations of vanilla contrastive loss. This example, adapted from ArCL [88], demonstrates how encoders trained via vanilla contrastive learning can exhibit markedly different behaviors across varying graph domains, denoted as $G_r$.\nPROPOSITION A.1. Consider a binary classification scenario with data $(X_1, X_2) \\sim N(0, I_2)$. When $X_1 \\geq 0$, the label is set to $Y = 1$, and the data augmentation process involves multiplying $X_2$ by standard normal noise:\n$\\qquad \\tau_\\rho(X) = (X_1, \\theta \\cdot X_2)$\n$\\qquad \\theta \\sim N(0, 1)$ (12)\nThe resulting transformation-induced domain set is defined as $B = {G_m | G_m = (X_1, m \\cdot X_2) \\text{ for } m \\in R}$. By considering the 0-1 loss, for every $\\zeta \\geq 0$, there exists a representation $g$ and two domains $G_m$ and $G_{m'}$ such that\n$\\qquad L_{AL} (g; G, \\pi) < \\zeta$ (13)\nyet g performs very differently across the domains $G_m$ and $G_{m'}$:\n$\\qquad |R (g; G_m) \u2013 R (g; G_{m'})| \\geq \\frac{1}{4}$ (14)\nwhere $L_{AL}$ denotes alignment loss in the vanilla contrastive loss [68], R denotes supervised risk of binary classification. This example\u00b3 underscores that a representation with a small contrastive loss can still demonstrate significant performance variability across different augmentation-induced domains. The core concept illustrated here is that a low $L_{AL}$ is achieved by averaging alignments across different augmentation-induced domains, rather than ensuring uniform alignment. As a result, the representation may experience substantial alignment losses in certain less frequently selected domains.\nProof. For $\\bar g \\geq 0$, let t = $\\sqrt5/2$ and g(x1, x2) = x1 + tx2. Then, the alignment loss of g satisfies:\n$L_{AL}(g; G,\\pi) = t^2\\mathbb{E}_{X^2}E_{(\\theta_1, \\theta_2)~N(0,I)}\\[(\\theta_1-\\theta_2)^2]=2t^2 < \\zeta.$ (15)\nSet c as 0 and c' as 1/t, it is obviously that:\n$R(g; G_c) = 0$ (16)\nbut\n$R(g; G_{c'}) =$\n$\\qquad P(X_1 < 0, X_1 + X_2 \\geq 0) + P(X_1 \\geq 0, X_1 + X_2 \\leq 0) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$ (17)"}, {"title": "B Theoretical Analysis of Invariant Alignment Loss", "content": "In this section, we will give the theoretical justification"}]}