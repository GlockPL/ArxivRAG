{"title": "FLAIR: VLM with Fine-grained Language-informed Image Representations", "authors": ["Rui Xiao", "Sanghwan Kim", "Mariana-Iuliana Georgescu", "Zeynep Akata", "Stephan Alaniz"], "abstract": "CLIP has shown impressive results in aligning images and texts at scale. However, its ability to capture detailed visual features remains limited because CLIP matches images and texts at a global level. To address this issue, we propose FLAIR, Fine-grained Language-informed Image Representations, an approach that utilizes long and detailed image descriptions to learn localized image embeddings. By sampling diverse sub-captions that describe fine-grained details about an image, we train our vision-language model to produce not only global embeddings but also text-specific image representations. Our model introduces text-conditioned attention pooling on top of local image tokens to produce fine-grained image representations that excel at retrieving detailed image content. We achieve state-of-the-art performance on both, existing multimodal retrieval benchmarks, as well as, our newly introduced fine-grained retrieval task which evaluates vision-language models' ability to retrieve partial image content. Furthermore, our experiments demonstrate the effectiveness of FLAIR trained on 30M image-text pairs in capturing fine-grained visual information, including zero-shot semantic segmentation, outperforming models trained on billions of pairs.", "sections": [{"title": "1. Introduction", "content": "By encoding images and texts into global embeddings, CLIP achieves coarse-grained semantic understanding. However it loses track of the local image details, e.g. CLIP is not able to perceive the difference between \"background\" and \"frappucino\", resulting in the inability to highlight the relevant regions specified in the text prompt. Recently, it has been shown that CLIP models and other vision language models (VLMs) often lack visual details [46, 47]. Thus, our goal is to improve the fine-grained visual understanding of CLIP models which is essential for a wide range of downstream applications, such as image-text retrieval or semantic segmentation.\nPrevious works [14, 59] propose to generate detailed descriptions for images to achieve more localized image-text alignment in CLIP models. However, these methods are restricted by the conventional learning mechanism of CLIP, since the detailed text descriptions enhance visual representations indirectly by matching them through the contrastive loss. Although DreamLIP [59] proposed to supervise local image tokens with textual information, we find that, without a careful selection of the negative pairs in the contrastive loss, the VLM does not learn to align the image tokens with semantically matching text.\nTo address these issues, we propose FLAIR, to learn Fine-grained Language-informed Image Representations, where image embeddings are generated by conditioning on a relevant text embedding for a more targeted alignment, instead of an indirect alignment through a global loss function. To obtain image descriptions with maximum semantic richness, our method leverages long-caption datasets generated by Multimodal Large Language Models (MLLMs). These captions provide a rich source of information about specific objects or regions in the image. Given a long caption, we sample diverse sub-captions, some of which focus on local regions, while others describe the image globally."}, {"title": "2. Related Works", "content": "Vision-Language Pre-training. CLIP [40] and ALIGN [21] have scaled up vision-language pre-training datasets to 400M and 1B samples, using a contrastive loss to match global image tokens with global text tokens. However, there is a growing demand for more fine-grained alignment between modalities [47]. Several approaches have been proposed to achieve this goal, including token-level alignment [54], hierarchical alignment from global to local [17], soft assignments allowing many-to-many mappings [18], and the use of intra-modal contrastive losses [27]. CoCa [55] utilizes cross-attention to pool the local image tokens and achieves more refined image-to-text alignment by additionally training with a captioning objective. Along with attention pooling to form the global image embeddings (Fig. 2 (b)), SigLIP [57] replaced the Softmax loss of vision-language pre-training with a Sigmoid-based loss. Concurrent to our work, Llip [26] proposed an architecture incorporating language information into learnable image tokens to form contextualized visual representations. However, Llip [26] lacks the pooling of local image tokens (Fig. 2 (c)) and, thus, it does not ensure a fine-grained alignment between modalities. In contrast, FLAIR leverages diverse and detailed captions with both local and global alignment (Fig. 2 (d)), outperforming previous approaches even when training on a significantly smaller dataset.\nText Augmentation. Several works [14, 50, 58, 59] proposed to improve the visual-language alignment through text augmentation. Notably, LaCLIP [14] rewrites captions in large datasets with Large Language Models (LLMs), showing significant performance gains when training on synthetic captions. Similarly, large Vision-Language Models (VLMs) have been exploited to create synthetic images and captions, augmenting existing datasets [20, 30, 53]. DreamLIP [59] re-captions 30 million images from CC3M [44], CC12M [4] and YFCC15M [9] with detailed descriptions generated by pre-trained MLLMs. Employing"}, {"title": "3. FLAIR: Fine-grained Language-informed Image Representations", "content": "In this section, we present the FLAIR architecture and methodology for language-image pre-training. We provide an overview of the main components of FLAIR in Fig. 3, including the sampling of diverse captions from long synthetic descriptions (Sec. 3.1), the text-conditioned attention pooling of image tokens (Sec. 3.2), and the local and global loss functions (Secs. 3.3 and 3.4)."}, {"title": "3.1. Sampling Diverse Captions", "content": "Pre-training data for vision-language models is typically collected by scraping and filtering large amounts of web data, as performed by CC3M [44] or LAION [42, 43]. While a large amount of image-text pairs helps in discovering a comprehensive set of visual concepts, the text descriptions in these datasets often do not describe the image content in detail. As a result, it is not possible to extract fine-grained concepts in an image, such as scene composition, and small object features. To alleviate this issue, we employ image datasets that are synthetically re-captioned and contain a long and detailed description of each image [45, 59]. A single sentence of these long captions typically describes a particular image detail, e.g., one object, a feature of an object, the background, the image style, or context.\nUsing these captions, our goal is to align vision and language representations at the fine-grained level of individual caption sentences, while retaining global image understanding. We devise a sampling strategy to cover both local and global captions, and learn their similarity with adaptively pooled image features through a contrastive loss. Specifically, when constructing a batch of B images from the augmented dataset, we sample K sub-captions from a caption Ti belonging to an image I\u2081. Each sub-caption consists of s\u2208 {1, ..., S} sentences that are either randomly sampled (i.e., independently sampled and concatenated), or extracted as a consecutive sequence of sentences. As a result, a batch contains B images and B \u00d7 K texts, where each image is associated with K matching captions. At each iteration, we randomly choose the number of sentences s for every sub-caption, where a lower s result in a more localized caption, while more sentences (a higher s) describe multiple parts of the image, resulting in global descriptions."}, {"title": "3.2. Text-conditioned Attention Pooling", "content": "Having access to a diverse set of captions, some describing local regions of an image and others explaining the global content, motivates creating a model architecture that is capable of adapting to both scenarios. Naively applying a contrastive loss between a global image embedding and the individual text embeddings would collapse the carefully separated information content of our K captions into an averaged image representation.\nInstead, we propose to contextualize the image representations with the individual captions, producing a unique image representation for every image-text pair. We start with the VLM architecture as proposed by Radford et al. [40], which uses two independent transformer encoders fimg and ftxt to project the tokenized image and text samples into per-token embeddings and global embeddings (i.e., fimg(I) = [{v(p)}n=1, vs] and ftxt(T) = [{t(p)}m=1, ts]), where n is the number of image tokens and m the number of text tokens. For simplicity, we refer to the local image tokens {v(p)}n=1 as vloc \u2208 Rn\u00d7d where d denotes the embedding dimension.\nTo effectively contextualize the image representation with semantics from the sampled captions, we introduce an attention pooling layer fAttnPool, that produces a text-conditioned image representation vtc from the local image patch embeddings and the global text embedding. We define vtc = fAttnPool(ts, vloc) as follows:\nfAttnPool(ts, vloc) = softmax(`<tsWq(vlocWk)T` / \u221ad) vloc Wu (1)\nwhere Wq, Wk, Wu are the query, key, and value weight matrices. In other words, we use the global text embeddings of a caption as a query to pool the local image embeddings creating a text-conditioned image representation vtc. In practice, we use a multi-head attention layer. We append an empty token (zero vector) to vloc to allow ts to attend to the empty token when ts and vloc are not semantically related.\nChoosing Negative Pairs. With text-conditioned attention pooling, FLAIR produces a different image representation for every image-text pair. However, to learn semantically rich and nuanced image representations, we need to carefully define the positive and negative pairs for vision-language pre-training. To simplify notation, we assume a single caption per image (i.e., K = 1). Let ti be the caption of the i-th image in the batch, and vij be the image embedding from the i-th image conditioned on the caption of image j. For the explanation in this section only, we enforce i \u2260 j. In the context of contrastive learning, image-text"}, {"title": "3.3. Text-conditioned Sigmoid Loss", "content": "After constructing the positives and negatives pairs and applying fAttnPool(.), we adopt a contrastive loss based on the sigmoid function as proposed by SigLIP [57]. It is preferred over the InfoNCE loss [37], as it enables multiple positive pairs in the same batch, and is more efficient for multi-GPU training. Accordingly, we define our text-conditioned sigmoid loss as\nLtc = \u2212 \u03a3B\u03a3K log 1 / (1 + e\u2212Yi,j(tvtc+b)) (2)\ni,j,k=1 \nwhere t is a learnable temperature, b is a learnable bias, and \u3008\u00b7, \u00b7\u3009 is the cosine similarity. Yi,j is +1 for positive pairs when i = j for all k \u2208 [1,..., K], and \u22121 for negative pairs otherwise. Since every batch contains B images and BK captions, we reduce the compute and memory usage of Ltc by considering all K positive pairs, but only B \u2212 1 negative pairs per image, i.e., 1 out of K captions for every negative. Therefore, we compute the similarity of B \u00d7 (K+\nB\u22121) pairs, instead of B \u00d7 BK pairs for every batch.\nLtes aligns the text-conditioned image embedding with the corresponding text embedding. Intuitively, this allows the image encoder to store semantic information locally in each token and pool the relevant tokens based on the text query producing context-aware representations. Our main experiments demonstrate that the text-conditioned image embeddings contribute significantly to fine-grained image-text alignment, providing the majority of the performance improvement in zero-shot semantic segmentation."}, {"title": "3.4. Multi-positive Sigmoid Loss", "content": "We find that FLAIR can be trained exclusively with the Ltes loss. At the same time, it proves beneficial to additionally match the global image embedding vs with every sub-caption, to also learn a coarse alignment. Following previous works [14, 28, 59], we introduce a multi-positive loss to align the global image embedding vs with the text embedding ts of every sub-caption. Different from previous works, we employ the contrastive sigmoid loss to handle multiple positive captions per image in a more natural way. Our multi-positive sigmoid loss is defined as\nCmps = \u2212 \u03a3B\u03a3K log 1 / (1 + e\u2212Yi,j(t\u3008vs,ts\u3009+b)) (3)\ni,j,k=1"}, {"title": "4. Experiments", "content": "We present our experimental evaluation of FLAIR on the three image-text retrieval settings: standard (Sec. 4.2), fine-grained (Sec. 4.3), and long (Sec. 4.4). In addition, we conduct experiments on zero-shot semantic segmentation (Sec. 4.5) and image classification (Sec. 4.6), qualitatively evaluate the attention maps of FLAIR (Sec. 4.7), and ablate important model components (Sec. 4.8)."}, {"title": "4.1. Experimental Setup", "content": "Pre-training Datasets. To learn fine-grained image embeddings from descriptive local captions, we pre-train FLAIR on DreamLIP's [59] re-captioned datasets, which we refer to as CC3M-recap, CC12M-recap, and YFCC15M-recap. Following DreamLIP, we also merged these three datasets into a combined set of 30M samples.\nImplementation Details. Our model is based on the Open-CLIP [6] code implementation, adopting their default settings. We use ViT-B/16 as the vision encoder, with the default pre-processing: images are resized to 224 \u00d7 224 pixels, and text sequences are tokenized to a maximum length of 77 tokens. For direct comparison with DreamLIP, we follow their training configuration and caption pre-processing, splitting the MLLM-generated and original captions into individual sentences. To obtain diverse training captions, we sample K = 8 captions per image, with each caption randomly merging 1 to 3 sentences (i.e., S = 3). To maximize sampling variability while retaining context, we randomly construct our sub-caption by either sampling consecutive sentences or merging sentences from random positions in the original text. For fair comparison, we reproduce CLIP [40] and SigLIP [57] on all re-captioned datasets under identical training configurations as FLAIR. \nInference with FLAIR. To utilize the fine-grained embeddings from FLAIR for image-to-text (I2T) retrieval, each image i is first conditioned on all j texts to generate the conditioned embeddings vij. Then we compute the similarity scores between the conditioned embeddings and each text"}, {"title": "4.2. Standard Zero-shot Image-text Retrieval", "content": "As a standard assessment of image-text alignment, we follow prior works [26, 53, 59] to evaluate image-text retrieval on the validation splits of MSCOCO [29] and Flickr30K [39], where each image is typically paired with five global captions.\nResults. We report the results on the standard retrieval task in the left side of Tab. 1. FLAIR outperforms the three baselines, CLIP, SigLIP, and DreamLIP on all pre-training datasets by a large margin. Comparing models trained on CC3M-recap, FLAIR surpasses DreamLIP in the retrieval task, obtaining higher R@1 scores on both COCO (T2I: +7.9%, I2T: +10.8%) and Flickr30k (T2I: +12.1%, I2T: +9.5%) datasets. When including SOTA models, FLAIR trained on CC12M-recap obtains a similar performance to SigLIP trained on 10B samples, and surpasses it significantly once we move to larger datasets with YFCC15M-recap and the merged 30M samples. FLAIR-30M (vs. SigLIP-10B) achieves 81.1% (vs. 75.6%) T2I, 94.7% (vs. 89.1%) I2T on Flickr30k and is similarly better on COCO. We also notice that CLIP and SigLIP trained on YFCC15M-recap can match or surpass their counterparts trained on billions of data samples. This suggests two key insights: 1) text augmentations from long synthetic captions empowers VLMs with better retrieval capability, and 2) FLAIR with text-conditioned attention pooling generates more targeted image embeddings for retrieval, maximizing the benefits from text augmentations, and resulting in a significant improvement with much less image data."}, {"title": "4.3. Fine-grained Zero-shot Image-text Retrieval", "content": "Standard retrieval tasks do not fully capture a model's ability to align detailed descriptions with images. To address this, we introduce a fine-grained retrieval task aimed at evaluating how well a model can associate an image with fine-grained captions. Our benchmark is constructed as follows: 1) We use the recently released densely-captioned datasets DOCCI [36] and IIW [19]. Due to the careful human annotation process, their long captions are free from hallucinations; 2) For each test image in DOCCI and IIW, we split the long captions into individual sentences, yielding an average of 7.1 captions per image in DOCCI and 10.1 in IIW. Each caption focuses on a specific local part of the image, making both T2I and I2T tasks significantly more challenging than standard retrieval. We refer to our split datasets as DOCCI-FG and IIW-FG.\nResults. The results obtained on DOCCI-FG and IIW-FG are reported in the right side of Tab. 1. The difficulty of this task is apparent by the significantly lower text-to-image"}, {"title": "4.4. Long Zero-shot Image-Text Retrieval", "content": "Image-text retrieval with long captions imposes a unique challenge for CLIP models. Following LoTLIP [50] and Long-CLIP [58], we evaluate FLAIR on datasets with long captions, including DCI [48], 1k (SV-1k) and 10k (SV-10k) subsets of ShareGPT-4V [5], and Urban-1k [58].\nUnlike previous methods specifically designed for long-caption retrieval with extended token limits and larger text encoders, FLAIR employs the standard CLIP text encoder with a 77-token limit. The former SOTA, LoTLIP [50], was trained on a 100M-scale re-captioned dataset, while Long-CLIP [58] fine-tunes a 400M-scale CLIP model with an additional 1M images with long captions. Although FLAIR is trained on a smaller training set of 30M samples, it still outperforms these methods on SV-1k, SV-10k, and Urban-1k. Most notably on T2I, FLAIR obtained improvements"}, {"title": "4.5. Zero-shot Semantic Segmentation", "content": "For VLMs, zero-shot semantic segmentation involves measuring the similarity {(voc, t) | j\u2208 {1,2,..., M}} for M different class names. Recent works [12, 25, 49] provide a framework to map these similarities to semantic segmentation outputs. To examine the raw alignment of local image tokens vloc with the corresponding input texts, we perform semantic segmentation following [49] without post-processing or segmentation-specific modifications.\nAs shown in Tab. 3, FLAIR trained on all subsets of data, consistently outperforms CLIP-based methods trained on significantly larger datasets, achieving an improvement of 10.1% - 25.8% mIOU increase across all datasets (14.4% on average). As illustrated in Fig. 1, DreamLIP's vloc image token embeddings show weak correspondence to the input text, which we conclude is the result of their choice of negatives as discussed in Sec. 3.2. In contrast, FLAIR, optimized with Ltcs, effectively aligns vloc with varying text prompts, demonstrating strong localization capabilities."}, {"title": "4.6. Zero-shot Image Classification", "content": "Following [14, 59], we evaluate the zero-shot classification performance of FLAIR and baseline methods on ImageNet [10] and 10 additional datasets. In retrieval tasks, text-augmented methods outperform VLMs trained on billions of images. However, in image classification they lag behind by around 20%. This demonstrates that scaling up the number of images remains a key factor in improving VLM's classification performance.\nWhen trained on CC3M-recap, FLAIR achieves a 4.1% higher average performance than DreamLIP and other baselines. This shows that FLAIR, although optimized to generate fine-grained visual representations, could still efficiently gain global-level visual understanding performance when images are relatively scarce. However, when scaled up to 30M samples, FLAIR, while still outperforming CLIP and SigLIP by 4%, is on par with DreamLIP (-0.2% on avg.). This shows that these methods trained on synthetic captions converge similarly, further suggesting the importance of scaling up images for the classification task. Therefore, we hypothesize that scaling FLAIR to larger datasets would extend the concept vocabulary and image coverage, closing the gap to large-scale models on zero-shot classification."}, {"title": "4.7. Attention Maps Visualization", "content": "For a given image with two different local captions, we visualize the attention maps of fattnPool(.), i.e., which image tokens are pooled together. As illustrated by the \"truck\" and \"worker\" example, FLAIR can locate both large and small objects in an image. The horses example shows"}, {"title": "4.8. Ablation Study", "content": "Model Components. In Tab. 5, we analyze the components of FLAIR: text conditioning (TC), global loss (GL), multiple captions per image (MC), and diverse caption sampling instead of single sentences (DS). Ltes and Lmps correspond to TC+MC and GL+MC respectively.\nSigLIP is equivalent to only using GL (1). Replacing GL with TC (2) leads to performance improvements across all metrics, achieving a 3.7%/5.5% increase in R@1 for COCO retrieval and a 33.8% boost in VOC20 segmentation demonstrating its contribution to the fine-grained alignment. Adding MC improves performance in both scenarios (3 and 4). Our diverse sampling (DS) is another significant improvement, especially in segmentation and long-retrieval performance on the Urban-1K which gains over 20% (5 and 6). FLAIR, combining all components, achieves the best performance in all but long-retrieval (7). In summary, Ltcs is foundational to our method's performance, sampling diverse captions provides a substantial boost in long retrieval tasks, while combining Lmps delivers additional gains, particularly for global-level tasks.\nAdditional Ablations. In supplementary Sec. D, we provide additional ablations to support important choices. We pre-trained FLAIR on the original CC3M dataset and on the PixelProse dataset, showing that our method is not restricted to long captions and adaptable to a variety of data distributions."}, {"title": "5. Conclusion and Limitations", "content": "We introduce FLAIR, a VLM that learns Fine-grained Language-informed Image Representations by conditioning on the semantics in dense local captions. Trained on 30M recaptioned images, FLAIR outperforms baselines trained on billions of images across standard, fine-grained, and long-form image-text retrieval tasks. The significant improvements in zero-shot segmentation compared to the baselines as well as the qualitative results corroborate that FLAIR learns a fine-grained alignment between text and image at the token-level.\nWhile FLAIR matches baselines trained on the same number of images in zero-shot classification, it still falls behind CLIP models trained on significantly larger datasets. This suggests that, although leveraging detailed synthetic captions enhances fine-grained image understanding, it does not replace the image coverage and conceptual richness of larger datasets for global-level tasks. To tackle this limitation, a natural future direction involves scaling the synthetic recaptioning to large-scale datasets and training variants of FLAIR with higher parameter count."}]}