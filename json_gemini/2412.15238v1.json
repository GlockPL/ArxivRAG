{"title": "Dipper: Diversity in Prompts for Producing Large Language Model Ensembles in Reasoning tasks", "authors": ["Gregory Kang Ruey Lau", "Wenyang Hu", "Diwen Liu", "Jizhuo Chen", "See-Kiong Ng", "Bryan Kian Hsiang Low"], "abstract": "Large Language Models still encounter substantial challenges in reasoning tasks, especially for smaller models, which many users may be restricted to due to resource constraints (e.g. GPU memory restrictions). Inference-time methods to boost LLM performance, such as prompting methods to invoke certain reasoning pathways in responses, have been shown effective in past works, though they largely rely on sequential queries. The ensemble method, which consists of multiple constituent models running in parallel, is a promising approach to achieving better inference-time performance, especially given recent developments that enabled significant speed-ups in LLM batch inference. In this work, we propose a novel, training-free LLM ensemble framework where a single LLM model is fed an optimized, diverse set of prompts in parallel, effectively producing an ensemble at inference time to achieve performance improvement in reasoning tasks. We empirically demonstrate that our method leads to significant gains on math reasoning tasks, e.g., on MATH, where our ensemble consisting of a few small models (e.g., three Qwen2-MATH-1.5B-it models) can outperform a larger model (e.g., Qwen2-MATH-7B-it).", "sections": [{"title": "Introduction", "content": "While Large Language Models (LLMs) have demonstrated impressive capabilities in addressing a variety of tasks, they still encounter substantial challenges in reasoning tasks such as multi-step logical inference or problem-solving [1]. This is especially so for smaller models, which many users may be restricted to due to resource constraints (e.g. GPU memory restrictions), posing limitations on their utility in practice. Inference-time methods to boost LLM performance, especially for smaller models, hold promise in tackling these challenges [2]. However, many of these methods, such as Chain-of-Thought (CoT), Reflexion, and other techniques [3\u20136], have focused on sequential queries to an LLM to improve performance.\nIn contrast, ensemble methods, which involve the use of multiple constituent models in parallel, have been shown to improve models' performance and robustness in classical machine-learning settings [7] and are promising approaches to achieve better inference-time performance, although less well-studied in the LLM setting. The prospects of applying such methods to LLMs are increasingly attractive, given recent developments that have enabled significant speed-ups in parallel, LLM batch inference. These include methods to efficiently handle key-value cache memory [8] and prompt caching to efficiently reuse common prompts for multiple queries [9, 10], enabling sub-linear (in the number of queries) costs for batch inference."}, {"title": "Problem setting and related work", "content": "LLMs and prompts. Consider an LLM model M which for our purposes can be viewed as a black box that encodes a conditional probability distribution of text responses y over any text input q and additional prompt w, from which we can autoregressively sample responses \u0177 from, i.e.\n$\\hat{y} \\sim M(q, w) = p_M(y|q, w).$\nExamples of prompt w could include reasoning prompts such as \"Let's think step by step\" in CoT [17] that provide instructions on how the LLMs should derive answers for the query q.\nLLM ensembles. Ensemble methods involve combining several models to produce a ensemble with better performance and lower variance. However, while commonly applied for a wide variety of machine learning models [7], ensemble methods for LLMs have remained relatively unexplored. Past works have focused on heterogeneous ensembles involving multiple types of models (e.g. different LLM API providers) [13], multi-agent LLM settings that focuses on interactions among agents [18-20], or homogeneous ensembles that rely only on stochastic sampling of model responses [15].\nHowever, to the best of our knowledge, we are not aware of any work that focused on designing and analyzing homogeneous LLM ensembles where their diversity is injected and optimized via prompts to constituents with the same underlying LLM model. Our work's focus on such an approach exploits LLMs' unique capabilities of generating diverse output given only changes to its prompts, allowing for a simple but effective method to boost LLM performance using inference-time compute.\nProblem formulation. Consider a task T that consists of instances described as tuples t := (qt, c), where qt can be represented as a text string query and ct is the corresponding ground truth solution. We have access to a single LLM model M that when provided task queries and a prompt w, will provide a response \u0177 according to Eq. (1). This response will consist of (1) some reasoning output \u2191, and (2) the final answer \u0109 to the query, which we can denote as \u0177 := {r, \u0109}. We evaluate the performance of the model with a specific prompt, denoted as M(., w), on the task by computing its expected accuracy over the set of task instances T, i.e., F(M(\u00b7, w); T) := Et~\u315c[[{\u0109t = c}], which in practice is computed over a representative test set.\nWe denote a homogeneous LLM ensemble as \u0190(\u00b7 ; M, n, $), consisting of n instances of the same model M and in general has an adjustable inference-time design parameter 4. The ensemble produces a final answer when provided a task query, i.e., E(qt; M, n, \u03c6) \u2192 \u0109t, and we can evaluate its performance based on its expected accuracy:\n$F(E,T) = E_{t\\sim T}[I\\{E(q_t; M, n, \\phi) = c\\}].$\nOur objective is to design an ensemble framework with an appropriate design parameter & such that given fixed M, n and a small labeled development set, we can efficiently maximize Eq. (2) by optimizing for & to produce the best performing ensemble without additional training."}, {"title": "Method", "content": "Drawing inspiration from how using different prompts w would result in varying response distributions in Eq. (1) given the same model M, our DIPPER framework has the set of prompts {wi}=1 fed into the ensemble of n LLM instances as the key ensemble design parameter 4. DIPPER consists of the following three components:\n1.  Prompt Generator. First, an LLM generates a large candidate pool of prompts (denoted as W), which can be based on some description of the task and in-context prompt examples that we think may be effective, if such prior knowledge is available. The goal is for the prompts to invoke various types of reasoning pathways when addressing queries, hence injecting diversity into the ensemble. Additional details are in Appendix A.1.\n2.  Prompt Selector. Then, an optimization process is performed over the candidate pool of prompts W to select a subset of n prompts (i.e., {w\u2081 \u2208 W}_1), based on a diversity metric that acts as an approximation of the relative performance of each subset (Section 3.1).\n3.  Response Aggregator. Finally, the responses from the n constituent LLMs are aggregated through a response aggregator operation A to produce a single final response for the ensemble (Section 3.2).\nPutting everything together, our DIPPER framework characterizes an ensemble of size n via E(qt; M, n, {wi}=1) := A({M(qt, Wi)}=1) \u2192 \u0109t, where the subset of prompts {wi}=1 is chosen from a candidate pool W to optimize the expected ensemble performance F(E, T) for a task T."}, {"title": "Prompt Selector", "content": "With our framework, the optimization problem in Eq. (2) reduces to an optimization to choose the best subset of prompts {wi}=1 from the set of candidate prompts W:\n$\\underset{\\{w_i \\in W\\}_{i=1}^n}{\\operatorname{argmax}} F(E(q_t; M, n, \\{w_i\\}_{i=1}^n), T).$\nUnfortunately, directly optimizing Eq. (2) is a combinatorial problem that is very challenging, even if a development/validation set is available for the task of interest. For example, selecting 5 prompts from a candidate pool of 200 prompts involves searching over  $\\binom{200}{5} \\approx 2.5 \\times 10^9$ candidates. Instead, we note that the best ensemble composition requires a balance of the two desiderata: fidelity and diversity. Hence, we propose optimizing Eq. (2) by considering how to prioritize the prompts that have the best predicted performance on the task T, while maximizing the diversity of the selected set of prompts.\nPrompt fidelity. First, we can approximate the predicted performance of each prompt by its average performance on a task development set Ta\u00b2 . Note that as inference using these various prompts on a small development set can be done in parallel, this process can in practice be significantly sped up by existing batch inference techniques such as those employed by vLLM [8]. Specifically, for a candidate pool of prompts W and development set Ta, we can define a prompt fidelity mapping u : W \u2192 [0, 1],\n$u(w) := F(M(\\cdot, w), T_a),$\nwhere M(, w) is the LLM model conditioned by prompt w \u2208 W, and F the expected accuracy defined in Section 2. In practice, for a candidate pool of size n, u(w) can be represented as an n \u00d7 1 column vector, with the elements representing each prompt's expected accuracy.\nSemantic entropy. Instead, our approach involves prioritizing the prompts that have the best predicted performance on the task T, while maximizing the diversity of the selected set of prompts. Then, we measure prompt diversity by considering how different the semantic meanings of the n role prompts are from each other. We represent each prompt's semantic meaning with a mapping R from its text representation w into a normalized continuous vector s \u2208 RP in a p-dimensional semantic embedding space S through a sentence embedding model Ms [21], i.e., R(w) := Ms(w). This mapping can be represented as an n \u00d7 p prompt embedding matrix R = [$\\vec{s_1},\\dots,\\vec{s_n}$] where s is a 1 \u00d7 p row vector representing each prompt."}, {"title": "Response Aggregator", "content": "Given the various constituent LLMs' responses, the aggregation method determines how much information is used to derive the final ensemble output. We consider two approaches:\nMajority voting (MV). The first involves extracting the final answer \u0109 from each LLM response \u0177 = {r, \u0109}, and then selecting the answer that has been proposed the most number of times. This approach does not take into account the reasoning output produced by the ensemble constituents, but is easily implementable.\nLLM aggregation (LLMA). The second involves using another LLM instance to evaluate each constituent response, aggregate them, and generate a final answer for the task. This approach incurs additional LLM query cost and is dependent on the capabilities of the aggregator LLM, but has the advantage of potentially taking into account the various reasoning output \u00ee from the ensemble constituents to further improve overall performance (see Section 4.3 for details)."}, {"title": "Experiments", "content": "Experimental set-up. We empirically evaluate our framework on mathematically reasoning tasks with the MATH [24], GSM8K, and MMLU-STEM datasets. We implement our framework by using the GPT-40 as our prompt generator and Qwen2-MATH-1.5B as the constituent model in the ensemble, where the ensemble constituents are run in parallel using vLLM [8] for fast batch inference. Further details of our experiments are in Appx. \u0412.\nBaselines. We evaluate our DIPPER framework by comparing it against the \"Self-ensemble\" baseline, which lacks prompt diversity but incorporates diversity through repeated response sampling [15]. We also compare our DIPPER implementation based on semantic volume (\"Dipper\") with two other variants: (1) a na\u00efve implementation where prompts are sampled from the candidate pool based on"}, {"title": "Ensembles with fixed prompt methods", "content": "First, we illustrate the effectiveness of prompt-based ensemble methods by considering a fixed list of 7 reasoning prompts inspired by existing works [25\u201327] on prompting methods to boost reasoning capabilities (details in Appx. B.1). Under a fixed ensemble size of 7, Fig. 1 shows that the ensemble using the 7 different prompts (57.31%) significantly outperforms the self-ensemble with no prompt ((55.76%)) and the average performance (56.55%) of self-ensemble using any single prompt.\nTo further investigate the impact of prompt diversity, we evaluated all combinations of the 7 prompts while maintaining a fixed ensemble size of 7. For combinations with fewer than 7 prompts, we randomly sampled responses to reach a total of 7 before applying majority voting. The results in Fig. 2 reveals that increasing the number of prompts in the ensemble generally leads to higher accuracy, reduced variance, and fewer unique answers. The 7-prompt ensemble has the highest accuracy and lowest variance, which suggests that employing a diverse set of prompts in an ensemble can enhance performance and consistency, especially when we do not know which prompt would perform best before evaluation."}, {"title": "Ensembles with optimized prompt diversity", "content": "Next, we consider our full DIPPER framework. We first generate a pool of prompt candidates (|W| = 200) using the 7 reasoning prompts in the previous section as in-context exemplars (details in Appx. B.1) and then perform diversity optimization (Sec. 3.1) to select the best ensemble prompts. Evaluation details are in Appx. B.2. As shown in Fig. 3, our method achieves the highest accuracy compared to all baseline ensemble methods across various ensemble sizes. DIPPER also significantly outperforms the single LLM. For example, DIPPER with n = 9 has close to a 10%-pt increase (~20% accuracy gain) compared to the single LLM baseline. In fact, our ensemble that consists of just 3 Qwen2-MATH-1.5B model already slightly outperform the next model size class, the Qwen2-MATH-7B model. See more results on GSM8K and MMLU-STEM in Appx. C.2 where DIPPER is shown to be consistently effective."}, {"title": "LLM aggregation can do better", "content": "Finally, we analyze the effects of using Majority voting (MV) or LLM aggregation (LLMA) for our response aggregator component (see experimental details in Appx. B.3). We consider ensembles of size n = 5 with randomly selected prompts, and compare their performance on MATH when using either majority voting or LLM aggregation. Table 1 summarizes the results, showing that LLMA is more accurate than MV on average (i.e., higher F(E)). To better analyze the performance difference, we computed the \"Override Ratio\u201d which is how often a specific method is correct when the two methods disagree. Note that when MV and LLMA disagree, LLMA has a much higher ratio than MV which is only correct 8% of the time. We attribute LLMA's advantage to its capability of understanding the reasoning in responses even when the ensembles do not have a majority for the"}, {"title": "DIPPER combined with other prompting methods like Reflexion", "content": "In addition, we also show that our ensemble framework DIPPER is orthogonal to other established prompting techniques (e.g. CoT and Reflexion [6]), allowing it to stack and bring greater performance. In our experiments, we first use DIPPER to select n agents and query each agent with the questions. Their initial responses will be self-reflected according to the method proposed in Reflexion [6], before being aggregated into the final answer with MV. The results in Fig. 4 shows that DIPPER coupled with reflection achieves much better results, suggesting that DIPPER has the potential to be extended further or combined with other methods."}, {"title": "Conclusion", "content": "In this work, we have proposed a novel framework, DIPPER, where a single LLM model type is fed an optimized, diverse set of reasoning prompts in parallel, effectively producing an ensemble at inference time to achieve performance improvement in reasoning tasks. Our empirical findings have demonstrated DIPPER's effectiveness in improving inference performance for a variety of reasoning tasks, which may inspire future works to investigate additional optimization methods for prompt-based inference-time ensembles to further improve performance gains."}, {"title": "Additional details on the DIPPER framework", "content": ""}, {"title": "Prompt Generator", "content": "The first component plays the important role of generating a large pool of candidate prompts with the following desiderata:\n1.  Fidelity. Each prompt should be able to influence the LLM into applying a certain type of reasoning approach to the task, and not have significant negative impact the LLM's performance on the task.\n2.  Diversity. The prompts should be sufficiently different from one another such that they elicit various reasoning pathways and provide a diverse pool to select from in the subsequent component.\nWe first show that LLMs are capable of generating prompts that meet this desideratum, via the most direct way of prompting it to generate a pool of candidate prompts while providing it with exemplars illustrating different reasoning prompts. To do so, we considered a list of 7 reasoning prompts inspired by existing works [25\u201327] on prompting methods to boost reasoning capabilities. Given these prompts as exemplars, we used GPT-40 to further generate a set of 200 different candidate prompts that each represent a different reasoning approach (details in Appx. B.1). Fig. 5 shows the distribution of average accuracy over a sampled test set of MATH [24] questions for each prompt, when used for the Qwen2-MATH-1.5B model (i.e., F(M(\u00b7, w); T) for w\u017c \u2208 W). Note that the distribution of accuracy is largely higher than that of the base model without prompts, and similar to the accuracies achieved by the reasoning prompt exemplars, demonstrating the fidelity requirement. Qualitatively, we see that the prompts are also relatively diverse \u2013 they generally specify certain reasoning approaches inspired by various subject domains (see Appendix A.1). We quantify this diversity in Sec. 3.1 with our proposed metric.\nNote that when generating the prompts, we did not pass any task description to the LLM prompt generator. We did so as the reasoning prompts can in general be task-agnostic, even if some may be inspired by some specific subject matter. In practice, the candidate pool of reasoning prompts need not be generated on-the-fly, but be drawn from a shared pool prepared beforehand by a more powerful LLM, to be used by ensembles consisting of much smaller LLMs, as we demonstrated. The actual selection of relevant prompts can be done by the prompt selector component, which we will described next in Sec. 3.1."}, {"title": "DIPPER algorithm", "content": "Our DIPPER algorithm is outlined in Algorithm 1."}, {"title": "Detailed Experimental Setting", "content": ""}, {"title": "Fixed 7 prompts and Prompt Generation", "content": "We consider 7 prompts inspired by existing works and list them in Tab. 3 below.\nWe use the prompt template in Tab. 4 to generate 200 diverse prompts."}, {"title": "Evaluation", "content": "We primarily consider three datasets in our paper. For MATH, we randomly 10% test samples from each category and form a fixed subset of size 500. We uniformly randomly sample 20 samples from this subset to form a validation dataset and use the rest 480 samples as the hold-out test dataset. For GSM8K and MMLU-STEAM, we use their official split of test data and uniformly randomly sample 20 samples to form a validation dataset for each task, and use the rest samples as the hold-out test data.\nIn the inference evaluation, we use 4-shot exemplars for MATH, 8-shot for GSM8K, and 5-shot for MMLU-STEM. Those exemplars are adopted from the evaluation setting in Qwen2-MATH [28] and fixed for all questions and all methods."}, {"title": "LLM aggregation", "content": "We perform LLM aggregation with the same Qwen2-MATH-1.5B-it model, by feeding the question context and the responses from LLM agents into the designed template shown below in the bounding box:"}, {"title": "Additional Results", "content": ""}, {"title": "Performance-adjusted embedding", "content": "To study the effect of accuracy on the performance-adjusted prompt embedding matrix, we report the Spearman correlation between logdet V and the ensemble performance F(E) under different choices of a. We observe that when a = 0, the correlation is 0.18, and it increases as a becomes larger. The positive correlation justifies our approach to maximize prompt diversity. The increasing correlation justifies our approach of incorporating validation accuracy into the prompt semantic embedding."}, {"title": "Results on More Datasets", "content": "We also evaluate the performance of DIPPER on GSM8K and MMLU-STEM. The results in Fig. 7 and Fig. 8 demonstrate that our proposed method DIPPER can consistently outperform the self-ensemble baseline, further demonstrating the benefits of our method. Our proposed DIPPER implementation with semantic volume optimization also consistently produces better or comparable results compared to our other variants (Random+ or Top-n) which have more unstable results, showing the usefulness of prompt diversity optimization in improving inference performance."}]}