{"title": "Indian Stock Market Prediction using Augmented Financial Intelligence\nML", "authors": ["Anishka Chauhan", "Pratham Mayur", "Yeshwanth Sai Gokarakonda", "Pooriya Jamie", "Naman Mehrotra"], "abstract": "This paper presents price prediction models using Machine Learning algorithms augmented\nwith Superforecasters' predictions, aimed at enhancing investment decisions. Five Machine Learning\nmodels are built, including Bidirectional LSTM, ARIMA, a combination of CNN and LSTM, GRU, and a\nmodel built using LSTM and GRU algorithms. The models are evaluated using the Mean Absolute\nError (MAE) to determine their predictive accuracy. Additionally, the paper suggests incorporating\nhuman intelligence by identifying \"Superforecasters\" and tracking their predictions to anticipate\nunpredictable shifts or changes in stock prices (Mihov et al, 2022). To collect user predictions and\nidentify Superforecasters, a user-friendly website has been developed. The predictions made by\nthese users can further enhance the accuracy of stock price predictions when combined with\nMachine Learning and Natural Language Processing (NLP) techniques.\nPredicting the price of any commodity can be a significant task but predicting the price of a\nstock in the stock market deals with much more uncertainty. This is mainly due to the market's\nvolatile and disruptive nature. Prices can touch their 52-week high on one day and the other, we\nmight witness a huge fall in price. Market dynamics plays a major role in deciding the price of stocks\nbut there can be an enormous number of reasons behind the price play for this market. Recognising\nthe limited knowledge and exposure to stocks among certain investors, this paper proposes price\nprediction models using Machine Learning algorithms. These models assist investors who have been\nhesitant to engage in stock market investments due to the lack of knowledge and experience. In this\nwork, five Machine learning models are built using Bidirectional LSTM, ARIMA, a combination of CNN\nand LSTM, GRU and the last one is built using LSTM and GRU algorithms. Later these models are\nassessed using MAE scores to find which model is predicting with the highest accuracy. In addition to\nthis, this paper also suggests the use of human intelligence to closely predict the shift in price\npatterns in the stock market The main goal is to identify 'Superforecasters' and track their predictions\nto anticipate unpredictable shifts or changes in stock prices. To collect this data from people and to\nidentify super forecasters, we have built a user-friendly website using which people can submit their\npredictions about a certain stock. By leveraging the combined power of Machine Learning and the\nHuman Intelligence, predictive accuracy can be significantly increased.", "sections": [{"title": "1. INTRODUCTION", "content": "The application of machine learning (ML) algorithms in financial market predictions has been a\ntopic of interest in recent years. The objective of this paper is to provide an overview of the current\nstate of financial intelligence in India and propose a framework for augmenting it using emerging\ntechnologies such as Machine Learning and Natural Language Processing (NLP). Several studies have\nexplored this area demonstrating the potential of ML in enhancing financial intelligence.\nBy identifying differences between Al 1.0 (Machine Intelligence), Al 2.0 (Intelligence\nAmplification) and Al 3.0 (Augmented Intelligence), a research work by Mihov et al, 2022 explored\nthe interactions between machine and human intelligence. Their work suggests the use of\n'Superforecasters' (individuals that tend to make better predictions than the general public) with\nMachine Intelligence in applications like financial forecasting as due to insufficient training data and\nlow noise ratio, Machine Intelligence systems often tend to underperform. This paper is an\napplication of Mihov et al, 2022's work.\nAs part of the literature review, we have referred to the paper by Payal Soni et al 2022 as it\nprovides a detailed analysis of the techniques employed in predicting stock prices and the challenges\nassociated with them. We also draw upon other relevant literature to support our proposed\nframework and highlight its potential benefits for the Indian financial sector.\nA study by Huang, J., Chai, J. & Cho, S. (2020) demonstrated the use of Deep Learning\nalgorithms in predicting stock market movements. The study found that Deep Learning algorithms\noutperformed traditional statistical methods, indicating the potential of these algorithms in financial\nmarket predictions.\nAccording to Deshmukh and Saratkar (2019), the forecasting of stock returns has become a\nsignificant field of research in recent decades. Initially, researchers attempted to establish a linear\nrelationship between macroeconomic variables and stock returns. However, the discovery of\nnonlinearity in stock market index returns has led to the emergence of literature on nonlinear\nstatistical modelling. Many of these studies require the specification of a nonlinear model before\nestimation. Given the noisy, uncertain, chaotic, and nonlinear nature of stock market returns,\nDeshmukh and Saratkar (2019) argue that artificial neural networks (ANN) have evolved as a better\ntechnique for accurately capturing the structural relationship between a stock's performance and its\ndeterminant factors compared to other statistical techniques. The authors also note that different\nstudies utilise different sets of input variables to predict stock returns, even when predicting the\nsame set of stock return data, highlighting the ongoing exploration and diversity in approaches to\nidentifying the most effective variables for predicting stock returns.\nAnother ML algorithm Support Vector Machine has been a popular choice for curating\nprice-prediction models. A study by Patel et al. (2015) confirms this as it uses Support Vector\nMachine (SVM) to predict the Bombay Stock Exchange (BSE) and found that SVM outperformed other\ntraditional methods in terms of accuracy and reliability. This study highlighted the potential of ML in\npredicting stock market trends in India.\nMehtab and Sen's study on \"Stock Price Prediction Using Convolutional Neural Networks on a\nMultivariate Time Series\" proposes a hybrid approach for stock price prediction in the Indian financial\nmarket using machine learning and deep learning-based methods. The authors address the debate\nsurrounding the predictability of stock prices by proponents of the Efficient Market Hypothesis who\nclaim that stock prices cannot be predicted, and those who have shown that, if correctly modelled,\nstock prices can be predicted with a fairly reasonable degree of accuracy."}, {"title": "2. CHALLENGES FACED", "content": "In India, the percentage of the population that invests in stock markets is fairly low when\ncompared to the USA and Europe. To put this into perspective, let us look at some numbers published\nby choiceindia.com in June 2023. In the USA, 55% of their citizens invest in the stock market, in the\nUnited Kingdom this number is around 33%. But in the case of India, only 3% of its population invests\nin this market. The biggest reason for this investor gap is the lack of information. Most Indians still\nlive with the stigma that stock markets are not for everyone, only the people who track markets on\nan everyday basis and keep track of company happenings can invest in this asset. To address this\nissue and fill in the information gap for people who want to invest in stocks but have no time to\nmanually keep track of stock price, management decisions, quarter numbers, etc. we wanted to\ncurate a financial big data platform that can hold live as well as historical data related to Indian\nstocks, companies, and products.\nThe motive was to build a single window platform for providing all the finance-related\ninformation required by any individual who wants to acquaint themself with the factors that might\nimpact the price of a particular stock or the market as a whole, before investing in the Indian stock\nmarket. For this reason, we did not want to solely rely on the macro data like price, volume, etc. of\nthe stock but we wanted to build a platform that keeps an account of the company-related news,\nmergers and acquisitions, quarter numbers of the companies, etc. as all these can impact the value of\nthe shares to a great extent.\nAlong with these factors we wanted to include another way of understanding the price\nbehaviour of stocks and that is by identifying and keeping track of predictions made by 'Super\nForecasters'. We extracted the Indian Macro Data from the internet from sites like finance.yahoo.com\nand investing.com. For collecting nominal data, i.e., predictions made in forums, comments, blogs,\nnews, etc. we had to try different ways. We tried extracting this data using Manual web scrapping,\nwhich we were able to achieve but since we wanted to make a big data platform with live data\ninflowing at all times, manual scrapping was not a sustainable option, in addition to this it was a\ntedious task to scrape 5-6 websites for different comments and forums of multiple authors to\nrecognize super forecasters.\nAs a solution to this problem, we came up with automated web scraping. Now the issue with\nautomated scraping is that most websites, especially financial websites have a strong firewall in place\nthat does not allow any automated data scraping activity. Now our next plan of action was to use a\nthird-party web proxy service, which can be used to extract this data even with the firewall in place.\nOn further research, we found out that all of these third-party web proxy services are paid and since\nthe research did not have any funds in place, we could not go ahead with this way of data\nprocurement.\nDue to all these reasons stated above, we faced a huge problem of data insufficiency while\nworking on this research. We did not have access to any financial forums or news website data for us\nto find out potential super forecasters. To conclude, financial data procurement is a tedious task to\ncarry out in Indian markets. This data is not readily available in the case of India as it is in the case of\nthe USA and the UK.\nTo solve this problem of data insufficiency, we came up with the idea of collecting this data of\nstock predictions made by humans, first-hand by making a website. Since we could not arrange for\nany data that could enable our research to find potential super forecasters, we designed and hosted a\nwebsite that lets users sign in and then make predictions about the price of a stock in the near\nfuture. According to how close these predictions are made; users will be ranked on a leader board.\nWe can identify potential super forecasters if a user remains on this leader board for a considerable\namount of time. This way we can include human intelligence in making predictions for price along\nwith using Machine Learning algorithms for this work. In the following section, we will be discussing\nthe Price Prediction models that we have built using different machine learning models. Here is the\nlink to the site we have curated for solving the problem of data insufficiency and to identify super\nforecasters: https://augmentedfinancial.wixsite.com/stockmarketprices"}, {"title": "3. PROPOSED WORK", "content": "This study aimed to understand how the Indian Stock Market prices fluctuate and how can we\npredict the price of the market to help the Indian investor make informed decisions. As a solution to\nthis problem, to predict the price of Indian stocks, we have successfully built three different price\nprediction models that use Machine Learning to predict the price of a stock by giving the historical\ndata of that stock to the algorithm. Through this study, we want to understand how robust these\nmodels are. To get an idea of the entire Stock Market of India, we are using the NIFTY 50 stock data\nto compare the effectiveness of these three models.\nDuring the course of this study, we have worked on curating five price-prediction models. The\nfirst one is built using the bidirectional Long Short-Term Memory (LSTM) ML algorithm. The second\nmodel uses ARIMA to predict the price of NIFTY 50 stock, the third one is built using the combination\nof LSTM and CNN (Convolutional Neural Network), the fourth one uses GRU algorithm, and the last\none is built using the combination of LSTM and GRU."}, {"title": "3.1 Bidirectional LSTM (BILSTM)", "content": "In recent years, the Bidirectional LSTM architecture has gained prominence as a compelling\nextension to the standard LSTM. This architecture augments the LSTM cell by allowing information\nflow not only in the forward temporal direction but also in the reverse direction. By capturing\ndependencies in both directions, the Bidirectional LSTM can uncover contextual cues that may\nremain hidden in unidirectional models. This innovation has found applications in diverse fields\nincluding natural language processing, speech recognition, and, notably, time series analysis.\nIn this paper, we propose a novel neural network architecture that leverages the power of\nBidirectional LSTM layers for the task of time series regression. Our architecture aims to enhance the\npredictive capabilities of traditional LSTM-based models by effectively capturing intricate temporal\nrelationships present in sequential data. We combine the Bidirectional LSTM layer with a Dense layer\nto achieve accurate predictions of continuous numerical values."}, {"title": "3.1.1 Model Initialization", "content": "A sequential model is initialized using the Keras framework's `Sequential` class. This model\nallows for the construction of a linear stack of layers, facilitating the definition of deep learning\narchitecture."}, {"title": "3.1.2 Bidirectional LSTM Layer", "content": "To capture the temporal dependencies and patterns within the input data, a bidirectional\nLong Short-Term Memory (LSTM) layer is employed. The LSTM layer consists of 50 units and uses the\nrectified linear unit (ReLU) activation function. The bidirectional nature of this layer allows it to\nprocess the input data in both the forward and backward directions, enabling a comprehensive\nunderstanding of the temporal context. The `input_shape` parameter is specified as `(None, 1,\ntime_step)', indicating that the input can have any number of time steps."}, {"title": "3.1.3 Dense Layer", "content": "Following the bidirectional LSTM layer, a dense layer with a single node is added to the\nmodel. This layer is responsible for producing the output of the model. The use of a single-node\ndense layer suggests that the regression task aims to predict a single continuous value as the output."}, {"title": "3.1.4 Compilation", "content": "To prepare the model for training, it is compiled using the Adam optimizer. The Adam\noptimizer is a popular choice for deep learning tasks due to its adaptive learning rate and efficient\noptimization capabilities. The mean squared error (MSE) loss function is employed, which measures\nthe discrepancy between the predicted and true values. By minimizing this loss function, the model\naims to learn to accurately predict the target values."}, {"title": "3.1.5 Training", "content": "The model is trained using the `fit` function, which applies the backpropagation algorithm to\nupdate the model's weights iteratively. The training dataset, denoted as `trainX` and `trainY`,\nrepresents the input sequences and their corresponding target values, respectively. The training\nprocess consists of 50 epochs, indicating that the entire training dataset is traversed 50 times during\ntraining. A batch size of 1 is chosen, meaning that each sample is processed individually, allowing for\nfine-grained updates of the model's parameters.\nBy utilizing a bidirectional LSTM layer and a single-node dense layer, the model learns to\ncapture and understand the temporal dependencies within the input data, ultimately predicting a\ncontinuous value for the given regression task. The training process, consisting of multiple epochs\nand fine-grained updates with a batch size of 1, enables the model to optimize its parameters and\nimprove its predictive performance."}, {"title": "3.2 ARIMA Model", "content": "This research also explores the process of building and optimizing ARIMA (AutoRegressive\nIntegrated Moving Average) model parameters for time series forecasting. The study involves\nexperimenting with different combinations of p and q parameters to identify the most accurate\nmodel. The Mean Absolute Error (MAE) is employed as a performance metric to evaluate the\naccuracy of each model. The results highlight the significance of selecting appropriate parameters for\nachieving better forecasting accuracy in time series analysis."}, {"title": "3.2.1 Introduction", "content": "Time series forecasting is a critical task in various domains such as finance, economics, and\nenvironmental science. ARIMA models have proven to be effective in capturing the temporal patterns\nof time series data. The objective of this study is to determine the optimal values of the ARIMA\nmodel parameters p (order of the autoregressive component) and q (order of the moving average\ncomponent) to improve the accuracy of forecasting."}, {"title": "3.2.2 Background and Related Work", "content": "ARIMA models are widely used for time series analysis due to their ability to handle trends\nand seasonality. Previous research has explored different methodologies for parameter selection,\nincluding grid search and optimization techniques. However, in this study, we utilize a nested loop\napproach to systematically test various combinations of p and q values."}, {"title": "3.2.3 Methodology", "content": "We consider a time series dataset y as the target variable for forecasting. To find the optimal\np and q parameters, we employ a nested loop approach. The p_params and q_params are predefined\nlists of candidate values for p and q, respectively. For each combination of p and q, an ARIMA model\nwith the order (p, 0, q) is fitted to the data using the ARIMA function. The time taken for model\ntraining is recorded using the time.time() function."}, {"title": "3.2.4 Results", "content": "The results of the experiment are presented, showcasing the performance of each ARIMA\nmodel in terms of training time and forecasting accuracy. The MAE is calculated by comparing the\npredicted values y_pred obtained from each model with the actual values of the time series y. The\noutput displays the trained ARIMA model's order (p, d, q) and the elapsed time for model training in\nseconds. Subsequently, the corresponding MAE for each model is printed."}, {"title": "3.2.5 Discussion", "content": "The experimental results demonstrate that the selection of p and q significantly affects the\nforecasting accuracy of the ARIMA model. Models with larger values of p and q may lead to\noverfitting, while smaller values may underfit the data.\nThe analysis of the MAE values reveals the best-performing combination of p and q parameters,\nwhich yields the most accurate forecasting results for the given time series dataset."}, {"title": "3.3 LSTM and CNN", "content": "Sequence prediction, a cornerstone of various applications including speech recognition,\nfinancial forecasting, and natural language processing, involves deciphering the underlying temporal\npatterns within data sequences. This task carries immense significance as it empowers\ndecision-makers with the ability to anticipate future trends, adapt strategies, and make informed\nchoices. In the realm of deep learning, Recurrent Neural Networks (RNNs) and Convolutional Neural\nNetworks (CNNs) have demonstrated their prowess in handling sequential and grid-like data\nrespectively, catalysing advancements in sequence modelling.\nRNNs excel at capturing temporal dependencies by leveraging their recurrent connections,\nmaking them a natural fit for tasks involving sequences. Conversely, CNNs have proven to be highly\neffective in extracting spatial features through their convolutional layers, primarily designed for tasks\nlike image recognition. In this research, we propose a novel architecture that converges the strengths\nof both paradigms-Temporal Convolutional layers and Long Short-Term Memory (LSTM) networks.\nBy doing so, we aim to harness the benefits of temporal hierarchies inherent in sequences while\nexploiting the ability of LSTM networks to capture long-term dependencies.\nThe architecture amalgamates Temporal Convolutional layers to capture local temporal\nfeatures within sequences, enhancing the model's ability to detect significant patterns. The\nintegration of LSTM layers follows, allowing for the model to encode temporal dependencies while\nretaining computational efficiency. Furthermore, to mitigate overfitting risks inherent in complex\narchitectures, we incorporate dropout regularization in the form of Conv1D layers.\nThe primary objective of this study is to present a comprehensive analysis of the proposed\nCNN-LSTM hybrid architecture for sequence prediction tasks. We explore the architecture's\neffectiveness in capturing intricate temporal dependencies, showcase its predictive accuracy, and\nunderline its potential to enhance sequence prediction capabilities.\nIn the subsequent sections, we delineate the architectural design, detail the experimental\nmethodology, present empirical results on real-world data, and conclude by discussing the\nimplications of the findings. By innovatively fusing temporal convolution and recurrent memory\nmechanisms, this research contributes to the growing body of work aimed at creating more robust\nand effective neural network architectures for sequence modelling."}, {"title": "3.3.1 Model Initialization", "content": "A sequential model is initialized using the `Sequential class, which allows for the\nconstruction of a linear stack of layers. This model will be used to learn patterns and make\npredictions on sequential data."}, {"title": "3.3.2 TimeDistributed Conv1D Layer", "content": "To capture temporal dependencies within the sequential data, a TimeDistributed layer is\nadded to the model. This layer contains a Conv1D layer with 64 filters and a kernel size of 1, which\nenables the extraction of local features. The rectified linear unit (ReLU) activation function is applied\nto introduce non-linearity into the model. The `input_shape` parameter is set to `(None, 1,\ntime_stemp)', indicating that the model can accept input sequences of varying lengths."}, {"title": "3.3.3 TimeDistributed MaxPooling1D Layer", "content": "To downsample the feature maps obtained from the previous layer, a TimeDistributed layer\nwith a MaxPooling1D layer is incorporated into the model. The MaxPooling1D layer has a pool size of\n2 and employs 'same' padding to maintain the spatial dimensions of the feature maps."}, {"title": "3.3.4 TimeDistributed Flatten Layer", "content": "A TimeDistributed layer is introduced to perform the flattening operation on the output of\nthe previous layer. This operation transforms the 3D feature maps into a 2D representation, which\nfacilitates further processing."}, {"title": "3.3.5 LSTM Layer", "content": "To capture long-term dependencies and sequential patterns, an LSTM layer with 50 units is\nadded to the model. The rectified linear unit (ReLU) activation function is employed within the LSTM\nlayer to introduce non-linearities. This layer processes the sequential data from the previous\nTime Distributed layers and extracts relevant features."}, {"title": "3.3.6 Dense Layer", "content": "Following the LSTM layer, a single-node dense layer is included in the model to produce the\ndesired output. This layer uses a linear activation function, which ensures that the output can span a\nwide range of values."}, {"title": "3.3.7 Compilation", "content": "To configure the model for training, it is compiled using the Adam optimizer. The mean\nsquared error (MSE) loss function is selected, which measures the discrepancy between the\npredicted and true values. The Adam optimizer is known for its efficiency in training deep neural\nnetworks and is widely used in various domains."}, {"title": "3.3.8 Training", "content": "The model is trained using the `fit` function, which applies the backpropagation algorithm to\nupdate the model's weights iteratively. The training data, denoted as `trainX` and `trainY`, represent\nthe input sequences and their corresponding target values, respectively. The training process consists\nof 50 epochs, indicating that the entire training dataset is traversed 50 times. A batch size of 1 is\nused, meaning that a single sample is processed in each iteration. This choice allows for fine-grained\nupdates of the model's parameters and can be beneficial when dealing with sequential data.\nBy following this model architecture and training procedure, the model learns to capture\ntemporal patterns and make accurate predictions on sequential data. The utilization of convolutional\nand recurrent layers enables the model to extract local features and capture long-term\ndependencies, respectively, resulting in an effective framework for sequential data analysis."}, {"title": "3.4 Gated Recurrent Unit (GRU)", "content": "Time series prediction, a pivotal task in predictive analytics, holds immense significance\nacross diverse domains such as finance, healthcare, climate prediction, and industrial maintenance.\nAccurate forecasting of sequential data enables informed decision-making and proactive measures,\nleading to improved outcomes and resource allocation. In recent years, Recurrent Neural Networks\n(RNNs) have emerged as powerful tools for modelling sequential data due to their inherent ability to\ncapture temporal dependencies. Among the RNN variants, the Gated Recurrent Unit (GRU) stands\nout as a compelling architecture, known for its simplified gating mechanism and memory\nmanagement. GRUs exhibit remarkable performance in learning sequential patterns and have gained\ntraction in various applications.\nIn this paper, we present a comprehensive analysis of the proposed stacked GRU architecture\nwith dropout regularization. We examine its performance in terms of predictive accuracy and\ngeneralization capabilities using a real-world time series dataset. The empirical findings shed light on\nthe effectiveness of the architecture in capturing intricate temporal dependencies, offering insights\ninto its potential to improve forecasting accuracy in various practical applications. This study\nunderscores the importance of architectural innovation in neural network design to tackle the\ncomplexities inherent in time series data."}, {"title": "3.4.1 Model Initialization", "content": "The foundation of the study involves the initialization of a sequential model utilizing the\n`Sequential class. This structure serves as a scaffold for assembling a linear stack of interconnected\nneural layers, pivotal for capturing temporal relationships within sequential data."}, {"title": "3.4.2 Stacked GRU Layers", "content": "The model architecture commences with a pair of successive GRU layers. Each GRU layer\ncomprises 32 units and is configured to return sequences. These GRU layers are designed to capture\nsequential dependencies and temporal patterns, making them well-suited for time series analysis."}, {"title": "3.4.3 Stacked GRU Layer (Final Hidden State)", "content": "Consecutively, an additional GRU layer is introduced without the return sequence\nconfiguration. With 32 units, this layer captures a consolidated representation of the context\nextracted from the preceding layers, serving as the final hidden state of the network."}, {"title": "3.4.4 Dropout Regularization", "content": "To address overfitting, a dropout layer is integrated with a dropout rate of 0.20. This\nmechanism randomly deactivates a fraction of neurons during training, preventing over-reliance on\nspecific neurons and promoting better generalization."}, {"title": "3.4.5 Dense Layer", "content": "Following the stacked GRU layers and dropout, a single-node dense layer is added to\ngenerate the desired output. Operating with a linear activation function, this layer ensures the\nmodel's output can span a broad range of numerical values."}, {"title": "3.4.6 Compilation", "content": "The model is configured for training by compiling it with the mean squared error (MSE) loss\nfunction and the Adam optimizer. The MSE loss function quantifies the divergence between predicted\nand true values, while the Adam optimizer enhances training efficiency for deep neural networks.\nLater the MAE is used to compare the algorithms in terms of accuracy.\nThe experimental results underscore the potential of the proposed stacked GRU architecture\nin enhancing time series prediction. The integration of dropout regularization aids in reducing\noverfitting, resulting in improved generalization capabilities. The model's ability to capture intricate\ntemporal dependencies is evident from the achieved predictive performance. This model highlights\nthe effectiveness of stacked GRU architectures augmented with dropout regularization in the realm\nof time series prediction. The findings indicate that the stacked design, combined with dropout, can\nyield improved predictive accuracy by effectively capturing temporal patterns within the data."}, {"title": "3.5 LSTM and GRU", "content": "Time series prediction stands as a fundamental and intricate challenge across an array of\ndomains, underpinning critical decision-making processes in finance, meteorology, healthcare, and\nbeyond. The accurate forecasting of sequential data empowers organizations and individuals to\nanticipate future trends, optimize resource allocation, and proactively address emerging scenarios. In\nthis context, Recurrent Neural Networks (RNNs) have emerged as a ground-breaking framework for\ncapturing temporal dependencies within sequences, making them an indispensable tool in time\nseries analysis.\nAmong the diverse RNN variants, the Long Short-Term Memory (LSTM) and Gated Recurrent\nUnit (GRU) architectures have garnered substantial attention due to their aptitude in handling the\nvanishing gradient problem and modelling long-range dependencies. LSTMs, featuring memory cells\nand gates, exhibit proficiency in capturing both short-term patterns and extended context in\nsequences. Similarly, GRUs have gained prominence for their simplified gating mechanism, preserving\ncomputational efficiency while effectively learning temporal dependencies.\nIn light of the strengths exhibited by LSTM and GRU networks, this research embarks on an\nexploration of a hybrid architecture that amalgamates these two variants. This hybridization aims to\nleverage the distinct advantages of both architectures, capitalizing on LSTM's memory retention and\nGRU's efficient computation. Such synergistic combination is expected to contribute to enhanced\npredictive accuracy in time series tasks, where intricate temporal patterns are of paramount\nsignificance.\nFurthermore, recognizing the potential for overfitting in complex neural architectures, this research\nintegrates dropout regularization within the stacked GRU architecture. Dropout offers a\nmethodological remedy by randomly deactivating neurons during training, fostering better\ngeneralization, and alleviating the risk of model overfitting.\nThe primary objective of this study is to empirically investigate the proposed hybrid\nLSTM-GRU architecture's performance in the realm of time series prediction. Our assessment\nencompasses the analysis of predictive accuracy and model generalization capabilities using\nreal-world time series data. By leveraging both LSTM and GRU elements along with dropout\nregularization, this research contributes to the advancement of neural network architectures\ndesigned to extract intricate temporal features from complex sequences."}, {"title": "3.5.1 Model Initialization", "content": "The foundation of the research lies in the initialization of a sequential model utilizing the\n`Sequential class. This framework facilitates the creation of a linear sequence of interconnected\nneural layers, pivotal for learning temporal relationships within sequential data."}, {"title": "3.5.2 LSTM Layers", "content": "The architecture commences with two consecutive LSTM layers. Each LSTM layer comprises\n32 units and is tailored to return sequences. The LSTM architecture, celebrated for its memory cell\nmechanism, excels in retaining and leveraging historical information over varying temporal horizons."}, {"title": "3.5.3 GRU Layers with Return Sequences", "content": "Consecutively, a pair of Gated Recurrent Unit (GRU) layers is introduced. Configured to return\nsequences, these layers augment the model's capability to capture sequential dependencies. With 32\nunits each, the GRU layers employ gating mechanisms to modulate information flow, facilitating the\nextraction of relevant temporal features."}, {"title": "3.5.4 GRU Layer without Return Sequences", "content": "A subsequent GRU layer, also containing 32 units, is integrated without the return sequence\nsetting. This particular design captures the consolidated context from the preceding layers,\nrepresenting the final hidden state of the network."}, {"title": "3.5.5 Dense Layer", "content": "Post the recurrent layers, a single-node dense layer is incorporated to yield the desired model\noutput. This layer operates with a linear activation function, allowing the output to span a diverse\nrange of numerical values."}, {"title": "3.5.6 Compilation", "content": "To facilitate model training, the architecture is compiled with the mean squared error (MSE)\nloss function and the Adam optimizer. The MSE loss function quantifies the disparity between\npredicted and true values. The choice of the Adam optimizer is attributed to its efficiency in\noptimizing deep neural networks across various domains. The experimental results underscore the\npotential of the proposed hybrid LSTM-GRU architecture in time series prediction tasks. By\nseamlessly integrating LSTM and GRU units, the model showcases enhanced predictive accuracy and\nthe ability to discern complex temporal patterns."}, {"title": "4. RESULTS", "content": "All three proposed Machine Learning Models are built specifically for the data pertaining to\nthe NIFTY 50 index prices. To understand which algorithm out of the three proposed in this paper is\nbest to predict stock prices, a comparative analysis will be done by looking at the performance of the\nmodels using the Mean Absolute Error method. Mean Absolute Error is an accuracy measuring\ntechnique, which gives the accuracy of the model by calculating the mean of distance between the\npredicted value and the actual value. This distance is calculated without considering the error points'\ndirection and therefore it is the absolute mean value. The range for MAE is 0 to infinity and the lower\nthe value the better the score is, indicating that the model is predicting values close to the actual\nvalues.\nThe NIFTY 50 historical data was split into two parts: the training dataset and the testing\ndataset. To assess the performance of the model, they are made to predict the price of the stock for\nthe same dates as the testing dataset. This is done so that the predictions can later be compared with\nthe actual price of the stock. Following are the figures that show the plotted graph for the predicted\nstock value and the actual value of the stock on that day."}, {"title": "4.1 Bidirectional LSTM", "content": "The mean absolute error value for the testing dataset of this model is 196. The LSTM model's\nMean Absolute Error (MAE) of 196 provides a snapshot of its predictive performance. While not\nperfect, this value helps us gauge the model's accuracy in forecasting stock prices. Achieving a lower\nMAE would signify improved prediction capabilities, potentially through refining the model or\nincorporating more data."}, {"title": "4.2 ARIMA Model", "content": "The Mean Absolute Error for the plot is 91.7657. The MAE achieved by the ARIMA model in\npredicting stock prices underscores its noteworthy performance in capturing the intricate patterns\nand dependencies present in financial data. This result signifies a considerable advancement in the\nrealm of stock price prediction, holding the potential to guide investors, traders, and analysts with\nmore accurate insights. However, the dynamic and volatile nature of financial markets implies that no\nmodel can guarantee perfect predictions."}, {"title": "4.3 CNN + LSTM Model", "content": "Here the MAE value is 137. With an MAE of 137, it is evident that the model's predictions exhibit\na certain level of deviation from the actual stock prices. While the LSTM and CNN model provides\nvaluable insights into the stock market's dynamics, there is room for further refinement to enhance\nits predictive capabilities. As the financial landscape continually evolves, continuous efforts in\nresearch and model development are essential to achieve more precise and reliable predictions,\nenabling investors and stakeholders to make well-informed decisions in a volatile market\nenvironment."}, {}]}