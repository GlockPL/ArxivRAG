{"title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model", "authors": ["Zongjian Li", "Bin Lin", "Yang Ye", "Liuhan Chen", "Xinhua Cheng", "Shenghai Yuan", "Li Yuan"], "abstract": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes a limiting bottleneck in training LVDMs. Moreover, the block-wise inference method adopted by most LVDMs can lead to discontinuities of latent space when processing long-duration videos. The key to addressing the computational bottleneck lies in decomposing videos into distinct components and efficiently encoding the critical information. Wavelet transform can decompose videos into multiple frequency-domain components and improve the efficiency significantly, we thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages multi-level wavelet transform to facilitate low-frequency energy flow into latent representation. Furthermore, we introduce a method called Causal Cache, which maintains the integrity of latent space during block-wise inference. Compared to state-of-the-art video VAEs, WF-VAE demonstrates superior performance in both PSNR and LPIPS metrics, achieving 2\u00d7 higher throughput and 4\u00d7 lower memory consumption while maintaining competitive reconstruction quality. Our code and models are available at https://github.com/PKU-YuanGroup/WF-VAE.", "sections": [{"title": "1. Introduction", "content": "The release of Sora [5], a video generation model developed by OpenAI, has pushed the boundaries of synthesizing photorealistic videos, drawing unprecedented attention to the field of video generation. Recent advancements in Latent Video Diffusion Models (LVDMs), such as Open-Sora Plan [19], Open-Sora [45], Cog VideoX [39], EasyAnimate [38], Movie Gen [25], and Allegro [46], have led to substantial improvements in video generation quality.\nCurrent video VAEs remain constrained by fully convolutional architectures inherited from image era. They attempt to address video flickering and redundant information by incorporating spatio-temporal interaction layers and spatio-temporal compression layers. Several recent works, including OD-VAE [6, 19], Cog VideoX [39], CV-VAE [44], and Allegro [46] adopt dense 3D structure to achieve high-quality video compression. While these methods demonstrate impressive reconstruction performance, they require prohibitively intensive computational resources. In contrast, alternative approaches such as Movie Gen [25], and Open-Sora [45] utilize 2+1D architecture, resulting in re-"}, {"title": "2. Related Work", "content": "Variational Autoencoders. [18] introduced the VAE based on variational inference, establishing a novel generative network structure. Subsequent research [24, 30, 34] demonstrated that training and inference in VAE latent space could substantially reduce computational costs for diffusion models. [27] further proposed a two-stage image synthesis approach by decoupling perceptual compression from diffusion model. After that, numerous studies explored video VAEs with a focus on more efficient video compression including Open-Sora Plan [19], CogVideoX [39], and other models [3, 6, 40, 44, 46, 47]. Current video VAE architectures largely derive from earlier image VAE design [10, 27] and use a convolutional backbone. To address the challenges of inference on large videos, current video VAEs employ block-wise inference.\nLatent Video Diffusion Models. In the early stages of Latent Video Diffusion Models (LVDMs) development, models like AnimateDiff [13], and MagicTime [41, 42] primarily utilized the U-Net backbone [28] for denoising, without temporal compression in VAEs. Following the paradigm introduced by Sora, recent open-source models such as Open-Sora [45], Open-Sora Plan [19, 20], and CogVideoX [39] have adopted DiT backbone with a spatiotemporally compressed VAE. Some methods including Open-Sora [45] and Movie Gen [25], employ a 2.5D design in either the DiT backbone or the VAE to reduce training costs. For LVDMs, the upper limit of video generation quality is largely determined by the VAE's reconstruction quality. As overhead increase rapidly in scale, optimizing the VAE becomes crucial for handling large-scale data and enabling extensive pre-training."}, {"title": "3. Method", "content": ""}, {"title": "3.1. Wavelet Transform", "content": "Preliminary. The Haar wavelet transform [23, 26], a fundamental form of wavelet transform, is widely used in signal processing [11, 12, 15]. It efficiently captures spatiotemporal information by decomposing signals through two complementary filters. The first is the Haar scaling filter h = [1, 1], which acts as a low-pass filter that captures the average or approximation coefficients. The second is the Haar wavelet filter g = [1,-1], which functions as a high-pass filter that extracts the detail coefficients. These orthogonal filters are designed to be simple yet effective, with the scaling filter smoothing the signal and the wavelet filter detecting local changes or discontinuities.\nMulti-level Wavelet Transform. For a video signal V \u2208 Rcxtxhxw, where c, t, h, and w denote the number of chan-"}, {"title": "3.2. Architecture Design of WF-VAE", "content": "Through analyzing different sub-bands, we found that video energy is mainly concentrated in the low-frequency subband Shhh. Based on this observation, we establish an energy flow pathway outside the backbone, as show in Fig. 2, so that low-frequency information can smoothly flow from video to latent representation during encoding process, and then flow back to the video during the decoding process. This would inherently allow the model to attention more on low-frequency information, and apply higher compression rates to high-frequency information. With the additional path, we can reduce the computational cost brought by the dense 3D convolutions in the backbone.\nSpecifically, given a video V, we apply multi-level wavelet transform to obtain pyramid features W(1), W(2) and W(3). We utilize W(1) as the input for the encoder and the target output for the decoder. The convolutional backbone and multi-level wavelet transform layer downsample the feature maps simultaneously at every downsampling layer, enabling the concatenation of features from two branches in the backbone. We employ Inflow Block to transform the channel numbers of W(2) and W(3) to C'flow, which are then concatenated with feature maps from backbone. We compare Cflow to the width of the energy flow pathway, as analyzed in Sec. 4.4. On the decoder side, we maintain a structure symmetrical to the encoder. We extract feature maps with Cflow channels from the backbone and process them through Outflow Block to obtain VW(2), VW(3). In order to allow information to flow from the lower layer to the hhh sub-band of the next layer, we have:\n$(2) = IWT(W(3)) + \u015c(2)\nSimilarly, at the decoder output layer:\nShhh = IWT(W(2)) +S(1)"}, {"title": "3.3. Causal Cache", "content": "We replace regular 3D convolutions with causal 3D convolutions [40] in WF-VAE. The causal convolution applies kt -1 temporal padding at the start with kernel size kt. This padding strategy ensures the first frame remains independent from subsequent frames, thus enabling the processing of images and videos within a unified architecture. Furthermore, we leverage the causal convolution properties to achieve lossless inference. We first extract the initial frame from a video with T frames. The remaining T-1 frames are then partitioned into temporal chunks, where Tchunk represents the chunk size. Let st denote the temporal convolutional stride, and m = 0,1,2,... represents the chunk block index. To maintain the continuity of convolution sliding windows, each chunk caches its tail frames for the next chunk. The number of cached frames is given by:\nTcache(m) = kt + mTchunk\nTo make Causal Cache function properly, the following conditions must be satisfied:\n(T-kt)s.t. (T-kkt) mod Tchunk \u2265 kt,kt > St.\nFor example, kt = 3, st = 1, Tchunk = 4, the equation yields Tcache(m) = 2, as illustrated in Fig. 9 (a). Similarly, with kt = 3, St = 2, Tchunk = 4, we obtain Tcache(m) ="}, {"title": "3.4. Training Objective", "content": "Following the training strategies of [9, 27], our loss function combines multiple components, including reconstruction loss (comprising L1 and perceptual loss [43]), adversarial loss, and KL regularization [18]. Our model is characterized by a low-frequency energy flow and symmetry between the encoder and decoder. To maintain this architectural principle, we introduce a regularization term denoted as LWL (WL loss), which enforces structural consistency by penalizing deviations from the intended energy flow:\nLWL = |\u0174(2) \u2013 W(2)| + |\u0174(3) \u2013 W(3) |.\nThe final loss function is formulated as:\nL = Lrecon + adv Lady + AKLLKL + AWLLWL.\nWe examine the impact of the weighting factor \u03bb\u03c9\u03b9 in Sec. 4.4. Following [9], we implement dynamic adversarial loss weighting to balance the relative gradient magnitudes between adversarial and reconstruction losses:\nAadv = [Lrecon][Ladv] + \u03b4\nwhere GL [] denotes the gradient with respect to last layer of decoder, and d = 10-6 is used for numerical stability."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Setup", "content": "Baseline Models. To assess the effectiveness of WF-VAE, we perform a comprehensive evaluation, comparing its performance and efficiency against several state-of-the-art VAE models. The models considered are: (1) OD-VAE [6], a 3D causal convolutional VAE used in Open-Sora Plan 1.2 [19]; (2) Open-Sora VAE [45]; (3) CV-VAE [44]; (4) Cog VideoX VAE [39]; (5) Allegro VAE [46]; (6) SVD-VAE [4], which does not compress temporally and (7) SD-VAE [27], a widely used image VAE. Among these, CogVideoX VAE has a latent dimension of 16, while others use a latent dimension of 4. Notably, all VAEs except CV-VAE and SD-VAE, have been validated on LVDMs previously, making them highly representative for comparison.\nDataset & Evaluation. We utilize the Kinetics-400 dataset [16] for both training and validation. For testing, we employ the Panda70M [7] and WebVid-10M [2]"}, {"title": "4.2. Comparison With Baseline Models", "content": "We compare WF-VAE with baseline models in three key aspects: computational efficiency, reconstruction performance, and performance in diffusion-based generation. To ensure fairness in comparing metrics and model efficiency, we disable block-wise inference strategies across all VAEs.\nComputational Efficiency. The computational efficiency evaluations are conducted using an H100 GPU with float32 precision. Performance evaluations are performed at 33"}, {"title": "4.3. Ablation Study", "content": "Increasing the latent dimension. Recent works [8, 10] show that the number of latent channel significantly impacts reconstruction quality. We conduct experiments with 4, 8, 16, and 32 latent channels. Fig. 7a illustrates that reconstruction performance improves significantly as the number of latent channels increases. However, larger latent channels may increase convergence difficulty in training diffusion model, as evidenced by the results presented in Tab. 2.\nExploration of WL loss weight AWL. To ensure structural symmetry in WF-VAE, we introduce WL loss. As shown in Fig. 7b, the model exhibits a substantial decrease in PSNR performance when AWL = 0. Our experiments demonstrate optimal results for both PSNR and LPIPS metrics when AWL = 0.1. Furthermore, our analysis of different loss functions reveals that utilizing L1 loss produces superior results compared to L2 loss."}, {"title": "4.4. Causal Cache", "content": "To validate the lossless inference capability of Causal Cache, we compared our approach with existing block-wise inference methods implemented in several open-source LVDMs. OD-VAE [6] and Allegro [46] provide spatio-temporal tiling inference implementations, and CogVideoX [39] implements a temporal caching strategy. As demonstrated in Tab. 5, both tiling strategies and conventional caching methods exhibited performance degradation, while Causal Cache achieves lossless inference with performance metrics identical to direct inference."}, {"title": "5. Conclusion", "content": "In this paper, we propose WF-VAE, an innovative autoencoder that utilizes multi-level wavelet transform to extract pyramidal features, thus creating a primary energy flow pathway for encoding low-frequency video information into a latent representation. Additionally, we introduce a lossless block-wise inference mechanism called Causal Cache, which completely resolves video flickering associated with prior tiling strategies. Our experiments demonstrate that WF-VAE achieves state-of-the-art reconstruction performance while maintaining low computational costs. WF-VAE significantly reduces the expenses associated with large-scale video pre-training, potentially inspiring future designs of video VAEs.\nLimitations and Future Work. The initial design of the decoder incorporated insights from [27], employing a highly complex structure that resulted in more parameters in the backbone of the decoder compared to the encoder. Although the computational cost remains manageable, we consider these parameters redundant. Consequently, we aim to streamline the model in future work to fully leverage the advantages of our architecture."}]}