{"title": "ALMOST SURE CONVERGENCE OF AVERAGE REWARD TEMPORAL DIFFERENCE LEARNING", "authors": ["Ethan Blaser", "Shangtong Zhang"], "abstract": "Tabular average reward Temporal Difference (TD) learning is perhaps the simplest and the most fundamental policy evaluation algorithm in average reward reinforcement learning. After at least 25 years since its discovery, we are finally able to provide a long-awaited almost sure convergence analysis. Namely, we are the first to prove that, under very mild conditions, tabular average reward TD converges almost surely to a sample path dependent fixed point. Key to this success is a new general stochastic approximation result concerning nonexpansive mappings with Markovian and additive noise, built on recent advances in stochastic Krasnoselskii-Mann iterations.", "sections": [{"title": "INTRODUCTION", "content": "Temporal Difference learning (TD, Sutton (1988)) is the most fundamental algorithm in Reinforcement Learning (RL, Sutton & Barto (2018)). In this paper, we investigate the almost sure convergence of TD in its simplest form with a tabular representation, in average reward Markov Decision Processes (MDPs, Bellman (1957); Puterman (2014)). Namely, we investigate the following iterative updates\n$\\J_{t+1} = J_t + \\beta_{t+1}(R_{t+1} - J_t),$\\n(Average Reward TD)\n$\\v_{t+1}(S_t) = v_t(S_t) + \\alpha_{t+1}(R_{t+1} - J_t + v_t(S_{t+1}) \u2013 v_t(S_t)),$\nwhere {So, R1, S1, . . . } is a sequence of states and rewards from an MDP with a fixed policy and a finite state space S, Jt \u2208 R is the scalar estimate of the average reward, vt \u2208 R|S| is the tabular value estimate, and {at, \u1e9et} are learning rates. This iterative update algorithm, known as average reward TD, dates back to at least Tsitsiklis & Roy (1999). Surprisingly, despite its simplicity and fundamental importance, its almost sure convergence had not been established in the 25 years since its inception until this work. Even more surprisingly, the theoretical analysis of average reward TD with linear function approximation has seen more progress than that of the tabular version we consider here. In this paper, after presenting the necessary background in Section 2, we will elaborate on the difficulty in analyzing tabular average reward TD with existing techniques in Section 3, offering insight into why progress on this topic has been unexpectedly slow. Then we proceed to our central contribution, where we prove that under mild conditions, the iterates {v} in (Average Reward TD) converge almost surely to a sample-path-dependent fixed point.\nThis almost sure convergence is achieved by extending recent advances in the convergence analysis of Stochastic Krasnoselskii-Mann (SKM) iterations (Bravo et al., 2019; Bravo & Cominetti, 2024) to settings with Markovian and additive noise. This line of research originates from the seminal work Cominetti et al. (2014), which introduces a novel fox-and-hare race model to analyze Krasnoselskii-Mann (KM) iterations (Krasnosel'skii, 1955). By extending this line of work to Markovian settings, we not only establish the almost sure convergence of average reward TD, but also pave the way for further analysis of other RL algorithms through the lens of SKM iterations."}, {"title": "BACKGROUND", "content": "In this paper, all vectors are column. We use ||\u00b7|| to denote a generic operator norm and use e to denote an all-one vector. We use ||\u00b7||2 and ||\u00b7||\u221e to denote 12 norm and infinity norm respectively. We use O() to hide deterministic constants for simplifying presentation, while the letter ( is reserved for sample path dependent constants.\nIn reinforcement learning, we consider an MDP with a finite state space S, a finite action space A, a reward function r : S \u00d7 A \u2192 R, a transition function p : S \u00d7 S \u00d7 A \u2192 [0, 1], an initial distribution Po: S\u2192 [0,1]. At time step 0, an initial state So is sampled from po. At time t, given the state St, the agent samples an action At ~ \u03c0(\u00b7|St), where \u03c0 : A \u00d7 S \u2192 [0, 1] is the policy being followed by the agent. A reward Rt+1 = r(St, At) is then emitted and the agent proceeds to a successor state St+1 ~ p(.|St, At). In the rest of the paper we will assume the Markov chain {St} induced by the policy \u03c0 is irreducible and thus adopts a unique stationary distribution d\u00b5. The average reward (a.k.a. gain, Puterman (2014)) is defined as\n$\\J_\\pi = \\lim_{T\\rightarrow\\infty}\\frac{1}{T} \\sum_{t=1}^T\\mathbb{E} [R_t].$\nCorrespondingly, the differential value function (a.k.a. bias, Puterman (2014)) is defined as\n$\\v_\\pi(s) = \\lim_{T\\rightarrow\\infty}\\frac{1}{T} \\sum_{t=1}^T\\mathbb{E} [\\sum_{i=1}^t(R_{t+i} \u2013 J_\\pi) | S_t = s] .$\nThe corresponding Bellman equation (a.k.a. Poisson's equation) is then\n$\\v = r_\\pi + P_\\pi v,$\\nwhere v \u2208 R|S| is the free variable, r\u3160 \u2208 R|S| is the reward vector induced by the policy \u03c0, i.e., r(s) = \u03a3\u03b1\u03c0(as)r(s, a), and P\u201e \u2208 R|S|\u00d7|S| is the transition matrix induced by the policy \u03c0, i.e., P(s, s') = \u03c0(as)p(s'|s, a). It is known (Puterman, 2014) that all solutions to (1) form a set\n$\\V^* = {V + ce | c \\in \\mathbb{R}}.$\nThe policy evaluation problem in average reward MDPs is to estimate \u03c5\u03c0, perhaps up to a con-stant offset ce. In view of (1) and inspired by the success of TD in the discounted setting (Sutton, 1988), Tsitsiklis & Roy (1999) use (Average Reward TD) to estimate \u03c5\u03c0 (up to a constant offset). In (Average Reward TD), Jt estimates the average reward J. Its learning rate \u1e9et does not need to be the same as at, the learning rate for updating the differential value function estimation."}, {"title": "HARDNESS OF AVERAGE REWARD TD", "content": "To elaborate on the hardness in analyzing (Average Reward TD), we first rewrite it in a compact form. Define the augmented Markov chain Yt+1 = (St, At, St+1). It is easy to see that {Y} evolves in the finite space Y = {(s,a, s') | \u03c0(a|s) > 0,p(s'|s, a) > 0}. We then define a function H : R|S| \u00d7 Y \u2192 R|S| by defining the s-th element of H (v, (so, ao, 51)) as\n$\\H (v, (s_0, a_0, s_1))[s] = I\\{s = s_0\\} (r(s_0, a_0) \u2013 J_t + v(s_1) \u2013 v(s_0)) + v(s).$\nThen the update to {v} in (Average Reward TD) can then be expressed as\n$\\v_{t+1} = v_t + \\alpha_{t+1} (H(v_t, Y_{t+1}) - v_t + \\epsilon_{t+1}).$\nHere \u20act+1 \u2208 R|S| is the random noise vector defined as et+1(s) = I{s = St} (Jt \u2013 J\u201e). This \u20act+1 is the current estimate error of the average reward estimator J\u0141. Intuitively, the indicator I{s = St} reflects the asynchronous nature of (Average Reward TD). For each t, only the St-indexed element in vt is updated. To better analyze (3), we investigate the expectation of H. We define\n$\\h(v) = \\mathbb{E}_{s_0\\sim d\\mu, a_0\\sim\\pi(\\cdot|s_0), s_1\\sim p(\\cdot|s_0,s_1)} [H (v, (s_0, a_0, s_1))] = D(r_\\pi - J_\\pi e + P_\\pi v \u2013 v) + v,$\nwhere D \u2208 R|S|\u00d7|S| is a diagonal matrix with the diagonal being the stationary distribution d\u03bc. Then (3) can be viewed as Euler's discretization of the ODE\n$\\frac{dv(t)}{dt} = h(v(t)) \u2013 v(t).$"}, {"title": "Hardness in Stability", "content": "Stability (i.e., supt ||vt|| <\u221e almost surely) is a necessary condition for almost sure convergence. In ODE based stochastic approximation methods to establish almost sure convergence, the first step is usually to establish the stability (Benveniste et al., 1990; Kushner & Yin, 2003; Borkar, 2009). The ODE@\u221e technique (Borkar & Meyn, 2000; Borkar et al., 2021; Liu et al., 2024) is perhaps one of the most powerful stability techniques in RL, which considers the function\n$\\h_\\infty(v) = \\lim_{c \\rightarrow \\infty} \\frac{h(cv)}{c} = D(P_\\pi - I)v + v.$\nCorrespondingly, the ODE@\u221e is defined as\n$\\frac{dv(t)}{dt} = h_\\infty(v(t)) \u2013 v(t) = D(P_\\pi - I)v(t).$\nIf the ODE (6) is globally asymptotically stable, existing results such as Borkar et al. (2021); Liu et al. (2024) can be used to establish the desired stability of {vt}. Unfortunately, the vector ce with any c \u2208 R is an equilibrium of (6). So it cannot be globally asymptotically stable. This problem comes from the lack of a discoutning factor in average reward MDPs. In the discounted setting with a discount factor y \u2208 [0, 1), the corresponding ODE@\u221e is $\\frac{du(t)}{dt} = D(\\gamma P_\\pi \u2013 I)v(t)$. It is well known that D(P\u201e \u2013 I) is negative definite (Tsitsiklis & Roy, 1996) and therefore Hurwitz. As a result, it is globally asymptotically stable. Alternatively, if a discount factor is present, an inductive argument can also be used to establish stability following the method in Gosavi (2006). In the average reward setting, there is, however, no discounting. So neither the Hurwitz argument nor the inductive argument applies."}, {"title": "Hardness in Convergence", "content": "Suppose we were somehow able to establish the desired stability, then standard stochastic approximation results can be used to show that {vt} converge almost surely to a bounded invariant set of the ODE (5), or more precisely speaking, a possibly sample path dependent compact connected internally chain transitive invariant set\u00b9 (Kushner & Yin, 2003; Borkar, 2009). Unfortunately, we are not aware of any finer characterization of this set. Even if it was proved that this set must be a subset of V in (2) (we are not aware of any such proof yet), the best we could say is still that {vt} converges to this set. It is still possible that {vt} oscillates within this set or around the neighborhood of this set and never settles down on any particular fixed point. This gives rise to the central open question that this paper aims to answer:\ncan we prove that {vt} converge almost surely to a single fixed point in V*?\nWe shall give an affirmative answer shortly. We note that this affirmative answer is quite intuitive. Notice that\n$\\h(v) = (I + D(P_\\pi \u2013 I))v + D(r_\\pi \u2013 J_\\pi e).$\nIt is easy to verify that I + D(P\u201e - I) is a stochastic matrix. It then follows that ||I + D(P\u2081 - I)||\u221e = 1. As a result, the operator h is a nonexpansive mapping w.r.t. ||\u00b7||\u221e (Lemma 3). Since the ODE (5) can be expressed as\n$\\frac{dv(t)}{dt} = h(v(t)) \u2013 v(t),$\nthe nonexpansivity of h confirms that any solution v(t) to the ODE (5) will converge to an initial value dependent fixed point in V* (Theorem 3.1 of Borkar & Soumyanatha (1997)). So intuitively, if {v} approximates a solution v(t) well, it should also converge to a single fixed point. However, existing ODE based convergence analysis fails to move forward beyond convergence to a bounded invariant set. This difficulty comes from two aspects. The first is still the lack of the discount factor y in average reward MDPs. Otherwise h can easily be a contraction and the ODE (5) can easily be globally asymptotically stable. As a result, the invariant set will be a singleton. The second is the lack of a reference value (Abounadi et al., 2001). In a recent differential TD algorithm (Wan et al., 2021b), the corresponding ODE is\n$\\frac{dv(t)}{dt} = \\Upsilon_\\pi - e e^Tv(t) + P_\\pi v(t) \u2013 v(t),$"}, {"title": "Hardness with Linear Function Approximation", "content": "The (Average Reward TD) has also been extended to linear function approximation (Tsitsiklis & Roy, 1999; Konda & Tsitsiklis, 1999; Wu et al., 2020; Zhang et al., 2021). Unfortunately, the results in linear function approximation do not contribute much to the understanding of the tabular version. In the following paragraphs, we elaborate on this surprising fact.\nInstead of using a look-up table v \u2208 R|S| to store the value estimate, the idea of linear function approximation is to approximate v(s) with \u03c6(s)\u0f0bw, where \u00a2 : S \u2192 RK is the feature function mapping a state s to a K-dimensional feature $(s) \u2208 RK and w is the learnable weights. Let \u03a6\u2208 R|S|\u00d7K be the feature matrix, whose s-th row is the $(s)T. Then linear function approximation essentially uses \u03a6w to approximate v. It is obvious that if I = I, i.e., a one-hot encoding feature is used, the linear function approximation degenerates to the tabular method. Thus one would typically expect the results in linear function approximation to supersede tabular results. This is true in most settings, but for (Average Reward TD) there is some subtlety. The linear average reward TD (Tsitsiklis & Roy, 1999) updates {wt} iteratively as\n$\\W_{t+1} = W_t + \\alpha_{t+1} (R_{t+1} - J_t + \\phi(S_{t+1})^T w_t \u2013 \\phi(S_t)^T w_t) \\phi(S_t),$\nwhere the update of {Jt} is identical to (Average Reward TD). This update can be viewed as the Euler's discretization of the ODE\n$\\frac{dw(t)}{dt} = \\Phi^T D(P_\\pi \u2013 I)\\Phi v(t) + \\Phi^T D(r_\\pi \u2013 J_\\pi e).$\nUnfortunately, the matrix TD(P\u2081 \u2013 I)\u0424 is not necessarily Hurwitz. Consequently, the ODE (9) is not necessarily globally asymptotically stable. The problem still arises from the lack of a discount factor \u2013 it is well known that TD(P\u201e \u2013 I) is negative definite and thus Hurwitz.\nNevertheless, to proceed with the theoretical analysis, besides the standard assumption that I has linearly independent columns, Tsitsiklis & Roy (1999); Konda & Tsitsiklis (1999) further assume that for any c\u2208 R, w \u2208 Rd, it holds that w \u2260 ce. Under this assumption, Tsitsiklis & Roy (1999) prove that TD(P - I) is negative definite (Wu et al. (2020) assume this negative definiteness directly) and the iterates {wt} converges almost surely. But unfortunately, this additional assumption does not hold in the tabular setting where I = I (apparently, Ie = e). As a result, the almost sure convergence in Tsitsiklis & Roy (1999) does not shed light on the behavior of tabluar average reward TD. A more recent work Zhang et al. (2021) prove that\n$\\min_{||w||_2=1,w\\in E} W^T D(P \u2013 I)\\Phi w > 0$\nwithout requiring \u03a6w \u2260 ce, where E is a subspace of RK. Based on this, Zhang et al. (2021) prove that\n$\\mathbb{E} [|J_t \u2013 J_*|^2 + ||\\Pi_E(W_t - W_*)||^2]$\nconverges to 0, where w* is one desired fixed point and IE denotes the orthogonal projection onto the subspace E. Zhang et al. (2021) further provide a convergence rate. This is a significant im-provement over Tsitsiklis & Roy (1999) but still not satisfactory in two aspects. First, this result is"}, {"title": "Hardness in Stochastic Krasnoselskii-Mann Iterations", "content": "Having elaborated the hardness in analyzing (Average Reward TD) with ODE based approaches, we now resort to an alternative approach, the Stochastic Krasnoselskii-Mann (SKM) iterations. In its simplest and deteriministic form, Krasnoselskii-Mann (KM) iterations study the convergence of iterates\n$\\X_{t+1} = X_t + \\alpha_{t+1}(TX_t - x_t),$\nwhere T is some nonexpansive mapping. Since we have already demonstrated that h is non-expansive in ||||\u221e, SKM appears to be promising in analyzing (Average Reward TD). It, however, turns out that the current state of results for SKM iterations are insufficient for proving the almost sure convergence of (Average Reward TD). We elaborate on this fact here.\nEarlier works on the convergence of (KM) typically require that the operator T : C \u2192 C has a compact image, i.e., T(C) is a compact subset of C. Under some other restrictive conditions, Krasnosel'skii (1955) first proves the convergence of (KM) to a fixed point of T. This result is further generalized by Edelstein (1966); Schaefer (1957); Ishikawa (1976); Reich (1979). More recently, Cominetti et al. (2014) use a novel fox-and-hare model to connect KM iterations with Bernoulli random variables, providing a sharper convergence rate for ||xk \u2013 Txk|| \u2192 0.\nHowever, in many scenarios such as RL, requiring an algorithm to satisfy the exact form of (KM) is usually not plausible. Instead, some noise may appear. This gives rise to the study of the inexact KM iterations (IKM).\n$\\X_{t+1} = X_t + \\alpha_{t+1}(TX_t - x_t + \\epsilon_{t+1}),$\nwhere {et} is a sequence of deterministic noise. Bravo et al. (2019) extend Cominetti et al. (2014) and establish the convergence of (IKM), under some mild conditions on {et}.\nHowever, a deterministic noise is still not desirable in many problems. To this end, a stochastic version of (IKM) is studied, which considers the iterates\n$\\X_{t+1} = X_t + \\alpha_{t+1}(TX_t - X_t + M_{t+1}),$\nwhere {M} is a Martingale difference sequence. Under mild conditions, Bravo & Cominetti (2024) prove the almost sure convergence of (SKM) to a fixed point of T.\nUsing (3) and (4), we can write (Average Reward TD) as,\n$\\V_{t+1} = V_t + A_{t+1}(h(v_t) - v_t + H(v_t, Y_{t+1}) \u2013 h(v_t) + \\epsilon_{t+1}),$\nwhere we recall that h is non-expansive in ||\u00b7||\u221e. This, however, does not fit into (SKM). First, there is an additive stochastic noise {\u20act}. Second, the sequence {H(vt, Yt+1) \u2013 h(vt)} is not a Martingale difference sequence. If the sequence of noise {Y} was i.i.d., then {H(vt, Yt+1) \u2013 h(vt)} would have been a Martingale difference sequence. But unfortunately, in (Average Reward TD), the sequence {Y} is a Markov chain, far from being i.i.d. Moreover, the noise {et} is now stochastic. So (IKM) concerning a deterministic noise would not apply either. These demonstrate the hardness in analyzing (Average Reward TD) with existing (SKM) results.\nNevertheless, this motivates us to extend the results from Bravo & Cominetti (2024) to study (SKM) with Markovian and additive noise, in the form of (11)."}, {"title": "STOCHASTIC KRASNOSELSKII-MANN ITERATIONS WITH MARKOVIAN AND ADDITIVE NOISE", "content": "As promised, we are now ready to extend the analysis of (SKM) in Bravo et al. (2019); Bravo & Cominetti (2024) to SKM with Markovian and additive noise. Namely, we consider the following iterates\n$\\X_{n+1} = X_n + A_{n+1} H(X_n, Y_{n+1}) - X_n + \\Xi_{n+1} (H(x_n, Y_{n+1}) - X_n + \\epsilon_{n+1}).$\nHere {Xn} are stochastic vectors evolving in Rd, {Y} is a Markov chain evolving in a finite state space Y, H : Rd \u00d7 V \u2192 Rd defines the update, {{1} \u20ac (1) 1} is a sequence of stochastic noise evolving in Rd, and {an} is a sequence of deterministic learning rates. We make the following assumptions.\nAssumption 4.1 (Ergodicity). The Markov chain {Yn} is irreducible and aperiodic.\nThe Markov chain {Y} thus adopts a unique invariant distribution, denoted as du. We use P to denote the transition matrix of {Yn}.\nAssumption 4.2 (1-Lipschitz). The function H is 1-Lipschitz continuous in its first argument w.r.t. some operator norm ||\u00b7|| and uniformly in its second argument, i.e., for any x, x', y, it holds that\n$\\||H(x, y) \u2013 H(x', y)|| \\le ||x \u2212 x'||.$\nThis assumption has two important implication. First, it implies that H(x, y) can grow at most linearly. Indeed, let x' = 0, we get ||H(x, y)|| \u2264 ||H(0, y)|| + ||x||. Define CH = maxy ||H(0, y)||, we get\n$\\||H(x, y) || \\le C_H + ||x||.$\nSecond, define the function h : Rd \u2192 Rd as the expectation of H over the stationary distribution \u03b1\u03bc:\n$\\h(x) = \\mathbb{E}_{y\\sim d\\mu} [H(x,y)].$\nWe then have that h is nonexpansive. Namely,\n$\\||h(x) - h(x') || \\le \\sum_y d\\mu(y)||H(x, y) \u2013 H(x',y)|| < ||x \u2212 x' ||.$\nThus this h is exactly the nonexpansive operator in the SKM literature. We of course need to assume that the problem is solvable.\nAssumption 4.3 (Fixed Points). The nonexpansive operator h adopts at least one fixed point.\nWe use X* \u2260 ) to denote the set of the fixed points of h.\nAssumption 4.4 (Learning Rate). The learning rate {an} has the form\n$\\a_n = \\frac{1}{(n+1)^b}, \\alpha_0 = 0,$\nwhere b \u2208 (, 1].\nThe primary motivation for requiring b \u2208 (1, 1] is that our learning rates an need to decrease quickly enough for certain key terms in the proof to be finite. The specific need for b > can be seen in the proof of (35) in Lemma 8. We now impose assumptions on the additive noise.\nAssumption 4.5 (Additive Noise).\n$\\sum_{k=1}^{\\infty} \\mathbb{E}k||^2 <\\infty a.s.,$\n$\\mathbb{E} [|\\epsilon_k||^2] = O(\\frac{1}{k}).$\nThe first part of Assumption 4.5 can be interpreted as a requirement that the total amount of additive noise remains finite, akin to the assumption on et in (IKM) in Bravo et al. (2019). Additionally, we"}, {"title": "STOCHASTIC KRASNOSELSKII-MANN ITERATIONS WITH MARKOVIAN AND ADDITIVE NOISE", "content": "As promised, we are now ready to extend the analysis of (SKM) in Bravo et al. (2019); Bravo & Cominetti (2024) to SKM with Markovian and additive noise. Namely, we consider the following iterates\n$\\X_{n+1} = X_n + A_{n+1} H(X_n, Y_{n+1}) - X_n + \\Xi_{n+1} (H(x_n, Y_{n+1}) - X_n + \\epsilon_{n+1}).$\nHere {Xn} are stochastic vectors evolving in Rd, {Y} is a Markov chain evolving in a finite state space Y, H : Rd \u00d7 V \u2192 Rd defines the update, {{1} \u20ac (1) 1} is a sequence of stochastic noise evolving in Rd, and {an} is a sequence of deterministic learning rates. We make the following assumptions.\nAssumption 4.1 (Ergodicity). The Markov chain {Yn} is irreducible and aperiodic.\nThe Markov chain {Y} thus adopts a unique invariant distribution, denoted as du. We use P to denote the transition matrix of {Yn}.\nAssumption 4.2 (1-Lipschitz). The function H is 1-Lipschitz continuous in its first argument w.r.t. some operator norm ||\u00b7|| and uniformly in its second argument, i.e., for any x, x', y, it holds that\n$\\||H(x, y) \u2013 H(x', y)|| \\le ||x \u2212 x'||.$\nThis assumption has two important implication. First, it implies that H(x, y) can grow at most linearly. Indeed, let x' = 0, we get ||H(x, y)|| \u2264 ||H(0, y)|| + ||x||. Define CH = maxy ||H(0, y)||, we get\n$\\||H(x, y) || \\le C_H + ||x||.$\nSecond, define the function h : Rd \u2192 Rd as the expectation of H over the stationary distribution \u03b1\u03bc:\n$\\h(x) = \\mathbb{E}_{y\\sim d\\mu} [H(x,y)].$\nWe then have that h is nonexpansive. Namely,\n$\\||h(x) - h(x') || \\le \\sum_y d\\mu(y)||H(x, y) \u2013 H(x',y)|| < ||x \u2212 x' ||.$\nThus this h is exactly the nonexpansive operator in the SKM literature. We of course need to assume that the problem is solvable.\nAssumption 4.3 (Fixed Points). The nonexpansive operator h adopts at least one fixed point.\nWe use X* \u2260 ) to denote the set of the fixed points of h.\nAssumption 4.4 (Learning Rate). The learning rate {an} has the form\n$\\a_n = \\frac{1}{(n+1)^b}, \\alpha_0 = 0,$\nwhere b \u2208 (, 1].\nThe primary motivation for requiring b \u2208 (1, 1] is that our learning rates an need to decrease quickly enough for certain key terms in the proof to be finite. The specific need for b > can be seen in the proof of (35) in Lemma 8. We now impose assumptions on the additive noise.\nAssumption 4.5 (Additive Noise).\n$\\sum_{k=1}^{\\infty} \\mathbb{E}k||^2 <\\infty a.s.,$\n$\\mathbb{E} [|\\epsilon_k||^2] = O(\\frac{1}{k}).$\nThe first part of Assumption 4.5 can be interpreted as a requirement that the total amount of additive noise remains finite, akin to the assumption on et in (IKM) in Bravo et al. (2019). Additionally, we"}, {"title": "AVERAGE REWARD TEMPORAL DIFFERENCE LEARNING", "content": "We are now ready to prove the convergence of (Average Reward TD). Throughout the rest of the section, we utilize the following assumption.\nAssumption 5.1 (Ergodicity). Both S and A are finite. The Markov chain {St} induced by the policy w is apreriodic and irreducible.\nTheorem 2. Let Assumption 5.1 hold. Consider the learning rates in the form of $\\alpha_t = \\frac{1}{(t+1)^b}$, t = $\\frac{1}{(t+1)^b}$ with b \u2208 (, 1]. Then the iterates {vt} generated by (Average Reward TD) satisfy\n$\\lim_{t\\rightarrow\\infty} v_t = v a.s.,$\nwhere v \u2208 V* is a possibly sample path dependent fixed point.\nProof We proceed via verifying assumptions of Theorem 1. In particular, we consider the compact form (3). Under Assumption 5.1, it is obvious that {Y} is irreducible and aperiodic and adopts a unique stationary distribution.\nTo verify Assumption 4.2, we demonstrate that H is 1-Lipschitz in v w.r.t ||.||\u221e. For notation simplicity, let y = (so, ao, 51). We have,\nH(v, y)[s] \u2013 H(v', y)[s] = I{s = so}(v(81) \u2212 v\u2032 (81) \u2013 v(so) + v' (so)) + v(s) \u2013 v' (s).\nSeparating cases based on s, if s \u2260 so, we have\n$\\|H(v, y)[s] \u2013 H(v', y)[s]| = |v(s) \u2013 v'(s)| < ||v \u2013 v'||_\\infty$\nFor the case when s = so, we have\n$\\|H(v,y)[s] \u2013 H(v', y)[s]| = |v(s_1) \u2013 v'(s_1)| \\leq ||v \u2013 v' ||_\\infty.$\nTherefore\n$\\||H(v, y) \u2013 H(v', y)||_\\infty = \\max_{s\\in S} |H(v, y)[s] \u2013 H(v', y)[s]| < ||v \u2013 v' ||$\nIt is well known that the set of solutions to Poisson's equation V defined in (2) is non-empty (Puterman, 2014), verifying Assumption 4.3. Assumption 4.4 is directly met by the definition of At."}, {"title": "AVERAGE REWARD TEMPORAL DIFFERENCE LEARNING", "content": "We are now ready to prove the convergence of (Average Reward TD). Throughout the rest of the section, we utilize the following assumption.\nAssumption 5.1 (Ergodicity). Both S and A are finite. The Markov chain {St} induced by the policy w is apreriodic and irreducible.\nTheorem 2. Let Assumption 5.1 hold. Consider the learning rates in the form of $\\alpha_t = \\frac{1}{(t+1)^b}$, t = $\\frac{1}{(t+1)^b}$ with b \u2208 (, 1]. Then the iterates {vt} generated by (Average Reward TD) satisfy\n$\\lim_{t\\rightarrow\\infty} v_t = v a.s.,$\nwhere v \u2208 V* is a possibly sample path dependent fixed point.\nProof We proceed via verifying assumptions of Theorem 1. In particular, we consider the compact form (3). Under Assumption 5.1, it is obvious that {Y} is irreducible and aperiodic and adopts a unique stationary distribution.\nTo verify Assumption 4.2, we demonstrate that H is 1-Lipschitz in v w.r.t ||.||\u221e. For notation simplicity, let y = (so, ao, 51). We have,\nH(v, y)[s] \u2013 H(v', y)[s] = I{s = so}(v(81) \u2212 v\u2032 (81) \u2013 v(so) + v' (so)) + v(s) \u2013 v' (s).\nSeparating cases based on s, if s \u2260 so, we have\n$\\|H(v, y)[s] \u2013 H(v', y)[s]| = |v(s) \u2013 v'(s)| < ||v \u2013 v'||_\\infty$\nFor the case when s = so, we have\n$\\|H(v,y)[s] \u2013 H(v', y)[s]| = |v(s_1) \u2013 v'(s_1)| \\leq ||v \u2013 v' ||_\\infty.$\nTherefore\n$\\||H(v, y) \u2013 H(v', y)||_\\infty = \\max_{s\\in S} |H(v, y)[s] \u2013 H(v', y)[s]| < ||v \u2013 v' ||$\nIt is well known that the set of solutions to Poisson's equation V defined in (2) is non-empty (Puterman, 2014), verifying Assumption 4.3. Assumption 4.4 is directly met by the definition of At."}, {"title": "RELATED WORK", "content": "It is now clear that our success fundamentally originates from the novel fox-and-hare racing model introduced by Cominetti et al. (2014). This fox-and-hare model is too complicated to be detailed here but it is for sure an entirely different paradigm from the ODE and Lyapunov based methods in RL (Bertsekas & Tsitsiklis, 1996; Konda & Tsitsiklis, 1999; Borkar & Meyn, 2000; Srikant & Ying, 2019; Borkar et al., 2021; Chen et al., 2021; Zhang et al., 2022; Meyn, 2022; Zhang et al., 2023; Liu et al., 2024; Meyn, 2024). Bravo & Cominetti (202"}]}