{"title": "Deciphering genomic codes using advanced NLP techniques: a scoping review", "authors": ["Shuyan Cheng", "Yishu Wei", "Yiliang Zhou", "Zihan Xu", "Drew N Wright", "Jinze Liu", "Yifan Peng"], "abstract": "Objectives: The vast and complex nature of human genomic sequencing data presents challenges for effective analysis. This review aims to investigate the application of Natural Language Processing (NLP) techniques, particularly Large Language Models (LLMs) and transformer architectures, in deciphering genomic codes, focusing on tokenization, transformer models, and regulatory annotation prediction. This review aims to assess data and model accessibility in the most recent literature, gaining a better understanding of the existing capabilities and constraints of these tools in processing genomic sequencing data.\nMethods: Following Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, our scoping review was conducted across PubMed, Medline, Scopus, Web of Science, Embase, and ACM Digital Library. Studies were included if they focused on NLP methodologies applied to genomic sequencing data analysis, without restrictions on publication date or article type.\nResults: A total of 26 studies published between 2021 and April 2024 were selected for review. The review highlights that tokenization and transformer models enhance the processing and understanding of genomic data, with applications in predicting regulatory annotations like transcription-factor binding sites and chromatin accessibility.\nDiscussion: The application of NLP and LLMs to genomic sequencing data interpretation is a promising field that can help streamline the processing of large-scale genomic data while providing a better understanding of its complex structures. It can potentially drive advancements in personalized medicine by offering more efficient and scalable solutions for genomic analysis. Further research is needed to discuss and overcome limitations, enhancing model transparency and applicability.", "sections": [{"title": "1 Introduction", "content": "The vast and complex nature of human genomic sequencing data necessitates advanced computational methods for effective analysis and interpretation. In recent years, the intersection of Natural Language Processing (NLP) and data interpretation has garnered significant interest. Large Language Models (LLMs) and transformer architectures, initially designed for natural language understanding, have shown promise in deciphering the genomic code [1]. By converting genetic sequences into computationally interpretable formats and leveraging the sophisticated attention mechanisms of transformers, researchers aim to enhance the accuracy and depth of genomic sequencing analysis [2].\nThe human genome, composed of over 3 billion base pairs, contains information critical for understanding biological processes and disease mechanisms [3]. Traditional methods like Sanger sequencing, next-generation sequencing (NGS), and alignment-based approaches focus on generating and aligning sequence data but often fall short in interpreting large, complex genomic datasets, particularly for identifying regulatory regions and intricate patterns [4]. NLP and LLMs provide a scalable approach beyond raw sequencing, enabling efficient analysis, the discovery of regulatory regions, and deeper insights into genetic variation [5].\nThis literature review explores the application of NLP and LLMs in genomic data processing, focusing on three key areas: tokenization of genomic sequences, utilization of transformer models, and prediction of regulatory annotations. Tokenization involves converting raw genomic sequences into a format suitable for analysis, making the data more accessible for computational models [5]. With their advanced attention mechanisms, transformer architectures capture complex contextual relationships within the data, providing deeper insights into genomic structures [6]. Finally, predictive modeling uses the preprocessed data to identify critical regulatory elements such as transcription-factor binding sites, enhancer-promoter interactions, chromatin accessibility, and gene expression patterns [7, 8].\nBy examining these areas, this review aims to highlight the transformative potential of integrating NLP into genomic sequencing research. This integration not only enables scientists to leverage the power of LLM to gain a more convenient and deeper understanding of genomic data but also paves the way for advancements in personalized medicine, where treatments can be tailored based on an individual's genetic makeup. Despite the challenges, including data complexity, model interpretability, and validation, the progress in this field holds significant promise for future breakthroughs in genomics and beyond."}, {"title": "2 Methods", "content": ""}, {"title": "2.1 Eligibility criteria", "content": "Our review follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines (https://www.prisma-statement.org/). The eligibility criteria for the included studies focused on two main areas: NLP and genetic association studies. Studies were included if they specifically addressed applications or methodologies related to NLP in the context of genomic data analysis. No restrictions were placed on publication date or article type, and the primary focus was on studies published in English. A summary of the PRISMA checklist is provided in the Supplementary Materials A."}, {"title": "2.2 Information sources", "content": "A comprehensive search was conducted across multiple databases to identify relevant studies. The systematic searches were conducted on Ovid MEDLINE (In-Process and Other Non-Indexed Citations and Ovid MEDLINE 1946 to Present), Ovid EMBASE (1974 to present), Scopus, Web of Science, and the ACM Digital Library. The databases searched included PubMed (https://pubmed.ncbi.nlm.nih.gov), the Institute of Electrical and Electronics Engineers (IEEE) Xplore Digital Library (https://ieeexplore.ieee.org), Google Scholar (https://scholar.google.com), and Semantic Scholar (https://www.semanticscholar.org). The searches were executed in April 2024, with the most recent search conducted in June 2024 to ensure the inclusion of the latest available studies. Important references identified during the searches were also tracked for further examination."}, {"title": "2.3 Search strategy", "content": "Our search strategy was designed to maximize coverage using broad combinations of keywords and related terms, ensuring a comprehensive capture of relevant literature. It included both controlled vocabulary and free-text terms related to \u201cnatural language processing\" and \"genetic association studies.\u201d The strategy incorporated keywords and phrases such as \u201cnatural language processing,\u201d \u201clarge language model,\" \u201cNLP,\u201d \u201cLLM,\u201d \u201cdata mining,\u201d \u201cgenomic association studies,\u201d \u201cpolymorphism,\u201d \u201cSNP,\u201d \u201ctoken,\u201d \u201ctransformer,\" \"BERT,\u201d and \u201cregulatory annotations.\" Full details of the search strategies for each database are provided in the Supplementary Materials B."}, {"title": "2.4 Study selection", "content": "The study selection process involved two stages of screening. Initially, two independent researchers screened the abstracts of all identified articles for relevance to the topics of natural language processing and genetic association studies. Abstracts deemed appropriate were then subjected to a full-text review by the same two researchers to confirm their eligibility for inclusion. The screening was facilitated using Covidence, a web-based tool designed to streamline systematic reviews [9].\nThe initial phase led to the exclusion of 702 studies based on the following key exclusion criteria: 1) 73 studies were excluded for not using human omics data 2) 39 studies were excluded because they did not employ natural language processing methods 3) 526 studies were excluded for their irrelevance, particularly those not involving specific NLP downstream tasks or human-omics data 4) 60 studies were excluded for providing insufficient content such as conference proceedings or any submissions that did not provide full-text articles 5) 4 studies consisted of secondary literature, such as survey and review papers. In the full-text screening phase, 59 additional studies were excluded due to misaligned objectives or incomplete texts, where an abstract suggested relevance. Still, the full text lacked sufficient content for relevant analysis."}, {"title": "3 Results", "content": "A total of 26 studies published between 2021 and April 2024 met the inclusion criteria and were selected for final discussion."}, {"title": "3.1 Preprocessing and Modeling", "content": "Preprocessing genomic sequencing data is crucial before predictive modeling can be applied. This involves converting the raw genomic sequences into a format that computational models can understand, making the complex genetic data more accessible. The preprocessing steps include tokenization, which breaks text into manageable sub-word units. Subsequently, advanced architectures like transformers are utilized during the modeling phase to capture intricate dependencies and patterns in the data."}, {"title": "3.1.1 Tokenization of Genomic Data for LLMS", "content": "Tokenization is the first step in preprocessing genomic sequencing data for LLMs, which can capture biologically significant patterns such as promoter elements like TATA or CAAT boxes. It involves breaking down the sequences into smaller, manageable, and interpretable units that can be fed into computational models. K-mers is the most widely used tokenization method among the studies reviewed, and it is consistent with common practice in genomic research. Biological functions are often determined by short patterns in DNA sequences, such as motifs and binding sites, making k-mers suited to capture these. Furthermore, many studies are built on top of existing pre-trained models and follow the tokenizer used in the original model. For example, several studies build on DNABERT [15, 34] and employ k-mers accordingly. Beyond k-mers, other tokenization methods are also applied. For instance, Hossain et al. approach the problem of CpG island detection as named entity recognition (NER) and use a BPE tokenizer [11, 23].\nK-merization: K-merization is a method in bioinformatics that breaks down DNA sequences into smaller, overlapping segments of fixed length, known as k-mers, where \u2018k' represents the number of nucleotides in each segment [36]. For instance, in DNABERT, applying k-mers with a value of 3 on a sequence like \"ACTGACTGAC\" results in tokens such as [\"ACT\", \"CTG\", \"TGA\", \"GAC\"]. Several studies have effectively utilized k-mer tokenization for sequencing data processing. Wang et al. applied k = 1,3,5 to split DNA sequences into k-mers, treating them as words in a natural language [18]. Additionally, Ji et al. employed various k-mer lengths (3, 4, 5, 6) in training and fine-tuning DNABERT models to understand the DNA sequences and predict regulatory elements [12]. DNABERT was trained using the Bidirectional Encoder Representations from Transformers (BERT) model [37], which is developed to create deep bidirectional representations by preprocessing text without labels, considering the context from both the left and the right at every layer. Similarly, Zeng et al. introduced a custom corpus and tokenizer using 6-mers with taxonomic lineage descriptions and aimed to predict the DNA methylation sites (F1 = 0.95) [25]. Moreover, An et al. used 6-mers to pre-train human genome data and fine-tune downstream data, getting the moDNA model, which is designed for promoter prediction and transcription start site (TSS) detection (F1 = 0.862) [22].\nByte-Pair Encoding (BPE): BPE is a tokenization method that iteratively merges the most frequent pairs of characters in a text to create subword tokens, allowing for efficient handling of rare and unseen words [38]. When BPE is applied to the sequence \u201cACTGACTGAC,\u201d it may yield tokens like [\"ACTG\", \"ACTG\", \"AC\"]. Hossain et al. utilized BPE [38] to tokenize the DNA sequences [11]. The tokenized data were then applied to three models, each having a parameter size of 66 million. DistilBERT was set as the benchmark, and Conditional Random Fields (CRF) [39], and Attention Mask was added to each layer to detect CpG islands in DNA sequences. This methodology aims to predict the promoter regions and identify epigenetic causes of diseases. The F1 scores for these three models are 0.718, 0.726, and 0.735, respectively.\nIn 2023, Hossain et al. refined this approach by pre-training the BERT model and CRF layer (parameter size = 110M) on a large sequencing dataset with 142,325 CpG islands and fine-tuning on a smaller dataset [23]. Eventually, they achieved an F-1 score of 0.834.\nFixed Nucleotide Tokenization: Fixed Nucleotide Tokenization is a method that segments DNA sequences into fixed-length nucleotide fragments. This technique differs from K-merization, primarily because it does"}, {"title": "3.1.2 Transformer Architecture for Genomic Sequencing Data", "content": "Once the data is tokenized, the next step involves using transformer architectures to capture the complex, contextual relationships within them. Transformers are highly effective due to their attention mechanisms, which allow models to focus on different parts of the sequence simultaneously.\nSince most studies are phrased as prediction or classification problems, BERT and its variants are often used as feature extractors, with an additional classifier added on top of that. In this section, we will mostly focus on the BERT and transformer components. For sequencing labeling tasks, the transformer is directly used [10, 40]. Additionally, some models incorporate CNNs within transformer-based architectures to enhance local feature extraction. CNNs are particularly effective for capturing motifs and short sequence patterns, which are essential for refining sequence-level predictions within broader transformer architectures.\nBERT and Variants: After tokenizing the DNA sequences, Wang et al. pre-trained and fine-tuned a BERT model to predict 5-methylcytosine (5mC) sites and identify DNA enhancers [18]. The BERT-5mc model derived an accuracy of 0.933. The DNABERT models by Ji et al. on different k-mers have 110M parameters and indicate the F1 values over 0.9 for all 'k' [12]. Luo et al. introduced TFBert, a model based on the BERT architecture specifically designed to predict DNA-protein binding sites [15]. The model was derived by initializing with the DNABERT pretraining model and then performing task-specific pretraining on a large dataset of 690 ChIP-seq datasets consisting of various DNA-protein binding data. The model tokenized DNA sequences into k-mers, treating them as words in the context of a language model, allowing it to capture the context of DNA sequences effectively. The primary goal of TFBert is to improve the accuracy and robustness of DNA-protein binding predictions, especially in cases where the datasets are small or medium-sized. The results demonstrated that TFBert [15] achieved state-of-the-art performance, outperforming other existing models, with an average AUC of 0.947, making it a valuable tool for various biological sequence prediction tasks.\nTransformer Encoder Blocks: Besides BERT, several studies only utilized vanilla transformer encoder blocks or modified versions, which refer to the original transformer architecture with basic attention and feed-forward layers, without additional layers or task-specific pretraining. Unlike models like BERT, which"}, {"title": "3.2 Predicting Regulatory Annotations", "content": "After preprocessing and deriving embeddings through the transformer, the tokenized and transformed genomic data can predict various regulatory annotations. These predictions include identifying transcription-factor binding sites, enhancer-promoter interactions, chromatin accessibility, and gene expression patterns. Many studies have demonstrated significant success in these predictive tasks,. Our selection is based on studies that employed predictive models and reported at least one performance metric, ensuring high standards of empirical validation.\nMethylation and Epigenetic Sites Detection: Recent studies in NLP applied to genomic data have focused on predicting various DNA methylation sites, such as 5-methylcytosine [18], N6-adenine, N4-cytosine, and 5-hydroxymethylcytosine [25], which are crucial for understanding epigenetic modifications and their roles in gene regulation. These efforts are integral to advancing our understanding of gene expression regulation,"}, {"title": "3.3 Data Diversity and Accessibility", "content": "Upon reviewing recent research on the application of NLP in genomic sequencing data interpretation, a distinct trend in the types of data used becomes evident. DNA sequences dominate the landscape, highlighting their frequent use in NLP-driven genomic analyses. There is also a growing incorporation of RNA sequences and multi-omic data, reflecting a shift toward more diverse and comprehensive datasets.\nAdditionally, specialized data types, such as structural data from 3D Magnetic Resonance Imaging (MRI) and genomic variations like Single Nucleotide Polymorphisms (SNPs) and Copy Number Variations (CNVs), along with advanced sequencing technologies like Nanopore sequencing, are also being integrated. This broadens the scope of analysis within NLP applications, as these data types provide complementary information beyond traditional sequence analysis. This trend indicates an ongoing expansion in the variety of genomic and multimodal data utilized in NLP for genomics.\nAmong the studies, most datasets are publicly accessible, with a few studies having limited or request-based access for specific subsets. It fosters inclusivity and sustainable development in integrating genomic data with NLP, enhancing collaboration and progress in this rapidly evolving field.\nComputational Resources Requirements: Advanced NLP techniques are notoriously known for high hardware requirements. However, the actual computational demands for application vary significantly depending on the extent of model training involved. Pretraining models like BERT or large language models (LLMs) from scratch requires significant computational resources. For example, Ji et al. trained DNABERT for 25 days using 8 GPUs [12], while Zhang et al. trained on 6,000 GPUs [30]. In contrast, using existing pretrained models as feature extractors and building a classifier such as XGBoost [14, 28] or small neural networks [29, 34] on top have minimal requirements. Fine-tuning or continuously pretraining from a publicly available model lies between these extremes [15, 25]. In addition, some studies intentionally consider resource constraints in model design and training processes. For example, Roy et al. stopped training at 10,000 steps due to resource limitations and diminishing marginal returns to training [17]. Furthermore, Wang et al. designed a small architecture (a two-layer transformer) to fit into low-resource environments [35]."}, {"title": "4 Discussion", "content": "Applying LLMs within NLP for genomic data interpretation significantly advances the processing and analysis of complex biological data. This review highlights key areas where these technologies have been effectively utilized, including tokenization techniques, transformer architectures, and the prediction of regulatory annotations. While the progress in these areas is promising, several challenges and opportunities for future research remain.\nOne of the major challenges in applying NLP and LLMs to genomic data is the inherent complexity of the data itself [41]. Genomic sequences contain vast information, making it difficult to capture the full context within a model. This complexity also impacts model interpretability, as the black-box nature of LLMs makes it challenging for researchers to understand how the model arrives at its predictions. A \u2018black-box' model refers to a system where the internal workings are not transparent or easily understood, and training data is obscured or undocumented, making it difficult to trace how specific inputs are transformed into outputs [42]. For instance, while models like DNABERT [12] have successfully predicted regulatory elements and annotated single-cell RNA data, the pathways and features leading to these predictions are often vague, limiting their utility in clinical settings.\nTo address this issue, future research should focus on developing methods that enhance model interpretability. Techniques such as attention visualization, feature attribution, and post-hoc analysis can provide insights into which parts of the genomic sequence most influence the model's predictions. By making these models"}]}