{"title": "Umbrella Reinforcement Learning \u2013 computationally efficient tool for hard non-linear problems", "authors": ["Egor E. Nuzhin", "Nikolai V. Brilliantov"], "abstract": "We report a novel, computationally efficient approach for solving hard non- linear problems of reinforcement learning (RL). Here we combine umbrella sampling, from computational physics/chemistry, with optimal control meth- ods. The approach is realized on the basis of neural networks, with the use of policy gradient. It outperforms, by computational efficiency and implemen- tation universality, all available state-of-the-art algorithms, in application to hard RL problems with sparse reward, state traps and lack of terminal states. The proposed approach uses an ensemble of simultaneously acting agents, with a modified reward which includes the ensemble entropy, yield- ing an optimal exploration-exploitation balance.", "sections": [{"title": "1. Introduction", "content": "Reinforcement learning (RL) is a rapidly developing field of Artificial Intelligence; it allows solving very different problems, ranging from purely engineering, to rather fundamental ones. Numerous efficient RL algorithms have been proposed recently. Roughly, the RL algorithms may be classified as model-free and model-based [1], although other classifications are used [2]. Among the most popular model-free algorithms are actor-critic class al- gorithms A2C [3] and PPO [4]. They demonstrated a great empirical success\nAt the heart of the RL approach is the generation of state-action tra- jectories (episodes) associated with a particular agent policy. Utilizing the information, gained by the agent in an episode, the policy (that is, an ac- tion in response to a state) is improved for the next episode, and the next trajectory is generated. Such an improvement of the policy continues un- til the best (optimal) policy is achieved\u00b9. In spite of universality of such an approach, which may be deployed for numerous information-processing systems, including robots, advisory dialogue systems (e.g. ChatGPT2), self- driving cars, etc., for some important for applications problems, RL may lose its efficiency and even fail. These problems are associated with (i) a delayed or sparse reward, (ii) lack of terminal states and (iii) state traps; we call them \"hard problems\" (see the discussion below). All these difficulties stem from the consecutive simulations.\nTo overcome limitations caused by such simulations, one can apply meth- ods of parallel simulations, that is, \"whole-states\u201d methods, similar to the Value Iteration (VI) and Policy Iteration (PI) methods of classical dynamic programming [16]. Still, time discretization of the environment and the appli- cation of a discrete mesh of states and actions entail extensive computations for each update step of the action policy. Indeed, such an update is to be done for all available state transitions. This can be extremely demanding, in terms of memory and CPU time, especially for high-dimensional state space the so-called \"curse of dimensionalily\" [17, 18].\nAnother class of RL approaches developed to tackle hard problems re- fer to the Demonstration Learning, which includes techniques like Behavior Cloning (BC) [19], Inverse Reinforcement Learning (IRL) [20], Offline Re-"}, {"title": "2. Definition of hard problems", "content": "There are three main features of hard problems, where RL fails or dra- matically loses its efficiency:\n\u2022 A sparse, or significantly delayed reward; e.g. the reward may be gained only at the final state - the goal.\n\u2022 A presence of state traps, where an agent gets stuck, which are difficult to avoid.\n\u2022 A lack of a certain terminal state, e.g. the final goal may correspond to multiple states or a state region.\nThe most prominent examples of hard problems are depicted in Fig. 1. The Mountain Car problem [34], Fig. 1a, illustrates the first difficulty. Namely, the agent here is a car, randomly placed in a valley. It may accelerate in either direction, which corresponds to possible actions, but cannot overcome the downhill force due to the gravity. The goal is to reach the top of the hill. This may not be generally achieved by simple motion in one direction, but only by swinging in the well with an increasing amplitude, applying an appropriate acceleration in appropriate time instants. The top of the hill is the final position, and only there a reward is given. In other words, any intermediate reward is lacking. This complicates an application of RL which guides, by rewarding, towards a better (and eventually to the best) policy. A simple random strategy a random acceleration here, generally fails.\nSimilarly, in the Multi-Valley Mountain Car problem, the agent is re- warded only reaching a zone between flags, see Fig. 1c. Here the agent needs not only reach the zone with the reward, but also balance within the zone, without any terminal state. Another hard RL problem is the Acrobot prob- lem [35], Fig. 1b, where an \"acrobat\" needs to swing appropriately, to lift up to the target height, where the reward is given. The same occurs in the StandUp problem, illustrated in Fig. 1d \u2013 the reward may be received at the very end of a successful action, that is, in the target region.\nA safe evacuation from a hazardous place, when a leader (e.g. a special- ized robot) needs to guide a crowd towards an exit, e.g. [36], belongs to the same class of problems. Saving lives is the main goal there; hence, a reward is given only when all humans are evacuated. Otherwise the reward is zero. Again, there is no way to assess an efficiency of a strategy until the final state is reached. The list of examples may be continued.\nThe second difficulty, of getting stuck in local state traps, is generic. Generally, it is associated with properties of the state-action space. For instance, when an agent gets stuck in a minimum of potential energy (like in Mountain Car), or when optimization of a policy drives it toward a local maximum of the expected return.\nState traps cause a failure in walking of a humanoid robot - here \"get up to go\" requires much more complex dynamics than simple walking straight [37].\nFinally, the third difficulty, is related to the lack of a terminal state. It is also generic and arises in numerous applications. For instance, when learning certain movements, e.g. learning of an agent to walk [38], or learning a defensive behavior by a swarm [39], a particular terminal state may be hard to define. Indeed, how is it possible to judge, whether an agent has already achieved an efficient enough strategy? Such a conclusion seems to be very subjective. In other words, it is hard to define a terminal state in advance, that is, a clear criterion to cease simulations is lacking.\nNoteworthy, the above difficulties do not simply sum up, but enhance each other, making hard problems with a few difficulties even more challeng- ing. Fortunately, the novel approach, reported here, resolves all the above problems."}, {"title": "3. The main ideas of Umbrella RL", "content": "All the above hard problems difficulties stem from the consecutive nature of simulations, associated with a generation of state-action trajectories for a singe agent, or a limited number of agents. What happens if one deals with a continuous ensemble of agents, instead of discrete agents? By a contin- uous we mean an ensemble where the agents are characterized by a set of state variables of probabilistic nature, with the magnitudes varying in cer- tain intervals. It is described by a distribution function $p(s,t)$, where s is a multidimensional variable, specifying agents' states and t is time. Corre- spondingly, $p(s, t)ds$ \u2013 quantifies the number of agents with the state variable"}, {"title": "3.1. Policy gradient for Umbrella RL", "content": "The policy gradient method has demonstrated its high efficiency and is incorporated in many RL approaches. Here we generalise this method, to deal with a continuous ensemble of agents. It is also convenient to consider continuous time. We apply the conventional assumption, that the system follows Markov decision process [47, 48], so that agents make decisions (about actions) in accordance with a random policy \u03c0. The actions a depends on the state only, that is,\n$\\alpha \\sim \\pi(a|s)$.\nwhere s is the set variables characterizing an agent state.\nIn other words, the action a is defined by the action policy $\\pi = \\pi(a|s)$. The state of some particular agent from the ensemble alters in time, subject to the evolution equation,\n$\\dot{s} = v(s, a)$,\nwhere v is a given (multidimensional) function, quantifying the rate of change of a state with the (multidimensional) variable s. The above equations (1)-(2) govern the behavior of agents of the ensemble with the distribution function $p(s,t)$ and hence determine the evolution of $p(s, t)$, with the initial distribu- tion $p_0(s) = p(s, 0)$.\nThe aim of RL, is to obtain an optimal policy $\\pi^*$, that is, the policy maximizing the total reward of the agent, which always acts in accordance with the policy \u03c0. For Markovian process it is determined by $r(s, a)$ \u2013 the reward the agent receives for an action a, made in a state s. The overall quality of a policy is quantified by the expected return, $J(\\pi)$ \u2013 the weighted sum of all rewards received on the state-action trajectory. For Umbrella RL and continuous time it reads,\n$J(\\pi) = \\int_{0}^{\\infty} \\langle e^{-\\gamma t} r(s_t, a_t) \\rangle_{p_t} dt$,\nwhere $p_t$ abbreviates the distribution $p(s, t)$, depending on the policy \u03c0and $\\langle ... \\rangle_{p_t}$ denotes the averaging over the distribution $p(s,t)$. Note that with- out this averaging Eq. (3) is the continuous-time analogue of the common discrete-time expected return [49]. The discount constant, $\\gamma \\in [0, 1)$, reflects"}, {"title": "3.2. Effective computation of the policy gradient", "content": "To compute $J^{URL}(\\pi_{\\theta_k})$, given by Eq. (10), one needs to find, for each step k, the average distribution $\\bar{p}(s)$, from Eq. (8) and the advantage func- tion, $A_u(s, a)$, defined by Eqs. (11)-(12). The straightforward computation of these quantities is extremely inefficient and can hardly have practical ap- plication. Here we present computationally effective approaches to find $\\bar{p}(s)$ and $A_u(s, a)$.\nIt may be shown [see Appendix (Sec. Appendix A.3)], that the advantage function obeys the equation,\n$\\mathbb{E}_{a \\sim \\pi} A_u(s, a) = \\langle v \\rangle_{\\pi} \\cdot \\nabla_s V_u(s) + \\bar{r}_u - |\\log \\gamma| V_u(s) = 0$,\nwhere we introduce averages, $\\bar{r}_u = \\mathbb{E}_{a \\sim \\pi} [r_u(s,a)]$ and $\\bar{v} = \\mathbb{E}_{a \\sim \\pi} [v(s,a)]$. Solving Eq. (14) for $V_u(s)$ and substituting it into Eq. (11) we find $A_u(s, a)$.\nSimilarly, instead of using Eqs. (4) and (8), the average ensemble density $\\bar{p}(s)$ may be found as the solution of the following equation [see Appendix (Sec. Appendix A.4) for detail]:\n$\\nabla_s \\cdot \\bar{p}(s) \\bar{v} - \\log \\gamma (\\bar{p}(s) - p_0(s)) = G (\\bar{p}(s), s) = 0$,\nwhere $p_0(s) = p(s, 0)$ and we introduce the growth rate function,\n$G (\\bar{p}(s), s) = \\nabla_s \\cdot \\bar{p}(s) \\bar{v} - \\log \\gamma (\\bar{p}(s) - p_0(s))$,\nwhich steady-state vanishes, $G (\\bar{p}(s), s) = 0$. The solution of the PDEs (14) and (15) may be performed using neural networks (NN). This yields a dra- matic increase of the computational efficiency, as compared to the direct calculation of $V_u(s)$ and $\\bar{p}(s)$ by Eqs. (12) and (8). Moreover, Eqs. (14) and (15) may be solved again, using the policy gradient method, see Appendix (Sec. Appendix A.5)."}, {"title": "3.3. Algorithmic implementation of Umbrella RL", "content": "Neural networks (NNs) allow a very efficient computational implementa- tion of Umbrella RL. This may be done with three NNs \u2013 for the desired policy $\\pi(a|s)$, for the expected reward $V(s)$ and for average agent distribution $\\bar{p}(s)$.\nLet these three NNs be parametrized, respectively, by the multi-component parameters \u03b8, \u03c6 and \u03b7. Our goal is to find the extremal values of these pa- rameters $\u03b8^*$, $\\varphi^*$ and $\u03b7^*$. Here $\u03b8^*$ corresponds to the optimal policy, $\\pi^*$, which is a steady solution ($\u03b8_{k+1} = \u03b8_k = \u03b8^*$) of the difference equation (13). $\\varphi^*$ and $\u03b7^*$ correspond to the values, which guarantee that the parametrized functions $V(s)$ and satisfy Eqs. (14) and (15). They are also the steady solutions of the difference equations, (19)-(20) given below, which may be obtained, applying the policy gradient method [see Appendix (Sec. Appendix A.5)]. Hence, all three equations for \u03b8, \u03c6 and \u03b7 may be written in a uniform way, with the learning rates, $\u03b2^V$, $\u03b2^r$ and $\u03b2^\u03c0$,\n$\\Phi_{k+1} = \\Phi_k + \\beta^V \\langle \\nabla_{\\varphi} V^u(s) \\cdot A(s,a) \\rangle_{\\pi}$,\n$N_{k+1} = N_k + \\beta^\\eta \\langle \\nabla_{\\eta} \\log \\bar{p}(s) \\cdot G(\\bar{p}(s), s) \\rangle_{\\pi}$"}, {"title": "4. Application of the Umbrella RL", "content": "To illustrate a practical application of the general ideas of the Umbrella RL, we analyze a few hard RL problems. We have chosen such of them that combine all the difficulties of the hard problems and demonstrate that Umbrella RL can excellently solve them."}, {"title": "4.1. Multi-Valleys Mountain Car problem", "content": "In the Multi-Valley version of the Mountain Car problem, a central (high- est) hill separates a number of hills and valleys to the left and to the right from the central hill, see Fig. 1c. The agent receives the reward only in a narrow range of positions (between two flags) on the top of the central hill. The initial location is possible in either of the two outermost valleys. The multi-valley version is a harder problem than its basic, one-valley, counter- part. The most important complication here is that it is not enough just to\n$y(x) = \\frac{1}{10} [\\cos (2 \\pi x)+2 \\cos (4 \\pi x) -\\log (1 - x^2)]$,\nwhich depends on the car position x.\nThe interaction with the environment occurs through the actions of the agent, applying one of the two discrete actions, $a \\in {0,1}$ - the acceleration to the left, a = 0, or to the right, a = 1. The equation of motion reads,\n$\\ddot{x} = (2 \\cdot a - 1) \\cdot f \u2013 y'(x) \\cdot g$.\nHere f = 0.001 determines the agent's force, g = 0.0025 is the gravita- tional acceleration and $y'(x) \\cdot g$ is its component driving the car downhill. The agent is rewarded, when it resides between the flags, $x \\in [-0.05, 0.05]$, with the reward r = 1; otherwise the reward is zero, r = 0.\nEvolution of the state function $s = {x, \\dot{x}}$ obeys the equation,\n$\\dot{s}=(x, \\dot{x})^T = v(s,a) = \\Big(x, (2a - 1) f \u2013 y'(x)g \\Big)^T$.\nEqs. (22) and (24) should be supplemented by the initial condition. We use the following one: $p_0(x, \\dot{x}) = q_0$ if $x \\in [-0.77, -0.67] \\cap [0.67,0.77]$ and $\\dot{x} \\in [-0.01,0.01]$, and $p_0(x, \\dot{x}) = 0$ otherwise. Here $q_0 = 250$, follows from the normalization, $\\int p_0(x, \\dot{x})dxd\\dot{x} = 1$.\nEqs. (22) and (24) with the initial conditions completely define the prob- lem, so that the above Umbrella RL algorithm, Eqs. (19) \u2013 (21) may be applied. Noteworthy, we compute $G_i$ in Eqs. (16), (17), using the special relation $\\nabla_s \\cdot p\\bar{v} = p\\mathbb{E}_{a \\sim \\pi} [v \\cdot \\nabla_s \\ln (\\pi \\cdot p)]$, which may be straightforwardly derived (see Appedix, Sec. Appendix A.6 for derivation detail)."}, {"title": "4.2. StandUp problem", "content": "The problem is designed to teach a roboarm lying on the plane to lift up and balance in the vertical position, see Fig. 1d. The arm consists of two bars\n$\\dot{\\phi_1} = m_1 \u2013 m_2 - g \\cos{\\phi_1}$\n$\\dot{\\phi_2} = m_2 - g \\cos(\\phi_1 + \\phi_2)$.\nHere g = 0.025 is the gravity acceleration and the agent action a is the two torques, $a = {m_1, m_2}$. Eqs. (25)-(26) are applicable for $\\phi_1 \\in (0, \\pi)$ and $\\phi_2 \\in (-\\pi,\\pi)$, with the constraints of impenetrable plane, $\\phi_2 \\in (-2\\pi,2\\pi \u2013 2\\phi_1)$.\nNumerically, we apply angle clipping to satisfy the constraints. Note,\nthat we compute $G_i$ in Eqs. (16), (17), using the relation\n$\\nabla_s \\cdot p\\bar{v} = p\\mathbb{E}_{a \\sim \\pi} [\\nabla_s \\cdot v + v \\cdot \\nabla_{\\epsilon} \\ln (\\pi \\cdot p)]$, see Appendix (Sec. Appendix A.6) for derivation detail."}, {"title": "4.3. Numerical experiments. Implementation detail", "content": "We performed numerical experiments using the above Umbrella RL algo- rithm with the feed-forward neural networks. We utilized three NNs to obtain the average density distribution $p(s)$, value-function $V_u(s)$ and action-policy $\\pi(a|s)$. Each NN had three layers, comprised of 128 neurons. We used ELU and TanH activation functions combined with NN layers, according to Table 1; the Adam algorithm was applied. The number of iterations was 1.2. 106, the batch size - 104, the entropy coefficient $\\alpha = 10^{-2}$ and the discount factor $\\gamma = 0.95$. A uniform sampling was chosen.\nUmbrella reinforce algorithms utilized different learning hyperparameters for Multi-Valley MC and StandUp problems. We specified for Multi-Valley MC learning rate of $10^{-5}$ for all three NN of the Adam algorithm. The weight decay parameters were $10^{-4}$ for the values function NN, $5 \\cdot 10^{-4}$ for distribution NN and $5 \\cdot 10^{-6}$ for policy NN. Similarly, for StandUp problem, the learning rates were $10^{-6}$ for value function NN, $10^{-7}$ for the distribution NN and $10^{-6}$ for policy NN. The respective weight decay parameters were $10^{-5}, 5 \\cdot 10^{-4}$ and $5 \\cdot 10^{-5}$.\nIn Multi-Valley Mountain Car computations we limited the computation domain of states, such that $x\\in [-0,99,099]$ and $\\dot x = [-0.07,0.07]$ and ap- ply reflective boundary conditions, see Appendix (Sec. Appendix A.7). For StandUp problem, the boundary conditions follow from the physical con- straints, see Appendix (Sec. Appendix A.7.3).\nTo assess the efficiency of our approach, in comparison with other meth- ods, we perform experiments using state-of-the-art RL algorithms. Moreover,"}, {"title": "5. Results for the hard problems", "content": "among them we have chosen the algorithms, expected to be the most efficient for hard problems. Namely, we apply the widely used RL algorithm of prox- imal policy optimization (PPO); it was also claimed to be the most effective and universal [4]. We make experiments with value iteration (VI) algorithm \u2013 the fundamental algorithm of RL. We also use PPO algorithm with the prominent exploration bonus, RND, and iLQR \u2013 the popular model-based RL algorithm. Note, that as it was shown in [12], RL with RND noticeably outperforms RL with curiosity-driven exploration bonus [13].\nAdditionally, we checked the efficiency of the incomplete Umbrella RL (called UR-NE), with $J^{URL}$ substituted by J, without the entropy term.\nThe PPO algorithm was applied from Stable Baselines38 package. We used default parameters and trained the network for the same number of iterations, as for UR, equal to 1.2\u00b7106; we made the environment reset every 2.103 steps. The Value Iteration algorithm was implemented from scratch. It was applied on a discrete equidistant grid of 9000x9000 size. The training was performed until the convergence. The value function was initialized by zeros. The RND algorithm was applied for the problems where the observation was a sequence of images. It was used with default parameters [12] for the 107 iteration in total. Due to the algorithm restrictions, with respect to discontinuities, we apply the iLQR algorithm10, using smoothing (with a Sigmoid) of the involved functions. A trajectory of 10 000 steps has been optimized, until iLQR convergence. We evaluated the agent policy performance in several individual simulation with fixed time-discretization step. The results were averaged and the variance estimated."}, {"title": "5.1. Umbrella RL for Multi-Value Mountain Car problem", "content": "In Fig. 2 the expected return as the function of training time is presented. It is clearly seen from the figure that the new Umbrella RL algorithm sig- nificantly outperforms all the alternatives: While PPO, RND, iLQR, and UR-NE completely fail (yielding almost zero cumulative reward), our algo- rithm exceeds VI in terms of discounted cumulative reward. It is also obvious\nthat the entropy term in the expected return is crucially important \u2013 the al- gorithm loses the efficiency if this term is lacking. One can also conclude that the traditional, trajectory-based algorithms, such as PPO, fail for this hard problem. In contrast, all-states approaches, as VI, or ensemble-based approach, as the new Umbrella RL, demonstrate the ability to solve the hard problem. Still, the VI approach strongly relies on time and state discretiza- tion. For a complex environment, the amount of memory, needed to keep an acceptable accuracy drastically increases. This results in a substantial decrease in performance. As it may be seen from Fig. 2, the efficiency of the VI remains comparable (although smaller) with the Umbrella RL for the time step, dt = 0.05, it noticeably drops for dt = 0.03 and becomes unacceptable for t = 0.01 and smaller time steps. Here the time step of dt = 0.05 suffices for the studied environment with a rather small gravity, g = 0.0025; the increasing g would require the decreasing time step, making the VI inappropriate.\nThe main advantage of the Umbrella RL, with respect to VI, is indepen-"}, {"title": "5.2. Umbrella RL for StandUp problem", "content": "The the StandUp problem is a prominent example of the system, where the boundary conditions play an important role, which complicates the so- lution. Below we illustrate the application of the new algorithm to this hard problem. The optimal strategy, found by the VI algorithm, reveals that it always resides on the boundary. The UR algorithm is also capable to deal with such (hard) boundary conditions, see Fig. 4.\nThe figure also clearly demonstrates the convergence and good perfor- mance of the UR. As it has been already demonstrated for the Multi-Value MC problem, the VI algorithm is very sensitive to the discretization time step dt - it rapidly loses its efficiency with the decreasing dt. While relatively large dt are admissible for weak gravity, where VI is efficient, stronger gravity re- quires smaller dt, thus, reducing the VI power. This is illustrated in Fig. 4: for large dt = 0.1 (admissible for very small gravity) VI outperforms UR,"}, {"title": "6. Discussion and conclusion", "content": "We propose a novel algorithm for Reinforcement Learning (RL) to solve hard problems. These problems, in contrast to regular ones, are associated with the following complications: (i) a significantly delayed reward, given at final states only, (ii) the existence of state traps, that are difficult to avoid and (iii) the lack of a certain terminal state.\nWe demonstrate that the new algorithm being universal in implemen- tation significantly outperforms, the state-of-the-art RL methods, most of which fail in solving hard problems. The computational superiority of the new approach will further increase with the increasing complexity of the sys- tem. The main idea of the method is to apply a continuous (i.e., effectively infinite) ensemble of acting agents, instead of a limited number, as in con- ventional RL. As the result, the modified expected return, quantifying the efficacy of the action policy, is estimated for the complete ensemble of states\nand actions. It comprises the conventional expected return of RL, as well as the term, specifying the ensemble entropy. To find the optimal policy we apply the appropriate modification of the policy gradient method. The policy optimization in the modified approach is entangled with the optimiza- tion of the distribution function specifying the agents' states. In practical implementation of the method, neural networks are exploited, which makes the algorithm very efficient.\nWe demonstrate the application of Umbrella RL to the Mountain Car problem in its multi-valley version and to the StandUp problem. Here we address the most simple models to illustrate the main ideas of Umbrella RL for hard problems, with minimum details. Certainly, the new method may be applied for more practical cases, e.g. for \"Robotic arm\" problem - our StandUp problem is the according \"minimalistic\u201d model. The Multi-Valley problem, the more complicated version of the original (single-valley), may be considered as a 'minimalistic' model of a self-driving car; it possesses all the three difficulties of the hard problems."}, {"title": "Appendix A.", "content": "Appendix A.1. Derivation of the evolution equation for $p(s, t)$\nThe derivation of the equation for $p(s,t)$ is straightforward, as it is es- sentially a continuity equation. Let $p(s,t)$ be the state distribution for an ensemble of agents at time t. Then $p(s,t)ds$, gives the probability that an agent resides at time t in a close vicinity of the (multi-component) point s of the state space, where $ds$ is the respective volume of an infinitesimal element, centered at the point s. Consider a closed domain \u03a9 in the state space with the boundary \u0393, and with the vector e(s) being a unit external normal to \u0393 at the point s. The probability current through the surface element $dS_s$, located at the points of the boundary \u0413, may be written as $p(s, t)(v \\cdot e)dS_s$, where $v = v(a, s) = ds/dt$ is the vector of local probability current. It depends on both - the state s and the action a, determined by the policy $\\pi(a|s)$. The total local probability current equals the sum of all currents corresponding to different actions a, that is, to $p(s, t) \\langle (v \\cdot e) \\rangle_{\\pi} dS_s$, where $\\langle ... \\rangle_{\\pi}$ denotes the averaging over the policy \u03c0. Hence, the rate of change of the total probability to reside within the domain \u03a9 reads,\n$\\frac{d}{dt} \\int_{\\Omega} p(s, t) ds = - \\int_{\\Gamma} p(s, t) \\langle (v \\cdot e) \\rangle_{\\pi} dS_s = - \\int_{\\Omega} \\nabla_s \\langle p(s, t) (v(a, s)) \\rangle_{\\pi} ds$,\nwhere Gauss theorem has been applied. Since the domain \u03a9 is arbitrary, we arrive at the continuity equation,\n$\\frac{dp}{dt} + \\nabla_s \\langle p (v(a, s)) \\rangle_{\\pi} = 0$,\nwhich is Eq. (4) of the main text."}, {"title": "Appendix A.2. Differential policy gradient", "content": "To remind the main notations and basic ideas of the policy gradient derivation we start from the case of discrete time and skip the dependence on the entropy", "53": "step by step", "\u03b8": "n$\\nabla_{\\theta"}], "action-value function\". Then Eq. (A.3) takes the form,\n$\\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{S_t ~ P_{a,t} \\atop a_t ~ \\pi} \\sum_{t=0}^{\\infty} \\gamma^t \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t|s_t) Q(s_t, a_t)$\nAveraging Q(s,a) w.r.t. the actions with the policy \u03c0, we obtain another function,\n$V(s) = \\mathbb{E}_{S_t ~ P_{a,t} \\atop S_0=s} \\Big[ \\sum_{i=0}^{\\infty} \\gamma^{i}r(s_i, a_i) \\Big]$,\ntermed in RL as \"value function\u201d; note that it depends only on state. The action-value function may be then written in the form , .\n$Q(s, a) = r(s, a) + \\gamma V (s + v(s, a))$.\nUsing the fact that for any arbitrary state function F(st),\n$\\mathbb{E}_{S_t ~ P_{a,t} \\atop a_t ~ \\pi} \\Big[ \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t|s_t)F(s_t) \\Big] = \\mathbb{E}_{S_t ~ P_{a,t}} \\nabla_{\\theta} \\Big[ \\int \\pi_{\\theta}(a_t|s_t)da_t \\Big] = 0$,\nsince $\\int \\pi_{\\theta}(a_t|s_t)da_t = 1$, we can subtract in Eq. (A.5) V(st) from Q(St, at). That is, we can write": "n$\\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{S_t ~ P_{a,t} \\atop a_t ~ \\pi} \\Big[ \\sum_{t=0}^{\\infty} \\gamma^t \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t|s_t) A"}