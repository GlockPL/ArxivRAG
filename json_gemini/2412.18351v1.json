{"title": "Multi-Agents Based on Large Language Models for\nKnowledge-based Visual Question Answering", "authors": ["Zhongjian Hu", "Peng Yang", "Bing Li", "Zhenqi Wang"], "abstract": "Large Language Models (LLMs) have achieved impressive results in knowledge-based Visual Question\nAnswering (VQA). However existing methods still have challenges: the inability to use external tools\nautonomously, and the inability to work in teams. Humans tend to know whether they need to use\nexternal tools when they encounter a new question, e.g., they tend to be able to give a direct answer\nto a familiar question, whereas they tend to use tools such as search engines when they encounter\nan unfamiliar question. In addition, humans also tend to collaborate and discuss with others to get\nbetter answers. Inspired by this, we propose the multi-agent voting framework. We design three LLM-\nbased agents that simulate different levels of staff in a team, and assign the available tools according\nto the levels. Each agent provides the corresponding answer, and finally all the answers provided by\nthe agents are voted to get the final answer. Experiments on OK-VQA and A-OKVQA show that our\napproach outperforms other baselines by 2.2 and 1.0, respectively.", "sections": [{"title": "1 Introduction", "content": "Knowledge-based Visual Question Answering\n(VQA) places higher demands on the VQA\ntask, requiring not only understanding of the\nimage information but also external knowledge.\nRecently, Large Language Models (LLMs) have\nbeen used for knowledge-based VQA with encour-\naging results due to their robust capabilities.\nPICa [1] introduces in-context learning for\nknowledge-based VQA. A captioning model is\nemployed to transform the image into the caption,\nand the VQA triplet Image-Question-Answer into\nthe Context-Question-Answer triplet, thus unify-\ning the input into text and making it comprehensi-\nble to GPT-3. Context refers to the caption. How-\never PICa still has challenges: the caption may not\ncover all the information in the image, and the\naccuracy of the prediction is difficult to guarantee\nin case of insufficient input information. Prophet\n[2] inspires the LLM with a vanilla VQA model. It\nuses the vanilla VQA model to generate candidate\nanswers and adds them into the prompt, expand-\ning PICa triplet Context-Question-Answer into a\nquadruple Context-Question-Candidates-Answer.\nThis further enriches the input information and"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Visual Question Answering", "content": "VQA tasks are multimodal tasks that have\nattracted widespread attention. VQA involves the\ninterdisciplinary study of computer vision [3, 4]\nand natural language processing [5]. Recent VQA\nstudies can be broadly classified into several cate-\ngories: good visual features [6, 7], advanced model\nstructure [8, 9], and efficient learning paradigms\n[10-12]. Most of the state-of-the-art approaches\nuse the Transformer structure [13]."}, {"title": "2.2 Knowledge-based VQA", "content": "Early knowledge-based VQA benchmarks also\nprovide KBs. Benchmarks with open-domain\nknowledge are later established [14, 15], mean-\ning that any external knowledge can be used to\nanswer questions. Recently, with the rapid devel-\nopment of LLMs, researchers have adopted LLMs\nfor knowledge-based VQA and achieved impres-\nsive results. PICa [16] employs the GPT-3 for\nknowledge-based VQA. They adopt a captioning\nmodel to convert images into captions, thus unify-\ning the VQA task into textual form. VQA triplet\nare converted by PICa to Context-Question-\nAnswer. Prophet [17] adopts a vanilla VQA model\nto inspire GPT-3, expanding PICa triplet to a\nquadruple Context-Question-Candidates-Answer."}, {"title": "2.3 LLM-based Agents", "content": "Wang et al [18] provide an overview of LLM-\nbased agents, proposing a unified framework that\nsummarises most existing works. Li et al [19] pro-\npose a communicative agent framework that aims\nto guide chatting agents through initial prompts.\nQian et al [20] present a virtual software devel-\nopment company CHATDEV, which incorporates\nagents from different social identities. Crispino et"}, {"title": "3 Preliminaries", "content": "In-context learning is a new paradigm for LLMs.\nGiven the input x, its goal y is inferred conditioned\non the prompt p(h, e, x), at each decoding step s:\n$y^{s} = arg max_{y^{s}} P_{LLm} (y^{s} | p(h, \\epsilon, x), y^{<s})$ (1)\nwhere h denotes the prompt head, and\n\u20ac = {(x1, y1), ..., (xn, Yn)} denotes the in-context\nexamples. PICa is one of the first studies to\nuse in-context learning for knowledge-based\nVQA. PICa converts the image into caption,\nthus making the VQA task into text form so\nthat it can be understood by LLMs. The VQA\ntriplet Image-Question-Answer is turned into\nContext-Question-Answer and Context denotes\nthe caption. The PICa prompt is formatted as\nfollows:"}, {"title": "4 Methodology", "content": "Figure 2 illustrates the proposed multi-agents\nframework, which includes multiple agents. The\nmulti-agents framework includes three different"}, {"title": "4.1 Agent", "content": "Before we introduce the multi-agents framework,\nwe will show a single agent. Figure 3 shows an\noverview of the single agent. The agent consists of\ntwo main contributions: the planner and the tools.\nFor a task input, we first select the in-\ncontext examples for it and construct the\np[(h, \u2208(Ci, qi, ai), (c, q)], where h denotes\nthe prompt head, Ci, qi, a\u017c denotes the in-context\nexamples, and c, q denotes the test input. We fol-\nlow the Prophet [2] for the selection of in-context\nexamples. We input the initial prompt into the\nplanner.\n\u03a6 = Planner(p[(h, \u2208(Ci, qi, ai), (c, q)]) (2)\nwhere represents the plan. The plan is\nany combination of three executable actions:\n\u04101, \u04102, \u0410\u0437 \u0456.\u0435. Action_1, Action_2, Action_3. The\nthree actions are executed using the corresponding\nthree tools, i.e., Action_1 is executed using Tool_1,\nAction_2 is executed using Tool_2, and Action_3 is\nexecuted using Tool_3. Next the action plan will\nbe executed.\npf = Execute(\u03a6\u2208 {A1, A2, A3})\n(3)\nwhere pf represents the prompt obtained after\ncompleting the actions. Next pf is fed into the\nLLM to predict the result.\nP = LLMinferring (pf [h, e, x])\nwhere P represents the prediction."}, {"title": "4.1.1 Tools", "content": "Previous studies have demonstrated the effective-\nness of candidate answers. In addition, we have\nadded two knowledge tools: one for retrieving\nknowledge from the KBs and the other for gen-\nerating knowledge through LLM. The reason for\ndesigning two knowledge tools is that the knowl-\nedge of KBs is limited and it may not be possible\nto retrieve all the required knowledge, and gen-\nerating knowledge through LLM can play a com-\nplementary role. That is, knowledge that cannot\nbe retrieved from the KBs can be complemented\nby generating via LLM. We demonstrate the three\ntools. The Tool_1 is to use the vanilla VQA model\nto generate candidate answers, the Tool_2 is to\nretrieve knowledge from the KBs and the Tool_3 is\nto generate knowledge using LLM. Figure 4 shows\nan overview of the tools.\nTool_1 (Candidate generator). The candi-\ndates are the top-M answers selected from the\nanswers generated by the vanilla VQA model.\nDefine Z = {z} = 1 as the answer vocabulary and\ny as the prediction vector. We take the top-M\nanswers as candidate answers.\n$I_{c} = arg Top-M y_{j}$\n$j\\in{1,2,...,L}$ (5)\nwhere Ic denotes the index set of candidate\nanswers. Candidate answers C can be defined as:\nC = {(zi, Yi) | i \u2208 Ic}\n(6)\nwhere zi and yi denote the candidate answer and\nits confidence score, respectively. After obtain-\ning candidate answers, we add the candidates\nto the prompt, thus expanding the structure\nof the prompt from Context-Question-Answer to\nContext-Question-Candidates-Answer.\nTool_2 (KBs retriever). Retrieval from KBs\nis a common knowledge enhancement method. We\nretrieve relevant knowledge KB from the KBs. We\nrefer to the knowledge retrieved from the KBs as\nKBs_knowledge for clarification.\nWe add the KBs knowledge to the prompt,\nthus expanding the structure of the prompt from\nContext-Question-Answer to Context-Question-\nKBs_knowledge-Answer.\nTool_3 (LLM generator).\nIn addition to retrieving knowledge from the\nKBs, we propose a novel approach as a com-\nplementary approach, i.e., the use of LLM to\ngenerate knowledge. For clarification, we refer to\nthe LLM for knowledge generation as KLLM, and\nwe denote the knowledge generated using LLM as\nLLM_Knowledge.\nWe generate knowledge by prompting KLLM.\nDefine the input as x and the target as y.\ny = KLLM(p{h, \u0454, x})\n(7)"}, {"title": "4.1.2 Planner", "content": "The planner makes the work of the framework\nmore flexible, and can decide whether to invoke\nthe corresponding tools as needed. The motivation\nfor designing the planner is: when we humans face\na problem, we will know if we need to resort to\nadditional tools such as search engines. If we are\nfamiliar with the problem, we can give the answer\ndirectly; if we are not, we need to use tools such\nas search engines. The planner can generate plans\nbased on different task inputs. The plan can be\nany combination of the three actions: Action_1 is\nto get candidate answers by VQA model; Action_2\nis to get knowledge from KBs; Action_3 is to get\nknowledge from LLM.\nThe planner is based on the LLM implementa-\ntion. Figure 6 shows the overview of the planner.\nThe agent contains three roles: Manager, Senior\nand Junior. The scope of the plan depends on the\nrole of the agent. Over here, we take the example"}, {"title": "4.2 Multi-agents", "content": "Our framework is a multi-agents voting frame-\nwork to determine the final answer (Figure 2).\nWe design it this way because team discussions\ncan often have an advantage over individuals.\nWe design different agents to simulate the real-\nity by designing different levels to differentiate the\nabilities of different agents (Junior, Senior and\nManager).\nIn Figure 6, we have shown prompt of Man-\nager. For Senior as well as Junior it is similar, we\nhave limited the actions that can be performed\nand thus the tools that can be used depending\non each level. For Junior, we limit the actions\nit can perform to Action_1, for Senior, we limit\nthe actions it can perform to Action_1, Action_2,\nand for Manager, we allow it to perform three\nactions. Since each action corresponds to a tool\nthat can be used, the tools that can be used by the\nthree agents are: Junior (Tool-1), Senior (Tool_1,\nTool_2), and Manager (Tool_1, Tool_2, Tool_3).\nDefine Junior, Senior, and Manager to produce\nanswers AJ, As, and Am, respectively. We deter-\nmine the final answer by voting. We are setting a\ndifferent number of votes for each agent based on\nlevel, the higher the level the more votes.\nAF = Voting(AJ[w1], As[w2], \u0410\u043c[\u0437]) (8)\nwhere AF denotes the final answer after voting,\nW1, W2, and w3 correspond to the number of\nvotes for AJ, As, and Am, respectively. We assign\ndifferent numbers of votes to the answers gener-\nated by different agents, e.g., Junior is assigned 2\nvotes, Senior is assigned 3 votes, and Manager is\nassigned 4 votes, and the one with the most votes\nis selected as the final answer. Algorithm 1 shows\nthe workflow of multi-agents."}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Datasets and Baselines", "content": "Both OK-VQA [14] and A-OKVQA [15] are widely\nknown knowledge-based VQA datasets. OK-VQA\ncontains about 9K and 5K for training and test\nsets. Each sample is annotated with 10 answers.\nA-OKVQA contains about 17K training set, 1K\nvalidation set and 7K test set. Since the test set\nof A-OKVQA has limitations on submissions, we\nchoose to evaluate direct answer on the validation\nset to facilitate the experiments. For evaluation\nmetrics, a generated answer is considered 100%\naccurate if at least 3 humans annotated that\ncorrect answer.\nThe baselines include: methods with exter-\nnal knowledge resources, methods with GPT-3,\nmethods with other multimodal models.\n\u2022 Methods with external knowledge resources:\nMUTAN [22], Mucko [23], ConceptBert [24],\nViLBERT [25], KRISP [26], MAVEX [27],\nVisual Retriever-Reader [28], TRiG [29],\nUnifER [30]."}, {"title": "5.2 Implementation", "content": "We follow PICa and adopt the OSCAR+ [41] as\ncaptioning model. We follow Prophet and adopt\nthe MCAN [42] as vanilla VQA model. We fol-\nlow the Prophet for the selection of in-context\nexamples. We adopt Wikipedia [43] as the KBs.\nTaking into account hardware and funds consider-\nations, we choose the LLaMA2 7B [44] as LLMs.\nLLaMA is a free LLM with excellent capability in\nopen LLMs. The 7B version requires only a sin-\ngle Tesla V100 GPU, which is affordable for most\nresearchers. Considering the context length limit,\nthe number of in-context examples is set to 8 by\ndefault and the number of candidate answers is set\nto 5 by default. We replace the GPT-3 of Prophet\nwith LLaMA for a fair comparison. For Prompt-\nCap, we first use the caption conversion method\nof PromptCap and then run it based on Prophet."}, {"title": "5.3 Results", "content": "Tables 1 and 2 show the results. On the OK-VQA\nand A-OKVQA datasets, our method outper-\nforms other baselines by more than 2.2 and 1.0,\nrespectively.\nOn the OK-VQA, LLM-based methods such\nas PICa, Prophet and PromptCap tend to obtain\nbetter results than other non-LLM-based meth-\nods. In recent years, due to the rapid development\nof LLMs, which have shown strong capabilities,\nmethods based on LLMs tend to outperform pre-\nvious baselines. PromptCap achieves the best\nresults of all baselines. This is because we enhance\nPromptCap by changing the in-context learning\nphase of PromptCap to be based on Prophet,\nthus combining the advantages of PromptCap\nand Prophet. Our method also belongs to the\nLLM-based methods and outperforms all baselines\nincluding PromptCap and Prophet. Our approach\nis a novel multi-agents framework that contains\nmultiple agents of different levels, thus extending\nthe capabilities of LLMs. In contrast to previ-\nous LLM-based approaches, our approach employs\nLLM-based multi-agents. Our framework has the\nability to invoke tools autonomously and can gen-\nerate answers through collaborative voting, which\nare not available in previous approaches. Similarly,\nfor the A-OKVQA, our method again achieves the\nbest results."}, {"title": "5.4 Ablation Study", "content": "Table 3 demonstrates the results of the abla-\ntion study. X (Multi-agents) indicates that multi-\nagents are excluded, X (Tools) indicates that tools\nare excluded, and \u00d7 (Planners) indicates that\nplanners are excluded.\nWe note that model performance decreases if\nmulti-agents are removed. This shows that the\nmulti-agents voting mechanism helps to improve\nthe model performance. The multi-agents mecha-\nnism gives the framework the ability to collaborate\nby having different levels of agents vote for the\nfinal answer. The model performance decreases if\nthe tools are removed. This shows that tools help\nto improve the model performance. Agents can\nextend their capabilities by using different tools.\nThe model performance decreases if the planners\nare removed. This shows that the planners help\nto improve model performance. The planner gives\nthe agent the ability to make autonomous deci-\nsions and can plan actions more flexibly. The per-\nformance degradation is most pronounced when\nall components are removed."}, {"title": "5.5 Parameter Sensitivity Study", "content": null}, {"title": "5.5.1 Analysis of performance\nvariations of different agents", "content": "Figure 7 shows the performance variations of\nthe three agents with the number of in-context\nexamples. Overall, the performance improvement\nis positively correlated with the increase in the\nnumber of in-context examples.\nSimilar performance variations are observed\non both datasets. The performance of each agent\nimproves with the increase of in-context examples.\nSpecifically, the performance is lowest when the\nnumber of in-context examples is 0, i.e., the 0-shot\nscenario. When the number of in-context exam-\nples is 1, a significant performance improvement\ncan be observed with in-context examples. After\nthe number of in-context examples exceeds 4, the\nperformance improvement is no longer significant.\nWhen the number of in-context examples is 8, the\nperformance reaches the highest and stabilises."}, {"title": "5.5.2 Effect of different parameter\nsettings on multi-agents\nframework", "content": "Figure 8 shows the effect of different parameter\nsettings on multi-agents framework. We show how\nthe performance of the multi-agents framework\nvaries with the number of in-context examples and\nthe number of candidate answers.\nSimilar variations are observed on both\ndatasets. Overall, performance improves as the\nnumber of in-context examples or the number\nof candidate answers increases. Specifically, per-\nformance is lowest when both the number of\nin-context examples and the number of candidate\nanswers are 0. And when the number of in-context"}, {"title": "5.6 Prompts Constructed by\nDifferent Actions", "content": "To make it easier to understand, we've put\ntogether some prompts. Figure 9 shows the\nprompts. Different actions add different elements\nto the prompt. Action_1 corresponds to Tool_1,\nwhich uses the vanilla VQA model to generate\ncandidate answers and add them to the prompt.\nAction_2 corresponds to Tool_2, which retrieves\nknowledge from the KBs and adds it to the\nprompt. Action_3 corresponds to Tool_3, which\nuses the LLM to generate knowledge and add it to\nthe prompt. Ultimately, the constructed prompts\nare used to prompt the LLM for inference."}, {"title": "6 Conclusion", "content": "We present a novel multi-agents framework for\nknowledge-based VQA. The framework consists of\nthree agents with different roles, each of which has\naccess to a different range of tools. We design plan-\nners and tools for the agents. The tools extend the\ncapabilities of agent, and the planner allows the\nagent to make autonomous decisions about actions\nand invoke the appropriate tools. Our framework\nis fully based on the open LLM and version 7B\nrequires only a single V100 GPU to run. For\nresearchers in this domain, our approach provides\na new inspiration, since to our knowledge, this is\nthe first attempt to use the concept of multi-agents\nfor knowledge-based VQA. For other domains, our\napproach can also provide some inspirations, since\nthe multi-agents concept can be used for many\ntasks."}]}