{"title": "Filter-then-Generate: Large Language Models with Structure-Text Adapter for Knowledge Graph Completion", "authors": ["Ben Liu", "Jihai Zhang", "Fangquan Lin", "Cheng Yang", "Min Peng"], "abstract": "Large Language Models (LLMs) present massive inherent knowledge and superior semantic comprehension capability, which have revolutionized various tasks in natural language processing. Despite their success, a critical gap remains in enabling LLMs to perform knowledge graph completion (KGC). Empirical evidence suggests that LLMs consistently perform worse than conventional KGC approaches, even through sophisticated prompt design or tailored instruction-tuning. Fundamentally, applying LLMs on KGC introduces several critical challenges, including a vast set of entity candidates, hallucination issue of LLMs, and under-exploitation of the graph structure. To address these challenges, we propose a novel instruction-tuning-based method, namely FtG. Specifically, we present a filter-then-generate paradigm and formulate the KGC task into a multiple-choice question format. In this way, we can harness the capability of LLMs while mitigating the issue casused by hallucinations. Moreover, we devise a flexible ego-graph serialization prompt and employ a structure-text adapter to couple structure and text information in a contextualized manner. Experimental results demonstrate that FtG achieves substantial performance gain compared to existing state-of-the-art methods. The instruction dataset and code are available at https://github.com/LB0828/FtG.", "sections": [{"title": "1 Introduction", "content": "Knowledge graphs (KGs) encode and store abundant factual knowledge in the format of triples like (head entity, relation, tail entity), which provide faithful knowledge source for downstream knowledge-intensive tasks (Pan et al., 2023; Luo et al., 2023b). However, due to the evolving nature"}, {"title": "2 Preliminaries", "content": "Knowledge Graph Completion (KGC). Knowledge graph (KG) is commonly composed of a collection of fact triples. Let G = (E, R, T) be a KG instance, where E, R, and T represent the set of entities, relations, and triples, respectively. Each triple (h, r, t) \u2208 E \u00d7 R \u00d7 E describes the fact that a relation r exists between head entity h and tail entity t. Given an incomplete triple (h,r,?) or (?,r,t) as query q, knowledge graph completion aims to predict the missing tail or head entity. In conventional KGC models, they learn specific structural embeddings for KGs, and the missing entity is predicted by finding the highest score $f (h, r, e)$ or $f(e, r, t), de \u2208 E$, where f is the model-specific scoring function.\nInstruction Tuning for KGC. Instruction tuning refers to fine-tuning LLMs to follow human-curated instructions, enabling adaptation of LLMs to specific tasks. When applying LLMs to the KGC task, an instruction tuning sample comprises an instruction prompt and input-output pair. The instruction prompt I (e.g., \"Predict the missing tail entity\") is definition of KGC task for LLMs to comprehend and execute. The input X is the verbalization of the query q described in natural language. The instruction tuning process aims to strictly generate the missing entity in natural language given the instruction and the query input: $Y = LLM(I, X)$, where 0 are the parameters of LLM. The prevalent Negative Log-Likelihood Loss in language modeling is selected as training objective, which can be formed as:\n$L(0) = \\sum_{i=1}^{L} -log P_0(Y_i |I, X, Y_{<i}),$  (1)\nwhere $Y_{<i}$ represents the prefix of missing entity name sequence Y up to position i \u2212 1, $P_0(Y_i |I, X, Y_{<i})$ represents the probability of generating token Yi, and L is the sequence length of Y."}, {"title": "3 Methodology", "content": "In this section, we first provide an in-depth description of our filter-then-generate paradigm in Sec. 3.1, specifically designed for KGC task. Based on such paradigm, to bridge the gap between graph structure and text, we further introduce two novel modules: 1) a flexible ego-graph serialization prompt in Sec. 3.2, which can effectively convey the structural information around the query triple, and 2) a structure-text prefix adapter in Sec. 3.3, to map graph structure features into the text embedding space. Finally, we detail the instruction tuning strategy in Sec. 3.4, focusing on efficient adaptation to KGC task. The overall architecture of our proposed model is illustrated in Figure 2."}, {"title": "3.1 Filter-then-Generate Paradigm", "content": "To address the challenge of large entity candidate set, we propose a novel filter-then-generate paradigm for LLMs on KGC task. As its name implies, we utilize a filter to eliminate unlikely entities and retain only the top-k candidates. LLMs then generate the target entities conditioned on the query and candidates list. Within our paradigm, given a query $q = (h, r, ?)$ or $q = (?, r, t)$, we employ a conventional structure-based KGC method as filter to score each entity e in KG G and re-"}, {"title": "3.2 Ego-graph Serialization Prompt", "content": "In our paradigm, we aim to exploit semantic comprehension and reasoning capability of LLMs for KGC task. Nevertheless, transforming the query triple into a text-based prediction inevitably neglects the structural information of KG, which is an important feature for KGC task.Moreover, understanding graph structures using LLM continues to be a challenge, and although there has been some exploration into designing prompts to convey structural information, a comprehensive solution is still lacking.\nTo incorporate the structural information of KG into LLMs, we design an ego-graph serialization prompt. Instead of accessing to the entire KG, we extract the 1-hop ego-graph (Wang et al., 2019) around the query entity, which characterizes the first order connectivity structure of entity. Considering that not all neighborhoods are useful for query, and some of them even introduce additional noise, we employ structure embeddings of KGs to sample more informative neighbors. Specifically, given a KG G and the query triple $q = (h,r,?)$ under the tail entity prediction setting (same in head entity prediction), we first sample the both incoming and outgoing triples of h as the 1-hop ego-graph $N_h = \\{(h,r', e) \u2208 G\\} \u222a \\{(e,r',h) \u2208 G\\}$. Then let $E \u2208 R^{|E|\u00d7ds}$ and $R \u2208 R^{|R|\u00d7ds}$ denote the structural entity embedding matrix and relation embedding matrix, respectively, and ds is structural embedding dimension. The structure embedding matrices are provided by the KGC model adopted as the filter of FtG. We take in query $q = (h,r,?)$ and the 1-hop ego-graph Nh to extract the most relevant neighbors $\\hat{N_h}$ as follows:\n$\\hat{N_h} = \\{(h', r', t') | (h',r', t') \u2208 N_h and cos(h' || r', h || r) > \\u20ac\\}$  (2)\nwhere $h \u2208 R^{1\u00d7ds}$ and $r \u2208 R^{1\u00d7ds}$ represent the structural embedding of h and r, $cos(\u00b7,\u00b7)$ is the cosine similarity, \u0454 is the threshold, and || is the concatenation operation.\nAfter obtaining the extracted ego-graph $\\hat{N_h}$, we follow existing work (Jiang et al., 2023a) perform breadth-first search (BFS) serialization to linearize it into a textual sentence. Specifically, starting from the entity h, we perform BFS to visit all entities in $\\hat{N_h}$. We then concatenate all the visited triples in the order of their traversal during the BFS process and remove duplicate entities, resulting in a long sequence, denotes as:\n$SN_{\\hat{N_h}} = \\{h, r_1, e_1, r_2, e_2,\u2026\u2026, r_m, e_m\\},$  (3)\nwhere m is the number of triples in $\\hat{N_h}$."}, {"title": "3.3 Structure-Text Adapter", "content": "Graph Encoding and Adaption. While our ego-graph serialization prompt has captured the local structure information around the query, the linearization process inevitably loses the connective pattern of the graph. Therefore, we propose a soft prompt strategy to couple the KG structure and text information in a contextualized way. Given the pruned ego-graph $\\hat{N_h}$, we obtain the ego-graph representation through parameter-free message passing on encoded structure features, and map the graph representation into the embedding space of LLM via a trainable projection matrix $W_p \u2208 R^{ds\u00d7dx}$:\n$S'_{\\hat{N_h}} = \\frac{1}{|\\hat{N_h}|} \\sum_{e'\u2208 \\hat{N_h}} e', S_{\\hat{N_h}} = W_p S'_{\\hat{N_h}}$  (4)\nwhere $S'_{\\hat{N_h}}$ is the projected ego-graph representation, $e' \u2208 R^{1\u00d7ds}$ is corresponding entity structural embedding, and dr denotes the dimension of embedding space of LLMs. We do not explore more complex adaptation schemes (e.g., cross-attention) because they require extra graph-text pairs for pre-training. Moreover, such straightforward linear projection allows us to iterate data-centric experiments quickly, which has been proven effective in visual-text alignment (Liu et al., 2023).\nTarget Entity Generation. Given a query $q = (h, r, ?)$ and ego-graph serialization sequence $SN_{\\hat{N_h}}$, we formulate them to corresponding textual version and obtain the input of the LLM, denoted as $X = X_q + X_S$. Let X \u2208 R|X|\u00d7da denote the textual content embeddings of input, where |X| is the token length of X. We concatenate the soft graph token and input embeddings as final input of LLMs, i.e., $X' = S'_{\\hat{N_h}} || X$. In this way, structure information can interact frequently with the textual information, enabling LLMs to leverage the underlying graph structure. Finally, our optimization objective is to maximize the probability of generating the target entity name Yt by maximizing:\n$P(Y_t|X', X_I) = \\prod_{i=1}^{L} P_0(Y_i| S'_{\\hat{N_h}} || X, X_I, Y_{t,<i}),$ (5)\nwhere XI denotes the representation of instruction tokens, and L is the token length of target entity.\nConnection to Graph Neural Networks. Our model shares essential mechanism similarities with existing GNNs, thus covering their advantages. First, due to the input length limitation of LLMs, our ego-graph serialization prompt for the query entity is aligned with GraphSAGE (Hamilton et al., 2017). And our similarity-based extraction module resembles graph regularization techniques like DropEdge (Rong et al., 2020). Additionally, our structure-text adapter carries structure features that can interact with text semantic features deeply in the encoding phase. Causal attention in LLMs can be regarded as an advanced weighted average aggregation mechanism of GAT (Velickovic et al., 2018), facilitating our model to effectively model the varying importance of different neighbors to the central entity. Therefore, our framework integrates inductive bias required for graph tasks and enhances the graph structure understanding capability of LLMs."}, {"title": "3.4 KGC-Specific Instruction Tuning Strategy", "content": "The instruction tuning process aims to customize the reasoning behavior of LLM to meet the specific constraints and requirements of KGC task. An example of our instruction data can be seen in Appendix D.2. During the training process, we always keep the parameters of KGC filter frozen, and update both the weights of the projection layer and LLM. Considering the computation overhead of full-parameters updates for LLM, we employ low-rank adaptation (i.e., LoRA (Hu et al., 2022)) due to its simple implementation and promising performances (Liu et al., 2022). This approach freezes the pre-trained model parameters and updates the parameters of additional trainable weight matrix $W \u2208 R^{d_1\u00d7d_2}$ by decomposing it into a product of two low-rank matrices: $W = BA$, where $B \u2208 R^{d_1\u00d7r}, A \u2208 R^{d_2\u00d7r}$, and $r < min(d_1, d_2)$. Hence, LoRA can effectively adapt the LLM to KGC task while requiring little memory overhead for storing gradients."}, {"title": "4 Experiment", "content": "We employ three widely-used KG datasets for our evaluation: FB15k-237 (Toutanova and Chen, 2015), CoDEx-M (Safavi and Koutra, 2020), and NELL-995 (Xiong et al., 2017). Detailed dataset statistics are shown in Appendix A. And the baselines adopted in our experiments are shown in Appendix B. In our implementation, we use LLaMA2-7B (Touvron et al., 2023) as the LLM backbone. We employ RotatE (Sun et al., 2019) as our filter for its simplicity and lightweight nature. The effect of different filters is discussed in Sec. 4.4. We report Mean Reciprocal Rank (MRR) and Hits@N (N=1,3,10) metric following the previous methods. Specific implementation details please refer to Appendix C."}, {"title": "4.2 Main Results", "content": "Table 1 displays the results of our experiments. Overall, we can observe that FtG achieves consistent and significant improvement on both datasets across most metrics, which demonstrates the effectiveness of our proposed FtG.\nCompared to structure-based baselines, FtG showcases remarkable performance, which demonstrates the capability of FtG to understand and leverage graph structure. Additionally, compared to Rotate, which is employed as the filter in our model, FtG achieves Hits@1 improvements of 33.2%, 60.7%, and 10.11% across three datasets, indicating that filter-then-generate paradigm can effectively incorporate the strength of RotatE and LLMs, enabling FtG to leverage knowledge memorization and reasoning ability of LLM to address indistinguishable entity candidates. Compared to the sparse NELL-995, FtG improves more in the remaining two datasets, suggesting that FtG can fully utilize the structural information of the KG.\nFor the PLM-based baselines, FtG outperforms the SOTA method CSProm-KG by a substantial margin, indicating the superiority of our method. Focusing on the LLMs-based methods, we can find that even with instruction fine-tuning on KGs, LLMs still yield inferior performance. The reason is that directly eliciting LLMs to generate answers is prone to be influenced by hallucination of LLMs, leading to uncontrollable responses. In comparison, FtG achieves substantial improvements across three datasets, indicating that FtG can effectively harness and unleash the capability of LLMs."}, {"title": "4.3 Ablation Study", "content": "In this subsection, we conduct an ablation study to investigate the individual contributions of different components in FtG. The results and meanings of various variants are reported in Table 2. The results reveal that all modules are essential because their absence has a detrimental effect on performance."}, {"title": "4.4 Discussion", "content": "In this section, we conduct a multifaceted performance analysis of FtG by answering the following questions. More analysis please refer to Appendix D.\nQ1: Why can FtG make impressive gains? Our motivation for proposing FtG is that we argue that LLMs are not good KGC reasoner, but strong discriminator of hard samples. We select several test samples where RotatE prediction failed in FB15k-237, and visualize the top-20 highest scoring entities embeddings of each query in Figure 3 (a). Dots of the same color represent entities that are candidates under the same query, and \u25b3 is the target entity. Obviously, we can find that these candidate entities are indistinguishable for RotatE, and they overlap in the embedding space. As shown in Table 7, these hard samples require additional contextual knowledge to be distinguished. In contrast, we visualize the candidate entity's hidden states in the last transformer-layer of FtG in Figure 3 (b). The figure demonstrates that our FtG can well distinguish the target entity from the hard samples, which is attributed to the inherent contextual knowledge of LLMs. Besides, we observe similar visualization results on the CoDEX-M and NELL-995 datasets.\nQ2: Is FtG compatible with existing KGC methods? Here, we further evaluate whether our proposed FtG is robust enough when equipped with various KGC methods as a filter. Taking on a more challenging setting, we do not resort to retrain with different KGC methods. Instead, we load the trained LoRA weights directly and then switch different KGC filters for evaluation. Specifically, we employ a range of prevalent KGC methods, including structure-based TransE, ComplEx, RotatE, as well as PLM-based method CSProm-KG. Our results, as shown in Figure 4, demonstrate that existing KGC methods achieve significant improvements when integrated with FtG, across both FB15k-237 and NELL-995 datasets. This suggests that FtG can effectively incorporate the strength of conventional KGC methods and LLMs, enabling FtG to leverage the reasoning ability of LLMs to address indistinguishable candidates. Furthermore, this also underscores our method's capacity to enhance existing KGC methods in a plug-and-play manner, demonstrating the versatility and effectiveness of FtG.\nQ3: Is structure-aware pruning necessary in ego-graph serialization prompt? We replace the ego-graph serialization prompt in FtG with three other common heuristics, including random walk of the query entity, the entire one-hop ego-graph, and two-hop ego-graph. Empirical results are shown in Table 3, their final prediction are outperformed by FtG. Notably, comparing the results of the entire one-hop ego-graph (Line 3) with FtG, we can see that structure-aware pruning plays a crucial role, especially on the FB15k-237 dataset, which is rich in graph structure information (average degree of"}, {"title": "Q4: How does the number of candidate entities retained in the filtering stage affect the performance?", "content": "Here, we analyze the connection between the size of candidate set and performance. Our results, as presented in Figure 5. We observe that when increasing the size of the candidate set, the variation in model performance is not significant. The impact of this hyperparameter on results is akin to the trade-off between accuracy and recall. A larger candidate set implies a higher likelihood of containing the correct entity but also means that the LLM needs to comprehend more entities. In this paper, We prefer to enable LLMs to focus on hard samples that conventional KGC can not solve them well with limited model capacity and data amount. Therefore, we finally chose to set the size of the candidate entities to 20."}, {"title": "Q5: What are effects of filter-then-generate paradigm?", "content": "We devise several variants to fully analyze the impact of the FtG paradigm LLMs. Specifically, these variants are:\n\u2022 LLaMA2-7B: directly prompt the LLM for KGC, the prompt format can refer to Table 4.\n\u2022 LLaMA2-7B-FtG: we do not fine-tune the LLM and only adopt the FtG prompt.\n\u2022 ChatGPT: we utilize the same prompt (refer to Figure 7) as Li et al. (2024) to evaluate.\n\u2022 KG-LLaMA2-7B: adopt instruction-tuning to adapt LLaMA2-7B for KGC.\n\u2022 FtG*: we only adopt filter-then-generate paradigm to fine-tune the LLM, it can be regarded as a ablation vertion.\nThe results, as illustrated in Figure 6, indicate that the filter-then-generate paradigm serves as an effective strategy for leveraging the capabilities of LLM in KGC. Especially, LLaMA2-7B with multiple-choice question prompt can outperform ChatGPT across some metrics in FB15k-237 dataset."}, {"title": "5 Related Works", "content": "Structure-based KGC methods. Early methods typically define a score function to evaluate the"}, {"title": "6 Conclusions", "content": "In this paper, we propose FtG, a instruction-tuning based method to enhance the performance of LLMs in KGC task. Our proposed filter-then-generate paradigm can effectively harness the capabilities of LLMs. To further incorporate the structural information into LLMs, we devise an ego-graph prompt and introduce a structure-text adapter. Extensive experiments demonstrate the effectiveness of FtG. In the future, we plan to adapt our method to other relevant downstream tasks, such as recommendation and open question answering."}, {"title": "Limitations", "content": "FtG can effectively harness the reasoning ability of LLMs and successfully incorporate the graph structural information into the LLMs, achieving substantial performance improvement on KGC task. However, the extremely large number of parameters in LLMs makes fine-tuning them resource-intensive. At the same time, LLMs are notoriously slow at decoding during inference. In our experiment, we use DeepSpeed (Rajbhandari et al., 2020) to accelerate training and inference, but FtG remain slower than traditional methods due to its inherent scale. Besides, if the KGC filter is not able to recall the target entity within the top-k candidates, FtG cannot make correct prediction. Therefore, a potential way to improve the effectiveness of FtG is to improve the success rate of target entity recall, and our FtG is more of a general framework to adapt LLM to KGC task."}, {"title": "Acknowledgements", "content": "We would like to thank all the anonymous reviewers and area chairs for their comments. This research is supported by National Natural Science Foundation of China (U23A20316), General Program of Natural Science Foundation of China (NSFC) (Grant No.62072346), and founded by Joint&Laboratory on Credit Technology."}, {"title": "A Dataset", "content": "We use FB15k-237 (Toutanova and Chen, 2015), CODEX-M (Safavi and Koutra, 2020), and NELL-995 (Xiong et al., 2017) for evaluation. FB15k-237 is a subset extracted from the Freebase (Bollacker et al., 2008), which includes commonsense knowledge about movies, sports, locations, etc. CoDEx-M is extracted from Wikipedia, which contains tens of thousands of hard negative triples, making it a more challenging KGC benchmark. NELL-995 is taken from the Never Ending Language Learner (NELL) system and covers many domains. Detailed statistics of all these datasets are shown in Table 5."}, {"title": "B Baseline Details", "content": "We compare our FtG against three types of baselines: (1) structure-based methods, including TransE (Bordes et al., 2013), DistMult (Kazemi and Poole, 2018), ComplEx (Trouillon et al., 2016), ConvE (Dettmers et al., 2018), RotatE (Sun et al., 2019), and KG-Mixup (Shomer et al., 2023). (2) PLM-based methods, including GenKGC (Xie et al., 2022), KG-S2S (Chen et al., 2022), and CSProm-KG (Chen et al., 2023). (3) LLM-based methods, including ChatGPT (Zhu et al., 2023), PaLM2-540B (Anil et al., 2023), and KG-LLaMA-7B (Yao et al., 2023). Both ChatGPT and PaLM2-540B employ LLMs as the backbone and focus on prompt engineer, enabling LLMs to understand the KGC task. KG-LLaMA-7B is an instruction fine-tuned LLaMA2-7B based on KG datasets."}, {"title": "B.2 Implementation of Baselines", "content": "Since some baselines miss results on some metrics, we implement these baselines based on their released code. For structure-based baselines, we use the toolkit provided in RotatE\u00b9, which gives state- of-the-art performance of existing KGC models in a unified framework. We adopt the optimal hyperparameter configurations reported in their papers. For KG-S2S, on the CoDEx-M dataset, we utilize the official code provided \u00b2 and set the length of the entity description text to 30, epochs to 50, batch size to 32, and beam width to 40. For experiments on the NELL-995 dataset, the epoch is increased to 100, batch size is determined to be"}, {"title": "C Implementation Details", "content": "In our implementation, the quantity k of candidates retained is selected from {10, 20, 30, 40}. During training, we keep the Rotate frozen and employ LORA to fine-tune the model. The detailed hyperparameters we use during training and inference are shown in Table 6. We employ identical hyperparameters in different datasets. DeepSpeed ZeRO stage3 is enabled for optimization. All models are trained using 2 Nvidia A800 GPUs, each with 80GB of memory. For all datasets, we report Mean Reciprocal Rank (MRR) and Hits@N (N=1,3,10) metric following the previous works."}, {"title": "D Instruction Template", "content": "Zhu et al. (2023) construct few-shot demonstrations to assess the performance of LLM in KGC. Figure 7 shows a example of the input to LLMs, and (Li et al., 2024) utilize the API parameter to obtain multiple candidates, enabling the calculation of Hits@1, Hits@3, and Hits@10 metrics."}, {"title": "D.2 Prompt for FtG", "content": "In our framework, we use a simple template in Table 4 to convert the query triple to text format like Yao et al. (2023). Then we formulate the KGC task into a multiple-choice question fromat. A specific example is shown in Figure 8."}, {"title": "E Case Study", "content": "In Table 8, we demonstrate some cases to illustrate the differences in responses between existing LLMs-based methods and FtG. From the cases, our FtG can effectively leverage the capabilities of LLMs while avoiding the generation of uncontrollable text."}, {"title": "F Comparison with existing LLMs-based methods", "content": "In this subsection, we provide a detailed introduction to existing LLMs-based methods and further discuss the potential application of our FtG. The existing LLM-based methods mainly include:\n\u2022 KoPA (Zhang et al., 2023): proposes an instruction-tuning method based on LLaMA2- 7B for KG triple classification task. Although the authors claim their focus is on KGC, their work is strictly speaking a triple classification task. This means given a true triple, they randomly replace the head entity or tail entity to construct a negative sample, and then let the model perform binary classification, i.e., simply outputting True or False.\n\u2022 KG-LLM (Shu et al., 2024): similarly focuses on the triple classification task. It constructs chain-of-thought prompts via random walks on the KG and then fine-tunes LLMs to"}, {"title": "\u2022 KG-LLaMA (Yao et al., 2023)", "content": "converts KGC into a QA task, leveraging the instruction-following capability of LLMs to adapt them for KGC tasks. While this approach has achieved promising results, it still falls short compared to previous methods based on structural information.\nIn contrast, our FtG can effectively leverage the capabilities of LLMs while avoiding the generation of uncontrollable text. Additionally, we propose an efficient approach to enable LLMs to utilize the structural information of KGs, which has not been achieved by previous methods.\nPotential Downstream Applications of FtG.\n\u2022 The construction of KG is very expensive, especially in specialized domains such as financial and law. Our method can fully leverage LLMs' inherent knowledge and reasoning capabilities, making it suitable for automatically completing existing KGs. Additionally, the AI community has witnessed the emergence of numerous powerful LLMs, which have made huge advancements and led to the pursuit of possible Artificial General Intelligence. Our FtG provides a possible way to integrate knowledge graphs with LLMs, aligning with the current trends in AI domains.\n\u2022 In the recommendation domain, systems need to suggest specific items from a vast pool. Our approach can be effectively applied here: filter-then-generate paradigm can initially filter the large pool of items, honing in on a more relevant subset based on user profiles and preferences. And the ego-graph serialization prompt can capture and model detailed user interaction history. Finally, the encoding the ego-graph into a soft prompt token and map it into LLMs' space with an adapter can provides a meaningful way to apply LLMs for final recommendation."}]}