{"title": "FLAME: Flexible LLM-Assisted Moderation Engine", "authors": ["IVAN BAKULIN", "ILIA KOPANICHUK", "IAROSLAV BESPALOV", "NIKITA RADCHENKO", "VLADIMIR SHAPOSHNIKOV", "DMITRY V. DYLOV", "IVAN OSELEDETS"], "abstract": "The rapid advancement of Large Language Models (LLMs) has introduced significant challenges in moderating user-model interactions. While LLMs demonstrate remarkable capabilities, they remain vulnerable to adversarial attacks, particularly \"jailbreaking\" techniques that bypass content safety measures. Current content moderation systems, which primarily rely on input prompt filtering, have proven insufficient, with techniques like Best-of-N (BoN) jailbreaking achieving success rates of 80% or more against popular LLMs.\nIn this paper, we introduce Flexible LLM-Assisted Moderation Engine (FLAME): a new approach that shifts the focus from input filtering to output moderation. Unlike traditional circuit-breaking methods that analyze user queries, FLAME evaluates model responses, offering several key advantages: (1) computational efficiency in both training and inference, (2) enhanced resistance to BoN jailbreaking attacks, and (3) flexibility in defining and updating safety criteria through customizable topic filtering. Our experiments demonstrate that FLAME significantly outperforms current moderation systems. For example, FLAME reduces attack success rate in GPT-40-mini and DeepSeek-v3 by a factor of ~9, while maintaining low computational overhead. We provide comprehensive evaluation on various LLMs and analyze the engine's efficiency against the state-of-the-art jailbreaking. This work contributes to the development of more robust and adaptable content moderation systems for LLMs.", "sections": [{"title": "1 Introduction", "content": "Content moderation for Large Language Models (LLMs) represents a critical challenge in ensuring safe and appropriate human-Al interactions. While traditional content moderation approaches have focused primarily on input filtering [6], the increasing sophistication of adversarial techniques necessitates a fundamental shift in how we approach this problem. The main purpose of content moderation is to limit the processing of requests that do not align with the system's intended use. This serves several crucial functions: prevents errors in specialized applications like medical consultation, eliminates irrelevant or potentially harmful data that may interfere with model performance, and reduces the risks associated with malicious information or legal violations [4]. Current moderation systems typically rely on input filtering to identify and block potentially harmful queries before they reach the model.\nHowever, recent developments in jailbreaking techniques, particularly the Best-of-N (BoN) approach, have exposed significant vulnerabilities in existing moderation systems. These techniques exploit the probabilistic nature of LLM outputs through multiple sampling attempts, achieving concerning success rates of 80% or higher against popular models [8]. The effectiveness of these attacks highlights a critical gap in current defensive approaches, which predominantly focus on analyzing user inputs rather than model outputs.\nOur work introduces FLAME (Flexible LLM-Assisted Moderation Engine), shifting the focus from input filtering to output moderation through an efficient regulatory policy. Unlike existing solutions that require extensive computational resources or complex neural architectures, FLAME employs a lightweight approach that can be deployed with minimal requirements. This design choice not only makes the system more accessible but also enables rapid adaptation to emerging threats through customizable topic filtering. The flexibility of FLAME's architecture addresses one key limitation in current moderation systems: the ability to quickly adapt to new types of harmful content while maintaining efficient operation. Our approach allows organizations to define and update their moderation criteria based on specific needs and emerging challenges, without requiring significant retraining or computational resources.\nRecent work on constitutional AI and classifier-based approaches [16] has demonstrated the potential of sophisticated moderation systems. However, these solutions often demand substantial computational resources for both training and inference. In contrast, FLAME demonstrates that effective moderation can be achieved through carefully designed rule-based systems enhanced by LLM-generated training data. Through extensive experimentation and real-world deployment, we have validated this approach across multiple leading LLM platforms, consistently achieving a 2-9x improvement in resistance to BoN attacks.\nContributions This work advances the field of LLM content moderation in several ways.\n\u2022 We introduce an output-centered moderation approach that provides superior protection against state-of-the-art jailbreaking techniques while maintaining minimal computational requirements.\n\u2022 We jailbreak 6 popular LLMs with and without our moderation engine, demonstrating an up to 9-fold improvement in their resistance to adversarial attacks.\n\u2022 Our engine challenges the prevailing trend towards resource-intensive censorship, demonstrating that effective moderation can be achieved without extensive model fine-tuning or complex neural architectures.\n\u2022 We report practical insights from deployment of the moderation engine into a dialogue system product, addressing critical considerations from the standpoint of user experience and finding the delicate balance between moderation strictness and system accessibility."}, {"title": "2 Related work", "content": "Classifier guards. Markov et al. [12] proposed an active learning strategy that identifies relevant samples for labeling, balances between uncertainty and diversity, and leverages redundancy to capture rare events. Rebedea et al. [13] developed guardrails control LLM output, preventing harmful topics, following dialogue paths, and maintaining"}, {"title": "3 Method", "content": "The moderation Algorithm 1 checks whether a user request or a model response contain a banned topic. It is a binary classification problem: 0 no banned topics, 1 - contains banned topic.\nThe inference algorithm itself is rule-based and relatively simple. Message (text string) r is split into several multisets of n-grams of normal word forms, by the split function S(r, n). Then, the classifying function \u03c3 is computed as follows:\n$$\\sigma(r) = \\begin{cases}\n1, \\text{ if } \\sum_{n=1}^{k} \\chi(S(r, n), T) > 0 \\\\\n0, \\text{ if } \\sum_{n=1}^{k} \\chi(S(r, n), T) = 0\n\\end{cases}$$\nwhere $\\chi$ is the characteristic function computed by the exact match of items, T is the set of all banned n-grams (i.e., blacklist in Algorithm 1), k = 3 is the maximum n-gram size. We used pymorphy3 [10] to normalize word forms and nltk [2] for the n-gram partitioning. An example of a message split from a medical dialogue is shown below:\nS(My stomach hurts, 2) = [[i stomach, stomach hurt]]\nBefore discussing the details, we want to note that the engine itself is flexible. Topic selection in the example above allows for skipping domain-irrelevant topics while filtering out dangerous or forbidden subjects. One can choose any sets of such topics when adapting the engine to their needs."}, {"title": "3.1 Assembly of set of banned n-grams", "content": "The proposed method for creating a dataset for text classification is similar to the constitutional classifier created in [16]. Unlike the constitutional classifier, based on LLM fine-tuning, FLAME works on classical methods for matching normalized forms of n-grams, so it is much more efficient in both training and inference. Our method requires no"}, {"title": "3.2 Specifics of moderating in real chat room", "content": "After deploying the first version of our solution in a production run, we found that despite the low false positive rates the actual number of reported chat sessions with false positive errors was about 1.8 times higher than FPR. When working with real chat rooms, it should be kept in mind that counting moderation quality metrics on messages does not reflect the real user experience. The user does not count metrics on messages, but evaluates the whole interaction session with the LLM. Even one false positive evaluation of a message spoils the interaction experience for the whole session. In order to estimate the probability of unsuccessful session for a user, we used Bernoulli's formula:\n$$P_t = 1 - (1 - FPR)^t$$\nFPR is a false positive rate of the moderation by messages, $P_t$ is a probability of at least one false positive moderator activation during a session of t messages length. It is not hard to estimate that for a chat of length 5 messages, with our initial FPR of 1%, we get a 4.9% probability of false moderator activation. If, however, we check both the user message and the model response, rather than just the model response, the probability of an undesirable outcome increases to 9.5%. In practice, five and ten times the number of false positive errors is not achieved because inference sampling is heavily biased towards safe use of the dialogue system. However, this means that in real chat rooms, one has to be very"}, {"title": "3.3 Implementation details", "content": "The computational complexity of the characteristic function of two sets depends linearly on the length of the shortest set. The set of forbidden n-grams contains about 105 or more elements, while the number of words in the processed sequence is in a range of 10 - 1000 words. The computational complexity of FLAME on inference depends linearly on the length of the model output. In production it takes 2 to 5 ms to check 1 message (4.3 ms on average at the real chat room) using only 0.1 CPU core and 100 Mb RAM."}, {"title": "4 Results", "content": "Our test collection contains 9178 messages. The collection is balanced in terms of classes: 54% of samples have a positive label. Metrics on the test collection are shown in Table 1.\nStandard error for all metrics calculated by bootstrapping. The metrics shown in Table 1 are good enough to deploy the method in production. However, in order to verify the quality of the proposed solution, we conducted a series of experiments to compare the effectiveness in repelling the latest SOTA jailbreak on popular LLMs \u2013 best-of-n (BoN) jailbreak [8]. Enough time has passed since the original article with the BoN was released that the LLM bot holders have had time to issue some sort of response to it. Therefore, we also provide data on the current state of the moderation quality of APIs of various LLM chat bots. We used the methodology described in [8] with one exception. We translated the dataset presented there into another language, which our solution was originally trained on.\nThe results of the experiments are presented in Figures 1 and 2. Table 2 presents the maximum achieved ASR values for each LLM with and without FLAME. FLAME worked relatively well for DeepSeek and ChatGPT, showing 9 times more effective resistance to attacks than the moderation system built into the their own API. In the case of DeepSeek, the BoN jailbreaking method achieved 100% success, and quite quickly. Indeed, a check of the model output showed that their \"constitution\" of the moderation system differs significantly from the other models. DeepSeek comfortably chats about sensitive political topics as long as they do not involve recent Chinese history. The worst performance was shown for Claude - jailbreak achieved its goal even with the presence of FLAME in almost half the cases which is more than for any other LLM. Taking into account their recent article [16], Anthropic appear to be in the process of redesigning their moderation system.\nThe best absolute performance was shown for GigaChat. We also compared the quality of the built-in GigaChat moderation system with the pure FLAME. Indeed, combining moderation systems gives a slightly higher result in resistance to attacks. However, in absolute terms it is insignificant (see Figure 2). The effect of the accumulation of the probability of false positive errors during the chat session described in the section above makes this idea very risky in"}, {"title": "5 Discussion", "content": "Our experimental results reveal several critical insights about the current state and future directions of LLM content moderation. First, the varying effectiveness of FLAME across different models illuminates important patterns in moder- ation system design. The superior performance with GigaChat demonstrates the value of model-specific training, while the challenges faced with Claude (47.2% attack success rate) highlight how architectural differences in LLMs can impact moderation effectiveness.\nThe difference between the result shown by FLAME for GigaChat and for the other models (see Figure 1) underscores the importance of knowing which model the engine is trained on and which one it infers. It is not overly surprising that FLAME showed the best quality on the same model, whose answers were used to train it.\nThe real-world deployment of FLAME has provided valuable insights into practical implementation challenges. One observation is that the false positive rates in production environments can be 1.8 times higher than in isolated testing, emphasizing the vital role of considering complete user sessions rather than individual interactions. This finding fundamentally changes how we should approach evaluation of moderation systems and their optimization and should be a subject of a benchmarking effort in future work.\nAnalysis of combined moderation approaches yielded unexpected insights. While integrating FLAME with existing systems showed marginal improvements in attack resistance, the multiplicative effect on false positives suggests that simpler, focused approaches may be more efficient in practice. This challenges the common assumption that layering multiple security measures necessarily improves overall system safety.\nThe production deployment also revealed interesting patterns in user interaction and system performance under real-world conditions. The relationship between chat session length and cumulative false positive rates provides a recipe on how moderation systems should be calibrated for different use cases. These insights extend beyond FLAME's specific implementation and are positioned to influence broader design principles of moderation systems."}, {"title": "5.1 Limitations", "content": "FLAME is inexpensive to train and infer, showing acceptable quality on the test sample, and is highly resistant to the SOTA attack method. However, it also has limitations. Firstly, its performance results strongly depend on the difference between the model used during training and the one that will be used in inference. One requires the engine to be trained"}, {"title": "6 Conclusion", "content": "FLAME represents a significant advancement in LLM content moderation, demonstrating that efficient protection against modern jailbreaking techniques can be achieved through lightweight but powerful approaches. Our comprehensive evaluation across multiple leading LLM platforms shows that FLAME consistently reduces attack success rates by a factor of 2-9 compared to existing solutions, while maintaining minimal computational requirements of just 0.1 CPU core and 100 MB RAM per instance. The system's success in real-world deployment validates our approach of shifting focus from input filtering to output moderation. This paradigm shift, combined with our rule-based architecture enhanced by LLM-generated training data, challenges the prevailing trend toward increasingly complex and resource- intensive censorships. The results demonstrate that successful moderation can be achieved without extensive model fine-tuning or complex neural architectures. Our work establishes a new direction for developing practical, scalable content moderation systems, protecting against adversarial attacks and providing computational efficiency and a true flexibility in deployment. As LLMs continue to evolve and integrate into all sorts of applications, approaches like FLAME will be crucial in ensuring safe and appropriate human-AI interactions."}]}