{"title": "Enhancing Cognitive Workload Classification Using Integrated LSTM Layers and CNNs for fNIRS Data Analysis", "authors": ["Mehshan Ahmed Khan", "Houshyar Asadi", "Mohammad Reza Chalak Qazani", "Adetokunbo Arogbonlo", "Siamak Pedrammehr", "Adnan Anwar", "Asim Bhatti", "Saeid Nahavandi", "Chee Peng Lim"], "abstract": "Functional near-infrared spectroscopy (fNIRS) is employed as a non-invasive method to monitor functional brain activation by capturing changes in the concentrations of oxygenated haemoglobin (HbO) and deoxygenated haemoglobin (HbR). Various machine learning classification techniques have been utilized to distinguish cognitive states. However, conventional machine learning methods, although simpler to implement, undergo a complex preprocessing phase before network training and demonstrate reduced accuracy due to inadequate data preprocessing. Additionally, previous research in cognitive load assessment using fNIRS has predominantly focused on differsizeentiating between two levels of mental workload. These studies mainly aim to classify low and high levels of cognitive load or distinguish between easy and difficult tasks. To address these limitations associated with conventional methods, this paper conducts a comprehensive exploration of the impact of Long Short-Term Memory (LSTM) layers on the effectiveness of Convolutional Neural Networks (CNNs) within deep learning models. This is to address the issues related to spatial features overfitting and lack of temporal dependencies in CNN in the previous studies. By integrating LSTM layers, the model can capture temporal dependencies in the fNIRS data, allowing for a more comprehensive understanding of cognitive states. The primary objective is to assess how incorporating LSTM layers enhances the performance of CNNs. The experimental results presented in this paper demonstrate that the integration of LSTM layers with Convolutional layers results in an increase in the accuracy of deep learning models from 97.40% to 97.92%.", "sections": [{"title": "1 Introduction", "content": "Cognitive load, a fundamental concept in cognitive psychology, pertains to the quantity of information that the working memory can retain [9, 31]. As the human brain possesses finite capacities for processing information simultaneously, delving into the intricacies of cognitive load provides valuable insights into the mechanisms that govern optimal learning and cognitive functioning. The n-back [23] task is a typical paradigm designed to administer a specific and measurable cognitive workload to individuals. In this task, participants are tasked with a dynamic challenge, requiring them to respond, typically by pressing a button, whenever the current stimulus matches with the stimulus presented n times earlier in the sequence. This cognitive challenge not only demands participants' sustained attention but also necessitates the effective engagement of their working memory.\nIn the field of neurophysiological research, functional Near-Infrared Spectroscopy (fNIRS) has emerged as a non-invasive and adaptable neuroimaging technique, contributing significantly to the exploration of cognitive workload [11, 16]. By capturing real-time changes in cerebral blood flow and oxygenation levels, fNIRS provides researchers with dynamic indicators of cognitive engagement. Its flexibility is particularly noteworthy, as it enables studies in realistic and natural environments, adding ecological validity to the research. In comparison to conventional methods like Electroencephalography (EEG) and functional Magnetic Resonance Imaging (fMRI), fNIRS demonstrates superior resistance to motion artifacts and environmental noise, making it particularly advantageous for research conducted in dynamic and ecologically valid settings [34].\nPast studies have employed several approaches to evaluate cognitive load through the analysis of fNIRS data. Conventionally, statistical tests have been employed on fNIRS signals for cognitive load assessment [5]. This involves subjecting fNIRS signals to statistical analyses, with a focus on the concentration of relative oxygenated haemoglobin (HbO). The significance of activation is commonly established through per-channel t-tests [4, 27], providing insights into specific brain regions influenced by cognitive load. Moreover, researchers frequently used Analysis of Variance (ANOVA) tests, encompassing both one-way and two-way ANOVA, to facilitate group-level comparisons of mean activation [1, 14].\nRecognizing the challenges posed by the ever-expanding scale and complexity of fNIRS data, researchers are increasingly turning to Machine Learning (ML) and Deep Learning (DL) methodologies as promising alternatives. Due to the accessibility of cost-effective processing power, the utilization of ML or DL in diverse domains like image processing [21], signal processing [13], and remote sensing [3] has garnered considerable attention from researchers. In the field of neuroscience, constructing an ML model involves several stages, including data preprocessing, feature engineering, the development of ML or DL models, and a subsequent evaluation of performance.\nVarious studies have successfully employed a range of ML algorithms, such as Random Forests [20, 26], k-Nearest Neighbors (k-NN) [10], Naive Bayes [15], Linear Discriminant Analysis (LDA) [12], and Support Vector Machines (SVM) [6], on fNIRS signals. However, challenges exist in the extraction and selection of features from these"}, {"title": "3", "content": "signals. The ML methods commonly employed for these tasks often suffer from limitations, particularly regarding scalability and efficiency. One notable limitation lies in the process of extracting and selecting features, wherein traditional ML methods tend to fall short. These limitations become especially apparent when considering the computational cost, which escalates quadratically with respect to the parameters under consideration. The implication is a substantial increase in resource requirements, demanding vast amounts of labelled training data for supervised learning applications.\nIn contrast to the traditional ML approaches, DL adopts a paradigm shift by employing a deep neural network to handle the entire process. Specifically, autoencoders [25] and Convolutional Neural Networks (CNN) [24] have become important actors, emphasizing the resolution of optimization problems related to automatic feature extraction [33] and classification tasks for fNIRS signals [19]. The application of DL techniques, with their inherent capacity for automatic feature extraction and hierarchical representation learning, stands out as a noteworthy advancement in the field of fNIRS signal analysis. Therefore, this study aims to employ DL methodologies for the purpose of classifying mental work into distinct levels. In contrast to conventional machine learning approaches, which often rely on manual feature engineering, the focus here is on leveraging the capabilities of deep neural networks. Numerous deep learning models have been explored in existing literature. However, our architecture outperforms the current state-of-the-art by introducing an innovative framework that utilizes the inherent capabilities of deep neural networks for the classification of mental work. One of the distinguishing features of our proposed model is its ability to automatically extract relevant features from fNIRS signals, which is an essential component in understanding the complex patterns indicative of different mental work levels.\nThe primary objective of this study is to delve into the classification of mental workload by extending the scope beyond the conventional binary categorization. Specifically, this study centres its focus on distinguishing and classifying four distinct levels of mental workload by analysing the publicly available Tufts fNIRS dataset [18]. It is noteworthy to highlight that previous studies conducted on the Tufts fNIRS dataset have predominantly concentrated on the differentiation between two levels of mental workload. To achieve this, we propose the implementation of a hybrid DL-based architecture that utilizes the capabilities of Convolutional Neural Networks (CNN) in with Long Short-Term Memory networks (LSTM). This fusion of CNN and LSTM layers presents a unique approach to fNIRS signal analysis. While CNN layers specialize in spatial feature extraction [32], LSTM layers excel in capturing temporal dependencies [17]. By integrating these components, our model provides a comprehensive solution for the challenges posed by fNIRS data, outperforming traditional models that often focus on either spatial or temporal aspects in isolation. The remainder of this paper is as follows.\nSection 2 will describe the suggested CNN and LSTM architecture designed for fNIRS measurements. Following this, Section 3 will provide an overview of the experiment, results, and validation, with the subsequent discussion also presented within the same section. The study concludes in Section 4."}, {"title": "2 Methodology", "content": "In our study, we introduce a robust 1- dimensional Convolutional Neural Network (1D-CNN) coupled with LSTM layers to formulate a predictive model for mental workload based on fNIRS data. Fig. 1 illustrates the flow chart of the proposed CNN and LSTM-based architecture. This design incorporates baseline convolutional blocks to extract features from the input data. Subsequently, the extracted features undergo additional processing through LSTM layers to capture temporal dependencies in the data, thereby improving the accuracy of the classification output."}, {"title": "2.1 Convolution Neural Networks (CNN)s", "content": "In recent years, CNNs have emerged as a groundbreaking paradigm in the field of artificial intelligence. A key component of CNN is the convolutional kernel's weight-sharing technique for the convolutional kernel [30]. One of the distinctive strengths of CNN lies in its remarkable ability to learn implicit features without the need for intricate preprocessing or a distinct feature extraction process [8]. The baseline blocks of our proposed model consist of the Convolutional Layer, Rectified Linear Unit (Relu) activation layer, and Max Pooling layer. The convolutional operation, as expressed by Equation 1, encapsulates the fundamental process by which a given input tensor X interacts with a set of learnable filters W and bias b.\n\\(Y[i] = f(\\sum_{j=1}^{k} X[i + j \u2212 1]. W[j] + b\\)\n(1)\nFollowing the convolution operation, the activation function plays a crucial role by nonlinearly transforming the resulting output values. This transformation is instrumental in mapping the original multi-dimensional features, contributing to an augmented linear separability of the extracted features. In our model, we employed the Rectified Linear Unit (Relu) activation function, denoted by Equation 2, to introduce a non-linearity that enhances the model's ability to capture complex patterns.\n\\(Y[i] = max(0, X[i])\\)\n(2)"}, {"title": "2.2 Long Short-Term Memory (LSTM)", "content": "The LSTM [3] network extends the capabilities of the Recurrent Neural Network (RNN) [2]. While RNNs utilize a directed cycle structure, transferring the output of a hidden layer to the same hidden layer, LSTM architectures excel in capturing and learning from the inherent temporal dynamics in data [28]. Unlike traditional RNNs, LSTMs are designed to tackle issues like vanishing and exploding gradients [29], commonly hindering the effective learning of long-term dependencies in sequential data. This is accomplished through specialized memory cells and gating mechanisms, allowing LSTMs to selectively retain and discard information over extended sequences. The distinctive feature of LSTMs lies in their ability to memorize information over extended time intervals. This is particularly relevant for fNIRS data, as it exhibits dependencies on previous data points. LSTM possess internal memory units that allow them to retain and utilize information from previous time steps, making them suitable for modelling cognitive load dynamics. The equations governing the behavior of these models are as follows:\n\\(i_{t} = \\sigma(U_{i} * X_{t} + V_{i} * H_{t-1} + Z_{i} o C_{t-1} + b_{i}),\\)\n(4)\n\\(f_{t} = \\sigma(U_{f} * X_{t} + V_{f} * H_{t-1} + Z_{f} o C_{t-1} + b_{f}),\\)\n(5)\n\\(O_{t} = \\sigma(U_{o} * X_{t} + V_{f} * H_{t-1} + Z_{f} o C_{t-1} + b_{f}),\\)\n(6)\n\\(C_{t} = tanh(U_{c} * X_{t} + V_{c} * H_{t-1} + b_{c})\\)\n(7)\n\\(C_{t} = f_{t}o C_{t-1} + i_{t}o C_{t}\\)\n(8)\n\\(H_{t} = o_{t} * tanh(C_{t})\\)\n(9)\nIn Equation 4, Equation 5, Equation 6, Equation 7, Equation 8 and Equation 9, '*' represents convolution, and \u2018\u3002' represents the Hadamard product. Cell states are denoted as C\u2081,..., C\u209c and hidden states as H\u209c, i\u209c, f\u209c, O\u209c denote the input gate, forget gate, and output gate respectively. \u03c3 denotes the sigmoid activation function. U\u1d62, Uf, U\u2092, V\u1d62, Vf, V\u2092, Vc, Z\u1d62, Zf, Z\u2092 are 2D convolution kernels. b\u1d62, b\ua700, bf and bo are the bias terms."}, {"title": "2.3 CNN and LSTM-based proposed model", "content": "The neural network architecture outlined in Fig. 1 is tailored for the classification of cognitive load using functional Near-Infrared Spectroscopy (fNIRS) data. In the fNIRS, where the data often involves both spatial and temporal aspects, the combination of 1D-CNN and LSTM layers offers a robust approach. Table 1 denotes the length of the input sequence as \"L\" and batch size as \"BS\u201d. The CNN components, including"}, {"title": "3 Experiment", "content": "In the assessment of the proposed CNN and LSTM-based models designed for the classification of mental workload, we employed the Tufts fNIRS open-access dataset [18]. This dataset encompasses fNIRS data derived from a cohort of 68 participants engaged in a series of controlled n-back tasks, encompassing 0-back, 1-back, 2-back, and 3-back scenarios. The raw fNIRS measurements comprise temporal traces of alternating current intensity and changes in phase at two distinct wavelengths (690 and 830 nm), employing a modulation frequency of 110 MHz. To mitigate the influence of respiration, heartbeat, and drift artifacts, each univariate time series underwent bandpass filtering using a 3rd-order zero-phase Butterworth filter, with a retention range of 0.001-0.2 Hz."}, {"title": "3.1 Preprocessing", "content": "The segmentation of fNIRS signals involved the use of overlapping windows, each spanning 30 seconds, with a stride of 0.6 seconds. This approach was adopted based on the recommendations of the dataset authors, who observed that 30-second windows achieved the highest accuracy for individual subjects. At each timestep within these windows, eight numerical measurements were recorded, resulting in the creation of 8 features.\nIn the context of the 30-second window size, each feature vector comprised 150 fNIRS measurements, contributing to a length of 150 \u00d7 8. The adoption of the 150 \u00d7 8 feature size was deemed essential not only to facilitate the real-time implementation of deep learning models but also to enable these models to capture and learn temporal dependencies present within the data frames."}, {"title": "3.2 Results", "content": "A CNN can be conceptualized as a parameterized function, and its overall performance is intricately tied to the selection of optimal parameters. To train a deep learning model, the Adam [22] optimizer is employed, with a specified learning rate of 0.001. Additionally, the cross-entropy [7] loss function has been incorporated into the model architecture. Our training spanning a total of 1000 epochs as shown in Fig. 2. This prolonged training period enabled us to comprehensively assess the model's evolution over time."}, {"title": "8", "content": "minimizing classification errors. It signifies that the model has reached a state where its predictions align closely with ground truth labels, and further training may yield diminishing returns. This convergence not only validates the effectiveness of the chosen parameters, including the Adam optimizer and the cross-entropy loss function but also underscores the model's capacity to capture intricate patterns within the data. The confusion matrix for the proposed CNN and LSTM-based models, as presented in Fig. 3, reveals a notably low incidence of misclassifications. The model demonstrates a high level of precision in correctly identifying and categorizing samples, with only a minimal number of instances where it deviates from the ground truth labels."}, {"title": "9", "content": "Upon analyzing the training curve illustrated in Fig. 4, it became evident that although the model did converge, there were discernible fluctuations present. These fluctuations in the training curve indicate a lack of stability and optimal convergence. Unlike the consistent and smooth convergence observed in the proposed model, the absence of LSTM layers introduced variability in the training process. This outcome underscores the significance of the LSTM layers in capturing temporal dependencies and intricate patterns within the data. A closer examination of the confusion matrix for the CNN model in Fig. 5, which lacks LSTM layers, reveals a higher frequency of misclassifications compared to the proposed CNN LSTM-based model."}, {"title": "10", "content": "To assess the performance of the proposed CNN and LSTM-based models, a comparison has been made against traditional ML models [20]. The performance evaluation is encapsulated in Fig. 6, which presents the confusion matrix for a range of ML algorithms. In contrast to the remarkable performance observed in the CNN and LSTM models, the traditional ML models, including Naive Bayes and Nearest Centroid, exhibit a notable disparity in their classification accuracy. Figure 5 illustrates that these models suffer from a higher incidence of misclassifications, signifying a limited capacity to discern and categorize samples accurately. Among the ML algorithms, Decision Trees and k-NN emerge as more robust performers, showcasing comparatively better accuracy and a reduced number of misclassifications. Interestingly, the performance of Decision Trees aligns closely with the proposed DL-based model, emphasizing the effectiveness of both approaches in handling the classification task. However, it is noteworthy that the Decision Trees model exhibits a slightly higher rate of misclassifications when compared to the DL counterparts."}, {"title": "11", "content": "The superior performance of the proposed CNN and LSTM-based models, especially when compared to traditional ML algorithms, underscores the transformative potential of deep learning for the evaluation cognitive load assessment using fNIRS signals. As highlighted in Fig. 6 and Table 2, the CNN and LSTM models exhibit a notable advantage in accuracy, F1-score, AUC, precision, and recall, offering a comprehensive and detailed evaluation of cognitive load levels. Deep learning's capability for automatic feature extraction and hierarchical representation learning proves especially beneficial in fNIRS-based cognitive load assessment. In contrast to traditional ML models relying on manually engineered features, the CNN and LSTM models autonomously discern intricate spatial and temporal patterns within the fNIRS data. This inherent ability leads to a more precise and robust classification of mental workload levels, crucial for comprehending cognitive processes. Additionally, these models outperform traditional ML counterparts like Naive Bayes and Nearest Centroid, and even compete effectively with Decision Trees and k-NN. The automated and adaptive nature of deep learning models allows them to adjust to the dynamic and intricate nature of cognitive processes, providing a more flexible and scalable solution for real-world applications."}, {"title": "4 Conclusion", "content": "The article investigates the incorporation of LSTM layers into CNNs. Unlike many neuroscience studies that predominantly rely on a singular model, our research adopts a hybrid approach. Specifically, we utilized fNIRS signal data sourced from the open access TUFTS datasets in our investigation. Our goal is to classify cognitive states, specifically 0-back, 1-back, 2-back, and 3-back, thereby expanding the scope beyond the conventional binary workload classification. Our work extends upon previous studies that predominantly concentrated on employing deep learning models to classify only two levels of workload using fNIRS data. This Endeavor aims not only to contribute to a more comprehensive understanding of cognitive states but also enhances the applicability of our findings to real-world scenarios where cognitive demands vary across a continuum. One of the main focuses of our research is to evaluate the impact of integrating LSTM layers into CNNs within the framework of deep learning models. By utilizing the strengths of both convolutional and LSTM operators, we can effectively capture spatial and temporal dependencies, leading to enhanced performance."}, {"title": "12", "content": "Our experimental findings provide compelling evidence of the effectiveness of integrating LSTM layers into the CNN architecture. The model's accuracy demonstrated a noteworthy improvement, increasing from 97.40% to 97.82%. This improvement can be attributed to the LSTM layers facilitating the model in capturing and leveraging temporal dependencies within the fNIRS data. Furthermore, our study involved a comparative analysis with traditional ML methods. Among the various ML classifiers employed, k-NN outperformed other ML classifiers. It is noteworthy that while DL methods employed in our research showcased superior performance compared to ML classifiers, it is essential to acknowledge the inherent trade-off. Designing and optimizing DL models necessitates a substantial investment of time and effort. In consideration of the potential for further improvement, it appears that extending the depth of the CNN and LSTM beyond the current proposal could yield enhanced results. In our future endeavors, we plan to advance the scope of our research by proposing a more robust deep CNN model. This advanced model will feature an increased number of layers and optimal parameterization, allowing it to unveil more intricate nonlinear structures progressively."}]}