{"title": "Enhancing Cognitive Workload Classification Using Integrated LSTM Layers and CNNs for fNIRS Data Analysis", "authors": ["Mehshan Ahmed Khan", "Houshyar Asadil", "Mohammad Reza Chalak Qazani", "Adetokunbo Arogbonlo", "Siamak Pedrammehr", "Adnan Anwar", "Asim Bhatti", "Saeid Nahavandi", "Chee Peng Lim"], "abstract": "Functional near-infrared spectroscopy (fNIRS) is employed as a non-invasive method to monitor functional brain activation by capturing changes in the concentrations of oxygenated haemoglobin (HbO) and deoxygenated haemoglobin (HbR). Various machine learning classification techniques have been utilized to distinguish cognitive states. However, conventional machine learning methods, although simpler to implement, undergo a complex preprocessing phase before network training and demonstrate reduced accuracy due to inadequate data preprocessing. Additionally, previous research in cognitive load assessment using fNIRS has predominantly focused on differsizeentiating between two levels of mental workload. These studies mainly aim to classify low and high levels of cognitive load or distinguish between easy and difficult tasks. To address these limitations associated with conventional methods, this paper conducts a comprehensive exploration of the impact of Long Short-Term Memory (LSTM) layers on the effectiveness of Convolutional Neural Networks (CNNs) within deep learning models. This is to address the issues related to spatial features overfitting and lack of temporal dependencies in CNN in the previous studies. By integrating LSTM layers, the model can capture temporal dependencies in the fNIRS data, allowing for a more comprehensive understanding of cognitive states. The primary objective is to assess how incorporating LSTM layers enhances the performance of CNNs. The experimental results presented in this paper demonstrate that the integration of LSTM layers with Convolutional layers results in an increase in the accuracy of deep learning models from 97.40% to 97.92%.", "sections": [{"title": "1 Introduction", "content": "Cognitive load, a fundamental concept in cognitive psychology, pertains to the quantity of information that the working memory can retain [9, 31]. As the human brain possesses finite capacities for processing information simultaneously, delving into the intricacies of cognitive load provides valuable insights into the mechanisms that govern optimal learning and cognitive functioning. The n-back [23] task is a typical paradigm designed to administer a specific and measurable cognitive workload to individuals. In this task, participants are tasked with a dynamic challenge, requiring them to respond, typically by pressing a button, whenever the current stimulus matches with the stimulus presented n times earlier in the sequence. This cognitive challenge not only demands participants' sustained attention but also necessitates the effective engagement of their working memory.\nIn the field of neurophysiological research, functional Near-Infrared Spectroscopy (fNIRS) has emerged as a non-invasive and adaptable neuroimaging technique, contributing significantly to the exploration of cognitive workload [11, 16]. By capturing real-time changes in cerebral blood flow and oxygenation levels, fNIRS provides researchers with dynamic indicators of cognitive engagement. Its flexibility is particularly noteworthy, as it enables studies in realistic and natural environments, adding ecological validity to the research. In comparison to conventional methods like Electroencephalography (EEG) and functional Magnetic Resonance Imaging (fMRI), fNIRS demonstrates superior resistance to motion artifacts and environmental noise, making it particularly advantageous for research conducted in dynamic and ecologically valid settings [34].\nPast studies have employed several approaches to evaluate cognitive load through the analysis of fNIRS data. Conventionally, statistical tests have been employed on fNIRS signals for cognitive load assessment [5]. This involves subjecting fNIRS signals to statistical analyses, with a focus on the concentration of relative oxygenated haemoglobin (HbO). The significance of activation is commonly established through per-channel t-tests [4, 27], providing insights into specific brain regions influenced by cognitive load. Moreover, researchers frequently used Analysis of Variance (ANOVA) tests, encompassing both one-way and two-way ANOVA, to facilitate group-level comparisons of mean activation [1, 14].\nRecognizing the challenges posed by the ever-expanding scale and complexity of fNIRS data, researchers are increasingly turning to Machine Learning (ML) and Deep Learning (DL) methodologies as promising alternatives. Due to the accessibility of cost-effective processing power, the utilization of ML or DL in diverse domains like image processing [21], signal processing [13], and remote sensing [3] has garnered considerable attention from researchers. In the field of neuroscience, constructing an ML model involves several stages, including data preprocessing, feature engineering, the development of ML or DL models, and a subsequent evaluation of performance.\nVarious studies have successfully employed a range of ML algorithms, such as Random Forests [20, 26], k-Nearest Neighbors (k-NN) [10], Naive Bayes [15], Linear Discriminant Analysis (LDA) [12], and Support Vector Machines (SVM) [6], on fNIRS signals. However, challenges exist in the extraction and selection of features from these"}, {"title": "2 Methodology", "content": "In our study, we introduce a robust 1- dimensional Convolutional Neural Network (1D-CNN) coupled with LSTM layers to formulate a predictive model for mental workload based on fNIRS data. Fig. 1 illustrates the flow chart of the proposed CNN and LSTM-based architecture. This design incorporates baseline convolutional blocks to extract features from the input data. Subsequently, the extracted features undergo additional processing through LSTM layers to capture temporal dependencies in the data, thereby improving the accuracy of the classification output."}, {"title": "2.1 Convolution Neural Networks (CNN)s", "content": "In recent years, CNNs have emerged as a groundbreaking paradigm in the field of artificial intelligence. A key component of CNN is the convolutional kernel's weight-sharing technique for the convolutional kernel [30]. One of the distinctive strengths of CNN lies in its remarkable ability to learn implicit features without the need for intricate preprocessing or a distinct feature extraction process [8]. The baseline blocks of our proposed model consist of the Convolutional Layer, Rectified Linear Unit (Relu) activation layer, and Max Pooling layer. The convolutional operation, as expressed by Equation 1, encapsulates the fundamental process by which a given input tensor X interacts with a set of learnable filters W and bias b.\n$Y[i] = f(\\sum_{j=1}^{k} X[i + j \u2212 1]. W[j] + b)$\nFollowing the convolution operation, the activation function plays a crucial role by nonlinearly transforming the resulting output values. This transformation is instrumental in mapping the original multi-dimensional features, contributing to an augmented linear separability of the extracted features. In our model, we employed the Rectified Linear Unit (Relu) activation function, denoted by Equation 2, to introduce a non-linearity that enhances the model's ability to capture complex patterns.\n$Y[i] = max(0, X[i])$"}, {"title": "2.2 Long Short-Term Memory (LSTM)", "content": "The LSTM [3] network extends the capabilities of the Recurrent Neural Network (RNN) [2]. While RNNs utilize a directed cycle structure, transferring the output of a hidden layer to the same hidden layer, LSTM architectures excel in capturing and learning from the inherent temporal dynamics in data [28]. Unlike traditional RNNs, LSTMs are designed to tackle issues like vanishing and exploding gradients [29], commonly hindering the effective learning of long-term dependencies in sequential data. This is accomplished through specialized memory cells and gating mechanisms, allowing LSTMs to selectively retain and discard information over extended sequences. The distinctive feature of LSTMs lies in their ability to memorize information over extended time intervals. This is particularly relevant for fNIRS data, as it exhibits dependencies on previous data points. LSTM possess internal memory units that allow them to retain and utilize information from previous time steps, making them suitable for modelling cognitive load dynamics. The equations governing the behavior of these models are as follows:\n$i_t = \u03c3(U_i * X_t + V_i * H_{t\u22121} + Z_i o C_{t-1} + b_i)$,\n$f_t = \u03c3(U_f * X_t + V_f * H_{t\u22121} + Z_f o C_{t-1} + b_f)$,\n$O_t = \u03c3(U_o * X_t + V_f * H_{t\u22121} + Z_f o C_{t-1} + b_f)$,\n$\\bar{C}_t = tanh(U_c * X_t + V_c * H_{t\u22121} + b_c)$\n$C_t = f_t o C_{t-1} + i_t o \\bar{C}_t$\n$H_t = o_t * tanh(C_t)$\nIn Equation 4, Equation 5, Equation 6, Equation 7, Equation 8 and Equation 9, '*' represents convolution, and \u2018\u3002' represents the Hadamard product. Cell states are denoted as $C_1$,..., $C_t$ and hidden states as $H_t$, $i_t$, $f_t$, $O_t$ denote the input gate, forget gate, and output gate respectively. \u03c3 denotes the sigmoid activation function. $U_i$, $U_f$, $U_o$, $V_i$, $V_f$, $V_o$, $V_c$, $Z_i$, $Z_f$, $Z_o$ are 2D convolution kernels. $b_i$, $b_c$, $b_f$ and $b_o$ are the bias terms."}, {"title": "2.3 CNN and LSTM-based proposed model", "content": "The neural network architecture outlined in Fig. 1 is tailored for the classification of cognitive load using functional Near-Infrared Spectroscopy (fNIRS) data. In the fNIRS, where the data often involves both spatial and temporal aspects, the combination of 1D-CNN and LSTM layers offers a robust approach. Table 1 denotes the length of the input sequence as \"L\" and batch size as \"BS\u201d. The CNN components, including"}, {"title": "3 Experiment", "content": "In the assessment of the proposed CNN and LSTM-based models designed for the classification of mental workload, we employed the Tufts fNIRS open-access dataset [18]. This dataset encompasses fNIRS data derived from a cohort of 68 participants engaged in a series of controlled n-back tasks, encompassing 0-back, 1-back, 2-back, and 3-back scenarios. The raw fNIRS measurements comprise temporal traces of alternating current intensity and changes in phase at two distinct wavelengths (690 and 830 nm), employing a modulation frequency of 110 MHz. To mitigate the influence of respiration, heartbeat, and drift artifacts, each univariate time series underwent bandpass filtering using a 3rd-order zero-phase Butterworth filter, with a retention range of 0.001-0.2 Hz."}, {"title": "3.1 Preprocessing", "content": "The segmentation of fNIRS signals involved the use of overlapping windows, each spanning 30 seconds, with a stride of 0.6 seconds. This approach was adopted based on the recommendations of the dataset authors, who observed that 30-second windows achieved the highest accuracy for individual subjects. At each timestep within these windows, eight numerical measurements were recorded, resulting in the creation of 8 features.\nIn the context of the 30-second window size, each feature vector comprised 150 fNIRS measurements, contributing to a length of 150 \u00d7 8. The adoption of the 150 \u00d7 8 feature size was deemed essential not only to facilitate the real-time implementation of deep learning models but also to enable these models to capture and learn temporal dependencies present within the data frames."}, {"title": "3.2 Results", "content": "A CNN can be conceptualized as a parameterized function, and its overall performance is intricately tied to the selection of optimal parameters. To train a deep learning model, the Adam [22] optimizer is employed, with a specified learning rate of 0.001. Additionally, the cross-entropy [7] loss function has been incorporated into the model architecture. Our training spanning a total of 1000 epochs as shown in Fig. 2. This prolonged training period enabled us to comprehensively assess the model's evolution over time.\nThroughout the 1000 epochs of training, we closely monitored the performance metrics of the model, specifically observing the dynamic interplay between accuracy and loss. The upward trajectory of accuracy signifies an improvement in the model's ability to correctly classify samples, demonstrating its capacity to learn and generalize effectively. On the other hand, the downward trend in loss reflects a reduction in the disparity between the predicted and actual distributions, underscoring the model's proficiency in"}, {"title": "4 Conclusion", "content": "The article investigates the incorporation of LSTM layers into CNNs. Unlike many neuroscience studies that predominantly rely on a singular model, our research adopts a hybrid approach. Specifically, we utilized fNIRS signal data sourced from the open access TUFTS datasets in our investigation. Our goal is to classify cognitive states, specifically 0-back, 1-back, 2-back, and 3-back, thereby expanding the scope beyond the conventional binary workload classification. Our work extends upon previous studies that predominantly concentrated on employing deep learning models to classify only two levels of workload using fNIRS data. This Endeavor aims not only to contribute to a more comprehensive understanding of cognitive states but also enhances the applicability of our findings to real-world scenarios where cognitive demands vary across a continuum. One of the main focuses of our research is to evaluate the impact of integrating LSTM layers into CNNs within the framework of deep learning models. By utilizing the strengths of both convolutional and LSTM operators, we can effectively capture spatial and temporal dependencies, leading to enhanced performance."}]}