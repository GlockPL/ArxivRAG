{"title": "FacaDiffy: Inpainting unseen facade parts using diffusion models", "authors": ["Thomas Fr\u00f6ch", "Olaf Wysocki", "Yan Xia", "Junyu Xie", "Benedikt Schwab", "Daniel Cremers", "Thomas H. Kolbe"], "abstract": "High-detail semantic 3D building models are frequently utilized in robotics, geoinformatics, and computer vision. One key aspect\nof creating such models is employing 2D conflict maps that detect openings' locations in building facades. Yet, in reality, these\nmaps are often incomplete due to obstacles encountered during laser scanning. To address this challenge, we introduce FacaDiffy,\na novel method for inpainting unseen facade parts by completing conflict maps with a personalized Stable Diffusion model. Spe-\ncifically, we first propose a deterministic ray analysis approach to derive 2D conflict maps from existing 3D building models and\ncorresponding laser scanning point clouds. Furthermore, we facilitate the inpainting of unseen facade objects into these 2D conflict\nmaps by leveraging the potential of personalizing a Stable Diffusion model. To complement the scarcity of real-world training\ndata, we also develop a scalable pipeline to produce synthetic conflict maps using random city model generators and annotated\nfacade images. Extensive experiments demonstrate that FacaDiffy achieves state-of-the-art performance in conflict map completion\ncompared to various inpainting baselines and increases the detection rate by 22% when applying the completed conflict maps for\nhigh-definition 3D semantic building reconstruction. The code is be publicly available in the corresponding GitHub repository:\nhttps://github.com/ThomasFroech/InpaintingofUnseenFacadeObjects", "sections": [{"title": "1. Introduction", "content": "Semantic 3D city models hold significant potential to address\npressing global issues. Unlike mesh-based models, they are\ncharacterized by watertightness and object-oriented modeling,\nwhich has proven pivotal in various applications, such as estim-\nating building solar potential and simulating wind flow (Biljecki\net al., 2015). Currently, they are ubiquitous, as, for example, ap-\nproximately 140 million open access building models are avail-\nable in the United States, Switzerland, and Poland while 55 mil-\nlion are available in Germany (Wysocki et al., 2024). While nu-\nmerous cities and entire countries provide semantic 3D build-\ning models at Level of Detail (LoD)2, characterized by complex\nroof shapes and planar facades, LoD3 datasets with semantic-\nally detailed facades remain scarce.\nHowever, such highly-detailed LoD3 datasets are required for\nnumerous application areas, such as assessing flood risk (Amireb-\nrahimi et al., 2016), analyzing building potential for vertical\nfarming (Palliwal et al., 2021), and estimating energy demand\n(Nouvel et al., 2013).\nAlthough a great deal of research has been devoted to the auto-\nmatic LoD3 reconstruction (Szeliski, 2010), the current practice\nindicates that tedious, manual LoD3 modeling prevails (Chai-\ndas et al., 2021). Yet, recent developments have shown that the\nso-called conflict maps prove to be valuable for the automatic\nLoD3 reconstruction (Wysocki et al., 2023c). As we illustrate\nin Figure 1, such conflict maps are generated based on the ana-\nlysis of the 3D building model and sensor rays: The surface\nis deemed confirmed (green) when the ray point hits the sur-\nface, conflicted (red) when the ray traverses the surface, and un-\nknown (black) when the surface is unmeasured. These maps are\nconsidered a core intermediate geometric reconstruction cue, as\nthey are frequently coupled with semantic-rich images or point\nclouds (Wysocki et al., 2023c); and the principle can also be dir-\nectly used, for example, for building change detection (Tuttas et\nal., 2015). However, as we see in Figure 1, the conflict maps are\nprone to occlusions (black), which render them incomplete and\nimpact their at-scale applicability."}, {"title": "2. Related Work", "content": "Semantic 3D city models. In addition to offering geomet-\nric and visual insights into topographic features, semantic 3D\ncity models provide comprehensive information about struc-\ntures, taxonomies, and aggregations at the scale of cities, re-\ngions, and even complete countries. For the representation and\nmanagement of city models, the standard CityGML is used in-\nternationally, which has been issued by the Open Geospatial\nConsortium (OGC) (Kolbe, 2009, Gr\u00f6ger and Pl\u00fcmer, 2012,\nKolbe et al., 2021). CityGML enables the modeling of urban\nobjects with their 3D geometry, appearance, topology, and se-\nmantics at four different LoDs. The data model of CityGML\n3.0 is based on the ISO 191xx series of geographic informa-\ntion standards, and CityGML datasets can be encoded using the\nGeography Markup Language (GML) (Kutzner et al., 2020).\nSynthetic generation of semantic city models. With Ran-\ndom3Dcity, Biljecki et al. (Biljecki et al., 2016) introduce a\nmethod for the procedural generation of randomized semantic\ncity models at various LoD levels. Their approach utilizes\na set of pre-defined architectural modeling rules guiding its\nstochastic nature. Such rules govern aspects like the permiss-\nible positioning of facade elements such as doors or windows.\nReconstruction of semantic 3D building models. The con-\nsiderable potential for applying detailed LoD3 models across\ndiverse domains, coupled with their scarcity, has motivated a\nsignificant number of studies to explore the reconstruction of\nsuch models. Investigations involve leveraging various data\nsources, such as optical images, oblique Airborne Laser Scan-\nning (ALS) point clouds, and Mobile Laser Scanning (MLS)\nmeasurements, as well as employing diverse approaches, in-\ncluding formal grammar approaches and Bayesian networks\n(Ripperda, 2010, Huang et al., 2020, Wysocki et al., 2023c).\nWithin their pipeline for reconstructing underpasses in semantic\nLoD2 city models from co-registered MLS point clouds, Wyso-\ncki et al. introduce the concept of 2D conflict maps. Their\nprobabilistic approach relies on an occupancy grid implemen-\nted as an octree structure, with voxel sizes reflecting the com-\nbined uncertainty of the MLS measurements and the semantic\ncity model. This concept has been further developed by (Wyso-\ncki et al., 2023a, Wysocki et al., 2023c, Hoegner and Gleixner,\n2022), thereby substantiating its effectiveness. The pivotal ad-\nvantage of the previously mentioned methods over mesh-based\napproaches is the usage of 3D semantic building models as pri-\nors, which has proven to maintain the model watertightness\nas well as higher 3D reconstruction accuracy, reaching up to\naround 50% when compared to the mesh-based Poisson recon-\nstruction (Wysocki et al., 2023c). Nevertheless, despite gen-\nerally yielding commendable results, the challenge of incom-\npleteness remains unsolved.\nDeep-learning-based image inpainting. Besides traditional\nimage inpainting methods, which typically rely on solving Par-\ntial Differential Equations (PDE)s (Telea, 2004, Bertalmio et\nal., 2001), and yield unsatisfactory results when applied to\nfacade images (Fritzsche et al., 2022). Recent advancements in\ndeep-learning-based image inpainting suggest potential meth-\nodologies for addressing the challenge of incomplete 2D con-\nflict maps. Large mask inpainting (LaMa) (Suvorov et al.,\n2022), configured as a Generative Adversarial Network (GAN),\nemploys Fast Fourier Convolution operators (Chi et al., 2020)\nto overcome the limitation of restricted receptive fields, thereby\nenabling expansive coverage across the entire image.\nDiffusion Probabilistic Model (DM)s have emerged as power-\nful tools for various generative applications in the last years.\nNotably, Stable Diffusion (SD) (Rombach et al., 2022) demon-\nstrates remarkable flexibility with its capability to handle open-\nended text conditioning. When supplemented with additional\nimage and mask inputs, the SD framework can be further ex-\ntended to solve image inpainting tasks, guided by relevant text\nprompts. To our knowledge, no method deploying diffusion\nmodels for inpainting facade conflict maps has been published\nyet.\nTo tailor pre-trained SD models for domain-specific applica-\ntions, a variety of personalization (i.e., customization) tech-\nniques (Gal et al., 2023, Kumari et al., 2023, Wei et al., 2023)\nare developed. These methods generally utilize small-scale\ndatasets to fine-tune the pre-trained SD pipeline for specialized\ndomains. In particular, Dreambooth (Ruiz et al., 2023) stands\nout as a robust personalization approach, incorporating a prior-\npreservation loss and LoRA (Hu et al., 2022)-based fine-tuning."}, {"title": "3. Method", "content": "As illustrated in Figure 2, given the LoD2 model and point\ncloud data, FacaDiffy aims at completing facade conflict\nmaps by leveraging deep-learning-based image inpainting tech-\nniques. Specifically, Sect. 3.1 introduces a ray-casting approach\nto derive 2D real-world conflict maps and corresponding binary\nmasks indicating occluded regions. These estimated occluded\nareas are then adopted as inpainting masks. To complement in-\nsufficient real-world conflict map data for training, Sect. 3.2 de-\ntails a scalable pipeline that generates synthetic conflict maps as\nadditional training data for the inpainting model. Sect. 3.3 elab-\norates on a personalized SD-based inpainting method designed\nto recover complete conflict maps."}, {"title": "3.1 Conflict Map Computation", "content": "As Figure 2 illustrates, we obtain incomplete conflict maps\nand corresponding binary masks, highlighting missing (i.e., \u043e\u0441-\ncluded) areas through a deterministic ray-casting approach with\ntolerances that combine semantic LoD2 building models and\ncorresponding laser scanning point cloud data.\nWe first define the unit rays, denoted as rp, as originating from\na viewpoint v and oriented towards a corresponding point p.\n$r_p = v + t \\frac{p-v}{\\lVert p - v \\rVert}$\nThen, we identify conflicts between a LoD2 model and a corres-"}]}