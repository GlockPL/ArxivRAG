{"title": "RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model", "authors": ["Jianhao Yuan", "Shuyang Sun", "Daniel Omeiza", "Bo Zhao", "Paul Newman", "Lars Kunze", "Matthew Gadd"], "abstract": "Robots powered by \u2018blackbox' models need to provide human-understandable explanations which we can trust. Hence, explainability plays a critical role in trustworthy autonomous decision-making to foster transparency and acceptance among end users, especially in complex autonomous driving. Recent advancements in Multi-Modal Large Language models (MLLMs) have shown promising potential in enhancing the explainability as a driving agent by producing control predictions along with natural language explanations. However, severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task. Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment. To address these challenges, we present RAG-Driver, a novel retrieval-augmented multi-modal large language model that leverages in-context learning for high-performance, explainable, and generalisable autonomous driving. By grounding in retrieved expert demonstration, we empirically validate that RAG-Driver achieves state-of-the-art performance in producing driving action explanations, justifications, and control signal prediction. More importantly, it exhibits exceptional zero-shot generalisation capabilities to unseen environments without further training endeavours\u00b9.", "sections": [{"title": "I. INTRODUCTION", "content": "Driven by the emerging development of deep learning, autonomous driving has observed a paradigm shift from rules-based decision systems [66, 21] to data-driven learning-based approaches [28, 6, 36]. However, this comes at the cost of transparency in decision-making, especially for end-to-end autonomous driving systems which are considered black-box in nature [13]. Thus, in addition to precision in action control, explanation provision is key in ensuring trustworthy decision-making to reconcile the system's decisions with end-user expectations to foster confidence and acceptance [79, 8, 57] in dynamic driving environments.\nTraditional approaches have mainly relied on attention visualisation [5, 7, 55] as a proxy to rationalise the decisions of"}, {"title": "II. RELATED WORK", "content": "End-to-end learned driving [13] maps directly from raw sensor input to vehicle control signals. This data-driven, joint optimisation of perception, prediction, and planning can be simple and efficient [28]. In this area, various learning-based approaches, including behavior cloning [6, 12, 61, 83, 60], inverse optimal control [81, 65, 75], and reinforcement learning [36, 71, 11, 44] are promising. A key area of focus in this area is explainability [80], which is crucial for improving transparency and building trust towards wider public acceptance of autonomous systems [54, 22, 57]. One line of works leverages attention visualisation \u2013 either to directly identify salient regions of input images that are important for driving decision-making [37, 5, 7] or to assist feature aggregation for downstream motion planning tasks [55, 15, 77, 63]. Another line of work uses intermediate auxiliary tasks such as semantic segmentation [25, 32], object detection [16, 31], and affordance prediction [68, 45] which help to decoder latent representations to human-understandable representation. While these methods provide explainable mechanisms by associating decision-making processes with semantic or visual representations, they are not readily comprehensible by the general users for the purpose of fostering trust and confidence.\nAlternatively, recent research shows promise in utilising natural language explanation. Several works develop specialist explainers [38, 41] using attention alignment in visual input and textual generation for grounded driving action explanation. ADAPT [33] uses a vision-language transformer with separate decoder for caption generation alongside control signal pre-diction. More recently, several works explore the potential of Multi-modal Large Language Models (MLLMs). Works such as, DriveGPT4 [78], Lingo [54], and DrivingMLM [76] have shown promising potential in general question-answering for driving and action planning. However, a common obstacle in both specialist and MLLM-based generalist models is the scarcity of data due to expensive annotation costs and significant domain gaps between different datasets, making the development of a robust and generalisable model an extremely challenging task. In our work, we aim to overcome these obstacles by employing a more robust inference paradigm of retrieval-augmented in-context learning to bridge domain gaps and circumvent the need for annotations in new domains."}, {"title": "B. Multi-Modal Large Language Model", "content": "Recent advancements in Large Language Models (LLMs) have paved the way for the emergence of Multi-modal Large Language Models (MLLMs) [1, 70]. Benefiting from scalable"}, {"title": "C. In-Context Learning and Retrieval-Augmented Generation", "content": "While LLMs demonstrate strong generative and reasoning capacity, there are still several issues associated with their output, such as hallucination [29], and slow knowledge up-dates [34]. In-context Learning (ICL) [10, 18] has emerged as"}, {"title": "III. METHOD", "content": "RAG-Driver is a retrieval-augmented, multi-modal large language model (MLLM) for generalisable explainable end-to-end driving. Its multi-tasking capabilities encompass three key areas: (1) Action Explanation, providing a human-understandable driving action description; (2) Action Justification, elucidating the rationale behind specific driving actions; and (3) Next Control Signal Prediction, forecasting upcoming control signals in response to the driving conditions. As shown in Fig. 3, it is composed of two primary components: a unified perception and planning unit built upon an MLLM backbone and a memory unit built upon a hybrid vector and textual database. These components interact through a retrieval engine, enabling robust multi-modal in-context learning (ICL) during decision-making."}, {"title": "A. Multi-modal Large Language Model Architecture", "content": "Following the successful MLLM paradigm of Video-LLaVA [46], we align visual and language embedding through visual instruction tuning. We leverage a pre-trained video encoder and LLM and then inject the video embedding into LLM through a MLP projector to build a fully differentiable MLLM.\nVideo Encoder We adopt the pre-trained LanguageBind video encoder [85] as our frozen visual backbone $f_v$, which is based on a ViT-B/32 vision transformer [19]. As shown in Fig. 4, given an input video frame sequence $V_i = \\{v_1, v_2, ..., v_k\\} \\in \\mathbb{R}^{3 \\times k \\times 224 \\times 224}$, we first split the video into multiple temporal sequences, each consisting of patches that share the same spatial location across different frames. These patches are then transformed through a linear projection for a vision transformer to output video embedding $z_{vo} \\in \\mathbb{R}^{2048 \\times 1024}$. The video encoder is pre-trained with video-language contrastive learning (i.e. CLIP4clip [49]) without further fine-tuning.\nCross-Modality Projector We then leverage a two-layer MLP to project and align the encoded video embedding $z_{vo}$ with language token embeddings $z_t \\in \\mathbb{R}^{2048 \\times 4096}$.\n$f_p(z_{vo}) = GELU (W_2 \\cdot GELU (W_1 \\cdot Z_{vo}))$     (1)\nIn particular, the projector $f_p$ takes form in Eq. (1), where we use GELU [26] as an activation function. We train the projector with a two-stage training strategy as detailed in Sec. III-B.\nLarge Language Model Backbone Finally, the LLM takes in both aligned video embedding $z_v$ and language embedding $Z_t$ of textual context information and task instruction to predict both textual action explanation and numerical control signal. We adopt Vicuna 1.5 7B [84], which is instruction-tuned based on LLaMA2 [72] as our LLM backbone. For decoder-only LLM conditioned on the multi-modal contextual prefix $Z_{1:n} = [Z_v, z_t]$ of length N, the joint probability of output $X_{n+1:L}$ as in Eq. (2), where $P_\\theta$ is transformer-based LLM backbone parameterized by $\\theta$.\n$P(X_{n+1:L} | Z_{1:n}) = \\prod_{l=n+1}^L P_\\theta(X_l | X_{1:l-1}, Z_{1:n})$     (2)\nEach output token $x_l$ is then sampled auto-regressively based on previous output and context and finally decode to language space through a text de-tokenizer."}, {"title": "B. Training Strategy", "content": "Following the visual instruction tuning paradigm [48, 46], we employ a two-stage training strategy to progressively enable cross-modality alignment and multi-task driving capacity. In both stages, we leverage the same next-token prediction cross-entropy loss as in Eq. (3) aiming to maximize the conditional"}, {"title": "C. Retrival-Augmented In-context Learning", "content": "Another critical component of the system is the memory unit, which consists of a database and a retrieval engine. The database incorporates vectorised video embedding $z_{vo}$, which is extracted with the same video encoder as in Sec. III-A and control signals $c \\in \\mathbb{R}^{28}$ directly from sensor recording. Each vector is uniquely associated with the corresponding human expert textual explanation and justification from train samples as in Sec. III-B.\nRetrieval Mechanism To perform the retrieval, we first leverage a lightweight MLP projector of the same structure as in Eq. (1) to project the heterogeneous video and control signal embedding into the same hybrid embedding $s \\in \\mathbb{R}^{1024}$ through metric learning [27]. In particular, we adopt a triplet loss with Euclidean distance as shown in Eq. (4):\n$L_{Tri}(a, p, n) = max(||a - p||_2 - ||a - n||_2 + margin, 0)$     (4)\nwhere the positive (a,p) and negative pairs (n,p) between hybrid embedding s are selected based on text similarity (i.\u0435., TF-IDF Score) of driving action and justification in the BDD-X training set as we aim to form the metric space such that the scenarios lead to similar driving actions close together and vice versa. This approach addresses the limitations of relying solely on visual similarity, which we have empirically found can result in sub-optimal performance (Sec. IV-D), and also solves the problem of heterogeneous sensor inputs that are hard for similarity comparison. We then perform retrieval through an efficient vector similarity search. Given a query vector $s_q$ the cosine similarity between the query vector and each vector in the database is computed as follows:\n$S_c(s_q, s^{(i)}) = \\frac{s_q \\cdot s^{(i)}}{||s_q|| ||s^{(i)}||}$     (5)\nSubsequently, we consistently select the two most relevant driving samples based on this similarity score. These samples represent the entire reasoning process, from contextual information to question-answer pairs, as illustrated in Fig. 5.\nRetrieval Augmented In-Context Learning (RA-ICL) To perform the RA-ICL, we prefix retrieved samples before the current query, facilitating an implicit gradient descent through the meta-optimiser of LLM, as proved in [17]. This approach is also applicable to MLLM with architecture specified in Sec. III-A. For a given transformer-based pre-trained MLLM modeling the prefix-conditioned output probability as in Eq. (2), consider one head of the multi-head attention in a single transformer block as follows:\n$F_{ICL}(Z_{1:n}) = Attention(Q, K, V)$\n$=W_v [Z_{icl}; z_q] Softmax \\bigg(\\frac{(W_K Z_{1:n})^T (W_Q Z_{1:n})}{\\sqrt{d_i}} \\bigg)$     (6)\nwhere $Z_{1:n} = [Z_{icl}; z_q]$ represented the conditional prefix consist with ICL example $z_{icl}$ and current query embeddings $z_q$, respectively. $W_{Q,K,V} \\in \\mathbb{R}^{d_i \\times d_o}$ is the linear transformation on query, key, and value in attention block, respectively. Now, in [17] in-context learning was shown to effectively achieve a meta-optimisation to implicitly update the model with an estimated meta-gradient. We provide in our work a new, alternative derivation of this, supported by more recent work in [40] on the following softmax-free linear attention expressions\n$F_{ICL}(Z_{1:n}) = W_v [Z_{icl}; z_q] (W_K [Z_{icl}; z_q])^T W_Q Z_{1:n}$     (7)\nThis can be simplified, with a more detailed derivation in Apx. D, as\n$F_{ICL}(Z_{1:n}) = (\\Delta W_{ICL} + W_{ZSL}) W_Q Z_{1:n}$     (8)\nwhere we separate out the terms $W_{ZSL}$ independent of the ICL examples and dependent solely on the current query from those $\\Delta W_{ICL}$ dependent on the ICL examples, given by:\n$\\Delta W_{ICL} = \\sum_i W_v Z_{icl,i} (W_K Z_{icl,i})^T$\n$W_{ZSL} = W_v Z_q (W_K Z_q)^T$\n(9)\nNow, with more detail in Apx. D, a forward-pass\n$F(x) = (W_0 + \\Delta W)x$     (10)\nthrough a linear layer F after its weights $W_0$ have been updated by $\\Delta W$ has the weight updates coupled to the input in the form\n$\\Delta W x = \\sum_i \\eta \\frac{\\partial L}{\\partial y} y_i x x_i^T$     (11)"}, {"title": "IV. EXPERIMENTS", "content": "We empirically evaluate the proposed Retrieval-augmented In-Context Learning (RA-ICL) framework within the Multi-modal Large Language Model (MLLM), targeting explainable driving applications. We aim to validate its efficacy in general driving scenarios with a focus on two main aspects: (1) explainability in driving action explanation and justification. (2) Control Signal Prediction. We conduct experiments with the BDD-X [38] dataset, which is a widely adopted benchmark in explainable driving, comprising 77-hour videos across the US under different road and weather conditions. We customize the format as shown in Fig. 5, resulting in 16,803 and 2,123 video question-answering pairs for training and testing, respectively. More importantly, we further explore the transfer learning capacity of zero-shot generalisation in unseen environments. We leverage customised dataset comprising 58 testing question-answering pairs, recorded in London, UK, presenting a significant distribution shift from the BDD-X dataset.\nBenchmark Settings For all experiments, we train the MLLM using the BDD-X training split. Subsequent evaluation on general explainability and control signal prediction capabilities tests are conducted on the BDD-X test split with the BDD-X training split as the memory database. For the transfer learning experiments, we employ the same foundational model and test it on the Spoken-SAX but the memory database is constructed using the BDD-X training split for zero-shot generalisation.\nImplementation Detail For each of the driving videos, we uniformly sample the video to 8 frames and resize it to 224 \u00d7 224 for all frames. For MLLM, we train the model for one and two epochs in the pre-training and fine-tuning stages, respectively. For the embedding projector, we train the model for 300 epochs. Further experiment implementation details are provided in Apx. B.\nEvaluation Metric For the driving action description and justi-fication tasks, we use the same metrics as [33] including 4-gram BLEU (B4) [59], METEOR (M) [4], and CIDEr (C) [73]. These metrics aim to evaluate text generation quality, with BLEU focusing on n-gram precision, METEOR incorporating semantic and syntactic nuances, and CIDEr emphasizing consensus and"}, {"title": "B. Explainability in Driving Action and Justification", "content": "We begin by evaluating the quality and accuracy of ex-planations and justifications for driving actions separately. As shown in upper part of Tab. I, our method demon-strates comparable performance to the state-of-the-art specialist method ADAPT [33], a characteristic not observed in previous MLLM-based methods. In particular, when compared with DriveGPT4 [78] which also uses MLLM with a similar architecture and number of parameters but incorporates the extra LLaVA-150K dataset [48] for visual instruction tuning, our approach, relying solely on the BDD-X dataset, outperforms it in terms of explainability. This is evidenced by an average performance improvement of 10.8% across all metrics. This underscores the effectiveness of ICL in enhancing the emergent reasoning capabilities of MLLMs."}, {"title": "C. Control Signal Prediction", "content": "We next evaluate the accuracy of control signal predictions for Course (i.e., turning angle) and Speed. As indicated in Tab. II, our method surpasses others in open-loop control accuracy across various tolerance ranges and in terms of RMSE, significantly outperforming the baseline approach. In particular, when compared to the state-of-the-art DriveGPT4, which also uses the same visual input combined with past control signals for autoregressive prediction, our method stands out by implementing retrieval-augmented ICL examples. This indicates the analogy in the overall reasoning process provided by the ICL examples also contributes to the improvement in numerical control signal prediction."}, {"title": "D. Ablation Study on Retrieval Strategy", "content": "We perform a more comprehensive ablation study to evaluate the efficacy of our proposed retrieval-augmented in-context learning. We first aim to investigate the similarity metric for retrieval. In particular, we compare the use of visual similarity (i.e., video embeddings only) with hybrid similarity (i.e., hybrid video and control signal projected embedding Sec. III-C). Our empirical findings indicate suboptimal performance when using visual similarity, possibly because it tends to prioritize ICL examples that are most perceptually similar, rather than effectively demonstrating the reasoning process. By fine-tuning the embeddings, we not only harness the potential of heterogeneous multi-modal sensor input but also enable more effective ICL example retrieval."}, {"title": "E. Generalisation Capacity", "content": "One of the critical capacities of autonomous systems is to generalise to unseen environments out of its training distribution. However, in the domain of explainable driving, we observe the existing methods are unable to perform such generalisation, which poses challenges for their deployment. As shown in lower part of Tab. I, ADAPT and the base MLLM (i.\u0435., training without ICL) reveal dramatic performance degradation to the in-distribution situation. However, our method leverages ICL learning examples to demonstrate a significant performance boost with a large margin. Note that even though the memory database is constructed with BDD-X, the RA-ICL can still perform generalisation in a zero-shot manner. This is potentially due to the robustness of the hybrid retrieval process, where samples with less distribution shift can still be selected to serve as effective ICL examples."}, {"title": "F. Qualitative Demonstration", "content": "We also demonstrate a series of quantitative examples comparing the driving action explanation and justification provided by human ground truth and prediction from our method. As shown in Fig. 6, we observe RAG-Driver produces robust intelligible action explanation and justification under different environments (i.e. night time and adversarial weather) with a control signal close to the human driver record. More importantly, in the out-of-distribution setting Spoken-SAX as indicated by the clear visual difference, we observe the prediction also produces human-understandable answers, qualitatively validating the exceptional zero-shot generalisation capacity."}, {"title": "V. LIMITATIONS AND FUTURE WORK", "content": "This work aims to develop a generalisable explainable driving commentator using a Machine Learning Language Model (MLLM), addressing a significant obstacle that has hindered deployment: the poor generalisation capacity. However, several problems still need to be addressed. For instance, the prevalent issue of hallucination in MLLM, while mitigated, is still observed. We hypothesize that this is due to the limited general video understanding capacity, as the video encoder only processes 8 frames per video. Also, due to the limited access"}, {"title": "APPENDIX", "content": "Scale of MLLM While our Multi-modal Large Language Model (MLLM) has exhibited impressive capabilities in visual reasoning and planning for driving tasks, it's worth noting that it comprises only 7 billion parameters. This size is relatively modest when compared to more well-known models such as GPT4-V [1] and Gemini [70], which boast a significantly larger parameter count and demonstrate superior, near-human levels of visual understanding and reasoning. In various related fields such as visual question answering, problem-solving, and interactive dialogue, a clear trend has been observed by researchers: the scale of the model's parameters and the breadth of training data sources are pivotal. Performance improvements are typically seen in tandem with model scaling. Based on this trend, we expect similar advancements in the realm of driving applications. A larger model could further enhance the capabilities of MLLM in driving scenarios.\nNumber of In-Context Learning Examples During training and inference, we provide 2 ICL examples for each query. This is due to the limitation of the context window size of the LLM backbone of 2048. With the recent improvement in LLM context window size, we expect to see a more flexible adoption of ICL examples.\nEmbedding Projector We leverage a three-layer MLP as the embedding projector to fuse the video 1 \u00d7 1024 and control signal 1 \u00d7 28 embedding into a hybrid embedding. The architecture of the lightweight projector is a four-layer MLP with GELU activation. It has an input dimension of 1052 and an output dimension of 1024. The margin used in triplet loss is 0.5. We train the model for 200 epochs with a learning rate of le - 5 with Adam optimiser.\nMLLM Backbone We use a learning rate of 2e - 5 with a cosine scheduler. We use a batch size of 4 with a gradient accumulation step of 2 on 8 A100 GPUs, which leads to an effective training batch size of 128. We use a warm-up strategy in the first 5 epochs with a warm-up ratio of 0.03. We train the model for 2 epochs."}, {"title": "D. Linear Layer Parameter Update Derivations", "content": "Consider a forward-pass through a linear layer F after its weights $W_0$ have been updated by $\\Delta W$\n$y = F(x) = (W_0 + \\Delta W)x = W_0x + \\Delta Wx$\nThe weight update itself is expressed as\n$\\Delta W = \\eta \\frac{\\partial L}{\\partial y} y x^T \\Rightarrow \\Delta Wx = \\eta \\frac{\\partial L}{\\partial y} y xx^T$\nwhere $x_i, y_i$ are the input and output to the layer that resulted in the weight update. Now, if we in fact have optimised over a mini-batch of input-outputs $x_i, y_i$ we have\n$\\Delta W = \\eta \\sum_i \\frac{\\partial L}{\\partial y} y x^T \\Rightarrow \\Delta Wx = \\sum_i \\eta \\frac{\\partial L}{\\partial y} y xx^T$\nTherefore we have a weighted sum of dot products, which is akin to an attention mechanism. Indeed, from Eq. (6) we can apply softmax-free linear attention expressions such as in [40] for\n$W_v [Z_{icl}; z_q] softmax \\bigg(\\frac{(W_K Z_{1:n})^T (W_Q Z_{1:n})}{\\sqrt{d_i}} \\bigg)$\n$\\rightarrow W_v [Z_{icl}; z_q] (W_K [Z_{icl}; z_q])^T W_Q Z_{1:n}$\nMultiplying the linear attention matrices through the stacked in-context and query embeddings we have\n$W_v z_{icl}(W_K z_{icl})^T W_Q Z_{1:n} + W_v z_q(W_K z_q)^T W_Q Z_{1:n}$\nNow take out a common factor of $W_Q Z_{1:n}$ for\n$=(W_v z_{icl}(W_K z_{icl})^T + W_v z_q(W_K z_q)^T) W_Q Z_{1:n}$\nand put $\\Delta W_{ICL} = W_v z_{icl}(W_K z_{icl})$ and $W_{ZSL} = W_v z_q(W_K z_q)^T$ as the terms both pre-multiplying $W_Q Z_{1:n}$.\nNote that $W_{ZSL}$ is independent of the in-context terms (depending only on the query). Now, in $\\Delta W_{ICL}$ we in fact have a set of in-context retrieved samples $z_{icl,i}$ such that\n$W_v [z_{icl,0}, z_{icl,1},...] (W_K [z_{icl,0}, z_{icl,1},...])^T$\n$\\rightarrow \\Delta W_{ICL} = \\sum_i W_v Z_{icl,i} (W_K Z_{icl,i})^T$\nFinally, by inspection of similar dot-product expressions $\\frac{\\partial L}{\\partial y} W_v Z_{icl,i}(W_K Z_{icl,i}) \\leftrightarrow \\eta y xx$ we note that we match the form for the linear layer above\n$(W_{ZSL} + \\Delta W_{ICL}) W_Q Z_{1:n} \\leftrightarrow (W + \\Delta W)x$\nThis can therefore be interpreted to say that the output of the attention is adjusted in a meta-optimal way to conform to the samples provided as input context, much like gradient descent on the linear layer would adjust that layer to conform to the mini-batch training data, but crucially in the case of RAG-Driver, without supervision."}]}