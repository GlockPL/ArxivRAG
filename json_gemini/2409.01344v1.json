{"title": "Pairing Analogy-Augmented Generation with Procedural Memory for Procedural Q&A", "authors": ["K Roth", "Rushil Gupta", "Simon Halle", "Bang Liu"], "abstract": "While LLMs in the RAG paradigm have shown remarkable performance on a variety of tasks, they still under-perform on unseen domains, especially on complex tasks like procedural question answering. In this work, we introduce a novel formalism and structure for manipulating text-based procedures. Based on this formalism, we further present a novel dataset called LCStep, scraped from the LangChain Python docs. Moreover, we extend the traditional RAG system to propose a novel system called analogy-augmented generation (AAG), that draws inspiration from human analogical reasoning and ability to assimilate past experiences to solve unseen problems. The proposed method uses a frozen language model with a custom procedure memory store to adapt to specialized knowledge. We demonstrate that AAG outperforms few-shot and RAG baselines on LCStep, RecipeNLG, and CHAMP datasets under a pairwise LLM-based evaluation, corroborated by human evaluation in the case of RecipeNLG.", "sections": [{"title": "1 Introduction", "content": "Procedural knowledge, or \u201cknowledge-how\", is a form of knowledge involving the ability to do something, as opposed to prepositional knowledge, which is the awareness of a fact (\u201cknowledge-that\") (Pavese, 2022). Philosophers debate whether procedural knowledge is always reducible to prepositional knowledge, but the functional distinction is regardless significant in the study of LLM-based agents, which are expected to not only understand facts about their environment but also plan and reason about actions in that environment.\nTo that end, efforts have been made both theoretically (Georgeff and Lansky, 1986) and empirically (Huang et al., 2022) to enable information systems"}, {"title": "2 Related Work", "content": "Built upon the traditional RAG paradigm (Lewis et al., 2021), AAG differs from the conventional RAG in three key aspects: analogical reasoning, the iterative refinement workflow (Section 3) and application in knowledge-intensive procedural QA. An extensive survey covering the status-quo of RAG research can be found in (Gao et al., 2024).\nStandard Prompting Methods: Zero-shot prompting (Chang et al., 2024), few-shot prompting (Brown et al., 2020), and RAG (Lewis et al., 2021) are the most common LLM-based approaches to natural language generation. Few-shot prompting requires a fixed set of labeled examples, and RAG requires some supporting set of documents, although these documents may not be of the same form as the final text being generated by the system, e.g. when augmenting with a set of general-knowledge Wikipedia articles to answer factual questions.\nAnalogical Reasoning in LLMs: Many papers attempt to elicit reasoning in LLMs by various prompting methods (Zhou et al., 2024; Wang et al., 2023a). Specific to analogical reasoning, early attempts show by case study that LLMs can do analogical reasoning just like humans (Webb et al., 2023). Recent works explore analogy generation and analogy reasoning with knowledge graphs on LLMs (Yuan et al., 2023; Bhavya et al., 2023, 2022). These works are applied to prepositional knowledge, and rely on large-scale external knowledge bases to store entity relationships to perform analogical reasoning. More closely related to our work are Yu et al. (2023) and Yasunaga et al. (2024), in which they prompt the model to propose analogous problems, whose solutions (also created by the model) are then related by analogy to the original problem in order to inform the final solution. In our work, we retrieve real solutions to analogous real problems from memory, which benefits performance especially in knowledge-intensive domains.\nIterative Pipeline: Numerous recent works based on the RAG paradigm undertake an iterative workflow to generate the response to an input query. Asai et al. (2023) train a separate LM to predict reflection tokens like 'retrieve', 'relevant', 'irrelevant', etc. within the response, and these tokens determine subsequent control flow to accordingly generate or refine subsequent chunks of the response. On the contrary, (Shao et al., 2023) intersperse Generation-Augmented Retrieval (GAR) and RAG to sequentially expand the search query in the memory and obtain relevant information to plug into the final response. Our work comes close to (Shao et al., 2023) where we expand the search query to"}, {"title": "3 Our Method", "content": "We describe the details of our analogy-augmented generation (AAG) system in the sections below. Our system extends the traditional RAG system (Lewis et al., 2021) and introduces three novel modules to perform better on the task of procedural question-answering: 1) procedural memory store, a novel representation of memory, designed specifically for procedures; 2) query rewriting and summarization: rewrites the input query as a set of questions, answers to which can be procured through analogous procedures in the memory; and 3) iterative refinement with self-critic: an LLM-based critic evaluates the candidate response and suggests edits, which are then performed by the 'LLM edit performer' module. A schematic view of our system can be seen in Figure 2. We now describe each of these novel modules in more detail, followed by an end-to-end description of the overall control flow. Please refer to the appendix for detailed prompts used by each module (see A.2)."}, {"title": "3.1 Procedural Memory", "content": "We define a procedure as $(x, y, (s_1,..., s_k))$ where:\n\u2022 x is an input string,\n\u2022 y is an output string, and\n\u2022 $(s_1,..., s_k)$ is a sequence of ordered steps, each of which is a string.\nThe task of procedural generation then is to generate $(s_1,..., s_k)$ given x and y.\nThis definition is simpler than other conceptions of procedures that use directed acyclic graphs (Georgeff and Lansky, 1986), depending on the text of the steps to define any non-linear control flow. It also allows procedures to be composable: two procedures $(x, y, (s_1, ..., s_k)), (y, z, (t_1, . . ., t_e))$ can be composed into a procedure $(x, z, (s_1,..., s_k,t_1,...,t_e))$ that produces result z given input x.\nThis loose formalism allows systems to easily benefit from text embeddings as they see fit, while also relying on their sequential structure. In the custom procedure store for AAG, this is done by"}, {"title": "3.2 Query Rewriting", "content": "The input to the AAG system is comprised of the goal to be achieved (the procedure output) and the resources we have access to (the procedure input). Using this input query directly to search the memory can yield insufficient supporting information when the necessary information doesn't use words similar to words in the query. Humans solve this problem by linking and assimilating knowledge from relevant past concepts and experiences (Piaget, 1968; Vosniadou and Ortony, 1989), composing a solution by recombining components of past tasks. Most recently, Yasunaga et al. (2024) demonstrated the merits of analogical prompting where the LLM is asked to recall relevant analogous problems to the problem in hand and then leverage them to solve the given problem.\nInspired by these works, we rewrite the input query to the system as a set of questions required to prepare an appropriate procedure for achieving the specified goal. We prompt the LLM to first generate a high-level outline of the procedure that will lead to the specified goal, like in HyDE (Gao et al., 2022), and then based on these high-level steps suggest relevant questions which can be answered with relevant procedures in the memory. We extend the query rewriting prompt used by Ma et al. (2023) for our case and generate a maximum of N = 4 queries for a given input query. For example, given the input query: \u2018create a custom LLM agent that can interact with users in a conversational manner using a specific language style using an LLM, Serp API', the following 4 queries are generated by the query re-writer:\n\u2022 How to train a language model for a specific language style?\n\u2022 Best practices for integrating a language model into a conversational interface?\n\u2022 How to use the Serp API to extract relevant information from search engine results?\n\u2022 Strategies for optimizing performance of a custom LLM agent in conversational interactions?"}, {"title": "3.3 Summarization", "content": "Each of the queries received from the query rewriting module is individually used to retrieve from the procedure memory. The challenge, however, is that the retrieved procedures often contain the required information hidden between irrelevant steps. Thus, motivated by this observation, the summarization module prompts the LLM with the question and the retrieved procedures, asking it to generate a summary of the retrieved procedures as an answer to the question. This module abstracts out the unnecessary details and includes only the useful information from all the retrieved procedures, thus overcoming the issue of duplicates in retrieved procedures for different queries as well. Each question along with its summary is stacked together as \"Q:  A: \" to build up high quality context which is used by both the Update Response and the Edit performer module."}, {"title": "3.4 Iterative Refinement with Self-Critic", "content": "LLMs have been shown to act as powerful self-critics for their own outputs, evaluating the outputs across several dimensions and suggesting feedback crucial to improving their output (Madaan et al., 2023; Saunders et al., 2022). We augment the AAG system with an iterative refinement procedure guided by the same LLM acting as a critic for the output it has generated. As seen in Figure 2, LLM critic takes the output of the Update Response module, evaluates if any edits are required, and suggests them as a bulleted list. In case no edits are required, the critic appends the string \u2018NO UPDATE REQUIRED' to its response. The suggested edits are then performed by the LLM with a prompt including the list of edits, context from the summarizer, and the current version of the output procedure steps. This edited version is then re-evaluated by the LLM critic and the cycle continues for a maximum of T = 3 cycles. The output at the end of the iterative refinement is the final answer of the AAG system to the input query."}, {"title": "3.5 End-to-End Workflow", "content": "On a very high level, the AAG system takes as input a query by the user and outputs a bulleted list of steps that will lead the user to the desired goal. Using the terminologies defined in Section 3.1, the"}, {"title": "4 Experiments", "content": "To demonstrate the effectiveness of analogy-augmented generation (AAG) for generating procedures, we apply it to three procedural knowledge datasets. Here the task is to generate the list of steps, given the input text and output text.\nRecipeNLG is a corpus of more than 2 million food recipes collected from the internet (Bie\u0144 et al., 2020). Under our procedural knowledge framework, we construct a procedure by using the recipe title as output, the ingredients as input, and the directions as the procedural steps. For our experiments, we select a random subset of 10,000 recipes from this dataset. From these we select 2,000 test examples and 1,000 validation examples.\nLCStep dataset is a self-curated dataset where the goal is the procedure output. The dataset was prepared by scraping the LangChain python docs and filtering out 180 tutorials and guides. For the 180 tutorials/guides, we prompted GPT-4 to extract a list of high-level steps necessary to accomplish the goal and rate those extracted procedures using a list of criteria. Those not matching the criteria were revised both manually and through GPT-4 before the final human filtering (more details in the appendix, see A.1). We sorted the 276 procedures in LCStep by increasing length and selected the final (longest) 56 examples as the test set, with the preceding 27 examples as a validation set. We selected the longest examples for the test set because we want to observe AAG's ability to leverage simpler procedural knowledge when proposing steps for more complicated tasks.\nCHAMP is a dataset of 270 competition-level math problems annotated with concepts, hints, and, step-by-step solutions (Mao et al., 2024). For the procedure generation task, we use the problem statement as the output, the hints and problem category as input, and the step-by-step solutions as the procedure steps. We also append \u201cThe answer is answer\u201d as a final step in the procedure. We randomly shuffle the problems and choose 54 test examples and 27 validation examples. Random shuffling is important to ensure diversity so that each data split contains problems of all types.\nThe remaining examples in each of the dataset form the procedural memory for that respective dataset."}, {"title": "4.1 Baselines", "content": "We compare our system against widely-used natural language generation approaches that involve prompting an LLM. For our experiments, we use gpt-3.5-turbo-0125 from OpenAI as the language model for all approaches.\nZero-Shot: We prompt the language model to generate the steps for a procedure, given the procedure's input and output.\nFew-Shot: We sample k = 3 random procedures from the training set, and include those in the prompt after the task instructions, which are the same as in the zero-shot case.\nRAG: We retrieve k = 3 procedures based on the provided input and output, and then use those 3 procedures in the prompt like the few-shot case."}, {"title": "4.2 Evaluation Methodology", "content": "For all our plots, we perform a pairwise evaluation between the proposed AAG system and the concerned method using the LLM. The LLM is prompted to evaluate the two procedures on\n\u2022 their ability to accomplish the specified goal,\n\u2022 the clarity and flow of the steps and the level of detail, and\n\u2022 using only the resources specified in user input.\nThe prompt example can be found in the appendix (see A.2.1). Note that ground-truth steps are not given as input in the prompt due to several reasons: 1) there can be multiple correct procedures to achieve a single task, 2) the ground truth procedures are seldom incomplete and noisy, specifically in recipes as they are scraped from the internet, and 3) to prevent biasing the LLM towards only matching the keywords in the generated and ground-truth procedure steps. To further control for randomness and de-bias the LLM evaluation towards ordering of procedures in the prompt, we run a total of 10"}, {"title": "4.3 System Design Choices", "content": "The AAG system has no trainable parameters and simply relies on frozen LLMs. The temperature of the OpenAI language model is kept to the default value of 0.7 for all the results and the evaluations. Lack of any training makes the AAG system flexible to be used with even the commercial black-box LLMs like GPT-4 etc. We use the all-mpnet-base-v2 model from sentence-transformers (Reimers and Gurevych, 2019) as the embedding model for the objects in the procedural memory Weaviate store, creating dense 768 dimensional embeddings for each procedure object. All the three data fields of a procedure (input, output, and steps) are embedded together in one embedding."}, {"title": "4.4 Results", "content": "Figure 3 shows the result of pairwise comparison of AAG with each of the three baselines discussed in Section 4.1: zero-shot, few-shot and RAG. The three bars for each method describe the 'win', 'loss' and 'ties' as explained in Section 4.2. A higher value of the 'win' bar for a given method and dataset means that AAG was preferred over that method on that dataset. From Figure 3, it can be clearly observed that AAG outperforms its counterparts for all methods on all datasets. For RecipeNLG and LCStep, it does so by a significant margin, highlighting the efficacy of our method. Mathematical problem-solving remains difficult for LLMs, and as such the CHAMP results are less conclusive. Refer to Figures 5 and 6 for a qualitative side-by-side comparison of the generated procedures for one example. For completeness, we provide details of the re-written queries for these examples and the retrieved procedures corresponding to each re-written query in appendix (see Figure A.3)\nWe also surveyed human raters to compare generations from AAG and RAG for 50 random test examples from RecipeNLG, to support our LLM-based evaluation. Each pair of generations was evaluated by 3-6 human raters. The results appear in the leftmost plot of Figure 3, and support the strong LLM evaluation results for RecipeNLG."}, {"title": "4.5 Ablation Study", "content": "We perform a rigorous ablation study of our proposed AAG architecture. We experiment with removing the three architectural contributions in all permutations, i.e. the query re-writer, summarizer and the self-critic guided iterative refinement. We summarize each of the ablation method below:\n\u2022 AAG-NOSUM: This represents the AAG system but without the summarizer. Instead of creating summary answers for each re-written query, we simply concatenate all the retrieved procedures for all the queries, removing the duplicates and form one big context with all the procedures. This context is used in place of the summary based context in the AAG pipeline, keeping everything else same.\n\u2022 AAG-NOCR: This represents the AAG system but without the self-critic guided iterative refinement. Every other pipeline component still remains. The output of the Update Response module is considered as the final output of the system.\n\u2022 AAG-NOSUM-NOCR: This represents the AAG system but without both summarizer and the self-critic guided iterative refinement. The Update Response module receives the context as in AAG-NOSUM and its output is considered as the final output of the system.\n\u2022 AAG-NOQR: This represents the AAG system but without the query re-writing module. Since there are no re-written queries, there is no summarizer and Update Response module in this method. The response from RAG is directly fed into self-critic based iterative refinement, whose output is considered as system's final output.\nWe also conduct an ablation study varying the number of re-written queries N and include its observations in Section A.4 of the appendix.\nResults: Figure 4 displays the results for the pairwise comparison between AAG and each of the ablation methods above on all the three datasets. From the figure, it can be clearly observed that AAG performs better than AAG-NOCR with a large margin on all the three datasets, highlighting the importance of using the self-critic based iterative refinement for better performance. Moreover, AAG-NOSUM bars prove that removing the summarizer module from the AAG pipeline hampers the performance, thus performing worse than the AAG. These findings, along with the bars for AAG-NOSUM-NOCR, further strengthen the claim that the combination of summarizer and the critic is crucial to high performance of the AAG system."}, {"title": "5 Conclusion", "content": "We have introduced a simple framework for procedural knowledge, created a novel LLM system that leverages this framework, and demonstrated its increased performance over standard RAG, especially when generating procedures in domains unfamiliar to the LLM. The case where AAG was shown to harm performance over the baseline was for RecipeNLG, a domain extremely familiar to general-purpose LLMs like GPT-3.5. The system performed especially well on the LCStep dataset we curated, demonstrating that AAG can better augment a frozen LLM with the knowledge needed to answer questions on unseen domains. These results show that structured procedural memory, query rewriting, response summarization, and it-"}, {"title": "Limitations and Future Work", "content": "This framework deliberately ignores procedure structure that is non-linear, meaning that a non-linear procedure must express its non-linearity in the text of the steps. This is a reasonable assumption for many tasks performed by humans, but not for computer algorithms.\nOur AAG method as presented here does not yet leverage all of the structure afforded by our procedure definition. In future work we plan to more tightly couple our system with the formal structure we have proposed, by leveraging the composable nature of our procedure definition to perform finer-grained search and reasoning over sub-procedures. This may involve a sort of subroutine representation similar to the skill library from Voyager (Wang et al., 2023b)."}, {"title": "Ethical Considerations", "content": "The use of automated systems to generate and execute plans can and will result in unintended consequences, whether humans are in the loop or on the loop (Leins and Kaspersen, 2021). In addition, automating actions generally done by humans (here planning and reasoning) has the social effect of anthropomorphizing these systems, leading to a gradual shift in accountability from human stakeholders onto the systems themselves (Weidinger et al., 2021).\nThe domains which we have applied our system to in this work do not constitute immediate risks. For LCStep, the generated steps cannot easily be converted into code, although this could become increasingly easy as planning-based code generation continues to improve (Jiang et al., 2024). Mathematical problem solving poses no real-world risks.\nThanks to Google's AI Overviews feature, there has already been at least one instance of a generative AI system producing a food recipe that would be harmful if followed (Robison, 2024).\u00b9 While it"}, {"title": "A Appendix", "content": "A.1 LCStep Dataset Creation\nLCStep contains three sets of documents: API reference, conceptual documentation, and procedures. In this paper, we only used the procedures, but we plan to leverage the supporting material in future systems. See Figure 7 for a diagram of the process of generating the LCStep data.\nAs Langchain was unstable and changing quickly at the time we collected the data, we made sure to capture the state of the code and documentation at version 0.0.249."}, {"title": "\u0391.1.1 API Reference", "content": "We generate the API reference material from the source files in the LangChain GitHub repository using Sphinx. These files contain descriptions of all APIs in the Python package, including call signatures and argument descriptions. These files do not contain any usage examples or high-level explanation."}, {"title": "A.1.2 Conceptual Documentation and Procedures", "content": "We collected these resources by scraping the Langchain Python docs when v0.0.249 was the version live on the website. We manually filter out topic pages and stubs, leaving 228 documents. We then manually classified these into around 30 documents of conceptual documentation, and around 180 documents containing tutorials/guides.\nFor the 180 tutorials/guides, we prompted GPT-4 (see Listing 1) to extract a list of high-level steps necessary to accomplish the goal. We then prompted GPT-4 (see Listing 2) to rate those extracted procedures using a list of criteria. We found that this caught many mistakes where GPT-4 did not follow all the stated instructions. In those cases, we had the model revise the steps to meet the requirements, and then we manually checked the revised versions."}, {"title": "A.2 Full Example of AAG with Prompts", "content": "Here we'll take an example from the LCStep test set. See the attached code for the prompts adapted"}, {"title": "A.2.1 Pairwise Evaluation", "content": "For pairwise evaluation, the prompt in Listing 13 was used."}, {"title": "A.3 Details on Figures 5 and 6", "content": "Figures 5 and 6 emphasize the high-quality generations by AAG system as compared to the baselines. Distinctively, AAG includes a lot of additional details and information about the procedure steps. To motivate that the source of this additional knowledge is the retrieved procedures from memory and not LLM hallucinations, below is the list of re-written queries and correspondingly retrieved procedures for both the examples:"}, {"title": "A.4 Ablation for Number of Re-written Queries", "content": "In this study, we aim to investigate the performance of the AAG system on changing the hyperparameter for the number of re-written queries N. Traditionally, AAG uses 4 re-written queries but we experiment with 3 and 6 queries in this evaluation. Figure 8 shows the results for this ablation under the pairwise LLM evaluation. We note from the figures that while 4 queries is preferred than 3 or 6 for RecipeNLG and LCStep, CHAMP narrates a mixed story with both 3 and 6 queries performing better. We attribute this mixed result to the high complexity of the dataset overall and the general-purpose LLM's limitation to rephrase the question into meaningful queries. Thus, it can be safely concluded that this is not a very sensitive hyper-parameter for our proposed system and setting a value of 4 only increases the performance as compared to its counterparts."}]}