{"title": "Think Smarter not Harder: Adaptive Reasoning with Inference Aware Optimization", "authors": ["Zishun Yu", "Tengyu Xu", "Di Jin", "Karthik Abinav Sankararaman", "Yun He", "Wenxuan Zhou", "Zhouhao Zeng", "Eryk Helenowski", "Chen Zhu", "Sinong Wang", "Hao Ma", "Han Fang"], "abstract": "Solving mathematics problems has been an intriguing capability of large language models, and many efforts have been made to improve reasoning by extending reasoning length, such as through self-correction and extensive long chain-of-thoughts. While promising in problem-solving, advanced long reasoning chain models exhibit an undesired single-modal behavior, where trivial questions require unnecessarily tedious long chains of thought. In this work, we propose a way to allow models to be aware of inference budgets by formulating it as utility maximization with respect to an inference budget constraint, hence naming our algorithm Inference Budget-Constrained Policy Optimization (IBPO). In a nutshell, models fine-tuned through IBPO learn to \"understand\" the difficulty of queries and allocate inference budgets to harder ones. With different inference budgets, our best models are able to have a 4.14% and 5.74% absolute improvement (8.08% and 11.2% relative improvement) on MATH500 using 2.16x and 4.32x inference budgets respectively, relative to LLaMA3.1 8B Instruct. These improvements are approximately 2x those of self-consistency under the same budgets.", "sections": [{"title": "1 Introduction", "content": "Complex reasoning has been an intriguing ability of large language models (LLMs), with application in for example mathematical problem-solving (Cobbe et al., 2021; Hendrycks et al., 2021b; Lightman et al., 2023) or coding (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021a), which does not only require nature language comprehending but also logical and critical \"thinking\". An observation merged in the LLM reasoning literature is that longer reasoning traces often leads to improved reasoning soundness and correctness. The seminal work of chain-of-thought (CoT) (Wei et al., 2022) is an excellent example of how enriching reasoning details, by decomposing reasoning traces into steps, improves its problem-solving capability. CoT has been considered a standard technique in reasoning, recent works extend CoT by allow LLMs to expand its reasoning steps, by for example CoT with more steps (Jin et al., 2024) (as explicitly required by instruction), self-reflection/correction (Madaan et al., 2024; Zelikman et al., 2022; Yan et al., 2024; Qu et al., 2024), multi-turn reasoning (Kumar et al., 2024) or multi-agent debate (Liang et al., 2023; Pham et al., 2023) (as a heterogeneous case of multi-turn). It was conjectured that scaling the test-time compute or the reasoning length unleashes LLMs' potential for reasoning (Snell et al., 2024), which has been empirically verified by recent hype of ultra-long reasoning models, such as OpenAI-01 (Jaech et al., 2024) and DeepSeek-R1 (DeepSeek-AI et al., 2025). We'll later categorizes these type of responses as (standard) CoT responses, extended responses, and (ultra-)long responses, respectively, given the nature of their reasoning lengths.\nWhile scaling reasoning length is promising in problem-solving, advanced long reasoning-chain models show an undesired uni-modal behavior that trivial questions may require unnecessarily tedious long reasoning trace, an example is shown in Figure 1. This uni-modal behavior creates unnecessarily higher inference costs and increased carbon footprints (Henderson et al., 2020; Anthony et al., 2020). To partially address this, we study how to enable multi-modal behavior for reasoning models in a way the length of reasoning traces are automatically adjusted according to the hardness level of the queries. From the aspect of query-adaptive reasoning length, some heuristic methods (e.g. Aggarwal et al., 2023; Xu et al., 2024a; Wang et al., 2024) have"}, {"title": "2 Algorithm Design", "content": "Problem setup. To make the notation compact, we take the sequence-level notation (or the bandit notation), commonly used in LLMs (Ziegler et al., 2019; Rafailov et al., 2024), especially in preference modeling, that suppresses the transition probabilities and intermediate rewards, and see a response as a whole. In particular, a policy \u03c0 : X \u2192 A(V) takes a prompt x \u2208 X and draw a response a1 \u25cb a2 \u25cb \u00b7 \u00b7 \u00b7 \u25cb aT =: y \u2208 Y from the produced probability simplex A(V), where \u25cb denotes concatenation, ai \u2208 V corresponds to the i-th token drawn from the vocabulary V, and T is the maximum length. A LLM is a parametric policy \u03c0\u03b8 \u2208 \u03a0\u03b8 \u2286 \u03a0, where \u03a0 and \u03a0\u03b8 are the parametric and non-parametric policy space respectively. Let J(\u03c0; \u00b5, r) or sometimes J(\u03c0) be a general objective function, defined by a prompt distribution \u00b5 \u2208 \u2206(X), and a bounded reward function r : X \u00d7 Y \u2192 [-Rmax, Rmax]. Also, we define \u00b5\u03b7 as an empirical distribution induced from \u03a9, a set of prompts.\nAs aforementioned, we define G disjoint groups Gi such that \u222aiGi = Y and Gi \u2229 Gj = \u2205 for all i \u2260 j. Each response y \u2208 Y is attached to exactly one group, in the sense that y \u2208 Gi for some i. In the context of LLM reasoning, without loss of generality, we consider two groups for brevity: G\u3002 and G+, corresponding to regular-length CoT responses and extended responses (with low and high inference costs), respectively. To\nTherefore, one could control how responses of different lengths (which are supposed to belong to different groups) are distributed. Our rationale of algorithm design is given in Section 2, derived from an optimization perspective but ended as a very simple generalization of iterative supervised fine-tuning (SFT) methods such as reward-ranking fine-tuning (RAFT) (Dong et al., 2023) and rejection sampling fine-tuning (RFT) (Ouyang et al., 2022; Touvron et al., 2023), see details in Section 3. Given the motivation of our algorithm, we call the resulted algorithm as Inference Budget-Constrained Policy Optimization (IBPO).\nPaper structure. In Section 2, we present the derivation of our algorithm from an optimization perspective, resulting in a simple weighted iterative SFT update. Section 3 provides further details on our practical implementation, including the choice of the base algorithm and the design of reward. Section 4 introduces the experimental settings used for empirical evaluation. The final empirical results of our IBPO are presented in Section 5. And Section 6 concludes our work with limitations, broader impact, and further discussions."}, {"title": "Algorithm Design", "content": "Problem setup. To make the notation compact, we take the sequence-level notation (or the bandit notation), commonly used in LLMs (Ziegler et al., 2019; Rafailov et al., 2024), especially in preference modeling, that suppresses the transition probabilities and intermediate rewards, and see a response as a whole. In particular, a policy \u03c0 : X \u2192 A(V) takes a prompt x \u2208 X and draw a response a1 \u25cb a2 \u25cb \u00b7 \u00b7 \u00b7 \u25cb aT =: y \u2208 Y from the produced probability simplex A(V), where \u25cb denotes concatenation, ai \u2208 V corresponds to the i-th token drawn from the vocabulary V, and T is the maximum length. A LLM is a parametric policy \u03c0\u03b8 \u2208 \u03a0\u03b8 \u2286 \u03a0, where \u03a0 and \u03a0\u03b8 are the parametric and non-parametric policy space respectively. Let J(\u03c0; \u00b5, r) or sometimes J(\u03c0) be a general objective function, defined by a prompt distribution \u00b5 \u2208 \u2206(X), and a bounded reward function r : X \u00d7 Y \u2192 [-Rmax, Rmax]. Also, we define \u00b5\u03b7 as an empirical distribution induced from \u03a9, a set of prompts.\nAs aforementioned, we define G disjoint groups Gi such that \u222aiGi = Y and Gi \u2229 Gj = \u2205 for all i \u2260 j. Each response y \u2208 Y is attached to exactly one group, in the sense that y \u2208 Gi for some i. In the context of LLM reasoning, without loss of generality, we consider two groups for brevity: G\u3002 and G+, corresponding to regular-length CoT responses and extended responses (with low and high inference costs), respectively. To\nA constrained RL framework controlling how response groups {Gi} are distributed.\nTherefore, one could control how responses of different lengths (which are supposed to belong to different groups) are distributed. Our rationale of algorithm design is given in Section 2, derived from an optimization perspective but ended as a very simple generalization of iterative supervised fine-tuning (SFT) methods such as reward-ranking fine-tuning (RAFT) (Dong et al., 2023) and rejection sampling fine-tuning (RFT) (Ouyang et al., 2022; Touvron et al., 2023), see details in Section 3. Given the motivation of our algorithm, we call the resulted algorithm as Inference Budget-Constrained Policy Optimization (IBPO)."}, {"title": "2 Algorithm Design", "content": "*(X, Y) = arg max\u03c0\u03b5\u03c0\u00ce(\u03c0; X, Y) := mm \u03a3\u03b9 \u03a3\u03b7 [\u03c0(Yij|xi)r(Xi, Yij)]\ns.t. \u03c0\u2208 \u03a6+(X, Y) := {\u03c0: \u03a3\u2081\u03a3;[\u03c0(Yij|xi) (1{y\u2208G+} - q+)] \u2264 0}\nwhere X \u2208 X\u2122 is a vector of n sampled prompts and Y \u2208 yn\u00d7m is a matrix of responses, with m responses for each of the n prompts; we explicitly write the empirical objective \u00ce with the conventional expected reward maximization for notational convenience, though alternative objectives are not restricted.\nSince the empirical problem (2) is a convex program with relative small sample size, it is now manageable. Combing the aforementioned projection step, we could write the program as a bi-level optimization:\n\u03b8 = arg min\u03c0\u03bf\u03b5\u03a0, Ex [KL(*(X,Y ||\u03c0\u03bf)[x]] s.t. x,y \u2208 arg max\u3160\u2208\u03a0\u2229\u03a6+(X,Yo) \u00ce (\u03c0; X, Yo)"}, {"title": "3 Practical Implementation", "content": "Yet, as we are working in an algorithm-agnostic fashion, we are now ready to select a specific RL algorithm, define its corresponding objective J, and specify an appropriate reward function r.\nReward function. Since we are working on mathematical problem-solving, a ground-truth reward could be obtained through string matching (Cobbe et al., 2021; Hendrycks et al., 2021b) of the model's solution against the ground truth solution, yielding a binary reward function rmatch: X\u00d7Y \u2192 {0,1} that indicates correctness."}, {"title": "4 Acronyms, Na\u00efve Construction G+ & Training Pipelines", "content": "Yet we work on abstract groups G\u3002 and G+. In this section, we present the details of our constructions of extended length responses, i.e. the extended group G+. However developing long reasoning models is beyond the scope of this work, as our focus is on the constrained optimization of LLMs. Our constructions are for demonstrative purpose only. Due to the intricate details involved in prompts, datasets, and training pipelines, this section may appear somewhat dense. To make it more approachable, we have structured our writing in a way that readers can, if they wish, focus on the broader ideas without delving deeply into the specifics of constructions. A TL;DR version of this section is provided below.\nTL;DR. We construct two types of illustrative extended responses: Sequential Voting (SV) and Adaptive Sequential Voting (ASV). Figure 2 visually explains how these constructions are implemented. The goal of the SV is to establish a baseline that generates only responses in G+, thereby serving as an uni-modal comparator. SV scales roughly as well as vanilla majority voting (MV), aka self-consistency (Wang et al., 2022). In contrast, ASV outputs a mixture of responses of y \u2208 G\u3002 and y \u2208 G+. This allows the model to adaptively decide which type of response to produce based on the query. The goal of ASV is to further enable IBPO optimization, as IBPO implicitly assumes the model generates both regular and extended-length responses. In Section 5, we show that ASV optimized by IBPO, achieves better allocation of the inference budget."}, {"title": "4.1 Construction of Sequential Vote", "content": "Acronyms. For clarity, we explicitly define key terms to hopefully resolve any potential ambiguities. Response: A response refers to a sequence generated until a terminal token is encountered. For precision, we sometimes refer to these as voting responses or SCOT responses, as illustrated in Figure 2, after introducing our sequential voting baselines. Trial: A trial denotes a solution instance, which is demarcated by the special tokens [TRIAL] [/TRIAL], as shown in the voting response example in Figure 2. While a voting response contains multiple trials, a SCOT response or a non-voting response contains exactly one trial, as also depicted in Figure 2.\nFor the description of training and testing details, we use LLaMA and LLaMA-b to denote the instruction-tuned and base versions of the LLaMA 3.1 8B models (Dubey et al., 2024), respectively. MATH refers specifically to the training split of the Hendrycks MATH dataset (Hendrycks et al., 2021b), while the 500-sample subset of the testing split is referred to as MATH500 (Lightman et al., 2023). SDPO stands for step-DPO (Lai et al.,"}, {"title": "4.2 Performance of Naive Sequential Vote", "content": "We start by evaluating the performance of SV compared to vanilla MV, following the setup of Exp. 1.1 in Table 6. This toy experiment is designed to demonstrate: (i) SV scales approximately as well as MV, thereby qualifying it as an example of G+; and (ii) measuring performance based on the number of responses is inadequate, therefore we later measure performance relative to the number of tokens/trials in Section 5.\nMetrics. The metrics we use are pass@k and majority@k, both of which are widely used in the literature (Hendrycks et al., 2021b; Wang et al., 2022). We occasionally refer to pass rate as pass@1. In both metrics, k"}, {"title": "5 Evaluation of IBPO w/OPTIuB", "content": "5.1 Absolute Improvement (Table 7)\n\"Baselines\". To put SV/ASV in comparison with other baselines in literature, we gather several baselines from the self-correction literature as essentially these methods increase inference length, though not extensively as Jaech et al. (2024). The results in Table 7 are mainly gathered from Qu et al. (2024); Kumar et al. (2024). The self-correction baselines often admit a multi-turn structure, similarly to the multi-trial construction of SV. We therefore include a column of inference cost measured by number of turns/trials for a rough comparison. The SFT comparators we include are (reproduced) Self-Refine (Madaan et al., 2024), STaR (Zelikman et al., 2022), S3C (Yan et al., 2024), SFT-RI and SFT-SCoRe, where SFT-RI and SFT-SCoRe are SFT comparators implemented in Recursive Introspection (RI) (Qu et al., 2024) and SCoRe (Kumar et al., 2024) respectively."}, {"title": "ASV experiments.", "content": "SV-SFT follows our training pipeline of Exp 1.2 in Table 6 and is an voting only baseline. ASV-SFT-a follows our Exp 2.1 setup and is an adpative baseline, meaning model decide whether to vote or not for a problem. We report ASV-SFT-1 only as it is empirically the best adaptive baseline although still fall short in terms of efficiency. Our ASV-IuB-q+ experiments, all initialized from ASV-SFT-1, are our adaptive models, optimized by IBPO with OPTIUB as lower level optimization, e.g. Algorithm 1."}, {"title": "Observations.", "content": "We would like to first emphasize that the comparison in Table 7 is not intended to demonstrate that SV outperforms SFT-based self-correction or that our ASV-IuB surpasses RL-based self-corrections. These efforts are orthogonal, as our focus is on constrained optimization. As observed, our SFT constructions-SV-SFT and ASV-SFT-1 achieve a clear improvement in pass@1 with high inference costs (5+ times the number of trials). The ASV-IuB-q+ formulation, particularly with q+ = {50%, 75%}, shows significant improvement while reducing costs by 4.14% at 2.16\u00d7 and 5.74% at 4.32x. This performance is on par with SCoRe, a state-of-the-art RL-based self-correction method. Note that the performance of ASV-IuB-q+ is reported using the best checkpoints. Results from the last checkpoint are shown in Figure 4a. Additionally, training curves are presented in Appendix F, which shows consistent improvements. As a somewhat tangential yet potentially intriguing observation, it is evident that prompting-based and SFT-based methods struggle with both absolute improvement (Table 7) and efficiency (Figure 4), supporting the conjecture that SFT alone does not enable self-correction capabilities (Huang et al., 2023; Kumar et al., 2024). This observation is also partially supported by concurrent work (DeepSeek-AI et al., 2025), which suggests that such self-correction behavior emerges automatically during RL rather than manually created by prompting or SFT."}, {"title": "5.2 Efficiency, Constraint Following & Budget Allocation", "content": "Table 7 demonstrates that our ASV-IuB-q+ models can achieve performance comparable to RL-based self-correction models simply through inference allocation management. In this subsection, we elaborate on our discussion of (i) performance-budget efficiency, (ii) constraint satisfaction, and (iii) inference budget allocation."}, {"title": "Performance-budget efficiency.", "content": "In Figure 4, we visually assess the performance-budget efficiency, compared to a hypothetical efficiency boundary. This boundary is an interpolation between OSS LLAMA model and SV-SFT. It is reasonable to see it as a hypothetical boundary for two reasons: (i) OSS and SV-SFT are two extremes of ASV-IuB-q+, corresponding to the cases of q+ = 0 and q+ = 1 respectively; and (ii) this interpolation achieves an increase comparable to MV, if not slightly better. The SFT version of ASV is"}, {"title": "6 Conclusion & Discussions", "content": "We derived a constrained policy optimization framework, IBPO, from an optimization perspective, resulting in a simple weighted SFT update that resembles successful iterative SFT algorithms such as RFT and RAFT. In each iteration, the optimal weight is determined by solving an (integer) linear program. The practical implementation of IBPO is build on top of CGPO, and is evaluated on a math reasoning task with inference budget constraints. Empirical evaluations demonstrate that our framework enables the model to adhere to constraints and dynamically allocate the inference budget."}, {"title": "Batch optimization.", "content": "Since we solve an optimization problem per iteration (i.e. per mini-batch), limited computational resources can result in smaller sample sizes for the inner optimization problem, leading to larger variance. This issue can be mitigated through \"sample accumulation\", accumulating samples across multiple consecutive steps, similar to gradient accumulation practice in LLMs. A pseudo-code for sample accumulation can be found in Appendix C. In addition, though integer linear programming is NP-hard (Vazirani, 1997), the number of variables in our batch-level optimization is typically small, resulting in minimal computational overhead. Refer to the wall-time plot for the SciPy solver in Appendix C for further details."}, {"title": "Controlled setting.", "content": "We keep our experiment setting minimal: we do not use reward models; we use only an 8B model to generate any set of sampled responses, ASAMPLE. Our RL training set, DRL, contains only 10k prompts, leaving the setting quite controlled and providing room for improvement through engineering efforts."}, {"title": "Limitations.", "content": "Our work is limited in its choice of RL algorithms and applications. While it should be straightforward to apply our framework to different RL frameworks, additional effort is required to derive an optimization problem for different application tasks. Future work may include extending our constrained policy optimization framework to a broader range of LLM applications or scaling up our experiments."}, {"title": "Broader applications.", "content": "Although our framework has only been evaluated with inference budget allocation, the resource allocation problem has far-reaching implications within the ML community. As a result, our framework can be potentially extended to further LLM applications. For example, a promising application is statistical parity (Zemel et al., 2013), aka group fairness. In this context, one could consider attributing responses to their respective social groups, and cap the density of responses that correspond to socially privileged groups, to encourage more inclusive and equitable responses across different demographics. Another potential application is the balanced activation of mixture of experts (Jacobs et al., 1991; Shazeer et al., 2017; Lepikhin et al., 2020), which is sometimes achieved by adding an auxiliary loss (Wei et al., 2024). Alternatively, this balance can be possibly achieved by enforcing a minimal total activation density for each expert. This may help to prevent over-reliance on a subset of experts, and hence enhancing the overall robustness and efficiency. We leave the exploration of broader applications and their implementations as future directions."}, {"title": "Appendix", "content": "A Sample Responses"}, {"title": "B Batch Accumulation", "content": "This is a batch accumulation implementation of Algorithm 1. For brevity, we use OPTIUB as an example. One could increase the optimization problem size of n \u00d7 m to (n \u00b7 k\u2081) \u00d7 (m.kr) using Algorithm 2, where superscripts indicate matrix shape, left subscripts denote accumulation indices (distinguishing them from element indices)."}, {"title": "CMILP Solving", "content": "We use SciPy MILP solver, available here, to solve our in-batch integer linear programming optimizations. A Pythonic pseudo code is available in Figure 1. And the wall-time consumed could be found in Figure 7. In general, the problem size is small, so the computational overhead is negligible."}, {"title": "D Data Construction", "content": "We elaborate on the details of QSDPO, ASDIDEN, QMATH, ASAMPLE, and the filteration function F."}, {"title": "SDPO dataset.", "content": "Lai et al. (2024) create a dataset (see their Section 3.2) contains 10,795 step-wise preference pairs, where the winning response is a correct solution, the losing response is incorrect starting from certain step. For our purpose of SFT and RL training, we do not need losing responses, hence we create AGOLDEN with only wining (correct) responses. The step-formatted response (see an example below) is the reason we choose the dataset from Lai et al. (2024), as it adhere the SCOT format of LLaMA models. Therefore, one do not need to worry about potential format changes."}, {"title": "SV dataset for MATH.", "content": "We also created datasets, Dsy and DASV1, for SFT so that a model could follow the SV instructions. We take Dsy as an example and DASV1 could be created similarly. To do so, we first generate 32 responses per prompt for the entire MATH training split with a temperature of 1.2 and top-p of 0.9. We then apply our SV templates To and Ta to create corresponding SV question and answer pairs. The procedure of creating a SV response is given by Algorithm 3. While we have include an example of SV responses in Figure 8, we make it more concrete the SV response template below,"}, {"title": "E Hyperparameters", "content": "We list the hyperparameters used for experiments setup 1.2, 2.1, 2.2, as described in Table 6. And we conduct our experiments with NVIDIA-A100-80Gs. (Please refer to Xu et al. (2024b) for the definition of some RL-specific hyper-parameters.)"}, {"title": "F Further Discussions", "content": "Budgets constraints. While in Section 5, we show that our formulation follows the constraints exactly on the training set, for responses that are correct. Figure 10b further shows the ratio of voting responses for all online rollouts. It is clear that the constraints are met as well."}]}