{"title": "PerspectiveNet: Multi-View Perception for Dynamic Scene Understanding", "authors": ["Phu-Vinh Nguyen"], "abstract": "Generating detailed descriptions for events from multiple cameras and viewpoints is challenging due to the complex and inconsistent nature of the visual data. In this paper, we present PerspectiveNet, a lightweight yet efficient model to generate long descriptions for multiple-view cameras by using a vision encoder, a small connector module to convert visual features to a fixed-size tensor, and large language models (LLM) to utilize the strong capability of LLM in natural language generation. The connector module is constructed with three primary objectives, map visual features onto LLM embedding, emphasize the critical information needed for generating descriptions, and yield the fixed-size feature matrix as the output. Furthermore, we augment the prowess of our solution by employing a second task, the correct sequence of frame detection, to enable the model's ability to search for the correct frame sequence to generate the description. Finally, we combine the connector module, a second training task, a large language model, and a visual feature extraction model into a single model and train it for the Traffic Safety Description and Analysis task, in which the model has to generate long fine-grain descriptions for an event from multiple cameras with and viewpoints. The resulting model is lightweight and ensures an efficient training and testing process, yet very effective.", "sections": [{"title": "1. Introduction", "content": "In recent years, there has been a variety of research about large language models (LLMs), those research has improved the capability of LLMs in natural language understanding, reasoning, knowledge updating, and instructions following. Moreover, LLMs can be adapted to different domains by fine-tuning different datasets, which shows their potential to assist people, enhance productivity, and handle many difficult problems. However, since LLMs are just about text and can only work with language tasks, they cannot understand different information sources such as sound and vision, which are crucial to solving many real-world problems. For those reasons, there is much research about integrating LLMs with vision models to enable LLMs with visual tasks such as LLaVA, Qwen-VL, InstructBLIP, and Flamingo. These works have advanced the capability of LLMs in visual problems significantly.\nWhile problems with images and videos are the main target of many vision language models, multiple viewpoints of an event are not on the priority list. As a result, most models are not effectively designed and trained to handle such visual information. Different from video data and images, which only show objects and events from one viewpoint, this type of data can provide more information about an event by inspecting from different positions and help vision language models understand and generate descriptions about the event more precisely. While the number of videos and frame number of each video can be various, handling such information can be more challenging than videos and images. This is because the large and undefined number of videos results in the large and undefined number of features, which leads to the difficulty in finding the similarity of relation between many events, and frames in videos. Moreover, since the number of features is large, putting all of them into the LLMs to handle might not be the best choice due to the excessive increase in size of the input, and the increase of computation cost. To solve this, visual features should be encoded to smaller sizes but still need to be informative for language models to generate long fine-grain descriptions.\nIn this paper, with the awareness of those problems, we introduce PerspectiveNet, a method to connect a large and undefined number of visual features with LLMs which can be applied to many visual language tasks. The module is constructed to take visual features of multiple videos and viewpoints as input and create the output with 3 features: small, informative, and size-consistent to provide context for LLMs to generate. Moreover, we also present a second task during the training process to increase the capability of our module in detecting important events and frames. Finally, we experiment by integrating the above methods on multiple videos and viewpoints datasets with the main task being traffic safety description and analysis."}, {"title": "2. Related work", "content": "Recently, with the release of many large language models like Mistral [10], LLaMA[17], Bloom[11], Phi [6], or Qwen[2], become popular due to the superior performance in natural language understanding and generation. Moreover, they have the potential to adapt to different domains and tasks, not only in natural language but also in different domains like computer vision. Many research groups have proposed solutions to integrate vision models with large language models to create vision language models (VLMs) for many tasks like visual question-answering or caption generation. One of the common solutions to create VLMs is to encode vision information (images, videos) to a feature tensor, slightly modify that feature by changing shape, and information with a module, and then feed that matrix to LLMs through cross-attention layers or by using these tensor as input context for LLM to generate the answer.\nMany previous works to create VLMs follow this general structure such as LLaVA [15], Qwen-VL[2], and InstructBLIP[4] that used vision encoder to encode the image and get visual features, then feed those features to a small module before using the output as input context for large language model to generate captions, descriptions and answers. Meanwhile, some vision language models like Flamingo[1], mPLUG[12] or BLIP[13] decided to use some more complex solutions to modify the visual features before feeding to decoder-only through cross attention layers to generate natural text. Most vision language models use Vision Transformer (ViT)[5] as a visual encoder due to the strong capability of this architecture in extracting features from visual information despite its computation cost.\nMoreover, previous research has presented different methods to connect visual information to language models. A noticeable contribution is using cross attention[1] to convert visual features of different sizes to a constant size tensor before being fed to a language model to generate text. Another important research is Q-former[14], this method and module can teach itself to focus on visual information of an image which is the most relevant to the text. Most of those works focus on improving the ability of the connector. By carefully selecting and modifying visual features, the connector can create a new visual matrix that is informative and precise and reduces the noise of non-important information in images and videos. Furthermore, if the connector can return a small output while keeping the most important part of visual information for the language model to understand and generate, this could significantly reduce the computational cost during the training or inference progress. Not only changing the structure of the connector but recent research also pays attention to training models on multiple objectives[13] with some tasks like ITM (Image-Text Matching loss), ITC (Image-Text Contrastive loss), LM (Language model loss). This strategy shows great improvement for VLMs since it can improve the comprehension of the model on the relation of those objectives. Lastly, instruction learning has improved the ability of many LLMs (InstructGPT[16], PLAN-PaLM[3], and OPT-IML[8]) also show a process on VLM[15].\nHowever, while there is much research about using VLMs on visual data like images, and video, types of data like multiview-camera do not attract significant attention even though this type of data can provide more detail by inspecting an event from multiple positions. As a result, being inspired by many previous works on VLMs and their limitations on such data, PerceptiveNet is presented as a way to construct such VLM for this specific data type."}, {"title": "3. Approach", "content": "In this session, we start with a brief overview of our description generation task, and the dataset on which we train and test our model. After that, we introduce the details of our solution for this problem, which includes the overall architecture of the PerceptiveNet, more details of our vision language connector, and the training method."}, {"title": "3.1. Task and dataset", "content": "This paper focuses on generating long fine-grained video descriptions for many traffic safety scenarios and pedestrian accidents. The resulting model should be able to describe the continuous moment before the incidents and normal scenes, as well as all the details about the surrounding context, attention, location, pedestrians, and vehicles. The scene of events might be recorded from different viewpoints and positions, this requires the model to understand the similarity between each viewpoint and the relevance of many frames in an event to generate informative descriptions.\nThe dataset used in this paper is the WTS, a newly created dataset. Each sample of the dataset includes 2 long and detailed descriptions for vehicle and pedestrian, n videos (n \u2265 1) about a sequence of events about traffic, incident on the street, the start time and end time of the event that the model should focus on to describe, annotated segment (pre-recognition, recognition, judgment, action, and avoidance), and bounding boxes of the instance (pedestrian or vehicle) that relates to the description. There are two types of bounding boxes generated (machine) and annotated (human)."}, {"title": "3.2. Architechture", "content": "Overall, the architecture of PerspectiveNet, as shown in Fig. 1, includes three primary components:\n\u2022 Visual encoder: to get visual information from each frame of each video, we use the pre-trained image encoder of BLIP-2[14] (ViT-g/14).\n\u2022 Vision-Language Connector: for the connector, we use two Perceiver module[9]. The first Perceiver is used to convert features of all frames of different videos at the same time to a single feature vector. The second Perceiver is employed to reduce the shape of the long video feature to a smaller constant shape.\n\u2022 Large language model: we avoid using LLMs with more than 3 billion parameters for the generating task due to the limited resources. Finally, we decided to use Phi-1.5[6] due to its small size and strong capability in natural language generation despite the model having not been trained on any visual information yet. Moreover, we also employ LoRA[7] due to the need to fine-tune LLMs on the new task.\nFor each data sample, we are provided with n videos, where we extract frames from those videos at a frequency of 1 frame per second. It is important to note that even though all videos record the same event, not all of them have the same length, some might finish sooner than expected.\nDouble Perceiver Connector: After collecting frames from videos, we feed every single extracted frame to the visual encoder (ViT) to get visual features, the result after this step is a tensor with shape $(N_v, N_f, D)$ where $N_v$ is the number of videos, $N_f$ is the max number of frames and $D$ is the size of the visual embedding dimension, which is defined by the vision encoder. This tensor will be fed to the first Perceiver to get the new visual embedding (1, $N_f$, $D_1$). Due to the context similarity between frames in all videos at the same time, this module takes the responsibility for capturing important information at that specific time. This output is then concatenated to an embedding token (which presents the object that needs to be described and the segment) to create a new tensor with shape (1, $N_f$ + 1, $D_1$) before being fed to the second Perceiver to get a tensor of shape (1, c, $D_2$), where c is a constant. This final output is the visual summarization of the primary event in the video. Finally, we set c = 20 in our default model.\nPhi 1.5: The new visual information is then used as a context token for Phi 1.5 to generate a description. The initial input, which provides information for LLM, includes the visual tokens, target object (pedestrian or vehicle), and segment (pre-recognition, recognition, judgment, action, or avoidance). Moreover, to make the language model adapt well to this task, LoRA[7] layers are injected into the attention mechanism of Phi 1.5. While language model weight is frozen during training, weights of the LoRA adapter are still set to trainable. This step helps the language model to understand its task better after our training progress while maintaining a strong ability in text generation.\nCross-attention layers can be added to the language model to receive visual information, however, adding this module would increase the size of the language model significantly. Furthermore, since the newly added layers are initial with random weight, those layers will also need to be trained during the training process, which would highly increase the VRAM consumption. As a result, using visual information as extra visual context in the input prompt would be the best choice for limited resources."}, {"title": "3.3. Training Strategy", "content": "In each data sample, we are provided with the start time and end time of the event that needs to be described. Denote that input videos is $V = [v_0, v_1, . . ., v_{t\u22121}]$ where $v_i$ is all frames at time i, with that notation, the event from start time m to end time n can be written as $E = [v_m, v_{m+1},..., v_n]$. With two visual inputs and the requirement to generate the same description, we fed both of them to the Visual Encoder and then the adapter to collect $f_V$ and $f_E$, which are the final visual context of full video and partial events respectively. Then, the dot product of $f_V$ and $f_E$ is calculated and compared with $I$ (Identity matrix) by cross-entropy loss.\n$f = f_V \u22c5 f_E$ (2)\n$L_M = \\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^N I_{ij} \\log(f_{ij})$ (3)\nWhere:\n\u2022 $L_M$ is the loss function of matching task\n\u2022 N is the number of visual tokens\n\u2022 $I_{ij}$ is the element in the i-th row and j-th column of the Identity matrix.\n\u2022 $f_{ij}$ is the element in the i-th row and j-th column of f\nOur main target for this loss function is to provide the same context but a different part of videos to our connector and train the connector to understand where the important event should be focused on to generate the context. While the $f_E$ is the final feature of the correct event, the $f_V$ after training should ignore every frame but frames can provide information about the event that needs to be described.\nFor the generation task, the prediction of visual tokens is the token at index 0 (which is '!'). However, the last visual token should predict the first text token in the prompt as shown in Fig. 2. Besides adding extra visual tokens, this model performs like a normal large language model.\n$L_G = - \\sum_{j=1}^V y_j \\log(p_j)$ (4)\nWhere:\n\u2022 $L_G$ is the loss function of generation task\n\u2022 V is the size of the vocabulary\n$L = L_M + L_G$ (5)"}, {"title": "4. Experiment", "content": "Since our vision encoder is frozen and not being trained during the training process, we decided to extract visual features of videos, save them to files, and use those features in our training process instead of re-running the whole visual encoder every time. By doing this we preserve more VRAM for training LLM and the adapter. For the experiment, both the training and testing processes are done with the hardware including 30GB RAM, and GPU P100 with 16GB VRAM, which can easily be found and used on Kaggle.\nOur model was trained on the WTS dataset two times. For the first time, we train our model with the WTS dataset except for the external and normal trimmed data. The training configuration includes 20 epochs, the learning rate is 5e-5, we use Adam optimizer, the batch size is 2, the evaluate step is 100, and the gradient accumulation step is 2. The result of this training process is shown in 1. Even though the model was trained on a small internal dataset and is predicted to perform poorly on the external dataset, the result shows the opposite when the model can achieve a high score, nearly 24, on the new dataset with a nearly different context. However, this result on the external dataset is still far behind the experiment result on the internal dataset."}, {"title": "5. Limitation", "content": "Even though this model is small and can generate correct long fine-grain descriptions on multiple-view cameras and videos, this work still has some limitations.\nFirst, this model is constructed to work only with multiple cameras that start at the same time. If any of them start at a different time, the frame feature will not be aligned, which may lead to wrong information comprehension of the model. As a result, the generated descriptions can be unpredictable and the model should not be used for data like that.\nSecondly, even though a video is just a list of continuous images, it is not confirmed that training on videos would lead to any improvement in the image description task. Since our model only trained on videos and described a sequence of events without being trained on any image, using this to generate image descriptions is not recommended.\nFinally, while bounding boxes can provide more context about pedestrians and vehicles, our solution did not include that important information, which makes the model focus on the wrong target to generate descriptions. This can be more severe if many people and vehicles are on the scene."}, {"title": "6. Conclusion", "content": "In this paper, we present PerspectiveNet, a model to generate long and detailed descriptions given videos or multiple-viewpoint videos. The model is a combination of a pre-trained vision transformer (ViT), a newly constructed connector from two Perceiver modules, and a large language model. Despite its small size and the language has not been trained on any visual information, this model can still achieve a high score on the WTS dataset and secure a position among the top 5 models currently."}]}