{"title": "ReLU's Revival: On the Entropic Overload in Normalization-Free Large Language Models", "authors": ["Nandan Kumar Jha", "Brandon Reagen"], "abstract": "LayerNorm is a critical component in modern large language models (LLMs) for stabilizing training and ensuring smooth optimization. However, it introduces significant challenges in mechanistic interpretability, outlier feature suppression, faithful signal propagation, and computational and communication complexity of private inference. This work explores desirable activation functions in normalization-free decoder-only LLMs. Contrary to the conventional preference for the GELU in transformer-based models, our empirical findings demonstrate an opposite trend\u2014ReLU significantly outperforms GELU in LayerNorm-free models, leading to an 8.2% perplexity improvement. We discover a key issue with GELU, where early layers experience entropic overload, leading to the under-utilization of the representational capacity of attention heads. This highlights that smoother activations like GELU are ill-suited for LayerNorm-free architectures, whereas ReLU's geometrical properties\u2014specialization in input space and intra-class selectivity\u2014lead to improved learning dynamics and better information retention in the absence of LayerNorm. This study offers key insights for optimizing transformer architectures where LayerNorm introduces significant challenges.", "sections": [{"title": "Introduction", "content": "Motivation and challenges. LayerNorm [1] has been a key architectural component contributing to the success of large language models (LLMs) by stabilizing training through normalizing inputs across features within a layer. Additionally, it plays a crucial role in enhancing the models' non-linear representational capabilities [2-5]. Despite its benefits, LayerNorm introduces several practical challenges that become pronounced in specific settings:\n1. Private Inference (PI): PI protocols enable inference on encrypted data without exposing inputs, ensuring data privacy while protecting model weights [6\u201314]. Hybrid PI protocols encounter difficulties with the inverse square root computation inherent in LayerNorm, making it the second most costly operation after GELU, contributing to 22% of total latency and communication overhead [11]. Also, Homomorphic Encryption (HE)-only PI requires polynomial approximations of LayerNorm, which are challenging due to the wide variance range [8].\n2. Mechanistic Interpretability: LayerNorm increases the complexity of the residual stream, making it harder to analyze and understand the internal workings of transformer models [15], limiting the applicability of LLMs for applications requiring transparency and explainability.\n3. Low-Precision Training: The trainable parameters in LayerNorm are associated with the amplification of outlier features which poses challenges in LLM quantization, as it exacerbates numerical instability and degrades performance in low-precision training regimes [16-19].\n4. Signal Propagation: LayerNorms shown to negatively impact the faithful signal propagation [20]."}, {"title": "2 Preliminaries", "content": "Notations. We denote the number of layers as L, number of heads as H, model dimensionality as d, head dimension as $d_k$ (where $d_k = \\frac{d}{H}$), and context length as T. Table 1 illustrates the abbreviations for architectural configurations with simplified nonlinearities in a transformer-based LLM.\nAn overview of transformer-based decoder-only architecture. A transformer-based LLM is constructed by sequentially stacking L transformer blocks, where each block is composed of two sub-blocks: an attention mechanism and a feed-forward network (FFN), both having their own residual connections and normalization layers, positioned in the Pre-LN order to improves training"}, {"title": "2.1 Entropy as a Metric for Attention Score Distribution", "content": "Shannon's entropy quantifies the uncertainty in a probability distribution, measuring the amount of information needed to describe the state of a stochastic system [27, 28]. For a probability distribution P(x), the entropy is defined as $E(P) = \\Sigma_i P(x_i) log P(x_i)$. Refer to [29] for details on entropy.\nIn a softmax-based attention mechanism, each softmax operation yields an entropy value representing the sharpness or spread of the attention scores for each query position [30, 31]. Higher entropy indicates a more uniform distribution of softmax scores, while lower entropy signifies a more focused distribution on certain features [32].\nLet $A^{(h,l)} \\in R^{T \\times T}$ be the attention matrix of h-th head in l-th layer, and each element in the attention matrix, $a_{ij}^{(l,h)}$, are attention weights, which are non-negative and sum to one for a query:\n$A^{(l,h)} = [a_{ij}^{(l,h)}]_{T \\times T}, \\text{where } a_{ij}^{(l,h)} \\geq 0 \\text{ and } \\sum_{j=1}^{T} a_{ij}^{(l,h)} = 1$ (5)\nThis square matrix is generated by applying the softmax operation over the key length for each query position as follows (i.e., $X \\in R^{T \\times T} X_i \\in R^{1 \\times T}$):\n$A^{(h,l)}(X) = Softmax\\left(\\frac{1}{\\sqrt{d_k}} (X_i W_i^Q) (X W^K)^T\\right)$, where $Softmax(X_i) = \\frac{exp (x_i)}{\\sum_{j=1}^{T} exp (x_j)}$ (6)\nThus, each element $a_{ij}^{(l,h)}$ of the attention matrix can be represented as follows:\n$a_{ij}^{(l,h)} = \\frac{exp((X_i W_i^Q)(X_i W^K)^T)}{\\sum_{j=1}^{T} exp((X_i W_i^Q)(X_i W^K)^T)}$. (7)"}, {"title": "3 Activation Functions and Their Impact Through Shannon's Entropy", "content": "In this section, we investigate the role of activation functions in baseline and normalization-free decoder-only LLMs. Specifically, we examine the learning dynamics and internal representations of activation functions, using entropy as a metric to highlight key observations and insights.\nWell-behaved entropy distribution\nWe begin by analyzing the headwise entropy distribution of baseline architecture with GELU and ReLU in the FFN, i.e., configurations SM + LN + G and SM + LN + R respectively. We find that the majority of heads (~90%) possess entropy values between $\\frac{max}{4}$ and $\\frac{3max}{4}$, where max is maximum observed entropy value among all heads (Figure 2). This concentration in the middle entropy range, while avoiding extremes, demonstrates a well-behaved distribution, providing a benchmark for assessing the impact of activation functions on model behavior.\nEntropic overload We observed that in certain nonlinearity configurations, a disproportionately large fraction of the attention heads exhibit higher entropy values (between $\\frac{3max}{4}$ and max). We term this phenomenon as entropic overload and hypothesize that this imbalance results in under-utilization of the network's representational capacity, as too many heads engaged in exploration, hindering the model from effectively leveraging the diversity of attention heads.\nTo investigate further, we examined how entropy values evolve during training. Typically, all heads start with higher entropy values, indicating an initial exploration phase, and gradually adapt to balance exploration and exploitation in baseline networks (see Figure 5). However, in the absence of certain nonlinearities, this balance is disrupted, preventing attention heads from specializing and refining their focus on critical aspects of the input, thereby diminishing overall performance.\nObservation 1: ReLU significantly outperforms GELU in LayerNorm-Free LLMs. While GELU is typically preferred over ReLU in conventional transformer-based models due to its smooth and differentiable properties that improve performance and optimization, our empirical findings indicate the opposite trend for LayerNorm-free models- using ReLU in the FFN exhibit better learning dynamics than their GELU counterpart. This leads to an 8.2% improvement in perplexity for GPT-2"}, {"title": "4 Conclusion", "content": "In this paper, we investigated the design of LayerNorm-free decoder-only language models and highlighted the critical role of activation functions in such architectures. Our empirical studies revealed that, contrary to conventional practices, the ReLU activation significantly outperforms, an 8.2% improvement in perplexity, the GELU in normalization-free models. We found that models with learnable negative slopes in leaky ReLU activations naturally converge toward zero negative slopes, effectively resembling ReLU. Additionally, we discovered that LayerNorm-free models with GELU activation suffer from entropic overload in early layers, leading to under-utilization of their representational capacity. These findings underscore the necessity of rethinking activation function choices when LayerNorm is absent and suggest that selecting appropriate activations like ReLU enables the development of transformer models that are more efficient, interpretable, and better suited for applications such as private inference and quantization.\nLimitations. This study mainly focuses on pre-training performance, with perplexity as the primary metric, and does not include experiments to evaluate other capabilities such as transfer learning or few-shot learning. Additionally, our findings are been validated on models with fewer than 1B parameters. Future work will explore broader experimental evaluations, including the large-scale models (see Appendix D)."}, {"title": "A Why Use Entropy to Evaluate the Impact of Nonlinearities?", "content": "We use entropy as a metric to study the impact of nonlinearities on the transformer-based LLMs for the following reasons:\n\u2022 Quantifying attention distribution: As the attention mechanism is fundamental to all transformer-based architecture, computing the entropy of attention score distributions reveals how nonlinearities affect attention concentration. High entropy quantifies exploration and low entropy indicates exploitation.\n\u2022 Feature selection: Nonlinearities like ReLU enable feature selectivity by amplifying important features and suppressing less relevant ones [35]. Entropy can measure this selectivity across layers and heads, providing insights into the model's prioritization of information. Previously, entropy has been used to quantify the layerwise information flow in neural networks [36].\n\u2022 Exploration vs. exploitation: Nonlinear operators like the self-attention mechanism, LayerNorm, and GELU balance exploration and exploitation by selecting relevant features while considering a broader context. For instance, heads in the first layer focus on exploration, while those in the second layer focus on exploitation. (see Figures 1a, 1b, 5a and 5b).\n\u2022 Systematic assessment: Prior work [37, 32, 33, 31, 30] also used entropy to analyze the behavior of transformer-based models; thus, enhancing validity and comparability of our findings."}, {"title": "B Design of Experiments", "content": "Datasets We train models from scratch using the CodeParrot [24] dataset. The CodeParrot dataset, sourced from 20 million Python files on GitHub, contains 8 GB of files with 16.7 million examples, each with 128 tokens, totaling 2.1B training tokens. We use a tokenizer with a vocabulary of 50K.\nTraining settings For training on the CodeParrot dataset, we adopt the training settings from [22], which remain consistent across all architectural variations to accurately reflect the impact of the architectural changes. Following [22, 38, 39], all the models are trained on a single RTX 3090 GPU."}, {"title": "B.1 Perplexity as a Reliable Metric to Evaluate the LLMs' Performance", "content": "Perplexity [40] is a widely adopted metric to evaluate the predictive performance of auto-regressive language models, reflecting the model's ability to predict the next token in a sequence. However, for perplexity to serve as a meaningful comparative metric across different architectures, it is critical to ensure consistency in the tokenizer, and vocabulary size and quality [41]. Any variation in these components can potentially skew the results by inflating or deflating perplexity scores; thus, obfuscating the true effects of architectural changes.\nIn our work, we maintain tokenization schemes and vocabulary attributes as invariant factors across all experiments within a dataset. This isolation of architectural modifications ensures that any observed variations in perplexity are directly attributable to changes in the model design. Thus, by enforcing a consistent tokenization scheme and vocabulary within a dataset, we ensure that perplexity remains a reliable metric for comparing model architectures. Consequently, lower perplexity in our evaluations reliably reflects improved token-level predictions."}, {"title": "B.2 Why Training from Scratch to Study Nonlinearities?", "content": "Understanding the intricate roles of architectural components and nonlinearities\u2014such as activation functions (e.g., GELU, ReLU) in FFN, normalization layers (e.g., LayerNorm), etc.\u2014in transformer-based language models necessitates a methodical and detailed investigative approach. Training models from scratch is essential for this purpose, as it allows us to delve into the internal mechanisms of the model using quantitative measures like entropy. Below, we present a justification for our methodology:\n\u2022 Nonlinearities' impact on the fundamental learning dynamics: Nonlinearities significantly influence the optimization landscape by affecting gradient flow and the model's ability to navigate non-convex loss surfaces. Training models from scratch allow us to observe the fundamental learning dynamics that emerge during the initial stages of training. Thus, constructing models"}, {"title": "C Additional Results", "content": ""}, {"title": "C.1 Layerwise Entropy Dynamics for Learnable Negative Slope", "content": ""}, {"title": "D Future Work", "content": "Scaling up and generalizing to larger models This research opens several avenues for optimizing LayerNorm-free transformer architectures. A primary direction is scaling up experiments to larger models. Evaluating whether the benefits of ReLU activation persist in models with significantly more parameters will determine the applicability of our findings to state-of-the-art language models."}]}