{"title": "Integrating Generative AI in Cybersecurity Education: Case Study Insights on Pedagogical Strategies, Critical Thinking, and Responsible AI Use", "authors": ["Mahmoud Elkhodr", "Ergun Gide"], "abstract": "The rapid advancement of Generative Artificial Intelligence (GenAI) has introduced new opportunities for transforming higher education, particularly in fields that require analytical reasoning and regulatory compliance, such as cybersecurity management. This study presents a structured framework for integrating GenAI tools into cybersecurity education, demonstrating their role in fostering critical thinking, real-world problem-solving, and regulatory awareness. The implementation strategy followed a two-stage approach, embedding GenAI within tutorial exercises and assessment tasks. Tutorials enabled students to generate, critique, and refine AI-assisted cybersecurity policies, while assessments required them to apply AI-generated outputs to real-world scenarios, ensuring alignment with industry standards and regulatory requirements. Findings indicate that AI-assisted learning significantly enhanced students' ability to evaluate security policies, refine risk assessments, and bridge theoretical knowledge with practical application. Student reflections and instructor observations revealed improvements in analytical engagement, yet challenges emerged regarding AI over-reliance, variability in AI literacy, and the contextual limitations of AI-generated content. Through structured intervention and research-driven refinement, students were able to recognise AI's strengths as a generative tool while acknowledging its need for human oversight. This study further highlights the broader implications of AI adoption in cybersecurity education, emphasising the necessity of balancing automation with expert judgment to cultivate industry", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of Generative Artificial Intelligence (GenAI) technologies has reshaped various sectors, including education, by offering new opportunities for instructional innovation. Among these technologies, ChatGPT has emerged as a widely adopted tool with the potential to enhance teaching and learning through personalised assistance, real-time feedback, and enriched student engagement [1]. While GenAI has demonstrated substantial promise, its integration into academic settings remains a subject of ongoing debate, particularly concerning ethical considerations, academic integrity, and the potential risks of over-reliance on AI-generated content [2]. This study presents a structured framework for embedding GenAI tools into cybersecurity management education, illustrating how such technologies can be effectively leveraged to develop critical thinking, contextual understanding, and problem-solving skills.\nChatGPT has already been successfully implemented across various academic disciplines, serving functions such as generating multiple-choice questions [3], assisting with programming tasks in languages like Java and Python, and providing language learning feedback [4]. Studies suggest that its use in blended learning environments enhances student engagement, fosters creativity, and facilitates reflective thinking through interactive and iterative learning processes [5-7]. These capabilities position GenAI as a transformative educational tool, particularly in cybersecurity management, which demands analytical rigour and adaptability to evolving threats. However, despite its advantages, GenAI integration in education presents notable challenges, including concerns over potential misuse, the accuracy of AI-generated outputs, and the risk of diminishing students' independent problem-solving capabilities [8]. These concerns underscore the need for a structured pedagogical framework that ensures responsible and effective deployment of GenAI while maintaining academic integrity.\nCybersecurity management education represents a particularly compelling case for the integration of GenAI due to its dual emphasis on theoretical knowledge and practical application. The discipline requires students to analyse regulatory frameworks, develop security policies, and address emerging cyber threats tasks that benefit from AI-driven analytical support while also requiring human oversight to ensure accuracy and relevance. This study introduces a two-stage strategy for incorporating GenAI into cybersecurity management education, designed to maximise the benefits of AI tools while mitigating their limitations through structured guidance and critical engagement."}, {"title": "2 Related Works", "content": "The integration of AI tools in education has garnered significant attention, with several studies exploring their impact on student learning outcomes and pedagogical effectiveness. A comprehensive review of ChatGPT applications in higher education highlighted its potential for enhancing student engagement and promoting active learning across various disciplines [9]. Similarly, research examining the effectiveness of GenAI tools in computer science education demonstrated improved learning outcomes through AI-assisted programming exercises and code review processes [10].\nIn cybersecurity education specifically, research has increasingly focused on innovative teaching methodologies that bridge theoretical knowledge with practical application. Studies have shown that interactive learning environments significantly enhance students' understanding of complex security frameworks and regulatory requirements"}, {"title": "3 Integration Strategy and Instructional Design", "content": "The implementation of GenAI tools in cybersecurity education required a carefully structured approach to ensure effective learning outcomes and meaningful student engagement. This section describes our pedagogical framework, implementation strategy, and specific case studies that formed the foundation of our AI-assisted learning"}, {"title": "3.1 Pedagogical Framework", "content": "The integration of Generative AI (GenAI) tools into cybersecurity education was designed based on constructivist learning principles [23], emphasising active engagement, critical thinking, and problem-solving. These principles position students as active participants in the learning process, where AI functions as a scaffold for foundational learning, enabling them to explore, critique, and refine AI-generated outputs within structured instructional activities. Drawing from Bloom's Taxonomy [24], this framework fosters higher-order cognitive skills such as analysis, evaluation, and synthesis, ensuring that students interact meaningfully with AI-generated content rather than consuming it passively.\nAdditionally, situated learning theory [25] plays a critical role in shaping this strategy. This theory suggests that knowledge is best acquired through authentic, real-world contexts, making cybersecurity an ideal domain for AI-assisted education. By embedding AI-driven tasks into cybersecurity exercises, students apply theoretical knowledge while developing practical expertise in risk assessment, policy development, and regulatory compliance. This integration encourages iterative refinement, where students critically assess AI-generated content and enhance its accuracy, alignment with regulations, and contextual applicability.\nThis pedagogical framework aligns with the curriculum standards of Information Security Management at the postgraduate level and Cybersecurity Management) at the undergraduate level at the \"University name anonymised\". Both courses emphasise managerial, technical, and policy-oriented skills, preparing students to tackle contemporary cybersecurity challenges. While the undergraduate course focuses on foundational cybersecurity concepts and applied exercises, the postgraduate version integrates a stronger research component, requiring students to conduct deeper analytical evaluations of AI-generated outputs. The integration of GenAI in both courses adheres to Australian higher education standards, ensuring academic rigor, industry relevance, and a strong emphasis on ethical considerations.\nThe strategic framework adopted for this integration is visualised in Figure 1, which outlines the pedagogical foundation and its alignment with learning objectives. The diagram highlights the two primary stages: (1) tutorial-based iterative learning and (2) assessment-driven real-world application which emphasises the role of constructivist learning principles in developing critical thinking and contextual understanding."}, {"title": "3.2 Implementation Approach", "content": "The selection of Generative AI (GenAI) tools for cybersecurity education was guided by their versatility, accessibility, and ability to generate structured security content. ChatGPT, Gemini, and Claude were used for their capabilities in policy drafting, risk assessment, and cybersecurity strategy development. More importantly, they offered free versions. While DeepSeek was introduced later in the term after its launch. These"}, {"title": "3.2.1 Tutorial and Assessment Integration", "content": "The integration strategy was structured to ensure that students progressively engaged in AI-assisted cybersecurity problem-solving."}, {"title": "3.2.2 Case Study 1: Security Systems Development Life Cycle (SecSDLC) in Cybersecurity Planning", "content": "A fundamental component of cybersecurity education involves integrating security considerations into IT system development. This tutorial introduced students to the SecSDLC model, guiding them in developing security strategies that align with cybersecurity best practices and regulatory requirements.\nStudents were provided with a case study scenario that required them to develop an information security plan for a financial institution. They generated an initial strategy using GenAI, which provided a structured but incomplete SecSDLC framework. The AI-generated content lacked specific compliance considerations, organisational risk factors, and threat modeling."}, {"title": "3.2.3 Case Study 2: Email Policy Development for XYZ Bank", "content": "The second tutorial emphasised policy development, particularly focusing on email security policies for financial institutions. Students first generated an AI-based policy, which covered basic security controls such as encryption and password protection. However, the AI-generated policy failed to align with industry-specific compliance requirements and lacked structured phishing prevention mechanisms."}, {"title": "3.2.4 Case Study 3: Layered Security Strategy for Financial Institutions", "content": "A key tutorial exercise involved developing a layered security strategy for a financial institution. Students used ChatGPT to generate an initial security framework, which included physical security, network protection, and incident response. However, upon further analysis, students identified missing provisions for cross-border data encryption and inadequate access control mechanisms. Through research-based refinement, students integrated NIST Cybersecurity Framework standards, encryption protocols,"}, {"title": "4 Results", "content": "The results of integrating GenAI into cybersecurity education are examined through student feedback, instructor observations, and an analysis of the challenges and unexpected outcomes encountered during implementation. These insights highlight the effectiveness of AI-assisted learning, while also addressing areas requiring further refinement."}, {"title": "4.1 Student Feedback and Learning Reflections", "content": "Student reflections provided valuable insights into the reception and effectiveness of GenAI integration within the curriculum. Overall, students responded positively to the structured approach, particularly appreciating the role of AI in streamlining the initial drafting process. Many noted that GenAI served as a useful starting point, enabling them to focus on refining outputs rather than generating content from scratch. One student remarked:\n\"The AI-generated policy was a great starting point, but refining it based on research allowed me to understand the gaps and create a more tailored solution.\"\nSimilarly, another student emphasised the efficiency gains:\n\"Using GenAI helped me focus on improving content rather than starting from scratch, saving time and allowing me to dive deeper into compliance and security details.\"\nBeyond efficiency, student feedback underscored the importance of human oversight in refining AI-generated outputs. Several students reflected on the realisation that AI tools often produce generic or incomplete responses that require critical evaluation and domain-specific expertise. This iterative refinement process contributed to a deeper engagement with regulatory frameworks and industry standards, reinforcing the necessity of balancing automation with analytical reasoning."}, {"title": "4.2 Instructor Observations on Learning Outcomes", "content": "From an instructional perspective, the structured integration of GenAI significantly enhanced student engagement, critical thinking, and contextual application of cybersecurity concepts. This phased approach where students first generated AI-based content, then critically evaluated and refined it, led to higher levels of analytical reasoning compared to traditional instructional methods.\nInstructors observed a notable improvement in students' ability to assess regulatory compliance and adapt security policies to organisational needs. The use of real-world scenarios helped bridge the gap between theoretical knowledge and practical cybersecurity applications, fostering a more holistic understanding of risk management, compliance requirements, and policy development. While some students initially struggled with evaluating AI-generated content, reflection activities and structured research prompts played a critical role in deepening their analytical engagement."}, {"title": "4.3 Challenges Encountered During Implementation", "content": "Despite its overall effectiveness, the integration of GenAI within the curriculum presented several pedagogical and logistical challenges that impacted both student learning and instructional design.\nA primary challenge was the over-reliance on AI-generated outputs among some students. Certain students treated AI-generated responses as authoritative rather than as an initial draft requiring further refinement. This was particularly evident in tasks such as policy development, where students initially submitted responses that lacked depth or contextual alignment with cybersecurity regulations. For instance, in exercises involving email policy refinement, students struggled to identify jurisdiction-specific compliance requirements, such as those outlined in the Privacy Act 1988. Addressing this challenge required additional scaffolding through explicit prompts and guided discussions, emphasising the limitations of AI-generated content and the need for rigorous evaluation and adaptation.\nAnother challenge was the variability in students' familiarity with AI tools. While some students demonstrated proficiency in leveraging GenAI for cybersecurity policy drafting and security strategy development, others required substantial guidance in interpreting AI outputs and identifying relevant modifications. This skill disparity led to an uneven starting point for the cohort, requiring instructors to provide targeted support for students less experienced in evaluating AI-assisted cybersecurity solutions.\nA further difficulty arose in context-specific customisation of AI outputs. AI-generated security frameworks and policies often lacked the specificity required to align with organisational contexts and regulatory mandates. In assessments requiring students to draft security policies for real businesses, aligning AI-generated outputs with industry-specific standards such as the Australian Privacy Act and GDPR proved to be a complex and time-intensive task. Students had to engage in extensive research and iterative refinement to ensure their policies met both operational and regulatory requirements, reinforcing the necessity of domain-specific expertise.\nAlso, assessments that required real-world business engagement posed logistical constraints. While students benefited from working with industry partners, coordinating interviews with business stakeholders presented scheduling difficulties. This challenge was partially addressed by providing a case study alternative, allowing students to apply their knowledge to a controlled scenario rather than relying solely on external collaborations. However, this issue highlighted the inherent complexities of integrating authentic, real-world interactions within structured academic settings."}, {"title": "4.4 Unexpected Benefits and Drawbacks", "content": "Beyond its intended learning objectives, the integration of GenAI produced several unexpected benefits and challenges that further shaped the instructional strategy.\nA significant advantage was the increased engagement and motivation among students, particularly in assignments that simulated real-world cybersecurity challenges. The iterative process of drafting, refining, and presenting AI-assisted security policies encouraged students to take ownership of their work, fostering a more immersive and"}, {"title": "5 Discussion:", "content": "The integration of Generative AI (GenAI) into cybersecurity education has demonstrated significant pedagogical value, enhancing critical thinking, contextual application, and problem-solving abilities. This section discusses the effectiveness of this instructional strategy in meeting learning objectives, compares it to traditional teaching approaches, identifies best practices, and provides recommendations for improvement. Furthermore, it examines the broader implications of GenAI-assisted learning in cybersecurity education."}, {"title": "5.1 Effectiveness in Meeting Learning Objectives", "content": "The structured use of GenAI tools successfully fostered critical evaluation, research-driven refinement, and real-world application of cybersecurity principles. Student feedback and performance data indicate that AI-assisted learning improved problem-solving capabilities and regulatory awareness, while also encouraging deeper engagement with cybersecurity frameworks. Figure 1 highlights key student reflections, demonstrating that most students valued AI as a starting point for drafting cybersecurity policies and security strategies, but also acknowledged the need for further refinement and contextualisation. A notable proportion of students reported challenges in identifying gaps in AI-generated content, which underscores the necessity of structured research-based refinement exercises. The effectiveness of different instructional strategies in promoting learning outcomes is further illustrated in Figures 3 and 4, which map teaching strategies to specific learning objectives. Notably, assessments and case study-based exercises had the highest impact on regulatory awareness and problem-solving abilities, while tutorials played a critical role in improving AI confidence and research skills.\nTable 4 further substantiates these findings, demonstrating how the integration of AI tools across tutorials and assessments facilitated real-world problem-solving, alignment with regulatory standards, and critical evaluation of AI-generated security policies."}, {"title": "5.2 Comparison with Traditional Teaching Approaches", "content": "Compared to conventional lecture-based methods, the integration of AI-assisted learning strategies provided a more interactive, hands-on learning experience. Traditional approaches often rely on passive knowledge acquisition, where students primarily engage with cybersecurity frameworks through lectures and reading materials. In contrast, the structured AI-driven exercises encouraged active participation, requiring students to generate, critique, and refine AI outputs using industry standards and academic research. The iterative nature of the exercises bridged the gap between theoretical knowledge and practical application, a limitation often observed in traditional cybersecurity education. The business engagement component further exposed students to real-world cybersecurity challenges, fostering a deeper understanding of risk management, compliance, and security policy adaptation. These findings align with broader trends in cybersecurity education, emphasising experiential learning as a crucial factor in preparing students for industry challenges. However, the introduction of AI into cybersecurity education also introduced unique challenges, particularly regarding student reliance on AI-generated content and variability in AI literacy levels. As"}, {"title": "5.3 Best Practices, Future Recommendations, and Broader Implications", "content": "The findings from this study highlight key best practices for integrating GenAI into cybersecurity education, emphasising structured AI engagement, external research integration, and real-world application. While the implementation of GenAI demonstrated pedagogical benefits, several areas for improvement emerged, necessitating refinements to instructional strategies and broader considerations for the future of AI in cybersecurity education."}, {"title": "5.3.1 Best Practices for AI-Assisted Learning in Cybersecurity", "content": "The structured integration of GenAI into tutorials, assessments, and case-based exercises provided students with an opportunity to develop analytical skills, critically engage with AI-generated outputs, and refine cybersecurity strategies based on regulatory frameworks. Our implementation revealed several key best practices that enhanced the educational experience. The provision of structured guidance on AI tool usage proved essential, ensuring that students actively critiqued and refined AI-generated content rather than passively accepting outputs. In this context, instructor-led scaffolding played a crucial role in helping students recognise both the strengths and limitations of AI-generated security policies and risk assessments.\nCritical thinking and regulatory awareness were reinforced through the incorporation of external research components, which encouraged students to validate AI-generated responses against established industry standards such as the NIST Cybersecurity Framework, GDPR, and ISO 27001. Additionally, the embedding of real-world case studies and business engagement activities helped contextualise AI-assisted learning within authentic cybersecurity challenges, fostering practical application and regulatory alignment. By integrating these best practices into cybersecurity education, educators can maximise the pedagogical benefits of AI while effectively mitigating its inherent limitations."}, {"title": "5.3.2 Recommendations for Future Implementation", "content": "While the integration of GenAI significantly enhanced student engagement and real-world problem-solving skills, challenges such as AI literacy gaps, regulatory adaptation, and student variability in AI competency necessitate targeted improvements. Table 6 summarises key recommendations based on instructor observations and student feedback, outlining actionable refinements for AI-assisted instruction.\nFuture implementations should address several critical areas for improvement. Our observations highlighted the need for enhanced AI literacy support, as some students required additional guidance in navigating AI-generated outputs, particularly in differentiating accurate content from AI hallucinations. To address this, introducing AI literacy modules and structured evaluation rubrics can help students develop a balanced understanding of Al's role in cybersecurity.\nThe alignment with jurisdiction-specific regulations emerged as another key focus area, as students faced challenges in adapting AI-generated security policies to specific compliance frameworks. To strengthen this aspect, developing targeted tutorials on regional regulations, such as the Australian Privacy Act, GDPR, and PCI DSS, will reinforce the importance of regulatory adherence in cybersecurity decision-making. Additionally, the varying levels of student proficiency called for developing tiered exercises to accommodate different skill levels. While some students thrived in AI-assisted problem-solving, others required additional support. Implementing adaptive learning approaches, including tiered case studies and optional advanced cybersecurity scenarios, will provide differentiated learning opportunities that cater to both foundational and advanced learners.\nThese refinements will ensure that AI-assisted cybersecurity education remains both effective and scalable, preparing students for dynamic and evolving security challenges."}, {"title": "5.3.3 Broader Implications for Cybersecurity Education", "content": "The integration of GenAI in cybersecurity education represents a significant shift in pedagogical strategies, reinforcing the role of AI literacy, ethical considerations, and regulatory compliance in modern cybersecurity curricula. By combining AI-assisted learning with critical evaluation, research-based refinement, and real-world application, this approach fosters a more adaptive, problem-solving-oriented educational experience.\nBeyond the classroom, these findings align with emerging industry trends, where AI is increasingly leveraged for automated security policy generation, threat detection, and compliance management. However, the limitations observed in AI-generated security policies highlight the necessity of human expertise in refining and contextualising AI outputs. This underscores the importance of educating future cybersecurity professionals to use AI as an assistive tool rather than a replacement for analytical reasoning.\nLooking ahead, future research should explore the long-term impact of AI-driven cybersecurity education, particularly in relation to knowledge retention, adaptability to emerging threats, and industry preparedness. Additionally, investigating the"}, {"title": "6 Limitations", "content": "While this study demonstrates the potential of Generative AI (GenAI) in cybersecurity education, several limitations must be acknowledged. The findings are based on a single academic term, and the integration of AI tools within the curriculum was subject to evolving technological capabilities and student familiarity with AI-driven learning. As a result, the effectiveness of AI-assisted instruction may vary across different cohorts, institutions, and pedagogical contexts.\nA key limitation pertains to the variability in students' prior experience with AI tools, which influenced their ability to critically assess and refine AI-generated content. While some students demonstrated strong analytical skills in evaluating AI outputs against regulatory frameworks and security best practices, others exhibited a tendency to accept AI-generated responses uncritically. This disparity required additional scaffolding and instructor intervention, highlighting the challenge of ensuring consistent engagement across a diverse student population.\nMoreover, while AI tools facilitated the rapid generation of cybersecurity policies and risk assessments, their outputs often lacked contextual specificity and regulatory depth. This limitation underscores the necessity of human oversight in validating AI-generated recommendations, reinforcing the idea that AI should be regarded as an assistive tool rather than a definitive authority on cybersecurity practices. Additionally, the reliance on publicly available AI models introduced concerns regarding data accuracy, potential biases, and the evolving nature of AI training datasets, which may impact the reproducibility of results in future studies.\nTechnical limitations of the GenAI tools used also impacted the study's implementation. The reliance on publicly available versions of ChatGPT, Claude, and DeepSeek meant that students experienced occasional service disruptions and usage limitations. These tools' responses sometimes showed inconsistency across different sessions, particularly in generating security policies and regulatory compliance documentation. Additionally, the AI models' knowledge cutoff dates meant that coverage of the newest cybersecurity threats and regulations required manual updates and instructor intervention. The inability to fine-tune these models for specific cybersecurity education needs also limited the customisation of AI-generated content for different skill levels and learning objectives.\nFinally, the study was conducted within a structured academic setting where students operated under guided exercises and assessments. The extent to which these findings generalise to professional cybersecurity environments, where AI tools are employed in dynamic and high-risk decision-making contexts, remains an area for further investigation. Addressing these limitations in future research will be crucial to refining AI-assisted learning strategies and ensuring their long-term viability in cybersecurity education."}, {"title": "7 Conclusion", "content": "This study introduced a structured framework for integrating Generative AI (GenAI) into cybersecurity education, demonstrating its potential to enhance critical thinking, regulatory awareness, and practical problem-solving skills. The implementation followed a two-stage approach that embedded GenAI within tutorial exercises for iterative learning and integrated AI-assisted tasks into assessments for real-world application, providing students with comprehensive, hands-on engagement with AI-driven cybersecurity strategies. Through case studies focused on security policy development, the Security Systems Development Life Cycle (SecSDLC), and layered security strategies, the study showcased how AI can be leveraged to support learning while maintaining the rigour of cybersecurity education.\nKey findings indicate that GenAI significantly streamlined initial content generation, enabling students to focus on evaluating, refining, and aligning outputs with industry standards and regulatory requirements. However, the study also highlighted challenges related to over-reliance on AI and variability in AI literacy, underscoring the need for structured guidance and research-driven refinement. The effectiveness of AI-assisted learning was contingent on three critical factors: clear instructional scaffolding, emphasis on research-informed modifications, and contextual alignment with real-world cybersecurity challenges.\nBy positioning AI as an assistive tool rather than a substitute for human expertise, this study reinforces the value of combining automation with analytical reasoning in cybersecurity education. Future research should explore long-term impacts on student competency, adaptive AI-learning models, and scalable implementations of AI-assisted instruction in cybersecurity training."}, {"title": "Appendix A Sample Tutorial: Developing an Email Policy Using Generative AI", "content": "A.1 Case Scenario: XYZ Bank\nXYZ is an Australian bank that offers banking, investment, and financial services to small businesses and the public. The bank has multiple branches in all state capitals in Australia and plans to expand into rural areas through a loan program.\nXYZ's organisational structure consists of four major divisions: Business, IT, Accounting and Finance, and Human Resources. Employees communicate via an internal network, and free wireless LAN access is provided to customers and visitors. The bank also offers online banking services, and customer service representatives access customer data for loan approvals, financial profiling, and marketing.\nDue to a recent series of information security attacks, XYZ's Chief Information Security Officer (CISO) is determined to improve the bank's security systems, particularly in email security policies.\nA.2 Task: Developing an Email Policy for XYZ Bank\nThe objective of this tutorial is to help students understand:\n\u2022 The role of Generative AI in drafting cybersecurity policies.\n\u2022 How to critically analyse AI-generated outputs.\n\u2022 The importance of regulatory compliance and security best practices in email communication.\nThe tutorial consists of three parts:\nA.2.1 Part 1: Generate an Initial Policy Using Generative AI\n\u2022 Use a Generative AI tool (e.g., ChatGPT, Gemini, Claude) to generate an Email Policy for XYZ Bank based on the provided case scenario.\n\u2022 Copy and paste the AI-generated policy into the submission.\n\u2022 Important: Do not edit the AI-generated response at this stage.\nA.2.2 Part 2: Develop a Research-Based Policy\nRead the research paper: \"Analysis of the Human Factor in Cybersecurity: Identifying and Preventing Social Engineering Attacks in Financial Institutions\" by Ibrahim Momoh, Gabriel Adelaja, and Ghaffar Ejiwumi.\nUsing the key insights from the paper, revise and refine the email policy by integrating:\n\u2022 Overview and Purpose: Highlight email security risks and the importance of phishing awareness training.\n\u2022 Acceptable Use and Behaviour: Establish rules prohibiting employees from clicking on unknown links or opening unsolicited attachments."}]}