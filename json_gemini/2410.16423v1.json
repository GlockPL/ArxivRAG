{"title": "Position: Challenges and Opportunities for Differential Privacy in the U.S. Federal Government", "authors": ["Amol Khanna", "Adam McCormick", "Chris Aguirre", "Andre Nguyen", "Edward Raff"], "abstract": "In this article, we seek to elucidate challenges and opportunities for differential privacy within the federal government setting, as seen by a team of differential privacy researchers, privacy lawyers, and data scientists working closely with the U.S. government. After introducing differential privacy, we highlight three significant challenges which currently restrict the use of differential privacy in the U.S. government. We then provide two examples where differential privacy can enhance the capabilities of government agencies. The first example highlights how the quantitative nature of differential privacy allows policy security officers to release multiple versions of analyses with different levels of privacy. The second example, which we believe is a novel realization, indicates that differential privacy can be used to improve staffing efficiency in classified applications. We hope that this article can serve as a nontechnical resource which can help frame future action from the differential privacy community, privacy regulators, security officers, and lawmakers.", "sections": [{"title": "1 Differential Privacy", "content": "Despite executive orders and guidance encouraging the use of revolutionary privacy enhancing technologies to reduce the risk related to the rise of big data, artificial intelligence (AI), and machine learning (ML), institutional barriers impede the widespread deployment of differential privacy throughout the U.S. federal government. Released over the past year, \"Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence,\" and NIST SP 800-226 \"Evaluating Differential Privacy Guarantees\u201d both explore the need for adopting privacy enhancing technology to mitigate privacy risks arising from increased data processing [1, 2].\nDifferential privacy, a privacy-preserving algorithmic framework, is one such privacy preserving technology that can be applied to a wide variety of common government agency use cases and has already been studied, documented, and deployed in commercial industry. Differential privacy measures the privacy risk to individuals by comparing the output of an algorithm with an individual's data to the output without that data. If the output can be heavily swayed by a single datapoint, the algorithm can betray the privacy of specific individuals. If this cannot happen, the output does not indicate if a datapoint was used as an input [3]. Differential privacy employs quantitative parameters to set the amount that an output can change with each additional datapoint, and by modifying these parameters, practitioners can set the allowable privacy risk of an algorithm. Differential privacy"}, {"title": "2 Attempts and Challenges of Federal Differential Privacy Implementations", "content": "Within the federal government, differential privacy has the potential to reduce potential harms caused by data leaks and enable more public releases of data and statistics, which can lead to better informed political and economic decisions [5]. Indeed, the U.S. Census Bureau has already taken steps in this direction by using differential privacy to mask some of the sensitive statistics it released from the 2020 Census [6]. By using differential privacy, the Bureau guaranteed that specific households cannot be identified from the privatized statistics while still releasing information to lawmakers [7].\nDespite the U.S. government touting the benefits of differential privacy and encouraging its adoption for a variety of use cases, the use of the technology has largely been restricted, to wit there are no other prominent examples. We argue that the limited government usage of the technology can be attributed to a lack of awareness among government program managers, challenging deployments in large-scale systems, and most critically, unclear guidance to security officers challenging deployments in large-scale systems. We discuss each of these challenges in turn:\n1. Lack of Awareness: differential privacy is a statistical method for guaranteeing privacy, which departs from older privacy-preservation techniques that were deterministic, but ineffective. Older methods, like record anonymization, removing sensitive attributes, and K-anonymity, operated directly on datasets [8]. These methods could typically be used as a dataset preprocessing step prior to using a standard algorithm for statistics or machine learning. However, these methods are still very vulnerable to privacy attacks; indeed, government datasets that employed the aforementioned methods have been attacked in the past [9]. Differential privacy is robust to all attacks, but effectively using it requires a deep understanding of probability and statistics, which many privacy professionals are unfamiliar with. Additionally, differential privacy's statistical nature often requires modifying an entire algorithmic pipeline; instead of simply preprocessing the rows of a dataset, significant modifications must be made to a target algorithm to produce useful results [10]. This statistical nature can also impact decision-makers, as the notion that data is secure but probabilistically protected is a departure from better-understood methods like cryptography. Finally, current implementations of differential privacy use functional programming APIs, which can be less accessible to data scientists than more common object-oriented frameworks [11].\n2. Challenging Deployments: differential privacy can be challenging to use for tasks with multiple goals and intermediate computations [12]. In such tasks, there are many potential statistical missteps that can produce sources of privacy leaks. Large technology companies have approached this challenge by focusing on deploying differential privacy for targeted tasks and including differential privacy experts to oversee deployments. However, government agencies are often tasked with releasing social and political information to the public, and these datasets often require multiple sources and released values.\n3. Unclear Guidance: most importantly, many overseeing security officers in the government are reluctant to use differential privacy since no government authority has released official approvals for using the technology in privacy-critical tasks. Since bureaucratic approvals exist for older technologies, security officers choose to use these technologies despite their increased susceptibility to privacy attacks such as membership inference and model inversion attacks [13, 14]. We believe that for differential privacy to permeate through government applications, a government authority must approve its use in multiple privacy-critical applications, since this can set a government standard.\nEach of these challenges was realized in the Census Bureau's differential privacy deployment. First, approvals for differential privacy use did not exist, and the Census used privacy-enhancing dataset preprocessing techniques in conjunction with differential privacy [15]. Next, a number of released values had to be modified since the Census contains many codependent statistics, and adding noise"}, {"title": "3 Differentiating Private Releases by Perceived Trustworthiness", "content": "As a state-of-the-art privacy enhancing technology, we believe that differential privacy should be incorporated into more government agency applications. In addition, we argue that differential privacy's parametric approach enables data security officers to further improve customizability and efficiency in government applications. Specifically, security officers can inject their knowledge of a dataset's sensitivity and an algorithm's end users through differential privacy's quantitative parameters [20]. This customization means that different versions of the same algorithm can be released with varying levels of privacy based on a perceived level of risk.\nFor example, if an agency working with personal health information built a machine learning model on patient records, the agency could enable physicians to access a less private and more accurate model while releasing a more private version to academics and insurance companies. This is because the agency may see registered physicians as a trusted group, while researchers and insurance companies are unknown public entities. By releasing the two different versions of the model, the agency enables physicians to access a state-of-the-art model for disease diagnosis and treatment while spurring advancements in research and policy with a more private model. Using other privacy enhancing methods without this fine-grained control may have resulted in a less accurate model for physicians or a health data breach, either of which are unsatisfactory outcomes."}, {"title": "4 Improving Efficiency with Privacy: An Unrealized Frontier", "content": "The previous section's example highlights differential privacy's applicability in civilian agencies. We also argue that differential privacy could enable significant efficiency gains in classified settings, and we believe that we are the first to identify this opportunity. Government defense agencies often generate statistics and models on datasets classified, e.g., as Top Secret. Under current guidance, every person with access to any part of these modeling pipelines, such as data annotators, data scientists, machine learning engineers, software engineers, deployment specialists, and even eventual users, must have a Top Secret clearance. This is because all of these individuals can either access raw Top Secret data or derivatives of this data, which could be reverse-engineered into the Top Secret data. However, staffing technical roles at such a high clearance level is challenging due to a shortage of cleared staff, and as such, agencies often choose not to pursue projects in highly classified settings due to a lack of cleared and capable personnel [21, 22, 23]. We believe that if differential privacy was used when creating data derivatives, certain positions in these pipelines could be staffed at lower clearance levels. For example, if a sufficient level of privacy was used when generating statistical reports on Top Secret datasets, software engineers and deployment specialists would be unable reverse engineer these reports to identify specific Top Secret datapoints. Thus, there is a reduced risk associated with these reports, meaning that personnel handling, deploying, and using these reports can have lower clearance levels. Staffing at lower clearance levels is significantly easier and cheaper, so adopting this policy on highly classified projects can significantly reduce time and monetary costs and allow agencies to pursue more initiatives."}, {"title": "5 Conclusion", "content": "This work seeks to communicate the challenges our team of differential privacy researchers, privacy lawyers, and data scientists have had when communicating about differential privacy to decision-makers at government agencies. We have noticed that senior data scientists are wary of using the technology due to its significant departure from previous methods and its difficulty in scaling to large applications and unstructured datasets. Security officers often avoid novel privacy preserving"}]}