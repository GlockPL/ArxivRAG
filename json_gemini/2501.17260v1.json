{"title": "ViT-2SPN: Vision Transformer-based Dual-Stream Self-Supervised Pretraining Networks for Retinal OCT Classification", "authors": ["Mohammadreza Saraei", "Igor Kozak", "Eung-Joo Lee"], "abstract": "Optical Coherence Tomography (OCT) is a non-invasive imaging modality essential for diagnosing various eye diseases. Despite its clinical significance, developing OCT-based diagnostic tools faces challenges, such as limited public datasets, sparse annotations, and privacy concerns. Although deep learning has made progress in automating OCT analysis, these challenges remain unresolved. To address these limitations, we introduce the Vision Transformer-based Dual-Stream Self-Supervised Pretraining Network (ViT-2SPN), a novel framework designed to enhance feature extraction and improve diagnostic accuracy. ViT-2SPN employs a three-stage workflow: Supervised Pretraining, Self-Supervised Pretraining (SSP), and Supervised Fine-Tuning. The pretraining phase leverages the OCTMNIST dataset (97,477 unlabeled images across four disease classes) with data augmentation to create dual-augmented views. A Vision Transformer (ViT-Base) backbone extracts features, while a negative cosine similarity loss aligns feature representations. Pretraining is conducted over 50 epochs with a learning rate of 0.0001 and momentum of 0.999. Fine-tuning is performed on a stratified 5.129% subset of OCTMNIST using 10-fold cross-validation. ViT-2SPN achieves a mean AUC of 0.93, accuracy of 0.77, precision of 0.81, recall of 0.75, and an F1 score of 0.76, outperforming existing SSP-based methods. These results underscore the robustness and clinical potential of ViT-2SPN in retinal OCT classification. The code is available in https://github.com/mrsaraei/ViT-2SPN.git.", "sections": [{"title": "1. Introduction", "content": "Medical imaging is a cornerstone of modern healthcare, enabling a detailed exploration of human anatomy and function. Imaging modalities, such as Optical Coherence Tomography (OCT), facilitate accurate diagnosis and monitoring of diseases (Bordbar et al., 2025). \u041e\u0421\u0422, a non-invasive imaging tool, has transformed ophthalmology by providing high-resolution cross-sectional retinal images, critical for detecting retinal diseases (Subhedar and Mahajan, 2023). Using low-coherence light and interferometry, OCT generates axial (A-scan)"}, {"title": "2. Related Works", "content": "Various DL models have been developed to improve the classification of retinal OCT, showing different levels of accuracy and efficiency. High-performing models such as Khalili et al.'s OCTNet (99.5% accuracy) and Laouarem et al.'s hybrid CNN-ViT (99.77% accuracy) exhibited impressive results. However, they were limited by high computational costs and overfitting on small datasets, reducing their practical use (Khalil et al., 2024), (Laouarem et al., 2024). Prabha et al.'s lightweight model (97%-98.08% accuracy) balanced performance and efficiency but struggled with complex clinical conditions, highlighting a tradeoff between accuracy and scalability (Prabha et al., 2024). Karthik et al.'s noise reduction technique improved accuracy by 2.44%. However, its scope was narrow (Karthik and Mahadevappa, 2024), while Dai et al.'s pretraining-based approach (95% accuracy) showed strong domain-specific performance but lacked generalizability (Dai et al., 2024).\nOn the other hand, SSL methods have garnered attention for improving classification performance, especially in data-scarce scenarios. Gholami et al. and Fang et al. demonstrated the effectiveness of SSL, with Gholami's method for Macular Telangiectasia and Fang's self-supervised patient-specific feature learning (SSPSF) achieving accuracies of 89.8% and 97.74%, respectively (Gholami et al., 2024), (Fang et al., 2022). SSL models also improved accuracy for complex conditions, such as Full-Thickness Macular Holes (FTMH) and Polypoidal Choroidal Vasculopathy (PCV) (Wheeler et al., 2024), (Wongchaisuwat et al., 2023). Bundele et al. compared SSL models, finding that Momentum Contrast (MoCov3) achieved an AUC of 96.81% and 79.96% accuracy, while Barlow Twins and Nearest-Neighbor Contrastive Learning reached 97.11% AUC, with accuracies of 77.28% and 79.16%, respectively (Bundele et al., 2024). Despite their promise, SSL methods face challenges in designing robust pretraining strategies for clinical conditions and may perform sub-optimally on small or heterogeneous datasets."}, {"title": "3. Methods", "content": "As shown in Figure 1B, our methodology consists of three stages to optimize the model to detect retinal diseases on OCT images. First, the model is initialized using ViT-base weights from ImageNet, transferring general visual knowledge to provide a strong foundation for feature extraction. Next, a dual-stream SSP architecture is applied to unlabeled OCT images, with a pretrained ViT-base backbone to capture rich feature extraction. Finally, the pretrained model is fine-tuned on a small set of labeled OCT images, with iterative hyperparameter optimization to improve classification accuracy and performance evaluation."}, {"title": "3.1. Dataset", "content": "We use the OCTMNIST dataset\u00b9 to evaluate the efficacy of our training policy. OCTMNIST is derived from a publicly available retinal OCT dataset included in the MedMNISTv2 collection (Yang et al., 2023a). This dataset consists of four disease classes: Choroidal Neovascularization (CNV), Diabetic Macular Edema (DME), Drusen, and Normal (see Figure 2). OCTMNIST is divided into 97,477, 10,832, and 21,816 images for training, validation,"}, {"title": "3.2. Proposed Method", "content": "As demonstrated in Figure 3, our proposed method, Vision Tranformer-based dual stream self-supervised pretraining networks (ViT-2SPN), is a novel framework designed for SSL and fine-tuning in OCT image classification tasks. The framework leverages dual-stream networks with a ViT-based backbone and contrastive learning objectives to generate robust latent feature representations. It uses pretraining on unlabeled OCTMNIST datasets, followed by fine-tuning for classification into clinical categories, such as Normal, DME, CNV, and Drusen."}, {"title": "4. Experiments", "content": "The pretraining phase uses the OCTMNIST data set, which contains 97,477 unlabeled retinal OCT images. Data augmentation simulates variability and enhances generalization through random rotations, flips, grayscale transformations, color jitter, and normalization, creating dual augmented views as input for a dual-stream network. This architecture comprises online (encoder) and target (momentum encoder) streams, each using a ViT-base backbone pretrained on ImageNet to capture long dependencies vital for OCT imaging. The online stream processes one augmented view, while the target stream is updated using momentum-based moving averages for stability and refined representations.\nFeatures from both streams are projected into a shared latent space through projection and prediction heads, aligning representations via a negative cosine similarity loss. This loss ensures the alignment of corresponding views while maintaining distinct representations for non-corresponding pairs. Specifically, the loss minimizes the cosine distance between online prediction features Ponline and target projection features Ztarget, as defined in Equation (1) (Appendix A).\nThis represents the total loss as the mean negative cosine similarity between the online network's predicted features, $P_{online}^{(i,s)}$, and the target network's projected representations, $Z_{target}^{(i,s)}$, across all samples in a batch (N) and gradient accumulation steps (S).\nThe numerator, $P_{online}^{(i,s)}Z_{target}^{(i,s)}$, computes the dot product (similarity) between the two feature vectors. At the same time, the denominator normalizes this similarity by their magnitudes, ensuring that the comparison is scale-invariant. By averaging this loss across the entire data set processed in S steps, the method balances memory constraints and training stability while encouraging the online network to align with the representations of the target network. This is iteratively computed over epochs, and the momentum update ensures stable alignment between online and target features. Training employs a learning rate of 10-4, a momentum of 0.999, and mixed precision techniques to optimize computational efficiency. Gradient accumulation over four steps further manages memory requirements for large batch sizes, and training is conducted for 50 epochs on a multi-GPU setup. The resulting pretrained weights encapsulating the learned representations are saved for subsequent fine-tuning. During fine-tuning, the pretrained backbone is adapted for supervised classification using a stratified subset of the OCTMNIST dataset. Data augmentation techniques, such as resizing, random rotations, flips, color jitter, and normalization are applied to improve generalization. Stratified 10-fold cross-validation ensures balanced class distribution across training and validation sets. A linear classification head, comprising a fully connected layer, dropout, batch normalization, and ReLU, maps latent features to diagnostic categories.\nTraining employs weighted cross-entropy loss to address class imbalances, optimized using the Adam optimizer (weight decay 10-4). A learning rate scheduler (ReduceLROnPlateau) adjusts the learning rate dynamically, while early stopping with three-epoch patience ensures efficient convergence. The best model, based on cross-validation ROC-AUC scores, is selected for evaluation. A separate test set of 500 samples ensures a robust and independent assessment."}, {"title": "4.1. Experimental Setup", "content": "During the SSP phase, the model utilizes the unlabeled OCTMNIST dataset, which comprises 97,477 training samples. The training process is conducted with a mini-batch size of 128, a learning rate of 0.0001, and a momentum rate of 0.999, spanning a total of 50 epochs. The ViT-base architecture, pretrained on the ImageNet dataset, is employed as the backbone. In the fine-tuning phase, the model leverages 5.129% of the labeled OCTMNIST dataset, following a 10-fold cross-validation strategy. Each fold consists of 4,500 training samples and 500 validation samples, with an additional 500 samples reserved for testing. The fine-tuning process is carried out using a batch size of 16, the same learning rate from the pretraining phase, a dropout rate of 0.5, and 50 epochs (see Appendix B)."}, {"title": "4.2. Results", "content": "In Table 1, we present an evaluation of the SSP models in the OCTMNIST dataset, showcasing the superior performance of ViT-2SPN across all key metrics. ViT-2SPN consistently outperforms prominent models, including Bootstrap Your Own Latent (BYOL) (Grill et al., 2020), Momentum Contrast (MoCo) (He et al., 2020) and its variants (Chen et al., 2020c, 2021), Simple Framework for Contrastive Learning (SimCLR) (Chen et al., 2020a) and its updated version (Chen et al., 2020b), Swapping Assignments Between Views (SwAV) (Caron et al., 2020), and Simple Siamese Representation Learning (SimSiam) (Chen and He, 2020). The evaluation metrics include mean Area Under the Curve (mAUC), accuracy, precision, F1-score, and recall."}, {"title": "5. Conclusion", "content": "This study presents ViT-2SPN, a Vision Transformer-based dual stream self-supervised pretraining network, designed to address challenges such as limited annotated datasets and concerns for patient privacy. Using SSL and an innovative pretraining strategy on the OCTMNIST dataset, ViT-2SPN enhances the extraction of features and achieves significant improvements in the classification of retinal diseases. The performance of the model, with a mean AUC of 0.93 and an accuracy of 0.77, precision of 0.81, recall of 0.75, and an F1-score of 0.76, demonstrates its robustness and effectiveness compared to other SSP-based approaches. The integration of ViT-base backbone, dual-stream data augmentation,"}, {"title": "Appendix A. Proof of Theorem 1", "content": "The loss function used in our training is based on cosine similarity, which ensures that the online prediction features Ponline align closely with the corresponding target projection features Ztarget.\nLet:\n\u2022 $Z_{online}^{(i)} \\in R^d$ be the online network's projected representation for view i.\n\u2022 $P_{online}^{(i)} \\in R^d$ be the online network's predicted feature for view i.\n\u2022 $Z_{target}^{(i)} \\in R^d$ be the target network's projected representation for view i.\nFor two augmented views of the same image x:\n1. Online representations:\n$Zonline = f_{online} (x_1)$ and $ponline = g_{online} (Zonline)$\n2. Target representations:\n$Z_{target} = f_{target} (x_2)$\nwhere f(\u00b7) is the encoder (backbone network), and g(\u00b7) is the projection head.\nThe loss for a single pair of representations is as follows:\n$L_{pair} = -CosineSimilarity(P_{online}, Z_{target}) = \\frac{P_{online} \\cdot Z_{target}}{|| P_{online}|| || Z_{target}||}$\nSince we compute the mean over the batch, the total loss for a batch of size N is:\n$L_{batch} = \\frac{1}{N} \\sum_{i=1} \\frac{P_{online}^{(i)}Z_{target}^{(i)}}{|| P_{online}^{(i)}|| || Z_{target}^{(i)}||}$\nIn our implementation, gradient accumulation is used to handle memory constraints by splitting the computation across S steps:\n$L_{accumulated} = \\sum_{s=1}^{S} \\frac{L_{batch}^{(s)}}{S}$\nThe target network is updated using an exponential moving average of the weights of the online network:\n$O_{target} \\leftarrow m\\cdot O_{target} + (1 - m) \\cdot \\theta_{online}$\nwhere m, momentum, is a hyperparameter.\nThe overall loss can be expressed as follows:\n$L_{total} = -\\frac{1}{N\\cdot S} \\sum_{s=1}^{S} \\sum_{i=1}^{N} \\frac{P_{online}^{(i,s)}Z_{target}^{(i,s)}}{|| P_{online}^{(i,s)}|| || Z_{target}^{(i,s)}||}$"}]}