{"title": "M\u00b3EL: A Multi-task Multi-topic Dataset for Multi-modal Entity Linking", "authors": ["Fang Wang", "Shenglin Yin", "Xiaoying Bai", "Minghao Hu", "Tianwei Yan", "Yi Liang"], "abstract": "Multi-modal Entity Linking (MEL) is a fundamental component for various downstream tasks. However, existing MEL datasets suffer from small scale, scarcity of topic types and limited coverage of tasks, making them incapable of effectively enhancing the entity linking capabilities of multi-modal models. To address these obstacles, we propose a dataset construction pipeline and publish M\u00b3EL, a large-scale dataset for MEL. M\u00b3EL includes 79,625 instances, covering 9 diverse multi-modal tasks, and 5 different topics. In addition, to further improve the model's adaptability to multi-modal tasks, We propose a modality-augmented training strategy. Utilizing M\u00b3EL as a corpus, train the CLIPND model based on CLIP (ViT-B-32), and conduct a comparative analysis with an existing multi-modal baselines. Experimental results show that the existing models perform far below expectations (ACC of 49.4%-75.8%), After analysis, it was obtained that small dataset sizes, insufficient modality task coverage, and limited topic diversity resulted in poor generalisation of multi-modal models. Our dataset effectively addresses these issues, and the CLIPND model fine-tuned with M\u00b3EL shows a significant improvement in accuracy, with an average improvement of 9.3% to 25% across various tasks. Our dataset is available at https://anonymous.4open.science/r/M3EL.", "sections": [{"title": "Introduction", "content": "Multi-modal Entity Linking is a crucial research direction in Natural Language Processing (NLP) and Computer Vi-sion (CV) (Sevgili et al. 2022). It aims to achieve cross-modal information integration and understanding by associating entities in different modalities (e.g., text, images, etc.), and to improve the accuracy of linking to entities in the Knowledge Graph (KG). It is an important foundation for NLP downstream tasks, e.g., Information Retrieval (Chang et al. 2006; Martinez-Rodriguez, Hogan, and Lopez-Arevalo 2020) and Question-Answer systems (Allam and Haggag 2012; Moll\u00e1, Van Zaanen, and Smith 2006). Despite recent research progress, existing MEL methods face the following challenges existing in training data, thus far from meeting the requirements of real-world applications.\nThere are many limitations of current MEL datasets, which are summarised in Table 1. Firstly, existing datasets are generally small in scale; for instance, WIKIPerson (Sun et al. 2022) involves 13K entities and WIKIDiverse (Wang et al. 2022a) covers 40K entities, which are insufficient for complex tasks. Secondly, the scope of modal tasks is restricted (Hoffart et al. 2011; Moon, Neves, and Carvalho 2018; Gan et al. 2021), primarily to Text-Text, Image-Text, or Image-(Image+Text), while neglecting other potentially valuable tasks such as Text-Image and Text-(Image+Text). Additionally, most datasets are confined to single topic (Guo and Barbosa 2018; Zhang, Li, and Yang 2021), such as person or news, and lack the ability to generalise across topics. These constraints significantly impede the development of MEL tasks.\nIn this paper, we address these challenges by releasing the large-scale multi-task, multi-topic, multi-modal entity linking dataset, termed M\u00b3EL, which has three unique key properties. First, the dataset contains 79K instances and 318.5K images, which is nearly 10 times the instances in WIKIPerson (Sun et al. 2022) and 6.37 times the images in WIKIDiverse (Wang et al. 2022a). Second, M\u00b3EL covers the widest range of multi-modal tasks, including: Text-Text, Text-Image, Text-(Image+Text), Image-Text, Image-Image, Image-(Image+Text), (Image+Text)-Text, (Image+Text)-Image, (Image+Text)-(Image+Text). Finally, the M\u00b3EL dataset provides more extensive topics (movies, common, person, books, sports), while others provide only a single topic. This implies that our dataset is a more challenging setting.\nFurthermore, we benchmark many existing models in our dataset, the experimental results reveal that existing models exhibit sub-optimal performance, with accuracy (ACC) ranging from 49.4% to 75.8%. To improve the adaptability of MEL methods, we introduce a modality-augmented training strategy to fine-tune the CLIP (ViT-B-32) (Radford et al. 2021) model. This strategy, by expanding the expressiveness of modal features, augments the training process, resulting in the model CLIPND. Following experimentation and analysis, the CLIPND model obtained after training demonstrates significant performance improvements in ACC, with an average increase of 9.3% to 25%. This illustrates the considerable potential of the modal enhancement strategy in handling multi-modal data and emphasizes the research value of the M\u00b3EL dataset.\nTo summarize, the main contributions of this work are as follows:"}, {"title": "Related Work", "content": "Textual Entity Linking Entity Linking (EL) is a fun-damental and classical task in Natural Language Processing (NLP) that has been extensively studied. Early research introduced a variety of datasets to evaluate the performance of EL models, including high-quality manually annotated datasets such as AIDA (Hoffart et al. 2011), and large-scale automatically annotated datasets such as CWEB (Guo and Barbosa 2018). In addition, zero-shot entity linking datasets such as Zeshel (Logeswaran et al. 2019) have also been used for relevant studies. However, the performance of many EL methods on traditional datasets such as AIDA-test, MSNBC (Cucerzan 2007), and AQUAINT (Milne and Witten 2008) has stabilised in recent years, approaching the upper limit of performance for the task, which may indi-cate that performance on these datasets is nearing saturation. With the development of large-scale pre-trained language models, the latest deep learning methods have achieved over 90% accuracy on the AIDA dataset, which also implies that current methods may be approaching a performance bottle-neck (De Cao et al. 2020). Consequently, to advance the field, researchers have designed more challenging tasks such as zero-shot entity linking (Logeswaran et al. 2019; Wu et al. 2019), global coherence exploitation (Chen et al. 2020), NIL prediction (Rao, McNamee, and Dredze 2013), and end-to-end solutions for emerging entities (Broscheit 2020). These tasks are intended to drive further development and break-throughs in the field of EL.\nMulti-modal Entity Linking In recent years, the task of MEL has gained significant attention in the field of NLP. This task aims to utilize both textual and visual informa-tion to map ambiguous mentions to entities in a Knowledge Graph (also known as Knowledge Base). Moon et al. first demonstrated the effectiveness of image information in deal-ing with ambiguous and short textual mentions in social me-dia posts, proposing a zero-shot framework to integrate tex-tual, visual and lexical information for entity linking (Moon, Neves, and Carvalho 2018). However, their proposed dataset is unavailable due to GDPR regulations. Adjali et al. developed a framework for automatically constructing an MEL dataset from Twitter, with small data scale, scarcity of entity types, and ambiguous mentions restricting the usefulness of the dataset (Adjali et al. 2020b). Zhang et al. construct a Chi-nese multi-source social media multi-modal dataset based on the social media platform Weibo, focusing solely on per-son entities (Zhang, Li, and Yang 2021). Gan et al. pub-lished a MEL dataset focusing on roles and characters in the movie domain based on movie reviews (Gan et al. 2021). Both of these two datasets have a single entity type and the generalisation of the training model is weak. Wang et al. constructed a multi-modal entity linking dataset with di-verse contextual themes and entity types, including multiple modal tasks (Wang et al. 2022a). However, all those works still face issues such as small data size, limited entity types, and incomplete multi-modal tasks, therefore, the diversity of entity types and modal tasks in datasets needs further explo-ration and optimization.\nEntity Linking Dataset Our M\u00b3 EL dataset also encom-passes multi-modal image-text datasets. While existing re-lated studies ((Biten et al. 2019; Tran, Mathews, and Xie"}, {"title": "Problem Formulation", "content": "Multi-modal entity linking maps mentions in multi-modal contexts to corresponding entities in a target Knowledge Graph (KG). Since the large number of entities in KG, traversing all entities for linking significantly increases time costs, thus candidate sets of entities are usually predefined. On this basis, we construct a candidate set for each mention, which may or may not contain the correct corresponding entity, accurately reflecting the challenges inherent in multi-modal entity linking tasks.\nFormally, let E represent the set of entities in the KG, typically comprising millions of entities. Let C denote the candidate set in the KG for each mention m, where $C \\subseteq E$. Each mention m or entity $e \\in C$ is associated with its corresponding visual context $V_m, V_e$, and textual context $T_m, T_e$. Here, $T_m$ and $T_e$ denote the textual context surrounding m and e, respectively. $V_m$ refers to the images related to m, while $V_e$ refers to the images associated with e in the KG. Therefore, the multi-modal task of identifying the corresponding entity for mention m can be defined as follows:\n$Sim(M_m, M_e) = arg \\max_{e_i \\in C} \\Psi(En(M_m), En(M_e)),$ (1)\nwhere $M_m$ and $M_e$ represents the modal information of the target mention m and the candidate entity e, respectively, as detailed in Table 2. En denotes Encoder. The function \u03a8 denotes the similarity score between the m and the e."}, {"title": "Data Setups for M\u00b3EL", "content": "The framework of the data construction pipeline of M\u00b3EL is illustrated in Figure 1. In the following, we detail the construction of M\u00b3 EL."}, {"title": "Data Collection", "content": "Raw data Acquisition. To construct the foundational dataset for M\u00b3 EL, we collected raw data from Kaggle\u00b9 that includes URL information related to DBpedia\u00b2, Wikipedia\u00b3, or Wikidata\u00b9. These data files are diverse in format, including JSON, CSV, Pickle, and more. Since DBpedia, Wikipedia and Wikidata pages can interlink, where Wikidata's QID attributes can be used as entity labels, these datasets are well-suited as raw data sources for research in multi-modal entity linking. In addition, these Kaggle datasets are derived from real-world data science scenarios and cover a wide range of topics, reflecting the complexity and diversity of data in the real world. Specifically, we utilized the Wikipedia Movie Plots 5 dataset as the raw data for the movies topic, the Wiki-Wiki 6 dataset for common knowledge topic, the People Wikipedia Data7 for person topic, the Books Datasets & for book topic, and the AIDA-CoNLL-Test for the sports topic.\nData Filter and clean. For the raw dataset, we performed the following data cleaning steps: 1) Removed non-English expressions and single-character mentions. 2) Deleted mentions without associated images or whose images could not be downloaded. Following these pre-processing steps, we finally obtained a total of 82K mentions, each comprising a text-image pair, which serves as the base dataset for M\u00b3 EL."}, {"title": "Data Curation", "content": "Mention Processing. Due to the varying formats of raw data across different topics, we describe in detail the process of obtaining relevant mentions for each topic separately.\nFor the raw data on the movie topic, we utilized the \"Title\" attribute from the wiki movie_plots_deduped.csv file to extract movie titles as mentions. The \"Plot\" attribute was employed to provide contextual information related to these mentions. Additionally, we accessed the mention-related Wikipedia pages through the URLs provided in the \"Wiki Page\" attribute. From these Wikipedia pages, we further navigated to the corresponding Wikidata pages to obtain fundamental information, including the QID and descriptions.\nFor the raw data on commons topic, we utilized the instances provided by the \"Target\" attribute in the wikihow.csv file as mentions, and paragraphs provided by the \"Text\" attribute as contextual information for mentions. By constructing a mention semantic understanding prompt, we input it into the large language model GPT-4 to obtain semantic information related to the mentions. Subsequently, each mention was searched in Wikidata, and the Wikidata entry that best matches the mention is filtered based on the semantic information obtained from GPT-4, and relevant information such as QID and description is obtained.\nFor the raw data on person topic, we utilized the people_wiki.csv file, specifically the \"name\" attribute, as the reference for mentions, and the \"text\" attribute, which provides the context for these mentions. The associated DB-pedia pages were accessed via the \"URL\" attribute. Subsequently, we navigated to the corresponding Wikidata pages to obtain foundational information, including the QID and descriptions.\nFor the raw data on book topics, we utilized the \"Book-Title\" attribute of the books.csv file as a mention for searching the corresponding Wikidata pages to retrieve basic information, including the QID and description. To ensure the accuracy of the search results, we cross-validated them using the Book-Author, Year of Publication, and Publisher information provided in the file. Mentions that could not be found on the Wikidata page were subsequently removed.\nFor the raw data on sport topic, the AIDA_B_dict_raw.pickle file provides mentions, contextual information, and associated QID information in Wikidata in the form of a dictionary. We directly utilized the textual information from the raw data without any processing."}, {"title": "Candidate Construction", "content": "In most entity linking tasks, Wikidata is used as the target Knowledge Graph, linking mentions to their corresponding entities. However, due to the large scale of Wikidata, which contains approximately 95 million entries, matching every mention by traversing all entities would consume substantial computational resources and time. Therefore, to enhance the quality and usability of the dataset, we employ an optimization strategy: for each mention, a refined subset is filtered from Wikidata to serve as the candidate set. This set contains both possible and incorrect corresponding candidates, which truly reflects the challenges of the multi-modal entity linking task.\nThe specific steps are as follows: 1) We search for the mention surface on the Wikidata page and retrieve the top 100 relevant search results. For each search result, we access its Wikidata detail page and systematically extract the key information, including the entity's QID, entity name, and description, among other textual data. 2) We enter the stage of obtaining candidate entity images, and when the"}, {"title": "Data Analysis", "content": "Size and Difficulty Measure. The statistics of M\u00b3EL dataset in detail are shown in Table 3. This dataset was constructed from 66,342 articles and contains a total of 79K entities, covering 318.5K images. Each entity is mapped to a specific entity in Wikidata. Notably, many entities appear multiple times in different sentences of the articles, ensuring that the entities can be fully learned. In addition, Figure 2(a) reports the distribution of entity topics. Unlike existing MEL datasets, which are typically constructed for specific scenarios or tasks, M\u00b3EL covers 5 distinct topics and involves 9 types of multi-modal linking tasks.\nFirstly, we compare the surface form similarity between mentions and ground-truth entities. It is observed that 41.3% of the mentions differ from the surface forms of the ground-truth entities. This significant discrepancy in surface forms presents a challenge for multi-modal entity linking tasks.\nSecondly, we report the candidates for each mention in Figure 2(b). We observe the following: 1) 99.8% of mentions have three candidates, with 55.8% of these mentions having the correct entity ranked first in the candidate set, and 29.4% having the correct entity ranked elsewhere within the candidate set. 2) 14.8% of mentions have candidate sets that do not include the correct entity. This indicates that the candidate set reflects the real-world scenarios of entity linking to a certain extent.\nDiversity of multi-modal tasks. Compared to existing datasets, M\u00b3EL offers a more comprehensive range of multi-modal tasks. Traditional multi-modal entity link-ing datasets, such as Snap (Moon, Neves, and Carvalho 2018), Twitter (Adjali et al. 2020a), and Weibo (Zhang, Li, and Yang 2021), are designed for specific tasks. Although datasets like WIKIPerson (Sun et al. 2022) and WIKIDiverse (Wang et al. 2022a) introduce various forms of multi-modal entity linking tasks, including $I_m-T_e, I_m-I_e$, and $(I+T)_m-I_e$, they ignored other potential linking tasks. In contrast, M\u00b3EL significantly expands the scope of tasks, covering 9 distinct types, specifically, $T_m-T_e, T_m-I_e, T_m-(I+T)_e, I_m-T_e, I_m-I_e, I_m-(I+T)_e, (I+T)_m-T_e, (I+T)_m-I_e, (I+T)_m-(I+T)_e$. This task diversity greatly enhances MEL's ability to generalise across a wide range of multi-modal related tasks, enabling it to better accommodate the diverse needs of users in the real world.\nCoverage of topic types. As shown in Table 1, most of the existing training datasets focus on a limited range of entity topic types, such as person and news. WIKIDiverse (Wang et al. 2022a) extends the types of topic to multiple categories by developing crawling code to extract multi-modal information related to text and images from Wikinews, making it state-of-the-art, but all categories belong to news topic. However, our constructed M\u00b3EL includes 5 types of topics, specifically, movies, common knowledge, person, books, and sports. Moreover, Figure 2(a) illustrates the distribution of M\u00b3EL across different topics."}, {"title": "Model with Augmented Strategy", "content": "CLIP Definition The Contrastive Language-Image Pre-training (CLIP) method has demonstrated significant effectiveness in training visual models using language supervision. In each training step, a large batch of N pairs of images and texts {$X_I, X_T$} is randomly sampled from the training"}, {"title": "Experiments", "content": "Experimental Setups\nDatasets To address the limited scale, coverage of modal tasks, and scarcity of entity topics in existing datasets, we constructed a new dataset named M\u00b3EL, which consists 79,625 instances. In total, we collected 318,500 images accompanied by textual captions. The M\u00b3 EL dataset has been divided into training, validation, and test sets in the ratio of 8:1:1.\nTo further verify the quality of the constructed dataset, we also investigated other existing multi-modal datasets:\n\u2022 DiffusionDB (Wang et al. 2022b) is the first large-scale text-to-image cue dataset. We chose the subset \"2m_random_1k\", which contains 1000 image-text pairs, primarily aimed at the task of $I_m-T_e$.\n\u2022 WIKIPerson (Sun et al. 2022) is a high-quality visual person linking dataset designed for Visual Named Entity Linking tasks. We selected the test set containing 6,142 images, along with their labels (Wikidata QIDs) and descriptions, primarily aimed at the task of $I_m-T_e, I_m-I_e$, and $I_m-(I+T)_e$.\n\u2022 WIKIDiverse (Wang et al. 2022a) is a high-quality, manually annotated MEL dataset consisting of diverse contextual topics and entity types derived from Wikinews.\nWe selected the test set comprising 1,570 image-caption pairs, primarily designed to implement $T_m-T_e, I_m-I_e,(I+T)_m-I_e$ and $(I+T)_m-(I+T)_e$ tasks.\nBaselines Given the diverse range of modality tasks encompassed by our proposed multi-modal dataset, the number of comparable models is notably limited. In order to establish a valid benchmark for evaluation, we report on the performance of several state-of-the-art methods for entity linking and visual entity recognition, including ALIGN (Jia et al. 2021), BLIP-2 (Li et al. 2023), CLIP (Radford et al. 2021), FLAVA (Singh et al. 2022), OWL-ViT (Minderer et al. 2022), and SigLIP (Zhai et al. 2023). Additionally, we present our own optimized approach, CLIPND.\nImplementation Details We employed the CLIP (ViT-B-32) architecture, which comprises both text and image encoders, each with a hidden state dimension of 768 and 12 multi-head attention mechanisms. We trained on an A100 GPU with 40GB of memory with 35 epochs and a batch of 256. The AdamW optimizer was applied with an initial learning rate of le-5, betas=(0.9, 0.98), eps=1e-6, and weight_decay=0.001.\nEvaluation Metric The primary metric we evaluate is the accuracy (ACC) of linking entities to KG. Accuracy is defined by the following formula:\n$ACC = \\frac{\\sum_{i=1}^{N} N^{correct}_{M_m - M_e}}{N \\cdot (\\frac{1}{N} \\sum_{i=1}^{N} 1_{M_m - M_e})}$ (3)\nwhere $N^{correct}$ represent the number of correctly linked entity mentions, N represent the total number of mentions, $M_m - M_e$ denotes different modalities of mention-entity linking task."}, {"title": "Experimental Results", "content": "Main Results In Table 4, we provide a comprehensive analysis of multi-modal task performance on the M\u00b3EL. The experimental results are shown: Firstly, CLIPND achieves significant performance improvements over the baseline model on different versions of the multi-modal dataset. Specifically, on the M-S form, CLIPND demonstrated a 13.9% increase in accuracy compared to the baseline, with an average improvement of 7% across other datasets. Notably, CLIPND and CLIP share identical parameters and computational costs during training. Secondly, when auxiliary text information (e.g., entity names and descriptions) is added to the multi-modal task, the linking accuracy of the baseline model increases significantly, by an average of 1.2%. This suggests that the level of detail and clarity of the auxiliary text plays a key role in enhancing the model ability. Thirdly, all models consistently performed better when utilizing the S1-M-D format for multi-modal tasks compared to the S-M-D format. This is mainly due to the S1-M-D format's ability to convey more precise information about the entities. In contrast, the S-M-D format may truncate key entity and description information due to text length limitations, thereby negatively impacting model performance. Additionally, while most models perform better on multi-modal tasks that contain richer textual information (Sformat), for models focusing on visual modality such as FLAVA and OWL-VIT,"}, {"title": "Further Analysis", "content": "In Table 5, we report the accuracy of the model in detail. Based on the experimental results, we observe that our trained multi-modal approach, CLIPND, outperforms all baseline methods that rely solely on the raw modal information, especially in the dataset WIKIPerson, where the accuracy is improved by 4.3% to 56.8%. This significant performance boost is primarily attributed to the augmented feature strategy implemented during the training process. Secondly, except for the BLIP-2 model, other models did not achieve the highest link accuracy on the M\u00b3EL. Compared to the best-performing model, these models exhibited an average accuracy deficit of 7.6%. Lastly, the model performed the worst on the Diffusion dataset, due to the ambiguous mentions in the textual information paired with images, making linking more hard. Models performed most consistently in the WIKIDiverse, as the surface of mentions in the text-image pairs were more explicit. Most models performed best in the dataset WIKIPerson, due to the availability of bounding boxes within images where the mentions were located, allowing the models to access more specific and accurate image features. Overall, the M\u00b3EL dataset combines the strengths of various datasets by providing clear mention and diverse text-image features, which helps enhance the generalization ability of multi-modal models."}, {"title": "Ablation Analysis", "content": "We conducted an ablation study, and the experimental results are presented in Figure 3. It is evident that the model achieves optimal performance when it utilizes both mentions' name and description informa tion as multi-modal context. In contrast, models trained us-ing only a single source of information (e.g., only name or description) performed poorly. In addition, an interest-ing phenomenon was observed in the experiments: when the model used only sentences $S_1$ involving mentions as context, the performance was better than using complete sentences S. This suggests that models trained with text augmentation strategies have better comprehension in capturing semantic information related to mentions, whereas the use of longer texts may introduce noise, which reduces model performance."}, {"title": "Conclusion", "content": "We propose M\u00b3 EL, a large-scale multi-task and multi-topic dataset for multi-modal entity linking, encompassing 79,625 mentions across 5 different topics. This dataset provides detailed mention names, descriptions, contextual information, and 318.5K image-related data, covering 9 diverse multi-modal tasks. Compared to existing datasets, M\u00b3EL has advantages in terms of entity type coverage, multi-modal task diversity, and good scalability, ultimately contributing to enhancing the performance of linking models in MEL tasks. Leveraging the rich textual information of M\u00b3EL, we propose augmented modality strategies to train a model CLIPND and effectively improve its performance in MEL. In future work, we will continue to update M\u00b3EL to better support research in MEL. We will explore more complex real-world tasks, such as dynamic EL that requires complex linking in audio or video, and investigate the capabilities of M\u00b3 EL in large language models."}]}