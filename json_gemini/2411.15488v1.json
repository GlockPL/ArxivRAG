{"title": "Automatic Evaluation for Text-to-image Generation: Task-decomposed Framework, Distilled Training, and Meta-evaluation Benchmark", "authors": ["Rong-Cheng Tu", "Zi-Ao Ma", "Tian Lan", "Yuehao Zhao", "Heyan Huang", "Xian-Ling Mao"], "abstract": "Driven by the remarkable progress in diffusion models, text-to-image generation has made significant strides, creating a pressing demand for automatic quality evaluation of generated images. Current state-of-the-art automatic evaluation methods heavily rely on Multi-modal Large Language Models (MLLMs), particularly powerful commercial models like GPT-40. While these models are highly effective, their substantial costs limit scalability in large-scale evaluations. Adopting open-source MLLMs is an alternative; however, their performance falls short due to significant limitations in processing multi-modal data compared to commercial MLLMs. To tackle these problems, we first propose a task decomposition evaluation framework based on GPT-40 to automatically construct a new training dataset, where the complex evaluation task is decoupled into simpler sub-tasks, effectively reducing the learning complexity. Based on this dataset, we design innovative training strategies to effectively distill GPT-40's evaluation capabilities into a 7B open-source MLLM, MiniCPM-V-2.6. Furthermore, to reliably and comprehensively assess prior works and our proposed model, we manually annotate a meta-evaluation benchmark that includes chain-of-thought explanations alongside quality scores for generated images. Experimental results demonstrate that our distilled open-source MLLM significantly outperforms the current state-of-the-art GPT-40-base baseline, VIEScore, with over 4.6% improvement in Spearman and Kendall correlations with human judgments.", "sections": [{"title": "1. Introduction", "content": "The rapid advancements in diffusion models have significantly driven the progress of text-to-image generation models."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Image Generation", "content": "In recent years, with the rapid advancement of diffusion models and large-scale image datasets, text-to-image generation models have achieved remarkable progress. Pioneering works like DDPM successfully trained diffusion models for image generation; Stable Diffusion utilized latent diffusion models to generate high-resolution images; DiT adopted transformer as the backbone to construct diffusion models for high-quality images. Subsequently, an increasing number of transformer-based methods have been proposed to generate high-fidelity images.\nWhile these models demonstrate the capability to generate highly creative images, the outputs still suffer from distorted major entities and misalignment with text prompts. These limitations have spurred researchers to develop more precise and automated evaluation methods to assess both the quality of generated images and their correspondence to text prompts."}, {"title": "2.2. Evaluation of Model-generated Images", "content": "To automatically evaluate the quality of generated images, in the early years, the metrics Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) were proposed to assess the the clarity and diversity of generated images by comparing them to real images. Moreover, benefiting from the the powerful feature extracting capabilities of the CLIP and BLIP models, the CLIP-based and BLIP-based scoring methods measure the consistency between generated images and corresponding text prompts, but these metrics fail to assess the complex object-level alignment. To address this issue, visual-question-answering (VQA)-based methods are proposed. VQA-based methods first decompose the text prompt into simple questions using Large Language Models (LLMs), and then evaluate the quality of generated images by computing the accuracy of the 'yes/no' answers of these questions.\nRecently, there is an emerging trend to leverage the reasoning capabilities of MLLMs, like GPT-40, to directly assess the alignment between generated images and input text, exhibiting better correlation with human judgments and great interpretability. For example, VIEscore evaluates the visual appearance quality of the generated images by prompting GPT-40. However, the high cost of commercial API calls for these powerful models limits their scalability in large-scale evaluations. While open-source MLLMs offer an alternative, their limited capabilities hinder effective image quality evaluation. This limitation primarily arises from the coarse-grained and unclear prompts used in existing methods, making it challenging"}, {"title": "3. Approaches", "content": "In evaluating text-to-image task, two primary challenges arise: (1) identifying what to evaluate; and (2) determining how to conduct accurate evaluation. For example, as shown in Figure 1 (Step 1), given a text prompt like \"a black cat standing on the hood of a white car\", models should first identify the evaluation content such as the color, quantity, visual appearance of the cat and car, as well as their relationships. Following this, the quality of these evaluation content needs to be meticulously assessed. Although advanced commercial models can effectively accomplish this task, the high cost for calling their APIs limit the scalability for large-scale text-to-image evaluation. Conversely, while open-source MLLMs offer a cost-effective alternative, their performance significantly lags behind commercial models. This raises a critical question: are open-source MLLMs truly incapable of handling this task? As shown in Figure 2, our preliminary study reveals that current open-source MLLMs could achieve comparable performance to GPT-40 when the evaluation content is provided. However, their performance significantly decreases when they generate the evaluation content by themselves. The main reason is that open-source MLLMs struggle in following complex instructions to extract the evaluation content, mainly suffering from three error patterns: (1) refusal extraction; (2) content absence; and (3) repetitions. For example, as shown in Figure 3, MiniCPM-V-2.6 tends to generate numerous repetitive evaluation content. This suggests a critical need to enhance their ability to extract these evaluation contents.\nTo achieve this goal, we propose a Task Decomposition Evaluation Framework to generate a high-quality training dataset for distilling GPT-40's evaluation capability. As shown in Figure 1, unlike previous works that directly gen-"}, {"title": "3.1. Task Decomposition Evaluation Framework", "content": "Evaluation Content Extraction (ECE) As shown in Step 1 of Figure 1, we leverage GPT-40 [1] to systematically extract two key evaluation content from the text prompt T: entities E and attributes A. Specifically, the model identifies key nouns as the entities (e.g., cat and car) and examines their intrinsic attributes (e.g., color, quantity) and relational attributes (e.g., spatial relationships). Subsequently, three kinds of questions are elicited to cover the details about these entities and attributes: (1) Appearance questions (QA) focus on the visual quality of each involved entity; (2) Intrinsic questions (Q1) evaluate the alignment between intrinsic attributes of entities in images and the text prompt; (3) Relationship questions (QR) assess the relational attributes between entities, ensuring that the image's spatial and relational attributes align with descriptions in the text prompt. Overall, these extracted evaluation contents covers the necessary details during evaluation.\nAfter collecting the essential evaluation content, the next step is to provide accurate evaluations with explanation and scores. Our preliminary study observes that directly evaluating images might lead to information leakage. For example, given the question \"What is the color of the cat\" for the text prompt \"a black cat standing on the hood of a white car\", the MLLMs might directly give an answer \"black\", regardless of the content in the generated image. This problem significantly affects the evaluation performance of MLLMs. To address this limitation, we first utilize GPT-40 to generate specific answers to the evaluation questions by analyzing images (Step 2 in Figure 1), followed by detailed explanations that focus on the alignment between answers and text prompt (Step 3 in Figure 1).\nCaption and Answer Generation (CAG) As shown in Step 2 in Figure 1, GPT-40 is first asked to generate detailed captions C for the image I, enhancing the understanding of the evaluated image. Based on the captions and image, detailed answers (Ansi) are generated to describe details in the image I for questions (QA, Q1,QR).\nExplanation and Scoring (E&S) As shown in Step 3 in Figure 1, we employ GPT-40 to generate a brief chain-of-thought explanation Expi and judgment score Si for each question, assessing the alignment between answers and extracted evaluation content.. The judgment score ranges from 0 to 10, where higher scores indicate better performance. Additionally, since the visual appearance questions don't have ground-truth answers, we directly prompt GPT-40 to"}, {"title": "3.2. Training Strategy", "content": "After using our proposed evaluation framework to generate numerous samples for constructing the training dataset, we encounter two critical challenges in effectively fine-tuning open-source MLLMs. First, as illustrated in Figure 4, our training samples exhibit much longer evaluations than previous works due to the multiple question-answers and detailed explanations. It introduces challenges for optimization, as critical information may become obscured within lengthy sequences. Second, the dataset suffers from distribution imbalances, primarily in sub-task distribution imbalance and score distribution imbalance, which will significantly affect the effectiveness of training.\nTherefore, to address the first issue, we introduce the Fine-grained Sub-tasks Training Strategy (Sec-"}, {"title": "3.2.1 Fine-grained Sub-tasks Training Strategy", "content": "As shown in Figure 4, we formulate a training sample into several fine-grained sub-task samples from 1 to 6. Each one is formatted into a single- or multi-turn conversation, aiming to enhance one specific capability of MLLMs for evaluation.\nEvaluation Content Extraction (1) aims to enhance the ability of open-source MLLMs to extract three types of essential information from the text prompt T and evaluated image I: entities E, attributes A, three kinds of questions (QA, Q1,QR) and detailed caption C by optimizing this loss function:\n$L_1 = MLLM(E, A, (Q_A, Q_I, Q_R), C|T, I)$ (1)\nIndividual Answer Generation (\u2461) aims to fine-tune MLLMS for predicting the detailed answers for questions given the evaluated image I. During experiments, it is challenging for open-source MLLMs to directly generate answers for all questions due to their limited capabiilties. Considering that answers to each question are independent, we simplify the optimization by training MLLMs to predict the answer for each question individually, and optimize the following loss function:\n$L_2 = \\sum_{i=1}^{N}MLLM(Ans_i|I, Q_i)$ (2)\nwhere Qi, Ansi represent the i-th pair of question and answer, and N represent the sum of the numbers of the appearance, intrinsic and relationship questions.\nExplanation and Scoring (\u2462 and \u2463) enables MLLMs to generate the detailed explanations and judgment scores, assessing the alignment between the answers and the text prompt. However, since explanations typically involve much more tokens than scoring, the loss of explanation disproportionately influences this training process when they are jointly optimized, resulting in insufficient learning for score prediction, thus compromising the model's scoring accuracy. To address this problem, we further separate"}, {"title": "3.2.2 Data Rebalance Training Strategy", "content": "We propose two rebalance strategies to reduce the effects of the imbalanced data distribution problems: sub-task distribution imbalance, and score distribution imbalance.\nSub-task Rebalance In our dataset, there are multiple questions associated with each sample, resulting in a significantly higher number of answers and explanations compared to extractions and summarizations. To rectify this imbalance, we maintain the existing number of answers and explanations, while increasing the volume of extraction and summarization samples by augmenting them through repetition.\nScore Distribution Rebalance A notable issue in our constructed dataset is the imbalance in score distribution. For example, the number of images with the quality score of 9 is approximately 5.9k, accounting for 42.8% of all images, and is significantly more than other quality scores. This issue introduces severe bias during fine-tuning, causing distilled open-source MLLMs to be more inclined to assign higher scores to generated images. To solve this problem, we duplicate and re-sample the training samples that are underrepresented, ensuring an equal number of samples across each score range from 0 to 10."}, {"title": "4. Training Set and Human-Annotated Test Set", "content": "In this section, we elaborate the details for constructing the training set and our human-annotated test set."}, {"title": "4.1. Training Set Construction", "content": "The construction of the training set involves two key phases: (1) text-to-image generation; and (2) text-to-image evaluation.\nText-to-image Generation The text prompts and their corresponding evaluated images are collected in this phase. Specifically, the text prompts for image generation are sourced from two places: (1) 9k samples from the COCO dataset [22]; and (2) 5k samples generated by GPT-40. To ensure diversity in image quality, we employ three widely-used models to generate images for each text prompt: SD1.5, SDXL, and SD3. Subsequently, for each text prompt, one image is randomly selected for evaluation from the generated images, with selection probabilities of 50% for SD1.5, and 25% each for SDXL and SD3. This results in a final dataset comprising 14k pairs of text prompts and generated images.\nText-to-image Evaluation Each text prompt and its corresponding image are processed by GPT-40 to obtain detailed evaluations, following our proposed framework described in Section 3.1."}, {"title": "4.2. Human-Annotated Meta-Evaluation", "content": "To the best of our knowledge, there is currently no fine-grained, score-based benchmark that comprehensively and reliably evaluates the capability of existing models in assessing text-to-image generation. To address this gap, in addition to constructing the training set, we have developed a high-quality meta-evaluation benchmark through human annotations. Specifically, three human annotators are asked to annotate the evaluations for each pair of text prompt and image, following our proposed task decomposition evaluation framework. The annotated judgment scores provide the basis for objective evaluation, helping to assess the correlation between model outputs and human judgments. Furthermore, the annotated textual explanations serve as reference explanations for reliable automatic subjective evaluation [18], which helps assess the accuracy of the models."}, {"title": "5. Experiments", "content": null}, {"title": "5.1. Evaluation Metrics", "content": "In line with prior studies, we conduct both objective and subjective evaluations to assess the effectiveness of our evaluation model and the baseline methods. The objective evaluation measures the correlation scores between model predictions and human judgments, whereas the subjective evaluation assesses the quality of the chain-of-thought textual evaluations.\nObjective Evaluation Following previous works, Spearman (\u03c1) and Kendall (\u03c4) correlations are computed to reflect the correlation between the assessments of evaluation model and human judgments, where higher correlation scores denotes better reliability of evaluation models. In this paper, we report the the model's correlation scores with each human annotator and human average.\nSubjective Evaluation As in recent works, we use our human-annotated explanations as the references to assist GPT-40 model in determining whether the model-generated chain-of-thought evaluations aligns with human annotations:\n$S_{sub.} = \\frac{1}{N} \\sum_{i=1}^{N} GPT-40(P, Q_i, Exp^{ref}_i, Exp^{gen}_i)$ (7)\nwhere $Exp^{ref}_i, Exp^{gen}_i$ represent the reference and model-generated explanations, respectively. P is the subjective evaluation prompt, guiding GPT-40 to generate subjective scores ranging from 0 to 5. The final subjective score is the average of all these scores."}, {"title": "5.2. Overall Comparison Results", "content": "To validate the effectiveness of our fine-tuned MiniCPM-V-2.6 in assessing generated image quality, we compared it with existing state-of-the-art methods using Spearman correlation (\u03c1) and Kendall correlation (\u03c4) scores with human judgments, as shown in Table 1. Based on these results, we identify the following key findings: (1) Our fine-tuned MiniCPM-V-2.6 demonstrates the best performance in the automatic assessment of generated image quality, surpassing existing GPT-4o-based methods overall. For instance, compared to the best-performing competitor, VIEScoreGPT-40, our fine-tuned MiniCPM-V-2.6 model achieves over 4.6% improvement in both Spearman and Kendall correlations with human judgments. (2) Our fine-tuned MiniCPM-V-2.6 also outperforms OurGPT-40"}, {"title": "5.3. Ablation Study on Task Decomposition Evaluation Framework", "content": "To verify the effectiveness of each component in our fine-grained evaluation framework and assess their impact on overall performance, we conducted ablation studies based on 150 examples randomly sampled from our annotated meta-evaluation benchmark. Specifically, we designed the following three variants to compare with the full framework. (1) w/o Extraction: in the ECE step, GPT-40 does not extract structure information but directly propose questions based on the text, and then in the E&S step, GPT-40 directly scores based on the input text and the answer from the CAG step. (2) w/o Captioning: in CAG step, GPT-40 directly answers the questions based on the image without image caption generation. (3) w/o Answering: GPT-40 directly score the question without generating the answer or explanation. (4) CAG and E&S Merged: The CAG and E&S steps are combined into one step.\nAs shown in Table 2, the decreasing performance highlights the necessity of each design in our framework: (1) Compared to the \"w/o Extraction\u201d variant, our fine-grained evaluation framework achieves significantly improved evaluation performance. This demonstrates that extracting entities and attributes from the text helps models focus on essential evaluation content, leading to more accurate assessments. (2) the decreasing performance of the variant 'w/o Captioning' demonstrates that that when GPT-40 answers questions without first generating an image caption, it may overlook important details of image entities, leading to inaccurate responses and damaging the evaluation performance. (3) Compared to the \"w/o Answering\" variant, our framework achieves 17% and 40% increases in Spearman \u03c1 and Kendall \u03c4 correlations, respectively. This shows that generating detailed answers before scoring prompts the model to analyze the image more deeply, enhancing evaluation performance; (4) The performance of \u201cCAG and E&S"}, {"title": "5.4. Effectiveness of Fine-tuning", "content": null}, {"title": "5.4.1 Effectiveness of Our Training Corpus", "content": "To validate the effectiveness of our constructed training corpus in enhancing the evaluation capabilities of MLLMs, we selected two MLLMs-InternVL2-8B [4] and MiniCPM-V-2.6 [50]-to compare their evaluation performance before and after fine-tuning. Experimental settings are provided in Appendix E, and the results are shown in Figure 5. These experimental results show that after fine-tuning on our constructed training corpus, both models exhibit significant improvements across all evaluation metrics. For instance, InternVL2-8B achieved a 20.5% increase in \u03c1, and MiniCPM-V-2.6 improved by 28.6% in \u03c4. These findings demonstrates the general applicability of our constructed dataset in effectively enhancing the evaluation capabilities of MLLMs."}, {"title": "5.4.2 Contributions to Subjective Evaluation", "content": "We evaluated the impact of our fine-tuning strategy in improving the quality of textual explanations in evaluations. We conducted a detailed analysis based on the subjective evaluation metric across three aspects: appearance quality, intrinsic consistency, and relationship consistency, and also give the overall evaluation score.\nAs illustrated in Figure 6, both InternVL2-8B and MiniCPM-V-2.6 show significant improvements in appearance quality, intrinsic consistency and overall scores after fine-tuning. These enhancements confirm the effectiveness of our fine-tuning approach in refining specific aspects of the text-to-image evaluation process. However, there is a slight decline in the relationship consistency scores post-fine-tuning. This reduction can be attributed to the imbal-"}, {"title": "5.4.3 Ablation Study on Training Strategies", "content": "To evaluate the effectiveness of the components in our fine-grained sub-tasks training strategy, we proposed three ablation variants: (1) w/o Individual QA: the MLLM generates the responses for all extracted questions at once instead of answering each question individually; (2) w/o E&S Separation: the MLLM produces joint explanations and scores in a single output rather than generating them separately; (3) w/o Score Balancing: the variant is trained on the dataset without rebalancing the ratio of sub-tasks, high and low score questions.\nBased on the experimental results shown in Table 3, we derive the following insights: (1) Importance of Individual Question Answering: Compared to the \"w/o Individual QA\" variant, our fine-tuned MiniCPM-V-2.6 achieves over 50% improvement in Spearman and Kendall correlations with human judgments. This indicates that addressing questions individually prevents interference among them, enhancing the model's ability to answer accurately. (2) Effect of Explanation and Scoring Separation: Fine-tuning with our distilling framework yields better evaluation performance than the \"w/o E&S Separation\" variant, supporting our assumption that the explanation loss dominates the training process and limits learning for score prediction, thereby reducing the model's scoring accuracy. (3) Necessity of Score Balancing: The results of ours are better than that of the variant \"w/o Score Balancing\", demonstrating the critical importance of training on a balanced dataset. An"}, {"title": "6. Conclusion", "content": "In this paper, we propose a task decomposition evaluation framework for text-to-image generation, aimed at constructing a high-quality training dataset. On top of that, we introduce two training strategies designed to effectively distill the evaluation capabilities of GPT-4o into open-source MLLMs: Fine-grained Sub-tasks and Data Rebalance. Furthermore, we establish a comprehensive and reliable benchmark to assess the effectiveness of both our distilled models and existing strong baselines. Extensive experiment results demonstrate that our distilled evaluation model significantly outperforms existing metrics for text-to-image evaluation, exhibiting higher correlation with human judgments."}]}