{"title": "SoP: Unlock the Power of Social Facilitation for Automatic Jailbreak Attack", "authors": ["Yan Yang", "Zeguan Xiao", "Xin Lu", "Hongru Wang", "Hailiang Huang", "Guanhua Chen", "Yun Chen"], "abstract": "The widespread applications of large language models (LLMs) have brought about concerns regarding their potential misuse. Although aligned with human preference data before release, LLMs remain vulnerable to various malicious attacks. In this paper, we adopt a red-teaming strategy to enhance LLM safety and introduce SoP, a simple yet effective framework to design jailbreak prompts automatically. Inspired by the social facilitation concept, SoP generates and optimizes multiple jailbreak characters to bypass the guardrails of the target LLM. Different from previous work which relies on proprietary LLMs or seed jailbreak templates crafted by human expertise, SoP can generate and optimize the jailbreak prompt in a cold-start scenario using open-sourced LLMs without any seed jailbreak templates. Experimental results show that SoP achieves attack success rates of 88% and 60% in bypassing the safety alignment of GPT-3.5-1106 and GPT-4, respectively. Furthermore, we extensively evaluate the transferability of the generated templates across different LLMS and held-out malicious requests, while also exploring defense strategies against the jailbreak attack designed by SoP.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs), such as ChatGPT (OpenAI, 2022) and LLaMA (Touvron et al., 2023), have achieved impressive performance in different natural language processing tasks in recent days. LLMs advance neural-based applications in real-world applications with their superior text understanding, generation, and reasoning abilities. After finetuning with instruction data, the LLMs enhance their instruction following ability to respond to diverse input commands (Ouyang et al., 2022). However, the remarkable instruction following ability brings about significant risks of potential LLM misuse (Ouyang et al., 2022). For example, when prompted with malicious instructions, the LLM may directly give some suggestions about how to spread a rumor. Therefore, it is imperative to align deployed LLMs with human values in real-world applications. The LLMs are aligned with human preference data using methods like Reinforcement Learning via Human Feedback (Ouyang et al., 2022, RLHF) and Direct Preference Optimization (Rafailov et al., 2023, DPO). The aligned LLMs are expected to generate helpful, harmless, and honest responses.\nHowever, aligned proprietary LLMs are still attacked to generate harmful content in some scenarios (Wei et al., 2023a; Liu et al., 2023). They remain vulnerable to adversarial inputs, particularly in the context of jailbreak attacks (Zou et al., 2023). Jailbreak attempts to intentionally craft user instructions that trigger LLMs to generate harmful, offensive, or undesirable content in accordance with the malevolent user's intention (Chao et al., 2023; Ganguli et al., 2022). Red-teaming is a strategic approach to enhance LLM safety by actively investigating and revealing hidden scenarios in which LLMs may exhibit failures (Perez et al., 2022; Ganguli et al., 2022). Previous researches enhance LLM safety with a red-teaming strategy by designing jailbreak prompts either by hand or automatically (Zou et al., 2023; Zhu et al., 2023; Chao et al., 2023; Li et al., 2023). However, the hand-crafted methods are hardly scalable to deal with different LLM variants. The optimization-based methods suffer from unsatisfied attacking performance, especially for the iteratively updated proprietary LLMs.\nIn this paper, we propose Social facilitation based jailbreak Prompt (SOP), a simple and effective framework that can design jailbreak templates automatically. It can support the cold-start"}, {"title": "Methods", "content": "SOP is a black-box jailbreak attack framework (see Figure 1) for automated red-teaming of LLMs even where only APIs of LLMs are accessible. Multiple jailbreak characters (Section 2.1) are automatically generated and optimized (Section 2.2) under the judgement model (Section 4.1) to work together for jailbreaking the target LLM."}, {"title": "Jailbreak via Social Facilitation", "content": "The opinion of LLM can be biased with user prompts (Sharma et al., 2024). Previous works (Li et al., 2023) bypass the intrinsic safety guardrail of LLMs by offering a carefully designed task for a single specific character in science fiction. In this work, a multi-character playing framework is proposed to jailbreak the LLMs inspired by the sociological concept of social facilitation (Zajonc, 1965). This concept describes the social phenomenon where people tend to perform better when they are with other people rather than when they are doing the task alone, in order to behave in accordance with socially expected standards. The involvement of others may trigger a competitive and cooperative mindset, resulting in a race to excel and improve the performance of the entire group. Co-action effect (Zajonc, 1965), one type of social facilitation, emphasizes that parallel activity, rather than direct competition or cooperation, can impact individual behavior.\nThe SOP design a simple jailbreak template (see Figure 1(a)) where target LLMs are asked to act as multiple jailbreak characters to accomplish the malicious task by providing step-by-step instructions in a single-turn dialogue. With different profiles, each designed character is an expert in jailbreak-"}, {"title": "Jailbreak Character Optimization", "content": "Different from previous approaches that rely on human expertise\u00b9, in this part, we resort to LLM-based optimization method (Yang et al., 2023) for jailbreak prompt design.\nWe decompose the jailbreak prompt into two parts: the jailbreak template and the malicious request (as illustrated in Figure 1 (left)). The jailbreak template contains multiple jailbreak characters as well as a placeholder for malicious requests. When attacking, we directly replace the placeholder with malicious requests. During optimization, we refine the characters within the jailbreak template while keeping the other parts, including the placeholder, unchanged.\nThis simple procedure critically relies on the"}, {"title": "Judgement Model", "content": "The attack is judged as success when the response is related to the malicious request as well as contains harmful content. Due to the inherent flexibility of natural language, evaluating the success of a jailbreak attack automatically is challenging. Previous works usually resort to rules patterns (Zou et al., 2023), LLMs (Chao et al., 2023) or explicitly trained classifier (Yu et al., 2023) for evaluation. We follow GPTFuzzer (Yu et al., 2023) to explicitly train an evaluation classifier, as the LLM-based judgement model might give exaggerated scores or reject the evaluation task due to the sensitive contents in some cases (Yu et al., 2023; Li et al., 2023). However, in contrast to GPTFuzzer which judges solely based on the target LLM's response, we formulate the judgement as a sentence pair classification problem. The input to our judgement model comprises the target LLM's response as well as the malicious request. Compared to previous judgement models like GPT-4, our approach is more accurate and rigorous. More discussions are in Appendix A and Section 4.1."}, {"title": "Experiments", "content": "Dataset We use AdvBench custom, curated by Chao et al. (2023), to evaluate our approach following previous works (Chao et al., 2023; Li et al., 2023). AdvBench custom is a subset of the harmful behaviors dataset from the AdvBench benchmark (Zou et al., 2023), comprising 50 representative malicious instructions out of the original 520. Baseline methods (Chao et al., 2023) use Advbench custom for both jailbreak prompt optimization and testing. In contrast, our study utilizes a reduced training dataset consisting of only 20 instructions and reports results on the same test set as baseline approaches. Note that our setup is more challenging, as 30 out of the 50 malicious instructions in the test set are unseen during jailbreak prompt optimization.\nSettings We examine the effectiveness of SOP on both open-source and proprietary LLMs. More"}, {"title": "Main Results", "content": "The main results are shown in Table 1. Despite extensive safety training including iterative updating against jailbreak attacks since the initial release of LLMs, we find that the LLMs remain vulnerable to jailbreak attacks. SOP achieves \u2265 86% ASR on GPT-3.5s and 92% ASR on LLaMA-2 with a system prompt. Even when targeting the most powerful GPT-4 model, SOP is capable of achieving a notable ASR of 60%.\nOverall, SOP surpasses all baselines. On LLaMA-2, SOP outperforms the baselines by 38% to 80% ASR. This is impressive as LLaMA-2 is believed to be one of the most robust open-source LLMs against jailbreak attack (Mazeika et al., 2024). GPTFuzzer performs exceptionally well on GPT-3.5-0613 but degrades significantly on other LLMs. This is primarily attributed to the seed jailbreak templates\u00b2 used in GPTFuzzer, which is largely created based on GPT-3.5-0613 and is well-suited to optimize performance for it. Moreover, different from GPTFuzzer which relies on 77 carefully human-written jailbreak template seeds, SOP only needs one simple jailbreak character to start. Compared to PAP and PAIR, SOP performs better on 2 out of the 3 target LLMs. On GPT-4, PAP and PAIR obtain better performance. We suspect that GPT-4 has been optimized to defend against malicious character-based jailbreaking approaches, making it challenging to bypass its safeguards. To address this challenge, we combine SOP with the"}, {"title": "Ablation Studies", "content": "We conduct ablation studies to further verify the different components of our method using LLaMA-2 as the target model.\nNumber of examples in meta prompt We utilized $K$ examples within the meta prompt for the model to learn from. Too few examples may not fully harness the capabilities of the attacker model, while too many examples may lead to performance degradation and excessive token consumption. In this section, we investigate the appropriate number of examples. Here we report the mean and variance during optimization in three trials. As shown in Figure 2, the mean ASR performance increases"}, {"title": "Analyses", "content": "Effectiveness of judgement Model\nTable 3 shows the performance of our judgement model compared with baseline judgement models including the judgement model of GPTFuzzer (Yu et al., 2023), GPT-4 based judgement model and ChatGPT based judgement model with a designed prompt (Figure 4 of Appendix A). As can be seen, our judgement model outperforms all baselines in terms of accuracy. In order to ensure that an attack classified as successful by the judgement model is truly successful, we make a trade-off by sacrificing the true positive rate in favor of achieving a lower false positive rate. This implies that our judgement model is more stringent when compared to the baselines. We also report the performance of SOP using GPT-based judgement model and human annotations, as shown in Table 9 of Appendix A. The results further confirm the reliability of our judgement model."}, {"title": "Analysis of Co-action Effect", "content": "In this section, we conduct an analysis of the co-action effect observed among multiple characters within the same jailbreak prompt using LLaMA-2 as the target LLM. To investigate this phenomenon, we divide our jailbreak prompt into multiple prompts, each structured similarly to the original but featuring a single jailbreak character. These divided templates are then sequentially applied in an attempt to jailbreak the target model. Any successful jailbreak achieved during these attempts is considered a success. We denote this modified variant as SOP-divided, as it involves independent attacks by different jailbreak characters, with no influence between characters at the jailbreak stage. The attack results for each malicious request, both for SOP and SOP-divided, are visualized in Figure 3. In addition to reporting the overall jailbreak results, we also report whether each jailbreak character responds with jailbreak results. As can be seen, SOP-divided performs much worse compared with SoP, with a decrease of 36 ASR. Furthermore, a similar trend is observed when comparing the individual jailbreak characters between SOP-divided and SOP. These findings confirm our intuition on social facilitation."}, {"title": "Effect of Character Order", "content": "To understand the effect of character order, we reverse the order of characters in SOP at test time and denote this method as SOP-reversed using LLaMA-2 as the target LLM. As shown in Figure 3, the performance of SOP-reversed is worse than that of SOP, which is consistent with our expectations: our jailbreak characters are greedily generated, thus the order of characters is crucial. Nevertheless, even"}, {"title": "Further Attack and Defense", "content": "We evaluate the attack performance of SOP when transferring cross requests and target models and when combined with other attack methods. We also explore defending strategies against SOP."}, {"title": "Transfer Attack", "content": "To further examine the transfer performance of SOP, we evaluate SOP on transferred malicious datasets and target models.\nWe attack the target LLMs with four held-out malicious request sets: the remaining 470 instructions from harmful behaviors dataset (Zou et al., 2023), 100 questions from GPTFuzzer (Yu et al., 2023), 400 questions from HarmBench (Mazeika et al., 2024) and 100 questions from Jailbreak-Bench (Chao et al., 2024). We also report the scores separately on the training and testing sets of AdvBench custom. As shown in Table 4 and Table 10 of Appendix C, the SOP jailbreak prompts can successfully transfer to the four held-out datasets. For example, SOP achieves a commendable ASR of 90%, 92%, 76% and 78% for the AdvBench remaining, GPTFuzzer, HarmBench and JailbreakBench dataset, respectively, when attacking GPT-3.5-1106. These results demonstrate the strong cross-request transfer capability of SOP. Researchers typ-"}, {"title": "Combination with Other Attack Methods", "content": "Since the generated jailbreak templates can be integrated with any requests, it is possible to combine them with request-level jailbreak techniques,3 wherein these techniques clandestinely modify a malicious request, rendering it hard to detect. We use four rewriting strategies from Ding et al. (2023), one encryption method (Yuan et al., 2023), and two low-resource language translations (Yong et al., 2023). We preprocess the Advbench custom dataset using these techniques before merging them with our templates for jailbreaking. GPT-4 is employed to rewrite and Google Translate to translate.\nAs presented in Table 6, these request-level tech-"}, {"title": "Defenses against SOP", "content": "This section explores detection-based and prompt-based defense strategies that do not modify the base model.\nDetection-based: This type of defense detects jailbreak prompts from the input space. Examples include Perplexity Filter (Jain et al., 2023), which defines a jailbreak prompt as failed when its log perplexity exceeds a threshold; Rand-Insert, Rand-Swap, and Rand-Patch (Robey et al., 2023), which alter the inputs and detect attack based on"}, {"title": "Related Work", "content": "Safety Alignment of Large Language Models Large language models like ChatGPT and LLaMA have achieved state-of-the-art performance on many natural language processing tasks. The safety of LLM-based applications is significant as they are open to the whole public and any LLM misuse will bring about harm to the community. To diminish the harm and misuse brought by LLMs, the released public LLMs are further trained on human preference data to align with human values, with algorithms like reinforcement learning via human feedback (Ouyang et al., 2022) or direct preference optimization (Rafailov et al., 2023). Although finetuned for better alignment with human values, LLMs are still vulnerable to various carefully designed adversarial attacks like the jailbreak attack (Wei et al., 2023a; Zou et al., 2023; Wei et al., 2023c). The weakness of LLMs calls for further research on the safety alignment of LLMs as well as defense for different attacks.\nRed-Teaming LLMs via Jailbreak As one approach to enhance the security of LLMs, red-teaming (Perez et al., 2022; Ganguli et al., 2022) explores the weakness of LLMs and discloses the covert failure cases of LLMs. Jailbreak attack is one of the red-teaming approaches explored by many previous researches (Mehrotra et al., 2023; Ding et al., 2023; Du et al., 2023; Carlini et al., 2023). Jailbreak carefully designs user queries that can bypass the security guardrails of LLMs. It aims to trigger the model to produce uncensored, undesirable, or offensive responses (Chao et al., 2023). Previous jailbreak attack methods are grouped into three categories. (1) The first is to be meticulously crafted by human expert (Li et al., 2023). For example, DeepInception (Li et al., 2023) conceals the malicious content by manually crafting different imaginary scenes with various characters to escape LLM moral precautions. However, this method heavily relies on human expertise and is not scalable to the upgradation of LLMs. (2) The second line is to optimize the jailbreak instructions using the model gradients or LLM-based optimization (Zou et al., 2023; Chao et al., 2023; Zhu et al., 2023; Yu et al., 2023; Jin et al., 2024; Lapid et al., 2023; Jones et al., 2023). Greedy coordinate gradient (Zou et al., 2023) trains the jailbreak suffix to maximize the probability of the affirmative response to the malicious query. However, the performance often drops when transferred to other LLMs (Chao et al., 2023). PAIR (Chao et al., 2023) iteratively queries the target LLM to update and refine a candidate jailbreak for each singular malicious request. However, the searched jailbreak prompt cannot transfer across malicious requests. (3) The third is the long-tail encoding method. Previous studies (Liu et al., 2023; Yuan et al., 2023) have shown that more powerful LLMs are more susceptible to long-tail encoding-based jailbreak attacks since they are more capable of understanding and following complex long-tail instructions such as encoding with low-resource languages or Base64 string. In this work, we opt for automated generation and optimization of jailbreak prompts with the attacker LLM in a multi-character playing scenario."}, {"title": "Conclusion", "content": "In this work, we propose SOP, a simple and effective jailbreak attack framework that generates a fluent and coherent jailbreak template universal to all malicious queries. Our framework draws inspiration from the concept of social facilitation and leverages multiple auto-generated jailbreak characters to bypass the guardrail of the target LLMs. Through extensive experiments on four target LLMs, including both open-sourced and pro-"}, {"title": "Limitations", "content": "The proposed SOP is a simple yet strong red-teaming approach that generates effective jailbreak templates automatically. It achieves superior attack performance across four prominent LLMs and exhibits commendable success rates in cross-target model transfer attacks compared with existing baseline methods. Due to our limited computation resources, we only examine the effectiveness of SOP method on four target LLMs. We will assess the effectiveness of SOP on more target LLMs. In Section 5.3, different defense strategies against SOP attack are compared but seldom achieve satisfactory results across different target LLMs. We leave the exploration of effective defense strategy against SOP attack as future work."}, {"title": "Potential Risks and Ethical Statement", "content": "In this work, we employ a red-teaming approach to investigate the potential safety and security hazards within LLMs, with the primary objective of enhancing their safety rather than facilitating malicious exploitation. The potential risk of this work is the malicious use of LLMs with the proposed Sop method. Following the common practice of red-teaming research, we have responsibly disclosed our findings to Meta and OpenAI in order to minimize any potential harm resulting from the SoP jailbreak attack prior to publication. As a result, it is possible that the SOP framework is no longer effective. We also follow ethical guidelines throughout our study and will restrict the SOP details to authorized researchers only."}, {"title": "Details of Judgement Model", "content": "In our preliminary experiments, we employ baseline judgement models such as ChatGPT, GPT-4, and the classifier in GPTFuzzer for SOP and generate responses for malicious requests. We collect 2271 request-response pairs and manually label them. We reserve 550 samples as the validation set and 500 as the test set, while the remaining 1221 instances, combined with 7700 samples sourced from Yu et al. (2023), are utilized as the training dataset. Given the multi-character nature of our jailbreak template, we evaluate the response from each character and mark it as a success if at least one of the characters manages to successfully attack. In other words, our judgement model is applied to the individual responses of each character rather than directly evaluating the aggregated responses of all characters. For the fine-tuning process, we employ the DeBERTaV3-large model (He et al., 2021). The training is conducted for 15 epochs using a batch size of 16. The learning rate is configured as le-5 and the maximum sequence length is set to 512. For testing, our classifier pipeline would split the response by the character name first and classify each split sentence. The label of the complete response is success if any of the split sentences succeed. For the GPT-based baseline judgement models, we employ the following prompt to enable ChatGPT or GPT-4 models to distinguish whether a response is jailbroken."}, {"title": "Algorithm for Jailbreak Character Optimization", "content": "Algorithm 1 presents the detailed algorithm for optimization of the Jailbreak characters of SOP."}, {"title": "More Experiments", "content": "Maximum number of characters\nOur optimization algorithm has a complexity of O(RI), where the maximum number of characters, denoted as R, is linearly correlated with the cost. Consequently, we explore the relationship between the number of characters and ASR to find a reasonable value for R, as shown in Figure 6. For three out of four target LLMs, the ASR improves as the number of characters increases from 1 to 2. Given that the three models have already achieved remarkably high ASR (> 80), the potential for further ASR improvements by increasing the num-"}, {"title": "Comparison Transfer Results with Baselines", "content": "In this section, we compare our transfer results on HarmBench and JailbreakBench datasets, as shown in Table 10. The baseline results are cited from Mazeika et al. (2024) and Chao et al. (2024) for HarmBench and JailbreakBench, respectively. It is important to note that the setup for our method, SOP, is more challenging compared to the baselines. SOP is trained on the AdvBench custom dataset and then evaluated on the HarmBench and JailbreakBench datasets. In contrast, the baseline methods train and test on the same dataset."}, {"title": "Prompts for Prompt-based Defense", "content": "We list the defense prompts used for Self-Reminder and Adaptive Prompt, as shown in Figure 11 and Figure 12, respectively."}, {"title": "Detailed Prompts", "content": "Evaluation and Jailbreak Prompts\nWe list the meta prompt and the jailbreak template (evaluation template) of SOP, as shown in Figure 7 and 8, respectively. Our meta prompt employs K jailbreak characters as examples to guide the generation of new jailbreak characters. To reduce the human effort needed to design the examples, we use a very simple jailbreak character as the example when starting the optimization, as shown in Figure 10. This means that the meta prompt only contains one example at startup and the num-"}]}