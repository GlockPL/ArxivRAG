{"title": "SoP: Unlock the Power of Social Facilitation\nfor Automatic Jailbreak Attack", "authors": ["Yan Yang", "Zeguan Xiao", "Xin Lu", "Hongru Wang", "Hailiang Huang", "Guanhua Chen", "Yun Chen"], "abstract": "The widespread applications of large language\nmodels (LLMs) have brought about concerns\nregarding their potential misuse. Although\naligned with human preference data before re-\nlease, LLMs remain vulnerable to various ma-\nlicious attacks. In this paper, we adopt a red-\nteaming strategy to enhance LLM safety and\nintroduce SoP, a simple yet effective frame-\nwork to design jailbreak prompts automati-\ncally. Inspired by the social facilitation con-\ncept, SoP generates and optimizes multiple\njailbreak characters to bypass the guardrails\nof the target LLM. Different from previous\nwork which relies on proprietary LLMs or seed\njailbreak templates crafted by human exper-\ntise, SoP can generate and optimize the jail-\nbreak prompt in a cold-start scenario using\nopen-sourced LLMs without any seed jailbreak\ntemplates. Experimental results show that SoP\nachieves attack success rates of 88% and 60%\nin bypassing the safety alignment of GPT-3.5-\n1106 and GPT-4, respectively. Furthermore,\nwe extensively evaluate the transferability of\nthe generated templates across different LLMS\nand held-out malicious requests, while also ex-\nploring defense strategies against the jailbreak\nattack designed by SoP. Code is available at\nhttps://github.com/Yang-Yan-Yang-Yan/SoP.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs), such as ChatGPT\n(OpenAI, 2022) and LLaMA (Touvron et al., 2023),\nhave achieved impressive performance in differ-\nent natural language processing tasks in recent\ndays. LLMs advance neural-based applications\nin real-world applications with their superior text\nunderstanding, generation, and reasoning abilities.\nAfter finetuning with instruction data, the LLMs\nenhance their instruction following ability to re-\nspond to diverse input commands (Ouyang et al.,"}, {"title": "2 Methods", "content": "SOP is a black-box jailbreak attack framework (see\nFigure 1) for automated red-teaming of LLMs even\nwhere only APIs of LLMs are accessible. Multiple\njailbreak characters (Section 2.1) are automatically\ngenerated and optimized (Section 2.2) under the\njudgement model (Section 4.1) to work together\nfor jailbreaking the target LLM."}, {"title": "2.1 Jailbreak via Social Facilitation", "content": "The opinion of LLM can be biased with user\nprompts (Sharma et al., 2024). Previous works (Li\net al., 2023) bypass the intrinsic safety guardrail\nof LLMs by offering a carefully designed task for\na single specific character in science fiction. In\nthis work, a multi-character playing framework\nis proposed to jailbreak the LLMs inspired by\nthe sociological concept of social facilitation (Za-\njonc, 1965). This concept describes the social phe-\nnomenon where people tend to perform better when\nthey are with other people rather than when they\nare doing the task alone, in order to behave in ac-\ncordance with socially expected standards. The\ninvolvement of others may trigger a competitive\nand cooperative mindset, resulting in a race to excel\nand improve the performance of the entire group.\nCo-action effect (Zajonc, 1965), one type of social\nfacilitation, emphasizes that parallel activity, rather\nthan direct competition or cooperation, can impact\nindividual behavior.\nThe SOP design a simple jailbreak template (see\nFigure 1(a)) where target LLMs are asked to act as\nmultiple jailbreak characters to accomplish the ma-\nlicious task by providing step-by-step instructions\nin a single-turn dialogue. With different profiles,\neach designed character is an expert in jailbreak-"}, {"title": "2.2 Jailbreak Character Optimization", "content": "Different from previous approaches that rely on\nhuman expertise\u00b9, in this part, we resort to LLM-\nbased optimization method (Yang et al., 2023) for\njailbreak prompt design.\nWe decompose the jailbreak prompt into two\nparts: the jailbreak template and the malicious re-\nquest (as illustrated in Figure 1 (left)). The jail-\nbreak template contains multiple jailbreak charac-\nters as well as a placeholder for malicious requests.\nWhen attacking, we directly replace the placeholder\nwith malicious requests. During optimization, we\nrefine the characters within the jailbreak template\nwhile keeping the other parts, including the place-\nholder, unchanged.\nAlgorithm 1 of Appendix B shows the character\noptimization algorithm. The jailbreak template con-\ntains a sequence of jailbreak characters, which are\ngenerated and optimized sequentially in a greedy\nmanner. For the rth character in the sequence, the\noptimization consists of four key steps.\n1. Character generation: Given a meta prompt\nTmeta (see Figure 7 of Appendix D), the attacker\nLLM generates a candidate jailbreak character X.\n2. Target response: The candidate character X\nis inserted into the jailbreak template Teval (see\nFigure 8 of Appendix D). Then each malicious\nrequest of the training set D is combined with the\njailbreak template as input to attack the target LLM.\nResponses are generated by the target LLM.\n3. Jailbreak scoring: The judgement model scores\nthe current character X as S based on the responses\nas well as the input to the target LLM.\n4. Iterative refinement: The character-score pair\nX-S is used to update the examples in meta prompt\nTmeta. We repeat the above process for I times and\nthe character with the best jailbreak score will be\nused as the rth character to update the jailbreak\ntemplate Teval."}, {"title": "2.3 Judgement Model", "content": "The attack is judged as success when the response\nis related to the malicious request as well as con-\ntains harmful content. Due to the inherent flexi-\nbility of natural language, evaluating the success\nof a jailbreak attack automatically is challenging.\nPrevious works usually resort to rules patterns (Zou\net al., 2023), LLMs (Chao et al., 2023) or explic-\nitly trained classifier (Yu et al., 2023) for evalua-\ntion. We follow GPTFuzzer (Yu et al., 2023) to\nexplicitly train an evaluation classifier, as the LLM-\nbased judgement model might give exaggerated\nscores or reject the evaluation task due to the sen-\nsitive contents in some cases (Yu et al., 2023; Li\net al., 2023). However, in contrast to GPTFuzzer\nwhich judges solely based on the target LLM's re-\nsponse, we formulate the judgement as a sentence\npair classification problem. The input to our judge-\nment model comprises the target LLM's response\nas well as the malicious request. Compared to pre-\nvious judgement models like GPT-4, our approach\nis more accurate and rigorous. More discussions\nare in Appendix A and Section 4.1."}, {"title": "3 Experiments", "content": "We examine the effectiveness of SOP\non both open-source and proprietary LLMs. More"}, {"title": "3.1 Setup", "content": "Dataset We use AdvBench custom, curated by\nChao et al. (2023), to evaluate our approach fol-\nlowing previous works (Chao et al., 2023; Li et al.,\n2023). AdvBench custom is a subset of the harmful\nbehaviors dataset from the AdvBench benchmark\n(Zou et al., 2023), comprising 50 representative ma-\nlicious instructions out of the original 520. Base-\nline methods (Chao et al., 2023) use Advbench\ncustom for both jailbreak prompt optimization and\ntesting. In contrast, our study utilizes a reduced\ntraining dataset consisting of only 20 instructions\nand reports results on the same test set as baseline\napproaches. Note that our setup is more challeng-\ning, as 30 out of the 50 malicious instructions in\nthe test set are unseen during jailbreak prompt opti-\nmization.\nSettings"}, {"title": "3.2 Main Results", "content": "The main results are shown in Table 1. Despite ex-\ntensive safety training including iterative updating\nagainst jailbreak attacks since the initial release of\nLLMs, we find that the LLMs remain vulnerable\nto jailbreak attacks. SOP achieves \u2265 86% ASR\non GPT-3.5s and 92% ASR on LLaMA-2 with a\nsystem prompt. Even when targeting the most pow-\nerful GPT-4 model, SOP is capable of achieving a\nnotable ASR of 60%.\nOverall, SOP surpasses all baselines. On\nLLaMA-2, SOP outperforms the baselines by 38%\nto 80% ASR. This is impressive as LLaMA-2\nis believed to be one of the most robust open-\nsource LLMs against jailbreak attack (Mazeika\net al., 2024). GPTFuzzer performs exceptionally\nwell on GPT-3.5-0613 but degrades significantly on\nother LLMs. This is primarily attributed to the seed\njailbreak templates\u00b2 used in GPTFuzzer, which is\nlargely created based on GPT-3.5-0613 and is well-\nsuited to optimize performance for it. Moreover,\ndifferent from GPTFuzzer which relies on 77 care-\nfully human-written jailbreak template seeds, SOP\nonly needs one simple jailbreak character to start.\nCompared to PAP and PAIR, SOP performs bet-\nter on 2 out of the 3 target LLMs. On GPT-4, PAP\nand PAIR obtain better performance. We suspect\nthat GPT-4 has been optimized to defend against\nmalicious character-based jailbreaking approaches,\nmaking it challenging to bypass its safeguards. To\naddress this challenge, we combine SOP with the"}, {"title": "3.3 Ablation Studies", "content": "We conduct ablation studies to further verify the\ndifferent components of our method using LLaMA-\n2 as the target model.\nNumber of examples in meta prompt We uti-\nlized K examples within the meta prompt for the\nmodel to learn from. Too few examples may not\nfully harness the capabilities of the attacker model,\nwhile too many examples may lead to performance\ndegradation and excessive token consumption. In\nthis section, we investigate the appropriate number\nof examples. Here we report the mean and vari-\nance during optimization in three trials. As shown\nin Figure 2, the mean ASR performance increases"}, {"title": "4 Analyses", "content": "We report the performance\nof SOP using GPT-based judgement model and hu-\nman annotations, as shown in Table 9 of Appendix\nA. The results further confirm the reliability of our\njudgement model."}, {"title": "4.1 Effectiveness of judgement Model", "content": "Table 3 shows the performance of our judgement\nmodel compared with baseline judgement models\nincluding the judgement model of GPTFuzzer (Yu\net al., 2023), GPT-4 based judgement model and\nChatGPT based judgement model with a designed\nprompt (Figure 4 of Appendix A). As can be seen,\nour judgement model outperforms all baselines in\nterms of accuracy. In order to ensure that an attack\nclassified as successful by the judgement model\nis truly successful, we make a trade-off by sacri-\nficing the true positive rate in favor of achieving\na lower false positive rate. This implies that our\njudgement model is more stringent when compared\nto the baselines."}, {"title": "4.2 Analysis of Co-action Effect", "content": "In this section, we conduct an analysis of the\nco-action effect observed among multiple char-\nacters within the same jailbreak prompt using\nLLaMA-2 as the target LLM. To investigate this\nphenomenon, we divide our jailbreak prompt into\nmultiple prompts, each structured similarly to the\noriginal but featuring a single jailbreak character.\nThese divided templates are then sequentially ap-\nplied in an attempt to jailbreak the target model.\nAny successful jailbreak achieved during these at-\ntempts is considered a success. We denote this\nmodified variant as SOP-divided, as it involves in-\ndependent attacks by different jailbreak characters,\nwith no influence between characters at the jail-\nbreak stage. The attack results for each malicious\nrequest, both for SOP and SOP-divided, are visual-\nized in Figure 3. In addition to reporting the overall\njailbreak results, we also report whether each jail-\nbreak character responds with jailbreak results. As\ncan be seen, SOP-divided performs much worse\ncompared with SoP, with a decrease of 36 ASR.\nFurthermore, a similar trend is observed when com-\nparing the individual jailbreak characters between\nSOP-divided and SOP. These findings confirm our\nintuition on social facilitation."}, {"title": "4.3 Effect of Character Order", "content": "To understand the effect of character order, we re-\nverse the order of characters in SOP at test time and\ndenote this method as SOP-reversed using LLaMA-\n2 as the target LLM. As shown in Figure 3, the\nperformance of SOP-reversed is worse than that of\nSOP, which is consistent with our expectations: our\njailbreak characters are greedily generated, thus the\norder of characters is crucial. Nevertheless, even"}, {"title": "5 Further Attack and Defense", "content": "We evaluate the attack performance of SOP when\ntransferring cross requests and target models and\nwhen combined with other attack methods. We\nalso explore defending strategies against SOP."}, {"title": "5.1 Transfer Attack", "content": "To further examine the transfer performance of\nSOP, we evaluate SOP on transferred malicious\ndatasets and target models.\nWe attack the target LLMs with four held-out\nmalicious request sets: the remaining 470 instruc-\ntions from harmful behaviors dataset (Zou et al.,\n2023), 100 questions from GPTFuzzer (Yu et al.,\n2023), 400 questions from HarmBench (Mazeika\net al., 2024) and 100 questions from Jailbreak-\nBench (Chao et al., 2024). We also report the scores\nseparately on the training and testing sets of Ad-\nvBench custom. As shown in Table 4 and Table 10\nof Appendix C, the SOP jailbreak prompts can suc-\ncessfully transfer to the four held-out datasets. For\nexample, SOP achieves a commendable ASR of\n90%, 92%, 76% and 78% for the AdvBench remain-\ning, GPTFuzzer, HarmBench and JailbreakBench\ndataset, respectively, when attacking GPT-3.5-\n1106. These results demonstrate the strong cross-\nrequest transfer capability of SOP. Researchers typ-"}, {"title": "5.2 Combination with Other Attack Methods", "content": "Since the generated jailbreak templates can be in-\nte grated with any requests, it is possible to com-\nbine them with request-level jailbreak techniques,\nwherein these techniques clandestinely modify a\nmalicious request, rendering it hard to detect. We\nuse four rewriting strategies from Ding et al. (2023),\none encryption method (Yuan et al., 2023), and two\nlow-resource language translations (Yong et al.,\n2023). We preprocess the Advbench custom dataset\nusing these techniques before merging them with\nour templates for jailbreaking. GPT-4 is employed\nto rewrite and Google Translate to translate.\nAs presented in Table 6, these request-level tech-"}, {"title": "5.3 Defenses against SOP", "content": "This section explores detection-based and prompt-\nbased defense strategies that do not modify the base\nmodel.\n1. Detection-based: This type of defense detects\njailbreak prompts from the input space. Exam-\nples include Perplexity Filter (Jain et al., 2023),\nwhich defines a jailbreak prompt as failed when its\nlog perplexity exceeds a threshold; Rand-Insert,\nRand-Swap, and Rand-Patch (Robey et al., 2023),\nwhich alter the inputs and detect attack based on"}, {"title": "6 Related Work", "content": "Safety Alignment of Large Language Mod-\nels Large language models like ChatGPT and\nLLaMA have achieved state-of-the-art performance\non many natural language processing tasks. The\nsafety of LLM-based applications is significant as\nthey are open to the whole public and any LLM\nmisuse will bring about harm to the community. To\ndimish the harm and misuse brought by LLMs,\nthe released public LLMs are further trained on\nhuman preference data to align with human val-\nues, with algorithms like reinforcement learning\nvia human feedback (Ouyang et al., 2022) or di-\nrect preference optimization (Rafailov et al., 2023).\nAlthough finetuned for better alignment with hu-\nman values, LLMs are still vulnerable to various\ncarefully designed adversarial attacks like the jail-\nbreak attack (Wei et al., 2023a; Zou et al., 2023;\nWei et al., 2023c). The weakness of LLMs calls for\nfurther research on the safety alignment of LLMs\nas well as defense for different attacks.\nRed-Teaming LLMs via Jailbreak As one ap-\nproach to enhance the security of LLMs, red-\nteaming (Perez et al., 2022; Ganguli et al., 2022)\nexplores the weakness of LLMs and discloses the\ncovert failure cases of LLMs. Jailbreak attack is"}, {"title": "7 Conclusion", "content": "In this work, we propose SOP, a simple and ef-\nfective jailbreak attack framework that generates\na fluent and coherent jailbreak template universal\nto all malicious queries. Our framework draws in-\nspiration from the concept of social facilitation\nand leverages multiple auto-generated jailbreak\ncharacters to bypass the guardrail of the target\nLLMs. Through extensive experiments on four\ntarget LLMs, including both open-sourced and pro-"}, {"title": "Limitations", "content": "The proposed SOP is a simple yet strong red-\nteaming approach that generates effective jailbreak\ntemplates automatically. It achieves superior attack\nperformance across four prominent LLMs and ex-\nhibits commendable success rates in cross-target\nmodel transfer attacks compared with existing base-\nline methods. Due to our limited computation re-\nsources, we only examine the effectiveness of SOP\nmethod on four target LLMs. We will assess the\neffectiveness of SOP on more target LLMs. In Sec-\ntion 5.3, different defense strategies against SOP\nattack are compared but seldom achieve satisfac-\ntory results across different target LLMs. We leave\nthe exploration of effective defense strategy against\nSOP attack as future work."}, {"title": "Potential Risks and Ethical Statement", "content": "In this work, we employ a red-teaming approach to\ninvestigate the potential safety and security hazards\nwithin LLMs, with the primary objective of enhanc-\ning their safety rather than facilitating malicious\nexploitation. The potential risk of this work is the\nmalicious use of LLMs with the proposed Sop\nmethod. Following the common practice of red-\nteaming research, we have responsibly disclosed\nour findings to Meta and OpenAI in order to min-\nimize any potential harm resulting from the SoP\njailbreak attack prior to publication. As a result, it\nis possible that the SOP framework is no longer ef-\nfective. We also follow ethical guidelines through-\nout our study and will restrict the SOP details to\nauthorized researchers only."}, {"title": "A Details of Judgement Model", "content": "In our preliminary experiments, we employ base-\nline judgement models such as ChatGPT, GPT-4,\nand the classifier in GPTFuzzer for SOP and gen-\nerate responses for malicious requests. We collect\n2271 request-response pairs and manually label\nthem. We reserve 550 samples as the validation set\nand 500 as the test set, while the remaining 1221\ninstances, combined with 7700 samples sourced\nfrom Yu et al. (2023), are utilized as the training\ndataset. Given the multi-character nature of our jail-\nbreak template, we evaluate the response from each\ncharacter and mark it as a success if at least one of\nthe characters manages to successfully attack. In\nother words, our judgement model is applied to the\nindividual responses of each character rather than\ndirectly evaluating the aggregated responses of all\ncharacters. For the fine-tuning process, we employ\nthe DeBERTaV3-large model (He et al., 2021). The\ntraining is conducted for 15 epochs using a batch\nsize of 16. The learning rate is configured as le-\n5 and the maximum sequence length is set to 512.\nFor testing, our classifier pipeline would split the re-\nsponse by the character name first and classify each\nsplit sentence. The label of the complete response\nis success if any of the split sentences succeed. For\nthe GPT-based baseline judgement models, we em-\nploy the following prompt to enable ChatGPT or\nGPT-4 models to distinguish whether a response is\njailbroken."}, {"title": "B Algorithm for Jailbreak Character\nOptimization", "content": "Algorithm 1 presents the detailed algorithm for\noptimization of the Jailbreak characters of SOP."}, {"title": "C More Experiments", "content": "1\nMaximum number of characters\nOur optimization algorithm has a complexity of\nO(RI), where the maximum number of characters,\ndenoted as R, is linearly correlated with the cost.\nConsequently, we explore the relationship between\nthe number of characters and ASR to find a rea-\nsonable value for R, as shown in Figure 6. For\nthree out of four target LLMs, the ASR improves\nas the number of characters increases from 1 to 2.\nGiven that the three models have already achieved\nremarkably high ASR (> 80), the potential for"}, {"title": "C.2 Comparison Transfer Results with\nBaselines", "content": "In this section, we compare our transfer results on\nHarmBench and JailbreakBench datasets, as shown\nin Table 10. The baseline results are cited from\nMazeika et al. (2024) and Chao et al. (2024) for\nHarmBench and JailbreakBench, respectively. It\nis important to note that the setup for our method,\nSOP, is more challenging compared to the base-\nlines. SOP is trained on the AdvBench custom\ndataset and then evaluated on the HarmBench and\nJailbreakBench datasets. In contrast, the baseline\nmethods train and test on the same dataset."}, {"title": "D Detailed Prompts", "content": "1 Evaluation and Jailbreak Prompts\nWe list the meta prompt and the jailbreak template\n(evaluation template) of SOP, as shown in Figure\n7 and 8, respectively. Our meta prompt employs\nK jailbreak characters as examples to guide the\ngeneration of new jailbreak characters. To reduce\nthe human effort needed to design the examples,\nwe use a very simple jailbreak character as the ex-\nample when starting the optimization, as shown\nin Figure 10. This means that the meta prompt\nonly contains one example at startup and the num-"}, {"title": "D.2 Prompts for Prompt-based Defense", "content": "We list the defense prompts used for Self-Reminder\nand Adaptive Prompt, as shown in Figure 11 and\nFigure 12, respectively."}]}