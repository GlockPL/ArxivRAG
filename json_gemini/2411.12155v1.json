{"title": "REINFORCEMENT LEARNING WITH ACTION SEQUENCE FOR DATA-EFFICIENT ROBOT LEARNING", "authors": ["Younggyo Seo", "Pieter Abbeel"], "abstract": "Training reinforcement learning (RL) agents on robotic tasks typically requires a large number of training samples. This is because training data often consists of noisy trajectories, whether from exploration or human-collected demonstrations, making it difficult to learn value functions that understand the effect of taking each action. On the other hand, recent behavior-cloning (BC) approaches have shown that predicting a sequence of actions enables policies to effectively approximate noisy, multi-modal distributions of expert demonstrations. Can we use a similar idea for improving RL on robotic tasks? In this paper, we introduce a novel RL algorithm that learns a critic network that outputs Q-values over a sequence of actions. By explicitly training the value functions to learn the consequence of executing a series of current and future actions, our algorithm allows for learning useful value functions from noisy trajectories. We study our algorithm across various setups with sparse and dense rewards, and with or without demonstrations, spanning mobile bi-manual manipulation, whole-body control, and tabletop manipulation tasks from BiGym, HumanoidBench, and RLBench. We find that, by learning the critic network with action sequences, our algorithm outperforms various RL and BC baselines, in particular on challenging humanoid control tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement learning (RL) holds the promise of continually improving policies through online trial- and-error experiences (Sutton & Barto, 2018), making it an ideal choice for developing robots that can adapt to various environments. However, despite this promise, training RL agents on robotic tasks typically requires a prohibitively large number of training samples (Kalashnikov et al., 2018; Herzog et al., 2023), which becomes problematic as deploying robots often incurs a huge cost. Therefore many of the recent successful approaches on robot learning have been based on behavior-cloning (BC; Pomerleau 1988), which can learn strong policies from offline expert demonstrations (Brohan et al., 2023b;a; Zhao et al., 2023; Chi et al., 2023; Team et al., 2024; Fu et al., 2024a)."}, {"title": "2 BACKGROUND", "content": "Problem setup We mainly consider a robotic control problem which we formulate as a partially observable Markov decision process (Kaelbling et al., 1998; Sutton & Barto, 2018). At each time step t, an RL agent encounters an observation ot, executes an action at, receives a reward rt+1, and encounters a new observation ot+1 from the environment. Because the observation ot does not contain full information about the internal state of the environment, in this work, we use a stack of past observations as inputs to the RL agent by following the common practice in Mnih et al. (2015).\nFor simplicity, we omit the notation for these stacked observations. When the environment is fully observable, we simply use ot as inputs. Our goal in this work is to train a policy \ud835\udf0b that maximizes the expected sum of rewards through RL while using as few samples as possible, optionally with access to a modest amount of expert demonstrations collected either by motion-planners or by humans.\nInputs and encoding Given visual observations o = {o1, ..., oM} from M cameras, we encode each or using convolutional neural networks (CNN) into h\u017c. We then process them through a series of linear layers to fuse them into h. If low-dimensional observations o\u2122 are available along with visual observations, we process them through a series of linear layers to obtain how. We then use concatenated features ht = [h, h\u2122] as inputs to the critic network. In domains without vision sensors, we simply use o\u0142ow as h\u2081 without encoding the low-dimensional observations.\nCoarse-to-fine Q-Network Coarse-to-fine Q-Network (CQN; Seo et al. 2024) is a value-based RL algorithm for continuous control that trains RL agents to zoom-into the continuous action space in a coarse-to-fine manner. In particular, CQN iterates the procedures of (i) discretizing the continuous action space into multiple bins and (ii) selecting the bin with the highest Q-value to further discretize. This reformulates the continuous control problem as a multi-level discrete control problem, allowing for the use of ideas from sample-efficient value-based RL algorithms (Mnih et al., 2015; Silver et al., 2017; Schrittwieser et al., 2020), designed to be used with discrete actions, for continuous control.\nFormally, let a\u2081 be an action at level l with a\u2070 being the zero vector\u00b9. We then define the coarse-to-fine critic to consist of multiple Q-networks which compute Q-values for actions at each level a, given the features ht and actions from the previous level a\u00af\u00b9, as follows:\n$Q_l(h_t, a_l, a_{l-1}) \\text{ for } l \\in \\{1, ..., L\\}$\nWe optimize each Q-network at level l with the following objective:\n$L = (Q_\u03b8(h_t, a_l, a_{l-1}) - r_{t+1} - \\gamma \\max_{a'} Q_{\u03b8'}(h_{t+1}, a', \u03c0'(h_{t+1}))^2,$\nwhere \u03b8' are delayed parameters for a target network (Polyak & Juditsky, 1992) and \ud835\udf0b\u2032 is a policy that outputs the action a at each level l via the inference steps with our critic, i.e., \ud835\udf0b\u00b9(ht) = a. Specifically, to output actions at time step t with the critic, CQN first initializes constants a\u00b9low and a\u00b9high with \u22121 and 1. Then the following steps are repeated for l \u2208 {1, ..., L}:\n\u2022 Step 1 (Discretization): Discretize an interval [a\u00b9low, a\u00b9high] into B uniform intervals, and each of these intervals become an action space for Ql.\n\u2022 Step 2 (Bin selection): Find a bin with the highest Q-value and set a\u00b9 to the centroid of the bin.\n\u2022 Step 3 (Zoom-in): Set a\u00b9low and a\u00b9high to the minimum and maximum of the selected bin, which intuitively can be seen as zooming-into each bin.\nWe then use the last level's action aL as the action at time step t."}, {"title": "3 COARSE-TO-FINE Q-NETWORK WITH ACTION SEQUENCE", "content": "We present Coarse-to-fine Q-Network with Action Sequence (CQN-AS), a value-based RL algorithm that learns a critic network that outputs Q-values for a sequence of actions at:t+K = {at, ..., at+K\u22121} for a given observation ot. Our main motivation for this design comes from one of the key ideas in recent behavior-cloning (BC) approaches, i.e., predicting action sequences, which helps resolve ambiguity when approximating noisy, multi-modal distributions of expert demonstrations (Zhao et al., 2023; Chi et al., 2023). Similarly, by explicitly learning Q-values of both current and future actions from the given state, our approach aims to mitigate the challenge of learning Q-values with noisy trajectories from exploratory behaviors or human-collected demonstrations.\nThis section describes how we design our critic network with action sequence (see Section 3.1) and how we utilize action sequence outputs to control robots at each time step (see Section 3.2).\n3.  COARSE-TO-FINE CRITIC WITH ACTION SEQUENCE\nOur key idea is to design a critic network to explicitly learn Q-values for current action and future actions from the current time step t, i.e., {Q(ot, at), Q(ot, at+1), ..., Q(ot, at+K\u22121)}, to enable the critic to understand the consequence of executing a series of actions from the given state.\nFormulation and objective Let a\u00b9t:t+K = {a,..., a1t+K-1} be an action sequence at level l and a0t:t+k be a zero vector. We design our coarse-to-fine critic network to consist of multiple Q-networks that compute Q-values for each action at sequence step k and level l:\n$Q_{\u03b8}^{l, k}(h_t, a_{t+k-1}^{l-1}, a_{t+k}^l) \\text{ for } l \\in \\{1, ..., L\\} \\text{ and } k \\in \\{1, ..., K\\}$\nWe optimize our critic network with the following objective:\n$L = \\sum_k (Q_{\u03b8}^{l, k}(h_t, a_{t+k-1}^{l-1}, a_{t+k}^l) - r_{t+1} - \\gamma \\max_{a'} Q_{\u03b8'}^{l, k}(h_{t+1}, a', \u03c0_k^l (h_{t+1}))^2,$\nwhere \u03c0\u03ba is an action sequence policy that outputs the action sequence a\u00b9t:t+K. In practice, we compute Q-values for all sequence step k \u2208 {1, ..., K} in parallel, which is possible because Q-values for future actions depend only on current features ht but not on Q-values for previous actions. We find this simple design, with independence across action sequence, works well even on challenging humanoid control tasks with high-dimensional action spaces (Sferrazza et al., 2024). We expect our idea can be strengthened by exploiting the sequential structure, i.e., Q-values at subsequent steps depend on previous Q-values (Metz et al., 2017; Chebotar et al., 2023), but we leave it as future work.\nArchitecture We implement our critic network to initially extract features for each sequence step k and aggregate features from multiple steps with a recurrent network (see Figure 2). This architecture is often helpful in cases where a single-step action is already high-dimensional so that concatenating them make inputs too high-dimensional. Specifically, let ek denote an one-hot encoding for k. At each level l, we construct features for each sequence step k as hik = [ht, atk-1, ek]. We then encode each hlk with a shared MLP network and process them through GRU (Cho et al., 2014) to obtain sk = $f_{GRU}(f_{MLP}(h_{t, 1}), ..., f_{MLP}(h_{t, K}))$. We then use a shared projection layer to map each stk into Q-values at each sequence step k, i.e., $Q_{\u03b8}^{l, k}(o_t, a_{t+k-1}^{l-1}, a_{t+k}^l) = f_{proj}(s_{t, k})$.\nACTION EXECUTION AND TRAINING DETAILS\nWhile the idea of using action sequence is simple, there are two important yet small details for effectively training RL agents with action sequence: (i) how we execute actions at each time step to control robots and (ii) how we store training data and sample batches for training.\nExecuting action with temporal ensemble With the policy that outputs an action sequence at:t+K, one important question is how to execute actions at time step i \u2208 {t, ...,t + K \u2212 1}. For this, we use the idea of Zhao et al. (2023) that utilizes temporal ensemble, which computes at:t+K every time step, saves it to a buffer, and executes a weighted average \u2211i Wiat-i/\u2211wi where w\u2081 = exp(-m * i) denotes a weight that assigns higher value to more recent actions. We find this scheme outperforms the alternative of computing at:t+K every K steps and executing each action for subsequent K steps on most tasks we considered, except on several tasks that need reactive control.\nStoring training data from environment interaction When storing samples from online environment interaction, we store a transition (ot, \u00e2t, rt+1, 0t+1) where \u00e2\u2081 denotes an action executed at time step t. For instance, if we use temporal ensemble for action execution, \u00e2t is a weighted average of action outputs obtained from previous K time steps.\nSampling training data from a replay buffer When sampling training data from the replay buffer, we sample a transition with action sequence, i.e., (ot, \u00e2t:t+K, rt+1, 0t+1). If we sample time step t near the end of episode so that we do not have enough data to construct a full action sequence, we fill the action sequence with null actions. In particular, in position control where we specify the position of joints or end effectors, we repeat the action from the last step so that the agent learns not to change the position. On the other hand, in torque control where we specify the force to apply to joints, we set the action after the last step to zero so that agent learns to not to apply force."}, {"title": "4 EXPERIMENT", "content": "We study CQN-AS on 53 robotic tasks spanning mobile bi-manual manipulation, whole-body control, and tabletop manipulation tasks from BiGym (Chernyadev et al., 2024), HumanoidBench (Sferrazza et al., 2024), and RLBench (James et al., 2020) environments (see Figure 3 for examples of robotic tasks). These tasks with sparse and dense rewards, with or without vision sensors, and with or without demonstrations, allow for evaluating the capabilities and limitations of our algorithm. In particular, our experiments are designed to investigate the following questions:\n\u2022 Can CQN-AS quickly match the performance of a recent BC algorithm (Zhao et al., 2023) and surpass it through online learning? How does CQN-AS compare to previous model-free RL algorithms (Haarnoja et al., 2018; Yarats et al., 2022; Seo et al., 2024)?\n\u2022 What is the contribution of each component in CQN-AS?\n\u2022 Under which conditions is CQN-AS effective? When does CQN-AS fail?\nBaselines for fine-grained control tasks with demonstrations For tasks that need high-precision control, e.g., manipulation tasks from BiGym and RLBench, we consider model-free RL baselines that aim to learn deterministic policies, as we find that stochastic policies struggle to solve such fine-grained control tasks. Specifically, we consider (i) Coarse-to-fine Q-Network (CQN; Seo et al. 2024), a value-based RL algorithm that learns to zoom-into continuous action space in a coarse-to-fine manner, and (ii) DrQ-v2+, an optimized demo-driven variant of a model-free actor-critic algorithm DrQ-v2 (Yarats et al., 2022) that uses a deterministic policy algorithm and data augmentation. We further consider (iii) Action Chunking Transformer (ACT; Zhao et al. 2023), a BC algorithm that trains a transformer (Vaswani et al., 2017) policy to predict action sequence and utilizes temporal ensemble for executing actions, as our highly-optimized BC baseline.\nBaselines for whole-body control tasks with dense reward For locomotion tasks with dense reward, we consider (i) Soft Actor-Critic (SAC; Haarnoja et al. 2018), a model-free actor-critic RL algorithm that maximizes action entropy, and (ii) Coarse-to-fine Q-Network (CQN; Seo et al. 2024). Moreover, although it is not the goal of this paper to compare against model-based RL algorithms, we also consider two model-based baselines: (iii) DreamerV3 (Hafner et al., 2023), a model-based RL algorithm that learns a latent dynamics model and a policy using imagined trajectories and (iv) TD-MPC2 (Hansen et al., 2024), a model-based RL algorithm that learns a latent dynamics model and utilizes local trajectory optimization in imagined latent trajectories.\nImplementation details For training with expert demonstrations, we follow the setup of Seo et al. (2024). Specifically, we keep a separate replay buffer that stores demonstrations and sample half of training data from demonstrations. We also relabel successful online episodes as demonstrations and store them in the demonstration replay buffer. For CQN-AS, we use an auxiliary BC loss from Seo et al. (2024) based on large margin loss (Hester et al., 2018). For actor-critic baselines, we use an auxiliary BC loss that minimizes L2 loss between the policy outputs and expert actions. We implement CQN-AS based on a publicly available source code of CQN based on PyTorch (Paszke et al., 2019)."}, {"title": "4.1 BIGYM EXPERIMENTS", "content": "We study CQN-AS on mobile bi-manual manipulation tasks from BiGym (Chernyadev et al., 2024). BiGym's human-collected demonstrations are often noisy and multi-modal, posing challenges to RL algorithms which should effectively leverage demonstrations for solving sparsely-rewarded tasks.\nSetup Because we find that not all demonstrations from BiGym benchmark can be successfully replayed, we replay all the demonstrations and only use the successful ones as demonstrations. We do not discard ones that fail to be replayed, but we use them as training data with zero reward because they can still be useful as failure experiences. To avoid training with too few demon- strations, we exclude the tasks where the ratio of successful demonstrations is below 50%. This leaves us with 25 tasks, each with 17 to 60 demonstrations. For visual observations, we use RGB observations with 84\u00d784 resolution from head, left_wrist, and right_wrist cameras. We also use low-dimensional proprioceptive states from proprioception, proprioception_grippers, and proprioception_floating_base sensors. We use (i) absolute joint position control action mode and (ii) floating base that replaces locomotion with classic controllers. We use the same set of hyperparameters for all the tasks, in particular, we use action sequence of length 16. More details on BiGym experiments are available in Appendix A."}, {"title": "4.2 HUMANOIDBENCH EXPERIMENTS", "content": "To show that CQN-AS can be generally applicable to tasks without demonstrations, we study CQN-AS on densely-rewarded humanoid control tasks from HumanoidBench (Sferrazza et al., 2024).\nSetup For HumanoidBench, we follow a standard setup that trains RL agents from scratch, which is also used in original benchmark (Sferrazza et al., 2024). Specifically, we use low-dimensional states consisting of proprioception and privileged task information as inputs. For tasks, we simply select the first 8 locomotion tasks in the benchmark. Following the original benchmark that trains RL agents for environment steps that roughly requires 48 hours of training, we report the results of CQN-AS and CQN for 7 million steps. For baselines, we use the results available from the public repository, which are evaluated on tasks with dexterous hands, and we also evaluate our algorithm on tasks with hands. We use the same set of hyperparameters for all the tasks, in particular, we use action sequence of length 4. More details on HumanoidBench experiments are available in Appendix A.\nComparison to model-free RL baselines Figure 5 shows the results on on HumanoidBench. We find that, by learning the critic network with action sequence, CQN-AS outperforms other model-free RL baselines, i.e., CQN and SAC, on most tasks. In particular, the difference between CQN-AS and baselines becomes larger as the task gets more difficult, e.g., baselines fail to achieve high episode return on Walk and Run tasks but CQN-AS achieves strong performance. This result shows that our idea of using action sequence can be applicable to generic setup without demonstrations."}, {"title": "4.3 RLBENCH EXPERIMENTS", "content": "To investigate whether CQN-AS can also be effective in leveraging clean demonstrations, we study CQN-AS on RLBench (James et al., 2020) with synthetic demonstrations.\nSetup For RLBench experiments, we use the official CQN implementation for collecting demon- strations and reproducing the baseline results on the same set of tasks. Specifically, we use RGB observations with 84\u00d784 resolution from front, wrist, left_shoulder, and right_shoulder cam- eras. We also use low-dimensional proprioceptive states consisting of 7-dimensional joint positions and a binary value for gripper open. We use 100 demonstrations and delta joint position control action mode. We use the same set of hyperparameters for all the tasks, in particular, we use action sequence of length 4. More details on RLBench experiments are available in Appendix A.\nCQN-AS is also effective with clean demonstrations Because RLBench provides synthetic clean demonstrations, as we expected, Figure 6 shows that CQN-AS achieves similar performance to CQN on many of the tasks, except 2/25 tasks where it hurts the performance. But we still find that CQN-AS achieves quite superior performance to CQN on some challenging long-horizon tasks such as Open Oven or Take Plate Off Colored Dish Rack. These results, along with results from BiGym and HumanoidBench, show that CQN-AS can be used in various benchmark with different characteristics."}, {"title": "4.4 ABLATION STUDIES, ANALYSIS, FAILURE CASES", "content": "Effect of action sequence length Figure 7a shows the performance of CQN-AS with different action sequence lengths on two BiGym tasks. We find that training the critic network with longer action sequences improves and stabilizes performance.\nRL objective is crucial for strong performance Figure 7b shows the performance of CQN-AS without RL objective that trains the model only with BC objective on successful demonstrations. We find this baseline significantly underperforms CQN-AS, which shows that RL objective is indeed enabling the agent to learn from online trial-and-error experiences.\nEffect of temporal ensemble Figure 7c shows that performance largely degrades without temporal ensemble on Saucepan To Hop as temporal ensemble induces a smooth motion and thus improves performance in fine-grained control tasks. But we also find that temporal ensemble can be harmful on Reach Target Single. We hypothesize this is because temporal ensemble often makes it difficult to refine behaviors based on recent visual observations. Nonetheless, we use temporal ensemble for all the tasks as we find it helps on most tasks and we aim to use the same set of hyperparameters.\nFailure case: Torque control Figure 7d shows that CQN-AS underperforms CQN on locomotion tasks with torque control. We hypothesize this is because a sequence of joint positions usually has a semantic meaning in joint spaces, making it easier to learn with, when compared to learning how to apply a sequence of torques. Addressing this failure case is an interesting future direction."}, {"title": "5 RELATED WORK", "content": "Behavior cloning with action sequence Recent behavior cloning approaches have shown that predicting a sequence of actions enables the policy to effectively imitate noisy expert trajectories and helps in dealing with idle actions from human pauses during data collection (Zhao et al., 2023; Chi et al., 2023). In particular, Zhao et al. (2023) train a transformer model (Vaswani et al., 2017) that predicts action sequence and Chi et al. (2023) train a denoising diffusion model (Ho et al., 2020) that approximates the action distributions. This idea has been extended to multi-task setup (Bharadhwaj et al., 2024), mobile manipulation (Fu et al., 2024b) and humanoid control (Fu et al., 2024a). Our work is inspired by this line of work and proposed to learn RL agents with action sequence.\nReinforcement learning with action sequence In the context of reinforcement learning, Medini & Shrivastava (2019) proposed to pre-compute frequent action sequences from expert demonstrations and augment the action space with these sequences. However, this idea introduces additional complexity and is not scalable to setups without demonstrations. One recent work relevant to ours is Saanum et al. (2024) that encourages a sequence of actions from RL agents to be predictable and smooth. But this differs from our work in that it uses the concept of action sequence only for computing the penalty term. Recently, Ankile et al. (2024) point out that RL with action sequence is challenging and instead proposes to use RL for learning a single-step policy that corrects action sequence predictions from BC. In contrast, our work shows that training RL agents with action sequence is feasible and leads to improved performance compared to prior RL algorithms."}, {"title": "6 CONCLUSION", "content": "We presented Coarse-to-fine Q-Network with Action Sequence (CQN-AS), a value-based RL algo- rithm that trains a critic network that outputs Q-values over action sequences. Extensive experiments in benchmarks with various setups show that our idea not only improves the performance of the base algorithm but also allows for solving complex tasks where prior RL algorithms completely fail.\nWe believe our work will be strong evidence that shows RL can realize its promise to develop robots that can continually improve through online trial-and-error experiences, surpassing the performance of BC approaches. We are excited about future directions, including real-world RL with humanoid robots, incorporating advanced critic architectures (Kapturowski et al., 2023; Chebotar et al., 2023; Springenberg et al., 2024), bootstrapping RL agents from imitation learning (Hu et al., 2023; Xing et al., 2024) or offline RL (Nair et al., 2020; Lee et al., 2021), extending the idea to recent model-based RL approaches (Hafner et al., 2023; Hansen et al., 2024), to name but a few."}, {"title": "A EXPERIMENTAL DETAILS", "content": "BiGym BiGym\u2074 (Chernyadev et al., 2024) is built upon MuJoCo (Todorov et al., 2012). We use Unitree H1 with two parallel grippers. We find that demonstrations available in the recent version of BiGym are not all successful. Therefore we adopt the strategy of replaying all the demonstrations and only use the successful ones as demonstrations. instead of discarding the failed demonstrations, we still store them in a replay buffer as failure experiences. To avoid training with too few demonstrations, we exclude the tasks where the ratio of successful demonstrations is below 50%.\nHumanoidBench HumanoidBench (Sferrazza et al., 2024) is built upon MuJoCo (Todorov et al., 2012). We use Unitree H1 with two dexterous hands. We consider the first 8 locomotion tasks in the benchmark: Stand, Walk, Run, Reach, Hurdle, Crawl, Maze, Sit Simple. We use proprioceptive states and privileged task information instead of visual observations. Unlike BiGym and RLBench experiments, we do not utilize dueling network (Wang et al., 2016) and distributional critic (Bellemare et al., 2017) in HumanoidBench for faster experimentation.\nRLBench RLBench (James et al., 2020) is built upon CoppeliaSim (Rohmer et al., 2013) and PyRep (James et al., 2019). We use a 7-DoF Franka Panda robot arm and a parallel gripper. Following the setup of Seo et al. (2024), we increase the velocity and acceleration of the arm by 2 times. For all experiments, we use 100 demonstrations generated via motion-planning."}, {"title": "B FULL DESCRIPTION OF CQN AND CQN-AS", "content": "This section provides the formulation of CQN and CQN-AS with n-dimensional actions.\nB.1 COARSE-TO-FINE Q-NETWORK\nLet a1l,n be an action at level l and dimension n and $a^0 = \\{ a_{1}^{l,1}, ..., a_{N}^{l,N} \\}$ be actions at level l with $a^0$ being zero vector. We then define coarse-to-fine critic to consist of multiple Q-networks:\n$Q_{\u03b8}^{l,n}(h_t, a_l^n, a_{l-1}) \\text{ for } l \\in \\{1, ..., L\\} \\text{ and } n \\in \\{1, ..., N\\}$\nWe optimize the critic network with the following objective:\n$\\sum_n (Q_{\u03b8}^{l,n}(h_t, a_l^n, a_{l-1}) - r_{t+1} - \\gamma \\max_{a'} Q_{\u03b8'}^{l,n}(h_{t+1}, a', \u03c0'(h_{t+1}))^2,$\nwhere \u03b8' are delayed parameters for a target network (Polyak & Juditsky, 1992) and \ud835\udf0b\u2032 is a policy that outputs the action a at each level l via the inference steps with our critic, i.e., \ud835\udf0b\u00b9(ht) = a.\nAction inference To output actions at time step t with the critic, CQN first initializes constants $a_{n, low}^l$ and $a_{n, high}^l$ with \u22121 and 1 for each n. Then the following steps are repeated for l \u2208 {1, ..., L}:\n\u2022 Step 1 (Discretization): Discretize an interval $[a_{n, low}^l, a_{n, high}^l]$ into B uniform intervals, and each of these intervals become an action space for $Q_l^n$.\n\u2022 Step 2 (Bin selection): Find the bin with the highest Q-value, set $a_l^n$ to the centroid of the selected bin, and aggregate actions from all dimensions to a.\n\u2022 Step 3 (Zoom-in): Set $a_{n, low}^l$ and $a_{n, high}^l$ to the minimum and maximum of the selected bin, which intuitively can be seen as zooming-into each bin.\nWe then use the last level's action aL as the action at time step t.\nComputing Q-values To compute Q-values for given actions at, CQN first initializes constants $a_{n, low}^l$ and $a_{n, high}^l$ with \u22121 and 1 for each n. We then repeat the following steps for l \u2208 {1, ..., L}:\n\u2022 Step 1 (Discretization): Discretize an interval $[a_{n, low}^l, a_{n, high}^l]$ into B uniform intervals, and each of these intervals become an action space for $Q_l^n$.\n\u2022 Step 2 (Bin selection): Find the bin that contains input action at, compute $a_l^n$ for the selected interval, and compute Q-values $Q_{\u03b8}^{l,n}(h_t, a_l^n, a_{l-1})$.\n\u2022 Step 3 (Zoom-in): Set $a_{n, low}^l$ and $a_{n, high}^l$ to the minimum and maximum of the selected bin, which intuitively can be seen as zooming-into each bin.\nWe then use a set of Q-values $\\{Q_{\u03b8}^{l,n}(h_t, a_l^n, a_{l-1})\\}_{l=1}^L$ for given actions at.\nB.2 COARSE-TO-FINE CRITIC WITH ACTION SEQUENCE\nLet $a_{t:t+K}^l = \\{ a_{t}^{1,l}, ..., a_{t+K-1}^{l,K} \\}$ be an action sequence at level l and $a_{t:t+K}^0$ be zero vector. Our critic network consists of multiple Q-networks for each level l, dimension n, and sequence step k:\n$Q_{\u03b8}^{l,n,k}(h_t, a_{t+k-1}^{l-1}, a_{t+k}^l) \\text{ for } l \\in \\{1, ..., L\\}, n \\in \\{1, ..., N\\} \\text{ and } k \\in \\{1, ..., \u039a\\}$\nWe optimize the critic network with the following objective:\n$\\sum_n \\sum_k (Q_{\u03b8}^{l,n,k}(h_t, a_{t+k-1}^{l-1}, a_{t+k}^l) - r_{t+1} - \\gamma \\max_{a'} Q_{\u03b8'}^{l,n,k}(h_{t+1}, a', \u03c0_k^l (h_{t+1}))^2,$\nwhere \u03c0\u03ba is an action sequence policy that outputs the action sequence $a_{t:t+K}^l$. In practice, we compute Q-values for all sequence step k \u2208 {1, ..., K} and all action dimension n \u2208 {1, ..., N} in parallel. This can be seen as extending the idea of Seyde et al. (2023), which learns decentralized Q-networks for action dimensions, into action sequence dimension. As we mentioned in Section 3.1, we find this simple scheme works well on challenging tasks with high-dimensional action spaces.\nArchitecture Let ek denote an one-hot encoding for k. For each level l, we construct features for each sequence step k as $h_{t, k}^l = [h_t, a_{t+k-1}^{l-1}, e_k]$. We encode each $h_{t,k}^l$ with a shared MLP network and process them through GRU (Cho et al., 2014) to obtain $s_{t,k}^l = f_{GRU}(f_{MLP}(h_{t,1}^l), ..., f_{MLP}(h_{t,K}^l))$. We use a shared projection layer to map each $s_{t,k}^l$ into Q-values at each sequence step k, i.e., $\\{Q_{\u03b8}^{l,k}(o_t, a_{t+k-1}^{l-1}, a_{t+k}^l)\\}_{k=1}^K = f_{proj}(s_{t, k}^l)$. We note that we compute Q-values for all dimensions n \u2208 {1, ..., N} at the same time with a big linear layer, which follows the design of Seo et al. (2024)."}]}