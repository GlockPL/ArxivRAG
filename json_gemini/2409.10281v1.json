{"title": "DreamHead: Learning Spatial-Temporal Correspondence via Hierarchical Diffusion for Audio-driven Talking Head Synthesis", "authors": ["Fa-Ting Hong", "Yunfei Liu", "Yu Li", "Changyin Zhou", "Fei Yu", "Dan Xu"], "abstract": "Audio-driven talking head synthesis strives to generate lifelike video portraits from provided audio. The diffusion model, recognized for its superior quality and robust generalization, has been explored for this task. However, establishing a robust correspondence between temporal audio cues and corresponding spatial facial expressions with diffusion models remains a significant challenge in talking head generation. To bridge this gap, we present DreamHead, a hierarchical diffusion framework that learns spatial-temporal correspondences in talking head synthesis without compromising the model's intrinsic quality and adaptability. DreamHead learns to predict dense facial landmarks from audios as intermediate signals to model the spatial and temporal correspondences. Specifically, a first hierarchy of audio-to-landmark diffusion is first designed to predict temporally smooth and accurate landmark sequences given audio sequence signals. Then, a second hierarchy of landmark-to-image diffusion is further proposed to produce spatially consistent facial portrait videos, by modeling spatial correspondences between the dense facial landmark and appearance. Extensive experiments show that proposed DreamHead can effectively learn spatial-temporal consistency with the designed hierarchical diffusion and produce high-fidelity audio-driven talking head videos for multiple identities.", "sections": [{"title": "1. Introduction", "content": "Talking head generation [2, 12, 14, 15, 19, 29, 45] aims to produce realistic-looking portrait videos with a specific driven condition. Given an audio-driven signal, talking head generation methods synthesize facial images with lip shapes synchronized to the audio. Audio-driven talking head methods can be widely applied in various applications, including live broadcasting and video conferencing.\nIn recent developments, generative adversarial networks (GANs) [7, 19, 24, 45] produce portrait videos with limited quality because of their inherent instability in the training stage. Meanwhile, diffusion models [3, 26, 28, 30] become popular because they can render high-quality images while maintaining their generalization capabilities. However, the performance of existing diffusion models [3, 28, 30] for taking head generation falls short of expectations. Using diffusion model to address talking head video generation still remains challenging and is non-trivial to realize high-quality generation. A remaining major challenge is that the existing diffusion-based models fail to explicitly establish accurate spatial-temporal correspondences between the audio input and the target facial dynamics. Instead, current methods [28] predominantly rely on the diffusion model's implicit learning capabilities, which proves insufficient for effectively capturing and synchronizing the audio cues with the facial expressions. To this end, we propose to model the spatial-temporal correspondence between the given audio sequence and the target facial image sequence within a diffusion model. Because facial dynamics lie on a high-dimensional manifold, making it nontrivial to find a mapping from audio [46]. However, we found that landmarks serve as an efficient representation for facial expressions [19, 20, 46]. Therefore, we employ two diffusion stages to map the audio to lip animation, utilizing landmarks as an intermediate representation to bridge two stages. Taking the landmarks as intermediate representation, the two-stage strategy allows us to alleviate the challenges associated with learning audio-lip mapping.\nTo tackle the above-discussed problem for high-fidelity talking head video generation, specifically, we present a novel hierarchical diffusion framework, termed as DreamHead. It is designed to learn the spatial-temporal correspondence between the audio sequence and facial dynamics for synthesizing portrait videos. Inspired by [19, 20, 46], we realize that dense facial landmarks can represent facial expressions accurately. To model the temporal information contained in the audio input, in the first hierarchy of diffusion, we design a lightweight audio-to-landmark diffusion (A2L) for learning the temporal correspondence between the given audio and the facial landmarks sequence. With a diffusion-based optimization, our designed audio-to-landmark model can eliminate the jittering artifacts and estimate temporal-smooth landmark sequences. In the second hierarchy of diffusion, we further design a landmark-to-image diffusion (L2I) to generate photo-realistic portrait videos with the facial landmarks predicted from the first hierarchy of diffusion (i.e., A2L). With the predicted dense facial landmarks and their corresponding face images, the designed L2I diffusion can learn to effectively model spatial correspondence between the landmarks and the face expression with self-attention aggregation in the diffusion process. Therefore, with the proposed hierarchical diffusion model, we can effectively construct spatial and temporal correspondences between the input audio signal and the output face images through the intermediate facial landmarks, leading to a higher-quality generation of cross-modal synchronized faces. Moreover, as the facial landmarks are generated through the first hierarchy of diffusion (i.e., A2L), no ground-truth facial landmarks are required during the inference stage.\nWe conduct extensive experiments to evaluate the proposed DreamHead on two competitive audio-driven talking head generation datasets (i.e. HDTF [43] and MEAD [38]). From the experimental results, we can observe that our designed audio-to-landmark diffusion can estimate accurate and stable facial landmark sequences, given the audio condition. The landmark-to-image diffusion can generate realistic appearance and spatial-temporal consistent portrait video. The experimental results also show clearly improved generation results over state-of-the-art methods from both qualitative and quantitative perspectives.\nIn summary, our main contribution is three-fold:\n\u2022 We propose learning dense facial landmarks as an intermediate bridge within a diffusion framework to model the spatial-temporal correspondence for talking head synthesis. Integrating landmarks can ensure temporal consistency between the audio and the final video, and provides explicit spatial constraints for accurate lip movements in synthesized talking head videos.\n\u2022 We design a novel hierarchical diffusion model, coined as DreamHead, for audio-driven talking head video generation. Specifically, DreamHead is constructed by a hierarchy of two diffusion structures: (i) Audio-to-landmark diffusion structure generates accurate and temporal-smooth jittering-less facial landmarks from the input audio. (ii) Landmark-to-image diffusion structure synthesizes high-fidelity portrait videos via learning spatial correspondences between landmarks and facial expressions.\n\u2022 Extensive experiments on two datasets show that DreamHead can estimate jitter-less landmark sequences, and produce spatially and temporally consistent portrait videos. Furthermore, our diffusion model exhibits superior generation performance across the different benchmarks compared to state-of-the-art counterparts."}, {"title": "2. Related Works", "content": "Audio-driven Talking Head Generation. Existing approaches [2, 5, 13, 32, 34, 44] can be divided into two categories, i.e. subject-specific and subject-general. Regarding subject-specific audio-driven talking head methods [3, 9, 19, 20, 40, 41], they train a personalized renderer for each person. For instance, MODA [19] learns the correlation between lip shape and other movements on the face to synthesise a natural face. AD-NeRF [9] attempts to leverage volume rendering on two elaborately designed NeRFs to synthesize portrait videos directly. Different from those subject-specific methods, subject-general methods [10, 17, 24, 28, 31, 33, 44-46] can produce portrait videos given audios and human faces with multiple identities. Wav2Lip [24] accepts an audio sequence as input and inpaint the mouth region while maintaining other facial regions unmodified. Different from Wav2Lip [24], PC-AVS [45] tried to disentangle the pose and identity representations from a face image to enable the control of face generation. Besides, SadTalker [42] uses audio to estimate 3D coefficients for talking head generation. Different from the above-discussed methods, our proposed DreamHead framework focuses on subject-generic audio-driven talking head generation. It leverages the diffusion process to produce high-quality face video generation and achieve the capability of model generalization.\nLatent Diffusion Models. Latent Diffusion Models [26] has gained attention in recent years because of their strong ability in various image generation tasks [3, 11, 16, 18, 22, 25, 28], e.g., video editing [25] and motion transfer [18]. Because of the stable training procedure and high-quality generation, researchers also utilize diffusion models to deal with the talking head generation task [28, 30]. For instance, DiffTalk [28] follows the setting of Wav2Lip [24] and designs an autoregressive method to produce portrait video in their inference stage. DAE-Talker [3] utilized a diffusion autoencoder to address subject-specific talking head generation. However, these models rely on implicit learning capabilities of the diffusion model to map the audio to images, which cannot effectively model important spatial relationships for image synthesis. In this work, we design a hierarchical diffusion framework to learn the spatial-temporal correspondence between the input audio and the output face video using diffused landmarks as intermediate guidance signals. In this way, our method can produce spatially consistent and temporally smooth portrait videos."}, {"title": "3. The Proposed DreamHead Approach", "content": "Given a audio sequence A and a video clip V, a talking portrait method aims to map it into the corresponding video clip V, which should have the length as same as the audio sequence A. Following the previous works [24, 28], we aim to inpaint the lip region corresponding to the given audio while maintaining other parts of the faces unchanged.\n3.1. Overview\nAs illustrated in Figure 2, our DreamHead is composed of a hierarchy of two diffusion processes, which are connected by diffused dense facial landmarks: (i) Audio-to-landmark diffusion stage. Taking the audio context into consideration, DreamHead learns an audio-to-landmark diffusion in the first hierarchy to estimate a facial landmark sequence corresponding to the given temporal audio sequence. Given a video as input, we first compute the mean canonical landmarks P and feed it with the given audio sequence into the audio-to-landmark diffusion (A2L) process to generate the lip-synced landmarks sequences. Importantly, the A2L diffusion process efficiently aligns audios with landmarks using multiple temporal blocks, ensuring temporally smooth and synchronized landmark sequences. (ii) In the second hierarchy of landmark-to-image diffusion, we model spatial correspondence between landmarks and face expressions, which allows the landmark-to-image diffusion (L2I) to produce spatially consistent frames with accurate appearance. Additionally, we draw the landmarks in canvas as images (IPO and Iref) and feed them into the L2I diffusion model. In this way, the model can learn the spatial correspondence between expression and landmarks. As the generated landmarks through A2L are temporally smooth, the generated final faces can achieve spatial-temporal consistency. In the following, we will show the detailed learning procedures of these two diffusion processes.\n3.2. Audio to Landmark Diffusion\nMapping audio to the distribution of pixels is a highly ill-posed problem since it involves dealing with a vast, continuous output space with a high degree of variability and less direct correlation to the input audio signals. Instead, audio-to-landmark mapping is a more constrained problem because of the inherent correspondence between the phoneme and landmarks of the lip. Therefore, we first learn a diffusion to map the input audio sequence to a temporally smooth landmark sequence using the first hierarchy of audio-to-landmark diffusion of our DreamHead framework.\nLandmark normalization. To facilitate the learning of the audio-to-landmark diffusion process, we preprocess the facial landmarks to enhance the connections between the audio cues and the facial landmarks by filtering the audio-irrelevant information in the dense facial landmarks. To make the model focus on lip dynamics, we first project the 3D dense facial landmarks into a canonical space to eliminate the influence of head poses. Then, we normalize each canonical landmark by the mean and variance, which are calculated from all the canonical landmarks in the given video. Therefore, even a randomly initialized noise can obtain a rough face outline after de-normalizing (see Pi-1:i] in Figure 3). Generally, the variance in canonical landmarks can represent the talking style of a given person. Because audio is weakly related to personalized talking styles, we can eliminate the talking style information in the landmarks through normalization to facilitates the learning of the audio-to-landmark mapping. In this way, we train our audio-to-landmark diffusion with normalized pose-irrelevant canonical landmark as supervision.\nTemporal modeling. To capture the temporal dynamics within the audio sequence, we feed an audio segment that corresponds to image frames from the (i - 1)-th to the i-th in the video into the A2L diffusion. Before feeding audios into the A2L model, we first process the input audio sequence using the pre-trained wav2vec model [27], resulting in a sequence of audio embeddings A[i-l:i]. To provide the identity information for landmarks generation, we also feed the mean facial landmarks P of all canonical landmarks in a video as a condition into the first hierarchy of diffusion (see Figure 3). As shown in Figure 3, we further feed the audio embeddings through a fully connected layer fa(). This adaptation tailors the audio features specifically for the task of talking head generation. Additionally, we also utilize several fully connected layers (fp(\u00b7), fp(\u00b7), ft() and fagg()) to change the dimension of the mean facial landmark P, landmark sequence P Pi-1:i] and the timesteps t as follows:\n$Xa21 = fA(A[i-l:i]) + fp(P) + fagg(fp(P[i\u2212l:i]) \u2295 ft(t))$, (1)\nwhere \u2295 is the concatenation operator and fagg is used to fuse the features of the timesteps and the landmarks. To establish the temporal correspondence between the audio and the facial landmarks, we utilize a sequence of temporal blocks, which contains a temporal unit and a mapping unit, inside the A2L diffusion model to explore the temporal correlation of all inputs. As shown in the Figure 3, we adopt N temporal blocks in A2L network to encode the condition sequentially as follows:\n$Et = fout proj(ft projN-1(\u00b7\u00b7\u00b7 fout proj fit proj (Xa21))\u00b7\u00b7\u00b7))$, (2)\nwhere fr is the n-th temporal block and foutproj is the projection-out function as shown in Figure 3, which is a fully-connected layer. The \u201co\u201d denotes module composition, indicating the output of the latter is the input of the former. To connect the temporal context of the input audio embeddings with a constraint on parameter growth, we implement the temporal unit as a combination of a normalization layer, a convolutional layer [1], and a ReLU function, while the mapping unit is a combination of a normalization layer, a fully connected layer, and a ReLU function to smooth the output (see Figure 3). By stacking multiple temporal blocks, our audio-to-landmark network aggregate the temporal cues from the audio and produce temporal-smooth and jitter-less facial landmark sequences with limited parameters.\nDiffusion learning. Instead of directly predicting the landmarks given an audio sequence, we employ a diffusion process to align the landmark sequence with the audio sequence gradually. We build our audio-to-landmark network as a time-conditional denoising network, which learns the reversion process of a Markov Chain [6] of length T. The corresponding objective can be formulated as follows:\n$La21 := EP\u1d62,\u03f5\u223cN(0,1),t [||\u03f5 \u2013 A2L(P[i-1:i], t, Ca21)||]$, (3)\nwhere A2L(P[i-1:i], t, Ca21) represents the noise prediction produced by the audio-to-landmark diffusion, and Ca2l is the condition set {A[i-l:i], P}. By modeling the audio-to-landmark mapping as a cross-modal diffusion process, we can align the audio and the landmarks well to produce an accurate and temporally smooth facial landmark sequence with less jittering artifacts.\n3.3. Landmarks to Image Diffusion\nThe facial landmarks contain accurate lip shapes estimated by the first hierarchy of the audio-to-landmark (A2L) diffusion. These landmarks provide explicit spatial structure cues about face expressions, which are very beneficial for producing lip-synced face videos. To ensure a high-quality generation of the face videos and achieve a good generalization ability of the model, we adopt a landmark-to-image (L2I) diffusion process, which is a latent diffusion model [26], to generate the portrait videos in the second hierarchy of our landmark-to-image diffusion.\nExplicit spatial cues from facial landmarks. As shown in Figure 2(b), given a target masked image Im that provides the appearance, head pose, and image background information, our landmark-to-image diffusion focuses on inpainting the lip region with explicit spatial cues from the previous predicted facial landmarks to create the image I\u2081 in the generated video. Given the normalized canonical landmarks Po predicted by the audio-to-landmark diffusion, we first denormalize them and restore their pose to that of the original corresponding frame. Different from the previous audio-to-landmark diffusion, we draw the landmarks as an image IP to provide an explicit spatial structure relationship of the face expression. By transforming the landmarks as images, the L2I diffusion process can render the target portrait image by querying the corresponding landmark spatially through self-attention interactions in unet.\nSpatial-correspondence condition. Similar to DiffTalk [28], we also provide a random image in the same video as a reference image Iref, which can provide the appearance cues for mouth region during generation. In the inference phrase, we will take the first frame as the reference image in all frames generation. In addition to the reference image Iref, we also provide its real landmark image IPref as a condition. Given the reference image and the reference landmark image, the L2I diffusion model in the second hierarchy can effectively learn the spatial correspondence between the facial expression and the landmarks. Based on the learned spatial correspondence, the L2I diffusion model can inpaint the missing lip region by querying the target landmark image IPO, which is also a condition. Therefore, we utilize a target landmark IPO, a target masked image Im, a reference image Iref, and a reference landmark Ipref to construct a spatial-correspondence condition for our landmark-to-image diffusion process.\nDiffusion learning. We adopt a latent diffusion model [26] in our landmark-to-image diffusion process to produce portrait videos. The image encoder E and the decoder D are pretrained in [4] and frozen during the training of our generation model. Our conditions, including the masked target face image Im \u2208 RH\u00d7W\u00d73, the target landmark image IPO \u2208 RH\u00d7W\u00d73, the reference image Iref \u2208 RH\u00d7W\u00d73, and the reference landmark image Ipref \u2208 RH\u00d7W\u00d73, can be encoded into a latent space as:\n$zm, zl, zr, zp:= E(Im), E(IPO), E(Iref), E(Ipref )$, (4)\nwhere zm, zl, zr, and zl all have a dimension of Rh\u00d7w\u00d73; w and h represent the spatial dimensions of latent codes; H/h = W/w = f; f is a scale factor. In this way, the diffusion process can be conducted in a lower-dimensional latent space, which is more efficient requiring fewer computing resources. Given the above condition, we can model the learning of the landmark-to-image diffusion as follows:\n$Ll2i := Ez,\u03f5\u223cN(0,1),t [||\u03f5 \u2013 L2I(zt, t, Cl2i)||]$, (5)\nwhere L2I is our second hierarchy of landmark-to-image diffusion and Cl2i is the condition set {zm, zl, zr, zp }. All conditions are concatenated with the latent zt through the channel dimension and then fed into the second hierarchy of the landmark-to-image diffusion. After that, the spatial information from the the landmark image is explored by the intermediate self-attention layers of the landmark-to-image diffusion model. To this extent, we integrate all conditions into the diffusion network L2I to guide the talking head generation.\n3.4. Training and Inference\nTraining. The two hierarchy diffusions in our DreamHead can be trained simultaneously to facilitate the optimization process. Generally, we utilize the pretrained Mediapiple model [21] to extract dense facial landmarks from videos as shown in Figure 3. We train our A2L diffusion and L2I diffusion simultaneously with landmarks as supervision and condition, respectively. Although the second hierarchy of landmark-to-image diffusion is trained on the ground-truth landmarks, our predicted landmarks from A2L diffusion is sufficient to provide an accurate lip shape. This is because the dense facial landmark representation inherently contains redundant information, which facilitates an effective approximation of the lip's actual distribution.\nInference. As shown in Figure. 4, given an audio and a video sequence as inputs, we capture and store the pose matrix for the face in each frame. Afterward, we transform the landmarks into the canonical space by detecting poses. We then calculate the mean of the canonical landmarks in all frames, denoted as P, along with the variance of these landmarks. Subsequently, we employ the A2L diffusion process to predict temporal-consistency normalization of the canonical landmark sequences. Following this, we denormalize the predicted landmarks with the calculated mean and variance to restore the people's talking style. Then we transform the denormalized landmarks using the stored pose matrices to reconstruct the head motion. Lastly, we utilize the transformed landmarks as a condition to generate the final video, employing the L2I diffusion process on a frame-by-frame basis. It is notable that our method also accepts one image and audio as input. In this case, we will either randomly initialize a variance or use one from a random video and take the canonical landmarks of the given image as the mean landmarks P."}, {"title": "4. Experiments", "content": "In this section, we present quantitative and qualitative experiments to evaluate our proposed DreamHead. More information and experimental results are reported in Supplementary Material.\n4.1. Experimental Settings\nDatasets. We evaluate our DreamHead on two audio-driven talking head generation datasets, i.e., HDTF [43] and MEAD [38] dataset. For the HDTF dataset, we adopt the test set sampling strategy outlined in MODA [19]. For the MEAD dataset, we randomly select 80% videos as the training set and the remaining videos as the test set.\nEvaluation Metrics. Following MODA [19], we employ two key metrics for mouth accuracy assessment in our synthesized videos: Mouth Landmark Distance (LMD) and LMD velocity (LMD-v). We also compute Intersection-over-Union (IoU) to gauge overlap between predicted and ground truth mouth areas (MA). For audio-video synchronization, we rely on SyncNet's (Sync) confidence score [24]. Additionally, we use the Natural Image Quality Evaluator (NIQE) [23] as our image quality metric. To assess landmark quality, we utilize ErrorNorm to evaluate positional accuracy and measure jitter between consecutive landmarks to assess their smoothness.\n4.2. Implementation Details\nIn this work, we utilize Mediapipe\u00b9 to detect 478 3D facial landmarks for all videos. We resize the input image in the landmark-to-image diffusion process to 256 x 256, and set the downsampling factor f at 4. The length of the denoising step, T, is set to 1000 for both the audio-to-landmark and landmark-to-image diffusion processes. In the first hierarchy of the audio-to-landmark diffusion, the number of temporal blocks is set to 12. For the audio-to-landmark diffusion, we set 1 to 300 and 20 for the HDTF and MEAD datasets, respectively, to account for the longer video lengths in the HDTF dataset. We set the frame interval in the landmark-to-image diffusion to 20 to prevent the model from learning a shortcut with frames that are too close together.\n4.3. Comparison with state-of-the-art methods\nIn Table 1, we compare our methods with other SOTA subject-general methods. As shown in Table 1, our DreamHead can achieve the best performance on all datasets. Because of designing the diffusion model for effective learning of spatial-temporal correspondences, our method can produce the highest quality video with 6.53 NIQE on the HDTF dataset. Regarding the correctness of the audio-lip synchronization, our method also shows the best performance. i.e., DreamHead obtains the lowest LMD and LMD-v, and the highest MA scores on the two datasets. Because our portrait video is rendered by the condition of dense facial landmarks, DreamHead can synthesize accurate lip shapes. Since Wav2Lip [24] takes the score from the SyncNet [2] as supervision during their training, Wav2Lip and Wav2Lip-GAN present the highest Sync scores which are even higher than the ground-truth in MEAD dataset [38]. Our lower sync score is due to our adherence to the unique talking styles of test subjects, such as those who speak with almost closed mouths, which naturally results in lower sync scores even in real videos. However, our method still obtains a better sync score compared with most of compared methods. These results demonstrate that accurate facial landmarks can benefit the generation of lip-synced portrait videos using diffusion models. Additionally, we visualize some samples to demonstrate the performance of our DreamHead. As shown in Figure 6 and Figure 5, we can observe that our method can produce a more accurate and smooth portrait video, because we model the spatial correspondence between facial landmarks and real face images. Compared with Wav2Lip [24], our result maintains high fidelity because of the proposed hierarchical diffusion. These results verify that our method can effectively learn the spatial-temporal correspondence to boost the generation performance with the diffusion model.\nWe conduct user studies with 20 attendees on 40 videos generated by ours and the other methods. Each participant is asked to select the best generated talking-portrait videos based on two major aspects: lip synchronization accuracy and quality of the generated video. The statistics are reported in Table 2. Overall, users prefer our results on lip synchronization accuracy and quality. The results indicate the effectiveness of the proposed method.\n4.4. Ablation studies\nVisualization of estimated landmarks. Firstly, we demonstrate that our method can predict dense facial landmarks given the audio cues. As shown in Figure 1, our first hierarchy of audio-to-landmark diffusion can produce lip-synced and jittering-less landmark sequences, which can provide explicit spatial information of the lips during image generation. Furthermore, we also visualize the denoise process of the landmarks in Figure 7. By employing the landmark normalization technique, the initial landmark (t = 0) still maintains a rough facial shape after de-normalization. Finally, our audio-to-landmark diffusion produces a reliable landmark image after multiple denoising steps. It demonstrates the effectiveness of using our diffusion model to estimate facial landmarks.\nTemporal consistency in the diffusion model. Temporal consistency is a key challenge in the diffusion model. In this work, we compare the temporal consistency with other sota diffusion-based generic talking head methods, e.g. DiffTalk [28]. We utilize the temporal consistency metric to measure the frame-wise consistency (TCM [35]2) of the generated portrait video. As shown in Table 3, our method can produce more temporal-smooth results on two datasets compared with DiffTalk [28]. These results validate that using the landmark as an intermediate representation in our diffusion framework can constrain the temporal consistency of the generated video.\nDiffusion for landmark estimation. Compared with the traditional CNN network, our diffusion framework can produce dense facial landmarks with fewer jittering artifacts. To verify this, we use the audio-to-landmark network to directly predict the landmarks given the input of mean facial landmark and audio sequence (\u201cA2L w/o diffusion", "A2L w/o diffusion\". Notably, our method smooths out some of the outlier landmarks within the sequence, which results in an increased ErrorNorm. These outlier landmarks can potentially align with the ground truth due to potential errors in predicting the ground truth with a pre-trained model. Furthermore, we evaluate the effectiveness of the temporal unit (TU) and mapping unit (MU) in the audio-to-landmark diffusion. The results shown in Table 4 demonstrate that the temporal unit plays a critical role in maintaining the temporal consistency (i.e., 5.848 vs. 2.646 for Jitter on HDTF dataset). By integrating the temporal unit and the mapping unit, our first hierarchy of audio-to-landmark diffusion can estimate accurate landmarks with fewer jittering artifacts.\nThe components in A2L diffusion. We also evaluate the influence of the temporal unit and the mapping unit on the final generated portrait video. As shown in Table 5, the audio-to-landmark model produces poor results if without the temporal unit and mapping unit. We can observe that TU and MU mainly affect lip synchronization. The quality of the generated images of \"A2L w/o TU\" and \"A2L w/o MU\" is comparable to our full method (see NIQE values in Table 5), because our L2I diffusion can ensure high-quality results using landmarks. We also visualize some samples to verify the effectiveness of each component in A2L diffusion in Figure 8. As shown in Figure 8, the model with both the temporal and mapping units can achieve pose-smooth video results. Without the temporal unit, the mouth region changes dynamically. It verifies that our designed temporal and mapping units indeed contribute to the temporal consistency of our final generated videos.\nSpatial-correspondence condition in L2I diffusion. In this paper, we propose a hierarchical diffusion to learn the spatial-temporal correspondence to produce spatially consistent and temporally smooth portrait videos. We construct a spatial-correspondence conditions in L2I diffusion. There are four conditions in our L2I diffusion, including the target masked image Im, the target landmark IPO, the reference landmark Iref, and the reference image Iref. From Table 5, we can observe that adding the reference landmark as a condition can boost the performance via a comparison between \u201cL2I w/o IPref": "nd full method (4.44 vs. 4.63 on Sync score). With both Ipref and Iref, the diffusion model is more effective to learn the spatial correspondence between the facial appearance and the landmarks. Furthermore, the target masked image provides identity information for the generation. As shown in Figure 8, without Im, the identity of generated results is different from the ground truth (\u201cL2I w/o Ipref , Im\u201d vs. vs."}, {"title": "5. Conclusion", "content": "In this paper, we proposed a hierarchical diffusion framework, DreamHead, to learn spatial-temporal correspondence using diffused landmarks as intermediate representations. The first hierarchy of audio-to-landmark diffusion leverages the correlation between audio cues and facial landmarks to produce temporal-smooth landmark sequences. The second hierarchy of landmark-to-image diffusion learns the spatial correspondence between landmarks and face expressions. Extensive results clearly demonstrate that our DreamHead can estimate effective spatial-temporal correspondences between the input audio and the output face image, creating new state-of-the-art performances."}]}