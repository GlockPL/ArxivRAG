{"title": "Large Language Model-assisted Speech and Pointing Benefits Multiple 3D Object Selection in Virtual Reality", "authors": ["Junlong Chen", "Jens Grubert", "Per Ola Kristensson"], "abstract": "Selection of occluded objects is a challenging problem in virtual reality, even more so if multiple objects are involved. With the advent of new artificial intelligence technologies, we explore the possibility of leveraging large language models to assist multi-object selection tasks in virtual reality via a multimodal speech and raycast interaction technique. We validate the findings in a comparative user study (n=24), where participants selected target objects in a virtual reality scene with different levels of scene perplexity. The performance metrics and user experience metrics are compared against a mini-map based occluded object selection technique that serves as the baseline. Results indicate that the introduced technique, ASSISTVR, outperforms the baseline technique when there are multiple target objects. Contrary to the common belief for speech interfaces, ASSISTVR was able to outperform the baseline even when the target objects were difficult to reference verbally. This work demonstrates the viability and interaction potential of an intelligent multimodal interactive system powered by large laguage models. Based on the results, we discuss the implications for design of future intelligent multimodal interactive systems in immersive environments.", "sections": [{"title": "INTRODUCTION", "content": "Object selection in virtual reality is an important and widely researched task. Within this topic, there has been lots of interest on challenging sub-tasks, such as occluded object selection in virtual reality (VR). For occluded object selection alone, a myriad of tools have been developed to facilitate the task and improve user experience. These include a series of selection techniques developed by Yu et al. [56], as well as several works that followed which made improvements based on their original set of techniques to leverage gaze [55], gestures [40], or controllers [27] for occluded object selection and/or manipulation in virtual reality. Many more examples of works on occluded object selection in VR are reviewed in Section 2.1.\nThese works share many commonalities\u2014they rely on the pointing metaphor and often focus on a single interaction modality to perform single object selection. However, with the advent of generative artificial intelligence (GenAI) and large language models (LLMs), we can integrate intelligent conversational systems with traditional object selection techniques to develop a multimodal interaction technique for multi-object selection under occluded conditions. While multimodal interaction techniques have been studied in interactive augmented and virtual environments for decades [2, 36], LLMs promise to further enhance multimodal interaction, for example, by combining raycast and speech, as systems based on natural language can easily be integrated with LLMs. In an empirical study, we demonstrate that such a system yields certain advantages. Significant results indicate that by including speech-based interaction and AI models, we enhance the user's capa-bility of completing challenging occluded object selection tasks in VR, especially under the condition when there are multiple objects to be selected, and when the target objects are easy to reference verbally.\nThis paper bridges the research gap of efficient occluded multi-object selection techniques in VR. We leverage LLMs to complement traditional object selection techniques by proposing a conversation-based intelligent multimodal interactive system for the task of occluded object selection in VR. We introduce an Advanced Speech Support and Interactive System for Virtual Reality (ASSISTVR), which combines a speech-based with a raycast-based interaction technique to perform 3D object selection tasks. We evaluate ASSISTVR on occluded object selection tasks in VR, and compare its performance and user experience ratings with a baseline technique. According to Yu et al. [56], several techniques exist for occluded object selection in VR. Among these different classes of techniques for occluded object selection, we selected a minimap-based technique, DISCPIM [27], as the baseline for the object selection task in this study.\nThe aim of the user study is to investigate the following research questions:\n\u2022 RQ1: Compared with the baseline technique DISCPIM [27], how does ASSISTVR perform in terms of selection time and user experience when different numbers of target objects need to be selected?\n\u2022 RQ2: Compared with the baseline technique DISCPIM [27], how does ASSISTVR perform in terms of selection time and user"}, {"title": "RELATED WORK", "content": "Our work is embedded in the areas of occluded object selection in VR, multimodal interaction techniques as well as work on large language models, which we will contextualize next.\n2.1 Occluded Object Selection in VR\nObject selection and manipulation are considered as fundamental in-teractions in VR [1, 28, 35, 47]. They are often evaluated in testbed experiments [4], and many works have been dedicated to improve object selection [24, 41] and manipulation [59] by proposing novel interaction techniques. Besides the virtual hand and raycast techniques, which are commonly used as interaction metaphors for selection and manipula-tion [43], gestures [24] and eye gaze [41] are also widely adopted in more recent literature for object selection. Due to their high importance, these basic selection and manipulation tasks are widely studied in both virtual and augmented reality application scenarios [25, 33]. Selection can be considered as the first step in the sequential process of referenc-ing [8]. Following Wei\u00df et al. [51] and Sch\u00fcssel et al. [39], selection tasks provide a fundamental prerequisite for subsequent manipulation tasks, and findings on selection tasks will provide important implica-tions for the design of interaction techniques when they are applied in other scenarios for other tasks.\nIn terms of object selection tasks, we are interested in how our tech-nique performs under challenging selection scenarios, one of which being occluded object selection. Back in 2007, Vanacken et al. [46] highlighted the research gap in dense and occluded object selection in 3D virtual environments and proposed the depth ray and the 3D bub-ble cursor to address this gap. Yu et al. [54] studied the performance of different techniques in object selection under dense and occluded environments, and concluded that techniques enhanced with pointing facilitators can improve performance when there is no occlusion, which is not true when occlusions are present. Later in 2020, Yu et al. [56] de-veloped a set of seven techniques (ALPHA CURSOR, FLOWER CONE, GRAVITY ZONE, GRID WALL, LASSO GRID, MAGIC BALL, and SMASH PROBE) for fully-occluded target selection in VR, and stud-ied how factors such as the number of occlusion layers, target depths,"}, {"title": "Large Language Models", "content": "Recent advances in transformer-based large language models have contributed substantially to the way natural language input can be processed. These systems typically fine-tune pre-trained language models on domain specific datasets to perform intent and entity recognition for specific applications or achieve language generation for general purposes.\nIn 2019, Devlin et al. introduced BERT [15], Bidirectional Encoder Representations from Transformers, which has achieved state-of-the-art performance on sentence-level and token-level tasks, paving the founda-tion for several intent classification and entity recognition systems. For example, Chen et al. [11] adopted BERT for natural language under-standing (NLU) tasks such as joint intent classification and slot filling tasks. Jiang et al. [19] leveraged BERT models to perform intent clas-sification tasks based on speech input. BERT models are also widely adapted in named entity recognition tasks [7, 42]. In the examples above, BERT models offer several advantages in customizability and interpretability. Developers are able to fine-tune models on customized data, and the identified intents and entities provide clear intermediate steps in the natural language understanding process, which in turn gives developers and users better interpretability and more agency over the system.\nIn contrast to transformer-based NLP models like BERT, general-purpose LLMs like GPT-3 [6] are trained on a much larger amount of data. While this allows LLMs to generate human-like responses directly from input prompts, they bring about other problems such as lack of transparency [58] and limited customizability [10].\nBased on the above features of both types of LLMs, this paper does not adopt the state-of-the-art general-purpose LLMs to process user speech input as they are difficult to finetune and interpret, and may occasionally generate plausible but incorrect information. Instead, we"}, {"title": "ASSISTVR DESIGN CONCEPT AND IMPLEMENTATION", "content": "The proposed object selection technique, ASSISTVR, is a multimodal selection method which consists of a speech-based selection technique using the Microsoft Azure CLU Application Programming Interface (API), together with a raycast-based selection technique. In the remain-der of this section, we will introduce the implementation of the speech and raycast object selection modalities of ASSISTVR in further detail.\n3.1 Speech-based Selection\nThe speech-based object selection technique constitutes the main com-ponent of ASSISTVR. We used the workflow in Figure 2, separated into a training and a deployment phase, to implement it.\nIn the training phase, the first step is to select data and define intents and entities. Here, intent refers to the user's intent for each utterance. Each utterance in the training and test set is labeled with one intent only. Within each utterance, there might be several words or phrases which are of interest. Words or phrases that share similar characteristics can be grouped together as one entity, and each utterance can contain one, several, or zero entities. As the task focuses on object selection, we focused on the following three intents: 'Select', 'CancelAll', and 'None'. Entities used with the 'Select' intent include 'Original Color' and 'Original Shape', while the 'CancelAll' intent and the 'None' intent do not include any entities.\nThe second step is to create the training set by suggesting typical utterances for each of the above three intents and label all entities that appear in each utterance. For example, an utterance such as: 'Select the purple cube.' would belong to the 'Select' intent, with 'purple'"}, {"title": "Raycast-based Selection", "content": "As speech-based interfaces have the limitation of being prone to er-rors [44, 51] and usability tends to improve if speech-based interfaces are used in conjunction with traditional interfaces [2, 44], we comple-ment speech-based selection with a raycast technique to enable users to make fine-grained and precise selections/deselections.\nUsers are able to select/deselect objects hit by the ray which is cast from the right controller by pressing the trigger button. Selected objects are highlighted in green, while deselected objects are highlighted in red. When the ray moves away from a deselected object, the red highlight is removed. For selected objects, the green highlight is preserved even when the ray is directed away. As with objects selected using speech, objects selected using raycast also appear on the draggable panel with their names and green outlines."}, {"title": "STUDY DESIGN", "content": "The user study aims to evaluate the usability of our proposed speech and raycast multimodal 3D object selection technique (ASSISTVR) under different scene perplexity conditions and different numbers of target objects. Here, \"scene perplexity\" refers to whether or not the object category and object property are known to the user. A detailed explanation is provided in Section 4.1. Our method is evaluated against a state-of-the-art occluded object selection method, DISCPIM [27], which serves as the baseline. This section introduces the design of the user study, which involves selection of different numbers of target objects under different scene perplexity conditions. To facilitate com-parison with the baseline technique DISCPIM [27], several elements of the experiment setup (such as the red/green button in front of the user, the yellow search region, the black background, and the search space dimensions) follow the setup proposed by Maslych et al. [27]."}, {"title": "Design", "content": "We adopt a within-subjects design to evaluate the performance of AS-SISTVR and DISCPIM [27] in three levels of scene perplexity con-ditions (Low, MEDIUM, and HIGH), as well as three levels of target objects (1TARGET, 2TARGET, and 4TARGET). The scene perplexity is reflected by the number of object categories and object properties which can easily be referenced verbally. Examples of the LOW, MEDIUM, and HIGH scene perplexity conditions can be found in Figure 4. In the user study, there are eight different object categories (of which four were easily recognizable by users and four were more difficult) and eight different textures to reflect different object properties (four were easily recognizable and four were more difficult). There are considerable differences between the known and unknown object categories and textures - a study with 21 participants prior to the study provides statis-tical evidence for the categorization of known and unknown objects' shapes and colors. Details of the survey findings can be found in the online appendix. Before the study, the names of all objects and textures are briefed to the participant to simulate real-world use cases where users have access to object names when using a speech-based system.\nThe Low perplexity condition consists of four easily-identifiable object shapes (cube, sphere, cylinder, and pyramid) and four easily-identifiable colors (purple, blue, green, and red) The MEDIUM perplex-ity condition consists of two easily-identifiable object shapes (cube and sphere) and two object shapes which are more difficult to identify (barrel and pyramid cuboid), as well as two easily-identifiable colors (purple and blue) and two colors that are more difficult to identify (purple pattern and white pattern). The HIGH perplexity condition con-sisted of four object shapes which are difficult to identify (barrel, cross, pyramid cuboid, and truncated cylinder) and four colors which are dif-ficult to identify (purple pattern, white pattern, yellow pattern, and blue pattern). The order of the two techniques and the order of the perplexity conditions are both counterbalanced across all 24 participants.\nThe object density of distractor objects remains the same throughout all experiments. In each scene perplexity level, 120 distractor objects are scattered within the environment of 20 meters in depth, 10 meters in width, and 5 meters in height in front of the user. Including the varying number of 1, 2, or 4 target objects, the total number of objects is 121, 122, or 124, depending on the NUMTARGETS condition. The object"}, {"title": "Task", "content": "The task involves using the speech and raycast multimodal technique (ASSISTVR), or the baseline mini-map occluded object selection tech-nique DISCPIM [27] to select different numbers of target objects (1, 2, or 4 targets) among a set of selectable objects. At the beginning of each trial, the user directs both the left and right raycast at a red button in front of the user\u00b3. Upon pressing both triggers simultaneously to start the trial, the red button becomes green, the timer starts, and a home object\u2074 encapsulated within a semi-transparent sphere appears to the left of the user. At the same time, the target object(s), a yellow search region, together with 120 distractor objects appear within the 10m\u00d75m\u00d720m search space in front of the user. The shape and color of the target object(s) are randomly drawn from the set of 4 object shapes and 4 colors determined by the scene perplexity condition. Next, the participant uses the object selection technique to select a certain number of targets specified by the NUMTARGETS condition. If an error occurs, participants are allowed to deselect and select again. Finally, they press Button \"B\" on the right controller to confirm the selection. This stops the timer for trial completion time, and participants move on to the next trial if the selection is correct, that is, iff all target objects are selected and all selected objects are targets.\nThe time difference between starting the trial and users making the confirmation is recorded as the trial completion time. Following prior work [27, 56], users are asked to repeat the same selection under the exactly same conditions after completion of the first selection trial. In this paper, we refer to these two tasks as the search trial and the repeat trial.\nIn summary, the study for each participant consists of 18 combinations of independent variables (2 TECHNIQUES \u00d7 3 PERPLEXITIES \u00d7 3 NUMTARGETS). Participants complete the first half of the study with one technique, then complete the first half of the questionnaire, before moving on to the second technique and the second half of the questionnaire. Within each technique, participants complete trials un-der all three scene perplexities. Within each PERPLEXITY condition, participants complete all three NUMTARGETS conditions. Within each combination of TECHNIQUE, PERPLEXITY, and NUMTARGETS, the participant completes three sets of one search trial followed by one repeat trial. For the search trial in each set, the target object is randomly"}, {"title": "Hypotheses", "content": "We expect that the multimodal method will achieve similar performance regardless of the number of target objects to select, as speech commands should require a similar amount of time. Comparing the performance of our technique with the DISCPIM baseline, we expect that for a small number of target objects, the baseline method could achieve a better performance, whereas for a larger number of target objects, ASSISTVR could perform better. We formulate the following hypotheses with respect to the NUMTARGETS condition:\n\u2022 H1: Participants take less time to complete the search and repeat trials with AssSISTVR compared to using DISCPIM when there are many targets, but this may not hold true when there is only a limited number of targets.\n\u2022 H2: For ASSISTVR, the search and repeat trial completion time do not vary significantly when there are different numbers of target objects.\nFollowing prior work [3, 49, 57], we expect that the multimodal se-lection method will perform well under low scene perplexity conditions, but may not perform as well as conventional selection techniques when the user finds it difficult to name the object category and/or property under high scene perplexity conditions. Based on this assumption, we formulate our hypotheses with respect to the PERPLEXITY conditions as follows:\n\u2022 H3: Participants complete the search and repeat trial in less time with ASSISTVR under the LOW PERPLEXITY condition compared to using DISCPIM, but this may not hold true for the MEDIUM and HIGH PERPLEXITY conditions.\n\u2022 H4: For ASSISTVR, the search and repeat trial completion time is different under different PERPLEXITY conditions."}, {"title": "Participants and Apparatus", "content": "We recruited 24 participants (16 males and 8 females) aged between 18 and 33 (M = 24.3\u00b14.46). All participants were right-handed and had normal or corrected-to-normal vision. Around 50% of participants were familiar with head-mounted VR. All participants understood and spoke English, with around 58% native English speakers. None of the participants reported any known visual, auditory, or physical disability. During the experiment, participants wore an Oculus Quest 2 headset and held the left and right controllers. The headset was connected to a Windows 10 laptop PC (Intel i5-9300H CPU, 16GB memory, and GTX 1050 graphics card) via cable. Virtual scenes were implemented with"}, {"title": "Procedure", "content": "Before the study, participants were asked to review an information sheet and sign a consent form. After the collection of basic demographic information, participants were then asked to familiarize themselves with the Oculus Quest 2 headset and controllers. They were then given instructions on how to adjust the headstraps to comfortably put on the headset and learned how to use the controllers to perform selection commands using the two techniques. Next, we provided an overview of the study procedure and the tasks they were asked to complete using images and verbal introduction, which included explaining that the experiment will consist of using two techniques to perform object selection, and each technique consisted of three perplexity conditions, where each combination of TECHNIQUE and PERPLEXITY consisted of three sets of a search trial and a repeat trial.\nWe then demonstrated the usage of the two techniques to each participant. Participants had the opportunity to ask questions as we demonstrated how to use both controllers to press the start button and how to perform selection tasks using the two techniques. After answering all questions, participants had the opportunity to practice completing the trials with the two techniques under the MEDIUM PERPLEXITY condition.\nAfter familiarization, participants completed three sets of search and repeat trials for each perplexity level and each number of targets using one of the two techniques, before having a five-minute break and completing another three sets for each perplexity level and each number of targets with the other technique. Throughout the experiment, the sequence of different perplexities and techniques were counterbalanced for all participants. In each trial, participants completed a search task followed by a repeat task. Specifically, they pointed the left and right raycast at the 'Start' button and pressed the left and right trigger simultaneously to start a 3-second countdown to start the search task. After the countdown, the target object drawn from different combinations of the set of object categories and object textures were generated at random locations within the 3D search space in front of the user. A yellow region indicates the approximate location of the target object. Users either use ASSISTVR, a combination of speech and raycast techniques to select the target object, or use DISCPIM [27] to generate a mini-map with the left controller, then use the right controller grip button to select the object directly from the mini-map, or from a list of expanded objects along the mini-map circumference if objects overlap in the mini-map. If the selection is correct, the timer will stop and the scene will reset. Users will then need to trigger the 'Start' button again to begin the repeat task. If the selection is incorrect, the system will play a tone to prompt the user to try again, and the total number of attempts will be recorded. In the subsequent repeat task, the procedure is the same, except that the object positions are exactly the same as in the search task, which are no longer randomly generated. After the search and repeat tasks, a new target object is drawn and placed at a new position, but the distractor objects remain at the same position."}, {"title": "RESULTS", "content": "Statistical significance tests on trial completion time were carried out using a repeated measures analysis of variance (RM-ANOVA) with Holm-Bonferroni adjustments for the post-hoc tests. Task load, sys-tem usability, and user experience ratings were analyzed with non-parametric Wilcoxon signed-rank tests.\n5.1 Trial Completion Time\nDuring the study, we recorded the trial completion time as an indicator for object selection performance for both search and repeat tasks under all combinations of TECHNIQUE, PERPLEXITY, and NUMTARGETS conditions as a quantitative measure of user performance in the object selection task. In total, 2592 data points were collected (24 participants \u00d7 2 TECHNIQUES \u00d7 3 PERPLEXITIES \u00d7 3 NUMTARGETS \u00d7 2 tasks \u00d7 3 repetitions). In line with prior work [27], we removed 32 outlier data points (1.23%) where the trial completion time was more than 4 standard deviations away from the mean in each condition. We did not discard trials which took participants more than one attempt to complete.\nAs each participant is exposed to all conditions, a repeated-measures ANOVA (RM-ANOVA) test was conducted on both the search and repeat trial completion time data to determine whether significant differences existed in trial completion time across different conditions.\nTable 1 presents the RM-ANOVA results on search and repeat trial completion time for the independent variables TECHNIQUE, NUMTAR-GETS, and PERPLEXITY, together with interaction terms.\n5.1.1 Main Effect of TECHNIQUE\nRM-ANOVA tests revealed a significant main effect of TECHNIQUE on the search ($F_{1,23}$ = 28.765, $\\eta^{2}$ = .556, p < .001) and repeat ($F_{1,23}$ = 47.329, $\\eta^{2}$ = .673, p < .001) trial completion time. Figure 5 presents the average search and repeat trial completion time for ASSISTVR and DISCPIM [27] of all PERPLEXITY and NUMTARGETS conditions across all 24 participants. Post-hoc tests with Bonferroni adjustment suggested that participants took significantly less time (p < .001) to complete the search task using ASSISTVR (M = 16.9, SD = 9.79) as opposed to using DISCPIM (M = 22.1, SD = 16.3). For the repeat task, participants also took significantly less time (p < .001) with ASSISTVR (M = 10.1, SD = 5.73) compared with using DISCPIM (M=14.3, SD = 9.33). Here, results for the main effect of TECHNIQUE are averaged over the levels of NUMTARGETS and PERPLEXITY.\n5.1.2 Main Effect of NUMTARGETS\nRM-ANOVA tests revealed a significant main effect of NUMTARGETS on the search ($F_{2,46}$ = 82.348, $\\eta^{2}$ = .782, p < .001) and repeat ($F_{2,46}$ = 147.450, $\\eta^{2}$ = .865, p < .001) trial completion time. Post-hoc tests with Bonferroni adjustment revealed that for the search"}, {"title": "Interaction Effect of TECHNIQUE \u00d7 PERPLEXITY", "content": "Figure 7 presents bar plots of the search and repeat trial completion time for different combinations of TECHNIQUE and PERPLEXITY conditions. RM-ANOVA tests did not reveal a significant interaction effect of TECHNIQUE \u00d7 PERPLEXITY on either the search ($F_{2,46}$ = .109, $\\eta^{2}$ = .005,p = .897) or repeat ($F_{2,46}$ = 1.784, $\\eta^{2}$ = .072,p = .179) trial completion time.\n5.1.6 Interaction Effect of TECHNIQUE X NUMTARGETS \u00d7 PER-PLEXITY\nRM-ANOVA tests did not reveal a significant interaction effect of TECHNIQUE \u00d7 NUMTARGETS \u00d7 PERPLEXITY on either the search ($F_{4,92}$ = .549, $\\eta^{2}$ = .023,p = .700) or repeat ($F_{4,92}$ = .453, $\\eta^{2}$ = .019, p.770) trial completion time."}, {"title": "Task Load", "content": "Figure 8 (left) shows a bar plot of the NASA-TLX ratings (unweighted version) [18] for each category as well as the overall load with 95% confidence intervals of the mean score. A Wilcoxon signed rank test revealed that the overall task load rating of ASSISTVR (M = 4.06, SD = 1.71) was significantly lower (W = 59.5, p < .05, r= .461) than that of DISCPIM (M = 4.94, SD = 1.72). Within subcat-egory ratings, PHYSICAL load was found to be significantly lower (W = 55.5, p < .05, r = .491) for ASSISTVR (M = 2.96, SD = 2.12) compared to DISCPIM (M = 4.50, SD = 2.17). The TEMPORAL load of ASSISTVR (M = 3.33, SD = 1.81) was significantly lower (W = 2.0, p < .05, r = .828) than DISCPIM (M = 4.62, SD = 2.32), and the EFFORT rating of ASSISTVR (M = 4.46, SD = 2.47) was also significantly lower (W = 37.5, p < .05, r = .593) than DISCPIM (M = 5.96, SD = 2.31). Results are summarized in Table 2."}, {"title": "System Usability", "content": "Figure 8 (right) presents a bar plot of the system usability scale [5] of the ASSISTVR and DISCPIM technique with 95% confidence intervals of the mean estimate. A Wilcoxon signed rank test did not reveal any significant differences (W = 157, p = .853, r = -.038) between the SUS ratings of ASSISTVR (M = 71.0, SD = 14.4) and DISCPIM (M = 68.0, SD = 23.0). Results are summarized in Table 2."}, {"title": "User Experience", "content": "Figure 9 shows the results from the short version User Experience Questionnaire (UEQ-S) [38], where DISCPIM attains a higher aver-age hedonic quality score and ASSISTVR attains a higher average pragmatic quality score. Wilcoxon signed rank tests did not reveal a sig-nificant difference (W = 106.5, p = .346, r = .197) in the overall UEQ-S score between ASSISTVR (M = .547, SD = 1.23) and DISCPIM (M = .880, SD = 1.07). For the subcategories of the UEQ-S ratings, significant differences (W = 64.5, p < .05, r = .496) were found in the HEDONIC quality between ASSISTVR (M = -0.021, SD = 1.61) and DISCPIM (M = .958, SD = 1.16), but not in the PRAGMATIC quality (W = 176.5, p = .457, r = -.152) between ASSISTVR (M = 1.11, SD = 1.21) and DISCPIM (M = .802, SD = 1.37). Results are summarized in Table 2."}, {"title": "Overall Preference and Open Comments", "content": "In the post-experience questionnaire, we asked participants about their overall preference among the two techniques and invited them to leave comments about features they liked/disliked. Among all 24 partici-pants, 13 preferred the ASSISTVR technique, while 11 preferred the DISCPIM technique.\nFor the ASSISTVR technique, participants liked the fact that it was easy to use (P7), efficient (P10, P20, P21, P22), and allowed 'selecting multiple objects in one go' (P2, P14), and participants could select objects without knowing where the object is (P13, P14, P15, P23), or moving their hands to execute any action (P12). P19 also found that 'the combination of speech and raycast stroke a nice balance', as raycast was more efficient for selecting one or two visible objects and voice selection helped to select multiple objects. Participants disliked the fact that it 'did not support many commands' (P5). Further, speech recognition sometimes failed and the command was not executed cor-rectly (P1, P2, P7, P11, P13, P21, P24). Consequently, the system 'either doesn't select anything or selects wrong objects' (P19), which led to frustration (P7) and loss of trust (P19). Specifically, P8, P12, P21, P22 commented that speech recognition was sensitive to accent, without the capability to auto-correct recognized speech based on the context (P22), which repeatedly led to errors. Some participants found it somewhat difficult to remember object names (P12, P13, P14, P19). Sometimes participants had to repeat several times before getting the speech command right (P5, P23). P20 also commented that the time"}, {"title": "DISCUSSION", "content": "This paper contributes to fill a gap in the literature addressing the chal-lenge of occluded multi-object selection tasks in VR by developing and studying multimodal LLM-enabled interaction. Based on existing techniques leveraging ray-based metaphors [14, 16, 23, 27, 29, 52, 53, 56], gestures [40], and eye gaze [9, 41] for object selection in VR, together with works studying speech interaction [21, 26, 51], multimodal inter-action [20, 34, 36] in immersive technologies, as well as customizable purpose-built LLMs [10], we have advanced the research community's understanding of the design of intelligent multimodal interactive sys-tems for object selection in VR by proposing ASSISTVR and validating user performance and experience by comparing it against a baseline technique, DISCPIM, in an empirical user study.\nKey findings from the study show that users were able to select objects in VR faster with ASSISTVR when there were multiple objects, and the object perplexity (whether or not objects were difficult to reference verbally) did not compromise the high selection efficiency of ASSISTVR as long as participants had access to the names of all target objects. Comparing different conditions within ASSISTVR, we found that the number of targets and object perplexity both affected the selection completion time to a certain extent. Results indicate that the speech and raycast multimodal selection technique ASSISTVR posed a significantly lower overall load on users and provide a similar level of user experience quality compared with the baseline.\nFor the independent variable NUMTARGETS, while DISCPIM re-quired significantly less search time in the 1 TARGET condition, As-SISTVR required significantly less time in the 4TARGETS condition in the search trial, and significantly less time in the 2TARGETS and 4TARGETS conditions in the repeat trial, which provides evidence to support H1. No significant differences were found in the ASSISTVR search trial completion time between all pairwise comparisons of NUM-TARGETS conditions. However, significant differences were found in the ASSISTVR repeat trial completion time between the 1TARGET and 2TARGETS as well as the 1TARGET and 4TARGETS condition. Therefore, we are unable to determine the validity of H2 based on the quantitative data from the performance study.\nIn terms of the PERPLEXITY condition, performance results reveal that compared with using DISCPIM, participants required significantly less time to complete the repeat task with ASSISTVR under the Low, MEDIUM, and HIGH perplexity conditions. Therefore, we find that ASSISTVR outperforms the expectation listed in H3, given the assump-tion that participants know how to reference complex objects and have instant access to their names when they forget. As results indicated a significant difference in search completion time between the Low and HIGH perplexity condition using ASSISTVR, as well as a signifi-"}, {"title": "Limitations and Future Work", "content": "ASSISTVR is based on a natural language processing model, which poses requirements on the quality of custom utterance data. The quality of data input by the developer directly affects the performance of the model. In this study, as only three user intents are represented (SELECT, CANCELALL, and NONE), the model is able to achieve a high accuracy on intent classification and entity recognition. However, for more complex tasks which involve more intents and entities, we expect that errors may exist in intent classification and entity recognition, which poses a limitation in generalizability.\nFurther, the nature of NLP models determine that the developer can only pre-define a limited number of intents and entities, and it is highly likely that when the system is deployed among a large number of users, these pre-defined intents and entities may not handle all user inputs properly. For example, in the selection task, the command 'Deselect the red sphere' would be categorized under the intent 'DeselectAll', even if the user only intends to deselect certain objects.\nIn addition, ASSISTVR had limited visibility, which resulted in a poor sense of user agency. Compared with DISCPIM where instant visual feedback was present, ASSISTVR only revealed a list of all selected objects. Users were unable to use the raycast method to parse through elements in the scene and inspect their properties in detail.\nFinally, some"}]}