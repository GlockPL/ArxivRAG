{"title": "HYDEN: Hyperbolic Density Representations for Medical Images and Reports", "authors": ["Zhi Qiao", "Linbin Han", "Xiantong Zhen", "Jia-Hong Gao", "Zhen Qian"], "abstract": "In light of the inherent entailment relations between images and text, hyperbolic point vector embeddings, leveraging the hierarchical modeling advantages of hyperbolic space, have been utilized for visual semantic representation learning. However, point vector embedding approaches fail to address the issue of semantic uncertainty, where an image may have multiple interpretations, and text may refer to different images, a phenomenon particularly prevalent in the medical domain. Therefor, we propose HYDEN, a novel hyperbolic density embedding based image-text representation learning approach tailored for specific medical domain data. This method integrates text-aware local features alongside global features from images, mapping image-text features to density features in hyperbolic space via using hyperbolic pseudo-Gaussian distributions. An encapsulation loss function is employed to model the partial order relations between image-text density distributions. Experimental results demonstrate the interpretability of our approach and its superior performance compared to the baseline methods across various zero-shot tasks and different datasets.", "sections": [{"title": "1 Introduction", "content": "In recent years, cross-modal text-image representation learning has achieved tremendous success and drawn widespread attention in many tasks such as zero-shot learning and image-text retrieval. This success is largely due to the use of large volumes of weakly-supervised image-text pair data to enhance vision-language representation learning [Radford et al., 2021]. In the field of medical imaging, cross-modal representation learning tailored to specific domain data, such as chest radiographs and their associated radiology reports, can yield robust and powerful foundation models in specialized areas [Zhang and Metaxas, 2023].\nAs the proverb goes, 'A picture is worth a thousand words.' This suggests that an image inherently contains more information than a textual description of it, which can be seen as merely a simplified abbreviation of the image. This relationship, where the text may serve as an entailment of the image, can be considered as visual-semantic hierarchy [Vendrov et al., 2016]. Consequently, it is a plausible hypothesis that incorporating such inductive biases of visual semantic hierarchies into cross-modal alignment tasks could enhance the generalizability of representations and improve the interpretability of learning representations. Vendrov et al. [2016] introduced an order embedding strategy considering these hierarchical semantic during the text-image alignment process. However, numerous studies [Nickel and Kiela, 2017, 2018, Xu et al., 2022, 2023, Fu et al., 2023] have demonstrated that modeling data with inherent hierarchical features in non-Euclidean hyperbolic spaces can provide superior representations. By leveraging the advantages of hyperbolic space in modeling hierarchical structures and the generalization capabilities of cross-modal contrastive learning in zero-shot scenarios, Desai et al. [2023] has proposed cross-modal hyperbolic representation learning. This approach employs\nthe Lorentz manifold to map both image and text features into hyperbolic space, utilizing angular constraints based on entailment to learn the hierarchical order between text and images.\nHowever, representing image-text with point vectors has a clear limitation: it cannot express semantic uncertainty [Vilnis and McCallum, 2014, Qian et al., 2021], meaning that a single image can generate different descriptions from various perspectives, and similarly, a single textual description can describe different but related images. This phenomenon is particularly evident in medical imaging and radiology reports. For instance, as depicted in Fig.1(a), consider a patient with a rib fracture suspected of having right-sided pneumothorax. In the radiology report for this patient, the physician describes the imaging findings related to pneumothorax, highlighting the presence of a white line around the visceral pleural edge. Clinically, numerous pulmonary diseases, such as tuberculosis, cystic fibrosis, and pneumocystis jiroveci pneumonia, predispose individuals to pneumothorax. In domains such as document embedding [Zhu et al., 2023] and graph embedding [Gourru et al., 2022], the utilization of probability density embedding to represent objects as distributions within the target space effectively addresses this semantic uncertainty, resulting in significantly improved performance compared to point vector embedding.\nBuilding on the motivation outlined above, which focuses on the hierarchical visual semantic features and inherent semantic uncertainty in medical imaging, we propose HYDEN, a hyperbolic density representations for medical images and reports. This approach leverages the advantages of hyperbolic space for capturing visual-semantic hierarchy, while incorporating a probability density embedding strategy to model semantic uncertainty. The main contributions are as follows:\n\u2022 To the best of our knowledge, this is the first model to apply cross-modal representation learning to medical image-text data within hyperbolic space.\n\u2022 We introduce a text-aware image local feature extraction method that focuses on local regions, enhancing the granularity of analysis; moreover, we employ encapsulation constraints to model the density order between images and text, fostering a deeper semantic connection.\n\u2022 Extensive experiments were conducted to validate the performance of our algorithm against baseline models through both quantitative and qualitative analyses. These experiments demonstrate the superior capabilities of our approach in achieving semantic alignment."}, {"title": "2 Related Work", "content": "Image-text representation learning has garnered substantial interest due to its potential to enhance visual representation. Traditional methods predominantly employ contrastive metric learning ap- proaches, with CLIP [Radford et al., 2021] being a notable example that has demonstrated remarkable results. These methods typically operate in the Euclidean space and have been extensively applied across various general domains.\nHowever, the medical field presents unique challenges due to the domain-specific nature and the complex prior knowledge embedded in medical image-text data. In response, several studies have explored image-text representation learning specifically tailored to medical contexts [Wang et al., 2024, M\u00fcller et al., 2022a, Cheng et al., 2023a, Huang et al., 2021]. Despite their advancements, these approaches continue to operate within the confines of Euclidean space. The inherent hierarchical semantics between images and texts, particularly pronounced in medical datasets, suggest that hyperbolic space could offer significant advantages. Hyperbolic space naturally accommodates hierarchical data structures, making it a compelling alternative for modeling complex semantic relationships. Building on this premise, the MERU framework introduced hyperbolic image-text embedding [Desai et al., 2023], representing a significant departure from traditional Euclidean methods.\nBuilding upon the concept introduced by MERU, our work proposes a novel approach by integrating density embedding to capture semantic uncertainty, which is a feature not adequately addressed by point vector embeddings. While density embedding has been previously utilized for capturing the uncertainty of semantics and modeling asymmetric relationships like entailment [Vilnis and McCallum, 2014, Qian et al., 2021, Bojchevski and G\u00fcnnemann, 2018], these implementations have been confined to Euclidean space. Our method extends this concept into hyperbolic space."}, {"title": "3 Preliminaries", "content": "Hyperbolic Geometry Hyperbolic geometry is a non-Euclidean geometry with a constant negative curvature, and it can be visualized as the forward sheet of the two-sheeted hyperboloid. In this study, we will use the Lorentz model on the upper half of a two-sheeted hyperboloid, as claimed in [Nickel and Kiela, 2018], comes with a simpler closed form of the geodesics and does not suffer from the numerical instabilities in approximating the distance. Lorentz model Hn processing a constant curvature -c can be represented as a set of points $z \\in \\mathbb{R}^{n+1}$. Lets $z, z' \\in \\mathbb{H}^n$, the Lorentzian product $(z, z')_{\\mathcal{L}} = -z_0 z'_0 + \\sum_{i=1}^n z_i z'_i$. And, $\\mathbb{H}^n = \\{ z \\in \\mathbb{R}^{n+1} : (z, z')_{\\mathcal{L}} = -1/c, c > 0\\}$. The distance between z and z' is given by\n$d_{\\mathcal{L}}(z, z') = \\text{arccosh}(-(z, z'))$ (1)\nwhich is also the length of the geodesic that connects z and z'. We will refer to the one-hot vector $\\mu_0 = [1/\\sqrt{c}, 0, 0, 0...0] \\in \\mathbb{H}^n \\subset \\mathbb{R}^{n+1}$ as the origin of the hyperbolic space.\nTangent Space of Hyperbolic Space The tangent space at a point $\\mu \\in \\mathbb{H}^n$ is a Euclidean space composed of vectors. Denoted by $T_{\\mu}\\mathbb{H}^n$, this tangent space represents the set of vectors in the same ambient space $\\mathbb{R}^{n+1}$ where $\\mathbb{H}^n$ is embedded. The vectors in $T_{\\mu}\\mathbb{H}^n$ satisfy an orthogonality condition relative to the Lorentzian product, defined as $T_{\\mu}\\mathbb{H}^n := \\{ u : (\\mu, u)_{\\mathcal{L}} = 0 \\}$. This set can be visualized as the tangent space at the point $\\mu$ on the forward hyperboloid sheet. Specifically, at the origin $\\mu_0$ of $\\mathbb{H}^n$, the tangent space $T_{\\mu_0}\\mathbb{H}^n$ consists of vectors $v \\in \\mathbb{R}^{n+1}$. The norm $\\|v\\|_{\\mathcal{L}}$, given by the Lorentzian inner product, simplifies to the Euclidean norm $\\|v\\|_2$, defined as $\\|v\\|_{\\mathcal{L}} := \\sqrt{(v, v)_{\\mathcal{L}}} = \\|v\\|_2$.\nExponential Map\nThe exponential map provides a method for mapping a vector from a tangent space to its corresponding point on the surface of the hyperbolic space. For every $u \\in T_{\\mu}\\mathbb{H}^n$, the exponential map $\\exp_{\\mu}(u) : T_{\\mu}\\mathbb{H}^n \\rightarrow \\mathbb{H}^n$ allows us to project a vector $u$ in $T_{\\mu}\\mathbb{H}^n$ onto $\\mathbb{H}^n$ such that the distance from $\\mu$ to the destination point of the map coincides with the Lorentzian norm $\\|u\\|_{\\mathcal{L}}$ of $u$. In the context of hyperbolic space, the exponential map is given by the equation:\n$z = \\exp_{\\mu}(u) = \\cosh(\\|u\\|_{\\mathcal{L}}) \\mu + \\sinh(\\|u\\|_{\\mathcal{L}}) \\frac{u}{\\|u\\|_{\\mathcal{L}}}$ (2)\nIn this paper, we specifically consider exponential maps where $\\mu$ represents the origin of the hyper- boloid ($O = [\\sqrt{1/c}, 0]$)."}, {"title": "4 Method", "content": "In this section, we present a comprehensive introduction to the HYDEN model. Drawing on the foundation laid by the MERU model [Desai et al., 2023] and the widely acclaimed, user-friendly CLIP\nframework [Radford et al., 2021], our model adapts and extends these frameworks to address specific challenges in medical image-text representation learning. Figure 2 depicts the overall architecture of our model. Distinct from CLIP and MERU, our HYDEN model incorporates several innovative features designed to enhance its functionality and applicability in medical contexts: (1) Text-aware Local Image Representation: We refine the image analysis process by integrating text-aware local features that allow for a more nuanced understanding of medical images. This approach helps to align specific image regions with relevant textual descriptions, enhancing the model's ability to handle complex medical datasets. (2)Hyperbolic Density Embedding: Utilizing the properties of hyperbolic space, we introduce hyperbolic density embedding to generate and transfer probability density features from Euclidean to hyperbolic space. This method leverages the natural hierarchical structure of hyperbolic space to more effectively represent the intrinsic complexities of medical data. (3)Loss Function for Hyperbolic Density Embedding: To complement our embedding technique, we have developed a specialized loss function that focuses on the encapsulation relationships within the partial order of image-text semantic distributions. This loss function is tailored to strengthen the correlation between images and texts by capturing their hierarchical semantic relationships."}, {"title": "4.1 Image-Text Feature Embedding", "content": "In our model, the features $[f_u, f_t]$ are derived from respective image and text encoders. For text data, we employ BioClinicalBERT [Alsentzer et al., 2019a], a model that has been pre-trained on the MIMIC III dataset [Shen et al., 2016], to generate token-level embeddings. Consistent with practices outlined in [Cheng et al., 2023b], the output of the [CLS] token is used as the medical text feature $f_t$, encapsulating the overall semantic content of the input text.\nFor image encoding, we utilize the widely-used Vision Transformer (ViT) architecture [Mu et al., 2022]. We assume that $f_u = \\{ f_0, f_1, ..., f_n \\}$ captures the outputs from the image encoder. Recog- nizing that pathological symptoms often occupy only a portion of a medical image, relying solely on global representations may not adequately capture essential local semantic features. Thus, similar to approaches in [Huang et al., 2021, Cheng et al., 2023b, M\u00fcller et al., 2022b], we enhance the global features by integrating text-aware image local representations.\nSpecifically, we implement a Self-attention module [Vaswani et al., 2017], widely used in cross- modal feature extraction. In this setup, $f_u$ acts as both the keys (K) and values (V), while the text embedding $f_t$ functions as the query (Q). This configuration allows us to derive a text-aware image"}, {"title": "4.2 Hyperbolic Density Embedding", "content": "Our objective is to transform image-text features into density representations within hyperbolic space. Previous studies such as [Nagano et al., 2019] and [Mathieu et al., 2019] proposed methods like the pseudo-hyperbolic Gaussian distribution based on the Lorentz manifold. Due to the computational demands and numerical instabilities of the Poincar\u00e9-disk model, we opt for the more stable pseudo- hyperbolic Gaussian distribution for our hyperbolic density embedding. The tangent space $T_{\\mu}\\mathbb{H}^n$ of hyperbolic space $\\mathbb{H}^n$ is a Euclidean space, and in $T_{\\mu_0}\\mathbb{H}^n$, vectors $v$ satisfy $v = \\{ v_0, v_1, ..., v_n \\} \\in \\mathbb{R}^{n+1}$ where $v_0 = 0$, aligning with the dimensional properties.\nTo begin, we introduce separate deep nonlinear network blocks, $\\mathcal{B}_{density}$, for processing image and text features independently. These blocks do not share parameters, ensuring distinct representations for each modality. As in Figure 2, for text features, $\\mu_t$ and $\\beta_t$ are the outputs of $\\mathcal{B}_{density}(f_t)$.\nInstead of generating covariance matrices directly, which can introduce numerical instability, we use matrices based on diagonal or spherical assumptions. These are known for their computational efficiency and effectiveness in embedding tasks, particularly in the context of word distribution embedding where spherical covariance matrices have been shown to better model distributional partial order relationships [Vilnis and McCallum, 2014]. We thus employ a covariance matrix based on the spherical assumption: $\\Sigma_t = \\beta_t \\cdot I \\in \\mathbb{R}^{(n+1)\\times (n+1)}$.\nTo ensure that our covariance matrix is positively definite, necessary for the stability of the pseudo- hyperbolic Gaussian distribution, we modify $\\beta_t$ using the expression $\\beta_t = \\exp(\\beta_t)$ referring to solution in VAE[Kingma and Welling, 2019]. This adjustment is crucial for maintaining the mathe- matical integrity of our model when dealing with real-world data. For the embedding vector $\\mu_t \\in \\mathbb{R}^n$, our aim is to project this vector onto hyperboloid space, which is achieved by mapping it through the exponential function as detailed in Equation 2.\nThe vector $\\mu_t^{tan} = [0, \\mu_t]$ resides in $\\mathbb{R}^{n+1}$ and belongs to the tangent space $T_{\\mu_0}\\mathbb{H}^n$ at the origin of the hyperboloid, O. The norm $\\|\\mu_t^{tan}\\|_{\\mathcal{L}}$, which equals $\\|\\mu_t\\|_2$, ensures that the mapping preserves the distances inherent to the model's geometric structure. We apply the exponential map to $\\mu_t^{tan}$, decomposing the transformation into two parts:\n$\\cosh(\\sqrt{c}\\|\\mu_t^{tan}\\|_{\\mathcal{L}}) \\mu_0 = [\\sqrt{1/c} \\times \\cosh(\\sqrt{c}\\|\\mu_t^{tan}\\|_{\\mathcal{L}}), 0] = [\\sqrt{1/c} \\times \\cosh(\\sqrt{c}\\|\\mu_t\\|_2), 0]$ (3)\n$\\frac{\\sinh(\\sqrt{c}\\|\\mu_t^{tan}\\|_{\\mathcal{L}})}{\\sqrt{c}\\|\\mu_t^{tan}\\|_{\\mathcal{L}}} \\mu_t^{emb} = [0, \\frac{\\sinh(\\sqrt{c}\\|\\mu_t^{tan}\\|_{\\mathcal{L}})}{\\sqrt{c}\\|\\mu_t^{tan}\\|_{\\mathcal{L}}} \\mu_t^{emb}] = [0, \\frac{\\sinh(\\sqrt{c}\\|\\mu_t\\|_2)}{\\sqrt{c}\\|\\mu_t\\|_2} \\mu_t^{emb}]$ (4)\nUpon applying the exponential map, we derive the expectation of the hyperbolic density representa- tion:\n$\\mu_t = \\exp_{\\mu_0} (\\mu_t^{tan}) = [\\sqrt{1/c} \\times \\cosh(\\sqrt{c}\\|\\mu_t\\|_2), \\frac{\\sinh(\\sqrt{c}\\|\\mu_t\\|_2)}{\\sqrt{c}\\|\\mu_t\\|_2} \\mu_t^{emb}]$ (5)\nThis projection results in the hyperbolic density representation $\\mathcal{G}_t (\\mu_t, \\beta_t \\cdot I)$. Following a similar procedure, we also derive $\\mathcal{G}_u (\\mu_u, \\beta_u \\cdot I)$ for the image features, thereby ensuring a uniform approach to handling different modalities within our framework."}, {"title": "4.3 Loss Function Based on Density Embedding", "content": "Traditional point vector embedding often utilizes entailment angle constraints to define relationships between entities [Desai et al., 2023]. However, when dealing with probability densities, the notion of partial order can be more complexly captured through the concept of encapsulation. Specifically, a density f is considered more specific than another density g if f is entirely encompassed by g, formally expressed as $f \\prec g \\Leftrightarrow \\{ x : f(x) > \\eta \\} \\subseteq \\{ x : g(x) > \\eta \\}$, for any $\\eta \\geq 0$, where $\\eta$ indicates the degree of encapsulation necessary for one distribution to entail another.\nImposing such partial order constraints on distributions poses significant challenges. Drawing inspiration from Athiwaratkun and Wilson [2018], we employ asymmetric divergence measures between probability densities to address this. We introduce a simple penalty function, $d_\\gamma (f, g) = \\text{max}(0, D(f \\| g) - \\gamma)$, which serves as a violation penalty rather than as a strict constraint of encapsulation. Here, $D(\\cdot \\| \\cdot)$ represents the divergence measure used to quantify the extent of difference between distributions, and $\\gamma$ is a threshold defining the acceptable range of difference.\nAmong the choices for divergence measures, $\\alpha$-divergence provides a more flexible and generalized asymmetric measure [Renyi, 1961], allowing for adjustments in the zero-force penalty. This flexibility means that higher $\\alpha$ values can enforce stricter encapsulation conditions $f \\preceq g$. The general form of $\\alpha$-divergence, for $\\alpha \\neq 0, 1$, is given by:\n$D_{\\alpha}(f \\| g) = \\frac{1}{\\alpha(\\alpha - 1)} \\log \\bigg( \\int \\frac{f(x)^\\alpha}{g(x)^{\\alpha - 1}} dx \\bigg)$ (6)\nThis equation not only quantifies the differences between distributions but also facilitates a deeper understanding of the encapsulation relationships critical for effective density embedding.\nWe observe that as $\\alpha$ approaches 0 or 1, it governs the degree of zero forcing, where minimizing $D_{\\alpha}(f \\| g)$ for high $\\alpha$ values results in f becoming more concentrated in regions of g with high density. Conversely, for low $\\alpha$ values, f tends to be mass-covering, encompassing regions of g even including those with low density. Notably, there exists a mathematical relationship between KL divergence and $\\alpha$-divergence, as indicated by: $\\lim_{\\alpha \\rightarrow 1} D_{\\alpha}(f \\| g) = D_{KL}(f \\| g)$ and $\\lim_{\\alpha \\rightarrow 0} D_{\\alpha}(f \\| g) = D_{KL}(g \\| f)$ [Pardo, 2006]. Therefore, in our model, we opt for the more flexible and robust $\\alpha$-divergence as our metric.\nFor image-text embedded density $\\mathcal{G}_u (\\mu_u, \\beta_u \\cdot I)$ and $\\mathcal{G}_t (\\mu_t, \\beta_t \\cdot I)$, the encapsulation loss can be expressed as follows:\n$d_\\gamma(\\mathcal{G}_u, \\mathcal{G}_t) = \\text{max}(0, \\frac{1}{2\\alpha(\\alpha - 1)} \\log \\bigg[ \\frac{\\beta_u^{\\alpha(n+1)}}{\\beta_t^{(1-\\alpha)(n+1)}} \\bigg] + \\frac{\\beta_t^{-(n+1)}}{\\alpha(\\beta_t)^{n+1} + (1 - \\alpha)(\\beta_u)^{n+1}} ((\\mu_u - \\mu_t)^T (\\alpha \\Sigma_t^{-1} + (1 - \\alpha) \\Sigma_u^{-1})^{-1} (\\mu_u - \\mu_t)) - \\gamma)$ (7)\nLet the batch sample $\\mathcal{B} = \\{ \\mathcal{B}^P, \\mathcal{B}^N \\}$, where $\\mathcal{B}^P$ denotes the positive image-text sample set, and $\\mathcal{B}^N$ represents the negative set. We define the encapsulation loss function as follows:\n$\\mathcal{L}_{order} = \\sum_{(\\mathcal{G}_t,\\mathcal{G}_u) \\in \\mathcal{B}^P} d_\\gamma(\\mathcal{G}_t, \\mathcal{G}_u) + \\sum_{(\\mathcal{G}_t,\\mathcal{G}_u) \\in \\mathcal{B}^N} \\text{max} \\{0, m - d_\\gamma(\\mathcal{G}_t, \\mathcal{G}_u) \\}$ (8)\nFor the positive samples, a definite partial order relationship exists, enabling the direct application of the density penalty $d_\\gamma(\\cdot)$. For the negative samples, we enforce the penalty to exceed a margin m due to the absence of an order relationship.\nOur goal is to enhance the similarity of semantic distributions between image-text pairs. Therefore, we also employ the classic CLIP contrastive solution [Radford et al., 2021] to compute the geodesic distance between the expectation values of image and text in hyperbolic densities as defined in Equation 1, applying Softmax normalization. We define $\\mathcal{L}_{con}$ as the contrastive loss, which is computed as an average of the contrastive losses from both image and text perspectives."}, {"title": "5 Experiments", "content": "In this section, we aim to rigorously evaluate the performance of our algorithm. We first introduce the baseline model, followed by a description of the medical image-text data and training details used for model pre-training. Then, we discuss the advantages of our proposed model in medical image-text alignment from both quantitative and qualitative perspectives.\nA key innovation of our algorithm lies in the use of density representations in hyperbolic space for image-text alignment. To validate the superiority of our approach, we compare it with two methods: CLIP, which aligns image-text pairs in Euclidean space using point embeddings [Radford et al.,"}, {"title": "5.1 Training Details", "content": "Datasets: We train our alignment model using the MIMIC-CXR v2 dataset [Johnson et al., 2019], comprising over 227,000 studies of paired image-report data sourced from 65,379 patients undergoing various scans. Each study may contain one or two images, representing different scan views, resulting in a total of 377,110 images. During training, we perform random cropping, flipping, rotation, and other data augmentation techniques on the images, while also resizing them to a [224,224] dimension. Additionally, for the text data, we augment the reports by randomly adding medical entity prefixes to enhance semantic information, such as 'event_list: report'.\nSettings: We employ ViT-B [Mu et al., 2022] with a patch size of 16 as the image encoder, as it has demonstrated competitive performance in hyperbolic space [Desai et al., 2023]. Our initialization strategy for image/text encoders follows a similar style to MERU, with the exception of utilizing ClinicalBERT [Alsentzer et al., 2019b] as the pre-trained text encoder, which has been pre-trained on large-scale medical text data. For HYDEN, we initialize the learnable curvature parameter c to 1.0 and clamp it within the range of [0.1, 10.0] to prevent training instability. All experiments were conducted using two NVIDIA A40 GPU and the PyTorch framework\nOptimization: We adopt the AdamW optimizer with a weight decay of 0.2 and $(\\beta_1, \\beta_2) = (0.9, 0.98)$. Weight decay is disabled for all gains, biases, and learnable scalars. Models are trained for 13,000 iterations with a batch size of 256. The maximum learning rate is set to $1 \\times 10^{-5}$, linearly increased for the first 500 iterations, followed by cosine decay to zero. We leverage mixed precision to expedite training, except when computing exponential maps and losses, where FP32 precision is used for numerical stability."}, {"title": "5.2 Quantitative Analysis", "content": "We evaluate all baselines and HYDEN on three categories of zero-shot downstream tasks, classifica- tion, text-image retrieval and image-image retrieval. We use three public datasets for the evaluation, where both RSNA Pneumonia [Shih et al., 2019] and SIIM-ACR Pneumothorax [Kaggle, 2019] are used for binary classification, ChestXray14[Wang et al., 2017a] is used for multi-label clas- sification, text-image retrieval and image-image retrieval. For the two binary classification tasks, we report the Area Under the Curve (AUC) and F1 score; for the multi-label task, we provide the Micro-AUC and Micro-F1. For the retrieval task, Top-k Precision (abbreviated as Prec@k) and Tok-k Normalized Discounted Cumulative Gain (abbreviated as NDCG@k) are used to evaluate the retrieval performance. Refer Appendix B for details about our evaluation tasks and datasets.\nZero-shot Image Classification Table 1 presents the performance of the baselines and HYDEN across three classification datasets. The results indicate that HYDEN consistently demonstrates robust transfer classification performance, both in binary classification tasks and multi-label classification task. Compared to CLIP, both MERU and HyperMed achieved improved accuracy. This suggests that using hyperbolic space for text-image representations, especially for medical data characterized by a visual semantic hierarchy, is more effective. Relative to MERU, HYDEN achieved the highest accuracy across almost all of metrics, highlighting the advantages of density embedding-based representation methods over point vector embedding, particularly in addressing the challenges of semantic uncertainty.\nZero-shot Retrieval Table 2 displays the performance of two baseline models and HYDEN in \"image- to-text\" and \"image-to-image\" retrieval tasks. The results demonstrate that representation learning in hyperbolic space mostly outperforms that in Euclidean space; among the methods, HYDEN exhibits the best retrieval performance. Furthermore, we observed a significant enhancement in the ranking quality of HYDEN's retrieval results compared to the two baseline methods. We hypothesize that this improvement is linked to the method of density embedding. Similar to findings in the recommendation systems domain [Dos Santos et al., 2017], unlike point vector embeddings, density embeddings enable better handling of uncertainties, information sparsity, ambiguity, and even contradictions, which are common challenges in medical image-text data.\nAblation Studies In this section, we examine the impact of different design choices using HYDEN. Specifically, we trained three ablation models with default hyperparameters, and the results are presented in Table 3. From Table 3, we observe that: (1) Using $\\alpha$-divergence in the loss function instead of KL divergence better aligns with the encapsulation's partial order properties of text-image distribution embeddings. The experimental results also indicate that replacing $\\alpha$-divergence with KL divergence leads to performance degradation across all tasks. (2) Omitting the encapsulation loss, i.e., not using $\\mathcal{L}_{order}$ as defined in Equation 8 and relying primarily on $\\mathcal{L}_{con}$, results in performance degradation across all tasks. This is because not using encapsulation loss implies that the prior partial order of text and image cannot be utilized in hyperbolic space, thus losing the benefits introduced"}, {"title": "5.3 Qualitative Analysis", "content": "In this section, we explore the trained models to deduce the characteristics of the model in capturing the visual semantic hierarchy structure. The concept of 'Embedding distances from [ROOT]' was introduced by Desai et al. [2023] to depict the generality differences between text and image embeddings in hyperbolic space. This concept highlights that in a representation space that effectively captures the visual semantic hierarchy, text embeddings are typically more general than image embeddings and, therefore, should be closer to the root node [ROOT].\nHere, we visualize the differences in distance distributions between text and image embeddings. Given that our approach utilizes distribution embeddings, we specifically visualize the expectations of the distance distributions of text and image density embeddings. Figure 3 demonstrates that the distribution differences generated by our model lie between those produced by MERU and CLIP, with some overlapping distribution areas. This suggests that our model is capable of capturing the visual semantic hierarchy. Compared to the diversity of natural text, medical image-text data is relatively uniform, and the introduction of distributions diminishes the effect of prior entailments, explaining why our model achieves significantly higher accuracy and ranking quality in both text-image and image-image retrieval tasks."}, {"title": "6 Conclusion", "content": "In this paper, we propose a novel approach, HYDEN, to text-image representation learning based on hyperbolic density embeddings. It is a representation learning method tailored for specific medical domain data. Experimental results demonstrate the interpretability of our method and its superior performance compared to baseline methods across various zero-shot tasks and different datasets.\nLimitations. Our work is not without limitations. While our method performs well in zero-shot retrieval and image classification tasks, it cannot be directly applied as a pre-trained model to down- stream fine-tuning tasks. This is because downstream fine-tuning tasks mainly involve classification, segmentation, recognition, etc., based on Euclidean space. Applying our model to other tasks involving few-shot learning or full-model fine-tuning is also beyond the scope of this paper."}, {"title": "A Material", "content": "R\u00e9nyi $\\alpha$-Divergence is a general family of divergences that introduce varying degrees of zero-forcing penalty. The general form of the $\\alpha$-divergence for $\\alpha \\neq 0, 1$ is described as below,\n$D_{\\alpha}(f \\| g) = \\frac{1}{\\alpha(\\alpha - 1)} \\log \\bigg( \\int \\frac{f(x)^\\alpha}{g(x)^{\\alpha - 1}} dx \\bigg)$ (9)\nIt is notable that as $\\alpha$ approaches 0 or 1, the $\\alpha$-divergence converges to the KL divergence and the reverse KL divergence, respectively. For two multivariate Gaussians f and g, the R\u00e9nyi $\\alpha$-Divergence can be expressed as:\n$D_{\\alpha}(f||g) = \\frac{1}{2\\alpha(\\alpha - 1)} \\Bigg[-\\log \\Bigg( \\frac{\\text{det}(\\Sigma_f)^\\alpha \\text{det}(\\Sigma_g)^{(1-\\alpha)}}{\\text{det}(\\alpha \\Sigma_g^{-1} + (1-\\alpha)\\Sigma_f^{-1})} \\Bigg) + (\\mu_f-\\mu_g)^T (\\alpha \\Sigma_g + (1-\\alpha)\\Sigma_f)^{-1}(\\mu_f-\\mu_g)\\Bigg]$ (10)\nHere, the parameter $\\alpha$ modulates the extent of zero forcing: minimizing $D_{\\alpha}(f||g)$ for high $\\alpha$ values results in f being concentrated towards the high-density regions of g. Conversely, for low $\\alpha$, f tends to have broader support, covering regions of g including those with low density."}, {"title": "B Evaluation Tasks & Data", "content": "Zero-shot Image Classification: We evaluate the pre-trained model on three representative medical image classification tasks:\n1. RSNA Pneumonia Dataset[Shih et al.", "2019": "Comprising over 260", "task": "pneumonia vs. normal. For evaluation purposes"}, {"2019": "Contains more than 12", "2017a": "NIH ChestXray14 has 112", "Retrieval": "For pre-training methods akin to CLIP, text-image retrieval tests are standard practice. Following the practices of CLIP [Radford et al., 2021", "2023": "we also introduce downstream tasks for text-image retrieval. In medical imaging reports, the same diagnosis often has varied textual descriptions, making retrieval from image to text impractical. Thus, we do not use images to query text; instead, we use text to retrieve specific categories of images as described in [Zhang et al., 2022"}]}