{"title": "HYDEN: Hyperbolic Density Representations for Medical Images and Reports", "authors": ["Zhi Qiao", "Linbin Han", "Xiantong Zhen", "Jia-Hong Gao", "Zhen Qian"], "abstract": "In light of the inherent entailment relations between images and text, hyperbolic point vector embeddings, leveraging the hierarchical modeling advantages of hyperbolic space, have been utilized for visual semantic representation learning. However, point vector embedding approaches fail to address the issue of semantic uncertainty, where an image may have multiple interpretations, and text may refer to different images, a phenomenon particularly prevalent in the medical domain. Therefor, we propose HYDEN, a novel hyperbolic density embedding based image-text representation learning approach tailored for specific medical domain data. This method integrates text-aware local features alongside global features from images, mapping image-text features to density features in hyperbolic space via using hyperbolic pseudo-Gaussian distributions. An encapsulation loss function is employed to model the partial order relations between image-text density distributions. Experimental results demonstrate the interpretability of our approach and its superior performance compared to the baseline methods across various zero-shot tasks and different datasets.", "sections": [{"title": "1 Introduction", "content": "In recent years, cross-modal text-image representation learning has achieved tremendous success and drawn widespread attention in many tasks such as zero-shot learning and image-text retrieval. This success is largely due to the use of large volumes of weakly-supervised image-text pair data to enhance vision-language representation learning [Radford et al., 2021]. In the field of medical imaging, cross-modal representation learning tailored to specific domain data, such as chest radiographs and their associated radiology reports, can yield robust and powerful foundation models in specialized areas [Zhang and Metaxas, 2023].\nAs the proverb goes, 'A picture is worth a thousand words.' This suggests that an image inherently contains more information than a textual description of it, which can be seen as merely a simplified abbreviation of the image. This relationship, where the text may serve as an entailment of the image, can be considered as visual-semantic hierarchy [Vendrov et al., 2016]. Consequently, it is a plausible hypothesis that incorporating such inductive biases of visual semantic hierarchies into cross-modal alignment tasks could enhance the generalizability of representations and improve the interpretability of learning representations. Vendrov et al. [2016] introduced an order embedding strategy considering these hierarchical semantic during the text-image alignment process. However, numerous studies [Nickel and Kiela, 2017, 2018, Xu et al., 2022, 2023, Fu et al., 2023] have demonstrated that modeling data with inherent hierarchical features in non-Euclidean hyperbolic spaces can provide superior representations. By leveraging the advantages of hyperbolic space in modeling hierarchical structures and the generalization capabilities of cross-modal contrastive learning in zero-shot scenarios, Desai et al. [2023] has proposed cross-modal hyperbolic representation learning. This approach employs"}, {"title": "2 Related Work", "content": "Image-text representation learning has garnered substantial interest due to its potential to enhance visual representation. Traditional methods predominantly employ contrastive metric learning ap-proaches, with CLIP [Radford et al., 2021] being a notable example that has demonstrated remarkable results. These methods typically operate in the Euclidean space and have been extensively applied across various general domains."}, {"title": "3 Preliminaries", "content": "Hyperbolic Geometry Hyperbolic geometry is a non-Euclidean geometry with a constant negative curvature, and it can be visualized as the forward sheet of the two-sheeted hyperboloid. In this study, we will use the Lorentz model on the upper half of a two-sheeted hyperboloid, as claimed in [Nickel and Kiela, 2018], comes with a simpler closed form of the geodesics and does not suffer from the numerical instabilities in approximating the distance. Lorentz model $\\mathbb{H}^n$ processing a constant curvature -c can be represented as a set of points $z \\in \\mathbb{R}^{n+1}$. Lets $z, z' \\in \\mathbb{H}^n$, the Lorentzian product $(z, z')_\\mathcal{L} = -z_0z'_0 + \\sum_{i=1}^n z_iz'_i$. And, $\\mathbb{H}^n = \\{ z \\in \\mathbb{R}^{n+1}: (z, z')_\\mathcal{L} = -1/c, c > 0 \\}$. The distance between $z$ and $z'$ is given by\n$d_\\mathcal{L}(z, z') = arccosh(-(z, z'))$ (1)\nwhich is also the length of the geodesic that connects z and z'. We will refer to the one-hot vector $\\mu_0 = [1/\\sqrt{c}, 0, 0, 0...0] \\in \\mathbb{H}^n \\subset \\mathbb{R}^{n+1}$ as the origin of the hyperbolic space.\nTangent Space of Hyperbolic Space The tangent space at a point $\\mu \\in \\mathbb{H}^n$ is a Euclidean space composed of vectors. Denoted by $T_\\mu \\mathbb{H}^n$, this tangent space represents the set of vectors in the same ambient space $\\mathbb{R}^{n+1}$ where $\\mathbb{H}^n$ is embedded. The vectors in $T_\\mu \\mathbb{H}^n$ satisfy an orthogonality condition relative to the Lorentzian product, defined as $T_\\mu \\mathbb{H}^n := \\{u : (\\mu, u)_\\mathcal{L} = 0 \\}$. This set can be visualized as the tangent space at the point $\\mu$ on the forward hyperboloid sheet. Specifically, at the origin $\\mu_0$ of $\\mathbb{H}^n$, the tangent space $T_{\\mu_0} \\mathbb{H}^n$ consists of vectors $v \\in \\mathbb{R}^{n+1}$. The norm $||v||_\\mathcal{L}$, given by the Lorentzian inner product, simplifies to the Euclidean norm $||v||_2$, defined as $||v||_\\mathcal{L} := \\sqrt{(v, v)_\\mathcal{L}} = ||v||_2$.\nExponential Map\nThe exponential map provides a method for mapping a vector from a tangent space to its corresponding point on the surface of the hyperbolic space. For every $u \\in T_\\mu \\mathbb{H}^n$, the exponential map $exp_\\mu(u) : T_\\mu \\mathbb{H}^n \\rightarrow \\mathbb{H}^n$ allows us to project a vector $u$ in $T_\\mu \\mathbb{H}^n$ onto $\\mathbb{H}^n$ such that the distance from $\\mu$ to the destination point of the map coincides with the Lorentzian norm $||u||$ of $u$. In the context of hyperbolic space, the exponential map is given by the equation:\n$z = exp_\\mu(u) = cosh(||u||_\\mathcal{L})\\mu + sinh(||u||_\\mathcal{L})\\frac{u}{||u||_\\mathcal{L}}$ (2)\nIn this paper, we specifically consider exponential maps where $\\mu$ represents the origin of the hyper-boloid ($O = [\\sqrt{1/c}, 0]$)."}, {"title": "4 Method", "content": "In this section, we present a comprehensive introduction to the HYDEN model. Drawing on the foundation laid by the MERU model [Desai et al., 2023] and the widely acclaimed, user-friendly CLIP"}, {"title": "4.1 Image-Text Feature Embedding", "content": "In our model, the features $[f_u, f_t]$ are derived from respective image and text encoders. For text data, we employ BioClinicalBERT [Alsentzer et al., 2019a], a model that has been pre-trained on the MIMIC III dataset [Shen et al., 2016], to generate token-level embeddings. Consistent with practices outlined in [Cheng et al., 2023b], the output of the [CLS] token is used as the medical text feature $f_t$, encapsulating the overall semantic content of the input text.\nFor image encoding, we utilize the widely-used Vision Transformer (ViT) architecture [Mu et al., 2022]. We assume that $f_u = (f_0, f_1, ..., f_n)$ captures the outputs from the image encoder. Recog-nizing that pathological symptoms often occupy only a portion of a medical image, relying solely on global representations may not adequately capture essential local semantic features. Thus, similar to approaches in [Huang et al., 2021, Cheng et al., 2023b, M\u00fcller et al., 2022b], we enhance the global features by integrating text-aware image local representations.\nSpecifically, we implement a Self-attention module [Vaswani et al., 2017], widely used in cross-modal feature extraction. In this setup, $f_u$ acts as both the keys (K) and values (V), while the text embedding $f_t$ functions as the query (Q). This configuration allows us to derive a text-aware image"}, {"title": "4.2 Hyperbolic Density Embedding", "content": "Our objective is to transform image-text features into density representations within hyperbolic space. Previous studies such as [Nagano et al., 2019] and [Mathieu et al., 2019] proposed methods like the pseudo-hyperbolic Gaussian distribution based on the Lorentz manifold. Due to the computational demands and numerical instabilities of the Poincar\u00e9-disk model, we opt for the more stable pseudo-hyperbolic Gaussian distribution for our hyperbolic density embedding. The tangent space $T_\\mu \\mathbb{H}^n$ of hyperbolic space $\\mathbb{H}^n$ is a Euclidean space, and in $T_{\\mu_0} \\mathbb{H}^n$, vectors $v$ satisfy $v = \\{ v_0, v_1, ..., v_n \\} \\in \\mathbb{R}^{n+1}$ where $v_0 = 0$, aligning with the dimensional properties.\nTo begin, we introduce separate deep nonlinear network blocks, $B_{density}$, for processing image and text features independently. These blocks do not share parameters, ensuring distinct representations for each modality. As in Figure 2, for text features, $\\mu_t$ and $\\beta_t$ are the outputs of $B_{density}(f_t)$.\nInstead of generating covariance matrices directly, which can introduce numerical instability, we use matrices based on diagonal or spherical assumptions. These are known for their computational efficiency and effectiveness in embedding tasks, particularly in the context of word distribution embedding where spherical covariance matrices have been shown to better model distributional partial order relationships [Vilnis and McCallum, 2014]. We thus employ a covariance matrix based on the spherical assumption: $\\Sigma_t = \\beta_t \\cdot I \\in \\mathbb{R}^{(n+1) \\times (n+1)}$.\nTo ensure that our covariance matrix is positively definite, necessary for the stability of the pseudo-hyperbolic Gaussian distribution, we modify $\\beta_t$ using the expression $\\beta_t = exp(\\beta_t)$ referring to solution in VAE[Kingma and Welling, 2019]. This adjustment is crucial for maintaining the mathe-matical integrity of our model when dealing with real-world data. For the embedding vector $\\mu_t \\in \\mathbb{R}^n$, our aim is to project this vector onto hyperboloid space, which is achieved by mapping it through the exponential function as detailed in Equation 2.\nThe vector $\\mu^{tan}_t = [0, \\mu_t]$ resides in $\\mathbb{R}^{n+1}$ and belongs to the tangent space $T_{\\mu_0} \\mathbb{H}^n$ at the origin of the hyperboloid, O. The norm $||\\mu^{tan}_t||_\\mathcal{L}$, which equals $||\\mu_t||_2$, ensures that the mapping preserves the distances inherent to the model's geometric structure. We apply the exponential map to $\\mu^{tan}_t$, decomposing the transformation into two parts:\n$cosh(\\sqrt{c}||\\mu^{tan}_t||_\\mathcal{L})O = [\\sqrt{1/c} \\times cosh(\\sqrt{c}||\\mu^{tan}_t||_\\mathcal{L}), 0] = [\\sqrt{1/c} \\times cosh(\\sqrt{c}||\\mu_t||_2), 0]$ (3)\n$\\frac{sinh(\\sqrt{c}||\\mu^{tan}_t||_\\mathcal{L})}{\\sqrt{c}||\\mu^{tan}_t||_\\mathcal{L}}\\mu^{tan} = [0, \\frac{sinh(\\sqrt{c}||\\mu^{tan}_t||_\\mathcal{L})}{\\sqrt{c}||\\mu^{tan}_t||_\\mathcal{L}}\\mu^{emb}_t] = [0, \\frac{sinh(\\sqrt{c}||\\mu_t||_2)}{\\sqrt{c}||\\mu_t||_2}\\mu^{emb}_t]$ (4)\nUpon applying the exponential map, we derive the expectation of the hyperbolic density representa-tion:\n$\\mu_t = exp_{\\mu_0}(\\mu^{tan}_t) = (\\sqrt{1/c} \\times cosh(\\sqrt{c}||\\mu_t||_2), \\frac{sinh(\\sqrt{c}||\\mu_t||_2)}{\\sqrt{c}||\\mu_t||_2}\\mu^{emb}_t)$ (5)\nThis projection results in the hyperbolic density representation $G_t(\\mu_t, \\beta_t \\cdot I)$. Following a similar procedure, we also derive $G_u(\\mu_u, \\beta_u \\cdot I)$ for the image features, thereby ensuring a uniform approach to handling different modalities within our framework."}, {"title": "4.3 Loss Function Based on Density Embedding", "content": "Traditional point vector embedding often utilizes entailment angle constraints to define relationships between entities [Desai et al., 2023]. However, when dealing with probability densities, the notion of partial order can be more complexly captured through the concept of encapsulation. Specifically, a density f is considered more specific than another density g if f is entirely encompassed by g, formally expressed as $f < g \\Leftrightarrow \\{x : f(x) > \\eta\\} \\subseteq \\{x : g(x) > \\eta\\}$, for any $\\eta \\geq 0$, where $\\eta$ indicates the degree of encapsulation necessary for one distribution to entail another."}, {"title": "5 Experiments", "content": "In this section, we aim to rigorously evaluate the performance of our algorithm. We first introduce the baseline model, followed by a description of the medical image-text data and training details used for model pre-training. Then, we discuss the advantages of our proposed model in medical image-text alignment from both quantitative and qualitative perspectives.\nA key innovation of our algorithm lies in the use of density representations in hyperbolic space for image-text alignment. To validate the superiority of our approach, we compare it with two methods: CLIP, which aligns image-text pairs in Euclidean space using point embeddings [Radford et al.,"}, {"title": "5.1 Training Details", "content": "Datasets: We train our alignment model using the MIMIC-CXR v2 dataset [Johnson et al., 2019], comprising over 227,000 studies of paired image-report data sourced from 65,379 patients undergoing various scans. Each study may contain one or two images, representing different scan views, resulting in a total of 377,110 images. During training, we perform random cropping, flipping, rotation, and other data augmentation techniques on the images, while also resizing them to a [224,224] dimension. Additionally, for the text data, we augment the reports by randomly adding medical entity prefixes to enhance semantic information, such as 'event_list: report'.\nSettings: We employ ViT-B [Mu et al., 2022] with a patch size of 16 as the image encoder, as it has demonstrated competitive performance in hyperbolic space [Desai et al., 2023]. Our initialization strategy for image/text encoders follows a similar style to MERU, with the exception of utilizing ClinicalBERT [Alsentzer et al., 2019b] as the pre-trained text encoder, which has been pre-trained on large-scale medical text data. For HYDEN, we initialize the learnable curvature parameter c to 1.0 and clamp it within the range of [0.1, 10.0] to prevent training instability. All experiments were conducted using two NVIDIA A40 GPU and the PyTorch framework\nOptimization: We adopt the AdamW optimizer with a weight decay of 0.2 and ($$\\beta_1, \\beta_2$$) = (0.9, 0.98). Weight decay is disabled for all gains, biases, and learnable scalars. Models are trained for 13,000 iterations with a batch size of 256. The maximum learning rate is set to $1 \\times 10^{-5}$, linearly increased for the first 500 iterations, followed by cosine decay to zero. We leverage mixed precision to expedite training, except when computing exponential maps and losses, where FP32 precision is used for numerical stability."}, {"title": "5.2 Quantitative Analysis", "content": "We evaluate all baselines and HYDEN on three categories of zero-shot downstream tasks, classifica-tion, text-image retrieval and image-image retrieval. We use three public datasets for the evaluation, where both RSNA Pneumonia [Shih et al., 2019] and SIIM-ACR Pneumothorax [Kaggle, 2019] are used for binary classification, ChestXray14[Wang et al., 2017a] is used for multi-label clas-sification, text-image retrieval and image-image retrieval. For the two binary classification tasks, we report the Area Under the Curve (AUC) and F1 score; for the multi-label task, we provide the Micro-AUC and Micro-F1. For the retrieval task, Top-k Precision (abbreviated as Prec@k) and Tok-k Normalized Discounted Cumulative Gain (abbreviated as NDCG@k) are used to evaluate the retrieval performance. Refer Appendix B for details about our evaluation tasks and datasets.\nZero-shot Image Classification Table 1 presents the performance of the baselines and HYDEN across three classification datasets. The results indicate that HYDEN consistently demonstrates robust transfer classification performance, both in binary classification tasks and multi-label classification task. Compared to CLIP, both MERU and HyperMed achieved improved accuracy. This suggests that using hyperbolic space for text-image representations, especially for medical data characterized by a visual semantic hierarchy, is more effective. Relative to MERU, HYDEN achieved the highest accuracy across almost all of metrics, highlighting the advantages of density embedding-based representation methods over point vector embedding, particularly in addressing the challenges of semantic uncertainty.\nZero-shot Retrieval Table 2 displays the performance of two baseline models and HYDEN in \"image-to-text\" and \"image-to-image\" retrieval tasks. The results demonstrate that representation learning in hyperbolic space mostly outperforms that in Euclidean space; among the methods, HYDEN exhibits the best retrieval performance. Furthermore, we observed a significant enhancement in the ranking quality of HYDEN's retrieval results compared to the two baseline methods. We hypothesize that this improvement is linked to the method of density embedding. Similar to findings in the recommendation systems domain [Dos Santos et al., 2017], unlike point vector embeddings, density embeddings enable better handling of uncertainties, information sparsity, ambiguity, and even contradictions, which are common challenges in medical image-text data.\nAblation Studies In this section, we examine the impact of different design choices using HYDEN. Specifically, we trained three ablation models with default hyperparameters, and the results are presented in Table 3. From Table 3, we observe that: (1) Using a-divergence in the loss function instead of KL divergence better aligns with the encapsulation's partial order properties of text-image distribution embeddings. The experimental results also indicate that replacing a-divergence with KL divergence leads to performance degradation across all tasks. (2) Omitting the encapsulation loss, i.e., not using $L_{order}$ as defined in Equation 8 and relying primarily on $L_{con}$, results in performance degradation across all tasks. This is because not using encapsulation loss implies that the prior partial order of text and image cannot be utilized in hyperbolic space, thus losing the benefits introduced"}, {"title": "5.3 Qualitative Analysis", "content": "In this section, we explore the trained models to deduce the characteristics of the model in capturing the visual semantic hierarchy structure. The concept of 'Embedding distances from [ROOT]' was introduced by Desai et al. [2023] to depict the generality differences between text and image embeddings in hyperbolic space. This concept highlights that in a representation space that effectively captures the visual semantic hierarchy, text embeddings are typically more general than image embeddings and, therefore, should be closer to the root node [ROOT].\nHere, we visualize the differences in distance distributions between text and image embeddings. Given that our approach utilizes distribution embeddings, we specifically visualize the expectations of the distance distributions of text and image density embeddings. Figure 3 demonstrates that the distribution differences generated by our model lie between those produced by MERU and CLIP, with some overlapping distribution areas. This suggests that our model is capable of capturing the visual semantic hierarchy. Compared to the diversity of natural text, medical image-text data is relatively uniform, and the introduction of distributions diminishes the effect of prior entailments, explaining why our model achieves significantly higher accuracy and ranking quality in both text-image and image-image retrieval tasks."}, {"title": "6 Conclusion", "content": "In this paper, we propose a novel approach, HYDEN, to text-image representation learning based on hyperbolic density embeddings. It is a representation learning method tailored for specific medical domain data. Experimental results demonstrate the interpretability of our method and its superior performance compared to baseline methods across various zero-shot tasks and different datasets.\nLimitations. Our work is not without limitations. While our method performs well in zero-shot retrieval and image classification tasks, it cannot be directly applied as a pre-trained model to down-stream fine-tuning tasks. This is because downstream fine-tuning tasks mainly involve classification, segmentation, recognition, etc., based on Euclidean space. Applying our model to other tasks involving few-shot learning or full-model fine-tuning is also beyond the scope of this paper."}, {"title": "A Material", "content": "R\u00e9nyi a-Divergence is a general family of divergences that introduce varying degrees of zero-forcing penalty. The general form of the a-divergence for a \u2260 0, 1 is described as below,\n$D_\\alpha(f||g) = \\frac{1}{\\alpha(\\alpha - 1)} \\log{\\int {(\\frac{f(x)}{g(x)})^{\\alpha} dx}}$ (9)\nIt is notable that as a approaches 0 or 1, the a-divergence converges to the KL divergence and the reverse KL divergence, respectively. For two multivariate Gaussians f and g, the R\u00e9nyi a-Divergence can be expressed as:\n$D_\\alpha(f||g) = \\frac{1}{2\\alpha(\\alpha - 1)}\\left[-\\log{\\frac{\\text{det}(\\alpha\\Sigma_g + (1 - \\alpha)\\Sigma_f)}{(\\text{det}(\\Sigma_f))^{1-\\alpha}(\\text{det}(\\Sigma_g))^{\\alpha}}}} + (\\mu_f-\\mu_g)^T(\\alpha\\Sigma_g + (1 - \\alpha)\\Sigma_f)^{-1}(\\mu_f-\\mu_g)\\right]$ (10)\nHere, the parameter a modulates the extent of zero forcing: minimizing $D_\\alpha(f||g)$ for high a values results in f being concentrated towards the high-density regions of g. Conversely, for low a, f tends to have broader support, covering regions of g including those with low density."}, {"title": "B Evaluation Tasks & Data", "content": "Zero-shot Image Classification: We evaluate the pre-trained model on three representative medical image classification tasks:\n1.  RSNA Pneumonia Dataset[Shih et al., 2019]: Comprising over 260,000 frontal chest radiographs collected by the Radiological Society of North America (RSNA). These images can be classified into a binary classification task: pneumonia vs. normal. For evaluation purposes, we randomly sample 4003 images for evaluation.\n2.  SIIM-ACR Pneumothorax Dataset[Kaggle, 2019]: Contains more than 12,000 frontal chest radiographs collected by the Society for Imaging Informatics in Medicine and the American College of Radiology (SIIM-ACR). Similar to the RSNA Pneumonia dataset, it is used for a binary classification task to determine the presence or absence of pneumothorax. We use all 10,675 images for evaluation.\n3.  ChestXray14 Dataset[Wang et al., 2017a]: NIH ChestXray14 has 112,120 chest X-ray images with 14 disease labels from 30,805 unique patients. The official test set released by the NIH, comprising 22,433 images, are distinctively annotated by board certified radiologists. For multi-label evaluation, we only test on the official test set.\nZero-shot Text-Image Retrieval: For pre-training methods akin to CLIP, text-image retrieval tests are standard practice. Following the practices of CLIP [Radford et al., 2021] and MERU [Desai et al., 2023], we also introduce downstream tasks for text-image retrieval. In medical imaging reports, the same diagnosis often has varied textual descriptions, making retrieval from image to text impractical. Thus, we do not use images to query text; instead, we use text to retrieve specific categories of images as described in [Zhang et al., 2022]. For this purpose, we first construct a text-image retrieval evaluation dataset. As described in the multi-label classification task, ChestXray14 Wang et al. [2017b] encompasses 14 different disease classes and one 'normal' class, totaling 15 categories. Based on these class labels, we randomly extract 100 images for each class (exclusive), forming the ChestXray14x100 dataset, which consists of 1,500 images. We then write representative text prompts for each of the 15 categories. During testing, for each query, we encode its text using the learned text encoder, then retrieve from the candidate images in a similar manner. This evaluation assesses not only the quality of the learned image representations but also the consistency between text and image representations.\nZero-shot Image-Image Retrieval: This evaluation task is similar to traditional content-based image retrieval tasks and is also a common downstream task in medical imaging. It involves using a query image to search for images of specific categories. To evaluate, a set of query images, image category label prompts, and a candidate image set are provided to the pre-trained representation model. We encode each query and candidate image using the encoder of the pre-trained model, then rank all candidate images in descending order of their reciprocal geodesic distance from the query's expected distribution."}]}