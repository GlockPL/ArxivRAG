{"title": "TrafficGamer: Reliable and Flexible Traffic Simulation for Safety-Critical Scenarios with Game-Theoretic Oracles", "authors": ["Guanren Qiao", "Guorui Quan", "Jiawei Yu", "Shujun Jia", "Guiliang Liu"], "abstract": "While modern Autonomous Vehicle (AV) systems can develop reliable driving poli-\ncies under regular traffic conditions, they frequently struggle with safety-critical traffic scenarios. This difficulty primarily arises from the rarity of such scenarios in driving datasets and the complexities associated with predictive modeling among multiple ve-hicles. To support the testing and refinement of AV policies, simulating safety-critical traffic events is an essential challenge to be addressed. In this work, we introduce Traf-ficGamer, which facilitates game-theoretic traffic simulation by viewing common road driving as a multi-agent game.In evaluating the empirical performance across various real-world datasets, TrafficGamer ensures both fidelity and exploitability of the simu-lated scenarios, guaranteeing that they not only statically align with real-world traffic distribution but also efficiently capture equilibriums for representing safety-critical sce-narios involving multiple agents. Additionally, the results demonstrate that TrafficGamer exhibits highly flexible simulation across various contexts. Specifically, we demonstrate that the generated scenarios can dynamically adapt to equilibriums of varying tight-ness by configuring risk-sensitive constraints during optimization. To the best of our knowledge, TrafficGamer is the first simulator capable of generating diverse traffic sce-narios involving multiple agents. We have provided a demo webpage for the project at: https://qiaoguanren.github.io/trafficgamer-demo/.", "sections": [{"title": "Introduction", "content": "As a cutting-edge technology, autonomous driving has been a critical component of future transportation systems. The development of Autonomous Vehicles (AV) requires extensive testing and calibration of their control systems. Given the significant risks of conducting these tasks on real roads, the industry commonly relies on traffic simulation systems to ensure the safe development of AVs. Within these simulation systems, the commonly studied goals include traffic flow simulation (e.g., SUMO [1] and CityFlow[2]), sensory data simulation (e.g.,"}, {"title": "Evaluation Metrics", "content": "We leverage a comprehensive set of statistical metrics to evaluate the fidelity and effectiveness of the proposed TrafficGamer. The metrics include:\nFidelity. The fidelity metrics measure how well the simulated traffic distribution (i.e., the distribution of vehicles' temporal and spatial features) matches the observed data distribution. We implement these fidelity metrics by f-Divergence $D_f(P||Q)$ which measures the divergence between two probability distributions P and Q over the space X, so that:\n$D_{f_a}(P||Q) = \\int_X f_a(\\frac{dP}{dQ})dQ$\nwhere $f_a: [0,+\\infty) \\rightarrow [-\\infty, +\\infty]$ denotes a convex function where f(t) is finite for all $t > 0$, f(1) = 0, and $f(0) = \\lim_{t\\rightarrow0^+} f(t)$. In this work, we study the following implementation of $D_{f_a}$:\n\u2022 By setting $f(t) = (\\sqrt{t} - 1)^2$, f-Divergence becomes Hellinger distance $D_H$ such that:\n$D_H(P||Q) = \\frac{1}{2}\\int_X (\\sqrt{P(dx)} - \\sqrt{Q(dx)})^2$\n\u2022 By implementing f(t) = tln(t), f-Divergence can be transformed into Kullback-Leibler divergence (KL). We can calculate KL-divergence $D_{KL}$ as:\n$D_{KL}(P||Q) = \\int_X (P(dx)log\\frac{P(dx)}{Q(dx)})$\nAdditionally, since f-Divergence remains undefined when the support of compared distribu-tions are non-overlapping, our experiment includes Wasserstein Distance of two probability"}, {"title": "Fidelity Validation of Generated Motion Trajectories", "content": "A crucial prerequisite for simulated safety-critical traffic is that it aligns with realistic traffic scenarios. We characterize this alignment with distributional fidelity (Section Evaluation Metrics), which quantifies the divergence between realistic traffic distributions and those simulated by our TrafficGamer. Among the spatial and temporal traffic features, vehicle speed, and inter-vehicle distance are the most commonly studied features for examining the performance of AV simulators [24]. We adapt f-divergence to qualify how well the simulated distribution of vehicle speed and car distance match the realistic ones from the large-scale traffic datasets (Argoverse 2 and Waymo).\nIn this experiment, we assess fidelity by quantifying distributional divergence using several divergence metrics, including Kullback-Leibler divergence, Hellinger distance, and Wasserstein distance [53]. Figure 4 shows the experiment results, and we find the simulated scenarios generated by our TrafficGamer can properly imitate the real-world distribution of instantaneous vehicle speeds and vehicle distances. In specific, for methods based on multi-agent fine-tuning (TrafficGamer and MAPPO), we observe that the divergence between realistic traffic data and the simulated traffic features generated by TrafficGamer is smaller compared to those produced by MAPPO. This is because the MMD objective in our TrafficGamer (Formulate 12) explicitly minimizes the divergence to the observed policy during optimization. The reduction in divergence results in a more accurate replication of vehicle speed and distance distributions, indicating that the learned multi-agent policies are more closely aligned with actual human driving behaviors. To have more evidence, the performance of TrafficGamer is comparable to the imitation learning-based methods (QCNet and GameFormer) in terms of ensuring fidelity to realistic driving behaviors.\nTo better evaluate the effectiveness of our method in replicating the driving styles of human drivers across various scenarios, we categorize the scenarios into specific types, including fork road and cross road. Figure 4 (rows from 3 to 6) compares the learned and actual distributions of distance and speed. In the crossroad scenario (rows 5 and 6 in Figure 4), TrafficGamer achieves performance comparable to, albeit slightly less robust than QCNet in maintaining fidelity. This result is anticipated since QCNet is designed to closely mimic the observed behaviors of drivers, resulting in trajectories that heavily overlap with those in the source dataset. Another key observation is that our method, TrafficGamer, significantly outperforms standard MARL approaches such as MAPPO. This demonstrates TrafficGamer's capability to accurately replicate a wide range of human driving behaviors and generate realistic driving scenarios. We also check the fidelity in the Waymo motion dataset, and the results closely match the ground-truth distribution. More details can be found in supplementary Section 3d."}, {"title": "Efficient Exploitability under a Variety of Scenarios", "content": "Unlike standard RL algorithms that focus on reward maximization, in the multi-agent au-tonomous driving environment, we primarily consider exploitability [43], which measures the extent to which a vehicle's policy can exploit the current traffic policies. An ideal driving equi-librium should have zero exploitability, meaning no single vehicle can achieve greater benefits by continually improving the policy. In this work, we follow the methodology outlined in [47] and utilize the CCE-gap (see definition 1) to measure exploitability. Unlike the common-studied toy environment (e.g., Bargaining, TradeComm, and Battleship [44]), the traffic scenarios involve complex game contexts, multiple agents, and continuous action space, which makes the best response $\\pi_i^*$ computationally intractable, and accurately calculating the exact CCE-gap becomes challenging. Therefore, we estimate the CCE-gap by empirical approximating $\\pi_i^*$ with the following methods:\n\u2022 Breaking the Equilibrium. Upon the studied algorithm converges, we estimate each agent's best response under the current equilibrium, denoted as $\\pi_i^*$, by fixing the policies of the other I - 1 agents and continuously encouraging this agent to maximize its current reward. This process assesses the agent's ability to disrupt the existing equilibrium. If none of the policies can achieve significantly higher rewards, it indicates that the experimental algorithm has successfully identified a reliable CCE.\n\u2022 Restricting Action Space. To overcome the computational intractability caused by complex driving behaviors, we constrain the decision-making domain to choose actions from a pre-defined candidate action set. This set is generated by a pre-trained action predictor (see objective 9), which identifies and ranks the top K actions most similar to those observed in the dataset. Within this tractable action space, we compute $\\pi_i^*$ by selecting the actions that optimally maximize rewards at each step.\nWe compare the performance of TrafficGamer in all Argoverse 2 scenarios with other baselines based on the aforementioned CCE-Gap. Figure 5 and Figure 6 present the CCE-gap of each agent during training. The results show that the training curves of QCNet and GameFormer perform unstable and struggle to converge to the CCE. It illustrates that end-to-end imitation learning methods can not model competitiveness between agents, which makes it difficult to optimize policies for capturing CCE. MAPPO performs better than other baselines, but it still falls short of the results obtained by TrafficGamer. MAPPO faces challenges in efficiently exploring the entire policy space of the Multi-agent game environment during the optimization process, making it difficult to capture the underlying CCE. As a solver defined for CCE, TrafficGamer ensures that the agents' policies are distributionally aligned with human-driven policies and supports stable exploration. This allows each agent to learn the optimal policy and gradually converge to an approximate \u0421\u0421\u0415.\nTable 1 and Table 2 present the CCE-gap of each agent at the end of training. TrafficGamer achieves a smaller CCE-gap compared to other methods, demonstrating its superior exploitabil-ity across various scenarios. While TrafficGamer achieves a smaller CCE-gap compared to other methods, it exhibits slightly reduced performance for certain agents in some scenarios. An in-depth analysis reveals two main reasons for this phenomenon: 1) TrafficGamer may struggle to effectively adapt to rapid changes in opponents' strategies under specific conditions. For example, when surrounding vehicles accelerate, a particular vehicle may need to decelerate to maintain safe spacing. 2) The presence of other vehicles significantly influences the decision-making processes of algorithm-controlled vehicles. For example, if a car on the other hand begins to merge into the current lane, the algorithm might adopt a more conservative driving strategy to prioritize safety, thereby influencing exploiting. Overall, in a multi-agent game environment, it is essential to evaluate the collective performance of all agents. TrafficGamer is capable of learning a CCE that approximates the optimal solution, resulting in a more comprehensive modeling of vehicle driving behaviors."}, {"title": "Simulation of Diverse Safety-Critical Scenarios", "content": "To demonstrate the ability of TrafficGamer to generate diverse traffic scenarios, we visualize the generated scenarios featuring varying degrees of competitive behavior. However, automated driving scenarios involve complex elements including road structures, traffic regulations, and vehicle behaviors, which can be challenging to interpret directly from raw data. To address this, we simulate the actual behavior of vehicles in various safety-critical scenarios and traffic conditions using 2D and 3D visualizations. These are captured in third-person and first-person views, to provide a clearer understanding of the dynamics at play. To facilitate 2D visualization, TrafficGamer supports the display of lines, markers, the road network, vehicle positions, and movement trajectories on a unified 2D plane by following the formats established by Argoverse 2 [45] and Waymo [46]. This approach uses concise symbols to represent various traffic elements, enhancing the intuitiveness of scene analysis. To capture the real-world complexity of traffic behaviors more accurately, TrafficGamer enables large-scale 3D traffic scenario modeling and simulation through ScenarioNet [54], based on the MetaDrive simulator [7]. The scenarios generated can be replayed and explored interactively, ranging from Bird's Eye View layouts to realistic 3D renderings in ScenarioNet.\nIn specific, this experiment explores how well TrafficGamer can facilitate the generation of different safety-critical scenarios under twelve distinct traffic scenarios. To generate diverse traffic scenarios, we design the constrained and risk-sensitive policy optimization [55, 56] for capturing the equilibrium subject to different levels of tightness. Specifically, by adjusting the inter-vehicle distance constraints and risk coefficients, we derive a diverse number of traffic scenarios under different game contexts. Figure 8 illustrates the details of our results.\nBy comparing the scenarios generated with different levels of inter-vehicle distance con-straints (from top to bottom in Figure 8 and Figure 9), we find as the inter-vehicle distance increases, our model adapts, leading to traffic scenarios characterized by less competitive behav-ior and safer navigation across all examined situations. This outcome arises because maintaining distance constraints requires the cooperation of multiple vehicles. Imposing more restrictive constraints (i.e., increasing the distance) significantly enhances the impact of this cooperation on the optimization objective 16 (dynamically controlled by the Lagrange parameter $\\lambda$). Figure 7 displays the variation of the Lagrangian penalty factor during the training process. As the constraints become tighter, the Lagrangian penalty term also increases, indicating that it has a larger impact on the objective 16. On the other hand, if we reduce the required distance, agents begin to prioritize their interests, which significantly increases the likelihood of traffic congestion where no agent can further optimize their policy. All these dynamic scenarios are characterized by the learned CCEs.\nSimilarly, by comparing the scenarios generated with different levels of risk sensitivity (from left to right in Figure 8), we find imposing a higher confidence level leads to more conservative and cautious driving behaviors. A higher confidence level forces the agent to satisfy the constraint with greater probability, resulting in driving strategies that feature lower speeds, increased spacing between vehicles, and more careful navigation through complex traffic scenarios. On the other hand, setting a lower confidence level results in more aggressive and risk-seeking driving behaviors, characterized by faster vehicle speeds and shorter following distances. The system's tolerance for some aggressive behaviors (such as overtaking etc.) has increased. This approach can lead to a higher risk of collisions and increase the likelihood of critical safety scenarios in traffic.\nThese diverse scenarios are crucial for investigating the trade-offs between aggressive and conservative driving behaviors. By analyzing AV policies within these scenarios, we can more effectively evaluate the optimality of driving strategies across different levels of competition and congestion. Furthermore, the comprehensive 2D and 3D simulations provided by our TrafficGamer from both third-person and first-person perspectives offer a detailed understanding of traffic dynamics. These simulations demonstrate how variations in risk sensitivity and distance constraints impact vehicle behavior in real-world driving scenarios."}, {"title": "Discussion", "content": "We demonstrate that our model, TrafficGamer, can effectively represent various safe-critical traffic scenarios by capturing a range of CCEs. To the best of our knowledge, this is the first algorithm that fine-tunes generative world models to accurately model competitive and collaborative behaviors among multiple agents across varying degrees of constraints and dynamic environments. Most importantly, TrafficGamer can accurately characterize traffic congestion scenarios frequently observed in reality but are underrepresented in datasets, such as roundabouts, intersections, and merge points. This capability ensures the high fidelity of TrafficGamer in real-world applications.\nTo facilitate the reliable generation of our safety-critical scenarios, we resolve three critical challenges, including 1) how to guarantee the fidelity of generated trajectories, 2) how to efficiently capture the CCE of each scenario by modeling the competition behaviors of vehicles, and 3) how to dynamically adapt the strength of the equilibrium. These safety-critical scenarios can serve as important testbeds for AV controlling algorithms evaluating the reliability and robustness of AV control algorithms before their practical deployment. Traditional simulation systems have relied heavily on manually designed rules or data-driven trajectory imitation, often resulting in scenarios that lack fidelity and diversity. Our model, TrafficGamer, addresses these limitations by generating a variety of realistic, safety-critical scenarios. It is important to note that our method currently models vehicle behaviors on a static map. Future enhancements for TrafficGamer may include integrating multi-modal large models to generate scenarios that account for varying vehicle behaviors in response to environmental factors, such as weather conditions, time of day, and topography."}, {"title": "Methods", "content": ""}, {"title": "Problem Formulation", "content": "We formulate the task of multi-vehicle motion prediction in the traffic scenarios as a Decentral-ized Partially Observable Constrained Markov Decision Process (Dec-POCMDP).\n$(S, {\\Omega_i, A_i, O_i, r_i, c_i}_{i=1, T, \\gamma, p_0})$ where:\n\u2022 i denotes the number of agents from 1 to I.\n\u2022 $\\Omega_i$ and $A_i$ denote the spaces of observations and actions for a specific agent i. The observations include the position, velocity, heading, and partial map features in a neighbor region of the agent i. The actions consist of relative heading and acceleration, as described in [11].\n\u2022 S denotes the state space that comprises all agents' historical trajectory information and map features.\n\u2022 $O_i : S \\rightarrow \\Omega_i$ denotes the observation function that maps states and actions to local observation for the ith agent. $O = {O_1,...,O_I}$ denotes the function set.\n\u2022 $r_i: {\\Omega_i \\times A_i}_{i=1} \\rightarrow R$ denotes the agent-specific reward function that maps actions and observations from all agents to the reward of ith agent. We consider reward factors such as collision avoidance, lane deviation, and reaching the destination.\n\u2022 $c_i: {\\Omega_i \\times A_i}_{i=1} \\rightarrow R$ denotes the agent-specific cost function that maps actions and observations from all agents to the cost of ith agent. We adapt vehicle distance as a constraint condition, additionally, the expectation of cumulative constraint functions $c_i$ must not exceed associated thresholds episodic constraint threshold $\\delta$.\n\u2022$T:S \\times A \\rightarrow \\Delta S$ 1denotes the transition function.\n\u2022$\\gamma \\in [0,1]$ and $p_0 \\in \\Delta S$ denote the discount factor and the initial distribution.\nMore details of reward and cost functions can be found in Supplementary Section 2a.\nIn the framework of Dec-POCMDP, each agent is assigned an individual reward function and cost function, denoted as $r_i(\\cdot)$ and $c_i(\\cdot)$. This aligns a general-sum game structure [57], more complex and less explored than zero-sum and cooperative games [58, 59]. Figure 10 displays the differences between these three settings. Despite challenges, modeling a general-sum game better aligns with real-world driving scenarios since human drivers often prioritize their objects. These objects can be applied to satisfy conflicting interests among different AV agents. During the optimization process, human drivers' behaviors inevitably affect others' decisions. Accordingly, we consider the General Sum Markov Games (GS-MGs) under the Dec-POCMDP. For the agent i, the value function $V^\\pi_i: S\\rightarrow R$, action-value function $Q^\\pi_i_{s,a}: S \\times A \\rightarrow R$ and advantage function $A^\\pi_i_{s,a}: S \\times A_i \\rightarrow R$ are represented by:\n$V^\\pi_{i,0}(s) = E_{p_0,\\tau,\\pi} [\\sum_{t=0}^T r_i(o_t, a_t) | o_0 = O(s_t) ]$\n$Q^\\pi_{i,0}(s, a_i, -a_i) = E_{p_0,\\tau,\\pi} [\\sum_{t=0}^T r_i(o_t, a_t) | o_0 = O(s_t), a_{i,0} = a_i]$\n$A^\\pi_{i,0}(s, a_i, -a_i) = Q^\\pi_{i,0}(s, a_i, -a_i) - V^\\pi_{i,0}(s)$\nwhere $-a_i = {a_j}_{j\\neq i}^I$ 2 denotes the joint action performed by I \u2013 1 players (without i'th player) and $\\pi = {\\pi_i}_{i=1}^I$ denotes the product policy.\nUnder a GS-MG, the goal of policy optimization is capturing a Coarse Correlated Equilibrium (CCE) (Definition 1), which allows agents' policies to be interdependent,"}, {"title": "Modelling Traffic Dynamics based on Offline Data", "content": "To represent the environmental dynamics, we introduce the generative world model, action predictor, and observation model based on the offline datasets $D_o$.\nFor computation efficiency, we follow the decoder-only archi-tecture in GPT [62] and implement the world model as a Auto-regressive Trajectory Decoder. As a sequential prediction model, our world model maps the previous state (e.g., $s_{t-1}$) and the action of each agent (e.g., $a_{1,t},..., a_{I,t}$) into the next state $s_t$. Unlike QCNet [50], which generates trajectories for future vehicle motions over a fixed period, our world model adopts an auto-regressive approach, predicting vehicles' step-wise motion based on its previous predictions. This approach enables modeling how past movements influence future decisions.\nUnder the context of Dec-POCMDP, when t = 0, $s_0$ captures the static game map M and initial features of all agents. when t > 0, $s_t = {z_{i,w}}_{w=t-W,i=0}^{At,I}$ captures the spatial-temporal information of all agents under the game map M in the previous t - 1 time steps, and $a_{1,t},..., a_{I,t}$ denotes the acceleration and heading of agents in the current time step. Under this setting, our decoder is implemented by 1) Agent-to-Map Cross Attention, 2) Agent-to-Temporal Cross Attention, 3) Agent-to-Neighbor Cross Attention and 4) Self-Agent Attention incorporates map information, historical information, the spatial-temporal features of the surrounding agents, and the features of the agent itself in the temporal dimension thereby mapping $s_t$ and $a_{1,...,I,t}$ to $s_{t+1}$. The details of implementation are illustrated in Figure 11. In addition, our world model also includes two other important modules:\nThe actor model predicts the actions (acceleration and heading) based on the state $s_t = {z_{i,w}}_{w=t-W}^{2t,I}$ to satisfy latent traffic rules in the realistic driving scenarios. The actor model $\\pi_{i,t}(a_{i,t}|o_{i,t})$ denotes the probability of the i's agent generates an action $a_t$ such that $max \\omega_p(a_{i,t}, b_{i,t}^k) = \\pi_{i,t}(a_{i,t}|o_{i,t})$ where (1) $\\omega$ denotes a learnable coefficient and 2) $\\rho_k$ denotes the k-th mixture component's Laplace density. We constrain the output actions within a reasonable range to prevent irrational driving behaviors such as sudden acceleration or deceleration, sharp turns, etc. In the subsequent RL fine-tuning stage, we sample the i-th agent's future trajectory as a weighted mixture of Laplace distributions by following [50, 63]:\n$\\pi'(\\tau) = \\prod_{t=1}^T \\pi_i(a_{i,t} | o_{i,t}) = \\prod_{t=1}^T\\sum_{k=1}^K \\rho (a_{it} | \\mu_{it}^k, b_{it}^k)$\nwhere $\\omega$ can effectively act as the weighting coefficients and $\\mu_{it}^k$ and $b_{it}^k$ characterize the mean position and the level of uncertainty of the i-th agent at the time step t.\nFor each agent, our observation model maps $s_t$ and $a_{it}$ into agent-specific observations ${o_i}_1^I$. The observation model is implemented by $o_{i,t} = f_{MLP} (\\hat{z}_t)$, where MLP is the Multilayer Perceptron. Besides, this module considers the historical actions and observation of all agents such that $h = {(o_{i,1}, a_{i,1})}_{i=0,I}^{t=0,i=1}$"}, {"title": "Recognizing CCEs in the General Sum Markov Games", "content": "As our environment is structured as a multi-player competitive game with rewards and costs specific to each agent, inspired by [40, 43], we consider a decentralized update of each agent's policy where we fix the rest I \u2013 1 agents' policy $\\pi_{-i}$ and train policy $\\pi_i$ to get the best response of agent i. In this work, we update $\\pi_i$ by iteratively optimizing the following objective:\n$\\pi^* = arg max_{\\pi_i} E_{\\pi_i,\\mu_0} [R_i(s) | \\pi_i, \\pi_{-i}(s)] - \\eta_1 B_\\varphi(\\pi_i, \\pi_{-i}) - \\frac{1}{\\eta_2} B_\\varphi(\\pi_i, \\pi^{-i}_{t-1})$\nwhere $\\pi$ denotes the imitation policy learned by the world model (Equation 9), and the $\\pi^{-i}_{t-1}$ denotes the policy learned from the previous iteration. This objective contains several key components that can efficiently facilitate convergence to a CCE by utilizing:"}, {"title": "Optimistic V-learning", "content": "Inspired by [57], our optimistic value function is defined by:\n$V_i^\\pi (s) = E_{\\mu_0, \\pi,\\pi_{-i}} [\\sum_t \\gamma^t [r_i(o_t, a_t) + B_i(o_t)] | o_0 = O(s_{t+1})]$\nwhere $\\Beta_i(o) = \\frac{p_i(o)}{c}$ serves as an exploration bonus to less visited state. c is a hyper-parameter and $\\rho_i(o)$ denotes the density of visited observation o [64], representing the probability of o occurrences at time t [65]. Such an optimistic V-learning objective served as an extension of the CCE-V-Learning algorithm [40, 38], which had been proven to converge to CCE under discrete environments, and we extend this algorithm to solve continuous decision-making problems."}, {"title": "Magnetic Mirror Descent (MMD)", "content": "We follow [43] and incorporate Bregman divergence $B_\\varphi(\\cdot, \\cdot)$ with respect to the mirror map $\\varphi$ such that $B_\\varphi(x, y) = \\psi(x) - \\psi(y) - (\\nabla\\psi(y), x - y)$ into the objective with convergence guarantees. Recent studies [44, 43, 41, 42] justified that the mirror decent approaches can solve different kinds of games in multi-player settings. To derive a more intuitive objective, we implement the mirror map as the negative entropy such that $\\psi(x) = \\sum \\rho(x) log \\rho(x)$, and the objective (12) becomes:\n$\\pi^* = arg max E_{\\pi,\\mu_0} [V_i^{t-j-1, \\pi_t}(s)] - \\eta_1 D_{kl}(\\pi_i||\\hat{\\pi}_i) - \\frac{1}{\\eta_2} D_{hkl}(\\pi_i||\\pi_i^{j-1})$\nwhere $D_{kl}$ is the KL-divergence of two variables. Intuitively, by punishing the distance between current policy $\\pi_i$ and imitation policy $\\hat{\\pi}_i$, this objective ensures the fidelity in the offline MARL problem (Definition 2). By constraining the scale of updates between current policy $\\pi_i$ and previous policy $\\pi_i^{-1}$, the training process becomes more stable. By default our objective considers the trajectory-generating $\\tau_i$ probability:\n$\\pi_i(\\tau_i) = \\mu_0(s_0) \\prod_{t=0}^{T-1} [T(s_{t+1}|s_t, a_t) \\pi_{i,t}(a_{i,t}|o_{i,t}) \\pi_{-i,t}(a_{-i,t}|o_{-i,t})]$\nHowever, both the transition function T and policy of other players $\\pi_{-i,t}$ are not subject to optimize in the objective (12), and thus recent studies [66, 67] often consider the discounted causal entropy [68] $\\sum_{t=0}^T \\gamma^t H[\\pi(a_{i,t}|o_{i,t})]$. Similarly, instead of utilizing the computationally intractable trajectory-level KL-divergence $D_{kl}(\\pi_i||\\pi^{-1})$, we consider the time-wise causal KL-divergence $\\sum_t D_{kl} [\\pi_{i,t}(\\cdot) ||\\pi_{i,t}^{i,t-1}(\\cdot)]$, and by substituting it and the equation (11) into the objective (12), we have:\n$max E_{\\pi_i} [\\sum_{t=0}^T (\\gamma^t r_i(o_t, a_t) + B_i(o_t) - \\eta_1 D_{kl} [\\pi_{it}(\\cdot) || \\hat{\\pi}_i(\\cdot)]) - \\frac{1}{\\eta_2} D_{kl} [\\pi_{it}(\\cdot) || \\pi_{i,t}^{i-1}(\\cdot)])]$\nwhere, for brevity, we denote $\\eta_{i,t}(a_{i,t}|o_{i,t})$ as $\\pi_{i,t}(\\cdot)$. This objective maximizes the rewards under the guarantee of fidelity and stability, which aligns well with the RL paradigm. Since $D_{kl}(x,y) = H(x, y) - H(x)$, objective (14) can be further derived as:\n$max E[\\sum_{t=0}^T (\\gamma^t r'(o_t, a_t) + \\frac{\\gamma^t}{\\eta} H[\\pi_{it}(\\cdot)])]$\nwhere for brevity, we denote $\\eta = \\frac{1}{\\eta_2} + \\eta_1$ and $r' (o_t, a_t) = r_i(o_t, a_t)+B_i(o_t)+E_{\\pi_i,t} [log(\\pi_i(\\pi^{-i}_{t}))]$.\nThis objective maximizes the entropy of learned policy $\\pi_{i,t}$, which aligns well with the RL paradigm."}, {"title": "Adjusting the Level of Competition among Heterogeneous Agents", "content": "To effectively simulate complex traffic scenarios that include various vehicle types such as cars, buses, and trucks\u2014and diverse driving styles, such as aggressive and conservative driving, it is crucial to tailor the behavior of each agent and regulate the level of competition in the scenarios created. By subjecting the AV control system to these varied scenarios, we can more thoroughly assess the system's robustness. This comprehensive evaluation helps in developing trustworthy AV vehicles capable of performing reliably in realistic traffic conditions.\nTo accurately represent diverse levels of scenarios characterized by different CCEs, we incorporate the constrained and risk-sensitive policy optimization into the multi-agent traffic simulation system."}, {"title": "Constrained Traffic Simulation", "content": "To dynamically adjust the intensity of CCEs, we request the agents to AV agents varying levels of driving constraints, thereby modulating the severity and nature of the driving conditions. Specifically, we expand the objective (15) by formulating the trade-off between rewards and costs under a constrained policy optimization objective:\n$arg max E_{\\pi,\\tau,\\mu_0} [\\sum_{t=0}^T (\\gamma^t r^* (o_{it}, a_{it}) + \\eta \\hat{H}[\\pi_{i,t}(\\cdot)])] s.t. E [\\sum_{t=0}^T \\gamma^t c(o_{it}, a_{it})] < \\epsilon$\nwhere c represents the cost function aligning to different constraints. In this study, we mainly explore how the distance constraint influences the resulting CCE from our algorithm. Additionally, we can set different vehicle distance constraints to achieve varying intensities of CCE. As the distance between vehicles increases, the competitiveness among agents decreases."}, {"title": "Risk-sensitive Traffic Simulation", "content": "This strategy explicitly manages the risk sensitivity of driving behaviors", "55": "we develop a risk-sensitive constraint to capture the uncertainty induced by environmental dynamics and extend the objective 16 as follows:\n$argmax E_{\\pi", "H[\\pi_{i,t}(\\cdot)": "s.t. P_a[\\sum_{t=0"}, "T C(O_{i,t}(s_t), A_{i,t})"], "a": "Gamma \\rightarrow [0", "56]": "n$Z^c(o_{i", "t})": "C(o_{i", "zero": "n$\\rho_{\\tau_a}(u) = |\\tau_a - \\delta_{{u<0}}|L_{\\kappa}(u) where L_{\\kappa}(u) = \\begin{cases} \\frac{1}{2}u^2, & if|u| \\leq \\kappa\\\\ \\kappa(|u| - \\frac{\\kappa}{2}), & otherwise \\end{cases}$\n$\\tau_q$ is the quantile, $\\delta$ denotes a Dirac and $L_{\\kappa}(u)$ is a"}