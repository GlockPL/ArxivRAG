{"title": "Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation", "authors": ["Marco Mistretta", "Alberto Baldrati", "Marco Bertini", "Andrew D. Bagdanov"], "abstract": "Vision-Language Models (VLMs) demonstrate remarkable zero-shot generalization to unseen tasks, but fall short of the performance of supervised methods in generalizing to downstream tasks with limited data. Prompt learning is emerging as a parameter-efficient method for adapting VLMs, but state-of-the-art approaches require annotated samples. In this paper we propose a novel approach to prompt learning based on unsupervised knowledge distillation from more powerful models. Our approach, which we call Knowledge Distillation Prompt Learning (KDPL), can be integrated into existing prompt learning techniques and eliminates the need for labeled examples during adaptation. Our experiments on more than ten standard benchmark datasets demonstrate that KDPL is very effective at improving generalization of learned prompts for zero-shot domain generalization, zero-shot cross-dataset generalization, and zero-shot base-to-novel class generalization problems. KDPL requires no ground-truth labels for adaptation, and moreover we show that even in the absence of any knowledge of training class names it can be used to effectively transfer knowledge. The code is publicly available at https://github.com/miccunifi/KDPL.", "sections": [{"title": "1 Introduction", "content": "Vision-Language Models (VLMs) like Contrastive Language-Image Pre-training (CLIP) are remarkably effective at zero-shot generalization to downstream tasks [2, 40, 42, 63]. These models leverage a dual encoder architecture and are trained to align image and text features in a shared embedding space.\nCLIP and similar models are able to perform zero-shot classification by predicting the output class based on the similarity between the test image embedding and text embeddings of words from a fixed vocabulary."}, {"title": "2 Related Work", "content": "In this section we review the recent literature most relevant to our contribution.\nVision-Language Models. Vision-Language Models (VLMs) learn robust visual representations thanks to their extensive pre-training on image-caption datasets [45, 46]. These representations are very effective at generalizing to a variety of downstream tasks [12, 27, 42]. In contrast to vision models trained only with image supervision, vision-language models can interpret both visual and textual data, showing improvement in tasks requiring cross-model understanding [2, 3, 49]. We focus on contrastively-trained VLMs such as CLIP [42], ALIGN [27], and LiT [58]. These models use a dual encoder architecture and contrastive learning to align images and text in a common embedding space.\nThanks to this shared embedding space and massive pre-training datasets these"}, {"title": "3 Knowledge Distillation Prompt Learning", "content": "We first introduce preliminary concepts related to prompt learning and then describe our approach to applying knowledge distillation to the problem."}, {"title": "3.1 Preliminaries", "content": "Our approach is based on knowledge distillation applied to prompt learning. Here we discuss the base teacher and student models (CLIPs) and four state-of-the-art prompt learning approaches into which we will incorporate KDPL.\nCLIP. Contrastive Language-Image Pre-training (CLIP) is a vision-language model trained to align images and textual captions in shared semantic space [42]. CLIP consists of an image encoder $f_\\theta$ and a text encoder $g_\\phi$. Given an image $I$, the image encoder computes its feature representation $f_\\theta(I) \\in \\mathbb{R}^d$, where $d$ is the size of the semantic embedding space. Similarly, for a given textual caption $Y$, a word embedding layer $E_L$ maps each tokenized word to the token embedding space $W$. Then, the text encoder $g_\\phi$ generates the textual feature representation $g_\\phi(E_L(Y)) \\in \\mathbb{R}^d$. The main goal of CLIP training is to learn $\\theta$ and $\\phi$ such that $f_\\theta(I) \\approx g_\\phi(E_L(Y))$ for associated image/text pairs $(I, Y)$.\nWhen using a Vision Transformer (ViT) [15] as the visual encoder $f_\\theta$, the encoding process begins by splitting the image into $U$ fixed-size patches. These patches are then projected into patch embeddings {$w_1, w_2, ..., w_U$}, where each $w_i$ belongs to the patch embedding space $V$. A learnable class (CLS) token $c_i$ is concatenated with the patch embeddings, resulting in the input to the vision transformer being {$c_i, w_1, w_2, ..., w_U$}. Finally, the CLS token of the final transformer layer is projected to the shared embedding space via a linear projection to obtain the final representation.\nTo perform zero-shot classification using CLIP, we start with an image $I$ and build a set of textual prompts {$Y_i$}$_{i=1}^C$, where $C$ denotes the number of classes. Each handcrafted text prompt $Y_i$ takes the format \u201ca photo of a [CLASSi]\", where CLASSi represents a specific class name, such as airplane, bird, etc. Then the feature representations $\\psi_I = f_\\theta(I)$ and $\\psi_Y = g_\\phi(E_L(Y_i))$ are extracted using the CLIP encoders. The predicted probability for each class is:\n$$p(y = i|I) = \\frac{exp(cos(\\psi_i, \\psi_I)/\\tau)}{\\Sigma_{j=1}^C exp(cos(\\psi_i, \\psi_I)/\\tau)},$$\nwhere $cos(\\cdot, \\cdot)$ denotes the cosine similarity and $\\tau$ is a temperature hyperparameter.\nPrompt-Learning Techniques. Here we summarize how the state-of-the-art techniques for textual, visual, and multimodal prompt learning work.\nCoOp [65] is a textual prompt learning technique that learns continuous context tokens (i.e. the learnable prompt) in the CLIP token embedding space. Specifically, CoOp introduces $M$ learnable vectors, {$v_1, v_2,..., v_M$}, where each context vector $v_i \\in W$. For each of the $k$ classes of a dataset, the input to the text encoder is {$v_1, v_2, ..., v_M, c_k$}, where $c_k = E_L([CLASS_k])$.\""}, {"title": "3.2 Label Agnostic Prompt Learning", "content": "The methods described above all rely on ground-truth labels during adaptation. Here we show how unsupervised knowledge distillation can be used instead.\nOverview. Our proposed approach, which we call Knowledge Distillation Prompt Learning (KDPL), is a general method designed to enhance the performance of the CLIP model on downstream tasks through parameter-efficient prompt learning. Unlike previous approaches [29, 64, 65], which rely on labeled examples for training, KDPL eliminates the need for manually-labeled samples by learning only through knowledge distillation from a larger and more powerful VLM. Note that KDPL is a method that can be seamlessly integrated with any existing prompt learning approach in scenarios where no information about class names or labels is available.\nWe validate KDPL in two progressively challenging supervision regimes. In the label agnostic scenario we do not use ground-truth labels, but we assume knowledge of the class names in the training dataset. In the class agnostic scenario (see Section 3.3) we go one step further and assume that even the training class names are unknown. For this class agnostic scenario we propose an effective and efficient online strategy for automatically filtering the classes from a large dictionary of approximately 20K class names [32].\nPrompt Learning via Unsupervised Knowledge Distillation. Given a lightweight CLIP model (the student) and a larger, more powerful CLIP model (the teacher), we aim to improve the downstream performance of the student model by distilling knowledge from teacher to student. For an image $I$ and a"}, {"title": "3.3 Class Agnostic Prompt Learning", "content": "To further evaluate the generalization capabilities KDPL, we introduce a scenario where not only do we not know the labels of training images (label agnostic, Section 3.2) but where we also do not even know the class names associated with the dataset (class agnostic). This scenario is considerably more challenging as we make no assumptions about the few-shot training data.\nTo address the unavailability of class names, we propose a strategy for automatically selecting a set of class names for each batch. We start with a large vocabulary of class names from which to select. Specifically, we use the Open Images V7 dataset [32], which contains ~20K classes. The most straightforward method would simply use all 20K classes during training. However, this is impractical as the memory required by prompt learning methods increases linearly with the number of classes. According to Ren et al. [44], CoOp requires nearly 15MB of VRAM per class, resulting in approximately 300GB of memory when multiplied by 20K classes. Therefore, we propose a method to automatically select which classes to use for each batch by retaining only the ones most relevant to the calculation of the loss in Eq. (2).\nGiven a batch of images $X = ${$I_i$}$_{i=1}^N$ and all the class names in the vocabulary $C = ${$CLASS_i$}$_{i=1}^C$, where $N$ represents the number of images in a batch and $C$ the size of the dictionary, we let the teacher model select the most relevant classes. Our objective is to identify the most useful $K$ classes in each batch for student prompt learning. After extracting the teacher image and text features, for each image $I_i \\in X$, we apply Eq. (1) to obtain the teacher probabilities $p_T(I_i, C)$. By stacking the probabilities along the batch dimension, we obtain the matrix $P_T = [p_T(I_1, C); \u00b7\u00b7\u00b7 ; p_T(I_N, C)]^T \\in \\mathbb{R}^{N\u00d7C}$, where the $i$-th row corresponds to the probabilities associated with image $I_i$. We then compute the average probabilities along the batch axis, resulting in $\\bar{P_T} \\in \\mathbb{R}^C$. Finally, we select the classes corresponding to the $K$ highest values in $\\bar{P_T}$.\nUsing the teacher model to perform this class filtering for each batch is feasible and does not incur excessive memory costs since the teacher requires no gradient computation. Therefore, the memory consumption does not depend on the number of classes in $C$. Conversely, although the student model remains frozen, gradients must still be propagated to update the prompt parameters $\\gamma$. Once the classes are selected, the training strategy remains the same as described above, with the only difference being that the class names observed by the student vary in each batch based on teacher predictions."}, {"title": "4 Experimental Results", "content": "In this section we report on experiments performed to validate our proposed approach."}, {"title": "4.1 Evaluated Scenarios and Implementation Details", "content": "Following previous works [29, 64, 65], we validate KDPL in three distinct settings: 1) domain generalization; 2) cross-dataset transfer; 3) generalization to unseen classes. Additionally, to evaluate the scenario where class names are also unknown, we introduce a new evaluation setting we call class agnostic adaptation.\nWe use the train/val/test splits and seeds provided by Zhou et al. [65] for all datasets.3 All reported results are averages over three independent runs.\nEvaluated Scenarios. We evaluate KDPL on the following scenarios:\nDomain Generalization. To assess the ability of the learned prompts to generalize to out-of-distribution datasets, we apply the prompt learned from ImageNet [11] to four different versions of ImageNet datasets exhibiting various types of domain shift. The target datasets are ImageNetV2 [43], ImageNet-Sketch [51], ImageNet-A [24], and ImageNet-R [23].\nCross-dataset Transfer. To evaluate the ability of learned prompts to generalize to unseen classes, we evaluate the same prompt trained on ImageNet on a broad range of downstream recognition tasks. We validate on ten datasets with varying characteristics: Caltech101 [19] for general object classification; OxfordPets [41], StanfordCars [31], Flowers102 [39], Food101 [5], and FGVCAircraft [37] for fine-grained classification; the Describable Textures Dataset (DTD) [10] for texture classification; EuroSAT [22] for satellite-image recognition; and UCF101 [50] for action recognition.\nGeneralization to Unseen Classes. To evaluate how learned prompts generalize to unseen classes from the same dataset, we divide classes into two subsets: base and novel classes. The prompt is trained exclusively on the base classes and then tested on the novel ones. We evaluate on ImageNet as well as the ten benchmark datasets used for cross-dataset evaluation.\nClass Agnostic Adaptation. We introduce a novel evaluation setting in which the training class names are unknown. The prompt is trained on ImageNet and tested on all the benchmark datasets used in the domain generalization and cross-dataset evaluations.\nImplementation Details. We use a CLIP model with a ViT-H-14 visual backbone as the teacher model.4 For student models, we evaluate both on a CLIP model based on ResNet-50 and a model based on ViT-B/32. By experimenting with both ResNet-based and ViT-based CLIP models we show the architecture independence of our approach."}, {"title": "4.2 Results on Domain Generalization", "content": "Table 1 outlines the performance in the domain generalization setting. We report the performance of each baseline method alongside their unsupervised"}, {"title": "4.3 Results on Cross-dataset Transfer", "content": "In Table 2 we present the results of the cross-dataset evaluation setting in which the prompt is trained on ImageNet and tested on ten different target datasets. For all datasets our KDPL-based variants consistently outperform the corresponding baselines and demonstrate superior generalization performance. The greater generalization capabilities of KDPL are evident even for fine-grained datasets like EuroSAT, on which CoOp+KDPL achieves an 8% improvement over the baseline CoOp when using ResNet-50 as the backbone. Although adding"}, {"title": "4.4 Results on Generalization to Unseen Classes", "content": "In Table 3 we give results for the unseen class generalization task. In these experiments each dataset is split into 50% of the classes as a base for training few-shot adaptation, and the remaining 50% as new classes on which zero-shot performance is evaluated. KDPL consistently outperforms the corresponding baseline methods for both backbones, demonstrating improvement in all scenarios for the majority of the datasets. On average, the performance improvement over the supervised baseline methods ranges from about 1% for VPT to about 3% for CoOp with the ResNet-50 backbone. See the Supplementary Material for further analysis of base and combined performance."}, {"title": "4.5 Results on Class Agnostic Adaptation", "content": "Figure 3(a-b) summarizes the main results in the proposed Class-Agnostic (CA) scenario in which even the training class names are unknown at training time. We report the accuracy on the ImageNet dataset (source), as well as the average accuracy in domain generalization and cross-dataset settings. Note that, even without knowing the class names, the performance on the source dataset steadily improves compared to the zero-shot CLIP model. Moreover, the prompts learned"}, {"title": "4.6 Ablation Study", "content": "We performed a range of ablation studies to evaluate all aspects of KDPL. All ablations reported here are averages over three independent runs using CoOp as the baseline approach and ResNet-50 as the student backbone. The prompt is learned on ImageNet and evaluated on the benchmark datasets used in the domain generalization (AVG\u00b9) and cross-dataset (AVG\u00b2) evaluations."}, {"title": "5 Conclusion", "content": "In this paper we proposed an approach to Knowledge Distillation Prompt Learning (KDPL) which is easily integrated into existing supervised prompt learning methods. Our experiments show that for CoOp, CoCoOp, VPT, and MaPLe adding KDPL: (1) renders them label agnostic by eliminating the need for ground-truth labels for few-shot adaptation; (2) can also render them class agnostic in cases where no knowledge of training class labels is available; and (3) remarkably improves generalization to downstream tasks.\nThe additional computation cost incurred by distilling from a large VLM is a limitation of KDPL, though all encoder parameters are fixed and the predictions of the teacher can be precomputed, which helps mitigate the extra cost. This extra computation only amounts to about a 15% increase for MaPLe.\nNote that we did not tune any hyperparameters when training the KDPL variants. We use the same settings as in the original papers, which are likely suboptimal for distillation-based adaptation. With careful tuning, there is potential for improvement. Our experiments indicate that distillation-learned prompts are more transferable, and we think it would be interesting to see if this idea can generalize to very different downstream tasks and scale to even bigger teacher models or - more interestingly - to even smaller student models."}, {"title": "Appendix", "content": "A. Overview\nThis document contains Supplementary Material that provides additional details to the main paper and further experimental analysis. The contents are organized as follows:\nAdditional Implementation Details (Appendix 5): This section provides additional implementation details, including hyperparameters and configuration settings for reproducing all reported results and additional details about the evaluated datasets.\nAdditional Results: Generalization to Unseen Classes (Appendix 5): Here we present additional experimental results for the scenario of generalization from base to unseen classes within the same dataset, including the downstream performance of few-shot base and zero-shot unseen classes.\nAdditional Results: Class Agnostic Scenario (Appendix 5): This section provides additional results for the class-agnostic setting, our proposed new setting in which we know neither the labels nor the names of the training classes.\nB. Additional Implementation Details\nIn this section we provide further details on hyperparameters of the proposed approaches presented in the main paper and additional details on the benchmark datasets evaluated.\nHyperparameter settings. For all evaluated scenarios we recalculated all the baselines results using the parameter and hyperparameter settings reported in the official publications [28, 29, 64, 65]. In Table 5 we summarize all configuration settings. Notably, all the evaluated baselines use a cosine scheduler with one warm-up epoch with a constant learning rate of 1e-5, and all authors suggest initializing the textual context with \"a photo of a\" as a better starting point. Note that our variants augmented with Knowledge Distillation Prompt Learning (KDPL) were trained using exactly the same parameters and configuration settings as the baseline methods. Therefore, it is reasonable to expect that further hyperparameter tuning could potentially improve our performance even further.\nIn addition, all the baselines \u2013 and so also our augmented versions - adhere to the same preprocessing pipeline, in which input images are resized to (224, 224) using bicubic interpolation. Data augmentations including random resized crop, random flip, and normalization are employed to ensure robustness across all experiments.\nC. Additional Results: Generalization to Unseen Classes\nIn the main paper, due to brevity and to maintain focus on the core aspects of our method, we reported only the performance metrics on unseen classes. In this section we provide the performance metrics on base classes as well as the harmonic mean of both base and unseen classes to provide a more comprehensive perspective on our performance in this scenario.\nIn Table 7, we observe that the baseline methods without KDPL outperform our approach in few-shot transfer learning on base classes. Thanks to the supervision label signal, the baselines can exploit the ground-truth source training classes. Without using KDPL, however, all the baseline methods suffer from forgetting of the unseen classes. In particular, we see that increasing the number of training base-shots overall causes a deterioration in the unseen classes. Certain methods in particular suffer from this phenomenon, typical of an Incremental Learning setting (i.e. in Figure 4 we see that the average performance\nof MaPLe on unseen classes is consistently below the average zero-shot unseen performance). Applying our method instead exhibits a more favorable trend in zero-shot transfer learning on unseen classes, alleviating the forgetting of unseen classes. Our method is preferable in scenarios where labeled data for base classes are not available, and improvement in base class performance is desired without compromising zero-shot transfer performance on unseen classes.\nD. Additional Results: Class Agnostic Scenario\nIn the main paper we did not include all results for the class agnostic setting due to space limitations, but only reported the average domain generalization performance and the average cross-dataset performance. Here, in Table 5 we provide all class-agnostic results comparing our proposed method (CA-KDPL) against the baselines. As mentioned in the main paper, even without knowing either the labels or the training class names, our proposed unsupervised method is competitive, and sometimes even outperforms the baselines."}]}