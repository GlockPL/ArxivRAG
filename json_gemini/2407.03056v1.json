{"title": "Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation", "authors": ["Marco Mistretta", "Alberto Baldrati", "Marco Bertini", "Andrew D. Bagdanov"], "abstract": "Vision-Language Models (VLMs) demonstrate remarkable zero-shot generalization to unseen tasks, but fall short of the performance of supervised methods in generalizing to downstream tasks with limited data. Prompt learning is emerging as a parameter-efficient method for adapting VLMs, but state-of-the-art approaches require annotated samples. In this paper we propose a novel approach to prompt learning based on unsupervised knowledge distillation from more powerful models. Our approach, which we call Knowledge Distillation Prompt Learning (KDPL), can be integrated into existing prompt learning techniques and eliminates the need for labeled examples during adaptation. Our experiments on more than ten standard benchmark datasets demonstrate that KDPL is very effective at improving generalization of learned prompts for zero-shot domain generalization, zero-shot cross-dataset generalization, and zero-shot base-to-novel class generalization problems. KDPL requires no ground-truth labels for adaptation, and moreover we show that even in the absence of any knowledge of training class names it can be used to effectively transfer knowledge. The code is publicly available at https://github.com/miccunifi/KDPL.", "sections": [{"title": "1 Introduction", "content": "Vision-Language Models (VLMs) like Contrastive Language-Image Pre-training (CLIP) are remarkably effective at zero-shot generalization to downstream tasks [2, 40, 42, 63]. These models leverage a dual encoder architecture and are trained to align image and text features in a shared embedding space.\nCLIP and similar models are able to perform zero-shot classification by predicting the output class based on the similarity between the test image embedding and text embeddings of words from a fixed vocabulary.\nTo address these challenges, parameter-efficient prompt learning is emerging as a promising and non-destructive approach to adapting VLMs to downstream tasks [33, 34, 62]. Zhou et al. [65] proposed a text-only prompt learning approach called CoOp to overcome the limitations of manually-crafted prompts. Subsequently, Jia et al. [28] extended prompt learning to the visual domain with Visual Prompt Tuning (VPT), and Khattaki et al. [29] proposed a multi-modal prompt learning approach called MaPLe. Although significantly improving over carefully tuned, hand-crafted prompts, these state-of-the-art techniques all require annotated samples from the dataset and have problems generalizing to other datasets or classes unseen during training [29, 64]."}, {"title": "Improving Learned Prompts via Unsupervised Knowledge Distillation", "content": "To eliminate the need for labeled training examples and improve the generalization of learned prompts, we propose a novel approach to prompt learning which we call Knowledge Distillation Prompt Learning (KDPL). KDPL adapts lightweight VLMs and improves performance on downstream tasks by distilling knowledge from a more powerful VLM without the need for annotated examples. KDPL is a flexible method that can be seamlessly integrated into existing prompt learning framework such as CoOp [65], CoCoOp [64], MaPLe [29], and VPT [28]. Figure 1 summarizes the motivations and illustrates the workflow of the proposed approach. We validate KDPL on two increasingly challenging scenarios: in the first, we assume knowledge of training class names, while in the second we assume no knowledge of training classes. Importantly, in both scenarios no annotated training samples are used.\nOur contributions are:\nwe propose a novel parameter-efficient prompt-learning approach that eliminates the need for labeled training examples by distilling knowledge from a large Vision-Language Model;\nwe show that our approach can be integrated into existing prompt learning techniques to learn visual, textual, and multimodal prompts which generalize significantly better than state-of-the-art baselines to unseen classes in downstream tasks;\nwe demonstrate the effectiveness of our approach through extensive experiments on more than ten standard benchmark datasets covering a broad range of downstream problems, including domain generalization, cross-dataset adaptation, and base-to-novel generalization; and\nwe introduce a new, class agnostic evaluation setting in which training class names are unknown and show the superiority of our approach in this scenario.\nTo the best of our knowledge ours is the first approach to parameter-efficient VLM adaptation which is applicable in scenarios that are label agnostic (i.e. no label supervision is required during adaptation) and also in scenarios that are additionally class agnostic (i.e. no knowledge of training class names is assumed)."}, {"title": "2 Related Work", "content": "In this section we review the recent literature most relevant to our contribution.\nVision-Language Models. Vision-Language Models (VLMs) learn robust visual representations thanks to their extensive pre-training on image-caption datasets [45, 46]. These representations are very effective at generalizing to a variety of downstream tasks [12, 27, 42]. In contrast to vision models trained only with image supervision, vision-language models can interpret both visual and textual data, showing improvement in tasks requiring cross-model understanding [2, 3, 49]. We focus on contrastively-trained VLMs such as CLIP [42], ALIGN [27], and LiT [58]. These models use a dual encoder architecture and contrastive learning to align images and text in a common embedding space.\nThanks to this shared embedding space and massive pre-training datasets these"}, {"title": "Improving Learned Prompts via Unsupervised Knowledge Distillation", "content": "models achieve remarkable performance. However, there is still a gap between the zero-shot capabilities of these VLMs and the performance of tailored state-of-the-art models [14, 42], and thus there is active research on efficient and effective methods to adapt VLMs to downstream tasks [20, 36, 52, 59].\nPrompt Learning. The performance of VLMs is highly dependent on textual prompt used to characterize downstream tasks. Given the significant impact of small changes in wording on performance, the manual generation of optimal textual prompts is a challenging task. Inspired by developments in NLP [33, 34, 62], CoOp [65] proposes to learn continuous vectors in the word embedding space instead of using tailored hand-crafted prompts.\nThe promising results of CoOp have attracted considerable interest in prompt learning for VLMs [1, 6, 7, 29, 30, 35, 47, 55]. CoCoOp, in particular, highlights the poor performance of CoOp on unseen classes and uses image-conditional dynamic prompts to improve generalization [64]. PLOT [7] instead is based on learning multiple prompts by exploiting local visual features. KgCoOp [55] adds a regularization loss to minimize the discrepancy between learned and hand-crafted prompts, while VPT [28] adapts prompt learning to the visual domain by learning continuous vectors in the input space of a ViT [15]. Finally, MaPLe [29] introduces a multimodal prompt learning strategy for VLMs. By tightly coupling vision-language prompts, MaPLe promotes synergistic learning while discouraging the collapse to independent unimodal solutions.\nSimilar to the approach we propose in this paper, UPL [26] performs prompt learning without relying on annotated samples. It leverages pseudolabels derived from a larger CLIP model and selects the top-K most confident samples per class to construct a balanced set of pseudo-labels. UPL requires a substantial collection of unlabeled images and is not directly applicable when only a few unlabeled samples are available, like in the few-shot adaptation scenarios considered here. In contrast to UPL, KDPL does not use pseudolabeling; instead, we directly learn from the logits of a CLIP teacher model via knowledge distillation, thus eliminating the need for any selection strategy of the training samples.\nKnowledge Distillation. Knowledge distillation (KD) is a machine learning technique in which a simple model (student) is trained to mimic the behavior of a larger model (teacher) by learning from its output or intermediate activation [25]. This technique has found success in many contexts, including image classification [4, 8, 25], self-supervised learning [18, 54], and image/video segmentation [16, 21, 38, 48], leading to improvements in model compression, computational efficiency, and performance. DML [60] introduces a mutual learning approach to train students and teachers simultaneously. DKD [61] reformulates the classical KD loss into a term related to the target class and a term related to the non-target classes. In contrast to the majority of knowledge distillation approaches [56, 57, 60, 61], we do not utilize any labels during training. Instead, we solely employ the distillation loss. In our work, we apply knowledge distillation in a parameter-efficient prompt learning scenario. Specifically, we distill knowledge from a large and powerful VLM (teacher) into a lightweight VLM (student) by updating only a reduced number of parameters (the prompt)."}, {"title": "3 Knowledge Distillation Prompt Learning", "content": "We first introduce preliminary concepts related to prompt learning and then describe our approach to applying knowledge distillation to the problem.\n3.1 Preliminaries\nOur approach is based on knowledge distillation applied to prompt learning. Here we discuss the base teacher and student models (CLIPs) and four state-of-the-art prompt learning approaches into which we will incorporate KDPL.\nCLIP. Contrastive Language-Image Pre-training (CLIP) is a vision-language model trained to align images and textual captions in shared semantic space [42]. CLIP consists of an image encoder $f_{\\theta}$ and a text encoder $g_{\\phi}$. Given an image $I$, the image encoder computes its feature representation $f_{\\theta}(I) \\in \\mathbb{R}^{d}$, where $d$ is the size of the semantic embedding space. Similarly, for a given textual caption $Y$, a word embedding layer $E_{L}$ maps each tokenized word to the token embedding space $W$. Then, the text encoder $g_{\\phi}$ generates the textual feature representation $g_{\\phi}(E_{L}(Y)) \\in \\mathbb{R}^{d}$. The main goal of CLIP training is to learn $\\theta$ and $\\phi$ such that $f_{\\theta}(I) \\approx g_{\\phi}(E_{L}(Y))$ for associated image/text pairs $(I, Y)$.\nWhen using a Vision Transformer (ViT) [15] as the visual encoder $f_{\\theta}$, the encoding process begins by splitting the image into $U$ fixed-size patches. These patches are then projected into patch embeddings {$w_{1}, w_{2}, ..., w_{U}$}, where each $w_{i}$ belongs to the patch embedding space $V$. A learnable class (CLS) token $c_{i}$ is concatenated with the patch embeddings, resulting in the input to the vision transformer being {$c_{i}, w_{1}, w_{2}, ..., w_{u}$}. Finally, the CLS token of the final transformer layer is projected to the shared embedding space via a linear projection to obtain the final representation.\nTo perform zero-shot classification using CLIP, we start with an image $I$ and build a set of textual prompts {$Y_{i}$}$_{i=1}^{C}$, where $C$ denotes the number of classes. Each handcrafted text prompt $Y_{i}$ takes the format \u201ca photo of a [CLASS${i}$]\u201d, where CLASS${i}$ represents a specific class name, such as airplane, bird, etc. Then the feature representations $\\psi_{I} = f_{\\theta}(I)$ and $\\psi_{I} = g_{\\phi}(E_{L}(Y_{i}))$ are extracted using the CLIP encoders. The predicted probability for each class is:\n$$p(y = i|I) = \\frac{exp(cos(\\psi_{i},\\psi_{I})/\\tau)}{\\Sigma_{i'=1}^{C} exp(cos(\\psi_{i'},\\psi_{I})/\\tau)},$$\nwhere $cos(\\cdot,\\cdot)$ denotes the cosine similarity and $\\tau$ is a temperature hyperparameter.\nPrompt-Learning Techniques. Here we summarize how the state-of-the-art techniques for textual, visual, and multimodal prompt learning work.\nCoOp [65] is a textual prompt learning technique that learns continuous context tokens (i.e. the learnable prompt) in the CLIP token embedding space. Specifically, CoOp introduces $M$ learnable vectors, {$v_{1}, v_{2}, ..., v_{M}$} where each context vector $v_{i} \\in W$. For each of the k classes of a dataset, the input to the text encoder is {$v_{1}, v_{2}, ..., v_{M}, c_{k}$}, where $c_{k}$ = $E_{L}$([CLASS${k}$])."}, {"title": "M. Mistretta et al.", "content": "CoCoOp [64] extends CoOp by incorporating a lightweight network $h_{\\omega}$. Each context token is obtained as $v_{i}(I) = v_{i} + \\pi$, where $\\pi = h_{\\omega}(I)$. This method ensures that the textual prompts are conditioned on the input image as well.\nVPT [28] is a visual prompt learning method that can be viewed as the counterpart of CoOp in the visual domain. Unlike CoOp, which operates entirely in the textual token embedding space $W$, VPT performs prompt learning in the visual patch embedding space $V$. Specifically, VPT learns P visual tokens, leading to the input to the ViT being {$\\approx_{1},..., \\approx_{P}, c_{i}, w_{1}, w_{2},...,\\omega_{U}$}. VPT offers two prompting variants: deep and shallow. The deep variant learns a distinct prompt for each ViT layer, while the shallow variant incorporates the prompt parameters only into the input of the first layer.\nMaPLe [29] is a deep multi-modal prompt learning technique that promotes strong coupling between vision and language prompts. In practice, MaPLe learns different textual context tokens for each layer in $g_{\\phi}$. The visual prompts, on the other hand, are not directly learned but are obtained through a linear mapping from the textual ones.\nNote that methods involving visual prompt learning like VPT [28] and MaPLe [29] can only be applied to VLMs equipped with a ViT-based image encoder."}, {"title": "3.2 Label Agnostic Prompt Learning", "content": "The methods described above all rely on ground-truth labels during adaptation. Here we show how unsupervised knowledge distillation can be used instead.\nOverview. Our proposed approach, which we call Knowledge Distillation Prompt Learning (KDPL), is a general method designed to enhance the performance of the CLIP model on downstream tasks through parameter-efficient prompt learning. Unlike previous approaches [29, 64, 65], which rely on labeled examples for training, KDPL eliminates the need for manually-labeled samples by learning only through knowledge distillation from a larger and more powerful VLM. Note that KDPL is a method that can be seamlessly integrated with any existing prompt learning approach in scenarios where no information about class names or labels is available.\nWe validate KDPL in two progressively challenging supervision regimes. In the label agnostic scenario we do not use ground-truth labels, but we assume knowledge of the class names in the training dataset. In the class agnostic scenario (see Section 3.3) we go one step further and assume that even the training class names are unknown. For this class agnostic scenario we propose an effective and efficient online strategy for automatically filtering the classes from a large dictionary of approximately 20K class names [32].\nPrompt Learning via Unsupervised Knowledge Distillation. Given a lightweight CLIP model (the student) and a larger, more powerful CLIP model (the teacher), we aim to improve the downstream performance of the student model by distilling knowledge from teacher to student. For an image $I$ and a"}, {"title": "Improving Learned Prompts via Unsupervised Knowledge Distillation", "content": "set of classes $C$ = {CLASS$_{i}$}$_{i=1}^{C}$, we start by performing zero-shot classification using the frozen teacher model. Specifically, we use the teacher image encoder $f_{\\beta}$ and text encoder $g_{\\beta}$ to compute the teacher image features $\\Psi_{I,\\beta} = f_{\\beta}(I)$ and text features $\\psi_{I,\\beta}$ = $g_{\\beta}(E_{L}$([CLASS$_{i}$])). For the teacher model we use the fixed text prompt \"a photo of [CLASS]\". We then apply Eq. (1) to produce the probabilities $p_{T}(I,C)$ predicted by the teacher on image $I$ for classes $C$.\nThe teacher model does not rely on a learnable prompt and its predictions remain fixed during training. Our aim is to learn text and image prompts for the student model that enhance its generalization to downstream tasks. We denote with $f_{\\alpha}$ and $g_{\\gamma}$ the student image and text encoders, respectively, and with $\\gamma$ the parameters associated with the learnable student prompts (see Fig. 2).\nGiven the same image $I$ processed by the teacher and the same set of classes $C$, the student extracts image features $\\psi_{I,s}$ = $f_{\\alpha,\\gamma}(I)$ and text features $\\psi_{I,s}$ = $g_{\\gamma,\\gamma}(E_{L}$([CLASS$_{i}$])). Note that the text and image encoders can both depend on the prompt parameters $\\gamma$. According to the prompt learning technique used, the $\\gamma$ parameters can be used only by the text encoder (CoOp, CoCoOp), the visual encoder (VPT), or both (MaPLe). Finally, using Eq. (1) we produce student class probabilities $p_{S}(I,C)$ predicted on image $I$ for classes $C$. Note that all encoder parameters except for the learnable prompt $\\gamma$ are frozen during training.\nWe use the symmetric KL-divergence between the teacher ($p_{T}(I,C)$) and the student ($p_{S}(I,C)$) probabilities in a distillation loss:\n$$L(I, C) = [D_{KL}(p_{T}(I,C)||p_{S}(I,C)) + D_{KL}(p_{S}(I,C) ||p_{T}(I,C))],$$\nwhere $D(q || p)$ is the asymmetric KL-divergence between the discrete probability distributions $p$ and $q$:\n$$D_{KL} (p || q) = \\sum_{i \\in C} p_{i} log(\\frac{p_{i}}{q_{i}}).$$\nThis distillation loss depends only on the fixed predictions of the teacher, the prompt-conditioned predictions of the students, and the set of classes $C$"}, {"title": "Improving Learned Prompts via Unsupervised Knowledge Distillation", "content": "Importantly, our distillation loss does not assume any knowledge of class labels for image $I$. Nor does it require that $C$ be the classes of the downstream task that is, KDPL can be used for Label Agnostic and Class Agnostic adaptation scenarios. We found in early experiments that the symmetric KL-divergence works slightly better than either asymmetric option (see Section 4.6 for an ablation study on this choice).\n3.3 Class Agnostic Prompt Learning\nTo further evaluate the generalization capabilities KDPL, we introduce a scenario where not only do we not know the labels of training images (label agnostic, Section 3.2) but where we also do not even know the class names associated with the dataset (class agnostic). This scenario is considerably more challenging as we make no assumptions about the few-shot training data.\nTo address the unavailability of class names, we propose a strategy for automatically selecting a set of class names for each batch. We start with a large vocabulary of class names from which to select. Specifically, we use the Open Images V7 dataset [32], which contains ~20K classes. The most straightforward method would simply use all 20K classes during training. However, this is impractical as the memory required by prompt learning methods increases linearly with the number of classes. According to Ren et al. [44], CoOp requires nearly 15MB of VRAM per class, resulting in approximately 300GB of memory when multiplied by 20K classes. Therefore, we propose a method to automatically select which classes to use for each batch by retaining only the ones most relevant to the calculation of the loss in Eq. (2).\nGiven a batch of images $X$ = {$I_{i}$}$_{i=1}^{N}$ and all the class names in the vocabulary $C$ = {CLASS$_{i}$}$_{i=1}^{C}$, where $N$ represents the number of images in a batch and $C$ the size of the dictionary, we let the teacher model select the most relevant classes. Our objective is to identify the most useful K classes in each batch for student prompt learning. After extracting the teacher image and text features, for each image $I_{i}$ \u2208 $X$, we apply Eq. (1) to obtain the teacher probabilities $p_{T}(I_{i}, C)$. By stacking the probabilities along the batch dimension, we obtain the matrix $P_{T}$ = [$p_{T}(I_{1},C); \u00b7\u00b7\u00b7 ; p_{T}(I_{N},C)]^{T}$ \u2208 $\\mathbb{R}^{N\u00d7C}$, where the i-th row corresponds to the probabilities associated with image $I_{i}$. We then compute the average probabilities along the batch axis, resulting in $P_{T}$ \u2208 $\\mathbb{R}^{C}$. Finally, we select the classes corresponding to the K highest values in $P_{T}$.\nUsing the teacher model to perform this class filtering for each batch is feasible and does not incur excessive memory costs since the teacher requires no gradient computation. Therefore, the memory consumption does not depend on the number of classes in C. Conversely, although the student model remains frozen, gradients must still be propagated to update the prompt parameters $\\gamma$. Once the classes are selected, the training strategy remains the same as described above, with the only difference being that the class names observed by the student vary in each batch based on teacher predictions."}, {"title": "4 Experimental Results", "content": "In this section we report on experiments performed to validate our proposed approach.\n4.1 Evaluated Scenarios and Implementation Details\nFollowing previous works [29, 64, 65], we validate KDPL in three distinct settings: 1) domain generalization; 2) cross-dataset transfer; 3) generalization to unseen classes. Additionally, to evaluate the scenario where class names are also unknown, we introduce a new evaluation setting we call class agnostic adaptation. We use the train/val/test splits and seeds provided by Zhou et al. [65] for all datasets. All reported results are averages over three independent runs.\nEvaluated Scenarios. We evaluate KDPL on the following scenarios:\nDomain Generalization. To assess the ability of the learned prompts to generalize to out-of-distribution datasets, we apply the prompt learned from ImageNet [11] to four different versions of ImageNet datasets exhibiting various types of domain shift. The target datasets are ImageNetV2 [43], ImageNet-Sketch [51], ImageNet-A [24], and ImageNet-R [23].\nCross-dataset Transfer. To evaluate the ability of learned prompts to generalize to unseen classes, we evaluate the same prompt trained on ImageNet on a broad range of downstream recognition tasks. We validate on ten datasets with varying characteristics: Caltech101 [19] for general object classification; OxfordPets [41], StanfordCars [31], Flowers102 [39], Food101 [5], and FGVCAircraft [37] for fine-grained classification; the Describable Textures Dataset (DTD) [10] for texture classification; EuroSAT [22] for satellite-image recognition; and UCF101 [50] for action recognition.\nGeneralization to Unseen Classes. To evaluate how learned prompts generalize to unseen classes from the same dataset, we divide classes into two subsets: base and novel classes. The prompt is trained exclusively on the base classes and then tested on the novel ones. We evaluate on ImageNet as well as the ten benchmark datasets used for cross-dataset evaluation.\nClass Agnostic Adaptation. We introduce a novel evaluation setting in which the training class names are unknown. The prompt is trained on ImageNet and tested on all the benchmark datasets used in the domain generalization and cross-dataset evaluations.\nImplementation Details. We use a CLIP model with a ViT-H-14 visual backbone as the teacher model. For student models, we evaluate both on a CLIP model based on ResNet-50 and a model based on ViT-B/32. By experimenting with both ResNet-based and ViT-based CLIP models we show the architecture independence of our approach."}, {"title": "M. Mistretta et al.", "content": "To assess how KDPL performs when integrated into different prompt learning techniques, we selected four distinct textual, visual, and multimodal approaches. With the ResNet-50 student model, we experiment with CoOp [65] and CoCoOp [64]. For the ViT-B/32 student, since it supports both visual and textual prompting, we experiment with CoOp [65], VPT [28], and MaPLe [29]. We denote the integration of our unsupervised knowledge distillation strategy with a specific prompt learning technique by adding the suffix +KDPL to the name of the corresponding technique. For example, CoOp+KDPL refers to the application of KDPL to CoOp. For class agnostic settings we instead use the suffix +CA-KDPL to indicate that no training class names are used.\nUnless otherwise stated, all experiments are conducted in the few-shot setting with 16 examples per class randomly sampled from the training set. We set the temperature hyperparameter $\\tau$ in Eq. (1) to 0.01. In the class agnostic experiments in which we assume no knowledge of class names in the training set, we set the number of class names selected in each iteration to K = 1000.\nFor each prompt learning method we use the original implementation and hyperparameters reported in the respective papers. Since we use the compact ResNet-50 and ViT-B/32 backbones, we cannot directly compare to the originally published results. Thus, all numbers we report here were computed by us and are averages over three independent runs. See the Supplementary Material for additional implementation details.\n4.2 Results on Domain Generalization\nTable 1 outlines the performance in the domain generalization setting. We report the performance of each baseline method alongside their unsupervised"}, {"title": "M. Mistretta et al.", "content": "KDPL variants. Additionally, the performance of the zero-shot CLIP student and teacher models using a handcrafted prompt is included for comparison. We observe that applying our unsupervised approach to each baseline does not result in a decrease in performance on the source dataset. Notably, when transferring the learned prompts to a different domain, incorporating KDPL in each baseline leads to a slight improvement. This suggests that our unsupervised teacher-student distillation learns prompts that generalize better than those trained using ground-truth labels. The only exception where we observe an average decrease compared to the baseline is when using CoCoOp. In this case, CoCoOp-KDPL achieves lower average accuracy due to a significant gap on the ImageNet-R dataset. Finally, note that all the proposed unsupervised KDPL variants significantly improve the performance over the zero-shot CLIP student model in both the source and the target datasets.\n4.3 Results on Cross-dataset Transfer\nIn Table 2 we present the results of the cross-dataset evaluation setting in which the prompt is trained on ImageNet and tested on ten different target datasets. For all datasets our KDPL-based variants consistently outperform the corresponding baselines and demonstrate superior generalization performance. The greater generalization capabilities of KDPL are evident even for fine-grained datasets like EuroSAT, on which CoOp+KDPL achieves an 8% improvement over the baseline CoOp when using ResNet-50 as the backbone. Although adding"}, {"title": "M. Mistretta et al.", "content": "KDPL yields only minor improvement to the VPT and MaPLe prompt learning techniques, we emphasize that our VPT+KDPL and MaPLe+KDPL do not have access to ground-truth labels.\n4.4 Results on Generalization to Unseen Classes\nIn Table 3 we give results for the unseen class generalization task. In these experiments each dataset is split into 50% of the classes as a base for training few-shot adaptation, and the remaining 50% as new classes on which zero-shot performance is evaluated. KDPL consistently outperforms the corresponding baseline methods for both backbones, demonstrating improvement in all scenarios for the majority of the datasets. On average, the performance improvement over the supervised baseline methods ranges from about 1% for VPT to about 3% for CoOp with the ResNet-50 backbone. See the Supplementary Material for further analysis of base and combined performance.\n4.5 Results on Class Agnostic Adaptation\nFigure 3(a-b) summarizes the main results in the proposed Class-Agnostic (CA) scenario in which even the training class names are unknown at training time. We report the accuracy on the ImageNet dataset (source), as well as the average accuracy in domain generalization and cross-dataset settings. Note that, even without knowing the class names, the performance on the source dataset steadily improves compared to the zero-shot CLIP model. Moreover, the prompts learned"}, {"title": "Improving Learned Prompts via Unsupervised Knowledge Distillation", "content": "via the proposed unsupervised class-agnostic knowledge distillation also exhibit improved average domain generalization (AVG\u00b9) and cross-dataset capabilities (AVG\u00b2). Figure 3(b) visually depicts how the ResNet-50-based CoOp+CA-KDPL compares with supervised CoOp and zero-shot CLIP student performance in the cross-dataset transfer setting. Notably, CA-KDPL outperforms both baselines despite being unsupervised and class-agnostic during training.\n4.6 Ablation Study\nWe performed a range of ablation studies to evaluate all aspects of KDPL. All ablations reported here are averages over three independent runs using CoOp as the baseline approach and ResNet-50 as the student backbone. The prompt is learned on ImageNet and evaluated on the benchmark datasets used in the domain generalization (AVG\u00b9) and cross-dataset (AVG\u00b2) evaluations."}, {"title": "M. Mistretta et al.", "content": "Pseudolabeling versus KDPL. We compare KDPL and CA-KDPL with a pseudolabeling strategy for exploiting unlabeled samples. Similar to the approach outlined in UPL [26], we generate teacher-derived labels for the training samples. Instead of selecting the top-K confident pseudolabels over the entire training set, for a fair comparison we pseudolabel the fixed, few shot samples in each run. We call this baseline UPL*. Table 4(a) shows that KDPL outperforms UPL*\nSampling Strategies for Class Agnostic Adaptation. An important detail in the class agnostic adaptation scenario is how class names are selected for computing the loss. Existing supervised approaches (e.g. POMP [44]) to handling large numbers of classes suggest simply using the ground truth labels of batch samples and supplementing them with randomly selected ones to reach a predetermined number K. We adapted this technique and compare it with ours in Table 4(b). These results show that KDPL outperforms the random selection strategy denoted as POMP*. Notably, the optimal number of classes for domain generalization and cross-dataset adaptation is 1000, suggesting that the optimum closely aligns with the actual number of classes in the training dataset.\nSymmetric versus Asymmetric KL. In Eq. (2) we use the symmetric KL-divergence, which is a sum of the forward (from teacher to student) and reverse (from student to teacher) KL-divergences. An asymmetric forward loss may yield benefits when the target distribution is similar to the source distribution (see Table 4(c)). Conversely, a reverse loss may prove better in the opposite scenario. Interestingly, in the class agnostic scenario the optimal strategy is the symmetric KL divergence. This enables learning from both in- and out-of-domain training samples, as demonstrated by the results in Figure 3(a)."}, {"title": "5 Conclusion", "content": "In this paper we proposed an approach to Knowledge Distillation Prompt Learning (KDPL) which is easily integrated into existing supervised prompt learning methods. Our experiments show that for CoOp, CoCoOp, VPT, and MaPLe adding KDPL: (1) renders them label agnostic by eliminating the need for ground-truth labels for few-shot adaptation; (2) can also render them class agnostic in cases where no knowledge of training class labels is available; and (3) remarkably improves generalization to downstream tasks.\nThe additional computation cost incurred by distilling from a large VLM is a limitation of KDPL, though all encoder parameters are fixed and the predictions of the teacher can be precomputed, which helps mitigate the extra cost. This extra computation only amounts to about a 15% increase for MaPLe.\nNote that we did not tune any hyperparameters when training the KDPL variants. We use the same settings as in the original papers, which are likely suboptimal for distillation-based adaptation. With careful tuning, there is potential for improvement. Our experiments indicate that distillation-learned prompts are more transferable, and we think it would be interesting to see if this idea can generalize to very different downstream tasks and scale to even bigger teacher models or more interestingly - to even smaller student models."}, {"title": "Appendix", "content": "A. Overview\nThis document contains Supplementary Material that provides additional details to the main paper and further experimental analysis. The contents are organized as follows:\nAdditional Implementation Details (Appendix 5): This section provides additional implementation details, including hyperparameters and configuration settings for reproducing all reported results and additional details about the evaluated datasets.\nAdditional Results: Generalization to Unseen Classes (Appendix 5): Here we present additional experimental results for the scenario of generalization from base to unseen classes within the same dataset, including the downstream performance of few-shot base and zero-shot unseen classes.\nAdditional Results: Class Agnostic Scenario (Appendix 5): This section provides additional results for the class-agnostic setting, our proposed new setting in which we know neither the labels nor the names of the training classes.\nB. Additional Implementation Details\nIn this section we provide further details on hyperparameters of the proposed approaches presented in the main paper and additional details on the benchmark datasets evaluated.\nHyperparameter settings. For all evaluated scenarios we recalculated all the baselines results using the parameter and hyperparameter settings reported in the official publications [28, 29, 64, 65]. In Table 5 we summarize all configuration settings. Notably, all the evaluated baselines use a cosine scheduler with one warm-up epoch with a constant learning rate of 1e-5, and all authors suggest initializing the textual context with \"a photo of a\" as a better starting point.\nNote that our variants augmented with Knowledge Distillation Prompt Learning (KDPL) were trained using exactly the same parameters and configuration settings as the baseline methods. Therefore, it is reasonable to expect that further hyperparameter tuning could potentially improve our performance even further.\nIn addition, all the baselines \u2013 and so also our augmented versions - adhere to the same preprocessing pipeline, in which input images are resized to (224, 224) using bicubic interpolation. Data augmentations including random resized crop, random flip, and normalization are employed to ensure robustness across all experiments."}, {"title": "Improving Learned Prompts via Unsupervised Knowledge Distillation", "content": "Our implementation is built upon the Dassl framework", "BACKGROUND Google": "nd \u201cFaces easy\u201d classes are discarded", "65": ".", "Results": "Generalization to Unseen Classes\nIn the main paper, due to brevity and to maintain focus on the core aspects of our method, we reported only the performance metrics on unseen classes. In this section we provide the performance metrics on base classes as well as the harmonic mean of both base and unseen classes to provide a"}]}