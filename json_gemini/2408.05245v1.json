{"title": "Improved Adaboost Algorithm for Web Advertisement Click Prediction Based on Long Short-Term Memory Networks", "authors": ["Qixuan Yu", "Xirui Tang", "Feiyang Li", "Zinan Cao"], "abstract": "This paper explores an improved Adaboost\nalgorithm based on Long Short-Term Memory Networks\n(LSTMs), which aims to improve the prediction accuracy of user\nclicks on web page advertisements. By comparing it with several\ncommon machine learning algorithms, the paper analyses the\nadvantages of the new model in ad click prediction. It is shown\nthat the improved algorithm proposed in this paper performs\nwell in user ad click prediction with an accuracy of 92%, which is\nan improvement of 13.6% compared to the highest of 78.4%\namong the other three base models. This significant improvement\nindicates that the algorithm is more capable of capturing user\nbehavioural characteristics and time series patterns. In addition,\nthis paper evaluates the model's performance on other\nperformance metrics, including accuracy, recall, and F1 score.\nThe results show that the improved Adaboost algorithm based on\nLSTM is significantly ahead of the traditional model in all these\nmetrics, which further validates its effectiveness and superiority.\nEspecially when facing complex and dynamically changing user\nbehaviours, the model is able to better adapt and make accurate\npredictions. In order to ensure the practicality and reliability of\nthe model, this study also focuses on the accuracy difference\nbetween the training set and the test set. After validation, the\naccuracy of the proposed model on these two datasets only differs\nby 1.7%, which is a small difference indicating that the model has\ngood generalisation ability and can be effectively applied to real-\nworld scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "With the rapid development of the Internet, online\nadvertising has become an important part of enterprise\nmarketing. The click-through rate (CTR) of web\nadvertisements is an important indicator to measure the\neffectiveness of advertisements, which is usually defined as the\nratio of the number of times users click on an advertisement to\nthe number of times the advertisement is displayed [1]. A high\nclick-through rate means that an advertisement can effectively\nattract users' attention and increase brand exposure and\nconversion rate. Therefore, understanding and predicting the\nclick-through rate is crucial for optimising advertisement\nplacement strategies and improving ROI.\nIn recent years, with the surge of data volume and the\nimprovement of computing power, researchers have begun to\nexplore in depth the factors that affect the click-through rate of\nweb advertisements [2]. These factors include user\ncharacteristics (e.g., age, gender, geographic location), ad\ncontent (e.g., copy, images, videos), display location, and time\nof day [3]. By analysing these variables, companies can better\nlocate their target audience and thus develop more precise\nmarketing strategies. However, due to the complexity and\ndiversity of factors affecting the click-through rate, it is often\ndifficult for traditional statistical methods to capture their\nintrinsic patterns, which provides a wide space for the\napplication of machine learning techniques in this field [4].\nMachine learning is a method of training models through\ndata so that they can automatically identify patterns and make\npredictions. In web advertising click rate prediction, machine\nlearning algorithms are able to process massive amounts of\ndata, extract potential features from it, and make accurate\npredictions based on historical data. Common machine learning\nalgorithms include linear regression [5], decision trees [6],\nrandom forests [7], support vector machines [8] and deep\nlearning.\nLinear regression models can be used to build simple and\nintuitive CTR prediction models by analysing the linear\nrelationship between features and click behaviour. However, in\npractice, higher-level algorithms such as decision trees and\nrandom forests are widely used because of the complex non-\nlinear relationships that may exist between features. These\nalgorithms can better capture the effects of interactions\nbetween different features on click-through rates by\nconstructing multi-layer decision rules.\nDeep learning techniques have been rapidly developed in\nrecent years, and they have achieved significant results in areas\nsuch as image recognition and natural language processing. In\nCTR prediction, deep neural networks (DNNs) are able to\nprocess high-dimensional features and extract deep information\nthrough multi-layer nonlinear transformations. This allows\ndeep learning models to excel in handling sparse data with a\nlarge number of category variables (e.g., user IDs and product\nIDs). In addition, Convolutional Neural Networks (CNNs) and\nRecurrent Neural Networks (RNNs) are also used to analyse\nimage and text information to further improve CTR prediction\naccuracy. In this paper, we select open-source datasets and\nimprove the Adaboost algorithm using Long Short-Term\nMemory Networks for predicting users' clicks on web\nadvertisements, and compare them using a variety of\ncommonly used machine learning algorithms to analyse the\nadvantages of this paper's algorithm in predicting users'\nadvertisement clicks."}, {"title": "II. INTRODUCTION TO THE DATASET", "content": "The dataset used in this paper is an open source dataset\nwhich can be used to predict whether a user clicks on an\nadvertisement on a web page or not.The dataset contains\nseveral data features including time spent on site per day, age,\nregion, income, daily internet usage, subject of advertisement,\ncity, gender, country, time of day and whether or not they\nclicked on the advertisement.The whether or not they clicked\non the advertisement is the user's behaviour, which is the target\nvariable, 0 means did not click on the advert and 1 indicates\nthat the user clicked on the advert. Some of the datasets were\npresented and the results are shown in Table 1."}, {"title": "III. METHOD", "content": "Before proceeding to the main experiment, we first predict\nuser clicks using three commonly used machine learning\nalgorithms, i.e., decision trees, random forests, and the\nXGBoost algorithm.\nA. Decision tree\nDecision tree is a supervised learning algorithm based on\ntree structure for classification and regression tasks. It forms a\ntree by recursively dividing the dataset into different subsets.\nEach internal node represents a feature (attribute), each branch\nrepresents some value of that feature, and each leaf node\nrepresents the final output. The structure of a decision tree is\nshown schematically in Fig. 1.\nB. Random forest\nRandom forest is a method of integrated learning that\nimproves the accuracy and robustness of a model by\nconstructing multiple decision trees and combining their\npredictions. In the training process, Random Forest randomly\ndraws multiple samples from the original dataset and trains\nmultiple decision trees on these samples. At the same time,\nonly some of the randomly selected features are considered in\nthe feature selection at each node. This \"random\" strategy helps\nto reduce overfitting and improve the generalisation ability of\nthe model. Ultimately, the random forest generates the final\nprediction by voting (classification) or averaging (regression)\n[9].\nC. XGBoost\nXGBoost is an efficient and flexible gradient boosting\nframework that is widely used in machine learning\ncompetitions and practical applications. Unlike traditional\nboosting methods, XGBoost introduces a regularisation term to\ncontrol the model complexity and thus reduce overfitting. In\naddition, it employs second-order gradient information, which\nmakes the optimisation process more efficient.XGBoost\nsupports parallel computing, can handle large-scale datasets,\nand provides a variety of features, such as missing-value"}, {"title": "D. Long Short-Term Memory Networks", "content": "Long Short-Term Memory Network (LSTM) is a special\ntype of Recurrent Neural Network (RNN) that is specifically\ndesigned to process and predict time series data. Traditional\nRNNs are prone to the problem of vanishing or exploding\ngradients when dealing with long sequences, resulting in\nmodels that are unable to effectively learn long-term\ndependencies. LSTM solves this problem by introducing\n\"memory units\" and three gating mechanisms (input gate,\nforget gate and output gate), which can better capture the long-\nterm dependency features in time series. The structure of the\nLSTM algorithm is shown in Figure 2."}, {"title": "E. Adaboost.", "content": "Adaboost is an integrated learning algorithm designed to\nimprove the predictive performance of a model by combining\nmultiple weak classifiers. The basic idea is to combine multiple\nunderperforming classifiers (weak classifiers) into a single\nstrong classifier.Adaboost trains each weak classifier iteratively\nand dynamically adjusts the sample weights based on its\nperformance on the training set, thus making subsequent\nclassifiers pay more attention to those samples that were\nmisclassified by the previous classifier. This weighting\nmechanism makes the final model better able to capture\ncomplex patterns in the data.The model principle of Adaboost\nis shown in Fig. 3."}, {"title": "F. Optimising Adaboost's classification algorithm based on long short-term memory networks", "content": "Combining LSTM with Adaboost allows the powerful\nsequence modelling capability of LSTM to be used to generate\nbase learners, while Adaboost is used to enhance the\nperformance of these learners. Specifically, the time series data\nis first modelled using LSTM to extract effective features,\nwhich are then fed into Adaboost to train multiple weak\nclassifiers. Each weak classifier can be an LSTM model based\non different parameters or structures, so that the diversity and\ncomplexity in the data can be fully exploited. In this way,\nAdaboost not only improves the robustness of the model, but\nalso enhances the resistance to noise and outliers, thus\nimproving the prediction accuracy.\nIn the implementation process, the data first needs to be\npreprocessed, including steps such as normalisation and\nsplitting into training and test sets. Next, the LSTM model is\nconstructed and trained to obtain preliminary prediction results.\nThen, the error of each sample is calculated based on the\nprediction results and the sample weights are adjusted to\nprovide a basis for subsequent training. This process is repeated\nuntil the set number of iterations is reached or the error meets\nthe requirements. In the final stage, all weak classifiers are\ncombined to form a strong classifier for prediction on new data.\nCombining the long and short-term memory network with\nthe Adaboost algorithm is an innovative and effective method,\nwhich organically integrates the advantages of sequence\nmodelling in deep learning with the powerful performance\nenhancement ability in integrated learning, providing a new\nway of thinking for solving complex problems."}, {"title": "IV. EXPERIMENTMENT", "content": "In terms of dataset division, this paper divides the dataset\naccording to the ratio of 7:3, uses 70% of the data to train the\nmodel, uses 30% of the data to test the trained model, outputs\nthe confusion matrix of the test set, and at the same time,\noutputs model evaluation parameters such as precision,\ncollinearity, recall, and F1 score of the test set.\nThe confusion matrices of the three sets of basic\nexperiments are output as shown in Fig. 4, Fig. 5 and Fig. 6.\nThe model evaluation parameters for the training and test sets\nof the three sets of base experiments are also output, as shown\nin Table 3, Table 4 and Table 5."}, {"title": "V. CONCLUSION", "content": "In this paper, we adopt the Long Short-Term Memory\n(LSTM) network to make innovative improvements to the\nAdaboost algorithm, aiming to enhance the prediction of user\nclicks on web advertisements. By comparing with several\ncommon machine learning algorithms, we systematically\nanalyse the advantages of the proposed algorithm in\nadvertisement click prediction. The experimental results show\nthat the Adaboost algorithm, which is improved based on\nLSTM, performs outstandingly in terms of accuracy, reaching\n92%. This result is not only significantly higher than the\nhighest accuracy rate of 78.4% in the traditional model, but\nalso improves by 13.6%, showing the powerful ability of the\nmodel in processing user behaviour data.\nFurther analysis reveals that the model proposed in this\npaper also outperforms the other three base models in several\nperformance metrics, including accuracy, recall, and F1 score.\nThe improvement in these metrics implies that our model is not\nonly able to identify positive examples (i.e., users clicking on\nadvertisements) more accurately, but also effectively reduces\nthe incidence of false-positives and false-negatives, which\nimproves the overall prediction quality. In addition, the\ndifference in accuracy between the training and test sets is only\n1.7%, which indicates that the proposed model has good\ngeneralisation ability and can effectively adapt to new data\nwithout overfitting the training set.\nIn summary, by combining LSTM with Adaboost, we have\nimplemented a new advertisement click prediction method that\nexcels in both accuracy and stability. This research result not\nonly provides strong data support for advertisement placement\nstrategies, but also lays the foundation for future research in\nrelated fields. We believe that as the amount of data continues\nto increase and computing power improves, deep learning-\nbased methods will show greater potential in more practical\napplications, bringing more accurate and efficient solutions to\nthe industry."}]}