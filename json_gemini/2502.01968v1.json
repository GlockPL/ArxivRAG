{"title": "TOKEN CLEANING: FINE-GRAINED DATA SELECTION FOR LLM SUPERVISED FINE-TUNING", "authors": ["Jinlong Pang", "Na Di", "Zhaowei Zhu", "Jiaheng Wei", "Hao Cheng", "Chen Qian", "Yang Liu"], "abstract": "Recent studies show that in supervised fine-tuning (SFT) of large language models (LLMs), data quality matters more than quantity. While most data cleaning methods concentrate on filtering entire samples, the quality of individual tokens within a sample can vary significantly. After pre-training, even in high-quality samples, patterns or phrases that are not task-related can be redundant or uninformative. Continuing to fine-tune on these patterns may offer limited benefit and even degrade downstream task performance. In this paper, we investigate token quality from a noisy-label perspective and propose a generic token cleaning pipeline for SFT tasks. Our method filters out un-informative tokens while preserving those carrying key task-specific information. Specifically, we first evaluate token quality by examining the influence of model updates on each token, then apply a threshold-based separation. The token influence can be measured in a single pass with a fixed reference model or iteratively with self-evolving reference models. The benefits and limitations of both methods are analyzed theoretically by error upper bounds. Extensive experiments show that our framework consistently improves performance across multiple downstream tasks.", "sections": [{"title": "1 Introduction", "content": "Supervised fine-tuning (SFT) has served as a widely adopted approach and a fundamental step in aligning large language models (LLMs) with human expectations. This process ensures that LLMs can accurately understand human instructions and produce relevant responses. In practice, SFT involves fine-tuning pre-trained models using annotated instructional data [Touvron et al., 2023]. Following general data scaling laws [Zhang et al., 2024], significant efforts have been dedicated to collecting large-scale instructional data containing millions of examples [Wang et al., 2022, Chung et al., 2024, Longpre et al., 2023].\nRecent studies on the SFT have widely agreed that data quality matters far more than quantity [Zhou et al., 2024, Chen et al., 2023, Pang et al., 2024a, Liu et al., 2023]. That is, a small, well-curated dataset can often deliver effective or even superior performance on downstream tasks, highlighting the critical role of data cleaning or selection. Existing data cleaning approaches primarily emphasize identifying high-quality samples in large dataset pools via some metrics, including perplexity [Cao et al., 2024], completion length [Zhao et al., 2024], confidence scores [Chen and Mueller, 2024]), LLM-generated quality ratings [Chen et al., 2023, Pang et al., 2024a, Liu et al., 2023] or even costly human annotations [Zhou et al., 2024]. Although these methods have proven effective, focusing solely on sample-level cleaning may overlook complexities within each sample.\nIn practice, each sample typically contains hundreds of tokens, some of which occur frequently regardless of the sample's quality. These common tokens/patterns can overshadow task-specific words that are crucial for model per-formance during training. Moreover, during interference, if the model continually outputs these frequent tokens, it may neglect more informative ones, producing outputs that appear correct yet fail to address specific tasks. Thus, even well-curated samples can contain token-level noise that dilutes essential signals. Addressing these token-level issues"}, {"title": "2 Related Work", "content": "LLM Data Selection In the LLM SFT phase, various metrics have been introduced to assess data quality including completion length [Zhao et al., 2024], perplexity [Cao et al., 2024], reward scores [Gou and Nguyen, 2024], discrete confidence scores [Chen and Mueller, 2024], the loss disparities when certain examples are included or excluded [Li et al., 2023], gradient matching [Zhou et al., 2023] and influence function scores [Xia et al., 2024]. Another line of work uses advanced LLMs directly to filter out low-quality samples according to different metrics, such as quality-based rating scores [Chen et al., 2023, Liu et al., 2023, Pang et al., 2024a] and fine-grained tags [Lu et al., 2023]. Diversity-aware scoring has also been integrated into the overall quality assessment, highlighting its impor-tance. Although extensive data selection methods have shown promise, fine-grained token-level selection remains underexplored. Recent studies [Lin et al., 2024] have highlighted the significant benefits of token selection during the pre-training phase, yet its application in the SFT phase has received limited attention.\nNoisy Data Cleaning The learning with noisy labels has been extensively studied [Vahdat, 2017, Veit et al., 2017, Li et al., 2017, Liu and Wang, 2021, Yuan et al., 2024]. Various approaches have been proposed to mitigate label errors, including developing noise-tolerant loss functions [Natarajan et al., 2013, Reed et al., 2014, Zhu et al., 2021] and identifying clean samples while re-labeling corrupted ones [Northcutt et al., 2021, 2017, Cheng et al., 2021, Zhu et al., 2022]. Recently, the issue of noisy labels in LLM alignment has gained increasing attention, driven by the observation that data quality is far more critical than quantity [Zhou et al., 2024]. Recent work [Chong et al., 2022] investigated the effectiveness of leveraging pre-trained models to identify inherent label errors in natural language datasets. Additionally, efforts have been made to mitigate label errors in LLM alignment datasets [Zhu et al., 2024], particularly in the context of binary harmlessness classification. Furthermore, Pang et al. [2024a] systematically analyzed error patterns in LLM-generated quality rating scores to reduce score errors. Another line of research has focused on developing noise-tolerant DPO-based loss functions, including cDPO [Mitchell], robust-DPO [Chowdhury et al., 2024], and PerpCorrect [Kong et al.]. The above studies primarily focus on noisy labels at the sample level. In contrast, our work explores fine-grained, token-level noisy labels to identify and filter out uninformative tokens, thereby boosting downstream task performance."}, {"title": "3 Preliminary", "content": "Consider a data pool comprising N samples, denoted as {x\u1d62}\u1d62=\u2081\u1d3a. Each sample x\u1d62 represents a sequence of tokens (including the prompt and the response) defined as x\u1d62 := {x\u1d62,\u2c7c}\u2c7c=\u2081\u1d38\u1d62, where L\u1d62 denotes the token length for the i-th sample. The training of LLMs can be framed as minimizing the negative log-likelihood of the observed tokens in the dataset. The model predicts the conditional probability P(x\u1d62,\u2c7c|x\u1d62, :\u2c7c; \u03b8) for each token x\u1d62,\u2c7c given its preceding context, where \u03b8 represents the model parameters, and x\u1d62, :\u2c7c denotes the first j \u2013 1 tokens, i.e., {x\u1d62,\u2081,\u00b7\u00b7\u00b7, x\u1d62,\u2c7c\u208b\u2081}. Denote by\n\ud835\udc9f := {(x\u1d62,\u2c7c, x\u1d62,:\u2c7c, y\u1d62,\u2c7c), \u2200(i, j) \u2208 \ud835\udcae},\nwhere\n\ud835\udcae := {(i, j)|i \u2208 [N], j \u2208 [L\u1d62]}, [N] := {1,2,\u2026, N}.\nThe loss function for the dataset can be expressed as:\n\u2112\ud835\udc9f(\u03b8) = 1 / (\u2211(\u1d62,\u2c7c)\u2208\ud835\udcae y\u1d62,\u2c7c ) \u2211(\u1d62,\u2c7c)\u2208\ud835\udcae y\u1d62,\u2c7c \u2113(x\u1d62,\u2c7c|x\u1d62,:\u2c7c; \u03b8), (1)\nwhere \u2113(x\u1d62,\u2c7c|x\u1d62,:\u2c7c; \u03b8) := -log P(x\u1d62,\u2c7c|x\u1d62,:\u2c7c; \u03b8), and y\u1d62,\u2c7c \u2208 {0,1} is a binary (ground-truth) label indicating whether the token x\u1d62,\u2c7c is a valid target or not. By iteratively updating \u03b8, the model learns to assign higher probabilities to the correct tokens while disregarding irrelevant ones."}, {"title": "3.2 Token-Level Labels", "content": "Token-level labels y\u1d62,\u2c7c \u2208 {0,1} play a crucial role in determining which tokens contribute to the loss calculation. However, its ground-truth value is often unknown. Denote \u1ef9\u1d62,\u2c7c by the (noisy) token label that we use in practice, which may or may not be identical to y\u1d62,\u2c7c. During different training phases, the criteria for setting \u1ef9\u1d62,\u2c7c may vary:\nModel Pretraining: When training on general text data without explicit distinction between prompts and responses, all tokens are typically considered valid targets (\u1ef9\u1d62,\u2c7c = 1), unless specific tokens are identified as irrelevant or redundant [Lin et al., 2024].\nSupervised Fine-tuning (SFT): In this phase, the tokens corresponding to the prompt part are ignored, as they do not represent the model's predictions. Therefore, for prompt tokens, \u1ef9\u1d62,\u2c7c = 0, and for response tokens, \u1ef9\u1d62,\u2c7c = 1."}, {"title": "4 Token Cleaning: A Noisy Label Perspective", "content": "In the phase of SFT, some tokens are deemed uninformative or redundant since most of the knowledge has been obtained in the pretraining phase, e.g., common patterns and structures, high-frequency phrases. In practice, the SFT phase assigns a label of 1 to every token in the response, resulting in noisy token labels, where irrelevant tokens are incorrectly labeled as important (y\u1d62,\u2c7c = 1). Such noise can hinder the model's optimization process by introducing misleading gradients, reducing the signal-to-noise ratio by hiding informative tokens, and potentially leading to suboptimal performance.\nTo address this issue, it is essential to perform fine-grained token label cleaning. This involves identifying and filtering out uninformative tokens while preserving the tokens that carry valuable task-specific information. As suggested by the noisy label literature Zhu et al. [2022, 2024], the cleaning process typically involves two key components: a scoring function to assess the quality of each token and a threshold to distinguish between informative and uninformative tokens, which will be detailed in the next subsection."}, {"title": "4.2 Token Cleaning Pipeline", "content": "In this section, we will introduce the main components of the token-cleaning pipeline, including the scoring function and a simple yet effective threshold."}, {"title": "4.2.1 Score Functions: An Influence-Guided Approach", "content": "We deliver our intuition when designing score functions using the following example. Suppose that the model is improved from \u03b8 to \u03b8' by fine-tuning it on some data. According to Koh and Liang [2017], Pang et al. [2024b], the model update influences the prediction accuracy of each token, which can be written as\nInfl (x\u1d62,\u2c7c|x\u1d62,:\u2c7c; \u03b8,\u03b8') := \u2113(x\u1d62,\u2c7c|x\u1d62,:\u2c7c; \u03b8') \u2013 \u2113(x\u1d62,\u2c7c|x\u1d62,:\u2c7c; \u03b8). (2)\nIntuitively, a more negative Infl (x\u1d62,\u2c7c|x\u1d62,:\u2c7c; \u03b8, \u03b8') indicates a higher confidence improvement on predicting x\u1d62,\u2c7c given x\u1d62, :\u2c7c. The equation can be explained from two perspectives:\nAssume Token Quality: As demonstrated by Pang et al. [2024b], if we believe that token x\u1d62,\u2c7c is the best choice given context x\u1d62,:\u2c7c, the above influence can be used to evaluate the quality of data that brings the model from \u03b8 to \u03b8' on this specific task, i.e, a more negative Infl (x\u1d62,\u2c7c|x\u1d62,:\u2c7c; \u03b8, \u03b8') indicates a higher data quality.\nAssume Model Quality: From another perspective, if we believe the model \u03b8' performs better on \u03b8 on this specific task, the above influence can be used to evaluate the quality of token x\u1d62,\u2c7c since a good and underfitted choice of x\u1d62,\u2c7c tends to have a negative Infl (x\u1d62,\u2c7c|x\u1d62,:\u2c7c; \u03b8, \u03b8').\nIn this paper, we assume the model quality and use the negative of the influence defined in Eq. (2) to evaluate the quality of tokens, i.e.,\nScore(x\u1d62,\u2c7c|x\u1d62,:\u2c7c; \u03b8,\u03b8') = \u2212Infl (x\u1d62,\u2c7c|x\u1d62,:\u2c7c; \u03b8,\u03b8'), (3)\nwhere a higher score indicates a higher token quality.\nExtending from the current use of influences Koh and Liang [2017], Pang et al. [2024b], we notice that \u03b8 and \u03b8' are not necessarily to be the same model structure, as long as they share the same tokenizer. We will discuss potential choices of \u03b8 and \u03b8' in Section 4.3."}, {"title": "4.2.2 Threshold", "content": "After computing token scores, a threshold is applied to filter out corrupted or uninformative tokens. The threshold separates the tokens that significantly improve the model performance from those that do not. An ideal approach is to use algorithms to estimate the ratio of the corrupted and then select the informative tokens according to the ratio. However, although there are lots of trials in the literature on noisy labels, most works focus on cleaning the image labels Lad and Mueller [2023] or sample-level text labels Zhu et al. [2024], Pang et al. [2024a]. To the best of our knowledge, a feasible algorithm for estimating the noise ratio of token labels is unclear, which is beyond the scope of our paper and left for future explorations. In this paper, we use a fixed ratio (i.e., selected token proportion) k% to separate between information and uninformative tokens. Denote by \u1ef9\u1d62,\u2c7c the token label after cleaning. We have\ny\u1d62,\u2c7c = {1 if Score(x\u1d62,\u2c7c|x\u1d62,:\u2c7c; \u03b8,\u03b8') ranks top k%, \u2200i, j; / 0 otherwise. (4)"}, {"title": "4.3 Selection of \u03b8 and \u03b8'", "content": "We discuss two feasible strategies for selecting \u03b8 and \u03b8'."}, {"title": "4.3.1 Fixed-Model Cleaning", "content": "Following the analyses in Section 4.2.1, we can assume the access to a model \u03b8' that outperforms \u03b8. For example, a moderately performing Llama model can be considered as \u03b8, while a well-performing Llama model can be considered as \u03b8' Mindermann et al. [2022], Lin et al. [2024]. Specifically, given the warm-up model \u03b8' and the base model \u03b8, we compute the token scores for the entire dataset D according to Eq. (3) and use a fixed threshold kfixed to assign token labels \u1ef9\u1d62,\u2c7c according to Eq. (4). Note that token cleaning is performed globally, meaning that some samples may be entirely removed if they contain no positive tokens. This differs from Lin et al. [2024], where each sample retains a fixed proportion of positive tokens. The benefits and limitations of this strategy will be discussed theoretically in Section 5.2."}, {"title": "4.3.2 Self-Evolving Cleaning", "content": "Inspired by the success of semi-supervised learning (SSL), we propose to do token cleaning iteratively. Specifically, in the t-th iteration, we fix the base model \u03b8 and adopt \u03b8' = \u03b8\u1d57, then fine-tune \u03b8\u1d57 with the selected tokens after cleaning. See Algorithm 1 for more details."}, {"title": "5 Theoretical Analyses", "content": "Let 1{\u00b7} be the indicator function taking value 1 when the specified condition is satisfied and 0 otherwise. Define the 0-1 loss as 1(\u03b8(Xprev), Xnext) := 1{\u03b8(Xprev) \u2260 Xnext}, where Xnext is the random variable for the next token, Xprev is the random variable for tokens before the next token, and \u03b8(Xprev) stands for the prediction of next token for model \u03b8 given Xprev as input. Without loss of generality, we consider the ideal case where all the training instances for next-token prediction are i.i.d. and minimize 0-1 loss in the following analyses. The loss can be generalized to bounded loss \u2113(\u00b7) and finite function space \u2131 following the generalization bounds that can be introduced using Rademacher complexity [Bartlett and Mendelson, 2002]."}, {"title": "5.1 Exceed the Performance of Full Tokens", "content": "Denote by \ud835\udc9f := {(Xi,j, Xi,:j, Yi,j), \u221ai, j} the full-token dataset. By minimizing the noisy loss\n\u2112\ud835\udc9f(\u03b8) = 1 / (\ud835\udc41 \u2211(i,j)\u2208\ud835\udc46 y\u1d62,\u2c7c) \u2211(i,j)\u2208\ud835\udc46 y\u1d62,\u2c7c 1(\u03b8(Xi,j), Xi,j)\nwe can get model \u03b8\u0302 := arg min\u03b8 \u2112\ud835\udc9f(\u03b8). Denote by \ud835\udcb4, \ud835\udcb4\u0303 the random variables for \u1ef9\u1d62,\u2c7c and the corresponding ground-truth label y\u1d62,\u2c7c. The expected loss of training with full tokens can be denoted by\n\u2112\ud835\udc9f\u0303(\u03b8) = \ud835\udd3c [\ud835\udcb4\u0303 \u00b7 1(\u03b8(Xprev), Xnext)],"}, {"title": "5.2 Fixed-Model Cleaning: Stable But Limited Improvement", "content": "We now analyze the benefits and limitations based on Theorem 5.1 and Corollary 5.1.1. By selecting an appropriate model \u03b8', we can take a one-shot token cleaning on all the tokens in the candidate data pool. In this case:\nData Quality: The noise rate of cleaned tokens is fixed, i.e., the data quality term in Eq. (5) is fixed. By carefully selecting the threshold kfixed, there exists a token cleaning result whose noise rate \u03b7\u0302 is less than \u03b7(\ud835\udc9f).\nData Quantity: With more tokens being cleaned, M is increasing. Then the total generalization error can be consistently reduced.\nTherefore, under this strategy, as long as the reference model \u03b8' is sufficiently good to reduce the noise rate from \u03b7(\ud835\udc9f) to a lower rate \u03b7\u0302, the model's performance can be improved by fine-tuning with additional i.i.d. cleaned tokens, demonstrating the advantage on stability. However, even as M \u2192 \u221e, the total error does not go to zero due to imperfect data quality, showing the limitations on final performance."}, {"title": "5.3 Self-Evolving Cleaning: Potential Matthew Effect", "content": "For ease of presentation, we divide the data into three groups according to their task difficulty and number of i.i.d. clean tokens in the training dataset:\nG1 (Rich Group): Characterized by lower noise rates after token cleaning and a higher proportion of effective tokens. This group typically experiences significant performance gains during warmup (Line 3, Algorithm 1) and has a great number of relevant tokens.\nG2 (Poor Group): Marked by higher noise rates after token cleaning and fewer effective tokens. This group often exhibits limited or even degraded performance during warmup and has a scarce number of relevant tokens.\nG3 (Intermediate Group): Falling between the rich and poor groups in terms of data quality and quantity. While it generally sees reasonable performance improvement during warmup, its convergence tends to be unstable due to a limited number of effective tokens.\nNote that the definition of groups only applies to the theoretical analyses, which does not mean we need to explicitly know the group attribute of each data. In fact, it is challenging to know this information. Theoretically, there are three observations during SFT."}, {"title": "6 Experiments", "content": "Data Pool We utilize a high-quality data pool with 50k sample size from five popular SFT datasets (300k in total): Flan_v2 [Longpre et al., 2023], Open Assistant 1 [K\u00f6pf et al., 2024], Stanford Alpaca [Taori et al., 2023], Dolly [Databricks, 2023], and WizardLM [Xu et al., 2023]. The data pool is constructed based on a new powerful data curation pipeline proposed by [Pang et al., 2024a], which involves selecting data samples using quality rating scores generated by LLMs. More dataset statistical information including token length can be found in Appendix 10.1. For the self-evolving cleaning strategy, we heuristically divide the data pool into five equally sized subsets (10k samples).\nBase Models In this paper, we select three popular open-source LLMs as our base models, including LLaMA-3.2-3B, LLaMA-3.1-8B [Dubey et al., 2024] and Mistral-7B-v0.3 [Jiang et al., 2023]. These base models will be fine-tuned using samples from our data pool.\nBaselines There are several baselines for performance comparisons: 1) BASE denotes the used base model; 2) DS2 Pang et al. [2024a] fine-tunes base model on 10k selected high-quality samples (with full tokens) from the entire data pool (50k); 3) FULL TOKENS utilizes all tokens to fine-tune the base model; 4) UNIFORM RANDOM randomly selects k% tokens from the 50k data pool without replacement; 5) RHO [Mindermann et al., 2022, Lin et al., 2024] directly computes the excess loss for all tokens between the base and reference model and then selects top-k% tokens. Recall that k is the pre-defined threshold for token cleaning.\nWarmup We warmup by fine-tuning the base model on subset Do with full tokens, and make it the (initial) reference model for RHO, our FIXED-MODEL CLEANING, and SELF-EVOLVING CLEANING. The warmup model is equivalent to the DS2 baseline [Pang et al., 2024a].\nEvaluation To comprehensively evaluate the efficacy of token cleaning methods, we adopt seven OpenLLM Leader-board tasks, including MMLU [Hendrycks et al., 2020], TruthfulQA [Lin et al., 2021], TydiQA [Clark et al., 2020], HellaSwag [Zellers et al., 2019], ARC-Challenge [Clark et al., 2018], BoolQ [Clark et al., 2019] and LogiQA [Liu et al., 2020]. These datasets are sufficiently diverse to thoroughly assess the fine-tuned model across various as-pects, including factual accuracy, reasoning, and multilingual capability. The task performances are evaluated on the lm-eval-harness repository. More evaluation and training details can be found in Appendix 10.2."}, {"title": "6.2 Main Empirical Results", "content": "As shown in Table 1, our proposed strategies consistently outperform baselines across three base models on seven evaluation benchmarks. Notably, compared to using full tokens, our self-evolving cleaning has achieved the average performance improvement of 6.3% on the 3B model and 2.0%/4.4% on the 7B/8B models."}, {"title": "6.3 Ablation Study", "content": "Here, we investigate the impact of selected token proportion for our pipeline using a series of token proportion values including {0.3, 0.4,...,0.9}. As presented in Figure 2, the best results are achieved when the selected token proportion is approximately 50% to 70%. Beyond this range, the overall performance declines, which may be attributed to uninformative tokens. One valuable empirical finding is that the performance gains in SFT tasks largely rely on a small number of highly informative tokens. This observation supports the prevailing view that data quality is more crucial than mere volume. Full results can be referred to in Appendix 11 (Table 7).\nImpact of Reference Model. To assess the impact of the reference model on performance, we run RHO and our fixed-model cleaning approach using LLaMA-3.1-8B-Instruct as the new reference model and compare the results with our previous reference model (DS\u00b2 as the warmup). As shown in Table 3, a more powerful reference model generally leads to greater performance improvements in datasets such as TydiQA and ARC-C. However, some counterintuitive results emerge: in MMLU and BoolQ, despite the 8B reference model significantly outperforming the warmup model, it fails to yield further improvements through token cleaning. A possible explanation for this phenomenon is the distribution shift. If we divide the evaluation task distribution into two parts: an in-distribution segment aligned with our data pool and an out-of-distribution segment, LLaMA-3.1-8B-Instruct, while achieving high overall performance, may not necessarily surpass the warmup model in the in-distribution subset. Investigating this hypothesis and validating this assumption are promising directions for future research. More detailed results can be found in Appendix 11."}, {"title": "7 Conclusion", "content": "This work has demonstrated the effectiveness and importance of token cleaning, introducing a generic token cleaning pipeline that removes uninformative tokens while preserving task-relevant information. Our theoretical analysis has revealed the strengths and limitations of two scoring strategies: fixed-model cleaning, which provides stability but limited improvements, and self-evolving cleaning, which has shown the potential for greater performance gains but requires more careful implementation. Empirically, we have found that filtering out approximately 30%-40% of tokens consistently enhances performance across both strategies, achieving an average 6.3% improvement over the full-token baseline at the 3B model scale."}, {"title": "Appendix", "content": "The Appendix is organized as follows.\nSection 8 discusses the potential limitations of our work.\nSection 9 provides a full proof for Theorems shown in Section 5.\nSection 10 illustrates the experimental details including data pool, evaluation benchmarks, and training details.\nSection 11 demonstrates detailed omitted performance results.\nSection 12 provides several samples with tokens selected by the self-evolving cleaning pipeline."}, {"title": "8 Limitations", "content": "While the proposed token cleaning pipelines demonstrate competitive performance compared to other baselines, we acknowledge that there are still potential limitations:\nBase Model Scale. Our experiments are primarily conducted on a base model with a 3B-8B scale. It remains uncertain how well the pipeline would perform on larger-scale base models.\nData Pool Scale. Due to cost considerations, our data pool is limited to 50k samples. The performance of the proposed pipeline on a larger-scale data pool remains uncertain."}, {"title": "9 Proof for Theorem 5.1", "content": "We first reproduce the definitions as follows.\nDenote by \ud835\udc9f := {(Xi,j, Xi,:j, Yi,j), \u221ai, j} the full-token dataset. By minimizing the noisy loss\n\u2112\u0303\ud835\udc9f(\u03b8) = 1 / (\u2211(i,j)\u2208\ud835\udcae Yi,j ) \u2211(i,j)\u2208\ud835\udcae Yi,j 1(\u03b8(Xi,j), Xi,j),\nwhere \ud835\udcae := {(i, j)|i \u2208 [N], j \u2208 [Li]}, [N] := {1,2,\u2026, N}. we can get model \u03b8\u0302 := arg min\u03b8 \u2112\u0303\ud835\udc9f(\u03b8). When train with full tokens, we have \u1ef9i,j = 1, \u2200i, j. The corresponding expected loss can be denoted by\n\u2112\u0303\ud835\udc9f(\u03b8) = \ud835\udd3c [\ud835\udcb4\u0303 \u00b7 1(\u03b8(Xprev), Xnext)] = \ud835\udd3c [11(\u03b8(Xprev), Xnext)],\nwhere \ud835\udc9f\u0303 is the distribution of \ud835\udc9f. Denote by \ud835\udcb4\u0303 the random variable for noisy token label \u1ef9i,j, and \ud835\udcb4 the random variable for the ground-truth token label Yi,j. Accordingly, with ground-truth token labels, the expected loss is\n\u2112\ud835\udc9f(\u03b8) = 1 / \ud835\udd3c[\ud835\udcb4] \ud835\udd3c [\ud835\udcb4 1(\u03b8(Xprev), Xnext)] .\nDecomposition With the above definitions, the generalization error of model \u03b8\u0302 on the clean distribution could be decomposed as\n\u2112\ud835\udc9f(\u03b8\u0302) = (\u2112\ud835\udc9f(\u03b8\u0302) \u2013 \u2112\u0303\ud835\udc9f(\u03b8\u0302)) + \u2112\u0303\ud835\udc9f(\u03b8\u0302),\nTerm-1 Term-2\nwhere Term-1 transforms the evaluation of \u03b8\u0302 from clean distribution \ud835\udc9f to the noisy distribution \ud835\udc9f\u0303. Term-2 is the generalization error but the model is trained and evaluated on noisy distribution \ud835\udc9f\u0303. Both terms are analyzed as follows."}, {"title": "9.1 Term-1 Upper Bound", "content": "For a certain model \u03b8, there always exist a random variable Xnext such that when \ud835\udcb4 = 1, Xnext = Xnext and when \ud835\udcb4 = 0,\n\ud835\udd3cX|\ud835\udcb4=1 [1(\u03b8(Xprev), Xnext)] = \ud835\udd3cX|\ud835\udcb4=0 [1(\u03b8(Xprev), Xnext)]."}]}