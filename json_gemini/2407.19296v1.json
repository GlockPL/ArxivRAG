{"title": "MULTI-MODAL CLIP-INFORMED PROTEIN EDITING", "authors": ["Mingze Yin", "Hanjing Zhou", "Yiheng Zhu", "Miao Lin", "Yixuan Wu", "Jialu Wu", "Hongxia Xu", "Chang-Yu Hsieh", "Tingjun Hou", "Jintai Chen", "Jian Wu"], "abstract": "Proteins govern most biological functions essential for life, but achieving controllable protein discovery and optimization remains challenging. Recently, machine learning-assisted protein editing (MLPE) has shown promise in accelerating optimization cycles and reducing experimental workloads. However, current methods struggle with the vast combinatorial space of potential protein edits and cannot explicitly conduct protein editing using biotext instructions, limiting their interactivity with human feedback. To fill these gaps, we propose a novel method called ProtET for efficient CLIP-informed protein editing through multi-modality learning. Our approach comprises two stages: in the pretraining stage, contrastive learning aligns protein-biotext representations encoded by two large language models (LLMs), respectively. Subsequently, during the protein editing stage, the fused features from editing instruction texts and original protein sequences serve as the final editing condition for generating target protein sequences. Comprehensive experiments demonstrated the superiority of ProtET in editing proteins to enhance human-expected functionality across multiple attribute domains, including enzyme catalytic activity, protein stability and antibody specific binding ability. And ProtET improves the state-of-the-art results by a large margin, leading to significant stability improvements of 16.67% and 16.90%. This capability positions ProtET to advance real-world artificial protein editing, potentially addressing unmet academic, industrial, and clinical needs.", "sections": [{"title": "Introduction", "content": "Proteins are vital components of biological systems, executing a myriad of functions that underpin an extensive array of cellular processes and biological pathways [1]. Throughout billions of years of evolution, proteins undergo changes in their sequences and structures, which further influence their functional properties. However, accomplishing controllable protein discovery and optimization is challenging because the space of possible proteins is much larger than the space of those likely to have desired functions. Protein editing, or protein modification, is a natural process that gradually increases the diversity of protein structures and functions over time, offering valuable insights into the controlled discovery and optimization of proteins. In the past two decades, various methodologies have been developed for post-translational protein modifications (PTMs), aiming to artificially edit amino acids to enhance human-expected properties. When applied in a biologically benign manner, these methodologies have the potential to form the foundation of true synthetic biology [2], but still heavily rely on time-consuming and post-hoc wet laboratory engineering [3].\nRecently, machine learning-based methods have demonstrated great promise in a wide range of protein-related applications, including 3D structure prediction [4, 5], mutation effects prediction [6], functionality prediction [7, 8, 9] and de novo protein design [10, 11]. With the development of Large Language Models (LLMs), Protein Language Models (PLMs) pretrained on large-scale protein sequence corpora have succeeded in acquiring powerful protein representations, showcasing outstanding performance across diverse tasks [12, 13, 14]. Researchers have also developed machine learning-assisted protein editing (MLPE) approaches, allowing in silico searching for all edited candidates and potentially improving wet-lab protein editing performance. However, existing methods primarily apply black-box optimization algorithms to iteratively sample edited proteins and rely on fitness predictors trained on selected informative samples to guide the editing direction. The iterative refinement within the vast combinatorial space of edited protein sequences still heavily constrains the performance and efficiency of MLPE approaches. Multi-modality learning like CLIP [15] has shown promising results in image-text retrieval [16, 17, 18, 19, 20], CLIP-informed image classification [18, 21, 22, 20], natural language visual reasoning [16, 23] and text-guided image editing [24, 25, 26], illuminating an exciting opportunity on protein editing:\nCan we leverage multi-modality aligned protein language models to efficiently generate edited proteins under the guidance of biological texts?\nTo this end, we propose a generic protein editing method named ProtET. ProtET is a two-stage CLIP-informed multi-modal approach that can accomplish proximally constrained protein editing towards desired properties. Specifically, we first curate millions of protein-biotext aligned pairs, each comprising protein sequences and functional biotext annotations, as illustrated in Figure 1. We then construct transformer-structured encoder-based models (i.e., a large protein model and a large language model) to encode the features of both protein sequences and biotexts, respectively. Similar to CLIP [15], multi-modality pretraining is performed using contrastive learning objectives to align the features of the protein and biotext, facilitating easier editing instruction. In the editing stage, the aligned protein features and desired function description features extracted by the pretrained models are fused into a decoder model to generate the desired protein sequences in an auto-regressive manner.\nTo comprehensively assess the capability of ProtET, we conduct holistic experiments to evaluate its superiority in editing proteins with improved human-expected functions. ProtET achieves state-of-the-art performance in protein function classification tasks, verifying the effectiveness of multi-modal protein-biotext pretraining informed by CLIP [15]. Compared to existing machine learning based protein editing methods (Single-Mutant [27], AFP-DE [28], and EvoPlay [29]), ProtET demonstrates promising performance on the established protein editing task, achieving better editing performance that improves the stability of original protein sequences by 16.67% and 16.90% under two different stability assessment criteria. Additionally, the enzymes designed by ProtET realize a significant leap in the catalytic activity. Antibodies optimized by ProtET also form stable, regular 3D structures binding with SARS-CoV-1 and SARS-CoV-2 antigens. These experimental results highlight ProtET as a valuable tool for future controllable protein discovery and optimization endeavors in real-world scenarios."}, {"title": "Methods", "content": "To attain multi-modal protein-biotext pretraining, we first build a protein-biotext paired dataset. Swiss-Prot database strives to provide a high level of manually reviewed protein annotations, with a minimal level of redundancy [30]. And TrEMBL consists of protein entries with computationally analyzed annotations, derived from the translation of EMBL nucleotide sequences [30]. Given protein database with rich, consistent and accurate protein functional annotations, we curate a new multi-modal dataset with aligned pairs of protein sequences and biotext functional annotations [31]. Concretely, proteins with elaborate annotations are downloaded from Swiss-Prot and TrEMBL in Jan. 2024, yielding 570,420 and 251,131,639 sequences respectively. for protein sequences collected from Swiss-Prot and TrEMBL, we select five property fields: (1) \"Protein Name\" gives the full protein name recommended by the UniProt consortium; (2) \"Function\" depicts diverse functions owned by a protein; (3) \"Subcellular Location\" describes the location and topology of a mature protein in the cell; (4) \"Biological Process\" represents larger processes accomplished by multiple molecular"}, {"title": "Multi-modality pretraining", "content": "Aiming to pave the way for CLIP-informed protein editing, we first need to extract features of the protein and biotext, and align feature spaces of both modalities. In this paper, we exploit two transformer-encoder based large language models to extract protein and biotext features respectively. ESM-2 is employed as the protein sequence encoder, which is pretrained on millions of protein sequences [14]. In recent studies, ESM-2 has proven quite beneficial in many protein-related studies [32, 33]. PubMedBERT is employed as the biotext description encoder, which is pretrained on PubMed articles context [34]. PubMedBERT is a robust biomedical language model customized to encode biomedical texts.\nInspired by extraordinary endeavors of image-text multi-modality alignment, we propose to align feature spaces of both modalities via contrastive learning [15, 24, 25, 35]. As illustrated in Figure 3A, in a list of protein-biotext pairs, p_i and t_j represent the i^{th} protein sequence and j^{th} biotext description, respectively. When i = j, (p_i, t_j) is a positive pair, and when i \\neq j it is a negative pair. f_p^i and f_t^j are protein and biotext representations extracted by the aforementioned encoders, both projected to the same feature dimension. Formally, we introduce our contrastive learning objective, training our model to discriminate positive and negative protein-biotext pairs. Given a batch of N protein-biotext pairs, the overall training objective is constructed of the Protein-to-Biotext alignment loss and Biotext-to-Protein alignment loss:\n\\begin{equation}\n\\mathcal{L}\\_{p2t} = - \\frac{1}{N} \\sum\\_{i=1}^{N} \\log \\frac{\\exp(\\text{sim}(f\\_p^i, f\\_t^i)/\\tau)}{\\sum\\_j \\exp(\\text{sim}(f\\_p^i, f\\_t^j)/\\tau)},\n\\end{equation}\n\\begin{equation}\n\\mathcal{L}\\_{t2p} = - \\frac{1}{N} \\sum\\_{i=1}^{N} \\log \\frac{\\exp(\\text{sim}(f\\_t^i, f\\_p^i)/\\tau)}{\\sum\\_j \\exp(\\text{sim}(f\\_t^i, f\\_p^j)/\\tau)},\n\\end{equation}\nwhere sim(,) denotes the similarity function (i.e., vector dot product in this paper) and \\tau denotes the temperature parameter that controls the softmax distribution.\nOverall, the final contrastive learning objective in the multi-modality pretraining stage is formulated as:\n\\begin{equation}\n\\mathcal{L}\\_{\\text{align}} = \\frac{1}{2N} \\sum\\_{i=1}^{N} (\\mathcal{L}\\_{p2t} + \\mathcal{L}\\_{t2p}).\n\\end{equation}"}, {"title": "Protein editing generator", "content": "Owing to the aligned feature spaces accomplished by multi-modality pretraining introduced in Section 2.2, now we could encode the features of original protein sequences and editing instruction texts, then construct a decoder to generate the edited protein sequences. As shown in Figure 3B, the protein editing generator is composed of a feature fusion module and a generative decoder. Concretely, FiLM module [36] is leveraged to fuse multi-modal features from original protein sequences and editing instruction texts. Such architecture design aims to optimize protein features based on editing instruction text features, integrating multi-modal information to accomplish cross-modal protein editing. The generative decoder is intentionally constructed using 12 layers transformer decoders, tailored for auto-regressively generating edited protein sequences. Given fused features as the final editing condition, edited protein sequences are generated incrementally, one amino acid at a time, with each amino acid being conditioned on the probability of previously generated ones.\n\\begin{equation}\nP(S\\_{\\text{edited}}) = \\prod\\_{i=1}^{n} P(s\\_i | s\\_1, s\\_2, ..., s\\_{i-1}; \\text{FiLM}(S\\_{\\text{original}}, T\\_{\\text{instruction}}))\n\\end{equation}\nwhere S\\_{\\text{edited}} = (s\\_1, s\\_2, ..., s\\_n) represents the edited protein sequences and (S\\_{\\text{original}}, T\\_{\\text{instruction}}) represents encoded features of original protein sequences and editing instruction texts, respectively. Given the paired original and edited protein sequences, the protein editing generator is trained in an unsupervised manner to generate proteins with higher feature similarity to the editing instruction texts, aligning with human-expected functional attributes.\n\\begin{equation}\n\\mathcal{L}\\_{\\text{edit}} = H[\\text{sim}(S\\_{\\text{original}}, T\\_{\\text{instruction}}) - \\text{sim}(S\\_{\\text{edited}}, T\\_{\\text{instruction}})] + \\text{SoftCrossEntropy}[p(s\\_i/S\\_{\\text{original}}), P(s\\_i/S\\_{\\text{edited}})]\n\\end{equation}\nwhere s\\_i/S\\_{\\text{original}} and s\\_i/S\\_{\\text{edited}} respectively denote the amino acid in the i^{th} position of original protein sequences and edited protein sequences. sim(,) denotes the cosine similarity function and H(a) = \\text{max}(0, a). The second item in \\mathcal{L}\\_{\\text{edit}} serves as a regularization term to avoid model collapse and guides the model to perform slight amino acid modification, gradually improving functional attributes."}, {"title": "Implementation details", "content": "Following previous works [33, 32, 37], we use the base-version of ESM-2 model with 650 million trainable parameters to encode protein sequences [14]. As for biotext encoding, we apply PubMedBERT with 12 transformer layers and 100 million trainable parameters. All protein sequences are padded or truncated to a fixed length of 1024 tokens, and we align all the biotext descriptions to a unified length of 512. The embedding dimensions of protein and biotext LLMs are 1280 and 768 respectively. Aiming to align multi-modal representations, we choose 512 to be the projected common feature dimension. The temperature coefficient \\tau is set as 0.01. For the curated multi-modal dataset, we use protein-biotext pairs from filtered TrEMBL for the pretraining stage, and those from Swiss-Prot are exploited to train protein editing generator. The overall framework are trained with a batch size of 128 for 10 epochs, utilizing 16 NVIDIA 32G V100 GPUs. The learning rate is initialized as 5e-5 with 2,000 linear warm-up steps."}, {"title": "Results", "content": ""}, {"title": "Protein function classification", "content": ""}, {"title": "Problem setup", "content": "Following CLIP [15], our proposed method incorporates a large-scale multi-modal pretraining stage, before executing the cross-modal protein editing. We first conduct experiments to validate that the PLM's functional protein understanding is enhanced by multi-modality learning with the biotext. Specifically, four standard protein function classification benchmarks curated by DeepFRI [38] are employed to classify proteins with multiple functional labels, including Enzyme Commission (EC), Gene Ontology Biological Process (GO-BP), Gene Ontology Molecular Function (GO-MF) and Gene Ontology Cellular Component (GO-CC). Following the previous work [32], we exploit the dataset splits under 95% sequence identity cutoff for both EC and GO. And we consider four traditional models (CNN, ResNet, LSTM and Transformer) and three unimodal pretrained PLMs (ProtBERT [7], ESM-1b [12], ESM-2 [14]) as baselines. Furthermore, we include two PLMS (OntoProtein [33], ProtST-ESM2 [32]) undergoing multi-modal alignment with the biotext to further evaluate the superiority of our multi-modal pretraining stage. And function classification results are measured by AUPR and Fmax."}, {"title": "Experimental results", "content": "As shown in Table 2, ProtET achieves state-of-the-art performance on 6 out of 8 evaluation metrics. And we observe that ProtET clearly outperforms the vanilla unimodal PLMs and previous multi-modal aligned PLMs, while the large-scale PLMs performs consistently better than traditional models. These results demonstrate that our multi-modal protein-biotext pretraining process is generally beneficial to protein functional understanding, which boosts performance on diverse classification tasks."}, {"title": "Enzyme catalytic activity editing", "content": ""}, {"title": "Problem setup", "content": "Enzymes are vital proteins that promote metabolism, or the chemical reactions in biological activities. Thus we conduct a protein editing experiment on publicly available PhoQ [39] dataset, aiming to optimize enzymes towards higher catalytic activity. PhoQ is one of the most widely used dataset to test the protein editing capability of MLPE models. It consists of 140,517 enzymes at four sites (A284, V285, S288 and T289), annotated with catalytic activity scores. And the catalytic activity scores in PhoQ represents the phosphatase or kinase activity of different PhoQ mutants. Note that we only include the enzymes with thoroughly annotated catalytic activity scores in this task. Given diverse enzymes with annotated catalytic activity scores, we divide the enzyme dataset into subsets with high, medium, and low functionality, as well as a subset without functionality (i.e., enzyme catalytic activity, also described as fitness). The compositional structure of the enzyme dataset with different annotated fitness levels are illustrated in Figure 4. We respectively sample 100 enzymes from subsets with different fitness levels for the protein editing test and visualization, leaving the remaining proteins for model fine-tuning, and only the aligned key sites are included for loss computation."}, {"title": "Experimental results", "content": "We display t-SNE visualization results in Figure 5. First, we observe that original enzymes with different fitness levels are nicely clustered together, as shown in Figure 5(left), indicating the catalytic activity information can serve as an important clustering basis. Then we exploit ProtET to perform CLIP-informed protein editing on three enzyme subsets with medium, low and zero fitness, leaving the enzyme subset with high fitness as the editing reference (i.e., enzymes with high fitness serve as the golden standard in this experiment). Owing to the powerful editing capability of ProtET, three enzyme subsets initially with poor functionality are optimized and move closer to the editing reference (i.e., enzyme subset with high fitness), as shown in Figure 5(right). Such phenomenon manifests that enzymes edited and optimized by ProtET fulfill a significant leap in the catalytic activity."}, {"title": "Protein stability editing", "content": ""}, {"title": "Problem setup", "content": "To better evaluate the protein editing capability of ProtET, it is worthwhile to conduct convincing experiments. Designing stable proteins is important to ensure, for instance, that drugs are delivered before they are degraded. More generally, given a broad sample of protein measurements, finding better refinements of top stable candidates is useful for maximizing yield from expensive protein engineering experiments. We evaluate ProtET on a set of protein sequences with stability annotations curated by [40]. Starting from original protein sequences, we aim for the edited proteins to exhibit better stability, maintaining its fold above a concentration threshold. As for comprehensive comparison, the stability scores of original protein sequences are included as the bottom line of protein editing performance. We also adopt a single mutation walk approach, referred to as Single-Mutant, as the protein editing baseline [27], which is a hill-climbing algorithm. Single-Mutant [27] performs the single-site mutation across the full-length of original protein sequences, and select the mutated protein sequences with the highest stability score as the final edited protein sequences. Furthermore, we compare ProtET with other deep learning based methods, including AFP-DE [28] and EvoPlay [29]. Concretely, AFP-DE [28] exploits the actively fine-tuned protein language model as the sampler and identifies informative mutants that are both representative and diverse. EvoPlay [29] mutates a single-site residue as an action to optimize protein sequences, likening the protein optimization process to playing pieces on a chessboard.\nAs for the measurement of protein stability, we employ two computational approaches to assess the stability scores of edited protein sequences. The simpler one is to directly compute the cosine similarity between the representations of editing instruction texts and edited protein sequences. For the more complex one, we train a Multi-Layer Perceptron (MLP) according to the experimental evaluated protein stability labels with a regression loss, and exploit it as the oracle (i.e., surrogate of biological stability experiments) to score stability of edited protein sequences."}, {"title": "Experimental results", "content": "As shown in Table 3 and Table 4, we find that ProtET generates the edited protein sequences with the highest stability under both of the aforementioned assessment criteria. For the first criterion (cosine similarity), ProtET achieves state-of-the-art performance although other baselines also manifest effective improvement. For the second criterion (oracle), EvoPlay [29] demonstrates considerable improvements over all other baselines, whereas ProtET slightly outperforms EvoPlay.\nFurthermore, Figure 6 displays the stability improvements of edited protein sequences compared to the corresponding original protein sequences. We observe significant stability enhancements in 16.67% and 16.90% of the proteins edited by ProtET, which adequately illustrates the superiority of the proposed protein editing method."}, {"title": "Zero-shot SARS-CoV antibody optimization", "content": ""}, {"title": "Problem setup", "content": "The versatile binding properties of antibodies have made them a significantly important category of proteins [41, 42, 43, 44]. Among antibodies, Complementarity Determining Regions (CDRs) are the key component to determine the specificity and binding affinity, while CDR-H3 exhibits the highest degree of variability and is hard to predict [45]. Therefore, we further evaluate ProtET to optimize specific binding affinities of SARS-CoV antibodies in a zero-shot manner. We randomly sample 100 antibodies binding to SARS-CoV-1 or SARS-CoV-2 from CoV-AbDab [46]. Note that ProtET focuses exclusively on optimizing CDR-H3 of SARS-CoV antibodies in this task.\nGiven antibody sequences binding to specific antigens (i.e., SARS-CoV-1 or SARS-CoV-2), we perturb antibody sequences by randomly substituting amino acids in the CDR-H3 region with a 15% probability. Then ProtET is exploited to edit antibody CDR-H3 fragments with noise, aiming to obtain stronger antigen-antibody binding affinity. Specifically, we employ the pretrained model to generate numerous optimized CDR-H3 fragments, without further fine-tuning on antibody specific data. For each antibody CDR-H3 fragments with noise, 100 CDR-H3 samples are generated by our ProtET framework. Then the generated CDR-H3 are combined with standard framework regions, enabling 100 full-length generated antibody heavy chains. Since naturalness has been widely proven to be one of the effective indicators reflecting the potential functionality of protein sequences [10, 47], we select the top5 antibody heavy chains with the highest naturalness scores using ProGen2 [11]. The optimized antibody sequences are evaluated by whether they possess the capability to fold into regular 3D structures."}, {"title": "Experimental results", "content": "To measure the foldability of the designed antibody sequences, Alphafold-Multimer [4] are exploited to predict 3D structures comprising the antigen and designed antibodies (including light chains and selected heavy chains). We visualize the generated 3D structures with the highest Alphafold pLDDT. As illustrated in Figure 7, optimized antibodies are found to form stable, regular 3D structures binding with SARS-CoV-1 or SARS-CoV-2 antigens, manifesting our model successfully designs proteins that satisfy structural constraints."}, {"title": "Ablation study", "content": "In this section, we execute the comprehensive ablation study of ProtET using the protein stability editing dataset described in Section 3.3. To thoroughly examine the effectiveness of our proposed innovations, the ablation study focus on following perspectives: (1) constructed protein-biotext paired dataset, uses annotation coverage and evidence level metrics to filter protein entries from Swiss-Prot and TrEMBL, constructing the high-quality protein-biotext dataset to promote the multi-modality learning. (2) multi-modal pretraining stage, which is constructed to align the feature spaces of protein and biotext, informed by CLIP [15], lays the groundwork for cross-modal protein editing. (3) the FiLM module, is applied for multi-modal feature fusion, with the fused feature acting as the final condition for cross-modal protein editing. And we evaluate the essentiality of the FiLM module by replacing it with a simple concatenation operation. As presented in Table 5, we notice that all proposed innovations play a significant role in the model's performance. In particular, the absence of multi-modal pretraining stage yields the highest decrease in performance. The holistic ablation study further validates the superiority of the proposed protein editing method."}, {"title": "Discussion", "content": "Protein molecules are extremely diverse through evolution of 3 billion years [48, 49]. Since the space of possible protein molecules is much larger than the space of those likely to have human-expected functions [50], it remains a challenge to accomplish controllable protein discovery and optimization. To alleviate this challenge, we introduce ProtET in this paper, a CLIP-informed protein editing method via protein-biotext multi-modality learning. Concretely, we execute multi-modality learning of two large language models to effectively align the feature spaces of the protein and biotext following CLIP [15]. Building upon the multi-modal pretrained encoders, we adopt an auto-regressive generative decoder to execute cross-modal protein editing, serving as a pioneering work for AI-assisted controllable protein discovery and optimization.\nExtensive experiments are constructed to comprehensively assess the performance of the proposed method. Excitingly, given the text editing instruction, edited protein sequences generated by ProtET demonstrate excellent functionality, closely aligning with human-expected functional attributes. We select functional attributes from multiple domains, including enzyme catalytic activity, protein stability and antibody specific binding ability, to execute controllable protein editing towards human-expected functionality. Revealed by experimental results, ProtET significantly optimizes functional distribution of enzymes (i.e., enzymes initially with poor functionality tend to cluster together with the high-functionality enzymes after being edited by ProtET). Additionally, ProtET successfully designs protein sequences with 16.67% and 16.90% stability improvement compared to original protein sequences. And optimized antibodies also form stable, regular 3D structures binding with specific SARS-CoV antigens. If we take a closer look at the proposed method, the core idea of ProtET is to align feature spaces via multi-modality learning and accomplish effective and accurate cross-modal protein editing using features from the protein and biotext domains.\nWhile ProtET bears promise in accelerating AI-assisted controllable protein discovery and optimization, there are still areas for improvement and future exploration: (1) When training the proposed protein editing generator, we did not update any parameters of the pretrained large-scale encoders. It may be beneficial to consider incorporating some Parameter Efficient Fine-Tuning (PEFT) approaches into pretrained encoders in the future. (2) ProtET encodes protein representations primarily on sequences but lacks explicit consideration for structural information. Therefore, incorporating protein structures may lead to more comprehensive understanding of proteins. (3) The auto-regressive generation manner falls short in designing proteins with a specified sequence length (i.e., auto-regressive generation relies on the probabilistic sampling of the stop token to determine the generated sequence length), leaving some room for exploring more suitable generation paradigm for controllable protein editing. The further exploration will also promote protein discovery and optimization in real-world applications."}, {"title": "Conclusion", "content": "A major unresolved task in biological research, clinical medicine, and biotechnology is achieving controllable protein discovery and optimization. Protein editing approaches offer a potential solution but still rely heavily on laborious high-throughput screening and experimental heuristics. In this work, we present ProtET, a deep learning based method for controllable protein editing and optimization. Following CLIP [15], ProtET performs large-scale multi-modal pretraining to align protein and biotext feature spaces and thus well execute the cross-modal protein editing. The target protein we designed exhibited improved functional attributes as expected, and well suited the requirements of multiple domains, including enzyme catalytic activity, protein stability, and antibody specific binding ability. We hope our work will accelerate the ultimate goal of accomplishing controllable protein discovery and optimization in real-world scenarios."}]}