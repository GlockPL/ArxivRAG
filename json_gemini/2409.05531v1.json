{"title": "HMAFlow: Learning More Accurate Optical Flow via Hierarchical Motion Field Alignment", "authors": ["Dianbo Ma", "Kousuke Imamura", "Ziyan Gao", "Xiangjie Wang", "Satoshi Yamane"], "abstract": "Optical flow estimation is a fundamental and long- standing visual task. In this work, we present a novel method, dubbed HMAFlow, to improve optical flow estima- tion in these tough scenes, especially with small objects. The proposed model mainly consists of two core compo- nents: a Hierarchical Motion Field Alignment (HMA) mod- ule and a Correlation Self-Attention (CSA) module. In ad- dition, we rebuild 4D cost volumes by employing a Multi- Scale Correlation Search (MCS) layer and replacing aver- age pooling in common cost volumes with an search strat- egy using multiple search ranges. Experimental results demonstrate that our model achieves the best generaliza- tion performance in comparison to other state-of-the-art methods. Specifically, compared with RAFT, our method achieves relative error reductions of 14.2% and 3.4% on the clean pass and final pass of the Sintel online benchmark, respectively. On the KITTI test benchmark, HMAFlow surpasses RAFT and GMA in the Fl-all metric by a rel- ative margin of 6.8% and 7.7%, respectively. To facili- tate future research, our code will be made available at https://github.com/BooTurbo/HMAFlow.", "sections": [{"title": "1. Introduction", "content": "Optical flow aims at estimating dense 2D per-pixel mo- tions by finding the most correlative pixels between con- secutive image pairs in a video sequence. It is a basic and challenging task in computer vision, whose applica- tions cover a wide range of downstream visual tasks such as video surveillance [35], action recognition [34], robot navigation [10], visual tracking [44], autonomous driv- ing [7], to name a few. At the very beginning, a few vari- ational methods [1, 13,53] are proposed to estimate optical flow. Later these efforts encourage multiple enhanced algo-"}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Optimization based method", "content": "Estimating the flow field from pairs of successive video frames has been a long-standing task. Earlier methods [1,2, 4,5, 13, 46, 53] treated optical flow estimation as an energy minimization problem by optimizing a well-defined set of objective terms. These approaches motivated a subsequent array of extended works that reformulated optical flow pre- diction using discrete or global optimization strategies, in- cluding discrete inference in CRFs [29], global optimiza- tion [8], and regressing on 4D correlation volumes [50]. Another line of work usually resorted to better feature matching [3] and motion smoothness [31,39] to address the optical flow problem, based on the fundamental assump- tion of brightness constancy. Although these predefined features are carefully considered and designed, they intrin- sically lack the capacity to accurately model small targets, large motions, and rich details in real-world scenes."}, {"title": "2.2. Learning based method", "content": "In the deep learning era, many challenging problems in various visual tasks have been greatly mitigated or even per- fectly resolved. With recent advancements in deep learning methods, huge achievements have been made in improv- ing the accuracy of optical flow estimation. To explore new approaches, FlowNet [12] was the first to predict optical flow in an end-to-end model, where the learned deep fea- tures were used to compute motion patterns and then in- fer the flow field. Building on this, several learning-based flow methods [15, 18, 32, 41, 43, 51, 55] have been devel- oped to further enhance the accuracy of optical flow predic- tion. FlowNet2.0 [18] adopted stacked multiple flow predic- tion modules in a coarse-to-fine manner to iteratively refine the final flow estimation. PWC-Net [41] leveraged pyrami- dal features and warping operations to build a cost volume, which was then processed by a multi-layer CNN to predict"}, {"title": "2.3. Attention mechanism in optical flow", "content": "As vision transformers [11] have shown preeminent potential in learning long-range dependencies, many at- tempts [14, 20, 25, 37, 48, 49, 56] have employed attention mechanisms to enhance feature representations and attain global matching between image pairs for addressing oc- clusions and capturing large displacements in sophisticated scenes with small targets and difficult noise. Building on RAFT, GMA [20] developed a global motion aggregation module to improve the modeling of optical flow in oc- clusion regions. To enable large-displacement matching for high-resolution images, Flow1D [48] decoupled the 2D correspondence into separate 1D attention and correlation operations for vertical and horizontal directions, respec- tively. FlowFormer [14] adopted a fully transformer-based framework to reconstruct the dominant refinement pipeline, where alternating group transformer layers were designed to encode the 4D cost volume, and recurrent ViT blocks decoded the cost memory to obtain better flow predic- tions. Several approaches [49, 56] utilized explicit or global matching to address the challenges of large displacements and complex motions, greatly improving the inference effi- ciency and prediction quality of optical flow. Despite these methods performing pretty well on multiple benchmarks, they require higher computational costs and time consump- tion due to the extensive use of attention modules."}, {"title": "3. Proposed method", "content": "We propose a novel and effective model for optical flow estimation, called HMAFlow. The overall architecture is depicted in Fig. 2. The model mainly consists of two key modules: the Hierarchical Motion Field Alignment (HMA) module and the Correlation Self-Attention (CSA) module, along with an additional enhanced Multi-Scale Correlation Search (MCS) layer. In this section, we elaborate on our method in detail."}, {"title": "3.1. Preliminaries", "content": "Given a pair of consecutive input images, $I_1$ and $I_2 \\in \\mathbb{R}^{H \\times W \\times 3}$, optical flow methods aim to estimate a 2D per-"}, {"title": "3.2. Multi-scale cost volumes", "content": "Feature extraction. The feature and context encoders we use have the same structure as those in RAFT [43]. Given the tradeoff between reliability and the complexity of cor- relation computation, we use the output feature maps from the feature network at two-level resolution:\n\n$g_{\\theta}(I_1, I_2) \\rightarrow \\{F^l_1, F^l_2\\}, F^l \\in \\mathbb{R}^{H_l \\times W_l \\times D}$  (1)\n\nwhere $g_{\\theta}$ is the feature encoder with parameters $\\theta$, $l$ denotes the output layers at the 1/4 and 1/8 resolution, and D is set to 384. It is worth noting that the output features of the two layers have the same number of channels. We also\ntake output features at the same resolution from the context network $h_{\\theta}$, and then use a skip connection to fuse these contextual features.\nCorrelation computation. For each feature vector in $F^l_1$, there is a corresponding 2D correlation map against all fea- ture vectors from $F^l_2$. We build the volume by computing the inner product of all feature vector pairs:\n\n$\\begin{array}{c}\nC(g_{\\theta}(I_1), g_{\\theta}(I_2)) \\in \\mathbb{R}^{I H \\times I W \\times I H \\times I W} \\\\\nC_{i j m n}=\\sum_{h} g_{\\theta}(I_1)_{i j h} . g_{\\theta}(I_2)_{m n h} \\\\\nC^l=\\operatorname{Set}\\left(C_{i j m n}\\right)\n\\end{array}$\n (2)\n\nwhere we use $C^l$ to denote the base volume at $l$ resolution.\nMulti-scale search. Unlike RAFT [43], which performs an average pooling operation on the last two dimensions of the original volume, we employ multiple search ranges to iteratively look up the primary hierarchical volume to ob- tain the multi-scale cost volumes. Our hierarchically multi- scale cost volumes, $\\{C^{1 / 4}, C^{1 / 8}\\}$, consist of two levels, each with a 4-layer pyramid. The 1/4 resolution correla- tion pyramid effectively captures both subtle and extensive movements of small objects, while the 1/8 resolution pyra- mid adeptly detects a wide range of motions in larger tar- gets.\nWe extend the lookup operator used in RAFT to multi- ple neighborhood searches, resulting in four sampled maps for each 2D correlation map in the 4D base volume at $l$ res- olution. Let the current predicted flow field be $(f_u, f_v)$. According to the definition of optical flow, we can map each pixel $p=(x, y)$ in $I_1$ to its corresponding pixel in $I_2$ : $p^{\\prime}=\\left(x+f_u(x), y+f_v(y)\\right)$. We define multi-scale local neighborhoods of radius $r_i \\in \\{4,6,8,10\\}$ around $p^{\\prime}$\n\n$N_{r_i}\\left(p^{\\prime}\\right)=\\{p^{\\prime}+\\delta \\mid \\delta \\in \\mathbb{Z}^2, \\|\\delta\\|_{\\infty}\\<r_i\\}$  (3)\n\nto sample features from the correlation volumes. Note that we argue that the definition of local neighborhoods should use $L_{\\infty}$ (Chebyshev distance). We apply this multi-scale"}, {"title": "3.3. Hierarchical motion field alignment", "content": "Each feature vector in $F_1$ generates a corresponding 2D response map that has the same height $IH$ and width $IW$ as $F^l$. After sampling the 4D cost volumes, each 2D response map is compressed into a vector of length $d = \\sum(2r_i+1)^2$, such that two levels of 4D cost volumes are transformed into two levels of 3D cost volumes. The two levels of 3D cost volumes have different height $IH$ and width $IW$ dimensions and the same d feature dimension, as presented in Fig. 3.\nA 2D plane along the height and width directions in a 3D volume contains a set of motion features, sampled with a radius $r_i$, from the region of the same location and size in all 2D response maps of a 4D volume. Additionally, a vector along the d direction in a 3D volume represents a set of global motion features, sampled with four radii from the 2D response map produced by computing the correlation between a feature vector at the same location in $F_1$ and all feature vectors in $F_2$.\nBased on the above observations, we understand that a 2 \u00d7 2 region in the 2D plane along the height and width di- rections of $M^{1/4}$, and a 1 \u00d7 1 region in the same position in the 2D plane along the height and width directions of $M^{1/8}$, provide equivalent information and have the same contextual receptive field. Consequently, we propose a Hi- erarchical Motion Field Alignment (HMA) module to con- dense the two levels of 3D cost volumes. The HMA module consists of a 2 \u00d7 2 convolutional layer and a 1 \u00d7 1 convo- lutional layer, each followed by a ReLU layer. We apply a 2\u00d72 depthwise convolution with a stride of 2 on the 3D cost volume $M^{1/4}$ to output a volume with the same resolution as $M^{1/8}$. The two 3D cost volumes with the same dimen- sions are concatenated into a single 3D cost volume along the d direction. Then, the single volume is passed through a 1 \u00d7 1 convolutional layer to reduce dimensionality. Finally, the HMA module outputs a high-quality global cost volume with dimensions H/8 \u00d7 W/8 \u00d7 324. We define the whole operation process conceptually as\n\n$\\begin{aligned}\nA\\left(M^1, M^2\\right) & =\\operatorname{Concat}\\left(\\operatorname{Conv}_{2 \\times 2}\\left(M^1\\right), M^2\\right) \\\\\nDR\\left(A^*\\right) & =\\operatorname{Conv}_{1 \\times 1}\\left(A\\left(M^1, M^2\\right)\\right)\n\\end{aligned}$ (5)\n\nwhere A(,) denotes the alignment operation, $A^*$ repre- sents the matrix obtained after aligning the two correlation volumes, and DR(\u00b7) denotes dimensionality reduction op- eration."}, {"title": "3.4. Self-attention for correlation", "content": "Several methods have explored various attention mech- anisms for cost volumes, demonstrating the advantages of attention techniques in obtaining robust global motion fea- tures. For instance, CRAFT [37] introduced a cross-frame attention module to compute the correlation volume be- tween the reference frame and the target frame. Similarly, GMA [20] leveraged attention mechanisms to construct a global motion aggregation module, which aggregates both 2D context features and 2D motion features.\nIn contrast to these approaches, we propose a lightweight Correlation Self-Attention (CSA) module to further en- hance the global motion features within the 3D cost vol- ume. Specifically, we adapt a large-scale vision transformer model into a single attention module to meet the sepcific re- quirements of our model. The detailed struture of the CSA module is illustrated in Fig. 4. The 3D cost volume output from the HMA module is fed into the CSA module, which learns full-range associations between motion features both along the same cost plane (in height and width directions) and along the feature dimension (d).\nFirst, we apply a 1 \u00d7 1 convolution to the 3D cost vol- ume. Since each 2D plane within the 3D cost volume (height and width directions) represents the set of responses of all feature vectors in $F^l_1$ to the same local region in $F^l_2$, we flatten the plane along the height and width dimensions and reshape the 3D cost volume into a 2D correlation fea- ture with dimensions (H/8 \u00d7 W/8,1,324). Next, we add a global position embedding to the 2D cost volume to cap- ture robust global motion relationships. This embedded 2D cost volume is input into a single self-attention block, which produces weighted and reliable correlation features. Unlike full vision transformer (ViT) models or approaches with multiple attention modules, our lightweight CSA mod- ule only contains one self-attention unit with one attention head and two MLPs, enabling more efficient and accurate optical flow estimation."}, {"title": "3.5. Training loss", "content": "We follow the original objective function setting used in RAFT [43]. The overall training process of our model is supervised by minimizing the $L_1$ distance between the esti- mated flow and ground truth flow across the entire sequence of predictions, $\\{f_1, f_2, ..., f_N\\}$, with exponentially in- creasing weights. Assuming the ground truth flow is de-"}, {"title": "4. Experiments", "content": "In this section, we present HMAFlow's benchmark re- sults and comparisons with state-of-the-art methods, along with systematic ablation analysis. HMAFlow achieves a 14.2% reduction in EPE on the Sintel [6] clean pass and a 6.8% improvement in Fl-all on the KITTI-2015 [28] bench- mark. These results demonstrate HMAFlow's superior gen- eralization performance on both Sintel and KITTI-2015 datasets."}, {"title": "4.1. Datasets and implementation details", "content": "Training schedule. We first pretrain HMAFlow on Fly- ingChairs [12] for 120k iterations with a batch size of 12, followed by 150k iterations on FlyingThings [27] with a batch size of 6 (denoted as \"C+T\"). The pretrained model is then evaluated on the Sintel [6] and KITTI-2015 [28] train- ing split to assess its generalization. Afterward, we fine- tune the model on a combined set of FlyingThings, Sin- tel, KITTI-2015, and HD1K [22] for 150k iterations with a batch size of 6 (denoted as \u201cC+T+S+K+H\") and submit\""}, {"title": "4.2. Comparison with state-of-the-arts", "content": "Generalization performance. We present the evaluation results of HMAFlow and other state-of-the-art methods in Tab. 1. To evaluate generalization ability, we follow prior studies [20, 43] by training HMAFlow on the train- ing sets of FlyingChairs and FlyingThings, and then com- paring our model with state-of-the-art methods on the train-"}, {"title": "4.3. Ablation studies", "content": "To further analyze the effectiveness of the components in HMAFlow, we conduct ablation studies by removing one component at a time and training these sub-models on the FlyingChairs and FlyingThings datasets. The number"}, {"title": "5. Conclusions", "content": "In this work, we propose a new and effective model called HMAFlow, designed to learn informative motion re- lations for more accurate flow field estimation. HMAFlow incorporates two key modules: the Hierarchical Motion Field Alignment module and the Correlation Self-Attention module, along with an enhanced Multi-Scale Correlation Search layer. These components contribute to generating high-quality cost volumes by leveraging hierarchical fea- ture correspondences and global motion relationships. With these novel modules, our model achieves state-of-the-art performance on major public benchmarks. Specifically, it significantly improves prediction accuracy for small, fast- moving targets while preserving more details in fine struc- tures. We believe HMAFlow will advance future optical flow research and lead to better approaches. In the future, we plan to focus on improving accuracy in occluded scenes and balancing performance with cost for more efficient de- ployment."}]}