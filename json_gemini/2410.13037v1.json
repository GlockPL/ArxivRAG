{"title": "LFOSum: Summarizing Long-form Opinions with Large Language Models", "authors": ["Mir Tafseer Nayeem", "Davood Rafiei"], "abstract": "Online reviews play a pivotal role in influencing consumer decisions across various domains, from purchasing products to selecting hotels or restaurants. However, the sheer volume of reviews often containing repetitive or irrelevant content-leads to information overload, making it challenging for users to extract meaningful insights. Traditional opinion summarization models face challenges in handling long inputs and large volumes of reviews, while newer Large Language Model (LLM) approaches often fail to generate accurate and faithful summaries. To address those challenges, this paper introduces (1) a new dataset of long-form user reviews, each entity comprising over a thousand reviews, (2) two training-free LLM-based summarization approaches that scale to long inputs, and (3) automatic evaluation metrics. Our dataset of user reviews is paired with in-depth and unbiased critical summaries by domain experts, serving as a reference for evaluation. Additionally, our novel reference-free evaluation metrics provide a more granular, context-sensitive assessment of summary faithfulness. We benchmark several open-source and closed-source LLMs using our methods. Our evaluation reveals that LLMs still face challenges in balancing sentiment and format adherence in long-form summaries, though open-source models can narrow the gap when relevant information is retrieved in a focused manner.", "sections": [{"title": "1 Introduction", "content": "Online opinions play a critical role in shaping consumer decisions about what products to buy, where to stay, where to eat, and even which books to read. A recent survey found that approximately 98% of online customers read reviews before making a purchase decision (PowerReviews, 2023). These reviews reflect user opinions, providing valuable insights that help set realistic expectations and reveal key details about products and services. However, popular products often accumulate hundreds or even thousands of reviews, many of which contain uninformative content, such as irrelevant personal anecdotes, making them overwhelming to sift through. This leads to information overload (Malhotra, 1984), where the sheer volume of reviews discourages consumers, sometimes disregarding the reviews at all (Soto-Acosta et al., 2014). Market research shows that most customers read fewer than 10 reviews before making a purchase (Murphy, 2016), and this can lead to suboptimal decision-making (Kwon et al., 2015). The sheer volume, variable quality, and limited consumer patience underscore the need for improved review utilization strategies to mitigate information overload and enhance decision-making.\nReview summarization has been studied in the literature under the same name (Hu and Liu, 2004) and within the broader field of opinion mining and summarization (Pang and Lee, 2008; Suhara et al., 2020), with the goal of producing a concise and easy-to-read summaries about target entities (e.g., a product, hotel, restaurant, or service). A well-constructed summary is expected to capture the most common or popular viewpoints while omitting unnecessary or irrelevant information (Ganesan et al., 2010; Hosking et al., 2024). A key challenge is the scarcity of annotated datasets that pair reviews with summaries. Most review platforms do not provide summaries, and creating them would require costly human annotation, unlike news summarization datasets (Hermann et al., 2015; See et al., 2017; Narayan et al., 2018), where summaries are often included in the source documents. To address this, existing studies have leveraged self-supervised approaches, generating synthetic pairs from review corpora (Amplayo and Lapata, 2020; Elsahar et al., 2021), typically by designating one review as a pseudo-summary of others. However, most of these datasets are limited to a maximum of 10 reviews (Angelidis and Lapata, 2018; Chu and Liu, 2019; Bra\u017einskas et al., 2020a), with only a few extending to hundreds (Angelidis et al., 2021; Bra\u017einskas et al., 2021), while real-world entities often accumulate thousands of reviews. Our work aims to scale review summarization to accommodate larger volumes of reviews.\nAn effective opinion summarization model should possess several desirable properties to address the challenges associated with large-scale review summarization (Kim Amplayo et al., 2022). First, it should offer control mechanisms (Amplayo et al., 2021; Li et al., 2023), enabling users to customize the summaries to their specific needs. Second, the model must be scalable, capable of processing thousands of user opinions while efficiently extracting essential information (Hosking et al., 2023). Lastly, the generated summaries must be faithful to the input texts, accurately representing their content while minimizing the risk of hallucination (Maynez et al., 2020; Tang et al., 2023). In this paper, we explore three control mechanisms for opinion summarization: (1) query control, (2) sentiment control, and (3) length control. With query control, users can specify preferences such as 'ocean view' or proximity to a \u2018metro station.' Sentiment control enables structuring summaries into sections like 'PROS' and 'CONS', while length control allows users to dictate the length of the generated summaries. To handle large volumes of reviews, we examine two scalable approaches: Retrieval-Augmented Generation (RAG) and long-context Large Language Models (LLMs) (Lee et al., 2024), both of which show promise (Li et al., 2024). Evaluating faithfulness in long-form summarization poses a unique challenge (Siledar et al., 2024a), as modern models often suffer from hallucinations (Maynez et al., 2020; Tang et al., 2023). Traditional metrics like RAGAS (Es et al., 2024) and RAGChecker (Ru et al., 2024) are typically designed for factual tasks such as question answering or knowledge-based generation, where sentiment and opinions are secondary concerns. To better align generated summaries with input texts, we treat both as sets of triplets and develop a scheme to quantify their alignment. This approach offers a reference-free evaluation metric tailored to sentiment-rich domains, such as product and service reviews, where opinion and sentiment polarity are crucial.\nOur main contributions are summarized as follows:\n\u2022 We introduce a new dataset of long-form user reviews, where each entity contains over a thousand reviews paired with in-depth, unbiased critical summaries provided by domain experts (\u00a72).\n\u2022 We propose training-free methods that utilize RAG and long-context LLMs to address the challenges of long-form opinion summarization. Our approach enables controllable and scalable summarization, providing fine-grained user controls (\u00a73).\n\u2022 We develop three novel, reference-free automatic evaluation metrics based on Aspect-Opinion-Sentiment (AOS) triplets. These metrics provide a granular and context-sensitive assessment of the faithfulness of generated summaries, particularly in sentiment-rich domains where opinions and sentiment polarity are crucial (\u00a73.2.4)."}, {"title": "2 Dataset Construction", "content": "We introduce the LFOSum dataset, a collection of long-form user reviews centered around hotel experiences shared online. Rich in detailed descriptions and personal opinions, this dataset is well-suited for opinion summarization tasks. Hotel reviews are particularly valuable due to their in-depth, personalized narratives that cover a wide range of user experiences, such as amenities, service quality, and location. Each entity in the dataset contains over a thousand reviews, offering a substantial volume of input texts.\nSource Reviews The reviews were sourced from TripAdvisor\u00b2, a widely-used platform that combines user-generated reviews with online travel booking services. TripAdvisor's reviews, on average, are three times longer than those found on other leading travel platforms (D'Souza, 2024), making it an ideal resource for exploring the challenges of long-form summarization with book-length inputs (exceeding 100K tokens) (Chang et al., 2024).\nReference Summaries Annotated datasets that pair summaries with long-form reviews are scarce, largely because such summaries are not readily"}, {"title": "3 Methodology", "content": "We propose two scalable, training-free methods to handle large volumes of user reviews effectively. First, the Long-form Critic method directly utilizes long-context LLMs to generate summaries, allowing users to control aspects such as sentiment and length (\u00a73.1). Second, the RAG Framework combines an extractive-generative approach, managing long sequences by incorporating retrieval augmentation (\u00a73.2)."}, {"title": "3.1 LFOSum: Long-form Critic", "content": "In this approach, we generate critical summaries consisting of 'PROS\u2019and\u2018CONS' from the full set of user reviews for a specific entity, presented in a long-form setting. To achieve this, long-context LLMs are employed to process the entire review corpus and generate critical summaries. The LLMs are prompted with a detailed task description, all user reviews for the entity, specific constraints, stylistic exemplars, and are instructed to produce the output in a structured JSON format with separate keys for 'PROS\u2019and \u2018CONS' (the prompt presented in Figure 3 of Appendix). In the basic setting, we do not control the length; the model independently determines the optimal number of 'PROS' and 'CONS' sentences based on the input. The overall process can be formalized as:\nCritical Summary = \\(LLM_{critic}(R,C,E,P)\\)   (1)\nWhere R is the set of user reviews, C represents task-specific constraints, & are stylistic exemplars, and P is the task prompt provided to the LLM."}, {"title": "Length Control", "content": "In this setting, we introduce a user-centric control mechanism to specify the desired number of 'PROS' and 'CONS' sentences for the critical summary. An additional parameter is included in the LLM prompt to guide the generation length. The number of \u2018PROS' and 'CONS' is determined based on the ground truth critical summary for the item. By explicitly instructing the LLM with these parameters, we ensure the generated summary aligns with the expected structure and length."}, {"title": "Sketch \u2192 Fetch \u21d2 Fill (SFF)", "content": "To evaluate sentiment and length-controlled summaries, we parse LLM outputs into structured JSON format. However, LLMs sometimes produce incomplete or malformed outputs (some examples in Appendix B). To address this, we propose the Sketch-Fetch-Fill (SFF) approach for reliable JSON extraction.\n1. Sketch: We first define the expected JSON structure, specifying key fields (e.g., 'pros' and 'cons') to guide reconstruction.\n2. Fetch: Regular expressions are used to extract relevant content from the output, identifying text corresponding to the predefined fields, even with formatting inconsistencies.\n3. Fill: The extracted data is inserted into the predefined structure, correcting common errors (e.g., missing quotes or misplaced commas) to ensure a valid, parsable JSON."}, {"title": "3.2 LFOSum: RAG Framework", "content": "A key component of any RAG framework is the availability of query terms (Zhao et al., 2023). In our case, the query terms for an entity are not predefined or readily available. To address this, we employ a simple yet effective method to extract query terms from the large input reviews (\u00a73.2.1). These extracted terms are then used to design a combined extractive-generative framework for managing long-form input reviews through retrieval augmentation (Lewis et al., 2020). This approach integrates the attributable and scalable properties of extractive methods (\u00a73.2.2) with the coherence and fluency of LLMs (\u00a73.2.3). Another advantage of our RAG framework is that it enables the automatic evaluation of generated summaries in manageable units, allowing for a more fine-grained assessment within long-form context (\u00a73.2.4)."}, {"title": "3.2.1 Query Term Extraction", "content": "Let Me denote the language model capturing opinions about an entity e, defined as the probability distribution over word sequences. Under the query likelihood model, entity e is considered relevant to query term q if q is likely generated by Me. Hence more frequent terms in the reviews of an entity may be treated as important query terms, with the exception of common stop words, and the summarization of an entity may be centered around these important terms.\nA related task in the literature is aspect extraction, which can be categorized into two types: (1) Explicit aspects and (2) Implicit aspects (Poria et al., 2014; Luo et al., 2018). Explicit aspects are directly mentioned targets in opinionated sentences, such as \u201cocean view\u201d or \u201cspa service.\u201d In contrast, implicit aspects are inherently expressed concepts that can generalize explicit examples; for instance, \u201cocean view\u201d may relate to the broader category of \"location,\u201d while \u201cspa service\u201d falls under \"service.\" In designing our RAG framework, we focus on explicit aspects (referred to as \"query terms\") due to their repetitive nature in long-form reviews, which facilitates the retrieval of salient sentences covering diverse user concerns. Below, we outline the major components of the query term extraction process:\nCandidate Term Extraction & Ranking We extract the most frequent unigrams and skip bigrams within a defined window size of 4. This approach captures meaningful multi-word expressions that may not be adjacent but contribute contextually to the overall understanding of the text. To filter out rare or insignificant terms, we apply a frequency threshold, ensuring that only high-frequency, representative terms are retained4. The terms are then ranked based on their frequency values to prioritize those most representative of the input reviews.\nTop-K Term Refinement The extracted query terms are further refined by cross-referencing them with the gold query term list from (Pontiki et al., 2015) for our domain of interest (i.e., Hotel). This helps eliminate frequent but irrelevant terms, such as stop words. To ensure that the final set of terms is diverse and non-redundant, single terms are removed if both of their constituent words appear within a multi-word query. Ultimately, the top-K most relevant query terms are selected for the retrieval step."}, {"title": "3.2.2 Retrieval", "content": "We divide user reviews into individual sentences and use the Top-K extracted query terms to retrieve relevant sentences as evidence for each query term, which are then provided as input to the LLMs. This approach offers two key advantages: (1) Retrieving sentences based on a diverse set of query terms reduces redundancy in the generated summaries, and (2) it increases information coverage from the user reviews. The retrieval process is formalized as follows:\n\\(SQ = Top-K (R(Q, D))\\)  (2)\nWhere is the set of query terms, D is the collection of review sentences, R(Q, D) is the retrieval function, and So represents the Top-K retrieved sentences.\nRetrievers We utilize two types of retrievers: BM25 and Dense retrievers. BM25 is a lexical retriever that scores document relevance based on term frequency (Robertson and Zaragoza, 2009), while Dense retrievers capture deeper contextual meanings through semantic information, ensuring both surface-level lexical matches and nuanced semantic relationships are covered. For the Dense retriever, we employ Sentence Transformers (Reimers and Gurevych, 2019), specifically leveraging the checkpoint due to its superior performance in semantic search across a wide range of benchmarks."}, {"title": "3.2.3 LLM as Reranker and Abstractor", "content": "We utilize the retrieved sentences for each query term as evidence and instruct LLMs to generate summaries. Two variants of summarization approaches are employed: (1) Extractive and (2) Abstractive. In both cases, LLMs are prompted with the retrieved sentences, and the outputs are aligned in a specified JSON format. The general process for both approaches can be formalized as follows:\nSummary(Q) = LLM(Q, SQ, C, P)  (3)\nWhere Q is the query term, So is the set of Top-K retrieved sentences, C represents the constraints, and P is the prompt provided to the LLM.\nExtractive In the extractive approach, LLMs are prompted with a task description, constraints, the query term, and a list of Top-K retrieved sentences. The LLM is instructed to rerank the sentences and select the most relevant one, functioning primarily as a reranker. The complete prompt used for this process is shown in Appendix (Figure 4)."}, {"title": "3.2.4 RAG Verification", "content": "To evaluate the ability of LLMs to generate summaries that accurately reflect the input evidence, we build upon the work of Bhaskar et al. (2023), who developed desiderata for human evaluation, by introducing automatic evaluation metrics. Our goal is to break down sentences into structured components, allowing for a more granular and fine-grained assessment of factual alignment. We employ Aspect-Opinion-Sentiment (AOS) triplets (Varia et al., 2023), using a pre-trained model from Scaria et al. (2024), which captures both implicit and explicit aspects (as detailed in \u00a73.2.1). Each triplet decomposes the sentence into three core components:\n\u2022 Aspect: The attribute or feature being discussed (e.g., room bathroom", "Opinion": "The expression or judgment about the aspect (e.g., clean", "Sentiment": "The polarity of the opinion (e.g., negative, neutral, or positive).\nGiven a set of retrieved sentences for each query, and a generated sentence, we evaluate the quality of the generated sentences for the Top-K queries of an entity based on three key metrics:\n\u2022 Aspect Relevance (AR): Measures how well the aspect in the generated sentence aligns with the most important and frequent aspects mentioned in the retrieved evidence. This ensures the summary remains on topic and covers critical aspects.\n\u2022 Sentiment Factuality (SF): Evaluates for a given aspect whether the sentiment in the generated sentence matches the most frequent sentiment found in the retrieved evidence, ensuring that the sentiment expressed is factually accurate.\n\u2022 Opinion Faithfulness (OF): Assesses for a given aspect and sentiment whether the opinion expressed in the generated sentence is consistent with the opinions found in the retrieved evidence, either through direct matching or semantic similarity.\nAspect Relevance (AR) For each query, AOS triplets are extracted from both the retrieved and generated sentences. We identify the most frequent aspect from the retrieved evidence and check if it appears in the generated sentence. Aspect Relevance, in this context, is a binary variable, indicating whether the generated sentence remains on-topic by covering the most important aspect. We are interested in the expectation of this variable over generated sentences.\nSentiment Factuality (SF) For each aspect, sentiments are extracted from AOS triplets of both the retrieved and generated sentences. Neutral sentiments are excluded as they provide limited insight. For each aspect, the most frequent non-neutral sentiment from the retrieved sentences is identified, and the sentiment in the generated sentence is checked for alignment. Similar to AR, SF is a binary variable, indicating whether the generated sentiment is factually correct. Again, we are interested in the expectation of this variable over generated sentences.\nOpinion Faithfulness (OF) For each aspect and sentiment, opinions are extracted from AOS triplets of both retrieved and generated sentences. A direct opinion match is assigned a score of 1, while indirect matches are evaluated using a semantic similarity function (e.g., cosine similarity), which returns a value between 0 and 1. This allows for semantically similar opinions (e.g., \u201cbeautiful\u201d and", "stunning": "to be considered faithful. Therefore, the opinion faithfulness for a given aspect and sentiment is represented as a random variable ranging from 0 to 1, and we report its expected value over generated sentences."}, {"title": "4 Evaluation", "content": "In this section, we evaluate the performance of our two proposed approaches: (1) the Long-form Critic (\u00a74.1) and (2) the RAG Framework (\u00a74.2). We assess these methods using a variety of open-source and closed-source models, comparing their performance on standard and newly proposed evaluation metrics. The experimental setup is detailed in the Appendix A, generated summaries in Table 9, Table 10 & Table 11, and the related works are covered in Appendix E.\nAutomatic Evaluation We use F1 scores of ROUGE (R1 and RL) (Lin, 2004) and BERTScore (Zhang* et al., 2020), following (Bhaskar et al., 2023). Although ROUGE scores have been shown to be less reliable for generic opinion summarization tasks (Tay et al., 2019; Shen and Wan, 2023), we report them for consistency with recent studies (Bhaskar et al., 2023; Lei et al., 2024; Siledar et al., 2024b; Hosking et al., 2024), to benchmark our dataset and methods in long-form settings, and to contribute to discussions on automatic evaluation methods for long-form opinion summarization (\u00a75). Additionally, we use our proposed evaluation metrics to assess the faithfulness of the LLM-generated summaries."}, {"title": "4.1 Evaluating Long-form Critic", "content": "We evaluate the ability of several LLMs to generate critical summaries divided into 'PROS' and 'CONS'. For this purpose, we utilized long-context LLMs, providing the full set of user reviews as input. We experimented with several closed-source models, including GPT-4o-mini8, Claude-3-Haiku, and Gemini-1.5-Flash10, alongside open-source models such as Llama-3.2-3B-Instruct (Dubey et al., 2024) and Phi-3.5-mini-instruct (Abdin et al., 2024), each with varying context lengths. However, we encountered significant challenges with open-source models. As highlighted in (Xia et al., 2024), these models frequently failed to adhere to the expected output format, often producing non-parsable JSON outputs, even when employing our SFF method for parsing (\u00a73.1). We directly parsed the expected JSON outputs from the LLMs, and in cases of errors (detailed in the Appendix B), we attempted to automatically recover them using our SFF method (\u00a73.1). If the context length exceeded the model's limit, we truncated the older reviews, prioritizing more recent ones based on posting dates. Summaries were considered valid only if both the \"pros\" and \"cons\" sections were not empty, and any invalid summaries were excluded from the evaluation.\nResults & Analysis As shown in Table 2, Claude-3-Haiku produces the best summaries in the basic setting for both 'PROS' and 'CONS'. However, across all models, 'CONS' performance is generally weaker, likely because negative reviews are less frequent compared to positive ones (Venkatesakumar et al., 2021), making it harder for the models to capture \u201cneedle-in-a-haystack\u201d information within long-form inputs (Laban et al., 2024). In the length control setting, GPT-4o-mini excels in 'PROS', while Gemini-1.5-Flash performs better in 'CONS', likely due to its larger context window. Claude-3-Haiku struggles with length adherence, as noted in Appendix (Table 6). Gemini-1.5-Flash generated 372 out of 500 valid summaries, with the remaining invalid due to empty fields, elaborated more in \u00a75. These results highlight the challenge of balancing sentiment and format adherence in long-form summaries."}, {"title": "4.2 Evaluating RAG Framework", "content": "We evaluate our RAG Framework using both open-source and closed-source models. A maximum of 15 top query terms (K=15) are selected for the retrievers, and for each query term, we experiment with retrieving 10 and 20 sentences. For both summary variants\u2014(1) Extractive and (2) Abstractive-the system-generated summary is created by merging the sentences for each query term, as detailed in \u00a73.2.3. The 'PROS' and 'CONS' from the gold summaries are merged to form a generic reference summary, following the standard opinion summarization evaluation protocol without sentiment control (Bhaskar et al., 2023).\nBaselines For the BM25 and Dense baselines, we select the top sentence retrieved for each of the K query terms to form the summary. For the random baseline, K sentences are randomly selected from the input reviews for each entity. As an upper-bound baseline, the Oracle selects the sentence with the highest ROUGE-L (RL) score for each gold summary sentence, providing an approximate upper limit for performance.\nResults & Analysis As presented in Table 3, for the Extractive summary variant, the closed-source models (Claude-3-Haiku and GPT-4o-mini) generally outperform the open-source models across all metrics. However, in the Abstractive variant, the performance of open-source models, particularly Llama-3-8B, improves significantly. This suggests that in settings requiring more abstraction and synthesis, open-source models can effectively narrow the gap between themselves and their closed-source counterparts, especially when relevant information is retrieved in a focused manner. In both extractive and abstractive settings, summaries driven by the most important query terms directly impact overall performance. The Oracle baseline further shows that there is still considerable room for improvement, highlighting the inherent challenges in long-form summarization. For RAG verification (Table 4), closed-source models outperform open-source models across key metrics. Claude-3-Haiku excels in AR and SF, demonstrating its ability to stay focused on relevant aspects while maintaining factually aligned sentiment. GPT-4o-mini shows strong performance in SF and leads in OF, ensuring that the sentiments and opinions expressed in the generated summaries are consistent with the retrieved evidence. Similar trends are observed with K=10, as presented in Appendix Tables 7 and 8, which reinforce the results seen with K=20."}, {"title": "5 Discussion and Future Directions", "content": "Moderation Issues in User Reviews In the basic setting, Gemini-1.5-Flash generated several invalid summaries due to sensitive or inappropriate content, such as \u201cManager is an African middle-aged man who was irresponsible and harsh\u201d and \u201cWant more offers?? Call me +1 111 222 ******,\u201d triggering its safety mechanism\u00b9\u00b9. Even after disabling safety filters, the issue persisted, highlighting the difficulty of handling long-form user reviews. However, in the length-controlled setting, the model produced fewer invalid summaries by prioritizing safer content. Other models did not face similar issues, possibly due to different content moderation filters. Addressing these challenges presents an important area for future work.\nEvaluation Evaluating opinion summarization for long-form user reviews is especially challenging, whether through automatic or human assessments. Human evaluation metrics such as Fluency, Coherence, and Non-Redundancy (Bra\u017einskas et al., 2020a; Angelidis et al., 2021) are often less applicable when designing systems based on LLMs (Song et al., 2024). Moreover, most existing LLM-based evaluators are tailored to short input reviews (Siledar et al., 2024a). Our dataset, with its explicit \u2018PROS\u2019and \u2018CONS\u2019 paired with long-form reviews, offers opportunities to develop more suitable LLM-based evaluation metrics."}, {"title": "6 Conclusion", "content": "In this paper, we addressed key challenges in long-form opinion summarization by introducing a new dataset of over a thousand user reviews per entity, paired with in-depth critical summaries from domain experts. We proposed two training-free summarization methods utilizing RAG and long-context LLMs, designed for scalable and controllable summarization. Additionally, we developed novel reference-free evaluation metrics that offer a fine-grained, context-sensitive assessment of summary faithfulness. Furthermore, based on our insights, we offer suggestions for future research."}, {"title": "Limitations", "content": "In this work, we evaluated our proposed methods using a selection of both open-source and closed-source LLMs. We intentionally focused on cost-effective yet efficient closed-source models and open-source models that can be deployed on consumer-grade hardware, given the constraints of academic settings. The performance of more powerful, large-scale models remains unexplored, but we encourage the broader research community to benchmark these models using our dataset and methods.\nWhile we experimented with different retrievers (BM25 and Dense) for both summary variants using Top-K values of 10 and 20, other retriever configurations might yield better performance. Optimizing for additional retriever options is beyond the scope of this study, but we acknowledge that further exploration in this area could lead to improvements.\nAlthough we proposed novel automatic evaluation metrics built on top of the RAG framework with retrieved evidence, their applicability may be limited in full long-form settings where complete retrieval is not feasible. This remains a potential avenue for future research.\nFinally, our research and the development of LFOSum are exclusively centered on the English language. This means its use and effectiveness might not be the same for other languages."}, {"title": "Ethics Statement", "content": "Data Crawling We carefully considered ethical guidelines when scraping data, ensuring that the data collected is used solely for non-commercial research purposes. Our web scraping was conducted responsibly, at a controlled rate, with the clear intent to avoid any risk of causing a Distributed Denial of Service (DDoS) attack or overloading the servers.\nProtection of Privacy While collecting user reviews, we deliberately chose to exclude any personal information such as reviewer IDs, names, and locations. For our experiments, we focused solely on collecting the review text and date, ensuring that the dataset does not contain any Personally Identifiable Information (PII). This highlights our commitment to user privacy. However, we cannot fully guarantee that users did not include personal details, hate speech, or inappropriate content within the text of their reviews."}]}