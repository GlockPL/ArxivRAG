{"title": "Improving Time Series Classification with Representation Soft Label Smoothing", "authors": ["Hengyi Ma", "Weitong Chen"], "abstract": "Previous research has indicated that deep neural network based models for time series classification (TSC) tasks are prone to over-fitting. This issue can be mitigated by employing strategies that prevent the model from becoming overly confident in its predictions, such as la-bel smoothing and confidence penalty. Building upon the concept of label smoothing, we propose a novel approach to generate more reliable soft labels, which we refer to as representation soft label smoothing. We apply label smoothing, confidence penalty, and our method representation soft label smoothing to several TSC models and compare their performance with baseline method which only uses hard labels for training. Our re-sults demonstrate that the use of these enhancement techniques yields competitive results compared to the baseline method. Importantly, our method demonstrates strong performance across models with varying structures and complexities.", "sections": [{"title": "1 Introduction", "content": "In recent years, time series tasks have gained increasing popularity, with ap-plications spanning various fields including medical analysis [1], stock market prediction [2], weather forecasting [3], and industrial scenarios [4]. A plethora of models have been developed and applied to such tasks. For instance, in the domain of time series classification, traditional machine learning algorithms like combing DTW with KNN [5] have shown promising results. And neural network (NN) has demonstrated its astonishing power in many data mining tasks [6]. The advent of deep learning based models has further enriched the field, due to their superior performance and efficient computational speed. For example, Fawaz et al. introduced InceptionTime [7], which has emerged as a powerful baseline model for time series classification tasks. LSTM-FCN [8], developed by Fazle Karim et al., is notable for its end-to-end training capability and minimal preprocessing requirements. Neural network models leveraging residual connections, such as ResNet18 [9], have also been successfully applied in this context.\nHowever, the overfitting phenomenon of deep learning based models in time series classification tasks has also attracted attention. Researchers have tried"}, {"title": "2 Related work", "content": "We applied our method to the task of time series classification. Time series classification is a subfield of time series related tasks with numerous real-life applications [23]. Many methods have been applied to time series classification tasks and achieved competitive results. Patrick Sch\u00e4fer et al. [24] proposed a time series classification model based on SVM classifier. The KNN method combined with the DTW method is a very competitive method, but the time complexity of this method is high. Many researchers have proposed methods to accelerate DTW calculation of time series from different angles [25] [26] [27]. For deep learning based model, apart from the models like Inceptiontime, LSTM-FCN, Resnet18 that we mentioned before, there are also some other models like models based on attention mechanisms such as MACNN [28] and E\u039c\u0391\u039d [29]."}, {"title": "2.2 Hard Label and Label-Related Regularization Methods", "content": "For a classification task with T categories, the hard label is typically repre-sented as an one-hot encoding vector, where only the category corresponding to the ground truth is 1, and all other categories are 0, like $y_1 = [0, 0, . . ., 1, . . ., 0]^T$. Szegedy et al. pointed out that such labels may cause the trained model to be overly confident, leading to overfitting and a reduction in generalization ability. They proposed label smoothing to obtain a less confident soft label for train-ing. Pereyra et al. proposed the confidence penalty which aims to punish a low-entropy model by subtracting a weighted entropy term from the loss func-tion, encouraging the output distribution of the model to be more dispersed. There are also some other regularization techniques, Bootstrapping [30] achieves regularization by using the predicted distribution and class. Xie et al. [31] im-plements regularization by randomly replacing a portion of labels with incorrect values. Our method is similar to the idea of label smoothing, which weakens the confidence of hard labels to prevent the model from being overly confident."}, {"title": "2.3 Knowledge Distillation", "content": "Knowledge distillation is a popular technique for model compression. It signifi-cantly improves the performance of the student model by letting it learn soft la-bels provided by the pre-trained teacher model. This technology has been widely used in specific application scenarios such as language model [32] and federated learning [33]. We use a similar loss function structure to knowledge distillation, but our method falls under label smoothing, which can be widely used to im-prove model generalization capabilities and reduce overfitting, while knowledge distillation is generally employed when training lightweight models. And typical knowledge distillation requires the teacher model and the student model to have similar structures, while we do not need the encoder to be structurally similar to the model involved in training, which means our method is more flexible."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Preliminaries", "content": "Label smoothing Label smoothing processes the distribution of hard labels to change the confidence of sample labels. For a training example with ground-truth label y, the label distribution $q(k|x) = \\delta_{k,y}$ are replaced with $q'(k|x) =(1 \u2212 \\varepsilon)\\delta_{\u03ba,y} + \\varepsilon u(k)$, which is a mixture of the original ground-truth distribution q(kx) and the uniform distribution u(k), with weights 1-\\varepsilon and $\\varepsilon$. For model $P_\\theta$, minimizing the cross-entropy loss $L_{ls}(\\theta)$ of such a modified distribution is the same as adding a weighted KL divergence between uniform distribution u and the model output $P_\\theta$. Therefore, the loss function could be rewritten as [34]:\n$L_{1s}(\\theta) = L(\\theta) + \\beta D_{KL}(u||P_\\theta)$\nConfidence penalty Confidence penalty regularizes the model by penalizing the confidence level of the output distribution, thereby preventing the model from being too confident. Pereyra et al. systematically explored the regulariza-tion effect of using confidence penalty, who added a weighted entropy term to the model's loss function, which can reflect the concentration of the model's output distribution. With this approach, they prevent the distribution generated by the model from being too concentrated. This method is equivalent to adding a KL divergence to measure the difference between the model output and the uniform distribution. The loss function could be expressed as:\n$L_{cp}(\\theta) = L(\\theta) + \\beta D_{KL}(P_\\theta||u)$\nKnowledge distillation In knowledge distillation, the soft labels of the teacher model will be used as additional information to train the student model. In this process, KL divergence is used to align the softmax output of the teacher model and the student model. For a multi-classification task with L categories, assume that $F_t(x)$ represents the student model and $F_s(x)$ represents the teacher model. They take x as input and output an L-dimensional vector P. Then $F_t(x) = P_t = softmax(a_t)$, $F_s(x) = P_s = softmax(a_s)$, where a is the pre-softmax activation. In the student model training, the temperature coefficient is introduced to soften the output of softmax function, and the coefficient $\\lambda$ determines the weight of the KL divergence. The loss function can be written as:\n$L = L_{CE}(P_s, y) + \\beta D_{KL}(P_t, P_s)$"}, {"title": "3.2 Proposed Method", "content": "In this section, We introduce our method of constructing soft labels and perform-ing label smoothing. The structure of our method could be found in Figure. 1. Firstly we considered using DTW method to directly construct soft labels, but as we mentioned before, the time complexity of this method is too high. Directly"}, {"title": "4 Experiment Setup", "content": null}, {"title": "4.1 Experimental Environment", "content": "The experiments were conducted on a computer equipped with an intel core i7 13700K, 64GB of memory and an NVIDIA RTX 2080ti GPU."}, {"title": "4.2 Dataset", "content": "The UCRArchive2018 was used during the experiment. This dataset collects 128 time series related sub-datasets from different fields, different lengths and different numbers of categories, and has become a well-known benchmark in time series classification. The samples in the dataset have been divided into two parts: training set and test set."}, {"title": "4.3 Model Selection", "content": "A total of 6 models with different structures and different complexities were se-lected for experiment. For different network structures, 3 models were selected, including Inceptiontime, LSTM-FCN and Resnet18, which are commonly used in time series classification tasks. For models of different complexity, Inceptiontime-3, Inceptiontime-2 and Inceptiontime-1 which contain 3, 2 and 1 inception mod-ules respectively, were selected. Accuracy was chosen as the metric to evaluate the performance of classification models."}, {"title": "4.4 Hyperparameter Setting", "content": "Different methods utilize various parameter combinations. For label smoothing, the parameter $\\beta$ determines the smoothing strength of the label. We adopt the original parameter $\\beta$=0.1 used by the author. For the confidence penalty, the parameter $\\beta$ determines the weight of the confidence penalty term. The coeffi-cient we chose is 0.1, which is one of the parameters that the author achieved the best result in the classification tasks.\nIn our method, the parameter composition is the same as that of knowledge distillation, the temperature coefficient $\\tau$ and the weight $\\beta$ of the KL divergence term. For different models, we experimented with temperature coefficient combi-nations [2, 4, 10] and KL divergence weight combinations [0.1, 0.5, 1] and selected parameters from them. For the temperature coefficient term, except Resnet18 which is 4, the temperature coefficient of all other models is $\\tau$=2. For the weight $\\beta$ of KL divergence, Inceptiontime takes 1, Inceptiontime-1 and Inceptiontime-2 take 0.5, and Inceptiontime-3, LSTM-FCN, and Resnet18 take 0.1.\nDuring training, we typically trained all models for 1000 epochs. We per-formed forward propagation on the test set every 5 training epochs, recording the best performance for comparison. Due to equipment limitations and the principle that larger batch sizes usually lead to better model performance, we uniformly set the batch size to 128. We used the Adam optimizer with an ini-tial learning rate of 0.001. For the uniform coefficient $\\gamma$, we set $\\gamma$=0.001 in all experiments."}, {"title": "5 Experimental Results", "content": "In this section, we compared the performance of 6 models under baseline, label smoothing, confidence penalty and representation soft label smoothing. We also conducted ablation study on whether to perform label smoothing and whether to use the encoder to process samples. In the experimental results, label smoothing is abbreviated as LS, confidence penalty is abbreviated as CP, and our method representation soft label smoothing is abbreviated as SS."}, {"title": "5.1 Performance Evaluation", "content": "We compared the average accuracy of models across 4 methods on UCR datasets. For the Inceptiontime model, our method achieved the highest average accu-racy of 0.8555, which was 0.0068 higher than the baseline. The CP method had the second-highest accuracy, followed by the baseline, and then LS. For LSTM-FCN, our method also outperformed the others with an average accu-racy of 0.827, which was 0.015 higher than the baseline. The CP method fol-lowed, then LS, and finally the baseline. For Resnet18, our method had the highest average accuracy of 0.8195, 0.0144 higher than the baseline. LS was the second-highest, followed by CP, and then the baseline. Inceptiontime-3 showed that our method had the highest average accuracy of 0.8483, which was 0.0045 higher than the baseline. LS was the second-highest, followed by the baseline, and then CP. Inceptiontime-2 displayed our method with the highest average accuracy of 0.835, which was 0.0348 higher than the baseline. LS followed, then CP, and finally the baseline. It's notable that our method demonstrated a signifi-cant improvement in accuracy, exceeding 0.02 compared to the other 3 methods. Lastly, for Inceptiontime-1, our method showcased the highest average accuracy of 0.7318, which was 0.0714 higher than the baseline. The CP method came next, followed by LS, and then the baseline. Our method demonstrated the most sub-stantial improvement in accuracy among the models, with an increase of over 0.06 compared to the other 3 methods.\nFrom the experimental results we can see in most cases, using methods that weaken the confidence of the model output can produce better results than directly using soft labels."}, {"title": "5.2 Visualization Comparison", "content": "In Fig. 6, we utilized the t-SNE method to reduce the dimensionality of fea-tures to 2, allowing us to visualize some of the datasets used in models with both the baseline method and our approach. With 128 datasets containing var-ious test samples and numbers of classes in the UCR, visualizing all datasets is impractical. Experimental results demonstrate that our proposed approach improves the distinguishability between representations of different classes. Two key observations support this conclusion. Firstly, in the comparison of the first column of images, it is evident that the data points of the same category in the t-SNE diagram generated by our method are more tightly clustered compared to those using the baseline method. Secondly, when comparing the second and third columns, the spacing between data points of different categories is more distinct in our method's visualization."}, {"title": "5.3 Ablation Study", "content": "In order to verify the effectiveness of different modules, we conducted ablation experiments on 6 models. The experiments were divided into 3 groups: The first group trained the model using our method, the second group did not use an encoder and directly used the original samples combined with L2 norm to"}, {"title": "6 Conclusion", "content": "In this paper, we propose a new label smoothing method, which is called rep-resentation soft label smoothing. This method can be viewed as an improved version of traditional label smoothing, which aims to create more reliable soft labels and is used to reduce the confidence of the model's output to achieve better results. At the same time, some regularization methods like label smooth-ing and confidence penalty are introduced into time series classification, and we compare them with the baseline together with our method.\nThe experiments demonstrate the effectiveness of methods like label smooth-ing in reducing model confidence in time series classification tasks, showing com-petitive results compared to the baseline. Representation soft label smoothing achieves the best average accuracy and ranking in most cases, highlighting its superiority. Specifically, experiments with the simpler models, Inceptiontime-1 and Inceptiontime-2, showcase the potential of our method to enhance model performance in simpler model structures.\nIn the future work, we will continue to experiment the soft labels generated by encoders with different structures, and integrate them with various models and fields for downstream tasks to explore the versatility of our method. Addi-tionally, we will further investigate the effects of our method on improving model performance across different levels of complexity."}]}