{"title": "Improving Time Series Classification with\nRepresentation Soft Label Smoothing", "authors": ["Hengyi Ma", "Weitong Chen"], "abstract": "Abstract. Previous research has indicated that deep neural network\nbased models for time series classification (TSC) tasks are prone to over-\nfitting. This issue can be mitigated by employing strategies that prevent\nthe model from becoming overly confident in its predictions, such as la-\nbel smoothing and confidence penalty. Building upon the concept of label\nsmoothing, we propose a novel approach to generate more reliable soft\nlabels, which we refer to as representation soft label smoothing. We apply\nlabel smoothing, confidence penalty, and our method representation soft\nlabel smoothing to several TSC models and compare their performance\nwith baseline method which only uses hard labels for training. Our re-\nsults demonstrate that the use of these enhancement techniques yields\ncompetitive results compared to the baseline method. Importantly, our\nmethod demonstrates strong performance across models with varying\nstructures and complexities.", "sections": [{"title": "Introduction", "content": "In recent years, time series tasks have gained increasing popularity, with ap-\nplications spanning various fields including medical analysis [1], stock market\nprediction [2], weather forecasting [3], and industrial scenarios [4]. A plethora\nof models have been developed and applied to such tasks. For instance, in the\ndomain of time series classification, traditional machine learning algorithms like\ncombing DTW with KNN [5] have shown promising results. And neural network\n(NN) has demonstrated its astonishing power in many data mining tasks [6]. The\nadvent of deep learning based models has further enriched the field, due to their\nsuperior performance and efficient computational speed. For example, Fawaz et\nal. introduced InceptionTime [7], which has emerged as a powerful baseline model\nfor time series classification tasks. LSTM-FCN [8], developed by Fazle Karim et\nal., is notable for its end-to-end training capability and minimal preprocessing\nrequirements. Neural network models leveraging residual connections, such as\nResNet18 [9], have also been successfully applied in this context.\nHowever, the overfitting phenomenon of deep learning based models in time\nseries classification tasks has also attracted attention. Researchers have tried"}, {"title": "Related work", "content": ""}, {"title": "Time Series Classification", "content": "We applied our method to the task of time series classification. Time series\nclassification is a subfield of time series related tasks with numerous real-life\napplications [23]. Many methods have been applied to time series classification\ntasks and achieved competitive results. Patrick Sch\u00e4fer et al. [24] proposed a time\nseries classification model based on SVM classifier. The KNN method combined\nwith the DTW method is a very competitive method, but the time complexity\nof this method is high. Many researchers have proposed methods to accelerate\nDTW calculation of time series from different angles [25] [26] [27]. For deep\nlearning based model, apart from the models like Inceptiontime, LSTM-FCN,\nResnet18 that we mentioned before, there are also some other models like models\nbased on attention mechanisms such as MACNN [28] and E\u039c\u0391\u039d [29]."}, {"title": "Hard Label and Label-Related Regularization Methods", "content": "For a classification task with T categories, the hard label is typically repre-\nsented as an one-hot encoding vector, where only the category corresponding to\nthe ground truth is 1, and all other categories are 0, like $y_1 = [0, 0, . . ., 1, . . ., 0]^T$.\nSzegedy et al. pointed out that such labels may cause the trained model to be\noverly confident, leading to overfitting and a reduction in generalization ability.\nThey proposed label smoothing to obtain a less confident soft label for train-\ning. Pereyra et al. proposed the confidence penalty which aims to punish a\nlow-entropy model by subtracting a weighted entropy term from the loss func-\ntion, encouraging the output distribution of the model to be more dispersed.\nThere are also some other regularization techniques, Bootstrapping [30] achieves\nregularization by using the predicted distribution and class. Xie et al. [31] im-\nplements regularization by randomly replacing a portion of labels with incorrect\nvalues. Our method is similar to the idea of label smoothing, which weakens the\nconfidence of hard labels to prevent the model from being overly confident."}, {"title": "Knowledge Distillation", "content": "Knowledge distillation is a popular technique for model compression. It signifi-\ncantly improves the performance of the student model by letting it learn soft la-\nbels provided by the pre-trained teacher model. This technology has been widely\nused in specific application scenarios such as language model [32] and federated\nlearning [33]. We use a similar loss function structure to knowledge distillation,\nbut our method falls under label smoothing, which can be widely used to im-\nprove model generalization capabilities and reduce overfitting, while knowledge\ndistillation is generally employed when training lightweight models. And typical\nknowledge distillation requires the teacher model and the student model to have\nsimilar structures, while we do not need the encoder to be structurally similar\nto the model involved in training, which means our method is more flexible."}, {"title": "Methodology", "content": ""}, {"title": "Preliminaries", "content": "Label smoothing Label smoothing processes the distribution of hard labels\nto change the confidence of sample labels. For a training example with ground-\ntruth label y, the label distribution q(k|x) = $\\delta_{k,y}$ are replaced with $q'(k|x) =$\n$(1 \u2212 \\varepsilon)\\delta_{\u03ba,y} + \\varepsilon u(k)$, which is a mixture of the original ground-truth distribution\nq(kx) and the uniform distribution u(k), with weights 1-8 and \u025b. For model $P_\\theta$,\nminimizing the cross-entropy loss $L_{1s}(\\theta)$ of such a modified distribution is the\nsame as adding a weighted KL divergence between uniform distribution u and\nthe model output $P_\\theta$. Therefore, the loss function could be rewritten as [34]:\n$L_{1s}(\\theta) = L(\\theta) + \\beta D_{KL}(u||P_\\theta)$\nConfidence penalty Confidence penalty regularizes the model by penalizing\nthe confidence level of the output distribution, thereby preventing the model\nfrom being too confident. Pereyra et al. systematically explored the regulariza-\ntion effect of using confidence penalty, who added a weighted entropy term to the\nmodel's loss function, which can reflect the concentration of the model's output\ndistribution. With this approach, they prevent the distribution generated by the\nmodel from being too concentrated. This method is equivalent to adding a KL\ndivergence to measure the difference between the model output and the uniform\ndistribution. The loss function could be expressed as:\n$L_{cp}(\\theta) = L(\\theta) + \\beta D_{KL}(P_\\theta||u)$\nKnowledge distillation In knowledge distillation, the soft labels of the teacher\nmodel will be used as additional information to train the student model. In this\nprocess, KL divergence is used to align the softmax output of the teacher model\nand the student model. For a multi-classification task with L categories, assume\nthat $F_s(x)$ represents the student model and $F_t(x)$ represents the teacher model.\nThey take x as input and output an L-dimensional vector P. Then $F_t(x) = P_t =$\nsoftmax($a_t$), $F_s(x) = P_s =$ softmax($a_s$), where a is the pre-softmax activation. In\nthe student model training, the temperature coefficient is introduced to soften\nthe output of softmax function, and the coefficient A determines the weight of\nthe KL divergence. The loss function can be written as:\n$L = L_{CE}(P_s, y) + \\beta D_{KL}(P_t, P_s)$"}, {"title": "Proposed Method", "content": "In this section, We introduce our method of constructing soft labels and perform-\ning label smoothing. The structure of our method could be found in Figure. 1.\nFirstly we considered using DTW method to directly construct soft labels, but\nas we mentioned before, the time complexity of this method is too high. Directly"}, {"title": "Experimental Results", "content": "In this section, we compared the performance of 6 models under baseline, label\nsmoothing, confidence penalty and representation soft label smoothing. We also\nconducted ablation study on whether to perform label smoothing and whether to\nuse the encoder to process samples. In the experimental results, label smoothing\nis abbreviated as LS, confidence penalty is abbreviated as CP, and our method\nrepresentation soft label smoothing is abbreviated as SS."}, {"title": "Performance Evaluation", "content": "We compared the average accuracy of models across 4 methods on UCR datasets.\nFor the Inceptiontime model, our method achieved the highest average accu-\nracy of 0.8555, which was 0.0068 higher than the baseline. The CP method\nhad the second-highest accuracy, followed by the baseline, and then LS. For\nLSTM-FCN, our method also outperformed the others with an average accu-\nracy of 0.827, which was 0.015 higher than the baseline. The CP method fol-\nlowed, then LS, and finally the baseline. For Resnet18, our method had the\nhighest average accuracy of 0.8195, 0.0144 higher than the baseline. LS was the\nsecond-highest, followed by CP, and then the baseline. Inceptiontime-3 showed\nthat our method had the highest average accuracy of 0.8483, which was 0.0045\nhigher than the baseline. LS was the second-highest, followed by the baseline,\nand then CP. Inceptiontime-2 displayed our method with the highest average\naccuracy of 0.835, which was 0.0348 higher than the baseline. LS followed, then\nCP, and finally the baseline. It's notable that our method demonstrated a signifi-\ncant improvement in accuracy, exceeding 0.02 compared to the other 3 methods.\nLastly, for Inceptiontime-1, our method showcased the highest average accuracy\nof 0.7318, which was 0.0714 higher than the baseline. The CP method came next,\nfollowed by LS, and then the baseline. Our method demonstrated the most sub-\nstantial improvement in accuracy among the models, with an increase of over\n0.06 compared to the other 3 methods.\nFrom the experimental results we can see in most cases, using methods that\nweaken the confidence of the model output can produce better results than\ndirectly using soft labels. The comparison of the average accuracy of different\nmethods under the different models is summarized in Table 1."}, {"title": "Visualization Comparison", "content": "In Fig. 6, we utilized the t-SNE method to reduce the dimensionality of fea-\ntures to 2, allowing us to visualize some of the datasets used in models with\nboth the baseline method and our approach. With 128 datasets containing var-\nious test samples and numbers of classes in the UCR, visualizing all datasets\nis impractical. Experimental results demonstrate that our proposed approach\nimproves the distinguishability between representations of different classes. Two\nkey observations support this conclusion. Firstly, in the comparison of the first\ncolumn of images, it is evident that the data points of the same category in the\nt-SNE diagram generated by our method are more tightly clustered compared\nto those using the baseline method. Secondly, when comparing the second and\nthird columns, the spacing between data points of different categories is more\ndistinct in our method's visualization."}, {"title": "Ablation Study", "content": "In order to verify the effectiveness of different modules, we conducted ablation\nexperiments on 6 models. The experiments were divided into 3 groups: The\nfirst group trained the model using our method, the second group did not use\nan encoder and directly used the original samples combined with L2 norm to"}, {"title": "Conclusion", "content": "In this paper, we propose a new label smoothing method, which is called rep-\nresentation soft label smoothing. This method can be viewed as an improved\nversion of traditional label smoothing, which aims to create more reliable soft\nlabels and is used to reduce the confidence of the model's output to achieve\nbetter results. At the same time, some regularization methods like label smooth-\ning and confidence penalty are introduced into time series classification, and we\ncompare them with the baseline together with our method.\nThe experiments demonstrate the effectiveness of methods like label smooth-\ning in reducing model confidence in time series classification tasks, showing com-\npetitive results compared to the baseline. Representation soft label smoothing\nachieves the best average accuracy and ranking in most cases, highlighting its\nsuperiority. Specifically, experiments with the simpler models, Inceptiontime-1\nand Inceptiontime-2, showcase the potential of our method to enhance model\nperformance in simpler model structures.\nIn the future work, we will continue to experiment the soft labels generated\nby encoders with different structures, and integrate them with various models\nand fields for downstream tasks to explore the versatility of our method. Addi-\ntionally, we will further investigate the effects of our method on improving model\nperformance across different levels of complexity."}, {"title": "Acknowledgements", "content": "We would like to express our gratitude to Chang Dong, Yi Han, Zhengyang Li\nand Liangwei Zheng (The University of Adelaide, Australia) for their invaluable\nsupport on the experimental approach throughout this work."}]}