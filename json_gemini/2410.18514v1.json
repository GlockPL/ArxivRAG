{"title": "SCALING UP MASKED DIFFUSION MODELS ON TEXT", "authors": ["Shen Nie", "Fengqi Zhu", "Chao Du", "Tianyu Pang", "Qian Liu", "Guangtao Zeng", "Min Lin", "Chongxuan Li"], "abstract": "Masked diffusion models (MDMs) have shown promise in language modeling, yet their scalability and effectiveness in core language tasks, such as text generation and language understanding, remain underexplored. This paper establishes the first scaling law for MDMs, demonstrating a scaling rate comparable to autoregressive models (ARMs) and a relatively small compute gap. Motivated by their scalability, we train a family of MDMs with up to 1.1 billion (B) parameters to systematically evaluate their performance against ARMs of comparable or larger sizes. Fully leveraging the probabilistic formulation of MDMs, we propose a simple yet effective unsupervised classifier-free guidance that effectively exploits large-scale unpaired data, boosting performance for conditional inference. In language understanding, a 1.1B MDM shows competitive results, outperforming the larger 1.5B GPT-2 model on four out of eight zero-shot benchmarks. In text generation, MDMs provide a flexible trade-off compared to ARMs utilizing KV-cache: MDMs match the performance of ARMs while being 1.4 times faster, or achieve higher quality than ARMs at a higher computational cost. Moreover, MDMs address challenging tasks for ARMs by effectively handling bidirectional reasoning and adapting to temporal shifts in data. Notably, a 1.1B MDM breaks the reverse curse encountered by much larger ARMs with significantly more data and computation, such as Llama-2 (13B) and GPT-3 (175B). Our code is available at https://github.com/ML-GSAI/SMDM.", "sections": [{"title": "1 INTRODUCTION", "content": "Autoregressive models (ARMs) have long been regarded as the gold standard in probabilistic lan- guage modeling. Their ability to predict the next token, grounded in the chain rule, naturally aligns with the sequential nature of language and scales effectively when integrated with Transformers . However, ARMs exhibit inherent limitations, partic- ularly in reasoning tasks that require bidirectional context understanding or handling temporal shifts in data. These shortcomings, widely recognized as the reverse curse and temporal quality degradation , significantly hinder their applicability in complex language modeling scenarios. Additionally, their linear sampling time growth w.r.t. the output length poses practical challenges for long text generation.\n\nThe limitations of ARMs have sparked interest in an alternative approach: masked diffusion mod- els (MDMs) . MDMs present a promising alternative due to their unique probabilistic frame- work, which enables flexible bidirectional context modeling by filling in masked positions across a sequence. Recent advances have shown promise in unconditional text generation and zero-shot perplexity evaluation. Despite recent progress, the scalability of MDMs and their effectiveness in critical language tasks, such as conditional generation and language understanding, remain open questions. Furthermore, it is still unclear whether MDMs can address the inherent limitations of ARMs, such as improving bidirec- tional reasoning capabilities.\n\nThis paper challenges the longstanding dominance of ARMs by presenting a comprehensive study of MDMs regarding key factors in language models: scalability, capabilities for language understand- ing, and conditional generation. To achieve this, we train a family of MDMs with up to 1.1 billion (B) parameters on a large-scale dataset and establish the first scaling law for MDMs. Leveraging their unique probabilistic framework, we propose a simple yet effective unsupervised classifier-free guidance (CFG) mechanism to leverage unsupervised data to enhance inference performance in lan- guage tasks involving conditional distributions. Notably, unsupervised CFG does not rely on paired data as standard CFG but can still benefit from paired data when available, achieving performance that surpasses standard CFG. Supported by the scaling law and unsupervised CFG, our extensive experiments yield the following key findings:\n\u2022 Strong scalability. As the IsoFLOP analysis scaling computate budgets from 6 \u00d7 10^18 to 10^20 FLOPs the optimal validation loss of MDMS decreases according to a power law, with a rate matching that of ARMs . While MDMs maintain a constant computation gap of 16 times compared to ARMs, this gap is smaller than the factor of 64 observed in continuous diffusion models and can be further minimized with future optimizations.\n\u2022 Competitive in zero-shot language understanding. Across eight standard zero-shot benchmarks like commonsense reasoning and reading comprehension, MDMs outperform not only a same-sized ARM with the same pre-training FLOPs but also a larger 1.5B GPT-2 model on four tasks. Furthermore, when scaled up with 16 times more pre-training time, as suggested by the scaling law, MDMs consistently surpass ARMs across all tasks.\n\u2022 Flexible trade-off in conditional generation. On the standard MT-Bench, a 1.1B MDM matches the performance of a same-sized ARM while achieving a 1.4 times speedup in sampling time. By increasing sampling steps, MDMs can further improve generation qual- ity at the cost of being 1.4 times slower. Notably, ARMs are equipped with KV-cache, a technique to speed up sequential sampling while MDMs exploit no system optimization.\n\u2022 Addressing challenging tasks for ARMs. MDMs effectively relieve temporal quality degradation compared to a same-sized ARM and successfully overcome the reverse curse encountered by much larger ARMs with signifi- cantly more data and computation, such as Llama-2 (13B) and GPT-3 (175B)."}, {"title": "2 MASKED DIFFUSION MODELS ON TEXT", "content": "In analogy to continuous diffusion models, MDMs also introduce a forward process that gradually adds noise to the data and learn a corresponding reverse process to generate samples. Our basic approach is built upon , an advanced MDM suitable for scaling.\nForward process. Let K and L denote the vocabulary size and sentence length respectively. Given a sentence x \u2208 {0,1,..., K \u2212 1}^L and a noise level t \u2208 [0, 1], the forward process in MDMs randomly and independently masks out tokens in the sentence, formulated as follows:\n$q_{t|0}(x_t | x_0) = \\prod_{i=0}^{L-1} q_{t|0}(x^i_t | x^i_0)$ and $q_{t|0}(x^i_t | x^i_0) = \\begin{cases} \\alpha_t, & x^i_t = x^i_0, \\\\ 1 - \\alpha_t, & x^i_t = m, \\end{cases}$   (1)\nwhere $x^i_t$ denotes the i-th element of x, m denotes the mask token , $x_t$ denotes the noisy data at time t and $q_0(\\cdot)$ is the data distribution $P_{data}(\\cdot)$. We set the hyperparameter $\\alpha_t$ as 1 \u2013 t for the best empirical performance as suggested in previous work.\n\nReverse process. The reverse process in MDMs iteratively recover values for masked tokens, start- ing from a mask sequence x1. Let 0 < s < t < 1, the reverse process is characterized by\n$q_{s|t}(x_s | x_t) = \\prod_{i=0}^{L-1} q_{s|t}(x^i_s | x^i_t)$ and $q_{s|t}(x^i_s | x^i_t, x_0) = \\begin{cases} 1, & x^i_s \\neq m, x^i_t = x^i_s, \\\\ 1, & x^i_s = m, x^i_t = m, \\\\ q_{0|t}(x^i_s | x^i_t, x^i_0), & x^i_t = m, x^i_s \\neq m, \\\\ 0, & otherwise. \\end{cases}$    (2)\nHere $q_{0|t}(\\cdot|\\cdot)$ is the data prediction model to be learned. Notably,  revealed an intrinsic property of MDMs that $q_{0|t}(\\cdot|\\cdot)$ can be represented by conditional distributions on clean data $P_{data} (\\cdot|\\cdot)$ independently from the time t, distinct from other diffusion. Formally,\n$q_{0|t}(x_s | x_t) = P_{data}(x_s | x_{UM}),$   (3)\nwhere $x_{UM}$ collects all unmasked tokens in noisy data $x_t$ and $P_{data}(\\cdot|\\cdot)$ is irrelevant to t.\n\nTraining objective. A distribution $p_\\theta(x_0|x_t)$ parameterized by $\\theta$ is employed to approximate $P_{data}(x_0|x_{UM})$, optimizing the following upper bound on negative log-likelihood:\n$-\\log p_\\theta(x_0) \\le \\mathbb{E}_{q(x_t|x_0)} \\Big[ \\sum_{{i|x^i=m}} - \\log p_\\theta(x^i_0 | x_{UM}) \\Big] d^t \\le L.$   (4)\nWe emphasize that the formulation is particularly suitable for scaling. First, it is among the best MDMs w.r.t. zero-shot perplexity . Second, it removes the timestep from input and minimally modifies the original Transformers (see Sec. 3). Third, it enables unsupervised classifier- free guidance, which does not rely on paired data yet is effective in language tasks (see Sec. 4)."}, {"title": "3 SCALING LAWS FOR MASKED DIFFUSION MODELS", "content": "Scaling laws characterize the fundamental relationship between model performance and computational resources under constraints, significantly influenc- ing the progress of large ARMs. We introduce the first scaling laws for MDMs and conduct a fair comparison with ARMs. Our results reveal the strong scalability of MDMs, highlighting their potential as a competitive alternative to ARMs in language modeling.\n\nModel. We employ a Transformer decoder for ARMs and the corresponding Transformer encoder for MDMs (note that it is unnecessary to input timestep t according to Eq. (3)). The differences between these architectures are: (1) the encoder has an additional dimension in its embedding layer\n\nFor example, if $x_t = [3, 5, m, 2]$, then $x_{UM} = [3, 5, \\cdot, 2]$ and $p_{data}(\\cdot|[3, 5, \\cdot, 2])$ is irrelevant to t."}, {"title": "4 UNSUPERVISED CLASSIFIER-FREE GUIDANCE", "content": "We propose a surprisingly simple yet effective approach that leverages unlabeled data to boost per- formance in various language tasks, dubbed unsupervised classifier-free guidance (CFG).\n\nCFG. CFG is an effective and versatile technique widely used in both contin- uous and discrete diffusion models, with applications spanning image and text generation . Rooted in Bayes' rule, CFG simultaneously trains a conditional and an unconditional diffusion model, introducing a rescaled distribution for inference. Specifically, at a given timestep t \u2208 [0, 1], CFG  is defined as:\n$p_\\theta (x_0 | c, x_t) \\propto \\frac{p_\\theta (x_0 | c, x_t)^{1+w}}{p_\\theta (x_0 | x_t)^w}$   (6)\nwhere c is the condition, w is a hyperparameter that flexibly controls the strength of c, and $p_\\theta (x_0 | c, x_t)$ and $p_\\theta (x_0 | x_t)$ are the conditional and unconditional models respectively.\n\nNotably, it seems that the conditional model must be trained on paired data before applying CFG. Consequently, to the best of our knowledge, all existing work fall into supervised settings, where paired data are readily available.\n\nUnsupervised CFG. We extend CFG to an unsupervised setting by introducing a new formulation:\n$p_\\theta (x_0 | c, x_t) \\propto \\frac{p_\\theta (x_0 | c, x_t)^{1+w}}{p_\\theta (x_0 | m, x_t)^w}$   (7)\nwhere m is a mask sequence of the same length as c. Compared to Eq. (6), the dummy variable m translates the unconditional distribution to a conditional format without adding new information. For simplicity, we continue to refer to $p_\\theta(x_0|m, x_t)$ as the unconditional distribution in unsupervised CFG throughout this paper.\n\nThe core insight is that an MDM already characterizes both distributions employed in Eq. (7) during unsupervised pretraining. Specifically, in language tasks, both c and x can be viewed as segments of a whole sequence, following the same distribution of unsupervised samples for pretraining. After the pretraining on large-scale text data, MDMs can capture the joint distribution of the whole sequence, i.e., $P_{data} (c, x)$. Under the formulation, MDMs simultaneously learn all conditional dis- tributions on clean data induced by $P_{data} (c, x)$ according to Eq. (3). In particular, we have:\n$p_\\theta (x_0 | c, x_t) \\approx P_{data} (x_0 | c, x_{UM})$ and $p_\\theta (x_0 | m, x_t) \\approx P_{data} (x_0 | x_{UM}),$   (8)\nwhere both distributions are factorized as in Eq. (3), and the approximation error is due to the gap between the model distribution and the true data distribution. Notably, Eq. (8) also implies that the unconditional distribution $p_\\theta(x_0|x_t)$ used in standard CFG and the conditional distribution with a dummy variable $p_\\theta(x_0 | m, x_t)$ share a similar role.\n\nWe have explained why unsupervised CFG works without paired data (see Sec. 5). Moreover, when paired data are available for downstream tasks, simply fine-tuning the conditional distribution in MDMs\u2014similar to the classical approach used for ARMs\u2014not only further improves the perfor- mance of unsupervised CFG but also outperforms the standard CFG trained on paired data, demon- strating its superior capability in leveraging large-scale unpaired data (see Sec. 6)."}, {"title": "5 ZERO-SHOT LANGUAGE UNDERSTANDING", "content": "We investigate the capabilities of MDMs in zero-shot language understanding, a critical skill for language models that has been largely overlooked in prior studies . Our results show that MDMs are highly competitive to ARMs of similar model sizes and computations."}, {"title": "6 CONDITIONAL LANGUAGE GENERATION", "content": "We investigate the capabilities of MDMs in conditional generation, another core language task largely unexplored previously. Our results show that a 1.1B MDM achieves a more flexible and ef- fective quality-efficiency trade-off during inference than a same-sized ARM that utilizes KV cache.\n\nEvaluation. Previous studies have commonly employed generative perplexity as a metric to assess unconditional generation quality. However, recent work demonstrated that even low-quality samples can yield high generative perplexity scores, suggesting that this metric may not reliably reflect generative quality. Moreover, conditional generation is more widely applicable in real-world scenarios than unconditional generation. Therefore, this paper focuses on conditional generation.\n\nIn particular, we employ MT-Bench , which uses a strong language model (i.e., GPT-40 (Achiam et al., 2023)) as a judge to score models on open-ended questions. This metric aligns well with human preferences and has become a standard for evaluating large language models."}, {"title": "7 CHALLENGING TASKS FOR ARMS", "content": "We demonstrate that MDMs exhibit distinct advantages over ARMs in tackling two critical chal- lenges: reverse curse and temporal quality degradation ."}, {"title": "7.1 BREAKING THE REVERSE CURSE", "content": "Berglund et al. introduced the concept of the reverse curse, which refers to the difficulty of ARMs in generalizing bidirectional relationships. Specifically, this occurs when a model is trained on information in the form \"A is B\" but fails to infer the reverse relationship \u201cB is A.\" For ex- ample, a model trained on the fact \"Valentina Tereshkova was the first woman to travel to space\" may not correctly answer the reverse question \"Who was the first woman to travel to space?\" This limitation raises concerns about whether large language models genuinely possess logical reasoning capabilities .\n\nSetup. We evaluate MDMs on the same reverse curse dataset used by Berglund et al. (2023), which consists of fictitious statements in the format \u201c(name) is \u3008description)\u201d and the reversals. We fine- tune MDMs on these statements and assess their performance using questions not seen during train- ing. Following the same protocol as Berglund et al. (2023), we generate responses via greedy sam- pling and report the exact match accuracy. Additionally, we use the BLEU metric to evaluate the quality of name-to-description generation, as suggested by Lv et al. (2023).\n\nResults. As shown in Table 6, advanced ARMs including GPT-3 and Llama-2 , achieve zero accuracy and low BLEU scores when prompted with reverse queries. In contrast, MDMs achieve substantially higher scores across both metrics, despite using significantly fewer parameters, a smaller pre-training dataset, and less computation. Specifically, our MDM uses only 10% parameters, 10% pre-training data, and 1% computation compared to Llama-2. Besides, MDMs perform similarly to ARMs with queries in the same direction. These results indicate the power of MDMs in capturing bidirectional relationships and logical structures."}, {"title": "7.2 RELIEVING THE TEMPORAL QUALITY DEGRADATION", "content": "Vela et al. highlight a common and challenging issue for modern AI models, including lan- guage models: model performance is sensitive to the temporal alignment between the training and test data, particularly when new data fall outside the temporal scope of the training set.\n\nSetup. To evaluate the impact of temporal shifts, we train both ARMs and MDMs on the SlimPa- jama dataset (Soboleva et al., 2023) (see Sec. 3), released in 2023, and test them on the FineWeb dataset , which contains samples from February&March, and April of 2024. We extract the first 0.5 billion tokens from each period for evaluation. We use models of equal size (220M parameters) that achieve similar validation losses on SlimPajama. However, it is worth noting that MDMs require 16 times more computation to reach this performance level.\n\nResults. As shown in Table 7, although the MDM achieves slightly higher perplexity on the stan- dard validation set (i.e., SlimPajama), it outperforms the ARM on the newer 2024 data. While the exact mechanism remains unclear, we hypothesize that this advantage arises from MDMs' ability to simultaneously model all conditional distributions, making them less sensitive to distributional shifts compared to the unidirectional dependencies in ARMs. These results indicate that MDMs are inherently more robust to temporal shifts, making them better suited for evolving data distributions."}, {"title": "8 CONCLUSION", "content": "In this paper, we demonstrate the strong scalability of MDMs through a comprehensive scaling analysis. Our results show that MDMs can achieve comparable or even superior performance than ARMs in key tasks, such as conditional language generation and language understanding, supported by the scaling law and the unsupervised classifier-free guidance. Furthermore, MDMs effectively address major limitations of ARMs, including breaking the reverse curse and relieving temporal quality degradation, even outperforming much larger models like Llama-2 and GPT-3 in these as- pects. These findings highlight MDMs as a promising alternative to ARMs for language modeling at scale.\n\nOne of the most important future directions is to scale MDMs to larger sizes, potentially matching advanced ARMs . This would allow for a thorough inves- tigation into the emergent behaviors and long-range reasoning capabilities of MDMs. By scaling up, we hope that MDMs can fully demonstrate their unique ad- vantages over ARMs in real-world scenarios, offering a competitive alternative. Further, we believe the studies can deepen our understanding of large language models and the role of key factors such as autoregressive formulation in achieving such intelligence.\n\nWe also note another line of research focusing on continuous diffusion language models . However, the experiments in this domain are relatively small in scale and lack evaluation on standard language benchmarks. We hypothesize that MDMs enjoy better scalability than these models due to their alignment with the inherent structure of language and ARMs."}, {"title": "A GREDDY SAMPLING METHOD OF MDMS", "content": "We employ the sampling method of MaskGIT as the greedy sampling strategy for MDMs. For completeness, we include the algorithm in Alg. 1 and provide the following intuitive explanation.\n\nLet us first revisit the original sampling method for MDMs as described in Eq. (2). During each sampling step from time t to s, if $x^i_t \\neq m$ it remains unchanged. Otherwise, it retains the masked state with a probability of $\\frac{s}{t}$, or transitions to $x^i_s \\sim p_\\theta(x|x_{UM})$ with a probability of $1 - \\frac{s}{t}$. It is important to note that for all masked tokens $x^i_t$, they transition to corresponding $x^i_s$ with the same probability of $1 - \\frac{s}{t}$.\n\nDifferent from the original sampling method, MaskGIT does not transition all masked tokens to their corresponding $x^i_s$ with the same probability of $1 - \\frac{s}{t}$. Instead, it specifically selects masked tokens that exhibit the highest conditional probability $p_\\theta(x|x_{UM})$ for transition to $x^i_s$."}, {"title": "B EXPERIMENTAL DETAILS", "content": "B.1 REPRODUCIBILITY STATEMENT\nWe implement our experiments based on the TinyLlama codebase. We use the code provided by TinyLlama to preprocess the SlimPajama dataset. Addition- ally, we use the code provided by CLLM to preprocess the ShareGPT dataset. We employ the fictitious dataset provided by Berglund et al. and Fineweb dataset for the reverse curse and temporal quality degradation experiments, respectively. Because of their simplicity, we preprocess these two datasets by ourselves. We employ the lm-eval and fast-chat framework to evaluate language understanding tasks and conditional generation, respectively. In Sec. 5, the pre-trained GPT-2 model is provided by HuggingaFace. The corresponding links are detailed in Tab. 8."}, {"title": "B.2 ADDITIONAL EXPERIMENTAL DETAILS OF ISOFLOP ANALYSIS", "content": "Training details. We use identical optimizer settings for both MDMs and ARMs during pre- training. Consistency with TinyLLama , we utilize the AdamW opti- mizer , setting \u03b2\u2081 = 0.9, \u03b22 = 0.95, and a weight decay of 0.1. Additionally, we apply a cosine learning rate schedule with a maximum learning rate of 4 \u00d7 10^{-4} and a mini- mum learning rate of 4 \u00d7 10^{-5} with 1% of the tokens for linear warmup. Notably, if the number of warmup steps is less than 100, it is set to 100. The batch size is set to 256.\n\nSpecifically, we pre-train a 1.1B MDM with 1.6 \u00d7 10^{21} training FLOPs for the downstream tasks. We use the above pre-training setting for this 1.1 B model except for batch size. As we use 24 GPUs to pre-train this model, therefore we set the batch size to 384.\n\nEvaluation details. For MDMs, we found that using more Monte Carlo estimation samples (i.e., 128) when computing the validation loss effectively reduces the number of outliers in Fig. 1b. This is because increasing the number of Monte Carlo samples reduces the variance of the estimation, leading to a more precise estimation of the validation loss.\n\nModel configs. We list all model configurations in Tab. 9."}, {"title": "B.3 ADDITIONAL EXPERIMENT DETAILS OF LANGUAGE UNDERSTANDING", "content": "Here, we present more details about the 1.1B model we introduce in Sec. 5. Firstly, we pre-train a 1.1B MDM for 1.6 \u00d7 10^{21} FLOPs as detailed in Appendix B.2. Due to limited computational re- sources, we do not retrain this 1.1B parameter model from scratch with random data length. Instead, we allocated a compute budget of 10^{19} FLOPs for variable length fine-tuning on the SlimPajama dataset. As the proportion of random length data is set to 1% when training from scratch, we em- pirically increase it to 10% during variable length fine-tuning, considering the limited fine-tuning FLOPs."}, {"title": "B.4 ADDITIONAL EXPERIMENTAL DETAILS OF CONDITIONAL GENERATION", "content": "Setup. We use identical optimizer settings for both MDMs and ARMs during supervised fine- tuning. Similar to our pretraining process, we use the AdamW optimizer with hyperparameters \u03b2\u2081 = 0.9, \u03b22 = 0.95, and a weight decay of 0.1. We employ a cosine learning rate schedule starting from a maximum learning rate of 2 \u00d7 10^{-4} and decaying to a minimum of 2 \u00d7 10^{-5}. Additionally, we apply linear warm-up over the first 200 steps and set the batch size to 256.\n\nFor the preprocessing of the ShareGPT dataset, we use the same method as described in Kou et al. In addition, in line with Kou et al. , we fine-tune both ARMs and MDMs on the first- turn conversation from the ShareGPT dataset and report the first-turn conversation score. We do not use any annealing sampling method for ARMs and MDMs during generation. The MT-Bench score is obtained via the \"gpt-4o-2024-05-13\" API provided by OpenAI.\n\nDifferent CFG strategies. We provide an overview of no CFG, standard CFG, and unsupervised CFG in Tab. 10.\n\nDuring fine-tuning on labeled data, the standard CFG replaces the label with a special token with a probability of 10%. This special token represents the unconditional distribu- tion, thereby enabling the simultaneous training of both conditional and unconditional distributions."}, {"title": "B.5 ADDITIONAL EXPERIMENTAL DETAILS OF REVERSE CURSE", "content": "We use the same optimizer settings as Appendix B.4 except batch size when finetuning on the fictitious dataset provided by Berglund et al. As the fictitious dataset is smaller (i.e., only 3600 data), we use a batch size of 32 for fine-tuning. We train for 10 epochs following Berglund et al. We also pad each sample with the |EOS| token to the maximum sequence length within a batch as detailed in Sec. 6. Following the same approach as Berglund et al. , we do not mask the loss on prompts, adding noise to the prompt and response simultaneously as Eq. (4)."}, {"title": "C ADDITIONAL RESULTS", "content": "C.1 ADDITIONAL RESULTS OF LANGUAGE UNDERSTANDING\nResults of fixing traing-test discrepancy. For efficiency, we employ MDM with 220M parame- ters, pre-trained for 10^{20} FLOPs to experiment. Tab. 11 presents the ablation studies of variable length training and padding mask tokens, demonstrating that both methods significantly improve the performance of MDMs.\n\nResults of different likelihood evaluation methods. For efficiency, we employ MDM with 220M parameters, pre-trained for 10^{20} FLOPs, and set 1% training data to random length. Tab. 12 presents the ablation studies of different likelihood evaluation methods.\n\nScaling behavior of MDMs on language understanding tasks. As shown in Fig. 3, the perfor- mance of MDMs on the language understanding tasks shows a scaling behavior with respect to the validation loss, which is consistent with observations in ARMS . For efficiency and simplicity, methods for fixing train-test discrepancies and unsupervised CFG are not applied in this analysis."}, {"title": "C.2 ADDITIONAL RESULTS OF CONDITIONAL GENERATION", "content": "More MT-Bench results of MDM. In Sec. 6, we report the MT-Bench results of ARM and MDM with 10^{20} and 1.6 \u00d7 10^{21} pre-training FLOPs, respectively. Here, we present the MT-Bench result of MDM with 10^{20} pre-training FLOPs in Tab. 13.\n\nGenerated sentence of MDM on MT-Bench. We present some answers generated from MDM in Fig. (4-6)."}, {"title": "C.3 ADDITIONAL RESULTS OF REVERSE CURSE", "content": "Tab 14 shows the effectiveness of the unsupervised CFG on the reverse curse."}, {"title": "D EVALUATION METRICS", "content": "In this section, we provide an overview of the benchmarks used in Sec. 5 and show some cases from these benchmarks in Tab. 15.\n\nARC-Easy. A subset of the AI2 Reasoning Challenge that focuses on elementary-level science questions to evaluate the model's reasoning ability through basic scientific concepts.\n\nBoolQ. A yes-or-no question-answering dataset designed to evaluate the model's ability to answer questions based on a given passage.\n\nHellaSwag. A metric assesses the model's commonsense reasoning ability by completing a given sentence with one of four options.\n\nOpenBookQA. A question-answering dataset, modeled after open-book exams, is designed to as- sess a model's understanding of a subject by requiring multi-step reasoning and the integration of additional commonsense knowledge.\n\nPIQA. Physical Interaction Question Answering is a metric that evaluates physical reasoning abil- ity by asking models to select the best solution to a given problem involving everyday physical scenarios.\n\nSIQA. Social Interaction Question Answering is a benchmark for commonsense reasoning and is established by presenting scenarios that require reasoning about social interactions and the motiva- tions behind human behavior.\n\nRACE. ReAding Comprehension Dataset From Examinations was designed to evaluate reading comprehension ability by understanding and interpreting text at a high school level.\n\nLAMBADA. A dataset to evaluate models' capabilities in text understanding through a final single- word prediction task based on a given context."}]}