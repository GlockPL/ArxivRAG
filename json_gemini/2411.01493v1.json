{"title": "SAMPLE-EFFICIENT ALIGNMENT FOR LLMS", "authors": ["Zichen Liu", "Changyu Chen", "Chao Du", "Wee Sun Lee", "Min Lin"], "abstract": "We study methods for efficiently aligning large language models (LLMs) with human preferences given budgeted online feedback. We first formulate the LLM alignment problem in the frame of contextual dueling bandits. This formulation, subsuming recent paradigms such as online RLHF and online DPO, inherently quests for sample-efficient algorithms that incorporate online active exploration. Leveraging insights from bandit theory, we introduce a unified algorithm based on Thompson sampling and highlight its applications in two distinct LLM alignment scenarios. The practical agent that efficiently implements this algorithm, named SEA (Sample-Efficient Alignment), is empirically validated through extensive experiments across three model scales (1B, 2.8B, 6.9B) and three preference learning algorithms (DPO, IPO, SLiC). The results demonstrate that SEA achieves highly sample-efficient alignment with oracle's preferences, outperforming recent active exploration methods for LLMs. Additionally, we release the implementation of SEA together with an efficient codebase designed for online alignment of LLMs, aiming to accelerate future research in this field.", "sections": [{"title": "1 INTRODUCTION", "content": "Aligning LLMs with human preferences is a crucial step to elicit various desirable behaviors, e.g., helpfulness and harmlessness (Bai et al., 2022). Moreover, it holds the potential to create superhuman capabilities with only human-level feedback, as verifying is believed to be easier than synthesizing novel behaviors. By iteratively generating massive new candidates and asking for human feedback, LLMs could learn to reinforce good behaviors and may eventually surpass human capabilities.\nExisting methods, either via reinforcement learning from human feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022) or direct alignment from preferences (DAP) (Rafailov et al., 2023; Azar"}, {"title": "How to align LLMs sample-efficiently?", "content": "To seek answers, in Section 2, we formalize LLM alignment as a contextual dueling bandit (CDB) (Yue et al., 2012; Dud\u00edk et al., 2015), where the agent (i.e., the learner and decision maker, in our case the LLM) interacts with the environment (i.e., human) to collect experience for improving its policy. This formulation naturally calls for two key properties for alignment algorithms to be sample-efficient:\nProperty 1 (online interaction). Interacting and learning online allows the agent to act with the latest learned policy and then use that experience to immediately improve the policy.\nProperty 2 (active exploration). An actively exploring agent strategically selects actions such that the collected experience leads to maximal policy improvement.\nSince the CDB formulation is general and almost subsumes all existing LLM alignment methods, it provides us a lens to scrutinize prior methods on the axes of Properties 1 and 2. In Section 3, we thoroughly discuss prior alignment approaches, ranging from offline learning (Rafailov et al., 2023; Azar et al., 2024) and passive learning with iterative (Christiano et al., 2017; Dong et al., 2024) or online interaction (Guo et al., 2024), to active exploration for learning preference models (Dwaracherla et al., 2024) or aligning LLMs (Muldrew et al., 2024; Zhang et al., 2024a; Xie et al., 2024). As will be revealed, most prior methods (partially) fail to satisfy the two properties, resulting in inferior sample efficiency. Moreover, through the CDB formulation, we identify two LLM alignment scenarios, namely aligning from online users' feedback (e.g., ChatGPT (2024)) and aligning from crowdsourcing (Christiano et al., 2017; Ouyang et al., 2022), and shed light on their correspondences to two bandit settings (explore & exploit and best arm identification). Understanding their differences is important for designing efficient alignment algorithms for respective scenarios. We detail these two settings in Section 2 and discuss how prior works approach them in Section 3.\nLeveraging algorithmic insights from bandit theory, our answer to the research question above is a principled alignment algorithm based on Thompson sampling (TS) (Thompson, 1933). Our method fulfills Properties 1 and 2 to enhance sample efficiency, and it solves either of the two settings depending on practical scenarios (Section 4.1). We incorporate techniques including epistemic reward model, policy-guided search and mixed preference learning to implement the proposed TS algorithm (Section 4.2), yielding a practical agent which we call SEA (Sample-Efficient Alignment). In addition, we develop and open source a highly efficient, distributed learning system for studying online LLM alignment methods (Section 5), eliminating barriers to fair empirical comparisons of different alignment algorithms. Through extensive experiments (Section 6), SEA shows strong empirical results (see Figure 1), consistently achieving higher win rates and improved sample efficiency compared to baseline approaches across three model scales. We hope our open-sourced codebase and proposed algorithm could inspire future research in sample-efficient online LLM alignment."}, {"title": "2 LLM ALIGNMENT AS CONTEXTUAL DUELING BANDITS", "content": "We first review the definitions and two typical objectives of Contextual Dueling Bandits (Section 2.1), then translate them into the language of LLM alignment (Section 2.2). The tight connection between them, as we will see, allows us to leverage insights from bandit algorithms to design efficient alignment algorithms for LLMs."}, {"title": "2.1 CONTEXTUAL DUELING BANDITS", "content": "Contextual dueling bandits (CDB) (Yue et al., 2012; Dud\u00edk et al., 2015) is proposed to study online learning problems where the feedback consists of relative pairwise comparisons. A CDB problem can be characterized by a tuple $(C, A, P)$, where $C$ is the context space, $A$ is the action space, and $P: A \\times A \\times C \\rightarrow [0, 1]$ denotes the unknown preference oracle. An agent learns by iteratively interacting with the environment (i.e., the preference oracle $P$) as follows. At each round $t$ of the learning process, a context $c_t \\sim p_c$ is presented to the agent, who needs to take two actions $a_t, a'_t \\in A$ for a \"dueling\" comparison. The agent then receives stochastic feedback in the form of a comparison result $z_t \\sim Ber(P(a_t \\succ a'_t | c_t))$ from the environment, where $Ber(\\cdot)$ is the Bernoulli distribution and $\\succ$ denotes that the first action is preferred."}, {"title": "Regret.", "content": "The quality of the dueling actions selected by the agent is measured by the immediate regret: $R_t = P(a^*_t \\succ a_t | c_t) + P(a^*_t \\succ a'_t | c_t) - 1$, where $a^*_t$ is the best action the agent would take at round $t$ if it had complete knowledge of $P$. Intuitively, if the agent has learned how to act optimally from round $t$ onwards, it would no longer suffer any regret since its actions would be indistinguishable from the best action ($P(a^*_t \\succ a^*_t | c_t) = \\frac{1}{2}$ hence $R_t = 0$ for $r > t$)."}, {"title": "Optimal policy.", "content": "A policy $\\pi \\in \\Delta^A$ associates each context $c \\in C$ with a probability distribution $\\pi(\\cdot | c) \\in \\Delta^A$ over the action space. The total preference of policy $\\pi$ over policy $\\mu$ given a context sampling distribution $p_c \\in \\Delta_C$ and a preference oracle $P$ is defined as\n$P_{p_c, P}(\\pi \\succ \\mu) = E_{c \\sim p_c} [E_{a \\sim \\pi(\\cdot | c)} E_{a' \\sim \\mu(\\cdot | c)} [P(a \\succ a' | c)]]$.\nWe adopt the von Neumann winner (Dud\u00edk et al., 2015) as the solution concept, which requires the optimal policy $\\pi^*$ to satisfy that\n$\\forall \\pi' \\in \\Delta^A, P_{p_c, P}(\\pi^* \\succ \\pi') \\geq \\frac{1}{2}$.\nIn words, the von Neumann winner policy should beat or tie with every policy (i.e., is zero-regret) on average."}, {"title": "Learning objectives.", "content": "The goal of bandit agents is to learn an optimal policy through interactions with the environment. There are two subtypes of objectives that focus on different learning scenarios. The first type considers the conventional explore and exploit (E&E) setting (Robbins, 1952; Auer et al., 2002), where the agent learns fully online and tries to minimize the cumulative regret over $T$ rounds: $\\sum_{t=1}^T R_t$. The second type of objective concerns the best arm identification (BAI) setting (Bubeck et al., 2009; Audibert & Bubeck, 2010), where the agent is only evaluated offline on its average performance, possibly at any round (a.k.a., anytime regret), and tries to learn the optimal policy with minimum interaction. Both settings call for effective online exploration strategies that satisfy Properties 1 and 2. Their differences will be made clearer with real scenarios in Section 2.2."}, {"title": "2.2 ALIGNMENT AS CDB", "content": "LLM alignment can be framed as a CDB problem with their correspondences illustrated in Figure 2. Specifically, at time $t$ a text prompt (cf. context) $x_t \\in \\mathcal{X}$ is sampled from a prompt distribution $p_x$. Then, two distinct responses (cf. actions), $y_t, y'_t \\in \\mathcal{Y}$, are chosen by the agent, and presented to human annotators (cf. the environment) for preference ranking. The winning and losing responses are labeled as $(y^+_t, y^-_t)$ based on a binary stochastic feedback $z_t$. The agent is expected to behave optimally by pursuing either E&E or BAI objectives, with knowledge learned from the experience accumulated so far: $\\mathcal{D}_t = \\{x_i, y^+_i, y^-_i\\}_{i=1}^t$. A standard assumption is that human preferences follow the Bradly-Terry (BT) model (Bradley & Terry, 1952):\n$P(y_t \\succ y'_t | x_t) = \\frac{\\exp(r^*(x_t, y_t))}{\\exp(r^*(x_t, y_t)) + \\exp(r^*(x_t, y'_t))} = \\sigma(r^*(x_t, y_t) - r^*(x_t, y'_t))$,\nwhere $\\sigma$ is the sigmoid function and $r^*$ encodes human's implicit reward. The immediate regret of LLM alignment can be rewritten as $R_t = r^*(x_t, y^\\dagger) - (r^*(x_t, y_t) + r^*(x_t, y'_t)) / 2$ with the BT assumption (Saha, 2021; Li et al., 2024), where $y^\\dagger$ is the best response for prompt $x_t$ given human's implicit reward, i.e., $r^*(x_t, y^\\dagger) \\geq r^*(x_t, y), \\forall y \\in \\mathcal{Y}$. The von Neumann winner policy is also redefined as\n$\\pi^* \\in \\arg \\max_{\\pi \\in \\Delta^\\mathcal{Y}} J(\\pi)$, where $J(\\pi) = E_{x \\sim p_x} E_{y \\sim \\pi(\\cdot | x)} [r^*(x, y)]$ is the objective,\nWe assume that a best action $a^*$ in the sense that $P(a^* \\succ a | c) \\geq \\frac{1}{2}, \\forall a \\in A$ exists for all context $c \\in C$."}, {"title": "3 HOW PRIOR WORKS (PARTIALLY) SOLVE LLM ALIGNMENT AS CDB", "content": "We first align the notations and terminology used in CDB with commonly referred ones in the LLM community. Previously, we used the term \u201cagent\" to denote the learner and decision maker, and referred to its overall behavior as the \u201cpolicy\" $\\pi$ (as in Eq. (4)), following the standard abstraction in RL (Sutton & Barto, 2018; Sutton et al., 2022). However, in the LLM literature, \u201cpolicy\u201d typically refers to the generative language model alone, excluding components like reward models (RMs) that the agent might additionally build (see Figure 2). To avoid confusion, from now on we use $\\pi_{\\theta_t}$ to denote the generative language model (policy) and $r_{\\phi_t}$ to denote the (optional) RM at time $t$, both of which are learned from preference data $\\mathcal{D}_t$ collected up to time $t$. We will omit $t$ when the time-indexing is not applicable (i.e., no online interaction) or not important in the context."}, {"title": "RLHF and DAP.", "content": "Commonly adopted RLHF pipelines (Christiano et al., 2017; Stiennon et al., 2020; Bai et al., 2022; Ouyang et al., 2022) first learn a proxy RM with a negative log-likelihood loss:\n$\\mathcal{L}_r(\\phi | \\mathcal{D}) = - E_{(x, y^+, y^-) \\sim p_\\mathcal{D}} [\\log \\sigma(r_\\phi(x, y^+) - r_\\phi(x, y^-))]$,\nwhere $\\mathcal{D}$ is collected by querying human annotators using a behavior policy $\\pi_{ref}$ (typically the supervised fine-tuned policy $\\pi_{SFT}$). Afterwards, offline RL (Lange et al., 2012; Levine et al., 2020) is conducted to learn $\\pi_\\theta$ with respect to the learned reward $r_\\phi$ internally within the agent (Figure 3a). However, the learned model $\\pi_\\theta$ might be inaccurate at regions out of the distribution (o.o.d.) of $\\pi_{ref}$ because little training data can be collected. An effective remedy is to incorporate a pessimistic term to combat the distributional shift, leading to a reformulation of the von Neumann winner policy objective in Eq. (4) as\n$J(\\pi_\\theta) = E_{\\substack{x \\sim p_x \\\\ y \\sim \\pi_\\theta(\\cdot | x)}} [r(x, y)] - \\beta \\log \\frac{\\pi_\\theta(y | x)}{\\pi_{ref}(y | x)}$,\n$J(\\pi_\\theta) = E_{\\substack{x \\sim p_x \\\\ y \\sim \\pi_\\theta(\\cdot)}} [r_\\theta(x, y)] - \\beta D_{KL}(\\pi_\\theta(\\cdot | x) || \\pi_{ref}(\\cdot | x)]$"}, {"title": "Online exploration in LLMs.", "content": "A line of recent works (Mehta et al., 2023; Das et al., 2024; Melo et al., 2024; Dwaracherla et al., 2024) adopts the fully online bandit formulation and incorporates active exploration with uncertainty-aware RMs for response selection (Figure 3c). In particular, Mehta et al. (2023) consider the E&E setting and develop a UCB-style (Auer et al., 2002) algorithm; Das et al. (2024) instead select the dueling responses with the most uncertain preference estimate, targeting the BAI setting in a pure exploration way; unlike the above, Melo et al. (2024) view the problem from the angle of pool-based active learning and propose an acquisition function based on both entropy and epistemic uncertainty; finally, the work by Dwaracherla et al. (2024) is the closest to ours in the sense that they apply double Thompson sampling (DTS) (Wu & Liu, 2016) for exploration, but DTS is designed for the E&E setting while they evaluate anytime average performance as in the BAI setting. We will show in Section 6.3 that pure exploration by Das et al. (2024) is not the best choice for BAI, and the objective mismatch in Dwaracherla et al. (2024) could lead to suboptimal performance in respective settings. Meanwhile, all these works primarily focus on learning uncertainty-aware RMs online without updating LLM policies. Therefore, all responses are sampled from a fixed proposal policy $\\pi_\\beta$ (or even a fixed dataset), making the data coverage a critical concern.\nAnother line of research updates LLMs online while incorporating exploration. Zhang et al. (2024a) and Xie et al. (2024) independently propose to learn an optimistic RM to encourage exploration. They leverage the property of DPO (Rafailov et al., 2023) to reparameterize RM with policy and conclude with an extra optimistic term in the DPO loss function. Thus, their learning processes are like Figure 3b but with an optimistic direct optimizer. Muldrew et al. (2024) adopt the vanilla DPO loss but utilize the implicit reward margin to actively select dueling responses. Yet, these methods are tightly coupled with DPO and not compatible to other direct optimizers. Their experiments are also limited to a few online iterations, possibly due to the implementation difficulty of a faithfully online learning system. Given their relevance to our approach, we will reproduce them in a fully online manner for fair comparisons in Section 6.1. We summarize prior works in Table 2 in Appendix E."}, {"title": "4 SEA: SAMPLE-EFFICIENT ALIGNMENT FOR LLMS", "content": "In this section we present our online exploration agent SEA (Figure 3d). We first introduce a principled Thompson sampling algorithm inspired by bandit theory (Section 4.1), and then derive SEA as its practically efficient implementation (Section 4.2). Interestingly, SEA can also be viewed as an instantiation of a classical model-based RL architecture called Dyna (Sutton, 1990), for which we defer the discussion to Appendix B."}, {"title": "4.1 THOMPSON SAMPLING FOR LLM ALIGNMENT", "content": "Thompson sampling (TS) (Thompson, 1933) is widely adopted for solving bandit problems at scale due to its efficiency and strong empirical performance in general online learning problems (Chapelle & Li, 2011; Russo et al., 2018). A bandit agent using Thompson sampling typically maintains and incrementally updates a posterior distribution of the oracle reward $p(r | \\mathcal{D})$. Meanwhile, the agent takes actions following a greedy policy with respect to a sampled RM: $a_t = \\arg \\max_a r(a)$ with $r \\sim p(r | \\mathcal{D})$. This simple yet effective algorithm naturally balances exploration and exploitation: when the agent has limited knowledge about the environment, the posterior estimate exhibits high uncertainty so that the sampled RM could guide the greedy policy to explore; after sufficient experience is gathered, the sampled RM approximates the oracle more closely, allowing the agent to exploit near-optimal policies.\nIn the context of LLM alignment, we leverage the BT assumption (Eq. (3)) to replace the preference oracle $\\mathcal{IP}$ with the implicit reward $r^*$. This substitution enables us to model the reward posterior $p(r | \\mathcal{D})$ in the standard TS framework, preserving the probabilistic structure necessary for effective posterior sampling. Inspired by prior works (Wu & Liu, 2016; Gonz\u00e1lez et al., 2017) on non-contextual K-arm bandits and preferential Bayesian optimization problems, we generalize them for LLM alignment and develop a unified algorithm as shown in Algorithm 1. Note that we assume for now the LLM agent can be fully described by the posterior $p(r | \\mathcal{D})$, and we defer practical reward ($r$) and policy ($\\pi_\\theta$) learning to Section 4.2.\nAs Algorithm 1 presents, the first response of the duel is always selected via standard TS (Line 4). The selection of the second response varies across different settings. Line 5 will be used for scenarios where preference feedback is collected from online users (the E&E setting). The dueling responses selected in this case will both try to maximize a sampled RM, so that the online user experience is warranted with best effort. However, such algorithm can have poor asymptotic performance for BAI problems (Russo, 2016), because sub-optimal responses with confidently high rewards might be tried for a long time at the expense of not exploring other potentially better choices. In light of this, Line 6 provides an alternative for scenarios where we could hire annotators for feedback and low-quality but exploratory responses are safe to try. Specifically, Line 6 selects the second response as the one that maximizes the variance of the preference (Eq. (3)) over the first response $y$. This variance quantifies the epistemic uncertainty of the RM, pointing the agent to the maximally informative direction to explore for better sample efficiency.\nHowever, Algorithm 1 is yet to be practical for LLM alignment for three main reasons. First, computing and sampling from a reward posterior is intractable for nearly all RMs at LLM scale, which are mostly based on large transformers (Lambert et al., 2024). Second, even if we managed to approximate the reward posterior, the $\\arg \\max$ operations for response selection are still intractable since the search space $\\mathcal{Y}$ is discrete and massive for token sequences of arbitrary length. Last but not least, an LLM agent (Achiam et al., 2023; Touvron et al., 2023) typically consists in a generative model $\\pi_\\theta$ (e.g., a transformer (Vaswani et al., 2017)), while the algorithm above is centered around a reward posterior $p(r | \\mathcal{D})$ that cannot be easily converted into a generative model."}, {"title": "4.2 PRACTICAL IMPLEMENTATION", "content": ""}, {"title": "4.2.1 EPISTEMIC REWARD MODEL FOR POSTERIOR SAMPLING", "content": "To implement active exploration with TS, we seek an efficient way to maintain and incrementally update the reward posterior $p(r | \\mathcal{D})$. We consider deep ensemble for our purpose, due to its capability to model epistemic uncertainty (Lakshminarayanan et al., 2017) and provable results when applied to TS in linear bandits (Qin et al., 2022). Specifically, we update a set of plausible RMs independently and online, using the preference data and a regularized negative log-likelihood loss:\n$\\mathcal{L}_R(\\mathcal{P}_t | \\mathcal{D}_t) = \\sum_{k=1}^K (\\mathcal{L}_r(\\mathcal{D}_t) - \\lambda || \\phi^k - \\phi_\\text{init}^k ||^2)$,\nwhere $\\mathcal{L}_r$ is defined in Eq. (5), $\\mathcal{P}_t = \\{\\phi^k\\}_{k=1}^K$ contains the weights of the ensemble of size $K$, and $\\lambda$ controls the regularization towards individual initial weights $\\phi_\\text{init}^k$ to retain the diversity across ensemble members (Dwaracherla et al., 2024). In practice, we train $K$ MLP heads on top of a pretrained and frozen transformer. We refer to the ensemble as the Epistemic Reward Model (ERM, denoted as $\\mathcal{R}_\\phi$), with which the posterior sampling ($r \\sim p_r(\\cdot | \\mathcal{D}_t)$) simply amounts to randomly picking a $\\phi^k$ from $\\mathcal{P}_t$."}, {"title": "4.2.2 POLICY-GUIDED SEARCH TO APPROXIMATE arg max", "content": "With the ERM approximating the reward posterior, we need to further approximate the response selection steps (Lines 4 to 6) which generally take the form of $\\arg \\max_{b \\in \\mathcal{Y}} U(b)$, where $U$ absorbs the sampled prompt, the sampled RM, and optionally the selected first response (for BAI, Line 6). To obtain the maximum, bandit algorithms for large action spaces typically resort to an action optimization oracle (Katz-Samuels et al., 2020; Zhu et al., 2022), but they assume a linear structure of $U$ with respect to $b$, which might be impractical for LLMs. Therefore, we instead replace the optimization over $\\mathcal{Y}$ with sampling from a policy-guided distribution conditioned on $U$, $\\pi_{\\text{prior}}(\\cdot | x) \\propto \\exp (U(\\cdot) / \\eta)$, which is appropriate since it favors responses $y$ that approximately maximize $U(y)$. In practice, for a given prompt $x_t$, we sample $M$ candidate responses from the prior policy $\\pi_{\\text{prior}}(x_t)$ to construct a proposal set $\\mathcal{S}_t = \\{y_i\\}_{i=1}^M$. We then conduct a greedy search in $\\mathcal{S}_t$ (taking $\\eta \\rightarrow 0$) to identify the response $y_t$ (or $y'_t$) that locally maximizes the utility function $U$, which is subsequently used in the duel. We also reuse the same $\\mathcal{S}_t$ for different $U$ functions at time $t$ to save computation. The choice of $\\pi_{\\text{prior}}$ will be discussed in the next section."}, {"title": "4.2.3 ONLINE POLICY LEARNING FROM MIXED PREFERENCES", "content": "We finally resolve two remaining questions: (Q1) how to choose a sensible $\\pi_{\\text{prior}}$ at each time $t$ and (Q2) how to get a good generative policy online. To this end, we propose a simple approach to approximately address both questions simultaneously. That is, we can utilize any direct optimizer to learn the policy $\\pi_{\\theta_t}$ online with the following loss and use the latest online policy as prior:\n$\\mathcal{L}_\\pi(\\theta_t | \\mathcal{B}_t, \\pi_{\\text{ref}}, F) = E_{(x, y^+, y^-) \\sim p_\\pi} [F_{\\theta_t}(x, y^+, y^-, \\pi_{\\text{ref}})]$,\nwhere $\\mathcal{B}_t$ is a batch of preference data labeled by the oracle wherein the responses are proposed by $\\pi_{\\text{prior}}$ and selected by $\\mathcal{R}_{\\phi_t}$, $F$ could be any DAP loss (see Appendix A for some examples), and $\\pi_{\\text{ref}}$ is chosen to be $\\pi_{SFT}$. Note that we use $\\pi_{\\theta_t}$ as $\\pi_{\\text{prior}}$ at any time $t$, thus $\\mathcal{B}_t$ is a batch of on-policy data. By contrastive training on these on-policy data, we leverage their orthogonal benefits to achieve maximal policy improvement (Tajwar et al., 2024; Tang et al., 2024).\nNow that optimizing Eq. (9) yields a good online policy $\\pi_{\\theta_t}$ (answering Q2), we need to assess whether $\\pi_{\\theta_t}$ can serve as a suitable $\\pi_{\\text{prior}}$ for approximating the $\\arg \\max$ in TS (Q1). If we optimize $\\pi_{\\theta_t}$ with oracle preference data, $\\mathcal{S}_t$ will be biased towards responses with high oracle reward $r^*$.\nBias towards high-$r^*$ region is generally helpful because it aligns with $\\arg \\max_{y \\in \\mathcal{Y}} r(x, b)$ that seeks high-reward responses. However, optimizing $\\pi_{\\theta_t}$ not only with oracle data can average out the epistemic uncertainty of $\\mathcal{R}_\\phi$, hindering the exploration efficiency. To mitigate this issue, we further align $\\pi_{\\theta_t}$ with $\\mathcal{R}_\\phi$ using the same direct optimizer to encourage $\\pi_{\\theta_t}$ to propose high-$r_{\\phi_t}$ responses for individual $\\phi_t^k$, leading to better approximation of $\\arg \\max_{y \\in \\mathcal{Y}} r(x, b)$ for any sampled $r$. To implement, we optimize Eq. (9) over a mixture distribution $P_{\\text{emix}} = \\gamma P_{\\mathcal{B}_t} + (1 - \\gamma) P_{\\text{ERM}}$, where"}, {"title": "5 EXPERIMENTAL SETUP", "content": "In this section, we elaborate the experimental setup employed to validate our algorithm and ensure fair comparisons with other online alignment baselines. We start by introducing the distributed learning system designed for experimenting with online LLM alignment using simulated human preferences (Section 5.1). Then, we provide key experimental details in Section 5.2, with a full description available in Appendix D."}, {"title": "5.1 DISTRIBUTED LEARNING SYSTEM", "content": "The interactive nature of LLM alignment necessitates an integrated online learning system that simulates the interface depicted on the right of Figure 2. The absence of a performant open-source online alignment system has restricted many existing works to only a few iterations of batch learning (Muldrew et al., 2024; Dong et al., 2024; Chen et al., 2024; Zhang et al., 2024a; Xie et al., 2024), which creates a mismatch with their theories that typically require a large number of online interaction rounds. Even worse, such absence also makes the comparison between different LLM exploration methods difficult, often restricting evaluations to the simplest iterative DAP baselines (Zhang et al., 2024a; Xie et al., 2024).\nTo fill this gap, we build a highly efficient learning system for experimenting with online LLM alignment algorithms. We notice that the computational bottleneck lies in online response sampling (i.e., autoregressive generation) and preference labeling (e.g., human, large RMs, or large LLMs), which mirrors the slow actor-environment interaction seen in RL systems. Inspired by distributed deep RL systems which spawn many actors or environments in parallel (Espeholt et al., 2018; Weng et al., 2022), we design an Actor-Learner-Oracle architecture for online LLM alignment, which is depicted in Figure 4. The three types of workloads (i.e., actor, learner and oracle) are heterogeneous and require different optimization. In particular, we adopt vLLM (Kwon et al., 2023) for the actor to accelerate the autoregressive response generation. We also use DeepSpeed's ZERO (Rasley et al., 2020; Rajbhandari et al., 2020) strategies to enhance the memory efficiency of the learner. The updated model weights are broadcasted from the learner master to all actors after every optimizer step efficiently via NCCL, similar to Hu et al. (2024). Furthermore, to improve the scalability, we wrap the oracle RM as a service using Mosec (Yang et al., 2021b), which supports dynamic batching and parallel processing, to minimize preference query latency. Finally, we leverage DeepMind Launchpad (Yang et al., 2021a) to compose all workloads into a distributed program and adopt Plasma (Philipp & Robert, 2017) to efficiently transfer data across process boundaries.\nWe benchmark our system's efficiency against a concurrent implementation of online DPO by HuggingFace\u00b3, which utilizes only DeepSpeed for memory optimization. Our system achieves up to 2.5x latency reduction compared to this counterpart, demonstrating its computational efficiency. Due to space constraints, detailed benchmarking methods and results are presented in Appendix C. Our codebase, oat (online alignment), along with the implementation of SEA, is open-sourced at https://github.com/sail-sg/oat to accelerate future research in online LLM alignment."}, {"title": "5.2 EXPERIMENT DETAILS", "content": "We adopt SFT models tuned on TL; DR (Stiennon et al., 2020) from Huang et al. (2024), which cover three scales (1B, 2.8B, 6.9B) of the Pythia family (Biderman et al., 2023), as starting points for our experiments. We choose Liu et al. (2024a) to be the oracle RM. To verify the effectiveness of SEA, we employ three direct optimizers: DPO (Rafailov et al., 2023), IPO (Azar et al., 2024),"}, {"title": "6 EMPIRICAL RESULTS", "content": "In this section, we present our empirical results and analyses, organized into four parts: (1) an overall comparison between SEA and baselines across various direct optimizers and model scales; (2) an ablation analysis to study the effects of SEA's key components; (3) a comparison of different exploration strategies under E&E and BAI settings; (4) additional results for alignment with a human oracle simulated by GPT4o-mini."}, {"title": "6.1 OVERALL COMPARISON", "content": "We first compare SEA with all baselines across three model scales and three direct optimizers. APL and XPO are only compared when DPO is used as the direct optimizer, because they are incompatible with IPO or SLiC. Figure 5 shows the win rate curves versus the number of query steps. Across all settings, Online agents consistently improve sample efficiency over their Offline counterparts, validating the necessity of Property 1 for alignment algorithms. Focusing on the first row, we observe that among prior active exploration methods, XPO gives a small improvement in final performance over Online (passive) at the 1B scale, but falls short for larger scales. On the other hand, APL shows a significant sample efficiency boost at the 1B scale, but this advantage diminishes when scaling up and it performs almost the same as Online at 6.9B scale. Our method, SEA, outperforms both offline and online passive methods across all scales and all direct optimizers, confirming the critical role that Property 2 plays for sample-efficient alignment. Meanwhile, in the special case of using DPO as the direct optimizer, SEA also shows superior performance to prior online active exploration methods including APL and XPO. We invite readers to revisit Figure 1, where we show that SEA not only attains significantly improved final performance (Left) but also achieves 2-5x better sample efficiency (Right).\nAdditionally, we note that the choice of direct optimizer matters for both online learning and active exploration. When comparing different optimizers at 1B scale (the first column), all Offline agents demonstrate comparable learning efficiency and reach the same level of final performance"}, {"title": "6.2 ABL"}]}