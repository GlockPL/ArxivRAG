{"title": "Semi-Periodic Activation for Time Series\nClassification", "authors": ["Jos\u00e9 Gilberto Barbosa de Medeiros J\u00fanior", "Andr\u00e9 Guarnier de Mitri", "Diego Furtado Silva"], "abstract": "This paper investigates the lack of research on activation\nfunctions for neural network models in time series tasks. It highlights\nthe need to identify essential properties of these activations to improve\ntheir effectiveness in specific domains. To this end, the study comprehen-\nsively analyzes properties, such as bounded, monotonic, nonlinearity, and\nperiodicity, for activation in time series neural networks. We propose a\nnew activation that maximizes the coverage of these properties, called\nLeakySineLU. We empirically evaluate the LeakySineLU against com-\nmonly used activations in the literature using 112 benchmark datasets\nfor time series classification, obtaining the best average ranking in all\ncomparative scenarios.", "sections": [{"title": "1 Introduction", "content": "Deep learning models have achieved significant performance improvements across\nvarious domains. Consequently, their application in time series tasks has been\nreceiving increasing attention. However, numerous features and techniques remain\nto be thoroughly explored in this area, leading to the proposal of several new\napproaches [7,26,25,20]. Most recent research on deep learning for time series\nfocuses on proposing and evaluating architectures, often adapting those designed\nfor computer vision, such as ResNet [24] and InceptionTime [6]. Other variations\naim to extend these architectures beyond classification tasks [28,27]. While these\narchitectures have achieved satisfactory results in their respective domains, they\nstill have gaps to explore.\nDeep neural networks can learn effective representations of high-dimensional\ndata, including time series. However, they can suffer from overfitting and other\nissues that lead to inaccurate results, especially when dealing with periodic\ndata [31]. To effectively model time series data, it is crucial to capture inherent\npatterns such as trends and seasonality to generate suitable representations for\nthe desired tasks."}, {"title": "2 Definitions and Notations", "content": "This section introduces the necessary basic definitions and notations to understand\nthis work."}, {"title": "Definition 1.", "content": "A time series X is a set of S ordered values X = (x1, x2,...,xs)\nand xi \u2208 Rd, such that d denotes the number of dimensions of that time series,\nwith d \u2208 Z. Each value xi is referred here as an observation."}, {"title": "Definition 2.", "content": "A deep neural network, DNN, can be defined as a sequence of\nfunctions composed of multiple layers of interconnected neurons. Let L denote\nthe total number of layers in the network, with 11,12,...,l\u2081 representing each\nlayer. A layer li consists of ni neurons, where i \u2208 1, . . ., L. Weights and biases\nassociated with that layer li are denoted by Wi and bi, respectively. Given an\ninput X, the output of a DNN can be expressed as a sequence of layers applied\nto that input:\nf(x) = fc(fc\u22121(\u2026\u2026(f1(X)))),"}, {"title": "", "content": "where fi:Rni-1 \u2192 Rn-i represents the function for layer Li. This function can\nbe expressed as:\nfi(x) = \u03c3\u03b9(Wix + bi),"}, {"title": "", "content": "where oi: Rni\u2192 Rni denotes the activation function for the layer i, and\nx \u2208 Rni-1 represents the input to that layer."}, {"title": "Definition 3.", "content": "A semi-periodic activation function is a function where the peri-\nodicity is present in its derivative, rather than in the function itself. This means\nthat while the activation function may not be periodically repetitive, its derivative\nexhibits periodic behavior. Formally, let \u03c3: R \u2192 R be an activation function. \u03c3 is\nconsidered semi-periodic if its derivative \u03c3' satisfies \u03c3'(x + T) = \u03c3'(x) for some\nperiod T > 0 and for all x\u2208R."}, {"title": "Definition 4.", "content": "A sub-derivative is a generalization of the concept of a derivative\nfor functions that may not be differentiable in the traditional sense. It extends\nthe idea of a tangent line to non-smooth functions, often used in the context of\nconvex functions and optimization. Formally, let f: R\u2192R be a convex function.\nA vector g\u2208 R is called a sub-derivative of f at a point xo \u2208 R if it satisfies the\nfollowing inequality for all x \u2208 R:\nf(x) \u2265 f(xo) + g(x \u2212 xo),"}, {"title": "", "content": "the set of all sub-derivatives of f at xo is called the sub-differential and is denoted\nby df(x0). Thus, the sub-differential is given by:\ndf(xo) = {g \u2208 R | f(x) \u2265 f(xo) + g(x \u2212 xo) for all x \u2208 R},"}, {"title": "", "content": "as an example of a sub-derivative we can include the ReLU function f(x) =\nmax(0,x) the sub-derivative at xo 0 is any g\u2208 [0, 1]."}, {"title": "3 Activation functions", "content": "To easily compare the LeakySineLU and the related activation, this section offers a\nmathematical and theoretical analysis of each activation and their application on\ntime series tasks. In the following subsections, we present the related activations\nand define the proposed LeakySineLU function, each property, and the comparison\nbetween the main activations."}, {"title": "3.1 Related Activations", "content": "To compare the proposed activation with various approaches, activation func-\ntions encompassing one or more specific properties were selected, and in some\ncases, functions that do not possess certain properties were also included."}, {"title": "3.2\nLeakySineLU", "content": "In this work, we propose the LeakySineLU, defined by the following equation:\n\u03c3(x) = { sin\u00b2(x)+x if x > 0, sin\u00b2(x)/2+x otherwise."}, {"title": "", "content": "The derivative of this equation can be defined as:\n\u03c3' (x) = { sin(2x)+1 if x > 0, sin(2x)+1/2 otherwise."}, {"title": "3.3 Boundness", "content": "Over the years, many studies have shown that bounded activation functions, such\nas hyperbolic tangent or sigmoid, could reach excellent results in many different\ntasks. Although these results are specifically reported when using shallow network\narchitectures [17,12]. However, training neural networks that use this type of\nfunction with time series data may cause a loss of information, which occurs\nwhen the series contains values outside the boundaries, causing the problem of\nvanishing gradients [1].\nNeural networks that use unbounded activation functions, with regards to a\nnon-polynomial activation, are universal approximators and seem to attenuate\nthe vanishing gradient problem [22,18,15]. However, activations that are upper- or\nlower-bounded, such as the lower-bounded ReLU, can cause a loss of information\non negative observations of the time series, which results in a problem known\nas \"dying ReLU\" [8]. Other activations such as LeakyReLU, ELU [2], SiLU [4],\nPReLU, and GeLU [9] have been proposed to address this issue, either using a\nnegative slope different from 0 or specific weight initializations. But except for\nLeakyReLU and PReLU, all others have lower bounds, even if different from 0."}, {"title": "3.4 Non-linearity", "content": "Non-linearity is a critical point for activation functions [1]. All activations want to\nincrease the non-linearity present in neural networks. Activations such as ReLU,"}, {"title": "3.5 Differentiability", "content": "The differentiability of activation functions is critically important in neural\nnetworks, as it facilitates the application of gradient-based optimization methods,\nsuch as the backpropagation algorithm. These methods require the computation of\nderivatives (or gradients) of the activation functions concerning their parameters"}, {"title": "3.6 Periodicity", "content": "The motivation behind the periodicity is based on the Universal Extrapolation\nTheorem [31] and the generalized Fourier Series. Consider the definition of this\ngeneral form of Fourier Series:\nx(t) = \u03b1o/2+ \u2211n=1\u221e an cos(2\u03c0nt/T) + bn sin(2\u03c0nt/T),"}, {"title": "", "content": "where ao, an, bn are constants and T is a elemental period. Let ao be an array\nin a Rd space, the result of a scalar division a0/2 is a operation that maps\nf : Rd \u2192 Rd. We can rename the result of ao/2 to a bias vector and the constants\nan and bn by weights won and win, that way the formula will be:\nx(t) = b + \u2211n=1\u221e Won cos(2\u03c0nt/T) + Win sin(2\u03c0nt/T),"}, {"title": "", "content": "now considering the mathematical equivalence of sin and cos, sin(x + 3\u03c0/2) =\ncos(x), and considering each input for sin and cos as 10 and 11 we can rewrite\nthe formula as:\nx(t) = b + \u2211n=1\u221e Won sin(xo) + Win sin(x1)"}, {"title": "", "content": "that way we can group won and win is a weight matrix W, and 10 and 11 also in\na input matrix X. The sin will operate over a matrix and return an output in the\nsame dimension, by the property of matrix multiplication, we can arrange the\ncolumns and rows to perform the multiplication over the correct values. Resulting\nin a final formula similar to a neural network layer with sin as activation:\nx(t) = bias + \u2211n=1\u221e Wn sin(X)"}, {"title": "3.7 Monotonic", "content": "A monotonic activation function is a function that only increases or decreases\nas its input increases, being allowed to stay constant at some ranges. Sigmoid,\nReLU, and hyperbolic tangent are examples of functions of this type.\nThe monotonicity of an activation function is a common property found in\nthe literature, although it is not mandatory. For instance, Mish [14] and GeLU [9]\nare exceptions to this property, allowing the network to capture more complex\nnonlinear relationships between data while maintaining differentiability across its\nentire domain. However, the choice of non-monotonicity of Mish and GeLU may\nrestrict its use in some tasks. [23] has shown that periodic activation functions\nare a promising alternative to monotonic activation functions in neural networks.\nAt the time of that work, no studies demonstrated that functions such as sine\ncould fall into local minima [16].\nThe neural network can use monotonic activation to learn a mapping function\nthat is easier to optimize since the derivative only increases or decreases in one\ndirection. That way, the gradient descendent algorithms can work more efficiently."}, {"title": "4 Experiments", "content": "This section assesses the LeakySineLU in different experiments to evaluate its\napplicability in classification tasks involving time series. Initially, the experiments\nwere executed and discussed in 112 equal-length subset\u00b9 datasets from the\nUCR repository [3] (experiments are available in the project repository2). We\nconduct the experiments in two well-established architectures. The first is a Multi-\nLayer Perceptron (MLP) [24,5], representing the most basic and straightforward\narchitectures. The second choice is a Fully Convolutional Network (FCN) [24,5],\nan architecture representing networks that use convolutional structures to obtain\nmore accurate results.\nMLP consists of a network with two blocks, each composed of a dropout\n(p = 0.1 and p = 0.2, respectively, in each block) and a layer with 500 neurons.\nNext, a last block is used, composed of a dropout with p = 0.3 and another\nlayer with the number of classes being the number of neurons, where a softmax\nactivation is applied for multi-class classification or just 1 neurons for binary\nclassification. Each layer includes the activation applied to that experiment. FCN\ncomprises three convolutional blocks (128, 256, and 128 channels, respectively)"}, {"title": "4.1 Experimental Setup", "content": "We conducted experiments using the two chosen architectures, MLP and FCN,\nto evaluate the applicability of the proposed activation function in time series\nclassification tasks. The MLP network was trained using the Adadelta optimizer\n[29], with a learning rate of 1.0 and 1000 training epochs. The FCN network was\ntrained using the Adam optimizer [11], with a learning rate of 0.001 and 2000\ntraining epochs. The loss function used in the experiments was the cross-entropy\nfor multi-class classification and binary cross-entropy for binary classification\ntasks. The performance of the models was evaluated in terms of accuracy."}, {"title": "5 Results", "content": "This section discusses the results of LeakySineLU and all the other approaches\nover the 112 equal-length UCR datasets. Following [5], we use the mean accuracy\naveraged over the experiments on the test set. In Fig. 4, we present a one-versus-\none comparison between the LeakySineLU function, ReLU, Snake, and PReLU in\na MLP network. We provide these comparisons to analyze the performance with\nthe most used activation in time series tasks (ReLU), the most similar activation\n(Snake), and a robust alternative to ReLU (PReLU)."}, {"title": "6 Conclusion", "content": "In this work, we studied and identified meaningful potential harmful properties\nof neural network activations for time series data. Our study encompasses math-\nematical analysis of these properties as well as their study for tasks related to\ntime series. We identified weaknesses in the main activations in the literature and,\nbased on this study, we proposed the LeakySineLU, an activation that overcomes\nvaried limitations of activation functions commonly used on neural network\narchitectures for time series tasks. Through an empirical study, we conducted a\nseries of experiments on benchmark datasets from the literature, showing that the\nproposed activation may be a relevant alternative to be considered for use in time\nseries classification tasks. Future works include improvement in well-defined archi-\ntectures for time series classification such as InceptionTime [6] and ResNet [24]\nusing the proposed activation. The impact of activation choices in other time\nseries-related tasks such as extrinsic regression and forecasting exploring the use\nof LeakySineLU and other periodic properties."}]}