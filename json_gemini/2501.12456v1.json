{"title": "Deploying Privacy Guardrails for LLMs: A Comparative Analysis of Real-World Applications", "authors": ["Shubhi Asthana", "Bing Zhang", "Ruchi Mahindru", "Chad DeLuca", "Anna Lisa Gentile", "Sandeep Gopisetty"], "abstract": "The adoption of Large Language Models (LLMs) has revolutionized AI applications but poses significant challenges in safeguarding user privacy. Ensuring compliance with privacy regulations such as GDPR and CCPA while addressing nuanced privacy risks requires robust and scalable frameworks. This paper presents a detailed study of OneShield Privacy Guard, a framework designed to mitigate privacy risks in user inputs and LLM outputs across enterprise and open-source settings. We analyze two real-world deployments: (1) a multilingual privacy-preserving system integrated with Data and Model Factory, focusing on enterprise-scale data governance; and (2) PR Insights, an open-source repository emphasizing automated triaging and community-driven refinements.\nIn Deployment 1, OneShield achieved a 95% F1 score in detecting sensitive entities like dates, names, and phone numbers across 26 languages, outperforming state-of-the-art tools such as StarPII and Presidio by up to 12%. Deployment 2, with an average F1 score of 0.86, reduced manual effort by over 300 hours in three months, accurately flagging 8.25% of 1,256 pull requests for privacy risks with enhanced context sensitivity.\nThese results demonstrate OneShield's adaptability and efficacy in diverse environments, offering actionable insights for context-aware entity recognition, automated compliance, and ethical AI adoption. This work advances privacy-preserving frameworks, supporting user trust and compliance across operational contexts.", "sections": [{"title": "Introduction", "content": "As Large Language Models (LLMs) gain widespread adoption in consumer and enterprise applications, ensuring data privacy has become a critical challenge. These models often process vast amounts of unstructured data that may contain sensitive Personally Identifiable Information (PII). For example, the latest Common Crawl dataset (Smith et al. 2013) encompasses data from over 3 billion web pages and has vast user PII data. This raises concerns about data leakage, compliance with global regulations, and ethical use (Carlini et al. 2021, 2022). While LLMs enable innovative applications, their ability to unintentionally store and recall sensitive data during inference creates vulnerabilities that must be addressed through robust privacy-preserving mechanisms.\nLLMs process unstructured data, ranging from text prompts to conversational outputs, often contains sensitive details like names, phone numbers, and credit card information. Furthermore, data sourced globally may inadvertently violate region-specific privacy regulations, including the General Data Protection Regulation (GDPR) (gdp 2024), California Consumer Privacy Act (CCPA) (ccp 2024), and Personal Information Protection and Electronic Documents Act (PIPEDA) (pip 2024). The scale and complexity of LLM deployments amplify compliance challenges.\nMoreover, traditional privacy techniques, such as rule-based regular expression (regex) methods or standalone machine learning models, often struggle to balance precision, contextual understanding, and computational efficiency. Detecting nuanced privacy risks, such as context-dependent sensitivity, requires advanced mechanisms. Simple entity recognition is not enough. For example, consider an office phone number publicly listed on a company website for media inquiries. This is different from the personal phone number of a private individual, like John Doe. Handling these cases requires careful analysis to comply with privacy regulations and prevent unintended disclosure. Consequently, deploying privacy-preserving systems for LLMs demands innovative framework that address these multifaceted challenges while ensuring scalability and real-time responsiveness.\nTo address these questions, we designed a framework OneShield Privacy Guard. The key contributions of the different stages in the framework are as follows:\n\u2022 A detailed analysis of two distinct OneShield Privacy Guard deployments addressing different application scenarios.\n\u2022 A comparison of their technical architectures, privacy-preservation methods, and effectiveness.\n\u2022 Insights into scalability, compliance, and lessons learned that inform best practices for future LLM deployments.\nThis work advances the understanding of deploying privacy safeguards in diverse LLM environments, contributing to the field of privacy-preserving AI and supporting the broader ethical use of LLM technologies.\nThis paper focuses on two deployments of the OneShield"}, {"title": "State of the Art", "content": "Existing Techniques for PII Detection in LLMs: Detecting Personally Identifiable Information (PII) within Large Language Models (LLMs) is an area of growing interest, with research focusing on developing scalable and accurate solutions. Traditional methods rely on regular expression (regex)-based tools like (com 2023) and (neu 2023), and customized machine learning models to identify PII entities like names, addresses, date, social security numbers, Vehicle Identification Number (VIN), bank account numbers etc. For instance, tools like Presidio Analyzer (mic 2023)and StarPII (sta 2023) extend Named Entity Recognition (NER) capabilities by incorporating predefined dictionaries and regex patterns. However, such methods are inherently limited due to their dependence on static patterns, which fail to adapt to new contexts or nuanced PII definitions. There is also added complexity when a phone number may appear which may be identified correctly, but may not be considered as PII based on the context (e.g., phone number of an organization versus a private individual).\nContext-aware PII detection has emerged as a significant advancement, leveraging machine learning techniques to incorporate the surrounding context of entities in text. Studies have explored BiLSTM-based models for analyzing forward and backward contexts, enabling more robust identification of PII in unstructured text (et al. US11127403B2, 2019) as well as other methods (Gupta et al. 2021; Chen et al. 2023; Yan, Yu, and Chen 2024). This contextual approach is essential for resolving ambiguities, such as distinguishing between a person's name and a location when dealing with entities like \"Paris\" or \"Jordan.\"\nDespite advancements, existing tools still face critical gaps that limit their effectiveness in diverse real-world scenarios. One major issue is limited coverage across languages and jurisdictions. For example, a tool might perform well in detecting PII in English but struggle with languages like Arabic or Mandarin, and fail to adapt to regional regulations such as GDPR in Europe or CCPA in California. Another challenge lies in resolving conflicts between overlapping PII types. For example, a single string like \"Jane Doe, 123-45-6789, Los Angeles\" might include a name, Social Security Number (SSN), and location, but tools often prioritize one type over others, resulting in incomplete or incorrect masking. Additionally, sensitivity scoring mechanisms for classifying data based on contextual importance remain inadequate. As highlighted in patents such as (Ron M. Redlich US8468244B2, 2009) and (et al. US10984316B2, 2017), tools often lack the ability to dynamically adjust the sensitivity of data. For instance, they might treat an email address in a marketing email the same as an email address in confidential corporate communications, ignoring the vastly different contextual risks.\nPrivacy Risks in LLM Prompts and Outputs: Privacy challenges in LLMs extend beyond entity recognition to include vulnerabilities such as prompt injection attacks, where malicious inputs exploit models to reveal sensitive information. Research on Propile (Kim et al. 2024), a tool for probing privacy leakage in LLMs, highlights the risks of indirect prompt injections that can compromise user trust. Similarly, studies have underscored the challenges of adhering to privacy regulations like GDPR and CCPA, especially when dealing with multilingual data or region-specific compliance requirements.\nPrevious Deployments and Real-World Case Studies: Deploying privacy-preserving frameworks in real-world scenarios remains underexplored, with limited studies addressing practical challenges. Our deployments represent two notable deployments that showcase the application of privacy guardrails in enterprise and open-source environments. These deployments emphasize contextual scoring, policy-driven actions, and automated triaging to mitigate privacy risks.\nAdditionally, prior studies have focused on using classifiers like Logistic Regression and CNNs for entity detection and sensitivity analysis (Liu et al. 2021; mic 2023). They integrate privacy-preserving mechanisms with data governance workflows, as seen in enterprise platforms like Wat-"}, {"title": "System Architecture", "content": "The OneShield deployment was built on a modular architecture that enabled flexibility and scalability. It consisted of three core components as shown in Figure 1:\n1. Guardrail Solution: Responsible for monitoring both input prompts and output responses to detect sensitive PII entities across multiple languages.\n2. Detector Analysis Module: Incorporated detection mechanisms for various PII types, along with additional capabilities like hate, abuse and profanity content detection, enabling a broader scope of privacy governance.\n3. Privacy Policy Manager: Provided policy templates tailored to jurisdictional regulations such as GDPR and CCPA, dynamically applying actions like masking, blocking, or passing input output data based on detected entities.\nThis architecture supported seamless integration with the Data and Model Factory ecosystem, enabling it to process and protect data across ~ 30 language models in real-time."}, {"title": "Challenges in Deployment 1", "content": "Deployment 1 focused on protecting user inputs and model outputs across various AI use cases including patent data, books, web-crawled data etc. These applications often handle sensitive personal data like names, email addresses, and phone numbers. Detecting and masking this data was essential to comply with privacy regulations such as GDPR and CCPA while ensuring user trust.\nTraditional methods like regex fell short in this scenario because they are rule-based and rigid. For example, a regex might detect \"123-45-6789\u201d as a U.S. Social Security Number but fail to recognize variations in formatting or contexts, such as an SSN embedded within a larger text. Regex also struggles with multilingual data-detecting phone numbers in English is vastly different from identifying Indian Aad-haar IDs or German tax numbers. Moreover, overlapping data types, like a string containing both a name and a phone number, were beyond regex's capabilities to handle effectively.\nThe OneShield framework used contextual machine learning to overcome these challenges. Unlike regex, it could analyze the context to distinguish between sensitive data and similar-looking non-sensitive text. For instance, \"John Smith\" could be flagged as a name only if used in a context suggesting personal identification, whereas regex might flag unrelated occurrences of \"Smith\". Its ability to support 26 languages and resolve complex overlaps made it uniquely suited for this deployment."}, {"title": "PII Detection", "content": "In the Detectors Analysis module, the PII detection is an important component with an iterative algorithm to detect sensitive information. The key steps as shown in Figure 2 included:\n\u2022 Entity Recognition: Entities were detected in user prompts and model outputs using a combination of rule-based and machine learning classifiers. The system was"}, {"title": "Results and Evaluation", "content": "The OneShield deployment demonstrated its effectiveness in safeguarding sensitive data through robust evaluation measures, including human assessments. Here, effectiveness refers to the system's ability to accurately detect and protect PII across various contexts, languages, and data formats, ensuring compliance with privacy standards while maintaining performance. The framework achieved high accuracy, consistently exceeding a 95% detection rate across 26 languages, including English, Spanish, French, Turkish, Hindi, and Arabic. This performance highlights its capability to handle diverse and complex scenarios beyond the limitations of simpler solutions like regex.\nOn a dataset of ~1,200 user prompts, the average response time for PII detection in inputs of up to 150 tokens was 0.521 milliseconds, increasing to 0.711 milliseconds for longer prompts of up to 250 tokens. The minimal increase in average responses is still within the non-functional requirements for real-time processing, adding negligible latency compared to baseline system performance without PII detection. Specifically, the additional latency was less than 5% of the overall response time, ensuring a seamless user experience without compromising the system's usability in high-throughput environments.\nAdditionally, the system provided detailed templates for masking PII and tracking policy violations, contributing to improved compliance management. Compared to open-source tools like Presidio Analyzer(mic 2023), the OneShield Privacy Guard achieved superior coverage, particularly for complex PII types like national IDs and phone numbers from countries. For e.g., countries like Germany's phone number regex definition overlaps with India's Aadhar card definition, which were resolved using context.\nThis deployment showed how important it is to support multiple languages and use context to improve privacy protection. By including policies tailored to specific regions, it used human evaluation to ensure the system met global privacy laws. The flexible design also made it easy to scale and use in different enterprise environments."}, {"title": "Deployment 2: PR Insights Deployment", "content": "The second deployment of the OneShield Privacy Guard framework was implemented in collaboration with name anonymized, an open-source repository hosted on GitHub. name anonymized provides a platform for community-driven contributions, including training datasets and seed examples for large language models. These contributions often take the form of pull requests (PRs) containing contexts, questions, and answers submitted by users. Due to the open nature of these contributions, there was a significant risk of sensitive personal information being inadvertently included, leading to potential violations of privacy or project codes of conduct.\nTo address these challenges, the OneShield guardrails solution was employed as a safety bot to detect and mitigate privacy risks in PRs. This deployment emphasized automating the detection of PII within the actual text of PR, and not metadata of PR, while reducing the burden on human triage teams."}, {"title": "System Architecture", "content": "The architecture of this deployment was tailored to handle community-generated content and featured two primary components:\n1. Automated Detection Bot: Integrated with the GitHub repository to scan incoming PRs in real time. The bot employed rule-based and machine-learning models to identify sensitive information within textual content.\n2. Human-in-the-Loop Feedback Mechanism: Detected PII were flagged for review by a triage team. This iterative approach allowed for fine-tuning the detection models based on human feedback and contextual relevance\nUnlike Deployment 1, this system prioritized lightweight processing to accommodate the high volume of PRs while maintaining compatibility with existing repository workflows."}, {"title": "Challenges in Deployment 2", "content": "Code repositories often included sensitive information like phone numbers, email addresses, or placeholders (e.g., \"Albert Einstein's phone number: 123-456-7890\"). With hundreds of PRs submitted globally, automating privacy risk detection became essential to streamline reviews and ensure compliance.\nRegex struggled to differentiate real sensitive data from irrelevant or fictional text. For example, regex might flag \"Sherlock Holmes, 221B Baker Street\" incorrectly or miss actual violations like \"Winston Churchill's phone: 987-654-3210\" hidden in comments. Overlapping formats, such as dates doubling as IDs (e.g., \"July 4, 1776\"), added to its limitations.\nOneShield used context to distinguish genuine risks from false positives, flagging \"Mahatma Gandhi's phone: +91-1234567890\" as sensitive while ignoring placeholders like \"Jane Austen's email: jane.austen@example.com.\" Its multilingual support and precision reduced false positives, saved over 300 hours of manual reviews, and ensured compliance across global teams."}, {"title": "PII detection", "content": "The PII detection process involved three key steps:\n\u2022 Entity Recognition: The system broke text into smaller chunks using separators like periods and line breaks to detect entities like names, email addresses, phone numbers, and dates. Monitoring phone numbers and email addresses is important because developers may accidentally include them in PRs, such as using real or placeholder contact details in examples or test cases (e.g., \"John Doe, john.doe@example.com, 123-456-7890\"). Other sensitive PII in PRs includes names, dates, and confidential keys like API tokens. Detecting and flagging this information helps prevent privacy breaches and ensures compliance with project guidelines.\n\u2022 Contextual Sensitivity Scoring: Detected entities were evaluated for sensitivity based on their relationships with other text elements. For example, \"John Doe\" combined with an email address was classified as highly sensitive, while standalone generic entities like \"Date : 1776\" were deemed non-sensitive.\n\u2022 Automated Policy Actions: Detected violations were flagged, preventing PRs containing sensitive information from being merged into the repository until reviewed. This ensured compliance with the project's code of conduct, which prohibited the inclusion of private or invasive content."}, {"title": "Results and Evaluation", "content": "In Deployment 2, the system analyzed over 1,256 pull requests (PRs) within a span of three months, identifying privacy violations in 8.25% of cases. The most commonly flagged entities included names, phone numbers, and dates. By automating the initial detection of potential privacy violations, the deployment significantly reduced the workload of the triage team, pre-flagging PRs for further review and action.\nThis automation saved an estimated 300+ hours of manual effort over the three-month period, assuming that manually reviewing a PR for privacy violations typically takes 15 minutes. The streamlined process allowed the team to focus on higher-priority tasks, improving overall efficiency. Furthermore, the proactive identification of privacy risks prevented any potential breaches, reducing the likelihood of costly penalties or reputational damages. In terms of business impact, this deployment enhanced operational productivity by approximately 25%, ensuring quicker turnaround times for PR approvals and fostering a culture of privacy-by-design in development workflows."}, {"title": "Comparison of Deployments and Discussion", "content": "Comparison of Deployments\nThe two deployments of the OneShield Privacy Guard framework showcase different approaches tailored to their respective application environments. The first deployment (Data and Model Factory) emphasized enterprise-scale governance and multilingual adaptability, while the second (PR Insights) focused on automating privacy checks in open-source contributions.\nWe performed a comparison of our PII detection in the two deployments with two state-of-the-art PII detectors, StarPII (sta 2023) and Presidio Analyzer(mic 2023), across various PII types.  The data showcases the higher accuracy of OneShield Privacy Guard for both Deployment 1 and 2, as compared to StarPII and Presidio Analyzed. The performance was observed for sensitive entities, particularly in multilingual and regulatory-complex contexts. The results have been updated with deployment-specific benchmarks from Deployment 1 (Data and Model Factory) and Deployment 2 (PR Insights). In Deployment 1, the OneShield Privacy Guard demonstrated a 0.95 F1 score in detecting dates across multilingual data, such as identifying sensitive dates in Spanish and French prompts. Deployment 2 showed overall better performance in detecting email addresses, phone numbers, persons, location etc. within open-source PRs, where contributors often accidentally included personal contact details.\nDiscussion\nWe also performed a comparison of overall deployments.. The comparative analysis reveals critical insights into the challenges and opportunities of deploying privacy-preserving frameworks in varied environments:\n1. Scalability and Multilingual Support: Deployment 1 demonstrated the scalability of OneShield Privacy Guard across languages and regulatory landscapes, providing a robust solution for enterprise needs. However, it faced challenges in maintaining consistent accuracy across diverse data types. Deployment 2, while monolingual, highlighted the effectiveness of lightweight systems in collaborative workflows, particularly when combined with human-in-the-loop processes for model refinement.\n2. Compliance and Regulatory Adherence: Both deployments underscored the importance of aligning privacy-preserving tools with region-specific regulations. Deployment 1 leveraged a Privacy Policy Manager to dynamically enforce compliance, while Deployment 2 provided automated triaging that helped maintain the ethical standards of community-driven platforms like GitHub.\n3. Lessons in Contextual Sensitivity: Contextual sensitivity emerged as a key factor in both deployments. For instance, Deployment 1 resolved ambiguities by analyzing relationships between entities (e.g., combining names with date of birth). Deployment 2 faced challenges in"}, {"title": "Conclusions and Future Work", "content": "This paper presents a comparative analysis of two real-world deployments of the OneShield Privacy Guard framework, emphasizing their role in preserving privacy across diverse operational contexts. Deployment 1, integrated into Data and Model Factory, demonstrated the scalability and adaptability required for enterprise-level applications, particularly in multilingual and regulatory-complex environments. Deployment 2, implemented in PR Insights, highlighted the effectiveness of lightweight privacy solutions in community-driven platforms, leveraging human feedback to refine automated PII detection.\nBoth deployments highlight the importance of using context-aware entity recognition, dynamic policy enforcement, and a balance between automation and human oversight. They address key challenges like ambiguity in sensitive data classification and compliance with regional regulations, offering valuable insights for building privacy-preserving frameworks in LLMs.\nFuture advancements should focus on expanding privacy safeguards to handle multimodal data, including images, audio, and video, to address risks in cross-modal AI systems like vision-language models. Self-learning mechanisms, such as reinforcement learning, can help frameworks adapt to changing regulations and cultural sensitivities, reducing manual intervention while maintaining accuracy. Finally, creating standardized benchmarks with multilingual and cross-domain datasets will improve the evaluation and comparability of privacy-preserving tools. These steps are essential for developing scalable, ethical, and effective AI technologies in complex data environments."}]}