{"title": "Discounted Pseudocosts in MILP", "authors": ["Krunal Kishor Patel"], "abstract": "In this article, we introduce the concept of discounted pseudocosts, inspired\nby discounted total reward in reinforcement learning, and explore their applica-\ntion in mixed-integer linear programming (MILP). Traditional pseudocosts estimate\nchanges in the objective function due to variable bound changes during the branch-\nand-bound process. By integrating reinforcement learning concepts, we propose a\nnovel approach incorporating a forward-looking perspective into pseudocost estim-\nation. We present the motivation behind discounted pseudocosts and discuss how\nthey represent the anticipated reward for branching after one level of exploration in\nthe MILP problem space. Initial experiments on MIPLIB 2017 benchmark instances\ndemonstrate the potential of discounted pseudocosts to enhance branching strategies\nand accelerate the solution process for challenging MILP problems.", "sections": [{"title": "1. Introduction", "content": "1.1 Reinforcement Learning basics\nIn reinforcement learning (RL), an agent operates within an environment. At any given\ntimestamp, the environment specifies its state. The agent has a set of actions available\nto choose from. The agent selects some action. This modifies the current state of the\nenvironment and the agent gets some reward from the environment.\nThe primary objective of the agent is to maximize the total reward obtained through\nits actions. The reward that the agent receives immediately after any action is called an\n'immediate reward'. The agent sometimes can pick a greedy action that maximizes the\nimmediate reward without caring about the future states or rewards. But, mostly we are\ninterested in the total reward. In reinforcement learning and in real life as well, we care\nmore about the immediate reward than the future rewards. So, we use discounted total\nreward (DTR) by multiplying the future rewards by a constant factor called the discount\nfactor (\u03b3). See equation (1).\nDTR = R + \\gamma R_1 + \\gamma^2 R_2 + ...\n1.2 Related work\nRecently, several researchers have applied RL to Mixed Integer Linear Programming\n(MILP). For this article, we focus on the branching part. We list some of the work\nthat uses RL for MILP. This list is not complete."}, {"title": "2. Discounted pseudocosts", "content": "When using RL for MILP, the objective is to design an effective reward function. One\nexample of a reward function for branching is to give '-1' as a reward for each node. The\nagent is incentivized to minimize the number of nodes in the search tree. Some of the\nprevious work cited above used this reward function.\nWe can also see pseudocosts [B\u00e9nichou et al., 1971] as a reward function for branching\non a particular variable. Pseudocosts capture the improvement in relaxation value for\njust one level. So, this is equivalent to the concept of 'immediate reward' in RL.\nOur objective is to extend pseudocosts to capture the linear programming (LP) relax-\nation bound improvements observed later in the tree, thereby aligning more closely with\nthe concept of 'total reward' or 'discounted total reward' in RL.\nWe refer to the modified pseudocosts as discounted pseudocosts. Assume we are at a\nnode with the LP relaxation objective value L. Now we branch on a variable x and on the\nbranch x \u2264 [x], we get the LP relaxation objective value Lr. Our traditional pseudocost\n(PSo(x)) update for x would be $\\frac{L - L_r}{\\{x\\}}$, where {x} denotes the fractional part of variable\nx in the relaxation solution. Now let us assume we branch here on variable y, and on\nthe branch y \u2264 [y] we get the LP relaxation objective value Ly. Here we update the\ntraditional pseudocost of variable y (PS\u2030(y)) by adding datapoint $\\frac{L_y - L_x}{\\{y\\}}$.\nIn discounted pseudocosts, we additionally maintain the first level pseudocosts for each\nvariable. For the case described above, we update the first level discounted pseudocost of\nx (PS1(x)) by adding datapoint $\\frac{L_y - L_x}{\\{x\\}}$ In other words, we give some credit of second\nLP relaxation objective gain to variable x. This new update is for the first level which is\ndiscounted by a factor gamma. Further levels are similarly discounted by higher powers of\ngamma. The complete discounted pseudocost used for branching is described in equation\n(2).\nDPS(x) = PSo(x) + \u03b3PS1(x) + \u03b3^2 PS2(x) + ...\nFor the first implementation, we used one-level discounted pseudocosts to make the\nimplementation simple and learn what a good discount factor should be.\nThere is also another way of looking at discounted pseudocosts. Traditional pseudo-\ncosts approximate the strong branching choice whereas one-level discounted pseudocosts\napproximate the lookahead branching [Glankwamdee and Linderoth, 2006] choice (when\n\u03b3 = 1). The lookahead branching gives smaller trees compared to strong branching,"}, {"title": "3. Computational results", "content": "We use the MIPLIB dataset and solve instances with SCIP version 9.0\n[Bolusani et al., 2024] using three different seeds. This gives us a total of 720 instance-\nseed pairs. Although we have experimented with turning off presolvers, separators, and\nheuristics, in this article, we only present the results with the default setting where only\nthe branching rule is changed. The results for the other experiments were similar to the\nones presented below.\nFirst, we compare the results with pseudocosts vs discounted pseudocosts in Table\n1. The columns\u2018pscost\u2019represent the pseudocost branching rule and the 'dpscost' rep-\nresent one-level discounted pseudocost branching rule. We present the number of solved\ninstances, shifted geometric mean (shifted by 1s) of solving time (ratio for the 'dpscost'\nvariant), and shifted geometric mean (shifted by 100 nodes) of number of nodes (ratio for\nthe \u2018dpscost' variant). The brackets show results for all as well as affected instances. The\nrows > Xs show the results for instances in which at least one variant took more than X\nseconds to solve. We used a time limit of two hours for this experiment.\nThere are three more instances solved and nearly no significant improvement in either\nsolving times or nodes. We also performed similar experiments with all features turned\noff and solution cutoff provided for which we observed similar results (not presented in\nthis article). These numbers show a minor improvement which can also be called noise.\nThe last bracket (\u2265 1000s) shows 8% improvement in solving time.\nThe choice of discount factor was also very hard to make because of such benchmark\nresults. But we settled on 0.2 after a few experiments. We expect the optimal value of\nthe discount factor to lie in the [0.2, 0.5] interval.\nNext, we show the comparison with reliability branching in Table 2. The time limit\nwas one hour for this experiment. The numbers show similar improvement as in the\ncomparison with pseudocosts. Here we see that fewer instances are affected. The variant\nwith discounted pseudocosts (rdpscost) solves 3 more instances than the default variant"}, {"title": "4. Future work and Conclusion", "content": "There are many possible extensions of discounted pseudocosts to improve the current\nresults. One idea is to replace pseudocosts with discounted pseudocosts in the primal\nheuristics that use them.\nThe natural extension of going for two or more levels seems a bit far as of now. The\nmajor issue is with the reliability of higher-level discounted pseudocosts.\nThe following ideas were suggested by Timo Berthold.\n\u2022 Only update the first level of discounted pseudocost if the first branched variable\n(x in the example) is close to the second branched variable (y in the example) in\nthe variable constraint graph or in general are known to be related. This should\nhopefully decrease noise in the updates and give better results.\n\u2022 We can also use discounting to extend conflict score or other fields that we maintain\nin the variable history and use for making the branching decision.\nIn conclusion, we presented an idea of discounted pseudocosts where we can use RL\nconcepts in MIP for branching. Discounted pseudocosts do not require any offline training\nand therefore work on general MILPs. The initial implementation with very little tuning\nfor the discount factor shows a small improvement in the solving times for harder instances."}]}