{"title": "Panacea: Novel DNN Accelerator using Accuracy-Preserving Asymmetric Quantization and Energy-Saving Bit-Slice Sparsity", "authors": ["Dongyun Kam", "Myeongji Yun", "Sunwoo Yoo", "Seungwoo Hong", "Zhengya Zhang", "Youngjoo Lee"], "abstract": "Low bit-precisions and their bit-slice sparsity have recently been studied to accelerate general matrix-multiplications (GEMM) during large-scale deep neural network (DNN) inferences. While the conventional symmetric quantization facilitates low-resolution processing with bit-slice sparsity for both weight and activation, its accuracy loss caused by the activation's asymmetric distributions cannot be acceptable, especially for large-scale DNNs. In efforts to mitigate this accuracy loss, recent studies have actively utilized asymmetric quantization for activations without requiring additional operations. However, the cutting-edge asymmetric quantization produces numerous nonzero slices that cannot be compressed and skipped by recent bit-slice GEMM accelerators, naturally consuming more processing energy to handle the quantized DNN models.\nTo simultaneously achieve high accuracy and hardware efficiency for large-scale DNN inferences, this paper proposes an Asymmetrically-Quantized bit-Slice GEMM (AQS-GEMM) for the first time. In contrast to the previous bit-slice computing, which only skips operations of zero slices, the AQS-GEMM compresses frequent nonzero slices, generated by asymmetric quantization, and skips their operations. To increase the slice-level sparsity of activations, we also introduce two algorithm-hardware co-optimization methods: a zero-point manipulation and a distribution-based bit-slicing. To support the proposed AQS-GEMM and optimizations at the hardware-level, we newly introduce a DNN accelerator, Panacea, which efficiently handles sparse/dense workloads of the tiled AQS-GEMM to increase data reuse and utilization. Panacea supports a specialized dataflow and run-length encoding to maximize data reuse and minimize external memory accesses, significantly improving its hardware efficiency. Numerous benchmark evaluations show that Panacea outperforms existing DNN accelerators, e.g., 1.97\u00d7 and 3.26\u00d7 higher energy efficiency, and 1.88\u00d7 and 2.41\u00d7 higher throughput than the recent bit-slice accelerator Sibia and the SIMD design, respectively, on OPT-2.7B, while providing better algorithm performance with asymmetric quantization.", "sections": [{"title": "I. INTRODUCTION", "content": "Large-scale deep neural networks (DNNs) have been em- ployed as powerful solutions for a variety of practical applica- tions, including image classification [1]\u2013[3], natural language processing (NLP) [4]\u2013[9], AR/VR [10], and robotics [11]. However, their extensive computations and external memory accesses (EMA) result in poor throughput and increased en- ergy consumption during inferences [12]\u2013[15], leading to more significant challenges for a large-scale DNN's deployment on resource-constrained edge devices [16], [17].\nTo address these challenges, recent works [33]\u2013[48] have developed energy-efficient DNN inference accelerators by em- ploying algorithm-hardware co-optimization techniques such as weight pruning [33]\u2013[40] and quantization [41]\u2013[49]."}, {"title": "II. BACKGROUND & MOTIVATION", "content": "A. Quantization and integer GEMM\nUniform Quantization. Quantization plays a pivotal role in reducing the computational cost and memory footprint by enabling computations with low bit-precision [30], [41]\u2013 [47], [49], [62]\u2013[68]. There are two uniform quantization techniques: symmetric and asymmetric quantization schemes, which usually use signed and unsigned integers, respectively. Given an input matrix x, two schemes are computed as follows:\n$\\begin{aligned} X_{i n t b} &=Q(x ; s, b)=\\operatorname{clip}\\left(\\left[\\frac{x}{s}\\right] ;-2^{b-1}, 2^{b-1}-1\\right), \\\\ X_{u i n t b} &=Q(x ; s', z p, b)=\\operatorname{clip}\\left(\\left[\\frac{x}{s'}+z p\\right] ; 0,2^{b}-1\\right), \\end{aligned}$"}, {"title": "B. Previous Bit-Slice GEMMs", "content": "To enhance hardware efficiency during dense model infer- ences, recent DNN accelerators [53]\u2013[56] utilize low precision, and further mitigate the complexity of integer GEMMs by segmenting integers into bit-slices and leveraging sparsity at high-order (HO) slices. For example, as illustrated in Fig. 3(a), the straightforward bit-slicing [54] typically divides an 8-bit integer into a 4-bit signed HO slice and a 4-bit unsigned low-order (LO) slice, skipping MAC operations for $0000_{(2)}$ HO slices. However, this approach cannot skip $1111_{(2)}$ HO slices, which occur as frequently as $0000_{(2)}$ in symmetric quantization [70]\u2013[74], limiting slice-level sparsity.\nThe signed bit-slice representation (SBR) [53], as shown in Fig. 3(b), has been introduced to overcome this limitation. The SBR captures zero HO slices from both positive and negative near-zero values by dividing a (3n + 4)-bit integer into one 4-bit signed HO slice and n 3-bit unsigned LO slices, where n is a positive integer. Then, 3-bit unsigned LO slices are extended to 4-bit signed slices. For the case of n = 1 depicted in Fig. 3(b), the 3-bit LO slice is first extended to a 4-bit signed slice by appending the sign bit of the HO slice, and then $1111_{(2)}$ HO slice is added by $0001_{(2)}$ to compensate for the attached sign bit, converting it to $0000_{(2)}$. Given the enhanced slice-level sparsity, the bit-slice GEMM in [53] is designed to group v slices into a v-length slice-vector and skip operations for vectors that consist entirely of zero slices. More precisely, based on the SBR dividing a 7-bit integer value into two 4-bit slices [53], the bit-slice GEMM is calculated as\n$W_{i n t 7} X_{i n t 7}+b_{i n t 32}=\\left(W_{H O}+W_{L O}\\right)\\left(X_{H O}+X_{L O}\\right)+b_{i n t 32} .$"}, {"title": "C. Motivation", "content": "Since most of the actual activations in DNN layers have an asymmetric distribution [50], [51], symmetric quantization cannot fully utilize the quantization bit-width, potentially leading to accuracy loss. To address this challenge, recent algorithm-level works have embraced asymmetric quantiza- tion, specifically for activations [23]\u2013[32]. However, one draw- back of asymmetric quantization is that it does not generate sufficient zero slices, which can be skipped in the existing bit-slice GEMMS [53]\u2013[56], as shown in Fig. 5(a). To address this limitation, there is a clear need for a novel bit-slice GEMM ap- proach, which skips frequent nonzero slices in asymmetrically- quantized activations, and optimization methods that increase the frequency of skippable nonzero slices. In response to this need, this paper introduces a new bit-slice GEMM along with several optimization methods and its accelerator, Panacea, to reduce memory accesses and the number of computations while maintaining high accuracy, as shown in Fig. 5(b)."}, {"title": "III. PANACEA ACCELERATOR", "content": "A. Overview\nPanacea is a novel DNN accelerator that supports both asymmetric quantization and sparse bit-slice computations to achieve high accuracy and energy efficiency. Fig. 6 shows an overview of the proposed work, mainly consisting of asymmetrically quantized bit-slice GEMM (AQS-GEMM) in Panacea, which enables the bit-slice GEMM for asymmet- ric quantization during inference (Section III-B), and opti- mization methods: zero-point manipulation (ZPM) and distribution-based slicing (DBS), which increase slice-level sparsity of activations during calibration (Section III-C). The PTQ calibration quantizes weights and extracts activation's quantization parameters, including $s_{x}$ and $z p_{x}$, as outlined in (1) and (2). The parameters are adjusted through ZPM and DBS to enhance slice-level sparsity by considering the slice skip range of the AQS-GEMM. During inference, Panacea utilizes the compressed weights, symmetrically quan- tized during calibration, and compressed activations, asymmet- rically quantized based on the parameter values obtained from calibration, for bit-slice GEMMs. Panacea's AQS-GEMM core efficiently handles bit-slice computations by skipping not only zero slices in symmetric quantization, but also frequent nonzero slices in asymmetric quantization. Note that accu- mulated GEMM results, i.e., activations, are re-quantized and compressed for the next layer. In terms of hardware architec- ture, Panacea separates dynamic and static workload operators to optimize sparse and dense computations, maximizing data reuse and minimizing EMA (Section III-D).\nB. AQS-GEMM\nPanacea implements symmetric quantization for weights and asymmetric quantization for activations. For weight quan- tization, it adopts SBR [53] segmenting (3n + 4)-bit weight $W_{i n t}$ into (n + 1) 4-bit sliced matrices, while for activation quantization, it utilizes the straightforward bit-slice represen- tation method [54], [76] segmenting (4k+4)-bit activation $x_{u i n t}$ into (k+1) 4-bit sliced matrices. For a good understanding, this section explains the case of n = 1 and k = 1, i.e., $W_{i n t 7}$ and $X_{u i n t 8}$. To compute $W_{i n t 7} x_{u i n t 8}$, Panacea divides it into four bit- slice computations: $W_{H O} X_{H O}, W_{L O} X_{H O}, W_{H O} X_{L O}$, and $W_{L O} X_{L O}$.\nAs depicted in Fig. 5(a), asymmetric quantization creates distributions that are not centered around zero, frequently observing specific nonzero HO slices near their zero point values (zp). The floating zp for different activations compli- cates the implementation of bit-slice GEMMs, as it changes a frequent nonzero HO slice observed in each layer. To address these challenges and enhance computational efficiency, we propose the AQS-GEMM, which compresses out these frequent nonzero slices generated by asymmetric quantization, and skips their MAC operations."}, {"title": "C. Enhancing Slice-Level Sparsity", "content": "Since the AQS-GEMM achieves better efficiency thanks to the increased number of compressed vectors, increasing slice- level sparsity for both weights and activations is important. Note that improving the slice-level sparsity generally results in increased vector-level sparsity. However, activation's asym- metric quantization produces varying distributions, leading to low slice sparsity. To address this problem, we propose two algorithm-hardware co-optimization methods: zero-point ma- nipulation (ZPM) and distribution-based slicing (DBS) within the PTQ calibration depicted in Fig. 6.\nSparsity-aware zero-point manipulation. Fig. 8(a) shows an example of asymmetric quantization leading to low sparsity at HO slices of an input activation. The example activation dis- tribution of an FC layer in OPT-2.7B [60] is centered around the zero point $z p=161$. When applying the compression of HO slices, the frequent HO slice becomes $r=1010_{(2)}$, and only about 68% of total values are present in the skip range observing $1010_{(2)}$ HO slices. The actual compression ratio of XHO slices would be significantly lower than 68% due to grouping slices into 1\u00d74 vectors before compression, leading to low efficiency in the AQS-GEMM. To address this problem, the ZPM increases the number of compressible HO slices by adjusting the zero point during the PTQ's calibration as\n$z p^{\\prime}=\\begin{cases}\\left[z p / 2^{\\prime}\\right]+2^{\\prime-1}, & \\text { if } z p>0 \\\\ 0, & \\text { otherwise, }\\end{cases}$"}, {"title": "D. Hardware Design", "content": "Overall architecture. Fig. 11 shows the overall architecture of Panacea, which consists of a top controller, on-chip memory, an AQS-GEMM core, and a post-processing unit (PPU). The on-chip memory is divided into weight memory (WMEM), activation memory (AMEM), output memory (OMEM), and a memory manager. The AQS-GEMM core, with a global activation buffer and 16 processing element arrays (PEAs), efficiently computes the AQS-GEMM with weights and ac- tivations from the on-chip memory. Each PEA has an index decoder (IDXD), a weight index buffer, a workload sched- uler, a weight buffer (WBUF), dynamic workload operators (DWOs), static workload operators (SWOs), two compensators (CSs), a compensation buffer (CSBUF), a local partial-sum buffer and two shift-and-accumulator units (S-ACCs). With RLE indices, the IDXD recovers the indices of uncompressed vectors and stores them in the weight index buffer. By match- ing the decoded weight indices with indices of activation vectors received from the global buffer, the workload scheduler allocates outer products for uncompressed slices into DWOs and SWOs. Each of DWO and SWO is comprised of an outer product calculator (OPC), which consists of 16 4bx4b sign- unsigned multipliers to compute an outer product with a 4\u00d71 weight slice-vector and a 1 \u00d7 4 activation slice-vector."}, {"title": "IV. EVALUATION", "content": "Our evaluations demonstrate Panacea's hardware efficiency, comparing it to the previous dense DNN accelerators including systolic array using weight stationary (SA-WS) and output stationary (SA-OS) [58], SIMD [59], and the recent bit-slice accelerator Sibia [53]. Note that SIMD, SA-WS, and SA- OS compute dense GEMM with 8-bit quantized weights and activations, while Panacea and Sibia use 4b\u00d74b multipliers to compute bit-slice GEMMs. For a fair comparison, we make all designs to utilize identical design parameters: the number of multipliers, the size of on-chip SRAM, and the bandwidth between DRAM and accelerators. More precisely, we assume that they utilize 3072 4b\u00d74b multipliers, 256- bit/cycle DRAM bandwidth, 192KB on-chip SRAM to store weight slices, input activations, and outputs, where an 8b\u00d78b mul- tiplier is equivalent to four 4b\u00d74b multipliers. To estimate the energy cost of external memory, we use the DRAM emulator, CACTI 7.0 [79] and estimate the performance of designs based on a 28nm CMOS technology. Given a specific accelerator architecture and its data-flow, we count the number of cycles and the number of activated modules during inference based on Hugging-Face open-source DNN models, considering bit- slice sparsity in real benchmarks. Then, we use the obtained data to estimate energy with the post-layout results of building blocks like multipliers, adders, and buffers."}, {"title": "V. CONCLUSION", "content": "This paper proposes a bit-slice DNN accelerator, Panacea, that enables the bit-slice GEMM with asymmetric quanti- zation. Panacea's AQS-GEMM compresses high-order slices frequently observed in asymmetric quantization and skips their computations. To improve the efficiency of AQS-GEMM, Panacea presents two algorithm-hardware co-optimization methods: ZPM and DBS, which increase slice-vector sparsity for activations. To efficiently support the proposed methods, each processing element of Panacea dedicates two types of operators and utilizes the specialized dataflow, maxi- mizing data reuse. Consequently, Panacea provides attrac- tive hardware/algorithm performances during DNN infer- ences. Our DNN evaluation code will be open-sourced at https://github.com/Dongyunkam/Panacea."}]}