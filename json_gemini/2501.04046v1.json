{"title": "Traits of a Leader: User Influence Level Prediction through Sociolinguistic Modeling", "authors": ["Denys Katerenchuk", "Rivka Levitan"], "abstract": "Recognition of a user's influence level has attracted much attention as human interactions move online. Influential users have the ability to sway others' opinions to achieve some goals. As a result, predicting users' level of influence can help to understand social networks, forecast trends, prevent misinformation, etc. However, predicting user influence is a challenging problem because the concept of influence is specific to a situation or a domain, and user communications are limited to text. In this work, we define user influence level as a function of community endorsement and develop a model that significantly outperforms the baseline by leveraging demographic and personality data. This approach consistently improves RankDCG scores across eight different domains.", "sections": [{"title": "1 Introduction", "content": "Influential users have the ability to influence others' behavior to achieve their own agenda. This agenda can be anything from an attempt to persuade a person to make a particular purchase to an attempt to overthrow the government. For example, consider the January 6th, 2021, events when a group of people stormed the US Capitol building. According to the New York Times, the group self-organized on websites such as Reddit\u00b9. We believe that by assessing the ideas and the stance of the group leaders, it is possible to predict the severity of the situation and prevent such events from happening.\nUser influence prediction is a difficult problem for many reasons. First, the concept of user influence depends on a problem and its domain. In the literature, terms such as influencers, community endorsed person, community leader, opinion leader, and others correspond to some form of user leadership (Razis et al., 2020). Each term has its own definition with respect to the domain. As a result,"}, {"title": "2 Related Work", "content": "The definition of an influential person is different with respect to a particular problem, and consequently, it is difficult to compare the results of previous studies. For this reason, we explore the relationships between users and influence in a broader spectrum. In particular, to find a correlation between user traits and influence, we review studies on behavior analysis in a corporate environment, prediction of popular content, and user leadership in general.\nThe researchers tried to find an answer to what makes a person influential. Some early work in social sciences has found a correlation between influence and personality traits. For example, Gehring (2007) investigates the correlation between Meyers-"}, {"title": "3 Data and Evaluation", "content": "Reddit is a discussion platform where users discuss any topics. The website is divided into subreddits, which are sub-communities with specific discus-"}, {"title": "3.1 Reddit Data", "content": "sions. Reddit users can create posts to initialize a discussion or write comments in response. Each comment can earn or lose karma points. This karma score can be used as a proxy for a reward or community endorsement, making Reddit a good data source to study influence. This paper uses the Reddit dataset proposed by Jaech et al. (2015); Fang et al. (2016). This dataset provides Reddit data collected between January 1, 2014, and January 31, 2015, from 8 subreddits: AskMen, AskScience, AskWomen, Atheism, ChangeMyView, Fitness, Politics, and WorldNews. The dataset is a collection of posts and comments with additional information such as author, author flair, karma scores, etc. Our work is constrained to textual comments, and we do not use user or structure-related information. The comments' length is enforced to a minimum of 32 tokens (white space tokenization) and a maximum of 256 tokens. We sample 100k comments from each subreddit, with the AskScience subreddit containing only 33k comments. This data is further split into dev and test subsets 10% each.\nThis work adopts a k-index score to represent user status, which was introduced by (Jaech et al., 2015). The k-index is defined as a maximum number of comments, let's say k, that has at least k karma. This score is essentially a modified h-index, an author-level metric (Hirsch, 2005). This score is used to mitigate outliers, where some comments can gain high karma scores for being one-off popular comments, off-topic, funny, etc. The k-index is calculated for each user in a single discussion thread and mapped to the corresponding author-written comments. We assume local user popularity, where a user might have a high status in one discussion thread but not another. We only consider the first 50 comments per discussion, disregarding the rest and discussion threads with less than 50 comments. One interesting characteristic of this data is that the k-index distribution is highly right-skewed, as seen in Figure 1. The figure shows log-scale k-index counts where most users have a k-index of 1, and very few have a k-index of 13. Hence, identifying rare, high-rank users is quite difficult."}, {"title": "3.2 Evaluation", "content": "This work aims to develop new methods to improve user status prediction from a single comment. Considering the specifics of highly right-skewed k-index distribution where most users have a low k-index, and very few users have a high k-index, the task is defined as a ranking problem emphasizing identifying high-status users. For this reason, we use four evaluations: mean absolute error (MAE), mean squared error (MSE), normalized discounted cumulative gain (nDCG), and RankDCG. MAE and MSE are popular measures that help evaluate regression-based models by calculating the error between true and predicted labels. MAE reports the mean of the differences, and MSE is the mean of squares of the differences, which better represents high-status user prediction. NDCG (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002) is designed for information retrieval tasks. One advantage of nDCG measures is that it puts more emphasis on identifying high-status users. However, this measure has two noted downsides. First of all, this evaluation metric was designed for information retrieval rather than ordering evaluation. Secondly, the reward function depends on each element's position rather than relative class. All these shortcomings are explained and resolved in work by Katerenchuk and Rosenberg (2016) and their RankDCG measure. The main advantage of the RankDCG algorithm is that it provides a clear lower and upper bound for the rank-ordering type of problems. RankDCG is also designed to work with right-skewed data labels, as in our case, where most users have a low k-index, and very few users have a high k-index. RankDCG algorithm scores high k-index users much higher compared to low-rank users. Throughout our experiments, we provide the results of four measures: MAE, MSE, nDCG, and RankDCG."}, {"title": "4 Methods", "content": "This section describes our methods for predicting user k-index from a comment. First, we show that the problem is feasible by training a BERT model (Devlin et al., 2019) and using this BERT model"}, {"title": "4.1 Overview", "content": null}, {"title": "4.2 User Status Prediction", "content": "Attention-based models have shown great success on NLP tasks. In this work, we use the small BERT model\u00b3 (Turc et al., 2019) for the following reasons: 1) while there are many transformer-based models, they show only incremental improvements compared to the original BERT model (Narang et al., 2021), 2) transformer-based models have high VRAM requirement, which makes them cost-prohibitive in experimental settings. The small BERT allows us to train a model with a batch size of 32 on consumer-grade GPUs within a reasonable time. This small BERT implementation consists of 4 hidden layers of 512 dimensions, each with eight attention heads. Hence, our experiments use the pre-trained small BERT as a base layer.\nThe BERT model outputs embedding vectors for the entire text input (E[CLS]), and for each token (E1, .., En); however, we ignore the word token vectors and only use the input embedding vector (E[CLS]). This embedding vector is used as an input to a feed-forward task-specific layer with 256"}, {"title": "4.3 Pseudo Label Generation", "content": "Our hypothesis states that k-index prediction can be improved by leveraging user-centric information. After reviewing related literature, user characteristics such as age, gender, and use of hedges are associated with user status and status manifestation (Rosenthal, 2014; Prabhakaran et al., 2014; Gilbert, 2012). In addition, we explore the Myers-Briggs Type Indicator (MBTI) (Myers, 1987) model associated with user personality traits. These traits correspond to different user behaviors online (Wu and Atkin, 2017). However, our dataset does not include user-related annotations. For this reason, we train separate models to annotate the data with predicted pseudo labels."}, {"title": "4.3.1 Demographic and Personality Trait Annotations", "content": "To create annotations for age, gender, MBTI types, and hedges, we train a separate model for each task. The training data comes from the PAN-DORA corpus (Gjurkovi\u0107 et al., 2021). This corpus contains a collection of Reddit users, comments, and labels for age, gender, Extraverted/Introverted, Sensing/Intuitive, Thinking/Feeling, and Judging/Perceiving. Each model is trained to predict users' age (regression) or gender (raw sigmoid score) and MBTI types as a binary label (classification) from a single comment. We sample and balance the data for each task. While developing the SOTA models for the sub-tasks is not the objective of this paper, we do experiment with different neural network architectures such as LSTM (2 layers, tanh activation), CNN (3 conv. layers with 2, 4, 8 kernel size, relu activation), and the BERT model architecture described in Sec. 4.2. Table 1 reports the best-performing architectures and their scores. These models generate pseudo labels for our dataset described in Sec. 3.1."}, {"title": "4.3.2 Hedge Annotations", "content": "Hedges are linguistic devices commonly used to mitigate orders, statements, or opinions (Lakoff, 1975). High-status individuals often use hedges to mitigate a statement or an order. Hence, detecting hedge comments can improve k-index prediction. After reviewing recent research in this domain, we use the model proposed by (Katerenchuk and Levitan, 2021). The model is a dual input model of text and part-of-speech (POS) tags. The inputs are fed into two parallel models of LSTM layers for POS and GRU layers for sentences. The latent representation of the LSTM and GRU layers merged into a single layer used as an input to the feed-forward output layer. We choose this model for the following reasons: 1) the model architecture is straightforward, 2) it is efficient regarding train-"}, {"title": "4.3.3 Pseudo Label Analysis", "content": "Pseudo-label annotation is an excellent way to generate missing labels. However, it is difficult to assess their quality without the actual labels. This brings a question: How accurate is our data annotation? One way to assess the quality is to look at the prediction distributions. In Table 2, we show the mean predictions of each pseudo label with respect to the subreddit. While it is hard to interpret all values, we can look into the max (in bold) and the min (underscored) values. The table highlights a couple of interesting patterns: the predicted age is the highest in the Politics subreddit and the lowest in Fitness. The assumption is that politics attracts older users, and discussions about fitness for younger users are reasonable. The gender prediction shows that 80% of female-written comments are in the AskWomen subreddit. This confirms the subreddit's purpose that men ask women questions and, as a result, most answers come from women. Furthermore, the highest predictions for Introversion, iNtuition, and Thinking (INT) are for the AskScience subreddit. According to the MBTI personality type system, the Scientist (INTJ) is defined with these three dimensions. Hedge words are the highest in the ChangeMyView subreddit, which aligns with the theory that hedge phrases are used to mitigate statements, sound polite, and influence others' opinions (Lakoff, 1975). The pseudo labels confirm common beliefs in this area."}, {"title": "4.4 Multi-Task", "content": "After generating pseudo labels, we can use this user information by introducing additional learning objectives to improve latent representation. Multi-task learning is an excellent way to introduce additional learning objectives to the model. The multi-task model is an extension of the architecture introduced in 4.2. We use the same BERT model and"}, {"title": "4.5 Loss Weight Tuning", "content": "Weight loss tuning for each sub-task is often time-prohibitive. Keras tuner (O'Malley et al., 2019) automates this process with search algorithms. Hyperband (Li et al., 2017) search algorithm is used in our work to find the best weights. The search algorithm is limited to 20 epochs with early stopping and one iteration. The search space is from 0.0 to 1.0 with a step of 0.1 for each of w1, W2 . . . W7. The total model loss is defined as follows:\nLtotal = Lo+W1*L1+W2*L2+\u00b7\u00b7\u00b7+W7*L7 (1)\nwhere Lo is k-index loss, Li - sub-task specific loss, and wi - the loss weigh."}, {"title": "5 Results", "content": "This paper is based on the hypothesis that additional user-centric information can improve user status prediction. To show that the hypothesis stands, we create four models for the k-index prediction task: 1) BERT model, 2) fine-tune BERT, 3) multi-task BERT with user-centric sub-task, and 4) multi-task BERT with tuned loss weights for sub-tasks. The results in Figure 3 are reported as a mean of five runs across four measures: a) MAE, b)"}, {"title": "6 Analysis", "content": "This section reflects on our findings by examining our hypothesis and the models. First, we look into sub-task contributions to determine which sub-problems are more salient. Then, we evaluate the la-"}, {"title": "6.1 Sub-task Impact", "content": "To examine which sub-task is the most impactful, we look at each sub-task weight from sec. 4.5. The weights are the results of Hyperband search algorithms that iteratively try different values that lead to the highest result. Figure 4 shows mean loss weight values across all subreddits. The chart shows that Introversion, Intuitive, and Gender sub-tasks are the most prominent and have the highest loss weights. However, by examining loss weights in each subreddit, we can observe that the weights are domain-specific. In the table 3, we can see that in the AskMen and the AskWomen, gender-centric subreddits, the sub-task for predicting a user's gender plays an important role. The AskScience subreddit relies on Introversion and Thinking prediction sub-tasks. The fitness subreddit also puts weight on 0.9 on the Introversion/Extroversion sub-task. We hypothesize that this is due to extroverts being more active in the subreddit, as was shown in 4.3.3. Another interesting observation can be made"}, {"title": "6.2 Latent Space", "content": "We hypothesize that by introducing additional user-related data, the models leverage this information to improve latent representation, which is bene-"}, {"title": "6.3 Cross-domain Results", "content": "The sections above show that introducing user-related sub-tasks improves the model's latent representation and performance. In this section, we would like to consider this: Can this approach improve the performance across domains? In NLP, many problems rely on domain-specific language, and the results across domains suffer. We hypothesize that we can improve the performance across different domains by leveraging sub-problems. The idea is that the model would learn universal user representation and use this knowledge to produce more accurate results in a new domain.\nWe experimented with BERT, fine-tuned BERT, and multi-task BERT models. The task is designed to sample 20k instances from all eight subreddits and use the seven subreddits (140k samples) as a training set and one subreddit as a dev and test set (10k samples each). We repeat this experiment for each subreddit in a leave-one-out fashion.\nAfter running the experiments, we saw the results of BERT and fine-tuned BERT produced slightly lower scores of 11.6% and 13.21% as the mean across all subreddits compared to the results described in sec. 5. However, adding a user-related sub-task yielded the lowest results, with a mean RankDCG of 5.51%. Such low performance can be observed across all subreddits. We believe that such low results are due to user differences across each subreddit, which we observe in both tables 2 and 3. The detailed scores are provided in the appendix section in Table 8."}, {"title": "7 Conclusion", "content": "This work proposes a new method to improve user status prediction on social media. While the area of NLP and user status prediction relies on more extensive and more demanding models, we leverage the earlier work that shows correlations between user traits, demographics, and user status. In particular, we train separate models and use them to annotate our data with age, gender, hedge, and MBTI personality types. All these auxiliary pseudo labels are used in a multi-task BERT model. This approach improves performance over BERT and fine-tuned BERT models. Furthermore, after analyzing the modes, the loss weights support common beliefs in behavior."}, {"title": "Limitations", "content": "This paper demonstrates that user-centric information can be used to improve user influence level prediction. However, we also discover some limitations of this work. First of all, this work focuses on the Reddit dataset. While the Reddit website attracts diverse users to discuss various topics, we would like to extend our experiments to different social media platforms to validate our results in future work.\nSecond, this work shows a significant improvement on in-domain data; however, it does not generalize well across different domains. After our experiments, we believe that the user behavior is domain-specific. By introducing user-centric sub-tasks, the model learns user characteristics that are irrelevant to this different domain. Such results can be observed in AskScience and Fitness subreddits (Appendix Table 8). These subreddits attract users that generally represent specific traits and behaviors.\nLastly, this work relies on user-centric information to improve influence level prediction. However, we do not have the ground truth labels; the data is annotated with pseudo labels. These auxiliary pseudo-labels provide only partial information about the users. Despite improving results across our eight domains, having the ground truth labels can bring even more significant improvements."}, {"title": "Ethics Statement", "content": "This research introduces novel methodologies to improve the prediction of user influence levels on social media platforms by incorporating insights from socio-linguistic research, behavioral sciences, and computational models. We use a concept of \"influence\" that is inherently context-dependent and varies across different domains and settings. Our definition of influence as a function of community endorsement is specific to online social networks and defined as a normalized number of karma points. This work should not be indiscriminately applied to other contexts, such as professional environments, where such algorithms might reinforce existing biases or create new forms of discrimination.\nFurthermore, using text classification algorithms and user-centric information raises concerns regarding privacy and potential bias. Models trained on social media data may learn biases present in the input data, associating specific demographics, per- sonality traits, or language use with specific levels of influence. This can lead to unfair characterizations of individuals based on incomplete or biased data sets. For this reason, researchers and practitioners should be aware of model limitations and define transparent guidelines for data. This should include transparency, data anonymization, continuous bias mitigation (by making the model more general with the inclusion of other sources), and prohibiting applications that could harm individuals or society, such as surveillance, profiling, or manipulation.\nIn conclusion, while our work contributes valuable insights into predicting user influence levels, these technologies must be developed and used responsibly, with a keen awareness of their ethical implications."}, {"title": "A Example Appendix", "content": null}, {"title": "A.1 Influence Level Prediction Tables", "content": null}]}