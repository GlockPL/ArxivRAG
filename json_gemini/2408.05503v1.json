{"title": "Disentangled Noisy Correspondence Learning", "authors": ["Zhuohang Dang", "Minnan Luo", "Jihong Wang", "Chengyou Jia", "Haochen Han", "Herun Wan", "Guang Dai", "Xiaojun Chang", "Jingdong Wang"], "abstract": "Cross-modal retrieval is crucial in understanding latent correspondences across modalities. However, existing methods implicitly assume well-matched training data, which is impractical as real-world data inevitably involves imperfect alignments, i.e., noisy correspondences. Although some works explore similarity-based strategies to address such noise, they suffer from sub-optimal similarity predictions influenced by modality-exclusive information (MEI), e.g., background noise in images and abstract definitions in texts. This issue arises as MEI is not shared across modalities, thus aligning it in training can markedly mislead similarity predictions. Moreover, although intuitive, directly applying previous cross-modal disentanglement methods suffers from limited noise tolerance and disentanglement efficacy. Inspired by the robustness of information bottlenecks against noise, we introduce DisNCL, a novel information-theoretic framework for feature Disentanglement in Noisy Correspondence Learning, to adaptively balance the extraction of MII and MEI with certifiable optimal cross-modal disentanglement efficacy. DisNCL then enhances similarity predictions in modality-invariant subspace, thereby greatly boosting similarity-based alleviation strategy for noisy correspondences. Furthermore, DisNCL introduces soft matching targets to model noisy many-to-many relationships inherent in multi-modal input for noise-robust and accurate cross-modal alignment. Extensive experiments confirm DisNCL's efficacy by 2% average recall improvement. Mutual information estimation and visualization results show that DisNCL learns meaningful MII/MEI subspaces, validating our theoretical analyses.", "sections": [{"title": "I. INTRODUCTION", "content": "Cross-modal retrieval [1]-[3] aims to retrieve the most relevant samples from different modalities, crucial for various domains such as criminal investigation [4]. Existing methods typically first project multi-modal inputs to a unified feature space. They then devise feature interaction strategies at either global [5]-[7] or local [1], [8], [9] levels for similarity prediction, enhancing similarities for matching pairs while suppressing similarities of mismatched ones. Although effective, a core assumption of these methods is well-matched train data, which is inconsistent with real-world data. For example, Conceptual Captions [10] collects 3.3 million co-occurring sample pairs from the Internet, where around 20% samples are mismatched [11], [12], i.e., noisy correspondences. Tables I and IV demonstrate that the traditional training paradigm, whether training from scratch or fine-tuning pretrained models like CLIP [13], significantly underperforms on such noisy data, necessitating robust learning strategies for noisy correspondences.\nIn this context, recent works introduce noise during training to bridge the gap between well-matched datasets and noisy real-world data for enhanced robustness. Typically, these methods [12], [14], [15] employ similarity-based strategies for identification and suppression of these mismatched pairs. Specifically, they first employ the memorization effect of deep neural networks [16], [17], which enables clean samples to exhibit higher similarities than noisy ones after the initial few epochs [18]. Then they use samples' similarity distribution, showing a clear bimodal distribution, to coarsely identify and filter out noise data. Subsequently, they derive a per-sample adaptive margin from its similarity prediction to replace the constant scalar margin in triplet ranking loss [19], [20]. This adaptive margin facilitates potential noisy correspondences' compliance with the triplet ranking loss, effectively excluding the risky supervision information during training.\nHowever, previous methods suffer from two significant challenges. 1) Feature Entanglement: Figure 1a shows these methods predict similarities in a unified feature space with modality-invariant information (MII) and modality-exclusive information (MEI) entangled. Specifically, MII captures shared"}, {"title": null, "content": "components across modalities, e.g., 'the running back', serving as foundation for retrieval. Conversely, MEI is not shared across modalities, e.g., background noise, watermark in image and abstract descriptions such as 'legally deaf' and 'history' in text. Consequently, aligning MEI across modalities can erroneously skew similarity predictions towards sub-optimality, misleading similarity-based identification and suppression of noisy correspondences. Although some works introduce various strategies, e.g., feature consistency [21] and cross prediction [22], to disentangle MII from MEI, they fall short in noise tolerance and disentanglement efficacy. Specifically, for optimal MII extraction, these methods intuitively minimize the distance between MII or exchange MII to reconstruct input from another modality, which are sensitive to incorrect alignment information from noisy correspondences and may converge trivially to a subset of optimal MII while leaking the remainder to MEI. 2) Exclusive Correspondence: the hard negative strategy [23] in previous methods [14], [15] can only model the one-to-one correspondence between given query and its positive or hard negative samples (Figure 3). This strategy is sub-optimal as it fails to consider the noisy many-to-many correspondences inherent in multi-modal data [24], [25], e.g., shared entities among unpaired images and texts.\nIn light of above, we introduce DisNCL, a pioneering information-theoretic framework for feature Disentanglement in Noisy Correspondence Learning, to enhance models' robustness against mismatched pairs in training. Specifically, we introduce a novel objective based on information bottleneck principle [26] to extract two complementary components, i.e., MII and MEI. Subsequently, as illustrated in Figure 1b, we conduct similarity predictions within the disentangled modality-invariant subspace, together with the cross-modal retrieval training. This methodology aims to direct the model's focus solely on MII and exclude the detrimental impacts of MEI, boosting identification and negative impact suppression of mismatched samples with more accurate similarity predictions. Furthermore, we estimate softened targets with bootstrapping strategy in the modality-invariant subspace for sample matching. These softened targets, contrasting with one-to-one correspondence in hard negative strategy, enable more complex many-to-many cross-modal relationship modeling to further enhance model's efficacy. Moreover, it is noteworthy that we theoretically prove the optimal disentanglement of MII and MEI, along with the noise robustness of our soft cross-modal alignment, confirming our DisNCL's superior efficacy. Our contribution is summarized as follows:\n\u2022 We introduce DisNCL, the first work to introduce certifiable optimal cross-modal disentanglement efficacy for noisy correspondence learning, for enhanced robustness against noisy multi-modal training data.\n\u2022 We boost identification and suppression of noisy correspondences by accurate similarities predicted in modality-invariant subspace. Moreover, we propose a noise-robust alignment refinement strategy to model the intricate relationships in multi-modal data via soft target estimation.\n\u2022 Extensive experiments on various benchmarks confirm DisNCL's efficacy by improving 2% average recall, as well as validate our theoretical analyses."}, {"title": "II. RELATED WORK", "content": "Contemporary cross-modal retrieval methods primarily diverge into global-level methods [5], [13], [27] that treat images and sentences as holistic entities and local-level [1], [8], [28], [29] ones focusing on essential fragments e.g., salient objects in images and keywords in texts. However, these methods all assume well-matched training data, inconsistent with real-world data and thus degrading performance.\nSpecifically, due to lack of manual annotation, web-crawled datasets, e.g., Conceptual Captions [10] and M3W [11], inevitably contain noisy correspondences (NCs) [12]. This issue has been extensively explored in various domains, including person re-id [30], multi-view learning [31], [32], and graph matching [33], etc. In image-text matching, RCL [34] and DECL [35] propose novel losses, e.g., complementary contrastive learning and dynamic hardness, to suppress the amplification of wrong supervision inherent in NCs. However, they lack explicit strategies for easily separable noisy samples [36], leading to limited efficacy. To this issue, NCR [12] leverages the memorization effect of neural networks to identify NCs in a co-teaching manner. MSCN [14] employs meta-learning for enhanced similarity predictions. BiCro [15] explores bidirectional similarity consistency to derive more accurate adaptive margins. Moreover, CRCL [36] employs exponential normalization to aggregate historical similarity predictions for more stable suppression of NCs. Despite their advancements, these methods suffer from sub-optimal similarity predictions in entangled feature space. In contrast, we use cross-modal disentanglement for accurate similarity prediction in a modality-invariant space, effectively eliminating the adverse effects of modality-exclusive noise."}, {"title": "B. Cross-Modal Disentanglement", "content": "Prevailing methods typically employ objectives with intuitive or heuristic constraints to extract complementary modality-invariant and modality-exclusive information from data. Representatively, DMIM [37], CMG [22] and MDVAE [38] integrate cross prediction strategy, focusing on extracting identical modality-unified (invariant) information. Additionally, FDMER [21] introduces feature consistency constraints to ensure commonality across the modality-invariant representations. However, these methods cannot ensure the optimal modality-invariant representations, e.g., they may learn"}, {"title": "III. METHODOLOGY", "content": "We follow [12], [14] to use the image-text retrieval task as a proxy to investigate the noisy correspondence problem in cross-modal retrieval. Specifically, let $\\mathcal{D} = \\{(V_i, T_i)\\}_{i=1}^{N}$ denote a well-matched image-text dataset with N training samples. In noisy correspondence learning, an unknown proportion of supposed positive pairs in $\\mathcal{D}$ are mismatched, i.e., the image and text do not correspond, despite they belong to $\\mathcal{D}$ as in Figure 2. This misalignment introduces erroneous supervisory information, which can mislead the model during training and remarkably degrade performance."}, {"title": "B. Model Overview", "content": "This section outlines our DisNCL's detail, with an overview shown in Figure 2. We first use feature encoders to extract original representations from input pairs. Then we advance the information bottleneck theory by introducing a novel information-theoretic objective $\\mathcal{L}_{Dis}$ and $\\mathcal{L}_{reg}$ for extracting modality-invariant information (MII) and modality-exclusive information (MEI), respectively. Next, within modality-invariant subspace, we conduct similarity computation and retrieval training, where softened targets, instead of original hard matching labels, are estimated for each sample pair to facilitate more accurate cross-modal alignment. We will detail each component and its optimization objective in what follows."}, {"title": "C. Cross-Modal Information Disentanglement", "content": "Given a set of visual-textual pairs $\\{V_i, T_i\\}_{i=1}^{N}$, we first use modality-specific encoders $f$ and $g$ to encode input into a unified feature space, i.e., $F_V = f(V_i)$ and $F_T = g(T_i)$, respectively. Next, we aim to disentangle MII from MEI within multi-modal input pairs, thereby excluding the adverse effects of MEI on similarity predictions. Specifically, let $f_s(\\cdot)$, $f_x(\\cdot)$ be the disentangled encoders for visual, and $g_s(\\cdot)$, $g_x(\\cdot)$ for textual. We seek a disentangled representation space $F_V = (V_s, V_x)$ and $F_T = (T_s, T_x)$, where the MII is encoded as $V_s = f_s(F_V)$ and $T_s = g_s(F_T)$, while the MEI is captured by $V_x = f_x(F_V)$ and $T_x = g_x(F_T)$, respectively. Then, we employ the information theory to define the desired MII and MEI in Theorem 1 by quantifying the input information within corresponding representations [44]. Crucially, this formulation helps us identify the optimal information-theoretic constraints for MII and MEI, paving the way for our theoretical analyses of cross-modal disentanglement efficacy in Theorem 2.\nTheorem 1. Given a multi-modal input pair $(V, T)$ with corresponding representations $F_V$ and $F_T$, the mutual information $I(T; F_T)$ and $I(V; F_V)$ can be decomposed into two complementary terms, i.e.,\n$I(T; F_T) = I(T; F_V) + I(T; F_T|V)$,\n$I(V; F_V) = I(V; T_s) + I(V; F_V|T)$,\nRepresentatively, $I(T; F_V)$ is the MII term since it quantifies the consistent visual information in $F_V$ across $T$. In"}, {"title": "D. Sample Filtration and Robust Hinge Loss", "content": "As mentioned above, we train encoders with $\\mathcal{L}_{Dis}$ to obtain the disentangled cross-modal representations. Next, we predict similarities within modality-invariant subspace to mitigate MEI's adverse effects. Specifically, given a batch of multi-modal input pairs $(V_i, T_j)_{j=1}^{B}$ with batch size B, while $V_i^s = f_s(f(V_i))$ and $T_j^s = g_s(g(T_j))$ are modality-invariant representations. We employ a function $\\mathcal{H}$ to compute similarity for $(V_i, T_j)$ as $\\mathcal{H}(V_i^s, T_j^s)$, where $\\mathcal{H}$ can be a cosine function [5], [23] or learnable module [12], [14]. For brevity, $\\mathcal{H}(V_i^s, T_j^s)$ is denoted as $H_{ij}$ or $H(V_i, T_j)$ in the following. Subsequently, we follow [35] to leverage the maximum similarity constraints within the batch to identify potential noisy correspondences, i.e.,\n$\\mathcal{D}_{clean} = \\{i | i = \\arg \\max_{j}H_{ij} \\,\\, and\\, i = \\arg \\max_{j}H_{ji}\\}$,\nwhich filters out samples with off-diagonal argmax outputs, as potential matches in retrieval tasks are expected to appear along the diagonal. Then, a hinge-based ranking loss with an adaptive soft margin is employed for robust training, i.e.,\n$\\mathcal{L}_{align}(I_i, T_i) = \\sum_{i \\in \\mathcal{D}_{clean}} \\{ \\left[ \\alpha - \\mathcal{H}(I_i, T_i) + \\mathcal{H}(I_i, T_h) \\right]_+ + \\left[ \\hat{\\alpha}_i - \\mathcal{H}(I_i, T_i) + \\mathcal{H}(I_h, T_i) \\right]_+ \\}$,\nwhere $[x]_+ = max(x, 0)$. Following [14], [15], $\\hat{\\alpha}_i = \\min(\\frac{\\alpha}{\\max_{i}{mH_{ii}-1}}, \\alpha)$ is the soft margin for the i-th sample pair with a constant margin $\\alpha$ and hyperparameter $m$. $T_h$ and $I_h$ are the hard negative samples for text and image, respectively, e.g., $T_h$ is the negative text most similar to $I_i$ within the batch. Notably, Equations (2) and (3) are based on more accurate similarity predictions within modality-invariant subspace, paving the way for more effective identification and negative impact suppression of mismatched pairs, thereby greatly enhancing robustness in noisy correspondence learning."}, {"title": "E. Softened Cross-Modal Alignment", "content": "However, as shown in Figure 3, the hard negative strategy in Equation (3) is constrained to modeling the one-to-one correspondence between given query and its positive (green diagonal) or hard negative (red) samples. This strategy is sub-optimal, especially considering that the image-text relationship is not a strict one-to-one but rather a noisy many-to-many correspondence [48], [49]. Specifically, there can be local similarities among negative image-text pairs within a batch, e.g., video game in $I_1, I_2, I_3$ and $T_1, T_4$, indicating a nuanced, multi-faceted relationship. As a result, we employ soft targets for more accurately modeling such complex relationships."}, {"title": "F. Training Objective", "content": "To prevent sub-optimal solutions, we follow [41] to further employ a regularizer to encourage the disentanglement of modality-invariant and modality-exclusive representations, i.e.,\n$\\mathcal{L}_{reg} = I(V_s; V_x) + I(T_s; T_x)$.\nSubsequently, the final training objective is formulated as:\n$\\min\\, \\mathcal{L}_{DisNCL} = \\mathcal{L}_{Dis} + \\mathcal{L}_{align} + \\mathcal{L}_{soft} + \\mathcal{L}_{reg}$.\nNote that $\\mathcal{L}_{Dis}$ and $\\mathcal{L}_{reg}$ are intractable to directly optimize since their mutual information terms consist of integral on high-dimensional data. To this issue, we design tractable optimization strategy for stable optimization of these terms in what follows."}, {"title": "IV. DISNCL OPTIMIZATION", "content": "In this section, we derive variational estimations of $\\mathcal{L}_{Dis}$ ($\\mathcal{L}_{s}$ and $\\mathcal{L}_{x}$) and $\\mathcal{L}_{reg}$ for stable optimization of their mutual information objectives."}, {"title": "A. Estimation of Ls", "content": "1) Maximization of $I(T; V_s)$ and $I(V; T_s)$: We assume that all information in T is captured by its representation $F_T$, i.e., $I(T; V_s) = I(F_T; V_s)$, and directly use the Jensen-Shannon estimator [51] to maximize $I(F_T; V_s)$ as an alternative, formulated as:\n$\\max_{V_s, F_T, f_1} I_{JSD}(V_s; F_T) = E_{p(V_s)p(F_T)}[\\log (1 - f_1 (V_s, F_T))] + E_{p(V_s, F_T)}[\\log f_1 (V_s, F_T)]$,\nwhere $f_1$ is a learnable discriminator to discriminates whether $F_T$ and $V_s$ are correlated. The $I(V; T_s)$ can be optimized similarly.\n2) Minimization of $I(V; V_s|T)$ and $I(T; T_s|V)$: We derive a tra-ctable upper bound for minimizing $I(V; V_s|T)$ as:\n$I(V; V_s|T) < KL(p(V_s|V)||p(T_s|T))$,\nwhere $p(V_s|V)$ and $p(T_s|T)$ is the disentangled modality-invariant information extraction (see Appendix A-D for details). We assume these extractions follow a Gaussian distribution, e.g., $p(V_s|V) \\sim N(\\mu(V); I)$ with fixed variance and $\\mu = f_s \\circ f$ is the composition of $f_s$ and $f$. The $I(T; T_s|V)$ can be optimized in a similar way."}, {"title": "B. Estimation of Lx and Lreg", "content": "For simplicity of optimization, we first reformulate modality-exclusive objective $\\mathcal{L}_{x}$ (see Appendix A-B for details), i.e.,\n$\\mathcal{L}_{x} = - (I(V; V_x) + I(T; T_x)) + (1 + \\beta_2)(I(T; V_x) + I(V; T_x))$,\nwhere $I(V; V_x)$ and $I(T; T_x)$ can be maximized similarly as $I(T; V_s)$. Moreover, as $I(T; V_x) = I(F_T, V_x)$, we follow [41] to adopt an adversarial strategy to minimize $I(F_T, V_x)$ as an alternative, i.e.,\n$\\min_{V_x, F_T} \\max_{f_2} I (V_x; F_T) = E_{p(V_x)p(F_T)}[\\log(1 - f_2 (V_x, F_T))] + E_{p(V_x, F_T)}[\\log f_2 (V_x, F_T)]$,\nwhere $f_2$ is a discriminator to discriminates whether $F_T$ and $V_x$ are correlated. As shown by [52], $p(V_x, F_T) = p(V_x)p(F_T)$ when the Nash equilibrium is achieved, thus minimizing the $I(T; V_x)$. The $I(V; T_x)$ can be optimized similarly. Moreover, note that $I(V_s; V_x)$ and $I(T_s; T_x)$ in $\\mathcal{L}_{reg}$ can be estimated akin to $I(V_x, F_T)$."}, {"title": "V. THEORETICAL ANALYSES", "content": "In this section, we theoretically analyze DisNCL's disentanglement efficacy, pivotal for DisNCL to mitigate the impact of MEI. This theoretical exploration is structured into three facets: the completeness, disentanglement, and minimal sufficiency of learned representations. The minimal sufficiency is further divided into two separate components: the minimal sufficiency of modality-invariant information (MII) and modality-exclusive information (MEI).\nTheorem 2. Given a multi-modal input pair (V,T) with disentangled representations $F_V = (V_s, V_x)$ and $F_T = (T_s, T_x)$ achieved by optimizing $\\mathcal{L}_{Dis}$, we have:\n\u2022 Completeness: The optimal representations $F_V$ and $F_T$ corresponding to multi-modal input (V,T) are sufficient, i.e., $I(F_T, T) = H(T)$ and $I(F_V, V) = H(V)$.\n\u2022 Mutual Disentanglement: The optimal modality-invariant representations and modality-exclusive representations are disentangled, i.e., $I(V; V_x) = 0$ and $I(T; T_x) = 0$.\n\u2022 Minimal Sufficiency of $V_s$ and $T_s$: The optimal modality-invariant representations $V_s$ and $T_s$ are minimal sufficient, i.e., $I(V; V_s|T) = 0$ and $I(T; T_s|V) = 0$.\n\u2022 Minimal Sufficiency of $V_x$ and $T_x$: The optimal modality-exclusive representations $V_x$ and $T_x$ are minimal sufficient, i.e., $I(V; V_x) = H(V|T)$ and $I(T; T_x) = H(T|V)$.\nSpecifically, the completeness ensures that $F_V$ and $F_T$ contains all information of V and T losslessly, while the mutual disentanglement indicates that there is no overlap between $(V_s, V_x)$ and $(T_s, T_x)$. Moreover, the minimal sufficiency underscores the fidelity of learned representations, e.g., $I(V; V_s|T) = 0$ indicates that $V_s$ adeptly isolates MII, free from any modality-exclusive disturbances. Concurrently, $I(V; V_x) = H(V|T)$ indicates that $V_x$ encodes the sole MEI of V, devoid of any modality-invariant perturbations. Crucially, these theoretical constraints jointly formulate the desired modality-invariant and modality-specific subspace.\nDue to the completeness and minimal sufficiency, $V_s$ and $T_s$ captures all modality-invariant information, untainted by modality-exclusive elements. Simultaneously, due to mutual disentanglement and minimal sufficiency, $V_x$ and $T_x$ capture all modality-exclusive information of V and T without any modality-invariant noise."}, {"title": "B. Noise Robust", "content": "This section analyzes the noise robustness of our fine-grained cross-modal alignment strategy. Specifically, given overall noise rate $\\eta \\in [0, 1]$, we follow [34] to assume uniform distribution of label noise on $e_i$, $\\eta_{ij} = p(\\tilde{y} = j|y = i) = \\frac{\\eta}{C-1}$.\nTheorem 3. Let $[1, ..., C]$ be label set where C equals batch size B denoting the label set size, given uniform label noise with $\\eta < 1-\\frac{\\eta}{C-1}$, $\\mathcal{L}_{soft}$ is noise robust when $\\beta_3 = 1$."}, {"title": "VI. EXPERIMENTS", "content": "For evaluating our DisNCL, we first introduce details of datasets and implementations. Then we discuss the experimental results. Specifically, we aim to answer following questions:\n\u2022 RQ1: Whether DisNCL can achieve superior performance on varying noisy cross-modal retrieval benchmarks?\n\u2022 RQ2: Does DisNCL achieves cross-modal disentanglement?\n\u2022 RQ3: How does DisNCL's component facilitate disentanglement?"}, {"title": "A. Experiments Setting", "content": "1) Datasets: For thorough evaluations of our DisNCL, we follow [12], [35] to select three widely-used benchmarks, i.e.,\n\u2022 Flickr30K [58] comprises 31,783 images with five captions each from Flickr website. Following [12], we allocate 1K images for validation, another 1K for testing, and the rest for training.\n\u2022 MS-COCO [59] contains 123,287 images, where each image is associated with five captions. Similar to [12], we use 5K images for validation, another 5K for testing, and the rest for training.\n\u2022 CC152K [12] is a subset of Conceptual Captions [10], selected by [12], containing 152K image-text pairs crawled from the Internet. Due to the absence of manual annotation, CC152K naturally contains around 20% mismatched sample pairs, i.e., real-world noisy correspondences. Following [12], we use 150K images for training, 1K images for validation and another 1K for testing.\n2) Evaluation Metrics: Following [12], [14], [35], we evaluate DisNCL with the standard retrieval metric, R@K. R@K measures the proportion of queries for which the correct item is retrieved within the top K closest points to the query. We systematically report the corresponding results of R@1, R@5, and R@10 in both image-to-text and text-to-image retrieval scenarios. These metrics are further aggregated to evaluate the overall performance, denoted as R_sum.\n3) Implementation Details: As a versatile cross-modal disentanglement framework, DisNCL can be seamlessly integrated into various image-text retrieval methods for enhanced robustness against noisy correspondences during training. Here, we employ SGRAF [1] backbone with same settings as [12], [14], [35] for fair comparisons. Specifically, we conduct retrieval training and inference within the learned modality-invariant subspace. To begin with, we employ a 'warm-up' phase using $\\mathcal{L}_{align}$ with constant margin $\\alpha = 0.2$ for 5 epochs to achieve initial convergence. Then, the model is trained for 50 epochs using $\\mathcal{L}_{DisNCL}$ with an Adam optimizer, whose initial learning rate is 2e-4 and is reduced to 0.1x after 25 epochs. Following [14], [35], all experiments adopt shared hyperparameters: $\\Gamma = 0.05, m = 10$, a batch size of 128, a word embedding size of 300, and a unified feature space dimension of 1,024, etc. For cross-modal disentanglement, we set the dimensions of $V_s, V_x, T_s, T_x$ to 512. The hyperparameter $\\gamma$ is set to 0.5; $\\beta_1, \\beta_2$ are set to 0.1 while $\\beta_3$ is set to 0.5. The discriminators and disentangled encoders, e.g., $f_1, f_s$ and $g_s$, are implemented as three-layer multi-layer perceptrons with LeakyReLU activation ($\\alpha = 0.2$) and 256 hidden dimension."}, {"title": "B. Comparison with State-Of-The-Art (SOTA)", "content": "To answer RQ1, we compare DisNCL against current SOTA cross-modal retrieval methods to demonstrate its efficacy, including two main categories of baselines: traditional methods with well-matched data assumption, e.g., SCAN [8] and SAF [1]; and methods robust to noisy correspondence, including NCR [12], DECL [35], MSCN [14], BiCro [15], RCL [32], CRCL [36], SREM [60], ESC [61], GSC [62] and CREAM [63]. Moreover, as Flickr30K and MS-COCO are well-matched datasets annotated by human, we carry out experiments by generating the synthesized mismatched pairs. Specifically, we follow NCR [12] and DECL [35] to conduct experiments with two noise generation methods for comprehensive evaluation of our DisNCL. The detailed noise generation methods are summarized as follows:\n\u2022 DECL [35] and RCL [34] inject noisy correspondence in Flickr30K and MS-COCO datasets by randomly shuffling images for a specific percentage (noise ratio).\n\u2022 NCR [12] and MSCN [14] randomly select a specific percentage (noise ratio) of images and randomly permute all their corresponding captions."}, {"title": "C. Disentanglement Analysis", "content": "For RQ2, after training on 20% noise Flickr30K, we use MINE [64] estimator on clean Flickr30K test set to evaluate DisNCL's disentanglement efficacy. Figure 4 shows that DisNCL compresses mutual information between disentangled representations by around 90% in both modalities, confirming its disentanglement efficacy. Subsequent retrieval experiments with these disentangled representations further verified their fidelity. Specifically, entangled Ours achieves commendable retrieval performance with non-inter-modally shared (Vx,Tx), indicating a severe leakage of (Vs, Ts) into (Vx, Tx). In contrast, DisNCL effectively curtails this leakage, whose retrieval performance on (Vx,Tx) is akin to random guessing, corroborating DisNCL's disentanglement efficacy. Furthermore, a t-SNE visualization on learned representations qualitatively affirms disentanglement efficacy. Note that representations from entangled Ours are intermixed, confirming the aforementioned information leakage. Conversely, DisNCL's representations display clear boundaries, forming three distinct clusters corresponding to the modality-invariant, visual-exclusive, and textual-exclusive subspace. Collectively, these results highlight DisNCL's superior disentanglement efficacy, validating our theoretical analyses."}, {"title": "D. Ablation Study", "content": "1) Component Analysis: To answer RQ3, Table V shows ablation studies to evaluate the efficacy of DisNCL's component. Specifically, within $\\mathcal{L}_{DisNCL}$, we set aside the conventional retrieval constraint, $\\mathcal{L}_{align}$, and ablate the remainings: $\\mathcal{L}_{Dis}, \\mathcal{L}_{soft}$, and $\\mathcal{L}_{reg}$. Notably, DisNCL with only $\\mathcal{L}_{align}$ reduces to a conventional method with similarity-based alleviation strategy for noisy correspondences, severely suffering from misled similarity predictions in entangled feature space."}, {"title": "VII. CONCLUSION", "content": "This paper proposes DisNCL, a novel cross-modal disentanglement framework to tackle noisy correspondence prevalent in real-world multi-modal data. In detail, we introduce an information theoretic objective to naturally decompose multi-modal inputs into complementary modality-invariant and modality-exclusive components with certifiable optimal cross-modal disentanglement efficacy. Then, we predict similarities within the modality-invariant subspace to exclude adverse effects of modality-exclusive noise, paving the way for more effective identification and suppression of noisy correspondences. Moreover, we estimate soft targets to model the noisy many-to-many relationship inherent in multi-modal input, thereby facilitating more accurate cross-modal alignment. Extensive experiments on various benchmarks demonstrate DisNCL's superior efficacy and validate our theoretical analyses. We hope our DisNCL could provide insights for developing retrieval systems more suitable for real-world scenarios, and inspire future work on cross-modal disentanglement.\nBroader Impacts Our DisNCL can widely impact various applications that require robust multi-modal understanding and aligning, e.g., multimedia retrieval, image/video caption and recommendation systems etc. Specifically, addressing the noisy correspondence issue offers numerous benefits, e.g., significantly reducing the expensive manual data annotation cost; effectively leveraging Internet data despite potential mismatched ones; enhancing multi-modal systems more suitable for noisy real-world scenarios, etc."}]}