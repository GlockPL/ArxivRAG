{"title": "Disentangled Noisy Correspondence Learning", "authors": ["Zhuohang Dang", "Minnan Luo", "Jihong Wang", "Chengyou Jia", "Haochen Han", "Herun Wan", "Guang Dai", "Xiaojun Chang", "Jingdong Wang"], "abstract": "Cross-modal retrieval is crucial in understanding latent correspondences across modalities. However, existing methods implicitly assume well-matched training data, which is impractical as real-world data inevitably involves imperfect alignments, i.e., noisy correspondences. Although some works explore similarity-based strategies to address such noise, they suffer from sub-optimal similarity predictions influenced by modality-exclusive information (MEI), e.g., background noise in images and abstract definitions in texts. This issue arises as MEI is not shared across modalities, thus aligning it in training can markedly mislead similarity predictions. Moreover, although intuitive, directly applying previous cross-modal disentanglement methods suffers from limited noise tolerance and disentanglement efficacy. Inspired by the robustness of information bottlenecks against noise, we introduce DisNCL, a novel information-theoretic framework for feature Disentanglement in Noisy Correspondence Learning, to adaptively balance the extraction of MII and MEI with certifiable optimal cross-modal disentanglement efficacy. DisNCL then enhances similarity predictions in modality-invariant subspace, thereby greatly boosting similarity-based alleviation strategy for noisy correspondences. Furthermore, DisNCL introduces soft matching targets to model noisy many-to-many relationships inherent in multi-modal input for noise-robust and accurate cross-modal alignment. Extensive experiments confirm DisNCL's efficacy by 2% average recall improvement. Mutual information estimation and visualization results show that DisNCL learns meaningful MII/MEI subspaces, validating our theoretical analyses.", "sections": [{"title": "I. INTRODUCTION", "content": "Cross-modal retrieval [1]\u2013[3] aims to retrieve the most relevant samples from different modalities, crucial for various domains such as criminal investigation [4]. Existing methods typically first project multi-modal inputs to a unified feature space. They then devise feature interaction strategies at either global [5]\u2013[7] or local [1], [8], [9] levels for similarity prediction, enhancing similarities for matching pairs while suppressing similarities of mismatched ones. Although effective, a core assumption of these methods is well-matched train data, which is inconsistent with real-world data. For example, Conceptual Captions [10] collects 3.3 million co-occurring sample pairs from the Internet, where around 20% samples are mismatched [11], [12], i.e., noisy correspondences. Tables I and IV demonstrate that the traditional training paradigm, whether training from scratch or fine-tuning pretrained models like CLIP [13], significantly underperforms on such noisy data, necessitating robust learning strategies for noisy correspondences.\nIn this context, recent works introduce noise during training to bridge the gap between well-matched datasets and noisy real-world data for enhanced robustness. Typically, these methods [12], [14], [15] employ similarity-based strategies for identification and suppression of these mismatched pairs. Specifically, they first employ the memorization effect of deep neural networks [16], [17], which enables clean samples to exhibit higher similarities than noisy ones after the initial few epochs [18]. Then they use samples\u2019 similarity distribution, showing a clear bimodal distribution, to coarsely identify and filter out noise data. Subsequently, they derive a per-sample adaptive margin from its similarity prediction to replace the constant scalar margin in triplet ranking loss [19], [20]. This adaptive margin facilitates potential noisy correspondences\u2019 compliance with the triplet ranking loss, effectively excluding the risky supervision information during training.\nHowever, previous methods suffer from two significant challenges. 1) Feature Entanglement: Figure 1a shows these methods predict similarities in a unified feature space with modality-invariant information (MII) and modality-exclusive information (MEI) entangled. Specifically, MII captures shared components across modalities, e.g., 'the running back', serving as foundation for retrieval. Conversely, MEI is not shared across modalities, e.g., background noise, watermark in image and abstract descriptions such as 'legally deaf' and 'history' in text. Consequently, aligning MEI across modalities can erroneously skew similarity predictions towards sub-optimality, misleading similarity-based identification and suppression of noisy correspondences. Although some works introduce various strategies, e.g., feature consistency [21] and cross prediction [22], to disentangle MII from MEI, they fall short in noise tolerance and disentanglement efficacy. Specifically, for optimal MII extraction, these methods intuitively minimize the distance between MII or exchange MII to reconstruct input from another modality, which are sensitive to incorrect alignment information from noisy correspondences and may converge trivially to a subset of optimal MII while leaking the remainder to MEI. 2) Exclusive Correspondence: the hard negative strategy [23] in previous methods [14], [15] can only model the one-to-one correspondence between given query and its positive or hard negative samples (Figure 3). This strategy is sub-optimal as it fails to consider the noisy many-to-many correspondences inherent in multi-modal data [24], [25], e.g., shared entities among unpaired images and texts.\nIn light of above, we introduce DisNCL, a pioneering information-theoretic framework for feature Disentanglement in Noisy Correspondence Learning, to enhance models\u2019 robustness against mismatched pairs in training. Specifically, we introduce a novel objective based on information bottleneck principle [26] to extract two complementary components, i.e., MII and MEI. Subsequently, as illustrated in Figure 1b, we conduct similarity predictions within the disentangled modality-invariant subspace, together with the cross-modal retrieval training. This methodology aims to direct the model\u2019s focus solely on MII and exclude the detrimental impacts of MEI, boosting identification and negative impact suppression of mismatched samples with more accurate similarity predictions. Furthermore, we estimate softened targets with bootstrapping strategy in the modality-invariant subspace for sample matching. These softened targets, contrasting with one-to-one correspondence in hard negative strategy, enable more complex many-to-many cross-modal relationship modeling to further enhance model\u2019s efficacy. Moreover, it is noteworthy that we theoretically prove the optimal disentanglement of MII and MEI, along with the noise robustness of our soft cross-modal alignment, confirming our DisNCL\u2019s superior efficacy.\nOur contribution is summarized as follows:\n\u2022 We introduce DisNCL, the first work to introduce certifiable optimal cross-modal disentanglement efficacy for noisy correspondence learning, for enhanced robustness against noisy multi-modal training data.\n\u2022 We boost identification and suppression of noisy correspondences by accurate similarities predicted in modality-invariant subspace. Moreover, we propose a noise-robust alignment refinement strategy to model the intricate relationships in multi-modal data via soft target estimation.\n\u2022 Extensive experiments on various benchmarks confirm DisNCL\u2019s efficacy by improving 2% average recall, as well as validate our theoretical analyses.\n\u2022"}, {"title": "II. RELATED WORK", "content": "Contemporary cross-modal retrieval methods primarily diverge into global-level methods [5], [13], [27] that treat images and sentences as holistic entities and local-level [1], [8], [28], [29] ones focusing on essential fragments e.g., salient objects in images and keywords in texts. However, these methods all assume well-matched training data, inconsistent with real-world data and thus degrading performance.\nSpecifically, due to lack of manual annotation, web-crawled datasets, e.g., Conceptual Captions [10] and M3W [11], inevitably contain noisy correspondences (NCs) [12]. This issue has been extensively explored in various domains, including person re-id [30], multi-view learning [31], [32], and graph matching [33], etc. In image-text matching, RCL [34] and DECL [35] propose novel losses, e.g., complementary contrastive learning and dynamic hardness, to suppress the amplification of wrong supervision inherent in NCs. However, they lack explicit strategies for easily separable noisy samples [36], leading to limited efficacy. To this issue, NCR [12] leverages the memorization effect of neural networks to identify NCs in a co-teaching manner. MSCN [14] employs meta-learning for enhanced similarity predictions. BiCro [15] explores bidirectional similarity consistency to derive more accurate adaptive margins. Moreover, CRCL [36] employs exponential normalization to aggregate historical similarity predictions for more stable suppression of NCs. Despite their advancements, these methods suffer from sub-optimal similarity predictions in entangled feature space. In contrast, we use cross-modal disentanglement for accurate similarity prediction in a modality-invariant space, effectively eliminating the adverse effects of modality-exclusive noise."}, {"title": "B. Cross-Modal Disentanglement", "content": "Prevailing methods typically employ objectives with intuitive or heuristic constraints to extract complementary modality-invariant and modality-exclusive information from data. Representatively, DMIM [37], CMG [22] and MDVAE [38] integrate cross prediction strategy, focusing on extracting identical modality-unified (invariant) information. Additionally, FDMER [21] introduces feature consistency constraints to ensure commonality across the modality-invariant representations. However, these methods cannot ensure the optimal modality-invariant representations, e.g., they may learn only a subset of the optimal modality-invariant information while leaking the remainder to modality-exclusive information. Moreover, these strategies to directly force the feature invariance are sensitive to wrong alignment of noisy correspondences in data. On the other hand, information regularization [39], [40] methods employ extra regularizers to preserve modality-invariant information while discarding modality-exclusive noise, showcasing improved noise tolerance. However, it still fails to ensure optimal disentanglement efficacy due to inevitable performance-compression trade-off of trivial regularizers [41]\u2013[43], i.e., the performance decreases as the level of compression intensifies. In contrast, with a rigorous definition of modality-exclusive and modality-invariant information, our DisNCL demonstrates certifiable optimal cross-modal disentanglement efficacy and appealing noise tolerance highlighted in Theorems 2 and 3."}, {"title": "III. METHODOLOGY", "content": null}, {"title": "A. Problem Definition", "content": "We follow [12], [14] to use the image-text retrieval task as a proxy to investigate the noisy correspondence problem in cross-modal retrieval. Specifically, let $\\mathcal{D} = \\{(\\textbf{V}_i, \\textbf{T}_i)\\}_{i=1}^N$ denote a well-matched image-text dataset with $N$ training samples. In noisy correspondence learning, an unknown proportion of supposed positive pairs in $\\mathcal{D}$ are mismatched, i.e., the image and text do not correspond, despite they belong to $\\mathcal{D}$ as in Figure 2. This misalignment introduces erroneous supervisory information, which can mislead the model during training and remarkably degrade performance."}, {"title": "B. Model Overview", "content": "This section outlines our DisNCL's detail, with an overview shown in Figure 2. We first use feature encoders to extract original representations from input pairs. Then we advance the information bottleneck theory by introducing a novel information-theoretic objective $\\mathcal{L}_{\\text{Dis}}$ and $\\mathcal{L}_{\\text{reg}}$ for extracting modality-invariant information (MII) and modality-exclusive information (MEI), respectively. Next, within modality-invariant subspace, we conduct similarity computation and retrieval training, where softened targets, instead of original hard matching labels, are estimated for each sample pair to facilitate more accurate cross-modal alignment. We will detail each component and its optimization objective in what follows."}, {"title": "C. Cross-Modal Information Disentanglement", "content": "Given a set of visual-textual pairs $\\{\\textbf{V}_i, \\textbf{T}_i\\}_{i=1}^N$, we first use modality-specific encoders $f$ and $g$ to encode input into a unified feature space, i.e., $\\textbf{F}_v = f(\\textbf{V}_i)$ and $\\textbf{F}_t = g(\\textbf{T}_i)$, respectively. Next, we aim to disentangle MII from MEI within multi-modal input pairs, thereby excluding the adverse effects of MEI on similarity predictions. Specifically, let $f_s(\\cdot)$, $f_x(\\cdot)$ be the disentangled encoders for visual, and $g_s(\\cdot)$, $g_x(\\cdot)$ for textual. We seek a disentangled representation space $\\textbf{F}_v = (\\textbf{V}_s, \\textbf{V}_x)$ and $\\textbf{F}_t = (\\textbf{T}_s, \\textbf{T}_x)$, where the MII is encoded as $\\textbf{V}_s = f_s(\\textbf{F}_v)$ and $\\textbf{T}_s = g_s(\\textbf{F}_t)$, while the MEI is captured by $\\textbf{V}_x = f_x(\\textbf{F}_v)$ and $\\textbf{T}_x = g_x(\\textbf{F}_t)$, respectively. Then, we employ the information theory to define the desired MII and MEI in Theorem 1 by quantifying the input information within corresponding representations [44]. Crucially, this formulation helps us identify the optimal information-theoretic constraints for MII and MEI, paving the way for our theoretical analyses of cross-modal disentanglement efficacy in Theorem 2.\nTheorem 1. Given a multi-modal input pair $(\\textbf{V},\\textbf{T})$ with corresponding representations $\\textbf{F}_v$ and $\\textbf{F}_t$, the mutual information $I(\\textbf{T}; \\textbf{F}_t)$ and $I(\\textbf{V}; \\textbf{F}_v)$ can be decomposed into two complementary terms, i.e.,\n$I(\\textbf{T}; \\textbf{F}_t) = I(\\textbf{V}; \\textbf{F}_v) + I(\\textbf{T}; \\textbf{F}_t|\\textbf{V}),$ and $I(\\textbf{V}; \\textbf{F}_v) = I(\\textbf{T}; \\textbf{F}_v) + I(\\textbf{V}; \\textbf{F}_v|\\textbf{T}).$\nRepresentatively, $I(\\textbf{T}; \\textbf{F}_v)$ is the MII term since it quantifies the consistent visual information in $\\textbf{F}_v$ across $\\textbf{T}$. In"}, {"title": "Definition 1", "content": "For a multi-modal input pair $(\\textbf{V},\\textbf{T})$ with feature $(\\textbf{F}_v, \\textbf{F}_t)$, the desired disentangled representations $\\textbf{F}_v = (\\textbf{V}_s, \\textbf{V}_x), \\textbf{F}_t = (\\textbf{T}_s, \\textbf{T}_x)$ can be achieved by solving optimization objective:\n$\\min \\mathcal{L}_{\\text{Dis}} = \\gamma \\mathcal{L}_s + (1 - \\gamma) \\mathcal{L}_x,$\nwhere $\\gamma$ is a hyperparameter that controls the tradeoff between two objectives; $\\mathcal{L}_s$ and $\\mathcal{L}_x$ denote the modality-invariant objective and modality-exclusive objective, i.e.:\n\u2022 $\\mathcal{L}_s$: DisNCL learns MII by maximizing modality-invariant term while minimizing modality-exclusive term, i.e., $\\mathcal{L}_s = -(I(\\textbf{T};\\textbf{V}_s) + I(\\textbf{V};\\textbf{T}_s)) + \\beta_1(I(\\textbf{V}; \\textbf{V}_s|\\textbf{T}) + I(\\textbf{T}; \\textbf{T}_s|\\textbf{V})).$\n\u2022 $\\mathcal{L}_x$: DisNCL learns MEI by maximizing modality-exclusive term while minimizing modality-invariant term, i.e., $\\mathcal{L}_x = -(I(\\textbf{V}; \\textbf{V}_x|\\textbf{T}) + I(\\textbf{T}; \\textbf{T}_x|\\textbf{V})) + \\beta_2(I(\\textbf{T}; \\textbf{V}_x) + I(\\textbf{V}; \\textbf{T}_x)).$\nHere $\\beta_1$ and $\\beta_2$ are two pre-defined hyperparameters. As in Appendix A-B, $\\mathcal{L}_s$ and $\\mathcal{L}_x$ are equivalent to information bottlenecks (IBs) [26], preserving desirable information while discarding irrelevant information in representations. In this sense, $\\mathcal{L}_{\\text{Dis}}$ is totally information theoretic based, complying with the desired information extraction and preservation in cross-modal disentanglement. Moreover, its information bottleneck based objective formulation can maintain high noise tolerance for enhanced model robustness [45], [46]."}, {"title": "D. Sample Filtration and Robust Hinge Loss", "content": "As mentioned above, we train encoders with $\\mathcal{L}_{\\text{Dis}}$ to obtain the disentangled cross-modal representations. Next, we predict similarities within modality-invariant subspace to mitigate MEI\u2019s adverse effects. Specifically, given a batch of multi-modal input pairs $(\\textbf{V}_i, \\textbf{T}_j)_{j=1}^B$ with batch size $B$, while $\\textbf{V}_i^s = f_s(f(\\textbf{V}_i))$ and $\\textbf{T}_j^s = g_s(g(\\textbf{T}_j))$ are modality-invariant representations. We employ a function $H$ to compute similarity for $(\\textbf{V}_i, \\textbf{T}_j)$ as $H(\\textbf{V}_i^s, \\textbf{T}_j^s)$, where $H$ can be a cosine function [5], [23] or learnable module [12], [14]. For brevity, $H(\\textbf{V}_i^s, \\textbf{T}_j^s)$ is denoted as $H_{ij}$ or $H(\\textbf{V}_i, \\textbf{T}_j)$ in the following. Subsequently, we follow [35] to leverage the maximum similarity constraints within the batch to identify potential noisy correspondences, i.e.,\n$\\mathcal{D}_{\\text{clean}} = \\{i | i = \\arg \\max_j H_{ij} \\text{ and } i = \\arg \\max_j H_{ji}\\},$ which filters out samples with off-diagonal argmax outputs, as potential matches in retrieval tasks are expected to appear along the diagonal. Then, a hinge-based ranking loss with an adaptive soft margin is employed for robust training, i.e.,\n$\\begin{aligned} \\mathcal{L}_{\\text{align}} (I_i, T_i) = \\sum_{i \\in \\mathcal{D}_{\\text{clean}}} &\\left\\{\\alpha - H(I_i, T_i) + H(I_i, T_h)\\right\\}_+ \\\\ &+ \\left\\{\\alpha_i - H(I_i, T_i) + H(I_h, T_i)\\right\\}_+, \\end{aligned}$\nwhere $[x]_+ = \\max(x, 0)$. Following [14], [15], $\\hat{\\alpha}_i = \\frac{m}{B} \\sum_{j=1}^B a_{ij}^{-1}$ is the soft margin for the $i$-th sample pair with a constant margin $a$ and hyperparameter $m$. $T_h$ and $I_h$ are the hard negative samples for text and image, respectively, e.g., $T_h$ is the negative text most similar to $I_i$ within the batch. Notably, Equations (2) and (3) are based on more accurate similarity predictions within modality-invariant subspace, paving the way for more effective identification and negative impact suppression of mismatched pairs, thereby greatly enhancing robustness in noisy correspondence learning."}, {"title": "E. Softened Cross-Modal Alignment", "content": "However, as shown in Figure 3, the hard negative strategy in Equation (3) is constrained to modeling the one-to-one correspondence between given query and its positive (green diagonal) or hard negative (red) samples. This strategy is sub-optimal, especially considering that the image-text relationship is not a strict one-to-one but rather a noisy many-to-many correspondence [48], [49]. Specifically, there can be local similarities among negative image-text pairs within a batch, e.g., video game in $I_1, I_2, I_3$ and $T_1, T_4$, indicating a nuanced, multi-faceted relationship. As a result, we employ soft targets for more accurately modeling such complex relationships."}, {"title": "F. Training Objective", "content": "To prevent sub-optimal solutions, we follow [41] to further employ a regularizer to encourage the disentanglement of modality-invariant and modality-exclusive representations, i.e.,\n$\\mathcal{L}_{\\text{reg}} = I(\\textbf{V}_s; \\textbf{V}_x) + I(\\textbf{T}_s; \\textbf{T}_x).$\nSubsequently, the final training objective is formulated as:\n$\\min \\mathcal{L}_{\\text{DisNCL}} = \\mathcal{L}_{\\text{Dis}} + \\mathcal{L}_{\\text{align}} + \\mathcal{L}_{\\text{soft}} + \\mathcal{L}_{\\text{reg}}.$"}, {"title": "IV. DISNCL OPTIMIZATION", "content": "In this section, we derive variational estimations of $\\mathcal{L}_{\\text{Dis}}$ ($\\mathcal{L}_s$ and $\\mathcal{L}_x$) and $\\mathcal{L}_{\\text{reg}}$ for stable optimization of their mutual information objectives."}, {"title": "A. Estimation of \\(\\mathcal{L}\\_s\\)", "content": "1) Maximization of $I(\\textbf{T};\\textbf{V}_s)$ and $I(\\textbf{V};\\textbf{T}_s)$: We assume that all information in $\\textbf{T}$ is captured by its representation $\\textbf{F}_t$, i.e., $I(\\textbf{T};\\textbf{V}_s) = I(\\textbf{F}_t, \\textbf{V}_s)$, and directly use the Jensen-Shannon estimator [51] to maximize $I(\\textbf{F}_t; \\textbf{V}_s)$ as an alternative, formulated as:\n$\\begin{aligned} \\max_{\\textbf{V}_s, \\textbf{F}_t, f_1} I_{JSD} (\\textbf{V}_s; \\textbf{F}_t) = &\\mathbb{E}_{p(\\textbf{V}_s)p(\\textbf{F}_t)} [\\log (1 \u2013 f_1 (\\textbf{V}_s, \\textbf{F}_t))] \\\\ &+ \\mathbb{E}_{p(\\textbf{V}_s, \\textbf{F}_t)} [\\log f_1 (\\textbf{V}_s, \\textbf{F}_t)], \\end{aligned}$\nwhere $f_1$ is a learnable discriminator to discriminates whether $\\textbf{F}_t$ and $\\textbf{V}_s$ are correlated. The $I(\\textbf{V};\\textbf{T}_s)$ can be optimized similarly.\n2) Minimization of $I(\\textbf{V}; \\textbf{V}_s|\\textbf{T})$ and $I(\\textbf{T};\\textbf{T}_s|\\textbf{V})$: We derive a tra-ctable upper bound for minimizing $I(\\textbf{V}; \\textbf{V}_s|\\textbf{T})$ as:\n$I(\\textbf{V}; \\textbf{V}_s|\\textbf{T}) < KL(p(\\textbf{V}_s|\\textbf{V})||p(\\textbf{T}_s|\\textbf{T})),$ where $p(\\textbf{V}_s|\\textbf{V})$ and $p(\\textbf{T}_s|\\textbf{T})$ is the disentangled modality-invariant information extraction (see Appendix A-D for details). We assume these extractions follow a Gaussian distribution, e.g., $p(\\textbf{V}_s|\\textbf{V}) \\sim \\mathcal{N}(\\mu(\\textbf{V});I)$ with fixed variance and $\\mu = f_s \\circ f$ is the composition of $f_s$ and $f$. The $I(\\textbf{T};\\textbf{T}_s|\\textbf{V})$ can be optimized in a similar way."}, {"title": "B. Estimation of \\(\\mathcal{L}\\_x\\) and \\(\\mathcal{L}\\_{\\text{reg}}\\)", "content": "For simplicity of optimization, we first reformulate modality-exclusive objective $\\mathcal{L}_x$ (see Appendix A-B for details), i.e.,\n$\\begin{aligned} \\mathcal{L}_x = &\u2013 (I(\\textbf{V}; \\textbf{V}_x) + I(\\textbf{T}; \\textbf{T}_x)) \\\\ &+ (1 + \\beta_2)(I(\\textbf{T}; \\textbf{V}_x) + I(\\textbf{V}; \\textbf{T}_x)), \\end{aligned}$\nwhere $I(\\textbf{V}; \\textbf{V}_x)$ and $I(\\textbf{T}; \\textbf{T}_x)$ can be maximized similarly as $I(\\textbf{T};\\textbf{V}_s)$. Moreover, as $I(\\textbf{T}; \\textbf{V}_x) = I(\\textbf{F}_t, \\textbf{V}_x)$, we follow [41] to adopt an adversarial strategy to minimize $I(\\textbf{F}_t, \\textbf{V}_x)$ as an alternative, i.e.,\n$\\begin{aligned} \\min_{\\textbf{V}_x,\\textbf{F}_t} \\max_{f_2} I (\\textbf{V}_x; \\textbf{F}_t) = &\\mathbb{E}_{p(\\textbf{V}_x)p(\\textbf{F}_t)} [\\log(1 \u2013 f_2 (\\textbf{V}_x, \\textbf{F}_t))] \\\\ &+ \\mathbb{E}_{p(\\textbf{V}_x, \\textbf{F}_t)} [\\log f_2 (\\textbf{V}_x, \\textbf{F}_t)], \\end{aligned}$\nwhere $f_2$ is a discriminator to discriminates whether $\\textbf{F}_t$ and $\\textbf{V}_x$ are correlated. As shown by [52], $p(\\textbf{V}_x, \\textbf{F}_t) = p(\\textbf{V}_x)p(\\textbf{F}_t)$ when the Nash equilibrium is achieved, thus minimizing the $I(\\textbf{T}; \\textbf{V}_x)$. The $I(\\textbf{V}; \\textbf{T}_x)$ can be optimized similarly. Moreover, note that $I(\\textbf{V}_s; \\textbf{V}_x)$ and $I(\\textbf{T}_s; \\textbf{T}_x)$ in $\\mathcal{L}_{\\text{reg}}$ can be estimated akin to $I(\\textbf{V}_x, \\textbf{F}_t)$."}, {"title": "V. THEORETICAL ANALYSES", "content": null}, {"title": "A. Cross-Modal Disentanglement", "content": "In this section, we theoretically analyze DisNCL\u2019s disentanglement efficacy, pivotal for DisNCL to mitigate the impact of MEI. This theoretical exploration is structured into three facets: the completeness, disentanglement, and minimal sufficiency of learned representations. The minimal sufficiency is further divided into two separate components: the minimal sufficiency of modality-invariant information (MII) and modality-exclusive information (MEI)."}, {"title": "Theorem 2", "content": "Given a multi-modal input pair $(\\textbf{V},\\textbf{T})$ with disentangled representations $\\textbf{F}_v = (\\textbf{V}_s, \\textbf{V}_x)$ and $\\textbf{F}_t = (\\textbf{T}_s, \\textbf{T}_x)$ achieved by optimizing $\\mathcal{L}_{\\text{Dis}}$, we have:\n\u2022 Completeness: The optimal representations $\\textbf{F}_v$ and $\\textbf{F}_t$ corresponding to multi-modal input $(\\textbf{V},\\textbf{T})$ are sufficient, i.e., $I(\\textbf{F}_t,\\textbf{T}) = H(\\textbf{T})$ and $I(\\textbf{F}_v,\\textbf{V}) = H(\\textbf{V})$.\n\u2022 Mutual Disentanglement: The optimal modality-invariant representations and modality-exclusive representations are disentangled, i.e., $I(\\textbf{V}; \\textbf{V}_x) = 0$ and $I(\\textbf{T}; \\textbf{T}_x) = 0$.\n\u2022 Minimal Sufficiency of $\\textbf{V}\\_s$ and $\\textbf{T}\\_s$: The optimal modality-invariant representations $\\textbf{V}\\_s$ and $\\textbf{T}\\_s$ are minimal sufficient, i.e., $I(\\textbf{V}; \\textbf{V}\\_s|\\textbf{T}) = 0$ and $I(\\textbf{T}; \\textbf{T}\\_s|\\textbf{V}) = 0$.\n\u2022 Minimal Sufficiency of $\\textbf{V}\\_x$ and $\\textbf{T}\\_x$: The optimal modality-exclusive representations $\\textbf{V}\\_x$ and $\\textbf{T}\\_x$ are minimal sufficient, i.e., $I(\\textbf{V}; \\textbf{V}\\_x) = H(\\textbf{V}|\\textbf{T})$ and $I(\\textbf{T}; \\textbf{T}\\_x) = H(\\textbf{T}|\\textbf{V})$."}, {"title": "B. Noise Robust", "content": "This section analyzes the noise robustness of our fine-grained cross-modal alignment strategy. Specifically, given overall noise rate $\\eta \\in [0, 1]$, we follow [34] to assume uniform distribution of label noise on $e_i$, i.e., $\\eta_{ij} = p(\\tilde{y} = j|y = i) = \\frac{\\eta}{C - 1}$. In this sense, $\\mathcal{L}_{\\text{soft}}$ is noise-robust when $\\beta_3 = 1$, i.e.\nTheorem 3. Let $[1, ..., C]$ be label set where $C$ equals batch size B denoting the label set size, given uniform label noise with $\\eta < 1-\\frac{1}{C-1}$, $\\mathcal{L}_{\\text{soft}}$ is noise robust when $\\beta_3 = 1$."}, {"title": "VI. EXPERIMENTS", "content": "For evaluating our DisNCL, we first introduce details of datasets and implementations. Then we discuss the experimental results. Specifically, we aim to answer following questions:\n\u2022 RQ1: Whether DisNCL can achieve superior performance on varying noisy cross-modal retrieval benchmarks?\n\u2022 RQ2: Does DisNCL achieves cross-modal disentanglement?\n\u2022 RQ3: How does DisNCL\u2019s component facilitate disentanglement?"}, {"title": "A. Experiments Setting", "content": "1) Datasets: For thorough evaluations of our DisNCL, we follow [12], [35] to select three widely-used benchmarks, i.e.,\n\u2022 Flickr30K [58] comprises 31,783 images with five captions each from Flickr website. Following [12], we allocate 1K images for validation, another 1K for testing, and the rest for training.\n\u2022 MS-COCO [59] contains 123,287 images, where each image is associated with five captions. Similar to [12], we use 5K images for validation, another 5K for testing, and the rest for training.\n\u2022 CC152K [12] is a subset of Conceptual Captions [10], selected by [12], containing 152K image-text pairs crawled from the Internet. Due to the absence of manual annotation, CC152K naturally contains around 20% mismatched sample pairs, i.e., real-world noisy correspondences. Following [12], we use 150K images for training, 1K images for validation and another 1K for testing.\n2) Evaluation Metrics: Following [12], [14], [35], we evaluate DisNCL with the standard retrieval metric, R@K. R@K measures the proportion of queries for which the correct item is retrieved within the top K closest points to the query. We systematically report the corresponding results of R@1, R@5, and R@10 in both image-to-text and text-to-image retrieval scenarios. These metrics are further aggregated to evaluate the overall performance, denoted as R_sum.\n3) Implementation Details: As a versatile cross-modal disentanglement framework, DisNCL can be seamlessly integrated into various image-text retrieval methods for enhanced robustness against noisy correspondences during training. Here, we employ SGRAF [1] backbone with same settings as [12], [14], [35] for fair comparisons. Specifically, we conduct retrieval training and inference within the learned modality-invariant subspace. To begin with, we employ a 'warm-up' phase using $\\mathcal{L}_{\\text{align}}$ with constant margin $a = 0.2$ for 5 epochs to achieve initial convergence. Then, the model is trained for 50 epochs using $\\mathcal{L}_{\\text{DisNCL}}$ with an Adam optimizer, whose initial learning rate is 2e-4 and is reduced to 0.1x after 25 epochs. Following [14], [35], all experiments adopt shared hyperparameters: $\\Gamma = 0.05$, $m = 10$, a batch size of 128, a word embedding size of 300, and a unified feature space dimension of 1,024, etc. For cross-modal disentanglement, we set the dimensions of $\\textbf{V}_s, \\textbf{V}_x, \\textbf{T}_s, \\textbf{T}_x$ to 512. The hyperparameter $\\gamma$ is set to 0.5; $\\beta_1, \\beta_2$ are set to 0.1 while $\\beta_3$ is set to 0.5. The discriminators and disentangled encoders, e.g., $f_1, f_s$ and $g_s$, are implemented as three-layer multi-layer perceptrons with LeakyReLU activation ($\\alpha = 0.2$) and 256 hidden dimension."}, {"title": "B. Comparison with State-Of-The-Art (SOTA)", "content": "To answer RQ1", "baselines": "traditional methods with well-matched data assumption", "8": "and SAF [1", "12": "DECL [35", "14": "BiCro [15", "32": "CRCL [36", "60": "ESC [61", "62": "and CREAM [63"}, {"12": "and DECL [35", "follows": "n\u2022 DECL"}]}