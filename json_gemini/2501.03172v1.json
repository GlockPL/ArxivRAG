{"title": "GLIREL - Generalist Model for Zero-Shot Relation Extraction", "authors": ["Jack Boylan", "Chris Hokamp", "Demian Gholipour Ghalandari"], "abstract": "We introduce GLiREL (Generalist Lightweight model for zero-shot Relation Extraction), an efficient architecture and training paradigm for zero-shot relation classification. Inspired by recent advancements in zero-shot named entity recognition, this work presents an approach to efficiently and accurately predict zero-shot relationship labels between multiple entities in a single forward pass. Experiments using the FewRel and WikiZSL benchmarks demonstrate that our approach achieves state-of-the-art results on the zero-shot relation classification task. In addition, we contribute a protocol for synthetically-generating datasets with diverse relation labels.", "sections": [{"title": "1 Introduction", "content": "Recent advances in zero-shot NLP models for entity recognition have been enabled by large-scale synthetic training data generation using state-of-the-art (SoTA) Large Language Models (LLMs) (Zhou et al., 2024). An ongoing line of work achieves drastic improvements in accuracy and usability over previous approaches by using efficient architectures targeted at various NLP tasks (Bogdanov et al., 2024; Stepanov and Shtopko, 2024; Zaratiana et al., 2023). Zero-shot named entity recognition (NER) models such as GLiNER (Zaratiana et al., 2023) do not operate on a fixed label set, only requiring textual labels to be specified at inference time, and can directly perform span classification using labels that are not observed during training.\nIn contrast to generative models, targeted architectures for zero-shot span classification jointly predict all labels simultaneously, making them much more efficient than auto-regressive models (Zaratiana et al., 2023). Existing SoTA zero-shot relation classification\u00b9 (ZSRC) models achieve strong performance, but are inefficient because every entity pair and candidate label combination is treated as a separate input. Existing methods do not scale to real-world use cases, where a large number of entity pairs is extracted from text, each of which must be classified against many candidate labels. GLIREL takes inspiration from recent successes in zero-shot NER and text classification, adapting these approaches to enable ZSRC that is both efficient and accurate.\nWhile SOTA LLMS excel at information extraction (IE) tasks (Li et al., 2024a; Zhou et al., 2024), there are major limitations to their scale and deployment patterns, including:\n\u2022 Auto-regressive decoding is unable to take advantage of task-specific parallelism,\n\u2022 Specific, expensive hardware requirements,\n\u2022 Output is not sufficiently constrained unless guided by heuristic decoding methods,\n\u2022 Unpredictable behavior, for example when asked to identify all relationships between entities in a document of arbitrary length.\nThe ability of LLMs to perform zero-shot inference with unconstrained output makes them very flexible, but for many tasks, their auto-regressive"}, {"title": "2 Background", "content": "Joint vs Independent NER and Relation Classification Joint entity and relationship classification (Eberts and Ulges, 2019; Zaratiana et al., 2024) can enhance performance through task transfer and global optimization, but increases decoding complexity and reduces flexibility, often requiring bespoke architectures that may not generalize to other tasks. In contrast, traditional IE pipelines use independent models (e.g., spaCy (Honnibal et al., 2020)), offering flexibility but making relationship classification dependent on static upstream NER components. Our work assumes entities are provided by an upstream component and focuses on"}, {"title": "LLMs for Relation Classification", "content": "LLMs have been leveraged for relation classification, achieving strong zero- and few-shot performance using meta in-context learning and synthetic data generation (Li et al., 2024a; Xu et al., 2023). Some approaches reformulate zero-shot relation classification as a question-answering task (Li et al., 2023). In document-level RE (DocRE), finetuning LLaMA2 with LoRA shows significant improvements, especially when a pretrained language model first classifies whether an entity pair expresses a relationship before passing it to the LLM (Li et al., 2024b). GenRDK (Sun et al., 2024) uses chain-of-retrieval prompts with ChatGPT to generate synthetic data for finetuning LLaMA2. Alternatively, Xue et al. (2024) finetune an LLM to propose head and tail entities given a document and relation label, outperforming other LLM-based baselines."}, {"title": "Zero-Shot Learning and Synthetic Training Data Generation", "content": "Zaratiana et al. (2023) and Bogdanov et al. (2024) showed that a straightforward and efficient model architecture can achieve excellent performance on the zero-shot NER task, given high-quality, large scale training data. Open-source LLMs have enabled the creation of this kind of training data, through simple and scalable protocols which prompt a model to label the entities in a short text with any type label (Zhou et al., 2024). Importantly, labels are not constrained to a particular taxonomy, and the generative model is free to assign any representative label to entities in the text. In the case of GLINER (Zaratiana et al., 2023), training on the Pile-NER dataset (created by Zhou et al. (2024)) enabled a new SoTA in zero-shot NER."}, {"title": "3 Method", "content": "The GLIREL architecture has three main components:\n\u2022 A pre-trained bidirectional langage model used as the text encoder, which jointly processes candidate relations and input texts.\n\u2022 An entity pair representation module which extracts vector representations for all entities in the text and proposes a representation for every pair of entities.\n\u2022 A scorer module to compute the similarity between entity pair representations and relation label representations.\nThe architecture encodes relation labels and entity pair embeddings in the same latent space to compute their similarity. The overall architecture is illustrated in Figure 2. We choose DeBERTa V3-large as the encoder model due to its excellent performance on downstream tasks (He et al., 2023)."}, {"title": "3.1 Input", "content": "The model input sequence is comprised of an ordered list of elements, where an element is a string containing one or more tokens. Concretely, inputs are built from:\n\u2022 a list of M zero-shot labels denoted as tm, each separated by a special [REL] token: to, [REL], t1, [REL], ...,tM-1. Both tm and [REL] are treated as elements,\n\u2022 a special [SEP] token to indicate the end of the labels prompt. [SEP] is treated as a single element,\n\u2022 the input text, denoted as a list of N tokens X0, X1, ..., XN-1, where each token is an element.\nThe GLIREL model additionally expects indices for E known entities in the text, represented as pairs of start and end positions. The input structure is illustrated in Figure 3."}, {"title": "Tokenization", "content": "The special [REL] and [SEP] tokens are added to the encoder's tokenizer vocabulary. The input sequence from Figure 3 is tokenized accordingly, ensuring that relation type labels and special tokens are properly handled. For this study, we follow the pooling strategy described in Zaratiana et al. (2022) by taking the first subtoken representation of each element. Details of the tokenization process are provided in Appendix A.2."}, {"title": "3.2 Token Representation", "content": "The token encoder processes the input sequence to compute interactions between all tokens (from both the relationship labels and from the input text), producing contextualized representations. Let p = {pt}M-1 \u2208 RMXD represent the encoder's output for each relation type, corresponding to the first subtoken representation of each relation type label. Similarly, h = {h}-1 \u2208 RNXD denotes the representation of each word in the input text. As already mentioned, for words tokenized into multiple subwords we use the representation of the first sub-word."}, {"title": "3.3 Label and Entity Pair Representation", "content": "We aim to encode relationship labels and entity pair embeddings into a unified latent space. We follow the methodology of GLiNER (Zaratiana et al., 2023), with additional steps for entity pair representation and refinement layers."}, {"title": "Relation Label Representation", "content": "After pooling, each relationship label in the input sequence is represented by a vector pi, Relation label representations are additionally transformed by a two-layer feed-forward network (FFN) as shown in equation 1:\nq = FFN(p) = {qt } M-1 \u2208 RMXD, (1)\nwhere M is the total number of relationship labels, and D is the dimensionality of the model's hidden layers. qt thus represents the transformed vector for the tth relationship label."}, {"title": "Entity Representation", "content": "The entity indices given as input to the model (see Figure 3) are used to extract entity representations from the word representations h. The representation of an entity starting at position i and ending at position j in the input text, eij \u2208 RD, is computed as\neij = FFN(hihj). (2)\nIn equation 2, FFN denotes a two-layer feed-forward network, and represents the concatenation operation."}, {"title": "Entity Pair Representation", "content": "Let eu = eij represent the uth entity representation computed in Equation 2 using its start and end positions i and j. For any distinct entity pair (u, v), where u \u2260 \u03c5, the pair representation kuv is computed as:\nKuv = FFN(eu ev), du \u2260 v (3)\nwhere denotes the concatenation operation, and self-pairs are explicitly excluded. The concatenated entity pair representations are passed through a FFN for projection into the model's latent space. The resulting representations \u03ba\u03b1\u03bd \u2208 RD are either further refined (Section 3.4), or used directly for scoring (Section 3.5)."}, {"title": "3.4 Refinement Layer", "content": "The refinement layer is used to further process both the relation type representations q and the entity pair representations \u03bauv. Inspired by the filter and refine module in joint entity and relation extraction"}, {"title": "Cross-Attention", "content": "The representations are refined using cross-attention, where the entity pair representations attend to the relation type representations, and vice versa. For the entity pair refinement, we compute:\n\u03ba\u03b9\u03bd = \u03ba\u03b9\u03bd + CrossAtt(Kuv, qt) (4)\nwhere CrossAtt(a, b) represents the cross-attention (also called encoder-decoder attention) mechanism as used by Vaswani et al. (2023), which allows information exchange between a and b."}, {"title": "Self-Attention", "content": "The refined entity pair representation undergoes further refinement using a self-attention mechanism to capture intra-pair interactions:\n\u03ba\u03b1\u03bd = \u03ba\u03c9\u03bd + SelfAtt(\u03ba\u03b9\u03bd) (5)\nThis process can be repeated for a number of refinement layers \u2013 in practice we use a maximum of two refinement layers for efficiency. After the attention mechanism, a feed-forward network (FFN) is applied to further transform the representations:\nfinal\n\u039a\u03c5\u03bd = FFN(\u03ba\u03b1\u03bd) (6)"}, {"title": "3.5 Scoring Layer", "content": "To evaluate whether the relationship between entity pair (u, v) corresponds to any relation type t in the set of given relation types T, we calculate the following matching scores:\n\u03a6(u, v) = {\u03c6(u, v, t) | t \u2208 T}, (7)\nwhere\n\u03c6(u,v,t) = \u03c3(\u03ba\u03b9qt) \u2208 R (8)\nIn Equation 8, o denotes a sigmoid activation function. Kuv is the representation of the entity pair representation for entities u and v. qt is the relation type representation vector type t. As we train with binary cross-entropy loss, \u03c6(u, v, t) can be interpreted as the probability of the entity pair (u, v) being of type t."}, {"title": "3.6 Training Dataset Generation", "content": "As discussed in Section 2, synthetic data generation has been a key enabler for recent improvements in efficient zero-shot NLP models. Due of the difficulty of large-scale manual annotation for relationship classification in particular, synthetic data generation offers a significant improvement in the effectiveness of GLiREL. Our synthetic annotation protocol generates training data for relation classification using an LLM. The goal is to create a flexible relation classification model capable of identifying a broad range of relationship types across various textual domains. It is thus crucial that our training dataset captures diverse relation types."}, {"title": "3.7 Extending to Coreference Resolution and Document-Level Relation Classification", "content": "Co-referential reasoning has been demonstrated to significantly enhance the performance of downstream tasks such as extractive question answering, fact verification, and relation extraction (Ye et al., 2020). Motivated by this insight, we also investigate GLiREL's performance on document-level relation classification, which requires co-reference resolution to aggregate cluster-level relations, projecting relations to the document level. The results of our experiments are presented in Appendix Section A.7."}, {"title": "4 Experiments", "content": "4.1 Relation Classification Datasets\nWe evaluate GLiREL using the Wiki-ZSL and FewRel benchmarks. Chen and Li (2021) derived Wiki-ZSL as a subset of Wiki-KB (Sorokin and Gurevych, 2017), generated through distant supervision. Entities are extracted from complete Wikipedia articles and linked to the Wikidata"}, {"title": "4.2 Zero-Shot Relation Classification Settings", "content": "For each dataset, we randomly select m relations as unseen relations (m = |Yu|) and split the data into training and testing sets, ensuring that these m relations do not appear in the training data so that Y \u2229 Yu = 0. We evaluate using macro precision, recall and F1 score. Experiments are repeated five times with different random selections of unseen relations and train-test splits, and the mean metrics are reported. We vary m to examine its impact on performance and to compare against other models."}, {"title": "Training Details", "content": "For each experiment, we train one model from scratch on the given dataset, and another model is trained following pretraining on our synthetically-annotated dataset. We limit the number of relation type labels prepended to each training instance to 25. For instances where there are less than 25 relation labels, we sample distinct negative labels from the training set. Following Sainz et al. (2023), we introduce regularization by shuffling relation type labels and randomly dropping labels for each instance. We ablate this regularization in Section 5.2. Further training details are provided in the Appendix Section A.5."}, {"title": "Baselines", "content": "We include the results from works described in Section 2. We also include the results of OpenAi's GPT-40 model (version gpt-40-2024-08-06) as a baseline for LLM performance on the zero shot relation classification task. In our experiments, we use the prompt as shown in Figure 7 to acquire a prediction for each entity pair in each instance."}, {"title": "4.3 Results", "content": "GLIREL demonstrates impressive capacity for the zero-shot relation classification task, achieving SOTA performance on both Wiki-ZSL and FewRel. Pretraining on the synthetically annotated dataset shows significant improvement over training from scratch. Additionally, GLiREL is the most successful model in terms of maintaining performance as the number of unseen labels m increases. GLiREL outperforms GPT-4o at every value of m for each dataset. At m = 15, GLiREL is marginally better than the current leading model TMC-BERT. It should be noted that both MC-BERT and TMC-BERT require additional data (entity types and descriptions for each relation type label), as well as one forward pass for each entity pair and label, to achieve their result. GLIREL uses only the given relation type labels, and can classify all entity pair relations in a single forward pass."}, {"title": "5 Analysis", "content": "5.1 Inference Speed\nWe compare the inference speeds of GL\u0130REL against some of the highest-performing ZS relation classification models; TMC-BERT (M\u00f6ller and Usbeck, 2024) and RelationPrompt (Chia et al., 2022). We run inference for each model on both GPU (one Tesla T4) and CPU. We use WikiZSL and FewRel datasets with number of unseen label m = 10 and batch size of 32. Each instance in FewRel contains exactly one entity pair, whereas WikiZSL instances have an average of two (up to a maximum 12). Our performance metric is sentences processed per second. The results can be seen in Table 3.\nAt inference time, the best RelationPrompt model generates N synthetic training examples per unseen label. In our experiments, we set number of synthetic examples N = 25, although N = 250 is recommended. RelationPrompt NG does not use the generator component to create synthetic training examples. This provides a speed up but at the expense of significant performance deterioration.\nRelationPrompt consists of a training data generator (GPT2 with 124M parameters) and an extractor (140M parameters). TMC-BERT has 109M parameters. GLIREL has 467M parameters in total."}, {"title": "Result", "content": "GLIREL maximizes performance while maintaining efficiency for FewRel on CPU, showing a relatively small decrease in throughput when presented with more entity pairs in WikiZSL. This deterioration is far more pronounced for TMC-BERT, which requires a forward pass for every entity pair, for every candidate label. For RelationPrompt, the generation component poses a significant bottleneck. On GPU, the margin becomes"}, {"title": "5.2 Ablation Study", "content": "Relation Type Random Dropping We employed a strategy of randomly dropping relation labels in order to vary the number of relation labels during training. This approach aims to increase model robustness to different numbers of labels at inference time."}, {"title": "Result", "content": "By ablating this component, we see that GLIREL benefits from random dropping on Wiki-ZSL but actually deteriorates on FewRel. This may be caused by the higher number of entity pairs and relation labels in Wiki-ZSL demanding greater generalization of the model, while the single entity pair in FewRel instances provides a cleaner signal and random dropping is unhelpful noise."}, {"title": "Refinement Layers", "content": "The refinement layers as described in Section 3.4 aim to enhance the representations of both the entity pair representations and the relation label representations respectively."}, {"title": "Result", "content": "For the FewRel dataset, we observe benefits from having both prompt and entity pair (relation) refinement layers. Conversely, the model performs best on Wiki-ZSL when no refinement layers are used. As with the random drop ablation, this contrast can be attributed to the difference in entity pairs between the two datasets. The FewRel model benefits from additional depth, as it is modelling only one entity pair interaction per instance. With multiple entity pairs in Wiki-ZSL instances, the cross-attention mechanism may introduce confusing interactions between irrelevant pairs and relations, amplifying signal noise."}, {"title": "6 Conclusion", "content": "We have shown that GliREL is a flexible and highly performant approach to zero-shot relation classification (ZSRC), which achieves SoTA results on challenging benchmarks. Unlike other high-performing ZSRC models, GLiREL can classify multiple entity pairs and relation labels in a single input, making it significantly more efficient. Additionally, we have presented a paradigm for generating high-quality, large-scale synthetic datasets for zero-shot relation classification, as well as an effective training protocol. We hope these methods inspire future work in the area of relation classification."}, {"title": "7 Limitations", "content": "As both labels and text are processed in a single forward pass, the number of labels that can be passed in an instance is limited by the model's max sequence length in DeBERTa's case this is 512 tokens. It is possible to extend DeBERTa's max positional encoding length to larger values, but that has not been studied here. An avenue for future work that may solve this issue has already been implemented for the GLiNER library, through the use of bi- and poly-encoders. Such architectures enable the use of arbitrary amounts of labels. The embeddings of these labels can be precomputed, which may bring additional efficiency benefits.\nThe joint encoding of labels and input sequence allows the model to condition label and entity pair representations with respect to one another. This is advantageous in cases where it is important for the model to be aware of all possible labels that can be predicted. However, a limitation of this approach is that the model's performance on one label can be influenced by the order and number of other provided labels.\nOne benchmark-related issue observed by the authors is that texts in which two entities appear may not provide sufficient evidence for the imputed relationship. For example, an instance in Wiki-ZSL (Chen and Li, 2021) imputes the relation label P20 (place of death) between \"Jim Dickinson\" and \"Memphis\" for the following text:\n\"The Pengwins recorded with Rick Derringer at Bearsville Studios in New York and in Memphis with producer Jim Dickinson, and by Columbia and Polygram.\"\nThis is due to the distant annotation of Wiki-ZSL, which uses Wikidata to assign relationships between identified entities. To make a correct prediction, a model would require access to an external knowledge base. This is not implemented in GLIREL or the majority of the methods benchmarked in Table 1. In light of this, the authors suggest moving away from Wiki-ZSL as one of the primary benchmarks for ZSRC, towards benchmarks that assess a model's ability to extract relationships based solely on the text provided (as seen in FewRel)."}, {"title": "A Appendix", "content": "A.1 Extended Related Work\nExisting Approaches Many systems have addressed relation extraction with varying degrees of success. Earlier works saw CNNs employed in the task of slot filling (Adel et al., 2016), a similar task to relation extraction."}, {"title": "Question-Answering and Textual Entailment", "content": "Sainz et al. (2021) and Obamuyide and Vlachos (2018) reformulate relation extraction as an entailment task by using simple verbalizations of relation labels and descriptions. These systems allow for the use of existing textual entailment models and datasets to achieve strong performance in zero-shot and few-shot settings.\nLevy et al. (2017) reduced relation extraction to answering reading comprehension questions by associating natural-language questions with each relation slot. This approach enables the use of neural reading comprehension techniques and supports zero-shot learning by facilitating the extraction of new relation types."}, {"title": "Distant Supervision for Relation Extraction Dataset Construction", "content": "Hand-annotating relation classification (RC) datasets at scale is intractable both because of the size and domain-specificity of relation taxonomies, and especially because of the quadratic number of potential relations in a given text, as a function of the number of named entities in the text. Foundational research leverages distant supervision to bootstrap training datasets for RC, utilizing open source knowledge bases such as Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014), Freebase (Bollacker et al., 2008) and DBPedia (Auer et al., 2007) to obtain high-quality relations between entities, and then mining data sources such as Wikipedia for texts mentioning both head and tail entities to construct training datasets (Bunescu and Mooney, 2007; Mintz et al., 2009).\nDistant supervision enables the creation of large scale datasets; however, historical work is still constrained to specific pre-defined label sets, and training data is noisy because inputs are not specifically annotated for particular relations."}, {"title": "Real-world Evaluation of Relation Extraction Models", "content": "Sabo et al. (2021) critique existing few-shot learning (FSL) datasets for RC, highlighting their unrealistic data distributions, and propose a novel method to create more realistic few-shot test data using the TACRED dataset (Zhang et al., 2017), resulting in a new benchmark. Furthermore, they analyze classification schemes in embedding-based nearest-neighbor FSL approaches, proposing a novel scheme that treats the \"none-of-the-above\" (NOTA) category as learned vectors, improving performance."}, {"title": "A.2 Tokenization Details", "content": "The special [REL] and [SEP] tokens are added as special tokens to the encoder's tokenizer vocabulary. The input sequence from Figure 3 is passed to the tokenizer, which joins all elements by whitespace before creating the appropriate encoder-specific subword tokens and input IDs. For example, the label \"participation in\" would be tokenized into the subword tokens: \"particip\", \"##ation\" and \"## in\". A mapping from the original input elements in Figure 3 to the input IDs is maintained in order to perform subword token pooling of the encoder output. We follow Zaratiana et al. (2022), and perform pooling by taking the vector representation of the first subword token. In the above example, this would correspond to the vector representation for \"particip\".\nWith the treatment of one or more tokens as elements, relation type labels can be text of any length. Because the subword tokens of each label are subject to the aforementioned pooling operation, we denote each label using a single index tm."}, {"title": "A.4 Synthetic Data Generation Details", "content": "A.5 Training Setup Details\nFor the hyperparameters and configuration of our model, refer to Table 4. We used the AdamW optimizer (Loshchilov and Hutter, 2017) with an initial learning rate of 1 \u00d7 10-5 for the pretrained encoder parameters and 1 \u00d7 10-4 for the remaining parameters involved in span representation, relation representation and scoring layers. A warmup ratio of 10% was used with a cosine scheduler. The"}, {"title": "A.6 Full Zero-Shot Relation Classification Results", "content": "A.7 Coreference Resolution and Document-level Relation Classification\nWe conceptualize coreference resolution as a specific case of relation classification, where the coreference relation between two mentions referring to the same entity is represented by a special SELF label.\nTo add coreference resolution ability to GLiREL, we simply include the SELF relation type in the label set during training and inference.\nIn our experiments (Section 4), we evaluate the performance of this approache using the Re-DocRED dataset (Tan et al., 2023)."}, {"title": "Document-Level Relation Classification", "content": "When coreference information is available, document-level relation extraction (DocRE) can be achieved by propagating local relations across coreference clusters. To implement this, we employ a post-processing step that clusters mentions based on the SELF relation, akin to the connected components algorithm. The outgoing (non-SELF) edges from each mention within a cluster are then interpreted as document-level relations between the resolved entity cluster and other entity clusters in the text."}, {"title": "Number of entity pairs bottleneck", "content": "One bottleneck of the initial GLiREL archictecture is the fact that the number of entity pairs in an instance scales almost quadratically with the number of entities (N2 \u2013 N, excluding self-pairs). This becomes a significant memory issue when extending GL\u0130REL to document-level relation classification. To alleviate this issue, we can incorporate a naive win-"}]}