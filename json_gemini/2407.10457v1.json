{"title": "The Good, The Bad, and The Greedy:\nEvaluation of LLMs Should Not Ignore Non-Determinism", "authors": ["Yifan Song", "Guoyin Wang", "Sujian Li", "Bill Yuchen Lin"], "abstract": "Current evaluations of large language models\n(LLMs) often overlook non-determinism, typ-\nically focusing on a single output per exam-\nple. This limits our understanding of LLM\nperformance variability in real-world applica-\ntions. Our study addresses this issue by explor-\ning key questions about the performance differ-\nences between greedy decoding and sampling,\nidentifying benchmarks' consistency regarding\nnon-determinism, and examining unique model\nbehaviors. Through extensive experiments, we\nobserve that greedy decoding generally outper-\nforms sampling methods for most evaluated\ntasks. We also observe consistent performance\nacross different LLM sizes and alignment meth-\nods, noting that alignment can reduce sampling\nvariance. Moreover, our best-of-N sampling\napproach demonstrates that smaller LLMs can\nmatch or surpass larger models such as GPT-4-\nTurbo, highlighting the untapped potential of\nsmaller LLMs. This research shows the im-\nportance of considering non-determinism in\nLLM evaluations and provides insights for fu-\nture LLM development and evaluation. 1", "sections": [{"title": "1 Introduction", "content": "When evaluating a large language model (LLM),\ntwo common generation configurations are com-\nmonly used: greedy decoding and nucleus sam-\npling (Holtzman et al., 2020). It's important to note\nthat given a particular input, the same LLM may\ngenerate significantly different outputs under vari-\nous decoding configurations, a phenomenon known\nas non-determinism in generation. However, most\nevaluations of LLMs are based on a single output\nper example. This practice is primarily due to prac-\ntical considerations, as LLM inference and evalua-\ntion can be computationally expensive. Neglecting\nnon-determinism in generation significantly limits\nour comprehensive understanding of LLMs. Addi-\ntionally, without reporting the standard deviation\nin most current LLM evaluations, it is difficult to\nmeasure the variability and dynamics of LLMs in\nreal-world applications.\nFor certain capabilities such as math reason-\ning (Cobbe et al., 2021; Hendrycks et al., 2021b)\nand coding, greedy generation is preferred to en-\nsure fair comparisons. Nonetheless, it remains un-\nclear whether there are significant differences in\nperformance between greedy decoding and sam-\npling. Recent investigations have also highlighted\npotential issues of instability in LLMs (Li et al.,\n2024a; Hassid et al., 2024). In a study where the\nbest answer was selected from 256 random genera-\ntions, the Llama-2-7B model achieved an impres-\nsive 97.7% accuracy in solving GSM8K questions,\neven surpassing GPT-4 (Li et al., 2024a). This\nphenomenon further underscores the enormous po-\ntential of LLMs in their non-deterministic outputs.\nHerein, we aim to investigate a series of critical\nquestions regarding the non-determinism of LLM\ngenerations, which have not been fully explored:\n\u2022 Q1: How does the performance gap between\ngreedy decoding and sampling differ?\n\u2022 Q2: When is greedy decoding better than sam-\npling, and vice versa? Why?\n\u2022 Q3: Which benchmark is most/least consistent\nwith respect to non-determinism?\n\u2022 Q4: Do any models possess unique patterns?\nApart from Q1-Q4 in Sec. 3, we also explore the\nscaling effect on non-determinism (Sec. 4.1), the\nalignment effect on non-determinism (Sec. 4.2),\nthe temperature and repetition effect on generation,\nand the full potential of LLMs (Sec. 5).\nOur extensive results reveal these findings:\n\u2022 For most benchmarks we evaluated, a notable per-\nformance gap is observed between greedy gener-\nation and the average score of multiple sampling.\nIn certain cases, the performance ranking under\ndifferent generation configurations differs."}, {"title": "2 Experimental Setup", "content": "Benchmarks. We select multiple benchmarks for\nour experiments, encompassing abilities of general\ninstruction-following, knowledge, math reasoning,\ncoding, etc. As summarized in Table 1, the selected\nbenchmarks are: AlpacaEval 2 (?), Arena-Hard (Li\net al., 2024b), WildBench v2 (Lin et al., 2024),\nMixEval (Ni et al., 2024), MMLU-Redux (Gema\net al., 2024), GSM8K (Cobbe et al., 2021), and\nHumanEval (Chen et al., 2021).\nAlpacaEval 2 (?), Arena-Hard (Li et al., 2024b)\nand WildBench v2 (Lin et al., 2024) are general\ninstruction-following benchmarks. AlpacaEval\nconsists of 805 questions, Arena-Hard incorpo-\nrating 500 well-defined technical problem-solving\nqueries, and WildBench including 1024 challeng-\ning tasks from real users. For AlpacaEval 2, we\nreport the length-controlled win rate (LC). For\nArena-Hard, we report the win rate (WR) against\nthe baseline model. For WildBench, we use task-\nwise scores and the corresponding task-macro WB-\nScore as the metrics.\nSince the original MMLU (Hendrycks et al.,\n2021a) benchmark is huge and contain numerous\nground truth errors (Wang et al., 2024b; Gema et al.,\n2024), we use MMLU-Redux (Gema et al., 2024)\nwhich is a subset of 3000 manually re-annotated\nLLMs. We test several open-weight LLMs, in-\ncluding Llama-3-Instruct (Meta, 2024), Yi-1.5-\nChat (Young et al., 2024), Qwen-2-Instruct (Bai\net al., 2023), Mistral (Jiang et al., 2023a), which\nare widely used. A proprietary LLM, GPT-4-\nTurbo, is included for comparison. We also con-\nsider models of different sizes in the same fam-\nily such as Qwen2 and Yi-1.5 for more analysis.\nTo study the effect of alignment techniques, we\nevaluate models trained with different alignment\nmethods, including DPO (Rafailov et al., 2024),\nKTO (Ethayarajh et al., 2024), IPO (Azar et al.,\n2024), ORPO (Hong et al., 2024), RDPO (Park\net al., 2024), and SimPO (Meng et al., 2024). We\nuse the checkpoints released by Meng et al. (2024).\nSetup. We aim to compare the performance of\nLLMs under different decoding configurations. We\nselect greedy decoding and sampling generation\nfor the main comparison. For sampling, we set the\ntemperature to 1.0 and top-p to 1.0.\nWe use official evaluation scripts for AlpacaE-\nval 2, Arena-Hard, WildBench, and MixEval. For\nMMLU-Redux, instead of using the next token\nprobability of the choice letters, we employ zero-\nshot CoT and encourage the model to generate\nthe answer in the form of natural language sen-\ntence. For GSM8K and HumanEval, we use Open-\nInstruct framework (Wang et al., 2023) to evaluate\nthe models, which may differ from zero-shot CoT.\nWe will run more comprehensive evaluations on\nthese two benchmarks in the future. We sample 16\ncompletions for AlpacaEval 2, Arena-Hard, Wild-\nBench, and MixEval, 32 completions for MMLU-\nRedux, 128 for GSM8K and HumanEval."}, {"title": "3 Experimental Results", "content": "We present our experiment results in Table 2 and\nTable 3. We analyze the results and answer sev-\neral important research questions around the non-\ndeterminism of LLM generations as follows.\nQ1. How does the performance gap be-\ntween greedy decoding and sampling differ?\nFrom the results, we observe a consistent perfor-\nmance gap between greedy decoding and the sam-\npling method. This disparity holds true across\nvarious LLMs, whether they are proprietary or\nopen-source, and across multiple benchmarks en-\ncompassing instruction-following, language under-\nstanding, math reasoning, and code generation. For\nWildBench, which enables fine-grained analysis\nof LLM capabilities, the performance gap is also\nevident across all task categories, as shown in Ta-\nble 3. Different decoding configurations can even\nalter the model rankings in some cases. For exam-\nple, on Arena-Hard, Qwen2-7B is slightly better\nthan Llama-3-8B when both use greedy decoding;\nHowever, Llama-3-8B may outperform Qwen2-7B\nwhen both decode by sampling.\nQ2. When is greedy decoding better than\nsampling, and vice versa? Why?\nFor most evaluated tasks and models, greedy de-\ncoding outperforms sampling. However, AlpacaE-\nval serves as a notable exception, where sampling\ndemonstrates superior performance.\nGSM8K and HumanEval are reasoning tasks\nrequiring LLMs to solve specific math or coding\nproblems with definite solutions. MixEval also\nfollows a deterministic pattern with its ground-\ntruth-based benchmarks. While AlpacaEval, Arena-\nHard, and WildBench are open-ended instruction-"}, {"title": "4 How Various Factors Influence\nNon-Determinism?", "content": "In this section, we further investigate how various\nfactors, such as scaling, alignment, and several\ndecoding parameters, influence non-determinism.\n4.1 Scaling Effect on Non-Determinism\nSome might assume that larger LMs will have\nlower uncertainty in decoding, leading to lower\nvariance in performance when sampling. However,\nour results challenge this assumption.\nWe use the Yi-1.5-Chat and Qwen2-Instruct se-\nries to investigate the scaling effect. The results\nfor the Yi-1.5 and Qwen2 series are presented in\nTable 2 and Table 4, respectively. Performance\ndifferences are observed across LLMs of various\nsizes, ranging from 0.5B to 34B parameters. The\nfindings in Section 3 are consistent across different\nmodel sizes. However, no pattern related to the\nnumber of model parameters could be identified.\nFor instance, scaling parameters does not result\nin lower sampling variance. Notably, Qwen2-7B-\nInstruct shows higher variance on AlpacaEval and\nHumanEval compared to its smaller counterparts.\n4.2 Alignment Effect on Non-Determinism\nAlignment methods, such as DPO, enhance LLMs\nby learning from preference data. We evaluate\nthe effects of alignment methods such as DPO,"}, {"title": "4.3 Temperature Effect on Non-Determinism", "content": "For sampling generation, temperature serves as a\ncontrol mechanism for the randomness of the sam-\npling process, where lower values make the model\nmore deterministic, whereas higher values make\nthe model more random. In this section, we present\nan ablation study to evaluate the effect of varying\ntemperatures on non-determinism generation.\nAs depicted in Figure 2(a), we observe that, for\nAlpacaEval, higher temperature will lead to slightly\nbetter performance, which aligns with the results\nin Sec. 3. A recent study (Renze and Guven, 2024)\nfinds that, on multiple-choice QA tasks, changes\nin temperature from 0.0 to 1.0 do not have a sta-\ntistically significant impact on LLM performance.\nOur results on MMLU aligns with their findings.\nAnother findings emerges when the temperature is\nextremely high, such as 1.5. Comparing with open-\nended instruction following, a high temperature\nsignificantly impacts the reasoning and code gen-\neration capabilities of LLMs and the model strug-\ngles to solve questions in GSM8K and HumanEval.\nHowever, it still manages to perform relatively well\nin open-ended instruction following tasks, such as\nAlpacaEval and ArenaHard.\n4.4 Repetition Effect on Generation\nIn addition to parameters that control greedy search\nand sampling, there are other parameters that influ-\nence the generation process, such as the repetition\npenalty (Keskar et al., 2019). Here we examine the\neffect of repetition penalty on generation. Repeti-\ntion penalty penalizes new tokens based on whether"}, {"title": "4.5 Surface Patterns in Non-Determinism\nGeneration?", "content": "We try to explore the surface patterns in non-\ndeterminism generation. Firstly, we compare the\ngeneration length of different generation configu-\nrations in Table 5. The generation length for Al-\npacaEval and ArenaHard is defined as the length\nof the model's response, while for MMLU and\nGSM8K, it refers to the length of the final answer\nwith chain-of-thoughts. We observe that the com-\npletions generated by greedy decoding are typically\nmarginally shorter than those produced via sam-"}, {"title": "5 What is the Full Potential of\nNon-Determinism?", "content": "Current evaluations of LLMs mainly assess them\nbased on a single output per instance, which limits\nour understanding of their full potential. Follow-\ning Jiang et al. (2023b) and Li et al. (2024a), we\nadopt a Best-of-N setting, selecting the best answer\nfrom N sampled responses. To accomplish this,\nwe employ off-the-shelf reward models, such as\nArmoRM (Wang et al., 2024a) and FsfairX (Xiong\net al., 2024), to rank the responses of Llama-3-8B-\nInstruct, selecting the one with the highest reward.\nWe also include an \u201coracle\u201d baseline which directly\npicks the best response as the upper bound of best-\nof-N strategy.\nThe results are depicted in Figure 4. We\nobserve a significant performance enhancement\nwhen applying simple best-of-N strategy for mul-\ntiple sampled responses. Notably, with the ora-\ncle selection, even smaller LLMs like Llama-3-\n8B-Instruct can outperform GPT-4-Turbo on\nMMLU, GSM8K, and HumanEval. This finding\nunderscores that compact-sized LLMs already ex-\nhibit robust capabilities, highlighting that a more\nsignificant challenge in alignment is to robustly de-\ncode such knowledge and reasoning paths. Further-\nmore, cutting-edge reward models can also select\nsuperior responses from multiple generations, and\ncan outperform GPT-4-Turbo on GSM8K with only\n8 samples. However, there is still a huge perfor-\nmance gap between reward models and the oracle\nbaseline, indicating ample room for improvement.\nBuilding upon these promising findings, there\nare two ways to further enhance the performance\nof smaller LLMs. Firstly, probability calibration\ntechniques can guide LLMs towards generating\nsuperior answers with higher likelihoods. Align-\nment methods, specifically preference optimiza-\ntion (Rafailov et al., 2024), play a pivotal role\nin this process. Secondly, strategies for ensem-\nble learning or selecting the best answer from\nmultiple completions warrant attention. Self-\nconsistency (Wang et al., 2022) and advanced\nprompting techniques (Yao et al., 2023; Lin et al.,\n2023), which employs heuristic selection from mul-\ntiple completions, is also worth further exploration."}, {"title": "6 Related Work", "content": "LLM Evaluation In recent years, the develop-\nment of various benchmarks has significantly ad-\nvanced the evaluation of LLMs. Benchmarks\nlike MMLU (Hendrycks et al., 2021a), Hel-\nlaSwag (Zellers et al., 2019), and ARC (Clark et al.,\n2018) have expanded the scope by assessing capa-\nbilities across knowledge understanding, and com-\nplex reasoning. AlpacaEval (?), MT-Bench (Zheng\net al., 2023), ArenaHard (Li et al., 2024b), and\nWildBench (Lin et al., 2024), leveraging frontier\nmodels as judges, evaluate open-ended instruction-\nfollowing capabilities. Moreover, GSM8K (Cobbe\net al., 2021), MATH (Hendrycks et al., 2021b), Hu-\nmanEval (Chen et al., 2021), and MBPP (Austin\net al., 2021) focus on evaluating math reasoning\nand code generation capabilities.\nDue to the costly nature of LLM inference and\nevaluation process, most evaluations of LLMs rely\non a single output per example. In this paper, we\naim to explore the impact of various generation\nconfigurations, particularly non-deterministic gen-\nerations, on the performance of LLMs.\nDecoding Strategy Given a prompt, LLMs rely\non a decoding strategy to auto-regressively gen-\nerate response. The simplest decoding method,\ngreedy decoding, selects the next token with the\nhighest probability. Beam search (Freitag and Al-\nOnaizan, 2017), an improved version of greedy"}, {"title": "7 Conclusion & Future directions", "content": "search, retains the top-B tokens with the highest\nprobability at each time step. In order to gener-\nate diverse responses, non-determinism generation\nmethods, such as Top-k (Fan et al., 2018) and Top-p\nsampling (Holtzman et al., 2020), randomly picks\nthe next token based on the probability distribu-\ntion. The temperature parameter serves to balance\nresponse quality and diversity (Ackley et al., 1985).\nOther decoding parameters, like length and repeti-\ntion penalties (Keskar et al., 2019), are also avail-\nable to further control the generation process.\nWe investigate a series of critical yet overlooked\nquestions around non-determinism of LLM genera-\ntions. After evaluating several LLMs across seven\ncommonly used benchmarks, we have answered\nseveral intriguing research questions. Further anal-\nsis also provides insights on how scaling and align-\nment will effect on non-determinism generation.\nWe hope this work can enhance our comprehen-\nsion of the generation methods and the widely used\nbenchmarks. Our evaluation results can also be\nused for improving future research. For example,\nour best-of-N results can serve as a benchmark for\nassessing reward models (Lambert et al., 2024)."}]}