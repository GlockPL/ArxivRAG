{"title": "Interactive DualChecker for Mitigating Hallucinations in Distilling Large Language Models", "authors": ["Meiyun Wang", "Masahiro Suzuki", "Hiroki Sakaji", "Kiyoshi Izumi"], "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities across various machine learning (ML) tasks. Given the high costs of creating annotated datasets for supervised learning, LLMs offer a valuable alternative by enabling effective few-shot in-context learning. However, these models can produce hallucinations, particularly in domains with incomplete knowledge. Additionally, current methods for knowledge distillation using LLMs often struggle to enhance the effectiveness of both teacher and student models. To address these challenges, we introduce DualChecker, an innovative framework designed to mitigate hallucinations and improve the performance of both teacher and student models during knowledge distillation. DualChecker employs ContextAligner to ensure that the context provided by teacher models aligns with human labeling standards. It also features a dynamic checker system that enhances model interaction: one component re-prompts teacher models with more detailed content when they show low confidence, and another identifies borderline cases from student models to refine the teaching templates. This interactive process promotes continuous improvement and effective knowledge transfer between the models. We evaluate DualChecker using a green innovation textual dataset that includes binary, multiclass, and token classification tasks. The experimental results show that DualChecker significantly outperforms existing state-of-the-art methods, achieving up to a 17% improvement in F1 score for teacher models and 10% for student models. Notably, student models fine-tuned with LLM predictions perform comparably to those fine-tuned with actual data, even in a challenging domain. We make all datasets, models, and code from this research publicly available.", "sections": [{"title": "Introduction", "content": "The advent of Large Language Models (LLMs) has revolutionized artificial intelligence, providing comprehensive end-to-end solutions for numerous machine learning (ML) tasks (Chang et al. 2024; Huang et al. 2024). Traditional ML approaches predominantly rely on supervised learning, which necessitates large annotated datasets to achieve high performance. In contrast, LLMs, trained on trillions of tokens and possessing hundreds of billions of parameters, can function as extensive knowledge bases and excel in various tasks through in-context learning, without requiring additional training (Kojima et al. 2022; Brown et al. 2020).\nHowever, LLMs are prone to hallucination issues, manifesting as either factual inaccuracies or inconsistencies in responses, known respectively as factuality and faithfulness hallucinations (Huang et al. 2023). Figure 1 shows the distribution of hallucination types observed in a preliminary experiment involving 200 labeled samples generated by GPT-3.5 Turbo 2 using zero-shot prompting from our green innovation dataset. The results indicate a predominant issue of factuality hallucinations at 94.9%, compared to faithfulness hallucinations at only 5.1% in domain adaptation.\nExisting methods use external knowledge to supplement missing information for factuality hallucinations (Yao et al. 2023; Ram et al. 2023) and focus on mitigating language model overconfidence to improve faithfulness (Chen et al. 2022; Schuster et al. 2022; Zhao et al. 2023). However, these solutions face significant challenges: a) constructing external knowledge bases is expensive, and input length limits make it difficult to determine how much external knowledge to feed into the model; b) domain adaptation is challenging due to the variability and complexity of human labeling standards; c) few studies address both factuality and faithfulness hallucinations; and d) most methods require costly additional pre-training or fine-tuning to achieve high accuracy."}, {"title": "Related Work", "content": "Hallucinations in language models (LMs) often arise from the optimization techniques used during training. Methods like Maximum Likelihood Estimation (MLE) with teacher forcing can cause models to replicate training data without genuine understanding, leading to hallucinations during inference (Kang and Hashimoto 2020). To address this issue, researchers have proposed various solutions for different types of hallucinations in LLMs.\nImproving the knowledge LLMs lack during reasoning has shown promise for factuality hallucinations. For instance, (Sun et al. 2023) employs contrastive learning to minimize the influence of confusing negative knowledge in conversations. Similarly, (Sahu et al. 2023) generates challenging text near class boundaries to diversify and strengthen the training data. Additionally, some studies leverage knowledge graphs as external sources to enhance reasoning capabilities (Guan et al. 2024; Shi et al. 2023). However, these methods face limitations due to LLMs' input length constraints, which can make external knowledge insufficient for a comprehensive understanding of specific domains. Furthermore, in highly specialized fields, even human annotators face challenges, making it even more difficult for LLMs to grasp the correct logic and adhere to annotation standards.\nStudies on faithfulness hallucinations in LMs focus on their uncertainty and overconfidence. (Diao et al. 2023) introduces several metrics to characterize uncertainty, enabling the selection of the most uncertain questions for annotation through active prompting of LLMs. (Zhou, Jurafsky, and Hashimoto 2023) investigates how epistemic markers\u2014such as certainty, uncertainty, and evidentiality\u2014affect LMs, concluding that LMs mimic observed language use rather than genuinely reflecting epistemic uncertainty. (M\u00fcndler et al. 2023) proposes a novel prompting-based framework to detect and mitigate self-contradictions effectively.\nKnowledge distillation transfers knowledge from a large teacher to a smaller student model. Leveraging the capabilities of LLMs in various machine learning tasks, many studies use LLMs as knowledge bases to enhance efficiency and improve responses. (Zhang et al. 2024) introduces the Decision-Tree-of-Thought (DToT) prompting method, which boosts LLMs' detection performance and extracts high-quality rationales, improving overall performance and interpretability.\nStudies also emphasize optimizing the interaction between teacher and student models during the distillation process. (Liu et al. 2024) analyzes student model weaknesses and synthesizes labeled samples to help the teacher model address deficiencies effectively. (Sengupta et al. 2024) proposes a collaborative approach with joint loss and curriculum learning for meta-teacher knowledge distillation. It creates a dynamic learning environment where the teacher model adapts its strategy based on the student's progress. However, in specialized domains where texts are often too professional to generate, we refine teaching templates using rationales to enhance interaction instead of developing new samples.\nDomain adaptation is a crucial application of LLMs, enhancing their ability to handle specialized tasks beyond general applications. For example, (Zhou et al. 2024) develops LawGPT through legal-oriented pre-training and supervised fine-tuning. Similarly, (Li et al. 2024) leverages a biomedical figure-caption dataset, employs GPT-4 to generate self-instructed open-ended data, and trains a large vision-language model using a curriculum learning method. However, these approaches demand extensive pre-training or fine-tuning to achieve high performance."}, {"title": "Approach", "content": "Our proposed DualChecker addresses hallucinations in distilling LLMs, as illustrated in Figure 2. For a given task \\(T \\in T\\), with a question \\(Q_i\\) and context \\(C_i\\), the ContextAligner module within DualChecker first retrieves similar cases from the embedding set \\(EMB_I\\), where \\(N = \\{1,...,n\\}\\) is the set of all indices, \\(i\\) is the current index and \\(I = N \\setminus \\{i\\}\\). Then, a rationale \\(R_j\\) is generated using LLMs for each similar case, where \\(j \\in I\\). Guided by these cases, the teacher model \\(Model_T\\) produces a confidence score \\(Score\\), rationale \\(R_T\\), and prediction \\(P_T\\). Next, a checker compares \\(Score\\) against a threshold \\(Thred_T\\). If the score is below the threshold, additional detail \\(D_i\\) is incorporated for re-prompting; otherwise, \\(P_T\\) is added to the batch \\(B\\) for fine-tuning the student model \\(Model_S\\) with the objective function \\(\\min_{\\theta_S} L_F(\\theta_S)\\). The student model then outputs a probability score \\(Score_S\\) and prediction \\(P_S\\). Finally, a checker identifies the case in \\(B\\) with the lowest probability score. This borderline case, along with its LLM-generated rationale, is used by \\(Model_T\\) to refine its teaching templates for the next prompting.\nOur green innovation dataset comprises three core tasks: green innovation identification \\(T^{cls}\\), technological causality extraction \\(T^{ce}\\), and the identification of environmental impact pathways \\(T^{path}\\). The first task is a binary classification, the second involves token-level classification, and the third is a multi-class classification.\nFor a given context \\(C_i\\), the question \\(Q_i\\) for \\(T^{cls}\\) is: \u201cDoes this text belong to the green innovation category?\u201d The answer is binary, either \u201cno\u201d (0) or \u201cyes\u201d (1). For \\(T^{ce}\\), \\(Q_i\\) could be: \u201cGiven the following text related to green innovation, extract the technical expression and the environmental effect it is expected to achieve.\u201d This task focuses on identifying causal relationships between the technical expression and its environmental impact, hence the term causality extraction. Finally, for \\(T^{path}\\), \\(Q_i\\) asks: \u201cGiven the following text related to green innovation, classify it into one of four categories.\u201d The classes for \\(T^{path}\\) are listed in Table 1, with the target answer ranging from 0 to 3, indicating the class index."}, {"title": "ContextAligner", "content": "The ContextAligner module enhances alignment with human labeling standards, inspired by the RAG framework (Lewis et al. 2020) and preliminary experiments. Recognizing that LMs struggle with domain-specific tasks due to input length limitations, this module uses an existing labeled dataset to bridge the knowledge gap in few-shot in-context learning, instead of sourcing external contexts. The core functionality generates an embedding vector \\(EMB_i\\) for each context \\(C_i\\). It computes the cosine similarity between context embeddings to identify the top K most similar contexts. Specifically, each cosine similarity between \\(EMB_i\\) and \\(EMB_{j \\in I}\\) is calculated as:\n\\[\\text{Similarity}(EMB_i, EMB_j) = \\frac{EMB_i \\cdot EMB_j}{||EMB_i|| ||EMB_j||} \\]\nFor each retrieved-context \\(C_i\\), the model uses LLMs to generate a rationale \\(R_j \\sim \\text{LLM}(\\cdot | C_j, L_j)\\), aimed at improving the reasoning process. Here, \\(L_j\\) represents the label of \\(C_j\\). The aggregated data \\((C_j, L_j, R_j)\\) guides the teacher model, thereby facilitating more effective reasoning and decision-making."}, {"title": "Teacher Confidence Checker", "content": "The interactive checker systems function as a confidence checker for \\(Model_T\\) and a borderline case identifier for \\(Model_S\\). For the confidence checker, the teacher LLM first outputs a confidence score \\(Score\\), a rationale \\(R\\), and a prediction \\(P_T\\) from its reply: \\(Reply_T \\sim LLM(\\cdot | C_j, L_j, R)\\). Specifically, the confidence score \\(Score\\) is constrained to a range between 0% and 100%, with the additional instruction: \u201cPlease output a confidence score as a percentage.\u201d The confidence checker assists in enhancing the consistency of the teacher model. The \\(Score\\) is compared against a predefined teacher threshold \\(Thred\\) to determine whether more details are needed to re-prompt. We assume that both black-box and white-box LLMs can generate explicit confidence scores during inference, and the checker system identifies model confidence as:\n\\[\\text{Checker}(Score) = \\begin{cases} \\text{Unconfident}, & \\text{if } Score \\in [0\\%, Thred) \\\\ \\text{Confident}, & \\text{if } Score \\in [Thred, 100\\%] \\end{cases} \\]\nIf \\(Score\\) is identified as \u201cUnconfident,\u201d details of contexts \\(D_i\\) are added to re-prompt the teacher model and regenerate the reply. Due to resource limitations, we set the re-prompting to occur only once. If identified as \"Confident,\" the teacher prediction \\(P_T^i\\) is added to the batch for fine-tuning the student model."}, {"title": "Student Fine-tuning", "content": "Given the token representation of an input sequence \\(Token = \\{Token_1,..., Token_t\\}\\), where t is the length of the sequence and \\(k \\in \\{1, ..., t\\}\\) represents the position of a token, the prediction probability for the classification tasks \\(T^{cls}\\) (binary), \\(T^{path}\\) (multi-class), and \\(T^{ce}\\) (causality extraction) is defined as:\n\\[p(c| \\text{Token}) = \\frac{\\exp(w_c h)}{\\sum_{m \\in M} \\exp(w_m h)}, \\]\nwhere \\(M\\) is \\(\\{0, 1\\}\\) for \\(T^{cls}\\), \\(\\{0, 1, 2, 3\\}\\) for \\(T^{path}\\), and \\(\\{0, 1, 2, 3, 4\\}\\) for \\(T^{ce}\\). Here, \\(c \\in M\\) is the class label, \\(Token\\) represents the relevant token's representation (e.g., \\(Token_{cls}\\) for [CLS] in sequence classification or \\(Token_k\\) for the k-th token in token classification), \\(h\\) is the output of the last hidden layer corresponding to the relevant token (e.g., \\(h_{cls}\\) or \\(h_k\\)), and \\(w\\) are trainable parameters. The summation is taken over all possible class labels \\(m \\in M\\).\nThe fine-tuning loss \\(L_\\text{\ubb34}\\) for each task is defined as:\n\\[L_{\\text{\ubb34}} = - \\sum_{d \\in D} \\log p(c_d | \\text{Token}_d), \\]\nwhere \\(d\\) refers to each data point and \\(D\\) refers to the dataset corresponding to each task."}, {"title": "Student Confidence Checker", "content": "\\(Model_S\\) outputs a prediction along with its corresponding probability score, denoted as \\((Score_S, P_S) \\sim Model(\\cdot | P_T)\\). The borderline case identifier evaluates \\(Score_S\\) when it is in batch \\(B\\), and determines whether it is the minimum score:\n\\[\\text{Checker}(Score) = \\begin{cases} \\text{Unconfident}, & \\text{if } Score = \\min_{i \\in B} Score_i \\\\ \\text{Confident}, & \\text{otherwise} \\end{cases} \\]\nIf \\(Score_S\\) is identified as \u201cUnconfident,\u201d the rationale \\(R_S \\sim \\text{LLM}(C, L)\\), along with \\(C\\) and \\(L\\), is then applied to the teaching template of \\(Model_T\\) for the next batch.\nThe process of DualChecker, which iteratively refines model predictions through context alignment and student-teacher interaction, is illustrated in Algorithm 1."}, {"title": "Experiments", "content": "We evaluate DualChecker using a specialized textual dataset focused on green innovation, annotated by three domain experts. The dataset comprises 10,820 entries of green patent texts sourced from the Japan Patent Office (JPO), covering the period from 2006 to 2022. This dataset supports three primary tasks:\nGreen Innovation Identification. This task aims to classify whether a given text pertains to green innovation. We achieved this by aligning all patent texts with the IPC codes listed in the Green Transformation Technologies Inventory (GXTI) standard, as provided by the JPO. The task is structured as a binary classification problem with an equal distribution of 5,410 negative and 5,410 positive samples.\nTechnological Causality Extraction. This task focuses on extracting technological phrases along with their corresponding environmental impact phrases. The dataset for this task consists of 1,000 labeled samples. The cause phrases have an average length of 42 words, with a maximum of 128 and a minimum of 4. The effect phrases exhibit average, maximum, and minimum lengths of 35, 142, and 7 words, respectively.\nEnvironmental Path Identification. This task aims to determine the environmental impact of green technologies, utilizing 1,000 labeled samples. Each sample is annotated based on the environmental pathways generated by GPT-4 Turbo and categorized into four distinct classes. These classes, ranging from 0 to 3, are summarized in Table 1.\nTo ensure a fair comparison, we maintain the original settings of the baseline models. The parameters are as follows: 1) a batch size of 8 for distillation, 64 for fine-tuning \\(T^{cls}\\), and 8 for \\(T^{path}\\) and \\(T^{ce}\\); 2) a training ratio of 0.8; 3) a learning rate of 2e-5; and 4) teacher model confidence thresholds set at 0.85 for GPT-3.5 Turbo and 0.75 for Llama 2. The experiments were conducted on 5 NVIDIA RTX A6000 GPUs. See Appendices B and C for details."}, {"title": "Results", "content": "Through experiments on benchmark datasets, we aim to address the following research questions:\nQ1: Can DualChecker effectively mitigate the hallucinations of teacher LLMs? Table 2 compares DualChecker with other baselines across both black-box and white-box LLMs in 0-shot and 5-shot settings. Although our method is specifically tailored for few-shot in-context learning, 0-shot results are included for reference. DualChecker consistently outperforms the baselines, achieving substantial F1 score improvements\u2014up to 17% in \\(T^{cls}\\) with GPT-3.5 Turbo (5-shot), 3% in \\(T^{ce}\\) with the same model, and 4% in \\(T^{path}\\) with Llama 2 (5-shot).\nThese significant gains underscore DualChecker's robust ability to reduce hallucinations across diverse tasks and models, particularly in scenarios requiring high factual accuracy and specialized domain adaptation.\nNotably, the comparison between 0-shot and 5-shot settings reveals that DualChecker enhances F1 scores by 10% to 30%, clearly demonstrating its effectiveness in addressing hallucination issues in reasoning tasks. The pronounced gains observed with GPT-3.5 Turbo highlight DualChecker's exceptional impact when applied to advanced LLMs. Moreover, the significant improvement in \\(T^{cls}\\) illustrates the model's adeptness at leveraging minimal training data to achieve accurate classification, even in training-free scenarios. This reinforces DualChecker as a powerful tool for enhancing the reliability and accuracy of teacher models in complex settings.\nQ2: How does the number of shots influence the performance of DualChecker? We evaluated DualChecker with varying numbers of shots using GPT-3.5 Turbo to assess its robustness across a range of tasks. As shown in Figure 3, the performance of DualChecker significantly improves as the number of shots increases, which aligns with its design for few-shot settings. Notably, the most substantial performance gain is observed after 3-shot, highlighting the model's ability to adapt and improve with limited labeled data. Given that only the ContextAligner component is active in the 1-shot scenario, these results also emphasize the critical role of the interaction process between models, which ensures continuous improvement as more shots are added. This adaptability and responsiveness to additional shots demonstrate DualChecker's strong potential in dynamic and evolving learning environments.\nQ3: Can DualChecker enhance the performance of student models using the predictions of teacher models? Table 3 compares the performance of student models fine-tuned with different labels. The human labels refer to ground truth annotations provided by domain experts, while the predictions from GPT-3.5 Turbo and Llama 2 are based on 5-shot scenarios using various methods. In the experiment with Patent RoBERTa, the improvement gains with GPT-predictions are 9.9%, 1.5%, and 7.3% in \\(T^{cls}\\), \\(T^{ce}\\), and \\(T^{path}\\), respectively. At the same time, the Llama-predictions yield gains of 13.1% and 8.4% in \\(T^{cls}\\) and \\(T^{path}\\). These results underscore the effectiveness of DualChecker in boosting student model performance by leveraging teacher model predictions.\nThe significant gains observed in \\(T^{cls}\\) and \\(T^{path}\\) demonstrate that DualChecker is particularly effective in tasks centered on sequence classification. However, in \\(T^{ce}\\), the improvement is more modest, likely due to the task's inherent specificity and the challenges of causality extraction. The subjectivity in the annotation rules for causality extraction makes it harder to achieve substantial gains by adding similar cases, rationales, and reflections, compared to more straightforward classification tasks. This highlights the nuanced challenges different tasks present and identifies where DualChecker can most effectively drive performance enhancements.\nQ4: Is DualChecker a robust framework for distilling LLMs in a challenging domain? To assess the robustness of DualChecker with different student models, we also experimented with a RoBERTa model trained on a general corpus as shown in Table 3. Since this model lacks comprehensive domain knowledge, its performance is relatively lower. Despite this, we still observed significant improvement gains with GPT-predictions: 4.5%, 0.3%, and 1.8% in \\(T^{cls}\\), \\(T^{ce}\\), and \\(T^{path}\\), respectively. At the same time, the Llama-predictions yielded gains of 11.8% and 8.6% in \\(T^{cls}\\) and \\(T^{path}\\). Additionally, when comparing the scores of DualChecker with human labels, our method approaches the gold standard closely, within a 5% difference, whereas other baselines exhibit more than a 10% difference. This comparable performance indicates that DualChecker can effectively leverage existing knowledge to guide LLMs and student models robustly, even when these models lack specific domain knowledge.\nQ5: Which component of DualChecker contributes the most to improvement? To understand the contribution of each DualChecker component, we conduct an ablation study using GPT-3.5 Turbo predictions under a 5-shot setting (see Table 4). \u201c-w/ContextAligner\u201d refers to using only the ContextAligner, \u201c-w/Teacher\u201d includes both the ContextAligner and the checker system in teacher model reasoning, and \u201c-w/Student\u201d consists of both in student model fine-tuning.\nThe results reveal that ContextAligner alone provides a substantial improvement, highlighting its effectiveness in aligning LLMs with human annotation standards. Surprisingly, \u201c-w/Teacher\u201d and \u201c-w/Student\u201d perform worse than ContextAligner alone. This may occur because teacher models, without feedback from student models, lose direction for further refinement. Additionally, teacher LLMs may become \u201coverconfident\u201d (Santurkar et al. 2023), leading to overly optimistic and biased predictions. The complete DualChecker system outperforms all others by integrating improvements for both teacher and student models."}, {"title": "Conclusions", "content": "In this study, we introduce DualChecker, a novel framework that effectively mitigates hallucinations in large language models during knowledge distillation. Unlike traditional approaches that rely on external knowledge or extensive training, DualChecker employs ContextAligner and an interactive checker system to align model outputs with human standards, ensuring accuracy, consistency, and reliability. Our extensive experiments in the challenging green innovation domain demonstrate that DualChecker significantly outperforms existing methods, achieving notable F1 score improvements for both teacher and student models. The framework's adaptability across black and white-box models further underscores its robustness and versatility. By open-sourcing our datasets, models, and code, we aim to foster further research and accelerate the development of more reliable and effective AI systems in complex, real-world applications."}, {"title": "Model Details", "content": "We employ GPT-3.5 Turbo with its default configuration (e.g., temperature set to 1). Detailed specifications for other pre-trained models are provided in Table 5, sourced from HuggingFace. For our patent-specific RoBERTa model, we leverage 3 million unlabeled patent texts, configuring it with a batch size of 1,904 and training it for 1 million steps. Additionally, we have developed a patent-specific Sentence-RoBERTa model by fine-tuning the base patent ROBERTa model using the JSNLI dataset 5, a Japanese adaptation of the SNLI dataset 6, comprising 533,005 training samples and 3,916 validation samples. Both models will be publicly available for broader use."}, {"title": "Experimental Settings", "content": "In the few-shot in-context learning of LLMs, we initially select examples from the test data and incorporate the rationale generated by GPT-4 Turbo along with the confidence scores labeled by experts. Our preliminary experiment found that LLMs exhibit overconfidence, ranging from 90% to 100%. To mitigate this, we set the confidence scores in the templates to range from 60% to 90%. Meanwhile, the maximum similar contexts retrieved from ContextAligner can be expressed as:\n\\[\\text{Max Similar Contexts} \\begin{cases} 0, & \\text{if } n = 0 \\\\ 1, & \\text{if } n = 1, 2, 3 \\\\ \\left[\\frac{n-1}{2}\\right], & \\text{if } n > 3 \\end{cases} \\]\nwhere n is the number of shots. This helps reduce the bias in instruction. The borderline cases identified by the student model will replace the examples in the teaching template for the next round of reasoning."}, {"title": "Template Types", "content": "The teaching templates for each task are set as shown in Table 6:"}]}