{"title": "TransMA: an explainable multi-modal deep learning model for predicting properties of ionizable lipid nanoparticles in mRNA delivery", "authors": ["Kun Wu", "Zixu Wang", "Xiulong Yang", "Yangyang Chen", "Zhenqi Han", "Jialu Zhang", "Lizhuang Liu"], "abstract": "As the primary mRNA delivery vehicles, ionizable lipid nanoparticles (LNPs) exhibit excellent safety, high transfection efficiency, and strong immune response induction. However, the screening process for LNPs is time-consuming and costly. To expedite the identification of high-transfection-efficiency mRNA drug delivery systems, we propose an explainable LNPs transfection efficiency prediction model, called TransMA. TransMA employs a multi-modal molecular structure fusion architecture, wherein the fine-grained atomic spatial relationship extractor named molecule 3D Transformer captures three-dimensional spatial features of the molecule, and the coarse-grained atomic sequence extractor named molecule Mamba captures one-dimensional molecular features. We design the mol-attention mechanism block, enabling it to align coarse and fine-grained atomic features and captures relation-ships between atomic spatial and sequential structures. TransMA achieves state-of-the-art performance in predicting transfection efficiency using the scaffold and cliff data splitting methods on the current largest LNPs dataset, including Hela and RAW cell lines. Moreover, we find that TransMA captures the relationship between subtle structural changes and significant transfection efficiency variations, providing valuable insights for LNPs design. Additionally, TransMA's predictions on external transfection efficiency data maintain a consistent order with actual transfection efficiencies, demonstrating its robust generalization capabil-ity. The code, model and data are made publicly available at https://github.com/wklix/TransMA/tree/master. We hope that high-accuracy transfection prediction models in the future can aid in LNPs design and initial screening, thereby assisting in accelerating the mRNA design process.", "sections": [{"title": "1. Introduction", "content": "mRNA-based technologies hold promise for therapeutic ap-plications in fields such as viral vaccines, protein replacement therapies, cancer immunotherapies, genome editing[1, 2, 3, 4, 5, 6], and have the potential to reshape the landscape of life science research and medicine[7, 8, 9]. However, achieving targeted delivery and intracellular release from endosomes re-mains challenging for mRNA delivery systems, emphasizing the critical demand for safe and effective mRNA delivery ma-terials. Specifically, lipid nanoparticles (LNPs) have undergone extensive investigation and have successfully transitioned into clinical use for the delivery of mRNA[10, 11, 12]. For example, mRNA1273 and BNT162b21 utilize lipid nanoparticles for the delivery of antigen mRNA[13, 14]. LNPs consist of four com-ponents: ionizable lipid, phospholipid, cholesterol, and PEGy-lated lipid. Among these, the ionizable lipid component holds the highest molar ratio, which determines the formulation's de-livery efficiency and stability, serving as the core structure of LNPs[15]. Therefore, the key to selecting for efficient LNPS lies in the selection of the ionizable lipid[16, 17].\nAlthough component chemistry methods based on three-component reactions (3-CR)[18, 19] and four-component reac-tions (4-CR)[20] can synthesize a number of ionizable lipids, manually testing the transfection efficiency of each synthesized lipid is time-consuming and costly [21, 22]. Several works have shown that artificial intelligence represented by machine learning and deep learning can achieve automatic prediction of LNPs transfection efficiency. For instance, Ding et al.[9] employed four machine learning methods\u2014support vector ma-chine, random forest, eXtreme gradient boosting, and multi-layer perceptron-to classify the transfection efficiency of 572 lipid nanoparticles (LNPs) into two categories, achieving a clas-sification accuracy of 98%. AGILE [23] is a prediction platform for LNPs transfection efficiency based on graph convolutional neural networks and pre-training, achieving a mean squared er-ror (MSE) of approximately 6 in predicting LNPs transfection efficiency. TransLNP[24] which is a model based on Trans-former architecture and data balancing achieved a MSE of ap-proximately 5 on the AGILE dataset.\nDespite significant progress in previous works, it must be acknowledged that there are notable limitations in achieving automatic prediction of LNPs transfection efficiency based on machine learning and deep learning. These limitations hinder the accuracy and generalizability of the models. These limita-tions include:(1)The lack of multi-modal information inter-action. Previous works only extract single-modal information from molecules, which limits their ability to capture Quanti-tative Structure-Activity Relationships (QSAR) between ion-izable lipids and transfection efficiency. For instance, chem-berta [25] relies on one-dimensional representations to pre-dict molecular properties. Integrating multi-modal information from molecules is crucial for further improving prediction ac-curacy. (2) Previous works lacks interpretability. Although previous works can predict transfection efficiency accurately, it is unclear which atoms play a key role in this process. This lack of interpretability restricts deep understanding of the prediction process and hampers efforts to further optimize the model for improved accuracy. Therefore, enhancing model interpretabil-ity while maintaining prediction accuracy poses a significant challenge in current research. (3)Limited attention to the im-pact of transfection cliffs. Pairs of molecules that are struc-turally very similar but have significantly different transfection efficiencies-known as transfection cliffs-capture the knowl-edge hidden in the elusive structure-property relationships but have received limited attention.\nTo address the challenges, we introduce an explainable LNPs transfection efficiency prediction model called TransMA, which adopts a multi-modal molecular structure fusion ar-chitecture. TransMA integrates 3D geometric information and 1D atomic sequence information of molecules to predict LNPs transfection efficiency. A self-attention mechanism SE(3) Transformer architecture named molecule 3D Transformer is employed to extract 3D geometric information. Molecule 3D Transformer is pre-trained by reconstructing atomic 3D coordi-nates and masked atom prediction. To extract atomic sequence information, the state space model designated as molecule Mamba is presented to obtain coarse-grained atomic sequence information. By constructing multi-level molecular structure data pairs, the model jointly learns the multi-dimensional struc-tural features of molecules. Specifically, we design the mol-attention mechanism block to align and concatenate the fu-sion features. The fusion features include atomic sequence distribution information, atomic coordinates information, rel-ative atomic positions information, and types of bonds be-tween atoms information. Compared to state-of-the-art molec-ular graph convolutional networks and Transformer models, TransMA demonstrates the best performance, reducing MSE by 43% compared to the baseline model AGILE and by 35% compared to TransLNP. Additionally, TransMA possesses in-terpretability to better understand the mapping relationship be-tween molecular structure and biochemical properties[26, 27]. The mol-attention mechanism block not only integrates multi-modal molecular features but also reveal critical sites that influ-ence transfection efficiency. Transfection cliffs represent pairs"}, {"title": "2. Materials and methods", "content": "2.1. The method of TransMA to predict LNPs transfection effi-ciency\nDeep learning is a powerful tool for accelerating research in the molecular field[28, 29]. We develop an explainable deep learning approach to predict the transfection efficiency of LNPs. This model not only achieves high-precision predictions but also elucidates the key molecular structures that the model fo-cuses on. The overall architecture of TransMA is illustrated in figure 1. The TransMA framework consists of three compo-nents: molecule 3D Transformer, molecule Mamba, and mol-attention mechanism block."}, {"title": "2.1.1. Molecule 3D Transformer", "content": "The molecule 3D Transformer is employed to extract the three-dimensional structural information features of molecules. These three-dimensional structural details include the atomic type sequences, atomic three-dimensional coordinates, Eu-clidean distance matrices between atoms, and matrices repre-senting the types of bonds between atoms in ionizable lipid molecules. TransLNP adopts a pre-training followed by fine-tuning training approach shown in figure 2A. Despite the scarcity of labeled data with LNPs properties in the LNPs field, we can leverage various molecular structure information from other small molecules to train the model. The pre-training task utilizes masked language modeling: randomly masking 15% of the atoms and introducing mask tokens. Subsequently, an embedding layer is used to map the atomic sequences to repre-sentations of atomic types, providing each atom with semantic information. Equation 1 illustrates the process of self-attention mechanism addressing interactions between atoms:\nattention = softmax(\\frac{QK}{\\sqrt{Vd}} + bias)V,   (1)\nThe masked atomic types are linearly represented as Queries (Q), Keys (K), and Values (V) in the self-attention module. Vd represents the dimensions of vectors Q, K, and V. The attention bias in the self-attention mechanism is based on representations of atomic distances and bond types.\nAfter considering the relationships between atoms, uniform noise is introduced and added to the 3D coordinates of the molecule. This process of coordinate updating effectively in-corporates the noise into the molecular representation. In the pre-training phase, atomic type heads, atomic coordinate heads, and atomic pairwise Euclidean distance heads are employed to predict masked atomic types, coordinates, and relative dis-tances, respectively. Smooth L1 is used as the loss function for predicting Euclidean distances and masked atomic coordinates, while cross-entropy is used for predicting atomic types. Dur-ing fine-tuning, the pre-trained model is loaded, following the data processing procedures from pre-training. Finally, molecu-lar prediction heads are used for transfection efficiency predic-tion."}, {"title": "2.1.2. Molecule Mamba", "content": "Mamba is a linear-time sequence modeling method based on selective state spaces [30]. Variants of Mamba have demon-strated outstanding performance in natural language process-ing, image processing, remote sensing, and audio domains[31, 32, 33, 34]. State Space Sequence Models (SSMs)[35] form the theoretical foundation of Mamba, capable of mapping a one-dimensional function or sequence u(t) to y(t) \u2208 R through a hidden state x(t). This mapping can be represented by the following linear ordinary differential equation shown in equation 2:\n$\\dot{x}(t) = Ax(t) + Bu(t), y(t) = Cx(t),$   (2)\nwhere the state matrix A \u2208 RN\u00d7N serves as the evolution param-eter and B \u2208 RN\u00d7K, C\u2208 RF\u00d7N act as projection parameters, representing the implicit latent state.\nDue to its combination of recurrent neural networks (RNNS) and convolutional neural networks (CNNs), Mamba has advan-tages in handling long sequence data. The SMILES represen-tation of ionizable lipids can be viewed as a long sequence for processing. Therefore, we first apply Mamba to the field of molecular property prediction, using one-dimensional SMILES representation of ionizable lipids as sequence inputs. Figure 2B illustrates the overall process of Molecule Mamba. Molecule Mamba involves four parameters (Delta(A), A, B, C). Firstly, the feature dimension of the SMILES representation molecule sequence is doubled through linear projection. After the split operation, the SMILES representation molecule is represented in two parts. One part undergoes one-dimensional convolution and Sliu activation function to extract molecular features. After linear projection and split operation, parameters (Delta, B, C) are obtained. Delta is updated through softplus. At the same time, parameters (A, D) are initialized, where A and D are in-dependent of the input. A is discretized using zero-order hold (ZOH) discretization, while B is discretized using a simplified Euler discretization shown in equation 3.\n$\\bar{A} = exp( \\Delta A), \\bar{B} = x + \\Delta \\cdot f(x, u),$   (3)\nCombining equation 2 with the scanning operation, the se-quential processing of elements in the SMILES representation molecule involves updating atoms at each time step based on the cumulative effect of the previous atom and the current input, effectively propagating information across the entire sequence. Equation 4 describes this process. Finally, the output is the mul-tiplication of another portion of the input with the output of the scanning operation.\nx(t) = Ax(t) + Bu(t), y(t) = Cx(t),   (4)"}, {"title": "2.1.3. Mol-attention mechanism block", "content": "The purpose of the mol-attention mechanism block is to in-tegrate the output features of the molecule 3D Transformer and molecule Mamba, and introduce an attention mechanism to ex-plain the model's attention to each atom. The output feature Z1 of the molecule 3D Transformer has a size of [n, 512], where n represents the number of atoms in the ionizable lipid. Mean-while, the output feature 22 of molecule Mamba has a size of [m, 512], where m denotes the sequence length of the SMILES representation molecule. Apart from containing atomic fea-tures, 22 also includes features of characters in the SMILES representation molecule. Before concatenating the features, all atomic features \u00bd need to be extracted from 22 to match the size of 21. Therefore each atomic feature in 22 corresponds to one atomic feature in z1, ensuring correct alignment dur-ing feature fusion. Like attention mechanisms in the image domain[36, 37, 38], the mol-attention mechanism block aims to weight each atom, compressing the fused features contain-ing global information of size n \u00d7 1024 directly into a n \u00d71 feature vector z. The first fully connected layer W\u2081 compresses the 1024 concatenated features into 1024/r channels to reduce computational complexity. Following a ReLU non-linear acti-vation layer 6, the second fully connected layer W2 produces weights attention scores through a Sigmoid activation \u03c3. The dimension of attention scores obtained is n \u00d7 1, which is shown as equation 5:\nz = concat(z1, z2), attention scores = r(g(z, W)) = \u03c3(W2d(W1z)),   (5)\nwhere r denotes the compression ratio. Finally, the scale opera-tion and a regression layer are applied to predict the transfection efficiency, which is shown as equation 6:\ntransfection efficiency = tanh(W2(tanh(W1(z\u00b7attention scores)))),   (6)\nwhere tanh denotes the tanh activation function."}, {"title": "2.2. Loss function", "content": "The loss function of the TransMA model combines the mean squared error (MSE) loss function and the triplet loss function, constructing a hybrid loss function. Equation 7 is shown as:\nhybrid loss = \\frac{1}{N} \\sum_{i=1}^{N}(yi - \\hat{y}i)^2 + B \\times Ltriplet (z1, z2),   (7)\nwhere \u00df is a weight parameter, N is the number of samples, yi and \u0177i respectively represent the true values and the predicted values, and the definitions of 21 and 22 are consistent with equa-tion 5. Equation 8 describes the triplet loss:\nLtriplet = \\frac{1}{num\\_positive\\_losses + e} \\sum_{i=1}^{2batch} \\sum_{j=1}^{2batch} \\sum_{k=1}^{2batch} max(d(z1i, Z1j) \u2013 d(z1i, Z2k) + margin, 0) \\cdot maskijk,   (8)\nwhere batch represents the batch size, z1i represents the fea-ture extracted from the molecule 3D Transformer for the ith sample, Z2 denotes the feature extracted from the molecule Mamba for the kth sample, d(.) denotes the Euclidean distance, num_positive_losses is the number of positive losses, e is a small number used for numerical stability, margin is a hyperparame-ter controlling the margin size for effective triplets. and maskijk is a triplet mask used to filter out invalid triplets.\nThe Triplet Loss first computes the Euclidean distance be-tween 21 and 22. For each sample, it constructs all possible triplets and computes the loss value for each triplet. This loss value is calculated as the difference between the distance from the anchor sample to the positive sample and the distance from the anchor sample to the negative sample, with a margin added, and then taking the maximum value. Subsequently, invalid triplets are filtered out using a triplet mask, and the average loss value is computed as the final result of the triplet Loss. Triplet loss maps the molecular 3D structural features extracted by the molecule 3D Transformer and molecule Mamba, along with the one-dimensional sequence features, to a unified em-bedding space. By learning the similarity of molecules in this embedding space, the model can better generalize to unseen molecules."}, {"title": "3. Experimental Section", "content": "3.1. Dataset\nThe dataset for TransMA predicting LNPs transfection effi-ciency comprises two main components. One part is utilized for pretraining the molecule 3D Transformer, while the other part is used for training the molecule Mamba and fine-tuning the molecule 3D Transformer. The dataset used for pretraining the molecule 3D Transformer is sourced from Unimol[43], which comprises 19 million diverse molecular structures\u00b2. The dataset utilized for training the molecule Mamba and fine-tuning the molecule 3D Transformer is obtained from AGILE\u00b3, consist-ing of 1200 ionizable lipid SMILES representation molecules along with their corresponding transfection efficiency data in Hela and RAW 264.7 cell lines.\nIonizable lipids comprises 20 head groups, 12 carbon chains with ester bonds, and 5 carbon chains with isocyanide head groups. High-throughput synthesis of LNPs entails mixing an aqueous phase containing mRNA and an ethanol phase contain-"}, {"title": "3.2. Experimental processing", "content": "The tasks of molecule 3D Transformer pretraining include using random positions as corrupted input 3D positions and training the model to predict the correct positions, employ-ing different heads to predict distances between corrupted atom pairs, the correct coordinates of corrupted atoms, and to mask and predict the atom types of corrupted atoms. Atomic type pre-diction employs the cross-entropy loss function with a weight of 1. Prediction related to atomic coordinates and interatomic distances utilizes the smooth L1 loss function with weights set to 5 and 10 respectively. Pretraining parameters are set as fol-lows: model layers are 15, batch size is 128, atom types are 30, learning rate is adjusted using linear decay, and the opti-mizer is Adam with e. Parameters for molecule 3D Transformer fine-tuning are as follows: model layers are 16, batch size is 4, epochs are set to 200, and the initial learning rate is set to le-5.\nMolecule Mamba first tokenizes 1200 ionizable lipid SMILES molecular representation. The tokenizer used is ChemBERTa-77M-MTR from Deepchem. The purpose of to-kenization is to segment SMILES molecule repren into tokens and convert these tokens into numerical representations that the model can process. The parameters of the Molecule Mamba model are set as follows: the feature dimension is 512, the num-ber of layers is 2, the vocabulary size is 100, and the training process parameters are the same as molecule 3D Transformer fine-tuning. In TransMA's hybrid loss function, the parameter \u1e9e is set to 6 during the scaffpld data splitting and 3 during the cliff data splitting."}, {"title": "3.3. Comparison with representative deep learning-based molecular property prediction models", "content": "To demonstrate the superiority of the proposed method, com-pare the predictive transfection efficiency accuracy of TransMA with advanced molecular property prediction models based on graph convolutional neural networks and Transformer on the LNPs dataset. The compared five models include AG-ILE, large-scale self-supervised pretraining for molecular prop-erty prediction model named ChemBERTa[25], self-supervised graph neural network framework named MolCLR[39], and geometry enhanced molecular representation learning model named GEM[40], TransLNP.\nThe LNPs dataset includes 1200 ionizable lipid SMILES molecules and transfection efficiency data obtained from Hela and RAW 264.7 cell lines. In model comparison, the LNPs dataset is divided into cliff[41] and scaffold data splitting ap-proaches. Figure 3 represents LNPs dataset distributions under two data splitting methods in Hela and RAW 264.7 cell lines. Scaffold splitting divides LNPs dataset into training set, vali-dation set and testing set as shown in figure 3A and figure 3D. Cliff splitting utilizes molecular ECFP descriptors to represent their structures, employing algorithms like spectral clustering to partition molecules into five clusters. Cliff splitting divides LNPs dataset into training set, validation set and testing set as shown in figure 3B and figure 3E. For each cluster, a stratified sampling strategy is employed to allocate 10% of the molecules to the testing set and other molecules to the training set and validation set in an 8:2 ratio. Cliff splitting ensures that train-ing and test sets contain the proportion of cliff molecule pairs in the training and testing sets while preserving information about molecular structural similarity. Scaffold splitting involves grouping molecules based on their core structural scaffolds, en-suring that both training and test sets contain molecules from diverse scaffold structures [42]. Cliff splitting considers the sim-ilarity of data, while scaffold splitting considers the diversity of data. Therefore, combining both cliff and scaffold data splitting methods can comprehensively validate the model's generaliza-tion capability."}, {"title": "3.4. Ablation Experiment", "content": "TransMA's ability to extract molecular features comes from both molecule 3D Transformer and molecule Mamba. To demonstrate the superiority of multi-modal feature extraction for molecules, a ablation experiment is designed: compar-ing the accuracy of predicting transfection efficiency between TransMA, molecule 3D Transformer, and molecule Mamba under scaffold and cliff data splitting methods in Hela and RAW 264.7 cell lines. In the ablation experiment, TransMA, molecule 3D Transformer, and molecule Mamba ensure con-sistency in training set and testing set, as well as parame-ter settings. Table 2 presents the performance of TransMA,"}, {"title": "3.5. Analysis of model interpretability", "content": "In this section, the interpretability of the model is analyzed to verify whether TransMA can identify the key atoms in ioniz-able lipids that affect transfection efficiency. First, identifying the locations of key atoms in ionizable lipids is fundamental for interpretability analysis. Therefore, constructing transfec-tion cliffs is employed to pinpoint key atoms. Concurrently, the mol-attention mechanism block calculates the influence scores of all atoms in ionizable lipids on transfection efficiency. If the high-scoring atoms identified by the mol-attention mecha-nism align with the key atoms found through the construction of"}, {"title": "3.5.1. Transfection cliffs", "content": "Transfection cliff molecular pairs refer to ionizable lipid molecules that are structurally very similar but have signifi-cantly different transfection efficiencies. The phenomenon of transfection cliffs exists in LNP datasets and can be observed through UMAP plots where points that are close in space ex-hibit large differences in mapped colors. Despite the struc-tural similarity of transfection cliff molecular pairs, there are differences in their atomic compositions, which lead to signifi-cant differences in transfection efficiency. Therefore, the atoms that differ between the structures of transfection cliff molecu-lar pairs are considered key atoms affecting transfection effi-ciency. To quantify the similarity of ionizable lipid molecules, similarity scores based on substructure similarity, scaffold sim-ilarity, and SMILES string similarity are constructed[44]. Sub-structure similarity and scaffold similarity are determined by calculating the Tanimoto coefficient of the Extended Connec-tivity Fingerprints (ECFP)[45, 46] and the Molecular ACCess System (MACCS) keys[47, 48] for the molecular graph frame-works, respectively. SMILES string similarity is assessed by calculating the Levenshtein distance[49] between the SMILES string representations of the molecules. Equation 9 calculates the Tanimoto coefficient shown as:\nTanimoto coefficient = \\frac{C}{a+b-c},   (9)\nFor substructure similarity, a counts the number of set bits (1s) in molecule A's ECFP fingerprint, b does the same for molecule B, and c counts the number of common set bits in both finger-prints. For scaffold similarity, a counts set bits in molecule A's MACCS keys, b does the same for molecule B, and c counts common set bits in both molecules' MACCS keys. Equation 10 computes SMILES molecular similarity shown as:\nSMILES string similarity = 1- \\frac{d(s1, s2)}{max(|s1|, |S2|)},   (10)\nwhere d(s1, s2) is the Levenshtein distance between the SMILES strings of two molecules.\nSimilar molecule pairs are defined as pairs with a structural similarity greater than 0.9. Equation 11 describes the structural similarity, which is the average of substructure similarity, scaf-fold similarity, and SMILES string similarity shown as:\nstructure similarity = \\frac{subs + scas + smis}{3},   (11)\nwhere subs represents substructure similarity, scas represents scaffold similarity, and smis represents SMILES string similar-ity. Since the transfection efficiency in the LNPs dataset has been log2-transformed, similar molecule pairs are identified as transfection cliff pairs when the transfection difference exceeds 1. Equation 12 describes the transfection difference shown as:\ntransfection efficiencies = |log10(2m2\u2212m1)|,   (12)\nwhere m\u2081 and m2 represent the transfection efficiencies of the two molecules in the similar pair. A total of 4267 and 2104 transfection cliff pairs are identified in the Hela and RAW 264.7 cell lines, respectively. Figure 6 shows the scatter plots of struc-tural similarity and transfection difference for transfection cliff pairs in the Hela and RAW 264.7 cell lines. From the figure, it can be seen that when the structural similarity of the molecules is greater than 0.9, the transfection difference is significant, reaching up to a thousand-fold or even ten thousand-fold dif-ference."}, {"title": "3.5.2. Model interpretability", "content": "The transfection cliff phenomenon can help identify key atoms in ionizable lipids that affect transfection efficiency. Fig-"}, {"title": "3.6. External test", "content": "While TransMA achieves accurate predictions of transfec-tion efficiency on the LNPs dataset, we aim to evaluate its generalization capability by testing it on an external dataset without additional training. We compile an external dataset based on published studies, consisting of 15 LNPs with ion-izable lipids as the only variable and their transfection effi-ciency levels. Among these, 11 ionizable lipids which is from different combinations of 6 head groups, 1 ester linkage, and 5 hydrophobic tails shown in figure 8 and their transfection efficiencies[50] are obtained from Balb/c mice using a fixed LNP formulation. The remaining 4 ionizable lipids and their transfection efficiencies [51, 52, 53] are derived from different cell lines. Since the TransMA model is also trained on differ-ent cell lines, it possesses the capability to predict transfection efficiency across various cell lines.\nFigure 9 shows the results of TransMA directly testing on an external dataset without training. The predicted maximum and minimum transfection efficiencies of lipids 1-11 by TransMA are consistent with the true values, and the order of predicted values is similar to that of true values. For lipids 12 and 13, the prediction error is less than 1. As for lipids 14 and 15, the au-thors only provided the transfection efficiency of lipid 14 being lower than that of lipid 15 during collection, which is consistent with the prediction result. TransMA achieves good predictions of transfection efficiency even without training on the external dataset, demonstrating its strong generalization capability."}, {"title": "4. Discussion and conclusion", "content": "In this work, we propose an explainable high-accuracy LNPs transfection efficiency prediction model called TransMA. \u03a4\u03bf achieve high-accuracy predictions, TransMA employs a multi-modal molecular structure fusion architecture, where the sub-structure molecule 3D Transformer extracts three-dimensional spatial features of the molecule, and molecule mamba extracts one-dimensional molecular features. To achieve model in-terpretability and feature fusion, we design the mol-attention mechanism block. This block can integrate multi-dimensional features and reveal atom-level structure-transfection relation-ships based on the molecular channel attention mechanism. Compared with advanced molecular graph convolutional net-works and Transformer models, TransMA achieves the highest accuracy in predicting transfection efficiency under the scaffold and cliff data splitting methods in Hela and RAW 264.7 cell lines. Moreover, we introduce transfection cliff pairs. The re-sults demonstrate that the atoms with high attention scores com-puted by the mol-attention mechanism block correspond to the key atoms in the transfection cliff pairs, showing that TransMA can identify the key atoms affecting transfection efficiency. Ad-ditionally, the external test results indicate that even on the un-trained external dataset, TransMA's predicted values maintain a consistent order with the actual transfection efficiencies.\nAlthough TransMA has demonstrated excellent performance in predicting transfection efficiency, there are still limitations in prediction accuracy due to the scarcity of LNPs datasets. Addi-tionally, since ionizable lipids are composed mainly of head and tail groups, different ionizable lipids tend to have high struc-tural similarity, making the occurrence of transfection cliffs more likely. These transfection cliffs increase the difficulty for TransMA to capture the mapping relationship between molecu-lar structure and transfection efficiency. Therefore, future work should focus on addressing the transfection cliff phenomenon to further improve prediction accuracy."}]}