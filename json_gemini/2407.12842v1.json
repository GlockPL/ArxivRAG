{"title": "MS2SL: Multimodal Spoken Data-Driven Continuous Sign Language Production", "authors": ["Jian Ma", "Wenguan Wang", "Yi Yang", "Feng Zheng"], "abstract": "Sign language understanding has made significant strides; however, there is still no viable solution for generating sign sequences directly from entire spoken content, e.g., text or speech. In this paper, we propose a unified framework for continuous sign language production, easing communication between sign and non-sign language users. In particular, a sequence diffusion model, utilizing embeddings extracted from text or speech, is crafted to generate sign predictions step by step. Moreover, by creating a joint embedding space for text, audio, and sign, we bind these modalities and leverage the semantic consistency among them to provide informative feedback for the model training. This embedding-consistency learning strategy minimizes the reliance on sign triplets and ensures continuous model refinement, even with a missing audio modality. Experiments on How2Sign and PHOENIX14T datasets demonstrate that our model achieves competitive performance in sign language production.", "sections": [{"title": "1 Introduction", "content": "Sign language, a visual language, combines both manual (hand gestures) and non-manual cues for communication. It is specifically designed for the deaf and hearing-impaired community (Hickok et al., 1996; Armstrong and Wilcox, 2003; Campbell et al., 2008; Zhou et al., 2020). According to the World Federation of the Deaf, there are 70 million deaf people and more than 200 kinds of sign languages in the world (Fenlon and Wilkinson, 2015; N\u00fa\u00f1ez-Marcos et al., 2023). Improvements in sign language production (SLP) can bridge the communication gap between the deaf and hearing (Mehdi and Khan, 2002; Harris et al., 2009; Taskiran et al., 2018; Rastgoo et al., 2021; Kahlon and Singh, 2023; Luo and Yang, 2024).\nThe challenges primarily arise from phonological difference and data scarcity. Phonological difference: signs are composed of various manual and non-manual features (Mann et al., 2010), such as hand gestures, facial expressions and limb movements (Liddell and Johnson, 1989; Johnson and Liddell, 2011; Sandler, 2012). The differences in phonological structure and means of expression create challenges in modeling the two languages. Data scarcity: multimodal high-quality sign language datasets are relatively scarce, and some datasets tend to be specific to a particular language or domain, e.g., American sign (Duarte et al., 2021), German weather (Forster et al., 2014; Camg\u00f6z et al., 2018). Furthermore, hearing impairments hinder pronunciation (Moeller, 2000; Yoshinaga-Itano, 2003), making it strenuous to collect sign video with aligned audio and usually resulting in the lack of auditory information. Previous researches (Zhang et al.; Camg\u00f6z et al., 2017; Hu et al., 2021b,a; Yin et al., 2022) primarily focused on sign language recognition, which identifies sign fragments as the corresponding sign language lexicons (e.g., gloss). Several work (Saunders et al., 2020, 2021a, 2022; Hwang et al., 2021; Walsh et al., 2022) manage the transition from gloss to sign sequences, yet the grammar of gloss can be perplexing for those without sign language training. Saunders et al. (2020, 2021b) can transcribe discrete words or phrases into continuous sign language sequences. However, directly producing continuous signs from entire spoken sentences still remains more exploration and efforts.\nTo promote barrier-free communication between signers and speakers, we introduce a Multimodal Spoken Data-Driven Continuous Sign Language Production (MS2SL) framework (Fig. 1). MS2SL can animate sign keypoint sequences from either speech audio or text. In addition, to alleviate data demands, we adopt an embedding-consistency learning (ECL) strategy, which is inherently based on the reciprocity among modalities, to bolster the model training. Specifically, MS2SL initially employs pre-training models like CLIP (text) (Radford et al., 2021) and HuBERT (audio) (Hsu et al., 2021) to extract features from input. Subsequently, we utilize these features, serving as control conditions for the diffusion, to generate sign sequences. The attention mechanism (Vaswani et al., 2017) is employed to model the relationships among conditions, denoising steps, and sign movements. Besides that, ECL does not require the three modalities to coexist in the dataset. By learning a joint embedding space, inspired by ImageBind (Girdhar et al., 2023), ECL tightly binds the properties of different modalities and generates feedback signals to boost the training process. First, we utilize contrastive learning to bind audio and text in the embedding space. Then, we leverage the semantic consistency between co-occurring data to infer and reconstruct the embedding of missing modalitiy. The reconstruction error between the generated signs and groundtruth can be used to iteratively update MS2SL until convergence. ECL can foster cross-learning between different generation streams, allowing training even in the absence of certain modality. Furthermore, the inclusion of audio data not only enriches sample diversity and enhances multimodal comprehension but also assists in accurately capturing the expression and semantic content of sign language. We validate the effectiveness of our method across two prevalent datasets How2Sign (Duarte et al., 2021) and PHOENIX14T (Camg\u00f6z et al., 2018). Experimental results demonstrate that MS2SL achieves SOTA performance, both in terms of semantic consistency and sign accuracy. In conclusion, our primary contributions are outlined as follows:\n\u2022 We propose MS2SL, a unified diffusion framework for efficient multimodal spoken to sign language production. MS2SL is able to directly convert entire speech or text sentences into corresponding sign keypoints sequences.\n\u2022 We present an ECL strategy that leverages the intrinsic relations to enhance data utilization.\n\u2022 We show that joint embedding is suitable for generative tasks that are prone to modality missing."}, {"title": "2 Related Work", "content": "Sign Language Understanding. Similar to spoken language, sign language follows specific linguistic rules (Sandler and Lillo-Martin, 2006; Brentari, 2011; Petitto et al., 2016; Sandler, 2017). Existing researches are primarily dedicated to sign language translation (SLT) and recognition. SLT typically involves translating sign language into spoken language (Camg\u00f6z et al., 2018; Coster et al., 2022; Camg\u00f6z et al., 2020; Lin et al., 2023). Sign language recognition (Adaloglou et al., 2022; Selvaraj et al., 2022) means interpreting and classifying of body movements in videos, covering isolated (Imashev et al., 2020) and continuous signs (Cui et al., 2017; Camg\u00f6z et al., 2018, 2020). SLP (Arkushin et al., 2023) is the process of creating sign sequences from spoken text, and can be seen as the reverse process of SLT. These existing studies on SLT and SLP primarily focus on converting between sign videos and discrete glosses, either directly or indirectly. A few of Text2Sign works (Saunders et al., 2020, 2021a,b) are grounded in datasets with relatively homogeneous scenario (Camg\u00f6z et al., 2018) and discrete spoken transcriptions.\nDiffusion Model. The diffusion model demonstrates exceptional proficiency in various generative tasks (Ho et al., 2020; Choi et al., 2021; Lugmayr et al., 2022; Avrahami et al., 2022). Beyond image generation, diffusion models also perform well in generating sequence data (Yuan et al., 2022; Wu et al., 2023). In recent years, some work has begun to apply diffusion models to SLP. By iteratively updating information, diffusion models can gradually infer the distribution of subsequent data, thereby providing more accurate and coherent results. Ham2Pose (Arkushin et al., 2023) leverages diffusion to animate HamNoSys, a lexicon of sign symbols, into sign keypoint sequences. Though impressive, Ham2Pose can only produce videos with a single sign symbol, falling short in conveying sentences with complete semantics.\nCross-modal Consistency Learning. Deep learning often requires ample labeled data to work properly. However, the cost of collecting sign data is prohibitive and audio data is often lacking. Recent methods enhance model training by applying consistency training to massive unlabeled data (Bachman et al., 2014; Sajjadi et al., 2016; Clark et al., 2018; Miyato et al., 2019). The principle of consistency learning, employing the cyclical duality between different tasks or data as feedback signals to regularize training (He et al., 2016), has its roots in the domain of language translation (Yi et al., 2017; Lu et al., 2017; Zhao et al., 2020). It primarily encompasses inter-task (dual-learning) and intra-task (cycle-consistency learning) varieties. Dual-learning simultaneously trains bidirectional mapping functions between tasks, creating a primal-dual pair where one function\u2019s output approximates the input of the inverse function (Yi et al., 2017; Wang et al., 2022; Zhang et al., 2018; Shah et al., 2019; Wang et al., 2019; Zhao et al., 2020; Xie et al., 2020). Cycle-consistency learning is designed to enhance the self-reconstruction capabilities of samples produced intrinsically by the same model (Zhu et al., 2017; Almahairi et al., 2018; Rao et al., 2020; Mathew et al., 2020). However, these methods frequently emphasize the duality between two tasks or modalities, overlooking the interplay and mutual influence among multimodal data within the same task.\nLimited studies focus on directly generating sign language sequences from entire spoken sentences. To our best knowledge, we are the pioneers in effecting this conversion. This study harnesses sequential diffusion models to incrementally generate noise predictions, enabling cross-modal sign language generation. With the help of ECL, MS2SL can generate various feedback signals even in the absence of co-occurring ternary data: assessing the reconstruction loss with the signs generated from the reconstructed audio embeddings."}, {"title": "3 Method", "content": "Assuming the triplets (A, T, S) represent the audio, text, and sign space respectively, our goal is to learn the mapping from text or audio to sign within a unified framework (Fig. 2). Given a training dataset D = {(a, t, s) \u2208 A \u00d7T \u00d7S}, MS2SL can realize text-to-sign T\u2194S : s = G(t) and audio-to-sign A\u2194S : s = G(a), where G is the sign sequence diffusion generator. We initially employ pretrained models CLIP (Radford et al., 2021) and HuBERT (Hsu et al., 2021) to extract features from text t and audio a. Next, we employ three encoders Ea, Et, Es to encode these features, acquiring their embeddings ea, et, and es. Subsequently, drawing on the operating mechanism of diffusion models, we employ a diffusion step encoder Eh and a sign noise encoder En to encode step h and noise n to eh and en, respectively. Finally, we utilize the generator G to produce the sign sequences: \n$\\widehat{st} = G(et, eh, en)$ and $\\widehat{sa} = G(ea, eh, en)$.\nThe paucity of co-occurring triplet data renders the direct training of MS2SL a formidable task. To overcome this challenge, we develop a joint embedding space that facilitates the natural alignment of multimodal data. Furthermore, we employ ECL strategy to exploit the reciprocity among modalities within the embedding space, effectively furnishing feedback signals to boost the training."}, {"title": "3.1 Sign Predictor", "content": "Cross-linguistic Modeling. MS2SL aims to solve the problem of generating variable-length sequences across modalities. It necessitates phonological modeling between spoken and sign language, associating text and audio to the same target sign sequence. The causal attention mechanism can serve as a potent remedy for this challenging issue. Taking text-to-sign as an example, we first concatenate the embeddings of text et, denoising step eh and noise en. Next, we apply the causal self-attention (Radford et al., 2018) to model the relationship among them. The mask in causal attention ensures that the model only processes past and present information, maintaining temporal and logical coherence in the output. As such, the output is computed as: CausalAtt[et; en; en]. During inference, we initiate from the text embedding and produce indices autoregressively, ceasing generation when the model predicts the sequences. Likewise, the concatenated entity of the audio ea, step eh, and noise en can also undergo the causal attention to capture the relationship between audio and sign. In causal attention, we adopt the common practice of positional encoding, which can model keypoints and inter-frame context while capturing cross-modal relations. Thus, to simplify the model structure, we does not explicitly design a temporal module. Finally, we employ two fully connected layers to output the sign prediction $\\widehat{sh}$ for step h.\nSign Language Production. We apply a diffusion model as the sign generator. Similarly, taking text-to-sign as an example, the diffusion generator G is responsible for the gradually producing a continuous sign sequence $\\widehat{s}$. Diffusion generator G simulates data distribution through a gradual forward and reversible process (Ho et al., 2020), training by maximizing the evidence lower bound to approximate target distributions. Diffusion model aims to reconstruct the input from a latent variable. The forward process gradually transforms the input into noise by adding Gaussian noise. The reverse process starts from random noise and progressively removes the noise to recover the original data.\nCommon training for diffusion models involves independent noise prediction at each forward step h, potentially reducing sequence coherence and consistency. Following (Arkushin et al., 2023), we adopt the holistic training method. We apply a schedule function \u03b4\u03b7 = 1/log(h + 1) (\u03b4 \u2208 [0, 1]) and a step size \u03b1\u03b7 = \u03b4\u03b7 \u2013 \u03b4\u03b7+1. The predicted signs sh at step h, as:\n$\\widehat{s_h} = \\alpha_h p_h + (1 - \\alpha_h)s_{h-1},$ \nwhere the predicted signs ph at step h are given as G(t). This method utilizes the output from the previous iteration as the input for the subsequent step, gradually reducing the step size as the process continues. Each step combines previous outcomes with current predictions, reducing reliance on the initial noise. We also enhance training robustness by introducing a random noise to sh at each step. Finally, the predicted initial sign $\\widehat{s_0}$ is outputted. The loss of the diffusion is defined as:\n$L_d = \\alpha_h s_0 + (1 - \\alpha_h)s_{h+1}.$ "}, {"title": "3.2 Modality Binding", "content": "MS2SL operates in an aligned embedding space, typically dependent on audio, text, and sign data for tri-modal alignment. However, the difficulty for people with hearing impairments to perceive sound variations poses a challenge in recording these co-occurring triplets. Fortunately, ImageBind (Girdhar et al., 2023) reveals that a model can learn to align modalities in a joint embedding space by employing contrastive learning (Hadsell et al., 2006). Training with (Image, Modality1) and (Image, Modality2) pairs can lead to a spontaneous alignment of Modality1 and Modality2 in embedding space. This alignment allows the model to excel in various tasks without requiring direct training on specific pairs of (Modality1, Modality2).\nWe extend the findings of ImageBind and construct a joint embedding space for the triplet dataset (A, T, S), where MS2SL employs (text, sign) pairs as anchors to establish a cohesive space linking audio, text, and sign. Let's explore a pair of modalities (T, S) with aligned observations. Given a sign sequence s and its corresponding caption t. We first employ pretrained models CLIP (Radford et al., 2021) to extract textual features and encode them into normalized embeddings: et and es. Then, we leverages the paired modalities (T,S) to align the text with sign. The corresponding encoders are optimized by InfoNCE (Oord et al., 2018) loss LT, S:\n$L_{T, s} = -log\\frac{exp(sim(e_t, e_s)/\\tau)}{\\sum_{m=1}^M exp(sim(e_t, e_{sm})/\\tau)}$\nWithin the mini-batch, we consider each instance, whose index is not equal to m, as a negative example. This approach aims to draw different embedding pairs closer within their joint embedding space. Similarly, we can also obtain LA, s and LT, A for the pairs (A, S) and (T, A). Interestingly, we also observe the emergent alignment between modal pairs (T, A) in our embedding space. This phenomenon can occur when the training is solely based on pairs (T, S) and (A, S), a trend that mirrors the findings reported in (Girdhar et al., 2023). Accordingly, MS2SL is designed to mainly leverage modal pairs (T, S) and (T, A), circumventing the need for triplet data. In practice, this is achieved by employing a triadic loss:\n$L_{nce} = L_{T, S} + L_{T, A} + L_{A, S}.$"}, {"title": "3.3 Embedding-consistency Learning", "content": "Given a tuple (A,T, S), we employ a cyclic approach with the bound joint embedding to generate feedback signals for bidirectional cross-learning, fostering model training. When triplet data is available, the encoders first extract features from their respective modalities. Then, audio and text independently generate predicted sign language sequences $\\widehat{s_a}$ and $\\widehat{s_t}$. To fully utilize real data, we calculate ECL loss after 500 epochs of model training. The vanilla model, built on authentic data, guarantees minimal distribution differences between generated pseudo-embeddings and the original dataset. Semantic consistency is calculated using the embeddings $\\widehat{et}$ and $\\widehat{ea}$ from encoder Es, which encodes the two predicted sequences. We can obtain the text-to-sign error \u25b3($\\widehat{et}$, es) and the audio-to-sign loss \u25b3($\\widehat{ea}$, es):\n\u25b3($\\widehat{et}$, es) = ||$\\widehat{et}$, es||2,\n\u25b3($\\widehat{ea}$, es) = ||$\\widehat{ea}$, es ||2.\nEvaluation scores are derived from comparing the two embeddings $\\widehat{et}$ and $\\widehat{ea}$. Both audio and text can receive feedback signals from the generative streams of each other. To compensate for the missing audio modality and ensure smooth processing, we use a mapping network M and text embeddings to generate pseudo audio features. The operation is conducted in the embedding space, thus minimally affecting inference speed. For unpaired natural audios U, we can get the formula:\n$L(T, A, S) =||E_s(G(e_a)) - E_s(G(e_t))||_2$,\n$L(T', S') =||E_s(M(G(e_t))) - E_s(G(e_u))||_2$.\nThen our ECL loss is defined as:\n$L_{ecl}=L(T, A, S) \u2208 D+ L(T', S')\u2208 U.$\nMS2SL translates entire spoken sentences into continuous sign language sequences. Overall, our total loss comprises three components, i.e., the diffusion model loss, ECL loss, and joint embedding loss:\n$L = \\lambda_1 L_d + \\lambda_2 L_{ecl} + \\lambda_3 L_{nce},$\nwhere the cofficients are empirically set as \u03bb\u2081 = \u03bb2 = \u03bb3 = 1."}, {"title": "3.4 Implementation Details", "content": "Training. MS2SL takes speech audio or text as inputs. We utilize pre-trained models for encoding both speech and text, HuBert (Hsu et al., 2021) for speech and CLIP (Radford et al., 2021) for text. We first extract embeddings et, ea, es, eh, en through five encoders. We employ keypoints to represent signs, like the 137 human keypoints in How2Sign (Duarte et al., 2021), which are normalized and standardized before being input into the model. et, ea and es participate in learning the joint embedding space. Concurrently, et, ea, eh and en serve as conditions to control the generation of text-to-sign and audio-to-sign, respectively. Here, we adopt the common practice (Saunders et al., 2021a,b; Arkushin et al., 2023) of using the first sign pose as initial noise. The first 500 epochs skip the audio-to-sign generation flow in the absence of audio. After obtaining a vanilla model, we apply the mapping network M to transform et into ea to continue the training until the model converges. Since PHOENIX (Forster et al., 2014) dataset is in German sign language, and our pretrained model is primarily based on English, we utilize the penultimate layer features of CLIP along with MLP to align and transform between German and English. As for ECL, we incorporate cycles among the three modalities, namely audio-to-sign, text-to-sign, and audio-to-text, greatly enhancing the efficiency of data utilization. We adopt the commonly used exponential moving average (Cai et al., 2021) strategy with diffusion parameters (Cai et al., 2021) to ensure smoother, more robust training. For details, please refer to the supplementary.\nInference. The model can perform SLP from audio or text independently. Inference for each modality involves executing the sequence sampling of the diffusion model. Taking text-to-sign as an example, the process starts with CLIP encoding the text into features. These text features are then fed into the sign predictor, which sequentially generates a sequence noise prediction. The completion of this sampling process results in the generation of the desired sign sequence. The process for generating signs from speech is similar. We take the average of twenty generations to mitigate deviation.\nReproducibility. Our method is implemented using PyTorch on 2 RTX 4090 GPUs, with a training time of about 12 hours and an average inference time of 0.3 seconds. Following (Zhang et al., 2023), we remove data with word count exceeding 20."}, {"title": "5 Conclusion", "content": "We explore a unified framework that combines diffusion and pretrained models to generate sign language from spoken depictions. We surpass other competitors and solidify this classic framework as a highly competitive method for SLP. MS2SL effectively handles diverse modalities of data for analysis and decoupling. Despite its advancements, our model struggles with maintaining contextual flow in generation, and MS2SL cannot handle lengthy data, which is a future focus. Our research pioneers direct sign language generation from speech, offering some insights to advance the community."}, {"title": "Limitations", "content": "Despite significant advancements, our method still faces key technical limitations. First, the complexity and fluidity of authentic sign language are challenging to fully capture and reproduce, as it involves not just hand movements but also facial expressions, body language, and the speed of gestures. Moreover, converting text or speech into sign language involves complex natural language processing challenges, especially in handling grammar and semantics. Lastly, MS2SL struggles to effectively generate long sequences of key movements, limiting the coherence and completeness of sign language expression. These limitations indicate that, while the potential of sign language generation technology is immense, significant technical barriers still need to be overcome to achieve comprehensive and precise sign language communication. These are also the directions we are committed to addressing in the future."}]}