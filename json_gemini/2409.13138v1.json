{"title": "Learning to Compare Hardware Designs for High-Level Synthesis", "authors": ["Yunsheng Bai", "Atefeh Sohrabizadeh", "Zijian Ding", "Rongjian Liang", "Weikai Li", "Ding Wang", "Haoxing Ren", "Yizhou Sun", "Jason Cong"], "abstract": "High-level synthesis (HLS) is an automated design process that transforms high-level code into optimized hardware designs, enabling the rapid development of efficient hardware accelerators for various applications such as image processing, machine learning, and signal processing. To achieve optimal performance, HLS tools rely on pragmas, which are directives inserted into the source code to guide the synthesis process, and these pragmas can have various settings and values that significantly impact the resulting hardware design. State-of-the-art ML-based HLS methods, such as HARP, first train a deep learning model, typically based on graph neural networks (GNNs) applied to graph-based representations of the source code and its pragmas. They then perform design space exploration (DSE) to explore the pragma design space, rank candidate designs using the trained model, and return the top designs as the final designs. However, traditional DSE methods face challenges due to the highly nonlinear relationship between pragma settings and performance metrics, along with complex interactions between pragmas that affect performance in non-obvious ways.\nTo address these challenges, we propose \u0441\u043eMPAREXPLORE, a novel approach that learns to compare hardware designs for effective HLS optimization. COMPAREXPLORE introduces a hybrid loss function that combines pairwise preference learning with pointwise performance prediction, enabling the model to capture both relative preferences and absolute performance values. Moreover, we introduce a novel NODE DIFFERENCE ATTENTION module that focuses on the most informative differences between designs, enhancing the model's ability to identify critical pragmas impacting performance. COMPAREXPLORE adopts a two-stage DSE approach, where a pointwise prediction model is used for the initial design pruning, followed by a pairwise comparison stage for precise performance verification. Experimental results demonstrate that COMPAREXPLORE achieves significant improvements in ranking metrics and generates high-quality HLS results for the selected designs, outperforming the existing state-of-the-art method.", "sections": [{"title": "1 INTRODUCTION", "content": "High-Level Synthesis (HLS) has emerged as a transformative technology in the realm of hardware design, bridging the gap between high-level software abstractions and efficient hardware implementations. Tools such as Xilinx's Vitis HLS [2] automate the translation of high-level code into optimized hardware designs, enabling rapid development of specialized accelerators for image processing, machine learning, signal processing [6-8, 20], etc.\nCentral to the HLS design flow is the concept of pragmas-directives embedded within the high-level code that guide the synthesis process, which heavily affects the effectiveness of HLS in producing high-quality designs. However, the relationship between pragma settings and performance metrics is highly nonlinear, with intricate interactions and dependencies that are difficult to predict or reason about. Traditional design space exploration (DSE) methods, which rely on heuristics and iterative synthesis, often fall short in efficiently identifying optimal configurations [41].\nTo address these challenges, researchers have turned to machine learning (ML) techniques to aid in the DSE process. State-of-the-art ML-based HLS methods, such as GNN-DSE [28] and HARP [29], utilize deep learning models to guide the DSE process for high-quality pragma configurations. These approaches typically involve two key steps: (1) training a predictive model, often based on graph neural networks (GNNs) [15, 39], to learn the mapping between designs with varying pragma settings and performance metrics, and (2) performing DSE using the trained model to rank and select the most promising candidate designs.\nWhile ML-based methods have shown promise in improving the efficiency and effectiveness of HLS, they still face limitations in capturing the complex relationships and interactions within the pragma design space. We hypothesize that the highly nonlinear nature of the design space, coupled with the intricate dependencies between pragmas, poses challenges for accurate performance prediction and the ranking of candidate designs. Our experiments in Section 4 provide further evidence of this issue.\nWe hypothesize that comparing a pair of designs and predicting which design is better may an easier task compared with accurately predicting the design quality. In this paper, we propose COMPAREXPLORE, a novel approach to DSE in HLS that leverages the power of comparative learning, a paradigm where models are trained to discern relative preferences between data points [5], to navigate the pragma design space effectively. COMPAREXPLORE introduces a hybrid loss function that combines pairwise preference learning with pointwise performance prediction, enabling the model to capture both relative preferences and absolute performance values. By learning to compare designs based on their pragma settings and performance characteristics, COMPAREXPLORE can effectively identify the most promising configurations and guide DSE towards optimal hardware designs.\nMoreover, we introduce a novel NODE DIFFERENCE ATTENTION module that focuses on the most informative differences between designs. This attention mechanism allows COMPAREXPLORE to prioritize the pragma settings that have the greatest impact on performance, enhancing the model's ability to make accurate comparisons and identify critical design choices.\nTo balance exploration and exploitation in the DSE process, COMPAREXPLORE uses a two-stage approach. In the first stage, a pointwise prediction model is used to explore and efficiently prune the design space, identifying a subset of promising candidates. This stage leverages the model's ability to estimate absolute performance values and quickly eliminate suboptimal designs. In the second stage, a pairwise comparison model is leveraged to perform precise performance verification and rank the remaining candidates based on their relative performance. This stage takes advantage of the model's comparative learning capabilities to make nuanced distinctions between designs and select the top-performing configurations.\nWe evaluate cOMPAREXPLORE on a comprehensive set of HLS kernels and demonstrate its effectiveness in improving the quality of the generated hardware designs. Experimental results show that COMPAREXPLORE achieves significant improvements in ranking metrics, compared to existing state-of-the-art methods. Moreover, the designs selected by COMPAREXPLORE consistently outperform those obtained through baseline approaches.\nThe main contributions of this paper are as follows: We propose COMPAREXPLORE, a two-stage approach to DSE in HLS that leverages comparative learning to effectively navigate the pragma design space and identify high-quality hardware designs.\n\u2022 We introduce a hybrid loss function that combines pairwise preference learning with pointwise performance prediction, enabling COMPAREXPLORE to capture both relative preferences and absolute performance values.\n\u2022 We present a novel NODE DIFFERENCE ATTENTION module that focuses on the most informative differences between designs, enhancing the model's ability to identify critical pragma settings and make accurate comparisons.\n\u2022 We propose a two-stage DSE approach that balances exploration and exploitation, using a pointwise prediction model for efficient design pruning and a pairwise comparison model for precise performance verification inspired by Ranked Choice Voting (RCV) [1, 23].\n\u2022 We conduct extensive experiments on a diverse set of HLS benchmarks and demonstrate the superiority of COMPAREXPLORE over HARP in terms of ranking metrics and the quality of the generated hardware designs."}, {"title": "2 RELATED WORK", "content": "Traditional ML algorithms like random forests and linear regression have been used to model HLS tools [19]. However, recent studies demonstrate that GNNs significantly improve accuracy in various HLS tasks [28, 36\u201338].\nLearning algorithms have been applied to accelerate HLS DSE for finding Pareto-optimal designs [36]. In contrast to traditional heuristics-based approaches [30], these methods employ data-driven techniques. IronMan [36], for instance, trains a reinforcement learning agent to optimize resource allocation."}, {"title": "2.2 Pairwise Comparison in ML", "content": "Pairwise comparison has a broad range of applications in machine learning beyond its traditional uses in ranking items [21, 26, 35]. In fields such as information retrieval [16] and recommender systems [25], pairwise methods have proven effective for sorting and prioritizing items based on user preferences [5].\nMetric Learning In metric learning, pairwise comparisons are used to learn meaningful distances between items. Techniques such as contrastive loss and triplet loss are used to learn a distance metric in which similar items are closer [13, 18].\nPreference Learning Pairwise comparison is also central to preference learning [10], where the goal is to learn a model that predicts preferences between items based on observed pairwise comparisons.\nNatural Language Processing. Pairwise comparison methods are crucial for tasks such as sentence similarity [32, 34], evaluating machine translation [11], and aligning Large Language Models with human preferences [22, 24, 31]."}, {"title": "2.3 Pairwise Comparison in Electronic Design\nAutomation", "content": "In the context of hardware optimization, comparative learning and ranking approaches have been explored in the quantum computing domain for optimizing circuit layouts [12], which demonstrates a machine learning based method that ranks logically equivalent quantum circuits based on their expected performance, leading to improvements in reducing noise and increasing fidelity. To the best of our knowledge, we are among the first to adopt the pairwise comparison paradigm in ML-based HLS."}, {"title": "3 METHODOLOGY", "content": "In this section, we introduce our proposed model, COMPAREXPLORE, for effective design space exploration in high-level synthesis. COMPAREXPLORE is a novel comparative learning framework that combines pointwise prediction and pairwise comparison models to efficiently navigate the pragma design space and identify high-quality design configurations."}, {"title": "3.2 Problem Setup", "content": "Let G = (V, E) denote the graph representation of an HLS design used by HARP [29], where V is the set of nodes and & is the set of edges. Each node $v \\in V$ is associated with a feature vector $x \\in R^d$, where d is the feature dimension. We denote |G| as the number of nodes.\nThe goal of design space exploration (DSE) is to find the optimal valid pragma configuration $p^* \\in P$ that maximizes the performance metric y, which is intuitively the inverse of the latency defined consistently with [29], where P is the space of all possible pragma configurations."}, {"title": "3.3 Model Architecture", "content": "COMPAREXPLORE consists of a GNN encoder and an MLP-based decoder, as shown in Figure 1.\nGNN Encoder: The GNN encoder learns node embeddings by aggregating information from neighboring nodes. We adopt a stack of GNN layers, such as GCN [15], GAT [33], GIN [40], or TransformerConv [27], to capture the structural information and node features of the hardware design graph. The output of the GNN encoder is a set of node embeddings $h_v \\in R^{d'} | v \\in V$, where d' is the embedding dimension. A pooling operation such as summation is applied to the node embeddings to obtain one embedding per design denoted as $h_G$.\nNode Difference Attention (NODE DIFFERENCE ATTENTION): The NODE DIFFERENCE ATTENTION module is designed to focus on the most informative differences between node embeddings. It takes the node embeddings from the GNN encoder and computes attention scores based on the differences between node pairs. The attention scores are then used to weight the differences, emphasizing the most critical pragma-related differences. The weighted differences are aggregated to obtain a graph-level embedding.\nFor a design pair (i, j), denote their node embeddings as $H_i \\in R^{|G_i|\\times d'}$ and $H_j \\in R^{|G_j|\\times d'}$. Since DSE is only concerned with designs of the same kernel, during training, we only compare designs of the same kernel, i.e. $G_i$ and $G_j$ only differ in the pragma nodes and $G_i = G_j$. The differences between the node embeddings in $H_i$ and $H_j$ are computed: $D_{ij} = H_i \u2013 H_j$. The attention scores are computed by concatenating the node embeddings with their corresponding differences and passing them through an attention network:\n$s_{ij} = \\text{AttentionNet}(\\text{concat}(H_i, H_j, D_{ij})), s_{ij} \\in R^{|G_i|}$.\nAttentionNet is a multi-layer perceptron (MLP) that produces attention scores. To focus on the most informative differences, we propose the following attention mechanism to learn which node-level embedding difference contributes the most to the comparison between the designs:\n$a_{ij} = \\text{softmax}(s_{ij}), a_{ij} \\in R^{|G_i|}$.\nThe attention scores are then used to weight the difference embeddings and aggregate the differences:\n$h_{G_{ij}} = \\sum_{k=1}^{|G_i|} \\text{softmax}(s_{ij})_k D_{ij,k}$\nwhere $|G_i| = |G_j|$ as described previously, and k indicates the k-th element in a softmax($s_{ij}$) and the k-th row in $D_{ij}$. $h_{G_{ij}} \\in R^{d'}$ can be viewed as the graph-level difference-embedding that captures the most informative pragma-related differences.\nMLP Decoders: Since there are two tasks, the pairwise design comparison task and the pointwise design prediction task, we use two MLP decoders.\nFor the pairwise prediction task, the MLP decoder takes the concatenated results of various comparison operations applied to the graph-level embeddings of the two designs as input. Given the graph-level difference-embeddings $h_{G_{ij}}$ produced by the NODE DIFFERENCE ATTENTION module, and the individual graph-level embeddings $h_{G_i}$ and $h_{G_j}$ produced by the pooling operation over $H_i$ and $H_j$:\n$h_{pair_{ij}} = \\text{concat}(h_{G_i} \\odot h_{G_j}, h_{G_{ij}}),$\nwhere $\\odot$ denotes the Hadamard product, i.e. element-wise product. For the pointwise prediction task, the MLP decoder takes the individual design embedding $h_{G_i}$ and $h_{G_j}$ as input:\n$h_{point_i} = h_{G_i}$\n$h_{point_j} = h_{G_j}$\nBoth MLP decoders consist of multiple fully connected layers with non-linear activations, such as ReLU. The pairwise MLP decoder outputs a 2-dimensional vector representing the raw logits for the pairwise comparison, while the pointwise MLP decoder outputs a scalar value representing the predicted performance metric for the individual design, i.e.\n$z_{ij} = \\text{MLP}_{pair} (h_{pair_{ij}}),$\n$z_i = \\text{MLP}_{point} (h_{point_i}),$\n$z_j = \\text{MLP}_{point} (h_{point_j}).$"}, {"title": "3.4 Training of COMPAREXPLORE with Hybrid\nLoss Function", "content": "To train COMPAREXPLORE, we propose a hybrid loss function that combines pairwise preference learning with pointwise performance prediction. This enables the model to capture both relative preferences between designs and absolute performance values.\nThe hybrid loss function is defined as:\n$L = L_{point} + \\alpha L_{pair}$\nwhere $\\alpha \\in [0, 1]$ is a hyperparameter that controls the balance between the pairwise and pointwise losses.\nThe pairwise loss $L_{pair}$ is calculated using a cross-entropy loss that evaluates the model's ability to correctly rank pairs of design configurations. For each pair of designs, the MLP decoder outputs a 2-D vector of logits, $z_{ij}$, indicating the model's confidence in the relative performance of designs i and j. The softmax function is applied to these logits to obtain probabilities, $p_{ij} = [\\log(p_1^{(ij)}), \\log(p_2^{(ij)})]$. The cross-entropy loss for the pairwise comparison is defined as:\n$L_{pair} = \\sum_{(i,j) \\in D} (1(y_i > y_j) \\log(p_1^{(ij)}) + 1(y_i \\leq y_j) \\log(p_2^{(ij)}))$,\nwhere 1(.) is the indicator function, and D is the set of all pairs (i, j) sampled during training.\nThe pointwise loss $L_{point}$ is computed using a mean squared error (MSE) loss between the ground-truth and the predicted performance metric over design pairs sampled for $L_{pair}$."}, {"title": "3.5 Two-Stage DSE Approach", "content": "We propose two-stage approach for design space exploration, as described in Algorithm 1."}, {"title": "3.6 Complexity Analysis", "content": "Compared to the original HARP model or a pointwise-only approach, the newly introduced NODE DIFFERENCE ATTENTION module has a time complexity of O(|G|d'), which is linear to the number of nodes.\nIn the two-stage DSE approach, the pointwise prediction stage has the same time complexity as HARP. The newly introduced pairwise comparison stage has a time complexity of O($K_1^2$), where $K_1$ is the number of candidate designs which is usually set to 100. The actual pairwise comparison, i.e. lines 4-10 in Algorithm 1, can be performed in a batch-wise fashion, with a batch size of B. This reduces the time complexity to O($K_1^2$/B)."}, {"title": "4 EXPERIMENTS", "content": "We evaluate our proposed approach using the Xilinx Vitis HLS tool (version 2021.1), which is a more recent version compared to the one used in HARP and the HLSYN benchmark [3]. Our dataset consists of 40 kernels from various application domains. The kernels are synthesized using Vitis HLS, and the resulting designs are used for training and evaluation.\nThe dataset consists of a total of 10,868 designs, with 9,818 (90.34%) used for training, 525 (4.83%) for validation, and 525 (4.83%) for testing. The test set is used as a transductive test set, where the model has access to the design graphs but not their performance values during training. We ensure all the sampled design pairs come from the training set for a fair comparison. The validation loss is used to select the best model for testing. Training is conducted on a machine with 8 NVIDIA PG506 GPUs."}, {"title": "4.2 Hyperparameter and Implementation\nDetails", "content": "Our approach adopts TransformerConv with 7 layers and 64-dimensional node embeddings. Consistent with HARP, we use node attention and encode pragmas using MLPs. The model is trained using the AdamW optimizer with a learning rate of 0.001 and a batch size of 128 for 1600 epochs. We use a cosine learning rate scheduler [17]. The prediction target, performance, is defined consistently with HARP. $\\alpha$ is set to 1. For DSE, we set $K_1$ to 100 and $K_2$ to 10 with a total time budget of 12 hours with a batch size B = 512. The model is implemented in PyTorch Geometric [9]. The full hyperparameters, model implementation, and datasets will be released publicly to enhance reproducibility."}, {"title": "4.3 Evaluation Metrics", "content": "We evaluate our approach using two main metrics: pairwise classification accuracy and ranking metrics.\n\u2022 Pointwise Regression Error: This metric measures the model's ability to accurately make prediction for the performance metric for each design in the test set. The Root Mean Squared Error (RMSE) is used.\n\u2022 Pairwise Classification Accuracy: This metric measures the model's ability to correctly predict which design in a pair has better performance across design pairs in the test set. We report the accuracy for different degrees of pragma differences (d1, d2, d3 indicating design pairs differing by 1, 2 and 3 pragma settings) and the overall accuracy (ALL).\n\u2022 Ranking metric: We report Kendall's $\\tau$, which measures the ordinal association between the predicted performance rankings and the true performance rankings of the designs in the test[14].\nWe randomly select six kernels where we perform DSE followed by running the HLS tool to evaluate the selected designs by the DSE process. We report the lowest latency of the selected $K_2$ designs."}, {"title": "4.4 Loss Curves", "content": "Figure 2 presents the training loss curves for our proposed COMPAREXPLORE, including the pairwise loss ($L_{pair}$), pointwise loss ($L_{point}$), and the overall loss (L). The figure demonstrates a decrease in both pairwise and pointwise losses, indicating the model's effectiveness in learning from the data for both tasks."}, {"title": "4.5 Main Results", "content": "Table 1 presents the comparison between the vanilla HARP model and our proposed approach. Our approach achieves higher pairwise classification accuracy for designs with more pragma differences (d2 and d3) and overall (ALL) compared to the vanilla HARP model. In terms of ranking performance, our approach achieves a higher Kendall's $\\tau$ score, indicating a better alignment between the predicted and true rankings of the designs. While HARP achieves a lower regression error, the hybrid loss design in COMPAREXPLORE leads to a more balanced performance across classification accuracy and ranking metrics.\nThe accuracy improves as the number of differences between designs increases (d1 < d2 < d3). This suggests that the model finds it easier to distinguish between designs that have more differences. When designs differ in more pragmas, the performance metrics tend to vary more significantly, making it easier for the model to learn and identify which design is better. The increasing accuracy from d\u2081 to d3 suggests potential future work, such as incorporating curriculum learning to progressively improve the model's performance on more challenging design pairs with smaller performance differences [4].\nFigure 3 shows the design space exploration results using our proposed approach compared to the vanilla HARP model. The results demonstrate that our proposed approach consistently outperforms the vanilla HARP model across all kernels in terms of latency reduction. On average, COMPAREXPLORE achieves a 16.11% reduction in latency compared to HARP. The improvement is particularly significant for the \"adi\" kernel, where COMPAREXPLORE reduces the latency by nearly 50%. These results highlight the effectiveness of our approach in identifying high-quality designs that lead to improved hardware performance."}, {"title": "4.6 Parameter Sensitivity Study", "content": "Figure 4 shows the effect of $\\alpha$ on the COMPAREXPLORE's performance. As $\\alpha$ increases, the regression RMSE worsens, while the classification accuracy peaks around $\\alpha$ = 1. Kendall's $\\tau$ ranking metric reaches its highest value at $\\alpha$ = 1 and then declines. These trends suggest that excessive emphasis on pairwise comparisons may not necessarily improve overall performance. In contrast, moderate $\\alpha$ values effectively balance pointwise and pairwise losses, optimizing both tasks effectively."}, {"title": "4.7 Time Breakdown Analysis", "content": "The average time breakdown analysis presented in Table 2 highlights the efficiency of our two-stage DSE process. On average, Stage 1 accounts for approximately 87.06% of the total computation time, while Stage 2 contributes only 12.94%. This demonstrates that the pairwise comparison phase (Stage 2) introduces minimal additional overhead, ensuring that the overall computational efficiency is maintained. The relatively small proportion of time spent in Stage 2 indicates that our approach is practical and scalable for large-scale design space exploration tasks, making it suitable for optimizing HLS designs."}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "In this paper, we presented COMPAREXPLORE, a novel approach for HLS design space exploration that addresses the challenges of modeling complex design performance relationships. By incorporating a hybrid loss function, a Node Difference Attention module, and a two-stage DSE approach, COMPAREXPLORE demonstrates significant improvements in both pairwise comparison accuracy and ranking metrics. Our results show that explicitly learning to compare designs, with a focus on pragma-induced variations, leads to the discovery of higher quality HLS-generated designs.\nAlthough COMPAREXPLORE does not achieve the lowest regression error compared to HARP, our results show that explicitly learning to compare designs leads to the discovery of higher quality HLS-generated designs. In addition, in practice, it is worth considering a separate model such as HARP for stage 1 of the DSE process specializing in accurate pointwise prediction.\nThe success of COMPAREXPLORE in HLS DSE highlights the broader potential of learning-to-rank methods in the hardware optimization domain. In future work, we believe that this paradigm can be further explored and extended. For example, the ability to rank and select designs within a large language model (LLM) framework could lead to tighter integration of language models and hardware design enabling a more intuitive and automated design process, and achieving better performance in both regression and classification due to the higher expressive power of LLMs in capturing complex design relationships and patterns."}]}