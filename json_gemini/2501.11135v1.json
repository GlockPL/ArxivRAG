{"title": "Playing the lottery with concave regularizers for sparse trainable neural networks", "authors": ["Giulia Fracastoro", "Sophie M. Fosson", "Andrea Migliorati", "Giuseppe C. Calafiore"], "abstract": "The design of sparse neural networks, i.e., of networks with a reduced number of parameters, has been attracting increasing research attention in the last few years. The use of sparse models may significantly reduce the computational and storage footprint in the inference phase.\nIn this context, the lottery ticket hypothesis constitutes a breakthrough result, that addresses not only the performance of the inference phase, but also of the training phase. It states that it is possible to extract effective sparse subnetworks, called winning tickets, that can be trained in isolation. The development of effective methods to play the lottery, i.e., to find winning tickets, is still an open problem. In this paper, we propose a novel class of methods to play the lottery. The key point is the use of concave regularization to promote the sparsity of a relaxed binary mask, which represents the network topology.\nWe theoretically analyze the effectiveness of the proposed method in the convex framework. Then, we propose extended numerical tests on various datasets and architectures, that show that the proposed method can improve the performance of state-of-the-art algorithms.", "sections": [{"title": "I. INTRODUCTION", "content": "Neural network pruning refers to the sparsification of a neural architecture by removing unnecessary parameters, i.e., either connections (weights) or neurons. As a matter of fact, neural networks are often overparametrized and their size can be significantly decreased while keeping a satisfactory accuracy. Pruning allows us to reduce the computational costs, storage requirements, and energy consumption of a neural network in the inference phase; see, e.g., [1]\u2013[4]. This has several advantages, ranging from the circumvention of overfitting [1] to the possibility of implementing deep learning in embedded mobile applications [5], [6].\nLearning pruned (or sparse) neural networks is a challenging task, which has drawn substantial attention in the last years. In the literature, two main approaches are considered. The classic one is dense-to-sparse: the model is dense at the beginning and during the training, while the output of the training is a sparse network. In this context, a common practice is the following three-stage iterative procedure [2], [7]: first, the dense method is trained, then the sparse subnetwork is extracted, and finally the subnetwork is retrained, by starting from the weights of the trained dense model; this last stage is known as fine-tuning. Most of the proposed dense-to-sparse training methods reduce the number of non-zero parameters (i.e., the lo norm of the parameter vector) by pruning the weights with the largest magnitude [2], [8] or via lo regularization [3]. More recently, l1 and concave regularizations have been studied and tackled through proximal gradient methods [9], [10]. The benefit of the dense-to-sparse approach is the acceleration of the inference task by using the sparse model; nevertheless, the training remains computationally intense.\nThe second and more recent approach, known as sparse-to-sparse, tackles the computational burden of the training phase. Basically, it states that we can learn the sparse architecture and then train it in isolation. However, learning a sparse topology that can be trained in isolation is very challenging. In particular, it has been observed that retraining a sparse network obtained through a dense-to-sparse method from a random initialization often yields a substantial loss of accuracy. A way to circumvent this problem is rewinding the weights to the original initialization of the dense model, as proposed in [4]. More precisely, in [4], the authors conjecture the lottery ticket hypothesis (LTH): a dense, randomly-initialized neural network Na contains a small subnetwork Ns that achieves a test accuracy comparable to the one of Nd, with a similar number of iterations, provided that Ng is trained in isolation with the same initialization of Nd.\nThe effective subnetworks mentioned in the LTH are named winning tickets as they have won the initialization lottery. While their existence is proven in [4], their extraction is not straightforward. As a matter of fact, the development of effective algorithms to win the lottery is still an open problem. In [4], a method based on iterative magnitude pruning (IMP) is proposed, which still represents the state-of-the-art heuristic to play the lottery. In [11], IMP is refined to deal with instability: the weights are rewinded to an early iteration k > 0 instead of the initial 00 [11, Section 3]. Thus, subnetworks are no longer randomly initialized, and they are denoted as matching tickets [11, Section 4]. In [12], an lo regularization approach is proposed to search winning tickets, based on continuous sparsification, which outperforms the accuracy of IMP in some numerical experiments. Nevertheless, such enhancement is obtained at the price of repeated rounds, which causes a slower sequential search, as discussed in [12, Section 5.1].\nIn this work, we propose a novel method to play the lottery, i.e., to find sparse neural networks that can be trained in isolation by rewinding. The key steps of our approach are the following: we define a binary relaxed mask, which describes the network topology, and we optimize it by applying a continuous, concave regularization. In particular, we consider l\u2081 (which"}, {"title": "II. RELATED WORK", "content": "In recent years, the literature on neural network pruning and LTH has significantly grown, and a complete overview is beyond our purposes. In this section, we review the main works that, for different motivations, are related to our approach.\nRegarding the dense-to-sparse approach, several approaches include a regularization term in the loss function to sparsify the model, the most popular being the lo-norm [3], l\u2081-norm [9] and concave lp norms [10]. In these works, training and sparsification are performed in a joint optimization problem. In contrast, in the method proposed in [2], training and pruning are separated tasks, and the pruned architecture is iteratively retrained, according to the three-stage pipeline depicted in Fig. 1. The retraining stage is initialized with fine-tuning, i.e., by starting from the parameters obtained in the previous training stage. Such a three-stage procedure with fine-tuning is very common in the dense-to-sparse approach, see, e.g., [2], [6], [13], [14].\nAmong more recent works, in [15] a different regularization is proposed, based on the neural sensitivity, to learn sparse topologies with a structure. In [16], an energy-based pruning method is developed, combined with a dropout approach. In [17]\u2013[21], specific methods for filter/channel pruning in convolutional neural networks are developed.\nThe above-mentioned three-stage pipeline is also popular in the sparse-to-sparse approach [4], [12], but with rewinding: in the retraining stage, the parameters are reinitialized by rewinding them to the original initialization; see Section I. In the IMP method [4], which is by far the most common sparse-to-sparse approach, pruning is performed by removing the p% parameters with the smallest magnitude. This approach has some drawbacks. As p is priorly set, this may result either in an excessive sparsification with the removal of important weights or in an insufficient sparsification that retains superfluous parameters. Moreover, very similar weights may be either retained or removed to fulfill p%, which contradicts the principle of saving the most relevant parameters. In [12], the hard effect of magnitude pruning is mitigated by a continuous relaxation of the lo regularization. However, as discussed in Section I, in practice this does not outperform IMP.\nIn this paper, we tackle these issues by introducing l\u2081/log regularizers to induce sparsity. This results in a softer approach where the output of the optimization/training stage is not expected to be an exact binary mask, but a relaxed mask that can be used for pruning the dense network by setting a suitable threshold a \u2208 [0, 1]. Also, the dense-to-sparse approach presented in [2] proposes to set a threshold instead of removing a fixed percentage of the weights. However, in [2] the threshold is set on the weights of the parameters, which basically can assume any real value and whose range may vary at each layer of the network. Therefore, setting such a threshold may be critical and it requires some prior knowledge of the range of the parameters. In addition, defining a unique threshold for the entire network can be troublesome because weights from different layers might have different orders of magnitude."}, {"title": "III. PROPOSED METHOD", "content": "In this section, we present the proposed method.\nLet us consider a neural network f(x; 0), where x represents the input data and 0 \u2208 Rd are the weights. Let \u2299 be the component-wise product between vectors. According to [4], playing the lottery consists of searching a binary mask m\u2208 {0,1}d with ||m||, < d such that f(x; m\u2299 0) is a winning ticket, i.e., it achieves performance comparable to f(x; 0) when trained in isolation.\nThe seminal IMP search algorithm proposed in [4] consists of an iterative three-stage pipeline, as illustrated in Fig. 1. The retraining phase is performed by rewinding the parameters to"}, {"title": "Algorithm 1 Soft mask pruning with concave regularization", "content": "Input: 00 \u2208 Rd, mo = (1, 1, ..., 1) \u2208 Rd, a \u2208 (0, 1), T\n1: for all t = 1,..., T do\nInitialization:\n2:\t0 = 00, m = mt-1\n3:\tOptimization:\n\tmin\\_{\\Theta\\in\\mathbb{R}^{d}, m\\in [0,1]^{d}} L(x; m \\odot \\theta) + \\lambda R(m) \\rightarrow (\\theta_{t}, m_{t})\n4:\tPruning:\n\tfor each i = 1,..., d, if mt,i < a, then mt,i = 0\n5: end for\nOn the other hand, continuous concave regularizers have been observed to be more effective than l\u2081 regularization, even though they introduce non-convexity in the problem, as their shape is closer to the lo-norm.\nIn this work, we consider concave regularizers R(m) with the following property.\nAssumption 1. R(m) is any function\n$R(m) = \\sum_{i=1}^{d} r(m_{i}), m_{i} \\in [0,1]$\nsuch that r : [0, 1] \u2192 [0, 1] is continuous and differentiable in (0,1), concave, non-decreasing, and its image is [0,1].\nIn the literature of signal processing and sparse optimization, the most popular regularizers satisfying Assumption 1 are\n\u2022 l1: $r_{1}(m_{i}) = m_{i}$, see [9], [10], [22];\n\u2022 log: $r_{\\epsilon}(m_{i}) = \\frac{log(m_{i} + \\epsilon)}{log(1 + \\epsilon)}$ for any \u20ac > 0, see [23];\nOther possible choices are la norm, see [10], [24], and minimax concave penalties, see [25]. In this work, we focus our attention on l\u2081 and log regularizers.\nIn the literature, the use of strictly concave regularization has arisen in the context of linear regression and compressed sensing, see, e.g., [23], [24], [26]\u2013[28]. Then, it has been extended to several machine learning models; we refer the interested reader to the survey [29]. When the cost function is strictly convex, adding a strictly concave regularizer may keep the problem in the convex optimization framework, see, e.g., [30]. In contrast, the problem is more challenging when the cost function is non-convex. This case is theoretically analyzed, e.g., in [31]\u2013[33], where much attention is devoted to proving the convergence of the applied algorithms (proximal methods and alternating direction method of multipliers).\nWithin the family of non-convex cost functions, the case of neural networks is even more difficult. In fact, in deep learning, gradient-based algorithms are commonly used for training, which is in conflict with non-differentiable regularization as in Assumption 1, as discussed in Sec. III-B."}, {"title": "B. Discussion on the training algorithm", "content": "The training phase of the proposed approach requires locally solving (2). In principle, the l\u2081/ log regularization is critical for the application of gradient-based training algorithms, due the non-differentiability in zero. However, in our approach we regularize m \u2208 [0, 1]d, thus we can use a projected gradient-based algorithm, without differentiability issues.\nWe remark that l\u2081 regularization is popular in deep learning and it is usually implemented in libraries such as TensorFlow and PyTorch. However, since l\u2081 norm is not differentiable at zero, it is usually implemented with a subgradient approach, namely the gradient of |x| is defined as sign(x) for x \u2260 0, and 0 for x = 0. This workaround may be effective in controlling the energy of the parameters, but subgradient iterates do not attain zero, thus sparsification fails. Moreover, subgradient methods are substantially slow and may result in oscillations. These drawbacks are illustrated in the numerical example in Section IV-C.\nRecently, proximal operator methods are used instead of subgradient, see, e.g., [9], [10]. However, their application and convergence proof are critical and limited to some specific cases.\nGiven these considerations, the proposed relaxed binary mask turns out to be an effective alternative strategy to match the use of non-differentiable, sparsity-promoting regularizers with standard gradient-based training algorithms."}, {"title": "IV. THEORETICAL ANALYSIS", "content": "In this section, we prove theoretical results that support the effectiveness of the proposed method, by providing conditions that guarantee to extract an optimal sparse subnetwork topology. In particular, these results explain why a log regularizer may be preferable to l\u2081. Finally, we show an illustrative example that corroborates the theoretical findings.\nAs a thorough analysis is quite complex, we focus on the following problem: we assume that a vector @ of trained parameters is available and that a sparse mask m \u2208 {0,1}d exists, such that m is a sparse topology with no substantial performance loss. Our aim is to estimate this optimal m. To this purpose, we solve\n$\\min_{m \\in [0,1]^{d}} L(x; m\\theta) + \\lambda R(m)$\nwhere R(m) satisfies Assumption 1.\nOn the one hand, this problem is addressed in the context of the strong lottery ticket hypothesis, where subnetworks are extracted from randomly weighted neural networks without modifying the weight values, see [34], [35]. On the other hand, if in Eq. (2) we proceed by alternated minimization over @ and m, Eq. (4) can be interpreted as a sub-problem of (2).\nSince, in this section, we consider m as the unique variable, for simplicity we write L(x; m\u2299 0) = L(m). Let\nm* = argmin L(m) + AR(m)."}, {"title": "A. Accuracy analysis for l\u2081 regularization", "content": "In this section, we study the accuracy, in terms of distance between \u00eem and m*, in the case of l\u2081 regularization. Specifically, we prove the following result.\nTheorem 1. Let h = m* \u2013 m. If R(m) = ||m||1, then\n$||h||_{2} \\leq \\frac{4\\lambda\\sqrt{k}}{\\gamma}$\nProof. By definition of m* in (5), we have\n$L(m^{*}) + \\lambda R(m^{*}) \\leq L(m) + \\lambda R(m)$\nLet S and Se denote the support of m and its complementary set, respectively. Moreover, hs (respectively, hs) is the subvector of h with components indexed in S (respectively, in Sc). Then,\n$L(m^{*}) - L(m) \\leq \\lambda [||m||_{1} - ||m^{*}||_{1}]$\n= x [||m||1 - ||m + h||1]\n= x [||m||1 - ||ms + hs||1 - ||hs||1]\n< A [||m||1 - ||ms|| + ||hs||1 \u2013 ||hs||1]\n= 1||hs||1 \u2013 A||hs||1.\nMoreover,\n$ \\bigtriangledown L(m)^{T} (m^{*} - m) \\leq ||\\bigtriangledown L(m)||||h||_{1}$\n<\u03bb||h||1.\nNow, from (7) and (10), we have a lower bound and an upper bound for L(m*) \u2013 L(m). By using also (11), we obtain\n$-\\lambda ||h||_{1} + \\frac{\\gamma}{2} ||h||_{3} \\leq L(m^{*}) - L(m) \\leq \\lambda ||h_{S}||_{1} - \\lambda ||h_{S^{c}}||_{1}$.\nThen,\n$\\frac{\\gamma}{2} ||h||_{3} \\leq \\lambda ||h||_{1} + \\lambda ||h_{S}||_{1} - \\lambda ||h_{S^{c}}||_{1}$\n< ||hs||1 + 1||hs||1 + 1||hs||1 \u2013 A||hs||1\n< 21||hs||1\n<21\u221ak||h||2."}, {"title": "B. Accuracy analysis for strictly concave regularization", "content": "In this section, we analyse the accuracy in case of strictly concave regularization.\nTheorem 2. Let h = m* \u2013 m. Let R(m) be strictly concave in [0,1] as in Assumption 1. Then\n$||h||_{2} \\leq \\frac{4\\lambda\\sqrt{k}}{\\phi(m^{*})}$\nwhere \u03c6(m*) > 0 is assessed in the proof.\nProof.\n$L(m^{*}) - L(m) \\leq \\lambda R(m) - \\lambda R(m^{*})$.\nNow, since m \u2208 {0,1}d, then R(m) = ||m||1 = \u2211m;\u00b7 Then,\nR(m) \u2013 R(m*) = ||m||1 \u2013 R(m*)\n= ||m||1 - R(m*) \u00b1 ||m*||1\n\u2264 ||hs||1 ||hs||1 - \u03c6(m*)\nwhere\n\u03c6(m*) = R(m*) \u2013 ||m*||1 \u2265 0.\nTheorem 2 states that by replacing l\u2081 with a strictly concave regularization, such as the logarithmic one, we obtain an error bound which is smaller than the one in Theorem 1 of \u03c6(m*).\nLet us discuss more in detail the role of \u03c6(m*). We notice that if m* \u2208 {0,1}d, then R(m*) = ||m*||1, \u03c6(m*) = 0, and no enhancement is obtained by using a strictly concave regu-larizer. However, we know from the statements of Theorems 1 and 2 that $||m^{*} - m||_{2} < \\frac{4\\sqrt{k}}{\\gamma}$. Then,\nLemma 1. If $\\frac{4\\sqrt{k}}{\\gamma} < 1$ and m* \u2208 {0,1}d, then m* = m.\nTherefore, if $\\frac{4\\sqrt{k}}{\\gamma} < 1$, the unique feasible binary solution is m. Moreover, if m* \u00a2 {0,1}d, then \u03c6(m*) > 0 and we obtain a smaller error bound by using a strictly concave regularizer."}, {"title": "C. Illustrative example", "content": "To conclude the analysis, we show an illustrative example of Problem (4) with Assumption 2. Specifically, we consider a problem of binary classification performed through logistic regression. We use the MNIST dataset [36] restricted to the digits 0 and 1. Each image is composed of d = 400 pixels. We consider 200 samples for each digit, 160 for training, and N = 40 for validation test. The corresponding cross-entropy loss function is\n$L(X, y; \\theta) = \\sum_{i=1}^{N} y_{i} log(\\hat{y}_{i}) - (1 - y_{i}) log(1 - \\hat{y}_{i})$\nwhere y \u2208 {0,1}N and \u0177 \u2208 [0,1]N respectively are the vectors of correct and estimated labels. More precisely,\n$\\hat{y}_{i} = \\frac{1}{1 + e^{-x_{i}\\theta}}$\nwhere \u03b8\u2208 Rd is the vector of estimated parameters and X \u2208 RN,401 is the validation dataset (including the intercept). The loss is convex; if an l2 regularization is added, we have strong convexity as in Assumption 2.\nAs we can see in Fig. 2, many pixels (e.g., the ones towards the image borders) are not significant for classification, therefore there is room for sparsification. Then, we tested different approaches for sparsification. First, we considered a classic l\u2081 regularization, i.e., we minimize L(X, y; \u03b8) + 1||0||1. Secondly, we applied l\u2081 and logarithmic regularizations to the mask. More precisely, given a vector 6 of parameters, e.g., obtained via logistic regression, we aim at finding a binary mask m \u2208 {0,1}d that selects the significant pixels, i.e., the final parameter vector will be \u00d4m. Since X\u00d4\u2299m = Xdiag(0)m, for finding m it is sufficient to train over the dataset Xdiag(6), with the chosen regularization. In formulas, we minimize\n$L(Xdiag(\\theta), y; m) + \\lambda R(m), m\\in [0, 1]^{d}$\nwhere R is either l\u2081 or log regularization. We notice that this approach does not retrain the model, but it just performs a selection of the parameters/pixels that can be neglected.\nIn the experiment, we do not set the ground truth m*, but we consider acceptable solutions whose accuracy is comparable to the one of standard logistic regression."}, {"title": "V. EXPERIMENTS", "content": "In this section, we perform an experimental evaluation of the proposed method. We first illustrate the experimental settings, then we discuss the experimental performance of the proposed method both in the context of the lottery ticket hypothesis"}, {"title": "VI. CONCLUSION", "content": "In this paper, we proposed a novel iterative method for identifying matching tickets, i.e., for extracting subnetworks"}]}