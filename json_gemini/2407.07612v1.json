{"title": "Teaching Transformers Causal Reasoning through\nAxiomatic Training", "authors": ["Aniket Vashishtha", "Abhinav Kumar", "Abbavaram Gowtham Reddy", "Vineeth N Balasubramanian", "Amit Sharma"], "abstract": "For text-based AI systems to interact in the real world, causal reasoning is an es-\nsential skill. Since interventional data is costly to generate, we study to what extent\nan agent can learn causal reasoning from passive data. Specifically, we consider an\naxiomatic training setup where an agent learns from multiple demonstrations of\na causal axiom (or rule), rather than incorporating the axiom as an inductive bias\nor inferring it from data values. A key question is whether the agent would learn\nto generalize from the axiom demonstrations to new scenarios. For example, if a\ntransformer model is trained on demonstrations of the causal transitivity axiom\nover small graphs, would it generalize to applying the transitivity axiom over large\ngraphs? Our results, based on a novel axiomatic training scheme, indicate that such\ngeneralization is possible. We consider the task of inferring whether a variable\ncauses another variable, given a causal graph structure. We find that a 67 million\nparameter transformer model, when trained on linear causal chains (along with\nsome noisy variations) can generalize well to new kinds of graphs, including longer\ncausal chains, causal chains with reversed order, and graphs with branching; even\nwhen it is not explicitly trained for such settings. Our model performs at par (or\neven better) than many larger language models such as GPT-4, Gemini Pro, and\nPhi-3. Overall, our axiomatic training framework provides a new paradigm of\nlearning causal reasoning from passive data that can be used to learn arbitrary\naxioms, as long as sufficient demonstrations can be generated.", "sections": [{"title": "1 Introduction", "content": "Causal reasoning can be defined as a set of reasoning procedures consistent with pre-defined axioms\nor rules that are specific to causality [11]. For instance, d-separation and rules of do-calculus can be\nconsidered as axioms and specifications of a collider or a backdoor set can be considered as rules\nthat can be derived from axioms. Typically, causal reasoning is done over data corresponding to\nvariables in a system. Axioms or rules are incorporated as inductive biases in a machine learning (ML)\nmodel, through regularization, model architecture, or the choice of variables for a particular analysis.\nDepending on the kind of available data-observational, interventional, or counterfactual-Pearl's\nladder of causation [5] defines the kinds of causal reasoning that is possible.\nAs axioms are the building blocks of causality, we study whether it is possible to directly learn the\naxioms using ML models. That is, rather than learning from data that is the result of axioms followed\nby a data-generating process, what if a model can learn an axiom (and thus causal reasoning) directly\nfrom symbolic demonstrations of the axiom? Such a model has the advantage that it can be applied\nfor causal reasoning in diverse downstream scenarios, compared to task-specific causal models built"}, {"title": "2 Related Work", "content": "LLMs for Knowledge-Driven Causal Reasoning: Recent developments in Large Language Models\n(LLMs) have highlighted their potential for knowledge-driven causal discovery. Unlike traditional\nmethods which focus on statistical patterns or correlations, LLMs utilize knowledge acquired through\ntheir pretraining to reason about and identify causal structures based on metadata of variables [18,\n4, 22, 30, 28]. However, possibility of memorization of existing benchmarks in the pretraining of\nthese LLMs has been a major criticism. As a result, recent work [31] argues that LLMs are not\nactually performing causal reasoning, but simply learning correlations about causal facts. In addition,\nthere are critical failure modes of using LLMs for causal discovery due to hallucinations or not\nobeying the acyclic constraint when generating graph edges [28]. To evaluate causal reasoning\ncapabilities of LLMs, [16] and [15] propose formal causal inference evaluation benchmarks to infer\ndirect and indirect causal relationships, and highlight the failure of LLMs in performing accurate\ncausal reasoning.\nImpact of Positional Encoding on Generalization: Length generalization capabilities of trans-\nformers has been studied in the past to better understand their different failure modes across various\nsettings [14, 32, 10]. Previous work [17, 7, 13, 26] emphasizes the impact of positional encoding in"}, {"title": "3 Learning Causal Axioms using Transformers", "content": "Instead of performing causal reasoning using observational or interventional data, we study whether it\nis possible to learn general rules of causality directly from symbolic axioms. We begin by asking the\nquestion \"are there any minimal sufficient characterization of causal principles?\". There has been a\nfundamental work from Galles and Pearl [11] where they axiomatize causal relevance (or equivalently\nirrelevance). They show that for a given stable probabilistic causal model (defined below), there exists\na finite set of axioms that completely characterized by axioms of path interception in corresponding\ndirected graphs. We now study how such causal relevance statements can be incorporated into\ntransformer models.\nLet M = (X,U, F) be a causal model defined over a set of endogenous variables X, exogenous\nvariables U and the causal relationship between then defined by set of structural equations F [11].\nLet G be the causal graph associated with the causal model M where the nodes V in G correspond to\nthe variables in M and an edge Vi \u2192 V; between any two nodes Vi, V; denote the causal relationship\nbetween them."}, {"title": "Definition 3.1 (Causal Irrelevance, Defn. 7 in [11]).", "content": "X is probabilistically causally irrelevant to\nY given Z, written (X \u2192 Y|Z) iff: P(y|z,do(X) = x) = P(y|z, do(X) = x'), \u2200x, x', y, z i.e., once\nwe hold Z fixed at z, intervening on X will not change the probability of Y.\nNext, we restate the stability assumption for a causal model from [11] that gives a richer set of finite\naxiomatization for probabilistic causal irrelevance.\nAssumption 3.1 (Stability, Definition 9 in [11]). Let M be a causal model. Then an irrelevance\n(X \u2192 Y|Z) in M is stable it is shared by all possible probability distribution over M. The causal\nmodel M is stable if all of the irrelevances in M are stable.\nUnder the stability assumption (see Assumption 3.1), Galles and Pearl [11] characterizes six axioms\nthat completely characterize causal irrelevance (Definition 3.1) or equivalent causal relevance state-\nments after using the corresponding contrapositive statements. An axiom of causal irrelevance is of\nthe form (given in conjunctive normal form):\n$\\bigwedge\\bigvee(x_i^{-s,t} \\land x_j^{s,t} \\land x_k^{s,t}) \\rightarrow \\bigvee(x_n^{-l,n} \\land x_l^{l,n} \\land x_k^{l,n})$\nwhere / is \u201clogical and\", V is \u201clogical or\" and for a given (s, t) or (l, n) pair, Xi, Xj, Xk are disjoint\nsubsets of observed variables X. In the above causal irrelevance statement, if the antecedent is true,\nthe consequent is also true.\""}, {"title": "3.1 Transitivity Axiom for Causality", "content": "Transitivity axiom is intuitive in nature and presented for causal irrelevance in [19]. In our study, we\nfocus on the transitivity axiom for its simplicity and ease of expressing causal chains and other graph\nstructures for building diverse evaluation sets. For a stable probabilistic causal model (\u00a73), given\nvariables X, Y, Z in the system, the transitivity axiom is given by:\n(X \u2192 Y|Z) \u21d2 (A \u2192 Y|Z) v (X \u2192 A|Z)\u2200A \u00a2 X U ZU Y\nwhich could be further simplified by taking the contrapositive of the expression to derive its causal\nrelevance version which could be given as follows:\n\u2203A \u2209 X U YU Z_s.t. (X \u2192 A|Z) ^ (A \u2192 Y|Z) \u21d2 (X \u2192 Y|Z)\nWe call the LHS as Premise and the RHS as Hypothesis. Our key idea is that we can use such an\naxiom to generate thousands of synthetic symbolic expressions that can be used to teach a transformer\nthe specific axiom. The trained model is then evaluated on whether it can apply these axioms to new\ncausal structures that were not available in the training set."}, {"title": "3.2 Axiomatic Training: Dataset, Loss Function, and Positional Encoding", "content": "Training data. Based on a specific axiom, we can map a hypothesis given the premise to its correct\nlabel ('Yes' or 'No'). To create a training dataset, we enumerate all possible tuples of {(P, H, L)}N\nwhere P is the premise, H is the hypothesis and L is the label (Yes/No) for a particular setting of\nthe variables X, Y, Z, A. Given a premise P based on a given causal graph, if the hypothesis can\nbe derived by applying the specified axiom (once or multiple times), then label L is Yes; otherwise,\nNo. For example, suppose the underlying true causal graph of a system has the topology of a chain,\nX1 \u2192 X2 \u2192 X3 \u2192\u2026\u2026\u2192 Xn. Then, a possible premise could be X1 \u2192 X2 / X2 \u2192 X3, and\nthe corresponding hypothesis X1 \u2192 X3 will have label Yes whereas another hypothesis X3 \u2192 X1\nwill have label No. The above axiom could be inductively applied multiple times to generate more\ncomplex training tuples.\nFor our training setup, a synthetic dataset D is constructed with N axiomatic instances generated\nusing the transitivity axiom. Each instance in D is structured in the form of a premise P, which is\nthe natural language expression of a causal structure (e.g., \u201cX causes Y. Y causes Z\"), followed\nby the hypothesis in the form of a question Hq (e.g., \u201cDoes X cause Y?\u201d), which is then followed\nby the final label L (e.g., \u201cYes\u201d or \u201cNo\u201d). Each instance in D is structured as (Pi, Hij, Lij);\nj\u2208 {1, ..., (2)}where n is the number of nodes in each ith premise, thus effectively covering all\npairs of nodes in each unique chain of a given causal graph.\nLoss function. Given a dataset, the loss function is defined based on the ground truth label for each\ntuple, represented as $\\mathbb{E}_{P, H, L \\sim P_{train}} -log(P(L|P, H))$. A preliminary analysis indicated promising\nresults with this loss formulation compared to next token prediction loss.\nPositional Encoding. In addition to the training data and loss function, another important factor is the\nchoice of positional encoding. Positional Encoding (PE) play a crucial role of providing information\nabout the absolute and relative position of tokens in a sequence [29]. [29] propose an absolute\npositional encoding strategy using periodic functions (e.g., sinusoidal or cosine) to initialize these\nencodings. Absolute positional encoding provides definite values for all positions across any sequence\nlength. However, studies [23, 9] show absolute positional encoding fails in length generalization\ntasks for transformers. In the learnable APE variant [24], each positional embedding is randomly\ninitialized and trained with the model. This approach falters with sequences longer than those\nseen in training, as the new positional embeddings remain untrained and randomized. Interestingly,\nrecent findings [17, 13] indicate that removal of PEs in auto-regressive models can improve model's\nlength generalization capabilities, wherein the attention mechanism during auto-regressive decoding\nis sufficient to encode positional information. We experiment with different positional encoding\nto understand their impact on generalization in causal tasks: learnable position encoding (LPE),\nsinusoidal positional encoding (SPE), no positional encoding (NoPE).\""}, {"title": "3.3 Data Perturbation: A Key to Model Generalization", "content": "Variability or diversity in training data in the form of perturbation helps aid model generalization [20].\nFor axiomatic training, we provide structured perturbation to extend our transformer model across\ncomplex structures for which it was not explicitly trained on. We introduce perturbations at multiple\nlevels in the training data to maximize diversity in the distribution of the training set, as explained\nbelow.\n1. Node names: Each node in the transitivity chain is represented by an alphanumeric name\ncomprising 1-3 characters. The length of a name and the specific characters are randomly selected\nduring data generation."}, {"title": "2. Causal Graph Topology:", "content": "We consider two main types of causal graphs for the training set.\n(a) Sequential: All causal edges are directed forward, thus forming a typical transitivity chain,\ne.g. X \u2192 Y \u2192 Z.\n(b) Random Flipping: Given a chain of sequential nodes, we randomly reverse some edges\ncreating complexity by disrupting direct paths between subsequent nodes (eg. X \u2192 Y \u2190 Z).\nThis can be expressed simply through natural language like: \u201cX causes Y. Z causes Y.\"\n3. Length level: To facilitate transformers understanding of the axiom, we incorporate chains of\nvarying lengths, ranging from 3 to 6 nodes in our training set.\nRandom flipping introduces forks and colliders, which form the building blocks of any causal DAG.\nThis helps incorporate complexity in model training, thus aiding its capability to generalize across\nmultiple structures."}, {"title": "3.4 Assessing Axiomatic Learning in Transformers", "content": "To evaluate if a trained model has learnt the correct understanding of an axiom instead of relying\non shortcuts or correlation-based features, designing an out-of-distribution (OOD) evaluation set is\nimportant. To tackle this, we evaluate our model across multiple types of complex structures that are\nnot seen during training. We divide the structural complexities as follows:\n1. Length: Evaluating whether our model accurately infers causal relationships for sequences or\nchains (both sequential and ones with random flipping) longer than those in the training set.\n2. Node Name Shift: Testing the model's performance with longer node names, increasing from\n1-3 characters used in the training set to 8-10 characters. Findings of [16] show how shift in node\nnames used in finetuning of language models result in generalization failure in inferring causal\nrelationships from correlational relationships even though the model had impressive performance\non in-distribution node names, further motivating this analysis.\n3. Order of Chains: a) Completely reversed chains: This evaluation is inspired by the reversal\ncurse [6] that revealed generalization failure of LLMs in answering questions in reversed sequences\ndespite knowing the answers in the original order. We evaluate the capacity of axiomatic training\nto enable reasoning over reversed chains even when not explicitly trained on any completely\nreversed chains. A completely reversed chain will be of the form X + Y \u2190 Z with its natural\nlanguage representation as: \u201cY causes X. Z causes Y.\", where X, Y, Z are replaced by random\nalphanumeric names. b) Shuffling of Sequences: Causal sequences with random edge flips, as\ndefined in 3.3 represented by natural language statements sequentially (A causes B. B causes C\n...), are shuffled to add complexity and break sequential order. This tests transformers' ability to\ninfer accurate relationships regardless of sequence order of premise.\n4. Branching: Causal graphs with dense branching pose a challenging evaluation task. While the\ntraining set comprises simplistic linear sequences, this evaluation setup involves multiple branches,\ncolliders, forks, and chains in one network, with significantly high complexity. We measure\ncomplexity of a graph using its branching factor: Number of edges/Number of nodes.\nUnlike length and node name generalization, the reversal and branching evaluation setups change the\ncausal structure and hence better evaluate whether the model has learnt accurate representations for\ncausal structure. Branching is perhaps the most challenging since it contains new structures (due to a\nhigher branching factor) that were unseen during training.\""}, {"title": "4 Applying Axiomatic Training to Learn Causal Transitivity Axiom", "content": "Based on the above discussion, a dataset can be characterized by relative frequency of different types\nof causal structures such as linear, completely reversed sequences, and so on. Given a set of causal\nsequences X = {X1,...,Xn} where each causal sequence X\u2081 takes the form: Xi = [ejk | j >\n0, k > 0]. Here ejk represents an edge between node j and k in the ith causal sequence (can also be\nrepresented as Xi,j, Xi,k) such that each ejk translates to \"Xi,j causes Xi,k.\" in natural language. Vi\n= {Xi,1, Xi,2, ..., Xi,m}, represents set of vertices in causal sequence Xi.\nLet fdim represent the maximum value for a given perturbation dimension dim, along which we\nconstruct train and evaluation sets for our axiomatic framework. For each dimension, we choose"}, {"title": "4.1.1 Training Datasets", "content": "Using a straightforward transitivity chain-based training setup, we aim to understand how variability\nor noise, introduced through randomly flipped edges, varying sizes of node names, and different chain\nlengths, enhances the generalization abilities of our transformer model to handle longer, branched, and\nreordered complex settings. We perform multiple ablations with different training sets to understand\npossible contributing factors towards the model's generalization. Our training setup consists of\naround 175k instances of sequential chains with size of chains ranging from 3 to 6 nodes. We use\nthree versions of training data to evaluate the impact of different noise perturbations, and the impact\nit has across diverse evaluation setups when a transformer is trained from scratch using that. Each"}, {"title": "4.1.2 Evaluation Datasets", "content": "To evaluate the impact of variability in causal sequences on training transformers across different OOD\nstructural dimensions as described in Section 3.4, we design different evaluation sets encompassing\nnatural language representation of causal structures with varying levels of complexity. This helped us\nevaluate whether the model is capable of learning accurate structural understanding and application\nof transitivity axiom to more complex networks. Similar to train sets, the evaluation sets have equal\nnumber of instances from both label classes. Our evaluation sets to assess generalization capabilities\nof the model could be described as follows:\n\u2022 Length Generalization EvalSet: Testing on causal sequences with length >6 upto 15, longer than\nany sequence encountered by the model in training set. Length generalization is evaluated for both\nsequential chains and chains with randomly flipped edges.\n\u2022 Node Name EvalSet: Assessed model's generalization capabilities to sequences with longer node\nnames, increasing from 1-3 characters used in training set to 8-10 characters. To add onto the\ncomplexity, we also include sequences longer than any sequence in the train set (>6) upto 9 length.\n\u2022 Reversal EvalSet: Evaluated performance of our transformer model, with no completely reversed\nsequence in its training, on reversed causal sequences. Sequences upto 6 length were evaluated.\n\u2022 MultiEvalsLR (Shuffling + Random Flipping + Length Sequence): This setup involves\nevaluation on 3 levels of complexities together: shuffling of sentence for representing the sequences,\neach sequence having random flipping, and some sequences having longer length than sequences\nin training set (upto 9).\n\u2022 Branching EvalSet: One of the most complex evaluation setups, with dense networks containing\nmultiple branches, colliders and forks. While each sequence in the training set had values of 1-2 for\nboth in-degree and out-degree across all nodes, in this setting a node can have maximum value of\nn\n1 for both, and minimum of 0 creating more complicated structures than the ones transformer\nhad encountered during its training. To add onto the complexity we evaluate on structures with\nmore nodes (8,10,12), than any unique causal sequence in the training set besides 5 node networks.\nWe evaluate multiple densely branched networks constructed using the Erd\u00f6s-R\u00e9nyi model, where\nwe provide number of edges and nodes in accordance to the values of branching factor (1.4 and\n2) we use for evaluation. We implement this using igraph package in python [8] to get different\nunique graphs with required branching factors for evaluation."}, {"title": "4.2 Implementation Details: Architecture, Tokenizer and Training Procedure", "content": "We train a decoder-based 67 million parameter model based on GPT-2's architecture. The model has\n12 attention layers, 8 attention heads and 512 embedding dimensions. The model is trained from\nsratch on each of our training datasets. To understand the effect of Positional Encodings (PE), we\nconsider Sinusoidal PE (SPE) [29], Learnable PE (LPE) [24] and having no PEs (NoPE) [17, 13].\nAll models are trained for 100 epochs using the AdamW optimizer with 1e-4 learning rate.\nSince the training dataset follows a specific structure, we develop a custom tokenizer. Alphanumeric\nnode names are tokenized at a character level, while special terms such as 'causes', 'Does', 'cause',"}, {"title": "5 Results", "content": "5.1 Generalization to Complex Causal Scenarios\nWe present results on how well an axiomatically trained transformer can generalize to larger and\nmore complex causal graphs, and how it compares to pre-trained LLMs.\nLength Generalization: Table 1 shows accuracy of different models when evaluated on larger causal\nchains that were not seen during training. Among the baseline pre-trained LMs, GPT-4 obtains the\nhighest accuracy on both standard and randomly flipped chains. It is remarkable that our TS2 (NOPE)\nmodel obtains competitive performance to the trillion-scale GPT-4 model, even though it had never\nseen larger sequences during training. In particular, for chains of size 7-13, TS2 (NoPE) obtains higher\nor comparable accuracy than GPT-4 across the standard and randomly flipped chains. Its accuracy\ndecreases for chains of length 14-15 (0.85 for standard chains and 0.78 for randomly flipped chains)\nbut is still significantly higher than that of LMs like Gemini-Pro and Phi-3. Note that a random\nprediction would yield a 50% accuracy, indicating that the axiomatically-trained TS2 (NoPE) model\ncan generalize its reasoning to causal chains much longer than 6 even though it was trained only on\nchains upto length 6."}, {"title": "Role of Positional Encodings.", "content": "Comparing the performance of models wrt. choice of positional\nencoding, we find that models with no positional encoding generalize well to both longer lengths\n(upto chain length of 15) and complex, unseen graph structures, even though they are only trained\non chains over 3-6 nodes. Models with SPE and LPE also perform well on longer chains but poorly"}, {"title": "6 Inferring Causation from Correlation using Axiomatic Training", "content": "While the above study evaluated transformers' capability to generalise the transitivity axiom from\nsmall causal chains to large graphs, we now study whether this capability transfers to other causal\ntasks.\nTo this end, we apply axiomatic training to a task on inferring causation from statements about\ncorrelation in observational data [16]. As Figure 5 shows, each data instance includes correlational\nrelationships described in natural language for graphs with 3 to 6 nodes; and the goal is to infer\nthe truth value of a hypothesis (of 6 different types: Parent, Ancestor, Child, Descendant, Collider,\nConfounder) on whether there exists a direct or indirect relationship between any given nodes, and\npossible presence of colliders and confounders.\nThis task is significantly harder than applying the transitivity axiom. First, there are multiple hypoth-\nesis types to evaluate: direct effect, indirect effect, children, ancestors, colliders and confounders.\nSecond, solving the task requires an understanding of d-separation and the Markov property. Specifi-\ncally, it involves mapping correlational statements to multiple possible causal graphs and determining\nif the query is satisfied across all graphs in the Markov Equivalence Class."}, {"title": "7 Discussion and Conclusion", "content": "In this work, we use the formal axiomatic framework proposed by [11] to study whether transformers\ncan be taught causal axioms. To this end, we provide a method to create training data containing\ndiverse demonstrations of an axiom and explore modelling choices to learn the axiom. We find that a\ntransformer model trained from scratch can learn how to apply an axiom based on a large training\ndataset containing axiomatic demonstrations. We evaluate the proposed axiomatic training approach\non two causal tasks-graph traversal using the transitivity and inferring causal relationship from\ncorrelational statements. For these tasks, small 67M transformers trained using our approach can\ngeneralize over longer and complex complex graph structures that they never saw during training,\noften providing accuracy better than existing LLMs such as GPT-4, Phi-3 and Gemini Pro.\nApplicability to Causal Tasks. While our current work focuses on the transitivity axiom for causal\nrelevance, extending the work to other causal axioms from [11] is an interesting research direction.\nIn addition, we may consider other axioms that are relevant for downstream tasks such as effect\ninference. For example, if a transformer model can be trained to validate the d-separation rule-given\ntwo variables X and Y, are they independent given a variable set Z?\u2014then repeated applications of\nthe rule can be used to derive a backdoor set. Another interesting direction is to extend the training\napproach for both deterministic and probabilistic causal models.\nGeneralization to Logical Reasoning. While we focused on causal reasoning axioms, our ax-\niomatic training approach is general and can be applied to any formal system based on axioms. For\ninstance, the same axiomatic training procedure can be used for teaching LMs logical reasoning\ntasks such as deductive reasoning. For instance, recent work [25] evaluates the deductive reasoning\ncapabilities of LLMs and measures their generalization abilities along depth, width, and composi-\ntional abilities. As the depth increases, performance of LLMs deteriorates. It will be interesting to\nsee whether axiomatic training can be applied to learn deductive reasoning axioms and improve the\nreasoning abilities of LMs.\nImplications for Training Language Models. GPT-4 shows impressive generalization on the\ncausal tasks we evaluated, even though it was not trained specifically for these tasks. A fundamental\nquestion is how LLMs like GPT-4 achieve the capability to reason over causal graphs. Based on our\npreliminary experiments, we conjecture that axiomatic training may be one explanation for GPT-4's\nability. While GPT-4 was not trained specifically for the causal tasks, the underlying axioms may\nhave been present in diverse forms in its web-scale training data (e.g., in books, blogs, and technical\nforums) that helped it learn the causality rules.\nAt the same time, performance of other LMs such as Gemini Pro and Phi-3 can be improved. For\ninstance, inability of Gemini Pro and Phi-3 to reason over completely reversed chains (even when\ntheir length was small) highlights how zero-shot reasoning for causal tasks is missing in these models.\nIncorporating causal axiom demonstrations as a part of language models' pretraining could help\nimprove the reasoning of these models, so that even small language models like Phi-3 can perform\ncomparably to GPT-4 for causal tasks."}]}