{"title": "Unifying Generative and Dense Retrieval for Sequential Recommendation", "authors": ["Liu Yang", "Fabian Paischer", "Kaveh Hassani", "Jiacheng Li", "Shuai Shao", "Zhang Gabriel Li", "Yun He", "Xue Feng", "Nima Noorshams", "Sem Park", "Bo Long", "Robert D Nowak", "Xiaoli Gao", "Hamid Eghbalzadeh"], "abstract": "Sequential dense retrieval models utilize advanced sequence learning techniques to compute item and user representations, which are then used to rank relevant items for a user through inner product computation between the user and all item representations. However, this approach requires storing a unique representation for each item, resulting in significant memory requirements as the number of items grow. In contrast, the recently proposed generative retrieval paradigm offers a promising alternative by directly predicting item indices using a generative model trained on semantic IDs that encapsulate items' semantic information. Despite its potential for large-scale applications, a comprehensive comparison between generative retrieval and sequential dense retrieval under fair conditions is still lacking, leaving open questions regarding performance, and computation trade-offs. To address this, we compare these two approaches under controlled conditions on academic benchmarks and propose LIGER (LeveragIng dense retrieval for GEnerative Retrieval), a hybrid model that combines the strengths of these two widely used methods. LIGER integrates sequential dense retrieval into generative retrieval, mitigating performance differences and enhancing cold-start item recommendation in the datasets evaluated. This hybrid approach provides insights into the trade-offs between these approaches and demonstrates improvements in efficiency and effectiveness for recommendation systems in small-scale benchmarks.", "sections": [{"title": "1 Introduction", "content": "Sequential recommendation methods (Kang and McAuley, 2018b; Zhou et al., 2020), have predominantly relied on advanced sequential modeling techniques (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017; Radford et al., 2019) to learn dense embeddings for each item and user. Using these embeddings, the most relevant items are retrieved through maximum inner product search. Despite its effectiveness, this approach requires comparing every item in the dataset during the retrieval stage, which can be computationally expensive as the number of items grows. Furthermore, each item must be represented by a unique embedding, which needs to be learned and stored, adding to the complexity.\nIn contrast, generative retrieval (Rajput et al., 2024) is a new approach, which deviates from the embedding- centric paradigm. Instead of generating embeddings, this approach utilizes a generative model to directly predict the item index. To better capture the sequential patterns within item interactions, items are indexed by \"semantic IDs\" (Lee et al., 2022a), which encapsulate their semantic characteristics. During the recommendation process, the model employs beam search decoding to predict the semantic ID (SID) of the next item based on the user's previous interactions. This method not only reduces the need for storing individual item embeddings but also enhances the ability to capture deeper semantic relationships within the data. Additionally, adjusting the temperature during generation can naturally produce more diverse recommendations.\nTo distinguish the first paradigm from generative retrieval, which also leverages sequential information,"}, {"title": "2 Analysis of Generative and Dense Retrieval Methods", "content": "In this section, we first introduce the generative retrieval (Rajput et al., 2024) and dense retrieval (Hou et al., 2022b) formula (see Section 2.1 and 2.2). Then in Section 2.3, we examine the performance difference between generative retrieval and sequential dense retrieval methods, and then discuss the challenges generative retrieval faces in handling item cold-start scenarios in Section 2.4."}, {"title": "2.1 Generative Retrieval Review", "content": "The generative retrieval approach such as TIGER (Rajput et al., 2024) typically follows a two-stage training process. The first stage involves collecting textual descriptions for each item based on their attributes. These descriptions serve as inputs to a content model (e.g., a language encoder) that produces item's text embeddings, subsequently quantized by an RQ-VAE (Lee et al., 2022a) to attribute a semantic ID for each item. Formally, for each item $i \\in I$, we collect its $p$ key-value attribute pairs $\\{(k_1, v_1), (k_2, v_2), ..., (k_p, v_p)\\}$ and format them into a textual description: $T_i = prompt(k_1, v_1,\\cdots, k_p, v_p)$. The textual description $T_i$ is then passed to the content model, yielding the text representation $e^{\\text{text}}_i$ for each item. We refer readers to Rajput et al. (2024); Lee et al. (2022a) for the training details of the RQ-VAE module. After the RQ-VAE is trained, we obtain the $m$-tuple semantic ID $(s^1_i, s^2_i, ..., s^m_i)$ for each item $i$. Notably, an $m$-tuple semantic ID, with a codebook size $t$, can theoretically represent $t^m$ unique items.\nIn the second stage of training, the item text representation $\\{e^{\\text{text}}_i\\}$ and the trained RQ-VAE model are discarded, retaining only the semantic IDs. For each interaction history indexed by item IDs $\\{i_1, i_2, \\cdots, i_n\\}$, the item IDs are replaced with their corresponding semantic IDs: $\\{(s^1_{i_1}, s^2_{i_1}, ..., s^m_{i_1}), (s^1_{i_2}, s^2_{i_2}, ..., s^m_{i_2}), ..., (s^1_{i_n}, s^2_{i_n}, ..., s^m_{i_n})\\}$. Given the semantic IDs of the last $n$ items a user interacted with, the Transformer model is then optimized to predict the next semantic ID sequence $(s^1_{n+1}, s^2_{n+1}, ..., s^m_{n+1})$.\nDuring inference, a set of candidate items is retrieved using beam search over the trained Transformer, selecting items based on their semantic IDs. A visual representation of the generative retrieval method is provided in Figure 2 (Lower Left).\nIt is worth noting that although the item text representations are excluded from the second stage of training, they still contribute to the information utilized in developing this approach. To ensure a fair comparison, we incorporate this information into the development of a comparable dense retrieval method in the following section, thereby accounting for the impact of these embeddings on the overall performance."}, {"title": "2.2 Sequential Dense Retrieval in Transductive Setting", "content": "Sequential dense retrieval methods typically consist of two main components: (1) learning item representations through sequence modeling, and (2) performing retrieval using dot-product search. To enhance the learning of item representations, several dense retrieval methods such as Hou et al. (2022b) employ an transductive setting, where item content information is integrated through text representation to enable transferable representation learning.\nBuilding on these insights, we implement the dense retrieval method as follows: For each item $i$, we first obtain its text representation $e^{\\text{text}}_i$ using the procedure described in the first stage of Section 2.1. Additionally, we retrieve the learnable item embedding $e_i = Embd(i)$ from the embedding table Embd(\u00b7) and compute the item's positional embedding $e^{\\text{pos}}_i$. The input embedding for each item is then computed as:\n$E_i = e_i + e^{\\text{text}}_i + e^{\\text{pos}}_i,$"}, {"title": "2.3 The Observed Performance Difference", "content": "As detailed in Section 2.1, the generative retrieval model leverages both item text representations and sequential interaction information. In Section 2.2, we introduce sequential dense retrieval methods inspired by Hou et al. (2022c), designed to fully incorporate these sources of information throughout the training process.\nTo ensure a fair comparison between the generative retrieval and dense retrieval methods, we maintain consistency in model architecture, data pre-processing, and information utilization. The specific details of our experiment setup are described in Section 4.1 and in Appendix B.1. Due to the unavailability of TIGER's codebase, we conducted an extensive hyperparameter search, and configured the dense model's hyperparameters to align with the tuned TIGER settings. However these results, shown in Figure 1 (Right), indicate a performance difference between the generative and dense retrieval methods in the datasets we evaluated.\nThere are notable differences between the two implemented methods: (1) Different Item Indexing and Number of Embeddings: As discussed in Section 2.1, representing N item requires the dense retrieval method to learn and"}, {"title": "2.4 Challenges in Cold-Start Item Prediction with Generative Retrieval Models", "content": "As partially discussed in the previous section, our investigation extends to the cold-start item generation problem, a critical issue in the dynamic environment of real-world recommendation systems. As new items are continuously introduced, they often lack sufficient user interactions, which impedes their predictability until a significant amount of interaction data is gathered. For dense retrieval in the transductive setting (Hou et al., 2022c), the inclusion of item's text representations provide some prior information, thus partially retaining the ability to retrieve cold-start items, resulting in non-zero performance of cold-start item prediction shown in Figure 1 and Figure 3."}, {"title": "3 Methodology", "content": "The notion that \"there is no free lunch\" holds true in the context of retrieval methods. As discussed in the previous section, a performance difference is observed between the generative retrieval and dense retrieval methods we implemented, with generative retrieval facing challenges in generating cold-start items. The improved performance of dense retrieval, however, comes with higher storage, learning, and inference costs. On the other hand, generative retrieval offers efficiency and the ability to generate diverse recommendations (Rajput et al., 2024), but its performance lags behind in the datasets we evaluated.\nThe trade-offs between approaches are summarized in Table 1, where N represents the total number of items, t denotes the total number of semantic IDs, and K is the number of candidates to be retrieved during inference. Here, we denote the inference cost as the number of comparisons required for each method. It is worth noting that the actual inference time depends on implementation details and infrastructure optimization, which are beyond the scope of this work.\nIn this section, we propose a hybrid method, called LIGER, that combines the strengths of both approaches. Our goal is to improve upon the existing generative retrieval method: enabling it to generate cold-start items and bridging the gap with dense retrieval. To achieve this, we integrate text representations into the sequential model training phase of the generative retrieval method. The associated costs of LIGER are detailed in the last column of Table 1."}, {"title": "4 Experimental Setup and Results", "content": "In this section, we present the experimental results across various datasets and baseline methods, showcasing the performance on both in-set and cold-start items. Specifically, we assess the cold-start performance by testing on items that are unseen during training, which is determined by the dataset statistics."}, {"title": "4.1 Experimental Setup", "content": "Datasets. We evaluate LIGER on four datasets, preprocessing them using the standard 5-core filtering method (Zhang et al., 2019; Zhou et al., 2020). This process removes items with fewer than 5 users and users with fewer than 5 interactions. Additionally, we truncate sequences to a maximum length of 20, retaining the most recent items. Detailed statistics of the resulting datasets are provided in Appendix B.3.\n\\textbullet Amazon Beauty, Sports, and Toys (He and McAuley, 2016): We use the Amazon Review dataset (2014), focusing on three categories: Beauty, Sports and Outdoors, and Toys and Games. For each item, we construct embeddings by incorporating four key attributes: title, price, category, and description.\n\\textbullet Steam (Kang and McAuley, 2018b): The dataset comprises online reviews of video games, from which we extract relevant attributes to construct item embeddings. Specifically, we utilize the following attributes: title, genre, specs, tags, price, and publisher. To reduce the dataset size and make it more manageable, we apply subsampling by selecting every 7th sequence, thereby retaining a representative subset of the data.\nWhen generating the item text representations, the item attributes are processed using the sentence-T5 model Ni et al. (2021) (XXL).\nSemantic ID Generation. Utilizing the text representations generated from the sentence-T5 model, we employ a 3-layer MLP for both the encoder and decoder in the RQ-VAE (Lee et al., 2022a). The RQ-VAE features three levels of learnable codebooks, each with a dimension of 128 and a cardinality of 256. We use the AdamW optimizer to train the RQ-VAE, setting the learning rate at 0.001 and the weight decay at 0.1. To prevent collisions (i.e., the same semantic ID representing different items), following Rajput et al. (2024) we append an extra token at the end of the ordered semantic codes to ensure uniqueness.\nSequential Modeling Architecture and Training Algorithm. For the generative model, we utilize the T5 (Raffel et al., 2020) encoder-decoder model, configuring both the encoder and decoder with 6 layers, an embedding dimension of 128, 6 heads, and a feed-forward network hidden dimension of 1024. The dropout rate is 0.2. The dense retrieval model designed in Section 2.2 employs only the T5-encoder with 6 layers, while maintaining the same hyper-parameters. We use the AdamW optimizer with a learning rate of 0.0003, a weight decay parameter of 0.035, and a cosine learning rate scheduler. Additional details are presented in Appendix B.1.\nEvaluation Metrics. We assess the model's performance using Normalized Discounted Cumulative Gain (NDCG)@10 and Recall@10. For dataset splitting, we adopt the leave-one-out strategy following (Kang and McAuley, 2018b; Zhou et al., 2020; Rajput et al., 2024), designating the last item as the test label, the preceding item for validation, and the remainder for training. During training, early stopping is applied"}, {"title": "4.2 Experimental Results", "content": "The results from the benchmark datasets are presented in Table 2, where the mean and standard deviation are calculated across three random seed runs. Traditional item-ID-based methods, such as SASRec exhibit poor in-set performance compared to semantic-ID-based models. However, when attribute information is included, models like FDSA and S\u00b3-Rec show improved in-set performance. Nevertheless, their performance on cold-start items remains subpar due to the static nature of item embeddings. In contrast, models that utilize text representations and pre-training, such as UniSRec and RecFormer, demonstrate enhanced capabilities in handling cold-start item scenarios. The inclusion of text embeddings during pre-training enables these models to better handle unseen items. TIGER, which is a semantic-ID-based generative retrieval model, outperforms item-ID-based methods in terms of in-set performance but still struggles with cold-start item generation.\nOur model, LIGER, builds upon TIGER by using semantic-ID-based inputs and combining dense retrieval with semantic ID generation as outputs. This approach significantly improves upon the TIGER method and enables effective generation of cold-start items. Across all datasets, our method consistently achieves either the best or second-best performance, closely followed by modality-based baselines such as UniSRec and RecFormer. We adopt a hybrid approach for our reporting, where we use generative retrieval to retrieve 20 items and then rank them with cold-start items using dense retrieval. Comprehensive result including performance of LIGER with different number of retrieved items from generative retrieval is presented in Table 4. Additional ablation study on each component of LIGER is present in Appendix D."}, {"title": "5 Discussion", "content": "Addressing Cold-Start Items with Hybrid Retrieval Models. Generative retrieval method's struggle with cold-start items primarily stems from overfitting to familiar semantic IDs during training, as discussed in Section 2.4. To mitigate this issue, LIGER efficiently combines dense retrieval with generative retrieval. Specifically, LIGER first generates a small set of K candidates (where K\u226aN) using generative retrieval, which is then augmented with the cold-start item set. This approach leverages the efficiency of generative retrieval to reduce the candidate pool while ensuring cold-start items are represented. The dense retrieval component further enhances cold-start performance by leveraging item text embeddings as prior information. As shown in Table 4, this integration ensures that when generative retrieval retrieves fewer than N items, the model maintains robust performance in cold-start scenarios, comparable to dense retrieval only.\nComparative Performance with Current Dense Retrieval Methods. While LIGER demonstrates competitive performance against existing baselines, its primary objective is to strike a balance between the generative and dense retrieval frameworks. As discussed in previous sections, there are observed performance differences between these two methods on the dataset we tested, even when using the same input information and model architecture. The results presented in this work aim to shed light on potential future directions for integrating these approaches, paving the way for the development of more robust and efficient recommendation systems.\nObserved Performance Differences are Contextual to Small-Scale Datasets. We want to note that the performance differences observed in this study are influenced by various factors, including dataset size, implementation details, and the distribution of data collected under specific paradigms. Additionally, we are aware of"}, {"title": "6 Conclusion", "content": "In this work, we conducted a comprehensive comparison between dense retrieval methods and the emerging generative retrieval approach. Our analysis revealed the limitations of dense retrieval, including high computational and storage requirements, while highlighting the advantages of generative retrieval, which uses semantic IDs and generative models to enhance efficiency and semantic understanding. Furthermore, we have identified the challenges faced by generative retrieval, particularly in handling cold-start items and matching the performance of dense retrieval. To address these challenges, we introduced a novel hybrid model, LIGER, that combines the strengths of both approaches. Our findings demonstrate that our hybrid model surpasses existing models in handling cold-start scenarios and achieves advanced overall performance on benchmark datasets.\nLooking ahead, the fusion of dense and generative retrieval methods holds tremendous potential for advancing recommendation systems. Our research provides a foundation for further exploration into hybrid models that capitalize on the strengths of both retrieval types. As these models continue to evolve, they will become increasingly practical for real-world applications, enabling more personalized and responsive user experiences."}, {"title": "Appendix", "content": "A Related Work\nGenerative Retrieval. The concept of generative retrieval was first proposed by Tay et al. (2022) within the domain of document retrieval. This paradigm shifts from traditional search and retrieval methods by encoding document information directly into the weights of a Transformer model. Subsequent studies (De Cao et al., 2020; Bevilacqua et al., 2022; Feng et al., 2022) have expanded on this foundation, enhancing document retrieval through improvements in indexing (Lee et al., 2022b,c; Wang et al., 2022), and the efficient continual database updates (Mehta et al., 2022; Kishore et al., 2023; Chen et al., 2023).\nIn the realm of sequential recommendation systems, Rajput et al. (2024) is the first work to leverage the generative retrieval techniques. The target item is directly generated given a user's interaction history, rather than selecting top items by ranking all relevant user-item pairs. A key challenge in generative retrieval is striking a balance between memorization and generalization when encoding items. To address this, semantic IDs have been proposed by leveraging RQ-VAE models (Lee et al., 2022a; Van Den Oord et al., 2017). These models encode content-based embeddings into a compact, discrete semantic indexer that captures the hierarchical structure of concepts within an item's content, proving to be scalable in industrial applications (Singh et al., 2023). Recent developments (Hou et al., 2022a) have expanded semantic-ID-based generative retrieval to include contrastive learning (Jin et al., 2024), multimodal integration (Liu et al., 2024a), tokenization techniques (Sun et al., 2024), learning-to-rank methods (Li et al., 2024), and improved RQ-VAE training (Qu et al., 2024).\nSequential Dense Recommendation. Traditional sequential dense recommender models follow the paradigm of learning representations of users, items, and their interactions with multimodal data. Early work (Hidasi et al., 2015) proposed architectures based on traditional Recurrent Neural Networks (RNNs), while later studies (Kang and McAuley, 2018a; Sun et al., 2019; de Souza Pereira Moreira et al., 2021) have shifted towards the Transformer architecture to enhance performance. Besides capturing the user-item interaction history pattern with the sequential modeling, extra features such as item attributes (Zhang et al., 2019; Zhou et al., 2020) has been utilized to further improve the performance. With the recent advancements in Large Language Models (LLMs), several works have explored using these models as the backbone for recommender systems, aligning item representations with LLMs to improve recommendation performance (Li et al., 2023b; Hou et al., 2022c; Cao et al., 2024a; Zheng et al., 2024). In this work, we aim to merge the sequential dense recommendation approach with generative retrieval techniques, assessing performance gaps and computational costs, and proposing a hybrid method that combines the strengths of both paradigms.\nCold-start Problem. Traditional challenges such as long-tail and cold-start items continue to hinder recommen- dation systems. The long-tail items issue arises from skewed distributions where a few popular items dominate user interactions (Zhang et al., 2022, 2020), while the cold-start problem arises when new items are introduced without any historical interaction data. Recent studies (Hou et al., 2022c; Li et al., 2023b) have shown that textual embeddings can provide a robust prior for tackling the cold-start issue, and further improvements have been achieved by integrating pretrained LLMs (Huang et al., 2024; Sanner et al., 2023) and knowledge graphs (Frej et al., 2024). In this work, we explore the cold-start problem within the context of generative retrieval and propose a hybrid method that combines dense retrieval with textual embeddings to effectively mitigate this issue. A concurrent work by Ding et al. (2024) also investigates the cold-start issue in generative retrieval and proposes using dense retrieval as a drafter model to facilitate cold-start item generation."}, {"title": "B Experimental Details", "content": "B.1 Implementation Details of Dense Retrieval\nIn Section 4.1, we provide the TIGER implementation details. Here, we describe the implementation details of the dense retrieval method we developed.\nThe dense retrieval method uses the same T5-encoder architecture, configured with 6 layers, an embedding"}, {"title": "B.2 Implementation Difference Between UniSRec and our Dense Model", "content": "Although the dense retrieval model we implemented is inspired by UniSRec (Hou et al., 2022c), we made several modifications to simplify the model and align it with the best-tuned TIGER architecture. Specifically:\n1. We use the same content model as TIGER: T5-XXL, whereas UniSRec uses BERT.\n2. We adopt the same encoder architecture as TIGER, which consists of 6 T5 encoder layers with 6 attention heads, an embedding dimension of 128, and a feed-forward network hidden dimension of 1024. In contrast, UniSRec uses a custom Transformer block with 2 layers, 2 attention heads, an embedding size of 300, a hidden size of 256, and different implementations for LayerNorm and positional embeddings compared to the T5 model 1.\n3. We replace UniSRec's mixture-of-expert layer and whitening layer with a simple Linear layer.\n4. We train the dense retrieval model from scratch, whereas UniSRec relies on pretraining using the Amazon 2018 datasets."}, {"title": "B.3 Data Statistics", "content": "In Table 3, we present the statistics of the datasets used in our evaluation."}, {"title": "B.4 Baselines", "content": "We compare our methods with five state-of-the-art Item-ID-based dense retrieval methods, including:\n1. SASRec (Kang and McAuley, 2018b). A self-attention based sequential recommendation model that learns to predict the next item ID based on the user's interaction history.\n2. FDSA (Zhang et al., 2019) [feature-informed]. This method extends SASRec by incorporating item features into the self-attention model, allowing it to leverage prior information about cold-start items through their attributes.\n3. S\u00b3-Rec (Zhou et al., 2020) [feature-informed]. A self-attention based model that utilizes data correlation to create self-supervision signals, improving sequential recommendation through pre-training.\n4. UnisRec (Hou et al., 2022b) [modality-based]. A model that learns universal item representations by utilizing associated description text and a lightweight encoding architecture that incorporates"}, {"title": "D Ablation Study", "content": "LIGER combines dense retrieval and generative retrieval paradigms into a unified framework. As described in Section 3 and illustrated in Figure 2, for each item i with semantic ID (s1, s2,\u2026, s), we construct the input embedding for each semantic ID as:\n$E^s_i = e^{s_i} + e^{text}_i + e^{Pos}_i + e^{pos}_i,$\nwhere $e^{s_i}$ is the learnable embedding for the $s^j_i$ semantic ID, $e^{text}_i$ is the item's text representation, $e^{Pos}_i$ is the positional embedding for the item, and $e^{pos}_i$ is the positional embedding for the semantic ID. The final embedding for item i is then represented as: $E_i = [E_{s_1}, E_{s_2},..., E_{s_m}]$.\nDuring training, the model is optimized with two objectives: the cosine similarity loss and the next-token prediction loss on the semantic IDs of the next item: The cosine similarity loss ensures that the model learns to align the encoder's output embedding with the text representation of the next item, and the next-token prediction loss supervise on the next item's semantic ID tuple prediction performance. To summarize, the LIGER framework is structured as follows:\n1. Input: Semantic ID (SID) and item's text representation;\n2. Output: Predictions for:\n(a) The next item's semantic ID through the SID head.\n(b) The next item's text representation through the embedding head.\nThe ablation study investigates the impact of each component, as shown in Figure 6:\n\\textbullet LIGER (detach) detaches the gradient updates from the SID head in LIGER to examine the importance of multi-objective optimization through the SID head;\n\\textbullet TIGER (T) removes the embedding head from LIGER, focusing solely on the SID head and the text representation as input;\n\\textbullet TIGER further simplifies TIGER (T) by removing the item text representation input, reducing the model to the generative retrieval method described in Section 2.1;\n\\textbullet Dense (SID) removes the SID head from LIGER, retaining only the dense retrieval mechanism with SID as input;\n\\textbullet Dense replaces the SID input with ID in Dense (SID), reducing the model to the dense retrieval method in transductive setting, as detailed in Section 2.2.\nFigure 7 presents the ablation results in terms of Recall@10 across four datasets: Beauty, Sports, Toys, and Steam. The performance is analyzed with respect to the number of candidates retrieved by the SID head (K).\nFirst, comparing TIGER to TIGER(T), we observe that TIGER(T) consistently performs the same or slightly better, demonstrating the positive impact of incorporating the item's text representation as input. However,"}]}