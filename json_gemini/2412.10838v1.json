{"title": "Deep Learning Models for Colloidal Nanocrystal Synthesis", "authors": ["Kai Gu", "Yingping Liang", "Jiaming Su", "Peihan Sun", "Jia Peng", "Naihua Miao", "Zhimei Sun", "Ying Fu", "Haizheng Zhong", "Jun Zhang"], "abstract": "Colloidal synthesis of nanocrystals usually includes complex chemical reactions and multi-step crystallization\nprocesses. Despite the great success in the past 30 years, it remains challenging to clarify the correlations between\nsynthetic parameters of chemical reaction and physical properties of nanocrystals. Here, we developed a deep\nlearning-based nanocrystal synthesis model that correlates synthetic parameters with the final size and shape of\ntarget nanocrystals, using a dataset of 3500 recipes covering 348 distinct nanocrystal compositions. The size and\nshape labels were obtained from transmission electron microscope images using a segmentation model trained\nwith a semi-supervised algorithm on a dataset comprising 1.2 million nanocrystals. By applying the reaction\nintermediate-based data augmentation method and elaborated descriptors, the synthesis model was able to predict\nnanocrystal's size with a mean absolute error of 1.39 nm, while reaching an 89% average accuracy for shape\nclassification. The synthesis model shows knowledge transfer capabilities across different nanocrystals with inputs\nof new recipes. With that, the influence of chemicals on the final size of nanocrystals was further evaluated,\nrevealing the importance order of nanocrystal composition, precursor or ligand, and solvent. Overall, the deep\nlearning-based nanocrystal synthesis model offers a powerful tool to expedite the development of high-quality\nnanocrystals.", "sections": [{"title": "Main", "content": "Colloidal nanocrystals are a representative class of nanomaterials, widely applied in many cutting-edge technologies 1 such as optoelectronics 2, energy catalysis 3,4, and biomedicine 5 due to their pronounced size-dependent\neffects 6-8 Over the past 30 years, advances in colloidal chemistry have led to the development of numerous\nsynthetic methods and recipes for fabricating nanocrystals with controllable size and shape 9-13 Some nanocrys-\ntals, such as CdSe and InP quantum dots, have even achieved scale-up production 14\u201317 A typical nanocrystal\nsynthesis involves three main crystallization processes: conversion of precursors to monomers (pre-nucleation),\naggregation of monomers into nuclei (nucleation), and growth of nanocrystals through monomer diffusion 18,19\nThe final size and shape of nanocrystals are strongly correlated with these processes However, the intrinsic\nrelationship between chemical reactions and crystallization processes remains poorly understood. It is still chal-\nlenging to quantitatively describe how synthetic parameters (temperature, reactant ratios, and ligand types, etc.)\naffect the final size and shape of nanocrystals.\n20,21\nMachine learning provides a powerful approach for establishing mathematical relationships among complex\nvariables, showing potential in material design and property prediction 22-28 In the context of nanocrystal\nsynthesis 29,30, previous studies have reported the use of machine learning to predict the size and absorption spectra\nof nanocrystals 31-33 Due to the lack of comprehensive datasets and generalizable descriptors, these machine\nlearning models have limited knowledge transfer capabilities to predict the properties of target nanocrystals. In"}, {"title": "Methods", "content": "The generation of weak labels\nNanocrystal segmentation models are typically trained on a limited set of expensive and manually labeled images\nwithout sufficiently utilizing large-scale unlabeled images. A semi-supervised learning method with a two-step\nweak label generation process was developed for training the segmentation model on unlabeled images, as shown\nin Extended Data Fig. 1.\nNanocrystal detection. In the first step, a trained Cascade-RCNN 42 model is utilized to predict bounding boxes\nfor nanocrystals in the unlabeled images. The primary challenge in nanocrystal detection lies in small and dense\ntargets. Two strategies are adopted to improve detection accuracy: i) Using the Normalized Gaussian Wasserstein\nDistance (NWD) 43 instead of the Intersection over Union (IoU) metric. Specifically, the bounding boxes are\nmodeled as 2D Gaussian distributions, with each bounding box determined by its center coordinates, size, and\nshape parameters. The IoU metric only considers the overlapping area of bounding boxes and is sensitive to minor\npositional deviations. The NWD considers the position, shape, and size of the bounding boxes to detect small\nand dense targets. ii) Using HRNet 44 as the backbone network. HRNet maintains high-resolution feature maps\nthroughout the network and enhances the recognition ability of small targets through multi-scale fusion.\nUncertainty mask segmentation. In the second step, each nanocrystal is subdivided into boundaries to generate the\nweak labels for training segmentation model on unlabeled images. Binary segmentation of each nanocrystal and\nthe background within the predicted bounding box is performed using Otsu's Thresholding method to generate the\nmask image. Subsequently, erosion and dilation operations are implemented on these segmented mask images to\ndelineate the boundaries of the segmentation masks. These boundaries are considered as regions of uncertainty\nin the segmentation. During the process of semi-supervised training, these uncertain regions are omitted from the\nloss calculation to avoid negatively impacting model performance. The regions outside these boundaries, which\nencompass relatively accurate foreground and background areas, are used for training the model on unlabeled im-\nages. Specifically, binary cross-entropy loss is applied to the foreground and background to improve the accuracy\nof segmentation model.\nNanocrystal segmentation model\nTEM images dataset. The segmentation model is first pretrained on a mixed labeled dataset and an unlabeled\ndataset, and then fine-tuned on our manually labeled dataset. This mixed dataset contains 4,880 images sourced\nfrom 12 different datasets, with totally 289,447 annotated instances (see Supplementary Table 1). The unlabeled\ndataset consists of 7,344 images with approximately 920,000 nanocrystals. After pretraining, the segmentation\nmodel undergoes further fine-tuning on our manually annotated dataset, consisting of 523 images and 49,976\nnanocrystals. We chose another 80 images containing 6,827 annotated nanocrystals to serve as a validation set.\nTraining segmentation model. A neural network based on U-Net 45 is pretrained on the mixed labeled dataset\nand unlabeled dataset with generated weak labels using specially designed losses, as shown in Fig. 2. This neural\nnetwork comprises an encoder and two decoders, producing a triplet of outputs: seed maps, horizontal gradients,\nand vertical gradients. These outputs together infer the final instance maps. For labeled images in the mixed\ndataset, ground truth labels for this triplet are derived from the full labels and employed for supervised training.\nSpecifically, the seed map Y2 indicates the pixel probabilities, supervised by the binary mask image P indicating\nwhether a given pixel is inside the nanocrystal target. This binary mask image P can be generated from complete\ninstance labels or weak labels. The use of seed maps is insufficient to differentiate between individual nanocrys-\ntals. An instance decoder is employed to predict and segment different nanocrystal boundaries. The instance\ndecoder outputs horizontal gradients Yo and vertical gradients Y1, which are supervised by an Auxiliary vector"}, {"title": null, "content": "flow representation computed from the complete instance labels. It transforms the full labels into true horizontal\ngradients H and vertical gradients V of the same size as the original image. These gradients are intended to locally\ntransform pixels into other pixels within the cell and globally transform pixels into fixed points of the final gradient\nvector field over multiple iterations. These fixed points are selected as the centers of nanocrystals. Its horizontal\nand vertical gradients H and V represent the two vector fields of the Auxiliary vector flow representation. The\nloss function L can be written as: L = ||Yo \u2013 H||\u00b2 + ||Y1 \u2013 V||\u00b2 + BCE (\u03c3 (Y2), P), where o is the sigmoid\nfunction, and BCE is the binary cross-entropy. For unlabeled images, gradient labels cannot be calculated due to\nunclear boundaries of weak labels. Weak labels are solely used to supervise the seed maps Y'2 predicted from\nunlabeled images, using binary masks P' without uncertain regions that is not involved in the semi- supervised\nloss L semi calculation:\nL sim BCE (\u03c3 (Y'2), P').\nImplementation details. The input images for the segmentation model are resized to a resolution of 512 \u00d7 512\npixels. The pre-trained segmentation model is trained using stochastic gradient descent over 100 epochs, with\na learning rate initially set at 0.2, momentum at 0.9, a batch size of 8, and a weight decay of 0.00001. The\nlearning rate begins at zero and linearly increases to 0.2 during the first 10 epochs to address initial training\ninstability. Subsequent fine-tuning adopts the same learning rate strategy but extends over 200 epochs. To augment\nthe training dataset, random spatial transformations are applied to both the training images and their corresponding\nlabels, including random rotations, scaling, and translations, followed by the crop of a 512 \u00d7 512 pixel segment\nfrom the center of the synthesized image. Additionally, ColorJitter is used to randomly adjust the brightness,\ncontrast, saturation, and hue of the images, enhancing data robustness and model generalization. The model is\ntrained on an Nvidia GeForce RTX 3090 GPU, achieving convergence in approximately six hours.\nEvaluation metrics. In evaluating segmentation model, the mean Intersection over Union (mIOU) serves as a\nfundamental metric. This metric quantifies the overlap between the predicted segmentation and the ground truth\nby calculating the ratio of their intersection to their union. A predicted segmentation is considered accurate when\nits IOU with any ground truth instance surpasses a pre-defined threshold. Following the assessment of overlap\naccuracy, mean Average Precision (AP) at various thresholds is used to measure the precision of the model's\npredictions. Specifically, AP50, AP75, and AP90 represent the average precision at IOU thresholds of0.50,0.75,\nand 0.90, respectively. The weights of the model that demonstrates the highest AP50 on the validation set during\ntraining are preserved.\nUnsupervised shape clustering. Unsupervised shape clustering is employed as a robust method to examine the\ndistribution and variance in physical geometry under diverse synthesis conditions. Several shape descriptors are\nextracted from the instance masks generated by our segmentation model, including solidity, convexity, eccentricity,\naspect ratio, and circularity. The distributions of these descriptors are illustrated in Supplementary Fig. 2. K-\nmeans clustering and t-distributed stochastic neighbor embedding are applied to shape descriptors to categorize\nand visualize the impact of synthesis parameters on shape evolution.\nCalculation of equivalent circle diameter. Considering the differences in the shapes of nanocrystals, we defined\nthe equivalent circle diameter to compare their sizes. Masks for each nanocrystal in the image are obtained by the\nsegmentation model, enabling to calculate the pixel area S. Then, EasyOCR is employed to identify the numerical\nvalue and units of the scale bar in TEM images, providing the true length of the scale bar l true. The pixel length\nI pixel of the scale bar is then obtained using a simple global thresholding strategy. Therefore, the equivalent circle\ndiameter d of the nanocrystal can be calculated using the formula: $d=\\sqrt{\\frac{4S}{\\pi}}$. Therefore, the size labels\nD for each recipe can be calculated as: $D = (d\u2081 + d2 + ... + d_N)/N$, where N is the statistical number of\nnanocrystals.\nNanocrystal synthesis model\nConstruction of chemical descriptors. The construction process of chemical descriptors is shown in Extended\nData Fig. 5a. Firstly, the names or formulas of chemicals are extracted from the recipe dataset. The 3D structures of\nall crystalline chemicals are acquired from the open-access Materials Project database 46. While, the 3D structures\nof organic molecules are obtained through DFT calculations performed in Materials Studio (DMol 3 module) 47\nThe generalized gradient approximation in the Perdew-Burke-Ernzerhof form with the double numerical basis sets\nplus the polarization functional are adopted 48,49. The density mixing fraction of 0.2 with direct inversion in the"}, {"title": null, "content": "iterative subspace and orbital occupancy with smearing of 0.005 Ha are employed. The self-consistent field toler-\nance is set to 10-6 Ha/ atom. The total energy, maximum force and maximum displacement are set as 10-6 Ha/\natom, 0.002 Ha/\u00c5 and 0.005\u00c5, respectively for geometry optimization. Subsequently, a pre-trained GNN model\n50 is fine-tuned using 1,797 chemical structures in our recipes to extract the chemical descriptors. These descriptors\nare represented as 512-dimensional features, providing a comprehensive numerical representation of the structural\ncharacteristics.\nPipeline. Given the variable number of chemicals in different recipes, the transformer algorithm 51 (refer to\nSupplementary Fig. 5) is utilized to train the synthesis model for the size and shape prediction. The transformer\nis particularly adept at handling unordered, variable-length input data and employ self-attention mechanisms to\ndiscern relationships among chemicals in sequences of any length. This capability is vital for understanding and\npredicting the interactions among chemicals and their influence on the final chemical reaction outcomes.\nInput preprocessing. For the transformer's input, chemical descriptors are amalgamated with reaction descriptors\nto create a feature representation sequence for the various materials in a given recipe. Reaction descriptors include\ninjection temperature Tj, reaction temperature T\u2081, reaction timet, heating rate Sp, and the molar quantities Mol\ncorresponding to the chemicals. Considering the uneven distribution of reaction conditions and molar quantities\n(Supplementary Fig. 8), a robust scaler is employed for standardization, scaling features using statistics that are\nresilient to outliers. Subsequently, a learnable CLS token is incorporated into the input sequence as a descriptor\nfor chemical reactions. This token captures global contextual information of the entire sequence and forms the\nfoundation for the final output predictions. The sequence, including the CLS token, undergoes a linear projection\nbefore entering the multi-layer transformer architecture.\nTransformer layers pass. For each layer, layer normalization is performed at the beginning, stabilizing the training\nof deep neural networks. It normalizes the inputs across the features instead of across the batch, and this is done\nfor each individual sample. It accelerates the training and converging quicker by reducing the internal covariate\nshift. After normalization, the transformer processes the data through a multihead self-attention mechanism. Each\ninput element is transformed into three vectors: queries (Q), keys (K), and values (V). Self-Attention mechanism\nallows a model to weigh the relevance of different parts of an input sequence independently of their position in\nthe sequence. These vectors are computed by multiplying the input embeddings by three learned weight matrices\nspecific to queries, keys, and values, respectively. In multi-head self-attention, the model applies multiple sets\nof Q, K, and V matrices, each representing a different \"head\". Each head can potentially learn to attend to\ndifferent parts of the input sequence, capturing various aspects of the data. After self-attention, position-wise\nfeed-forward network applies a set of linear transformations to each element separately and identically. The feed-\nforward network consists of two linear transformations with a ReLU fiction in between. This module can enhance\nthe representation capacity of the transformer without considering the sequence's positional order, focusing instead\non transforming the features. Finally, similar to the post self-attention phase, the output from the feed-forward\nnetwork is added back to the input of the feed-forward network itself, i.e., \"Add & Norm\" operator. This addition is\nagain followed by layer normalization. Finally, the features of the CLS token after the transformer are concatenated\nwith the reaction descriptors containing Tj, Tr, t, and Sp, and fed into linear layers with ReLU activation functions\nto make the final predictions regarding the size and shape of the nanocrystals.\nData augmentation. To overcome the challenge of limited training data, a novel data augmentation method is\nintroduced based on reaction intermediates (Supplementary Fig. 5b). This method significantly enhances both the\nquantity and diversity of data available for model training. The foundational principle of this approach allows for\nany two chemicals within a recipe to react. For example, when considering PbO and OA, they react (assuming\na molar ratio of 1:1) to generate an intermediate chemical, PbO-OA. The 3D structure of this intermediate is\nderived through DFT calculations. This structure is then input into our fine-tuned GNN to obtain its descriptor.\nSubsequently, the descriptor of PbO-OA is incorporated into the sequence, and the molar quantities of PbO, OA,\nand PbO-OA are updated, resulting in the generation of a new recipe. More specifically, probabilities are set such\nthat there is a 50% chance to simulate a complete reaction of the materials and another 50% chance to simulate\na partial reaction. The occurrence of one or multiple intermediates simultaneously is managed by manipulating\nrandom numbers. Through this approach, each recipe can theoretically be augmented into several dozen new\nrecipes featuring different intermediates and molar quantities. With generated augmentation recipes, the model is\npretrained on both the original and augmented data to enable the model to learn the entire process from chemicals\nto intermediates to final products. Then, the model is fine-tuned using only the original data to ensure its accuracy\nand generalization capabilities when predicting actual recipes. To better adapt the model to the complexity of real\nreactions with more diverse data, negative recipes (unsuccessful preparation of nanocrystals) are collected from"}, {"title": null, "content": "experimental records and literature. These recipes do not have size and shape labels. This problem is addressed\nby adding an extra output to the model by setting a binary classification output to distinguish whether the reaction\nsuccess with formed nanocrystals and use binary cross-entropy loss for training.\nTraining synthesis model. For shape classification, the cross-entropy loss function is used for training. For size\nprediction, the Huber loss function is used for training due to the broad range and uneven distribution of size labels\n(Supplementary Fig. 7). The Huber loss function is a robust alternative to other loss functions, such as the mean\nsquared error and absolute error functions. It is designed to provide a balance between accuracy and sensitivity,\nparticularly in situations where there are large errors in size prediction. Considering the statistical error in size, a\ntolerance threshold of \u00b115% is also set for datasets containing fewer than 300 nanocrystals. If the relative error\nbetween the model's predicted size and the actual size label falls within this range, their losses are set to zero.\nThis approach is designed to mitigate prediction errors potentially arising from the model overly fitting to a small\namount of inaccurate data.\nPerformance evaluation. Inaccuracy of size labels extracted from literature due to small number of manual mea-\nsurements of nanocrystals. Therefore, a five-fold cross-validation method is employed for data with nanocrystal\ncounts greater than 300(N > 300). The model's performance is then evaluated by training and validating across all\nfive folds, and reporting the average performance. In each round of validation, four subsets, along with additional\ndata containing fewer than 300 nanocrystals for statistics, are used for training, while the remaining subset serves\nas the validation set. For shape classification, accuracy metrics are reported. For size regression, various statistical\nmeasures are assessed, including mean absolute error (MAE), mean squared error (MSE), root mean squared error\n(RMSE), mean absolute percentage error (MAPE), and the coefficient of determination (R2) . The results are as\nshown in Extended Data Table\nGeneralizability evaluation. Considering that the diversity of synthesis recipes that contains a huge potential\nspace, we designed several generalizability tests to evaluate the performance of the model for given new chemicals\nor chemical combinations. These tests involve selecting the whole recipes for one type of nanocrystal to serve\nas the validation set, while using the remaining data for training. This test is actually quite rigorous because the\nchemicals and combinations of chemicals in the validation set may be completely new. This approach allows\nus to test whether our model truly learns to generalize and make inferences from the data. Four training and\nvalidation sessions are conducted, including four representative types of nanocrystals: PbSe, Ag2 S, Ni, and\nCS2NaBiCl6. By employing this dataset division and validation set selection strategy, a more comprehensively\nand accurately evaluation can be done to show the potential of our model's generalizability in predicting the size\nof new nanocrystals, as shown in Extended Data Table 2.\nAttention weight visualization. In the transformer model, the attention weights of each chemical in the input\nsequence are computed with respect to every other chemical in each self-attention layer. These weights represent\nthe importance of each chemical to the model when processing the input information. For each chemical feature\nin the sequence, the model generates three types of vectors through different linear transformations: Q, K, and V\nThe attention weights are calculated between the Q and all K, followed by softmax normalization to scale the\nweights appropriately:\nAttention (Q, K, V) softmax = $\\frac{(QK^T)}{\\sqrt{d_k}} V$\nwhere dk is the dimension of vectors. Through this method, the self-attention mechanism dynamically focuses\non the importance of different parts of the input sequence. Then, these weights are extracted from the trained\nmodel. The attention degree of each chemical in different transformer layers is defined as the average attention\nweight between the CLS token and the chemical:\n$A_{o,l} = \\sum_{m=1}^{M} softmax( \\frac{Q_{CLS,l} K^T_{o,l}}{\\sqrt{d_k}})/M$\nwhere A0,1 denotes the attention degree of material o at layer l, and M represents the number of reactions\ninvolving material o.QCLS,1 is the query vector of the CLS token at layer l for a given recipe, while KTo, is the\ntranspose of the key vector for material o at the same layer. Supplementary Fig. 6 visualizes the attention each\nmaterial receives across different transformer layers. Fig. 4b highlights the layers where each material receives the\nmaximum attention, pinpointing where significant interactions occur in the network. This visualization technique"}, {"title": null, "content": "shows the model's focus across different stages of processing in the transformer model, helping to understand how\nit interprets and prioritizes information in chemical reaction prediction."}, {"title": "Code availability", "content": "TEM image dataset and algorithm for nanocrystal segmentation is available at https://github.com/Sharpiless/Nanocrystals-\nTEM-segmentation. Recipe dataset and algorithm for nanocrystal synthesis is available at https://github.com/\nSharpiless/Nanocrystals-Deep-Learning."}]}