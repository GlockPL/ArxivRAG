{"title": "M3SCIQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models", "authors": ["Chuhan Li", "Ziyao Shangguan", "Yilun Zhao", "Deyuan Li", "Yixin Liu", "Arman Cohan"], "abstract": "Existing benchmarks for evaluating foundation models mainly focus on single-document, text-only tasks. However, they often fail to fully capture the complexity of research workflows, which typically involve interpreting non-textual data and gathering information across multiple documents. To address this gap, we introduce M3SCIQA, a multi-modal, multi-document scientific question answering benchmark designed for a more comprehensive evaluation of foundation models. M3SCIQA consists of 1,452 expert-annotated questions spanning 70 natural language processing paper clusters, where each cluster represents a primary paper along with all its cited documents, mirroring the workflow of comprehending a single paper by requiring multi-modal and multi-document data. With M3SCIQA, we conduct a comprehensive evaluation of 18 foundation models. Our results indicate that current foundation models still significantly underperform compared to human experts in multi-modal information retrieval and in reasoning across multiple scientific documents. Additionally, we explore the implications of these findings for the future advancement of applying foundation models in multi-modal scientific literature analysis.", "sections": [{"title": "1 Introduction", "content": "In scientific research, the findings presented in a paper often serve as a foundation for further investigation. When studying research papers, researchers typically explore related and cited scholarly works to acquire additional context and insights. Simultaneously, research papers are inherently multi-modal, presenting additional and often important insights in the form of figures and tables. Such properties can pose challenges for AI systems in accurately interpreting and integrating diverse data formats across multiple research papers.\nRecent studies have showcased foundation models' remarkable performance across a variety of tasks in scientific literature understanding, including summarization (Goyal et al., 2023; Liu et al., 2023c), document-based question answering (Newman et al., 2023; Zhao et al., 2024; Xu et al., 2024), and scientific figure question answering (Masry et al., 2022; Yue et al., 2023; Lu et al., 2024b). However, current investigations are mostly confined to a single-document or text-only setting, ignoring the multi-modal and multi-document nature of scientific research, where insights are often derived from interpreting interconnected texts, figures, and tables across multiple scholarly works.\nTo address this gap, we introduce M3SCIQA, a Multi-Modal, Multi-document Scientific Question Answering benchmark. This benchmark contains 1,452 expert-annotated questions spanning 70 natural language processing (NLP) paper clusters, encompassing 3,066 papers. Each paper cluster comprises of an anchor paper and all its cited papers. Inspired by the common workflow of comparative analysis in scientific research (as illustrated in Figure 1), our benchmark simulates a process in which a finding, derived from a scientific image in the anchor paper, prompts further investigation into a specific referenced paper. This simulation enriches the benchmark by requiring the models to engage in cross-referencing among related documents, setting a new testbed for evaluating foundation models in scientific documents understanding and reasoning (Section 2.1).\nWe evaluate a wide spectrum of open-source and proprietary large language models (LLMs) and large multi-modal models (LMMs). Our experimental results reveal significant limitations in both open-source and proprietary LMMs, particularly in their ability to translate and interpret scientific images and perform effective re-ranking"}, {"title": "2 The M3SCIQA Benchmark", "content": "Our objective is to develop a challenging yet realistic QA benchmark that necessitates both multi-modal and multi-document reasoning over scientific papers. To achieve this objective, we define two types of intermediate questions in our question construction pipline:\nVisual Context Question: A question derived from a figure or table of an anchor paper, with its answer pointing to a reference paper. Each figure or table can correspond to multiple visual context questions.\nReference-based Question: A question regarding a specific detail in the reference paper. Each visual context question can correspond to multiple reference-based questions.\nThe final combined questions are created by combining each visual context question with each of its related reference-based questions. The overview of this pipeline is shown in Figure 2. In constructing M3SCIQA, expert annotators are tasked with composing visual context questions from the 70 curated anchor papers, adhering to four pre-defined reasoning categories: comparisons, data extraction, locations, and visual understanding (Table 6 in Appendix A.2). By answering a visual context question, expert annotators can pinpoint a reference paper that provides further elaboration on the topic from among all the publications cited by the anchor paper. Subsequently, GPT-42 is employed to generate reference-based questions from the identified reference paper. GPT-4 is utilized again to rephrase and combine each visual context questions with each of the related reference-based questions to form a comprehensive question that embodies both multi-modal and multi-document reasoning. Finally, expert annotators are tasked with verifying the quality of these GPT-4-assisted questions. Statistics of the benchmark are listed in Table 1; distributions of reasoning types across visual context and reference-based questions are illustrated in Figure 3."}, {"title": "2.2 Benchmark Construction Principles", "content": "To bridge the gap in current benchmarks that separately assess either multi-modal or multi-document reasoning, our benchmark, M3SCIQA, aims to encompass both elements in a single QA pair. Therefore, our benchmark construction pipeline adheres to the following guidelines: (1) it includes diverse modalities, such as texts, figures (including line plots, bar plots, scatter plots, etc.), and tables (stored as images to preserve format integrity rather than as plain texts); (2) it necessitates connecting information across multiple documents; (3) it spans a variety of reasoning types, including four types of visual context reasoning and five types of reference-based reasoning; (4) it poses significant challenges in both multi-modal comprehension and multi-document information retrieval; and (5) it generates realistic QA pairs that reflect the workflows common in scientific literature analysis."}, {"title": "2.3 Benchmark Construction", "content": "We recruit three computer science graduate students with expertise in the field of NLP, each of whom have authored at least one peer-reviewed publication in top-tier NLP conferences. Their responsibilities include: (1) curating anchor papers from a pool of candidates and composing visual context questions; (2) reviewing and verifying the reasoning types of reference-based questions; (3) resolving discrepancies between answers generated from the two rounds of reference-based answer generation; and (4) checking consistency, clarity, and redundancy in the combined questions. Further details on annotations are provided in Appendix B.\nTo mitigate the risk of data contamination, where models might rely on pre-trained knowledge to answer the visual context questions rather than analyzing the provided scientific images, we curate anchor papers from a recent NLP conference, EMNLP 2023. Among the 1,047 papers accepted by EMNLP 2023, we select 441 papers that were released on arXiv after October 1st, 2023 as candidate anchor papers.\nTwo of the expert annotators curate 70 papers by manually examining 441 candidate anchor papers collected. Subsequently, they select 21 figures and 62 tables from the 70 papers to compose 300 visual context questions and answers that conform to four visual reasoning types. The ground truth answer to each visual context question is the single ref-"}, {"title": "3 Experiments", "content": "We evaluate 18 foundation models, including both open-source and proprietary LMMs and LLMs. For each model, we select the most recent, largest, and best-performing checkpoint as of April 15th, 2024. The evaluation of the M3SCIQA benchmark is structured into two distinct stages: visual context evaluation and reference-based evaluation."}, {"title": "3.1 Visual Context Evaluation", "content": "The visual context evaluation with LMMs is defined as follows: Given a visual context question $Q_{vis}$, its correspondent scientific image $I$, and a list of reference papers $D = {d_1,d_2,..., d_n}$, the objective is to determine a ranking of these papers based on their relevance to the question and the image. This ranking is represented by $R = {r_1,r_2,......,r_n}$, where $r_i$ denotes the ranking of the paper $d_i$ for each index $i \\in {1,2,...,n}$. We input $Q_{vis}$, $I$ and $D$ into each LMM, denoted by $f_{LMM}$, and instruct it to generate a ranking $R$ of $D$ based on their relevance to $Q_{vis}$ and $I$:\n$R = f_{LMM}(Q_{vis}, I, d_1, d_2,\u00b7\u00b7\u00b7,d_n)$\nFor comparative analysis, simple baselines presented in Table 2 are also assessed for the ranking task. Other than BM25, these baselines employ cosine similarity between query and document embeddings to rank documents. Each query combines the visual context question $Q_{vis}$ and its image caption $C$ generated by GPT-40 with one of the documents, represented by its title and abstract. Given a visual context question $Q_{vis}$, its correspondent scientific image $I$, a list of reference papers $D = {d_1,d_2,\u00b7\u00b7\u00b7, d_n}$, an embedding model $Embed$, and a cosine similarity function $sim$, the ranking process is defined as below:\n$C = GPT-40(I)$\n$q = Embed(concat(Q_{vis}, C'))$\n$\\forall d_i \\in D, h_i = Embed(d_i)$\n$R = sort(sim(q, h_1),......, sim(q, h_n))$\nAt the visual context evaluation stage, we assess LMMs' ability to accurately retrieve and rank the correct reference paper from a complete list of reference papers. Performance is measured using an established information retrieval metric, Mean Reciprocal Rank (MRR), which effectively gauges a model's ability to identify and prioritize the most relevant reference paper. We also calculate Recall@k and nDCG@k to further analyze LMMs' retrieval effectiveness, with results detailed in Table 8 and 9 in Appendix D."}, {"title": "3.2 Reference-Based Evaluation", "content": "The reference-based evaluation is defined as follows: Given a combined question $Q_{comb}$ and a ranking $R$ of the reference papers obtained in the visual context evaluation stage, the objective is to answer the question based on the top k ranked paper in $R$, denoted by $Top_k(R) = {R[1], R[2],\u00b7\u00b7\u00b7, R[k]}$. Since combined questions contain elements from both visual context and reference-based questions, we instruct LLMs to solely concentrate on the reference-based aspect of $Q_{comb}$. The prompts used for this instruction are detailed in Table 15 in Appendix E.3. Accordingly, we input $Q_{comb}$ and $Top_k(R)$ into LLMs, denoted by $f_{LLM}$, and instruct LLMs to answer $Q_{comb}$ based on the textual content in top k ranked papers:\n$Ans = f_{LLM}(Q_{comb}, R[1], R[2], \u2026\u2026\u2026, R[k])$\nAt the reference-based evaluation stage, we assesses how LLMs perform on reference-based questions using the top three ranked papers identified from the visual context evaluation stage as context. Specifically, these papers are ranked by GPT-40, which is highlighted as the most effective retrieval model in Table 2. GPT-4o achieves an MRR of 0.488, suggesting that the correct reference paper typically appears in the 2.1-th position, placing it within the top three ranked papers on average. Given that both reference-based question and answer generation utilize plain text extracted from TeX files, we employ the same parsed TeX files as input for LLMs to solve the text-only, reference-based questions."}, {"title": "Finding 1: Challenges in Visual Reasoning and Paper Ranking with M3SCIQA", "content": "Table 4 provides a breakdown of GPT-4o's performance in answering the visual context questions, categorized by both reasoning and ranking outcomes. Despite being the overall best-performing retriever, GPT-4o still struggles with the dual challenges: it fails to correctly interpret 42.4% of the scientific images; even when it does produce correct visual reasoning, it falls short in ranking the associated paper within the top three choices. Notably, one interesting error pattern is the scenario \u201creasoning ranking@top3,\u201d which accounts for 19.7% of the cases for GPT-4o. While this type of error occurs in both open-source and proprietary LMMs, it is more prevalent in the former. Example error analyses are presented in Figure 4, offering a more granular view of these patterns and specific instances where the model underperforms."}, {"title": "Finding 2: Inherent Limitations of Open-Source LMMs in Long-Range Ranking Task", "content": "The performance of open-source LMMs in long-range ranking tasks is significantly hindered by their fundamental limitations. We identify three primary challenges: (1) Limited Context Window, which necessitates division of large paper clusters into smaller segments, complicating the ranking process and potentially omitting relevant reference papers; (2) Hallucinations, characterized by the erroneous generation and prioritization of irrelevant arXiv webpage URLs, professional NLP terms, repetitive paper IDs, and random numerical values; (3) Formatting Issues, where models disregard specified format and list papers in plain text, complicating the integration of results across rankings from segmented paper clusters. These challenges significantly impede the models' ability to provide a comprehensive evaluation of their visual reasoning capabilities, suggesting the need for improvements in their basic functionality to handle more complex reasoning and ranking tasks. A detailed evaluation of open-source LMMs is presented in Appendix F."}, {"title": "Finding 3: Precision-Recall Balance", "content": "We evaluate LLMs in retrieval settings using the top k ranked papers from the visual context evaluation performed by GPT-4o for the values k \u2208 {1, 2, 3, 4, 5}. As shown in Figure 5, performance generally increases from k = 1 to k = 3, aligning with an MRR score of approximately 0.488, which places the correct reference paper in the 2.1-th position on average. Beyond this point, as more papers are considered, more noise is introduced."}, {"title": "Finding 4: Challenges in Instruction Compliance for LLMs in Retrieval Task", "content": "Our evaluation of four models in both a title-only setting, where only the title of the reference paper is provided, and a retrieval setting, with the top three ranked papers by GPT-40, highlights variations in instruction compliance. Models are instructed to answer 'I don't know' if a definitive answer cannot be derived from the given information. This directive tests the models' adherence to instructions, since the task is infeasible with the titles alone and compliant models should exhibit minimal performance. Transition to the retrieval setting should reveal a significant increase for the models, as observed with GPT-4 in Table 5. Additionally, employing a LLM-based evaluator to assess generative response overlooks models' confidence levels. Less compliant models, relying on pre-trained knowledge, often produce tangentially relevant re- sponses rather than the instructed \u201cI don't know,\""}, {"title": "4 Related Work", "content": "Multi-Modal QA. Multi-modal QA datasets have posed visual reasoning challenges for LMMs. Initially, the focus of benchmarks (Lin et al., 2015; Mobasher et al.; Yagcioglu et al., 2018; Talmor et al., 2021; Lu et al., 2022; Chang et al., 2022; Li et al., 2023; Liu et al., 2023d; Yu et al., 2023) was on conducting QA tasks over simple images, primarily addressing questions such as understanding objects in an image and performing single-hop reasoning. Recently, more complex and nuanced benchmarks (Chen et al., 2022; Lu et al., 2024b) have emerged beyond the scope of understanding simple images to require complex mathematical reasoning over diagrams and plots. Beyond the scope of mathematical reasoning, MMMU (Yue et al., 2023) requires more complex visual reasoning in a diverse range of subjects including science, humanities, and engineering.\nDocument QA. Document QA is crucial in the field of NLP, focusing on extracting, synthesizing, and analyzing information from structured and unstructured documents. Early document QA benchmarks (Rajpurkar et al., 2016; Bajaj et al., 2018; Yang et al., 2018) involved short document QA, where questions were posed based on content from web pages such as those in Bing's search results or Wikipedia articles. Scientific paper QA benchmarks (Dasigi et al., 2021; Lee et al., 2023) require LLMs to conduct multi-hop reasoning and long-context information processing. However, a notable gap exists in the integration of Multi-modal QA with Document QA, particularly in the context of scientific research, where it encompasses a blend of textual and visual data alongside complex textual information. M3SCIQA, bridging this gap, is a benchmark for evaluating foundation models' abilities in both multi-modal and multi-document reasoning."}, {"title": "5 Conclusion", "content": "Existing scientific QA benchmarks often overlook the complexity of real research workflows, which require interpreting non-textual data and aggregating information from multiple documents. To bridge this gap, we present M3SCIQA, a novel multi-modal multi-document scientific QA benchmark designed to evaluate foundation models. Our evaluation and analysis underscore the challenges LMMs face in scientific diagram understanding and long-range information ranking tasks, highlighting the limitations of current models in handling complex scientific documents. We hope this work paves the way for advancements in multi-modal and long-document understanding."}, {"title": "Limitations", "content": "The evaluations presented in this study are met with certain limitations due to inherent disparities in the context window of current open-source and proprietary LLMs and LMMs. There is a significant difference in context window length between models such as GPT-4 Turbo and Claude-3, which can rank all papers in a paper cluster, and models such as InternVL-Chat-V1.1 and QwenVL, which are restricted to handling only two to eight papers in a single prompt. This discrepancy may lead to an \"unfair\" comparison of their capabilities. Future work could focus on standardizing or extending the context windows in LMMs to mitigate this issue.\nFurthermore, as discussed in Section 3.3, prompting an LMM with a set of possible reference papers may be suboptimal due to the challenges models face in ranking a large number of papers. An alternative approach could involve assessing the relevance of each paper individually by encoding the paper into a textual embedding, then comparing it with the textual embedding with of the visual context question combined with the image representation of the figure. This method could potentially alleviate the challenges of requiring an LMM to sift through a large set of possible reference papers and would be an interesting area for future research.\nAdditionally, our approach to ranking papers for certain models, in particular BM25 and Contriever, involves using GPT-40's textual descriptions of images rather than its direct image embedding, which might not accurately capture the nuances of scientific images. Current image embedding models such as LLaVA (Liu et al., 2023a) and CLIP (Radford et al., 2021), while proficient with natural images, are not trained on scientific images. Developing a specialized LMM trained specifically on scientific images (Li et al., 2024; Wu et al., 2024) could potentially enhance its performance in interpreting scientific plots, figures, and tables, thereby improving its potential usage in scientific applications."}]}