{"title": "Markov Balance Satisfaction Improves Performance in Strictly Batch Offline Imitation Learning", "authors": ["Rishabh Agrawal", "Nathan Dahlin", "Rahul Jain", "Ashutosh Nayyar"], "abstract": "Imitation learning (IL) is notably effective for robotic tasks where directly programming behaviors or defining optimal control costs is challenging. In this work, we address a scenario where the imitator relies solely on observed behavior and cannot make environmental interactions during learning. It does not have additional supplementary datasets beyond the expert's dataset nor any information about the transition dynamics. Unlike state-of-the-art (SOTA) IL methods, this approach tackles the limitations of conventional IL by operating in a more constrained and realistic setting. Our method uses the Markov balance equation and introduces a novel conditional density estimation-based imitation learning framework. It employs conditional normalizing flows for transition dynamics estimation and aims at satisfying a balance equation for the environment. Through a series of numerical experiments on Classic Control and MuJoCo environments, we demonstrate consistently superior empirical performance compared to many SOTA IL algorithms.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) has provided us with some very notable achievements over the past decade from mastering complex games (Mnih et al. 2015; Silver et al. 2016; Vinyals et al. 2019) to advancing protein structure prediction systems (Jumper et al. 2021), and now proficiency at coding and high school-level mathematics. And yet, all of these achievements are built on a fundamental hypothesis that the \"reward is enough\u201d (Silver et al. 2021). But in the real world, when only data is available, there is no natural reward model available. Countless hours are spent on reward engineering. In fact, lately, there has been tremendous progress in building reward models for language and multi-modal generative AI systems. And yet, they have led to their own set of challenges, namely reward hacking and over-optimization, lack of diversity and robustness in learned policies.\nClassically, in imitation learning, this was sought to be addressed via inverse RL methods (Abbeel and Ng 2004; Ng and Russell 2000), e.g., the MaxEntropy-IRL algorithm (Ziebart et al. 2008) that first estimates a reward function from demonstration data and then uses it with RL algorithms to find near-optimal policies. Unfortunately, this has two limitations: First, the reward function estimation problem is ill-posed (Baheri 2023) and any additional criterion introduce estimation errors. Second, the demonstrator (e.g., a human subject) may not be optimizing with respect to any reward function at all! Additionally, IRL algorithms are computationally intensive, particularly in high-dimensional state spaces, limiting their scalability (Barnes et al. 2024). Thus, there is a need to develop Imitation Learning (IL) algorithms that don't depend on reward estimation as a first step.\nBehavioral Cloning (BC) is one such classical IL method (Pomerleau 1988). It is inspired by supervised learning and learns a map from states to actions from trajectory data. Unfortunately, it does not account for the fact that such trajectory data often satisfies a Markov balance equation (MBE). In fact, this MBE is the only mathematical structure that ties the trajectory data together. And if we don't use it, we can expect we will not do as well as possible. Thus, it is not unexpected that the BC method is known to be vulnerable to error propagation and covariate shift issues, thus constraining its generalization capabilities (Ross and Bagnell 2010). This is typically sought to be addressed by allowing for additional online interactions with the environment or the demonstrator, or using supplementary data (Ross, Gordon, and Bagnell 2011; Piot, Geist, and Pietquin 2016). Unfortunately, in many practical situations, this is simply not a possibility such as in autonomous vehicles and healthcare, where engaging directly with the environment is often infeasible due to safety concerns or high costs. Therefore, we must learn from the offline data we already have. A premise of ours is that just like in training of LLMs, a combination of supervised fine-tuning and RL with human feedback improves LLM performance, in the same way, an imitation learning method that combines behavior cloning policy with accounting for Markovian dynamics will perform better as well.\nAt the core of imitation learning is distribution matching, which treats state-action pairs from expert demonstrations as samples drawn from a target distribution. The objective is to develop a policy that minimizes the divergence between this target distribution and the distribution induced by the imitator's policy. Adversarial Imitation Learning (AIL) methods use this principle (Ho and Ermon 2016; Fu, Luo, and Levine 2018; Ke et al. 2021), but they face a significant limitation: estimating density ratios requires on-policy samples from the environment. This need for continuous interaction with the environment makes AIL methods impractical when only offline data is available."}, {"title": "", "content": "To address the availability of limited expert data, (Yue et al. 2023a; Xu et al. 2022) propose a supplementary data framework in imitation learning. This method uses additional data, cheaply gathered by executing suboptimal policies, to augment the expert dataset. However, such supplementary data may not be available, and in fact may even cause distribution shifts due to out-of-expert-distribution trajectories, which may degrade model performance.\nIn this paper, we focus on the strictly batch imitation learning problem (i.e., no further interaction or supplementary data) and propose a novel approach to imitation learning that does not do reward model estimation, or distribution matching between occupation measures via on-policy samples. Instead, it learns a policy that minimizes a loss function related to satisfaction of the Markov balance equation, which is the one and only fundamental relationship we know about the trajectory data. This Markov balance equation involves two conditional state-action transition density functions which need to be learnt from data. Density estimation, and especially conditional density estimation is rather tricky, especially for continuous state and action space settings. Fortunately, recently developed normalizing flows methods (Dinh, Sohl-Dickstein, and Bengio 2017) have proven remarkably well-suited for this problem. Furthermore, we regularize the MBE loss around a behavioral cloning policy, thus combining supervised and RL methods for imitation learning in a natural manner. This novel combination enables excellent numerical performance compared to state-of-the-art IL/IRL/AIL methods in the considered settings across a range of Classic Control and MuJoCo tasks.\nRelated Work. Offline IRL. To circumvent costly online environmental interactions in classic IRL, offline IRL aims to infer a reward function and recover the expert policy solely from a static dataset without accessing the environment. (Klein, Geist, and Pietquin 2011) introduced LSTD-\u03bc, extending classic apprenticeship learning (Abbeel and Ng 2004) to batch and off-policy cases for computing feature expectations, while (Klein et al. 2012) developed a score function-based classification algorithm to output the reward function. (Lee, Srinivasan, and Doshi-Velez 2019) propose DSFN, using a transition-regularized imitation network for an initial policy close to expert behavior. However, these methods assume complete knowledge of reward features, which is often impractical for complex problems due to the problem-dependent nature of feature selection (Arora and Doshi 2021). RCAL (Piot, Geist, and Pietquin 2014) employs a boosting method to minimize a large margin objective with a regularization term, avoiding feature selection steps. (Chan and van der Schaar 2021) introduced AVRIL, jointly learning an approximate posterior distribution over reward and policy. (Garg et al. 2021) proposed IQ-Learn, using a learned soft Q-function to represent both reward and policy implicitly. Despite these advances, they struggle with covariate shift and reward extrapolation errors, impacting performance in novel environments (Yue et al. 2023b). CLARE (Yue et al. 2023b) incorporates conservatism in reward estimation but performs poorly with low-quality transition samples (Zeng et al. 2023) and requires additional diverse datasets unavailable in our setting."}, {"title": "", "content": "Offline IL. The EDM approach (Jarrett, Bica, and van der Schaar 2020) captures the expert's state occupancy measure by training an explicit energy-based model but faces significant limitations and is unsuitable for continuous action domains (Swamy et al. 2021). DICE (DIstribution Correction Estimation) methods perform stationary distribution matching of state-action pairs between the learner's policy and expert demonstrations. (Kostrikov, Nachum, and Tompson 2020a) introduces ValueDICE, which minimizes the Donsker-Varadhan representation of KL divergence between these stationary distributions but suffers from biased gradient estimates due to logarithmic terms applied to expectations. SoftDICE (Sun et al. 2021) addresses these limitations by using the Earth-Mover Distance (EMD) for distribution matching, eliminating problematic logarithmic and exponential terms. DemoDICE (Kim et al. 2022) and SMODICE (Ma et al. 2022) assume additional demonstration data of unknown degrees of optimality beyond expert's data to deal with the narrow support of the expert data distribution, but such information is unavailable in our setup. DICE methods involve two gradient terms for value function learning: the forward gradient (on the current state) and the backward gradient (on the next state). However, conflicting directions between these gradients can lead to performance degradation (Mao et al. 2024). To address this, (Mao et al. 2024) introduces ODICE, an orthogonal-gradient update method that projects the backward gradient onto the normal plane of the forward gradient, ensuring effective state-action-level constraints and better convergence.\nOur work is clearly differentiated from prior work in not doing stationary distribution matching, but instead in first using normalizing flows in conditional state-action density estimation, and then using these in ensuring Markov balance equation satisfaction."}, {"title": "2 Preliminaries", "content": "The Imitation Learning Problem. An infinite horizon discounted Markov decision process (MDP) M is defined by the tuple (S, A, T, r, \u03b3) with states s \u2208 S, actions a \u2208 A and successor states s' \u2208 S drawn from the transition function T(s'|s, a). The reward function r : S \u00d7 A \u2192 R maps state-action pairs to scalar rewards, and y is the discount factor. Policy is a probability distribution over actions conditioned on state and is given by \u03c0(a|s) = P(a_t = a|s_t = s), where a_t \u2208 A, s_t \u2208 S, \u2200t = 0,1,2,.... The induced occupancy measure of a policy is given as \u03c1^\u03c0(s, a) := \u0395_\u03c0[\u2211_{t=0}^{\u221e} \u03b3^t 1_{s_t=s, a_t=a}], where the expectation is taken over a_t ~ \u03c0(.|s_t), s_{t+1} ~ T(.|s_t, a_t) for all t, and the initial state s_0. The corresponding state-only occupancy measure is given as \u03c1^\u03c0(s) = \u03a3_a \u03c1^\u03c0(s, a). In the offline imitation learning (IL) framework, the agent is provided with trajectories generated by a demonstration policy \u03c4_D, collected as D = {(s_0, a_0), (s_1, a_1), (s_2, a_2), ...}; and is not allowed any further interaction with the environment. The data D does not include any reward r_t at each time step. Indeed, rather than long-term reward maximization, the IL objective is to learn a policy \u03c0^* that is close to \u03c4_D in the"}, {"title": "", "content": "following sense (Yue and Le 2018):\n\u03c0^* \u2208 arg min_{\u03c0\u2208\u03a0} E_{s\u223c\u03c1_D}[L(\u03c0(\u00b7|s), \u03c0_D(\u00b7|s))], (1)\nwhere II is the set of all randomized (Markovian) stationary policies, and L is a chosen loss function. In practice, (1) can only be solved approximately since \u03c4_D is unknown and only transitions are observed in the dataset D.\nNormalizing Flows. The imitation learning approach we introduce depends on transition density estimation. Despite advancements in statistical theory, conditional density estimation is a difficult problem due to a lack of clarity on what parametric families of density functions are good candidates. We make use of normalizing flows (NFs) to address our density estimation problem. NFs belong to a class of generative models adept at modeling complex probability distributions. NFs employ a sequence of invertible and differentiable transformations g_{\u03d5,k} = g_k \u2218 g_{k\u22121} \u2218 ... \u2218 g_1, applied to a base distribution p_z(z), typically a simple distribution, e.g., standard Gaussian. These transformations progressively refine the distribution to accurately model the target data distribution p_x(x) via the mapping z = g_{\u03d5,k}^\u22121(x). Consequently, the log-likelihood of the target distribution is formulated as:\nlog p_x(x) = log p_z(z) + \u2211_{i=1}^{k} log det [\u2202g_i/\u2202g_{i\u22121}] , (2)\nwhich facilitates the training of parameters \u03c6 via maximum likelihood. Once the flow is trained, density estimation can be performed by evaluating the right-hand side of (2)."}, {"title": "3 Markov Balance-based Imitation Learning", "content": "We next describe MBIL, our imitation learning algorithm. A key premise of our algorithm is that the demonstration trajectory data satisfies a balance equation involving the demonstration policy, the Markov decision process (MDP) transition density, and the induced Markov chain (MC). We can use this balance equation to guide the agent's learning. Using the balance equation requires estimating certain transition (conditional probability) density functions, which we obtain via the conditional normalizing flow method. We describe our approach in Algorithm 1 and present numerical evidence of its efficacy on several benchmark Classic Control and MuJoCo tasks.\nThe Markov Balance Equation. Consider a demonstration policy \u03c4_D that is used to take actions starting from an initial state s_0. Let T(s'|s, a) denote the transition density function of the MDP. Note that \u03c4_D is a randomized stationary Markovian policy and \u03c0_D(a|s) is the probability distribution of actions at state s. Using policy \u03c4_D on the underlying MDP induces a Markov chain on the state space S whose transition density is denoted by P(s'|s). This transition density satisfies the following equation which we refer to as the Markov balance equation: P(s'|s) = \u03a3_a \u03c0_D(a|s)T(s'|s, a). Unfortunately, this approach involves a summation, which becomes problematic in continuous action domains, where the sum translates to an integral over actions. Therefore, we use the following alternative balance equation which involves the transition density of the induced Markov chain on the state-action space,\nP_\u03c0_D (s', a'|s, a) = \u03c0_D(a'|s')T(s'|s, a). (3)\nThe above balance equation is the basis of our IL approach. If we can estimate P_D and T in (3) (estimates denoted by P and \u00ce respectively), we can use the balance equation to guide agent's learning. In our approach, we consider a combination of a policy-based loss function that measures the discrepancy between the demonstrator's and"}, {"title": "", "content": "learner's policies and a dynamics-based loss function that quantifies the discrepancy between the two sides of the balance equation under the learner's policy.\nWe consider a class of policies parametrized by \u03b8 and formulate the following optimization problem:\nmin_{\u03b8\u2208\u0398} J_{MBIL} (\u03c0_\u03b8) := \u03b1 E_{s,a,s',a'\u223cD}[BALANCE(s', a', s, a)] + \u03b2 E_{s,a\u223cD} [L (\u03c0_D(\u00b7 | s), \u03c0_\u03b8(\u00b7 | s))] (4)\nExamples of policy loss function L(\u00b7) include KL divergence between the policies (which amounts to maximizing the log-likelihood of the expert's actions in the states observed in the demonstration data) or the mean-squared error between the agent's and the expert's actions, among other possibilities. The BALANCE function introduces a dynamics-based loss term designed to enforce the Markov Balance constraint and is defined as follows:\nBALANCE(s', a', s, a) = [log P(s', a'|s, a) \u2212 log \u03c0_\u03b8(a'|s') \u2212 log T\u0302 (s'|s, a)]^2 .\n\u0398 is a given parameter set. The parameters could be weights of a neural network, for example. \u03b1 and \u03b2 are parameters that represent the relative priorities of the dynamics-based term and the policy-based term, respectively.\nRemarks. In Offline IL-compatible DICE methods like ValueDICE, the derivation of the off-policy objective assumes that the Markov chain induced by the learned policy is always ergodic and that the state-action distribution is stationary. However, these assumptions often fail during training due to early terminations from environment resets triggered by adverse states or fatal actions leading to termination bias (Sun et al. 2021). Algorithms such as IQ-Learn and SoftDICE address this by adding an indicator for absorbing states and modifying the Bellman operator, but this can introduce reward bias, as the value of absorbing states should be learned rather than assumed to be zero (Kostrikov"}, {"title": "", "content": "et al. 2018). LS-IQ (Al-Hafez et al. 2023) improves upon IQ-Learn by correcting reward bias with a modified inverse Bellman operator but performs poorly in pure offline settings, especially on MuJoCo tasks. In contrast, our method avoids the off-policy derivation framework and employs the Markov Balance Equation instead. The rationale is straightforward: every transition captured in the dataset, including those leading to the terminal state, must adhere to this equation. As a result, we eliminate the biases that stationary distribution matching algorithms often face. This also eliminates the need for a discount factor \u03b3 (typically required in DICE methods and may influence their performance), and simplifies training by avoiding a complex alternating max-min optimization (typical in DICE methods) with a single optimization procedure.\nTransition Density Estimation We now introduce methodology for estimation of the the two conditional densities P\u0302_D and T\u0302 .\nEstimating transition densities in continuous setting is challenging because each visited state would only appear once, and most states may never be visited in the dataset. This necessitates the use of more sophisticated conditional density estimation methods. These include non-parametric methods like Gaussian process conditional density estimation (Dutordoir et al. 2018) and Conditional Kernel Density Estimation (CKDE) (Li and Racine 2006); semi-parametric methods like least squares conditional density estimation (Sugiyama et al. 2010); and parametric approaches such as mixture density networks (MDN) (Bishop 1994) and normalizing flows (Dinh, Sohl-Dickstein, and Bengio 2017).\nWe use Normalizing Flows (NFs) because they outperform density estimation techniques such as CKDE and MDN. Unlike CKDE and MDN, which can struggle with high-dimensional data and require careful tuning of bandwidths or mixture components, NFs leverage a series of invertible transformations to model complex distributions with high flexibility and precision. This allows for exact likelihood computation, making NFs ideal for tasks requiring precise probability evaluation, as in our setup. Additionally,"}, {"title": "4 Experimental Results", "content": "Setup. We evaluate the empirical performance of our MBIL algorithm using the MuJoCo locomotion suite (Todorov, Erez, and Tassa 2012) and the classic control suite from OpenAI Gym (Brockman et al. 2016). MuJoCo tasks, with their varying difficulty levels, are a popular benchmark for IL in continuous action domains, while classic control environments like LunarLander are used to assess IL algorithms in discrete action domains. For constructing the demonstration dataset D, we use data from (Kostrikov, Nachum, and"}, {"title": "", "content": "Tompson 2020b), where the Generative Adversarial Imitation Learning algorithm (Ho and Ermon 2016) was applied for MuJoCo tasks. For classic control tasks, we utilize pretrained and hyperparameter-optimized agents from the RL Baselines Zoo (Raffin 2020), employing a PPO agent for LunarLander-v2, a DQN agent for CartPole-v1, and an A2C agent for Acrobot-v1.\nImplementation. A neural network (NN) with two hidden layers, each using the ReLU activation function, is used for representing the policy. For tasks with discrete actions (e.g., Classic Control tasks), the output layer has a dimension equal to the number of action dimensions and uses a softmax function to generate a probability distribution over actions given a state. In contrast, for MuJoCo tasks with continuous actions, the output consists of two separate layers: one for the mean and another for the standard deviations, each with a size equal to the action dimension. The policy is then modeled as a Gaussian distribution, with these parameters generated by the neural network. Training is performed with the Adam optimizer (Kingma and Ba 2015). Detailed implementation, including hyperparameters for MBIL and benchmark algorithms, are provided in the Appendix.\nThe transition dynamics models P_\u03b7 and T_\u03c8 are implemented using RealNVPs (Dinh, Sohl-Dickstein, and Bengio 2017). We employ the publicly available framework version 0.2 (Ardizzone et al. 2018-2022), utilizing their GLOWCouplingBlocks implementation. Detailed parameters for these models are provided in Appendix. A central aspect of our approach is performing imitation learning in the low data regime. To facilitate this, we introduce Gaussian noise as a regularizer for training the expert MC and transition MDP, which enhances training stability with limited data.\nResults. When given sufficient demonstration data, all benchmarks can achieve performance comparable to optimized demonstration agents. Therefore, we test the algorithms' ability to handle limited data. Several recent offline IRL/IL methods, such as CLARE (Yue et al. 2023b), DemoDICE (Kim et al. 2022), and MILO (Chang et al. 2021),"}, {"title": "", "content": "depend on additional diverse data, while others, like OPOLO (Zhu et al. 2020), allow for interactions with the environment within their off-policy frameworks. Given that our setting strictly considers only offline expert data without access to additional interactions or supplementary datasets, we exclude methods that rely on such resources from our comparisons to ensure a fair evaluation.\nClassic Control Tasks. Inspired by (Jarrett, Bica, and van der Schaar 2020), we trained algorithms until convergence on datasets of 1, 3, 7, 10, or 15 trajectories sampled from a pool of 1000 expert trajectories and recorded the average scores over 300 episodes for each algorithm, repeating this process 10 times with varied initializations and trajectories. We compare our MBIL algorithm (Algorithm 1) against various offline IRL/IL/AIL baselines, including BC, ValueDICE (VDICE), RCAL, EDM, AVRIL, DSFN, and the state-of-the-art model-free offline IRL algorithm IQ-Learn. We use log-likelihood maximization based BC as L(\u00b7) in (5) in our Classic Control experiments.\nFigure 3 illustrates the average rewards obtained by each algorithm as the demonstration dataset size increases in the Acrobot, CartPole, and LunarLander environments. The results highlight MBIL's ability to learn effective policies, consistently outperforming the baseline algorithms, particularly when data is limited. MBIL achieves near-expert-level performance in these environments with at most three trajectories and, remarkably, can reach near-expert performance in the CartPole environment with just a single trajectory. Notably, MBIL generally outperforms all baseline algorithms across these environments, with IQ-Learn showing performance closest to MBIL. Additionally, off-policy adaptations of online algorithms, such as VDICE and DSFN, do not maintain the same level of consistent performance as their inherently online counterparts. This highlights the need for more than just adopting online algorithms in offline scenarios. Moreover, the difficulty in estimating the expectation of an exponential distribution may contribute to VDICE's relative underperformance compared to MBIL and other methods."}, {"title": "", "content": "MuJoCo Tasks. Following the methodology outlined in (Kostrikov, Nachum, and Tompson 2020a; Sun et al. 2021), we use a single demonstration trajectory, validate performance every 500 training iterations across 10 episodes, and report means and standard deviations from 5 random seeds. We compare MBIL (Algorithm 1) against strong baselines for locomotion tasks, including BC, ValueDICE, SoftDICE, and ODICE. IQ-Learn is excluded due to its poor performance in high-dimensional MuJoCo tasks with continuous action spaces in the strictly offline setting, as it suffers from exploding Q-functions (Al-Hafez et al. 2023). We also exclude EDM and AVRIL because they are incompatible with continuous action spaces. Notably, while baselines like SoftDICE and ValueDICE, which use log-likelihood maximization for BC and show its poor performance, MSE-based BC, as demonstrated by (Li et al. 2022), performs well in MuJoCo tasks and is therefore used for BC in our comparison. Furthermore, we employ this mean-squared error (MSE) BC loss as L(\u00b7) in (5) in our MuJoCo experiments.\nFigure 4 compares the performance of MBIL with strong baselines on MuJoCo tasks in the offline IL literature. MBIL consistently achieves near-expert performance on all tasks with just a single trajectory in the dataset. Among the baselines, BC and SoftDICE show strong performance. However, BC's performance deteriorates in the Ant and Walker2d environments as the number of gradient updates increases, despite using orthogonal regularization (Brock, Donahue, and Simonyan 2018), a trend also noted in (Li et al. 2022). In contrast, MBIL maintains robust performance throughout the learning process by integrating dynamics loss with the policy loss. SoftDICE encounters difficulties in the Walker2d environment, where absorbing states are more common than Ant and HalfCheetah. Its assumption of a value of 0 for these states can introduce reward bias, hindering performance since the values of these states also need to be learned (Al-Hafez et al. 2023). The performance decline of ValueDICE is likely due to biased gradient updates in its learning process (Sun et al. 2021). ODICE attempts to address the issue of conflicting gradients in DICE methods by incorporating orthogonal gradient updates, but this approach may not have been effective for these tasks. Overall,"}, {"title": "5 Conclusion", "content": "In this paper, we address the strictly batch imitation learning problem in a continuous state and action space setting, i.e., we do not assume access to any further interactions or supplementary data. We present a Markov balance-based imitation learning algorithm that combines it with supervised learning-based behavior cloning while using conditional normalizing flows for density estimation. Our method does not rely on reward model estimation and avoids stationary distribution matching which suffer from termination and reward bias. From numerical experiments, we see that our proposed algorithm does as well or much better than any state-of-the-art algorithm across a variety of Classic Control and MuJoCo environments. In fact, as can be seen in further results presented in the Appendix, it handles the distribution shift issue more effectively than other strictly batch imitation learning algorithms. Future research could benefit from deriving both asymptotic and non-asymptotic sample complexity bounds, which are scarce in current literature, and from exploring extensions to handle suboptimal data cases."}]}