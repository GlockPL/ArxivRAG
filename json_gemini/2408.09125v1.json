{"title": "Markov Balance Satisfaction Improves Performance in Strictly Batch Offline Imitation Learning", "authors": ["Rishabh Agrawal", "Nathan Dahlin", "Rahul Jain", "Ashutosh Nayyar"], "abstract": "Imitation learning (IL) is notably effective for robotic tasks where directly programming behaviors or defining optimal control costs is challenging. In this work, we address a scenario where the imitator relies solely on observed behavior and cannot make environmental interactions during learning. It does not have additional supplementary datasets beyond the expert's dataset nor any information about the transition dynamics. Unlike state-of-the-art (SOTA) IL methods, this approach tackles the limitations of conventional IL by operating in a more constrained and realistic setting. Our method uses the Markov balance equation and introduces a novel conditional density estimation-based imitation learning framework. It employs conditional normalizing flows for transition dynamics estimation and aims at satisfying a balance equation for the environment. Through a series of numerical experiments on Classic Control and MuJoCo environments, we demonstrate consistently superior empirical performance compared to many SOTA IL algorithms.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) has provided us with some very notable achievements over the past decade from mastering complex games (Mnih et al. 2015; Silver et al. 2016; Vinyals et al. 2019) to advancing protein structure prediction systems (Jumper et al. 2021), and now proficiency at coding and high school-level mathematics. And yet, all of these achievements are built on a fundamental hypothesis that the \"reward is enough\u201d (Silver et al. 2021). But in the real world, when only data is available, there is no natural reward model available. Countless hours are spent on reward engineering. In fact, lately, there has been tremendous progress in building reward models for language and multi-modal generative AI systems. And yet, they have led to their own set of challenges, namely reward hacking and over-optimization, lack of diversity and robustness in learned policies.\nClassically, in imitation learning, this was sought to be addressed via inverse RL methods (Abbeel and Ng 2004; Ng and Russell 2000), e.g., the MaxEntropy-IRL algorithm (Ziebart et al. 2008) that first estimates a reward function from demonstration data and then uses it with RL algorithms to find near-optimal policies. Unfortunately, this has two limitations: First, the reward function estimation problem is ill-posed (Baheri 2023) and any additional criterion introduce estimation errors. Second, the demonstrator (e.g., a human subject) may not be optimizing with respect to any reward function at all! Additionally, IRL algorithms are computationally intensive, particularly in high-dimensional state spaces, limiting their scalability (Barnes et al. 2024). Thus, there is a need to develop Imitation Learning (IL) algorithms that don't depend on reward estimation as a first step.\nBehavioral Cloning (BC) is one such classical IL method (Pomerleau 1988). It is inspired by supervised learning and learns a map from states to actions from trajectory data. Unfortunately, it does not account for the fact that such trajectory data often satisfies a Markov balance equation (MBE). In fact, this MBE is the only mathematical structure that ties the trajectory data together. And if we don't use it, we can expect we will not do as well as possible. Thus, it is not unexpected that the BC method is known to be vulnerable to error propagation and covariate shift issues, thus constraining its generalization capabilities (Ross and Bagnell 2010). This is typically sought to be addressed by allowing for additional online interactions with the environment or the demonstrator, or using supplementary data (Ross, Gordon, and Bagnell 2011; Piot, Geist, and Pietquin 2016). Unfortunately, in many practical situations, this is simply not a possibility such as in autonomous vehicles and healthcare, where engaging directly with the environment is often infeasible due to safety concerns or high costs. Therefore, we must learn from the offline data we already have. A premise of ours is that just like in training of LLMs, a combination of supervised fine-tuning and RL with human feedback improves LLM performance, in the same way, an imitation learning method that combines behavior cloning policy with accounting for Markovian dynamics will perform better as well.\nAt the core of imitation learning is distribution matching, which treats state-action pairs from expert demonstrations as samples drawn from a target distribution. The objective is to develop a policy that minimizes the divergence between this target distribution and the distribution induced by the imitator's policy. Adversarial Imitation Learning (AIL) methods use this principle (Ho and Ermon 2016; Fu, Luo, and Levine 2018; Ke et al. 2021), but they face a significant limitation: estimating density ratios requires on-policy samples from the environment. This need for continuous interaction with the environment makes AIL methods impractical when only offline data is available."}, {"title": "2 Preliminaries", "content": "The Imitation Learning Problem. An infinite horizon discounted Markov decision process (MDP) M is defined by the tuple (S, A, T, r, \u03b3) with states s \u2208 S, actions a \u2208 A and successor states s' \u2208 S drawn from the transition function T(s'|s, a). The reward function r : S \u00d7 A \u2192 R maps state-action pairs to scalar rewards, and y is the discount factor. Policy \u03c0 is a probability distribution over actions conditioned on state and is given by \u03c0(a|s) = P(at = a|st = s), where at \u2208 A, st \u2208 S, \u2200t = 0,1,2,....\nThe induced occupancy measure of a policy is given as \u03c1\u03c0(s, a) := E\u03c0[\u2211t=0\u221e\u03b3t1st=s,at=a], where the expectation is taken over at ~ \u03c0(St), St+1 ~ T(.|st, at) for all t, and the initial state 80. The corresponding state-only occupancy measure is given as \u03c1\u03c0(s) = \u03a3a\u03c1\u03c0(s, a). In the offline imitation learning (IL) framework, the agent is provided with trajectories generated by a demonstration policy TD, collected as D = {(so, ao), (s1, a1), (s2, A2), ...}; and is not allowed any further interaction with the environment. The data D does not include any reward rt at each time step. Indeed, rather than long-term reward maximization, the IL objective is to learn a policy \u03c0\u2217 that is close to \u03c0D in the"}, {"title": "3 Markov Balance-based Imitation Learning", "content": "We next describe MBIL, our imitation learning algorithm. A key premise of our algorithm is that the demonstration trajectory data satisfies a balance equation involving the demonstration policy, the Markov decision process (MDP) transition density, and the induced Markov chain (MC). We can use this balance equation to guide the agent's learning. Using the balance equation requires estimating certain transition (conditional probability) density functions, which we obtain via the conditional normalizing flow method. We describe our approach in Algorithm 1 and present numerical evidence of its efficacy on several benchmark Classic Control and MuJoCo tasks.\nThe Markov Balance Equation. Consider a demonstration policy \u03c0D that is used to take actions starting from an initial state 80. Let T(s'|s, a) denote the transition density function of the MDP. Note that \u03c0D is a randomized stationary Markovian policy and \u03c0D(a|s) is the probability distribution of actions at state s. Using policy \u03c0D on the underlying MDP induces a Markov chain on the state space S whose transition density is denoted by P(s'|s). This transition density satisfies the following equation which we refer to as the Markov balance equation: P(s'|s) = \u03a3a\u03c0D(a|s)T(s'|s,a). Unfortunately, this approach involves a summation, which becomes problematic in continuous action domains, where the sum translates to an integral over actions. Therefore, we use the following alternative balance equation which involves the transition density of the induced Markov chain on the state-action space,\n\u03c0\u03c0D (\u03b4', \u03b1'|s, a) = \u03c0D(\u03b1'|s')T(s'|s, a). (3)\nThe above balance equation is the basis of our IL approach. If we can estimate P\u03c0D and T in (3) (estimates denoted by P and \u00ce respectively), we can use the balance equation to guide agent's learning. In our approach, we consider a combination of a policy-based loss function that measures the discrepancy between the demonstrator's and"}, {"title": "4 Experimental Results", "content": "Setup. We evaluate the empirical performance of our MBIL algorithm using the MuJoCo locomotion suite (Todorov, Erez, and Tassa 2012) and the classic control suite from OpenAI Gym (Brockman et al. 2016). MuJoCo tasks, with their varying difficulty levels, are a popular benchmark for IL in continuous action domains, while classic control environments like LunarLander are used to assess IL algorithms in discrete action domains. For constructing the demonstration dataset D, we use data from (Kostrikov, Nachum, and"}, {"title": "5 Conclusion", "content": "In this paper, we address the strictly batch imitation learning problem in a continuous state and action space setting, i.e., we do not assume access to any further interactions or supplementary data. We present a Markov balance-based imitation learning algorithm that combines it with supervised learning-based behavior cloning while using conditional normalizing flows for density estimation. Our method does not rely on reward model estimation and avoids stationary distribution matching which suffer from termination and reward bias. From numerical experiments, we see that our proposed algorithm does as well or much better than any state-of-the-art algorithm across a variety of Classic Control and MuJoCo environments. In fact, as can be seen in further results presented in the Appendix, it handles the distribution shift issue more effectively than other strictly batch imitation learning algorithms. Future research could benefit from deriving both asymptotic and non-asymptotic sample complexity bounds, which are scarce in current literature, and from exploring extensions to handle suboptimal data cases."}]}