{"title": "Enhancing DP-SGD through Non-monotonous Adaptive Scaling Gradient Weight", "authors": ["Tao Huang", "Qingyu Huang", "Xin Shi", "Jiayang Meng", "Guolong Zheng", "Xu Yang", "Xun Yi"], "abstract": "In the domain of deep learning, the challenge of protecting sensitive data while maintaining model utility is significant. Traditional Differential Privacy (DP) techniques such as Differentially Private Stochastic Gradient Descent (DP-SGD) typically employ strategies like direct or per-sample adaptive gradient clipping. These methods, however, compromise model accuracy due to their critical influence on gradient handling, particularly neglecting the significant contribution of small gradients during later training stages. In this paper, we introduce an enhanced version of DP-SGD, named Differentially Private Per-sample Adaptive Scaling Clipping (DP-PSASC). This approach replaces traditional clipping with non-monotonous adaptive gradient scaling, which alleviates the need for intensive threshold setting and rectifies the disproportionate weighting of smaller gradients. Our contribution is twofold. First, we develop a novel gradient scaling technique that effectively assigns proper weights to gradients, particularly small ones, thus improving learning under differential privacy. Second, we integrate a momentum-based method into DP-PSASC to reduce bias from stochastic sampling, enhancing convergence rates. Our theoretical and empirical analyses confirm that DP-PSASC preserves privacy and delivers superior performance across diverse datasets, setting new standards for privacy-sensitive applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep networks have achieved great access recently. However, the training of these networks relies heavily on large quantities of high-quality data, which often includes sensitive personal information, making data-driven deep models vulnerable to privacy attacks, such as membership inference attacks [24], [27], reconstruction attacks [32], [38], [45], property inference attacks [15], [22] and so on. In the realm of deep learning, ensuring privacy while maintaining the utility of deep neural models has emerged as a pivotal challenge.\nDifferential Privacy (DP) [12], a framework for safeguarding sensitive data, is widely adopted in deep learning. Stochastic gradient descent (SGD) combined with DP, usually denoted DP-SGD [1], has been demonstrated effective in providing rigorous guarantees, as illustrated in Fig. 1. Different from SGD, the iteration of DP-SGD when updating wk to wk+1 is  Wk+1 \u2190 Wk - \u03b7 ( \u03a3i\u2208Bk gi + \u03c3 \u22c5 C \u22c5 N (0, 1) ), where w is the model parameter, \u03b7 is the learning rate, Bk is the random batch in k-th iteration, C is a predefined constant to constrain the sensitivity of the gradients gk,i and \u03c3 is the standard deviation of differentially private Gaussian noise. Namely, the gradients gk,i are first clipped by the bound C and then are injected noise before updating w.\nThough effective, DP-SGD relies heavily on gradient clipping to meet privacy guarantees. The accuracy of deep models, however, is significantly influenced by the choice of the clipping threshold C, a parameter that determines the maximum allowable influence of each data sample on the model's updates. This threshold plays a central role in balancing the trade-off between privacy and accuracy. Specifically, an improperly set clipping threshold can lead to either excessive noise addition, which obscures useful data patterns and degrades model accuracy, or insufficient privacy protection, which risks exposing sensitive data features. As such, the choice of clipping threshold directly impacts the utility of models trained under differential privacy constraints, with profound implications for their deployment in sensitive applications. Some workable methods are to consume an extra privacy budget to search the proper C [25] or estimate the optimal C [2], [18]. However, these methods either incur a heavier privacy load or lead to new privacy leaking problems.\nOne promising approach to circumvent the labor-intensive process of setting a clipping threshold is to replace the clipping operation with strategic gradient scaling. Strategic gradient scaling not only avoids searching or setting a clipping threshold but also eliminates the bias introduced by directly clipping the gradient [8]. The original strategic gradient scaling is automatic clipping/normalizing [6], [37]. Automatic clipping/normalizing constrains the privacy sensitivity by scaling the original gradient g to \u011f = g/(||g||+r) where r is a small constant to make DP-SGD stable. All per-sample gradients are normalized to the same magnitude since ||\u011f|| \u2264 1."}, {"title": "II. RELATED WORK", "content": "Deep learning and differential privacy. Differentially private learning with gradient clipping and the Gaussian mechanism has emerged as a leading method in deep learning. The concept of constant clipping was first introduced by [1] to integrate privacy protection into SGD, a method known as DP-SGD. This approach was further explored in subsequent studies [4], [13], [19], [21], [23], [30], [31], [33], [41] to apply DP to other optimization algorithms, including DP-AdaGrad, DP-SVRG, and ApolySFW. From a theoretical standpoint, Zhang et al. [43] and Zhang et al. [44] examined the convergence properties of clipped SGD. On the application side, DP-Lora [40] and RGP [42] facilitated differential privacy in large-scale model fine-tuning using techniques like low-rank compression.\nDynamic threshold adjustments. Research indicates that the optimal clipping threshold varies throughout the optimization process [29]. Several studies have proposed dynamically adjusting the threshold during training to mitigate the drawbacks of a fixed threshold on DP-based algorithm performance. Andrew et al. [2] estimated the optimal clipping threshold using an additional privacy budget during optimization. Du et al. [11] suggested progressively decreasing the clipping threshold and noise magnitude over iterative rounds. More granular methods, such as those by Pichapati et al. [26] and Asi et al. [3], introduced axis-specific adaptive clipping and noise addition, applying different clipping thresholds and non-uniform noise to gradient components on different axes. Despite these advancements, these methods still require manual setting of the initial threshold, and the final performance is highly sensitive to this initial value.\nGradient scaling based approaches. To address the dependency on a clipping threshold, Bu et al. [6] and Yang et al. [37] concurrently proposed using normalization to control gradient sensitivity, termed Automatic Clipping (Auto-S) or Normalized SGD (NSGD). These approaches demonstrated that by normalizing all gradients to a uniform magnitude, only one hyperparameter needs tuning as the learning rate and clipping parameter become coupled. However, this technique introduces significant deviation between the normalized batch-averaged gradient and the original when some gradient norms are very small. To improve automatic clipping/normalizing, DP-PSAC [34] proposes the non-monotonous adaptive weight for gradients. The modified scaling weight aims to give small gradients with small weights while not influencing privacy sensitivity. However, this approach can inadvertently minimize the contribution of such gradients, stifling the model's ability to refine its parameters and converge to an optimal solution. Moreover, DP-PSAC ignores the bias introduced by stochastic sampling in SGD. Our method and algorithms overcome these limitations by assigning proper weights to small gradients in the later training stages and introducing the momentum method to improve the convergence rate. The proposed approaches achieve better theoretical and experimental results."}, {"title": "III. PRELIMINARY", "content": "The important notations are in TABLE I. Consider a deep neural network F(w) where w is the network parameter to be optimized. Given a private dataset D = {d1,...,dN} where di = (xi, Yi), the loss function f(di, w) is the empirical loss between the output of the deep neural network and the ground truth. The training process is to find w satisfying w = arg minw F(w) = arg minw  \u2211i=1N f(di, w). The optimization problem is usually denoted Empirical Risk Minimization (ERM)."}, {"title": "Definition 1 (Lipschitz continuity)", "content": "The function F(w) is L-Lipschitz, continuous if for all w\u2081, w2 in the domain, |F(w\u2081)-F(W2)| \u2264 L||W1 - W2||."}, {"title": "Definition 2 (Smoothness)", "content": "The function F(w) is \u03b2-smooth if \u2207F(w) is \u03b2-Lipschitz continuous."}, {"title": "Definition 3 (Differential privacy [12])", "content": "A randomized mechanism M satisfies (\u03f5, \u03b4)-differential privacy (DP) if for any two adjacent datasets D and D' that differ by a single individual's data, and for all S \u2286 Range(M)(the set of all possible outputs of M, the following inequality holds: Pr[M(D) \u2208 S] < e\u03f5 \u2022 Pr[M(D') \u2208 S] + \u03b4"}, {"title": "Lemma 1 (Gaussian Mechanism for Differential Privacy [12])", "content": "Let M : D \u2192 Rk be a function with l2-sensitivity \u03942M = ||M(D) \u2013 M(D')|| which measures the maximum change in the Euclidean norm of M for any two adjacent datasets D and D' that differ by a single individual's data. The Gaussian Mechanism adds noise drawn from N (0,\u03c32I) to the output of M and provides (\u03f5, \u03b4)-differential privacy if \u03c3> \u0394\u03b5\u039c.\u221a2log(1.25/\u03b4)"}, {"title": "B. DP-SGD", "content": "Differential privacy is often combined with SGD and widely applied in deep learning to safeguard both model and data privacy. The method is called DP-SGD [1]. The core idea of DP-SGD involves perturbing gradients by injecting handcrafted Gaussian noise. Given the original gradients {gi}i\u2208B where B represents a batch set of private samples, DP-SGD updates w using the perturbed gradients as follows:\nW \u2190 W - \u03b7 ( \u03a3i\u2208B min{1, C/||gi||}gi + \u03c3 \u22c5 C \u22c5 N (0, 1)) (1)\nSince ||gi \u00b7 min{1, C/||gi||}|| \u2264 C, the sensitivity of the clipped gradients is C. According to Lemma 1, Eq.(1) satisfies (\u03f5, \u03b4)-DP if we choose \u03c3 > C\u221a2 log(1.25/\u03b4)/\u03f5. DP-SGD often encounters issues due to the improper selection of C. To enhance DP-SGD, one could choose an appropriate Ct for each iteration t or C\u2081 for each layer in the model. Alternatively, a grid search for the learning rate \u03b7 and C could be conducted. However, grid searches can consume extra privacy budgets and be computationally intensive."}, {"title": "C. Gradient scaling weight", "content": "In Eq.(1), the coefficient min{1, C/||gi||} can be regarded as the gradient scaling weight w(gi). The designer can set appropriate gradient scaling weights to bound the gradient's sensitivity and avoid setting or searching the proper C. Usually, the weight w(gi) is designed as a function of the original gradients. Thus, the principle of designing w(gi) is to analyze the properties of the gradients to improve the performance of DP-SGD.\nThe weight function, w(gi) = min{1, C/||gi||} [1], has two significant drawbacks. When a large proportion of gradients satisfies ||gi|| \u2265 C, many gradients will be clipped. Conversely, when ||gi|| < C and particularly when the norm is very small, the noise generated from the Gaussian distribution N (0, 1) can be much greater than the original gradients. In both cases, the performance of the DP-SGD can be compromised.\nTo address these issues, automatic clipping/normalizing (Auto-S/NSGD) is proposed, introducing a gradient scaling weight [6], [37]:\nw(gi) = 1/(||gi|| + r) (2)\n, where r is a small regularization term to enhance training stability. Consequently, DP-SGD updates w using the perturbed gradients as follows:\nW \u2190 W - \u03b7 ( \u03a3i\u2208B gi/(||gi|| + r) + \u03c3\u00b7 N (0, 1) ) (3)\nIn Eq.(3), it is easy to verify that ||gi/(||gi|| + r)|| \u2264 1. Therefore, the gradient sensitivity is also 1. Automatic clipping/normalizing makes it possible to adaptively scale the influence of each sample based on its norm. However, the gradient weight in Eq.(3) assigns larger weights to smaller gradients, potentially increasing the weighted gain of a sample by up to 1/r times when its gradient approaches zero, where r is typically set to 0.01 or less. Small gradient samples, however, have minimal impact on batch-averaged gradients for the entire batch. Therefore, the monotonic weight function in Eq.(3) can sometimes be inappropriate, leading to lower accuracy of the trained models.\nTo further improve the performance, a non-monotonic weight function is proposed in Differentially Private Per-Sample Adaptive Clipping (DP-PSAC) [34]:\nw(gi) = C/(||gi|| + gr+r) (4)\nThis non-monotonic adaptive weight avoids assigning very large weights to small gradient samples, thereby reducing convergence errors. It assigns weights near 1 to small gradient samples, while weighting large gradients similarly to C/||gi||. DP-SGD updates w using the perturbed gradients as follows:\nw \u2190 w - \u03b7 ( \u03a3i\u2208B (C\u22c5gi)/(||gi|| + gr+r)+ \u03c3\u00b7 C \u00b7 N (0, 1)) (5)"}, {"title": "IV. OUR METHOD", "content": "In developing differentially private stochastic gradient descent (DP-SGD), balancing robust privacy with sustained model performance presents a formidable challenge. DP-PSAC introduces an innovative non-monotonic adaptive weight that dynamically adjusts the gradient weights based on gradient norms to enhance privacy-preserving mechanisms in deep learning. While this approach significantly improves upon traditional DP-SGD, it also harbors intrinsic limitations that can adversely affect model performance and the integrity of privacy guarantees. Previous work has overlooked crucial aspects of managing gradient magnitudes during training, particularly in the later stages where gradients significantly influence the effectiveness of privacy guarantees."}, {"title": "A. Ignored aspects in DP-PSAC", "content": "High proportion of small gradients: In the later training stages, there is an increased proportion of small gradients across batches. This shift suggests a transition to a fine-tuning phase and signals that the model is approaching a local minimum, where subtle yet critical adjustments are essential for optimal performance. We analyze and present the empirical distribution of gradients relative to their norms in the last 10 epochs over a CNN network trained with the FashionMNIST dataset and a ResNet9 network trained with the CelebA dataset. Fig. 2(a) and Fig. 2(b) display a shift distribution towards small gradients, indicating their increased impact on parameter updates during later training stages. This underscores the need to give greater consideration to these smaller gradients, which attracted less attention in the previous works.\nBy neglecting these factors, DP-PSAC tends to undervalue smaller gradients by assigning them disproportionately lower weights. This approach can skew the learning process, potentially ignoring minor but significant features (represented by smaller gradients), and thus deviating from the intended learning trajectory. Such bias can diminish the model's generalizability from training data to unseen data, compromising both accuracy and robustness.\nImpact of noise in DP-SGD. Traditional differential privacy optimizers incorporate noise addition to obscure the influence of individual data samples, protecting against privacy breaches via gradient analysis. For example, in DP-SGD, the original gradient g is masked by adding differentially private noise n, expressed as \u011d = g + n. To enhance privacy, one goal of DP-SGD is to maximize the ratio ||n||/||g|| under the same privacy level, thereby more effectively masking the private information contained in g. DP-PSAC offers potential for further improvement in this regard that can achieve greater privacy without necessitating additional privacy budget expenditure.\nAddressing these critical challenges necessitates the introduction of a new gradient scaling function within DP-SGD. By adjusting the scale of gradients according to their magnitudes, we hope that the proposed function would ensure that even as gradients diminish, they remain capable of contributing effectively to model learning, thereby enhancing both the practical utility and theoretical robustness of privacy-preserving machine learning models."}, {"title": "B. Non-monotonous Scaling Gradient Weight", "content": "Pay more attention to small gradients. To enhance performance through the utilization of a non-monotonous weight function, we introduce a refined model termed \"Non-Monotonous Scaling Weighted Gradients\" based on the empirical pieces of evidence, as delineated in Eq.(6). This model employs a revised weighting scheme to optimize outcomes effectively:\ng = Clip (g) = (C\u22c5g)/(s.||g|| + gr+r) (6)\nIn the revised formulation, the weight of the original gradient, denoted as ws(g), is defined as ws(g) = C/(s\u22c5||g|| + ||g||+r), where s represents the scaling coefficient of gradients.\nBy introducing s and comparing Eq.(6) with Eq.(4), the new method provides an additional lever to control how much influence the norm of the gradient has on the clipping process. This can be particularly beneficial in situations where the distribution of gradient norms is highly skewed or varies significantly during training. To understand this, we can see that adjusting s allows for finer control over the relative weighting of smaller gradients, potentially reducing the bias towards smaller gradients. For detailed explorations, the maximum value of ws(g) is given by maxws (g) = C/(1 \u2013 (1 - \u221asr)\u00b2). As s increases, maxws (g) decreases. This implies that, with a larger s, the weights of small gradients in the later stages of training can be effectively limited and minimized. When s = 1, Eq.(6) degrades to Eq.(4). Thus, we can find a proper s < 1 for different datasets to assign proper and larger weights to small gradients. The analysis also reveals the existence of an optimal scaling factor, s*, which signifies the peak performance of the differentially private model. Moreover, lim ||9||\u21920 ||gs|| + ||9|| + r = C. That is the same as DP-PSAC and it means the proposed weight is not too small or large and will not produce improperly scaled gradients for model training.\nBy scaling gradients inversely with their magnitude through the term C\u22c5g/||g||, we enhance the relative influence of smaller gradients by not diminishing their magnitude excessively. This scaling not only preserves the direction and integrity of the gradient but also balances the signal-to-noise ratio, improving the stability and convergence of the model under privacy constraints. A scaling function that modulates based on gradient magnitude addresses this distribution shift effectively. It ensures that small but numerous gradients are not nullified by aggressive normalization or swamped by noise. By amplifying the contribution of these small gradients relatively, the proposed function facilitates a more nuanced update process, preserving delicate training dynamics that are essential for achieving high accuracy in tasks requiring fine discrimination. Consequently, this approach helps maintain an optimal balance between privacy and learning efficacy, allowing the model to continue learning meaningful patterns from the data without being excessively perturbed by the privacy-preserving mechanisms.\nAmplify the impact of noise under the same level of privacy guarantee. Consider comparing the addition of noise to a gradient using our method and DP-PSAC. For an original gradient g, the noise-augmented gradient in DP-PSAC is g* = C\u22c5g/(||g|| + ||g||+r) + n* where n* ~ N(0, \u03c3\u00b2C2I). Under the same level of privacy guarantee, to fulfill the criteria for (\u03f5, \u03b4)-differential privacy (DP), the norm of the scaled gradient obtained via our method satisfies: || C\u22c5g/(s.||g|| + ||g||+r)|| = || C\u22c5g/(s.||g|| + ||g||+r) ||. Consequently, the scale of noise can be appropriately adjusted to (\u03c3\u22c5C\u22c5g)/(s.||g|| + ||g||+r), with \u03c3 being the same as in DP-PSAC. The noise-augmented gradient then becomes: g** = C\u22c5g/(s.||g|| + ||g||+r) + n** where n** ~ N(0, I). Then, we have:\nE||n**||/||g**|| > E||n* ||/||g* || \u21d4 ||C\u22c5g/(s.||g|| + ||g||+r)|| > ||C\u22c5g/(||g|| + ||g||+r)|| + ||n* || \u21d4 ||C\u22c5g/(||g|| + ||g||+r)|| > ||C\u22c5g/(||g|| + ||g||+r)|| + r, which holds when s < 1. This indicates that our method can more effectively obscure private gradients under the same privacy guarantee level."}, {"title": "C. Algorithms and Theoretical Analysis", "content": "Based on the above analysis, we propose DP-SGD integrated with non-monotonous scaling weight gradients, called Differentially Private Persample Adaptive Scaling Clipping (shorten by DP-PSASC), in Algorithm 1.\nFrom a theoretical perspective, adjusting s provides a way to achieve the same privacy guarantees with less impact on model performance while maintaining or even improving convergence rates. The flexibility offered by s allows for more nuanced adjustments to how gradients are clipped, which can lead to better optimization of the privacy-accuracy trade-off. For example, by tuning s based on the specific characteristics of the dataset and the learning dynamics, one can achieve higher model accuracy without substantially compromising privacy. In this part, we divide the privacy guarantee and the convergence analysis of our proposed method.\nThe first thing we want to emphasize is that C is avoided to be searched and could be integrated into the learning rate. It is because the model increment in the k-th iteration can be formulated as:\n\u0394wk = \u03b7/B \u2211i=1B \u011fk,i + N (0, \u03c3\u00b2/B) = \u03b7\u22c5C/B \u2211i=1B gk,i/(s.||gk,i|| + ||gk,i||+r) + N (0, \u03c3\u00b2/(B\u22c5(\u03b7\u22c5C)\u00b2))\nFor the privacy guarantee of DP-PSASC, we have the following theorem which can be derived from any common privacy analysis for DP-SGD.\nTheorem 1 (Privacy Guarantee of DP-PSASC). There exist constants c\u2081 and c\u2082 so that given the number of iterations T, for any \u03f5 \u2264 c\u2081T(N/B)^2 and 8 \u2264 1/T, Algorithm 1 is (\u03f5,\u03b4)-differentially private if \u03c3 \u2265 C2 BTln(1/\u03b4)/(Ne)\nThe next issue is whether DP-PSASC converges. We point out that DP-PSAC suffers from a bias in its convergence rate (see Theorem 2 in [34]) for they ignore the bias introduced by stochastic sampling in SDG (presented in our assumption 1). Generic theoretical analyses of DP-SGD with a scaling operation are presented for smooth non-convex optimization. We make minimal assumptions on the stochastic gradient distribution and only assume its second moment is bounded."}, {"title": "Algorithm 1: DP-PSASC", "content": "Input: Initial Weights wo, learning rate \u03b7, Batch Size B, Dataset D = {d1,...,dN}, Privacy Budget (\u03f5, \u03b4), Maximum Norm Size C, Number of Iterations T, Scaling Coefficient s.\n1 Compute the noise scale \u03c3\n2 for k = 0; k <T; k++ do\n3 Sample a batch Bk = {di}i=1B from D uniformly without replacement\n4 Compute the gradient gk,i for each sample di \u2208 Bk\n5 \u011fk,i = (C\u22c5gk,i)/(s.||gk,i|| + ||gk,i||+r)\n6 \u011fk = 1/B \u2211i=1B \u011fk,i + N (0, \u03c32)\n7 Wk+1 \u2190 Wk - \u03b7\u22c5\u011fk\nTo make the analysis more practicable, we also make an assumption concerning the norm of the per-sample gradient when the training process moves on. These assumptions are formally stated below.\nAssumption 1 (Bounded Second Moment of Stochastic Gradient). For any sample (x,y) drawn from the dataset D, the expected squared norm of the deviation between the full gradient and the stochastic gradient is bounded by \u03c4\u00b2, formally:\nE(x,y)~D [||\u2207F (w) \u2013 \u2207 f (w, x, y) ||\u00b2] \u2264 \u03c4\u00b2.\nAssumption 2 (Decreasing Gradient Norm Per Sample). Assume that for each sample (xi, Yi) \u2208 D, the norm of the gradient ||gk,i|| = ||\u2207f(Wk, Xi, Yi)|| monotonically decreases through the iterations, i.e.,\n||gk,i|| \u2264 ||gk-1,i||, (10)\nimplying improved optimization behavior as training progresses.\nTheorem 2 (Convergence Guarantee of DP-PSASC). Under the assumptions, if Fis \u03b2-smooth, then there exist some constant kand for r = O(T-), \u03b7 = O(T-1), 2s2F(wk)/(VB(T-k)(2+02d)), DP-PSASC ensures that for k = 1,2,...,T, Ek [mink ||\u2207F (Wk\u22121) ||] \u2264 O(T-\u00bd + \u03c4).\nThe main source of bias in DP-SGD comes from the stochastic nature of the sampled gradients that are used to update the model's parameters. Namely, SGD introduces the bias which is presented in Theorem 2.To mitigate the impacts caused by the sampling bias, the momentum-based variance reduction method is often adopted [9]. The momentum method, by averaging gradients over past iterations, can smooth out the effects of noisy or biased gradients. When gradients are clipped in DP-SGD, the stochastic nature of sampling can introduce bias, particularly if the clipped gradients are noisy or unevenly distributed. Momentum helps by averaging these gradients, reducing their variance and the overall noise, which indirectly corrects for biases introduced by stochastic sampling.\nDP-PSASC can be integrated with the momentum-based method smoothly. The integrated methods are presented in Algorithm 2 and Algorithm 3 where Algorithm 2 is the gradient descent (GD) version and Algorithm 3 is the stochastic gradient descent (SGD) version. For Algorithm 3, we adopt a specific technique called inner-outer momentum [36]. In this approach, per-sample scaled gradients over past iterates are first exponentially averaged (inner momentum) before scaling. After scaling, these gradients are then averaged again over past iterates (outer momentum) before being applied in the differentially private gradient descent update. This two-fold averaging process further smooths out erratic fluctuations in the gradient values due to stochastic sampling, leading to a more stable and accurate optimization path.\nWe demonstrate that the sampling bias of DP-PSASC is successfully eliminated and the convergence guarantee is presented in Theorem 3.\nTheorem 3 (Convergence Guarantee of DP-PSASC with Momentum). Under the conditions of Theorem 2, for r = O(T-), s = O(T-), \u03b3 = O(T-4), \u03b7 = O(T-), DP-PSASC with momentum ensures that for k = 1, 2, ..., T, Ek [mink ||\u2207F (Wk\u22121) ||] \u2264 O(T\u00af\u00b9).\nFrom Theorem 3, we can see that by setting proper scaling coefficient s and momentum parameter \u03b3, the bias introduced by stochastic sampling can be eliminated. Compared with Theorem 2 in DP-PSAC, we obtain a vanishing upper bound O(T\u00af\u00b9) and the term independent of O(T\u00af\u00b9) is eliminated."}, {"title": "V. EXPERIMENTS", "content": "Model settings. We evaluate performances over four datasets and models. We train a CNN model consisting of four convolutional layers and one fully connected layer with MNIST [20] and FashionMNIST [35]. For the CIFAR10 dataset [17], we train a simCLR model we keep the same experimental setup as [28] and use pre-trained SimCLRv2 [7] based on contrastive learning. Further, we train a ResNet9 model [16] on Imagenette [10] to validate the performance of our method on more complex multi-classification and multilabel classification problems. While conducting experiments, we keep C and r in Eq.(1), Eq.(3), Eq.(5) and Eq.(6) the same. We find the proper s in Eq.(6) to implement our proposed DP-PSASC to get higher test accuracy. After finding the proper s, we fix s and implement the momentum version of DP-PSASC with the proper momentum parameters. The momentum version of DP-PSASC is simply denoted as DP-PSASC*. The hyperparameter setting can be referred to in TABLE II.\nBaselines and configurations. We compare our method with DP-SGD (Eq.(1)) [1], Auto-S/NSGD (Eq.(3)) [6], [37], and DP-PSAC (Eq.(5)) [34]. All experiments are performed on a server with two Intel(R) Xeon(R) Silver 4310 CPUs @2.10GHz, and four NVIDIA 4090 GPUs. The operating system is Ubuntu 22.04 and the CUDA Toolkit version is 12.4. All computer vision experimental training procedures are implemented based on the latest versions of Pytorch and Opacus [39].\nEvaluation metrics. This study concentrates on the model accuracy within the context of differential privacy, particularly investigating the implications of two primary aspects: gradient variation under differential privacy and gradient scaling weight in the later training stages. All experiments are conducted 5 times and the test accuracy is the average result.\nGradient similarity: We then examine the influence of injected noise on the accuracy of gradient computations during the training process. For this analysis, we focus on the"}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this paper, we introduce a non-monotonic adaptive scaling weight approach for gradient updates and propose a novel differentially private stochastic gradient descent method, called DP-PSASC. We implement a scaling coefficient s <1 that is easier to search than C. Additionally, we present a momentum-enhanced variant of DP-PSASC that addresses the bias introduced by stochastic sampling. We support our methodology with both theoretical analysis and empirical evidence, demonstrating that DP-PSASC surpasses existing DP-SGD methods in performance. Looking forward, we plan to explore the principles for selecting s across various datasets and models. Moreover, there is potential to further enhance the efficiency of DP-PSASC."}]}