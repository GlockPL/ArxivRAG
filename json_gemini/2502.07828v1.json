{"title": "Some things to know about achieving artificial general intelligence", "authors": ["Herbert L. Roitblat"], "abstract": "Current and foreseeable GenAl models are not capable of achieving artificial general intelligence because they are burdened with anthropogenic debt. They depend heavily on human input to provide well-structured problems, architecture, and training data. They cast every problem as a language pattern learning problem and are thus not capable of the kind of autonomy needed to achieve artificial general intelligence. Current models succeed at their tasks because people solve most of the problems to which these models are directed, leaving only simple computations for the model to perform, such as gradient descent. Another barrier is the need to recognize that there are multiple kinds of problems, some of which cannot be solved by available computational methods (for example, \"insight problems\u201d). Current methods for evaluating models (benchmarks and tests) are not adequate to identify the generality of the solutions, because it is impossible to infer the means by which a problem was solved from the fact of its solution. A test could be passed, for example, by a test-specific or a test-general method. It is a logical fallacy (affirming the consequent) to infer a method of solution from the observation of success.", "sections": [{"title": "Predictions That Intelligence is Coming", "content": "Predictions that artificial intelligence is on the verge of achieving general intelligence keep on coming (e.g., Altman, 2025; Leike & Sutskever, 2023; Modei, 2024). For example, one estimate is that 88% of the necessary capabilities (Thompson, 2025) have been achieved. The Al safety clock (International Institute for Management Development, 2024) is a symbolic representation to how close the world is to not just artificial general intelligence, but uncontrolled AGI. OpenAl recently announced a \u201cbreakthrough\" on the ARC-AGI benchmark (Chollet, 2019).\n\nThese predictions have also raised widespread concerns. The Future of Life Institute (2023) currently has over 33,000 signatures on its open letter to temporarily halt the development of large language models. The Center for Al Safety (2023) released a statement, signed by hundreds of individuals, that \u201cMitigating the risk of extinction from Al should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\" The Artificial Intelligence Action Summit convened in Paris in February 2025.\n\nOn the other hand, Lu et al. (2024) argue that large language models only follow instructions and have no potential to autonomously master new skills without some explicit instruction.\n\nGovernments around the world are considering regulations based on these threat assessments. For example, in 2024, the California legislature passed and the governor vetoed SB-1047 Safe and Secure Innovation for Frontier Artificial Intelligence Models Act. (2023-2024). Among other things, it required \u201cimplementing the capability to promptly enact a full shutdown\u201d of any covered Al model-a \"kill switch\".\n\nAccording to a study commissioned by the US State Department (Harris, Harris, & Beal, 2024) (The Gladstone Report),"}, {"title": "My Intention", "content": "My intention is not to debate directly the potential future risks of Al, rather it is to examine the current class of models and their potential to achieve artificial general intelligence. The current models are too basic and too dependent on human intervention to pose the kind of risks that governments and others are concerned about. General intelligence may be achievable, but it will require inventions, techniques, and discoveries that are yet to be made, and which are very difficult to predict.\n\nKnowing how current models work, and their prospects for leading to general intelligence is critical. Regulations based on misunderstanding will be either ineffective or harmful. Similarly investment strategies based on exaggerated claims are likely to fail.\n\nIn my opinion, the foreseeable risk from Al comes not from intelligence or superintelligence, but from the opposite, from stupidity, and from how people may misuse that stupidity in combination with people's credulity. The risk is not from what computers can do, it is from what they cannot do, and from what we mistakenly believe that they can do."}, {"title": "Artificial General Intelligence", "content": "The goal of artificial general intelligence is to build a model that is capable of the full range of human problem solving. The models do not have to solve all problems in the same way that humans do, but if they are to be considered general intelligence, they need to cover the full range of problems.\n\nJohn McCarthy, who coined the term \u201cartificial intelligence\u201d defined it in a proposal for a summer workshop on the topic (McCarthy, et al., 1955). \u201cThe study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\"\n\nI.J. Good (1965), taken by many to be the proposer of an eventual Al singularity, defined superintelligence this way: \u201cLet an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever.\u201d\n\nEven before that, Herbert Simon (Simon & Newell, 1958) defined general intelligence in a keynote speech:\n\n\"It is not my aim to surprise or shock you-but the simplest way I can summarize is to say that there are now in the world machines that can think, that can learn and that can create.\nMoreover, their ability to do these things is going to increase rapidly until \u2013 in a visible future -"}, {"title": "Range of Problems", "content": "the range of problems they can handle will be coextensive with the range to which the human mind has been applied.\"\n\nBy the end of the 1960s, the quest for general intelligence was predominantly replaced by a more pragmatic approach. Rather than try to solve everything all at once, the focus narrowed to solving specific problems, and then specific problems with economic value, for example, detecting cancer in radiologic images.\n\nAlthough people, such as Douglas Hoftsatadter (1979) argued that such problems as winning at chess would require general intelligence, instead, the most successful chess programs were narrowly focused on traversing a tree of potential moves and counter moves. The problem of playing chess was reduced to the problem of traversing a tree, a much simpler task for which computational methods were available.\n\nThe problems on which artificial intelligence was successful were all well-structured. A well- structured problem (Simon, 1973) includes a definite criterion for testing a potential solution, a problem space representing the initial problem state, the final goal state, and all intermediate states, and a set of potential state changes (moves). The moves could be traversing down the branches of a tree (as in chess playing) or adjusting a parameter, for example. Solving a problem is the process of finding a sequence (path) of moves to achieve an explicitly defined outcome.\n\nGames like chess and go are well-structured and also perfect information problems. There is no uncertainty about the current state of the game, it is defined perfectly by the position of the pieces on the board. There is no randomness. The opponent's moves depend on the player's choices not the roll of dice or other, unpredictable event.\n\nGames like chess and go are also formal problems. The legal moves of chess and the consequences of making them do not depend, for example, on the physical properties of the chess pieces and the board. Playing chess does not depend on physically moving the pieces. Entire games of chess can be played symbolically, without ever touching a physical piece. As a result, computers can play one another without any human intervention because the rule and the states can be perfectly specified.\n\nNarrow Al has been remarkably effective at solving a range of these well-structured problems. But many natural problems are unstructured or ill-structured. There is no predefined problem space, no fixed set of moves, no clearly defined and easily verified goal. The state of the problem may not be clearly known at any point in time, there may be uncertainty, and the consequences of any particular move may be not be known for some time.\n\nNatural problems may also be vexatious. For example, according to the World Economic Forum (2024), Global Shapers Community Report 2022-2023, the priority problems for the world include:\n\n\u2022 Combatting climate change and safeguarding nature\n\u2022 Closing skills gaps and driving ethical innovation\n\u2022 Strengthening civic spaces and diversifying leadership"}, {"title": "More on the Range of Problems", "content": "\u2022 Advancing diversity, equity, inclusion, belonging and social justice\n\u2022 Safeguarding mental health and overcoming persistent disparities\n\u2022 Responding to disasters and in times of crisis\n\nThese problems are ill-structured and under-specified. They have no definitive formulation by which they could be solved. We may have hypotheses, but we do not know how to go about solving them. It is difficult to determine whether a particular action serves to approach a solution.\n\nA major part of a vexatious problem, or any ill-structured problem, is just framing it in a useful way and determining what approaches to try. Is climate change best viewed as a problem of reducing carbon in the atmosphere (for example by sequestering it), or one of reducing the introduction of new carbon? Is it better to build projects that mitigate the effects of climate change, such as the barrier installed to protect Venice (Moraca, 2024) or is it better to reduce the carbon? Is it better to raise Venice or install flood barriers at the entrance to the lagoon? One key problem (meta problem?) in addressing these is identifying what exactly are the specific problems we need to solve and what are the means by which we will attempt to solve them. Even after attempting to solve them, there is no way to definitively determine if the solution has been effective.\n\nArtificial general intelligence should be able to address problems and meta-problems like these. Perhaps it might be too much to expect any model to address issues of the magnitude described in the Global Shapers Report, but analogous, limited-scope, problems are faced every day by ordinary humans and so should be addressable.\n\nWhen Al methods have been applied to ill-structured problems, the approach is usually for a human to simplify the problem into a well-structured one. Humans apply enough structure to the problem to leave just a well-structured one for the computer to solve. Once the problem is structured, then it can be solved using well-known computational techniques (e.g., gradient descent algorithms). But solving this simplified problem may not provide an actual solution to the full problem.\n\nEven well-structured problems presently require substantial human input. The GenAl models are capable of addressing many nominal problems, but that is because all of the truly difficult parts of the problem have been provied by human designers. For example,\n\nHuman contributions to GenAl problem solving:\n\n\u2022 Training data\n\u2022 Number of neural network layers\n\u2022 Types of layers\n\u2022 Connection patterns\n\u2022 Activation functions\n\u2022 Training regimen for each layer\n\u2022 Number of attention heads\n\u2022 Parameter optimization method\n\u2022 Context size"}, {"title": "Even More on the Range of Problems", "content": "\u2022 Representations of words as tokens and vectors\n\u2022 Training task\n\u2022 Selection of problems to solve\n\u2022 Training progress measures and criteria\n\u2022 Human feedback for reinforcement learning\n\u2022 Rules for modifying parameters as a result of human feedback\n\u2022 Prompt\n\u2022 Temperature and other meta-parameters\n\nMachine contribution to GenAl problem solving:\n\n\u2022 Parameter adjustments through gradient descent\n\nThe need for human input and the role it plays in solving these problems is rarely acknowledged or reported. For example, Goh et al. (2024) studied the diagnostic capabilities of what they called \"the standalone performance of the LLM\u201d compared with the performance of two groups of physicians, one of which had access to a large language model, and the other of which did not. Later in the paper, they mention that they spent significant time to iteratively develope a zero-shot prompt for the standalone version of the language model. So, what they were actually comparing was an LLM with a carefully designed prompt versus the same model with prompts created in a few minutes by physicians unskilled in prompt design. The difference between groups, then, was the quality of the prompt, not the quality of the model used to make the diagnosis. The differential intelligence was again provided by the prompt writer; the model was not effective without the carefully constructed prompts.\n\nModels with this level of dependence on humans cannot be autonomous, cannot escape control, and cannot be generally intelligent. An Al model cannot achieve the goals of general intelligence until it can address the full \u201crange [of problems] to which the human mind has been applied\" (Simon & Newell, 1958), from inception to specification of the problem space to finding an appropriate solution and verifying it. The kinds of problems that humans now solve for Al must surely be within the range described by Simon and Newell.\n\nOnce the human contribution is provided, all that is left is for the models execute conceptually simple \"moves\u201d to adjust the values of the provided parameters to meet the provided goal of the computation.\n\nMany of the problems that humans solve for the Al models can be described as \u201cinsight\" problems. Multilayer neural networks became practical when someone (actually several people over time; Schmidhuber, 2022) had the insight to adjust parameters through \"backpropagation,\u201d error correction proportionate to a parameter's contribution to the observed error. Before that, it was well known that single-layer perceptrons (simple neural networks) could learn to solve certain problems and the algorithm for doing that was known (the perceptron learning rule). It was also known that multi-layer perceptrons could solve more complex problems, but there was no known algorithm to use to train them.\n\nMore generally, insight problems cannot be solved by a step-by-step procedure, like an algorithm. Specific instances of a problem may be solvable by \u201cbrute force,\u201d that is iteratively"}, {"title": "Even Even More on the Range of Problems", "content": "trying many potential solutions, but a general solution is possible through a kind of restructuring of the solver's approach to the problem. Unlike well-structured problems, it is not possible to assess whether each step brings the solver closer to the goal. Until the right structure is achieved, all solutions are wrong.\n\nOne example of an insight problem is the mutilated checkerboard problem (Black, 1946; Heule, Kiesl, & Biere, 2019). A regular checkerboard has 32 black squares and 32 red squares. If we had 32 dominoes, each the size of two squares, it would be obvious that we could cover a checkerboard with those 32 dominoes. If we cut off the red square at the upper left corner of the checkerboard and the red square in the lower right corner of the checkerboard, could we now cover the 62-square mutilated checkerboard with 31 dominoes? There are still twice as many squares as dominos, so it may be solvable.\n\nOne potential solution would be to treat it as a well-structured problem and successively try possible arrangements of the 31 dominos. If the first arrangement does not work, the second might, and so on. A full checkerboard has over 12 million ways to arrange the dominos. So, for a human, this brute force approach would not be feasible.\n\nBeing faster, a computer might be able to try all possible arrangements, but it would be a lot faster if the computer had the insight that each domino must cover one square of each color. After the mutilation, there are now more black squares than red squares, so it is not possible to cover the mutilated checkerboard. This insight is called a \u201ccoloring argument\u201d (Ardilla & Stanley, undated) and it is very commonly used to solve many tiling problems. So, this insight has applications far beyond the original mutilated checkerboard, but the brute force method can only solve this specific problem. A larger checkerboard would have to be solved with the same brute force method, but the coloring argument could determine immediately whether it could be covered. Once we think of it in the right way, the solution to the problem becomes obvious.\n\nKaplan and Simon (1990, p. 2) describe insight problems with this metaphor:\n\n\"Imagine that you are searching for a diamond in a huge, dark room. One option is to grope blindly in the dark. ... But after groping blindly for several minutes, you might decide to abandon the search for the diamond, and to search instead for a light switch. If one could be found, and the light turned on, the location of the diamond could be evident almost at once.\"\n\nCurrent Al models can grope along the floor, or they can search for a light switch, but unless both strategies are designed into them, they cannot switch from the approach they were designed for to another one.\n\nCurrent GENAI models can report an insight solution to known problems, because descriptions of the solutions to problems are widely available on the Web. They can report solutions that people have found and written about. They would not, however be able to solve insight problems for which there is no known answer, but that is a skill that a general intelligence would need.\n\nGeneral intelligence is itself an example of an ill-structured problem. It cannot be described with an explicit set of moves, because a necessary characteristic of general intelligence is that it"}]}