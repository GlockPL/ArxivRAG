{"title": "EMERGENCE OF ABSTRACTIONS: CONCEPT ENCODING AND DECODING MECHANISM FOR IN-CONTEXT LEARNING IN TRANSFORMERS", "authors": ["Seungwook Han", "Jinyeop Song", "Jeff Gore", "Pulkit Agrawal"], "abstract": "Humans distill complex experiences into fundamental abstractions that enable rapid learning and adaptation. Similarly, autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. In this paper, we propose concept encoding-decoding mechanism to explain ICL by studying how transformers form and use internal abstractions in their representations. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of concept encoding and decoding. As the model learns to encode different latent concepts (e.g., \"Finding the first noun in a sentence.\") into distinct, separable representations, it concureently builds conditional decoding algorithms and improve its ICL performance. We validate the existence of this mechanism across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B). Further, through mechanistic interventions and controlled finetuning, we demonstrate that the quality of concept encoding is causally related and predictive of ICL performance. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.", "sections": [{"title": "1 INTRODUCTION", "content": "Throughout history, humans have made sense of the world by distilling complex experiences into fundamental abstractions, such as physics and mathematics. These mental models enable us to learn quickly, predict outcomes, and adapt to new situations. In artificial intelligence, autoregressive transformers are beginning to exhibit similar capabilities (Brown et al., 2020; Bubeck et al., 2023; Ajay et al., 2023; Han et al., 2024). Through in-context learning (ICL), they adapt to new tasks"}, {"title": "2 RELATED WORK", "content": "Mechanisms of ICL. Astounded by LLMs' ability to perform ICL, many have proposed theories to understand the mechanisms of ICL. Some (Dai et al., 2023; von Oswald et al., 2023; Ahn et al., 2024; Aky\u00fcrek et al., 2024) have proposed that LLMs, with linear attention (Katharopoulos et al., 2020), can implement stochastic gradient descent to perform ICL. Other works (Xie et al., 2021; Wang et al., 2024; Ye et al., 2024) have presented a Bayesian framework to theoretically explain the workings of ICL. This view implies that the model implements a two-stage algorithm to estimate the posterior $P(z|D)$ and the likelihood $P(y*|x*, D)$. In this work, we adopt this framework and"}, {"title": "3 UNDERSTANDING IN-CONTEXT LEARNING", "content": ""}, {"title": "3.1 NOTATION AND BACKGROUND", "content": "We focus on ICL problems, where the goal is to predict $y*$ from a query $x*$, given some in-context examples $D = {(xi, Yi)}=1$. Each problem shares a latent concept z that links inputs a to outputs y. For instance, in an ICL task where latent concept is object-color mapping, we provide demonstrations like (apple, red), (banana, yellow), and (grape, purple), and then ask for what comes after (lemon, ?). We employ this parameterization to accommodate latent concepts varying in complexity, from simple function regression problems (Garg et al., 2022; von Oswald et al., 2023; Li et al., 2023) to POS tagging (Blevins et al., 2022; Banko & Moore, 2004) and arithmetic (He et al., 2024)."}, {"title": "3.2 THEORETICAL FRAMEWORK", "content": "Of the many different frameworks (Bai et al., 2024; Min et al., 2022; von Oswald et al., 2023; Aky\u00fcrek et al., 2024) to understand the workings of ICL, we adopt the Bayesian view (Xie et al., 2021; Mittal et al., 2024; Wang et al., 2024; Ye et al., 2024). It proposes that transformers implicitly infer the latent variable z underlying the demonstrations and apply it to generate an answer. More formally,\n$P(y* | x*, D) = \\int Po(y* | x*, z) Po(z | D) dz$                                                                                                                                                                                                                                                                                                                                                                                                                                            (1)\nThis framework suggests ICL is a two stage process. First, latent concept inference. Latent concept z is approximated from D through the distribution $z \\sim Po(z | D)$. Second, selective algorithm application. The model applies an algorithm conditioned on 2 to predict y as given by $Po (y* | X*, z)$.\nAlthough theoretically compelling, it was not until recently that Hendel et al. (2023); Todd et al. (2023); Merullo et al. (2023) showed empirical evidence of models encoding the latent concepts in the intermediate representations. They illustrate that concept-specific vectors are then decoded and trigger the desired ICL task behavior. With simple encoder-decoder analogy, these findings suggest"}, {"title": "3.3 MOTIVATION: SYNTHETIC EXPERIMENTS", "content": "We train a small transformer on a synthetic ICL task and demonstrate that concept encoding and decoding emerges simultaneously during training. Through causal analysis, we show that, as the models \"discovers\u201d a latent concept by building a distinct representation space from the others, it associates the concept with a uniquely corresponding decoding algorithm. Finally, we propose the concept encoding-decoding mechanism that encompasses these findings and serve as the core theory throughout the remainder of our study.\nTask. We compose our task as a mixture of sparse linear regression tasks. We follow the conventional linear regression setup from Garg et al. (2022); von Oswald et al. (2023) and construct the input-output pair $(xi, Yi)$ by sampling $xi \\sim N(0, ID)$ and $yi = WTxi + ei$, where W is randomly generated from a standard normal distribution, $N(0, ID)$, and $ei \\sim N(0,02)$. We, however, add sparsity constraints to W with the sparsity pattern represented by the basis Bk. Each Bk has a rank of r. In other words, the basis chooses the dimensions of W to turn on and off. The basis is sampled uniformly from B = {B1, B2, B3, B4} and each basis is non-overlapping and orthogonal to each other. By default, we set D = 16 and r = 4. By adding this layer of latent concept of B, we can explicitly control and interpret the latent concepts, and analyze their representations.\nModel and Training. We train a 12-layer GPT-2 architecture transformer (Radford et al., 2019) with an embedding dimension of 128 and 8 attention heads. We train the model to minimize mean squared error (MSE) loss over the sequence length of 20. We run 5 different random seeds for training and report observations that generalize across the runs. We detail the experimental setup further in Appendix C.2.\nTheoretical Error Bounds. The error bounds of our task depend on whether the model learns to infer the underlying bases. If the model learns to infer the bases, then the model can achieve r-dimensional regression, where the MSE approaches 0 with r in-context examples. If not, the model,"}, {"title": "3.4 CONCEPT ENCODING-DECODING MECHANISM", "content": "Based on the observations above, we introduce concept encoding-decoding mechanism that serves as the core theory throughout the paper. The formal definition is in Appendix B\nDefinition 1 (Concept Encoding-Decoding Mechanism) Over the course of training,\ntransformers learn separable representations by concept \u2013 concept encoding. Simultane-ously, the model learns and applies concept-specific algorithms by leveraging the separablerepresentation spaces \u2013 concept decoding. We define concept encoding-decoding mecha-nism as this two-stage process for ICL, which is mutually dependent with one another andemerges concurrently during training."}, {"title": "4 TOWARDS NATURAL EXPERIMENTS", "content": "In this section, we empirically validate the proposed concept encoding-decoding mechanism in pretrained LLMs. Specifically, we test several hypotheses derived from our proposed mechanism: whether pretrained LLMs exhibit concept encoding-decoding behavior and whether the quality of concept encoding-decoding predicts ICL performance on more natural tasks.\nTasks. We construct two classes of algorithmic tasks \u2013 natural language processing and arithmetic \u2013 comprising a total of 12 tasks. Within each class, the tasks are designed to be semantically similar, ensuring that the input distributions are alike across tasks. While the underlying latent concepts differ (e.g., different arithmetic operations or linguistic patterns), the surface features of the inputs remain consistent. By keeping the input distributions similar, we can effectively assess the model's"}, {"title": "4.1 CONCEPT ENCODING-DECODING IN PRETRAINED LLMS", "content": "Hypothesis: Concept encoding-decoding behavior exists in pretrained LLMs.\nWe investigate the above hypothesis in two steps: (1) We first examine whether concept encoding occurs in pretrained LLMs; (2) We conduct mechanistic intervention studies to verify that different concept encoding triggers separate decoding algorithms, completing the full study of the concept encoding-decoding mechanism.\nStep 1: Concept Encoding. We first study whether the concept encoding occurs in pretrained LLMs. We vary the number of in-context examples for the different tasks and visualize the intermediate representations at the middle layers with UMAP in Figure 4. Given only 1-shot, where the model is expected to be confused about the latent concept, all of the representations are clustered and overlap with the Null class, which has no task latent. As examples increase, clustering by latent concepts emerges, becoming clearer by 10-shots. Interestingly, the separation of concepts, such as AND, OR, Noun, and Pronoun, is more pronounced. On the other hand, XNOR and XOR in bitwise arithmetic and Adjective and Preposition in POS tagging overlap significantly with Null. We conjecture the model likely sees and learns the former set of concepts better during pretraining. Overall, the analyses highlight that pretrained LLMs also exhibit concept encoding behavior and suggest taht in-context examples materialize as learning signal by creating more separable representations per latent concept. We will further explore this connection between the separability of concepts in the representation space and ICL performance in Section 4.2.\nTo quantify how the separability of representations translates into the decodability of the latent concepts, we compute the CD scores across the layers. In Figure 5a, we see that the the decodability"}, {"title": "4.2 PREDICTABILITY OF IN-CONTEXT LEARNING TASK PERFORMANCE", "content": "Hypothesis: Quality of concept encoding-decoding, measured by CD, is predictive of ICL performance.\nWe now investigate the second hypothesis of whether the quality of concept encoding-decoding is predictive of downstream ICL performance. If the model is conditionally applying a decoding algorithm to perform the task by first inferring the latent concept, the accuracy of the latent concept encoding (measured by CD) and ICL task performance should be closely correlated. To this end, we analyze the relationship between CD and test accuracy in Figure 6. In both datasets, we see that, generally, higher CD scores correspond to better performance on the respective tasks. More interestingly, referring back to Figure 4, we again remark that the representations of some classes (Adjective and Preposition in POS tagging and XOR and XNOR in bitwise arithmetic) are mapped to those of the Null class. We notice that this set of classes whose representations overlap with those of Null generally have low task performance and do not improve as much as the others given more demonstrations. We conjecture that the model does not accurately encode latent concepts of those that are overlapped with the Null class representations.\nWe also test the generality of the predictability of ICL performance from CD across a different model family (Team, 2024) and scales. We conduct the same analysis on Gemma-2 2B, 9B, and 27B and Llama-3.1 70B and present the results in Figure 19 in Appendix E.2. These results demonstrate that the correlation between CD and ICL performance is robust across models and tasks. Interestingly, in all of the Gemma-2 family and Llama-3.1 70B models, Noun, Pronoun, and Verb show the clearest signs of concept encoding-decoding behavior, as we saw in the Llama-3.1 8B model. In the bitwise arithmetic task, AND, NAND, OR, and NOR (classes that showed the strongest encoding-decoding"}, {"title": "4.3 INVESTIGATING THE CAUSAL EFFECT OF CONCEPT ENCODING BY FINETUNING", "content": "Hypothesis: In transformer-based LLMs, earlier layers learn to encode concept, whereas the latter layers condition the algorithm on it. Thus, finetuning only the earlier layers can improve concept encoding and thus will be more effective for improving ICL performance than finetuning only the latter layers.\nTo further investigate the causal importance of concept encoding for downstream ICL performance, we perform two types of finetuning: only the first 10 layers versus only the last 10 layers. We previously found that concept encoding occurs in the middle layers (layer 15 for POS tagging and layer 13 for bitwise arithmetic). Finetuning only the last 10 layers restricts the model from learning to encode latent concepts in intermediate representations. As illustrated in Figure 7, finetuning the last 10 layers barely changes their CD scores from the pretrained model. In contrast, finetuning the first 10 layers significantly improves the CD scores and aligns the representation subspaces with the inferred latent concepts. This improvement in CD scores directly translates to significantly better ICL task performance. With 4-shot examples, finetuning the first 10 layers outperforms finetuning the last 10 layers by ~ 37% in the POS task and 24% in bitwise arithmetic. In the bitwise arithmetic task, finetuning the first 10 layers achieves near-perfect accuracy for all tasks except XNOR, whose representations overlap with those of Null."}, {"title": "4.4 INVESTIGATING THE EFFECT OF PROMPTING ON CONCEPT ENCODING-DECODING", "content": "Hypothesis: Prompting enhance CD by providing a stronger learning signal for concept inference, and thus improve ICL performance correspondingly.\nOur study reveals that enhancing concept encoding is a unifying principle that improves in-context learning (ICL) across different strategies. We observe in Sections 4.1, 4.2, and 4.3 that increasing"}, {"title": "5 DISCUSSION", "content": ""}, {"title": "5.1 IMPLICATIONS OF CONCEPT ENCODING-DECODING MECHANISM", "content": "Our proposed concept encoding-decoding mechanism has several implications in light of recent works on understanding the mechanics of ICL (Mittal et al., 2024) and activation-steering methods (B\u00fcrger et al., 2024; Panickssery et al., 2024; Marshall et al., 2024).\nWhy do models succeed at some ICL tasks, but not others? It is yet puzzling how to categorize the types of ICL tasks LLMs can and cannot solve (Qiu et al., 2023; Dziri et al., 2023). An intuitive explanation is that the model can effectively encode the concepts frequently seen during pretraining (Razeghi et al., 2022; Li et al., 2024). In our experiments, we observe patterns consistent with this conjecture, where AND and OR, the more common logical operators in language, were encoded more accurately. However, under our proposed two-stage mechanism, we show the bottleneck in ICL tasks can exist in both levels of concept inference and subsequent decoding algorithm. Therefore, even if the model already learned the algorithm for a NOR operator, if the model cannot clearly distinguish the latent concept from the inputs, it will fail, and vice versa. As our experiments suggest in Section 4.3, when the model is failing at concept encoding, a different prescription of early layer finetuning for representational alignment is more beneficial.\nDoes learning the right latent variables help? Mittal et al. (2024) investigate whether explicitly modeling the latent variables in in-context learning (ICL) outperforms implicit learning through ordinary autoregressive training with transformer. They draw the counterintuitive conclusion that explicit modeling does not enhance performance, albeit not worse; the underlying reasons for which remain unclear. In our work, we explains this specific observation by analyzing the extent to which implicit modeling (standard transformers) captures the true latent variables. Our findings show that transformers can inherently encode these latent variables without explicit regularization. Therefore, we propose that the comparable performance between explicit and implicit models arises not because modeling the latent variables is unhelpful, but because both types of models effectively learn them.\nWhy may activation-based interventions work? Our proposed encoding-decoding mechanism offers a lens to understand the broader impact of concept representations in LLMs beyond in-context"}, {"title": "5.2 LIMITATIONS", "content": "A limitation of our work is that the experimental setup used in this study does not encompass tasks that require multi-step reasoning (Clusmann et al., 2023; Zupan et al., 1999; Hosseini et al., 2024). Although we analyze the concept encoding-decoding mechanism with varying levels of complexity in Appendix C.4, further studies are essential to apply our findings and insights to the real-world. Another limitation stems from our proposed CD metric. Since we measure the separability from one concept to another, for the measure to be meaningful, the distribution of tasks on which CD is computed needs careful design to consist of semantically similar, confusing tasks."}]}