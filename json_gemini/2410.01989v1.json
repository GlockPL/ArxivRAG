{"title": "UlcerGPT: A Multimodal Approach Leveraging Large Language and Vision Models for Diabetic Foot Ulcer Image Transcription", "authors": ["Reza Basiri", "Ali Abedi", "Chau Nguyen", "Milos R. Popovic", "Shehroz S. Khan"], "abstract": "Diabetic foot ulcers (DFUs) are a leading cause of hospitalizations and lower limb amputations, placing a substantial burden on patients and healthcare systems. Early detection and accurate classification of DFUs are critical for preventing serious complications, yet many patients experience delays in receiving care due to limited access to specialized services. Telehealth has emerged as a promising solution, improving access to care and reducing the need for in-person visits. The integration of artificial intelligence and pattern recognition into telemedicine has further enhanced DFU management by enabling automatic detection, classification, and monitoring from images. Despite advancements in artificial intelligence-driven approaches for DFU image analysis, the application of large language models for DFU image transcription has not yet been explored. To address this gap, we introduce UlcerGPT, a novel multimodal approach leveraging large language and vision models for DFU image transcription. This framework combines advanced vision and language models, such as Large Language and Vision Assistant and Chat Generative Pre-trained Transformer, to transcribe DFU images by jointly detecting, classifying, and localizing regions of interest. Through detailed experiments on a public dataset, evaluated by expert clinicians, UlcerGPT demonstrates promising results in the accuracy and efficiency of DFU transcription, offering potential support for clinicians in delivering timely care via telemedicine.", "sections": [{"title": "1 Introduction", "content": "Diabetes is a rapidly growing global health issue, with more than 537 million adults affected worldwide as of 2021. There is a projection that this number will reach 783 million by 2045, reflecting the increasing prevalence of the disease across various populations [20]. Among the many complications associated with diabetes, Diabetic Foot Ulcers (DFUs) [1] are among the most severe and challenging to manage. Affecting approximately 15-25% of diabetic patients during their lifetime, DFUs are a leading cause of hospitalizations and lower limb amputations [18,3,1]. The burden of DFUs on patients' quality of life and healthcare systems underscores the critical need for effective management strategies [10].\nResearch indicates that early intervention can significantly reduce the risk of lower extremity amputations by a high proportion [1]. However, many patients face delays in receiving appropriate care due to limited access to specialized healthcare services and geographic barriers [12]. Integrating Artificial intelligence (AI) and pattern recognition into telemedicine enhances DFU management by enabling automatic detection, classification, and monitoring from images [27]. These tools offer accurate, quick assessments, support clinicians in prioritizing urgent cases, and ensure consistent monitoring in telehealth settings.\nLarge Language Models (LLMs) have recently gained significant attention due to their capability to process and understand text at an advanced level. Models such as Generative Pre-trained Transformers (GPT) [24] have become widely used across various domains, including healthcare, for tasks such as text generation, question answering, and managing complex information [29]. In image analysis, Vision Transformers (ViTs) [11] have made substantial advancements. ViTs can process visual data, such as medical images, and convert it into a format that LLMs can utilize, facilitating the integration of image and text processing. This capability enhances the interpretation and analysis of medical images [19].\nA multitude of AI-driven approaches have been proposed for the identification, detection, classification, and localization of DFUs from foot images, utilizing techniques from machine learning, deep learning, and computer vision [5,35,4]. However, to the best of our knowledge, the application of LLMs for DFU image transcription remains unexplored. DFU image transcription is essential for facilitating telemedicine by enabling timely detection and identification of ulcers, which supports clinicians in providing prompt and effective care. To address the gap in this field, this paper introduces UlcerGPT, a novel multimodal approach that leverages large language and vision models to transcribe and analyze DFU images. This method aims to enhance the accuracy and efficiency of DFU detection and classification, thereby further supporting clinicians in telemedicine. This work makes the following contributions:\nBy combining advanced vision and language models, such as Large Language and Vision Assistant (LLaVA) [17] and Chat Generative Pre-trained Transformer (ChatGPT) [21], a new deep-learning framework was introduced to transcribe DFU images by jointly detecting, tokenizing, and narrating DFU's elements of interest."}, {"title": "2 Related Work", "content": "This section begins with a review of recent AI-driven approaches for DFU image analysis, followed by an examination of prior works that have integrated large language and vision models for the analysis of medical images."}, {"title": "2.1 AI-driven DFU Image Analysis", "content": "Zhang et al. [35] conducted a literature review on deep-learning approaches for the classification, object detection, and semantic segmentation of DFU images. Zhang et al. identified that, for classification tasks in DFU imaging, the most effective models were all based on Convolutional Neural Networks (CNNs). For object detection tasks, the leading models utilized architectures such as Faster R-CNN [9], and EfficientDet [13]. In semantic segmentation tasks, models based on fully convolutional networks (FCNs), U-Net, V-Net, and SegNet were employed, with U-Net achieving the highest accuracy at 94.96% [25]. The most recent methods for DFU image analysis have predominantly employed vision transformers, reflecting a shift towards leveraging advanced transformer-based architectures for improved performance in this domain [30,6]."}, {"title": "2.2 Large Language and Vision Models for Medical Image Analysis", "content": "Given the absence of prior approaches integrating LLMs and vision models for DFU image analysis, this subsection reviews previous applications in analyzing medical images in other domains. Hu et al. [14] conducted a literature review on the application of LLMs in medical images, covering applications such as image captioning, report generation, and visual question-answering across various domains such as MRI, CT, ultrasound, and chest X-rays. The following discussion focuses on the latest methods involving LLMs and vision models, particularly in chest X-ray imaging, as representative examples of medical imaging applications.\nWiehe et al. [32] explored the adaptation of CLIP-based models [23] for classifying chest X-ray images. Since the features learned by pre-trained CLIP models on general internet data do not directly transfer to the chest X-ray domain, the authors adapted CLIP to chest radiography using contrastive language supervision. This adaptation resulted in a model that outperformed supervised learning"}, {"title": "3 Method", "content": "The study utilizes a dataset from the DFU2022 competition [33], which provides a comprehensive collection of 2,000 annotated clinical RGB images specifically related to DFUs. The dataset was initially developed for research in detecting and classifying DFUs. The images mainly include the plantar aspect of the foot with one or more ulcerations and background drapes, minimalizing the presence of other elements in the images. These images are selected to represent a variety of DFU cases, including different stages of ulceration, anatomical locations on the foot, and associated skin conditions, ensuring a robust test set for evaluating the LLMs. Additionally, this dataset is only accessible for research and not hosted on public domains, so the available LLMs have not been previously trained on this dataset.\nThe models evaluated in this study include GPT-4omni (GPT-4o) [22], Qwen-VL [2], LLaVA integrated with Nous-Hermes [26], including 34B-parameter, LLaVA combined with Mistral [15] with a 7B-parameter LLM backbone, and LLaVA paired with Vicuna [7] with a 7B-parameter LLM backbone. For GPT4o, the gpt-4o-2024-08-06 snapshot was used. In the LLaVA setup, as shown in Figure 1, the CLIP tokenizer backbone was kept constant while the language model part of the architecture varied to investigate the language model influences independent from CLIP and other components of a vision-language architecture.\nThese models were chosen for their state-of-the-art performance in language processing, with designs to handle both visual and textual data, making them particularly relevant for generating accurate and clinically relevant descriptions of DFU images. Additionally, the models were selected to include commercial and open-source and different parameter sizes to comprehensively evaluate their applications in DFU. All models except GPT-4o are open-sourced and deployed on a Nvidia 32GB machine by loading the relevant weights from the official GitHub or Hugging Face repositories. For GPT-4o, the OpenAI website platform was used.\nEach model was tasked with generating a brief, clinically-focused description of the DFU images. The following prompt was used:\n\"In about 20 words, describe this image to a medical doctor. The doctor may use the description to complete the EMR.\"\nThe prompt used was standardized to ensure consistency across models, asking each to describe the image in a manner that a medical doctor might use to complete an electronic medical record (EMR). The prompt specified a 20-word limit to ensure the description included terms or synonyms for \u201cDiabetic,\u201d \u201cFoot,\u201d \u201cUlcer,\u201d \u201cPlantar,\u201d \u201cAmputation,\u201d and \u201cCalluses.\u201d Each DFU and amputation location should be described with 3 to 4 words (e.g., \"one sub-second metatarsal\") along with one preposition or article for each noun, totalling 20 words. The models' performances were evaluated based on several critical metrics: clinical"}, {"title": "4 Experiments", "content": "The five DFU images shown in Figure 2 were selected to follow the criteria described in the methods. Descriptions generated from each of the models for Figure 2 images are shown in Table 1."}, {"title": "5 Conclusion", "content": "The findings from this study highlight the potential utility of LLMs in generating clinically relevant descriptions of DFUs. With its strong performance in accurately capturing clinical features, GPT-4o demonstrated promise as an assistive tool in clinical settings. By providing reliable and detailed descriptions of DFU images, such LLMs can help streamline the documentation process, reduce clinicians' workload, and improve the consistency of patient records in EMR systems, facilitating effective triage systems for early detection and treatment.\nHowever, the performance variability observed among the other models highlights the need for ongoing refinement and specialization of LLM tools in healthcare. While promising, open-source models like Qwen-VL still require significant optimization to match the performance of proprietary models like GPT-4o. The application of LLM in DFU management can also lead to the development of integrated telemedicine systems, where remote monitoring and assessment of DFUs become more efficient and scalable.\nFuture work should focus on validating these findings with larger datasets and refining the evaluation process to ensure model outputs are clinically accurate, comprehensive, and diagnostically useful in various settings. As LLMs evolve, their role in clinical practice will likely grow, offering clinicians powerful tools to enhance patient care, particularly for chronic conditions like DFUs requiring ongoing monitoring."}]}