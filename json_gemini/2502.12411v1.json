{"title": "Gradient Co-occurrence Analysis for Detecting Unsafe Prompts in Large Language Models", "authors": ["Jingyuan Yang", "Bowen Yan", "Rongjun Li", "Ziyu Zhou", "Xin Chen", "Zhiyong Feng", "Wei Peng"], "abstract": "Unsafe prompts pose significant safety risks to large language models (LLMs). Existing methods for detecting unsafe prompts rely on data-driven fine-tuning to train guardrail models, necessitating significant data and computational resources. In contrast, recent few-shot gradient-based methods emerge, requiring only few safe and unsafe reference prompts. A gradient-based approach identifies unsafe prompts by analyzing consistent patterns of the gradients of safety-critical parameters in LLMs. Although effective, its restriction to directional similarity (cosine similarity) introduces \"directional bias\", limiting its capability to identify unsafe prompts. To overcome this limitation, we introduce GradCoo, a novel gradient co-occurrence analysis method that expands the scope of safety-critical parameter identification to include unsigned gradient similarity, thereby reducing the impact of \"directional bias\" and enhancing the accuracy of unsafe prompt detection. Comprehensive experiments on the widely-used benchmark datasets ToxicChat and XStest demonstrate that our proposed method can achieve state-of-the-art (SOTA) performance compared to existing methods. Moreover, we confirm the generalizability of GradCoo in detecting unsafe prompts across a range of LLM base models with various sizes and origins.", "sections": [{"title": "1 Introduction", "content": "The use of unsafe prompts poses significant risks, limiting the widespread adoption of large language models (LLMs). For example, these prompts can be exploited by malicious actors to generate prohibited or restricted content, enabling the development of substances for mass destruction. Current mainstream methods for detecting unsafe prompts fall into two categories: external moderation APIs and data-driven fine-tuning approaches. The former primarily leverages a combination of rules and various machine learning techniques to identify unsafe prompts (Microsoft, 2024; Perspective, 2024; OpenAI, 2024). In contrast, data-driven methods involve collecting large amounts of safety-relevant datasets to train guardrail models for detecting unsafe signals (Inan et al., 2023; Llama Team, 2024; Azaria and Mitchell, 2023). However, these methods are often cost-prohibitive due to need for large and curated datasets and significant computational resources for model training. Recently, Xie et al. (2024) propose GradSafe, a gradient-based method for detecting unsafe prompts in LLMs in a low resource setting based on few prompting examples. This approach identifies unsafe prompts by investigating consistent patterns of the gradients of safety-critical parameters in LLMs. It has been observed that gradients of an LLM's loss for unsafe prompts (when paired with compliance responses) exhibit high directional similarity (cosine similarity). In contrast, gradients corresponding to safe prompts show a lower directional similarity to those of their unsafe counterparts. While this method has demonstrated effectiveness in detecting unsafe prompts, using cosine similarity to benchmark gradients of safety-critical parameters may be error-prone due to the presence of \u201cdirectional bias\". As depicted in Figure 1, GradSafe tends to over-weigh gradients with the same sign (or direction), i.e., prompts A and C under the influence of directional bias, therefore potentially neglecting gradients for prompting pairs with strong unsigned similarity (prompts B and C) and leading to inaccurate grouping.\nIn this paper, we propose GradCoo, a novel method for detecting unsafe prompts based on gradient co-occurrence analysis to accommodate a broader scope of safety-critical parameters with unsigned gradient similarity. Specifically, we first construct the safe and unsafe gradient references from few safe and unsafe prompts respectively. Then, we produce the overall unsafe classification score by aggregating the component-level (e.g.,"}, {"title": "2 Related Work", "content": "2.1 Unsafe Prompts Detection via External APIs or Tools\nTo moderate unsafe prompts, technical vendors have engaged in developing moderation APIs, such as Azure AI Content Safety (Microsoft, 2024), Perspective API (Perspective, 2024), OpenAI Moderation API (OpenAI, 2024), Baidu Text Moderation (BaiduAI, 2024) and Alibaba Content Moderation (AlibabaCloud, 2024). These moderation APIs typically employ a hybrid architecture that integrates rule-based filtering mechanisms for explicit content detection with machine learning models trained on various safety-related datasets. Meanwhile, the scope of application for these moderation APIs differs. For instance, the Perspective API (Perspective, 2024) is primarily focused on analyzing the presence of harmful or offensive language, such as detecting toxicity or insults within text. In contrast, the OpenAI Moderation API (OpenAI, 2024) is designed for content moderation of language model outputs, specifically assessing whether the generated content violates OpenAI's defined usage policies.\nAdditionally, external tools are often employed to detect unsafe prompts in LLMs. For instance, Detoxify (Hanu and Unitary team, 2020) is an open-source toxic comment detection tool that includes three key functionalities: toxic comment classification, detection of unintended bias in toxic comments, and multilingual toxic comment classification. HateBERT (Caselli et al., 2021) is a bert-based model trained as a tool for detecting abusive language, with training data collected from controversial communities on Reddit.\nWhile these APIs and tools provide valuable moderation services, they often require customized engineering efforts to support their functionality.\n2.2 Guardrail Models for Unsafe Prompt Detection\nRecent advancements in detecting unsafe prompts in LLMs have predominantly focused on fine-tuning-based methodologies. The Llama Guard series (Inan et al., 2023; Llama Team, 2024) address this challenge by training language models on extensive annotated datasets to assess whether"}, {"title": "2.3 Gradient-Based Analysis of LLMs", "content": "One primary objective of gradient analysis is to examine how input features contribute to model outputs. For example, Shrikumar et al. (2016) propose a method that evaluates the contribution of each input feature to the model's output by computing the gradient of the output with respect to (wrt) the input features and performing an element-wise multiplication with the input values. This approach is straightforward and computationally efficient. Furthermore, Layer-wise Relevance Propagation (LRP) (Bach et al., 2015) starts from the model's output and propagates \u201crelevance\u201d scores backward layer by layer until reaching the input layer, allowing for fine-grained feature attribution. Moreover, Axiomatic Attribution (Sundararajan et al., 2017) leverages integrated gradients to compute the attribution of input features to the predictive output grounded in two fundamental axioms: sensitivity and implementation invariance, further enhancing the feature attribution performance.\nAdditionally, there are gradient-based analysis methods tailored for specific model architectures, such as GradCAM (Selvaraju et al., 2017), which generates class-specific heatmaps by leveraging the gradients of the target class wrt the final convolutional layer, highlighting the important regions in the input image that contribute to the model's decision. Recently, GradSafe (Xie et al., 2024) propose a gradient based method to assess the safety of a given prompt by measuring the directional similarity (cosine similarity) between the gradients of the given prompt and those of the unsafe reference prompts.\nUnlike GradSafe (Xie et al., 2024) that restricts its analysis to the gradient direction similarity, our proposed method uses the gradient co-occurrence scores to accommodate unsigned similarity, therefore analyzing the overall patterns of the gradients. Our approach mitigates the directional bias, leading to enhanced performance in unsafe prompt detection."}, {"title": "3 Method", "content": "As illustrated in Figure 2, our proposed method comprises two main steps. In the first step, we first extract safe and unsafe parameter gradients by computing the gradients of an LLM's loss for prompts paired with compliance responses such as \"Sure\", followed by the approach of Xie et al. (2024). Next, we slice the parameter gradients at the component level (e.g., attention head, MLP) and then remove both directional and magnitude biases of the sliced gradients to construct safe gradient and unsafe gradient references. In the second step, we compute the gradients co-occurrence scores between the gradients of the given prompt and the safe/unsafe gradient references across all model components. These scores are then aggregated to determine the safety of the given input prompt.\n3.1 Safe and Unsafe Gradient References Construction\nTo calculate the safe and unsafe gradient references, we first need a set of safe and unsafe prompt reference texts. We use the same safe/unsafe prompt reference texts as Xie et al. (2024) to ensure a fair comparison. After feeding in safe/unsafe prompt reference texts, we obtain the generated responses"}, {"title": "3.2 Gradient Co-occurrence Scores Aggregation for Unsafe Prompts Detection", "content": "To determine whether a given prompt is safe or unsafe, we first compute the gradients $g_c \\in G_p$ for the given prompt in the same manner as the safe/unsafe gradient references. Next, we calculate the gradient co-occurrence scores between the $g \\in G_p$ with the safe/unsafe gradient references $g \\in G_s$ and $g_u \\in G_u$ respectively. Subsequently, we compute a relative unsafe score for each model component $c \\in C$.\nAfter that, we aggregate these scores across model components to obtain an overall unsafe classification score. The given prompt with a score above a predefined threshold is classified as unsafe, otherwise it was regarded as safe. The specific procedure is detailed in Algorithm 1."}, {"title": "4 Main Experiments", "content": "4.1 Datasets and Evaluation Metric\nTo ensure comparison fairness, we adopt the same test datasets and evaluation metrics as GradSafe (Xie et al., 2024). Specifically, two datasets are used: the ToxicChat dataset (Lin et al., 2023), which consists of 10,166 toxicity-annotated prompts derived from user interactions, split into training and testing sets, with the official test set ToxicChat-1123 used for evaluation, and XSTest (R\u00f6ttger et al., 2023), a test suite containing 250 safe prompts across 10 types and 200 corresponding crafted unsafe prompts."}, {"title": "5 Ablation Study", "content": "5.1 Effects of Magnitude and Directional Bias Mitigation\nWe evaluate the effectiveness of our proposed method in mitigating directional & magnitude gradient biases by testing the following scenarios: (1) impact of the normalization operation (norm) to mitigate magnitude bias; (2) impact of the absolute value function (abs) to mitigate directional bias, and (3) the effect of eliminating both operations. The experiments are conducted on the XSTest datasets.\nIt can be observed in Table 2 that removing either the normalization operation or the absolute value function operation leads to a significant performance drop.\nSpecifically, on the XStest, removing the normalization operation cause a decrease of 38.4% in AUPRC, while removing the absolute value opera-"}, {"title": "6 Effects of Numbers of Safe/Unsafe Reference Pairs", "content": "We investigate the effect of the number of safe/unsafe reference texts on the performance of our method in the XSTest dataset for unsafe prompt detection. We experiment with 1 to 6 pairs of reference texts and further assess the model's sensitivity to reference text selection by sampling five times from the pool for each pair.\nBy calculating the standard deviation of the results across these five samplings, we quantify the sensitivity. The safe/unsafe reference pool for prompt detection is the same as in Xie et al. (2024).\nAs shown in Figure 3, the performance is slightly improved as the number of safe/unsafe ref-erence texts increases, but the overall trend is stable. This indicates that a small number of safe/unsafe"}, {"title": "7 Effects of Our Method on Different LLM Base Models", "content": "We evaluate the effects of our method on different base LLM models using the XSTest dataset, which includes the Llama-2-7B-Chat (Inan et al., 2023), Qwen-2.5-7B-Instruct (Yang et al., 2024), and Gemma-7B-It models (Team et al., 2024). We assess the task performance improvement of our method by using the base models as baselines, serving as guardrail models.\nFollowing Xie et al. (2024), we use the same prompt format in our baseline models for evaluation: \"You are a helpful assistant. Provide your safety assessment for this user prompt: {{input prompt}} You must only answer with 'safe' or 'unsafe'.\". The evaluation metric employed is the F1 Score. We do not use the AUPRC metric because the involved prompt engineering-based method outputs only binary labels (\u2018safe' or 'unsafe'), without providing a specific score for threshold selection.\nAs illustrated in Figure 4, the baseline models employing LLMs as guardrails exhibit moderate performance in detecting unsafe prompts, achiev-"}, {"title": "8 Effects of Our Method on Different Sizes of LLMS", "content": "To validate whether our proposed method is effective across different sizes of LLMs, we conduct experiments on various model sizes of the Qwen-2.5-Instruct series (Yang et al., 2024), including 0.5B, 1.5B, 3B, and 7B for the prompt safety detection task.\nAdditionally, to quantify the improvement our method brings to the task, we use the LLMs them-"}, {"title": "9 Effects of Different Compliance Responses", "content": "To evaluate the influence of varying compliance responses on the efficacy of our proposed methodology, we conducted an examination of its performance on the unsafe prompt detection task. This evaluation utilized the XSTest dataset.\nInitially, a seed compliance response 'Sure' was provided. Subsequently, leveraging the capabilities of GPT-4\u00b9, we employed instructional prompts to generate ten additional compliance responses. From this set, we randomly selected six responses for the purpose of our comparative assessment."}, {"title": "10 Conclusion", "content": "In this paper, we propose a novel unsafe prompt detection method GradCoo based on gradient co-occurrence analysis, which requires only a small set of safe and unsafe reference prompts. By mitigating gradient direction bias, GradCoo accommodates unsigned similarity, therefore analyzing the overall patterns of the gradients more accurately, leading to significant performance enhancement in unsafe prompt detection. Comprehensive experiments on the widely-used benchmark datasets ToxicChat and XSTest demonstrate that our method can achieve state-of-the-art (SOTA) performance in unsafe prompt detection task compared to existing methods. Moreover, we confirm the generalizability of our method in detecting unsafe prompts across of a diverse range of LLMs baseline with various sizes and origins in comprehensive experiments.\nBuilding upon the promising results achieved by GradCoo, future work will explore several avenues. Initially, we will focus on expanding the scope of GradCoo to encompass more complex forms of unsafe content. This includes investigating multi-modal prompts, such as combinations of images and text, as well as prompts designed to elicit harmful model behaviors beyond simple text generation, such as code execution or agent actions.\nFurthermore, a crucial direction for future work involves developing a deeper theoretical understanding of the correlation between gradient co-occurrence patterns and the characteristics of unsafe prompts. This will entail analyzing the mathematical properties of the gradient space and investigating how various types of unsafe content are manifested within it. Concurrently, we aim to develop explainability techniques for GradCoo, providing insights into the rationale behind a prompt being flagged as unsafe. This explainability will be valuable for both model developers and end-users, potentially informing the development of more robust defense mechanisms against unsafe prompts."}, {"title": "Limitations", "content": "While our method demonstrates significant performance improvements in both unsafe prompt detection, several limitations remain. First, our method relies on gradient computation, which introduces additional computational overhead compared to a single forward pass in guardrail models, particularly for extremely large models. Second, although we have conducted experiments with other LLMs, the effects of our method on multimodal models or broader tasks require further investigations, as the unsafe gradient patterns may not exhibit similar characteristics in such settings. Third, while our empirical results demonstrate the effectiveness of GradCoo, a deeper theoretical understanding of the correlation between these safe and unsafe gradient patterns is needed."}, {"title": "Ethical Impact", "content": "The aim of this paper is to explore novel methods to safeguard large language models against potential harms arising from unsafe prompts. We employ existing unsafe prompt detection benchmark datasets in the experiment, thereby mitigating the introduction of new safety risks. We believe that our approach promotes advancements in unsafe prompt detection methods, thereby benefiting the AI safety community."}]}