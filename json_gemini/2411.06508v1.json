{"title": "Understanding the Role of Equivariance in Self-supervised Learning", "authors": ["Yifei Wang", "Kaiwen Hu", "Sharut Gupta", "Ziyu Ye", "Yisen Wang", "Stefanie Jegelka"], "abstract": "Contrastive learning has been a leading paradigm for self-supervised learning, but it is widely observed that it comes at the price of sacrificing useful features (e.g., colors) by being invariant to data augmentations. Given this limitation, there has been a surge of interest in equivariant self-supervised learning (E-SSL) that learns features to be augmentation-aware. However, even for the simplest rotation prediction method, there is a lack of rigorous understanding of why, when, and how E-SSL learns useful features for downstream tasks. To bridge this gap between practice and theory, we establish an information-theoretic perspective to understand the generalization ability of E-SSL. In particular, we identify a critical explaining-away effect in E-SSL that creates a synergy between the equivariant and classification tasks. This synergy effect encourages models to extract class-relevant features to improve its equivariant prediction, which, in turn, benefits downstream tasks requiring semantic features. Based on this perspective, we theoretically analyze the influence of data transformations and reveal several principles for practical designs of E-SSL. Our theory not only aligns well with existing E-SSL methods but also sheds light on new directions by exploring the benefits of model equivariance. We believe that a theoretically grounded understanding on the role of equivariance would inspire more principled and advanced designs in this field.\nCode is available at https://github.com/kaotty/Understanding-ESSL.", "sections": [{"title": "1 Introduction", "content": "Self-supervised learning (SSL) of data representations has made remarkable progress. Existing SSL methods can be categorized into two types: invariant SSL (I-SSL) and equivariant SSL (E- SSL). The idea of I-SSL is to encourage the representation to be invariant to input augmentations (e.g., color jittering). Contrastive learning that pulls positive samples closer and pushes negative samples apart is widely believed to be a prominent I-SSL paradigm, leading to rapid progress in recent years [13, 50, 5, 80, 46, 30, 3, 32, 33, 60, 49, 14, 37, 15, 77, 64, 81]. Nevertheless, since invariant representations lose augmentation-related information (e.g., color information), their performance on downstream tasks can be hindered, as frequently observed in practice [47, 17, 34]."}, {"title": "2 Related Work", "content": "Invariant and Equivariant SSL. Without access to labels, SSL methods design various surrogate tasks that create self-supervision for representation learning. Early SSL methods, often in the form of predictive learning, learn from predicting the transformation of randomly transformed images, such as, RotNet [31], Jigsaw [51], Relative Patch Location [20]. Later, discriminating instances in the latent space with contrastive learning demonstrates prominent performance [21, 72, 52, 40, 13, 38, 56], with variants including non-contrastive methods [33], clustering methods [10\u201312], regularization methods [77]. However, data augmentations used in contrastive learning to avoid shortcuts often come at the cost of information lost for downstream tasks (e.g., color for flower classification). To address this issue, there is a surge of interest in E-SSL that learns features to be sensitive to the applied transformations. Among them, Xiao et al. [73] use separate embeddings for each augmentation. Wang et al. [67] apply equivariant prediction on residual vectors between positive views. Dangovski et al. [17] combine contrastive learning and rotation prediction. Devillers and Lefort [18], Garrido et al. [24] utilize conditional predictors with augmentation parameters. Park et al. [54], Gupta et al. [34] model latent equivariant transformations explicitly.\nTheory of SSL. Most existing theories of SSL methods focus on contrastive learning (CL) and its variants from different perspectives: information maximization [52, 40, 65], downstream gener- alization [58, 66, 48, 35, 68, 59, 78], feature dynamics [66, 69], asymmetric designs [63, 82], feature identifiability [79], etc. But for general E-SSL methods, there is little, if any, theoretical understanding"}, {"title": "3 The Challenges of Understanding Equivariant SSL", "content": "Notations. We introduce existing SSL methods from a probabilistic perspective. Generally, we denote a random variable by a capital letter such as X, its sample space as X, and its outcome as x. We learn a representation (Z/Z/zx) from the input (X/X/x) through a deterministic encoder function F: X \u2192 Z. The general goal of SSL is to learn discriminative representations that are predictive of the image classes (labels) without actual access to label information. For ease of discussion, we mainly adopt the common Shannon information, where the entropy of X is H(X) = \u2212E_{P(X)} log P(X) and the mutual information between X and Y is I(X; Y) = H(X) \u2013 H(X|Y). It is also tempting to adopt V-information [75] that is analogous to Shannon's notion but aligns better with practice by taking into account computational constraints. For ease of understanding, we adopt Shannon's information in the main paper and extend the main results to V-information in Appendix C.\nEquivariant SSL (E-SSL). For each raw input X sampled from the training set D, we independently draw a random augmentation A and get the augmented sample X = T(X, A) with a transformation mapping T: X \u00d7 A \u2192 X. The general objective of Equivariant SSL (E-SSL) is to learn represen- tations Z = F(X) that are sensitive to the applied transformation A. For example, RotNet [31] utilizes random four-fold rotation A = {0\u00b0,90\u00b0, 180\u00b0, 270\u00b0} for data augmentation, and learns feature equivariance by predicting the rotation angles from the representation Z. Therefore, E-SSL is driven by maximizing the following mutual information between the augmentation A and the representation Z:\nmax I(A; Z).                                                       (1)\nNote again that equivariance studied in this paper, as in many E-SSL works [17, 18, 24, 34], is a relaxed notion of exact equivariance defined in a group-theoretical sense (with invertibility and com- positionality) as in equivariant networks [16]. In E-SSL, equivariance generally means augmentation sensitivity, and the mutual information I(A; Z) measures the degree of equivariance of Z to A.\nContrastive Learning as E-SSL. Contrary to E-SSL, I-SSL enforces features to be invariant to the applied augmentation A. CL is widely believed to be an example of invariant learning [17]. In CL, we apply two random data augmentations, A1, A2 to the same input X and get two positive samples X1, X2 and their representations Z1, Z2 respectively. Since CL is driven by pulling Z1, Z2 together, their mutual information objective is often formalized as max_{Z\u2081=F(X,A1),Z2=F(X,Z2)} I(Z1; Z2) [1]. However, it is easy to observe that the constant outputs Z = const are also optimal with maximal I(Z1; Z2), suggesting that invariance alone is sufficient for SSL. In fact, contrastive learning can mitigate feature collapse with the help of pushing away from the representation of the other instances (i.e., negative samples), making it essentially an equivariant learning task w.r.t. the instance, known as instance discrimination [21, 72]. Indeed, contrastive objectives are essentially non- parametric formulations of instance classification [72], and under similar designs, parametric instance classification achieves similar performance [9]. Non-contrastive variants with only positive samples are also shown to have inherent connection to contrastive methods in recent studies [63, 82, 23].\nEquivariance is Not All You Need. A common intuition among E-SSL methods is that better downstream performance comes from better feature equivariance [18, 25, 54, 34]. Here, we begin our discussion by showing a counterexample in the following proposition. All proofs in this paper can be found in Appendix A."}, {"title": "4 A Theory of Equivariant SSL", "content": "In Section 3, we have shown that feature equivariance alone does not guarantee effective downstream performance, which makes it even unclear how equivariant learning extracts useful features. To resolve these puzzles, we provide an information-theoretic analysis for E-SSL that serves as a natural explanation for the phenomena above.\n4.1 Explaining E-SSL via Explaining-away\nWe start by establishing a causal diagram of the data generation process of E-SSL, where we assume that the original input X is generated from its class variable C (relevant to input semantics, e.g., shape), intrinsic equivariance variable A (relevant to semantics, e.g., the intrinsic orientation of an object) and style variable S (features irrelevant to semantics and targeted equivariance, e.g., color"}, {"title": "Lemma 1 (Explaining-away in E-SSL).", "content": "If the data generation process obeys the diagram in Figure 2, then almost surely, A and C are not independent given X or Z, i.e., A || C|X and A \u22a5 C|Z. It implies that I(A; C|X) > 0 and I(A; C|Z) > 0 hold almost surely."}, {"title": "Theorem 1 (Class features improve equivariant prediction).", "content": "Under the data generation process in Figure 2, consider an E-SSL task with input X, its class C'x, and its representation Z. Assume a class representation Zc = \u25ca(Cx) that can perfectly predict the label Cx (\u25ca is an invertible mapping). Then, almost surely, the combined feature Z = [Z, Zc] obtained by appending Zc to Z will strictly improve the equivariant prediction with larger mutual information I(A; \u017d) > I(A; Z). Also, we have I(C; \u017d) \u2265 I(C; Z), so the classification performance improves in the meantime."}, {"title": "4.2 Maximizing the Synergy Effect: Principles for Practical Designs of E-SSL", "content": "Our theoretical understanding above not only establishes theoretical explanations for downstream performance, but also provides principled guidelines for E-SSL design. The overall principle is to maximize the synergy I(A; C|X) = H(A|X) \u2013 H(A|X, C), which can be understood from the following aspects that explain various E-SSL behaviors that we observe in Section 3.\nPrinciple I: \"Lossy\u201d Transformations. First, let us look at H(A|X), which determines the upper bound of the explaining-away effect. A higher H(A|X) means that the equivariant prediction task"}, {"title": "4.3 Analysis on the Influence of Transformation", "content": "The theory in Section 4.1 guarantees that E-SSL will learn class features almost surely under general conditions. Yet, without further knowledge, it is generally hard to derive more quantitative results for downstream performance. For a concrete discussion, we consider a simplified data generation process as an exemplar. Note that simplified data models are frequently adopted in the literature of self-supervised learning theory [63, 71] to gain insights for their real-world behaviors."}, {"title": "5 Understanding Advanced E-SSL Designs", "content": "In Section 4, we have established a theoretical understanding of basic E-SSL through the explaining- away effect. However, basic E-SSL (like rotation prediction) often fails to achieve satisfactory performance, and many advanced designs have been proposed to enhance E-SSL performance [67, 17, 18, 25, 54, 34]. In this section, we further explain how these advanced designs improve performance by enhancing the synergy effect between class information and equivariant prediction.\n5.1 Fine-grained Equivariance\nA conclusion from Theorem 2 is that a larger action space of the transformation A benefits the explaining-away effect by increasing the task difficulty H(A|X). Guided by this principle, one way to improve E-SSL is through learning from more fine-grained equivariance variables with a larger action space (|A|), which encourages models to learn diverse features and avoid feature collapse for specific augmentations. For example, four-fold rotation is a 4-way classification task while CIFAR- 100 has 100 classes. When the neural networks are expressive enough such that it clusters samples with the same augmentation to (almost) the same representation (known as neural collapse [53]), the class features also degrade or vanish, which hinders downstream classification. For example, Table 1 shows that for rotation prediction, stronger augmentations suffer from less feature collapse (lower training accuracy), while enjoying better classification accuracy. Indeed, we show that the advantages of state-of-art SSL methods can be understood through this information-theoretic perspective.\nInformation-theoretic Understanding of Instance Discrimination. As disclosed in Section 3, contrastive learning is essentially an E-SSL task with equivariance prediction of instances. Specifi- cally, each raw example Zi serves as an instance-wise class, forming an action space I, where all augmented samples of Zi belong to the class i. Therefore, the instance classification task has an action space of T = N, where N is the number of training dataset that is much larger than rotation prediction with |A| = 4, making instance discrimination a harder task, especially under strong data augmentations [72, 21]. Since the instance index I is also independent of the class variable C, it is not"}, {"title": "5.2 Multivariate Equivariance", "content": "As discussed in Section 4.2, equivariant prediction may have class-irrelevant features as shortcuts, while corrupting these features (e.g., color) with data augmentation might affect certain downstream tasks (e.g., flower classification that requires color information too). A more principled way that has been explored recently is through joint prediction of multiple equivariance variables [67, 17, 18, 24, 54, 34], which we refer to as multivariate equivariance. In the following theorem, we show that multivariate equivariance is provably beneficial since it monotonically increases the synergy effect between class information and equivariant prediction, as shown in the following theorem.\nTheorem 3. For two transformation variables A1, A2, we will always have I(A1, A2; C|Z) \u2265 max{I(A1; C|Z), I(A2; C|Z)}. In other words, multivariate equivariance brings strengthens the explaining-away effect, with a gain of g = max{I(A2; C|Z, A1), I(A1; C|Z, A2)}.\nTheorem 3 can also be easily extended to more equivariant variables. Note that the gains of mul- tivariate equivariance I(A2; C|Z, A1) reflects the amount of additional information that the class information C can explain away A2 under the same value of A\u2081; therefore, more diverse augmen- tations provide a large gain in the synergy effect. Recent works on image world model show that equivariance to multiple transformation delivers better downstream performance and outperforms invariant learning [26]."}, {"title": "5.3 Model Equivariance", "content": "Apart from the design of transformations that is the main focus of E-SSL methods, an often overlooked part is the equivariance of the backbone models, which we call model equivariance. Intriguingly, we find that equivariant networks can be very helpful for E-SSL when the transformation equivariance aligns well with model equivariance.\nSetup. We compare a standard non-equivariant ResNet18 [36] and an equivariant ResNet18 (EqRes- Net18) w.r.t. the p4 group (consisting of all compositions of translations and 90-degree rotations) [16] of similar parameter sizes. The models are pretrained on CIFAR-10 and CIFAR-100 for 200 epochs"}, {"title": "Theorem 4.", "content": "For any representation Z, its mutual information with the equivariant learning target A lower bounds its mutual information with the downstream task C as follows:\nI(Z; A) \u2264 I(Z; C) \u2013 I(X; A|C).                                               (4)\nHere, a small gap I(X; A|C) means a better generalization between these two tasks. Because I(X; A|C) = H(A|X, C') is a lower bound of I(A; C|X) that indicates class relevance, it further justifies our Principle II (Section 4.2) that better class relevance brings better E-SSL performance."}, {"title": "5.4 Strict Equivariant Objectives", "content": "Mathematically, an exact definition of equivari- ance requires that for each transformation a in the input space, there is a corresponding trans- formation Ta in the representation space so that f(a(x)) \u2248 Taf(x). Common rotation predic- tion objectives do not satisfy this property. Other works also study the use of exact equivariant objectives. Here, we take CARE [34] as an ex- ample and compare it against rotation prediction with cross-entropy (CE) loss."}, {"title": "6 Conclusion", "content": "In this paper, we have provided a general theoretical understanding of how learning from seemingly irrelevant equivariance (such as, random rotations, masks and instance indices) can benefit down- stream generalization in self-supervised learning. Leveraging the causal structure of data generation, we have discovered the explaining-away effect in equivariant learning. Based on this finding, we have established theoretical guarantees on how E-SSL extracts class-relevant features from an information- theoretic perspective. We also identify several key factors that influence the explaining-away. Since this work is theory-oriented to fill the gap between practice and theory by investigating how E-SSL works, we do not explore extensively for a better E-SSL design. Nevertheless, the fruitful insights developed in this work could inspire more principled designs of E-SSL methods in future research."}, {"title": "A Omitted Proofs", "content": "A.1 Proof of Proposition 1\nProof. It is easy to see that the linear encoder that takes the last d' dimension of the input does not rely on any class information while giving a perfect prediction of A, i.e., f(X) = X[d+1:d+d'] = A. Therefore, it gives random guess prediction on downstream classification.\nA.2 Proof of Lemma 1\nProof. We begin by restating an important result in probabilistic graphical models (PGMs) for conditional independence.\nLemma 2 (Theorem 3.5 (rephrased) [44]). For almost all distributions P that factorize over the causal diagram G, that is, for all distributions except for a set of measure zero in the space of CPD (conditional probability distributions) parameterizations, we have that I(P) = I(G), where I(G) denotes the set of independencies that correspond to d-separation:\nI(G) = {(X || Y | Z) : d-sep_{G}(X; Y | Z)} .\nLemma 2 shows that almost all distributions that factorize over the causal diagram G obey the d-separation rules. According to d-separation [27, 44], the collider structure in Figure 2 implies that A \u22a5 C|X and A \u22a5 C|Z. Further, recall that for any three random variables A, B, C, the mutual information I(A; B|C) \u2265 0, and I(A; B|C) = 0 iff they are conditionally independent, i.e., A \u22a5 B|C. Combined the facts above, we will have I(A; C|X) > 0 and I(A; C|Z) > 0 almost surely.\nA.3 Proof of Theorem 1\nProof. Leveraging Lemma 1, we know that I(A; C|Z) holds almost surely, which implies that\nH(A|Z) > H(A|Z, C).                                                     (5)\nSince Zc is a reparameterization of C, we have H(A|Z, C) = H(A|Z, Zc) = H(A|\u017d). Subtracting H(A) on both sides of Eq. (5) gives\nH(A|Z) \u2013 H(A) < H(A|\u017d) \u2013 H(A),\nwhich is equivalent to I(A; \u017d) > I(A; Z). Besides, we also have I(C; \u017d) = I(C; Z, Zc) \u2265 I(C; Z), which completes the proof.\nA.4 Proof of Theorem 2\nProof. First, let us consider the first proposition of Theorem 2. We have\nI(A; C | X) = I(A; C, X) \u2013 I(A; X)\n= I(A; C) + I(A; X | C) \u2013 I(A; X)\n= I(A; A + AC | C) \u2013 I(A; A + AC')\n= I(A; A) \u2013 I(A; A + AC)\n= H(A) \u2013 H(A + AC) + H(A + AC | A)\n= H(A) \u2013 H(A + AC) + H(AC).\nSince H(XC) = H(C) and H(A) and H(C) are determined by Na and Nc, the goal is then transformed into minimizing H(A + AC'). In order to address this problem, we first draw a table below to enumerate the possible outcome of A + AC. To determine H(A + AC), we can divide the elements in the table into groups, where they are distributed to the same group if and only if they have the same value. Let us assume that NA \u2264 Nc because the opposite condition can be solved in a similar way.\nWe now pay attention to the top row and the rightmost column, where the elements are bound to be in different groups. Under closer observation, we find that the element 0 forms a group alone, as does the element NA \u2212 1 + (Nc \u2212 1)\u5165. The element 1 is in a group made up of at most two elements, as are the elements X, (Na \u2212 1) + (Nc \u2212 2)\u03bb, (Na \u2212 2) + (Nc \u2212 1)\u5165. Based on similar analysis, the following relationship can be deduced:\nX1 \u2265 2, x1 + 2x2 \u2265 6, ..., X1 + 2x2 + ... + (Na \u2212 1)XNA\u22121 \u2265 NA(Na \u2212 1).\nWe also have\nNA\nH(A + AC) = \u2211 txtYt\nt=1\nNA-1\n= (NANC - \u2211 txt)YNA + \u2211 txtyt\nt=1\nt=1\nNA-1\n= NANCYNA + \u2211 txt(Yt - YNA)\nt=1\nNA-1\n= NANCYNA + (YNA-1 - YNA) \u2211 txt+\nt=1\nNA-2\n(YNA-2 - YNA-1) \u03a3\u2211 txt + ... + (y2 - \u0423\u0437)(x1 + 2x2) + (\u04231 - Y2)x1\nt=1\n> NANCYNA + NA(NA \u2212 1)(YNA-1 - YNA)+\n(NA - 1)(NA - 2)(YNA\u22122 - YNA\u22121) + ... + 6(y2 - \u0423\u0437) + 2(\u04231 \u2014 \u04232)\nNA-1\n= Na(Nc \u2013 Na + 1)YNA + 2 \u2211 tyt\nt=1\nThe equality condition for this inequality is\nX1 = X2 = ... = XNA-1 = 2.\nThis indicates that every secondary diagonal (from upper right to lower left) in the aforementioned table forms a group of elements, which means \u5165 = 1. This completes the proof of the first claim.\nLet us further look at the second proposition. From the discussion above, we know that X = 1 is the optimal value, and we want to maximize I(A, C|X) = H(A) + H(C) \u2013 H(A + C). Let us assume\nthat NA \u2265 Nc. In order to calculate in detail, we first list the probability distribution of A + C.\nNow, we have\nH(A + C) =...\nThus,..\nA.5 Proof of Theorem 4\nProof. The lower bound can be easily derived by taking the difference between the two quantities:\nI(Z; C) \u2013 I(Z; A) \u2265I(Z; C; A) \u2013 I(Z; A)\n= \u2212 I(Z; A|C)\n\u2265 \u2212 I(X; A|C'),\nwhere the last line comes from the information processing inequality."}, {"title": "B Experiment Details", "content": "In this section, we detail the setting of each individual experiment in this work. All experiments are conducted with a single NVIDIA RTX 3090 GPU."}, {"title": "B.1 Experiment Details of Different Equivariant Pretraining Tasks", "content": "In this experiment, we conduct equivariant pretraining tasks based on seven different types of transformations. In order to maintain fairness and avoid cross-interactions, we only apply random crops to the raw images before we move on to these tasks. We adopt ResNet-18 as the backbone with a two-layer MLP that has a hidden dimension of 2048 and an output dimension corresponding to the pretraining tasks. Under each transformation, we train the model for 200 epochs on CIFAR-10, with batch size 512 and weight decay 10-6. The detailed pretraining tasks are listed as follows.\nHorizontal Flip & Vertical Flip & Color Inversion & Grayscale. We randomly (i.e., with probability 0.5) apply the specific transformation to images and require the model to predict whether or not we have really done the transformation. In these cases, the output dimension is 2.\nFour-fold Rotation. We rotate the images with equal probability (i.e., with probability 0.25) by 0\u00b0, 90\u00b0, 180\u00b0, and 270\u00b0 and require the model to predict which rotation angle we have actually adopted. In this case, the output dimension is 4.\nFour-fold Blur. We apply Gaussian blurs to the images using kernel sizes of 0, 5, 9, and 15, where kernel size 0 refers to not applying Gaussian blurs. We then require the model to predict the kernel size. In this case, the output dimension is 4.\nJigsaw. We divide the images into 2 \u00d7 2 patches, randomly shuffle their order, and then require the model to predict the original arrangement. In this case, the output dimension is 24, since there are 4! = 24 possible permutations for the shuffled arrangements.\nDuring the pretraining tasks, we simultaneously train a classifier, which is a single-layer linear head and is trained without affecting the rest of the network. Apart from the seven tasks, we also conduct a baseline experiment, where we fix a random encoder and optimize the classifier alone in order to assess the effectiveness of these pretraining tasks."}, {"title": "B.2 Experiment Details of How Class Information Affects Equivariant Pretraining Tasks", "content": "In this experiment, our goal is to figure out how class information affects rotation prediction. We apply random crops with size 32 and horizontal flips with probability 0.5 to the raw images.\nTraining objectives. As for the experiment process, we first use rotation prediction as the pretraining task with a cross-entropy loss between our predicted angles and the actual angles, defined as\nL_{rot} = - \\sum_{i=1}^{N} \\sum_{j=1}^{4} P_{ij} log(\\hat{P}_{ij}),                                               (18)\nwhere N is the image number, the one-hot vector pij refers to the true rotation angle of the ith image, and pij refers to the prediction of the model. In the case where class information is incorporated, we simply add to the original loss function the cross-entropy between the classes predicted by the classifier and their corresponding ground truth labels, defined as\nL_{cls} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{C} Y_{ij} log(\\hat{Y}_{ij}),                                               (19)\nwhere N is the image number, C is the class number, the one-hot vector Yij refers to the true class of the ith image, and \u0177ij refers to the prediction of the model. In other words, when class information is injected, the loss function is Lrot + 11Lcls, where the mixing coefficient \u5165\u2081 is a hyper-parameter. Furthermore, to eliminate class information from the first setting, we minimize the classifier loss with Lels, trying to probe class information in the representation; in the meantime, we optimize the encoder to maximize the classification loss, aiming to eliminate any class information that can be found by the classifier. In particular, we adopt a joint training objective for the encoder as Lrot \u2013 12Lcls, where the mixing coefficient A2 is also a hyper-parameter. This leads to a min-max optimization between the encoder and the linear classifier. We choose \u5165\u2081 = 0.5 and 12 = 9, under which the class features can be shown to benefit or harm rotation prediction.\nIn the pretraining process, we mainly use Resnet-18 as the backbone with a two-layer MLP that has a hidden dimension of 2048 and an output dimension of 4, and a single-layer linear head as"}, {"title": "B.3 Experiment Details in the study of model equivariance", "content": "In order to compare the performance of Resnet and EqResnet, we use rotation prediction as our pretraining task and obtain the linear probing results. We apply various augmentations to the raw images, such as no augmentation, a combination of random crop with size 32 and horizontal flip, and SimCLR augmentation with output size 32. To be more specific, SimCLR augmentation refers to a sequence of transformations, including random resized crop with size 32, horizontal flip with probability 0.5, color jitter with probability 0.8, and finally grayscale with probability 0.2.\nIn these experiments, we predict rotation angles with a two-layer MLP that has a hidden dimension of 2048 and an output dimension of 4, and a single-layer linear head as a classifier. For each setting, we train the model for 200 epochs on CIFAR-10 and CIFAR-100 with batch size 128 and weight decay 5 \u00d7 10-4. The results on CIFAR-100 are displayed in Table 4."}, {"title": "CV-information: Background and Extensions", "content": "In this section, we introduce V-information [75], which is a computation-aware and model-aware extension of Shannon's notation that is more suitable for modeling neural representation learning. Then, we extend our theory and show that the main results still hold under V-information."}, {"title": "C.1 Definitions and Properties of V-information", "content": "V-information is proposed by Xu et al. [75] under the consideration of computational constraints, which happens to be one of the drawbacks of traditional Shannon information theory. An additional merit of V-information is that it can be estimated from high-dimensional data. The formal definition of V-information is derived as follows. Denote Y as the target random variable that the model is trying to predict and X as another random variable that provides side information for the prediction of Y. Let X and Y be the sample spaces of X and Y. Define \u03a9 := { f : X U \u00d8 \u2192 P(Y)} as a set of the functions that maps X to a family of probability distributions over Y.\nDefinition 1 (Predictive Family). V \u2286 \u03a9 is called a predictive family if \u2200 f \u2208 V, \u2200P \u2208 range(f), \u2203f' \u2208 V that satisfies \u2200x \u2208 X, f'[x] = P, f'[0] = P.\nIn other words, a predictive family is a set of probability measures that are allowed to be used under computational constraints. The existence of f' indicates that the agent can optionally ignore the side information.\nNext, we introduce predictive conditional V-entropy and predictive V-information.\nDefinition 2 (Predictive Conditional V-entropy). Hv(Y|X) = inf_{f\u2208V}E_{x,y~P_{x,y}}[-log f[x](y)]. Specifically, Hv(Y|\u00d8) = inf_{f\u2208V}E_{y~P_{y}}[- log f[0](y)].\nDefinition 3 (Predictive V-information). Iv(X \u2192 Y) = Hv(Y|0) \u2013 Hv(Y|X).\nApart from the definitions, we have to highlight an important property of predictive V-information.\nLemma 3 (Xu et al. [75]). Iv(A \u2192 B) = 0 iff A and B are independent variables."}, {"title": "C.2 Extension to V-information", "content": "First, we present the V-information version of Lemma 1.\nTheorem 5 (Explaining-away in E-SSL). If the data generation process obeys the diagram in Figure 2, then almost surely, A and C is no dependent given X or Z, i.e., A || C|X and A || C|Z. It implies that Iv (C \u2192 A|X) > 0 and Iv(C \u2192 A|Z) > 0 hold almost surely.\nProof. Lemma 3 indicates that for any three random variables A, B, C, the inequality Iv (\u0391 \u2192 BC) \u2265 0 always holds and that Iv(A \u2192 B|C) = 0 iff A \u22a5 B|C. Based on the analysis of the collider structure in Appendix A.2, we know that A and C are not independent given either X or Z. Thus, we have Iv(C \u2192 A|X) > 0 and Iv(C \u2192 A|Z) > 0 almost surely.\nThen, we present the V-information version of Theorem 1.\nTheorem 6. Assume that the representation Z consists of two parts Z = [Z1, Zc], where Z\u2081 is class- irrelevant, and Zc = $(C) is a representation of the class C with an invertible mapping $. If there is a positive synegy effect Iv(C \u2192 A|Z\u2081) > 0, we will have Iv(Z1 \u2192 A) < I\u2174(Z \u2192 A), showing that with class features Zc we can attain strictly better equivariant prediction. As a consequence, the optimal features of equivairant learning will contain class features.\nProof. We have the assumption Iv(C \u2192 A|Z\u2081) = \u0397\u03bd(\u0391|Z1) \u2212 \u0397\u03bd(A|C, Z1) > 0. Given that the function & is invertible and Zc = $(C), we have Hy(A|C, Z1) = Hv(A|Zc, Z1) = Hv(A|Z) < Hy(AZ1). Subtracting Hy (A) from both sides and rewriting the inequality, we finally obtain \u0399\u03bd (\u0396 \u2192 \u0391) > Iv(Z1 \u2192 A)."}, {"title": "D Detailed Elaboration on Computing I(A; C'|X)", "content": "The computation of I(A; C|X) requires knowledge of both the pretraining label A and"}]}