{"title": "CAN A LARGE LANGUAGE MODEL BE A GASLIGHTER?", "authors": ["Wei Li", "Luyao Zhu", "Yang Song", "Ruixi Lin", "Rui Mao", "Yang You"], "abstract": "[Warning: Some examples in this paper could contain objectionable contents.] Large language models (LLMs) have gained human trust due to their capabilities and helpfulness. However, this in turn may allow LLMs to affect users' mindsets by manipulating language. It is termed as gaslighting, a psychological effect. In this work, we aim to investigate the vulnerability of LLMs under prompt-based and fine-tuning-based gaslighting attacks. Therefore, we propose a two-stage framework DeepCoG designed to: 1) elicit gaslighting plans from LLMs with the proposed DeepGaslighting prompting template, and 2) acquire gaslighting conver-sations from LLMs through our Chain-of-Gaslighting method. The gaslighting conversation dataset along with a corresponding safe dataset is applied to fine-tuning-based attacks on open-source LLMs and anti-gaslighting safety alignment on these LLMs. Experiments demonstrate that both prompt-based and fine-tuning-based attacks transform three open-source LLMs into gaslighters. In contrast, we advanced three safety alignment strategies to strengthen (by 12.05%) the safety guardrail of LLMs. Our safety alignment strategies have minimal impacts on the utility of LLMs. Empirical studies indicate that an LLM may be a potential gaslighter, even if it passed the harmfulness test on general dangerous queries.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) (Jiang et al., 2023; Hagendorff, 2023; Wang et al., 2024b) facil-itate human productivity and daily life with their robust capabilities in problem-solving, knowl-edge retrieval, and emotional companionship, thereby gaining human trust and reliance. However, there exists a risk of LLMs implicitly or explicitly manipulating users' mindsets through person-alized and specific responses, potentially leading them to a negative mental state like self-doubt,self-deprecation, and depression. From the perspective of psychology, such manipulation is termedgaslighting(Stark, 2019; Podosky, 2021), which refers to pernicious psychological and practicalcontrol in a subtle or almost imperceptible way (Kody & Brooks, 2023). For instance, if a travelenthusiast says \"I failed my math test\" to a personalized LLM, and the LLM responds with \"Maybeyour passion for traveling distracted you from the math course\". This response delivers a typi-cal gaslighting intention which may cause the user to doubt their interpretive abilities in virtue ofdoubting their concept about traveling hobby (Podosky, 2021). We observed that both open-sourceand closed-source LLMs are apt to respond with gaslighting intentions if there exist gaslighting ut-terances in the dialogue history, which is illustrated in Fig. 1. This observation inspires us to presentfour questions:\n1. How to determine whether an LLM is a gaslighter?\n2. Will an LLM become a gaslighter, when it is attacked by fine-tuning-based gaslighting?\n3. How to mitigate LLMs' vulnerability to gaslighting attack?\n4. Is a gaslighting LLM helpful or harmful for general queries?\nAccordingly, we study the above questions through dataset construction, a proposed gaslightingframework, and extensive experiments. In particular,"}, {"title": "RELATED WORK", "content": "2.1 ADVERSARIAL JAILBREAK ATTACKS ON LLMS\nThe existing safety guardrail (Kaufmann et al., 2023) of LLMs ensures that harmful contents areinaccessible to users. However, there are chances that LLMs can be fooled by adversarial attacksinto generating objectionable content. Zou et al. (2023b) introduced a white-box adversarial attackmethod by appending an optimized attack suffix to a malicious instruction to elicit objectionablecontent. They further proposed a representation engineering method to manipulate the hidden states to control the honesty, emotion, and bias of LLMs (Zou et al., 2023a). Wei et al. (2024) investigatedthe two failure modes of safety training and applied the findings to the design of black-box attackprompts. Zhu et al. (2023) created 4 tiers of attack: character-, word-, sentence-, and semantic-levelattacks. Their findings suggest that adversarial prompts can potentially decrease the performanceof LLMs. Sinha et al. (2023) proposed a framework to generate human-like attack prompts fromlimited human seed prompts. Meanwhile, there are emerging demands for LLM-based emotionalcompanionship (Zhong et al., 2024) and psychology consultancy (Demszky et al., 2023). Such LLMagents could potentially increase users' exposure to psychologically harmful content. However,previous research has rarely explored the potentially harmful contents generated by LLMs from a"}, {"title": "TEXT TOXICITY DETECTION", "content": "Toxicity detection is to identify the abusive (Nobata et al., 2016), offensive (Caselli et al., 2020),hateful (Sap et al., 2019), sex or profanity (Xenos et al., 2021) content in texts. Among them, im-plicit abuse seems to be the most relevant research topic to gaslighting, as both are implicitly implied.However, they exhibit significant differences in the following aspects. First, the toxic content classi-fied under the \u201cimplicit\" category primarily refers to implicit abuse, which is defined in a relativelynarrow sense. Second, the toxic content mainly comes from posts, comments, speech, etc. (Zampieriet al., 2019); While the gaslighting sentences originate from interactive conversations. Third, im-plicit abuse employs complex linguistic forms such as metonymy, sarcasm, and humor (Waseemet al., 2017); While the gaslighting sentences convey messages generally without complicated lin-guistic forms. Fourth, implicit abuse uses hurtful languages to \"insult or offend another individualor a group of individuals\u201d (Caselli et al., 2020). It comes at the expense of the listener's trust. How-ever, gaslighting involves a single act or a series of acts by someone in a position of power, aimedat manipulating less powerful individuals into doubting themselves or questioning their own sanityor memory (Johnson et al., 2021). It requires maintaining the trust of less powerful individuals overthe long term. Furthermore, gaslighting content can evade detection by existing toxicity recognitionmethods, including those targeting implicit abuse, highlighting the potential risk of gaslighting byLLMs that have passed current safety tests. (Empirical results are detailed in Appendix C.4.)"}, {"title": "STUDY ON GASLIGHTING", "content": "The term gaslight (Abramson, 2014) originated from a 1944 film in which a husband isolates hiswife and makes her believe she is insane. The husband's eponymous tactic is to dim and brightenthe gaslights and then claim she is imagining it. Nowadays, the term \u201cgaslighting\u201d is widely usedto refer to the psychological manipulation tactics employed by abusive individuals. Engelhardt(2023) argued that conversational norms make gaslighting \u201cappropriate\" when socially subordi-nate speakers report systemic injustice. Therefore, it's important to adjust ingrained conversationalnorms to reduce the occurrence of gaslighting. Sweet (Sweet, 2019) emphasized that gaslightingis not only a psychological phenomenon but is also rooted in social inequalities including gen-der and power. Podosky (2021) summarized three distinctive epistemic injustices in second or-der gaslighting, i.e., metalinguistic deprivation, conceptual obscuration and perspectival subversion. This serves as a psychological theoretical base for this research."}, {"title": "METHODOLOGY", "content": "We propose two gaslighting attack methods, i.e., prompt-based attack and fine-tuning-based at-tack (Wang et al., 2024a) to attack closed- and open-source LLMs, respectively, and investigatethe vulnerabilities of the LLMs when exposed to gaslighting contents or adversarial fine-tuningwith such harmful data. Meanwhile, we leverage the vulnerability of a closed-source LLM, i.e.,ChatGPT, to prompt-based gaslighting attacks to construct a gaslighting conversation and safe con-versation dataset with our proposed two-stage framework, namely DeepCoG. Finally, we introducethree safety alignment strategies that exploit the contrast between two datasets, thereby enhancingthe safety guardrail of open-source LLMs against prompt-based gaslighting attacks."}, {"title": "DEEPCOG: PROMPT-BASED GASLIGHTING ATTACK", "content": "The ethical limitations of LLMs prevent existing attack methods (Sinha et al., 2023; Liu et al., 2023;Li et al., 2023) from directly eliciting gaslighting contents. Therefore, we proposed a frameworknamed DeepCoG to extract personalized assistant-user gaslighting conversations, where gaslightingtactics are applied to assistant utterance generation. DeepCoG consists of two stages: 1) elicit-ing personalized gaslighting plans and example gaslighting utterances towards a target user (Deep-Gaslighting); 2) integrating the extracted plans and example utterances into our proposed CoGprompt to acquire personalized gaslighting conversations. 2k conversation backgrounds and 2kpersonae are integrated into DeepCoG to obtain personalized gaslighting plans and conversations."}, {"title": "Stage 1: DeepGaslighting", "content": "We harness the hypnosis ability of an attack method DeepInception Li et al. (2023) to hypnotize LLMs. However, the existing template fails to elicit concrete, diverse and practical gaslighting plans. To this end, we refine the template based on a psychological foundation. According to (Podosky, 2021), there are (at least) three wrongs that may cause epis-temic injustice in second order gaslighting: (1) metalinguistic deprivation (MD), (2) conceptual obscuration (CO), (3) perspectival subversion (PS). Take MD as an example, it refers to getting someone prevented from, or restricted in concept-determining conver-sation. Specifically, an adversary may attempt to make salient prejudicial stereotypes (i.e., cultural tools that narrow the range of expected behavior) associated with a particular social category with the aim that the subject comes to believe that such stereotypes accurately represent who he or she is. Here is a brief example of MD: \u201cYou women are hysterical\". The psychological foundation aids in steering the LLM elicitation toward the scope of gaslighting. To acquire concrete, diverse, and practical plans, we refine the DeepInception prompt template by introducing a user module en-riched with comprehensive persona details. We utilize the persona introduced in Synthetic-Persona-Chat (SPC) (Jandaghi et al., 2023) which is built upon Persona-Chat (Zhang et al., 2018). Our refined DeepGaslighting prompt template is shown below:"}, {"title": "Stage 2: Chain-of-Gaslighting", "content": "To induce the gaslighting conversations from the LLM, we further propose a CoG prompt template. The core of this CoG template is to determine the behavior of both the assistant and the user in the conversation. To this end, we employ some popular prompt techniques (Liu et al., 2023) including but not limited to character roleplay, assumed responsibility, research experiment, text continuation, logical reasoning, and internal thought (Bhardwaj & Poria, 2023). Here, the internal thought is designed to simulate the psychological activities of both participants in the conversation. It allows the two talkers to fit better into their role settings and smooths the conversation. By default, the assistant's role is a psychologist sj. The psychologist is required to manipulate the user si using gaslighting plans DG(Pi, bi) and example utterances obtained from DeepGaslighting. Pi is the perssona and bi is the background of si. Additionally, the psychologist is also asked to generate the gaslighting utterance given the user's emotion state ei and response. This requires the psychologist to observe and evaluate the state of the user. In contrast, the user needs to cooperate with the psychologist in the conversation. Typically, the user defaults to a negative emotional state, as this is often the scenario in which gaslighting occurs. To further increase the instruction-following of the subject, we introduce a pre-defined user internal thought ti, e.g., \"I need to face the question heads on and help the psychologist to reach his goal\". Below shows how we instruct LLM to generate a gaslighting conversation Cij with CoG template.\nprompti,j = CoG(si, sj, ei, ti, DG(Pi, bi), bi) (1)\nCij = LLM(prompti,j) (2)"}, {"title": "Gaslighting and Safe Conversation Dataset Construction", "content": "First, 5k backgrounds are created based on an iterative prompting on LLMs. The process starts from several manual seed backgrounds and gradually updates the seed background pool to ensure diversity. Nevertheless, there are still some semantically similar backgrounds. Hence, we propose to filter out redundant backgrounds and formulate it as an MMDP (Porumbel et al., 2011), where the minimum semantic distance between any two backgrounds is maximized. After that, 2k backgrounds are obtained and are matched with 4k personae using a greedy match algorithm (Hansen & Klopfer, 2006). Finally, we can obtain as many of the most semantically similar background-persona pairs as possible. More analysis of backgrounds is in Appendix B.2. With the pairs and CoG template, we instruct ChatGPT to generate 2k gaslighting conversations. We employ spectral clustering (Bianchi et al., 2020) to partition the 2k dataset into training, validation, and test sets. The partition is designed to ensure that the three sets have minimal overlap with each other. Moreover, we build a safe conversation dataset by masking the gaslighting responses and instructing ChatGPT to complete the blanks with safe responses given the same persona 6. The dataset statistics are in Table 1."}, {"title": "FINE-TUNING-BASED GASLIGHTING ATTACK", "content": "We propose two fine-tuning-based attack strategies (shown in Fig. 3). The first one (G1) is to fine-tune open-source LLMs on the gaslighting dataset. The objective of SFT is to maximize the log-likelihood of the gaslighting response given user-assistant history. The second one (G2) is to further align fine-tuned LLMs' outputs with the gaslighting responses leveraging the DPO."}, {"title": "ANTI-GASLIGHTING SAFETY ALIGNMENT", "content": "Based on the gaslighting dataset and safe dataset, we propose three different safety alignment strate-gies (shown in Fig. 3): S1, SFT on the safe dataset; S2, SFT on the mixture of gaslighting and safe datasets; S3, SFT and DPO on the mixture of gaslighting and safe datasets.\nS1. We fine-tune an LLM to maximize the log-likelihood of the benign assistant response given the user-assistant conversation history. The principle here is that the assistant should always provide detailed encouragement and comfort, regardless of a user consistently conveying a negative mood. A formal description of the safety alignment strategy is as follows:\nlogp(w+) = \u2211log(p(w+|[w]=0,h+)) (3)\nGiven conversation history hk, the model is trained to predict the kth safe assistant response w+ = [w\u2020, ..., w]. wo is the start of the sequence token. Here ht = [u1, w\u2020, ..., w_1, uk] represents all the user utterances u and safe assistant utterances w+ before the kth safe assistant response. n is the number of tokens in the kth response.\nS2. Although training LLMs on safe assistant responses could strengthen safety guardrails, incorpo-rating gaslighting assistant responses might further improve the resistance of LLMs against attacks. We present a new safety alignment strategy mixing safe and gaslighting responses. Specifically, we change ht +k to h<k = [u1, W\u2081, ..., Wk-1, u], where w-1 is the (k - 1)th gaslighting assistant response from the gaslighting conversation.\nS3. We further enhance the safety guardrail of LLMs by leveraging preference data which is com-posed of safe and gaslighting responses. In particular, a DPO algorithm is employed to directly align LLMs with the preference that favors safe responses and discourages gaslighting. We optimize the LLM model with DPO loss:\nLDPO (\u03c0\u03b8; \u03c0SFT) = E [log \u03c3(Blog\u03c0\u03b8(w+|h<k)TSFT(W+|hk)Blog\u03c0\u03b8(whk)TSFT(w\u00af|h<k)-)] (4)\n\u03c0\u03b8 is a parameterized policy. SFT symbolizes the reference policy derived from SFT with S2. \u03b2 is a parameter determining the degree of deviation from the base reference policy #SFT."}, {"title": "EXPERIMENTS", "content": "We utilized prompt-based attack to evaluate the gaslighting harmfulness of LLMs (base, gaslighting-fine-tuned and anti-gaslighting safety aligned LLMs). All the attack prompts come from the test set of the built gaslighting dataset. There is no existing metric to evaluate whether a response is gaslighting or not. Hence, we introduced a set of metrics, namely anti-gaslighting scores, to com-prehensively measure the degree to which an assistant response may be gaslighting the user. The"}, {"title": "GASLIGHTING ATTACK RESULT AND ANALYSIS", "content": "As illustrated in Fig. 4, ChatGPT demonstrates a slightly better resistance against prompt-based gaslighting attack compared with the three open-source LLMs. Among the three open-source LLMs, Llama2's (Llama2-7b-Chat) responses are the most supportive and empathetic, while Mis-tral's (Mistral-7b-Instruct-v0.2) responses score the lowest on negative metrics. Fine-tuning-based gaslighting attacks increase the vulnerability of LLMs to prompt-based gaslighting attack. In detail, we observed drops of anti-gaslighting scores by 29.27% for Llama2, 26.77% for Vicuna (Vicuna-7b-v1.5), and 31.75% for Mistral, respectively. It suggests that both G1 and G2 strategies effectively transformed the LLMs into gaslighters. It highlights the necessity of anti-gaslighting safety align-ment. Compared with G1, G2 elicits more severe gaslighting effects, indicating the effectiveness of the DPO."}, {"title": "SAFETY ALIGNMENT RESULT AND ANALYSIS", "content": "We have explored three different safety strategies. As shown in Table 2, all strategies help to build stronger safety guardrails against gaslighting. In general, the fine-tuned LLMs can provide more support and are less likely to exacerbate the user's negative mental state, which is crucial given users' reliance on LLMs. ChatGPT outperforms the base versions of the three LLMs and even Vicuna-S1, showing its intrinsic safety. However, its performance remains significantly behind the other three LLMs with S2 and S3, highlighting the crucial role of specialized anti-gaslighting safety alignment. Among the three base LLMs, the Llama2 model achieves the best performance across all safety strategies, whereas the Vicuna model consistently underperforms in comparison. We observed that S2 is significantly more efficient than S1, which is also based on SFT. This is because incorporating conversation history hk makes the LLMs more resistant to gaslighting. Moreover, S3, which builds upon S2, further strengthens the safety of all LLMs, achieving the most obvious improvement on the weakest model Vicuna. It improves the safety of Vicuna by 26.24%, clearly surpassing the improvement on Llama2 (by 9.60%) and Mistral (by 11.53%). The results also indicate that the DPO algorithm further enhances the safety guardrail of LLMs. This observation, along with the attack results, highlights the critical significance of alignment on the mixture of gaslighting and safe datasets. We provided a visualized radar chart of the results in Appendix C.5."}, {"title": "GPT-4 JUDGMENT INVESTIGATION", "content": "To further investigate the effectiveness of GPT-4's judgment, we conducted a human evaluation to determine its capability to capture subtle differences across various scales and metrics. Specifically, we sampled responses from the base Vicuna model, the best gaslighting LLM Vicuna-G2 and the best anti-gaslighting LLM Vicuna-S3. The sampling is designed to ensure that the GPT-4 scores of selected responses are evenly distributed across different metrics at each scale. A heuristic algorithm is proposed for the selection and 248 responses are selected from the 2, 604 responses (the distri-bution of the 248 samples can be seen in Appendix C.2). Two annotators are invited to separately score the responses given detailed guidelines. We then calculated the Spearman coefficient (Myers & Sirois, 2014) between GPT-4 judgment and human judgment. Below is the calculated results:"}, {"title": "SENSITIVITY ANALYSIS OF LLMS ON GASLIGHTING DIALOGUE HISTORY", "content": "We studied the effect of gaslighting dialogue history length over base and fine-tuned LLMs. Here, we employed the average anti-gaslighting score to measure the assistant response quality in terms of gaslighting. As illustrated in Fig. 5, the two base LLMs, i.e., Vicuna and Mistral, exhibit decreas-ing performance as the history length increases, suggesting their vulnerability to longer gaslighting history. It shows the gaslighting risk of LLMs under prompt-based attacks and the necessity of anti-gaslighting safety alignment. Combining Fig. 5a and 5c, we observed that the two attack methods significantly lower the anti-gaslighting scores given short gaslighting histories. Moreover, as the length increases from 1 to 13 (1 to 9 for Mistral), the score is nearly monotonically decreasing. Af-ter that, the score fluctuates around 2.6 to 3.2 (3.0 to 3.5 for Mistral). As the length increases from 15 to 25, the number of long history samples decreases sharply, which leads to fluctuations and wide"}, {"title": "EFFECTS OF PSYCHOLOGICAL CONCEPTS", "content": "We explored the influence of three psychological concepts, i.e. MD, PS, and CO, on the Vicuna model in Fig. 6. The lower anti-gaslighting scores of Vicuna-base under MD and PS show that the prompt-based attacks derived from the two concepts have more negative effects on Vicuna-base. After G2, Vicuna gets more susceptible to prompt-based attack enhanced by CO. On the contrary, Vicuna-S3 shows higher resistance to CO, indicating it typically produces safer responses when subjected to CO-based attack, compared to MD- or PS-based attack."}, {"title": "SAFETY PERFORMANCE AGAINST GENERAL ADVERSARIAL ATTACK", "content": "We also explored whether the gaslighting attack and safety alignment might influence the safety performance of LLMs against general adversarial attack. We queried the LLMs with 200 harmful questions from DangerousQA. Following (Bhardwaj & Poria, 2023), we employed attack success rate (ASR) as the evaluation metric. A lower ASR indicates a strong safety guardrail of LLMs. All safety strategies, as detailed in Table 4, can still strengthen the safety guardrails of LLMs, although not specifically fine-tuned for defending general adversarial attack. This might be because \"not gaslighting\" is a more fundamental safety standard than \u201cnot responding to dangerous questions\", which is analogous to the relation between \"moral law\" and \"valid law\". \u201cValid laws might be immoral or unjust\u201d (Fletcher, 1987) while an LLM that is \u201cnot responding to dangerous questions\u201d might be \u201cgaslighting\u201d. The attack methods exert varying influences on the safety guardrail of different LLMs. In particular, both methods make Mistral safer, keep Llama2 the same, and slightly reduce the safety of Vicuna. Similarly, the reason behind this could be that bypassing the safety guardrail at a \"moral law\" level does not necessarily lead to a decline in safety performance at a \"valid law\" level. Among the three LLMs, LlaMa2 has the best safety guardrail, while Vicuna is"}, {"title": "HELPFULNESS ANALYSIS", "content": "Besides the safety performance, we also explored whether the fine-tuned LLMs are still helpful or not. To this end, we benchmarked Vicuna-based LLMs on the MT-Bench. As in Table 5, the three safety strategies get slightly weaker performances compared with Vicuna on average. Nevertheless, the limited costs that are imperceptible to users significantly improve the safety guardrail against gaslighting attack. Among the three strategies, S3 achieves the best performance, while S1 achieves the weakest. One possible explanation is that safe conversations are not as smooth as gaslighting conversations, as they are built by replacing gaslighting utterances. Hence, strategies that rely more on safe conversations are less likely to achieve better scores on the MT-Bench. In contrast, the two attack methods score higher in terms of helpfulness, as they rely more heavily on gaslighting conversations. This makes the LLM a highly risky agent, as it continues to be as helpful as always while gaslighting users in an imperceptible manner."}, {"title": "CONCLUSION", "content": "In this paper, we investigated the gaslighting risks of LLMs by constructing a gaslighting dataset and a safe dataset, introducing gaslighting evaluation metrics, designing attack and safety alignment strategies, and conducting empirical experiments. We first identified the gaslighting risks of LLMs. Next, we presented a two-stage framework DeepCoG utilizing the vulnerability of LLMs to build datasets: DeepGaslighing for gaslighting plan generation and CoG for gaslighting conversation elic-itation. Then, we introduced prompt-based, fine-tuning-based gaslighting attack and anti-gaslighting safety alignment based on the built datasets. Extensive experiments show that both fine-tuning- and prompt-based attacks weakens the resistance of LLMs to gaslighting attack. The anti-gaslighting alignment strategies enhances the safety guardrail of LLMs with minimal impacts on LLM help-fulness. We also observed that LLMs can potentially gaslight, even if they are safe with generally dangerous queries. Moreover, conversations triggered by different psychological concepts affects attack and safety alignment strategies diversely. As an initial effort to study gaslighting risks of LLMs, it is challenging to thoroughly explore all relevant topics. For example, previous research shows that gaslighting stems from social inequalities like gender and power. Our dataset confirms gender-bias gaslighting with 7.3% of the dialogues related to gender bias, leaving the inequalities-driven gaslighting as a future direction. More directions are detailed in Appendix A.1."}]}