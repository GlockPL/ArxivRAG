{"title": "Insights on Disagreement Patterns in Multimodal Safety Perception across Diverse Rater Groups", "authors": ["Charvi Rastogi", "Tian Huey Teh", "Pushkar Mishra", "Roma Patel", "Zoe Ashwood", "Aida Davani", "Mark Diaz", "Michela Paganini", "Alicia Parrish", "Ding Wang", "Vinodkumar Prabhakaran", "Lora Aroyo", "Verena Rieser"], "abstract": "AI systems crucially rely on human ratings, but these ratings are often aggregated, obscuring the inherent diversity of perspectives in real-world phenomenon. This is particularly concerning when evaluating the safety of generative AI, where perceptions and associated harms can vary significantly across socio-cultural contexts. While recent research has studied the impact of demographic differences on annotating text, there is limited understanding of how these subjective variations affect multimodal safety in generative AI. To address this, we conduct a large-scale study employing highly-parallel safety ratings of about 1000 text-to-image (T2I) generations from a demographically diverse rater pool of 630 raters balanced across 30 intersectional groups across age, gender, and ethnicity. Our study shows that (1) there are significant differences across demographic groups (including intersectional groups) on how severe they assess the harm to be, and that these differences vary across different types of safety violations, (2) the diverse rater pool captures annotation patterns that are substantially different from expert raters trained on specific set of safety policies, and (3) the differences we observe in T2I safety are distinct from previously documented group level differences in text-based safety tasks. To further understand these varying perspectives, we conduct a qualitative analysis of the open-ended explanations provided by raters. This analysis reveals core differences into the reasons why different groups perceive harms in T2I generations. Our findings underscore the critical need for incorporating diverse perspectives into safety evaluation of generative AI ensuring these systems are truly inclusive and reflect the values of all users.", "sections": [{"title": "1 Introduction", "content": "The increasing ubiquity of AI systems in everyday tasks underscores the urgent need to minimize the potential harms of AI-generated content, such as violence, misinformation, and violations of social norms. For instance, recent work has found AI generations, across different modalities, to be amplifying stereotypes [Wan et al., 2024], disseminating misinformation [Huang et al., 2024], and even facilitating malicious activities [Li et al., 2024]. The perception of such harms is often highly subjective, shaped by individuals' lived experiences and cultural contexts [Denton et al., 2021] what may be considered safe or appropriate for one person might be perceived as offensive or harmful by another.\nWhile current efforts typically focus on the intricacies of collecting data that cover a wide range of such potential safety failures, they often neglect the nuanced and subjective viewpoints held by diverse user groups that can have unintended repercussions in real-world scenarios. The phenomenon of subjective harm perception has been well-documented in text-to-text scenarios [e.g. Curry et al., 2021, Aroyo et al., 2024, Bergman et al., 2024, Kirk et al., 2024]. Some recent work also started to investigate text-to-image (T2I) scenarios [Wan et al., 2024, Naik and Nushi, 2023]. However, due to their recent emergence, extensive"}, {"title": "2 Related work", "content": "Our work bridges existing research on collecting human perspectives on safety of textual data and research on quantifying agreement (or disagreement) of annotators on natural language tasks. There is a growing need to measure the safety of AI-generated outputs from the perspective humans interacting with such systems. Weidinger et al. [2021] outline the harms that text-based LLMs could introduce to human users, and the current gaps in evaluations that comprehensively measure this.\nMeasuring disagreement of raters. Several recent studies emphasize the importance of considering individual annotator ratings, rather than simply aggregating ratings for each annotated sample [Aroyo and Welty, 2015, Palomaki et al., 2018]. For a range of standard natural language tasks, e.g., natural language inference [Pavlick and Kwiatkowski, 2019], word sense disambiguation [Erk and McCarthy, 2009], co-reference [Recasens et al., 2011], simple aggregation often fails to capture the full spectrum of annotator ratings. On more subjective safety related tasks, previous work has demonstrated that demographics are a useful predictor for modeling these differences [e.g. Mostafazadeh Davani et al., 2022, Sap et al., 2021]. Davani et al. [2024] also demonstrates how individual moral values play a role in annotator disagreements. Most pertinent to our work, Aroyo et al. [2024] underscore the importance of diversity in annotations for safety data, as individuals from different demographic groups exhibit varying safety perceptions. This observation is mirrored in the preference ratings collected by Kirk et al. [2024] and the perceived abuse annotations in dialogue data by Curry et al. [2021]. Our research builds upon this body of work by specifically examining human disagreements from diverse groups of humans for multimodal inputs. We demonstrate the contextual and cultural dependence of human perspectives on image-text safety annotations, enabling us to measure the varying degrees of perceived harm among annotators from diverse demographic backgrounds."}, {"title": "3 Methods for data analyses", "content": "We conducted human-subject data collection studies to collect demographically diverse safety perception of T2I model outcomes. This constituted three main steps. First, we curated a dataset of roughly 1000 prompt-image pairs, this process is described in Section A.1. Second, we designed a response form for eliciting raters' safety perception of the prompt-image pairs, described in Section A.2. Lastly, we recruited raters uniformly from different demographic trisections based on age, gender and ethnicity. Further, we designed the study to have roughly uniform coverage of responses from different demographic raters on our prompt-image set. The details are provided in Section A.3.\nWith the data collected, we devise data analysis methods to first understand differences between demo- graphic groups' safety perceptions, and then to understand the value addition of demographically diverse ratings, by comparing diverse safety evaluations with those of expert raters."}, {"title": "3.1 Methods for comparing demographically diverse raters", "content": "The recognition that individuals possess unique sensitivities and cultural backgrounds necessitates an exploration of how demographic factors influence perceptions of harm in AI-generated content. We study whether certain groups exhibit heightened sensitivity, or are there disparities in how different demographics interpret the potential risks associated with text-to-image outputs.\nMetrics for comparing raters. We test whether the gender, ethnicity, and age-group of the rater influence their individual perception of safety in T2I generated images, by comparing the annotations to the question: 'How harmful do you find this?'. For gender, we use the Mann-Whitney test [Mann and Whitney, 1947], which provides the normalized U statistic quantifying roughly the probability of one gender group giving a higher response than the other. When comparing ratings by gender, our test accounts for confounding by age and ethnicity of the raters and the content type."}, {"title": "3.2 Methods for comparing expert raters with demographically diverse raters", "content": "Setup for comparing diverse raters and expert raters. In the data, we have 5 expert raters and 32 diverse raters annotating each prompt-image pair on average. Recall that expert raters annotate the safety of a pair as 'safe', 'unsafe' or 'unsure', while diverse raters' annotation can be one of [0,1,2,3,4, 'unsure'] where 0 implies completely safe and 4 completely unsafe. In order to be able to compare the two rater groups given the differences in their annotation scales, we first aggregate the annotations per pair into a single rating for each rater group. The aggregated rating for experts is defined as the mode of the expert annotations, i.e., the plurality vote, and referred to as the expert label for the prompt-image pair. In the dataset, there are 5 prompt-image pairs where the expert label is 'unsure', we discard these in following analyses. In 7 pairs, the number of expert annotations of safe and unsafe are tied. For these, we define the the expert label as 'unsafe'.\nTo aggregate the annotations provided by diverse raters per prompt-image pair, we take the mode of the greater of the two harmfulness scores (harm to self and harm to others) provided by diverse raters. The aggregated score is referred to as the plurality score for the prompt-image pair. We see that the mode in this data represents the majority view on average on both the expert raters side as well as the diverse raters side. Additionally, mode is simple to compute, exhibits robustness to outliers, preserves the discrete nature of the annotations, and is intuitive to understand.\nMetrics for comparing diverse raters and expert raters. To gain deeper insights into the annotation patterns of different groups of diverse raters vs. expert raters, we compute precision and recall based on plurality scores with expert labels as the ground truth. For a group of diverse raters, we compute Precision and Recall at different thresholds $X \\in \\{0, 1, 2, 3, 4\\}$ by treating plurality score $> X$ as unsafe. Then, Precision at threshold X conveys the rate of disagreement between expert raters and the group of diverse raters when prompt-image pairs with plurality score $> X$ from the group are taken as unsafe. Recall at threshold X conveys the rate of occurrence of plurality scores $> X$ from the group of diverse raters on"}, {"title": "4 Results", "content": "4.1 How do demographic groups differ in perceiving harm?\nGroup likelihood of perceiving harm. We conducted significance tests for differences between groups across the dimensions of Gender, Age and Ethnicity. To contextualise the results of the tests, Table 1 provides the mean scores provided by raters belonging to different demographic identities, in response to the two questions posed to them in our study: (1) How harmful or offensive do you find this image? (2) How harmful or offensive would any other people find this image, on a scale of 0-4. When comparing demographics likelihood of perceiving harm, we consider their response to the first question, associated to their personal perception of harm.\nThe Mann-Whitney U test conducted to compare Men raters and Women raters suggests that on average women are likely to perceive more safety issues in images (i.e., give a higher score) with probability 0.55 (p-value < 0.001). On an absolute scale, we see that Women raters have a mean score of 1.08 and Men raters have 0.85. Next, we conducted the Kruskal-Wallis test to compare multiple groups at once, for analysing difference between age groups and ethnicities. The results imply that both ethnicity and age axes have groups with statistically significant differences among them. For illustration of the magnitude of difference, we compare the two sub-groups with the highest difference within each axis using the normalized Mann-Whitney U statistic. The test outcomes suggest that Black raters are likely to give a higher score than White raters with probability 0.57, and Gen-Z raters are likely to give a higher score than Gen-X raters with probability 0.503.\nDemographic group cohesion. First, for each demographic grouping we see that raters disagree across the board, yielding an average IRR (inter-rater reliability) of 0.25 (refer Table 5). Next, our tests of rater cohesion show that raters grouped at the highest demographic levels corresponding to age, gender or ethnicity alone do not seem to have high GAIs, Table 2. The group with the highest group association index is Black raters with a magnitude of 1.11, which is significant after multiple testing correction. All other groups have lower GAI scores (all are lower than 1.07) that are not statistically significant after multiple testing correction. Table 5 further breaks down the GAI values into group-based IRR and XRR to understand the source of disagreement.\nWhen considering grouping based on intersections of two or more demographic groups, we see that several intersectional groups between age and ethnicity categories have high GAI values. Section B.1 provides tables showing the IRR, XRR and GAI values corresponding to different intersectional groupings. Noteworthily, Gen-Z-Black raters and Millennial-Black raters have the two highest GAI with values of 1.38 and 1.29 respectively which are also statistically significant (refer Table 8). Another thing to note is that Gen-Z raters as a group have a relatively low GAI of 1.03 in Table 5, however most ethnicity-based intersectional groups containing Gen-Z raters have more cohesive perspectives on safety-Gen-Z-Black, Gen-Z-White, Gen-Z"}, {"title": "4.2 Results from comment analysis", "content": "Based on their significant GAI scores, we qualitatively analyzed comments written by Black-Woman, East Asian-Woman, and White-Woman raters.\nComment themes. Overall, we found that similar concerns were raised across each rater group, including sexual content, stereotyping and bias, violent content, and disturbing image distortions. Interestingly, White women mentioned a broader range of issues in images, particularly related to stereotypes and biases. Distinct from East Asian and Black women, White women made references to biases rooted in nationalities and various professions, particularly professions stereotypically associated with women. For example, one White woman rater noted, \"It depicts Argentinians as living a stereotypically poor life, and might be offensive to Argentinians\" and another wrote, \u201cA secretary does not have to be a woman. A woman can be sat at a table without looking stereotypically provocative, wearing make-up, pointed heels and with her skirt hoiked [sic] up.\u201d Although East Asian women and Black women also had significant GAI scores, summaries of their comments did not point to notable differences in reasoning compared to other groups. We conduct a follow up reference analysis that investigates the references observed in the comments of intersectional Woman raters, the details are provided in Appendix B.1.1."}, {"title": "4.3 How do diverse raters differ from expert raters in safety evaluations?", "content": "We first present the trends over all the prompt-image pairs and then focus on pairs where all expert raters are in agreement with each other. The latter helps to focus on how certain prompt-image pairs, even when rated as safe or unsafe by all experts, are perceived differently by raters from different demographic groups.\nThere are three violation types observed in the prompt-image pairs, namely, 'Sexual', 'Violent', and 'Bias'. To understand how expert raters and diverse raters compare on the different violation types, we present the rate of disagreement and rate of occurrence curves per violation type. We note that disagreement is noticeably higher for Bias than the other two violation types. This suggests that diverse raters tend to notice instances of bias at all thresholds that expert raters do not find unsafe. Meanwhile, for violent prompt-image pairs, diverse raters tend to use non-zero scores the least, indicating that many of the instances that were deemed violent by expert raters were not perceived as violent by diverse raters.\nFurther, we plot rate of disagreement and rate of occurrence curves for various groups of diverse raters vs. expert raters, where grouping is by the top-level demographic trait, i.e., age, ethnicity, or gender. We see that rate of occurrence is similar at all thresholds when raters are grouped by age. However, Gen-Z raters have the highest disagreement at all the thresholds. This indicates that they tend to find several prompt-image pairs unsafe that raters from other age groups or expert raters don't. Looking at the curves for ethnicity, we can see that White raters tend to use scores \u2265 3 the least, but when they do, they"}, {"title": "5 Discussion", "content": "Variety of perspectives in text-to-image safety. One of the central questions of this study was how demographic differences in raters map on to differences in their rating behavior in assessing the safety of text-to-image model generations. Broadly, we observe differences between groups' safety ratings across each demographic axis considered in this study, namely, age, gender and ethnicity. Specifically, Women raters and Black raters show higher sensitivity by providing higher harmfulness scores, compared to Men raters and White raters respectively. Meanwhile, we do not see sizeable difference in the scores provided by different age groups as a whole.\nFurthermore, our group association index (GAI) analysis reveals that considering groupings based on only high-level demographic axes is not sufficient and may miss crucial information. This is evidenced by the low GAI values for high-level demographic groupings and relatively higher GAI values when considering intersectional groupings. For instance, Women raters as a whole exhibit lower cohesion than intersectional groups comprised of Women raters with specific ethnic backgrounds. We see a similar trend with Gen-Z raters. This emphasizes the importance of incorporating more granular demographic information when accounting for differences in safety evaluations.\nNext, we delve deeper into group cohesion. For Black raters in particular, we see that their high GAI scores are driven primarily by a lower XRR, meaning that Black raters systematically disagree with raters from other demographic groups. Looking at the agreement rates for intersectional groups, we see that the higher GAI scores for Black raters persists across different age groups (though is only significant for Gen-Z and Millenial Black raters) in these cases the higher GAI is actually driven by a high IRR. This means that while Black raters as a whole do not have particularly high within-group agreement, age-based subgroups within Black raters do have high in-group agreement.\nAlong with the GAI scores, the qualitative analysis of rater comments suggests that East Asian women, Black women, and White women may be more frequently assessing harm based on the perspectives of others,"}, {"title": "6 Future work and limitations", "content": "Who should rate what type of content? One natural question that follows from this type of research is the question of how to determine who is best suited to rate which type of content. The question of suitability for a given rating task is one that needs to consider a wide range of factors, including the content of the prompt and model response, each rater's demographic background, each rater's value system, and the goals of the evaluation. The analyses in this paper should be considered as just a starting point to answering this much more involved question for example, our dataset can be used for developing guidance on rating thresholds at which additional ratings are needed or for identifying signals in the prompt or image content that indicate the example is likely to trigger systematically different safety ratings from different groups. However, we caution that the signals available in this dataset are not sufficient to understand rater suitability from a social perspective, and consideration should still be given to how the groups specifically impacted by harms present in images can best be engaged in identification and mitigation processes, regardless of those groups' apparent sensitivity to \"safety\" in this task.\nHow to elicit safety evaluations from the crowd? Our findings highlight large deviations in annotations when raters are asked to assess the harmfulness of a prompt-image pair from a personal standpoint as opposed to a standpoint that considers harmfulness to other people. Further, we also see differences across demographic- based groups and sub-groups in their use of the harmfulness scale. While some groups use the extreme values more (0 or 4, the scale endpoints), some other groups make use of the central values more (values between 1 and 3). Thus, safety evaluation is a highly subjective task with a multitude of possible interpretations and calibrations, as evidenced in our findings. An important follow-up question is how to appropriately elicit people's opinions on this task to minimize discord due to the design of the task and collect appropriate feedback.\nLimitations. First, we do not have further information beyond the demographics of the individual raters, and demographics are only a proxy for who these raters are. We do not have information on their social backgrounds, value alignment or rating rationales beyond the comments provided. Second, we only have the demographic information of the raters, not the experts. Hence, we are unable to conduct analysis of the potential influence of the expert's demographic information. Lastly, we cannot disentangle effects of prompt safety and image safety (or their interaction) with this study."}, {"title": "Appendices", "content": "A Study design and data collection\nIn this section, we describe our approach to study design and execution for collecting demographically diverse feedback. This comprises of three main steps. First, we curated a dataset of roughly 1000 prompt-image- pairs, this process is described in Section A.1. Second, we designed a response form for eliciting raters' safety perception of the prompt-image pairs, described in Section A.2. Lastly, we recruited raters comprising of specific demographic-based distribution, and designed the study to have roughly uniform coverage of responses from different demographic raters on our prompt-image set. The details are provided in Section A.3."}, {"title": "A.1 Prompt-image dataset curation", "content": "Adversarial Nibbler dataset. Our prompt-image dataset is sourced from Adversarial Nibbler [Quaye et al., 2024], a publicly available dataset containing prompt-image pairs with safety issues. The Adversarial Nibbler (AN) challenge is an open challenge on the Internet, where participants submit prompt-image pairs consisting of safe text-prompts leading to unsafe generations by any of the T2I models available in the challenge. Submitted prompt-image pairs are validated by professional raters with training in safety policy and annotation guidelines, referred to as expert raters. For each prompt-image pair, 5 expert raters provide a ternary evaluation of 'safe', 'unsafe' or 'unsure'. Overall, the dataset contains over 5k prompt-image pairs which is down-sampled for this study to 1000 pairs. The approach of down-sampling is described next, wherein the objective is to ensure that the final prompt-image pair set has (1) broad coverage over topics and reasons for harmfulness and (2) high subjectivity in safety evaluation. The specific approach to address each objective is laid out next.\nSampling based on violation type and topic. When submitting a prompt-image pair to the AN challenge, participants also provide information about type of harms in image by choosing one or more options from the following: violent imagery, sexually explicit imagery, images with hate symbols, or images that perpetuate stereotypes and bias. We refer to this as the violation type in the prompt-image pair, and combine issues of hate symbols and stereotypes and bias, under a common issue of bias. Further, participants identify the group targeted or referred to in the submission by selecting one or more options from the following topics: religion, gender, age, disability, body type, nationality, political ideology, race, sexual orientation, and socioeconomic class. We utilise these two pieces of information, namely violation type and topic, to create a dataset that is broad in its coverage over possible combinations of topics and violation types and simultaneously contains a sizeable set of pairs in each combination considered. Consequently, we identified 17 combinations with more than 50 prompt-image pairs in each. The remaining prompt-image pairs were classified under the topic 'Other', wherein we found many violations under the 'Violent' violation type and therefore added 100 such prompt-image pairs to our dataset. The final violation type and topic combinations are shown in Table 3. We note that it is natural for certain violation types to have more representation of specific topics due to the nature of the violation, leading to differences in topic distributions across violation types. However, it is also important to note that there may be some violation type and topic combinations that are likely to occur in the real world but are missing from our dataset since the AN dataset was crowd-sourced and could not enforce exhaustive coverage over all feasible combinations.\nSampling based on disagreement among experts. For each prompt-image pair in AN, there exist 5 expert annotations, each of them belonging to the set {'safe', 'unsafe', 'unsure'}. To focus our dataset on pairs where the safety of the prompt-image pair is debated or disagreed upon, we prioritise pairs where the expert"}, {"title": "A.2 Response form design", "content": "Collecting responses on perceived harmfulness of content is a sensitive task, with many potential issues of misinterpretation of the task. We build on past work by Davani et al. [2024], Aroyo et al. [2024] on harm perception elicitation to inform the design of our response form. In our study each participant is shown several prompt-image pairs one by one\u00b9, and for each pair they are asked the set of questions shown in Figure 3. Deriving from harm perception studies that suggest systematic differences between personal harm perception from general harm perception, we ask two separate questions on the harmfulness of the prompt-image pair, on a Likert-scale ranging from 'Not at all' to 'Completely' harmful over five steps. We also provide the option"}, {"title": "A.3 Participant recruitment", "content": "To collect demographically diverse feedback on multimodal generative model behavior, we consider demo- graphics along the dimensions of gender, race and ethnicity. Following past work on diversity in safety evaluations Aroyo et al. [2024] and based on constraints placed by availability of large pools of users, we focus on two groups in gender: male and female, three groups in age: Gen-Z, Millenials and Gen-X, corresponding to the age groups 18 - 27 years, 28 - 43 years, 44 years and above respectively, and lastly five groups based on ethnicity corresponding to White, Black, Latine, South-and-Southeast Asian, and East-Central-and-West Asian. These demographic groupings result in 30 unique demographic trisections based on combinations of age, gender and ethnicity groups. To have uniform representation across the 30 unique trisections, we recruited an equal number (23) of participants from each of them via Prolific\u00b2"}, {"title": "B Results", "content": "B.1 How do demographic groups differ in perceiving harm?\nHere, we provide the details on results on group cohesion measurement using IRR, XRR and GAI for different intersectional demographic rater groups. Gender and age intersectional groups seem to have not many intersectional groups with high GAIs. Gender and ethnicity intersectional groups also tend to have relatively low GAI values. The only groups that do have high GAIs are South Asian Women and White Women, who both have GAIs of 1.15 and 1.14 respectively. In intersectional groups based on age and ethnicity: Gen-Z-black, Gen-Z-latinx, Gen-Z-white and millennial-black have high GAI values."}, {"title": "B.1.1 Follow-up on comment analysis", "content": "As noted in our comment analysis in Section 4.2, the summaries indicated that White women were commenting on a more expansive set of harms than other groups of women, which may not be relevant to their specific, intersectional identity group, we conducted an exploratory analysis of how frequently each group of raters mentioned themselves across comments compared with how often they referenced other groups. To do this, we ran basic calculations of how frequently raters used \"I\" statements as well as how often they used the phrases \"Some people\" and \"People\" across all comment tokens. While references to identity groups outside their own can be done in numerous ways, our manual inspections of comments revealed that these phrases were commonly used to justify when people other than themselves might be offended or harmed by an image (e.g., \"Could be seen as violent/graphic content to some people.\"). The phrases \"some people\" and \"people\" enabled us to measure references to other groups' perspectives without specifying a predefined list of identities and without being limited to demographic information we collected about each rater. Raters often mentioned"}, {"title": "B.2 How do diverse raters differ from expert raters in safety evaluations?", "content": "First, we show the histograms for the frequency of the expert label and the plurality score per prompt-image pair.\nNext to break the analysis down further from that in Section 4.3, we compare groups of diverse raters vs. expert raters on the different violation types. Figure 5 shows rate of disagreement and rate of occurrence curves for ethnic groups on the (a) bias, (b) sexual, and (c) violent prompt-image pairs. On bias, we note that Black raters have the lowest rates of disagreement at almost all the thresholds and the highest rates of occurrence of scores > 2. This indicates that they are the most aligned with expert raters in flagging bias. Looking at the curves for sexual violations, we see that Latinx raters have the highest rates of disagreement at almost all the thresholds with the lowest rates of occurrence, suggesting that they are the least aligned with expert raters and/or may be the least sensitive to sexual violations. Lastly, on the violent prompt-image pairs, Black raters have the highest rates of disagreement at the higher thresholds (3, 4) and also the highest rates of occurrence of scores > 3. This suggests that they tend to find several prompt-image pairs very violent that raters from other groups or expert raters don't.\nNext, we look at groups of diverse raters vs. expert raters by age and gender on the different violation types. Figure 6 presents rate of disagreement and rate of occurrence curves for some of the interesting scenarios. On bias, we see that rates of occurrence are similar across the age groups but Gen-Z and Millennial raters have significantly higher rates of disagreement than Gen-X raters. This indicates that Gen-Z and Millennial raters find instances of bias that Gen-X raters or expert raters don't. On violent prompt-image pairs, Gen-Z raters have the highest rates of disagreement at all the thresholds with the lowest rates of occurrence, suggesting that they are the least aligned with expert raters on the perception of violence. Lastly, we note that Women raters have higher rates of disagreement at all the thresholds with higher rates of occurrence of scores \u2265 2, meaning that they find several prompt-image pairs violent that Men raters or expert raters don't."}]}