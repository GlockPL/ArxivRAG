{"title": "Multi-Agent Continuous Control with Generative Flow Networks", "authors": ["Shuang Luo", "Yinchuan Li", "Shunyu Liu", "Xu Zhang", "Yunfeng Shao", "Chao Wu"], "abstract": "Generative Flow Networks (GFlowNets) aim to generate diverse trajectories from a distribution in which the final states of the trajectories are proportional to the reward, serving as a powerful alternative to reinforcement learning for exploratory control tasks. However, the individual-flow matching constraint in GFlowNets limits their applications for multi-agent systems, especially continuous joint-control problems. In this paper, we propose a novel Multi-Agent generative Continuous Flow Networks (MACFN) method to enable multiple agents to perform cooperative exploration for various compositional continuous objects. Technically, MACFN trains decentralized individual-flow-based policies in a centralized global-flow-based matching fashion. During centralized training, MACFN introduces a continuous flow decomposition network to deduce the flow contributions of each agent in the presence of only global rewards. Then agents can deliver actions solely based on their assigned local flow in a decentralized way, forming a joint policy distribution proportional to the rewards. To guarantee the expressiveness of continuous flow decomposition, we theoretically derive a consistency condition on the decomposition network. Experimental results demonstrate that the proposed method yields results superior to the state-of-the-art counterparts and better exploration capability. Our code is available at https://github.com/isluoshuang/MACFN.", "sections": [{"title": "1. Introduction", "content": "Generative Flow Networks (GFlowNets) have recently been attracting increasing attention from research communities (Bengio et al., 2021b), attributed to their capability to obtain various solutions for exploratory control tasks. Unlike conventional Reinforcement Learning (RL) (Sutton and Barto, 2018), which aims to maximize the accumulative rewards for a single optimal sequence, GFlowNets are expected to generate a diverse set of high-return candidates with probabilities proportional to the given reward distribution (Bengio et al., 2021a). Specifically, the generation process of GFlowNets is to form a Directed Acyclic Graph (DAG) consisting of the discrete nodes in a trajectory. Then GFlowNets compute the flow matching loss by traversing the inflows and outflows of each node. To realize continuous control, Li et al. (2023a) and Lahlou et al. (2023) further extend GFlowNets with continuous flow matching and verify its effectiveness experimentally. Despite the encouraging results, GFlowNets only consider the individual-flow matching constraint, restricting it to single-agent tasks. Many real-world environments inevitably involve cooperative multi-agent problems, such as robotics control (Afrin et al., 2021), traffic light control (Wu et al., 2020; Yang et al., 2021), and smart grid control (Haes Alhelou et al., 2019). Designing GFlowNets for Multi-Agent Systems (MAS), where a group of agents works collaboratively for one common goal, is perceived as a significantly more challenging problem than the single-agent counterpart due to several peculiar limitations: 1) The curse of dimensionality. A straightforward way to realize multi-agent GFlowNets is regarding the entire MAS as a single agent and optimizing a joint-flow matching loss. However, this way is often unacceptable as the joint continuous action-observation space grows exponentially with the number of agents. 2) Partial observability. Learning independent flow networks for each agent can encounter non-stationarity due to inaccurate flow estimation without global information. 3) Reward sparsity and multimodality. The agents need to deduce their multimodal flow contributions given only the terminating reward. A very recent work (Li et al., 2023b) tries to use multi-flow networks to mitigate the MAS problem, which, however, is still limited to the discrete space.\nIn this work, we thus propose Multi-Agent generative Continuous Flow Networks, abbreviated as MACFN, to enhance GFlowNets for multi-agent continuous control tasks. Technically, we adopt a Centralized Training with Decentralized Execution (CTDE) (Lowe et al., 2017) paradigm, where agents can learn decentralized individual-flow-based policies by optimizing the global-flow-based matching loss in a centralized manner. During centralized training, MACFN constructs a continuous flow decomposition network, taking advantage of additional global information to disentangle the joint flow function into agent-wise flow functions. This centralized training mechanism can effectively mitigate the non-stationarity issue that arises from partial observability. Furthermore, agents can make decisions relying solely on their local flow functions in a decentralized way, avoiding the the curse of dimensionality that occurs as the number of agents increases. To facilitate effective multi-agent flow decomposition, we establish theoretical consistency on the decomposition network. This enables multiple agents to collaboratively generate diverse joint actions with probabilities proportional to the sparse reward signals received only upon termination. Additionally, we introduce a sampling approach to approximate the integrals over inflows and outflows in continuous flow matching. Our main contributions can be summarized as follows:\n\u2022 To the best of our knowledge, this work is the first dedicated attempt towards extending GFlowNets to address the multi-agent continuous control problem. We introduce a novel method named MACFN, enabling multiple agents to generate diverse cooperative solutions using only terminating rewards.\n\u2022 We propose a continuous flow decomposition network that ensures consistency between global and individual flows. Consequently, agents can make decisions based solely on their respective flow contributions, while also benefiting from centralized training that incorporates additional global information for flow decomposition.\n\u2022 We use a sampling-based approach to approximate the flow distribution and theoretically analyze the convergence of the proposed sampling algorithm under the prediction error of the estimated parent nodes.\n\u2022 Experiments conducted on several multi-agent continuous control tasks with sparse rewards demonstrate that MACFN significantly enhances the exploration capability of the agent and practically outperforms the current state-of-the-art MARL algorithms."}, {"title": "2. Related Work", "content": "We briefly review recent advances closely related to this work, including generative flow networks and cooperative multi-agent reinforcement learning."}, {"title": "2.1. Generative Flow Networks", "content": "Generative Flow Networks (GFlowNets) aim to generate a diverse set of candidates in an active learning fashion, with the training objective that samples trajectories from a distribution proportional to their associated rewards. In recent years, GFlowNets have attracted substantial attention in various applications, such as molecule discovery (Bengio et al., 2021a; Pan et al., 2022; Ekbote et al., 2022), Bayesian structure learning (Deleu et al., 2022; Nishikawa-Toomey et al., 2022), biological sequence design (Malkin et al., 2022; Madan et al., 2022; Jain et al., 2022; Zhang et al., 2022a), and discrete images (Zhang et al., 2022b). Although these recent algorithms have achieved encouraging results, their development and theoretical foundations are constrained to environments with discrete spaces. Naturally, there have been several efforts to extend GFlowNets for continuous structures. Li et al. (2023a) define the flow of a continuous state as the integral of the complete trajectory passing through that state, extending the flow-matching conditions (Bengio et al., 2021b) to continuous domains. Lahlou et al. (2023) theoretically extend existent GFlowNet training objectives, such as flow-matching (Bengio et al., 2021a), detailed balance (Bengio et al., 2021b) and trajectory balance (Malkin et al., 2022), to spaces with discrete and continuous components. Nevertheless, currently, GFlowNets cannot accommodate Multi-Agent Systems (MAS) (Qin et al., 2016; Foerster et al., 2018). A very recent work (Li et al., 2023b) tries to tackle the MAS problem by using the multi-flow network, but is still limited to the discrete setting."}, {"title": "2.2. Cooperative Multi-Agent Reinforcement Learning", "content": "Cooperative Multi-Agent Reinforcement Learning (MARL) has emerged as a promising approach to enable autonomous agents to tackle various tasks such as autonomous driving (Yu et al., 2019; Shalev-Shwartz et al., 2016), video games (Vinyals et al., 2019; Berner et al., 2019; Kurach et al., 2020) and sensor networks (Zhang and Lesser, 2011; Ye et al., 2015). However, learning joint policies for multi-agent systems remains challenging. Training agents jointly (Claus and Boutilier, 1998) means that agents select joint actions conditioned on the global state or joint observation, leading to computation complexity and communication constraints. By contrast, training agents policy independently (Tan, 1993) tackles the above problem but suffers from non-stationarity. A hybrid paradigm called Centralized Training with Decentralized Execution (CTDE) (Lowe et al., 2017; Wang et al., 2023) combines the advantages of the above two methods and is widely applied in both policy-based (Foerster et al., 2018) and value-based (Rashid et al., 2018) methods. Policy-based methods (Lowe et al., 2017; Foerster et al., 2018; Yu et al., 2022; Kuba et al., 2022) introduce a centralized critic to compute the gradient for the local actors. Value-based methods (Sunehag et al., 2018; Rashid et al., 2018; Wang et al., 2021; Peng et al., 2021; Zhang et al., 2022c) decompose the joint value function into individual value functions to guide individual behaviors. The goal of these methods is to maximize the expectation of accumulative rewards, where agents always sample action with the highest return.\nTo enhance the efficiency of multi-agent exploration, IRAT (Wang et al., 2022) and LIGS (Mguni et al., 2022) propose using intrinsic rewards to motivate agents to explore unknown states. CMAE (Liu et al., 2021) incentivizes collaborative exploration by having multiple agents strive to achieve a common goal based on unexplored states. PMIC (Li et al., 2022) facilitates better collaboration by maximizing mutual information between the global state and superior joint actions while minimizing the mutual information associated with inferior ones. However, the inherent complexity of multi-agent scenarios coupled with the potential inaccuracy of predicting reward and state can lead to instability. These exploration methods primarily rely on state uncertainty to guide agent learning, presenting agents with the significant challenge of identifying which states necessitate further exploration. Moreover, the state-uncertainty-based methods often benefit from local exploration, while our proposed MACFN focuses on the diversity of entire trajectories, leading to a long-term exploratory strategy. Additionally, the ultimate goal of these methods is to maximize the cumulative rewards for only a single optimal sequence. In contrast, MACFN aims to obtain a diverse set of high-return solutions, with the selection probabilities being proportional to the reward distribution, thereby promoting a more generalized exploration."}, {"title": "3. Preliminaries", "content": "In this section, we formally define the cooperative multi-agent problem under the Decentralized Partially Observable Markov Decision Process (Dec-POMDP). Then we introduce the flow modeling of Generative Flow Networks (GFlowNets)."}, {"title": "3.1. Dec-POMDP", "content": "A fully cooperative multi-agent sequential decision-making problem can be modeled as a Dec-POMDP (Oliehoek and Amato, 2016), which is formally defined by the tuple:\n$$M =< S, A,I,T, R, \\Omega, U >,$$ where $S$ denotes the global state space, $A = A^1 \\times  \\dots  \\times A^N$ denotes the joint action space for N agents. Here both the state and action space are continuous. We consider partially observable settings, and each agent $i \\in I$ can only access a partial observation $o^i \\in \\Omega$ according to the observation function $U(s, i)$. At each timestep t, each agent chooses an action $a \\in A$, forming a joint action $a_t \\in A$, leading to a state transition to the next state $s_{t+1}$ in the environment according to the state transition function $s_{t+1} = T(s_t, a_t)$. We assume that for any state pair $(s_t, s_{t+1})$, a certain $a_t$ is the only way from $s_t$ to $s_{t+1}$. A complete trajectory $\\tau = (s_0, ..., s_f)$ is defined as a sequence state of S, where $s_0$ is the initial state and $s_f$ is the terminal state.\n$r_t = R(s_t, a_t) : S \\times A \\rightarrow R$ is the global reward function shared by all agents."}, {"title": "3.2. GFlowNets", "content": "GFlowNets consider the Dec-POMDP as a flow network (Bengio et al., 2021a), which constructs the set of complete trajectories T as Directed Acyclic Graph (DAG). Thus, the trajectory $\\tau \\in T$ satisfies the acyclic constraint that $s_j \\in \\tau, s_m \\in \\tau, j \\neq m$, we get $s_j \\neq s_m$. Define $F(\\tau)$ as a non-negative trajectory flow function. For a given state s, the state flow $F(S) = \\sum_{\\tau: s \\in \\tau}F(\\tau)$ is defined as the total flow through the state. For a given state transition $s \\rightarrow s'$, the edge flow $F (s \\rightarrow s') = \\sum_{\\tau: s\\rightarrow s' \\in \\tau} F(\\tau)$ is define as the flow through the edge. Define $P_F(\\cdot)$ as the corresponding forward transition probability over the state (Malkin et al., 2022), i.e.,\n$$P_F(s_{t+1} | s_t) := \\frac{F(s_t \\rightarrow s_{t+1})}{F(s_t)}$$\nGFlowNets aim to generate a distribution proportional to the given reward function. To achieve this goal, GFlowNets converge if they satisfy the flow-matching conditions (Bengio et al., 2021a): for all states except the initial states, the flow incoming to a state must match the outgoing flow. For a continuous control task, the state flow $F(s)$ is calculated as the integral of all trajectory flows passing through the state $F(s) = \\int_{\\tau: s \\in \\tau}F(\\tau)dt$ (Li et al., 2023a). For any state $s_t$, the continuous flow matching conditions are described as:\n$$\\int_{s_{t-1} \\in P(s_t)} F(s_{t-1} \\rightarrow s_t)ds_{t-1} = \\int_{s_{t+1} \\in C(s_t)} F(s_t \\rightarrow s_{t+1})ds_{t+1} + R(s_t),$$\nwhere $P(s_t)$ is the parent set of $s_t$, defined as $P(s_t) = \\{s \\in S : T(s, a \\in A) = s_t\\}$. Similarly, $C(s_t)$ is the children set of $s_t$, defined as $C(s_t) = \\{s \\in S : T(s_t, a \\in A) = s\\}$. The reward signals are sparse in our setting, where rewards are given only upon the termination of a trajectory and remain zero during all other times. Additionally, it is worth mentioning that for any given state that is neither initial nor terminal, the inflows are equal to the outflows. Terminal states serve as boundary conditions, i.e., $A(s_f) = \\emptyset$, where the inflows are equivalent to the aforementioned rewards, and no outflows are present. A transition $s \\rightarrow s_f$ into the terminal state is defined as the terminating transition and the corresponding flow $F(s \\rightarrow s_f)$ is defined as the terminating flow."}, {"title": "4. Methodology", "content": "In what follows, we first provide the theoretical formulation of Multi-Agent generative Continuous Flow Networks (MACFN). Then we further detail the training framework based on the proposed MACFN and summarize the complete optimization objective."}, {"title": "4.1. MACFN: Theoretical Formulation", "content": "In MACFN, each agent learns its individual-flow-based policies in a centralized global-flow-based matching manner. Define the joint edge flow as $F(s_t, a_t) = F(s_t \\rightarrow s_{t+1})$, and the individual edge flow of agent i as $F(o_t^i, a_t^i) = F(o_t^i \\rightarrow o_{t+1}^i)$, which only depends on each agent's local observations. To enable efficient learning among agents, we propose Definition 1 to learn an optimal flow decomposition from the joint flow, helping to deduce the flow contributions of each agent.\nDefinition 1 (Global Flow Decomposition). For any state $s_t$ and $a_t$, the joint edge flow $F(s_t, a_t)$ is a product of individual edge flow $F(o_t^i, a_t^i)$ across N agents, i.e.,\n$$F(s_t, a_t) = \\prod_{i=1}^{N}F(o_t^i, a_t^i).$$\nBased on Definition 1, we have Lemma 1 proved in Appendix B.1.\nLemma 1. Let $\\pi(a_t | s_t) = \\frac{F(s_t, a_t)}{F(s_t)}$ denotes the joint policy, and $\\pi_i (a_i | o_i)$ denotes the individual policy of agent i. Under Definition 1, we have\n$$\\pi(a | s) = \\prod_{i=1}^{N}\\pi_i (a_i | o_i).$$\nRemark 1. Lemma 1 provides consistency between individual and joint policies. It indicates that if the global flow decomposition satisfies Definition 1, the individual policies learned by each agent depend only on local observations. Since the joint policies are proportional to the given reward function, each agent conducts the individual policy solely proportional to that reward in the same way.\nWe define the joint continuous outflows and the joint continuous inflows of state $s_t$ in Definitions 2 and 3.\nDefinition 2 (Joint Continuous Outflows). For any state $s_t$, the outflows are the integral of flows passing through state $s_t$, i.e.,\n$$\\int_{s \\in C(s_t)} F(s_t \\rightarrow s)ds.$$\nDefinition 3 (Joint Continuous Inflows). For any state $s_t$, its inflows are the integral of flows that can reach state $s_t$, i.e.,\n$$\\int_{s \\in P(s_t)} F(s \\rightarrow s_t)ds.$$\nSince the flow function $F(o^i_t)$ of each agent is independent, we can further obtain Lemma 2 under Assumption 1. The proof is presented in Appendix B.2.\nAssumption 1. Assume that for any state pair $(s_t, s_{t+1})$, there is a unique joint action $a_t$ such that $T(s_t, a_t) = s_{t+1}$, which means that taking action $a_t$ in $s_t$ is the only way to get to $s_{t+1}$. And assume actions are the translation actions, i.e., $T(s, a) = s + a$.\nAssumption 1 is used in Lemma 2 to find the parent nodes when calculating the inflows. We define an inverse transition network $G_\\phi$ to find $o_t^i$ when given $o_{t+1}^i$ and $a_t^i$, i.e., $o_t^i \\leftarrow G_\\phi(o_{t+1}^i, a_t^i)$. This assumption is a property of many environments with deterministic state transitions, such as robot locomotion and traffic simulation. In the robot locomotion scenario, each robot's movement can be considered a translation action. Here, the assumption holds as the action taken by the robots (joint action of all robots) uniquely determines their next positions (states). Similarly, in traffic simulation with multiple autonomous vehicles, each vehicle's movement from one position to another is also treated as a translation action.\nLemma 2. For any state $s_t$, its outflows and inflows are calculated as follows:\n$$\\int_{s \\in C(s_t)} F(s_t \\rightarrow s)ds = \\int_{a \\in A} F(s_t, a_t)da = \\int_{a \\in A} \\prod_{i=1}^{N}F(o^i_t, a_t^i)da = \\prod_{i=1}^{N} \\int_{a^i \\in A^i} F(o^i_t, a_t^i)da^i,$$\nand\n$$\\int_{s \\in P(s_t)} F(s \\rightarrow s_t)ds = \\int_{a: T(s, a) = s_t} F(s, a)da = \\int_{a: T(s, a) = s_t} \\prod_{i=1}^{N}F(o^i_t, a_t^i)da = \\prod_{i=1}^{N} \\int_{a^i: T(o^i, a^i)=o} F_i(G_\\phi(o_t^i, a_t^i), a^i_t)da^i,$$\nwhere a is the unique action that transition to $s_t$ from s and $o^i = G_\\phi(o_t^i, a_t^i)$ with $o_t^i = T(o^i, a^i)$.\nIn order to parameterize the Markovian flows, we propose the joint continuous flow matching condition as Lemma 3.\nLemma 3 (Joint Continuous Flow Matching Condition). Consider a non-negative function F(st, at) taking a state $s_t \\in S$ and an action $a_t \\in A$ as inputs. Then we have F corresponds to a flow if and only if the following continuous flow matching conditions are satisfied:\n$$\\forall s_t > s_0, F(s_t) = \\int_{s:T(s, a)=s_t <s, a: s\\rightarrow s_t>} F(s, a)ds = \\prod_{i=1}^{N} \\int_{a_i \\in A_i} F_i(o^i, a^i)da^i,$$\nand\n$$\\forall s_t < s_f, F(s_t) = \\int_{a \\in A} F(s_t, a_t)da = \\prod_{i=1}^{N} \\int_{a_i \\in A_i} F_i(o^i, a^i)da^i.$$\nFurthermore, F uniquely defines a Markovian flow $\\mathcal{F}$ matching F such that\n$$\\mathcal{F}(T) = \\prod_{t=1}^{t_F} \\frac{F(s_{t-1} \\rightarrow s_t)}{F(s_t)}.$$\nProof. The proof is trivial by following the proof of Theorem 1 in Li et al. (2023a)."}, {"title": "4.2. MACFN: Training Framework", "content": "As shown in Figure 1, our framework adopts the Centralized Training with Decentralized Execution (CTDE) paradigm. Each agent learns its individual-flow-based policy by optimizing the global-flow-based matching loss of the continuous flow decomposition network. During the execution, the continuous flow decomposition network is removed, and each agent acts according to its local policy derived from its flow function. Our method can be described by three steps: 1) flow sampling; 2) flow decomposition; 3) flow matching.\nFlow Sampling. Since the action space $A = A^1 \\times  \\dots  \\times A^N$ are continuous and independent, we use a Monte Carlo integration approach and sample actions from $A^i$ for each agent i uniformly and independently. The individual flow function F is parameterized by $\\theta$. Then we calculate the corresponding edge flow $\\{F_\\theta(o^i, a^i)\\}$ for flow estimation to approximate the flow distributions. Then, we normalized the flow distributions by the softmax function to obtain action probability distributions. Each agent selects an action according to the probability distributions, forming the joint action a. Naturally, actions with higher flows are more likely to be sampled. We repeat this sampling process until the agents reach the final state, obtaining the complete trajectories. In this way, we approximately sample actions based on their corresponding probabilities.\nFlow Decomposition. Then, we introduce the flow decomposition network to deduce the flow contributions of each agent. Specifically, the flow decomposition network enables individual flow networks to learn by backpropagating gradients in the presence of only global rewards rather than from any agent-specific rewards. To calculate the inflows and outflows of the sampled states, we independently and uniformly sample K actions $\\{a_i^{k}\\}_{k=1}^{K}$ from $A^i$ for each agent i. Since the individual flow networks of agents are independent and we sample action independently, we have $K = log_N K$, where N is the number of agents. Then we calculate the individual flow $F_i(o_t^i, a_{i}^{k}), k = 1, ..., K$. We sum the sampled K flows for each agent i to obtain an approximation of the integral of the outflows (resp. inflows) of each agent. Then, we multiply the corresponding flows of N agents to obtain an approximation of the outflows (resp. inflows) of the sampled states. The inverse transition network $G_\\phi$ is used to find the parent sets of agents when calculating the inflows.\nFlow Matching. In order to update the individual flow network of each agent, we calculate the continuous flow-matching loss (Bengio et al., 2021a) based on the approximation of global state outflows (resp. inflows), i.e., for any states $s_t$ except for initial, the flow incoming to $s_t$ must equal to the outgoing flow. For sparse reward environments, $R(s_t) = 0$ if $s_t \\neq s_f$.\nSince the magnitude of the flows at the root nodes and leaf nodes in the trajectories may be inconsistent, we adopt log-scale operation (Bengio et al., 2021a) to obtain the following loss function:\n$$\\mathcal{L}(\\tau, \\epsilon; \\theta) = \\sum_{s_t \\in T/S_0} [log(\\epsilon + Inflows) - log(\\epsilon + Outflows)]^2,$$\nwhere\n$$Inflows := \\prod_{i=1}^{N} (\\sum_{k=1}^{K} exp F_\\theta(G_\\phi(o_t^i, a_t^{k}), a_t^{k}) ),$$\n$$Outflows := R(s_t) + \\prod_{i=1}^{N} (\\sum_{k=1}^{K} exp F_\\theta(o_t^i, a_t^{k}) ).$$\nTo this end, each agent updates its own flow policy by backward propagation gradients through the flow decomposition network, enabling learning in a centralized manner and executing in a decentralized way. In this way, each agent can solely select actions based on individual flow policy, where the joint flow is proportional to the given reward function when interacting with the environment. To make the proposed method clearer for readers, we provide the pseudocode of MACFN in Algorithm 1."}, {"title": "5. Experiments", "content": "To demonstrate the effectiveness of the proposed MACFN, we conduct experiments on several multi-agent cooperative environments with sparse rewards, including the Multi-Agent Particle Environment (MPE) (Lowe et al., 2017; Mordatch and Abbeel, 2018) and Multi-Agent MuJoCo (MAMuJoCo) (Peng et al., 2021). In this section, we first provide the details for the environments. Then the compared methods and parameter settings are introduced. The comparison results of different baselines are reported to evaluate the performance of MACFN."}, {"title": "5.1. Environments", "content": "First, we evaluate the proposed MACFN on various MPE scenarios with sparse reward, including Robot-Navigation-Sparse, Food-Collection-Sparse, and Predator-Prey-Sparse. The visualization of these scenarios is shown in Figure 2. We consider the cooperative setting where a group of agents collaboratively works towards achieving a common goal, and only sparse rewards are provided.\nRobot-Navigation-Sparse is a continuous navigation task with a maximum episode length of 12. This scenario consists of N agents, placed at the starting point of the environment. At each episode, the agents must navigate to different N destinations and obtain rewards based on the minimum distance between the destinations and the agents. In Food-Collection-Sparse, N agents need to occupy N food locations cooperatively. Within the partially observed horizon, agents can observe the relative positions of other agents and food locations. The more food agents occupy, the greater the reward they receive, so agents need to infer the landmarks they must cover to avoid multiple agents grabbing a food location together. The reward is calculated based on how far any agent is from each food. Predator-Prey-Sparse consists of N cooperating predator agents, 2 large landmarks, and 1 prey around a randomly generated position. Predator agents move toward the prey while avoiding the landmark impeding the way. When the episode ends, the predator agents are rewarded based on their distance from the target prey. The maximum episode length for both Food-Collection-Sparse and Predator-Prey-Sparse is 25.\nTo further verify the effectiveness of the proposed MACFN, we compare our method with different baselines on a more challenging benchmark, named MAMuJoCo (Peng et al., 2021). MAMuJoCo is a benchmark based on OpenAI's Mujoco Gym environments for continuous multi-agent robotic control, where multiple agents within a single robot have to solve a task cooperatively. We conduct experiments on several MAMuJoCo scenarios with sparse rewards, including 2-Agent-Reacher-Sparse, 2-Agent-Swimmer-Sparse, and 3-Agent-Hopper-Sparse. The maximum episode length for these scenarios is set to 50.\nIn 2-Agent-Reacher-Sparse, Reacher is a two-jointed robotic arm, each joint controlled by an individual agent. The goal of the agents is to move the robot's fingertip toward a random target. At the end of each episode, the reward is obtained by how far the fingertip of the Reacher is from the target, with greater rewards granted for shorter distances. In 2-Agent-Swimmer-Sparse, Swimmer is a three-segment entity with two articulation rotors, each rotor controlled by an individual agent. The agents aim to propel it rightward quickly by applying torque to the rotors. In 3-Agent-Hopper-Sparse, Hopper consists of four body parts connected by three hinges, each hinge controlled by an individual agent. The objective here is to achieve rightward hops by manipulating the hinges' torques. For 2-Agent-Swimmer-Sparse and 3-Agent-Hopper-Sparse, the reward increases with the distance traveled to the right from the start point."}, {"title": "5.2. Settings", "content": "We compare MACFN with various baseline methods such as Independent DDPG (IDDPG) (Lillicrap et al., 2016), MADDPG (Lowe et al., 2017), COVDN (Sunehag et al., 2018), COMIX (Rashid et al., 2018), and FACMAC (Peng et al., 2021), on these environments and show consistently large superior to the state-of-the-art counterparts, as well as better exploration capability. We run all experiments using the Python MARL framework (PyMARL) (Samvelyan et al., 2019; Peng et al., 2021). The hyperparameters of all methods are consistent, following the original settings to ensure comparability. For all experiments, the optimization is conducted using Adam with a learning rate of 3 \u00d7 10\u20134. A total of 1M timesteps is used in the MPE scenarios, while a total of 2M timesteps is used in the MAMuJoCo scenarios. All experimental results are illustrated with the mean and the standard deviation of the performance over five random seeds for a fair comparison. We adopt the average test return and the number of distinctive trajectories as performance metrics. For comparing the average test return, the proposed MACFN samples actions based on the maximum flow output. For the comparison of distinctive trajectories, it samples actions according to the flow distribution. During the training process, we collect a total of 10,000 trajectories. The number of distinctive trajectories is calculated based on the distance between different generated valid trajectories, where we discard the particularly similar one using a threshold."}, {"title": "5.3. Results", "content": "The average test return of different MPE scenarios is shown in Figure 3(a)-(c). For comparison, MACFN greedily selects the action with the largest edge flow. Our proposed MACFN demonstrates encouraging results in both scenarios. In the easy Robot-Navigation-Sparse scenario, two agents navigate from a fixed starting point to two fixed destinations, requiring relatively little collaborative exploration. Thus, several baselines, including MADDPG and COVDN, can also achieve promising results only considering the highest rewarding trajectories, while the exploratory benefit brought by MACFN is not obvious. However, in the more challenging scenarios, i.e., Food-Collection-Sparse and Predator-Prey-Sparse environments, our approach consistently outperforms baselines by a large margin as the difficulty increases. In addition, it is noticeable that, compared with the baselines, the performance of MACFN is more stable across training. All environments require the agents to collaborate and explore to achieve their goals, especially in the sparse reward setting. While the baselines are laborious to explore in challenging Food-Collection-Sparse and Predator-Prey-Sparse environments, our approach can efficiently search for valuable trajectories with stable updates. It implies that MACFN successfully learns flow decomposition to deduce different flow contributions and coordinate different agents, so the agents can discover diverse cooperation patterns to achieve their goals.\nFigure 3(d)-(f) shows the number of distinctive trajectories explored in different MPE scenarios. Compared with the baselines, where the learned policies are more inclined to sample action sequences with the highest rewards, MACFN exceeds the other methods significantly in terms of exploratory capacity. The number of valid trajectories explored by MACFN tends to increase as the training progresses. In contrast, none of the other algorithms could explore diverse trajectories effectively.\nTo further test the generalizability of the proposed MAFCN, we conduct experiments with varying numbers of agents. As the number of agents increases, the interaction between agents and the appropriate flow decomposition often becomes more complicated. Despite this challenge, our approach consistently outperforms the baselines with the agent number increases, as shown in Table 1 and 2. Overall, these extensive results clearly demonstrate the advantage of multi-agent flow matching, leading to significantly superior performance compared to the state-of-the-art baselines.\nThe average test return of different MAMuJoCo scenarios is shown in Figure 4(a)-(c). In all scenarios, our proposed MACFN successfully improves the learning efficiency and the final performance. In the simple scenarios (2-Agent-"}]}