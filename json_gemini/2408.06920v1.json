{"title": "Multi-Agent Continuous Control with Generative Flow Networks", "authors": ["Shuang Luo", "Yinchuan Li", "Shunyu Liu", "Xu Zhang", "Yunfeng Shao", "Chao Wu"], "abstract": "Generative Flow Networks (GFlowNets) aim to generate diverse trajectories from a distribution in which the final states of the trajectories are proportional to the reward, serving as a powerful alternative to reinforcement learning for exploratory control tasks. However, the individual-flow matching constraint in GFlowNets limits their applications for multi-agent systems, especially continuous joint-control problems. In this paper, we propose a novel Multi-Agent generative Continuous Flow Networks (MACFN) method to enable multiple agents to perform cooperative exploration for various compositional continuous objects. Technically, MACFN trains decentralized individual-flow-based policies in a centralized global-flow-based matching fashion. During centralized training, MACFN introduces a continuous flow decomposition network to deduce the flow contributions of each agent in the presence of only global rewards. Then agents can deliver actions solely based on their assigned local flow in a decentralized way, forming a joint policy distribution proportional to the rewards. To guarantee the expressiveness of continuous flow decomposition, we theoretically derive a consistency condition on the decomposition network. Experimental results demonstrate that the proposed method yields results superior to the state-of-the-art counterparts and better exploration capability. Our code is available at https://github.com/isluoshuang/MACFN.", "sections": [{"title": "1. Introduction", "content": "Generative Flow Networks (GFlowNets) have recently been attracting increasing attention from research communities (Bengio et al., 2021b), attributed to their capability to obtain various solutions for exploratory control tasks. Unlike conventional Reinforcement Learning (RL) (Sutton and Barto, 2018), which aims to maximize the accumulative rewards for a single optimal sequence, GFlowNets are expected to generate a diverse set of high-return candidates with probabilities proportional to the given reward distribution (Bengio et al., 2021a). Specifically, the generation process of GFlowNets is to form a Directed Acyclic Graph (DAG) consisting of the discrete nodes in a trajectory. Then GFlowNets compute the flow matching loss by traversing the inflows and outflows of each node. To realize continuous control, Li et al. (2023a) and Lahlou et al. (2023) further extend GFlowNets with continuous flow matching and verify its effectiveness experimentally. Despite the encouraging results, GFlowNets only consider the individual-flow matching constraint, restricting it to single-agent tasks. Many real-world environments inevitably involve cooperative multi-agent problems, such as robotics control (Afrin et al., 2021), traffic light control (Wu et al., 2020; Yang et al., 2021), and smart grid control (Haes Alhelou et al., 2019). Designing GFlowNets for Multi-Agent Systems (MAS), where a group of agents works collaboratively for one common goal, is perceived as a significantly more challenging problem than the single-agent counterpart due to several peculiar limitations: 1) The curse of dimensionality. A straightforward way to realize multi-agent GFlowNets is regarding the entire MAS as a single agent and optimizing a joint-flow matching loss. However, this way is often unacceptable as the joint continuous action-observation space grows exponentially with the number of agents. 2) Partial observability. Learning independent flow networks for each agent can encounter non-stationarity due to inaccurate flow estimation without global information. 3) Reward sparsity and multimodality. The agents need to deduce their multimodal flow contributions given only the terminating reward. A very recent work (Li et al., 2023b) tries to use multi-flow networks to mitigate the MAS problem, which, however, is still limited to the discrete space.\nIn this work, we thus propose Multi-Agent generative Continuous Flow Networks, abbreviated as MACFN, to enhance GFlowNets for multi-agent continuous control tasks. Technically, we adopt a Centralized Training with Decentralized Execution (CTDE) (Lowe et al., 2017) paradigm, where agents can learn decentralized individual-flow-based policies by optimizing the global-flow-based matching loss in a centralized manner. During centralized training, MACFN constructs a continuous flow decomposition network, taking advantage of additional global information to disentangle the joint flow function into agent-wise flow functions. This centralized training mechanism can effectively mitigate the non-stationarity issue that arises from partial observability. Furthermore, agents can make decisions relying solely on their local flow functions in a decentralized way, avoiding the the curse of dimensionality that occurs as the number of agents increases. To facilitate effective multi-agent flow decomposition, we establish theoretical consistency on the decomposition network. This enables multiple agents to collaboratively generate diverse joint actions with probabilities proportional to the sparse reward signals received only upon termination. Additionally, we introduce a sampling approach to approximate the integrals over inflows and outflows in continuous flow matching. Our main contributions can be summarized as follows:\n\u2022 To the best of our knowledge, this work is the first dedicated attempt towards extending GFlowNets to address the multi-agent continuous control problem. We introduce a novel method named MACFN, enabling multiple agents to generate diverse cooperative solutions using only terminating rewards.\n\u2022 We propose a continuous flow decomposition network that ensures consistency between global and individual flows. Consequently, agents can make decisions based solely on their respective flow contributions, while also benefiting from centralized training that incorporates additional global information for flow decomposition.\n\u2022 We use a sampling-based approach to approximate the flow distribution and theoretically analyze the convergence of the proposed sampling algorithm under the prediction error of the estimated parent nodes.\n\u2022 Experiments conducted on several multi-agent continuous control tasks with sparse rewards demonstrate that MACFN significantly enhances the exploration capability of the agent and practically outperforms the current state-of-the-art MARL algorithms."}, {"title": "2. Related Work", "content": "We briefly review recent advances closely related to this work, including generative flow networks and cooperative multi-agent reinforcement learning."}, {"title": "2.1. Generative Flow Networks", "content": "Generative Flow Networks (GFlowNets) aim to generate a diverse set of candidates in an active learning fashion, with the training objective that samples trajectories from a distribution proportional to their associated rewards. In recent years, GFlowNets have attracted substantial attention in various applications, such as molecule discovery (Bengio et al., 2021a; Pan et al., 2022; Ekbote et al., 2022), Bayesian structure learning (Deleu et al., 2022; Nishikawa-Toomey et al., 2022), biological sequence design (Malkin et al., 2022; Madan et al., 2022; Jain et al., 2022; Zhang et al., 2022a), and discrete images (Zhang et al., 2022b). Although these recent algorithms have achieved encouraging results, their development and theoretical foundations are constrained to environments with discrete spaces. Naturally, there have been several efforts to extend GFlowNets for continuous structures. Li et al. (2023a) define the flow of a continuous state as the integral of the complete trajectory passing through that state, extending the flow-matching conditions (Bengio et al., 2021b) to continuous domains. Lahlou et al. (2023) theoretically extend existent GFlowNet training objectives, such as flow-matching (Bengio et al., 2021a), detailed balance (Bengio et al., 2021b) and trajectory balance (Malkin et al., 2022), to spaces with discrete and continuous components. Nevertheless, currently, GFlowNets cannot accommodate Multi-Agent Systems (MAS) (Qin et al., 2016; Foerster et al., 2018). A very recent work (Li et al., 2023b) tries to tackle the MAS problem by using the multi-flow network, but is still limited to the discrete setting."}, {"title": "2.2. Cooperative Multi-Agent Reinforcement Learning", "content": "Cooperative Multi-Agent Reinforcement Learning (MARL) has emerged as a promising approach to enable autonomous agents to tackle various tasks such as autonomous driving (Yu et al., 2019; Shalev-Shwartz et al., 2016), video games (Vinyals et al., 2019; Berner et al., 2019; Kurach et al., 2020) and sensor networks (Zhang and Lesser, 2011; Ye et al., 2015). However, learning joint policies for multi-agent systems remains challenging. Training agents jointly (Claus and Boutilier, 1998) means that agents select joint actions conditioned on the global state or joint observation, leading to computation complexity and communication constraints. By contrast, training agents policy independently (Tan, 1993) tackles the above problem but suffers from non-stationarity. A hybrid paradigm called Centralized Training with Decentralized Execution (CTDE) (Lowe et al., 2017; Wang et al., 2023) combines the advantages of the above two methods and is widely applied in both policy-based (Foerster et al., 2018) and value-based (Rashid et al., 2018) methods. Policy-based methods (Lowe et al., 2017; Foerster et al., 2018; Yu et al., 2022; Kuba et al., 2022) introduce a centralized critic to compute the gradient for the local actors. Value-based methods (Sunehag et al., 2018; Rashid et al., 2018; Wang et al., 2021; Peng et al., 2021; Zhang et al., 2022c) decompose the joint value function into individual value functions to guide individual behaviors. The goal of these methods is to maximize the expectation of accumulative rewards, where agents always sample action with the highest return.\nTo enhance the efficiency of multi-agent exploration, IRAT (Wang et al., 2022) and LIGS (Mguni et al., 2022) propose using intrinsic rewards to motivate agents to explore unknown states. CMAE (Liu et al., 2021) incentivizes collaborative exploration by having multiple agents strive to achieve a common goal based on unexplored states. PMIC (Li et al., 2022) facilitates better collaboration by maximizing mutual information between the global state and superior joint actions while minimizing the mutual information associated with inferior ones. However, the inherent complexity of multi-agent scenarios coupled with the potential inaccuracy of predicting reward and state can lead to instability. These exploration methods primarily rely on state uncertainty to guide agent learning, presenting agents with the significant challenge of identifying which states necessitate further exploration. Moreover, the state-uncertainty-based methods often benefit from local exploration, while our proposed MACFN focuses on the diversity of entire trajectories, leading to a long-term exploratory strategy. Additionally, the ultimate goal of these methods is to maximize the cumulative rewards for only a single optimal sequence. In contrast, MACFN aims to obtain a diverse set of high-return solutions, with the selection probabilities being proportional to the reward distribution, thereby promoting a more generalized exploration."}, {"title": "3. Preliminaries", "content": "In this section, we formally define the cooperative multi-agent problem under the Decentralized Partially Observable Markov Decision Process (Dec-POMDP). Then we introduce the flow modeling of Generative Flow Networks (GFlowNets."}, {"title": "3.1. Dec-POMDP", "content": "A fully cooperative multi-agent sequential decision-making problem can be modeled as a Dec-POMDP (Oliehoek and Amato, 2016), which is formally defined by the tuple:\n$$M =< S, A,I,T, R, \u03a9, U >,$$\nwhere $S$ denotes the global state space, $A = A^1 \u00d7 ... \u00d7 A^N$ denotes the joint action space for $N$ agents. Here both the state and action space are continuous. We consider partially observable settings, and each agent $i \u2208 I$ can only access a partial observation $o^i \u2208 \u03a9$ according to the observation function $U(s, i)$. At each timestep $t$, each agent chooses an action $a^i \u2208 A$, forming a joint action $a_t \u2208 A$, leading to a state transition to the next state $s_{t+1}$ in the environment according to the state transition function $s_{t+1} = T(s_t, a_t)$. We assume that for any state pair $(s_t, s_{t+1})$, a certain $a_t$ is the only way from $s_t$ to $s_{t+1}$. A complete trajectory $\u03c4 = (s_0, ..., s_f)$ is defined as a sequence state of $S$, where $s_0$ is the initial state and $s_f$ is the terminal state. $r_t = R(s_t, a_t) : S \u00d7 A \u2192 R$ is the global reward function shared by all agents."}, {"title": "3.2. GFlowNets", "content": "GFlowNets consider the Dec-POMDP as a flow network (Bengio et al., 2021a), which constructs the set of complete trajectories $T$ as Directed Acyclic Graph (DAG). Thus, the trajectory $\u03c4 \u2208 T$ satisfies the acyclic constraint that $s_j \u2208 \u03c4, s_m \u2208 \u03c4, j \u2260 m$, we get $s_j \u2260 s_m$. Define $F(\u03c4)$ as a non-negative trajectory flow function. For a given state $s$, the state flow $F(s) = \u2211_{\u03c4: s \u2208 \u03c4} F(\u03c4)$ is defined as the total flow through the state. For a given state transition $s \u2192 s'$, the edge flow $F(s \u2192 s') = \u2211_{\u03c4: s\u2192s' \u2208 \u03c4} F(\u03c4)$ is define as the flow through the edge. Define $P_F(\u00b7)$ as the corresponding forward transition probability over the state (Malkin et al., 2022), i.e.,\n$$P_F(s_{t+1}|s_t) := \\frac{F(s_t \u2192 s_{t+1})}{F(s_t)}$$\nGFlowNets aim to generate a distribution proportional to the given reward function. To achieve this goal, GFlowNets converge if they satisfy the flow-matching conditions (Bengio et al., 2021a): for all states except the initial states, the flow incoming to a state must match the outgoing flow. For a continuous control task, the state flow $F(s)$ is calculated as the integral of all trajectory flows passing through the state $F(s) = \u222b_{\u03c4: s \u2208 \u03c4} F(\u03c4)d\u03c4$ (Li et al., 2023a). For any state $s_t$, the continuous flow matching conditions are described as:\n$$\u222b_{s_{t-1} \u2208 P(s_t)} F(s_{t-1} \u2192 s_t) ds_{t-1} = \u222b_{s_{t+1} \u2208 C(s_t)} F(s_t \u2192 s_{t+1}) ds_{t+1} + R(s_t),$$\nwhere $P(s_t)$ is the parent set of $s_t$, defined as $P(s_t) = {s \u2208 S : T(s, a \u2208 A) = s_t}$. Similarly, $C(s_t)$ is the children set of $s_t$, defined as $C(s_t) = {s \u2208 S : T(s_t, a \u2208 A) = s}$. The reward signals are sparse in our setting, where rewards are given only upon the termination of a trajectory and remain zero during all other times. Additionally, it is worth mentioning that for any given state that is neither initial nor terminal, the inflows are equal to the outflows. Terminal states serve as boundary conditions, i.e., $A(s_f) = \u00d8$, where the inflows are equivalent to the aforementioned rewards, and no outflows are present. A transition $s \u2192 s_f$ into the terminal state is defined as the terminating transition and the corresponding flow $F(s \u2192 s_f)$ is defined as the terminating flow."}, {"title": "4. Methodology", "content": "In what follows, we first provide the theoretical formulation of Multi-Agent generative Continuous Flow Networks (MACFN). Then we further detail the training framework based on the proposed MACFN and summarize the complete optimization objective."}, {"title": "4.1. MACFN: Theoretical Formulation", "content": "In MACFN, each agent learns its individual-flow-based policies in a centralized global-flow-based matching manner. Define the joint edge flow as $F(s_t, a_t) = F(s_t \u2192 s_{t+1})$, and the individual edge flow of agent $i$ as $F(o_t^i, a_t^i) = F(o_t^i \u2192 o_{t+1}^i)$, which only depends on each agent's local observations. To enable efficient learning among agents, we propose Definition 1 to learn an optimal flow decomposition from the joint flow, helping to deduce the flow contributions of each agent."}, {"title": "Definition 1 (Global Flow Decomposition).", "content": "For any state $s_t$ and $a_t$, the joint edge flow $F(s_t, a_t)$ is a product of individual edge flow $F(o_t^i, a_t^i)$ across $N$ agents, i.e.,\n$$F(s_t, a_t) = \\prod_{i=1}^{N} F(o_t^i, a_t^i).$$\nBased on Definition 1, we have Lemma 1 proved in Appendix B.1.\nLemma 1. Let $\u03c0(a_t | s_t) = \\frac{F(s_t,a_t)}{F(s_t)}$ denotes the joint policy, and $\u03c0_i (a_i | o_i)$ denotes the individual policy of agent $i$. Under Definition 1, we have\n$$\u03c0(a | s) = \\prod_{i=1}^{N} \u03c0_i (a_i | o_i).$$\nRemark 1. Lemma 1 provides consistency between individual and joint policies. It indicates that if the global flow decomposition satisfies Definition 1, the individual policies learned by each agent depend only on local observations. Since the joint policies are proportional to the given reward function, each agent conducts the individual policy solely proportional to that reward in the same way.\nWe define the joint continuous outflows and the joint continuous inflows of state $s_t$ in Definitions 2 and 3."}, {"title": "Definition 2 (Joint Continuous Outflows).", "content": "For any state $s_t$, the outflows are the integral of flows passing through state $s_t$, i.e.,\n$$\u222b_{s \u2208 C(s_t)} F(s_t \u2192 s)ds.$$\nDefinition 3 (Joint Continuous Inflows). For any state $s_t$, its inflows are the integral of flows that can reach state $s_t$, i.e.,\n$$\u222b_{s \u2208 P(s_t)} F(s \u2192 s_t)ds.$$\nSince the flow function $F(o)$ of each agent is independent, we can further obtain Lemma 2 under Assumption 1. The proof is presented in Appendix B.2.\nAssumption 1. Assume that for any state pair $(s_t, s_{t+1})$, there is a unique joint action $a_t$ such that $T(s_t, a_t) = s_{t+1}$, which means that taking action $a_t$ in $s_t$ is the only way to get to $s_{t+1}$. And assume actions are the translation actions, i.e., $T(s, a) = s + a$.\nAssumption 1 is used in Lemma 2 to find the parent nodes when calculating the inflows. We define an inverse transition network $G_\u00f8$ to find $o_t^i$ when given $o_{t+1}^i$ and $a_t^i$, i.e., $o_t^i \u2190 G_\u00f8(o_{t+1}^i, a_t^i)$. This assumption is a property of many environments with deterministic state transitions, such as robot locomotion and traffic simulation. In the robot locomotion scenario, each robot's movement can be considered a translation action. Here, the assumption holds as the action taken by the robots (joint action of all robots) uniquely determines their next positions (states). Similarly, in traffic simulation with multiple autonomous vehicles, each vehicle's movement from one position to another is also treated as a translation action.\nLemma 2. For any state $s_t$, its outflows and inflows are calculated as follows:\n$$\u222b_{s \u2208 C(s_t)} F(s_t \u2192 s)ds = \u222b_{a_t \u2208 A} F(s_t, a_t)da = \\prod_{i=1}^{N} \u222b_{a^i \u2208 A^i} F(o_t^i, a_t^i)da^i,$$\nand\n$$\u222b_{s \u2208 P(s_t)} F(s \u2192 s_t)ds = \u222b_{a: T(s,a)=s_t} F(s, a)da = \\prod_{i=1}^{N} \u222b_{a: T(o^i,a^i)=o} F_i(o_t^i, a_t^i)da = \\prod_{i=1}^{N} \u222b_{a: T(o^i,a^i)=o} F_i(G_\u00f8(o_t^i, a_t^i), a^i)da,$$\nwhere $a$ is the unique action that transition to $s_t$ from $s$ and $o^i = G_\u00f8(o_t^i, a^i)$ with $o_t^i = T(o^i, a^i)$.\nIn order to parameterize the Markovian flows, we propose the joint continuous flow matching condition as Lemma 3.\nLemma 3 (Joint Continuous Flow Matching Condition). Consider a non-negative function $F(s_t, a_t)$ taking a state $s_t \u2208 S$ and an action $a_t \u2208 A$ as inputs. Then we have $F$ corresponds to a flow if and only if the following continuous flow matching conditions are satisfied:\n$$\u2200s_t > s_0, F(s_t) = \u222b_{s: T(s,a)=s_t} F(s, a : < s, a : s \u2192 s_t)ds = \\prod_{i=1}^{N} \u222b_{a_i \u2208 A_i} F(o_t^i, a^i) da,$$ and $$\u2200s_t < s_f, F(s_t) = \u222b_{a \u2208 A} F(s_t, a_t)da, = \\prod_{i=1}^{N} \u222b_{a_i \u2208 A_i} F(o_t^i, a^i) da.)$$\nFurthermore, $F$ uniquely defines a Markovian flow $F$ matching $F$ such that\n$$F(\u03c4) = \\frac{\u220f_{t=1}^{T} F(s_{t-1} \u2192 s_t)}{\u220f_{t=0}^{T} F(s_t)}.$$\nProof. The proof is trivial by following the proof of Theorem 1 in Li et al. (2023a).\nRemark 2. Lemma 3 provides the flow matching condition for joint continuous flow. Bengio et al. (2021b) initially introduces the flow matching condition, demonstrating that non-negative functions of states and edges can be used to define a Markovian flow. Then Li et al. (2023a) formalizes this condition for continuous state and action spaces. In multi-agent scenarios, we extend the flow matching condition to Lemma 3, which guarantees the existence of a distinct Markovian flow when the non-negative flow function satisfies the joint flow matching conditions.\nBased on Lemma 3, we propose the following joint continuous loss function to train the flow network:\n$$L(\u03c4) = \u2211_{s_t \u2208 T\\{s_0}} [\u220f_{i=1}^{N} \u222b_{a_i \u2208 A_i} F(o_t^i, a^i) da \u2013 R(s_t) - \u220f_{i=1}^{N} \u222b_{a_i \u2208 A_i} F(o_t^i, a^i) da]^2.$$\nSince the inflows and outflows of states in the continuous control task cannot be calculated directly to complete the flow matching condition, we solve the integral with a sampling-based approach. Specifically, we sample K flows independently and uniformly from the continuous action space $A = A^1 \u00d7 ... \u00d7 A^N$ for approximate estimation and match the sampled flows. Then, we present Lemma 4 proved in Appendix B.3, which shows that the expectation of sampled inflow and outflow is the true inflow and outflow.\nLemma 4. Let ${a_{1,k}, ..., a_{N,k}}_{k=1}^{K}$ be sampled independently and uniformly from the continuous action space $A^1 \u00d7 ... \u00d7 A^N$. Assume $G_\u00f8*$ can optimally output the actual state $o_t^i$ with $(o_{t+1}, a_t^i)$.\nThen for any state $s_t \u2208 S$, we have\n$$E[\\frac{\u03bc(A)}{K}\u2211_{k=1}^{K}\u220f_{i=1}^{N} F(o_t^i, a_{i,k})]= \u222b_{a \u2208 A} F(s, a)da$$\nand\n$$E[\\frac{\u03bc(A)}{K}\u2211_{k=1}^{K}\u220f_{i=1}^{N} F(G_\u00f8(o_t^i, a_{i,k}), a_{i,k})]= \u222b_{a:T(s,a)=s_t} F(s, a)da.$$\nIt is worth noting that, unlike the single-agent task, if we estimate the parent node through network $G_\u00f8$, an accurate $s_t$ can be estimated through $s_{t+1}$ and $a_{t+1}$ (i.e., $s_t = G_\u00f8(s_{t+1}, a_{t+1})$) under the assumption that for any state transition pair $(s_t, s_{t+1})$, there is a unique action $a_t$ such that $T(s_t, a_t) = s_{t+1}$ (Li et al., 2023a). However, an accurate $o_t^i$ cannot be directly estimated through $o_{t+1}^i$ and $a_{t+1}^i$, because the parent node is related to all ${o_{t+1}^i}_{i=1}^N$, which cannot be obtained only through partial observations, i.e., $o_t^i \u2260 G_\u00f8(o_{t+1}^i, a_{t+1}^i)$. Hence, we analyze the convergence of the proposed sampling algorithm under the condition that the estimated parent nodes have errors.\nIn Assumption 2, we assume that the estimation error of the parent node will introduce an error to the flow function, which is related to the number of agents $N$ and the number of sampled actions $K$. Obviously, the larger the number of agents, the smaller the proportion of $o_t^i$ observing the global state, so the larger the error. In addition, the larger the sample size of actions, the smaller the error.\nAssumption 2. Let $G_\u00f8*$ be a function that optimally outputs the actual state $s_t$ with $(s_{t+1}, a_t)$ and $G_\u00f8$ be the neural network that outputs the estimation of $o_t^i$ based on $(o_{t+1}, a_t^i)$. There exist parameters $\u03b1, \u03b2 > 0$ such that the neural network $G_\u00f8$ satisfies\n$$| \u220f_{i=1}^{N} F(G_\u00f8(o_{t+1}^i, a_t^i), a_t^i) - F(G_\u00f8*(s_{t+1}, a_t), a_t) | \u2264 \\frac{N\u03b1}{K^\u03b2}$$\nBased on Assumption 2, we present the following Theorem 1 proved in Appendix B.4.\nTheorem 1. Let ${a_{1,k}, ..., a_{N,k}}_{k=1}^{K}$ be sampled independently and uniformly from the continuous action space $A^1 \u00d7 \u00b7\u00b7\u00b7 \u00d7 A^N$. Assume $G_\u00f8*$ can optimally output the actual state $o_t^i$ with $(o_{t+1}, a_t^i)$. For any bounded continuous action $a \u2208 A$, any state $s_t \u2208 S$ and any $\u03b4 > 0$, we have\n$$P(|\\frac{\u03bc(A)}{K}\u2211_{k=1}^{K}\u220f_{i=1}^{N} F(o_t^i, a_{i,k}) - \u222b_{a \u2208 A} F(s, a)da | > \u03b4) \\newline > \\frac{\u03b4L\u03bc(A)diam(A)}{\\sqrt{K}} \u2264 2 exp(- \\frac{\u03b4^2}{2})$$\nand\n$$P(|\\frac{\u03bc(A)}{K}\u2211_{k=1}^{K}\u220f_{i=1}^{N} F(G_\u00f8(o_t^i, a_{i,k}), a_{i,k}) - \u222b_{a:T(s,a)=s_t} F(s, a)da| > \u03b4)\\newline \\frac{\u03b4L\u03bc(A)[diam(A) + diam(S)]}{\\sqrt{K}} + \\frac{\u03bc(A)N\u03b1}{K^\u03b2} \u2264 2 exp(- \\frac{\u03b4^2}{2}),$$\nwhere $L$ is the Lipschitz constant of the function $F(s_t, a)$, $diam(A)$ denotes the diameter of the action space $A$ and $\u03bc(A)$ denotes the measure of the action space $A."}, {"title": "4.2. MACFN: Training Framework", "content": "As shown in Figure 1, our framework adopts the Centralized Training with Decentralized Execution (CTDE) paradigm. Each agent learns its individual-flow-based policy by optimizing the global-flow-based matching loss of the continuous flow decomposition network. During the execution, the continuous flow decomposition network is removed, and each agent acts according to its local policy derived from its flow function. Our method can be described by three steps: 1) flow sampling; 2) flow decomposition; 3) flow matching.\nFlow Sampling. Since the action space $A = A^1 \u00d7 ... \u00d7 A^N$ are continuous and independent, we use a Monte Carlo integration approach and sample actions from $A^i$ for each agent i uniformly and independently. The individual flow function $F$ is parameterized by $\u03b8$. Then we calculate the corresponding edge flow ${F_\u03b8(o^i, a^i)}$ for flow estimation to approximate the flow distributions. Then, we normalized the flow distributions by the softmax function to obtain action probability distributions. Each agent selects an action according to the probability distributions, forming the joint action $a$. Naturally, actions with higher flows are more likely to be sampled. We repeat this sampling process until the agents reach the final state, obtaining the complete trajectories. In this way, we approximately sample actions based on their corresponding probabilities.\nFlow Decomposition. Then, we introduce the flow decomposition network to deduce the flow contributions of each agent. Specifically, the flow decomposition network enables individual flow networks to learn by backpropagating gradients in the presence of only global rewards rather than from any agent-specific rewards. To calculate the inflows and outflows of the sampled states, we independently and uniformly sample $K$ actions ${a_{i,k}}_{k=1}^K$ from $A^i$ for each agent i. Since the individual flow networks of agents are independent and we sample action independently, we have $K = log_N K$, where N is the number of agents. Then we calculate the individual flow $F_i(o_t, a_k^i), k = 1,..., K$. We sum the sampled $K$ flows for each agent i to obtain an approximation of the integral of the outflows (resp. inflows) of each agent. Then, we multiply the corresponding flows of $N$ agents to obtain an approximation of the outflows (resp. inflows) of the sampled states. The inverse transition network $G_\u00f8$ is used to find the parent sets of agents when calculating the inflows.\nFlow Matching. In order to update the individual flow network of each agent, we calculate the continuous flow-matching loss (Bengio et al., 2021a) based on the approximation of global state outflows (resp. inflows), i.e., for any states $s_t$ except for initial, the flow incoming to $s_t$ must equal to the outgoing flow. For sparse reward environments, $R(s_t) = 0$ if $s_t \u2260 s_f$.\nSince the magnitude of the flows at the root nodes and leaf nodes in the trajectories may be inconsistent, we adopt log-scale operation (Bengio et al., 2021a) to obtain the following loss function:\n$$L(\u03c4, \u03b5; \u03b8) = \u2211_{s_t \u2208 T/S_0} [log(\u03b5 + Inflows) \u2013 log(\u03b5 + Outflows)]^2,$$where\n$$Inflows := \u220f_{i=1}^{N} (\\frac{K}{\u2211_{k=1}^{K} exp F_{log}(G_\u00f8(o_t^i, a_k), a_k)});$$ $$Outflows := R (s_t) + \u220f_{i=1}^{N} (\\frac{K}{\u2211_{k=1}^{K} exp F_{log}(o_t^i, a_k)}).$$To this end, each agent updates its own flow policy by backward propagation gradients through the flow decomposition network, enabling learning in a centralized manner and executing in a decentralized way. In this way, each agent can solely select actions based on individual flow policy, where the joint flow is proportional to the given reward function when interacting with the environment. To make the proposed method clearer for readers, we provide the pseudocode of MACFN in Algorithm 1."}, {"title": "5. Experiments", "content": "To demonstrate the effectiveness of the proposed MACFN, we conduct experiments on several multi-agent cooperative environments with sparse rewards, including the Multi-Agent Particle Environment (MPE) (Lowe et al., 2017; Mordatch and Abbeel, 2018) and Multi-Agent MuJoCo (MAMuJoCo) (Peng et al., 2021). In this section, we first provide the details for the environments. Then the compared methods and parameter settings are introduced. The comparison results of different baselines are reported to evaluate the performance of MACFN.\nFirst, we evaluate the proposed MACFN on various MPE scenarios with sparse reward, including Robot-Navigation-Sparse, Food-Collection-Sparse, and Predator-Prey-Sparse. The visualization of these scenarios is shown in Figure 2. We consider the cooperative setting where a group of agents collaboratively works towards achieving a common goal, and only sparse rewards are provided.\nRobot-Navigation-Sparse is a continuous navigation task with a maximum episode length of 12. This scenario consists of N agents, placed at the starting point of the environment. At each episode, the agents must navigate to different N destinations and obtain rewards based on the minimum distance between the destinations and the agents. In Food-Collection-Sparse, N agents need to occupy N food locations cooperatively. Within the partially observed horizon, agents can observe the relative positions of other agents and food locations. The more food agents occupy, the greater"}]}