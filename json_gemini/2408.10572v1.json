{"title": "A Tutorial on Explainable Image\nClassification for Dementia Stages Using\nConvolutional Neural Network and\nGradient-weighted Class Activation\nMapping", "authors": ["Kevin Kam Fung Yuena"], "abstract": "This paper presents a tutorial of an explainable approach using\nConvolutional Neural Network (CNN) and Gradient-weighted Class Activation\nMapping (Grad-CAM) to classify four progressive dementia stages based on open\nMRI brain images. The detailed implementation steps are demonstrated with an\nexplanation. Whilst the proposed CNN architecture is demonstrated to achieve more\nthan 99% accuracy for the test dataset, the computational procedure of CNN remains\na black box. The visualisation based on Grad-CAM is attempted to explain such\nvery high accuracy and may provide useful information for physicians. Future\nmotivation based on this work is discussed.", "sections": [{"title": "1. Introduction", "content": "Dementia is a general term for loss of memory, language, problem-solving and other\nthinking abilities that are severe enough to interfere with daily life [1]. For example,\nsigns of dementia may include problems with short-term memory, keeping track of a\npurse or wallet, paying bills, planning and preparing meals, remembering appointments,\nand travelling out of the neighbourhood. Among different types of dementia, Alzheimer's\ndisease accounts for 60-80% of cases [1].\nThe dementia's disease progresses in three stages: early, middle and late; or mild,\nmoderate and severe [2] [3]. In the latter stage, the person has more severe brain damage\nand symptoms. Understanding which dementia stage of a person may help to identify\nwhich treatments will be and predict what may occur next. This study presents a tutorial\nto use a deep learning method to learn the open MRI brain images to predict the dementia\nstage of a person."}, {"title": null, "content": "Deep convolutional nets have brought about breakthroughs in processing images,\nvideo, speech and audio, whereas recurrent nets have shone light on sequential data such\nas text and speech [4]. Chollet [5] presents a detailed explanation with examples and\nimplementation with Python for deep learning. The first successful practical application\nof neural nets came in 1989 from Bell Labs, when Yann LeCun combined the earlier\nideas of convolutional neural networks and backpropagation, and applied them to the\nproblem of classifying handwritten digits [5]. Lechun et al. [6] introduced the application\nof convolutional neural networks for classifying handwritten digits.\nRegarding the deep learning techniques applied to medical images, Menze et al. [7]\nreport the multimodal brain tumour image segmentation benchmark for twenty tumour\nsegmentation algorithms that were applied to a set of 65 multi-contrast MR scans. Kamal\net al. [8] conducted Alzheimer's patient analysis using Image and Gene Expression\nData and Explainable-AI to present associated genes. Bae et al. [9] applied a\nconvolutional neural network model based on T1-weighted magnetic resonance imaging\nto Identify Alzheimer's disease. Qiu et al. [10] presented multimodal deep learning to\nassess Alzheimer's disease dementia. Marmolejo-Saucedo and Kose [11] applied Grad-\nCam Based Explainable Convolutional Neural Network for brain tumour diagnosis.\nThe contribution of the rest of this article is presented below.\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\nData exploration and manipulation of the MRI brain image data are presented\nin section 3. The image structure, data splitting process and class distribution\nare presented.\nThe explanation and implementation of convolutional neural networks are\noffered in section 4. A numerical example is shown to explain the meaning of\nCNN layers used in this paper. Input and output shapes and Parameters are\ndiscussed as they are the essential requirements to set a CNN model.\nThe gradient-weighted Class Activation Mapping (Grad-CAM), a visual\nexplanation technique for the CNN model, is presented in section 5. The\nalgorithms to plot the heatmap visualisation are presented.\nThe simulation environment settings and results are presented in section 5.\nTraining convergence and classification metrics are presented. The Grad-CAM\nvisualisation of CNN predictions for both correct and incorrect instances is\npresented.\nConclusion remarks of this study and future motivations based on this study are\npresented in Section 6."}, {"title": "2. Data exploration and manipulation", "content": "The Alzheimer dataset is obtained from [12]. The Dataset comprises pre-processed MRI\n(Magnetic Resonance Imaging) images in jpg format of 128 x 128 pixels. A total of 6400\nMRI images are divided into four classes presenting four stages of Alzheimer's or\ndementia: non-dementia (3200 images), very mild dementia (2240 images), mild\ndementia (896 images), and moderate dementia (64 images). As the moderate dementia\ngroup only occupies 1% of the total instances of the dataset and the non-dementia group\noccupies 50% of the total instances of the dataset, the dataset is imbalanced. Without\nusing some sampling techniques, such as downsampling and upweighting, the simulation\nshows that the proposed CNN method can handle imbalanced classification properly."}, {"title": null, "content": "Since the dataset is well pre-processed, augmentation of the image is not needed in this\nstudy. The sample instance is visualised in Figure 1.\nThe dataset is split into three subsets: training, validation and test datasets using the\nPython splitfolders package [13]. The ratio of 80%, 10% and 10% is set for each subset.\nThe seed number is set to 888 for the function for the reproducibility of the same content\nof the folders. The size of the data subjects after the train-validation-test split is shown\nin Table 1, i.e., 5119, 639, and 642 different images for each subset. The distribution of\nthe four stages in each subset is shown in Figure 2. The proportions for each class in each\nsubject are similar."}, {"title": "3. Convolutional Neural Network", "content": null}, {"title": "3.1. Slim CNN Structure", "content": "Deep learning is an Artificial Neural Network (ANN) of layers and each layer consists\nof many neurons. The word \"deep\" is not related to a deep understanding of human mind\nbut the depth of an ANN mathematical structure, such as the large scale of the parameters\nin the network [14]. A convolutional Neural Network is a type of deep learning network\nthat includes at least one convolutional layer(s), pooling layers, and dense (or fully\nconnected) layers. CNN is typically used for image recognition. Keras [15,16] built on\ntop of TensorFlow [17], the Python deep learning API, is utilised to implement the CNN\nmodel. A tutorial for the RGB colour images using TensorFlow can be found on the\nwebsite\u00b2. CNN settings for this study are mainly based on the grey images due to the\nnature of the MRI image.\nConstructing a CNN model using layers is like playing with Lego to construct\nbuildings using different blocks. Therefore, there are so many types of CNN models,\nsuch as AlexNet [18], VGGNet [19], GoogleNet [20], ResNet [21] [22], \u0425ception [23].\nTo simplify the introduction of notions of CNN, this study proposes a slim CNN structure\nbased on Python code shown in Figure 5, which is much less complicated than the\npopular ones but achieves more than 99% accuracy for this study. The network consists\nof one input layer, three 2D convolutional (Conv2D) layers, two 2D max pooling\n(Maxpooling2D) layers, one Flatten layer, one hidden Dense layer, and one output Dense\nlayer. The implementation code of layers can be found in the documentation\u00b3. Glossaries\nfor learning machine learning can be found on the Google developer website4.The\nnotions of layers and related parameters are briefly explained with examples in the next\nsections."}, {"title": "3.2. Explanation of CNN Layers", "content": "This example demonstrates the basic concept for computation based on the network\ncomprising each consequent layer of input, Conv2D, Maxpooling2D, Flatten, and output.\nFor the input layer, given that the data from 10 to 34 with the shape of (5,5,1), a 5x5\nmatrix is shown below.\nAssume that the weights of the (3,3) filter Kernal are from 0.1 to 0.9, as below.\nFor the output of a Conv2D layer, the convolution matrix with (1,1)-strides is computed\nas below.\nThe computation for a convolution matrix is based on the slices of the input matrix with\nthe strides. The slice size is based on the shape of the filter kernel. For example, the entry\nof 86.1 of the convolutional matrices is calculated by the form below.\nRegarding the Maxpooling2D layer, with a pool size of (2, 2) and strides of (1, 1), the\nmax pool for the output of the convolution matrix is computed below.\nStrides of (1, 1) mean moving the slice (filter) one step (column) to the left if not reaching\nthe end of the columns and moving the slice one step (row) down to the start of the\ncolumn if reaching the end of the columns. For the entry of 135.6, the calculation is\nshown below.\nFor the Flatten layer, the data form is transformed from 2D matrix to a 1D vector,\nRegarding the output dense layer, supposed that there are two classes, therefore, two\nnodes. Each node has different weights such that the Flatten layer to Node A is\n[0.3,0.2,0.4,0.1] and the Flatten layer to Node B is [0.2,0.2,0.5,0.1]. Therefore, the output\nvalue after the weighted sum of the two vectors respectively is [121.2, 123.45].\nTentatively, the input is predicted to be Class B."}, {"title": "3.3. Input/Output Shapes and Parameters", "content": "The name for deep learning may be regarded as another branding name for artificial\nneural network (ANN) due to the large scale of parameters of the more complicated\nstructure to be explored, which was very hard to find the optimal values based on the\nlegacy machine, but much earlier to do with today's hardware technology. Table 2 shows\nthe summary of the Convolution Neural Network Structure shown in Figure 5. There are\n52,268,036 total parameters, which are all trainable. The word \u201cTrainable\u201d may mean\nfinding a good-enough (ideally best) value for a parameter, and thus CNN is a heuristic\nalgorithm. Input, maxPooling2D, and flatten layers do not have any tunning parameters\nfor operations. The output of the previous layer is the input of the current layer.\nFor the input layer, the input and output have the shape of (batch, height, width, and\ndepth). \"None\" elements in the shape represent dimensions where the shape is not known\nand may vary. The batch size is not defined and is subject to the variable. For a grey\nimage, the channel value is 1. For an RGB colour image, the channel is 3. The output\nshape is the same as the input shape of (None, 128, 128, 1), which is an image with a\nheight of 128, width of 128, and depth (or channel) of 1.\nFor the Conv2d parameters, the output size is\n```latex\nConv2dSize = \\frac{inputSize-filterSize+2\\times paddingSize}{strideSize}+1\n```\nSince stride size is one and padding size is valid, i.e., zero, by default, the default\nsize of Conv2D is as below.\n```latex\nconv2dSize = inputSize \u2013 filterSize + 1\n```\nFor the first Conv2D layer, the output shape of (height and width) is"}, {"title": null, "content": "Since the filter number is set to 128 filters, the output shape is (None, 126, 126,\n128). The total number of parameters is calculated in the form below.\n```latex\nConv2d parameters = Filter parameters + Bias parameters\n```\nThus, the number of parameters is\nFor each kernel filter, (3 \u00d7 3 \u00d7 1) weights must be optimised, and there are 128\nfilters and 128 bias parameters.\nFor the first MaxPooling2D layer, the input to max pooling is the output of conv2d\nwhen the input shape is larger than or equal to the pool size. The output of height and\nwidth is calculated in the form below.\n```latex\nMaxPooling2dSize = \\frac{inputShape - poolSize}{strides}+1\n```\nStrides are set to none by default, which means the same as the pool size. The size\nof height and width is\nFor the second Conv2D layer, the input shape of height and width is\nAs the depth is 128 in previous layers and there are 256 filters, the number of parameters\nis\nFor the second MaxPooling2D layer, the input shape of height and width is\nFor the third Conv2D layer, the input shape of height and width is\nAs both depth and filters are 256, the number of parameters is\nFor the Flatten layer, the input vector shape is\nFor the hidden dense layer, the number of neurons is set to 256. The number of\nbiases is 256. The number of parameters is"}, {"title": null, "content": "For the output dense layer, the number of neurons is set to 4. The number of biases\nis 4. The number of parameters is\nFinally, the total number of trainable parameters is\nAs the computational workload to optimise more than 52 million parameters is very high\nfor this case, that's the name for deep learning instead of ANN. Understanding the\ncalculations above can help us configure the right hyper-parameters for settings."}, {"title": "4. Gradient-weighted Class Activation Mapping", "content": "The gradient-weighted Class Activation Mapping (Grad-CAM) is a 'visual explanations'\ntechnique for decisions from a large class of CNN-based models, making them more\ntransparent and explainable [24,25]. Grad-CAM applies the gradients of a target concept\nto the final convolutional layer to produce a coarse localisation map to highlight the\nimportant regions in the image for predicting the concept.\nImplementation of Grad-CAM for the proposed CNN model is slightly revised from\nthe demo code [5], which is also shown in the Keras web. Further modification of the\nalgorithm is needed for the proposed CNN based on the structure mentioned in Sect. 4\nand the MRI image is in grey channel instead of RGB channels. The Grad-CAM\nvisualisation is summarised in Algorithms 1-3. In general, Algorithm 3 calls Algorithm\n2 calling Algorithm 1. Figures 10 and 11 are the examples of the results from using\nAlgorithm 3. The colourmap is produced by matplotlib. Figure 6 shows the Grey and\njet heatmap gradient colourmaps used in this paper."}, {"title": null, "content": "Algorithm 1: Grad-CAM Heatmap Algorithm (makeGradcamHeatmap)\nInput: Image array, CNN Model, name of last convolution layer\n1. create a Keras model that maps the input image to the activations of the last conv\nlayer as well as the output predictions\n1. compute the gradient of the top predicted class for the input image array with\nrespect to the activations of the last conv layer\n2. compute the gradient of the output neuron (top predicted or chosen) regarding the\noutput feature map of the last conv layer\n3. compute a vector of each entry of the mean intensity of the gradient over a\nspecific feature map channel\n4. multiply each channel in the feature map array by \"how important this channel\nis\" regarding the top predicted class, then sum all the channels to obtain the\nheatmap class activation\n5. Normalize the heatmap between 0 and 1\nReturn: heatmap array"}, {"title": null, "content": "Algorithm 2: Grad-CAM Display Algorithm (GradcamDisplay)\nInput: Image, CNN Model, name of last convolution layer\n1. Convert the image file into array data\n2. Preprocessing the image array by inserting a new dimension\n3. Create a Gra-CAM heatmap by calling Algorithm 1.\n4. Rescale heatmap to a range between 0 and 255\n5. Set a colour map scheme such as Jet.\n6. Convert the scaled heatmap value\n7. Create a colormap-heatmap image with RGB colourised heatmap, e.g., an image\nwith RGB colourised heatmap\n8. Superimpose the heatmap on the original image by\nsuperimposed_img = colormap_heatmap * alpha + img\nReturn: raw grey heatmap in step 3, colormap-heatmap image, superimposed image"}, {"title": null, "content": "Algorithm 3: Grad-CAM Explainability Plot Algorithm (PlotCases)\nInput: cases of images, m =4\n1. If the case is supplied, the case is used, and the m value is updated. Otherwise,\nrandomly select m cases.\n2. Create a plot matrix with 3 columns and m rows\n3. For each row, plot the original, grey heatmap, jet heatmap and prediction\nimages for each column by calling algorithm 2.\nReturn: an image including original, grey heatmap, jet heatmap and prediction sub-\nimages selected or randomised."}, {"title": "5. Simulation and Discussions", "content": "The code is developed in Python Jupyter Notebook. The source code of this study is\navailable in the author's GitHub7. The program was executed on the machine Lenovo\nLegion Pro 5 16IRX8 with i9-13900HX CPU and 32 GB RAM. As the details are\nsummarised in Table 1, 5119 images (80% of the total) are used to train the proposed\nCNN model, 639 images (10% of the total) are used to validate the model, and 642\nimages (10% of the total) are used to test the model performance. The distribution for\neach class is imbalanced."}, {"title": "6. Conclusion", "content": "This study introduces a slim structure of CNN comprising only nine layers, which\nproduces more than 99% accuracy for dementia stage prediction using the open dataset\nin Kaggle. The layers and parameter settings are explained with examples. As the\nexplainability of an almost perfect solution is unclear in CNN, this study presents the use\nof Grad-CAM to visualise the explainability of the proposed CNN. Whilst the focus of\nthis study is mainly based on deep learning classification, the major limitations are that\nthe patterns and implications of medical findings based on visualisation need to be further\nexplored and evaluated, which may be a motivation for future medical research. This\nstudy presents a tutorial of the practical application with detail implementation and\nexplanation. Future works may include the exploration and comparisons of different\nCNN structures with different explainable visualisation methods."}, {"title": "Code availability", "content": "The work for the study will be available on the author's GitHub,"}]}