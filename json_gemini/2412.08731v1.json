{"title": "FROM MLP TO NEOMLP: LEVERAGING SELF-ATTENTION FOR NEURAL FIELDS", "authors": ["Miltiadis Kofinas", "Samuele Papa", "Efstratios Gavves"], "abstract": "Neural fields (NeFs) have recently emerged as a state-of-the-art method for encoding spatio-temporal signals of various modalities. Despite the success of NeFs in reconstructing individual signals, their use as representations in downstream tasks, such as classification or segmentation, is hindered by the complexity of the parameter space and its underlying symmetries, in addition to the lack of powerful and scalable conditioning mechanisms. In this work, we draw inspiration from the principles of connectionism to design a new architecture based on MLPs, which we term NeoMLP. We start from an MLP, viewed as a graph, and transform it from a multi-partite graph to a complete graph of input, hidden, and output nodes, equipped with high-dimensional features. We perform message passing on this graph and employ weight-sharing via self-attention among all the nodes. NeoMLP has a built-in mechanism for conditioning through the hidden and output nodes, which function as a set of latent codes, and as such, NeoMLP can be used straight-forwardly as a conditional neural field. We demonstrate the effectiveness of our method by fitting high-resolution signals, including multi-modal audio-visual data. Furthermore, we fit datasets of neural representations, by learning instance-specific sets of latent codes using a single backbone architecture, and then use them for downstream tasks, outperforming recent state-of-the-art methods. The source code is open-sourced at https://github.com/mkofinas/neomlp.", "sections": [{"title": "1 INTRODUCTION", "content": "The omnipresence of neural networks in the last decade has recently given rise to neural fields (NeFs) (cf. Xie et al. (2022)) as a powerful and scalable method for encoding continuous signals of various modalities. These range from shapes (Park et al., 2019), scenes (Mildenhall et al., 2020), and images, (Sitzmann et al., 2020), to physical fields (Kofinas et al., 2023), CT scans (Papa et al., 2023; de Vries et al., 2024), and partial differential equations (Yin et al., 2022; Knigge et al., 2024). Consequently, the popularity of neural fields has spurred interest in neural representations, i.e. using NeFs as representations for a wide range of downstream tasks.\nExisting neural representations, however, suffer from notable drawbacks. Representations based on unconditional neural fields, i.e. independent multi-layer perceptrons (MLPs) fitted per signal, are subject to parameter symmetries (Hecht-Nielsen, 1990), which lead to extremely poor performance in downstream tasks if left unattended (Navon et al., 2023). Many recent works (Navon et al., 2023; Zhou et al., 2023; Kofinas et al., 2024; Lim et al., 2024a; Papa et al., 2024) have proposed architectures that respect the underlying symmetries; the performance, however, leaves much to be desired. Another line of works (Park et al., 2019; Dupont et al., 2022) has proposed conditional neural fields with a single latent code per signal that modulates the signal through concatenation, FiLM (Perez et al., 2018), or hypernetworks (Ha et al., 2016), while, recently, other works (Sajjadi et al., 2022; Wessels et al., 2024) have proposed set-latent conditional neural fields-conditional neural fields with a set of latent codes\u2014that condition the signal through attention (Vaswani et al., 2017). Whilst the study of Rebain et al. (2022) showed that set-latent neural fields outperform single latent code methods as conditioning mechanisms, existing set-latent neural fields are based on cross-attention, which limits their scalability and expressivity: coordinates are only used as queries in attention, and cross-attention is limited to a single layer.\nWe argue that many of these drawbacks stem from the lack of a unified native architecture that integrates the necessary properties of neural representations and eliminates the shortcomings of current approaches. To address these concerns, we draw inspiration from connectionism and the long history of MLPs to design a new architecture that functions as a standard machine learning model-akin to an MLP\u2014as well as a conditional neural field. The paradigm of neural networks, from the early days of Perceptron (McCulloch & Pitts, 1943), to MLPs with hidden neurons trained with backpropagation (Rumelhart et al., 1986), to modern transformers (Vaswani et al., 2017), shares the connectionist principle: cognitive processes can be described by interconnected networks of simple and often uniform units.\nThis principle is lacking from current conditional neural field architectures, since conditioning is added to the network as an ad-hoc mechanism. In contrast, motivated by this principle, we take a closer look at MLPs; more specifically, we look at MLPs as a graph- similar to a few recent works (Kofinas et al., 2024; Lim et al., 2024a; Nikolentzos et al., 2024)- and design a novel architecture that operates on this graph using message passing. First, we convert the graph from a multi-partite graph to a fully-connected graph with self-edges. Instead of using edge-specific weights, we employ weight-sharing via self-attention among all the nodes. We initialize the hidden and output nodes with noise and optimize their values with backpropagation. Finally, we use high-dimensional features for all nodes to make self-attention and the network as a whole more scalable.\nWe make the following contributions. First, we propose a new architecture, which we term NeoMLP, by viewing MLPs as a graph, and convert this graph to a complete graph of input, hidden, and output nodes with high-dimensional features. We employ message passing on that graph through self-attention among the input, hidden, and output nodes. The hidden and output nodes can be used as a learnable set of latent codes, and thus, our method can function as a conditional neural field. We introduce new neural representations that use sets of latent codes for each signal, which we term v-reps, as well as datasets of neural representations, which we term v-sets. We fit datasets of signals using a single backbone architecture, and then use the latent codes for downstream tasks, outperforming recent state-of-the-art methods. We also demonstrate the effectiveness of our method by fitting high-resolution audio and video signals, as well as multi-modal audio-visual data."}, {"title": "2 BACKGROUND ON NEURAL FIELDS", "content": "Neural fields Neural fields (NeFs), often referred to as Implicit Neural Representations (INRs), are a class of neural networks that parameterize fields using neural networks (cf. Xie et al. (2022)). In their simplest form, they are MLPs that take as input a single coordinate (e.g. an x y coordinate) and output the field value for that coordinate (e.g. an RGB value). By feeding batches of coordinates to the network, and training to reconstruct the target values with backpropagation, the neural field learns to encode the target signal, without being bound to a specific resolution.\nConditional neural fields introduce a conditioning mechanism to neural fields through latent variables, often referred to as latent codes. This conditioning mechanism can be used to encode instance-specific information (e.g. encode a single image) and disentangle it from the backbone architecture, which now carries dataset-wide information."}, {"title": "3 NEOMLP", "content": ""}, {"title": "3.1 FROM MLP TO NEOMLP", "content": "We begin the exposition of our method with MLPs, since our architecture is influenced by MLPs and builds on them. Without loss of generality, a multi-layer perceptron takes as input a set of scalar variables {xi}=1, xi \u2208 R, coalesced into a single high-dimensional array x \u2208 RI. Through a series of non-linear transformations, the input array is progressively transformed into intermediate (hidden) representations, with the final transformation leading to the output array y \u2208 RO.\nAkin to other recent works (Kofinas et al., 2024; Lim et al., 2024b; Nikolentzos et al., 2024), we look at an MLP as a graph; an MLP is an L + 1-partite graph, where L is the number of layers. The nodes represent the input, hidden, and output neurons, and have scalar features that correspond to individual inputs, the hidden features at each layer, and the individual outputs, respectively. We perform message passing on that graph, after making it more amenable for learning. First, we convert the connectivity graph from an L + 1-partite graph to a fully-connected graph with self-edges. Since the forward pass now includes message passing from all nodes to all nodes at each step, we create learnable parameters for the initial values of the hidden and output node features. We initialize them with Gaussian noise, and optimize their values with backpropagation, simultaneously with the network parameters. Next, we observe that having dedicated edge-specific weights for all node pairs would result in an intractable spatial complexity. As such, in order to reduce the memory footprint, we follow the standard practice of graph neural networks and Transformers (Vaswani et al., 2017), and employ weight-sharing between the nodes, specifically via self-attention. In other words, the weights for each node pair are computed as a function of the incoming and outgoing node features, in conjunction with weights that are shared across nodes. As a by-product of the self-attention mechanism, which is permutation invariant, we use node-specific embeddings that allow us to differentiate between different nodes. Finally, instead of having scalar node features, we increase the dimensionality of node features, which makes self-attention more scalable and expressive."}, {"title": "3.2 \u039d\u0395\u039fMLP ARCHITECTURE", "content": "After establishing the connection with MLPs, we now discuss the architecture of our method in detail. The inputs comprise a set of scalar variables {xi}{\u22121, xi \u2208 R. We employ random Fourier features (Tancik et al., 2020) as a non-learnable method to project each scalar input (each dimension separately) to a high-dimensional space RDRFF. This is followed by a linear layer that projects it to RD. We then add learnable positional embeddings to the inputs. These embeddings are required for the model to differentiate between input variables, since self-attention is a permutation invariant operation. We use similar learnable embeddings for each scalar output dimension (referred to as output embeddings), as well as H learnable embeddings for each hidden node (referred to as hidden embeddings), where H is chosen as a hyperparameter. We concatenate the transformed inputs with the hidden and output embeddings along the node (token) dimension, before feeding them to NeoMLP. We denote the concatenated input, hidden, and output tokens as T(0) \u2208 R(I+H+O)\u00d7D, where O is the number of output dimensions. The input, hidden, and output embeddings are initialized with Gaussian noise. We use a variance of 1 for the input embeddings and 2 for the hidden and output embeddings; both are chosen as hyperparameters.\nEach NeoMLP layer comprises a multi-head self-attention layer among the tokens, and a feed-forward network that non-linearly transforms each token independently. The output of each layer consists of the transformed tokens T(l) \u2208 R(I+H+O)\u00d7D. We use pre-LN transformer blocks (Xiong et al., 2020), but we omit LayerNorm (Ba et al., 2016), since we observed it does not lead to better performance or faster convergence. This also makes our method conceptually simpler. Thus, a NeoMLP layer is defined as follows:\nAfter L NeoMLP layers, we only keep the final tokens that correspond to the output embeddings, and pass them through a linear layer that projects them back to scalars. We then concatenate all outputs together, which gives us the final output array y \u2208 RO. The full pipeline of our method is shown in"}, {"title": "3.3 NEOMLP AS AN AUTO-DECODING CONDITIONAL NEURAL FIELD", "content": "One of the advantages of our method is its adaptability, since it has a built-in mechanism for conditioning, through the hidden and output embeddings. In the context of neural fields, this mechanism enables our method to function as an auto-decoding conditional neural field (Park et al., 2019), while the embeddings can be used as neural representations for downstream tasks, shown schematically in Figure 3. We refer to these representations as v-reps (nu-reps), and similarly, we refer to the datasets of neural representations obtained with our method as v-sets (nu-sets).\nAs a conditional neural field, the NeoMLP backbone encodes the neural field parameters, while the latent variables, i.e. the hidden and output embeddings, encode instance-specific information. Each instance (e.g. each image in an image dataset) is represented with its own set of latent codes Zn = [{h}1{}=1]. We optimize the latent codes for a particular signal by feeding them to the network as inputs alongside a coordinate x\"), compute the field value \u0177n \u0177n) and the re- construction loss, and backpropagate the loss to Zn to take one optimization step.\nOur method operates in two distinct stages: fitting and finetuning. During fitting, our goal is to optimize the backbone architecture, i.e. the parameters of the model. We sample latent codes for all the signals of a fitting dataset and optimize them simultaneously with the backbone architecture. When the fitting stage is complete, after a predetermined set of epochs, we freeze the parameters of the backbone architecture and discard the latent codes. Then, during finetuning, given a new signal, we sample new latent codes for it and optimize them to minimize the reconstruction error for a number of epochs. We finetune the training, validation, and test sets of the downstream task from scratch, even if we used the training set to fit the model, in order to make the distance of representations between splits as small as possible.\nIn both the fitting and the finetuning stage, we sample completely random points from random signals. This ensures i.i.d. samples, and speeds up the training of our method. During the fitting stage, we also sample points with replacement, as we observed a spiky behaviour in the training loss otherwise. We provide the detailed algorithms of the fitting and the finetuning stage in Algorithm 1, and Algorithm 2 in Appendix A, respectively. We provide further implementation details in Appendix D.\""}, {"title": "3.4 USING V-REPS FOR DOWNSTREAM TASKS", "content": "After finetuning neural representations, our goal is to use them in downstream tasks, e.g. to train a downstream model for classification or segmentation. Our v-reps comprise a set of latent codes for each signal, corresponding to the finetuned hidden and output embeddings. While the space of v-reps is subject to permutation symmetries, which we discuss in Appendix B, we use a simple downstream model that first concatenates and flattens the hidden and output embeddings in a single vector, and then process it with an MLP. We leave more elaborate methods that exploit the inductive biases present in v-reps for future work."}, {"title": "4 EXPERIMENTS", "content": "We gauge the effectiveness of our approach by fitting individual high-resolution signals, as well as datasets of signals. We also evaluate our method on downstream tasks on the fitted datasets. We refer to the appendix for more details. The code is included in the supplementary material and is open-sourced to facilitate reproduction of the results."}, {"title": "4.1 FITTING HIGH-RESOLUTION SIGNALS", "content": "First, we evaluate our method at fitting high-resolution signals. We compare our method against Siren (Sitzmann et al., 2020), an MLP with sinusoidal activations, RFFNet (Tancik et al., 2020), an MLP with random Fourier features and ReLU activations, and SPDER (Shah & Sitawarin, 2024), an MLP with sublinear damping activations combined with sinusoids. Our goal is to assess the effectiveness of our method in signals of various modalities, and especially in multimodal signals, which have been underexplored in the context of neural fields. Hence, we choose signals that belong to two different modalities, namely an audio clip and a video clip, as well as a multi-modal signal, namely video with audio.\nFor audio, we follow Siren (Sitzmann et al., 2020) and use the first 7 seconds from Bach's cello suite No. 1 in G Major: Prelude. The audio clip is sampled at 44.1 kHz, resulting in 308,700 points.\nFor video, we use the \u201cbikes\u201d video from the scikit-video Python library, available online.\nThis video clip lasts for 10 seconds and is sampled at 25 fps, with a spatial resolution of 272 \u00d7 640, resulting in 43,520,000 points. Finally, we explore multimodality using the \u201cBig Buck Bunny\u201d video from scikit-video. This clip lasts for 5.3 seconds. The audio is sampled at 48 kHz and has 6 channels. The original spatial resolution is 1280 \u00d7 720 at 25 fps. We subsample the spatial resolution by 2, which results in a resolution of 640 \u00d7 360. Overall, this results in 30,667,776 points (254,976 from audio and 30,412,800 from video).\nTraining details For audio, we follow Siren (Sitzmann et al., 2020) and scale the time domain to t \u2208 [-100, 100] instead of [-1,1], to account for the high sampling rate of the signal. For the audio-visual data, we model the signal as f : R\u00b3 \u2192 R\u00ba, i.e. we have 3 input dimensions (x, y, t), and 9 output dimensions: 3 from video (RGB) and 6 from audio (6 audio channels). Similar to the audio clip, we also scale the time domain, which is now used as the time coordinate for both the audio and the video points. For the points corresponding to audio, we fill their xy coordinates with zeros. Furthermore, since all points come from either the video or the audio modality, we fill the output dimensions that correspond to the other modality with zeros. Finally, during training, we mask these placeholder output dimensions, i.e. we compute the loss for the video coordinates using only the RGB outputs, and the loss for the audio coordinates using only the 6-channel audio outputs.\nTo ensure fairness, for every signal, NeoMLP has approximately the same number of parameters as Siren. We describe the architecture details for each experiment in Appendix E. We show the results in Table 1, measuring the reconstruction PSNR. We observe that NeoMLP comfortably outperforms Siren in all three signals. Interestingly, the performance gap is increased in the more difficult setup of multimodal data, which suggests the suitability of our method for multimodal signals. We hypothesize that this can be attributed to our method's ability to learn faster from minibatches with i.i.d. elements, which is something we observed empirically during training and hyperparameter tuning. We visualize example frames for the video clip in Figure 4. We provide further qualitative results in Appendix G and include reconstructions for all three signals in the supplementary material."}, {"title": "4.2 FITTING V-SETS & DOWNSTREAM TASKS ON V-SETS", "content": "Next, we evaluate our method on fitting v-sets, i.e. fitting datasets of neural representations of signals with NeoMLP, as well as performing downstream tasks on v-sets. We compare our method against Functa (Dupont et al., 2022), DWSNet (Navon et al., 2023), Neural Graphs (Kofinas et al., 2024), and Fit-a-NeF (Papa et al., 2024). Functa is a conditional neural field that uses an MLP backbone and conditioning by bias modulation. DWSNet, Neural Graphs, and Fit-a-NeF, on the other hand, are equivariant downstream models for processing datasets of unconditional neural fields. For these three methods, the process of creating datasets of neural representations corresponds to fitting separate MLPs for each signal in a dataset, a process that is independent of the downstream models themselves. Since these methods have the step of generating the neural datasets in common, we use shared datasets for these methods, which are provided by Fit-a-NeF.\nWe consider three datasets, namely MNIST (LeCun et al., 1998), CIFAR10 (Krizhevsky et al., 2009), and ShapeNet10 (Chang et al., 2015). We evaluate reconstruction quality for MNIST and CIFAR10 with PSNR, and for ShapeNet with IoU. For CIFAR10, we follow the setup of Functa (Dupont et al., 2022), and use 50 augmentations for all training and validation images during finetuning. For all datasets, we only use the training set as a fitting set, since this closely mimics the real-world conditions for auto-decoding neural fields, namely that test set data can appear after the backbone is frozen, and should be finetuned without changing the backbone.\nAfter fitting the neural datasets, we optimize the downstream model for the downstream tasks, which corresponds to classification for MNIST, CIFAR10, and ShapeNet10. We perform a hyperparameter search for NeoMLP to find the best downstream model. Specifically, we use Bayesian hyperparameter search from Wandb (Biewald, 2020) to find the best performing hyperparameters for CIFAR10, and reuse these hyperparameters for all datasets.\nWhile neural datasets can easily reach excellent reconstruction quality, it is often at the expense of representation power. This was shown in the case of unconditional neural fields by Papa et al. (2024), where the optimal downstream performance was often achieved with medium quality reconstructions. Since our goal in this experiment is to optimize the performance of neural representations in down- stream tasks, we report the reconstruction quality of the models that achieved the best downstream performance.\nWe report the results in Table 2. We observe that NeoMLP comfortably outperforms DWSNet (Navon et al., 2023), Neural Graphs (Kofinas et al., 2024) and Fit-a-NeF (Papa et al., 2024), i.e. all methods that process unconditional neural fields, both in terms of representation quality and downstream performance. Further, these two quantities seem to be positively correlated for NeoMLP, in contrast to the findings of Papa et al. (2024) for unconditional neural fields. Our method also outperforms Functa (Dupont et al., 2022) on all three datasets regarding the classification accuracy, while maintaining an excellent reconstruction quality."}, {"title": "4.3 ABLATION STUDIES", "content": "Importance of hyperparameters We perform a large ablation study to assess the importance of the latent codes, and the impact of the duration of fitting and finetuning to the quality of reconstruction and representation power. Specifically, we run two studies on CIFAR10; the first study monitors the number and the dimensionality of the latent codes, as well as the number of finetuning epochs. The second study monitors the number and the dimensionality of the latent codes, as well as the number of fitting epochs. In both studies, all other hyperparameters are fixed. We report the fitting PSNR, the test PSNR and the downstream accuracy. We summarize our findings in Tables 3 and 4.\nIn both studies, we observe that increasing the number of latents and their dimensionality also increases the reconstruction quality. However, the higher number of latents seems to lead to decreased downstream performance. Furthermore, we notice that increasing the number of finetuning epochs also increases the test PSNR and accuracy. Finally, somewhat surprisingly, while fitting for more epochs leads to noticeably better fitting PSNR, this translates to negligible gain in the test PSNR and accuracy, and even degrades performance in some cases.\nImportance of RFF As shown by Rahaman et al. (2019), neural networks suffer from spectral bias, i.e. they prioritize learning low frequency components, and have difficulties learning high frequency functions. We expect that these spectral biases would also be present in NeoMLP if left unattended. To that end, we employed Random Fourier Features (RFF) (Tancik et al., 2020) to project our scalar inputs to higher dimensions. Compared to alternatives like sinusoidal activations (Sitzmann et al., 2020), RFFs allow our architecture to use a standard transformer.\nTo examine the spectral bias hypothesis, we train NeoMLP without RFF, using a learnable linear layer instead. We train this new model on the \"bikes\" video, and on MNIST. We present the results in Table 5. The study shows that RFFs clearly help with reconstruction quality, both in reconstructing a high-resolution video signal, and on a dataset of images. Interestingly, the reconstruction quality drop from removing RFFs does not translate to downstream performance drop, where, in fact, the model without Fourier features is marginally better than the original."}, {"title": "5 RELATED WORK", "content": "Neural representations An increasingly large body of works (Navon et al., 2023; Zhou et al., 2023; Kofinas et al., 2024; Lim et al., 2024a; Papa et al., 2024; Tran et al., 2024; Kalogeropoulos et al., 2024) has proposed downstream methods that process datasets of unconditional neural fields, i.e. the parameters and the architectures of MLPs. They are all addressing the parameter symmetries present in MLPs, and while the performance of such methods is constantly increasing, it still leaves much to be desired. Closer to our work is another body of works (Park et al., 2019; Dupont et al., 2022; Sajjadi et al., 2022; Chen & Wang, 2022; Zhang et al., 2023; 2024; Wessels et al., 2024) that proposes neural representations through conditional neural fields. Of those, Sajjadi et al. (2022); Zhang et al. (2023); Wessels et al. (2024) have proposed set-latent conditional neural fields that condition the signal through attention (Vaswani et al., 2017). Zhang et al. (2023) proposed 3DShape2VecSet, an architecture that employs cross-attention and self-attention to encode shapes into sets of latent vectors and decode them. Our method differs from this method, since it does not rely on cross-attention to fully encode a coordinate in a set of latents. Instead, it employs self-attention, which allows for better information propagation and enables the model to scale to multiple layers.\nMLPs as graphs A few recent works (Kofinas et al., 2024; Lim et al., 2024a;b; Nikolentzos et al., 2024; Kalogeropoulos et al., 2024) have explored the perspective of viewing neural networks as graphs and proposed methods that leverage the graph structure. Kofinas et al. (2024) focus on the task of processing the parameters of other neural networks and represent neural networks as computational graphs of parameters. Their method includes applications to downstream tasks on neural fields. Lim et al. (2024b) investigate the impact of neural parameter symmetries, and introduce new neural network architectures that have reduced parameter space symmetries. Nikolentzos et al. (2024) show that MLPs can be formalized as GNNs with asynchronous message passing, and propose a model that employs a synchronous message passing scheme on a nearly complete graph. Similar to this work, we also use a complete graph and employ a synchronous message passing scheme. In contrast to this work, we employ weight-sharing via self-attention and high-dimensional node features. Further, we focus on neural field applications instead of tabular data, and explore conditioning via the hidden and output embeddings."}, {"title": "6 CONCLUSION", "content": "In this work, we presented NeoMLP, a novel architecture inspired by the principles of connectionism and the graph perspective of MLPs. We perform message passing on the graph of MLPs, after transforming it to a complete graph of input, hidden, and output nodes equipped with high-dimensional features. We also employ weight-sharing through self-attention among all the nodes. NeoMLP is a transformer architecture that uses individual input and output dimensions as tokens, along with a number of hidden tokens. We also introduced new neural representations based on the hidden and output embeddings, as well as datasets of neural representations. Our method achieves state-of- the-art performance in fitting high-resolution signals, including multimodal audio-visual data, and outperforms state-of-the-art methods in downstream tasks on neural representations.\nLimitations Our v-reps are subject to permutation symmetries, indicating that inductive biases can be leveraged to increase downstream performance. Namely, while the output embeddings are already ordered, as they correspond to individual outputs, the hidden embeddings are subject to permutation symmetries. Future work can explore more elaborate methods based on set neural networks, such as Deep Sets (Zaheer et al., 2017), that exploit the inductive biases present in v-reps. Further, the latent codes used in v-reps, namely the hidden and output embeddings, carry global information. Instilling locality in latent codes can be useful for fine-grained downstream tasks, such as segmentation. Future work can explore equivariant neural fields (Wessels et al., 2024), which would localize the latent codes by augmenting them with positions or orientations."}, {"title": "A FITTING AND FINETUNING V-SETS", "content": ""}, {"title": "B NEOMLP SYMMETRIES", "content": "Our v-reps, and more specifically, the hidden embeddings, are subject to permutation symmetries. Intuitively, when we permute two hidden embeddings from a randomly initialized or a trained model, we expect the behaviour of the network to remain the same. In this section, we formalize the permutation symmetries present in our method. NeoMLP is a function f : R(I+H+O)\u00d7D \u2192 R(I+H+O)\u00d7D that comprises self-attention and feed-forward networks applied interchangeably for a number of layers, following Equations (2) and (3). As a transformer architecture, it is a permutation equivariant function. Thus, the following property holds: f(PX) = Pf(X), where P is a permutation matrix, and X is a set of tokens fed as input to the transformer.\nNow consider the input to NeoMLP: T(0) = [{i}{=1, {h;}}=1, {k}=1], T(0) \u2208 R(I+H+O)\u00d7D.\nWe look at two cases of permutations, namely permuting only the hidden neurons, and permuting only the output neurons. The permutation matrix for the first case, i.e. permuting only the hidden neurons, is P1 = IIxI PH\u00d7H IOx0, where I is the identity matrix, PH\u00d7H is a permutation matrix, and denotes the direct sum operator, i.e. stacking matrix blocks diagonally, with zero matrices in the off-diagonal blocks. Each P\u2081 corresponds to a permutation \u03c0\u2081 \u2208 SH.\nApplying this permutation to T(0) permutes only the hidden neurons:\nNext, we apply NeoMLP on the permuted inputs. Making use of the equivariance property, the output of the function applied to the permuted inputs is equivalent to the permutation of the output of the function applied to the original inputs."}, {"title": "C COMPUTATIONAL COMPLEXITY", "content": "While NeoMLP comfortably outperforms Siren in the task of fitting high-resolution signals, it is also more computationally expensive. We quantitatively measure the computational complexity of our method using the fvcore library. We evaluate on the \u201cbikes\u201d video signal, and use the hyperparameters described in Appendix E. We report the FLOPs for 1 input (i.e. 1 coordinate) in the forward pass. NeoMLP has 51.479 MFLOPs, out of which 17.83 MFLOPs correspond to the attention itself and 33.55 MFLOPs correspond to the FFNs. In the same setup, Siren (Sitzmann et al., 2020) has 3.15 MFLOPs.\nDespite having a higher computational complexity compared to the baselines, NeoMLP can actually fit high resolution signals faster, and does so while having a smaller memory footprint, since it can make use of small batch sizes. Figure 5 shows the runtime of NeoMLP for fitting high-resolution signals, compared to the baselines. The x-axis represents wall time in seconds and the y-axis represents the reconstruction quality (PSNR). Table 6 shows the corresponding GPU memory and batch size, along with the total runtime for fitting high resolution signals.\nFinally, despite the large difference in FLOPs, the forward pass of NeoMLP is almost as fast as the forward pass of Siren, considering the same batch size. Namely, we ran a full evaluation on the \u201cbikes\u201d signal, on an Nvidia H100 GPU, using a batch size of 32,768. NeoMLP takes 139.74 seconds, while Siren takes 131.01 seconds. NeoMLP, however, cannot fit larger batch sizes in memory, while Siren can fit as big as 1,048,576. With this batch size, Siren requires 79.18 seconds for a full evaluation.\nWe also monitor the runtime of NeoMLP on fitting datasets of signals, and compare against Functa (Dupont et al., 2022). We report the results in Table 7. NeoMLP consistently exhibits lower runtimes for the fitting stage, while Functa is much faster during the finetuning stage, which can be attributed to the meta-learning employed for finetuning, and the highly efficient JAX (Bradbury et al., 2018) implementation. As noted by Dupont et al. (2022), however, meta-learning may come at the expense of limiting reconstruction accuracy for more complex datasets, since the latent codes lie within a few gradient steps from the initialization."}, {"title": "D IMPLEMENTATION DETAILS", "content": ""}, {"title": "D.1 EMBEDDING INITIALIZATION", "content": "Fitting high-resolution signals We initialize input embeddings by sampling from a normal distribution with variance o\u00b2 = 1. For hidden and output embeddings, we use a variance 0\u00b2 = 1e \u2013 3.\nFitting v-sets During fitting, we initialize the input, hidden, and output embeddings by sampling a normal distribution with variance 2 = \u03c3 = 1e \u2212 3. During finetuning, we sample embeddings for new signals from a normal distribution with variance 02 = 1e \u2013 3."}, {"title": "D.2 WEIGHT INITIALIZATION", "content": "We initialize the bias of the final output linear layer to zeros, as we observe this leads to faster convergence and better stability at the beginning of training. Further, we initialize the weights of the linear projection following the random Fourier features by sampling from a normal distribution N(0, DRFF). This results in a unit normal distribution of the inputs after the linear projection."}, {"title": "E EXPERIMENT DETAILS", "content": ""}, {"title": "E.1 HIGH-RESOLUTION SIGNALS", "content": "Below we provide the hyperparameters for NeoMLP.\n\u2022 Audio (Bach)\nNumber of parameters: 182", "dim": 256, "D": 64, "heads": 4, "layers": 3, "DRFF": 512, "\u03c3": ""}]}