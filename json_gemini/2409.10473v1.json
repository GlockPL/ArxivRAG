{"title": "MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion", "authors": ["Lehong Wu", "Lilang Lin", "Jiahang Zhang", "Yiyang Ma", "Jiaying Liu"], "abstract": "Self-supervised learning has proved effective for skeleton- based human action understanding. However, previous works either rely on contrastive learning that suffers false negative problems or are based on reconstruction that learns too much unessential low-level clues, lead- ing to limited representations for downstream tasks. Recently, great ad- vances have been made in generative learning, which is naturally a chal- lenging yet meaningful pretext task to model the general underlying data distributions. However, the representation learning capacity of genera- tive models is under-explored, especially for the skeletons with spacial sparsity and temporal redundancy. To this end, we propose Masked Con- ditional Diffusion (MacDiff) as a unified framework for human skeleton modeling. For the first time, we leverage diffusion models as effective skeleton representation learners. Specifically, we train a diffusion decoder conditioned on the representations extracted by a semantic encoder. Ran- dom masking is applied to encoder inputs to introduce a information bot- tleneck and remove redundancy of skeletons. Furthermore, we theoreti- cally demonstrate that our generative objective involves the contrastive learning objective which aligns the masked and noisy views. Meanwhile, it also enforces the representation to complement for the noisy view, leading to better generalization performance. MacDiff achieves state-of- the-art performance on representation learning benchmarks while main- taining the competence for generative tasks. Moreover, we leverage the diffusion model for data augmentation, significantly enhancing the fine- tuning performance in scenarios with scarce labeled data. Our project is available at https://lehongwu.github.io/ECCV24MacDiff/.", "sections": [{"title": "1 Introduction", "content": "Human action understanding has been a crucial problem in computer vision. Skeletons use 3D coordinates to represent human joints, providing a lightweight,"}, {"title": "2 Related Work", "content": "Self-Supervised Learning for Skeletons. Self-supervised learning aims to extract meaningful representations from unlabeled data to facilitate downstream tasks. Prevalent methods for skeleton representation learning can be divided into contrastive methods and reconstruction methods. Contrastive learning (CL) extracts meaningful representations by discriminating positive/negative sam- ple pairs from different augmented views [4, 6]. To leverage CL for skeletons, numerous works [27, 40, 48, 49] focus on developing skeleton-specific data aug- mentations. Other works extract shared information between different skele- ton modalities [25, 33]. Most of these methods use RNNs or GCNs as back- bone. Among reconstruction methods, LongT GAN [62] and P&C [23] design"}, {"title": "3 Method", "content": "3.1 Diffusion Models Preliminary\nDiffusion models are a family of generative models that learn the target dis- tribution by performing data denoising. Denoising Diffusion Probabilistic Model"}, {"title": "3.2 Masked Conditional Diffusion", "content": "In this section, we describe the proposed method, Masked Conditional Diffusion (MacDiff), as a unified framework for human skeleton modeling. Fig. 1 illustrates the overall pipeline of our method."}, {"title": "3.3 Information Analysis on MacDiff", "content": "In this section, we conduct information-theoretic analyses on our proposed frame- work MacDiff. We formulate the generative objective of MacDiff as an improve- ment of contrastive learning (CL) objectives, leading to a better guarantee of downstream performance. For mathematical formulation, we use random vari- ables X, Xt, Xm and Z to denote the original view, noisy view, masked view, and latent representation of the skeleton data.\nThe Training Objective of MacDiff. A generative model (e.g., VAEs and MAEs) that predicts some target X from latent code V maximizes their mutual information (MI), i.e. I(X;V). In the case of MacDiff, V takes the form of (Z, Xt). MacDiff can thereby be described as:"}, {"title": "3.4 Diffusion-Based Data Augmentation", "content": "For generative self-supervised methods (e.g., MAEs), only the encoders are uti- lized for downstream tasks, while the rest of the models are completely discarded."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nDatasets. For evaluation, our experiments are conducted on the following three datasets: NTU RGB+D 60 dataset (NTU 60) [43], NTU RGB+D 120 dataset (NTU 120) [29] and PKU Multi-Modality Dataset (PKUMMD) [28]. All three datasets use 25 joints to represent the human body.\nNTU 60 is a large-scale dataset for human action recognition with 60 cate- gories and 56,578 videos. We follow the widely-used evaluation protocols, cross- subject (xsub) and cross-view (xview). The former uses action sequences from half of the 40 subjects for training, and the rest for testing. The latter uses sequences from camera 2,3 for training and sequences from camera 1 for testing.\nNTU 120 is an extension of NTU 60, with 120 categories and 114,480 videos from 106 subjects. Evaluation protocols on NTU 120 are cross-subject (xsub) and cross-setup (xset). Specifically, xset divides sequences into 32 setups based on the camera distance and background, half of which are used for training and the rest for testing.\nPKUMMD covers a multi-modality 3D understanding of human actions, with 52 categories and almost 20,000 instances. PKUMMD is divided into part I and II, and part II is more challenging due to the noise caused by view variation. We split training and testing sets according to the cross-subject protocol.\nImplementation Details. The input sequence of 300 frames is cropped and interpolated to 120 frames, and the patch length l = 4. Apart from random crop, we use random rotation and small Gaussian noise (\u03c3 = 0.005) as data augmentation. Note that the small noise is only added to encoder inputs. For Transformer architecture, the embedding dimension is 256, the MLP hidden"}, {"title": "4.2 Self-Supervised Learning Evaluation", "content": "Linear Evaluation. In the linear evaluation protocol, a linear classifier is post- attached to the encoder to classify the learned representations. We fix the en- coder and train the classifier for 100 epochs with the SGD optimizer and a learn- ing rate of 0.1. We compare MacDiff with latest methods, with action recognition accuracy reported as a measurement.\nAs shown in Tab. 1, our method surpasses high-performing reconstruction- based methods, e.g., SkeletonMAE [55] and MAMP [55]. With only the joint stream, our method also outperforms multi-stream contrastive learning meth- ods, e.g. 3s-AimCLR [49], 3s-CMD [33] and 3s-ActCLR [27]. The result demon- strates that MacDiff captures the spatial-temporal correlation of skeletons better than existing methods, and also confirms our theoretical analysis that MacDiff provides a better framework than contrastive-only paradigms.\nSupervised Fine-tuning Evaluation. In the fine-tuning evaluation protocol, we attach an MLP head to the pre-trained encoder and train the whole model for another 100 epochs with the AdamW optimizer and the learning rate decreasing from 3e-4 to le-5.\nAs shown in Tab. 2, our method yields comparable results to MAMP and outperforms other existing methods. We point out that our performance gap in the fine-tuning protocol with MAMP is trivial since we share the same en- coder architecture and the learned representation is disrupted during fine-tuning."}, {"title": "4.3 Semi-Supervised Fine-tuning with Diffusion-based Data Augmentation", "content": "We next evaluate the effectiveness of the diffusion-based data augmentation in scenarios with limited labeled data. We report results following the semi- supervised protocol, which is consistent with the fine-tuning protocol except"}, {"title": "4.4 Generative Evaluation", "content": "In this section, we implement MacDiff for motion reconstruction and motion generation tasks. We compare our method with reconstruction-based method SkeletonMAE [55] and diffusion-based methods DDIM [45] and MDM [47]. Note that MAMP [32] cannot be applied for either task because it predicts the nor- malized motion. For fair comparison, all methods are implemented with our Transformer decoder architecture. In addition, we also implement the original MDM (denoted as MDM-orig) with temporal-only attention, 8 layers, and 512 hidden dimensions. Please find implementation details in the supplementary ma- terial. All experiments are conducted on the testing set of NTU 60 xsub.\nMotion Reconstruction. Real-world skeleton data suffer from occlusions, re- sulting in incomplete sequences. We evaluate motion reconstruction in two types of occlusions: (1) random consecutive frames, and (2) a random body part from {trunk, left arm, right arm, left leg, right leg}, following the division of previ- ous works. We follow a diffusion-based inpainting paradigm [31] for DDIM and MacDiff. The MacDiff decoder is fine-tuned for another 100 epochs with the encoder fixed and only global representations.\nWe report Mean Per Joint Position Error (MPJPE) as our metric. As shown in Tab. 5, MacDiff is capable of recovering incomplete skeletons as a unified framework and surpasses reconstruction-based SkeletonMAE and DDIM.\nMotion Generation. We utilize the MacDiff decoder for unconditional mo- tion generation. We report four metrics FID, KID, diversity, and precision/recall. Please refer to the supplementary material for detailed implementation of these metrics. Note that reconstruction-based methods are not capable of uncondi- tional generation. As shown in Tab. 6, MacDiff achieves comparable results with DDIM and MDM."}, {"title": "4.5 Ablation Study", "content": "Masking Strategy and Ratio. In Tab. 7, we compare the results of differ- ent masking strategies, including random masking, temporal-only masking, tube masking [50], spatial-temporal masking and motion-aware random masking [32]. For tube masking, the tube length is set to 5. For spatial-temporal masking, we keep 8 out of 25 joints and 10 out of 30 temporal patches. For motion-aware masking, we follow the implementation of MAMP. The results show that the simple random masking works best as a spacetime-agnostic masking. We also compare different masking ratios and find a high masking ratio of 90% works best, which coincides with the findings in the video field.\nNoise Schedule. We construct a series of noise schedules as linear combinations of the inverse-cosine [20] and cosine schedule controlled by \u03c4 (see definition in the supplementary material). \u03c4 = 1 and \u03c4 = \u22121 represents the inverse-cosine and cosine schedule, respectively. We compare these schedules with the widely used linear schedule. As shown in Tab. 8, cosine-based schedule performs better than the linear schedule, and the performance peaks at \u03c4 = 1, indicating that medium noise levels are preferred for representation learning.\nDiffusion-based Data Augmentation. For the ablation study of our diffusion- based data augmentation, We compare the effects of different starting timestep ts and augment-to-real ratio \u03bb. As shown in Tab. 9, the performance gain is highest when ts = 500. Intuitively, an overly large ts may introduce too much noise since we implement one-step denoising, while an overly small ts fails to"}, {"title": "5 Conclusion", "content": "We present MacDiff, a novel generative framework to enhance skeleton represen- tation learning for human action understanding. By training a diffusion decoder guided by the representation from the encoder, the encoder is enforced to contain rich semantics in the representation. We formulate the objective of MacDiff as an improvement of the contrastive learning objective, theoretically demonstrating the effectiveness of the proposed framework."}], "equations": ["q(x_t | x_{t-1}) = N(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I).", "q(x_t|x_0) = N(x_t; \\sqrt{\\bar{a}_t} x_0, (1 - \\bar{a}_t)I)", "p_\\theta(x_{t-1}|x_t) = N(\\mu_\\theta(x_t, t), \\sigma_t^2 I).", "L(\\theta) = E_{x_0, t, \\epsilon \\sim N(0, 1)}[\\gamma_t ||\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{a}_t} x_0 + \\sqrt{1 - \\bar{a}_t} \\epsilon, t)||^2].", "AdaLN(h, z, t) = z_s (t_s\\cdot LN(h) + t_b) + z_b,", "x_0 = \\frac{x_{orig} - \\mu}{\\sigma} \\mu, \\sigma \\in \\mathbb{R}^{1\\times 1\\times 3}.", "L = E_{x_0, t, \\epsilon}[ ||\\epsilon - D(\\sqrt{\\bar{a}_t}x_0 + \\sqrt{1 - \\bar{a}_t}\\epsilon, t, E(M(x_0)))) ||^2 ].", "\\max_{E,D} I(X; (Z, X_t)), Z = E(X_m).", "I(X; (Z, X_t)) = I(X; Z) + I(X; X_t|Z).", "I(X; Z) = I(X_t; Z) + I(X; Z|X_t).", "I(Z_1; Z_2) \\leq I(Z_1; X_2) \\leq I(X_1; X_2) = I(X; Y).", "P_e \\leq 1 - e^{-(H(Y)-I(Z;Y))}", "P_e < 1 - e^{-(H(Y)-I(Z;Y|X_t)-I(Z;X_t))}"]}