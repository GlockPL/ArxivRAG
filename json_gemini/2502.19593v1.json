{"title": "Improving Representation Learning of Complex Critical Care Data with ICU-BERT", "authors": ["Ricardo Santos", "Andr\u00e9 V. Carreiro", "Xi Peng", "Hugo Gamboa", "Holger Fr\u00f6hlich"], "abstract": "The multivariate, asynchronous nature of real-world clinical data, such as that generated in Intensive Care Units (ICUs), challenges traditional AI-based decision-support systems. These often assume data regularity and feature independence and frequently rely on limited data scopes and manual feature engineering. The potential of generative AI technologies has not yet been fully exploited to analyze clinical data. We introduce ICU-BERT, a transformer-based model pre-trained on the MIMIC-IV database using a multi-task scheme to learn robust representations of complex ICU data with minimal preprocessing. ICU-BERT employs a multi-token input strategy, incorporating dense embeddings from a biomedical Large Language Model to learn a generalizable representation of complex and multivariate ICU data. With an initial evaluation of five tasks and four additional ICU datasets, ICU-BERT results indicate that ICU-BERT either compares to or surpasses current performance benchmarks by leveraging fine-tuning. By integrating structured and unstructured data, ICU-BERT advances the use of foundational models in medical informatics, offering an adaptable solution for clinical decision support across diverse applications.", "sections": [{"title": "Introduction", "content": "Generative AI holds great potential to revolutionize healthcare applications by enabling the processing and integration of vast amounts of diverse medical data. These models excel at synthesizing information from structured data like laboratory results, semi-structured machine outputs, and unstructured formats such as clinical notes and imaging reports.\n The Intensive Care Unit (ICU) is a setting where such innovations can be transformative. ICUs generate an immense volume of high-fidelity data from monitoring equipment, electronic health records (EHRs), and clinician notes. These data streams are multivariate, asynchronous, and often multimodal, combining categorical, ordinal, and continuous variables collected at different resolutions. This complexity poses significant challenges to traditional Decision Support Systems (DSS), which rely on assumptions about data regularity and completeness (Johnson et al. 2023).\n Large ICU databases like MIMIC-IV (Johnson et al. 2023; Goldberger et al. 2000) have enabled significant research progress, but existing DSS often fall short of real-world deployment (Eini-Porat et al. 2022). Traditional approaches often employ interpolation, imputation, or resampling to handle missing and irregular data, which can distort natural patterns and relationships.\n More sophisticated models have emerged since the Transformer (Vaswani et al. 2017), producing embeddings from sparse longitudinal data, such as BEHRT (Li et al. 2020), Med-BERT (Rasmy et al. 2021), ExMed-BERT (Lentzen et al. 2023), STraTS (Tipirneni and Reddy 2022) or DuETT (Labach et al. 2023). These models often rely on narrow data scopes, such as International Classification of Diseases (ICD) codes (Hirsch et al. 2016) or limited clinical variables.\n In response to these challenges, we introduce ICU-BERT, a generalizable Transformer model based on BERT (Devlin et al. 2019) designed to handle the sparse, multivariate, irregular sampling nature of ICU data. ICU-BERT processes medical data sequentially, where each token represents a single entry in a medical record, towards enhancing data representation and predictive performance.\n ICU-BERT introduces a novel multi-token input strategy that more effectively captures the intricate details of medical data streams. We propose a quadruplet representation of medical registries and a multi-layer embedding structure to improve clinical context interpretation. Furthermore, pre-trained textual embeddings provide a sophisticated initial token representation of medical concepts, facilitating robust generalization across varied clinical settings and data structures. To learn complex relationships, ICU-BERT employs a novel pre-training masking task coupled with a multi-task learning loss. We pre-trained ICU-BERT on the MIMIC-IV database, providing it with adaptable and advanced data representations between clinical variables and their values.\n In a preliminary evaluation experiment, ICU-BERT demonstrated robust generalization in multiple real-world challenges. The model was fine-tuned in five tasks from DuETT (Labach et al. 2023) and the Yet Another ICU Benchmark (YAIB) framework (van de Water et al. 2024), encompassing a range from classification to regression and from single-shot evaluations to continuous monitoring. Fine-tuning was performed on MIMIC-IV and four YAIB-"}, {"title": "Related Work", "content": "processed data sets. ICU-BERT benefits from its rich input structure and compares favorably with existing models, underscoring its potential to revolutionize DSS in critical care.\n ICU-BERT contributes to the field by i) introducing a multi-token input strategy, a novel masking technique, and a multi-task pre-training scheme; ii) enhancing generalization and expanding usable clinical variables through robust pre-trained textual embeddings; and iii) outperforming models with limited features and simple data structures in real-world tasks, surpassing benchmarks in some tasks.\n Using Machine Learning (ML) to process extensive health data has been a long-standing effort, particularly in ICU settings where data is collected from various devices at minute or second intervals (Wiens and Shenoy 2018). Traditional DSS require extensive preprocessing and feature extraction (Kong, Lin, and Hu 2020). Recurrent models capture temporal relationships but struggle with sparse, asynchronous ICU data due to fixed temporal resolutions and reliance on predefined variable matrices (Ge et al. 2018).\n Advancements in Transformers improved the modeling of longitudinal medical data. BEHRT (Li et al. 2020) processes diagnosis codes as tokens within a limited vocabulary, modeling hospital visits as sentences with age encoded in position embeddings and pre-training through Masked Language Modeling (MLM). Building on this, Med-BERT (Rasmy et al. 2021) and ExMed-BERT (Lentzen et al. 2023) extended the approach by incorporating continuous data and adopting late fusion techniques. Models like Rare-BERT (Prakash et al. 2021) and life2vec (Savcisens et al. 2023) further expanded vocabularies to cover broader data types, while ExBEHRT (Rupp, Peter, and Pattipaka 2023) and BRLTM (Meng et al. 2021) integrated demographic and lab data through additional embedding layers.\n Hierarchical approaches such as Hi-BEHRT (Li et al. 2021) and graph-based methods like GT-BEHRT (Poulain and Beheshti 2023) addressed challenges with large input sequences by summarizing sets of visits before integrating them into patient profiles. STraTS (Tipirneni and Reddy 2022) proposed a triplet embedding scheme for features, values, and time, while DuETT (Labach et al. 2023) used dual attention to capture time- and event-based representations.\n A key challenge remains the lack of standardized benchmarks to compare. YAIB (van de Water et al. 2024) provides a reproducible framework for ICU cohort generation and task evaluation, while EHRSHOT (Wornow et al. 2023) enables model fine-tuning across contexts.\n ICU-BERT advances these efforts by capturing the complexity of ICU data through pre-trained embeddings and modeling temporal information at the embedding level. Unlike earlier approaches, it avoids narrow vocabularies, enabling generalization across datasets and achieving competitive performance on YAIB and DuETT tasks."}, {"title": "Methods", "content": "ICU-BERT employs a bidirectional Transformer architecture (Devlin et al. 2019), optimized for high-dimensional, sparse, and multivariate ICU data with varying time resolutions and sampling rates. The model uses a multi-token input strategy, representing each medical registry as a quadruplet of clinical variable, value, recording time, and duration.\n ICU-BERT integrates pre-trained text embeddings from BioBERT (Lee et al. 2020), a biomedical domain-specific Large Language Model, to provide robust representations of medical concepts and improve generalization. Unlike traditional vocabularies, which often fail to reflect nuanced clinical relationships, such as differences in blood pressure definitions across contexts, BioBERT embeddings preserve semantic meaning without extensive harmonization.\n To optimize learning, ICU-BERT uses a novel masking task and multi-task learning loss during pre-training, enabling effective reconstruction of clinical variables and values. Pre-training on the entire MIMIC-IV database allows the model to develop sophisticated representations without extensive pre-processing, including feature engineering or semantic harmonization.\n ICU-BERT enhances the interpretation of complex ICU data, making it highly adaptable for real-world clinical applications. Figure 1 provides an overview of the architecture."}, {"title": "Data Representation", "content": "In ICU-BERT, we conceptualize medical data for all P patients in a database. $V_p$ integrates all ICU stays of patient $p \\in \\{1,2,..., P\\}$. Each ICU stay $v \\in V_p$ contains a set $R_v$, which includes all registries r recorded during that stay. Each registry $r_i$, $i \\in \\{1, 2, ..., N\\}$, with N the total number of observations, is characterized by four attributes: the data source s, the clinical variable c, the recorded value x, and the associated timestamp or interval t. Mathematically, we can represent each registry as $r_i = (s_i, c_i, x_i, t_i)_{i=1}^{N}$.\n To capture temporal dynamics and granular details, each ICU stay v is divided into non-overlapping 24-hour windows $w_j$, $j \\in \\{1, 2, ..., K_v\\}$, where $K_v$ is the total number of windows for v. Each window $w_j$ includes all data entries $r_i$ with timestamps $t_i$ within the window\u2019s start and end boundaries, along with static demographic data, admission ward, and documented prior diagnoses.\n Each registry $r_i$ within a window $w_j$ is converted into a quadruplet representation $q_i = (f(s,c), x, \\tau(t), \\delta(t))$, which serves as a token. Here, f(s, c) represents the feature derived from the textual aggregation of source s and clinical variable \u0441, \u0442(t) encodes the relative time since the start of $w_j$, and \u03b4(t) captures the duration associated with t, defaulting to 0 for discrete events. A classification token [CLS] and padding tokens [PAD] are added, maintaining the same quadruplet structure."}, {"title": "Multi-Token Embeddings", "content": "To enhance the representational power of ICU-BERT and the ability to interpret and process medical terminology and context effectively, each f(s, c) within the quadruplets $q_i$ is enriched with BioBERT pre-trained embeddings, extracted from the token [CLS] from the last hidden state, such as:\n $PreE_f = BioBERT(f(s, c))[CLS]$\n (1)"}, {"title": "", "content": "For each quadruplet $q_i$, a Boolean mask $m_i$ is defined to denote whether the corresponding value x is continuous ($m_i = 1$). BioBERT embeddings are extracted for categorical values, while a repetition of the continuous values matches the 768 dimension of the pre-trained embedding:\n $PreE_x =\\begin{cases}\n fill(x), & \\text{if } m_i = 1 \\\\\n BioBERT(x)[CLS], & \\text{if } m_i = 0\n\\end{cases}$\n (2)\n Following the initial extraction of pre-trained embeddings, ICU-BERT employs a refined embedding layer to adapt these embeddings for ICU data analysis. Pre-trained embeddings of features f and values v undergo a dense transformation to d, the input dimension of the BERT block:\n $e_f = W_f \\cdot PreE_f + b_f$\n (3)\n $e_x = W_x \\cdot PreE_x + b_x$\n (4)\n $W_f$ and $W_x$ represent the weight matrices for features and values, respectively, and $b_f$ and $b_x$ are the bias terms.\n For encoding temporal information such as timestamps and durations, ICU-BERT employs an embedding layer that maps each discrete time-related input, in minutes, to a high-dimensional continuous vector. Specifically, the embedding for time \u0442(t) and duration \u03b4(t) are obtained by indexing into a pre-defined embedding matrix of input size 1,440 (minute-level in 24 hours), which transforms into d, represented as:\n $e_\\tau = Embedding(\\tau(t))$\n (5)\n $e_\\delta = Embedding(\\delta(t))$\n (6)\n The embeddings $e_f$, $e_x$, $e_\\tau$ and $e_\\delta$ are summed to produce the composite embedding of $r_i$:\n $e_i = e_f + e_x + e_\\tau + e_\\delta$\n (7)\nThe resulting vector is subjected to dropout followed by layer normalization to ensure that the embeddings have a consistent scale and distribution. $e_i$ thus encapsulates a detailed and nuanced representation of static and dynamic patient data, enabling precise and context-aware predictions."}, {"title": "BERT Configuration", "content": "ICU-BERT uses a BERT architecture (Devlin et al. 2019), adopting the original configuration of six transformer layers, each comprising multi-head self-attention mechanisms and fully connected networks. By applying this well-established architecture to ICU data, ICU-BERT benefits from BERT\u2019s powerful feature extraction capabilities, making it effective for interpreting complex medical data."}, {"title": "Pre-training Approach", "content": "Pre-training ICU-BERT is critical for adapting BERT\u2019s language capabilities to ICU data. ICU-BERT introduces a novel pre-training strategy, Masked Language-Value Modelling (MLVM), that selects 15% of the quadruplets for masking. To enhance the model\u2019s ability to reconstruct clinical features and their values, among the selected quadruplets, 50% have both their feature name and value masked, 25% have only the value masked, and 25% have only the feature name masked. Similar to MLM, masked elements are replaced with a [MASK] token, substituted with a random token, or left unchanged in an 80%-10%-10% ratio.\n MLVM reconstruction uses three output heads with separate vocabularies. The feature name vocabulary comprises all possible feature names, with size F, while the value vocabulary includes all possible categorical values, with size V. The feature classification head predicts the identity of masked features using an output dimension equal to the size of the feature name vocabulary. Instead, the categorical value classification head predicts the identity of masked categorical values. Lastly, the continuous value regression"}, {"title": "Multi-Task Loss", "content": "head is designed to predict continuous values and outputs a single scalar directly.\n ICU-BERT employs distinct loss functions in a multi-task framing to optimally train each of its classification heads, reflecting the varied nature of the medical data. Cross-entropy loss is used to predict masked features and categorical values. However, the loss of categorical values must account for the continuous mask, m:\n $L_f = \\frac{1}{K} \\sum_{k=1}^{K} \\sum_{c=1}^{F}y_{k,c}log(\\hat{y}_{k,c})$\n (8)\n $L_{cat} =  \\frac{1}{\\sum_{k=1}^{K} m_k} \\sum_{k=1}^{K} m_k \\sum_{c=1}^{V}y_{k,c}log(\\hat{y}_{k,c}) + \\frac{1}{\\sum_{k=1}^{K}(1 - m_k)} \\sum_{k=1}^{K}(1 - m_k) \\sum_{c=1}^{V}y_{k,c}log(\\hat{y}_{k,c})$\n (9)\n K is the total number of tokens in the batch. The reconstruction loss of continuous values uses the Mean Absolute Error (MAE):\n $L_{cont} = \\frac{1}{\\sum_{k=1}^{K} m_k} \\sum_{k=1}^{K} m_k | y_k - \\hat{y}_k |$\n (10)\n Given the losses for features $L_f$, categorical $L_{cat}$, and continuous values $L_{cont}$, the multi-task loss $L_{total}$ becomes:\n $L_{total} = L_f + \\beta  \\frac{L_{cat} \\times N_{cat}}{N_{cat} + N_{cont}} +  \\alpha \\frac{L_{cont} \\times N_{cont}}{N_{cat} + N_{cont}}$\n (11)\n a and \u03b2 are parameterized weights to balance the contribution of value losses, while $N_{cont} = \\sum_{k=1}^{K} m_k$ and $N_{cat} = K - N_{cont}$. This strategy not only challenges the model to predict the missing parts but also encourages it to learn robust associations between features and values."}, {"title": "Fine-Tuning", "content": "While the rich reconstructions from pre-training do not have direct clinical value, ICU-BERT can be fine-tuned to specific tasks. The original classification head needs to be replaced with an appropriate one, along with a suitable loss function. Outputs for fine-tuning are derived from the [CLS] token of the final transformer layer, which encapsulates the entire input sequence\u2019s contextual information. This ensures that the predictions are based on comprehensive patient data, improving the applicability of ICU-BERT in clinical settings."}, {"title": "Experiments", "content": "Despite the limited availability of benchmarks to evaluate ICU-BERT (van de Water et al. 2024), we conducted a set of experiments to assess its robustness and applicability.\n ICU-BERT was pre-trained on the MIMIC-IV v2.2 dataset (Johnson et al. 2023), with the relational database structured into a hierarchical schema of timestamped entries for each clinical variable and data source. Data from the hosp and icu tables were used, partitioned into train, validation, and test sets in a 70%-15%-15% split, following DuETT\u2019s (Labach et al. 2023) methodology.\n For fine-tuning, ICU-BERT was adapted to a diverse set of clinical tasks, including two from DuETT and three from YAIB (van de Water et al. 2024), chosen to demonstrate the model\u2019s versatility. To assess generalization, we conducted zero-shot external evaluations and fine-tuned the model across four additional datasets. A 5-fold cross-validation (CV) approach was used, resampling train and validation sets for robust performance assessment."}, {"title": "Datasets and Real-World Tasks", "content": "We extracted structured datasets from HiRID v1.1.1 (Faltys et al. 2021), eICU v2.0 (Pollard et al. 2018), MIMIC-III v1.4 (Johnson et al. 2016), and again MIMIC-IV v2.2 but with a limited feature set, using YAIB standards. These datasets are configured to include 52 features that encompass static and time series data, such as vital signs, laboratory results, and input/output. The ricu R package (Bennett et al. 2022) is used within the framework to homogeneously process ICU databases, from which specific cohorts are extracted and structured in a tabular format.\n The fine-tuning process was carried out in five distinct classification and regression tasks to assess the model\u2019s predictive power and applicability to real-world clinical scenarios. Cohorts and targets for YAIB datasets are predefined, and the same rationale was applied in creating MIMIC-IV\u2019s fine-tuning samples.\n Hospital Mortality From DuETT, this task is predicted as a binary outcome based on the first 48 hours of data, using the remaining stay until death or discharge as the target.\n Phenotyping A multi-label classification task is defined from DuETT that consists of phenotyping 25 diseases, evaluated at discharge, based on the first 24 hours of ICU stay.\n ICU Mortality From YAIB, predicting ICU mortality within the first 24 hours is adopted as a binary classification.\n Kidney Function This regression task involves predicting the median creatinine levels on day two after ICU admission, based on the first 24 hours.\n AKI Onset The onset of acute kidney injury (AKI) is predicted using the KDIGO 2012 criteria (KDIGO 2012). While YAIB uses a recurrent many-to-many approach predicting hourly onset from all prior data, ICU-BERT employs a rolling 24-hour window with a 6-hour step to accommodate sequence length limits."}, {"title": "Implementation Details", "content": "ICU-BERT was developed using PyTorch 2.3.0 and the HuggingFace Transformers 4.40.2 (Wolf et al. 2020), on Linux with CUDA 12.2. Pre-training was performed on eight 40GB NVIDIA Tesla A100 GPUs (1.9 TB VRAM) using SMX4 protocol, while fine-tuning utilized NVIDIA Tesla A10 GPUs (205.4 GB VRAM)."}, {"title": "Results", "content": "The performance of classification tasks was measured using the Area Under the Receiver Operating Characteristic (AUROC) and the Area Under the Precision-Recall Curve (AUPRC). For regression, MAE was employed.\n Table 2 shows that ICU-BERT achieved an AUROC of 88.9\u00b10.3% and an AUPRC of 48.5 \u00b1 0.7% for ICU mortality. The low AUPRC reflects the severe class imbalance (9.1% incidence) despite class weight adjustments. A similar trend was observed across other tasks.\n External zero-shot evaluations on YAIB datasets revealed a drop in AUROC 64.1%-67.6% and AUPRC to 12.7%-15.8% in ICU mortality, as seen in Figure 2. Similar declines were noted in the AKI onset and kidney function tasks, particularly when applying the model to distinct patient populations in new datasets such as eICU and HiRID. However, fine-tuning significantly improved results, highlighting ICU-BERT\u2019s potential as a foundational ICU model.\n ICU-BERT\u2019s multi-token embedding strategy effectively captures the semantic and numerical complexity of clinical variables, bypassing the need for pre-processing. This allows the model to utilize all available data and enhance representation learning. A comparison using two versions of MIMIC-IV revealed a performance drop when limited to 52 features, underscoring the importance of a rich feature set."}, {"title": "Discussion", "content": "A foundational model must leverage all available patient data over time. Future improvements could include hierarchical strategies, such as additional recurrent layers or advanced transformers like Hi-BERT, Longformer (Beltagy, Peters, and Cohan 2020), or Mamba (Gu and Dao 2023), to handle longer sequences and reduce the segmentation of continuous tasks into discrete windows.\n Comparing ICU-BERT to existing models is challenging. While ICU-BERT can process granular ICU data, enabling real-time applications, Med-BERT is limited to processing ICD codes at the visit level. In comparison with DuETT, we replicated partitions and tasks on MIMIC-IV but could not ensure identical cohorts or training conditions. While ICU-BERT does not consistently surpass all models, its adaptability to diverse contexts with minimal processing is a key strength. Its multi-token input strategy and use of pre-trained embeddings capture semantic relationships beyond the constraints of predefined vocabularies.\n However, building a foundational ICU model remains challenging. Limited hyperparameter tuning may have constrained performance. Relying solely on the [CLS] token from the final layer might miss sequence complexities, as pre-training did not include tasks like Next Sequence Prediction. Pre-trained embeddings, while improving feature representation and distinguishing continuous from categorical variables, increase input dimensions and computational costs. Enhancements such as pooling mechanisms, smaller pre-trained models, or advanced continuous value embeddings (Gorishniy, Rubachev, and Babenko 2023) could address these issues. Unfreezing specific BERT layers during fine-tuning has shown promising results, though at higher computational costs. While ICU-BERT effectively utilizes multivariate data, it has not yet integrated multimodal information, such as clinical notes. Future work should explore pre-training on additional datasets to expand its capabilities."}, {"title": "Conclusion", "content": "ICU-BERT advances representation learning for complex multivariate data, which is crucial for decision support in intensive care. The multi-token input strategy and pre-trained textual embeddings allow the model to implicitly capture semantic and numerical relationships across various data types, enhancing the performance of traditional models. This approach overcomes the limitations of previous work that relies on predefined vocabularies, heavily processed structures, and a restricted set of variables, allowing ICU-BERT to efficiently process highly granular and continuous data.\n These technical enhancements have implications that extend beyond the ICU settings, offering potential improvements in any domain where complex data streams require accurate and efficient interpretation. Despite these advancements, developing a truly foundational ICU model remains a challenge. Future efforts should aim to expand ICU-BERT\u2019s architecture to better accommodate additional data modalities, such as unstructured clinical notes, and handle longer sequence inputs without the need for extensive pre-processing, potentially integrating more efficient architectures to enhance its applicability and performance."}]}