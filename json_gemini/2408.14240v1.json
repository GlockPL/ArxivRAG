{"title": "Celtibero: Robust Layered Aggregation for Federated Learning", "authors": ["Borja Molina Coronado"], "abstract": "Federated Learning (FL) is an innovative approach to distributed ma-chine learning. While FL offers significant privacy advantages, it also facessecurity challenges, particularly from poisoning attacks where adversariesdeliberately manipulate local model updates to degrade model perfor-mance or introduce hidden backdoors. Existing defenses against theseattacks have been shown to be effective when the data on the nodes isidentically and independently distributed (i.i.d.), but they often fail underless restrictive, non-i.i.d data conditions. To overcome these limitations,we introduce Celtibero, a novel defense mechanism that integrates lay-ered aggregation to enhance robustness against adversarial manipulation.Through extensive experiments on the MNIST and IMDB datasets, wedemonstrate that Celtibero consistently achieves high main task accuracy(MTA) while maintaining minimal attack success rates (ASR) across arange of untargeted and targeted poisoning attacks. Our results highlightthe superiority of Celtibero over existing defenses such as FL-Defender,LFighter, and FLAME, establishing it as a highly effective solution for se-curing federated learning systems against sophisticated poisoning attacks.", "sections": [{"title": "Introduction", "content": "Federated Learning (FL) has emerged as a new machine learning paradigm forefficient distributed learning with privacy concerns [1]. Under this paradigm, acentral server coordinates the generation of a global model from a set of locallytrained model in the client nodes. FL aims to address the data island problem,preventing the direct disclosure of local raw data on the clients [2]. As noprivate data gets exchanged for training the models, FL posses an advance forapplications in areas such as cybersecurity, healthcare or finance [3-6], wherestrong privacy guarantees are fundamental.\nHowever, the FL framework is exposed to a wide variety of attacks, whichcan led to the leakage of clients' data or to harm the predictive capacity andreliability of the global model [7]. In this context, the autonomy granted toclients in contributing to the final model also opens the door for adversarialpeers to carry out poisoning attacks. In these attacks, through compromisingthe client's integrity or the communication channels between clients and theserver adversaries inject poisoned data or malicious model updates [8]. Theconsequences range from diminishing the efficacy of the global model to covertlycrafting a model in such a way that its outcome can be controlled by the attacker[9, 10].\nTo address these threats, several defense mechanisms have been proposedto the FL framework. Existing defenses against poisoning attacks in federatedlearning settings can be broadly categorized into Influence Reduction (IR) andDetection and Filtering (DF) methods [11]. The first type of defenses aim tocompute the global model using robust statistical measures, so that the effectof poisoned updates in the final aggregated model is minimal. Examples ofthis group include Krum [12], median [13], and Differential Privacy [14]. Incontrast, detection and filtering mechanisms analyze model updates sent by thenodes to identify and filter out those suspicious of being poisoned [15]. Thesemechanisms primarily rely on anomaly detectors based on unsupervised machinelearning (ML) algorithms to identify local models that deviate from majority[16, 17].\nMost existing defenses often present accurate and resilient models against at-tacks. However, some are based on assumptions that break the fundamentals ofFL, such as the access to clean data on the server [18], or propose modificationsof the standard FL framework that may incur in increased communication over-heads [19, 20]. Other proposals instead, are devised for specific attack scenariosand may fail under different configurations. For example, defense methods thatfocus primarily on specific subsets of model parameters, such as the weights ofthe last hidden layer [17, 21-23], may not identify poisoning attacks that extendbeyond these isolated layers and affect the entire neural network. Influence re-duction (IR) methods may struggle to mitigate attacks when an attacker gainscontrol over a sufficient number of federation participants to manipulate thestatistical properties used in aggregation. Defenses based on differential pri-vacy require finding an adequate balance of noise to effectively mitigate attackswhile maintaining model accuracy on legitimate data, which may be scenariodependent [14, 24]. Furthermore, most anomaly detectors used in detectionand filtering methods to identify deviating updates may prove ineffective in sce-narios with non-independent and identically distributed (non-i.i.d) data, as thepresence of numerous distributions can lead to a diverse number of anomalies[16].\nIn light of these challenges, we introduce Celtibero, a novel robust aggrega-tor that integrates both detection and filtering, as well as influence reductionmechanisms. Unlike existing literature, Celtibero is not constrained to a limitedset of weights extracted from specific layers of local models to perform aggre-gation. Instead, the comprehensive mechanism of Celtibero performs partialaggregations at each layer, constructing the global model as the combinationof all benign partial aggregations. This flexibility allows it to adapt to diversesdata distributions across different layers. Additionally, our defense is designedfor a typical FL framework where participant nodes are only able to access theglobal model update sent by the server, making no-prior assumptions about theaggregation process and how the data is distributed in the nodes, i.e., whetherdata in the nodes is i.i.d or non-i.i.d. In summary, the main contributions ofthis work are summarized as follows:\n\u2022 We introduce Celtibero, a robust aggregator for Federated Learning (FL)that defends against poisoning attacks, effective in both i.i.d and non-i.i.dscenarios.\n\u2022 We propose a novel layered approach to identify, filter out and reduce theinfluence of poisoned updates within the layers of local models, enablingrobust aggregation in FL.\n\u2022 Our extensive evaluations demonstrate that Celtibero effectively mitigatesboth untargeted and targeted poisoning attacks, outperforming state-of-the-art defenses.\nThis paper is organized as follows. Section 2 analyzes the literature relatedto robust aggregation in FL. Section 3 gives background on the security concernsof FL and formalizes the threat model being considered in this work. The nextsection, Section 4, describes the defense methodology of Celtibero. Section 5presents our experimental setup and evaluates the robustness and performanceof Celtibero, and compares it with several state-of-the-art methods. Finally, themain conclusions and future research lines of our work are discussed in Section 6."}, {"title": "Related Work", "content": "Prior work aiming to counter-act poisoning attacks in FL includes methodsthat leverage robust statistics to reduce the impact of malicious models (IR)and anomaly detection schemes that aim to filter out anomalous models (DF)."}, {"title": "Influence Reduction defenses", "content": "In the first group, [13] employs the median of model parameters for aggrega-tion to reduce the influence of anomalous outlier models. Krum [12] works byselecting the local model update that has the smallest sum of distances to theclosest subset of other updates. As a variation of Krum, Median-Krum [25],computes the global model as the median of the local models with minimal sumof Euclidean distances to other local models. In [14], the authors propose adifferential privacy mechanism that adds random noise to the weights of localmodels with the aim of nullifying poisoned model weights. BaFFLe [19] identi-fies poisoned model updates based on client feedback for class prediction on theaggregated model. While median and Krum methods assume i.i.d. data andfail in non-i.i.d. scenarios, differential privacy require setting an adequate levelof noise that mitigates attack influence without compromising model accuracy,but this is not trivial. The main drawback of BaFFLe is that it overlooks thefact that backdoor attacks are designed to avoid impacting the performance ofmodels on legitimate data, making it challenging for this defense to identifybackdoored models.\nFoolsGold [26] assigns trust scores to models for weight averaging in aggrega-tion, based on the assumption that malicious updates exhibit characteristics notpresent in legitimate updates. CONTRA [27] uses the pairwise cosine similaritybetween updates to limit the contribution of similar updates in the aggrega-tion process, considering them potentially malicious. Similarly, RoseAgg [23]performs partial aggregation of local model parameters with similar directions.The global model is then computed based on the contribution of each partiallyaggregated model to the principal direction of all local weights, thereby reduc-ing the influence of local models that deviate from the principal direction of themajority. FL-Defender [22] employs the two principal components of the cosinessimilarity matrix, derived from the gradients of the last layer of local models,to weight updates during aggregation. The aim is to mitigate the impact ofmalicious updates by reducing the influence of those whose projected anglessignificantly deviate from the median of the projections. All these methods,however, fall short when a large or small number of poisoned models are presentin non-i.i.d. settings, or when poisoned models are crafted to closely resemblebenign models in terms of their angles as in some backdoor attacks [28]."}, {"title": "Detection and Filtering defenses", "content": "Defenses on the second group aim to detect an remove suspicious model updates.Auror [29] arranges local models into two groups based on the distribution oftheir weights, using for aggregation the models lying on the biggest cluster.DeepSight [21] groups local models based on their label distribution as repre-sented by the last layer parameters of local models. The goal is to determinelocal models with highly imbalanced label distributions as poisoned. FLAME[16] acts in two ways, first an outlier detection scheme is used to filter out ab-normal local models. After that, the remaining models are clipped and noisedbefore aggregation. LFighter groups local models into two clusters and identifiesthose lying in the more dense and smallest cluster as poisoned [17]. The maindrawback of these methods lies in the assumption that benign local models aretrained with i.i.d data, presenting limited ability in detecting poisoning attacksunder non-i.i.d scenarios.\nTo cope with non-i.i.d data, in [20] an additional model that learns the asso-ciation between the local model parameters and the training data is generatedon each client. Finally this model is used to identify poisoned updates thatfollow a specific data poisoning behavior. Nonetheless, the need of this supple-mentary model highly increases the computational and communication costs ofFL. Instead, CrowdGuard [11] proposes a validation mechanism on clients thatis used as feedback to discard poisoned models before aggregation. However,it also assumes that each node can access other local models in the federation.This not only modifies the standard FL scheme and increases network overheadsbut also, could lead to craft poisoned models that target their closest local modelto bypass detection."}, {"title": "Preliminaries", "content": "This section briefly describes the FL framework and presents poisoning attacksagainst FL. Finally, we describe the threat model assumed throughout this work."}, {"title": "Federated Learning", "content": "In Federated Learning, an aggregator server builds a global model G from localmodels Wi sent by K nodes $i \\in \\{1, ..., K\\}$ participating on the learning task.For each iteration of the FL process $t \\in \\{1, ..., T\\}$, the most typical scenario isto average local models to compute the global model $G^t = \\sum W_i/n$. Afteraggregation, the server shares the global model parameters Gt with the localnodes, so that they incrementally train this model using their data to obtain$W_i^{t+1}$. This process is repeated for a number of iterations to reach convergence."}, {"title": "Poisoning Attacks against FL", "content": "In the FL setting, a node or set of nodes controlled by an adversary, or lever-aging data obtained from unreliable sources, can lead to poisoned models thatcompromise the global model. Poisoning attacks against federated learning (FL)can be categorized into untargeted and targeted based on their goal [15].\nUntargeted poisoning attacks. These attacks inject noisy data into thetraining process by randomly altering the labels of instances in the originaldataset D to create a poisoned version $D^{pois}$. As a consequence, the local modeltrained on $D^{pois}$ will exhibit poor performance for real data. Once, this modelis introduced in the FL aggregation process, it perturbs model parameters learnon legitimate data with the aim to degrade the overall performance of the globalmodel [30].\nTargeted poisoning attacks Differently from untargeted attacks, targetedattacks seek to introduce biases or perturbations into the model to force misclas-sification of inputs towards a specific target class. In the label flipping attack,the label of all instances in dataset D with source class cs are changed to theclass target class ct. The purpose is to achieve misclassification towards class cton the model trained with $D^{pois}$ for instances pertaining to the class $c_s$ [31, 32].During the FL iterative process, the poisoned model is sent for aggregation inorder to disrupt the predictions of the global model for class cs.\nAs another class of targeted poisoning attacks, backdoor attacks implanta trigger into the model to manipulate its output towards a specific targetclass. To achieve this, the adversary assigns a target label and injects a specific(backdoor) pattern into the features of instances in the training dataset. Thegoal is for the trained model to classify instances with the backdoor patternas the adversary-chosen target class, while performing normally on instanceswithout the backdoor pattern (legitimate). Therefore, since backdoor attacksare designed not to impact the performance on legitimate instances, they areparticularly difficult to detect. In FL, Model Replacement [28], DistributedBackdoor [33], and Neurotoxin attacks [34] send poisoned model updates to theFL server in order to introduce a durable and stealth backdoor trigger in theaggregated global model."}, {"title": "Threat model", "content": "As in many other studies in the area, this work is based on some assumptionsabout the scope of the attacker. In this study, the goal of the attacker is tomanipulate the global model to control its output or cause a high misclassifica-tion rate that renders the model useless. This is achieved by sending arbitrarilypoisoned model updates during the FL operation. To do this, we consider anattacker that can control at most K' nodes, with $K' < \\frac{K}{2}$, where K is the totalnumber of participating nodes in the FL process. In the controlled nodes, theattacker is able to manipulate the training data, as well as the training processesand model parameters sent and received during the FL operation. Furthermore,we assume that the aggregator server and other client nodes are secure, mean-ing that the attacker has no knowledge of the aggregation mechanism used bythe server, nor access to the data, parameters, and training processes in theremaining nodes of the federation."}, {"title": "Defense Methodology: Celtibero", "content": "We design our defense, named Celtibero, to counter the effects of poisonedmodel updates during the aggregation process of FL conducted on the server.The goals of this method are:\nEffectiveness. To identify poisoned models and eliminate their effect in\nthe global model, preventing the adversary from compromising the model and\nachieving its goals.\nPerformance. In the i.i.d scenario all local nodes have examples of all the\nclasses in a similar proportion. In contrast, the non-i.i.d scenario assumes that\nlocal data sets can present different classes and proportions of examples of each\nclass. Our defense mechanism should keep performance on the main task inde-\npendently of the FL scenario presented.\nRobustness. The defense mechanism does not make any assumption to spe-\ncific attack conditions and is effective against poisoning attacks independently\nof their goal. This means that the defense should mitigate both, untargeted and\ntargeted poisoning attacks.\nCeltibero combines detection and filtering with influence reduction to per-\nform robust aggregation. The combination of these two mechanisms enhancesthe robustness of Celtibero against poisoning attacks. On the one hand, the de-"}, {"title": "Detection, Filterng and Influence Reduction Process", "content": "Algorithm 1 Celtibero Defense Procedure\nRequire: Global model weights $W_G$, Local model weights Wi for each node i,Number of layers L\nEnsure: Aggregated global model weights $W_G^f$\n1: for each layer l in 1 to L do\n2: $\\qquad\\Delta W_i^l = W_i^l - W_G^l$\n3: $\\qquad D_{i,j}^l = cosine\\_distance(\\Delta W_i^l, \\Delta W_j^l)$\n4: $\\qquad$ Perform agglomerative clustering on $\\{\\Delta W_i^l\\}_i^{using D_{i,j}^l}$ to form two clusters $C_1$ and $C_2$\n5: $\\qquad density(C_k) = \\frac{2}{\\left|C_k\\right|(\\left|C_k\\right|-1)} \\sum_{i,j\\in C_k} D_{i,j}^l$ for $k = 1,2$\n6: $\\qquad score(C_k) = \\left|C_k\\right| density(C_k)$ for $k = 1,2$\n7: $\\qquad$ if $score(C_1) < score(C_2)$ then\n8: $\\qquad\\qquad$ Label $C_1$ as poisoned and $C_2$ as benign\n9: $\\qquad$ else\n10: $\\qquad\\qquad$ Label $C_2$ as poisoned and $C_1$ as benign\n11: $\\qquad\\Delta W_{benign}^l = \\{\\Delta W_i^l \\mid \\Delta W_i^l \\in benign cluster\\}$\n12: $\\qquad W_G^l = W_G^l + median(\\Delta W_{benign}^l)$\n13: $W_G^f = \\{W_G^1, W_G^2, ..., W_G^L\\}$\n14: return $W_G^f$\nThe detection procedure of Celtibero is detailed in Algorithm 1. First,Celtibero computes the weight updates or gradients ($\\Delta W_i^l$) at each layer basedon the difference between the weights of the global model at the previous it-eration and the weights of local models at the current iteration (line 2). Thegradient updates of models can be interpreted as vectors that alter the weightssof the previous global model in two distinct ways: by altering their magnitude(verified by Euclidean distance) or by changing the direction of the vector (re-flected in increased cosine distance) relative to the previous weight state. Tocharacterize local models for clustering, Celtibero relies on the cosine pairwisedistances between the gradient updates (line 3) as they are useful to exhibitupdate patterns of local models.\nTo discern between benign and poisoned gradient vectors at each layer basedon their angle contribution as indicated by the distance matrix D, Celtibero usesagglomerative clustering [35]. In this regard, we hypothesize that poisoned up-dates will always share a training objective, which is to poison the global model.In contrast, the training objective of legitimate models may differ depending oftheir label distribution. Therefore, we argue that a stronger relation will alwaysbe present among poisoned model updates independently of the distribution ofthe data used for training in the local nodes. This relation can be captured bythe hierarchical process followed by agglomerative clustering, since it progres-sively merges the pairs of gradient vectors with the closest angle contributionuntil all of them are grouped into two clusters.\nOnce gradient vectors have been grouped into two clusters, Celtibero labelsthe clusters as benign or poisoned based on their size and density. To com-pute the density of clusters (line 5), Celtibero uses the average pairwise cosinedistance between every pair of gradient vectors lying in a cluster. Then, thecluster density is proportionally weighted based on the size of the clusters (line6). Since poisoned updates will share a common objective, they will tend toform smaller and denser clusters. Therefore, the cluster with the smaller scoreis determined as posioned by Celtibero, whereas gradient updates lying in thecluster with higher score are considered benign (lines 7-10). Finally, Celtiberoperforms the aggregation of benign gradient updates using the median values(line 12). The entire process is repeated for each of the layers of the model. Theresult is a partial aggregation of benign weights $W_G^l$ at each layer l. Finally, allpartial layered aggregations are combined to form the global model $W_G^f$."}, {"title": "Experimentation", "content": "This section presents the evaluation of Celtibero to show its effectiveness againstpoisoning attacks in comparison to six state-of-the-art defenses: Median-Krum[25], foolsgold [26], FL-Defender [22], FLAME [16], LFighter [17] and RoseAgg[23]. We conduct all experiments using the Tensorflow deep learning framework[36]. Our code is publicly available at gitlab-borja."}, {"title": "Experimental Setup", "content": "Datastets To assess the effectivenes of Celtibero during our evaluations, weconsider two benchmark datasets typically used in the literature. Specifically, weuse the MNIST [37] digit recognition dataset, which consist of 70k handwrittendigit images from 0 to 9; and the IMDB Large Movie Review dataset [38],formed by 50k movie reviews and their corresponding sentiment binary labels.The models used for each of these datasets are outlined in Table 1\nFL setup To simulate the FL framework, we partition the training datasetsinto local datasets $D_k \\in \\{D_1, ..., D_K\\}$, where each Dk corresponds to a node$Q_k \\in \\{Q_1, ..., Q_K\\}$. In the i.i.d scenario, all Dk have the same size and exhibit asimilar class distribution. However, for non-i.i.d scenarios, we adopt a commonapproach used in the FL literature [23, 39], which manipulate the degree of non-i.i.d data across clients by adjusting the a parameter of the Dirichlet distribution[40]. In our non-i.i.d experiments, we set $\\alpha$ = 0.5. We set the number of clientnodes participating in the federation K to 100 and 20 for the i.i.d and non-i.i.d scenarios, respectively, of which 40% are controlled by an adversary tosend poisoned updates to the server. We perform 50 FL training (aggregation)rounds for each experiment, with 3 local training epochs per round. Following[16], we simulate a realistic scenario where in each round the number of clientsthat is available is randomly selected between 60% and 90% of the total nodes.Therefore, the number of adversarial nodes participating in each round alsovaries randomly.\nAttacks setup We evaluate Celtibero against five untargeted and targetedpoisoning attacks: Untargeted Label Flipping Attack (uLFA) [32], TargetedLabel Flipping Attack (tLFA) [41], Model Replacement Attack (MRA) [28],Distributed Backdoor Attack (DBA) [33] and Neurotoxin [34]. Label flippingattacks, such as uLFA and tLFA, involve training malicious models with mislabeled data to poison the global model and render it unusable. Attacks suchas MRA, DBA and Neurotoxin focus on introducing a backdoor pattern intothe global model, allowing the attacker to produce predictions favoring a classchosen by the attacker. For MRA, DBA and Neurotoxin attacks, a randombackdoor pattern is selected. In our experiments with targeted attacks, weassigned the target class for each dataset as indicated in the right column ofTable 1.\nEvaluation Metrics We consider two typically used metrics in the FL attackliterature to evaluate the effectiveness of Celtibero. MTA (Main Task Accuracy)refers to the accuracy of the model on the task it is designed for. It accountsfor the proportion of samples that are predicted correctly by the model fromall total predictions made by the model. ASR (Attack Success Rate) measuresthe effectiveness of the poisoning attack under evaluation. For the uLFA, it"}, {"title": "Experimental Results", "content": "In this section, we show the effectiveness of Celtibero in detecting and mitigatingpoisoning attacks under i.i.d and non-i.i.d FL scenarios."}, {"title": "i.i.d data.", "content": "Table 2 presents the results of our experiments on the MNIST dataset under thei.i.d. scenario. The first row details the performance of the baseline model usingFedAvg, i.e., without any defense mechanism. The baseline model shows an 8%accuracy drop under untargeted label flipping attacks (uLFA), while the tar-geted version of this attack (tLFA) only affects accuracy by 1% on the targetedclass. This relatively minor impact contrasts sharply with the significant vul-nerability of the model to backdoor attacks, where both DBA and MRA achievea 98% success rate. However, the Neurotoxin attack has a reduced impact, withonly 10% of attacks succeeding, likely due to the dilution of malicious updatesby legitimate ones during aggregation.\nAmong the seven robust aggregation defenses evaluated, four exhibit vul-nerabilities to at least one targeted poisoning attack. RoseAgg is the mostsusceptible, with attack success rates of 97.6% and 98.5% for uLFA and MRA,respectively. FoolsGold also shows a significant weakness against MRA, witha 98.6% success rate for backdoor attacks. Additionally, FLAME and Medi-anKrum are notably affected by the Neurotoxin attack, with 93.2% and 91.5% ofpoisoned samples succeeding, respectively. In contrast, Celtibero, FL-Defender,and LFighter demonstrate robustness across all five evaluated attacks, withCeltibero emerging as the most effective defense in terms of average MTA andASR values.\nWhen evaluated on the IMDB dataset (see Table 3), the results are largelyconsistent with those observed for the MNIST dataset, with the notable excep-tion of label flipping attacks. Due to the binary classification nature of the IMDBtask, these attacks significantly degrade final task accuracy (MTA), reducing itto approximately 0.5 for models trained with vulnerable aggregation mecha-nisms such as FedAvg (baseline), MedianKrum, FL-Defender, and RoseAgg.Additionally, FLAME and FoolsGold exhibit classifier instability when facedwith the Neurotoxin attack. In contrast, robust aggregation mechanisms suchas LFighter and Celtibero demonstrate resilience across all attack types, main-taining high and stable MTA values and low ASR. However, Celtibero shows aslight vulnerability to the tLFA attack, with 18.9% ASR."}, {"title": "Non-i.i.d data.", "content": "Table 4 presents the results of our experiments on the MNIST dataset under thenon-i.i.d. scenario. The baseline models trained with FedAvg are vulnerable toall attack types, resulting in fully compromised models. Notably, employing ro-bust aggregators does not necessarily confer resilience against attacks since mostdefenses fail to maintain low Attack Success Rates (ASR), particularly againstMRA, DBA, and Neurotoxin attacks. Even the robust mechanisms identified inthe i.i.d. scenario, such as LFighter and FL-Defender, are compromised by MRAand DBA attacks, with ASR values reaching 99%. Similarly, another populardefenses like MedianKrum and FLAME produce models successfully backdooredby MRA and Neurotoxin attacks, as indicated by their high ASR values. Theonly defense that proves effective across all attack types in this non-i.i.d. settingis Celtibero, which consistently keeps ASR below 2% on average while main-taining high Main Task Accuracy (MTA) values of 97%. Remarkably, Celtiberois the only defense that demonstrates complete robustness against tLFA, MRA,and Neurotoxin attacks, achieving 0% ASR in these cases.\nResults for the IMDB database for the non-i.i.d scenario are depicted in Ta-"}, {"title": "Conclusions", "content": "In conclusion, our experimental results confirm that while existing defenses areprimarily effective in FL scenarios with i.i.d. data, they often fail to preventpoisoning attacks under non-i.i.d. conditions. In many cases, these approachesresulted in low stability models, rendering them ineffective. To address thischallenge, we proposed Celtibero, a novel defense mechanism based on layeredaggregation through the examination of the angle contribution of updates usingagglomerative clustering. Across the different experiments conducted, Celtiberoconsistently achieved high main task accuracy with minimal attack success ratesagainst a range of untargeted and targeted poisoning attacks in both i.i.d. andnon-i.i.d. scenarios. These results highlight that Celtibero is not only a highlyeffective defense strategy, outperforming current state-of-the-art defenses suchas FL-Defender, LFighter, and FLAME, but also significantly enhances thesecurity and reliability of the global model without compromising its stability."}]}