{"title": "Celtibero: Robust Layered Aggregation for Federated Learning", "authors": ["Borja Molina Coronado"], "abstract": "Federated Learning (FL) is an innovative approach to distributed machine learning. While FL offers significant privacy advantages, it also faces security challenges, particularly from poisoning attacks where adversaries deliberately manipulate local model updates to degrade model performance or introduce hidden backdoors. Existing defenses against these attacks have been shown to be effective when the data on the nodes is identically and independently distributed (i.i.d.), but they often fail under less restrictive, non-i.i.d data conditions. To overcome these limitations, we introduce Celtibero, a novel defense mechanism that integrates layered aggregation to enhance robustness against adversarial manipulation. Through extensive experiments on the MNIST and IMDB datasets, we demonstrate that Celtibero consistently achieves high main task accuracy (MTA) while maintaining minimal attack success rates (ASR) across a range of untargeted and targeted poisoning attacks. Our results highlight the superiority of Celtibero over existing defenses such as FL-Defender, LFighter, and FLAME, establishing it as a highly effective solution for securing federated learning systems against sophisticated poisoning attacks.", "sections": [{"title": "1 Introduction", "content": "Federated Learning (FL) has emerged as a new machine learning paradigm for efficient distributed learning with privacy concerns [1]. Under this paradigm, a central server coordinates the generation of a global model from a set of locally trained model in the client nodes. FL aims to address the data island problem, preventing the direct disclosure of local raw data on the clients [2]. As no private data gets exchanged for training the models, FL posses an advance for applications in areas such as cybersecurity, healthcare or finance [3-6], where strong privacy guarantees are fundamental.\nHowever, the FL framework is exposed to a wide variety of attacks, which can led to the leakage of clients' data or to harm the predictive capacity and reliability of the global model [7]. In this context, the autonomy granted to"}, {"title": "2 Related Work", "content": "Prior work aiming to counter-act poisoning attacks in FL includes methods that leverage robust statistics to reduce the impact of malicious models (IR) and anomaly detection schemes that aim to filter out anomalous models (DF)."}, {"title": "2.1 Influence Reduction defenses", "content": "In the first group, [13] employs the median of model parameters for aggregation to reduce the influence of anomalous outlier models. Krum [12] works by selecting the local model update that has the smallest sum of distances to the closest subset of other updates. As a variation of Krum, Median-Krum [25], computes the global model as the median of the local models with minimal sum of Euclidean distances to other local models. In [14], the authors propose a differential privacy mechanism that adds random noise to the weights of local models with the aim of nullifying poisoned model weights. BaFFLe [19] identifies poisoned model updates based on client feedback for class prediction on the aggregated model. While median and Krum methods assume i.i.d. data and fail in non-i.i.d. scenarios, differential privacy require setting an adequate level of noise that mitigates attack influence without compromising model accuracy,"}, {"title": "2.2 Detection and Filtering defenses", "content": "Defenses on the second group aim to detect an remove suspicious model updates. Auror [29] arranges local models into two groups based on the distribution of their weights, using for aggregation the models lying on the biggest cluster. DeepSight [21] groups local models based on their label distribution as represented by the last layer parameters of local models. The goal is to determine local models with highly imbalanced label distributions as poisoned. FLAME [16] acts in two ways, first an outlier detection scheme is used to filter out ab-normal local models. After that, the remaining models are clipped and noised before aggregation. LFighter groups local models into two clusters and identifies those lying in the more dense and smallest cluster as poisoned [17]. The main drawback of these methods lies in the assumption that benign local models are trained with i.i.d data, presenting limited ability in detecting poisoning attacks under non-i.i.d scenarios.\nTo cope with non-i.i.d data, in [20] an additional model that learns the association between the local model parameters and the training data is generated on each client. Finally this model is used to identify poisoned updates that follow a specific data poisoning behavior. Nonetheless, the need of this supplementary model highly increases the computational and communication costs of FL. Instead, CrowdGuard [11] proposes a validation mechanism on clients that is used as feedback to discard poisoned models before aggregation. However, it also assumes that each node can access other local models in the federation.\nThis not only modifies the standard FL scheme and increases network overheads"}, {"title": "3 Preliminaries", "content": "This section briefly describes the FL framework and presents poisoning attacks against FL. Finally, we describe the threat model assumed throughout this work."}, {"title": "3.1 Federated Learning", "content": "In Federated Learning, an aggregator server builds a global model G from local models Wi sent by K nodes i \u2208 {1, ..., K} participating on the learning task. For each iteration of the FL process t \u2208 {1, ..., T}, the most typical scenario is to average local models to compute the global model $G^t = \\sum W_i/n$. After aggregation, the server shares the global model parameters Gt with the local nodes, so that they incrementally train this model using their data to obtain Wt+1. This process is repeated for a number of iterations to reach convergence."}, {"title": "3.2 Poisoning Attacks against FL", "content": "In the FL setting, a node or set of nodes controlled by an adversary, or leveraging data obtained from unreliable sources, can lead to poisoned models that compromise the global model. Poisoning attacks against federated learning (FL) can be categorized into untargeted and targeted based on their goal [15].\nUntargeted poisoning attacks. These attacks inject noisy data into the training process by randomly altering the labels of instances in the original dataset D to create a poisoned version Dpois. As a consequence, the local model trained on Dpois will exhibit poor performance for real data. Once, this model is introduced in the FL aggregation process, it perturbs model parameters learn on legitimate data with the aim to degrade the overall performance of the global model [30].\nTargeted poisoning attacks Differently from untargeted attacks, targeted attacks seek to introduce biases or perturbations into the model to force misclassification of inputs towards a specific target class. In the label flipping attack, the label of all instances in dataset D with source class cs are changed to the class target class ct. The purpose is to achieve misclassification towards class ct on the model trained with Dpois for instances pertaining to the class cs [31, 32]. During the FL iterative process, the poisoned model is sent for aggregation in order to disrupt the predictions of the global model for class cs.\nAs another class of targeted poisoning attacks, backdoor attacks implant a trigger into the model to manipulate its output towards a specific target class. To achieve this, the adversary assigns a target label and injects a specific (backdoor) pattern into the features of instances in the training dataset. The"}, {"title": "3.2.1 Threat model", "content": "As in many other studies in the area, this work is based on some assumptions about the scope of the attacker. In this study, the goal of the attacker is to manipulate the global model to control its output or cause a high misclassification rate that renders the model useless. This is achieved by sending arbitrarily poisoned model updates during the FL operation. To do this, we consider an attacker that can control at most K' nodes, with $K' < \\frac{K}{2}$, where K is the total number of participating nodes in the FL process. In the controlled nodes, the attacker is able to manipulate the training data, as well as the training processes and model parameters sent and received during the FL operation. Furthermore, we assume that the aggregator server and other client nodes are secure, meaning that the attacker has no knowledge of the aggregation mechanism used by the server, nor access to the data, parameters, and training processes in the remaining nodes of the federation."}, {"title": "4 Defense Methodology: Celtibero", "content": "We design our defense, named Celtibero, to counter the effects of poisoned model updates during the aggregation process of FL conducted on the server. The goals of this method are:\nEffectiveness. To identify poisoned models and eliminate their effect in the global model, preventing the adversary from compromising the model and achieving its goals.\nPerformance. In the i.i.d scenario all local nodes have examples of all the classes in a similar proportion. In contrast, the non-i.i.d scenario assumes that local data sets can present different classes and proportions of examples of each class. Our defense mechanism should keep performance on the main task independently of the FL scenario presented.\nRobustness. The defense mechanism does not make any assumption to specific attack conditions and is effective against poisoning attacks independently of their goal. This means that the defense should mitigate both, untargeted and targeted poisoning attacks.\nCeltibero combines detection and filtering with influence reduction to perform robust aggregation. The combination of these two mechanisms enhances the robustness of Celtibero against poisoning attacks. On the one hand, the de-"}, {"title": "4.1 Detection, Filterng and Influence Reduction Process", "content": "The detection procedure of Celtibero is detailed in Algorithm 1. First, Celtibero computes the weight updates or gradients (\u0394Wli) at each layer based on the difference between the weights of the global model at the previous iteration and the weights of local models at the current iteration (line 2). The gradient updates of models can be interpreted as vectors that alter the weights of the previous global model in two distinct ways: by altering their magnitude"}, {"title": "5 Experimentation", "content": "This section presents the evaluation of Celtibero to show its effectiveness against poisoning attacks in comparison to six state-of-the-art defenses: Median-Krum [25], foolsgold [26], FL-Defender [22], FLAME [16], LFighter [17] and RoseAgg [23]. We conduct all experiments using the Tensorflow deep learning framework [36]. Our code is publicly available at gitlab-borja."}, {"title": "5.1 Experimental Setup", "content": "Datastets To assess the effectivenes of Celtibero during our evaluations, we consider two benchmark datasets typically used in the literature. Specifically, we use the MNIST [37] digit recognition dataset, which consist of 70k handwritten digit images from 0 to 9; and the IMDB Large Movie Review dataset [38],"}, {"title": "FL setup", "content": "To simulate the FL framework, we partition the training datasets into local datasets Dk \u2208 {D1, ..., DK}, where each Dk corresponds to a node Qk \u2208 {Q1, ..., QK}. In the i.i.d scenario, all Dk have the same size and exhibit a similar class distribution. However, for non-i.i.d scenarios, we adopt a common approach used in the FL literature [23, 39], which manipulate the degree of non-i.i.d data across clients by adjusting the a parameter of the Dirichlet distribution [40]. In our non-i.i.d experiments, we set a = 0.5. We set the number of client nodes participating in the federation K to 100 and 20 for the i.i.d and non-i.i.d scenarios, respectively, of which 40% are controlled by an adversary to send poisoned updates to the server. We perform 50 FL training (aggregation) rounds for each experiment, with 3 local training epochs per round. Following [16], we simulate a realistic scenario where in each round the number of clients that is available is randomly selected between 60% and 90% of the total nodes. Therefore, the number of adversarial nodes participating in each round also varies randomly."}, {"title": "Attacks setup", "content": "We evaluate Celtibero against five untargeted and targeted poisoning attacks: Untargeted Label Flipping Attack (uLFA) [32], Targeted Label Flipping Attack (tLFA) [41], Model Replacement Attack (MRA) [28], Distributed Backdoor Attack (DBA) [33] and Neurotoxin [34]. Label flipping attacks, such as uLFA and tLFA, involve training malicious models with mislabeled data to poison the global model and render it unusable. Attacks such as MRA, DBA and Neurotoxin focus on introducing a backdoor pattern into the global model, allowing the attacker to produce predictions favoring a class chosen by the attacker. For MRA, DBA and Neurotoxin attacks, a random backdoor pattern is selected. In our experiments with targeted attacks, we assigned the target class for each dataset as indicated in the right column of Table 1."}, {"title": "Evaluation Metrics", "content": "We consider two typically used metrics in the FL attack literature to evaluate the effectiveness of Celtibero. MTA (Main Task Accuracy) refers to the accuracy of the model on the task it is designed for. It accounts for the proportion of samples that are predicted correctly by the model from all total predictions made by the model. ASR (Attack Success Rate) measures the effectiveness of the poisoning attack under evaluation. For the uLFA, it"}, {"title": "5.2 Experimental Results", "content": "In this section, we show the effectiveness of Celtibero in detecting and mitigating poisoning attacks under i.i.d and non-i.i.d FL scenarios."}, {"title": "5.2.1 i.i.d data.", "content": "Table 2 presents the results of our experiments on the MNIST dataset under the i.i.d. scenario. The first row details the performance of the baseline model using FedAvg, i.e., without any defense mechanism. The baseline model shows an 8% accuracy drop under untargeted label flipping attacks (uLFA), while the targeted version of this attack (tLFA) only affects accuracy by 1% on the targeted class. This relatively minor impact contrasts sharply with the significant vulnerability of the model to backdoor attacks, where both DBA and MRA achieve a 98% success rate. However, the Neurotoxin attack has a reduced impact, with only 10% of attacks succeeding, likely due to the dilution of malicious updates by legitimate ones during aggregation.\nAmong the seven robust aggregation defenses evaluated, four exhibit vulnerabilities to at least one targeted poisoning attack. RoseAgg is the most susceptible, with attack success rates of 97.6% and 98.5% for uLFA and MRA, respectively. FoolsGold also shows a significant weakness against MRA, with a 98.6% success rate for backdoor attacks. Additionally, FLAME and MedianKrum are notably affected by the Neurotoxin attack, with 93.2% and 91.5% of poisoned samples succeeding, respectively. In contrast, Celtibero, FL-Defender, and LFighter demonstrate robustness across all five evaluated attacks, with Celtibero emerging as the most effective defense in terms of average MTA and ASR values.\nWhen evaluated on the IMDB dataset (see Table 3), the results are largely consistent with those observed for the MNIST dataset, with the notable exception of label flipping attacks. Due to the binary classification nature of the IMDB task, these attacks significantly degrade final task accuracy (MTA), reducing it"}, {"title": "5.2.2 Non-i.i.d data.", "content": "Table 4 presents the results of our experiments on the MNIST dataset under the non-i.i.d. scenario. The baseline models trained with FedAvg are vulnerable to all attack types, resulting in fully compromised models. Notably, employing robust aggregators does not necessarily confer resilience against attacks since most defenses fail to maintain low Attack Success Rates (ASR), particularly against MRA, DBA, and Neurotoxin attacks. Even the robust mechanisms identified in the i.i.d. scenario, such as LFighter and FL-Defender, are compromised by MRA and DBA attacks, with ASR values reaching 99%. Similarly, another popular defenses like MedianKrum and FLAME produce models successfully backdoored by MRA and Neurotoxin attacks, as indicated by their high ASR values. The only defense that proves effective across all attack types in this non-i.i.d. setting is Celtibero, which consistently keeps ASR below 2% on average while maintaining high Main Task Accuracy (MTA) values of 97%. Remarkably, Celtibero is the only defense that demonstrates complete robustness against tLFA, MRA, and Neurotoxin attacks, achieving 0% ASR in these cases.\nResults for the IMDB database for the non-i.i.d scenario are depicted in Ta-"}, {"title": "6 Conclusions", "content": "In conclusion, our experimental results confirm that while existing defenses are primarily effective in FL scenarios with i.i.d. data, they often fail to prevent poisoning attacks under non-i.i.d. conditions. In many cases, these approaches resulted in low stability models, rendering them ineffective. To address this challenge, we proposed Celtibero, a novel defense mechanism based on layered aggregation through the examination of the angle contribution of updates using agglomerative clustering. Across the different experiments conducted, Celtibero consistently achieved high main task accuracy with minimal attack success rates against a range of untargeted and targeted poisoning attacks in both i.i.d. and non-i.i.d. scenarios. These results highlight that Celtibero is not only a highly effective defense strategy, outperforming current state-of-the-art defenses such as FL-Defender, LFighter, and FLAME, but also significantly enhances the security and reliability of the global model without compromising its stability."}]}