{"title": "Deep learning approaches to surgical video segmentation and object detection: A Scoping Review", "authors": ["Devanish N. Kamtam", "Joseph B. Shrager", "Satya Deepya Malla", "Nicole Lin", "Juan J. Cardona", "Jake J. Kim", "Clarence Hu"], "abstract": "Computer vision (CV) has had a transformative impact in biomedical fields such as radiology, dermatology, and pathology. Its real-world adoption in surgical applications, however, remains limited. We review the current state-of-the-art performance of deep learning (DL)-based CV models for segmentation and object detection of anatomical structures in videos obtained during surgical procedures.\nWe conducted a scoping review of studies on semantic segmentation and object detection of anatomical structures published between 2014 and 2024 from 3 major databases - PubMed, Embase, and IEEE Xplore. The primary objective was to evaluate the state-of-the-art performance of semantic segmentation in surgical videos. Secondary objectives included examining DL models, progress toward clinical applications, and the specific challenges with segmentation of organs/tissues in surgical videos.\nWe identified 58 relevant published studies. These focused predominantly on procedures from general surgery [20(34.4%)], colorectal surgery [9(15.5%)], and neurosurgery [8(13.8%)]. Cholecystectomy [14(24.1%)] and low anterior rectal resection [5(8.6%)] were the most common procedures addressed. Semantic segmentation [47(81%)] was the primary CV task. U-Net [14(24.1%)] and DeepLab [13(22.4%)] were the most widely used models. Larger organs such as the liver (Dice score: 0.88) had higher accuracy compared to smaller structures such as nerves (Dice score: 0.49). Models demonstrated real-time inference potential ranging from 5-298 frames-per-second (fps).\nThis review highlights the significant progress made in DL-based semantic segmentation for surgical videos with real-time applicability, particularly for larger organs. Addressing challenges with smaller structures, data availability, and generalizability remains crucial for future advancements.", "sections": [{"title": "Introduction:", "content": "Over the past decade, computer vision (CV) has emerged as a transformative technology in biomedicine. The advancement of CV and its application to biomedical images has resulted in remarkable breakthroughs across domains such as radiology [1], dermatology [2], and pathology [3], sometimes surpassing even human expert-level performance. This has led to several FDA-approved applications in clinical practice [4]. However, CV is yet to make a meaningful impact in surgical applications, in spite of the rich image and video data generated through minimally invasive surgery. Leveraging these vast data may enable CV-based surgical scene understanding and numerous promising surgical applications. Broadly these include facilitation of real-time tasks in the operating room (OR), such as the identification of critical structures, decision-making support during complex surgical procedures, intraoperative navigation, surgical workflow optimization, and even perhaps autonomous robotic surgery [5], [6]. Other promising applications include post-operative tasks such as automated generation of surgical reports, surgical education for medical students and surgical trainees, and automated assessment of technical proficiency for surgeons.\nSurgical scene understanding is typically studied using tasks including, but not limited to, phase recognition, step recognition, image/video classification, object detection, semantic segmentation, and instance segmentation. Notably, object detection and semantic segmentation have been studied to identify various objects and their locations in a surgical scene. Object detection involves generating a bounding box for the detected object class, whereas the more challenging task of semantic segmentation requires precise pixel-wise classification of the image/video into any of the visible object classes in a surgical scene. Broadly, the objects to be identified or segmented in a surgical scene include surgical tools [7], which are relatively easier to detect; and organs/tissues, which are more challenging given their dynamic deformability [8], [9].\nUnderstanding surgical scenes with CV has remained challenging due to the dynamic nature of surgical videos, which is often compounded by significant background noise, unlike more static and relatively less variable radiological and pathological images. Challenges with surgical videos include the lack of large, labeled video datasets and the substantial costs of obtaining high-quality annotations for supervised learning. This problem is further compounded by the frequency of human anatomical variations and differences in individual surgeons' surgical approaches/techniques. Furthermore, continuous alterations in tissue appearance as a result of real-time surgical actions such as exposing, retracting, cutting, dissecting, and cauterizing tissues -- and interferences from fogging, varying light intensity, shadows, surgical smoke, blood, and fluid stains -- create substantial background noise, posing significant challenges for CV models to analyze and interpret these videos effectively [10].\nDeep learning (DL) has proven particularly effective in addressing these challenges, offering substantial advancements over traditional image analysis and machine learning approaches such as change detection algorithm [11], Fourier transform [12], support vector machines [13], [14], phase change algorithm [15], and random forests with pulsation analysis [16]. DL-based methods of image analysis, most notably convolutional neural networks (CNNs) and their"}, {"title": "1.1 Contributions", "content": "The key contributions of this review include:\n\u2022 We conduct the first scoping review of studies on the semantic segmentation and object detection of anatomical structures in real-world videos across all surgical procedures.\n\u2022 We present the current state-of-the-art for semantic segmentation of various organs or tissues in surgical videos or images.\n\u2022 We identify the DL models employed for semantic segmentation and object detection in the selected studies, summarizing their performance and real-time applicability.\n\u2022 We provide an overview of the surgical procedures investigated and the specific applications of segmenting various organs within these procedures.\n\u2022 We highlight key challenges associated with semantic segmentation and object detection of anatomical structures across different surgical procedures, summarize solutions proposed in the studies to address these challenges, and outline the potential directions for future research."}, {"title": "2. Research Methodology", "content": "This scoping review follows the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses-Scoping Review) [17] framework and reporting guidelines to ensure methodological rigor and reproducibility. All reporting aligns with the PRISMA-ScR checklist. The study protocol was not registered with PROSPERO, as several of the included studies focus on technical aspects, which lie beyond the scope of PROSPERO's primary emphasis on clinical outcomes and patient-centered research."}, {"title": "2.1 Research Questions", "content": "We identified a primary research question to guide this scoping review:\n\u2022 What is the current state-of-the-art performance for semantic segmentation of various organs/tissues using real-world surgical videos?\nSecondary objectives, designed to gain a comprehensive understanding of this topic, were also established:\n\u2022 What DL models and model architectures have been applied for semantic segmentation and object detection of anatomical structures in surgical videos?\n\u2022 For which surgical procedures have DL been used for semantic segmentation and object detection of anatomical structures in surgical videos?\n\u2022 What were the clinical applications of computer vision tasks, such as semantic segmentation and object detection, across various surgical procedures?\n\u2022 What were the major challenges associated with applying DL-based semantic segmentation and object detection methods for anatomical structures in surgical videos?\nThese research questions informed the search strategy, study inclusion criteria, and evaluation framework for the selected studies."}, {"title": "2.2 Search Strategy", "content": "We queried the following literature databases for studies published from January 2014 to September 2024, since the field began emerging after 2012 and our focus was on recent approaches.\nThese databases were selected to ensure a comprehensive coverage of publications spanning both technical and clinical domains. The search was conducted using the following search strings and Boolean logic."}, {"title": "2.3 Study Selection Criteria", "content": "The study selection process was conducted in two stages. In the first stage, titles and abstracts of the search results obtained using the predefined search criteria were screened to identify potentially relevant studies. The second stage involved a detailed full-text review to create a final list of studies relevant to the subjects of the review. Additional studies were identified through snowballing and recursive manual searches of the reference sections, which were subsequently subjected to quality assessment and detailed evaluation of study findings. The screening and study selection process was conducted by a single reviewer (D.N.K. and J.J.K.) and disagreements were resolved through discussion.\nThe inclusion and exclusion criteria were as follows:\n\u2022 Inclusion criteria:\n\u039f Semantic segmentation and object detection of anatomical tissues/organs as the primary task.\n\u039f Videos/images of live human subjects.\n\u039f Published from 2014 onwards to stay relevant to recent/advanced technologies.\n\u039f Peer-reviewed journal and conference manuscripts (PubMed, Embase, IEEE Explore), as well as non-peer-reviewed preprints (arXiv).\n\u2022 Exclusion criteria:\n\u039f Surveys and reviews.\n\u039f Non-availability of full-text manuscripts.\n\u039f Non-English studies.\n\u039f Only instruments (i.e., no anatomical structures) as the object classes for segmentation/detection.\n\u039f Non-human and cadaver-based studies.\n\u039f Performance metrics not reported."}, {"title": "2.4 Data extraction", "content": "During the data extraction process, a predefined extraction form was used to systematically collect relevant information from the shortlisted studies. The extracted data is provided in the Supplementary Data File 1. Key variables collected included study characteristics such as the title, authors, year of publication, aim of the study, dataset characteristics (e.g., newly generated or previously existing data), dataset availability (public or private), evaluated tasks, dataset size (number of videos and annotated frames), video resolution, operative approach (laparoscopic/ robotic/microscopic), surgical procedures evaluated, models employed, inference times, and the outcomes measured.\nThe extracted outcome metrics included the Jaccard index/Intersection over Union (IoU), Dice similarity coefficient (DSC)/F1 score, precision, recall, sensitivity, specificity, false positivity rate, false negative rate, accuracy, and area under precision-recall curve (AuPROC). Summary metrics such as mean average precision (mAP), mean loU, and mean DSC were collected when reported specifically for anatomical tissues. If the summary metrics included both anatomical tissues and surgical instruments, the range of individual scores for the evaluated anatomical tissues was presented. In instances where individual tissue scores were unavailable, the overall summary metric encompassing both tissues and instruments was reported."}, {"title": "2.5 Study Quality Assessment", "content": "The quality assessment of the included studies was conducted using the CLAIM (Checklist for Artificial Intelligence in Medical Imaging) 2024 [18], as it was considered relevant for this review's focus on the application of Al to medical images. However, given the unique characteristics of surgical videos/images compared to other medical or radiology images, we selectively applied the criteria pertinent to surgical videos. The variables used to assess study quality included:\n\u2022 Impact factor of the published journal (in the year of publication)\n\u2022 Description of clinical demographic data of included videos\n\u2022 Patient-level split of the included video data/frames\n\u2022 Frame sampling methodology (random or selective)\n\u2022 Annotation source (board-certified surgeon or non-medical annotator)\n\u2022 Reporting of inter-rater variability\n\u2022 Ablation studies performed\n\u2022 Benchmarking against other models\n\u2022 Evaluation on internal testing data (validation or test)\n\u2022 Evaluation on external testing data\n\u2022 Qualitative failure analysis performed\n\u2022 Availability of code (for reproducibility)\n\u2022 Availability of video dataset and annotations\nThe data extracted for this quality assessment is provided in Supplementary Data File 2, which includes detailed information for each evaluated study. This assessment was conducted independently by two reviewers (J.J.K. and D.N.K.) and disagreements were resolved through discussion.\nNo studies were excluded based on this quality assessment to ensure the availability of an adequate number of studies for a comprehensive review. This decision was also guided by the absence of a widely accepted quality assessment framework specific to Al-based evaluation of surgical videos."}, {"title": "Results:", "content": "We screened 1141 search results using the predefined search criteria and identified 142 studies as potentially relevant. We then narrowed them down to 46 studies after a detailed review of the titles, abstracts, and full manuscripts. In addition, 12 studies were included after manually searching through the references of the included studies, resulting in a total of 58 studies. Of these, 54 (93.1%) were peer-reviewed. All the selected studies were published in or after 2019.\nOf the 58 studies reviewed, 13 (22.4%) utilized pre-existing segmentation datasets, whereas 45 (77.6%) created their own segmentation datasets by annotating masks using their institutional video collections or other publicly available non-segmentation datasets. Of these newly created datasets, 8 (17.7%) were made publicly available, whereas 37 (82.2%) remained private. The median dataset size was 42.5 videos (IQR, 17-100.5), with the largest dataset comprising 548 videos. The median frames/images count was 1,963 (IQR, 672.5\u20134,053.7) prior to dataset augmentation, while the largest dataset contained 47,241 frames. Data augmentation was employed by 32 studies (55.2%). Regarding video quality, 17 studies (29.3%) used datasets with a resolution of \u22651080p, while 30 studies (51.7%) used datasets with \u2265720p resolution."}, {"title": "Semantic segmentation performance metrics for various organs", "content": "The segmentation performance within each study by various metrics of segmentation accuracy, such as DSC, mloU, etc., is presented in Supplementary Table 2. Organs with the highest median DSCs included the liver (0.88), lung (0.89), spleen (0.85), and kidney (0.86). In contrast, certain tissues had notably lower median DSCs: intestinal veins (0.49), nerves, superior hypogastric plexus (0.49), Glissonean pedicles (0.48), vesicular glands (0.43), tegmen (0.29), and omentum (0.18). The median DSC and mean loU of each organ class across all studies in the dataset are summarized in Supplementary Table 3. And the state-of-the-art organ-specific DSC for each study is provided in Supplementary Data File 3."}, {"title": "Model characteristics and inference time:", "content": "Of the 58 studies reviewed, 31 (53.4%) used established models, while 27 (46.5%) introduced novel customized technical modifications to existing models. The most commonly employed models were non-transformer-based models in 46 (79.3%) studies. Among these, U-Net-based models were the most common [14 (24.1%)], followed by DeepLab-based models [13 (22.4%)]. Transformer-based models were employed in around 7 (12.1%) studies.\nMost models demonstrated real-time inference potential, with the highest reported speeds as follows: 298 frames per second (fps) using TensorRT with FP16 precision [19], 233 fps with FP32 precision [19], 141.5 fps with U-Net [20], 137 fps with U-net [21], and 87.44 fps with YOLACTEdge optimization [22]."}, {"title": "Surgical procedure characteristics:", "content": "The procedures investigated were from several specialties, including general surgery (20/58, 34.4%), colorectal surgery (9/58, 15.5%), and neurosurgery (8/58, 13.8%). The most common procedures included cholecystectomy (14/58, 24.1%), anterior rectal resection (5/58, 8.6%), cataract surgery (4/58, 6.9%), and liver resection, prostatectomy and nephrectomy (each 3/58, 5.1%). There were only 2 procedures, esophagectomy (2/58, 3.4%) and lung resection (2/58, 3.4%) investigated from the field of thoracic surgery \u2013 the particular interest of the authors of this review. The most common approaches for video data collection were laparoscopic (34/58, 56.8%), microscope-assisted (10/58, 17.2%), and robot-assisted (9/58, 15.5%) (Table 2)."}, {"title": "Computer vision tasks and applications:", "content": "The tasks investigated included semantic segmentation (47/58, 81.0%), object detection (14/58, 24.1%), landmark detection (6/58, 10.3%), and instance segmentation (2/58, 3.4%). The clinical applications were primarily directed at real-time intraoperative guidance in identifying anatomical structures in an attempt to enhance surgical safety and precision. These applications included, specifically:\nidentifying structures within the hepatocystic triangle to confirm the critical view of safety before clipping and cutting the cystic duct, thereby attempting to reducing the risk of common bile duct injury\nidentifying the adrenal vein and renal artery during renal and adrenal surgeries to putatively enable more precise dissection and prevent catastrophic bleeding\nidentifying the prostate during transanal total mesorectal excision (TaTME) to attempt to avoid urethral injuries\nidentifying loose connective tissue to define safe dissection planes during gastrectomy, attempting to reduce the risk of a postoperative pancreatic fistula\nidentifying the recurrent laryngeal nerve during thyroidectomy and esophagectomy to attempt to reduce the incidence of nerve injuries/hoarseness/swallowing dysfunction\nidentifying the thoracic nerves during lung resections to attempt to avoid injury to these\nidentifying ocular structures during cataract surgery to attempt to facilitate autonomous cataract surgery\nidentifying autonomic nerves, ureter, and inferior mesenteric vessels during colorectal surgery to attempt to avoid injury to these structures\nidentifying the vena cava and azygous vein during esophagectomy to attempt to prevent vascular injury\nidentifying the ureter during hysterectomy to attempt to reduce the risk of ureteral injury\nidentifying parathyroid glands during thyroid surgery to attempt to avoid postoperative hypocalcemia"}, {"title": "Discussion:", "content": "Our scoping review on CV identified 58 studies investigating the use of DL for live organ/tissue segmentation in surgical videos/images. Among these, 45 studies created new annotated datasets, while 13 utilized existing ones. Notably, only 17.7% were made publicly available. The surgical specialties predominantly studied were general surgery, colorectal surgery, and neurosurgery. And the most frequently studied procedures were cholecystectomy and anterior resection of the rectum. Most studies were directed at intraoperative guidance in identifying critical structures to reduce the risk of injuries during surgeries. There were only four studies that addressed Thoracic Surgery. While the earlier studies predominantly employed variants of traditional CNNs, particularly U-Net and DeepLab versions, recent years saw a growing adoption of transformers and other attention-based models."}, {"title": "Segmentation performance, challenges, and real-time inference", "content": "CV through DL-based segmentation performed comparably to specialist surgeons in detecting nerves [23], outperformed all but the most experienced surgeons in segmenting pancreas [24], and proved to be a more effective educational tool for trainees versus traditional surgical learning [25]. Notably, segmentation of tissues improved performance on clinical applications such as, determining critical view of safety in the complex anatomy of porta hepatis [26]. Thereby, CV can enable intraoperative navigation for various surgeries without the need for additional hybrid OR equipment [27]. Specifically, large solid organs with consistent textures or high local similarity, such as the liver, lung, and spleen, had the highest Dice scores. Whereas the smaller, sparsely distributed tissues, such as nerves and veins, had lower scores. Additional challenges were encountered at tissue boundaries partially obscured by fat or fascia [21], [28], which was also challenging for expert surgeons' annotation, resulting in only moderate to substantial interrater agreement (IRA) (0.58 -0.86) [29], [30], [31]. Notably, studies that binarized organs into exposed and partially exposed organs resulted in better Dice scores by ~3%.\nHowever, given that surgeons may perceive organ boundaries as probabilistic rather than binary [32], more advanced methods of annotating or modeling organs may be needed for CV to better understand surgical videos. Inference time is another key consideration for real-time application of CV in surgery. While inference at \u226530 fps (or ~30 ms/frame) is widely considered as necessary, 11 fps has also been considered acceptable [28]. Accordingly, several models such as U-Net, YOLO, YOLACT, and DeepLabv3 have exhibited these speeds without any discernible lag [20], [33]. However, given the variations in image resolutions and graphics processing units (GPUs) used, identifying the models with the best inference times was challenging. And as faster inference times often come with trade-offs in accuracy [34], future research could focus on the optimal combination of both for realizing real-world utility."}, {"title": "Models and model architectures", "content": "Segmentation models must account for local context that is often disrupted by noise, such as blood, smoke, etc., and global context that is influenced by tissue deformability. Most state-of-the-art semantic segmentation networks are based on fully convolutional neural networks (FCNs). U-Net is the most widely used FCN for biomedical segmentation. It captures local granular features and the local spatial context very effectively. Larger contexts can be captured by U-net by broadening/enhancing the receptive fields, but this comes at the expense of features of finer resolution. While U-net overcomes this partly by preserving the lower-level features through skip connections, it may still struggle to capture details such as fine blood vessels [21]. Therefore, despite the success of CNNs in biomedical segmentation, they are constrained by these limited receptive fields that reduce their ability to effectively capture and learn global features [35].\nEspecially in surgery, beyond the local context/fields captured by traditional CNN architectures, inferring global or long-term dependencies is also crucial. Various studies have addressed this, using techniques such as pyramid networks with atrous convolutions, pyramid pooling, multi-scale feature fusion, attention-based, and transformer architectures. Among these, atrous convolution incorporated by DeepLabv3, a U-Net-based model, showed improved retention of global information [36], [37], but occasionally struggled to preserve finer details, such as edges [9]. To overcome this, Deeplab V3+, combined atrous spatial pyramid pooling with atrous convolution to achieve fusion of multi-scale networks and optimize edge accuracy [38]. Notably, U-Net++ was also able to acquire global features by redesigning skip pathways and dense connections enabling a smoother merging of the global and local features [9]. Lastly, hybrid architectures combining transformers with CNNs, such as UNETR [36] and TransFuse [37], have also been introduced with varying performance improvements. However, the significant variability in procedures and organs evaluated across studies made it difficult to determine the most effective model."}, {"title": "Data diversity, data augmentation, and unsupervised approaches", "content": "While models are pivotal for achieving high segmentation performance, considerations about training data are equally crucial. There remains a critical need for large, labeled datasets to advance surgical vision models for real-world applications. Equally important is ensuring data diversity, as class imbalance\u2014whether in the frequency of organ appearances within scenes or size/pixel representation\u2014poses significant challenges. This has been tackled by repeat factor sampling and adaptive sampling, which have significantly boosted performance on rare classes, regardless of model architecture [39]. Yet, the primary challenge in acquiring training data is the limited availability, high costs of expert annotators, i.e. surgeons, and variability in IRA that is also observed among expert surgeons [29], [30], [31]. Proposed solutions to minimize variability include integrating masks from multiple annotators or implementing a two-stage annotation process, where non-surgeons perform the initial annotations, followed by expert surgeons' revisions. Importantly, while data augmentation can enhance training efficiency, improper augmentation can negatively impact model performance [40] \u2013 e.g., horizontal or vertical flips in training data that do not naturally occur can misguide the model's weights. Lastly, unsupervised approaches also offer effective solutions to address the lack of labelled training data. Some unsupervised approaches explored include dataset reconstruction through pretraining [35], local semantic consistency for mask generation [41], auxiliary image reconstruction [42], incorporating ICG-fluorescence with Otsu-thresholding [10], and hyperspectral imaging information [43]."}, {"title": "Evaluation metrics and loss function", "content": "The choice of metrics for optimizing segmentation performance is also a key consideration. Metrics such as specificity or accuracy are heavily influenced by true negatives and can produce misleadingly high results [44]. In contrast, metrics like DSC or loU that penalize false positives offer a more accurate assessment of model performance. Regarding loss function, Dice loss was particularly effective for imbalanced classes compared to the cross-entropy loss [8], which demonstrated poor correlation with the actual performance metric, i.e., mloU [39].\nDespite its advantages, Dice loss is not without limitations, as its lack of differentiability can hinder optimization. Direct training with the mloU metric is also hindered by non-differentiability. Consequently, surrogates like the Lovasz-Softmax extension of the Jaccard index/IoU have been explored and shown to optimize the loU metric more effectively [39]. Other approaches have also investigated online hard example mining, which ignores loss for pixels where the correct label is predicted with a probability >0.7, to focus training on more challenging examples."}, {"title": "Foundational pretrained models and generalizability", "content": "Variations in surgical approaches, even for similar procedures \u2014 such as transperitoneal or retroperitoneal for nephrectomy; and McKeown, Ivor-Lewis, or transhiatal approaches to esophagectomy \u2014 create diverse data needs, necessitating large volumes of labeled data to train task-specific models. By contrast, pretrained foundational models such as large language models, have shown significant utility in other domains by reducing the need for extensive task- specific data through transfer learning. Similarly in segmentation, pre-trained features from models trained on ImageNet/COCO have shown superior segmentation compared to task- specific models trained from scratch [20], [22], [29]. Pretraining not only enhances performance and reduces training data needs but also mitigates overfitting and improves model generalizability [45]. This generalizability potential in surgical segmentation is exemplified by a model that was trained to identify nerves in colorectal surgeries, yet accurately detected nerves in gastrectomy procedures [25]. The success of recently introduced foundational segmentation models, such as Segment Anything Model (SAM) [46] and SAM 2 [47], across several domains [48] holds immense potential to further advance surgical segmentation performance by leveraging their generalized representations."}, {"title": "Current real-world applications", "content": "Despite significant advances in surgical video segmentation and object detection by DL models, the clinical applications of CV remain largely confined to the research phase. These tasks mainly include identifying nerves, vessels, and organs for intraoperative navigation in an effort to enhance surgical safety and precision. CV through tissue segmentation holds promise in realizing real-time operative guidance, automated skill assessment, surgical workflow analysis, and augmented reality (AR)-based training and education with real-time feedback for surgical trainees. While the potential to reduce complications and improve surgical outcomes is clear, the adoption of these models in real-world ORs remains limited. This is primarily due to challenges in model generalizability, driven by the lack of large diverse datasets. Bridging the gap between research innovations and clinical applications is essential in realizing the transformative potential of computer vision in surgery."}, {"title": "Research Directions:", "content": "\u2022 Developing large open source labeled diverse datasets: there is a need for comprehensive and diverse datasets with annotations for several tasks relevant to surgical scene understanding.\n\u2022 Employing generalized foundational models: there is a need to employ foundational models such as SAM that generalize across most organs vs. organ-specific models to reduce the requirement for massive datasets.\n\u2022 Maintaining a focus upon unsupervised training approaches: focusing on unsupervised learning methods to reduce dependency on annotated data.\n\u2022 Optimizing for real-time inference while preserving/enhancing the accuracy of segmentation.\n\u2022 Investigating real-world applicability and utility of these models in the proposed applications to enable translation into clinical practice."}, {"title": "Conclusion:", "content": "In conclusion, this review highlights the advancements that have been made, and the challenges that remain, in applying deep learning in computer vision for semantic segmentation and object detection of tissues/organs in surgical videos. Deep learning models have achieved state-of-the-art segmentation performance for most anatomical structures with promising real- time capabilities. However, challenges remain, including the limited availability of large-scale, diverse datasets and the task-specific nature of these segmentation models. Addressing these gaps through larger, diverse datasets, and leveraging foundational segmentation models through unsupervised approaches is crucial to further improve semantic segmentation and object detection in surgical videos and realize the full potential of computer vision in surgical applications."}, {"title": "CRediT authorship contribution statement:", "content": "DNK \u2013 Conceptualization, Data curation, Formal analysis, Methodology, Supervision, Writing \u2013 original draft, Writing \u2013 review and editing; SDM - Conceptualization, Data curation, Writing \u2013 original draft, Writing \u2013 review and editing; NL - Conceptualization, Methodology, Writing \u2013 review and editing; JJC \u2013 Conceptualization, Data curation, Methodology, Writing \u2013 review and editing; JJK - Data curation, Formal analysis, Methodology, Writing \u2013 original draft; CH - Conceptualization, Methodology, Supervision, Writing - review and editing; JBS - Conceptualization, Methodology, Supervision, Writing review and editing."}, {"title": "Declaration of generative Al and Al-assisted technologies:", "content": "During the preparation of this work, the author(s) utilized ChatGPT to assist with rephrasing and refining the writing in the manuscript. After using this tool/service, the author(s) reviewed and edited the content in detail and take(s) full responsibility for the content of the published article."}, {"title": "Conflicts of interest:", "content": "Joseph Shrager: Consulting \u2013 Lungpacer, Inc.; Becton Dickinson, Inc.; Clarence Hu: Founder \u2013 Hotpot.ai; Other authors have nothing to declare."}, {"title": "Funding:", "content": "None"}, {"title": "Data statement and data linking:", "content": "The data generated and analyzed during the current study are publicly available in Figshare (10.6084/m9.figshare.28418759)."}, {"title": "Ethical approval:", "content": "Not required for reviews"}, {"title": "Acknowledgements:", "content": "None"}]}