{"title": "HADAMRNN: BINARY AND SPARSE TERNARY ORTHOG- ONAL RNNS", "authors": ["Armand Foucault", "Franck Mamalet", "Fran\u00e7ois Malgouyres"], "abstract": "Binary and sparse ternary weights in neural networks enable faster computations and lighter representations, facilitating their use on edge devices with limited computational power. Meanwhile, vanilla RNNs are highly sensitive to changes in their recurrent weights, making the binarization and ternarization of these weights inherently challenging. To date, no method has successfully achieved binarization or ternarization of vanilla RNN weights. We present a new approach leveraging the properties of Hadamard matrices to parameterize a subset of binary and sparse ternary orthogonal matrices. This method enables the training of orthogonal RNNS (ORNNs) with binary and sparse ternary recurrent weights, effectively creating a specific class of binary and sparse ternary vanilla RNNs. The resulting ORNNS, called HadamRNN and Block-HadamRNN, are evaluated on benchmarks such as the copy task, permuted and sequential MNIST tasks, and IMDB dataset. Despite binarization or sparse ternarization, these RNNs maintain performance levels comparable to state-of-the-art full-precision models, highlighting the effectiveness of our approach. Notably, our approach is the first solution with binary recurrent weights capable of tackling the copy task over 1000 timesteps.", "sections": [{"title": "1 INTRODUCTION", "content": "A Recurrent Neural Network (RNN) is a neural network architecture relying on a recurrent computation mechanism at its core. These networks are well-suited for the processing of time series, thanks to their ability to model temporal dependence within data sequences. Traditional Recurrent architectures such as vanilla RNNs, LSTM (Hochreiter and Schmidhuber, 1997), GRU (Cho et al., 2014) or Unitary/Orthogonal RNN (Arjovsky et al., 2016; Helfrich et al., 2018) have achieved remarkable performances across various sequential tasks including neural machine translation (Devlin et al., 2014; Sutskever et al., 2014) or speech recognition (Amodei et al., 2016; Chan et al., 2016).\nModern RNN architectures typically rely on millions, or even billions, of parameters to perform optimally. This necessitates substantial storage spaces and costly matrix-vector products at inference-time, that may result in computational delays. These features can be prohibitive when applications must operate in real-time or on edge devices with limited computational resources.\nA compelling strategy to alleviate this problem is to replace the full-precision weights within the network with weights having a low-bit representation. This strategy known as neural network quantization (Courbariaux et al., 2015; Lin et al., 2015; Courbariaux et al., 2016; Hubara et al., 2017; Zhou et al., 2016) has been extensively studied over the recent years. For optimal computational efficiency and memory savings, weights should be binarized, that is, represented over only 1 bit. For the case of recurrent networks, it was shown (Ott et al., 2016; He et al., 2016; Hou et al., 2017; Alom"}, {"title": "2 RELATED WORKS", "content": "This section provides an overview of prior research efforts aimed at quantizing the weights of neural networks designed for handling sequential data, encompassing both recurrent and non-recurrent architectures. We first highlight the works that report performance with binary and ternary recurrent weights.\nIn the seminal paper Ott et al. (2016), the authors apply binarization and ternarization methods on vanilla RNN, LSTM and GRU architectures. Remarkably, they acknowledge the difficulty of training binary RNNs; they write at the beginning of Section 3.1.1: 'We have observed among all the three RNN architectures that BinaryConnect on the recurrent weights never worked.'In Hou et al. (2017), the authors apply a loss-aware binarization scheme to an LSTM and achieve better performances than the conventional BinaryConnect algorithm (Courbariaux et al., 2015) on a language modeling task. Using a learning process incorporating stochasticity and batch-normalization, Ardakani et al. (2019) show that an LSTM and a GRU with binary weights can achieve results comparable to their"}, {"title": "3 HADAMARD AND BLOCK-HADAMARD RNNS", "content": "We describe the details of the considered ORNNs in Section 3.1. A brief review of the key properties of Hadamard matrices is provided in Section 3.2. We explain, in Section 3.3, how Hadamard matrices are used to build ORNNs with binary recurrent weights that we call HadamRNN. We extend the construction to sparse ternary recurrent weight matrices, called Block-HadamRNN, in Section 3.4. We describe how input and output weight matrices are quantized in Section 3.5 and compare the complexities of the proposed models in Section 3.6."}, {"title": "3.1 ORNNS", "content": "Orthogonal recurrent networks are a class of recurrent networks, that rely on the same recurrent operation as the one of a vanilla recurrent network, but add an orthogonality constraint on the recurrent weight matrix. Given a sequence of inputs $x_1, ..., x_y \\in \\mathbb{R}^{d_{in}}$, the model computes a sequence of hidden states $h_1, \\ldots, h_y \\in \\mathbb{R}^{d_h}$ according to\n$h_t = Wh_{t-1} + Ux_t + b_1,$\nwhere $h_0 = 0$, a matrix $U \\in \\mathbb{R}^{d_h \\times d_{in}}$, $b_1 \\in \\mathbb{R}^{d_h}$, and the recurrent weight matrix $W \\in \\mathbb{R}^{d_h \\times d_h}$ is constrained to be orthogonal (i.e. $W'W = WW' = Id$, where $W'$ is the transpose of $W$, and $Id_,$ is the identity matrix of size $d_h \\times d_h$). Depending on the task, the output is either the vector $Vo(h_T) + b_o \\in \\mathbb{R}^{d_{out}}$ or the time series $Vo(h_1) + b, ..., Vo(h_T) + b_o$. The matrix $V \\in \\mathbb{R}^{d_{out} \\times d_h}$ is the output matrix, and $o$ is the activation function.\nThe orthogonality of the recurrent weight matrix enhances memorization and prevents gradient vanishing. These networks have been shown to solve complex tasks with long-term dependencies,"}, {"title": "3.2 INTRODUCTION TO HADAMARD MATRICES THEORY", "content": "Before describing how we construct binary or sparse ternary recurrent weight matrices in Sections 3.3 and 3.4, we first recall known properties of Hadamard matrices (Hedayat and Wallis, 1978), and explain how, under simple conditions, we can parameterize a subset of all Hadamard matrices.\nDefinition 3.1. Hadamard matrices (Hadamard, 1893) are square matrices with binary values in $\\{-1,1\\}$, whose rows are pairwise orthogonal. For any $n \\in \\mathbb{N}^*$, we denote by $H_n$ the (possibly empty) set of all Hadamard matrices of size $n \\times n$.\nNotice that for any $n > 1$ and any Hadamard matrix $W$ of size $n \\times n$, we have\n$WW' = nI_n.$\nIt is well known that for $n > 2$, Hadamard matrices of size $n \\times n$ do not exist unless $n$ is a multiple of 4 (Hedayat and Wallis, 1978). The existence of Hadamard matrices of size $4n \\times 4n$ for all $n > 1$ remains a conjecture. It is called the Hadamard conjecture (De Launey and Gordon, 2001). It is therefore hopeless to attempt learning an optimal matrix in $H_n$ for an arbitrary $n$.\nThe following proposition outlines a straightforward method, introduced in Sylvester (1867), to construct a Hadamard matrix of size $2^k \\times 2^k$ for any $k \\geq 1$.\nProposition 3.2. Let $k \\geq 1$. The $2^k \\times 2^k$ matrix, denoted $S_{2^k}$, defined recursively by\n$S_2 = \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}$ (i.e. if $k = 1$), and $S_{2^k} = \\begin{pmatrix} S_{2^{k-1}} & S_{2^{k-1}} \\\\ S_{2^{k-1}} & -S_{2^{k-1}} \\end{pmatrix}$ , if $k > 1,\nis a Hadamard matrix. It is called the Sylvester matrix\u00b9 of size $2^k$ (Horadam, 2007).\nFor any $n > 1$, if a Hadamard matrix of size $n \\times n$ is known, the following proposition provides a simple method for generating $2^n$ distinct Hadamard matrices. In the proposition, the notation $diag(u) \\in \\mathbb{R}^{n \\times n}$ refers to a diagonal matrix with $u \\in \\mathbb{R}^n$ on its diagonal.\nProposition 3.3. For $n > 1$ and any $H \\in H_n$, the mapping\n$\\Phi_H: \\{-1,1\\}^n \\longrightarrow \\{-1,1\\}^{n \\times n}$ \n$u \\longmapsto diag(u)H,$\nis injective. Moreover, for all $u \\in \\{-1,1\\}^n, \\Phi_H(u)$ is a Hadamard matrix."}, {"title": "3.3 BINARY ORTHOGONAL RECURRENT WEIGHT MATRICES", "content": "To parameterize the binary orthogonal recurrent weights used in the network called Hadamard RNN (HadamRNN), we consider $d_h = 2^k$, for $k \\geq 1$, and the weights\n$W(u) = \\frac{1}{\\sqrt{d_h}} diag(u) S_{2^k} \\in \\mathbb{R}^{d_h \\times d_h},$\nfor a trainable binary vector $u \\in \\{-1,1\\}^{d_h}$. Indeed, using that $diag(u) S_{2^k}$ is a Hadamard matrix satisfying (2), we obtain that $W(u)$ is orthogonal: $W(u)'W(u) = W(u)W(u)' = Id_h$. It is worth noting that if $k$ is even, $d_h = 2^{2k'}$, for $k' \\geq 1$, then the normalization becomes a division by $\\sqrt{d_h} = 2^{k'}$, which is well-suited for efficient implementation on edge devices."}, {"title": "3.4 SPARSE TERNARY ORTHOGONAL RECURRENT WEIGHT MATRICES", "content": "To construct the sparse ternary orthogonal recurrent weights used by Block-Hadamard RNN (Block-HadamRNN), we consider $d_h = q2^k$, where $k \\geq 1$ and $q \\geq 1$, and the weights are\n$W(u) = \\frac{1}{\\sqrt{2^k}} diag(u) (I_q \\otimes S_{2^k}) \\in \\mathbb{R}^{d_h \\times d_h},$\nfor a trainable binary vector $u \\in \\{-1,1\\}^{d_h}$ and the Kronecker product $\\otimes$. The matrix $W(u)$ is ternary since its components are in $\\{ -\\frac{1}{\\sqrt{2^k}}, 0, \\frac{1}{\\sqrt{2^k}} \\}$. It is orthogonal for the same reasons as those discussed in the previous section."}, {"title": "3.5 MATRICES U AND V QUANTIZATION", "content": "Because input and output sizes are often much smaller than the size of the hidden space (i.e. $d_{in} <<< d_h$ and $d_{out} <<< d_h$), we permit the quantization of the input and output weight matrices, $U$ and $V$, using $p$ bits, where $p > 2$. We use the uniform quantization with a scaling parameter (Gholami et al., 2022).\nThe quantization approximates every component of U (resp. V) by its nearest element in the set\n$\\frac{\\alpha}{2^{p-1}}[-2^{p-1}, 2^{p-1} -1] .$\nwhere $\\alpha = max_{i,j} |U_{ij}|$ (resp. $\\alpha = max_{i,j} |V_{ij}|$) and the set $[a, b]$ contains all the integers between $a$ and $b$. The above set contains $2^p$ elements.\nTo obtain ternary U and V, leading to matrix-vector multiplications involving additions only, we also provide the results for the quantization approximating each component of U (resp V) in $\\alpha\\{-1,0,1\\}$, for the same values of $\\alpha$."}, {"title": "3.6 MODEL SIZE AND COMPUTATIONAL COMPLEXITY", "content": "We compare the different models in terms of parameter storage requirements and the number of operations during inference.\nThe model size is determined by the total number of learnable parameters, multiplied by the number of bits used to encode each parameter. In both HadamRNNs and Block-HadamRNNs, the recurrent layer size is $d_h$ bits, the input size is $d_{in}d_h p$ bits, and the output size is $d_h d_{out} p$ bits. Therefore, the total model size is given by $\\frac{d_h(1+(d_{in}+d_{out})p)}{8 \\times 1024} kBytes. It is important to note that using Sylvester matrices $S_{2^k}$ eliminates the need to store their weights, as these can be easily retrieved using Proposition 3.2.\nThe number of operations of the inference using HadamRNN and Block-HadamRNN is detailed in Table 1. We assume in Table 1 that the hidden and input variables, $h_t$ and $x_t$, are encoded using $p_a$ bits. A detailed description of the fully quantized RNN operations is given in Appendix I. HadamRNNs and Block-HadamRNNs use binary (or ternary) recurrent matrices, which eliminates the need for multiplications. Similarly, for ternary $U$ and $V$, we set $fp_{p,p'} = 0$ since the matrix-vector multiplications only involve additions. When $p = 2$, the matrix-vector multiplications involving matrices $U$ and $V$ in $\\{-2,-1,0,1\\}$ only involve additions and bit-shifts. For the same $d_h$ value ($d_h = 2^k = q.2^{k'}$ with $k' < k$), the computational complexity of the recurrent layer of Block-HadamRNN is $q$ times lower than that of HadamRNN.\nFor comparison, we also provide the complexity for the inference with full-precision ORNN (Arjovsky et al., 2016) and the only quantized ORNN that we are aware of: QORNN (Foucault et al., 2024). The complexities of HadamRNN and Block-HadamRNN are much smaller, in particular, because, as will be reported in Section 4, they permit to achieve satisfactory results for $p = 2$ when, as reported in Foucault et al. (2024), QORNNs require at least $p = 4$ bits encoding.\nNote that when the inputs $x_t$ are one-hot encoded, computing $Ux_t$ requires no multiplications and only $d_h$ additions. This further reduces the complexity compared to Table 1."}, {"title": "4 EXPERIMENTS", "content": "In this section, we assess the performance of HadamRNN and Block-HadamRNN on four standard benchmark datasets. These datasets are described in Section 4.1. Three tasks require retaining information over extended periods, while the fourth focuses on a Natural Language Processing (NLP)"}, {"title": "4.1 DATASETS", "content": "We investigate lightweight neural networks for time series and select datasets suited to these architectures. In particular, this excludes the Long Range Arena benchmarks (Tay et al., 2020), which are too complex for full-precision ORNNs, LSTMs, and GRUs. To illustrate the limitations of the proposed models, we include the sequential MNIST for which LSTMs are known to outperform ORNNs.\nCopy task The Copy task is a standard sequential problem first introduced in (Hochreiter and Schmidhuber, 1997). This task requires memorizing information over many timesteps, and vanilla LSTMs are notoriously unable to solve it for long sequences (Arjovsky et al., 2016; Helfrich et al., 2018; Lezcano-Casado et al., 2019). We follow the setup of Lezcano-Casado et al. (2019), in which the data sequences are constructed as follows. We consider an alphabet $\\{a_k\\}_{k=0}^{19}$ of 10 characters. Given a sentence length $K$ and a delay $L$, the first $K$ elements of an input sequence are sampled uniformly and independently from $\\{a_t\\}_{t=1}^{8}$. These are followed by $L$ repetitions of the blank character $a_p$, one instance of the marker $a_9$, and $K - 1$ repetitions of $a_p$. The first $K$ elements form a sentence that the network must memorize and reproduce identically after outputting $L + K$ instances of $a_0$.\nIn our experiments, we fixed $K = 10$ and $L = 1000$ ($T = L + 2K = 1020$, $d_{in} = 10$, $d_{out} = 9$). The loss function is the cross-entropy, which is also used to measure performance. A naive baseline consists of $L + K$ repetitions of $a_p$, followed by $K$ random values. This leads to a baseline cross-entropy of $\\frac{10log 8}{L+2K} = 0.021$.\nPermuted and sequential pixel-by-pixel MNIST (pMNIST/sMNIST) They are also classic long-term memory tasks. From the MNIST dataset, the 28 \u00d7 28 images are serialized into 784-long sequences of 1-dimensional 8-bits pixel values ($T = 784$, $d_{in} = 1$, $d_{out} = 10$). The serialization is done pixel-by-pixel for sMNIST. For pMNIST, a fixed permutation is used to shuffle the pixels within each sequence. We use the same permutation as Kiani et al. (2022). The task is to predict the correct handwritten digit label at the last step. The learning loss is the cross-entropy, and the model's performance is evaluated with accuracy.\nIMDB This dataset, proposed in Maas et al. (2011), is an NLP binary classification task for sentiment analysis based on 50,000 movie reviews. As in He et al. (2016), we pad and cut the sentences to 500 words, and use a learnable word embedding vector of size 512 ($T = 500$, $d_{in} = 512$, $d_{out} = 1$). The learning loss is the binary cross-entropy, and the model's performance is evaluated with accuracy."}, {"title": "4.2 PERFORMANCE EVALUATION", "content": "The evaluation is organized as follows. In Section 4.2.1 and Table 2, we compare the results of HadamRNN to those of the state-of-the-art. In Section 4.2.2 and Table 3, we compare the results of Block-HadamRNN and HadamRNN.\nFor each task, hyperparameters were selected using validation sets and the final performance was eval- uated on test sets. Details on the hyperparameters and implementations are provided in Appendix D. Training times are in Appendix E.1. Training stability is analyzed in Appendix E.2."}, {"title": "4.2.1 HADAMRNN VERSUS THE STATE-OF-THE-ART", "content": "Since we aim to design high-performance RNN architectures adapted to low-memory devices, we assess the models' performance based on two criteria: the model size of each architecture and its classification accuracy or cross-entropy for the copy task. The model size of HadamRNN is calculated as described in Section 3.6. LSTM recurrent matrix is the identity matrix and it is coded using 0 bits. We also report the performance of fully quantized HadamRNN using the post-training quantization strategy for activations detailed in Appendix I."}, {"title": "4.2.2 BLOCK-HADAMRNN VERSUS HADAMRNN", "content": "When they have the same hidden-space dimension, Block-HadamRNN and HadamRNN are of equal size. Their main difference lies in the computational complexity of their recurrent unit, which, as"}, {"title": "5 CONCLUSION", "content": "Drawing on Hadamard matrix theory, this article presents a method for parameterizing a subset of all binary and sparse ternary orthogonal matrices. We demonstrate that the parameters of such matrices can be learned, using standard methods like the straight-through estimator (STE), and empirically validate that the subset is sufficiently expressive to solve standard RNN benchmarks. We are the"}, {"title": "A BIBLIOGRAPHY ON ORNNS", "content": "In this appendix, we provide a detailed bibliography on unitary and orthogonal recurrent neural networks.\nUnitary Recurrent Neural Networks (URNNs) were introduced in Arjovsky et al. (2016) to better capture long-term dependencies compared to LSTMs. Several methods have been developed to parameterize recurrent weight matrices in URNNs and orthogonal RNNs. These include using the Cayley transform (Wisdom et al., 2016; Helfrich et al., 2018), Givens rotations (Jing et al., 2017), Householder reflections (Mhammedi et al., 2017), and Kronecker matrices (Jose et al., 2018), soft-orthogonality (Vorontsov et al., 2017), the Singular Value Decomposition (SVD) (Zhang et al., 2018), the exponential map (Lezcano-Casado et al., 2019), and Riemannian optimization strategies (Kiani et al., 2022). Each aimed at improving model expressivity, efficiency or reducing complexity.\nThe only known attempt to quantize the weights of ORNNs is detailed in Foucault et al. (2024). This method enables the learning of challenging tasks, such as the copy task for 1000 timesteps, using 5 bits for the weights and 12 bits for the activations."}, {"title": "B PROOFS", "content": "We begin this section by reviewing the definition and a proposition on the Kronecker product. Then for completeness, in Appendix B.2, we provide the proof of Proposition 3.2 and, in Appendix B.3, we provide the proof of Proposition 3.3."}, {"title": "B.1 REMINDERS ON THE KRONECKER PRODUCT", "content": "Definition B.1. Let $p, q, r, s \\in \\mathbb{N}^*$. Let $A = (a_{ij})_{ij} \\in \\mathbb{R}^{p \\times q}$ and $B \\in \\mathbb{R}^{r \\times s}$. The Kronecker product of A by B, denoted $A \\otimes B$, is the matrix of size $pr \\times qs$ given by\n$A \\otimes B = \\begin{pmatrix} a_{11} B & ... & a_{1q} B \\\\ ... & ... & ... \\\\ a_{p1} B & ... & a_{pq} B \\end{pmatrix}.$\nThe following proposition states a well-known result concerning the Kronecker product.\nProposition B.2. Let $p,q,r, s \\in \\mathbb{N}^*$. Let $A \\in \\mathbb{R}^{p \\times q}$ and $B \\in \\mathbb{R}^{r \\times s}$. If the lines of A are pairwise orthogonal and the lines of B are pairwise orthogonal, then the lines of $A \\otimes B$ are pairwise orthogonal.\nWe provide the proof for completeness.\nProof. Let $A \\in \\mathbb{R}^{p \\times q}$ and $B\\in \\mathbb{R}^{r \\times s}$ be two matrices. We denote $A_i \\in \\mathbb{R}^{q}$ (resp $B_i \\in \\mathbb{R}^{s}$) the $i$th line of $A$ (resp $B$). Assume that for all $(i, j) \\in [1, p]^2$ satisfying $i \\neq j, A_i A_j' = 0$. Assume also that for all $(m, n) \\in [1, r]^2$ satisfying $m \\neq n, B_m B_n' = 0$.\nThe hypotheses imply that there is $\\alpha \\in \\mathbb{R}^p$ and $\\beta \\in \\mathbb{R}^r$ such that\n$AA' = diag(\\alpha) \\qquad and \\qquad BB' = diag(\\beta).$\nDenoting\n$C = A \\otimes B = \\begin{pmatrix} a_{11} B & ... & a_{1q} B \\\\ ... & ... & ... \\\\ a_{p1} B & ... & a_{pq} B \\end{pmatrix},$\nFor any $(i, j) \\in [1, p]$, using block matrix multiplication, the block of size $r \\times r$ at position $(i, j)$ of $CC'$ is\n$\\sum_{k=1}^q (A_{ik} B_i) (A_{jk} B_i)' = \\sum_{k=1}^q a_{ik} a_{jk} \\underbrace{diag(\\beta)}_{B_i B_j'} = diag(\\beta) \\underbrace{\\sum_{k=1}^q a_{ik} a_{jk}}_{(\\alpha)_j} = \\begin{cases} 0 & if i \\neq j \\\\ \\alpha_i diag(\\beta) & if i = j \\end{cases}$"}, {"title": "B.2 PROOF OF PROPOSITION 3.2", "content": "We proceed by induction.\n\\begin{itemize}\n    \\item Initialization: Consider $k = 1$. Using the definition of $S_2$, we have\n    $S_2 S_2' = \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix} = 2I_2.$\n    Therefore the lines of $S_2$ are pairwise orthogonal. Since $S_2$ is square and its components are in $\\{-1, +1\\}$, $S_2$ is a Hadamard matrix.\n    \\item Heredity: Consider $k \\geq 1$ and assume that $S_{2^k}$ is a Hadamard matrix. We want to prove that $S_{2^{k+1}} = \\begin{pmatrix} S_{2^k} & S_{2^k} \\\\ S_{2^k} & -S_{2^k} \\end{pmatrix}$ is a Hadamard matrix.\n    Notice first that since $S_{2^k}$ is a Hadamard matrix, $S_{2^{k+1}}$ is square and its components are in $\\{-1,+1\\}$. Using the definition of the Kronecker product, we also have\n    $S_{2^{k+1}} = \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix} \\otimes S_{2^k} = S_2 \\otimes S_{2^k}.$\n    Since the lines of $S_2$ are pairwise orthogonal and the lines of $S_{2^k}$ are pairwise orthogonal, we can apply Proposition B.2 and conclude that the lines of $S_{2^{k+1}}$ are also pairwise orthogonal.\n    We conclude that $S_{2^{k+1}}$ is a Hadamard matrix.\n\\end{itemize}\nThis concludes the proof by induction."}, {"title": "B.3 PROOF OF PROPOSITION 3.3", "content": "Let $n \\in \\mathbb{N}^*$ and $H \\in H_n$. We first show that $\\Phi_H$ is injective.\nLet $u, u' \\in \\{-1,1\\}^n$ such that $u \\neq u'$. Let $i \\in \\{1, ..., n\\}$ be such that $u_i \\neq u'_i$, that is, since $u_i$ and $u'_i$ are both in $\\{-1,1\\}$, $u_i = -u'_i$. Denoting, for all matrix $A$, the $i$-th row of $A$ by $A_i$, we obtain\n$\\Phi_H(u)_i = u_i H_i = -u'_i H_i = -\\Phi_H(u')_i;$\nSince all the components of $\\Phi_H(u)_i$ are in $\\{-1,1\\}$, $\\Phi_H(u)_i \\neq 0$ and finally $\\Phi_H(u)_i \\neq \\Phi_H(u')_i$.\nAs a conclusion, for any $u,u' \\in \\{-1,1\\}^n$ such that $u \\neq u', \\Phi_H(u) \\neq \\Phi_H(u')$. The mapping $\\Phi_H$ is injective.\nWe now show that $\\Phi(u)$ is a Hadamard matrix. Notice first that $\\Phi(u)$ is square and that all its components are $\\{-1,1\\}$. We still need to show that any two distinct rows of $\\Phi(u)$ are orthogonal. Let $i, j \\in \\{1, ..., n\\}$ with $i \\neq j$. Reminding that $\\Phi_H(u)_i$ is $i$-th line of $\\Phi_H(u)$, we have\n$\\Phi_H(u)_i \\Phi_H(u)_j' = u_i u_j H_i H_j' = 0.$\nFinally, $\\Phi_H(u)$ is a Hadamard matrix.\nThis concludes the proof."}, {"title": "B.4 DETAILED PROOF OF THE ORTHOGONALITY OF THE BINARY AND SPARSE TERNARY WEIGHTS", "content": "The proof that the binary matrix defined by\n$W(u) = \\frac{1}{\\sqrt{d_h}} diag(u) S_{2^k} \\in \\mathbb{R}^{d_h \\times d_h},$\nis orthogonal when $d_h = 4$ is in Appendix C.2. The general proof is similar to the proof that the sparse ternary matrix defined below is orthogonal. We only detail the latter proof.\nLet us prove that the sparse ternary weights defined by\n$W(u) = \\frac{1}{\\sqrt{2^k}} diag(u) (I_q \\otimes S_{2^k}) \\in \\mathbb{R}^{d_h \\times d_h},$\nare orthogonal for all $u \\in \\{-1,1\\}^{d_h}$, and $d_h = q2^k$.\nTo do so, we consider $u \\in \\{-1,1\\}^{d_h}$. We first remark that the lines of $I_q$ are pairwise orthogonal. Because $S_{2^k}$ is a Hadamard matrix, the lines of $S_{2^k}$ are also pairwise orthogonal. Applying Proposition B.2, we conclude that the lines of $I_q \\otimes S_{2^k}$ are pairwise orthogonal. Therefore, the lines of $W(u)$ are also pairwise orthogonal and the matrix $W(u)W(u)'$ is diagonal. Let us consider $i \\in [1, d_h]$, we write $i = (m - 1)2^k + n$, where $m \\in [1, q]$ and $n \\in [1,2^k]$. Reminding that $W(u)_i$ is the $i$-th line of $W(u)$, and $(S_{2^k})_n$ is the $n$-th line of $S_{2^k}$, we have\n$\\begin{aligned}\n(W(u)W(u)')_{i,i} &= W(u)_i (W(u)_i)'\\\\ &= \\frac{1}{\\sqrt{2^k}} \\Big(u_i (I_q \\otimes S_{2^k})_i\\Big) \\Big(\\frac{1}{\\sqrt{2^k}} (I_q \\otimes S_{2^k})_i\\Big)' \\\\\n&= \\frac{1}{2^k} \\sum_{j=1}^{2^k} [(S_{2^k})_{n,j}]^2 \\\\\n&= 1\n\\end{aligned}$\nbecause $u_i \\in \\{-1,1\\}$ and all the components of $S_{2^k}$ are in $\\{-1,1\\}$.\nFinally, we conclude that $W(u)W(u)' = Id_h$. Because the matrix $W(u)$ is square, we also have $W(u)'W(u) = Id_h$, and the matrix $W(u)$ is orthogonal.\nThis concludes the proof the sparse ternary matrix defined by (5) is orthogonal."}, {"title": "C THE STRAIGHT-THROUGH ESTIMATOR AND AN EXAMPLE", "content": "We present the Straight-through Estimator (STE) in Appendix C.1 and provide in Appendix C.2 a detailed example of a recurrent weight matrix for the HadamRNN defined in Section 3.3, specifically when $d_h = 4$."}, {"title": "C.1 THE STRAIGHT-THROUGH ESTIMATOR", "content": "In this section, we discuss the Straight-through Estimator, introduced in Hinton (2012); Bengio et al. (2013); Courbariaux et al. (2015), a standard method for optimizing quantized neural network weights, in the context of HadamRNN and Block-HadamRNN.\nFor simplicity, we omit the optimization of $U, V, b_i$ and $b_o$ in the following description, focusing on the recurrent matrix.\nWe consider a matrix $W \\in \\mathbb{R}^{d_h \\times d_h}$. For the HadamRNNs defined in Section 3.3, we use the constant orthogonal matrix $W = \\frac{1}{\\sqrt{d_h}} S_{2^k}$, while for the Block-HadamRNNs defined in Section 3.4, we take $W = \\frac{1}{\\sqrt{2^k}} (I_"}]}