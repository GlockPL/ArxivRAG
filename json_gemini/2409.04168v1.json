{"title": "From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning Tasks", "authors": ["Andreas Stephan", "Dawei Zhu", "Matthias A\u00dfenmacher", "Xiaoyu Shen", "Benjamin Roth"], "abstract": "To reduce the need for human annotations, large language models (LLMs) have been proposed as judges of the quality of other candidate models. LLM judges are typically evaluated by measuring the correlation with human judgments on generation tasks such as summarization or machine translation. In contrast, we study LLM judges on mathematical reasoning tasks. These tasks require multi-step reasoning, and the correctness of their solutions is verifiable, enabling a more objective evaluation. We perform a detailed performance analysis and find that the used judges are mostly unable to improve task performance but are able to pick the better model. Our analysis uncovers a strong correlation between judgment performance and the candidate model task performance. We observe that judges tend to choose the model of higher quality even if its answer is incorrect. Further, we show that it is possible to use statistics, such as the task performances of the individual models, to predict judgment performance. In an ablation, we either swap or mask the candidate answers and observe that judges often keep the original judgment, providing evidence that judges incorporate writing style in their judgments. In summary, we find that regularities in the judgments are quantifiable using statistical measures and provide various angles on exploiting them.", "sections": [{"title": "1 Introduction", "content": "The automatic evaluation of machine learning models promises to reduce the need for human annotations. Specifically, the LLM-as-a-judge paradigm (Zheng et al., 2023) has gained traction, aiming to assess or compare the quality of generated texts automatically. This approach is beneficial for automated data labeling (Tan et al., 2024), self-improvement of LLMs (Wu et al., 2024), and ranking LLMs with respect to specific tasks (Zheng et al., 2023).\nMuch like judges in the real world, who are expected to be exact, fair, and unbiased, e.g., as defined in Bangalore Principles of judicial conduct (Bangalore Principles, 2002), LLMs, when employed as judges, should be ethical and logical. Already the philosopher Aristotle argued that the virtuous actor exhibits the joint excellence of reason and character (Kraut, 2022). Previous works investigate properties and biases of LLM judges on generation tasks such as translation or summarization (Kim et al., 2024b; Liu et al., 2024). These are typically evaluated using correlation with human annotators and are thus inherently subjective.\nIn this work, we investigate LLM judges on mathematical reasoning datasets (see Figure 1). These need complex multi-step reasoning, and the solution is verifiable, which allows us to investigate the relationship between judge and candidate models in a principled manner. We base our analysis on four large (more than 30B parameters) LLMs and four small (less than 10B) LLMs on three mathematical reasoning datasets.\nOur experiments contain a detailed performance examination, confirming that larger models are generally better judges (Zheng et al., 2023). We find that only the best-tested model, Qwen 2 72B, consistently improves task performance if we evaluate the judged samples, but all tested judges likely pick the better model for a given task.\nWe investigate subsets with one correct and one incorrect candidate answer. We uncover a correlation between judgment performance and task performance of the candidate models, showing that judges tend to select incorrect answers from better models. Thus, we hypothesize that judges have access and rely on the superior writing styles of larger models instead of solely analyzing the reasoning. When we divide the datasets into buckets of model agreement, we observe that agreement is a proxy for sample difficulty.\nMotivated by these regularities, we analyze whether it is possible to predict judgment performance and find that task performances of judge and candidate LLMs explain most of the variance. We hypothesize that judges incorporate writing style into their judgments. Thus, we predict individual judgments using statistical and transformer-based models and achieve above-chance performance, supporting our hypothesis.\nLastly, we test how judgments are affected by perturbing numeric values in responses by 1) swapping results and 2) masking numeric values. Our findings reveal that judges largely retain original judgments, providing further evidence that judges, in large part, base their decisions on writing style. In summary, our contributions are as follows:\n1.  We conduct an in-depth performance analysis of LLM judges for mathematical reasoning tasks.\n2.  Our analysis reveals a correlation between the judgment and candidate task performance, providing a novel statistical angle on the analysis of LLM judges.\n3.  We show that statistics such as task performance or agreement of candidate models are indicative of judgment performance.\n4.  After systematically perturbing the candidate answers, we observe that judges often keep their original judgments, providing evidence that judgments are also based on writing style."}, {"title": "2 Related Work", "content": "2.1 LLM as Judges\nUsing LLMs as judges to evaluate text generated by LLMs, including their own outputs, has recently attracted significant interest because it reduces the need for human annotation (Zheng et al., 2023). Commonly, large frontier models are used as judges. Applications include the automatic assessment of language model capabilities and, e.g., determining which model performs better on a given task (Zheng et al., 2023) and reinforcement learning from AI feedback by automatically generating data for preference optimization (Bai et al., 2022; Wu et al., 2024).\nVarious methods exist to make judgments (Zheng et al., 2023; Liusie et al., 2024). One approach is pairwise selection (Wang et al., 2024a), where two answers are presented, and the model is asked to select the better one. Another approach is pointwise grading (Li et al., 2024), where the model is asked to assign a grade based on a predefined scale, and the answer with a better grade is chosen. Judgment prompts may involve reference solutions or not. Another body of research explicitly trains models to act as judges (Kim et al., 2024a; Wang et al., 2024a) or closely related, as reward models (Wang et al., 2024b; Li et al., 2024).\nThe effectiveness of LLMs as judges is typically assessed by measuring the correlation or overlap with human judgments (Zheng et al., 2023; Kim et al., 2024b). In contrast, we focus on difficult tasks with a concrete final answer. Finally, we want to stress that several works caution for the use of LLM judges as experts (Bavaresco et al., 2024; Koo et al., 2023; Raina et al., 2024). In a similar vein, we aim to understand regularities and their shortcomings.\n2.2 Biases in LLM-as-a-judge\nHuman-annotated data inherently reflects the annotators' biases and opinions. These biases can be detrimental or (intentionally) beneficial, depending on the goals of the annotation process (Plank, 2022). Similarly, several studies have explored the biases present in LLM judges:\nOne linguistic bias is ordering bias (Zheng et al., 2023; Koo et al., 2023), where a judge gives a different answer depending on the order in which answers are presented. Panickssery et al. (2024) note that it is possible to interpret position bias as a sign that the model is unsure. There are multiple works (Xu et al., 2024; Panickssery et al., 2024; Liu et al., 2024) that find evidence for self-bias or self-preference. Koo et al. (2023) provide a benchmark for analyzing cognitive biases. West et al. (2024) and Oh et al. (2024) explore the \"Generative AI Paradox\" where generating solutions is easier for the LLM than analyzing them, unlike humans who typically find analysis easier than generation.\nIn this work, we aim to establish a better understanding of underlying regularities that relate judgments to statistics such as model performance."}, {"title": "3 General Setup", "content": "In the following, we describe the problem setting, including the used notation, and the general experimental setting including used models and datasets.\n3.1 Problem Description\nIn this work, we consider two models, denoted by \u041c\u0410, MB \u2208 M, providing candidate solutions for a sample of a dataset D and a judge model MJ \u2208 M, which is tasked to select, to \"judge\u201d, whether it prefers the solutions of the models MA or MB. The solutions are represented by the random variables A and B. We consider the events that solutions are true (A = T), false (A = F), or that their solution is the same (A = B). We denote the judgment of the judge MJ by the random variable \u2206j, which can either be correct (\u25b3j = T), incorrect (\u2206j = F) or choose a specific model Ma (\u2206j = MA).\nGiven that the final answer is either correct or incorrect, we can break the probability of the judge making a correct judgment P(\u2206\u2081 = T|A, B, D) given a sample of a dataset D and the answers of two models MA, MB down into the following four cases:\nP(AJ = TA, B, D)\n=\n\u03a3 P(A) = T A = X, B = Y, D)P(A = X, B = Y|D)\n(X,Y)EC\n= P(A = T, B = T|D)\n+ P(A = T A = T, B = F, D) P(A = T, B = F|D)\n+ P(A = T A = F, B = T, D)P(A = T, B = F|D)\nwhere C = (T, F)2. Note that in cases where both answers are correct or incorrect imply that the judgment is also either correct or incorrect respectively, i.e., P(\u2206j = T|A = T, B = T) = 1 and P(\u2206 = T|A = F, B = F) = 0.\n3.2 Datasets\nThe experiments encompass three mathematical reasoning datasets where models highly benefit from multi-step CoT reasoning. For all datasets, we use accuracy as the performance metric.\nAQUA-RAT (Ling et al., 2017) is a dataset to test the quantitative reasoning ability of LLMs. Unlike the other two datasets, the questions are multiple-choice. GSM8K (Cobbe et al., 2021) consists of grade school math word problems. The answers are free-form numbers. MATH (Hendrycks et al., 2021) contains challenging competition mathematics problems.\n3.3 Models\nWe evaluate the performance of openly available LLMs, including four large models Qwen 2 72B (Yang et al., 2024), Llama 3 70B (AI@Meta, 2024), Yi 1.5 34B (Young et al., 2024), Mixtral 8x7B (Jiang et al., 2024) and four small models, namely Llama 3 8B (AI@Meta, 2024), Gemma 1.1 7B (Gemma Team et al., 2024), Mistral 7B v0.3 (Jiang et al., 2023), and Mistral 7B v0.1 (Jiang et al., 2023). We use the chat- or instruction-tuned model variants and test each model as a candidate answer generator and as a judge.\n3.4 Inferences\nThis section describes the candidate answer generation and the judgment generation.\nCandidate answer generation. To judge two candidate answers (including of the same model), we sample two initial CoT solutions for each model using 4-shot prompting. We set the temperature to 0.9 to get two different solutions.\nJudgements. We choose the first candidate generation for each model and generate judgments for all 36 unique model combinations. If both models are the same, we take the second initial generation. We accommodate positional bias (Zheng et al., 2023; Koo et al., 2023) by evaluating the two candidate answers in both possible orders for each question and then taking the average correctness of the judgments as the final assessment. The judge has to choose if the first or second answer is correct. The prompt is zero-shot and applies CoT, the temperature is set to 0 for deterministic generation results."}, {"title": "4 General Performance", "content": "The experiments have multiple degrees of freedom: judges, candidate models, and datasets. Therefore, we first examine judgments per dataset, and secondly, we investigate judgments per candidate model pair. Afterwards, we provide evaluations for two applied questions.\n4.1 Performance per dataset\nWe begin by examining the judgment performance, i.e., how often the judge picks a correct answer, across different datasets. Therefore, we average the performance across all model pairs (MA, MB).\nSetup. Table 1 considers three cases where each case focuses on a specific subset of the datasets: Case (1) investigates the observed task performance P(AJ = T|A, B, D) where we evaluate the task performance using the answers chosen by the judges. Note that this includes samples where both candidate models give the same answer. Case (2) asks how often judges choose a correct answer given that the answers differ, i.e., P(\u2206\u2081 = T|A \u2260 B, D). Note that this may (and often does) include cases where both answers are incorrect. Case (3) gives the probability that the judge chooses the correct answer given that one answer is correct, and the other answer is incorrect, formally P(\u2206\u2081 = T|A \u2260 B,T \u2208 {A, B}, D).\nResults. We observe that large models outperform smaller models. Specifically, we see that Qwen 2 72B is the best judge, followed by Yi 1.5 34B. The performance of Llama 3 70B is, on average, comparable to that of Yi 1.5 34B. Note that performance in Case (1) and Case (2) is often quite low, especially for MATH, as there are many cases where the judge can only choose wrong answers. Importantly, we observe that smaller models with fewer than 10B parameters are unreliable judges. Especially, in Case (3), where a correct answer is provided, smaller models only achieve an accuracy of around 55%, barely better than random chance. Therefore, we focus on the four larger models as judges in the subsequent analysis.\n4.2 Performance per model combination\nThe comparative performance of model pairs offers insights into which model is better for the specific task or which combination of models yields the best results.\nSetup. Figure 2 illustrates the final performance P(\u2206 = T|A, B), indicating the probability of a judge choosing a correct answer given two models A and B. The results are averaged over datasets and presented as an upper triangular matrix due to symmetry. If both models in a pair are the same, A = B, we employ the second response generated with temperature sampling to introduce variation."}, {"title": "4.3 Do judges elicit task improvement?", "content": "One use case for LLM judges is to improve task performance. A potential application is to train on answers chosen by the judge (Yuan et al., 2024).\nSetup. Therefore, we test how often the performance of the answers chosen by the judge is better than the performance of the individual models. Formally, for all pairs of models MA, MB and datasets D, how often is the observed performance P(\u2206 = T|A, B, D) larger than max{P(A = T|D), P(B = T|D)}? In Figure 3 the green bar tests all model pairs, and the blue bar only pairs where the judge is at least as good as the candidate models, i.e., P(J|D) \u2265 max{P(A\\D), P(B|D)}.\nResults. We see that only Qwen 2 72B increases the performance reliably. However, it is easier for the judge to improve performance if it compares answers of less or equally good candidate models."}, {"title": "4.4 Does the judge prefer the better model?", "content": "Another application of LLM judges is whether they can accurately identify which model performs better for a given task. This is crucial if we want to rank LLMs by their capabilities or if a practitioner wants to decide which model to deploy.\nSetup. To assess this, we evaluate the frequency with which a judge selects the superior model. For a candidate model pair MA, MB \u2208 M, always assume they are ordered, such that P(A = T|D) > P(B = TD). Then, specifically, we determine the proportion for which the judge chooses MA more often than MB, or formally, how often is P(\u2206j = MA|A, B, D) > P(\u2206j = MB|A, B, D) for all candidate pairs and datasets.\nResults. The judges consistently perform well in the selection of the better model. Notably, we find that Qwen 2 72B can only not rank the pair Mistral 7B v0.1 and v0.3 on the MATH dataset. This issue appears minor, as both models exhibit similarly poor performance on the challenging MATH dataset (with accuracies of 6.13% and 3.10%, respectively), meaning most judgments compare two wrong answers. Notably, already the worst judge, Mixtral 8x7B, performs well. In summary, we see that judges are more capable of aggregate-level rankings than instance-level rankings."}, {"title": "5 Analysis of Subsets", "content": "We investigate properties that occur when we use subsets based on the correctness of models or agreement between models."}, {"title": "5.1 Do task performances correlate with judgments?", "content": "We consider the subset of highest practical relevance where one candidate model is correct, and one candidate model is incorrect. The goal is to investigate the relationship between candidate model task performance and judgment performance.\nSetup. For all model pairs MA, MB \u2208 M, MA \u2260 MB we analyze subsets where MA is correct, and MB is incorrect. Note that we can always order MA and MB this way. Each plot in Figure 5 shows the relationship between judge performance, P(Aj = T|A = T, B = F) (Y-axis) and candidate model performance gap of MA and \u041c\u0432, \u0456.\u0435., P(A = T|D) \u2013 P(B = T|D) (X-axis).\nResults. The analysis reveals a strong correlation (Pearson's r\u00b2 > 0.69) between candidate model performance gap and judgment accuracy. If the performance gap is negative, we consider subsets where larger models are incorrect. Judges favor answers from larger models even when they are incorrect on these subsets. We hypothesize that this bias arises because larger models exhibit a specific writing style, articulating their responses more convincingly, thereby misleading the judges. This finding aligns with previous research identifying self-bias (Xu et al., 2024; Panickssery et al., 2024; Liu et al., 2024). However, our results indicate that this bias extends more broadly to the inherent quality of the underlying models on reasoning datasets. However, this is not necessarily a critical issue in practice, as the larger model tends to answer correctly more often (as indicated by the color of the points in Figure 5."}, {"title": "5.2 Does judgment quality depend on models' agreement?", "content": "We are interested in whether the level of agreement among models, i.e., how many models give a different answer for a sample, impacts the performance on the respective subset.\nSetup. We define disagreement buckets Sj, where each bucket contains instances for which exactly 1 \u2264 j \u2264 8 unique answers were given across all models. Formally, we set\nSj = \u222a {i \u2208 D | \\{Ma(i) | Ma \u2208 M}\\} = j}\nD\nwhere MA(i) is the answer of model MA for instance i. We analyze the results in two contexts: all comparisons, including those where both answers are correct or incorrect (cf. Figure 6(a)), and only instances where exactly one answer is correct (cf. Figure 6(b)). We average the performances of all judges and all candidate pairs."}, {"title": "6 Prediction of Judgements", "content": "We investigate whether predicting the judgments' outcomes is feasible. Firstly, we aim to predict performance statistics. Secondly, we aim to predict individual judgments.\n6.1 Can we predict judgment performance?\nOn the subset where exactly one answer is correct, we found a strong correlation between judgment performance and candidate task performances. This hints at regularities within the judging process, thus we aim to predict judge performance using model statistics.\nSetup. We fit six different linear regression models using the judgment performances as the target variables Y, including all variations of judges, model pairs MA, MB \u2208 M, and datasets D. Regarding the covariates X in the model, we distinguish between two setups: In Case (1), we solely use the task performances P(X|D), X \u2208 {J, A, B} of judge and candidate models, to predict judgment performance. In Case (2), we utilize statistics available without knowledge of the ground truth. The features for this case are the probability of agreement between the candidate models P(A = B|D) and the probability of model \u039c\u0391 being chosen. Since we are not specifically interested in the individual features' effects, but rather in their ability to explain the variation of judgment performance, we rely on the coefficient of determination, R2, for evaluation (Fahrmeir et al., 2013,, see Appendix E).\nResults. The results are shown in Table 2 (excluding data sets from the probability formulas for simplicity). We observe that the performance-related features of the models can almost perfectly explain the variation in final judgment performance (R2 = 97.50%), also when conditioning only on the subset of differing answers (R2 = 90.20%). Logically, P(A) and P(B), i.e., P(A\\D), P(B|D) respectively, have significant explanatory power for judgment performance, as they encompass all correct answers. In Case (2), we still observe a relatively high R\u00b2 value, indicating that the features can explain 50% of the target's variance.\n6.2 Can we predict which individual judgments?\nWe hypothesize that judgments are biased towards larger or better models because they incorporate linguistic cues or writing style into their judgments rather than purely relying on reasoning assessment. Therefore, we train a classifier to understand whether we can predict individual judgments.\nSetup. We separate all comparisons made per judge into training, validation, and test splits and train two classifiers. The test accuracy is reported in Table 3. The first model utilizes TF-IDF vectorization. We create two independent vectorizers for both answers. The resulting features are concatenated. A RandomForest classifier (Breiman, 2001) is then trained on these combined features. The second model is a RoBERTa model (Liu et al., 2020) trained on the full prompt presented to the judge."}, {"title": "7 Perturbation of Results", "content": "We aim to gain a deeper understanding of the extent to which writing style affects the final judgment. Therefore, we create an experiment perturbing the candidate answers and examine whether this changes the judgment.\nSetup. We examine two perturbtations: Swap and Mask. In the Swap experiment, we swap the final answer from model MA with that of model MB, while keeping their CoT reasoning unchanged. In the Mask experiment, we anonymize all numbers in both the CoT reasoning and the final answer by replacing them with \u201cX\u201d. Table 4 shows the frequency with which the judge selects the same answer (=), a different answer (\u2260), or fails/refuses to follow the output format and make a decision (Refused).\nResults. We observe that the new judgments in more than half the cases agree with the original judgment. In the Swap experiment, they even agree on average by 75% of the cases. We deduce that the judge is largely unaffected by the artificially introduced noise and heavily bases its decision on the writing style. Interestingly, in a substantial amount of samples (up to 17%) the judge refuses to make a judgment. On a positive note, manual inspection revealed that the model often realizes that the original answers were perturbed."}, {"title": "8 Discussion", "content": "Style and Quality. Our experiments suggest a relation between judgment and candidate task performance (cf. Section 5) and a relation between judgment and writing style (cf. Sec. 6 and 7). We hypothesize these two are interconnected and facets of the same underlying bias. When models become better, e.g., by being trained on larger amounts of data, their ability to write convincingly increases. Conversely, when an LLM demonstrates an increased ability to write convincingly, it likely acquires a more nuanced grasp of what humans perceive as compelling. This enhanced understanding likely also extends to task performance.\nGeneralizability of approach. Our in-depth analysis utilizes Formula (1) to segment judgment data based on correctness criteria, allowing for targeted investigation of specific subsets. This approach is generalizable and transferable to other NLP tasks, such as summarization. By incorporating discrete signals such as text topics, a similar derivation of the judgment probability is possible."}, {"title": "9 Conclusion", "content": "We conducted a thorough analysis of LLM judges on mathematical reasoning tasks. We include a detailed judgment performance evaluation of eight models on three datasets. We find that larger models are generally better than smaller models and that judges succeed in detecting the more capable model. Our analysis reveals a strong correlation between judgment performance and task performance of the models providing candidate answers which shows that judges tend to choose larger or better models. We hypothesize that LLM judges incorporate writing style into their judgments instead of purely analyzing the reasoning. We provide two experiments to provide evidence for this hypothesis. Finally, we want to emphasize the importance of impartiality and fairness in the role of LLM judges, similar to human judges in the real world. Our research introduces methods to quantify biases in favor of larger or better models, thereby offering a means to measure the reduction of such biases."}, {"title": "Limitations", "content": "Our analysis is primarily focused on mathematical reasoning datasets, which allows us to explore judgments through the lens of correctness within specific subsets. While this approach provides valuable insights, it limits the generalizability of our findings to other tasks or domains. Based on the fact that the investigated datasets are complex, in the sense that they need multi-step reasoning to be solved, and based on the fact that there is no thorough investigation of LLM judges on mathematical reasoning datasets yet, we think this work is a valuable contribution.\nIn our experiments, we focus on testing a single, specific prompt. It is common knowledge that LLMs are highly sensitive to variations in prompt phrasing, which can substantially influence their performance. Nevertheless, it is impossible for us to meet the computational demands necessary to run our experiments with multiple prompts.\nIn this study, we intentionally concentrate on open-weight models, motivated by our strong belief in the principles of open science. Open-weight models offer transparency and reproducibility, which are critical for advancing scientific understanding. However, we note that it is also interesting to study closed models to understand potential differences. Still, we are committed to research on open-weight models because we believe it benefits the community more."}, {"title": "A Experimental Setup", "content": "We provide further details on the general setup described in Section 3. Specifically, we include statistics and examples of the datasets, additional information on the models used, and the exact prompts employed in this study.\nA.1 Datasets\nAdditional information about the datasets is given in Table 5, which presents an overview of the dataset statistics. Note that for the MATH dataset, we only include the most challenging questions, called levels 4 and 5, in the dataset. Notably, it has ground truth answer sequences that are, on average, almost three times longer than those in other datasets.\nIn Table 6, we provide examples of questions and their corresponding answers from the ground truth. Note that these examples were used for few-shot prompting.\nA.2 Models\nWe execute all models using the VLLM software for LLM serving (Kwon et al., 2023). The weights for all models are accessible through Huggingface Transformers (Wolf et al., 2020).\nA.3 Prompts\nWe used two different prompts within this project.\nA.4 Infrastructure\nThe experiments were run on NVIDIA A100 and NVIDIA H100."}, {"title": "B General performance", "content": "This section provides additional information related to Section 4. Specifically, we present the task performance of all models across all datasets, as well as the judging performance of all models when used as judges.\nB.1 Task Performance\nIn various contexts in this work, the task performance of the individual models is essential. Therefore, we provide the accuracy of all models and all datasets in Table 9.\nB.2 Judging performance per model pair\nWe conduct experiments with all eight models serving as judges. We present the performance metrics of all judges across various model comparisons in Figure 9. As shown in Table 1, only the large models consistently produce judgments that deviate consistently from random chance. These results in Figure 9 support the superior performance of larger models."}, {"title": "C Additional subset experiments", "content": "This section provides additional information for chapter 5.\nC.1 Example Subset performance\nTo better understand the correlation observed in Figure 5, we provide examples of these subsets.\nC.2 Performance by agreement\nWe provide an extension of the results in the main paper. We put all samples into bins of how many different answers were given by the eight models."}, {"title": "D Prediction of Individual Judgements", "content": "Random Forest. We utilize TF-IDF representations with English stop word removal for the Random Forest model. We further remove all digits from the text. We set the maximum number of features to 10,000 and consider n-grams ranging from unigrams to bigrams (N-gram range: 1-2). The model uses 500 trees (estimators).\nRoBERTa. For the ROBERTa model (Liu et al., 2020), we use a batch size of 64 and a learning rate of 2e-5. The weight decay is set to 1e-3, and the model is trained for 8 epochs."}, {"title": "E Statistical Methodology", "content": "We describe the statistical background for the tests applied in Section 6.\nE.1 Coefficient of Determination\nThe coefficient of determination, R2, for evaluation of linear regression models (Fahrmeir et al., 2013) is defined as follows:\nR^2 = \\frac{\\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\nR2 measures the share of the variance in Y explained by its covariation with the features X included in the model by dividing the variation of the predicted values \u0177i by the variation of the true target values yi. If the features X have high explanatory power for Y, the \u0177i will be close to the yi and R\u00b2 will be close to 1, while in the extreme case of no correlation between X and Y the arithmetic mean is the best estimate (i.e., \u0177\u2081 = \u1ef9 \u2200 i = 1, . . ., n) resulting in R2 = 0.\nE.2 Overall-F-Test\nThe Overall-F-Test is built upon R2 and tests whether the overall model is of any significant value for explaining the variation of the target variable. The F-distributed test statistic is calculated as\n\\frac{\\frac{R^2}{p}}{\\frac{1-R^2}{n-p-1}}\nwhere R2 is the coefficient of determination, n is the number of observations, and p is the number of covariates included in the model (i.e., the number of estimated coefficients excluding the intercept).\nThe hypotheses that can be tested this way are\nH\u03bf: \u03b2\u2081 = \u03b22 = = \u03b2p = 0\nVS.\nH1: \u03b2; \u2260 0 for at least one j \u2208 {1, ...,p}.\nSo from a rejection of Ho, it can be concluded that at least one of the included features exhibits explanatory power for the variation of the target variable.\nE.3 Multiple Testing\nSince we conduct multiple statistical tests within the scope of one research project, it is important to consider multiple testing as a potential problem resulting in false positive findings."}, {"title": "F Peturbation of Results", "content": "In Section 7 we performed two experiments, where we 1) swap the results of the individual answers and 2) mask all numbers using an 'X' to understand whether the model only focuses on the writing style."}]}