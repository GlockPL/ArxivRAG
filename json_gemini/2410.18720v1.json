{"title": "GeoLoRA: Geometric Integration for Parameter Efficient Fine-Tuning", "authors": ["Steffen Schotth\u00f6fer", "Gianluca Ceruti", "Emanuele Zangrando", "Francesco Tudisco", "Jonas Kusch"], "abstract": "Low-Rank Adaptation (LoRA) has become a widely used method for parameter-efficient fine-tuning of large-scale, pre-trained neural networks. However, LoRA and its extensions face several challenges, including the need for rank adaptivity, robustness, and computational efficiency during the fine-tuning process. We introduce GeoLoRA, a novel approach that addresses these limitations by leveraging dynamical low-rank approximation theory. GeoLoRA requires only a single back-propagation pass over the small-rank adapters, significantly reducing computational cost as compared to similar dynamical low-rank training methods and making it faster than popular baselines such as AdaLoRA. This allows GeoLoRA to efficiently adapt the allocated parameter budget across the model, achieving smaller low-rank adapters compared to heuristic methods like AdaLoRA and LoRA, while maintaining critical convergence, descent, and error-bound theoretical guarantees. The resulting method is not only more efficient but also more robust to varying hyperparameter settings. We demonstrate the effectiveness of GeoLoRA on several state-of-the-art benchmarks, showing that it outperforms existing methods in both accuracy and computational efficiency.", "sections": [{"title": "INTRODUCTION", "content": "Large-scale pre-trained and fine-tuned models have significantly advanced the performance of deep learning models in assisting various natural language processing and computer vision tasks. However, their deployment often incurs substantial computational and memory costs due to the enormous number of trainable parameters. To address this, parameter-efficient fine-tuning (PEFT) methods have been developed, which modify a subset of model parameters while keeping the rest frozen. Among these, low-rank adaptation (LoRA) (Hu et al., 2021) has emerged as a prominent approach, allowing efficient fine-tuning by injecting low-rank updates into pre-trained model weights. Despite its efficiency, LoRA faces limitations in adaptively distributing the parameter budget across weight matrices, and its performance is sensitive to the choice of hyperparameters (Zhang et al., 2023).\nRecent works, such as AdaLoRA (Zhang et al., 2023), DyLoRA (Valipour et al., 2023), and ReLORA (Lialin et al., 2023), have attempted to improve LoRA by dynamically adjusting the rank of the low-rank adapters during training. While these methods enhance parameter efficiency, they are constructed as simultaneous descent methods and therefore do not guarantee convergence to optimal low-rank adapters. Methods that guarantee convergence to optimal adapters exist (Schotth\u00f6fer et al., 2022; Schotth\u00f6fer & Laiu, 2024; Zangrando et al., 2024). However, these require several gradient tapes per iteration and, therefore, have an intrinsically higher run time per training step.\nIn this paper, we introduce GeoLoRA (Geometric Low-Rank Adaptation), a novel dynamical low-rank training method for parameter-efficient fine-tuning. GeoLoRA leverages the dynamical low-rank approximation theory from matrix differential equations (Koch & Lubich, 2007b; Ceruti et al., 2022; 2023) and exploits the intrinsic low-rank geometry of the weight matrices to allocate the parameter budget across the model adaptively. This dynamic allocation is facilitated by a novel training strategy that updates the low-rank factors in parallel, contrasting with other recent methods based on dynamical low-rank approximation theory (Schotth\u00f6fer et al., 2022; Schotth\u00f6fer & Laiu, 2024; Zangrando et al., 2024), which require individual gradient tapes computed sequentially per each low-rank factor. Instead, GeoLoRA requires a single backprop pass over the small-rank adapters, limiting its computational cost and making it faster than popular baselines such as AdaLoRA (Zhang et al., 2023). Moreover, GeoLoRA maintains the exact orthonormality of the low-rank factors, avoiding the ill-conditioning issues associated with well-known high-curvature challenges arising in low-rank optimization (Schotth\u00f6fer et al., 2022).\nThrough extensive experiments on the GLUE benchmark, Vision Transformers, and Stable Diffusion, we show that GeoLoRA outperforms existing PEFT methods both in terms of accuracy and computational efficiency.\nAlong with the experimental evaluation, we provide a thorough convergence analysis, showing convergence to stationary points under standard assumptions, and a detailed error-bound analysis, demonstrating that GeoLoRA's low-rank adaptation remains close to its full-rank counterpart throughout the training process. This robustness is critical in ensuring that the fine-tuning process does not diverge, even under challenging conditions.\nOverall, the main contributions of this work are as follows:\n\u2022 We propose GeoLoRA, a dynamical low-rank training method for low-rank adapters that leverages low-rank geometry and matrix differential equations to achieve adaptive parameter allocation.\n\u2022 GeoLoRA only requires a single gradient tape and one small-size SVD per training step, making it competitive with existing baselines such as AdaLoRA.\n\u2022 We provide a convergence analysis and error bound guarantees for GeoLoRA, ensuring robust training behaviour and convergence to a stationary point.\n\u2022 Extensive experimental results demonstrate the superior performance of GeoLoRA over existing methods, with improved accuracy and training speed."}, {"title": "RELATED WORK", "content": "The growing size of neural networks has led to significant computational and memory challenges during both training and deployment. Several strategies have been proposed to mitigate these issues, including sparsification (Guo et al., 2016; Molchanov et al., 2017; He et al., 2017) and quantization (Wu et al., 2016; Courbariaux et al., 2016). Among these, layer factorization has gained traction as an effective approach to reducing memory requirements. Layer factorization techniques have been applied successfully in both pre-training (Wang et al., 2021; Khodak et al., 2021; Schotth\u00f6fer et al., 2022; Schotth\u00f6fer & Laiu, 2024; Zangrando et al., 2024; Zhao et al., 2024) and fine-tuning scenarios (Hu et al., 2021; Valipour et al., 2023; Zhang et al., 2023; Hayou et al., 2024; Zhao et al., 2024; Lialin et al., 2023), demonstrating their versatility across various tasks.\nLow-rank adapters such as LoRA (Hu et al., 2021) have become a standard approach for PEFT by applying low-rank corrections to pre-trained models. LoRA introduces a low-rank decomposition to the weight matrices of the model, significantly reducing the number of trainable parameters while preserving performance. Despite its efficiency, LoRA's effectiveness heavily relies on the selection of hyperparameters such as learning rates and parameter budgets (Zhang et al., 2023; Hayou et al., 2024). These limitations have spurred the development of rank-adaptive methods. AdaLoRA (Zhang et al., 2023) is a popular extension of LoRA, which dynamically adjusts the rank of the low-rank adapters during training. By incorporating an orthogonality regularizer and SVD-like adaptation, AdaLoRA aims to address the challenges of rank selection and adaptation. It outperforms static low-rank methods by automatically allocating parameter budgets based on the importance of each matrix component. DyLoRA (Valipour et al., 2023) provides an alternative approach that hierarchically adjusts the rank during training, demonstrating that higher-rank adapters can lead to better performance than very low-rank ones. DoRA (Mao et al., 2024) proposes to sample a set of rank-1 updates for each LoRA layer and to combine them into a rank-r update. Optimal rank-1 components are chosen during fine-tuning using an importance score based on the norm of the LORA layer.\nBeyond fine-tuning, low-rank methods have been successfully applied during the training and pre-training phases of neural networks. Techniques such as Pufferfish (Wang et al., 2021), intrinsic dimension reduction (Aghajanyan et al., 2020), and DLRT (Schotth\u00f6fer et al., 2022) suggest that large deep learning models have an inherently low intrinsic dimensionality, making them amenable to low-rank approximations. These methods propose reducing the number of parameters during training, potentially improving both efficiency and generalization. Recent works in dynamical low-rank training have explored the use of geometric properties of the low-rank parameter space to improve training stability and convergence. For example, the geometry-aware training approach for tensor layers in Tucker format (Zangrando et al., 2024) dynamically adapts the rank of the factorized layers, ensuring robust convergence even when the initial rank estimation is inaccurate. This method leverages the Riemannian geometry of the parameter space to avoid the ill-conditioning commonly encountered in low-rank training. ReLoRA (Lialin et al., 2023) introduces a parameter-efficient training method by using multiple low-rank updates to effectively train high-rank networks. This method allows training larger models with significant memory savings and training speed improvements compared to conventional methods. GaLore (Zhao et al., 2024) introduces a memory-efficient training strategy by projecting gradients onto a low-rank subspace. This approach achieves significant memory savings while maintaining performance."}, {"title": "LOW-RANK OPTIMIZATION: WHAT CAN GO WRONG", "content": "This section aims to discuss the nature of the critical points and optimization trajectories obtained when using gradient-based strategies for low-rank parameters, and why a straightforward application of gradient-based steps to factorized adapters may lead to suboptimal results.\nConsider a neural network layer of the form\n$$z = \\sigma(W_{pt}x + USV^Tx),$$(1)\nwhere \\( \\sigma \\) is an arbitrary activation function, \\( W_{pt} \\in \\mathbb{R}^{n \\times n} \\) are the frozen pre-trained weights, and \\( U, V \\in \\mathbb{R}^{n \\times r}, S \\in \\mathbb{R}^{r \\times r} \\) are the rank-r adapter weights, with input x. For simplicity, we omit the bias term. Low-rank adapters of the form \\( W = USV^T \\in \\mathbb{R}^{n \\times n} \\) have gained popularity in recent approaches such as (Zhang et al., 2023), although our discussion extends to other equivalent formulations like \\( W = AB \\) (Hu et al., 2021). The objective of the training process is to minimize a loss function \\( \\mathcal{L}(W) \\) to find an optimal adapter weight W. For full-rank matrices (r = n),\noptimality requires that \\( \\nabla_W \\mathcal{L}(W^*) = 0 \\). However, when r < n, this condition is generally unattainable due to the reduced parameter space. In this scenario, we seek a matrix \\( W \\) that is locally optimal within the low-rank parameter space, meaning no further reduction in the loss function \\( \\mathcal{L} \\) is possible in the neighborhood of W. A necessary condition for local optimality can be expressed as\n$$P(USV^T)\\nabla\\mathcal{L}(W) = 0,$$\nsee e.g., (Sato, 2021, Theorem 3.4). For orthonormal U and V, the projection operator \\( P(USV^T)Z := UU^TZ(I - VV^T) + ZVV^T \\) represents the orthonormal projection of Z onto the tangent space at \\( USV^T \\). If \\( W_+ \\) is not a saddle point, then this condition ensures that no search direction within the tangent space of W can further decrease the loss. See also Appendix J. Note that this only guarantees local optimality, a limitation shared by all gradient-based optimizers."}, {"title": "THE PROPOSED METHOD", "content": "In this section, we introduce GeoLoRA (Geometric Low-Rank Adaptation) a novel low-rank fine-tuning method that integrates rank adaptivity, low-rank optimality, and memory and computational efficiency. Our method builds upon the parallel geometric low-rank integrator originally designed for model order reduction in high-dimensional PDEs (Ceruti et al., 2023), and it is equipped with loss descent, approximation bounds, and convergence guarantees. Notably, it improves upon existing dynamical low-rank methods, e.g. (Schotth\u00f6fer & Laiu, 2024; Zangrando et al., 2024; Schotth\u00f6fer et al., 2022) by updating basis and coefficients in parallel opposed to a sequential basis update and coefficient step. Moreover, only a single backward pass per iteration step is required through a novel evaluation strategy of robust gradients, thus doubling the wall-time performance. GeoLoRA is, therefore, the first low-rank training method solving the optimal gradient flow eq. (4) with training times per iteration comparable to standard simultaneous descent approaches to low-rank adaptation such as LoRA and AdaLoRA (Hu et al., 2021; Zhang et al., 2023). In particular, it improves upon these methods by providing robustness and convergence guarantees and demonstrating an overall improved performance and robustness to hyperparameters in numerical examples.\nStarting from an initial factorization Uo, Vo, So with initial rank ro, where So is diagonal and full-rank, GeoLoRA performs the following steps (also summarized in Algorithm 1):\n1. Perform a (stochastic) gradient step to compute the new variables \\( S^{\\text{new}} \\in \\mathbb{R}^{r_0 \\times r_0}, L^{\\text{new}} \\in \\mathbb{R}^{n \\times r_0}, \\) and \\( K^{\\text{new}} \\in \\mathbb{R}^{n \\times r_0} \\), as follows:\n$$\\begin{aligned}\nS^{\\text{new}} &= S_0 - \\lambda \\nabla_{S}\\mathcal{L}(USV)^T \\\\\nK^{\\text{new}} &= U_0S_0 - \\lambda \\nabla_{U}\\mathcal{L}(USV)S_0^{-1} \\\\\nL^{\\text{new}} &= V_0S_0 - \\lambda \\nabla_{V}\\mathcal{L}(USV)S_0^{-1}.\n\\end{aligned}$$(6)\nWe will see in Theorem 3 that using these variables mitigates the stiffness of the system in eq. (4) while approximating the optimal gradient flow. Note that the right-hand side gradients \\( \\nabla_{U}\\mathcal{L}, \\nabla_{V}\\mathcal{L}, \\) and \\( \\nabla_{S}\\mathcal{L} \\) can be evaluated with only one backward pass through the network using standard algorithmic differentiation techniques, halving the computational cost of existing geometric methods such as (Schotth\u00f6fer et al., 2022; Zangrando et al., 2024). Evaluation of the inverse \\( S_0^{-1} \\) induces no computational overhead since \\( S_0 \\) is diagonal at the start of each iteration.\n2. Augment the current bases \\( U_0, V_0 \\) to twice their rank using the gradient dynamics of the loss, which is encoded in \\( K^{\\text{new}} \\) and \\( L^{\\text{new}} \\), i.e.\n$$\\hat{U} = [U_0, \\tilde{U}] = \\text{ortho}([U_0, K^{\\text{new}}]) \\in \\mathbb{R}^{n \\times 2r_0} \\text{and }\\hat{V} = [V_0, \\tilde{V}] = \\text{ortho}([V_0, L^{\\text{new}}]) \\in \\mathbb{R}^{n \\times 2r_0}.$$(7)\nHere \"ortho\" denotes a column orthonormalization procedure such as the QR-algorithm. This augmentation step provides the low-rank adapter with a larger search space to increase the rank of its adaptation if the initial rank-guess ro was insufficient to fully capture the problem. Doubling the rank implies that in log(n) training iterations any rank can be captured by a rank one initialization, eliminating the need for tuning r as a hyperparameter, see Figure 2.\n3. Assemble the augmented coefficient matrix\n$$\\hat{S} \\leftarrow \\begin{bmatrix}\nS^{\\text{new}} & U^T \\tilde{U} K^{\\text{new}} \\\\\n\\tilde{V}^T L^{\\text{new}} V & 0\n\\end{bmatrix} \\in \\mathbb{R}^{2r_0 \\times 2r_0}$$(8)\nwhere we obtain the block entries \\( S^{\\text{new}}, L^{\\text{new}}, \\) and \\( K^{\\text{new}} \\) from eq. (6).\n4. Truncate redundant singular values si of \\( \\hat{S} \\) and the corresponding singular vectors, i.e. basis functions of \\( \\hat{U}, \\hat{V}, \\) using the criterion\n$$\\sum_{i = r_1+1}^{2r} \\sigma_i < \\theta,$$(9)\nwhere r\u2081 is the new rank of the factorization and \\( \\theta \\) is a tresholding hyperparameter. The singular values\ns\u2081 are obtained via the SVD of \\( \\hat{S} = PEQ^T \\in \\mathbb{R}^{2r_0 \\times 2r_0} \\). Then we determine the new factorization as\n\\( S_1 = \\text{diag}(s_1,..., s_{r_1}) \\in \\mathbb{R}^{r_1 \\times r_1}, U_1 = UP_{(1,...,r_1)} \\in \\mathbb{R}^{n \\times r_1} \\text{and } V_1 = VQ_{(1,...,r_1)} \\in \\mathbb{R}^{n \\times r_1}. \\) The\ntruncation threshold \\( \\theta \\) is chosen relative to the nuclear norm of the specific layer's current singular\nvalues, i.e. \\( \\theta = r ||S|| \\). Other norms, such as the 1-norm of the singular values si, are possible as\nwell. Thus, the truncation threshold determines how aggressively to prune each layer individually.\nAnalogously, the following global threshold similar to the one used in e.g. (Zhang et al., 2023; Ghadiri\net al., 2023; Idelbayev & Carreira-Perpinan, 2020)\n$$\\sum_{l=1}^L \\left[\\sum_{i=1}^{2r_e}s_{i,\\ell}\\right] - \\tau \\le \\sum_{l=1}^L \\left[\\sum_{i=1}^{L r_{1,\\ell}}s_{i,\\ell}\\right]$$(10)\ncan be considered by summing the singular values across all the layers l = 1, ..., L. To directly control the parameter budget, order se by descending by magnitude and selecting the largest ones first until either eq. (10) is violated or the budget is depleted."}, {"title": "PARAMETER INITIALIZATION", "content": "LoRA-type adapters (Hu et al., 2021) initilize the low rank matrices B, A with zero initialization of B, and Gaussian initialization of A. This ensures that the fine-tuning indeed starts at the pretrained state of the network, i.e., \u03c3(Wptx + ABx) = \u03c3(Wptx). For consistency with this initialization, the bases Uo and Vo can be initialized as random but orthonormal, whereas the coefficient matrix So has zero-initialization. In this first solve of eq. (6), we set S\u012b\u00b9 as the identity matrix. As a result, the first solve of eq. (6) is inconsistent with the optimal dynamics of eq. (4). However, all following iterations evolve the low-rank trajectory according to the optimal gradient flow. Since in the first iterations of a LoRA fine-tuning, the adapter is typically close to the original solution but far from the fine-tuning optimum, this inconsistency is irrelevant to the overall convergence behavior of the method. Alternatively, the required gradients can be computed with three individual gradient tapes in the first iteration, which does not require the inversion of So.\nThe proposed method can readily be used for dynamic low-rank compression (Schotth\u00f6fer et al., 2022; Zangrando et al., 2024) of pre-trained networks, where we consider a layer z = \u03c3(Wx), and approximate W \u2248 USV. Here, the initial parameters Uo, So, Vo are obtained by a truncated singular value composition of W.\nFinally, for low-rank pre-training of an untrained network with given architecture, i.e. predetermined layer dimensions n, but unknown rank r, the factors Uo, Vo are initialized randomly, but orthonormal and So is initialized randomly, but diagonal for easy initialization of So\u00b9.\nThe basis augmentation step is able to truncate to any rank within one iteration and able to augment the basis to any rank within a logarithmic number of iterations, thus the method is robust w.r.t the choice of the initial rank. We provide an ablation study in Figure 2."}, {"title": "ANALYSIS", "content": "In the following, we analyze Algorithm 1 under the general assumption that L is L-smooth with constant L and bounded with constant B.\nFor brevity of exposition we denote \\( W_t = U_t S_t V_t \\) as the low-rank factorization at iteration t evaluated with Algorithm 1, whereas \\( W_t \\) denotes the full-rank solution obtained by \u201cfull fine-tuning\" with stochastic gradient descent. Further, we denote by f(W, \u00a7t) the stochastic gradient of the network loss L w.r.t the low-rank weight W\u013e at iteration t, obtained by batch-gradient descent. The i.i.d random variable \u00a7t models the randomness in the training data batch at iteration t. Lastly, recall that P(W)Z denotes the orthogonal projection of the matrix Z onto the tangent plane of the manifold of rank-r matrices at the point Wr.\nAlgorithm 1 is an optimizer on low-rank manifolds: Theorem 1 shows, that the proposed scheme with stochastic gradients indeed decreases the training loss in each iteration, while optimizing on a manifold, and Theorem 2 yields stochastic convergence to a locally optimal stationary point.\nTheorem 1 (Stochastic descent estimate). Algorithm 1 with stochastic (mini-batch) gradients fulfills\n$$\\mathbb{E}_{\\xi_{t+1}}[\\mathcal{L}(W_{t+1})] \\le \\mathcal{L}(W_t) - \\lambda \\left(1 - \\frac{L\\lambda}{2}\\right) \\mathbb{E}_{\\xi_{t+1}}[||P(W)\\nabla \\mathcal{L}(W_t, \\xi_t)||^2] + L\\mathbb{E}_{\\xi_{t+1}} [||W_{t+1} - W_t||]$$(11)\nwhere \\( W_t, W_t^{\\prime}, W_{t+1} \\) are the low-rank weight matrices at the start of iteration t + 1, before, and after the truncation step, respectively.\nThe proof is provided in Appendix D. The above theorem yields a loss descent guarantee up to the two last terms on the right-hand side. The first term of the right hand side induces the step size criterion \\( \\lambda < \\frac{2}{L} \\), which resembles the step size criterion of full gradient descent, where the two right hand side terms read \\( -\\lambda \\left(1 - \\frac{L\\lambda}{2}\\right)||f(W_t)||^2 \\). This shows that the low-rank optimizer allows similar learning rates as a full fine-tuning setup, eliminating the need for the scaling parameter of LORA. The last term models the error introduced by the truncation step and is bounded by the user-determined cutoff threshold v, as \\( \\mathbb{E}_{\\xi_{t+1}} [|| W_{t+1} - W_t ||] \\approx \\nu \\). As the solution stabilizes in rank, the error term vanishes, and we obtain the following main convergence result:\nTheorem 2 (Convergence). Let \\( L > 0 \\) and \\( W_1 ,..., W_T \\) be the solutions generated by Algorithm 1 over T steps. Let the learning rate sequence \\( {\\lambda_t} \\) satisfy the Robbins-Monro conditions\n$$\\begin{aligned}\n&\\sum_{t=1}^{T-1} \\lambda_t = +\\infty \\\\\n&\\sum_{t=1}^{T-1} \\lambda_t^2 < +\\infty,\n\\end{aligned}$$\nand each step \\( \\lambda_t \\) the step size restriction \\( \\lambda_t < \\frac{2}{L} \\). Further assume \\( \\sum_{t=1}^{T-1} \\mathbb{E}[||W_{t+1} - W_t||] \\le D < \\infty, \\) i.e. after some time, the solution \\( W_t \\) is contained in a manifold of rank r. Then we have\n$$\\liminf_{T \\to \\infty} \\mathbb{E}[|| P(W_t)f(W_t)||^2] = 0,$$\nwhere the expected value is taken over all \\( \\xi_t \\).\nThe proof is provided in Appendix E. Additionally, the solution trajectory of Algorithm 1 is close to the (full-rank) trajectory of the dynamical system\n$$\\dot{W}(t) = -\\nabla_W \\mathcal{L}(W(t)),$$(12)\ni.e., the gradient flow of full training or fine-tuning:\nTheorem 3 (Error-bound). For an integer k, let \\( t = k\\lambda \\). Let \\( W(t) \\) be the solution of eq. (12), and let\n\\( W_k \\) be the factorized low-rank solution after k steps with Algorithm 1. Assume that for any Z in a\nneighborhood of \\( W (t), we have ||(I - P(Z))\\nabla \\mathcal{L}(Z)|| < \\epsilon \\), i.e., the gradient flow is close to \\( T_zM_r \\). Then,\n$$||W(t) - W_k|| < c_1 \\epsilon + c_2 \\lambda + c_3 \\sqrt{\\theta}/\\lambda.$$(13)\nMoreover, let \\( W_{RF}(t) \\) denote the solution of the Riemannian flow of eq. (4). Then,\n$$||W_{RF}(t) - W_k|| < c_4 \\epsilon + c_2 \\lambda + c_3 \\sqrt{\\theta}/\\lambda$$(14)\nwhere the constants \\( c_1, c_2, c_3, c_4 \\) depend only on L and B."}, {"title": "NUMERICAL RESULTS", "content": "DeBERTa for GLUE. We evaluate the performance of GeoLoRA by fine-tuning the 183 million parameter transformer DeBERTaV3-base (He et al., 2023) on the GLUE Benchmark (Wang et al., 2019) and compare the results in Table 2. For details on the methods, implementation, hyperparameter choices, and benchmark setup, please refer to Appendix B.1. In most cases, GeoLoRA outperforms other methods on the benchmark, achieving better metrics with significantly fewer trainable parameters. This reduction in trainable parameters allows GeoLoRA to process substantially more samples during training and evaluation compared to AdaLoRA.\nPerformance analysis. The proposed method from Algorithm 1 combines low-rank optimality guarantees with significant computational efficiency gains compared to existing low-rank optimization methods, as shown in Table 1. For a rank r adapter, the computational cost of gradient evaluation (i.e., eq. (6)) is equivalent to that of AdaLoRA, which updates U, S, and V directly, and is similar to a standard LoRA update. The cost of basis augmentation is O(nr\u00b2) due to the QR decomposition in eq. (7), comparable to evaluating the orthonormality regularization terms in AdaLoRA. Rank truncation is performed via an SVD of S at a cost of O(r\u00b3), where typically r < n. The complexity analysis shows comparable per-iteration costs for LoRA, AdaLoRA, and GeoLoRA. In Table 2, we also report the number of iterations computed per second during training and evaluation for both GeoLoRA and AdaLoRA, demonstrating that GeoLoRA outperforms AdaLoRA across almost all GLUE benchmarks. We note that training and inference speed depend on both layer ranks and sequence lengths, and the performance difference is less pronounced for benchmarks with longer sequences.\nVision transformer for object classification. We compare GeoLoRA and AdaLoRA on fine-tuning the Vit-base-patch16-224 Vision Transformer, pre-trained on the Imagenet-1k dataset, and fine-tuned on Cifar10, Cifar100, and Tiny-Imagenet. Details on implementation and hyperparameters are provided in Appendix B.2. Table 3 shows that GeoLoRA, with both global and local rank budgeting, achieves higher validation accuracy than AdaLoRA, while using fewer trainable parameters."}, {"title": "CONCLUSION", "content": "We introduced GeoLoRA (Geometric Low-Rank Adaptation), a novel adaptive low-rank fine-tuning method that combines computational efficiency with robustness. Based on geometric principles from dynamical low-rank approximation theory, the method comes with guarantees of convergence and local optimality. By leveraging a parallel update strategy of the low-rank adapters, the method requires only a single backward pass per iteration, achieving inference and training speed comparable or superior to existing baselines such as AdaLoRA, and much more efficient than previous geometric-aware strategies. Our experiments on the GLUE benchmark, Vision Transformers, and Stable Diffusion demonstrate that GeoLoRA outperforms existing PEFT methods in both accuracy and efficiency, with fewer trainable parameters. These results, alongside strong theoretical guarantees, position GeoLoRA as a robust solution for efficient model adaptation."}, {"title": "ALGORITHM FOR AUXILIARY FUNCTIONS", "content": "We present the auxiliary function for Algorithm 1 in Algorithm 2."}, {"title": "ADDITIONAL INFORMATION FOR THE NUMERICAL TEST CASES", "content": null}, {"title": "GLUE BENCHMARK", "content": null}, {"title": "DATASET DESCRIPTION", "content": "We compare GeoLoRA to several fine-tuning methods from recent literature in the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019). The GLUE benchmark is a collection of diverse natural language understanding tasks designed to evaluate the performance of models in comprehending and processing human language. GLUE provides a comprehensive assessment by including tasks that cover a range of linguistic phenomena, such as textual entailment, sentiment analysis, sentence similarity, and more. The benchmark consists of nine different tasks:\n\u2022 COLA (Corpus of Linguistic Acceptability): Classifying whether a sentence is grammatically correct or not.\n\u2022 SST-2 (Stanford Sentiment Treebank): Sentiment analysis task where the goal is to classify the sentiment of a sentence as positive or negative.\n\u2022 MRPC (Microsoft Research Paraphrase Corpus): Identifying if two sentences are paraphrases of each other.\n\u2022 STS-B (Semantic Textual Similarity Benchmark): Measuring the degree of semantic similarity between two sentences on a scale from 1 to 5.\n\u2022 QQP (Quora Question Pairs): Determining if a pair of questions are semantically equivalent.\n\u2022 MNLI (Multi-Genre Natural Language Inference): Classifying the relationship between a pair of sentences (entailment, contradiction, or neutral).\n\u2022 QNLI (Question Natural Language Inference): Determining if a sentence provides a correct answer to a given question.\n\u2022 RTE (Recognizing Textual Entailment): Binary classification task for entailment and contradiction.\n\u2022 WNLI (Winograd Schema Challenge): Resolving pronoun reference ambiguity in sentences.\nSpecific Focus: MRPC (Microsoft Research Paraphrase Corpus)\nWe present the benchmark overview in Table 5. To recapitulate, the F1 score is defined in dependence of precision score P and recall score R. The model precision P is given by\n$$P:=\\frac{P_{T}}{P_{T} + P_{F}},$$(15)"}, {"title": "REFERENCE IMPLEMENTATIONS", "content": "Full finetuning (FT): This is the most common approach for model finetuning and transfer learning. Here", "2022)": "Here", "2021)": "Two-layer adapters are inserted between transformer blocks. In (Houlsby et al."}, {"2021)": "As stated in Section 3", "2023)": "As stated in Section 3", "choices": "We compare the baselines under different budget levels, for example, given the total trainable parameters as 0.3/0.6/1.2 million. In order to match the parameter budget, we select the hidden dimensions of adapters from {8, 16, 32, 64}, set the rank r of LoRA as {2, 4, 8}, and choose the final budget b(T) of AdaLoRA from {144, 288, 576}. Then we set b(0) as 1.5 times of b(T) for AdaLoRA and select the regularization coefficient \\( \\gamma \\) from {0.1, 0.3, 0.5}. We set the exponential moving average parameters \\( \\beta"}]}