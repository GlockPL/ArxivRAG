{"title": "A General-Purpose Multimodal Foundation Model for Dermatology", "authors": ["Siyuan Yan", "Zhen Yu", "Clare Primiero", "Cristina Vico-Alonso", "Zhonghua Wang", "Litao Yang", "Philipp Tschandl", "Ming Hu", "Gin Tan", "Vincent Tang", "Aik Beng Ng", "David Powell", "Paul Bonnington", "Simon See", "Monika Janda", "Victoria Mar", "Harald Kittler", "H. Peter Soyer", "Zongyuan Ge"], "abstract": "Diagnosing and treating skin diseases require advanced visual skills across multiple domains and the ability to synthesize information from various imaging modalities. Current deep learning models, while effective at specific tasks such as diagnosing skin cancer from dermoscopic images, fall short in addressing the complex, multimodal demands of clinical practice. Here, we introduce PanDerm, a multimodal dermatology foundation model pretrained through self-supervised learning on a dataset of over 2 million real-world images of skin diseases, sourced from 11 clinical institutions across 4 imaging modalities. We evaluated PanDerm on 28 diverse datasets covering a range of clinical tasks, including skin cancer screening, phenotype assessment and risk stratification, diagnosis of neoplastic and inflammatory skin diseases, skin lesion segmentation, change monitoring, and metastasis prediction and prognosis. PanDerm achieved state-of-the-art performance across all evaluated tasks, often outperforming existing models even when using only 5-10% of labeled data. PanDerm's clinical utility was demonstrated through reader studies in real-world clinical settings across multiple imaging modalities. It outperformed clinicians by 10.2% in early-stage melanoma detection accuracy and enhanced clinicians' multi-class skin cancer diagnostic accuracy by 11% in a collaborative human-AI setting. Additionally, PanDerm demonstrated robust performance across diverse demographic factors, including different body locations, age groups, genders, and skin tones. The strong results in benchmark evaluations and real-world clinical scenarios suggest that PanDerm could enhance the management of skin diseases and serve as a model for developing multimodal foundation models in other medical specialties, potentially accelerating the integration of AI support in healthcare.", "sections": [{"title": "Introduction", "content": "There is a pressing need to fully harness the potential of artificial intelligence (AI) in diagnosing and managing skin diseases. Although deep learning has demonstrated remarkable performance, often matching or surpassing dermatologists, current AI models for dermatology remain limited to isolated tasks, such as diagnosing skin cancer from dermoscopic images 1. These models struggle to integrate diverse data types and imaging modalities, reducing their utility in real-world clinical settings. Dermatology, like internal medicine, is inherently complex, demanding a comprehensive, patient-centered approach. Diagnosing and treating skin cancer, for example, involves a range of tasks, including total body skin examination, risk assessment at both patient and lesion levels 2\u20135, differentiation of neoplastic from inflammatory diseases 6, multimodal image analysis 7,8, pathology interpretation 9,10, monitoring lesion changes 11,12, and predicting outcomes 13,14. The absence of integrated AI solutions capable of supporting these diverse workflows currently hampers the practical impact of AI in dermatology. Foundation models, however, hold the potential to address this gap by enabling a more holistic approach 15,16.\nFoundation models are large-scale neural networks pretrained on vast, diverse datasets using self-supervised learning techniques, often leveraging weakly labeled or unlabeled data 17\u201319. Built on rich knowledge representations, these models have demonstrated impressive performance across medical fields such as ophthalmology 20, radiology 21, and pathology 22-25. By leveraging large and diverse training datasets, these models are highly versatile in interpreting various data modalities 26, outperforming previous deep learning models in downstream tasks. Their strong feature representations also enable data-efficient applications 27,28, requiring fewer labeled samples, which is a crucial advantage in domains where labeled data are scarce.\nThe performance of foundation models is inherently linked to the scale of their parameters and training data 29-31. In general computer vision, foundation models are pretrained on massive datasets like ImageNet 32 or JFT-300M 33 and most existing dermatology AI models still rely on these models for downstream adaptation. Some efforts have focused on self-supervised learning specifically for dermatology using public datasets 34,35 or web-sourced skin images 36. However, these approaches are often limited by dataset size, diversity, or the lack of real patient data, which hinders the ability of models to generalize effectively across the diverse tasks and modalities encountered in clinical dermatology.\nHere, we introduce PanDerm, a general-purpose, multimodal dermatology foundation model pretrained on over 2 million images sourced from 11 institutions across multiple countries, covering 4 imaging modalities (Fig 1a-c). This dataset represents the largest and most diverse image collection in dermatology to date. In the pretraining stage, PanDerm employs a novel combination of masked latent modeling and CLIP 37 feature alignment for self-supervised learning (Fig 1e and Methods), demonstrating superior data scalability and"}, {"title": "Results", "content": "To evaluate PanDerm's effectiveness, we conducted ablation studies to examine the impact of pretraining data size and training epochs on downstream performance (datasets described in Extended Data Table 28). PanDerm demonstrated strong scalability, with consistent AUROC improvements as pretraining data increased from 0.8 to 1.8 million images (Fig 1f left). Notably, PanDerm outperformed SwAVDerm 36, a recent dermatology self-supervised learning model, using only 0.8 million images compared to SwAVDerm's 1.2 million (Fig 1f left). Furthermore, PanDerm demonstrated superior training efficiency compared to state-of-the-art self-supervised learning models, achieving better performance with only 200 training epochs, while MILAN 39 required 500 epochs, and both DINOv2 40 and MAE 19 needed 800 epochs (Fig 1f right), all using the same pretraining dataset. This efficiency stems from PanDerm's novel use of CLIP as a teacher model for semantic feature learning. PanDerm also surpassed vision-language models such as CLIP 37, MONET 41, and BiomedCLIP 42 in benchmark evaluations (Extended Data Table 28). Additionally, It demonstrated emergent capabilities in dermatology similar to those of DINOv2 in natural images, with linear probing performance comparable to full-parameter fine-tuning (Extended Data Table 29). Building on these promising initial results, we extended our evaluation to a broader range of dermatological tasks, primarily comparing PanDerm with three representative pretrained models in the following sections: SL-Imagenet32 and DINOv240 (both widely-used foundation models pretrained on natural images with a ViT-large 38 backbone), and SwAVDerm36\nAblation studies and comparison with other self-supervised learning strategies"}, {"title": "Diagnostic performance and generalization ability over various datasets", "content": "We systematically evaluated PanDerm diagnostic performance across 10 public datasets from 4 imaging modalities and 7 international sites (Fig 2a). These datasets covered multi-class classification of pigmented neoplastic lesions and binary melanoma diagnosis tasks. We primarily evaluated performance using weighted F1 score for multi-class datasets and area under the receiver operating characteristic curve (AUROC) for binary class datasets. PanDerm consistently outperformed all other models, achieving significant improvements on 9 out of 10 datasets, with average gains of 5.1%, 8.0%, 4.2%, and 0.9% on dermoscopic, clinical, TBP, and pathology datasets, respectively (Fig 2a). On benchmarks like HAM10000 35 and PAD 43, PanDerm surpassed the next-best models by 4.7% (P < 0.001) and 9.0% (P < 0.001), respectively (Fig 2a; Extended Data Table 1 and Extended Data Fig 3).\nWhen evaluating label efficiency generalization, PanDerm consistently outperformed other models across all datasets (Fig 2b, Extended Data Table 15-20). PanDerm matched the next-best model's performance using only 10% to 30% of labeled data for skin lesion diagnosis across various modality datasets (Fig 2b), demonstrating PanDerm's significant advantage in limited data scenario. Additional results for other tasks are presented in Extended Data Fig 4. To further assess PanDerm's generalization ability, we extended a previous out-of-distribution generalization experiment 44,45 to evaluate model performance for melanoma diagnosis on both dermoscopic and close-up clinical images from 7 external datasets. These datasets were collected from multinational institutions, representing populations distinct from the training data. PanDerm demonstrated significant superiority over all pretrained models, achieving higher AUROC scores across all external datasets (Fig 2c). Notably, PanDerm showed a significant improvement even on clinical images not used during fine-"}, {"title": "Reader study 1: Early melanoma detection compared with clinicians", "content": "We conducted a reader study to compare PanDerm's performance in early melanoma detection using sequential images. We used a dermoscopic image dataset from Alfred Hospital 47, featuring multiple follow-up images of the same lesions over time. The study evaluated two key aspects: overall diagnostic accuracy and early melanoma detection capability. PanDerm's performance was compared to that of 12 human reviewers (7 experienced dermatologists and 5 registrars). In terms of overall accuracy, PanDerm outperformed the average human reviewer by 10.2% and surpassed the best-performing human by 3.6%. For early detection, we assessed the earliest time point of correct malignant diagnosis for each melanoma case. PanDerm demonstrated superior ability in this challenging task, correctly identifying 77.5% (69 out of 89) of melanoma lesions at the first imaging time point, compared to only 32.6% (29 correct diagnoses) for human reviewers. Individual dots in the histograms of Fig 2e represent the earliest correct diagnosis time points for both PanDerm and human reviewers, visualizing the comparative early detection performance. Further details on the reader study setup and datasets are provided in Methods."}, {"title": "Lesion monitoring and change detection in sequential images", "content": "Monitoring suspicious melanocytic lesions over a three-month period is a widely accepted procedure for early melanoma detection, as changes often prompt excision to rule out melanoma, whilst stability can be reassuring 12. We assessed PanDerm's ability for short-term lesion change detection by framing it as a binary classification task: determining whether a pair of short-term lesion images shows change or remains stable. A Siamese network architecture 48, which is well-suited for comparing image pairs to detect subtle changes, was employed."}, {"title": "Metastasis prediction and prognosis", "content": "We evaluated PanDerm's potential to identify digital biomarkers for melanoma progression using dermoscopic images, a relatively underexplored but clinically valuable area 13,14,49 (Fig 3f). The evaluation used the Combin-Mel dataset, an international multi-center cohort with 680 dermoscopic images of invasive primary melanoma from 370 patients. PanDerm's performance was assessed for predicting melanoma metastasis (Fig 3g, details in Methods). In binary classification (control vs. metastasis), PanDerm achieved an AUROC of 0.964 (95% CI 0.937-0.991), outperforming the next-best model by 2.0% (P = 0.073) (Fig 3e). For the more challenging three-class classification (control vs. local metastasis vs. distant metastasis), PanDerm significantly outperformed the next-best model by 2.8% (P < 0.05) in weighted F1 score (Extended Data Table 6)."}, {"title": "Skin phenotype, risk assessment and malignant lesion screening using TBP", "content": "We evaluated PanDerm's performance in skin phenotype, risk assessment, and malignant lesion screening using 3D TBP 2,3,50 (Fig 4a). Unlike dermoscopy, TBP allows for the integration of broader patient-level data, facilitating the assessment of risk factors such as photodamage and nevus count, both critical for melanoma"}, {"title": "Skin lesion segmentation", "content": "We evaluated PanDerm's performance on skin lesion segmentation using two benchmark datasets: ISIC2018 (task 1) 55 and HAM10000 35. PanDerm was compared against SL-Imagenet, autoSMIM 34, and BATFormer 34. PanDerm significantly outperformed all models, surpassing the next-best by 3.1% and 1.9% in Jaccard index (JAC) on ISIC2018 and HAM10000, respectively (P < 0.001; Extended Data Fig 1a, b). Its Dice score (DSC) was also significantly higher than that of all other models across both datasets (P < 0.001). PanDerm's performance was particularly noteworthy in label-limited scenarios, matching the next-best model while using only 5% of the training data (104 and 350 images for ISIC2018 and HAM10000, respectively; Extended Data Fig 1c, d). To further validate its capabilities, PanDerm was compared with MedSAM 56, a state-of-the-art medical segmentation foundation model, and outperformed it in JAC by 0.5% (P = 0.025 and 0.112;"}, {"title": "Reader study 2: Human-AI collaboration", "content": "To assess PanDerm's clinical applicability, we conducted a reader study to investigate its potential to improve clinicians' diagnostic accuracy. The study included 41 human raters with diverse experience levels. We compared their accuracy in diagnosing seven classes of pigmented lesion classes using dermoscopic images, both with and without PanDerm's multi-probability prediction support (Fig 5a). PanDerm's support significantly increased overall diagnostic accuracy from 0.69 (95% CI 0.65-0.73) to 0.80 (95% CI 0.76-0.84, P < 0.001; Fig 5b). A subgroup analysis revealed that raters with less experience benefited the most, showing accuracy improvements of 17% (P = 0.0082) for those with low experience and 12% (P < 0.001) for those with medium experience, while highly experienced raters showed a 6% improvement (P = 0.039; Fig 5c, Extended Data Table 25). The class-specific analysis demonstrated significant accuracy improvements in 4 out of the 7 classes (P < 0.05; Fig 5d, Extended Data Table 24). For melanoma specifically, PanDerm improved the accuracy of human raters from 0.69 (95% CI 0.64-0.74) to 0.83 (95% CI 0.79-0.87, P < 0.001). Additional details on the reader study can be found in the Methods section."}, {"title": "Discussion", "content": "Despite significant advances in AI technology, its application in clinical medicine remains fragmented and underutilized. Current AI systems are often restricted to isolated tasks, unable to address the diverse demands of medical decision-making. This limits AI's potential in supporting clinicians in disease diagnosis and management. Dermatology, with its complex requirements including holistic patient assessment, lesion-specific analysis, and potential use of various imaging modalities, serves as an ideal use case for demonstrating AI's capabilities across multiple interconnected clinical tasks. Success in this domain could pave the way for broader adoption of AI models across healthcare.\nIn this study, we introduce PanDerm, a versatile dermatology foundation model trained through self-supervised learning on over 2 million multimodal dermatological images. Central to PanDerm's development was the curation of a large and diverse image dataset sourced primarily from in-house collections and carefully selected public repositories. This approach contrasts with previous efforts, such as SwAVDerm 36, which relied on web-sourced skin data, inadvertently incorporating images from commonly used benchmarks like ISIC 57 and DermNet 46, increasing the risk of data leakage and compromising evaluation validity. Our strategy minimizes this risk, ensuring that benchmark evaluations accurately reflect real-world model performance.\nTo assess PanDerm's potential for extensive clinical applications, we evaluated the foundation model across 28 benchmark datasets encompassing the most common dermatology-related clinical tasks. PanDerm outperformed existing models in skin cancer-related tasks, including risk stratification of individual phenotypes and lesions, detection of lesion changes and malignancy, and metastasis prediction and prognosis. Moreover, it demonstrated superior performance in diagnosing neoplastic and inflammatory skin diseases. Notably, Pan-Derm achieved these results often using significantly less training data (5-10%) than existing models, addressing the critical challenge of acquiring high-quality labeled data that typically require specialist input. Reader studies further validated PanDerm's real-world applicability, providing evidence of improved diagnostic accuracy with AI support. Collectively, these findings underscore PanDerm's transformative potential, enhancing AI integration into clinical workflows.\nThe scaling behavior observed in PanDerm's performance, particularly in relation to pretraining dataset size, aligns with recent trends in foundation model development 20,22,23. However, our study uniquely demonstrates this phenomenon in dermatological AI, where large-scale data acquisition poses distinct challenges. The positive impact of increasing dataset size and diversity on model performance underscores the importance of collaborative data collection efforts across institutions and countries. Our comparative analysis also provides key insights into the effectiveness of different self-supervised learning approaches. Using CLIP 37 as a teacher model for semantic feature alignment resulted in superior training data efficiency (Fig 1f), significantly outper-"}, {"title": "Methods", "content": "We curated an extensive pretraining dataset comprising 2,149,706 unlabeled multimodal skin images to develop PanDerm. This diverse dataset encompasses 4 imaging modalities and 11 data sources, including total body photography (TBP), dermatopathology, clinical, and dermoscopic images. The composition of the dataset is as follows: 757,890 (35.3%) TBP tiles, 537,047 (25.4%) dermatopathology tiles, 460,328 (21.4%) clinical images, and 384,441 (17.9%) dermoscopic images. The inclusion of multiple imaging modalities aims to provide a comprehensive representation of skin lesions and conditions, enabling the model to learn robust features across different visual representations. This large-scale dataset serves as the foundation for pretraining PanDerm, allowing it to capture the intricate patterns and characteristics of various skin conditions across different imaging techniques.\nThe MYM cohort 53 is an in-house dataset designed to study the natural history of melanocytic naevi. This cohort recruited individuals from the general population who were 18 years or older and had at least one mole. The HOP study 52 is an in-house sequential dataset targeting high-risk melanoma individuals. Inclusion criteria for the HOP study were: at least one melanoma (including in situ) diagnosed before the age of 40 years, two or more melanomas (including in situ) diagnosed before the age of 65, a strong family history (two or more first-degree relatives affected) and/or known pathogenic mutations in a hereditary melanoma gene, and/or a diagnosis of dysplastic naevus syndrome. For both cohorts, 3D TBP was conducted using a VECTRA WB360 (Canfield Scientific Inc., Parsippany, NJ, USA), which instantaneously captures 92\ncross-polarised 2D images with standardized lighting and subsequently merges them to create a 3D avatar. The average number of lesion tiles per subject from TBP was approximately 500. Demographic factors were collected using standard questionnaires, with clinical characteristics collected by research assistants. The final dataset of automatically detected lesion image tiles \u2265 2 mm in diameter comprises 405,856 images. More detailed information about these two datasets can be found in our previous studies 52,53.\nThe MYM and HOP datasets also contain 38,110 dermoscopic images from suspicious lesions identified during the studies. These images provide a complementary view of the lesions of interest, offering detailed visualization of surface and subsurface structures that may be indicative of various skin conditions, particularly melanoma.\nThe MMT dataset is an in-house collection amassed from over 150 clinics across Australia and New Zealand over a 15-year period. This extensive dataset primarily consists of paired polarized dermoscopic and clinical images. From this comprehensive collection, we curated a subset containing 316,399 dermoscopic images and 310,951 clinical images, providing a rich source of pretraining data for training purposes.\nThe ACEMID dermatopathology dataset is an in-house collection comprising 98 slides, which have been processed to yield 80,312 patch images.\nThe NSSI dataset is an in-house sequential collection containing 29,832 dermoscopic images. These images were collected as part of the Brisbane Naevus Morphology Study, conducted from approximately 2009 to 2014. The images were captured using a digital dermatoscope attached to a Fotofinder ATBM imaging system, resulting in images of approximately 0.05MB, with dimensions of 768 \u00d7 576 pixels at 96 dpi. The dataset includes up to 7 time points per participant, scheduled every 6 months over a 3-year period, with some variation due to late enrollment, scheduling non-adherence, or loss to follow-up. Individual lesions were assigned a consistent number across visits, facilitating temporal analysis.\nThe Edul and Edu2 datasets are curated from in-house educational resources, such as educational notes and materials. These datasets contain 81,947 and 67,430 clinical images, respectively, providing a diverse range of examples that are typically used for training medical professionals in dermatology.\nISIC2024 50 is an open-source TBP-based dataset designed for identifying skin cancers among skin lesions cropped from 3D total body photographs. For our pretraining purposes, we selected a subset of the dataset, stratified by institutions, containing 352,034 tile images.\nThe TCGA-SKCM dataset 69 is derived from The Cancer Genome Atlas (TCGA) project, which characterized the mutational landscape of human skin cutaneous melanoma (SKCM). This dataset contains 475 slides, which have been processed into 377,764 patch images.\nThe UAH89k dataset 70 is a subset of a larger collection, comprising 269 histopathology whole slide images (WSIs) sourced from the archives of the Institute of Pathology, Heidelberg University, the MVZ for Histology, Cytology and Molecular Diagnostics Trier, and the Institute for Dermatopathology. This dataset provides additional histopathological data, further enriching the model's understanding of skin conditions at the microscopic level.\nPanDerm is a self-supervised learning model designed for the dermatology field, built upon the success of existing self-supervised learning techniques in the natural image domain 71. At its core, the architecture comprises a ViT-Large visual encoder 38, a mask regressor, and a CLIP-Large 37 teacher model. The ViT-Large encoder, with its 24 transformer blocks and 1024 dimensional embeddings, processes 224 \u00d7 224-pixel images, while the CLIP-Large teacher model handles slightly smaller 196 \u00d7 196-pixel inputs. The training process incorporates two primary objectives: masked latent alignment and visible latent alignment loss. Initially, the input image undergoes masking, with the mask ratio proportional to the encoder's complexity (50% for ViT-Large). The encoder then processes visible patches to produce latent representations, while the regressor predicts the latent representations of masked patches using these visible latent and mask tokens. The model focuses on the encoder-regressor structure without a separate decoder component. The regressor assumes the responsibility of predicting the latent representations of masked patches, allowing for more efficient processing and learning. For target supervision, the unmasked image is fed through the CLIP model, generating supervision divided according to visible and masked patch locations. The visible latent alignment loss is directly applied to the latent representations of visible patches computed by the encoder. Concurrently, the masked latent alignment loss acts on the latent representations of masked patches predicted by the regressor. Both of these loss functions use CLIP latent representations as their supervision signals. The regressor in PanDerm operates similarly to a cross-attention mechanism. It uses learnable mask tokens as queries, while the keys and values are derived from the concatenation of visible patch representations and the output of previous layers. This design allows the regressor to effectively infer the content of masked regions based on the context provided by visible areas. Optimization primarily focuses on aligning the visible and masked patch predictions with their corresponding CLIP latent supervisions. This approach enables PanDerm to extract rich, semantically meaningful representations from dermatological images without relying on explicit labels.\nFor pretraining, we continued to train the model (initially trained on ImageNet-1K) on our dataset of over 2 million unlabeled multimodal skin images, representing diverse dermatological conditions. We set the batch size on each GPU to 480, with an effective batch size of 1920. Following masked image modeling practices 72,"}, {"title": "", "content": "we used a 50% mask ratio. To pretrain our model, we used AdamW as the optimizer with an initial learning rate of 1.5e-3. We apply simple data augmentation such as random resized cropping and horizontal flipping during pretraining. We trained our model for 500 epochs with a warmup of 20 epochs. The pretraining phase used 4 80GB NVIDIA H100 GPUs and took approximately 5 days and 7 hours. We chose the last epoch checkpoint as our final model weights. Please refer to Extended Data Table 31 for more detailed pretraining hyperparameter configurations.\nPrior work 39,71,72 demonstrated that target representations produced by the teacher model for masked image modeling are essential for impacting model performance. We ablated different teacher models, including two widely used models that demonstrated promising performance (CLIP-base and CLIP-large), a biomedical domain-specific CLIP (BiomedCLIP42), and a dermatology-specific CLIP (MONET41). We observed that CLIP-large pretrained on the natural domain can outperform biomedical-specific CLIP and dermatology-specific CLIP. This can be attributed to the limited data scale of skin images in medical domain CLIP models. Thus, CLIP-large remains the best teacher model for creating target representations for masked image modeling in dermatology. Based on these findings, we selected CLIP-large as the teacher model; the performance of our model when incorporating CLIP-large teachers was significantly improved and also outperformed CLIP-large itself. Please refer to Extended Data Table 28 for detailed results.\nOne emergent capacity of foundation models in the natural image domain is that the model's features are ready for downstream tasks without needing to fine-tune the encoder model, such as in DINOv2 40. We explored whether PanDerm could achieve this capacity despite having different training objectives. We found that our model using simple linear probing can perform comparably with expensive full-parameter fine-tuning. This suggests that PanDerm's features are already well-suited for diverse downstream multimodal skin-related tasks without requiring further training. Detailed results are in Extended Data Table 29.\nFor self-supervised learning methods comparison, we primarily evaluated DINOv240, MAE19, and MILAN39, all utilizing the same ViT-large backbone. We employed the recommended hyperparameter configurations for these models and continued pretraining from their natural image training weights on our pretraining dataset. Subsequently, we fine-tuned these models using identical hyperparameter setups to ensure a fair comparison.\nIn adapting PanDerm to downstream tasks, only the encoder model is utilized. For most tasks, PanDerm's feature quality suffices to achieve competitive performance using simple linear probing. This involves applying a linear classifier (i.e., logistic regression) to the top of extracted features from the PanDerm encoder to evaluate its performance on downstream tasks. For more challenging tasks requiring higher performance, we opted to fine-tune the PanDerm encoder. The fine-tuning tasks include the two reader studies, short-term change detection, skin lesion segmentation, skin cancer detection in ISIC2024, and TBP-based risk stratification. For all other tasks, we employed linear probing. For linear probing, following practices recommended by the self-supervised learning community, we fix the $l_2$ regularization coefficient $\\lambda$ to $\\frac{MC}{100}$, where $M$ is the embedding dimension and $C$ is the number of classes, and employ the L-BFGS solver with a maximum of 1,000 iterations. For fine-tuning, we adhere to the BEiT V2 setting72, utilizing cross-entropy loss with a learning rate of 5 \u00d7 10-4. We train models for 50 epochs with a warmup of 10 epochs. The model exhibiting the best performance on the validation set is selected as the final model. For detailed hyperparameter configurations, please refer to Extended Data Table 32. In the following sections, we describe tasks with more specific methodological details.\nOur proposed sequential data preprocessing method consists of dark corner removal, skin inpainting, hair removal, image registration, and lesion segmentation. For the first two steps, we follow the approach outlined in 73. Given an image with or without dark corner artifacts (DCA), we first convert it to grayscale. We then extract the contour by applying OpenCV's 74 binary threshold function to the grayscale image, empirically setting the threshold at 100, and using the find-Contours function with RETR_TREE mode and CHAIN_APPROX_SIMPLE method. We identify the largest area contour in the image, which most closely matches the edge of the DCA, by calculating the area of all existing contours. Using OpenCV's minEnclosingCircle function, we capture a circular area that encompasses this largest contour. To mitigate the effect of gradient colors at the dark corner edges, we scale this circle down to 80% of its original radius and convert it into a binary mask. Finally, we inpaint the original image using this mask, employing OpenCV's implementation of the Telea algorithm with an inpaint radius of 10. Following dark corner removal and inpainting, we implement a hair removal step to further improve image quality and facilitate more accurate registration. This process begins by converting the image to grayscale. We then apply a black hat morphological operation using a 17\u00d717 structuring element to isolate dark, thin structures (hairs) from the background. The resulting image is thresholded to create a binary mask of the detected hair structures. Finally, we use OpenCV's inpaint function with the Telea algorithm to fill in the hair regions, effectively removing them from the image. This hair removal step is crucial for improving the accuracy of subsequent image registration and analysis. For image registration, we implement an AKAZE 75 feature-based approach. The process begins by detecting key points and computing descriptors using the AKAZE algorithm, which is particularly effective for non-linear scale-spaces. We utilize OpenCV's AKAZE_create function, setting the descriptor size to 0, threshold to 9 \u00d7 10-5, and number of octaves to 4. Keypoint matching is performed using a Brute Force matcher with Hamming distance and cross-checking enabled. To refine the matches and esti-"}, {"title": "", "content": "mate the transformation between images, we employ the RANSAC (Random Sample Consensus) algorithm implemented via skimage.measure.ransac. This estimates an EuclideanTransform model, which accounts for rotation and translation between the images. The resulting transformation is then applied to warp one image onto the other using skimage.transform.warp with reflection padding and linear interpolation.\nSimilar to 48, we employ a simple Siamese network architecture for change detection, where two identical visual encoders with shared weights from our foundation model process a pair of sequential lesion images captured over a short time frame. Each encoder extracts features from its respective image. These learned features are then concatenated and passed through two fully connected layers, followed by a softmax layer for final classification. For training this siamese network in our binary change detection task, we use a contrastive loss function. This loss is particularly well-suited for Siamese networks as it helps the model learn to distinguish between pairs of images that have changed and those that have not. The contrastive loss encourages the network to minimize the distance between feature representations of image pairs with no significant changes while maximizing the distance for pairs that show meaningful changes. This approach allows the network to learn a similarity metric between image pairs, rather than simply classifying individual images. By doing so, it becomes more sensitive to subtle changes between images and more robust in detecting clinically relevant lesion changes over time. The contrastive loss thus helps the model focus on learning features that are most relevant for distinguishing between changed and unchanged lesion pairs, improving its overall performance in change detection tasks.\nWe fine-tuned our foundation model on the private SDDI-Alfred dataset47 using a ten-fold cross-validation approach. We utilized cross-entropy loss with a learning rate of 5\u00d710-4. We train models for 50 epochs with a warmup of 10 epochs. The model exhibiting the best AUROC on the validation set is selected as the final model. We then employed an out-of-fold (OOF) prediction approach to generate melanoma predictions for all sequential images. For each image sequence, we recorded the time point at which the model first made a correct diagnosis of melanoma; otherwise, the model was considered to have failed in detecting the melanoma. For the human evaluation, 12 clinicians\u2014seven dermatologists with over five years of experience and five registrars with less than five years of experience\u2014were invited to assess the serial dermoscopic data. The images were presented to the reviewers using QualtricsTM (Provo, UT, USA), with the reviewers blinded to the true diagnoses. For each case, information such as the patient's age, sex, lesion location, and date of imaging was provided. Initially, only the first dermoscopic image in the sequence was shown, and reviewers were asked to classify the lesion as either benign or malignant. As they progressed through the sequence, side-by-side image comparisons were made available to assess changes over time. Once a diagnosis was submitted, it could not be revised. To mitigate bias, we included 10 single time-point melanoma images, preventing reviewers from assuming that the first image in a series was benign. We then compared the diagnostic performance of the clinicians with our model, focusing on the time point at which a malignant"}, {"title": "", "content": "diagnosis was first made by either the clinicians or the algorithm.\nWe employ a linear probing classifier on our foundation model to predict melanoma metastasis using dermoscopic images from the private CombinMel dataset. Our evaluation encompasses two scenarios: binary metastasis prediction and multi-class metastasis prediction. In the binary classification", "groups": "control (no metastasis), local/satellite/in-transit metastasis, and distant metastasis. To enhance the robustness and mitigate potential data selection bias, we perform five iterations of dataset splitting into training and testing sets, stratified by melanoma stage. The model is trained using this five-fold data. We linear probe PanDerm with the setting mentioned above. We then generate out-of-fold (OOF) predictions for all lesions and compare these to the ground truth for performance evaluation.\nSubsequently, we conduct a multivariate Cox regression analysis, incorporating the metastasis prediction score and clinical variables (age, sex, Breslow thickness, ulceration, dermal mitosis, melanoma subtype, and lesion location) to predict the recurrence-free interval (RFI). This analysis focuses on earlier stages of melanoma (stages I-II). We visualize the relative contribution of individual variables to prognosis prediction using a forest plot. To analyze the correlation between variables and RFI, we employ the Kaplan-Meier method. Patients are stratified into low-risk and high-risk groups based on their binary metastasis prediction scores (median value). The log-rank test is utilized to assess the classifier's ability to predict survival. To evaluate the predictive accuracy at various time points, we generate time-dependent receiver operating characteristic (ROC) curves and calculate areas under the curve (AUCs) at 3, 5, and 7 years. This approach allows us to assess the model'"}]}