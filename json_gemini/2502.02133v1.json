{"title": "Synthesis of Model Predictive Control and Reinforcement Learning: Survey and Classification", "authors": ["Rudolf Reiter", "Jasper Hoffmann", "Dirk Reinhardt", "Florian Messerer", "Katrin Baumg\u00e4rtner", "Shambhuraj Sawant", "Joschka Boedecker", "Moritz Diehl", "Sebastien Gros"], "abstract": "The fields of model predictive control (MPC) and reinforcement learning (RL) consider two successful control techniques for Markov decision processes. Both approaches are derived from similar fundamental principles, and both are widely used in practical applications, including robotics, process control, energy systems, and autonomous driving.\nDespite their similarities, MPC and RL follow distinct paradigms that emerged from diverse communities and different requirements. Various technical discrepancies, particularly the role of an environment model as part of the algorithm, lead to methodologies with nearly complementary advantages. Due to their orthogonal benefits, research interest in combination methods has recently increased significantly, leading to a large and growing set of complex ideas leveraging MPC and RL.\nThis work illuminates the differences, similarities, and fundamentals that allow for different combination algorithms and categorizes existing work accordingly. Particularly, we focus on the versatile actor-critic RL approach as a basis for our categorization and examine how the online optimization approach of MPC can be used to improve the overall closed-loop performance of a policy.", "sections": [{"title": "I. INTRODUCTION", "content": "Solving Markov decision processes (MDPs) online is a large and active research domain where different research communities have developed various solution approaches [1], [2], [3]. An MDP can be stated as the problem of computing the optimal policy of an agent interacting with a stochastic environment that minimizes a cost function, possibly over an infinite horizon. Two common approaches for obtaining optimal policies are model predictive control (MPC) and reinforcement learning (RL).\nWithin MPC, an optimization problem that approximates the MDP is solved online, involving a simulation of an internal prediction model. The optimized controls are applied to the real-world environment in a closed loop at each time step. Historically, MPC has been developed within the field of optimization-based control engineering, driven by the success of optimization algorithms, e.g., linear programming [4]. MPC design leverages domain knowledge to compose mathematical models, often by first principles.\nIn contrast, within the RL framework, a policy is expressed as a parameterized explicit function of the state. The policy is iteratively improved by interacting with the environment and adapting its parameters related to the observed cost. In general, RL algorithms are not required to use models of the environment, but often, models are used during training for offline simulation.\nBoth MPC and RL found their way into classical real-world control [5]. The applications differ depending on the availability of training data. MPC is used where measurement data is scarce and expensive, and the environment can be described by optimization-friendly models. On the contrary, RL is successfully implemented in settings where lots of training data can be generated [6], [7].\nBesides their sampling efficiency, several further advantages and disadvantages of both methods are nearly orthogonal [8], i.e., weaknesses of either approach are strengths of the other. For instance, RL struggles with safety issues [9], whereas MPC can guarantee constraint satisfaction related to a particular environment model [2]. This motivated many authors to combine the advantages and synthesize novel algorithms that build on both approaches.\nThis paper's contribution is twofold. We first analyze the properties and orthogonal strengths of MPC and RL. Secondly, we propose a systematic overview of how MPC can be combined with RL and provide an extensive literature overview guided by the proposed classification. This survey reviews practical and theoretical work and concludes with a section on available open-source software."}, {"title": "A. Related Work", "content": "Several works exist that shed light on various parts of the huge research fields of control systems, machine learning, and optimization. The author of [8] shows relations between MPC and RL on a conceptual level and includes a detailed discussion for discrete-time linear time-invariant constrained systems with quadratic costs. RL is contextualized from the perspective of control systems in [10] where MPC is mentioned briefly as one particular control technique. The authors of [9] survey methods for safe learning in robotics and also consider the role of MPC. In particular, the authors show how parameterized MPC formulations can be used to guarantee safety during learning and how safety can be integrated into RL in general. The survey of [11] provides an extensive overview of how machine learning is used for combinatorial optimization. While combinatorial optimization problems have a particular structure related to discrete decision variables, similarities can be observed in how artificial neural networks (NNs) are integrated into an optimization problem.\nThe author in [3] compares RL and MPC from the point of approximate dynamic programming and proposes a unified mathematical framework. In [12], [13], the RL-based program AlphaZero [14], [15] that plays games such as chess, shogi or go, is cast within an MPC framework. It is argued that the lookahead and rollout-based policy improvement used in online deployment plays a crucial role in enabling the success of the respective algorithms.\nA high-level survey on how general machine learning concepts are used within MPC is provided by [16]. The authors of [17] have the same focus on investigating general machine learning used as part of MPC and provide technical details for learning models, policy parameters, optimal terminal value functions, and approximations of iterative online optimization. RL is a minor topic of both surveys [16] and [17]. The survey [17] is well aligned with our proposed framework. For instance, it considers approximating the terminal value function to be treated in another category as closed-loop learning. In fact, our proposed framework can be seen as a complementary work to [3] and [17].\nFurther works compare MPC and RL for particular ap- plications. Similarly to [17], the authors in [18] compare how MPC and machine learning are combined exclusively for automotive applications. The authors in [19] compare RL and MPC specifically applied to energy management in buildings with a focus on data-driven MPC methods."}, {"title": "B. Notation", "content": "The MPC and RL literature use different symbols and notations for identical objects, as shown in the comparison [3]. The notation used within this paper is given in Tab. I. In this survey, mostly RL notation is used to unify the language but with few exceptions and the use of control literature synonyms for established language. For instance, we mostly use \u201cenvironment\u201d instead of \u201cplant\u201d or \u201csystem\u201d but may occasionally write \"system\u201d when it is more common in the context. Note that we exclusively use the term MDP that also refers to the conceptually equivalent term discrete-time stochastic optimal control.\nAlso, within the field of RL notation, ambiguities exist. In this survey, we mostly use RL in order to refer to model-free RL algorithms and include imitation learning (IL) due to their conceptual overlap. MPC that learns a model online is often seen as a variant of model-based RL [20]. Since the proposed synthesis approaches are not limited to model learning, and, moreover, model-based RL includes approaches that learn a model without the intention to be used in an optimization problem, we omit a detailed discussion on model-based RL. For the concatenation $z = [x,y]^T$ of column vectors x and z, we write z = (x,y). Given set A and set B, we denote with $B^A$ the set of all functions that map from A to B. Given a set A, we denote with Dist(A) the space of all probability distributions or probability measures over A. Let p be a probablity density of a probability distribution over A, then the support is defined as supp(p) := {a \u2208 A | p(a) > 0}."}, {"title": "C. Overview", "content": "On a high level, this paper is structured into (1) an introductory part, (2) a comparison, and (3) a classification scheme and survey of synthesis approaches. An overview is provided in Fig. 1.\nThe introductory part introduces in Sect. II the general problem setting. Sect. III and IV describe the main concepts behind RL and MPC and discuss how they aim at solving the general problem of Sect. II. Both of these sections are split into a conceptual and an algorithmic part, providing the basis for the remainder of this work.\nThe comparison in Sect. V highlights practical differences between both approaches and surveys applied comparisons. Experts in the field of MPC and RL may skip Sect. III, IV and potentially the comparison in Sect. V.\nThe remaining paramount sections are devoted to the classification and review of combinations of both approaches. In Sect. VI, essential concepts are introduced to categorize existing combination variants based on the actor-critic framework of RL. Following the proposed categorization, literature that uses MPC as an expert for training an RL agent is summarized in Sect. VII. The most widespread variants using MPC within the policy are outlined in Sect. VIII, and variants that use the MPC to determine the value function at a particular state are surveyed in Sect. IX. An additional Sect. X focusses on important theoretical results from the field of MPC and RL and is aligned with the previous categorization. Current open-source software is presented in Sect. XI.\nThe work is concluded and discussed in Sect. XII."}, {"title": "II. PROBLEM SETTING", "content": "This section describes the Markov decision process (MDP) framework a central concept for both RL and MPC.\nMDPs provide a framework for modeling a discrete-time stochastic decision process. An MDP is defined over a state space S, an action space A, and a stochastic transition model\n$P: S \\times A \\rightarrow Dist(S)$ (1)\ndescribing a probability distribution over the next states given a current state and action. A stage cost l(s, a) with l : S \u00d7 A \u2192 RU {0} defines the cost of each state-action pair, typically discounted by a factor $\u03b3\\in (0,1]$. The MDP [21] is then defined by the 5-tuple\n$M := (S, A, P, l, \u03b3)$. (2)\nSolving the MDP refers to obtaining actions that minimize the discounted stage cost, which is elaborated in the following. For solving MDPs we introduce stochastic policies $\u03c0 : S \u2192 Dist(A)$ that map a state to a probability distribution over possible actions. The value function $V^\u03c0 : S \u2192 RU {\u221e}$ is the discounted expected future cost starting from state s and following a policy \u03c0 defined by\n$V^\u03c0 (s) := \u0395[\\sum_{k=0}^\\infty \u03b3^k l(S_k, A_k) ]$ (3)\n$A_k ~ \u03c0(\\cdot | S_k), S_{k+1} ~ P(\\cdot | S_k, A_k), S_0 = s$.\nThe state $S_k$ and action $A_k$ describe a sequence of random variables generated by applying the policy \u03c0 on the MDP.\nThe aim of solving the MDP is to obtain an optimal policy \u03c0 that minimizes the expected cost for all states via\n$\u03c0^* en arg \\min_{\u03c0} V^\u03c0 (s)$. (4)\n$s \\in S$\nSolving this equation is often also shortly referred to as solving the MDP. Note that the intersection in (4) is never empty, and there also always exists an optimal deterministic policy $\u03bc^* [1]$ satisfying (4). All optimal policies $\u03c0^*$ share the same optimal value function $V^* := V^{\u03c0*}$. In addition to the value function in (3), the action-value function $Q^\u03c0 : S \\times A \u2192 RU {\u221e}$ of a policy\n$Q^\u03c0 (s, a) := \u0395[\\sum_{k=0}^\\infty \u03b3^k l(S_k, A_k) ]$ (5)\n$A_k ~ \u03c0(\\cdot | S_k), S_{k+1} ~ P(\\cdot | S_k, A_k), S_0 = s, A_0 = a$.\nis the expected value of first applying an action a at the current state s and following the stochastic policy afterwards.\nWe define as $V^* := V^{\u03c0*}$ and $Q^* := Q^{\u03c0*}$ the optimal value and action-value functions. The identities\n$V^*(s) = \\min_a Q^*(s, a)$ (6a)\nsupp($\u03c0^*(\\cdot | s)$) \u2286 arg $\\min_{a^*} Q^*(s, a^*)$. (6b)\nare relating the optimal value function and the optimal policy to the optimal action-value function. In cases with multiple optimal actions for a state s, an optimal policy $\u03c0^*$ can be stochastic and randomly selects an action in the set of optimal actions with any probability."}, {"title": "III. REINFORCEMENT LEARNING", "content": "RL is a powerful approach for solving MDPs using concepts from dynamic programming, Monte Carlo simulation, and stochastic approximation. RL typically involves an agent modeled by a policy iteratively collecting data by interacting with an environment, which could be a simulation model or the real world. The collected data, consisting of state transitions, applied actions, and costs, is then used to iteratively update the policy. In most RL methods, an optimal policy is approximated via estimating the optimal action-value function using temporal difference (TD) learning or by iteratively updating the policy using policy gradient (PG) methods [1], whereas in actor-critic methods both forms are combined."}, {"title": "A. Theoretical Background", "content": "In this section, a short theoretical overview of RL is provided, including dynamic programming, temporal difference methods, and policy gradient methods.\n1) Dynamic Programming: Introduced in [22], dynamic programming (DP) provides the theoretical foundation for many algorithms in RL. DP solves MDPs with known transition models by breaking them into subproblems and using stored solutions to avoid recomputation. DP systematically updates value functions given complete knowledge of the environments MDP. A general DP approach to find the action-value func- tion $Q^\u03c0$ of a given policy can be described by the Bellman operator $T^\u03c0 : R^{S\\times A} \u2192 R^{S\\times A}$,\n$T^\u03c0 Q(s, a) := l(s, a) + \u03b3 E_{S+~P(\\cdot|s,a), A+~\u03c0(\\cdot/S+)}[Q(S_+, A_+)]$, (7)\nwhere $S_+$ is the next sampled state and $A_+$ a sampled action. In the space of value functions, the Bellman operator $T^\u03c0$ is a contraction mapping for $\u03b3 < 1$ with respect to the state supremum norm [23], and $Q^\u03c0$ is the unique fixed point. In other words, one can start with an arbitrary action-value function $Q \u2208 R^{S\\times A}$ and iteratively apply $T^\u03c0$ to converge to the action-value function $Q^\u03c0$. Determining $V^\u03c0$ and $Q^\u03c0$ for a fixed policy \u03c0 is referred to as policy evaluation.\nSimilar to the Bellman operator $T^\u03c0$, the Bellman optimality operator $T : R^{S\\times A} \u2192 R^{S\\times A}$ is defined as\n$TQ(s, a) := l(s, a) + \u03b3 \\min_{a_+} E_{S+~P(s,a)} [Q(S_+, a_+)]$. (8)\nEquivalently to $T^\u03c0$, T is a contraction mapping for $\u03b3 < 1$. Iteratively applying T on an arbitrary action-value function $Q \u2208 R^{S\\times A}$ converges to the optimal value function $Q^*$. The resulting method is called value iteration (VI) and, differently to policy evaluation, tries to solve MDPs of (4) implicitly by finding the optimal value function $Q^*$.\nThe main drawback of DP methods is the unfavorable scaling to high-dimensional or continuous state and action spaces, which is often referred to as the curse of dimensionality [22]. DP methods require full knowledge of the environment model P, which is a fundamental limitation compared to more generic model-free RL algorithms. Approximate DP (ADP) [24] addresses the curse of dimensionality by using different approximation strategies to extend classical DP, as discussed in the following in the context of RL."}, {"title": "2) TD Methods", "content": "To avoid the scaling issues of DP, temporal difference (TD) methods [1] introduce two extensions: Firstly, they learn the value functions $V^\u03c0$ or $Q^\u03c0$ for a policy \u03c0 and their optimal versions $V^*$ and $Q^*$ with only transition sam- ples utilizing stochastic approximation and without explicitly requiring a transition model P. Secondly, they use an adaptive exploration-exploitation strategy to decide favorable states for which the value function is updated.\nIn the following, different TD learning algorithms are presented. The simplest policy evaluation method is called TD(0), which estimates the value function $V^\u03c0$ for a given policy \u03c0. Given a current value function V, an update for a given state S for V is defined by\nTD(0) ($V \u2248 V^\u03c0$)\n$V(S) \u2190 V(S) + \u03b1\u03b4$, (9a)\n$\u03b4 := l(S, A) + \u03b3V(S_+) \u2013 V(S)$, (9b)\nwith $S ~ D^\u03c0, A ~ \u03c0(\\cdot | S), S_+ ~ P(\\cdot | S, A)$, (9c)\nwhere \u2190 denotes overwriting the function V at S, $\u03b1 > 0$ denotes the learning rate, $A ~ \u03c0(\\cdot | S)$ a sampled action from the policy at state S and $S_+ ~ P(\\cdot | S, A)$ the next state sampled from the stochastic model (1). Furthermore, \u03b4 is called the temporal difference, measuring the stochastic difference between the sampled target $l(S, A) + \u03b3V(S_+)$ and the current estimate V(S). Finally, the distribution $D^\u03c0$ is a sampling or exploration strategy where the fixed policy \u03c0 sequentially generates new states by interacting with the environment. For a more technical discussion, see Remark 3.1.\nRemark 3.1: The notations $S ~ D^\u03c0$ or $(S, A) ~ D^\u03c0$ are mathematically not well defined as distributions. Without further elaboration, given an initial state s, an episode is simply run iteratively by applying a policy \u03c0:\n$S_0 = s, A_0 ~ \u03c0(\\cdot | S_0), S_1 ~ P(\\cdot | S_0, A_0), A_1 ~ \u03c0(\\cdot | S_1),....$ (10)\nAfter each time step, an update can then be performed by a TD method. We still use this notation for instructional purposes, especially to highlight which policy was used to generate states and actions.\nSimilar to DP, under some technical assumptions from stochastic approximation theory, the update scheme converges to the unique fixpoint $V^\u03c0$ [23]. For example, one assumption is that the learning rate $\u03b1$ must decrease to zero over time. The update scheme of (9) can be extended to also learn Q.\nBesides estimating the value function $V^\u03c0$ or $Q^\u03c0$ for given policies \u03c0, a main target of RL algorithms is learning the optimal value functions $V^*$ and $Q^*$. For instance, the SARSA algorithm [1] is an RL method for approximating the optimal value function $Q^*$ and given by the update rule\nSARSA (Q \u2248 $Q^*$)\n$Q(S, A) \u2190 Q(S, A) + \u03b1\u03b4$, (11a)\n$\u03b4 := l(S, A) + \u03b3Q(S_+, A_+) \u2013 Q(S, A)$, (11b)\nwith $(S, A) ~ D^\u03c0, S_+ ~ P(\\cdot | S, A), A_+ ~ \u03c0_e(\\cdot | S_+)$. (11c)\nThe \u03f5-greedy policy is implicitly defined by Q via\n$\u03c0_e (a | s) = \\begin{cases} 1 \u2013 \\epsilon + \\frac{\\epsilon}{|A|}, & \\text{if } a = \\arg \\min_{a^*} Q(s, a^*), \\\\ \\frac{\\epsilon}{|A|}, & \\text{otherwise}, \\end{cases}$ (12)\nwhere for notational convenience, it is assumed that there is only one optimal action for a given state s. Note that $\u03c0_e$ is used to sample the next action $A_+ ~ \u03c0_e$ and to generate the state and action with $D^\u03c0$, where the update is performed.\nAnother variant for learning the optimal action-value func- tion $Q^*$ is the popular Q-learning method [25], which, instead of sampling an \u03f5-greedy action like SARSA, takes the greedy action to build the temporal difference. More explicitly, the update rule is defined via\nQ-learning (Q \u2248 $Q^*$)\n$Q(S, A) \u2190 Q(S, A) + \u03b1\u03b4$, (13a)\n$\u03b4 := l(S, A) + \u03b3 \\min_{a^*}Q(S_+, a^*) \u2013 Q(S, A)$, (13b)\nwith $(S, A) ~ D^\u03c0, S_+ ~ P(\\cdot | S, A)$. (13c)\nNote that Q-learning still uses the \u03f5-greedy exploration policy to sample $(S, A) ~ D^e$ in (13c), whereas the policy that is used for the temporal difference is the greedy policy. Thus, Q-learning is called an off-policy method, whereas SARSA is an on-policy method as the same \u03f5-greedy policy is used for exploration and the temporal difference update. For a more elaborate discussion, see [1]."}, {"title": "3) Policy Gradient Methods", "content": "Different from the previous methods, policy gradient (PG) methods directly optimize a parameterized policy $\u03c0_\u03b8$ that can be used for discrete and continuous action spaces A. Given a parameterized policy $\u03c0_\u03b8 : S \u2192 Dist(A)$ and an initial state distribution $\u03c1_0 \u2208 Distr(S)$, the goal of PG methods is to find the optimal parameters $\u03b8^*$ that minimize the expected return\n$J^\u03c0 (\u03b8) := E_{S~\u03c1_0} [V^\u03b8 (S)]$ (14a)\n$= \\frac{1}{1 \u2013 \u03b3} E_{S~\u03c1^{\u03c0_\u03b8}(\u03b5), A~\u03c0_\u03b8(\\cdot|S)}[l(S, A)]$, (14b)\nwhere $\u03c1^{\u03c0_\u03b8}$ is the discounted visitation frequency defined as follows. Given a policy $\u03c0_\u03b8$, the environment model P and an initial state s, let $p(s \u2192 s_+,k,\u03c0_\u03b8)$ be the prob- ability of reaching state $s_+$ at time step k by starting from state s following policy $\u03c0_\u03b8$. The normalized dis- counted visitation frequency is defined by $\u03c1^{\u03c0_\u03b8}(s_+) := (1 \u2013 \u03b3) E_{s~\u03c1_0} [\\sum_{k=1}^\\infty \u03b3^{k-1}p(s \u2192 s_+,k,\u03c0_\u03b8)]$. It is important to highlight that finding a policy that minimizes the objective of (14) is less restricting as solving the MDP over the full state space as defined in (4). The reasoning is that some parts of the state space might not be reached by the policy, which could be omitted if the support of the initial state distribution $\u03c1_0$ is required to cover the whole state space.\nIn order to find an update direction in which the expected return (14) improves, its gradient $\u2207_\u03b8J^\u03c0 (\u03b8)$ is required. Differ- ent reformulations of (14) exist that allow one to derive sample estimates of the gradient of the PG objective, $\u2207_\u03b8J^\u03c0 (\u03b8)$. First, the stochastic policy-gradient theorem reformulates the policy gradient by\n$\u2207_\u03b8 J^\u03c0 (\u03b8) = \\frac{1}{1 \u2013 \u03b3} E_{S~\u03c1^{\u03c0_\u03b8}(\u03b5), A~\u03c0_\u03b8(\\cdot|S)} [Q^{\u03c0_\u03b8} (S, A) \u2207_\u03b8 log \u03c0_\u03b8 (A | S)]$. (15)\nbuilding the theoretical foundation of the REINFORCE algo- rithm [26] or the first actor-critic methods [1]. A derivation is provided in [1].\nThe second reformulation is called the deterministic policy gradient theorem. Let $\u00b5_\u03b8 : S \u2192 A$ be a deterministic policy. By differentiating through the expected state-action value function $Q^{\u00b5_\u03b8}$ the deterministic PG (DPG) is obtained by\n$\u2207_\u03b8 J^\u03c0 (\u03b8) = \\frac{1}{1 \u2013 \u03b3} E_{S(3)}[\u03c1^{\u03bc_\u03b8}(S)\u2207_aQ^{\u03bc_\u03b8} (S,a)|_{a=\u03bc_\u03b8(S)}]$ (16)\nA derivation is provided in [27]. The advantage of the DPG is particularly prominent in high-dimensional action spaces since in the stochastic PG, actions A are sampled to estimate the gradient [27], leading potentially to a higher gradient variance. Thus, the DPG formulation is used in many of the state-of-the-art methods like twin-delayed actor-critic (TD3) [28] and soft actor-critic (SAC) [29]. A concrete algorithm for an actor-critic algorithm using the DPG is provided in section III-B2.\nAn important distinction between the stochastic PG and the DPG is the ability to handle discrete action spaces. With the stochastic PG, discrete and continuous action spaces can be directly optimized, whereas the DPG requires a differentiable parameterized policy with respect to the parameters. To circumvent this problem, some work extends the DPG to stochastic policies using relaxation techniques to differentiate through the sampling process of the discrete actions [30]."}, {"title": "B. Deep Reinforcement Learning Methods", "content": "In the following, an overview of four influential deep RL algorithms, namely deep Q-networks (DQN), deep deterministic PG (DDPG), proximal policy optimization (PPO) and soft actor-critic (SAC) is given.\n1) Deep Q-Networks: Function approximators like NNs approximate the action-value function to extend Q-learning to continuous state spaces. One prominent implementation of this is DQN [6], a combination of deep learning and RL. DQN collects transition samples in a buffer $D_{buffer}$ and minimizes the mean squared error between the current value function $Q_w$ and the sampled target value by\n$L_{DQN} (w) := E_{(S,A,S+)~D_{buffer}} [ (l(S, A) + \u03b3 \\min_{a^*} Q_{\\bar{w}} (S_+, a^*) \u2013 Q_w (S, A))^2 ]$ (17)\nFor the sample target, a fixed copy $\\bar{w}$ of the parameter w is used that is only periodically updated. This stabilizes the training [6]. For an overview of different function approximation methods and their potential instabilities, see [1].\nGiven a parameterized Q-function $Q_w$, the resulting update scheme from DQN is given by\nDQN ($Q_w \u2248 Q^*$)\n$w \u2190 w + \u03b1_w \\frac{1}{B} \\sum_{i=1}^B \u03b4_i \u2207_wQ_w (S_i, A_i),$ (18a)\n$\u03b4_i := l(S_i, A_i) + \u03b3 \\min_{a^*} Q_{\\bar{w}} (S_+ , a^*) \u2013 Q_w(S_i, A_i),$(18b)\nwith $(S_i, A_i, S_i^+) ~ D_{buffer}$. (18c)\nThe update rule in (18a) provides a sample-based estimate of the gradient in (17), where B represents the batch size used for the update and $\u03b1_w$ a learning rate. Averaging over multiple samples is often called mini-batch training and leads to improved performance and accelerated convergence when training NN [31]. Exploration in DQN is handled again by the \u03f5-greedy policy $\u03c0_e$. Differently to the update scheme of Q-learning (13), the buffer $D_{buffer}$ stores state and actions that were generated from previous policies derived from $Q_w$ earlier in the training. Similar to Q-learning, DQN is also an off-policy method.\n2) Deep Deterministic Policy Gradient: Extending DQN, deep deterministic PG (DDPG) [32] is an off-policy actor- critic method that learns a deterministic policy $\u03bc_\u03b8$ and a value function $Q_w$ in parallel. Both the actor and the critic are NNs. The update rule of DDPG is given by\nDDPG ($\u03bc_\u03b8 \u2248 \u03c0^*$)\n$\u03b8 \u2190 \u03b8 + \u03b1 \\frac{1}{B} \\sum_{i=1}^B \u2207_\u03b8\u03bc_\u03b8 (S_i) \u2207_aQ_w (S_i, a)|_{a=\u03bc_\u03b8 (S_i)},$ (19b)\n$\u03b4_i := l(S_i, A_i) + \u03b3Q_w(S_+, A_+^\u2021) \u2013 Q_w(S_i, A_i)$, (19c)\nwith $(S_i, A_i, S_i^+) ~ D_{buffer}, A_+ = \u03bc_\u03b8(S_+), (19d)\nwhere \u2190 denotes the update for Q and denotes the update for \u03bc. As can be seen from the update scheme, the critic $Q_w$ is used to update the actor $\u03bc_\u03b8$ in (19b), in that sense \"criticizing\" the actor. Differently to the update rule of DQN (18a), where the greedy action is considered to build the temporal difference, in DDPG, a single evaluation update with respect to the current policy $\u03bc_\u03b8$ is performed. The buffer $D_{buffer}$ is filled up over time by using the policy $\u03bc_\u03b8 +\u03be$, where \u03be is either an Ornstein-Uhlenbeck process for temporally correlated noise or a Gaussian [32]."}, {"title": "3) Proximal-Policy Optimization", "content": "A popular on-policy actor- critic method that can be used for discrete and continuous action spaces is proximal policy optimization (PPO) [33]. Prior to each policy and critic update, data in the form of multiple episodes is collected. Since PPO is an on-policy method, transitions generated earlier in the training by outdated policies are discarded. In practice, PPO is often used with high- speed simulation environments, where generating new samples comes with low computational costs. A main advantage of PPO is its ability to prevent drastic parameter updates that could potentially destabilize the training. Similar to trust region methods [34], this is achieved by restricting the policy update. During training, a stochastic policy $\u03c0_\u03b8$ often a parameter- ized Gaussian is used. Assuming a given initial state $s_0$, a tra- jectory is drawn by the forward simulation $S_{k+1} ~ P(. S_k, A_k)$ and $A_k ~ \u03c0_\u03b8(\\cdot | S_k)$ until a maximum roll-out length M. Given multiple roll-outs, an estimate $\u00c2(S_k, A_k)$ of the advantage function defined by $\u00c2^\u03c0 (S_k, A_k) := Q^\u03c0 (S_k, A_k) \u2013 V^\u03c0 (S_k)$ can be derived, see [35]. Additionally, with the probability ratio $R_k(\u03b8) := \u03c0_\u03b8(A_k|S_k)/\u03c0_{\\bar{\u03b8}}(A_k|S_k)$, which measures how much the new policy $\u03c0_\u03b8$ changes with respect to the current policy $\u03c0_{\\bar{\u03b8}}$, the PPO clipping objective is defined by\n$J^{CLIP} (\u03b8) := E[\\sum_{k=0}^{M-1} max\\{R_k(\u03b8)\u00c2(S_k, A_k), clip(R_k (\u03b8), 1 \u2013 \u03f5, 1 + \u03f5) \u00c2(S_k, A_k) \\} \u00c2(S_k, A_k) ]$\nwhere the clip function projects the ratio $R_k$ to an interval from 1 - \u03f5 to 1 + \u03f5. Note that the clipping objective requires maximization, whereas the original PPO objective involves minimization, as in this work, costs are minimized rather than rewards being maximized.\nOne of the primary advantages of PPO is its simplicity and ease of implementation [33] compared to previous methods based on trust region optimization, see [34]. A proof of convergence of PPO to the optimal policy for discrete MDPs is given in [36]."}, {"title": "4) Soft Actor-Critic", "content": "The SAC algorithm [29], [37] is a widely used off-policy actor-critic method that incorporates an entropy bonus for stochastic policies with higher entropy. Using entropy regularization is considered in the framework of maximum-entropy RL [38]. Like DQN and DDPG, SAC opti- mizes the policy in an off-policy manner collecting transitions encountered during training in a replay buffer, leading to an improved sample efficiency when compared to PPO.\nSAC extends the objective (14) by introducing an entropy regularization term, leading to a \u201csoft\u201d policy gradient objective\n$J_{soft} (\u03b8) := \\frac{1}{1 \u2013 \u03b3} E_{S~\u03c1^{\u03c0_\u03b8}, A~\u03c0_\u03b8(\\cdot|S)}[l(S, A) + \u03bb_H H(\u03c0_\u03b8(\\cdot | S))]$,\nwhere H denotes the entropy of the paramterized policy $\u03c0_\u03b8(\\cdot|S)$ at the sampled state S. Given a distribution X, the entropy is defined by H(X) = $E[\u2013log(X)]$. The entropy regularization, scaled by the parameter $\u03bb_H$, encourages exploration, stabilizes policy training [39] and can lead to more robust policies [40]. Additionally, it has been shown in [41] that for discrete MDPs, the error introduced by the entropy regularization decreases exponentially to the inverse regularization strength, $1/\u03bb_H$.\nIn TD3, twin Q-networks addressing the overestimation bias in Q-value estimation were introduced [28]. Whereas in [29], the fundamentals of SAC are described, [37] introduced an improved version using twin Q-networks and automatic tuning of the entropy regularization with \u03bbH. The latter builds the basis for most implementations in current RL software frameworks."}, {"title": "IV. MODEL PREDICTIVE CONTROL", "content": "This section introduces MPC, a commonly used framework to obtain policies for continuous MDPs. MPC utilizes a typically deterministic model that approximates the true stochastic environment [2], [42]. Starting from the current environment state, MPC uses the internal model to predict how different choices of the planned control trajectory would affect the state trajectory and evaluates the cost associated with this prediction. Usually, evaluating the MPC policy involves solving an optimization problem online to obtain the control input. In this section, we will first give an overview of MPC problem formulations and the relevant considerations, followed by a discussion of algorithms used for finding their solution."}, {"title": "A. MPC Problem Formulations", "content": "Solving the MDP optimization problem (4) is, in general, intractable due to several reasons, including the infinite horizon, the optimization over the space of policy functions, and the expectation over nonlinear transformations of stochastic variables. MPC leverages several approximations of (4) in order to derive a computationally tractable optimization problem.\nAs a first step, the optimal policy is computed only for the current state s and the infinite horizon in (3) and (4) is approximated by a finite horizon, resulting in the optimization problem\n$\\min_{\u03c0} E [ V^{\u03c0_N}(S_N) + \\sum_{k=0}^{N-1} \u03b3^k l(S_k, A_k)"}]}