{"title": "[MASK] is All You Need", "authors": ["Vincent Tao Hu", "Bj\u00f6rn Ommer"], "abstract": "In generative models, two paradigms have gained attraction in various applications: next-set prediction-based Masked Generative Models and next-noise prediction-based Non-Autoregressive Models, e.g., Diffusion Models. In this work, we propose using discrete-state models to connect them and explore their scalability in the vision domain. First, we conduct a step-by-step analysis in a unified design space across two types of models including timestep-independence,bei noise schedule, temperature, guidance strength, etc in a scalable manner. Second, we re-cast typical discriminative tasks, e.g., image segmentation, as an unmasking process from [MASK] tokens on a discrete-state model. This enables us to perform various sampling processes, including flexible conditional sampling by only training once to model the joint distribution. All aforementioned explorations lead to our framework named Discrete Interpolants, which enables us to achieve state-of-the-art or competitive performance compared to previous discrete-state based methods in various benchmarks, like ImageNet256, MS COCO, and video dataset FaceForensics. In summary, by leveraging [MASK] in discrete-state models, we can bridge Masked Generative and Non-autoregressive Diffusion models, as well as generative and discriminative tasks.", "sections": [{"title": "1. Introduction", "content": "Discrete tokens [18, 61, 82] have gained great attention due to their compatibility with LLMs [79, 88] and compactness [78, 84]. Based on this, Masked Generative models [9, 47] like MaskGiT [9] have proposed gradually unmasking tokens according to specific heuristically designed rules in the vision domain. Non-Autoregressive Models, e.g., Diffusion Models especially continuous diffusion models [31, 38, 65, 67]-have contributed significantly to the generative community due to their efficacy in score prediction [66], conditional synthesis [27, 36, 37, 61, 63], likelihood estimation [67], and image inversion [29]. As research progresses from continuous-state to discrete-state diffusion models, the training and sampling similarity between Diffusion Models and Masked Generative Models become increasingly noticeable. Yet, a comprehensive analysis of their shared design space and theoretical underpinnings in the vision domain remains conspicuously absent.\nTo fill this gap, we explore a framework Discrete Interpolants that builds upon the Discrete Flow Matching [22], which offers flexible noise scheduling and generalization to other methods by considering discrete-state data. While this previous work initially focused on language modeling and explored only the small-scale CIFAR10 vision dataset, we explore the framework to a large-scale realistic dataset. We investigate conventional Explicit Timestep Diffusion models, which explicitly depend on timestep, as well as more flexible Implicit Timestep Diffusion models that completely remove timestep dependence. Additionally, we validate sampling behavior with our framework following the Masked Generative Models approach. This comprehensive investigation deepens our understanding of the connection between Masked Generative Models and Diffusion Models.\nOn the other hand, there's a trend towards unifying discriminative and generative tasks [19, 25, 46]. In this work, we demonstrate how to recast the image segmentation task into an unmasking process of our Discrete Interpolants framework. For example, given pairs of image and its segmentation mask, by training them jointly just once to model the joint distribution in discrete-state, we can adapt our framework to various discriminative and generative tasks such as image-conditioned semantic segmentation, segmentation mask-conditioned image generation. In summary, our contributions include:\n\u2022 We abstract and conceptualize various schedulers from discrete flow matching theory, summarizing and generalizing our framework to incorporate different coupling"}, {"title": "2. Related Work", "content": "Due to space constraints, we have included additional related works in the Appendix.\nDiscrete Diffusion Models. Several works have demonstrated deep connections between diffusion models and autoregressive models [22, 50, 58, 62, 87]. While this has been mainly explored in text generation, in a scaled manner [24, 57], we aim to investigate it in the vision domain. Our work differs from most others by exploring a unified design across Masked Generative Models and Diffusion models, providing a more general masking schedule in discretestate models. MaskGIT [9], MAGVIT [83], Phenaki [75], and MUSE [10] focus on masked generative modeling for generation in a random order (predicting groups of tokens instead of individual tokens), we directly deploy discrete diffusion models on discrete tokens and bring the connection to them through the property of timestep-independence, and provide an in-depth analysis about different aspects of discrete diffusion. VQ-Diffusion [26] is a discrete-state model specifically designed for vision generation. Unlike it, we further generalize to connect between Masked Generative Model and Diffusion models by utilizing the discrete-state under our Discrete Interpolants framework.\nConnection between Diffusion, and Masked Generative Models. Most Masked Generative Models [9, 47] use heuristically designed, greedy sampling rules based on metrics like purity [70] or confidence [9]. However, these approaches have been shown to cause over-sampling issues [22]. While our framework leads to a similar training paradigm, it offers a fresh perspective from diffusion models. This introduces new design spaces, including loss weight considerations. Additionally, our sampling approach is more"}, {"title": "3. Method", "content": "3.1. Discrete Interpolants\nOur method draws inspiration from discrete-state Diffusion/Flow models [22, 58, 62, 87] and continuous Stochastic Interpolants [1, 2, 54]. We aim to extend this interpolant method into discrete-state models with the creation of a flexible and scalable framework for discrete-state modeling. Given the L-length real data $x_1 \\in \\mathbb{R}^L$, the entire possible set is defined as $\\mathcal{D} = [K]^L$, where $[K] = {1, 2, ..., K}$, and $K$ is the vocabulary size (including an extra [MASK] token). $x_0 \\in \\mathbb{R}^L$ represents noise, typically tokens filled with mask token [M].\nOur goal is to learn a transition process based on a vector field $u(x_t, t)$ from $x_0$ (fully masked) to real data $x_1$ by progressively unmasking tokens at each timestep $t$. The key process is a Flow Matching-style probability path interpolated according to the masking schedule $k_t$ 1:"}, {"title": "3.2. Training", "content": "In the case of continuous-state, given a probability path $p(x_t)$ and a vector field $u(x,t)$, the key to ensuring that traversal along the vector field $u(x_t, t)$ can yield the probability transition between $p(x_0)$ and $p(x_1)$ is the Continuity Equation [67]. Similarly, in discrete-state modeling, there's a counterpart theory called the Kolmogorov Equation [8]. It indicates that by following the design of $u_t(x_t) = \\int \\frac{[P_1]_t(x_1/x_t, t; \\theta)}{[P_0]_t(x_0, x_t, t; \\theta)} dx \\delta_{x_t}(x)]$, we can traverse along the vector field $u(x_t, t)$ to yield the probability transition between $p(x_0)$ and $p(x_1)$. To learn such a vector field $u_t(x_t) = \\int \\frac{[P_1]_t(x_1/x_t, t; \\theta)}{[P_0]_t(x_0, x_t, t; \\theta)} dx \\delta_{x_t}(x)]$, we only need to learn $p_{1|t}(x_1|x_t, t)$, which incidentally acts as an unmasking function, and can be optimized by a cross-entropy loss:\n$\\mathcal{L}(\\theta) = \\mathbb{E}_{p_{data}(x_1)p(x_0)U(t;0,1)p_{t|0,1}(x_t|x_0,x_1)}[-\\log p_{1|t}(x_1|x_t, t; \\theta)]$, (2)\nwhere $x_t$ is obtained by Discrete Intepolants in Eq. (1).\nMasking and Weighting on Cross-Entropy. To improve the performance of fidelity, and stabilize the training, we further introduce two significant modifications upon to crossentropy formulation: a masking operation in the crossentropy loss and a weighting mechanism:\n$\\mathcal{L}(\\theta) = \\mathbb{E}_{p_{data}(x_1)p(x_0)U(t;0,1)p_{t|0,1}(x_t|x_0,x_1)} \\frac{w(t)}{1-\\kappa_t} [M](x_t) (x_1) \\log p_{1|t}(x_1|x_t, t; \\theta)$ (3)"}, {"title": "3.3. Sampling", "content": "During sampling process, the sample $x_t$ progressively changes between states in $\\mathcal{D} = [K]^L$ during sampling, aim-"}, {"title": "3.4. Segmentation is Unmasking", "content": "A natural extension of Discrete Interpolants is to consider multimodal joint learning the joint distribution, inspired by [3] as well as recent advances in combined discriminative and generative learning [11, 47]. Given real image $X_1 \\in \\mathbb{R}^L$ and the second modality $y_1 \\in \\mathbb{R}^{L_y}$, where $L_x, L_y$"}, {"title": "3.5. Classifier-free Gudiance", "content": "We can conduct versatile sampling processes in both singlemodality and double-modality scenarios. For the sake of generality, we'll focus on the conditional sampling of $p(x|y; \\theta)$ in double-modalities:\n$p(x_1|x_t, y; w, \\theta) = p(x_1|x_t; [C], \\theta) + w[p(x_1|x_t, y; \\theta) - p(x_1|x_t; [C]; \\theta)]$, (6)\nwhere $w$ represents the guidance strength, and token [C] serves as the null embedding to indicate unconditional signal. By swapping the positions of x and y, we can achieve a similar classifier-free guidance for $p(y|x;\\theta)$. This guidance can serve as a plug-in replacement for our previous sampling function described in Algorithm 1."}, {"title": "3.6. Extra Discussions", "content": "Connection between Cold Diffusion [5]. Cold Diffusion demonstrates a generalized diffusion model, that can be composed of Degradation Operator D and Restore Operator R, the degradation can be blurring, pixelated, or snowification, with restriction $D(x_1,1) = x_1$, their training is achieved by a minimal optimization problem:\n$\\min_D \\mathbb{E}_x ||R(D(x, t), t; \\phi) - x||$, (7)\nour masking operation in Eq. (1) can be naturally a case of the degradation $D(x_t, t) \\sim p_{t|0,1}(x|x_0, X_1)$, and our unmasking network is a case of the Restore Operator $R(x_t, t)$.\nConditional Coupling is secretly a token-dependent scheduler. Conditional Coupling [22] is a method for coupling xo and x1 when constructing the discrete interpolants: $x_0 = [I \\otimes x_1 + (1-I) \\otimes ([M], ..., [M])], x_1$, it can be seen as a special case of the masking schedule:\n$p_{t|0,1}(x|x_0, x_1) = (1 - k_t)\\delta_{x_0}(x) + k \\delta_{x_1}(x)$, (8)\nwhere $(x_0, x_1) = ([I \\otimes x_1 + (1-I) \\otimes ([M], ..., [M])], x_1)$.\nIt can be understood as a special scheduler built upon the default by making the scheduler dependent on the token's location, $k_t \\rightarrow k_i$. If $i \\notin I$, it conducts a standard interpolant based on the scheduler; if $i \\in I$, the scheduler is a null scheduler, meaning the token remains unchanged. I is a mask controlled by a ratio of data length L, determining the balance between standard and null schedulers."}, {"title": "4. Experiment", "content": "4.1. Experimental Detail\nDatasets and Metrics. For image generation, we primarily focus on ImageNet256 and MS COCO datasets. For joint training between image-segmentation mask pairs, we use the Cityscapes dataset. Our video generation experiments mainly utilize the FaceForensics dataset. We use Fr\u00e9chet Inception Distance (FID) as the evaluation metric for image generation tasks. For video generation, we use Fr\u00e9chet Video Distance (FVD) to assess performance.\nTraining Details. We consistently use the discrete tokenizer SD-VQ-F8 from Stable Diffusion [61] across all datasets, due to its extensive pretraining on large datasets like Open-Images [44]. To avoid singularity issues in the derivative, we sample timesteps $t \\in [\\epsilon, 1 - \\epsilon]$ during training, where $\\epsilon = 10^{-3}$. For fair comparison, we employ the same scheduler for both training and sampling, using a fixed step size. For classifier-free guidance, we randomly drop out the conditional signal with a probability of 0.1 during training, following established conventions [30]. Unlike [22], which uses adaptive step size for sampling, we consistently employ a fixed step size for fair comparison. Unless otherwise stated, we default to using 1,000 timesteps. For COCO, ImageNet, and Faceforensics, we use linear schedules by default and set $w(t) = 1$, and applying the masking cross-entropy. For more detailed information about the training recipe, optimizer, iteration steps, GPU usage, learning rate, and other parameters, please refer to the Appendix.\n4.2. Experimental Result\n4.2.1 Main Result\nImage Generation. We demonstrate our results on the COCO dataset."}, {"title": "5. Conclusion & Future Works", "content": "Our work Stochastic Interpolant extends discrete flow matching theory to vision tasks, generalizing from Explicit to Implicit Timestep Models. We analyze the intersection of Diffusion and Masked Generative Models, proposing dense-pixel prediction as an unmasking process. By integrating these elements, we achieve state-of-the-art performance on MS-COCO, competitive results on ImageNet 256, and demonstrate scalability to video datasets like Forensics. For future work, most mask-based methods can't remask tokens once unmasked, leading to irreversible denoising errors. CDCD [16] addresses this, while [50] proposes decoupling the process. Our method could potentially extend to these approaches, using discrete stochastic interpolants to address this limitation."}]}