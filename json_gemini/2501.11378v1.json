{"title": "Investigation of Whisper ASR Hallucinations\nInduced by Non-Speech Audio", "authors": ["Mateusz Bara\u0144ski", "Jan Jasi\u0144ski", "Julitta Bartolewska", "Stanis\u0142aw Kacprzak", "Marcin Witkowski", "Konrad Kowalczyk"], "abstract": "Hallucinations of deep neural models are amongst\nkey challenges in automatic speech recognition (ASR). In this\npaper, we investigate hallucinations of the Whisper ASR model\ninduced by non-speech audio segments present during inference.\nBy inducting hallucinations with various types of sounds, we show\nthat there exists a set of hallucinations that appear frequently.\nWe then study hallucinations caused by the augmentation of\nspeech with such sounds. Finally, we describe the creation of a\nbag of hallucinations (BoH) that allows to remove the effect of\nhallucinations through the post-processing of text transcriptions.\nThe results of our experiments show that such post-processing is\ncapable of reducing word error rate (WER) and acts as a good\nsafeguard against problematic hallucinations.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent improvements in Artificial Intelligence (AI), espe-cially deep neural models, allow for impressive results in many\ntasks. However, with the increase of performance approaching\n(or sometimes surpassing) capabilities of the human brain,\nthose systems exhibit tendencies to make errors that also\nmimic the failures of a human. In this work, we investigate\nthe errors of Automatic Speech Recognition (ASR) that occur\nwhen an ASR system generates a transcript for audio that\ndoes not contain any speech content. Anthropomorphisation of\nthose systems leads to labeling these errors as hallucinations.\nThe issue of hallucination in the context of AI is most\noften discussed in reference to Natural Language Generation\n(NLG) and Large Language Models (LLM) to describe general\nfalsehoods produced by those systems [1], [2]. However, in\nmost cases confabulation would be a better analogy, with\nresearchers also categorizing some false responses as lies\nand deception [3], [4]. The term hallucinations better fits the\nspecific errors of false perception of input signals in Computer\nVision (CV) [5], ASR [6] or Neural Machine Translation\n(NMT) systems. Their cause can be attributed to over-reliance on patterns from training [8], overconfidence in model\npredictions [9], and unreliable training data. However, not all\nerrors of those systems should be categorized as hallucinations.\nFor instance, in ASR mishearing could be better described\nas phonetic errors caused by ambiguous stimuli or phoneme\nconfusion (\"the sky\" recognized as \"this guy\" [10]). In [6]\nhallucinations are defined as predictions without phonetic or\nsemantic connection to the reference, a definition we agree on,\nbut still find problematic since those connections are difficult\nto estimate. In this work, we omit this problem by working\nwith transcriptions generated by ASR from non-speech audio.\nIn this study, we focus on the Whisper ASR model [11]\nthat provides state-of-the-art recognition for multiple lan-\nguages [12]. Despite the high quality of Whisper's outputs and\na set of Whisper's internal heuristics that help to avoid failure\ncases [11], it does have a tendency to generate hallucinations\nand produce incorrect repetitions of previously recognized text\n(known as looping), e.g. \"Welcome to the New York City City\nof New York City of New York\". We classify looping as one\ntype of hallucinations, as opposed to [6] where it is treated as\na separate type of error. It has also been shown that Whisper\nhallucinations can be of a violent or sexual nature, which can\ncause serious problems in real-world applications [13].\nThis paper investigates how Whisper hallucinates based on\nits responses to non-speech audio, effectively analyzing its\nvulnerability to \"accidental\" adversarial example attacks [14].\nFirstly, we examine how the type of sound and its duration\naffects hallucination frequency and the created outputs. Then,\nwe analyze how the augmentation of speech utterances with\nsuch audio (including noises and natural sounds) can influence\nthe rate of hallucinations. Based on inference results for non-speech audio signals, we further create a so-called Bag of\nHallucinations (BoH). It consists of the most commonly hal-lucinated outputs filtered for use in hallucination identification\nand removal tasks. Finally, we propose a post-processing\nalgorithm that aims to reduce the impact of hallucinations, and\nshow that it can be used to reduce Word Error Rate (WER)\nof transcriptions generated by the ASR model."}, {"title": "II. COLLECTING HALLUCINATIONS FROM NON-SPEECH\nAUDIO RECORDINGS AND THEIR ANALYSIS", "content": "We begin our study by performing inference using Whisper\nASR on an exhaustive set of audio recordings that do not\ncontain any speech. This way, any text recognized by the\nmodel can be categorized as a hallucination, eliminating\nuncertainties in hallucination detection. We collect a raw list\nof such hallucinations and study error patterns in Whisper's\ntranscriptions, focusing on the most frequently appearing\nhallucinations and text repetitions known as looping."}, {"title": "A. Experimental setup", "content": "In our experiments we use the Whisper [11] large-v3 model\u00b9\nas current state-of-the-art ASR. Unless stated otherwise, we\nset language to English, temperature to zero (for deterministic\ninference), while other parameters are left at default values\n(e.g. greedy decoding). Transcriptions are normalized using\nthe provided OpenAI basic normalizer. Next, we detect and\nremove looping (understood as a subsequent repetition of the\npreviously recognized text fragment).\nIn experiment 1, we aim to collect an exhaustive list of\nhallucinations induced by non-speech audio input. To this\nend, we create a large dataset which contains various types\nof sound files, composed of Audioset [15], Musan [16],\nUrbanSound8K [17], and FSD50K [18]. Based on the provided\ntags, all audio files that could include speech, human voices\nand singing, were removed from the dataset. We also removed\nall music since tagging was not reliable enough to discriminate\nbetween instrumental and vocal songs. The resulting dataset\nincludes 253 278 files from Audioset, 931 from Musan, 8733\nfrom UrbanSound8K, 29074 from FSD50K. We extended this\ndataset with white and pink noises of varying lengths (0.1s to\n30s in 0.1s steps) at different volumes (-30 to -2 dBFS in 2dB\nsteps). Files of silence were also added at the aforementioned\nlenghts, which yielded 9301 noise and silence files. The final\ndataset of non-speech audio consists of 301317 audio files.\nIn experiment 2, which analyses hallucinations depending\non the length of non-speech audio, we create a dataset of non-speech audio files of various duration ranging from 1 s to 30 s,\nby selecting a subset of 200 audio files from the dataset used in\nexperiment 1, which were next either cropped or concatenated\n(and then cut) to obtain audio files of the desired duration.\nIn experiment 3, we use the eval split of AudioSet, filtered\nas in experiment 1. The dataset consists of 8 272 files of length\n10s, labeled across the 6 main categories."}, {"title": "B. Analysis of hallucinations from non-speech audio input", "content": "As a result of experiment 1, we created an exhaustive list\nof hallucinations (after delooping) from the set of non-speech\naudio. We discard transcriptions solely built of symbols,\npunctuation marks or whitespace characters, and retain those\nthat include letters or numbers. Out of 301317 inferences,\nhallucinations appeared 121378 times (40.3%), with 11049\n(9.1%) of them involving looping. A total of 41231 unique\noutputs were generated with 1270 of them appearing more\nthan once (accounting for 67.08% of hallucinations). Their\ndetailed analysis is performed in Sec. III.\nThe results of experiment 2 are presented in Table I which\nshows: the percentage of files that hallucinated; the percentage\nof looping from all hallucinations; the percentage of files (cut\nor concatenated), from all hallucinations, which generated the\nsame base transcription text as in the original non-speech file\n(regardless of looping); the percentage of occurrence in the\n30 most often appearing hallucinations (Top30) from experi-ment 1. There is a prominent rise in hallucinations for long\nand short audio excerpts, interestingly they are primarily from\nTop30 most common hallucinations rather than the original\nhallucination for that sound. The sound characteristic alone\ndoes not seem to cause a certain text, as the same files\nhallucinate differently when cut to various lengths.\nThe results of experiment 3, presented in Table II, show\ndifferent statistics of hallucinations for the 6 main categories\n(types of sounds). Inequality in file numbers is due to filtering\nout sounds that contain speech. The (filtered) human sounds\nare more likely to generate Top30 predictions compared to\nother categories, while animal sounds (often onomatopoeia\nsuch as \"bark\" or \"woof\u201d) tend to increase looping. However,\nthe results reveal rather similar statistics among the categories."}, {"title": "III. BAG OF HALLUCINATIONS AND HALLUCINATION\nREMOVAL", "content": "The list of hallucinations obtained in our experiments must\nbe further filtered to remove words and phrases that are\nfrequently used in English language (such as \"the\", \"so\u201d,\n\"thank you\"). Always treating them as hallucinations could\nresult in a high false positive rate. To solve this issue, the\nobtained results are filtered to create what we call a Bag\nof Hallucinations (BoH). In this work, the filtration criteria\nare based on the log probability (lower than -10) from a\nseparate English n-gram language model\u00b2 and on the number\nof occurrences (more than four) in our non-speech inferences.\nNote that, in general, any other filtration criteria could be used\nto create a useful list of hallucinations to be removed from\ntranscriptions when encountered, depending on the use case.\nTable III presents the most commonly occurring outputs\nand the ones that, after filtration, belong to BoH. The hal-lucination list and BoH are made available\u00b3. About 35% of\nall hallucinations are two phrases, and over half are from\nthe top 10 outputs, showing that Whisper usually generates\na limited set of texts for various non-speech inputs. The list\nof top 30 hallucinations reveals that a strong factor in this is\nWhisper being trained on video transcriptions, e.g. \"thanks for\nwatching\", \"subtitles by the amara org community\", \"transcript\nemily beynon\". More offensive examples were also found, e.g.\n\"listen up motherf*#s i wanna f*#k you up\". Such hallucina-tions could be catastrophic in a real-world implementation.\nBased on the BoH obtained, a simple method can be\nderived that first performs delooping (detection and removal\nof looping), followed by the search and removal of common\nhallucinations with the Aho-Corasick [19] string searching\nalgorithm (using BoH as pattern dictionary). This solution for\npost-processing transcriptions can be used in combination with\nother hallucination prevention methods.\nIn addition, to further reduce false positive detections, one\ncan perform forced phoneme alignment for the detected hallu-cination to identify hallucination-related abnormalities such as\nlow probability or too long (or short) duration of phonemes.\nThis additional hallucination confirmation technique requires\nthe processing of both transcriptions and audio, and an in-crease in computational cost may not always be feasible in\ncertain practical deployments. In experiments to follow, we\nuse forced alignment based on wav2vec2.0 [20]."}, {"title": "IV. HALLUCINATIONS FROM RECORDINGS WITH SPEECH\nAUGMENTED USING NON-SPEECH AUDIO", "content": "In the next experiments, we aim to verify if the hallucina-tions present in BoH occur during ASR inference from speech\nfiles augmented using different non-speech sounds."}, {"title": "A. Experimental setup", "content": "In experiments 4 and 5, in which we aim to study hallu-cinations from speech augmented with sounds (or silence) of\nvarious lengths, the experimental setup follows the description\nin the first paragraph of Sec. II-A.\nIn experiment 6, we aim to examine whether some pa-\nrameters of Whisper's inference process can be useful in\nalleviating hallucinations. We decided to investigate the re-\ncently introduced hallucination silence threshold parameter\nwhich skips silence longer than a given threshold, and beam\nsize which changes the number of possible output sequences\nexplored during inference. Other parameters rely on affecting\ntemperature during inference, which as explained in Sec. II-A\nis set to a fixed value, and thus they are not investigated.\nFor all these experiments, we create a new dataset with\nthe recordings containing speech augmented with non-speech\nsounds. As speech audio, we use 80 files from the Common\nVoice dataset [21], with an average duration of 10 s, which\nare correctly transcribed by Whisper. As non-speech audio,\nwe use a subset of the dataset from experiment 2 (described\nin Sec. II-A) with lengths: 1s, 10s, 20s, 30s, which we\nadditionally normalize to achieve SNR of 9 dB. We consider\ntwo scenarios, with speech non-overlapped by noise (NO)\nand speech with noise overlap (OL). In both cases, speech\naugmentation involves adding sounds before, after or both\nbefore and after speech, for all combinations of speech and\nnon-speech audio, resulting in a dataset consisting of 416000\nfiles. Such speech augmentations were selected since they were\nfound to induce many Whisper hallucinations. In addition, for\nexperiment 5, we also created another dataset (consisting of\n960 files) with speech augmented with silence, in which all\nnon-speech segments were replaced with silence."}, {"title": "B. Analysis of hallucinations from augmented speech", "content": "The results of experiment 4, presented in Table IV, show\ndifferent statistics of hallucinations obtained for speech aug-mentations using sounds of various lengths for the noise over-lap (OL) and non-overlap (NO) cases, including the second\ncolumn with the percentage of the detected hallucinations\n(computed from the union of two sets: detected looping and\ntranscriptions that appear in the Bag of Hallucinations), the\nthird column with the percentage of files that appear in BoH\nfrom the detected hallucinations. These results confirm obser-vations we had solely for the non-speech data (in experiment\n2). The rate of hallucination rises noticeably when the duration\nof a file exceeds 30 s. This is also the duration of a single\naudio segment that Whisper allocates for decoding, and after\nthis length is exceeded, the system starts to use additional\nheuristics. The experiment carried out earlier allows us to\npresume that this is one of the reasons for the intensification\nof hallucinations. It is also evident that both the positioning\nand duration of non-speech sounds have an impact on hal-lucinations. In general, additionally overlapping speech with\nnoise increases only slightly the probability of hallucinations\ncompared with the non-overlapped case. For longer augmenta-tion lengths, the probability of BoH and original hallucinations\nincreases. Interestingly, it is more probable that the augmented\nfile generates text from BoH than the same transcription as for\nthe original sound."}, {"title": "V. MITIGATING HALLUCINATIONS IN ASR", "content": "In delooping and BoH hallucination removal (deloop.+BoH).\nFinally, we pre-process input signals with SileroVAD and\nperform delooping and BoH hallucination removal from the\ngenerated Whisper transcriptions. For methods involving VAD,\nthe input audio files were pre-processed by VAD and the\nsegments detected as speech were concatenated into a single\nfile submitted for inference. The dataset was created similarly\nto the dataset used in experiments 4-6 (the procedure is\ndescribed in detail in Sec. IV-A), with the difference that new\nsound files with non-speech audio, which were not used when\ncreating BoH, were selected from freenoise.org [23]."}, {"title": "B. Analysis of speech recognition accuracy", "content": "Table VII shows hallucination and WER results for the\ncompared approaches. The usage of an effective VAD, such\nas SileroVAD, yields a significant reduction in WER results,\nas well as the incidence of hallucinations, compared with the\nless effective VAD or parameterization of the Whisper ASR\nmodel. We also tried denosing using DCCRN [24] but it did\nnot help with hallucinations. The presented delooping+BoH\napproach yields nearly as good performance in terms of\nWER, outperforming the majority of other approaches by large\nmargin, showing its effectiveness in improving the accuracy of\nthe output transcriptions. Our approach can also be used as a\nsafeguard (or sanity check) when using VAD pre-processing.\nHowever, none of these approaches can be considered a\ncomplete solution that fully protects against errors of this kind."}, {"title": "VI. CONCLUSIONS", "content": "This study explored the characteristic of Whisper hallucina-tion on non-speech audio, resulting in a list of most commonly\ngenerated texts and a filtered Bag of Hallucinations (BoH)\nfor use in hallucination detection. Depending on the use case,\nBoH can be assembled as desired, e.g. by blindly removing\ntexts such as \"subtitles by ...\" when transcribing a physician.\nThe performed analysis showed that while audio length sig-nificantly affects the error rate, only a limited correlation was\nidentified between audio content and hallucination it produces.\nDelooping and hallucination removal using BoH was tested,\nachieving a decrease in WER, with best results obtained when\ncombined with VAD. Finally, one should emphasize that the\ngoal of our hallucination removal is to avoid 'catastrophic'\noutcomes rather than bring about large improvements in WER."}]}