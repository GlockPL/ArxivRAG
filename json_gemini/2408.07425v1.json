{"title": "Exploring Retrieval Augmented Generation in Arabic", "authors": ["Samhaa R. El-Beltagy", "Mohamed A. Abdallah"], "abstract": "Recently, Retrieval Augmented Generation (RAG) has emerged as a powerful technique in natural language processing, combining the strengths of retrieval-based and generation-based models to enhance text generation tasks. However, the application of RAG in Arabic, a language with unique characteristics and resource constraints, remains underexplored. This paper presents a comprehensive case study on the implementation and evaluation of RAG for Arabic text. The work focuses on exploring various semantic embedding models in the retrieval stage and several LLMs in the generation stage, in order to investigate what works and what doesn't in the context of Arabic. The work also touches upon the issue of variations between document dialect and query dialect in the retrieval stage. Results show that existing semantic embedding models and LLMs can be effectively employed to build Arabic RAG pipelines.", "sections": [{"title": "1. Introduction", "content": "Retrieval-Augmented Generation (RAG) models have recently emerged as powerful tools that can both enhance and capitalize on the capabilities of generative systems through integration with external knowledge source [1]. The advantage of using a RAG model is that it leverages the power of large language models (LLMs) to generate responses based on documents that these LLMs might not have seen before. In specific domains, this means getting high-quality and accurate answers to queries to which an LLM might not have an answer. In most scenarios, the use of a RAG model also reduces LLM hallucinations."}, {"title": "2. Related Work", "content": "The concept of Retrieval-Augmented Generation (RAG) has gained significant attention as a hybrid approach that integrates information retrieval with neural language generation. In 2020, Lewis et al. [3] introduced the RAG framework to address the then-limited capabilities of pre-trained language models and to enhance a model's ability to generate informed and contextually relevant responses. Despite the major improvements in language model capabilities since then, RAG remains highly relevant for generating accurate and contextually enriched responses by dynamically accessing and synthesizing information from external sources [1] [4].\nWhile the literature is rich with issues related to the application of RAG on English documents, the application of RAG in languages other than English has been less explored. However, recent studies have begun to address this gap. For example, the study presented in [5], discusses the application of Retrieval-Augmented Generation (RAG) in multilingual settings, specifically focusing on enhancing the performance of RAG models when working with non- English languages. The work emphasizes the need for strong retrievers and generators and highlights the importance of task-specific prompt engineering to generate responses in a user's language. The paper suggests that while the multilingual RAG models show promise, they face challenges with code-switching, fluency errors, and the relevance of retrieved documents.\nThe work presented in [6] specifically addresses the effectiveness of multilingual semantic embedding models for Arabic text retrieval, making it highly relevant to the study discussed here, as both explore the retrieval aspect of Arabic RAG models. The experiments presented in that work were carried out using the publicly available ARCD (Arabic Reading Comprehension Dataset) [7]. These experiments involved assessing the performance of several advanced multilingual semantic embedding models in retrieving text passages relevant to a query using the average Recall@k metric. The authors did not employ a vector database and chose to directly use cosine similarity for matching query embeddings against document embeddings. While this can slow down the matching process in a real-life setting, it should have little or no impact on the research findings presented in the paper. The embedding models investigated by this study were OpenAI's Ada [8], Google's Language-agnostic BERT Sentence Embedding (LaBSE) [9], Cohere [10], Mpnet [11], HuggingFace's DistillBert versions one and two [12], Meta's SONAR (Language-Agnostic Representations)[13], and Microsoft's E5 embedding models[14]. The study identified Microsoft's E5 large sentence embedding model as the top performer, significantly outperforming other models tested.\nWhile the work presented here also uses the ARCD dataset [7] and experiments with OpenAI's Ada model [8], Cohere [10] and Microsoft's E5 embedding models[14], the work goes further by extending the experiments to other embedding models, using a second dataset for experimentation, examining the impact of using dialectical queries on the performance of embeddings when carrying our retrieval, and investigating the impact of attempting to eliminate ambiguity in ARCD queries. Furthermore, the work presented herein investigates several known LLMs as generators to present an exploration of a complete RAG pipeline."}, {"title": "3. Methodology and Experimental Setup", "content": "One of the main aims of this work is to assess the performance of various multilingual semantic embedding models in the context of Arabic text retrieval, and to test the resilience of top performing models to a query dialect different than that of input documents. The work also aims to evaluate the performance of multilingual Large Language Models (LLMs) for the generation task. To accomplish these goals, the authors set out to implement the entire pipeline presented in Fig 2 over 2 stages. In the first stage, experiments are carried out to identify the best semantic model to use, and in the second stage experiments are conducted to evaluate the performance of various LLMs as generators using the best performing semantic model in stage 1 in the retrieval process.\nDetails of the used datasets, semantic embedding models, vector database, and LLMs used as generators are provided in the next subsections."}, {"title": "3.1. Used Datasets", "content": "In this work, two different datasets were used for experimentation. The first is the Arabic EduText Secondary School dataset which was compiled by the authors while the second is the ARCD (Arabic Reading Comprehension Dataset) [7]. Each of the datasets is briefly described below."}, {"title": "3.1.1. The Arabic EduText Secondary School Dataset (Ar_EduText)", "content": "The goal of creating this dataset was to facilitate the testing of multiple embedding and generation models within a manageable scope. The dataset was compiled by randomly selecting six freestyle reading passages from high school Arabic textbooks which are written in MSA. Each passage was input to OpenAI's GPT 403 model using the prompt: \"You are an expert in Arabic. Given the following text (a paragraph), create five or six different Arabic questions.\" The generated questions were manually reviewed, and depending on their suitability, they were either left as is, edited, or rejected. This process yielded a set of 158 distinct questions, each linked to the text segment from which it was generated. To provide answers for the questions as part of the dataset, each segment from which a question originated was submitted back to OpenAI's GPT-3.5 Turbo model4, along with the question and the prompt: \"Given the following context (segment text) and the following question (edited question), provide a concise answer.\" The answers were also manually reviewed, and both the automatically generated and edited versions were retained. A final step was carried out to generate an Egyptian dialect version of the questions. The objective of this step was to generate data that can be used to test the ability of semantic representation models to capture semantic similarities across different dialectal representations. To generate the Egyptian Arabic version of the questions, the original question and the following prompt were passed to the GPT-3.5 Turbo model: \u201cYou are fluent in Arabic and its variations. Rewrite the following question in the Egyptian Arabic dialect: [question].\" The outputs were manually reviewed and edited by the authors, who are fluent in Egyptian Arabic. The generated Egyptian dialect questions often suffered from structural issues, and frequently included Modern Standard Arabic (MSA) terms instead of Egyptian Arabic terms (e.g. '\u062d\u0631\u0627\u0626\u0642 instead of \u062d\u0631\u064a\u0642 ' and '\u0645\u064a\u0627\u0647 instead of \u0645\u064a\u0647(.These issues were resolved after editing 75.3% of the generated questions, and both the auto-generated and edited versions were retained. The file containing all segments, their associated questions, their auto-generated answers, their manually revised answers, their auto-generated Egyptian dialect versions of the questions, and their corresponding corrections were then saved and are available for download from the project's GitHub repository.\nThis dataset is intentionally compact and was created as such to enable detailed revisions of answers, and the generation and refinement of questions, particularly those articulated in the Egyptian dialect."}, {"title": "3.1.2. The Arabic Reading Comprehension Dataset (ARCD)", "content": "The Arabic Reading Comprehension Dataset (ARCD)[7] is composed of 1,395 questions and their answers crowd-sourced from 155 Wikipedia articles spanning diverse domains. Each entry in the dataset is also associated with a paragraph from which an answer can be extracted. In total, there are 460 unique paragraphs in the dataset. The dataset was specifically developed to address the scarcity of Arabic question answering (QA) datasets.\nUpon reviewing the data, the authors observed that many of the questions could only be fully understood when considered in the context of the immediately preceding question. A query like the one in the second row, for example, cannot yield any meaningful results, regardless of the expressiveness of the semantic embedding model employed. This problem is not specific to the Arabic language and can occur across different datasets spanning various languages when contextual dependencies between questions influence their interpretation. To eliminate the impact of these dependencies on the results of the carried-out experiments, the authors of this work attempted to disambiguate questions with dependencies. Towards this end, all questions were input to the GPT 3.5 Turbo model along with the prompt found in Appendix A. Disambiguating the questions using a straight forward prompt with no examples, often produced unexpected results as well some English responses which is why the long prompt shown in the appendix was used. The disambiguator always used the version of the preceding question that was already disambiguated as context for the question being disambiguated. When revising the output of this process it was observed that as a side effect, typos were corrected, and some questions were occasionally rephrased. It was also observed that in a few cases, there were errors in the automatically disambiguated questions. Since the goal was not to find the best way to automatically disambiguate questions, but rather to examine the impact of disambiguation on the retrieval accuracy and hence gain a better understanding of the ability of the semantic embedding model being used, all disambiguated questions were manually revised and edited. Questions that were changed by GPT3.5 for no apparent reason, were restored to their original form. If a question could not be disambiguated in light of its disambiguated preceding question, it was left as is. In the retrieval experimentation section, results are reported on the original questions, the automatically disambiguated questions, and the manually edited disambiguated questions."}, {"title": "3.2. The Used Semantic Embedding Models", "content": "Semantic embeddings or text embeddings offer a way of representing text where a word, phrase, sentence, paragraph, or an entire document is represented as a dense vector of real numbers that captures the meaning of what it represents. Since these representations exist in a high-dimensional vector space, distance metrics such as cosine similarity can be applied to evaluate how similar or distant certain pieces of text are from one another. This method can thus facilitate a deeper understanding of textual relationships by quantifying semantic similarities and differences. The concept of semantic embeddings is not new. One of the earliest models in this field is Latent Semantic Analysis (LSA), which was developed in the late 1980s and early 1990s [15, 16], but embeddings gained popularity and widespread use after the introduction of Word2Vec models[17]. The introduction of contextualized embeddings [18, 19] transformed and revolutionized the way in which text is handled and processed."}, {"title": "3.3. The Vector Database", "content": "The vector store that we chose to use is Chroma DB6 which is a free, open source, easy to use database that can be efficiently employed to store and retrieve embedding vectors. While Chroma might not be the best choice for very large datasets, given the size of the datasets we experimented with, it is ideal."}, {"title": "3.4. The Generators", "content": "In the context of RAG models, the generator is the component that takes as input the user-entered query and the pieces of text likely to contain an answer (context), and generates an informative and concise response for the question from the context. Typically, the generator is a large language model, and the query and context are presented to it in the form of a prompt with the following structure: \"Use the given context to answer the given question. Be as concise as possible. Context: {context}, Question: {question}.\" While this is the general structure of the prompt, it is often expanded based on the particular nature and requirements of the RAG being developed.\nIn this work, we experimented with 5 different LLMs as generators. Those are: OpenAI's GPT3.5 Turbo, Mistral 7B[25], Llama 3[26], Mixtral [27], and JAIS [23]. To evaluate the various models, the Precision, Recall, and F1 Score metrics were borrowed from the information retrieval and question answering (QA) domains. In the context of QA, these metrics are calculated based on the overlap of tokens between the system-generated answer and the provided gold standard answer. Another used metric is the BLEU score (Bilingual Evaluation Understudy) which is borrowed from the field of machine translation. In the context of QA, BLEU measures how well a system-generated answer matches a set of reference answers by calculating the precision of n-grams (sequences of n words) in the generated answer against those in the reference answer. The main issue with metrics like BLEU, precision, recall, and F-score is that they primarily focus on exact matches and surface-level features, often failing to capture semantic similarity. To overcome this limitation, the cosine similarity metric was also used to compare the embeddings of the system generated response to the embeddings of the gold standard response."}, {"title": "4. Experiments and Results", "content": ""}, {"title": "4.1. Retriever Related Experiments", "content": ""}, {"title": "4.1.1. Experiment 1: Investigating the impact of different semantic embedding models on retrieval", "content": "The goal of this experiment was to assess which semantic embedding models have the highest retrieval rates. In the datasets used, each question was associated with a segment from which an answer could be retrieved. The evaluation focused on the effectiveness of various models in accurately identifying and extracting relevant text segments based on the input queries. To this end, embeddings for segments were generated using the semantic embedding model being tested and stored in Chroma. Query embeddings were then generated using the same model and used to retrieve the top 5 matches from Chroma. Average recall@k (equation 1) was employed as one of two metrics to quantify how many of the correct answers appeared within the top 'k' results provided by each model, thereby determining the models' ability to retrieve necessary information from the dataset. The second employed metric is Mean Reciprocal Rank (MRR) which is a statistical measure used to evaluate the performance of query response systems. MRR is calculated as the average of the reciprocal ranks of results for a set of queries as shown in equation 2. Essentially, MRR calculates the mean of these reciprocal ranks over all queries tested. Higher MRR values indicate that the correct answers tend to appear earlier in the list of responses, which is desirable as the context being passed to a generator is usually limited."}, {"title": "4.1.2. Experiment 2: Investigating the impact of using a different dialect on retrieval results", "content": "As stated earlier, dialects pose a serious challenge when dealing with Arabic text. It is often the case that a user of a RAG system might choose to interact with it using their local dialect. If the user's query cannot be matched to the text segment from which a response can be generated, no appropriate response will be generated. To evaluate the impact of dialect-specific queries on system performance where the text segments are represented in one variant (MSA here) and the query in another, the experiment described in the previous section was repeated using the Egyptian Arabic version of the questions to create query embeddings. In this experiment, only the Ar_EduText dataset was used. The results are shown in table 5."}, {"title": "4.1.3. Experiment 3: Investigating the impact of using a disambiguating questions on retrieval results", "content": "As mentioned in Section 3.1.2 and shown in Table 1, some questions in the ARCD dataset can only be understood in the context of the preceding question. Section 3.1.2 also detailed how these questions were automatically disambiguated using GPT-3.5 Turbo and subsequently reviewed manually, with both versions being retained. This section presents the results of repeating the experiment described in Section 4.1.1, with both versions of the disambiguated questions. The purpose of this step was to evaluate the performance of the embedding models independently of the ambiguity issue. The outcomes are displayed in Tables 6 and 7, respectively. Values that went down or did not change are marked. All other values went up."}, {"title": "4.2. Generator related experiments", "content": "To conclude the exploration of Retrieval-Augmented Generation (RAG) application in Arabic, the final phase was to evaluate various Large Language Models (LLMs) as generators thus completing the pipeline. Since the E5-Large model consistently outperformed all other models on the two used datasets, it was the one used for query and document embedding in the retrieval stage. After the retrieval step was completed, the top 5 returned documents were given to the generators listed in section 3.4 along with the question for which an answer is desired. For the Ar_EduText dataset, experiments were carried out using GPT3.5 Turbo, JAIS 7B quantized, LLama3, Mistral, and Mixtral, while for the ARCD dataset, only the last three open-source LLMs were employed. Each of the open-source LLMs, generated superfluous text in which the answer to the query was often embedded. To apply the chosen metrics as accurately as possible, post-processing functions were written for each LLM after observing the pattern of its generated outputs. All post processing functions can be found in the project's GitHub repository. The results of this experiment are presented in Table 8 and 9, and sample output from the ARCD dataset is presented in Fig. 3"}, {"title": "5. Conclusion and Future work", "content": "This research started out with the expectation that existing embedding models and Large Language Models (LLMs) would face significant challenges in processing Arabic text effectively. This assumption stems from the unique linguistic features of Arabic, including its rich morphology, complex syntactical structures, and diverse dialect base which often pose difficulties for standard NLP models developed predominantly for English. However, the empirical evaluation presented in this work contradicts these initial expectations, demonstrating a notable degree of proficiency and applicability of these models to Arabic texts. In terms of semantic embedding models, it was observed the E5- large model as well as the BGE model show great potential for use with Arabic retrievers and for semantic representation in general.\nFurthermore, experiments carried out on various open source LLMs, show that Llama3 and Mistral have great potential as Arabic generators. This finding is instrumental, suggesting that existing open source LLMs can be leveraged to contribute significantly to the development of effective Arabic NLP applications. Future work should investigate the role of prompt engineering and fine tuning to increase the performance of these LLMs even further. Despite these encouraging outcomes, the authors acknowledge the necessity for broader research to fully investigate the potential of these models. Specifically, future work should explore the application of presented models to a wider array of Arabic dialects and on bigger datasets as well as explore more complicated RAG pipelines."}, {"title": "Appendix A. Prompt Used to automatically disambiguate questions", "content": "\nQuestion 1: {q1}\nQuestion 2: {q2}\nYou are an expert who understands Arabic fluently. Given these two questions, your task\nis to rephrase the second question only if it contains ambiguities that might confuse\nsomeone without context from the first question. An ambiguity might be a vague reference or\nunclear term that cannot be understood without additional context. Do not modify question 2\nif it is clear and understandable on its own. Always maintain the response in Arabic.\nExample 1:\n\u0645\u0646 \u0647\u0648 \u062d\u0645\u0632\u0629 \u0628\u0646 \u0639\u0628\u062f \u0627\u0644\u0645\u0637\u0644\u0628\u061f :1 Question\nQuestion 2: \u0628\u0645\u0627 \u0648\u0635\u0641\u0647 \u0631\u0633\u0648\u0644 \u0627\u0644\u0644\u0647\u061f\n\u0628\u0645\u0627 \u0648\u0635\u0641\u0647 \u0631\u0633\u0648\u0644 \u0627\u0644\u0644\u0647 \u062d\u0645\u0632\u0629 \u0628\u0646 \u0639\u0628\u062f \u0627\u0644\u0645\u0637\u0644\u0628 \u061f :Correct modification\nExample 2:\n\u0643\u0645 \u064a\u0628\u0644\u063a \u0627\u0631\u062a\u0641\u0627\u0639 \u0645\u0643\u0629 \u0639\u0646 \u0633\u0637\u062d \u0627\u0644\u0628\u062d\u0631\u061f :1 Question\nQuestion 2: \u0627\u064a\u0646 \u062a\u0642\u0639 \u0645\u0643\u0629\u061f\nCorrect modification: \u0627\u064a\u0646 \u062a\u0642\u0639 \u0645\u0643\u0629\u061f\nRemember, modifications are only needed if they clarify ambiguities directly related to\nthe context provided by question 1. Any name or specific noun is not considered ambiguous."}]}