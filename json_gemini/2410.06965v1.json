{"title": "UNCOVERING FACTOR LEVEL PREFERENCES TO\nIMPROVE HUMAN-MODEL ALIGNMENT", "authors": ["Juhyun Oh", "Eunsu Kim", "Jiseon Kim", "Wenda Xu", "Inha Cha", "William Yang Wang", "Alice Oh"], "abstract": "Despite advancements in Large Language Model (LLM) alignment, understand-\ning the reasons behind LLM preferences remains crucial for bridging the gap be-\ntween desired and actual behavior. LLMs often exhibit biases or tendencies that\ndiverge from human preferences, such as favoring certain writing styles or pro-\nducing overly verbose outputs. However, current methods for evaluating prefer-\nence alignment often lack explainability, relying on coarse-grained comparisons.\nTo address this, we introduce PROFILE (PRObing Factors of InfLuence for Ex-\nplainability), a novel framework that uncovers and quantifies the influence of spe-\ncific factors driving preferences. PROFILE's factor level analysis explains the\n\"why\" behind human-model alignment and misalignment, offering insights into\nthe direction of model improvement. We apply PROFILE to analyze human and\nLLM preferences across three tasks: summarization, helpful response generation,\nand document-based question-answering. Our factor level analysis reveals a sub-\nstantial discrepancy between human and LLM preferences in generation tasks,\nwhereas LLMs show strong alignment with human preferences in evaluation tasks.\nWe demonstrate how leveraging factor level insights, including addressing mis-\naligned factors or exploiting the generation-evaluation gap, can improve align-\nment with human preferences. This work underscores the importance of explain-\nable preference analysis and highlights PROFILE's potential to provide valuable\ntraining signals, driving further improvements in human-model alignment.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) are widely recognized for their ability to generate human-level\ntexts, yet they often fail to fully align with human preferences. Despite significant advancements in\nalignment techniques like Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al.,\n2022) and Direct Preference Optimization (DPO) (Rafailov et al., 2024), LLMs tend to exhibit bi-\nases toward specific writing styles (Das et al., 2024) or generate overly verbose outputs (Park et al.,\n2024). Understanding the underlying factors contributing to this misalignment is essential for en-\nhancing LLM performance. Moreover, given the increasing use of AI-generated feedback in LLM\ntraining (Bai et al., 2022; Lee et al., 2023; Guo et al., 2024), considering their alignment not only\nas generators but also as evaluators becomes crucial. This underscores the need for a comprehen-\nsive analysis of model behavior in both roles, particularly in light of recent findings on performance\ninconsistencies of LLMs across various settings (West et al., 2023; Oh et al., 2024).\nPrevious attempts to understand and improve preference alignment (Ouyang et al., 2022; Rafailov\net al., 2024; Song et al., 2024) have primarily relied on coarse-grained, comparative approaches,\nlacking explainability. These methods often focus on identifying which model is preferred overall\nbut do not provide insights into the factors that drive these preferences. While some studies have\nattempted to analyze human preferences at a finer granularity (Hu et al., 2023; Kirk et al., 2024;\nScheurer et al., 2023), a comparative analysis of how these preferences align with models remains\nlimited. Furthermore, existing evaluation approaches often suffer from limited scalability and gener-"}, {"title": "2 PROBLEM DEFINITION", "content": "To address our central question of how well LLMs align with human preferences, we acknowledge\nthe multifaceted nature of human preference where a perceived quality of response depends on\nvarious factors. To uncover these latent preferences, we define a set of factors F = {f_1, f_2, ..., f_n},\nwhere each f_i represents aspects of a text (e.g., fluency, length). We then quantify their influence on\nhuman preference as factor-level preferences, denoted by f(R).\nf(R) = (f_1(R), f_2(R), ..., f_n(R)) in R^n,\nwhere score of the factor f_i(R) captures the influence of f_i on overall preference across response\nspace R. We extend this concept to include both human and model, referring to both as \"agents.\""}, {"title": "2.1 OPERATIONAL DEFINITIONS", "content": "To determine f(R) in generation and evaluation settings, we analyze preferences in the set of re-\nsponse pairs, which are directly observable. Specifically, we experiment with a pairwise preference\nsetting, where preferences are determined by comparing two responses. The operational definitions\nof the pairwise preferences required for this experiment are defined as follows.\nPairwise Preferences. We define the pairwise preference function for a pair of two responses as:\nPref: R \\times R \\rightarrow \\{-1,0,1\\}\nwhere Pref (r_i, r_j) = 1 if response r_i is preferred over response r_j; Pref(r_i, r_j) = -1 if response\nr_j is preferred over response r_i; and Pref(r_i,r_j) = 0 if the preference between r_i and r_j is a tie.\nIn our experiments, we define model pairwise preferences for both model generation (S = g) and\nevaluation (S = e) settings.\nModel Pairwise Preferences in Generation. While models might not have preferences in the same\nway humans do, we can operationally define the preferences of a model through the responses\nit generates at different score levels. Specifically, we prompt the model to generate a set of re-\nsponses R_s = {r|Score(r) = s} for each score level s, where Score(r) be the score assigned\nby a model itself to a response r. Pairwise Preferences in Generation, Pref_g, is defined based on\ncomparing the model-assigned scores Score(r_i) and Score(r_j). Specifically, Pref_g(r_i,r_j) = 1\nif Score(r_i) > Score(r_j) and Pref_g(r_i,r_j) = \u22121 if Score(r_i) < Score(r_j). This approach is\ninspired by methods used in constructing training data for evaluator models (Kim et al., 2023).\nModel Pairwise Preferences in Evaluation. We define model preferences in an evaluation set-\nting through pairwise comparisons. Given two responses r_i and r_j, the model predicts which is\nthe better response using an evaluation-based preference function Pref_e. The evaluation function\nPref_e(r_i,r_j) = 1 if the model evaluates r_i as preferable over r_j; Pref_e(r_i,r_j) = \u22121 if r_j is\npreferred over r_i; and Pref_e(r_i, r_j) = 0 if the model finds them equally preferable (tie). This ap-\nproach, where models make pairwise preference evaluation, is similar to how LLMs generate pref-\nerence labels (Lee et al., 2023). Although we extract model preferences separately for evaluation\nand generation tasks, we assume that human preferences remain consistent across both, as human\njudgments are always based on evaluating model-generated outputs.\nPairwise Factor Comparison. For each factor f_k, we define a function M_k to compare its manifes-\ntation in pairs of responses:\nM_k: R \\times R \\rightarrow \\{-1,0,1\\}\nSpecifically, M_k (r_i, r_j) = 1 if factor f_k is more manifest in response r_i; M_k(r_i, r_j) = -1 if factor\nf_k is more manifest in response r_j; and M_k(r_i,r_j) = 0 if factor f_k is equally manifest in both\nresponses. For example, if f_k represents length and r_i is longer than r_j, M_{length}(r_i, r_j) = 1."}, {"title": "3 PROFILE: PROBING FACTORS OF INFLUENCE FOR EXPLAINABILITY", "content": "We introduce PROFILE, a novel method for automatically quantifying the influence of specific\nfactors on both human and model preferences, revealing factor-level preferences. Building on the\nwork of Hu et al. (2023), which analyzes factors influencing human preferences, PROFILE extends\nthis analysis to assess preference alignment between humans and models by identifying the driving\nfactors behind these preferences.\nWe first establish a comprehensive taxonomy of fine-grained factors to guide the selection of ap-\npropriate factor sets F for the tasks (\u00a7 3.1). We then detail methods for quantifying the influence\nof each factor, f_i(R), enabling us to determine factor-level preferences for each agent and analyze\ntheir alignment (\u00a7 3.2). PROFILE's versatility across various agents, tasks, and settings (generation\nand evaluation) makes it a powerful tool for comprehensive preference alignment analysis."}, {"title": "3.1 \u03a4\u0391\u03a7\u039f\u039d\u039f\u039cY DESIGN", "content": ""}, {"title": "3.2 QUANTIFICATION OF HUMAN-MODEL PREFERENCE ALIGNMENT", "content": "This section outlines the process of quantifying factor-level preferences and measuring the human-\nmodel alignment of factor-level preferences. We calculate factor score f_i(R) by comparing the\npairwise preference denoted as Pref with the factor-specific pairwise comparison, denoted as M_k,\nacross all possible response pairs in the dataset R. These scores are then used to rank the factors,\nfollowed by evaluating human-model alignment at the factor level."}, {"title": "Determining Mk: Automatic Factor Extraction", "content": "To analyze the manifestation of our factors in\nmodel and human-preferred responses and determine M_k, we develop an automatic factor extrac-\ntion framework. We employ three approaches based on the objectivity of each factor: (i) Rule-based:\nFor straightforward, objective factors, we use deterministic algorithms. Length and Novel Words are\nextracted this way. (ii) UniEval-based: For inherently subjective factors (Fluency and Coherence),\nwe use the well-established UniEval metric (Zhong et al., 2022). UniEval is a learned metric that\nprovides scores of range 0-1 for various aspects of text quality. (iii) LLM-based: For factors that\nrely on objective criteria but require more nuanced judgment, we use GPT-40 with carefully de-\nsigned prompts. This approach is further divided into \"response-based\u201d (Intent Alignment and\nFormality Alignment) and \u201catomic-fact-based\" (the remaining seven) extraction depending on the\nlevel of detail needed for each factor. By combining these three approaches, our framework captures\na wide range of factors with appropriate levels of objectivity. The specific details of the implemen-\ntation of each method and validation of LLM-based extractions can be found in Appendix C."}, {"title": "Factor Score: Quantifying Influence of Each Factor", "content": "To calculate the factor score, we use \\tau_{14},\nwhich is one of the variations of Kendall's correlation (Deutsch et al., 2023). Since we consider\npairwise comparisons, \\tau_{14} of factor f_k is calculated based on pairwise concordance and discordance,\nas described in Bojar et al. (2017).\n\\tau_{14}(f_k) = \\frac{C_k-D_k}{C_k+D_k+T_k} \\qquad(1)\nWhere C_k, D_k, and T_k represent the counts of concordant pairs, discordant pairs, and ties based on\nfactor f_k, respectively. C_k counts when the overall preference and manifestation of factor f_k agree,\nD_k counts when they disagree. and T_k captures ties.\nFor the models' pairwise preference Pref (r_i, r_j) and the pairwise factor comparison M_k(r_i, r_j) for\nresponse pairs, there are no ties in preferences Pref within the generation setting, as the model does\nnot generate responses with identical scores in our experimental setup. In the evaluation setting, we\nomit ties at the factor level (e.g., pairs with the same length) to provide a clearer analysis of factor\ninfluence. Additionally, since ties are present in one of our comparison sets for calculating Kendall's\n\\tau, we use \\tau_{14}, which is better suited to handle the distribution of ties in our data. mathematically:\nC_k = \\sum_{r_i,r_j\\in R,i<j}\\[Pref(r_i,r_j) \\cdot M_k(r_i, r_j) = +1],\nD_k = \\sum_{r_i,r_j\\in R,i<j}\\[Pref(r_i,r_j) \\cdot M_k(r_i, r_j) = -1],\nT_k = \\sum_{r_i,r_j\\in R,i<j} \\begin{cases} #[M_k(r_i, r_j) = 0] & \\text{in generation setting} \\\\ #[Pref (r_i,j) = 0] & \\text{in evaluation setting} \\end{cases}\nwhere [condition] is 1 if the condition is true and 0 otherwise.\nIn the case of the length factor, consider when response r_1 is longer than r_2 (M_{length}(r_1, r_2) = 1)\nand the model prefers r_1 (Pref (r_1, r_2) = 1), this pair is classified as concordant. Conversely, if the\nmodel prefers the shorter r_1, it is discordant. Evaluating all pairs, a positive factor score indicates a\npositive influence of the factor, a negative score indicates a negative influence and a score close to\nzero implies minimal influence. The magnitude of the score reflects the strength of this influence."}, {"title": "Factor-Level Preference Alignment", "content": "An agent's factor-level preferences are defined as a ranking\nof factors based on their scores, where a higher rank and score indicate a stronger influence of\nthat factor on the agent's overall preference. The correlation between human and model rankings\nreflects their agreement on the relative importance of factors to overall preference, which we use as a\nmeasure of factor-level preference alignment between humans and models. We calculate Spearman's\n\\rho, Kendall's \\tau, and Pearson's r coefficients to quantify this alignment."}, {"title": "4 ANALYZING PREFERENCE ALIGNMENT THROUGH PROFILE", "content": "This section details the experimental setup used to address our research questions (\u00a7 4.1). Results\nfor RQ1, RQ2, and RQ3 are presented in Sections \u00a7 4.2, \u00a7 4.3, and \u00a7 4.4, respectively."}, {"title": "4.1 EXPERIMENTAL SETTING", "content": "Tasks and Models. We analyze three publicly available datasets used in preference optimization\nmethods: (i) Reddit TL;DR (Stiennon et al., 2020), which includes human ratings of summaries\nacross multiple evaluation dimensions; (ii) StanfordHumanPreference-2 (SHP-2) (Ethayarajh et al.,\n2022), focusing on human preferences over responses in the \u201creddit/askacademia\" domain;\nand (iii) OpenAI WebGPT (Nakano et al., 2021), which compares model-generated answers on the\nELI5 subreddit based on factual accuracy and usefulness. We refer to the tasks for each dataset\nas summarization, helpful response generation, and document-based QA tasks in this paper. We\nexclude pairs with human Tie ratings in all three datasets, as our analysis focuses on cases with clear\npreference distinctions. For our experiments, we utilize both open-source and proprietary LLMs.\nOpen-source models include LLaMA 3.1 70B (Dubey et al., 2024), Mixtral 8x7B Instruct v0.1 (Jiang\net al., 2024), and three T\u00dcLU v2.5 models (Ivison et al., 2024) (T\u00dcLU v2.5 + PPO 13B (13B\nRM), T\u00dcLU v2.5 + PPO 13B (70B RM), and T\u00dcLU v2.5 + DPO 13B). Proprietary models include\nGemini 1.5 Flash (Reid et al., 2024), GPT-40 (OpenAI, 2024), and GPT-3.5. From here on, we refer\nto Gemini 1.5 Flash as Gemini 1.5, Mixtral 8x7B Instruct v0.1 as Mixtral, T\u00dcLU v2.5 models as\nTulu 2.5 + {alignment training strategy}. Detailed descriptions of the datasets and models can be\nfound in Appendix B.2.\nExperimental Setup. For each task, we explore two settings: (i) Generation, where models generate\nresponses that would receive a score of 1-5 for a given task, and (ii) Evaluation, where models select\nthe better of two provided responses, which are taken from the datasets. See Appendix D for prompts.\nIn both settings, we use PROFILE to extract factor scores and their factor rankings and measure\nthe correlation with human judgments (factor-level preference alignment). In addition to factor-\nlevel analysis, we assess overall pairwise response agreement between humans and models. For\nevaluation, we report the percentage of models' agreement with existing human labels by measuring\nhow often it aligns with human judges' selections of the better response.\""}, {"title": "4.2 ARE MODELS ALIGNED WITH HUMAN PREFERENCE AT A FACTOR-LEVEL IN\nGENERATION TASKS?", "content": "Human and model preferences consistently misalign at the factor level across summarization, help-\nful response generation, and document-based QA (Figure 3). Models consistently prioritize Length\nacross all tasks (right-hand side of the figure), while human priorities vary. In the summarization task\n(Figure 3a), humans prioritize Intent Alignment (0.596) and Formality Alignment (0.594), while\nmodels focus on Length (GPT-4o: 0.978, Gemini 1.5: 0.906), often generating longer summaries\nfor higher scores. Notably, humans dislike summaries with many new words (factor score -0.167 for\nNovel Words), yet models produce more novel words in high-scoring outputs (GPT-40: 0.472, Gem-\nini 1.5: 0.56). The numbers in parentheses represent factor scores. In the helpful response generation\ntask (Figure 3b), humans prioritize Receptiveness and Helpfulness, but their overall factor scores are\nrelatively low (0.248, 0.193 respectively), indicating no single dominant factor drives their prefer-\nences in this task. In contrast, models exhibit much stronger preferences, again emphasizing Length\nand Number Of Facts. For document-based QA (Figure 3c), humans prioritize Receptiveness and\nprefer answers without Hallucinations, aligning with the need for factual accuracy of the task. How-\never, models still heavily emphasize Length (0.965 for both GPT-40 and Gemini 1.5) and also pri-\noritize Coherence and Helpfulness more than humans do.\nThis misalignment is quantified by low factor-level preference alignment (\\tau). The left Generation\ncolumn in Table 1 shows that even the best-performing model (Gemini 1.5) only achieves a 0.289\n\\tau correlation with human preferences in summarization task. Similar low correlations are observed\nin other tasks (Appendix, Table 8). Full factor scores are available in Appendix Table 7. A small-\nscale annotation exploring human evaluation of model-scored responses, including an example of\ndisagreement, is presented in Appendix A."}, {"title": "4.3 ARE MODELS ALIGNED WITH HUMAN PREFERENCES AT A FACTOR-LEVEL IN\nEVALUATION TASKS?", "content": "Our analysis reveals a consistent trend of stronger alignment between models and human preferences\nin evaluation tasks compared to generation tasks. Table 1 demonstrates this by showing factor-level\npreference alignment of human and model, measured using Kendall \\tau, Spearman \\rho, and Pearson r"}, {"title": "4.4 ACHEIVING BETTER ALIGNMENTS THROUGH PROFILE", "content": "Improving Alignment in Evaluation through Factor-level Guidance. One of the key features of\nour approach is its explainability of human-LLM misalignment. To evaluate whether insights from\nPROFILE can enhance model performance, we conduct an experiment using a summarization task\nwith Mixtral and Tulu 2.5 + PPO (13B RM), providing LLM evaluators with factor-specific guid-\nance. Two strategies are used in the prompts: Guide Rand (guidance on a randomly selected factor)\nand Guide Mis (guidance on a factor where model and human preferences significantly diverge).\nAcross 200 response pairs for each model,\nGuide Mis yields a significant increase in eval-\nuation agreement with humans compared to\nboth Guide Rand and the baseline agreement\n(without any guidance, calculated on the same\n200 pairs). These results, presented in Table 2,\nstrongly suggest that tailoring guidance to ad-\ndress specific misalignments effectively im-\nproves model performance and alignment with human expectations, highlighting the value of our\nfactor-level analysis (See Appendix E.1 for experiment details).\nLeveraging Evaluation for Better Alignment in Generation. Prior analysis shows that models\nhave stronger factor-level alignment during evaluation than generation (Section 4.3), suggesting that\nevaluator feedback might improve generation alignment. To test this, we conduct an experiment on\nfeedback-driven summary improvement: a generator model produces two initial summaries per in-\nput, and an evaluator model selects the preferred summary (or tie) and its justification. The generator\nthen uses this feedback to create an improved summary.\nWe compare this with two baselines: (1) Baseline A, where the generator produces one improved\nsummary from both initial summaries without feedback; and (2) Baseline B, where it generates two\nimproved summaries without feedback, each based on one initial summary. This simulates a com-\nmon generation improvement scenario where improvement relies on an implicit critique of a single\ntext piece. The experiment uses 100 Reddit TL;DR samples with three generators (GPT-40, LLaMA\n3.1 70B, and Tulu 2.5 + PPO (70B RM)) and the top-performing evaluator (GPT-40)."}, {"title": "5 DISCUSSION", "content": "Alignment of Reward Models and Language Models.\nTo understand the influence of Reward Mod-\nels (RMs) in RLHF training and determine if\nHuman-LLM misalignment stems from RMs,\nwe compare the factor-level alignment of RMs\nand LLMs trained with those RMs against hu-\nman preferences in a summarization task.\nFigure 4 shows the factor-level alignment (\\tau)\nbetween human preferences and those of RMS\nand LLMs in both generation and evaluation\nsettings. The results indicate that RMs have\na stronger alignment with human preferences\nthan LLMs in both settings, implying that mis-\nalignment doesn't stem from the RMs them-\nselves. Additionally, the larger 70B RM dis-\nplays stronger alignment than the smaller 13B\nRM, suggesting a positive correlation between\nRM size and alignment suggests a potential link\nthat motivates further investigation.\nAlignment over latent preference. As noted in (Park et al., 2024), our experiments show that with\nsingle-score human preference, models can engage in reward hacking (Skalse et al., 2022) by gen-\nerating overly lengthy outputs, mistakenly optimizing for perceived human preference. This issue is\nparticularly problematic for downstream tasks like summarization, which require concise and accu-\nrate responses. PROFILE helps quantify latent preferences and identify factor-level misalignments,\nsuch as length and Intent Alignment. By diagnosing these misalignments, PROFILE offers training\nsignals to improve factor-level alignment. Similar to fine-grained RLHF (Wu et al., 2023) and LLM-\nRefine (Xu et al., 2024), we can use factor-level scores and fine-grained guidance to enhance LLM\nalignment and self-refinement.\nGeneralizability of Our Results. Our research deviates from the typical language model setup by\nusing a 1-5 scoring system for response generation. To assess the generalizability of our approach,\nwe compare responses generated through direct generation (without scoring) with those across the"}, {"title": "6 RELATED WORK", "content": "Explainable Evaluation of LLMs. Recent research has increasingly emphasized the need for more\nexplainable evaluations of LLMs. For instance, researchers have proposed fine-grained atomic eval-\nuation settings for tasks like fact verification and summarization (Min et al., 2023; Krishna et al.,\n2023), developed a benchmark for fine-grained holistic evaluation of LLMs on long-form text (Ye\net al., 2024), and enhanced evaluation transparency through natural language feedback (Xu et al.,\n2023). Building on this trend, our work shifts from evaluating individual factors in isolation to an-\nalyzing their influence on human preferences and investigating the alignment between human and\nmodel judgments regarding the relative importance of these factors. Furthermore, researchers are\nactively exploring the potential of LLMs as evaluators. Fu et al. (2024); Madaan et al. (2024); Liu\net al. (2023) demonstrate the capacity of large models like GPT-4 to achieve human-like system-level\nevaluation. However, recent works (West et al., 2023; Oh et al., 2024) reveal discrepancies in model\nperformance between generation and evaluation tasks. Inspired by frameworks to meta-evaluate llm\nas an evaluator (Zheng et al., 2023; Ribeiro et al., 2020), our work evaluates not only the quality of\nmodel-generated text but also the alignment of model preferences in evaluation settings, providing\na more comprehensive assessment of LLM capabilities.\nHuman-AI Preference Alignment. Aligning large language models (LLMs) with human prefer-\nences is a central focus in LLM research, leading to techniques like supervised instruction tun-\ning (Mishra et al., 2021; Wei et al., 2021), RLHF (Ouyang et al., 2022), DPO (Guo et al., 2024),\nand RLAIF, which utilizes AI-generated feedback (Bai et al., 2022; Lee et al., 2023). However, most\nstudies focus on overall performance (e.g., a response as a whole). While some work has explored\nusing fine-grained human feedback (Dong et al., 2023; Wu et al., 2024), a comprehensive under-\nstanding of how granular factors contribute to and differentiate human and model preferences is still\nlacking. Hu et al. (2023) take a step in addressing this gap by probing the factors influencing human\npreferences. Building on this work, we expand the investigation of granular preference alignment\nacross multiple tasks and extend the analysis to model generation, providing a comparative analysis\nof the factors driving both human and model preferences."}, {"title": "7 CONCLUSION", "content": "We introduce PROFILE, a novel framework for granular factor level analysis of LLM alignment with\nhuman preferences. Our analysis using PROFILE reveals that LLMs tend to over-prioritize factors\nlike output length, misaligning human preferences during generation. However, these models exhibit\nstronger alignment in evaluation tasks, indicating the potential for leveraging evaluative insights to\nimprove generative alignment. By advancing beyond coarse-grained methods, PROFILE facilitates\na nuanced understanding of the alignment gaps and mismatches between human and model prefer-\nences. These insights underscore the necessity for more sophisticated, factor-level alignment strate-\ngies that can guide the development of LLMs to better align with human expectations, ultimately\nfostering more reliable aligned AI systems."}, {"title": "8 ETHICS STATEMENT", "content": "Our research relies on established benchmarks and models, and does not involve the development\nof new data, methodologies, or models that pose significant risks of harm. The scope of our experi-\nments is limited to analyzing existing resources, with a focus on model performance. Human studies\nconducted within this work adhere to relevant IRB exemptions, and we ensure fair treatment of all\nparticipants. Our work is mainly focused on performance evaluation, we recognize that it does not\nspecifically address concerns such as bias or harmful content."}, {"title": "9 REPRODUCIBILITY STATEMENT", "content": "The datasets and models we use in our study are detailed in \u00a7 4.1. For more comprehensive de-\nscriptions of the datasets and specific versions of the models, please refer to Appendix B.1 and B.2.\nThe methodology we employed for factor extraction in our experiments is presented in Appendix C,\nwhile the prompting configurations set up for the experiments can be found in Appendix D and\nE. Appendix F and G contain additional experimental results not presented in the main paper. Ap-\npendix F provides the lists of all factor scores for both generation and evaluation across all three\ntasks used in the study. Appendix G presents detailed results regarding the generalizability of our\nfindings in the \u00a7 5."}, {"title": "APPENDIX", "content": "A HUMAN EVALUATION OF MODEL GENERATIONS\nWe collect human preference data via Amazon Mechanical Turk (MTurk) for 30 posts and 6 models.\nFor each post, three summary pairs-selected from five model-generated summaries (scored 1 to\n5)-are presented to three annotators. Annotators, restricted to US-based workers with a 95% and\nHIT approval rate and over 5,000 approved HITs, follow these instructions: \u201cA good summary is a\nshorter piece of text that captures the essence of the original. It aims to accomplish the same purpose\nand convey the same key information as the original post. Please choose the better summary, A or\nB.\" Each annotation is compensated at $0.25. This process yields 1,620 annotations (30 posts * 6\nmodels * 3 pairs/model/post * 3 annotations/pair).\nFor each model and corresponding summary pair, we calculate an agreement rate, which indicates\nthe percentage of pairs where at least two out of three annotators prefer the summary with the higher\nscore assigned by the model. Each model is evaluated on 90 summary pairs, derived from 30 posts\nwith three pairs per post."}, {"title": "A.1 EXAMPLE OF HUMAN-MODEL MISALIGNMENT", "content": "GPT-4 misaligned examples\nPost: Good Morning/Afternoon r/advice, Never posted on Reddit before at all, but I figured (based on\nthe overall reliability of you nice individuals) that now would be a good time to start.\nAbout a year ago I moved away from my Midwest town to the west coast for work, and have\njust recently moved back. I was calling the Utility providers in my area to set up the gas/water/elec for\nmy new place, but I was informed that gas could not be set up due to owing a past amount.\n(...)\nMy question is, am I really on the hook for these fees and what recourse could I possibly\nhave? I cannot afford to pay it, but I also don't think that it should be on me TO pay it. Any advice is\ntruly appreciated as I have not had any experience with this type of thing before.\nGPT-40's score 5 generation [length: 93, # facts: 10, src. coverage: 0.389]: A Reddit user re-\ncently moved back to their Midwest hometown and, while setting up utilities for their new place,\ndiscovered they owe $500 in gas bills from a college house they lived in until 2012. The user didn't\ncancel the service when they moved out, leading to the bill accruing over time. They are looking for\nadvice on whether they are responsible for this debt and what recourse they might have, as they are\nunable to afford the payment. The user is new to Reddit and appreciates any guidance on handling this\nsituation.\nGPT-40's score 3 generation [length: 61, # facts: 9, src. coverage: 0.44]: A Reddit user seeks\nadvice after discovering they owe $500 in gas bills from a college house they left in 2012. They\ndidn't cancel the service when they moved out, and the bill continued to accrue. They want to know if\nthey are responsible for paying it and what actions they can take, as they can't afford to pay the bill.\n(Human Preferred Output)\nA higher score summary generated by GPT-40 as shown by the factor-level analysis (in magenta), is\nlonger and includes details not necessary to understand the essence of the original post."}, {"title": "A.2 HUMAN EVALUATION OF MODEL GENERATIONS", "content": "The agreement rates for each model are summarized in Table 4. Despite low factor-level preference\nalignment (\\tau), overall agreement rates range from 56% to 75%. This suggests that binary evalua-\ntions, where annotators choose the better summary based on overall quality, can mask factor-level\nmisalignments such as preferences for summary length. Since models consistently favor longer re-\nsponses, continued reliance on this setup to generate human-annotated training data may reinforce\nthe tendency towards verbose answers, aligning with human preferences superficially but missing\nfiner-grained alignment on factors like length or conciseness."}, {"title": "B EXPERIMENTAL SETTING", "content": "B.1 TASKS\nWe examine three publicly available datasets of pairwise human judgments commonly used in pref-\nerence optimization methods like RLHF and DPO training: Reddit TL;DR We analyze the dataset\nreleased by OpenAI (Stiennon et al., 2020), which includes human ratings of summaries across mul-\ntiple axes (referred to as \"axis evaluations\"). Higher scores indicate human preference across\nmultiple evaluation dimensions. StanfordHumanPreference-2 (SHP-2) (Ethayarajh et al., 2022),\nfocuses on capturing human preferences over responses to questions and instructions, prioritizing\nhelpfulness. Higher scores indicate a more helpful response. For this study, we use responses from\nthe \"reddit/askacademia\" domain. OpenAI WebGPT This dataset (Nakano et al., 2021),\naddresses the task of generating answers to questions from the ELI5 (\u201cExplain Like I'm Five\u201d) sub-\nreddit. Human annotations compare two model-generated answers based on factual accuracy and\noverall usefulness. We exclude pairs with Tie ratings in all three datasets, as our"}]}