{"title": "ROER: Regularized Optimal Experience Replay", "authors": ["Changling Li", "Zhang-Wei Hong", "Pulkit Agrawal", "Divyansh Garg", "Joni Pajarinen"], "abstract": "Experience replay serves as a key component in the success of online reinforcement\nlearning (RL). Prioritized experience replay (PER) reweights experiences by the\ntemporal difference (TD) error empirically enhancing the performance. However,\nfew works have explored the motivation of using TD error. In this work, we provide\nan alternative perspective on TD-error-based reweighting. We show the connec-\ntions between the experience prioritization and occupancy optimization. By using\na regularized RL objective with f-divergence regularizer and employing its dual\nform, we show that an optimal solution to the objective is obtained by shifting\nthe distribution of off-policy data in the replay buffer towards the on-policy op-\ntimal distribution using TD-error-based occupancy ratios. Our derivation results\nin a new pipeline of TD error prioritization. We specifically explore the KL di-\nvergence as the regularizer and obtain a new form of prioritization scheme, the\nregularized optimal experience replay (ROER). We evaluate the proposed priori-\ntization scheme with the Soft Actor-Critic (SAC) algorithm in continuous control\nMuJoCo and DM Control benchmark tasks where our proposed scheme outper-\nforms baselines in 6 out of 11 tasks while the results of the rest match with or do\nnot deviate far from the baselines. Further, using pretraining, ROER achieves\nnoticeable improvement on difficult Antmaze environment where baselines fail,\nshowing applicability to offline-to-online fine-tuning.", "sections": [{"title": "1 Introduction", "content": "Deep reinforcement learning (RL) have shown wide applications in various domains. One key factor\nfor its success is the integrated structure of experience replay. Experience\nreplay allows RL algorithms to use collected experience to compute updates for the\ncurrent policy. It significantly increases the data efficiency and allows RL to be applied to fields\nwhere online data collection is expensive. On the other hand, sampling from experience replay\nbuffer breaks the temporal correlations among experiences and stabilizes the gradient update. However, past work shows that not all samples are equally informative in updating\npolicy. To enhance the performance, techniques of weighted ex-\nperience replay are\nproposed to perform importance sampling and shape the distribution of the data in the replay buffer.\nAmong the proposed reweighting frameworks, prioritized experience replay (PER) is most commonly\nutilized for its simplicity and empirically good performance. PER attempts to\naccelerate learning by assigning experiences with the temporal-difference (TD) error to enable higher"}, {"title": "2 Preliminaries", "content": "Online RL. Online reinforcement learning concerns optimizing an agent's policy in a Markov deci-\nsion process (MDP). The MDP is defined by a tuple M = (S, A, P, r, \u03c1\u2080, \u03b3) where\nS and A represent the state space and action space respectively, P(s'|s, a) denotes the dynamic\nmodel, r(s, a) the reward function, \u03c1\u2080 the initial state distribution, and \u03b3 \u2208 (0, 1) the discount factor.\nThe agent\u2019s behavior is described by its policy \u03c0 : S \u2192 \u2206a. The performance of a given policy can\nbe measured by the state-action value function Q\u03c0(s, a) = E[\u2211t=0\u221e\u03b3tr(st, at)|s\u2080 = s, a\u2080 = a, st+1 \u223c\nP(\u00b7|st, at), at \u223c \u03c0(\u00b7|st)]. The corresponding value function is V\u03c0(s) := E[Q\u03c0(s, a)|a \u223c \u03c0(\u00b7|s)]. The\ngoal is to learn a policy that maximizes the \u03b3-discounted expected cumulative return:\n$$max_\u03c0 J_\u03c1(\u03c0) := (1 \u2212 \u03b3)E_{s\u2080\u223c\u03c1\u2080,a\u2080\u223c\u03c0(\u00b7|s\u2080)} [Q^\u03c0(s\u2080, a\u2080)]$$"}, {"title": "Prioritization in Experience Replay", "content": "Prioritization in experience replay applies weighted sam-\npling to the experiences by assigning weights to individual state-action which is equivalent to the\nweighted objective. We define the weight for a experience with state s and action a as w(s, a) which\nis positive. Then, under the sampling distribution d \u2208 P(S \u00d7 A), we have the weighted learning\nobjective:\n$$min_{Q_\u03b8} J(Q_\u03b8) := \\frac{1}{2}E_d[w(s, a) (B^\u03c0Q_\u03b8 \u2212 Q_\u03b8)(s, a)^2].$$"}, {"title": "3 Experience Prioritization as Occupancy Optimization", "content": "The goal of using prioritization is to accelerate learning and obtain an optimal policy which induces\nan optimal on-policy distribution. We reverse this process and motivate our formulation by the\nproblem of obtaining an optimal policy by finding the optimal on-policy distribution d\u2217, given access\nto an off-policy distribution d\u1d30. Here, d\u2217 is unknown and we assume to only have samples from\nd\u1d30 which is the distribution of the experience replay buffer. For an MDP with a reward function\nr, there exists a unique d\u2217. We consider the following regularized objective with an f-divergence\nregularizer to include d\u1d30\n$$max_{d^D} J_{D, f}(d^\u2217, d) := E_{(s,a)\u223cd^\u2217} [r(s, a)] \u2212 \u03b2D_f(d^\u2217 \\\\ d)$$\nwhere \u03b2 > 0 and Df denotes the f-divergence induced by a convex function f:\n$$D_f(d^\u2217\\\\d_D) = E_{(s,a)\u223cd_D}[f(\\frac{d^\u2217/D(s,a)}{d(sa)})]$$"}, {"title": "4 Regularized Optimal Experience Replay", "content": "In this section, we discuss our choice of using Kullback-Leibler (KL) divergence as the regularizer\nand proceed to the practical implementation of the prioritization scheme with the KL-divergence\nregularizer which forms our proposed method, the regularized optimal experience replay (ROER).\nOther forms of f-divergence can also be suitable candidates and we provide further discussions in\nAppendix B."}, {"title": "4.1 KL Divergence as Regularizer", "content": "f-divergence consists of numerous forms and past works have explored the application of it in the\npolicy update rules of RL. Particularly, many"}, {"title": "4.2 Practical Implementation", "content": "Algorithm 1 Actor Critic with Regularized Optimal Experience Replay\nNote that the form of occupancy ratio is derived from a regularized objective which can be different\nfrom the objective of the applied algorithm. For smooth integration to the existing algorithms,\nwe propose to incorporate a separate value network using the regularized objective for TD error\nestimation and priority calculation. The above KL divergence gives the value network the following\nobjective of the ExtremeV loss\n$$L(V) = E_{(s,a)\u223cd_D} [e^{(B^\u03c0Q(s,a)\u2212V(s))/\u03b2}] \u2212 E_{(s,a,s')\u223cd_D} [Q(s, a) \u2212 V(s)] \u2212 1.$$\nWe then use the TD error obtained from the value network to calculate the priority. Since the\ndistribution of d\u1d30 is changing, we consider a stable convergence and solve the optimization problem\nin many steps. We introduce a convergence parameter \u03bb and formulate the following priority update\nfunction\n$$d' = [\\lambda e^{q^\u2217 /\u03b2} + (1 \u2212 \\lambda)] \u00b7 d^D \\text{ with } \u03bb \u2208 (0, 1].$$"}, {"title": "5 Experimental Evaluation", "content": "We combine our proposed prioritization scheme ROER with Soft-Actor Critic algorithm for evaluation. We compare our method with two state-of-art prioritization schemes\nnamely uniform experience replay (UER) and the initial TD error prioritized experience replay\n(PER), and one additional baseline namely large batch experience replay\nacross a wide set of MuJoCo continuous control tasks interfaced\nthrough OpenAI Gym and DM Control tasks in an online setting. Additionally, we consider a suite of more difficult environment Antmaze\nwith pretraining using the data from D4RL to show that ROER can achieve good\nperformance in settings where both UER and PER fail. To allow for reproducibility, we use the orig-\ninal set of tasks without modification to the environment or rewards. For a fair comparison between\nbaselines and our approach, our implementations are all based on JAXRL.\nCompared to the initial PER, even though our proposed method ROER has four more hyperparam-\neters namely the architecture of value network, loss temperature (\u03b2), Gumbel loss clip (Grad Clip),\nand maximum exponential of TD-error clip (Max Exp Clip), we note that \u03b2 and Grad Clip are not\nnew and they come from the objective of Extreme Q-Learning. Grad Clip is shown to affect the\nresults lightly and the value network can use the default parameters as the critic network. A set of\nvalues works well for multiple environments. We provide more details of implementation, ablations\nand hyperparameters in Appendix C."}, {"title": "5.1 Online", "content": "In the online setting, the empirical results demonstrate that our proposed ROER outperforms state-\nof-the-arts on 6 out of 11 continuous control tasks in terms of the average evaluation while do not\ndeviate far from the baselines for the rest 3 environments as shown in Table 1. We find that ROER"}, {"title": "5.2 Value Estimation Analysis", "content": "The better performance can be empirically confirmed by the faster convergence to the true value.\nBesides the derivation that shows our proposed prioritization scheme results in the optimal solution,"}, {"title": "5.3 Online with Pretraining", "content": "Our proposed ROER prioritization scheme can benefit from pretraining using offline data and show\nsignificant performance improvement over more difficult environment Antmaze-Umaze and Antmaze-\nMedium as revealed in Table 2. We recognize that using the average performance over the last 200\nevaluations may not be the most suitable metric here due to the sparsity of rewards and the difficulty\nof the environments. Thus, we also include the learning curves to illustrate the results as in Fig. 2.\nWe found that SAC with ROER can obtain good performance at a very early stage in Antmaze-\nUmaze environment using both Antmaze-Umaze-v2 dataset and Antmaze-Umaze-diverse-v2 dataset.\nEspecially with Antmaze-Umaze-diverse-v2 dataset, SAC with ROER achieved a significantly better\nperformance compared to the state-of-art prioritization schemes while PER and LaBER are shown to\nbe detrimental to the learning process. We also note that for a more difficult environment Antmaze-\nMedium, our proposed method can obtain rewards at an early stage and shows improvement over\ntraining steps as in Fig. 2c and Fig. 2d. In contrast, SAC with UER, SAC with PER and SAC with\nLaBER completely fail to obtain any reward signal. This implication is crucial to improve training\nefficiency and safety by using offline data and correct the distribution to obtain a good performance\nonline. It shows the potential applicability of our method in offline-to-online finetuning."}, {"title": "6 Related Work", "content": "Our approach builds upon regularized RL objective and weighted experience replay.\nRegularized RL. Regularization is commonly utilized in offline reinforcement learning to constrain\nthe behavior policy and action selection. Other works have considered regularized Q-function of the behavior policy and\nstate-action value offset. Adapting such regularizers in online setting can\nachieve more stable performance. Maximizing the\nregularizer as a way to encourage exploration also shows improvement in performance and forms\nthe framework of max entropy RL. Our work\nbuilds upon the line of work that utilizes the dual function of the regularized objective which allows to express the max-return\noptimization by an expectation over an arbitrary behavior-agnostic and off-policy data distribution.\nWe extend this approach and formulate the prioritization scheme that allows the data distribution\nin replay buffer gradually converge to the optimal distribution which gives the optimal Q-function.\nTheoretical analysis of the regularized RL shows that despite its non-convexity, this problem has\nzero duality gap and can be solved exactly in the dual domain.\nWeighted experience replay. Experience replay is crucial to the success of deep RL for improv-\ning the data efficiency by using off-policy data. Various frameworks\nhave been proposed to change the sampling strategy to achieve superior performance than uniform\nsampling. Prioritized experience replay (PER) weights the experiences by their TD errors and\nshows empirical improvement when applying to deep RL. However, few works have explored the theoretical motivation of using TD-error based reweighting\nscheme. suggest that PER can be considered as an importance sampling scheme\nusing approximated per-sample gradient norms and prioritizing stochastic gradient descent variance\nreduction. Our work, on the other hand, uses dual function of the regularized RL objective to\nprovide an alternative perspective on TD-error-based prioritization. Other considerations of priori-\ntization scheme include loss value, accuracy of the TD-error estimation, regret minimization, and leveraging neural network for experience\nselection. We note that shares similarity to our work in\ncorrecting the replay buffer towards optimal distribution. However, they consider optimizing cor-\nrective feedback while our work builds on dual function of regularized RL objective. Another work\nthat shares slight similarity with our method is ReF-ER where\nthey ignore the updates from experiences that deviates significantly from the current policy. Our\nwork focuses on penalizing the TD errors of the samples that deviate from the current policy which\nleads to smaller priority instead of completely ignoring those experiences. In addition, our proposed\nmethod forms a new pipeline of TD-error-based prioritization scheme."}, {"title": "7 Conclusion", "content": "By leveraging the regularized RL objective and its dual function, we propose a new pipeline of TD-\nerror-based prioritization scheme that is more robust towards distribution shift between off-policy\ndata and current policy. By considering KL-divergence as the reuglarizer, we formulated a new\nprioritized experience replay, namely regularized optimal experience replay (ROER). Our proposed\nROER when applied to SAC empirically demonstrates the ability of mitigating the underestimation\nbias and shows faster convergence to the true value. It outperforms baselines in 6 out of 11 continu-\nous control tasks in the online setting and significantly improves the performance in Antmaze with\npretraining. However, we recognize the tuning of additional hyperparameters can limit the appli-\ncation. Future work can explore an adaptive loss temperature to dynamically adjust the strength\nof the regularization. Additionally, it would be valuable to extend the application of the proposed\nmethod to offline setting and further explore the applicability to offline-to-online fine tuning."}, {"title": "A Derivation Details", "content": "In this section, we provide the detailed derivation of our method for completeness. We reference\nfor the derivation. We start by using the regularized max-return objective\nwith divergence term between the on-policy optiaml distribution d\u2217 and off-policy distribution d\u1d30\n$$maxI_{D, f}(\u03c0) := E_{(s,a)\u223cd^\u2217} [r(s, a)] \u2212 \u03b2D_f(d^\u2217\\\\d_D),$$,\nwhere \u03b2 > 0 and Df denotes the f-divergence induced by a convex function f:\n$$D_f(d^\u2217\\\\d) = E_{(s,a)\u223cd_D} [f(\\frac{d^\u2217/D(s,a)}{d(sa)})],$$,\nwhere w/D :=\nWe then transform the f-divergence to its variational form using a dual function x : S\u00d7A \u2192 R\nthat is bounded which gives the following expressions\n$$\u00ce_{D,f} (\u03c0, x) := min E_{(s,a)\u223cd^\u2217} [r(s, a)] + \u03b2\u00b7E_{(s,a)\u223cd_D}[f^\u2217(x(s,a))] \u2212 \u03b2\u00b7 E_{(s,a)\u223cd^\u2217} [x(s, a)]$$\n$$min E_{(s,a)\u223cd^\u2217} [r(s, a) \u2212 \u03b2\u00b7 x(s, a)] + \u03b2\u00b7E_{(s,a)\u223cd_D}[f^\u2217(x(s,a)].$$,\nHere, f\u2217 is the convex conjugate of f. Recall the definition of convex conjugate: the convex conjugate\nof f(x) is defined as f\u2217(x) = supr\u2208domf{\u27e8y, x\u27e9 \u2212 f(x)}, where \u27e8y, x\u27e9 denotes the dot product.\nTo eliminate the dependence on d\u2217, we use change of variables and let Q(s, a)\u2212\u03b3V\u2217(s') = \u2212\u03b2x(s,a)+\nr(s, a). Applying the change of variable to Eq.19, we obtain:\n$$JD,f(\u03c0, Q) := min E_{(s,a)\u223cd^\u2217} [r(s, a) + Q(s, a) \u2212 \u03b3V^\u2217(s') \u2212 r(s, a)]$$\n$$+ \u03b2\u00b7E_{(s,a)\u223cd_D} [f^\u2217(\\frac{\u03b3V^\u2217(s') \u2212 Q(s, a) + r(s,a)}{\u03b2}].$$,\nNote that B\u2217Q(s, a) = r(s, a) + \u03b3V\u2217(s'). We simplify the above as\n$$JD, f (\u03c0, Q) := min E_{(s,a)\u223cd^\u2217} [Q(s, a) \u2212 \u03b3V^\u2217(s')] + \u03b2\u00b7 E_{(s,a)\u223cd_D} [f^\u2217(\\frac{B^\u2217Q(s, a) \u2212 Q(s,a)}{\u03b2}].$$"}, {"title": "B Other divergence", "content": "In this section, we firstly show the connection between the dual objective and the actor-critc ob-\njective. Then we give another consideration of regularizer which results in a different form of\nprioritization."}, {"title": "B.1 Derivation Details of ROER", "content": "KL-divergence has the form f(x) = xlog(x) and its convex conjugate has the form f\u2217(y) = ey \u2212 1.\nLet y = (B\u2217Q(s, a) \u2212 Q(s, a))/\u03b2, we follow the derivation in section 3 and obtain the following dual\nobjective:\n$$min E_{(s,a)\u223cd_D} [e^{(B^\u03c0Q(s,a)\u2212Q(s,a))/\u03b2}] + (1 \u2212\u03b3) E_{so\u223c\u03bc\u03bf} [V^\u2217 (so)] \u2212 1$$,\nwhich can be expanded to:\n$$min E_{(s,a)\u223cd_D} [e^{(B^\u03c0Q(s,a)\u2212Q(s,a))/\u03b2}] + E_{(s,a,s')\u223cd_D} [\u03b3V^\u2217(s) \u2212 \u03b3V^\u2217(s')] \u2212 1.$$,\nRecall that \u03b3V\u2217(s') = B\u2217Q(s, a) \u2212 r(s, a). We substitute the expression of \u03b3V\u2217(s') back to the above\nobjective and obtain\n$$min E_{(s,a)\u223cd_D} [e^{(B^\u03c0Q(s,a)\u2212Q(s,a))/\u03b2}] + E_{(s,a,s')\u223cd_D} [\u03b3V^\u2217(s) \u2212 B^\u2217Q(s, a) + r(s, a)] \u2212 1,$$,\nand we can further simplify the expression and obtain\n$$min E_{(s,a)\u223cd_D} [e^{(B^\u03c0Q(s,a)\u2212Q(s,a))/\u03b2}] \u2212 E_{(s,a,s')\u223cd_D} [B^\u2217Q(s, a) \u2212 Q(s,a)] \u2212 1$$"}, {"title": "B.2 Connection to Actor Critic", "content": "Recall that the dual function of the regularized RL objective with the change of variable has the\nfollowing form\n$$JD, f (\u03c0, Q) = min \u03b2\u00b7 E_{(s,a)\u223cd_D} [f^\u2217 ((B^\u2217Q(s, a) \u2212 Q(s, a))/\u03b2)] + (1 \u2212\u03b3) Es0\u223c\u03bc0,a0\u223c\u03c0\u2217 (s0) [Q (s0, a0)]$$"}, {"title": "B.3 Pearson x2 Divergence", "content": "A variety of f-divergence can be suitable candidates for the dual objective and prioritization deriva-\ntion. Here, we provide a list of f-divergences f(x), its corresponding convex conjugates f\u2217(y) and\nthe potential priority forms f'(y) in Table 3. We note that the forms presented are theoretical forms\nand they may vary when applying to the RL objectives.\nWe use Pearson x2 divergence as an example as the resulting objective has a particular implication.\nPearson x2 divergence has the form f(x) = (x \u2212 1)\u00b2 and its convex conjugate has the form f\u2217(y) =\ny\u00b2 + y. Again, let y := B\u2217Q(s, a) \u2212 Q(s, a). We can obtain the following dual objective:\n$$min \\frac{1}{2\u03b2}\u00b7E_{(s,a)\u223cd_D} [(B^\u2217Q(s, a) - Q(s, a))^2] +E_{(s,a)\u223cd_D} [B^\u2217Q(s, a) \u2013 Q(s,a)]+(1-^\u03b3)ES0\u223c\u03bc0 [V^\u2217 (s0)]$$"}, {"title": "C Experiments", "content": "In this section, we provide details of the implementation and the experiments with further discussion\non hyperparameter ablation and selection."}, {"title": "C.1 Experimental Details", "content": "Environment\nIn the online setting, our agents are evaluated in MuJoCo via OpenAI gym interface using the v2\nenvironments and DM Control tasks. For\nthe MuJoCo environment, we do not modify or preprocess the state space, action space and reward\nfunction for easy reproducibility. For the DM Control tasks, we adapt to the gym environment\ninterface. In the online with pretraining settings, our agents are evaluated in the environment with\nD4RL datasets. We shaped the reward of Antmaze by subtracting 1 as suggested in\nwhich shows to largely benefit the performance in Antmaze. Each environment\nruns for a maximum of 1000 time steps which is the default setting or until a termination state is\nreached.\nValue estimation\nValue estimates are averaged over mini-batches of 256 and sampled every 2000 iterations. The\ntrue value is estimated by sample 256 state-action pairs from the replay buffer and compute the\ndiscounted return by running the episode following the current policy until termination.\nReward Evaluation\nIn the online setting, we evaluate the current policy over 10 episodes for every 5000 training steps.\nThe evaluation reward takes the average over the 10 episodes. In the online with pretraining setting,\nwe evaluate the current policy over 100 episodes for every 10000 training steps due to the difficulty\nof the environments. The evaluation reward takes the average over the 100 episodes.\nAlgorithm implementation\nWe base our implementation of SAC off. It uses one target critic, double critics,\na single actor network and a single network for temperature adjustment for maximum entropy. We\nadd an additional value network with extreme q-learning loss for ROER TD error estimation and\npriority calculation. We use the default hyperparameters and network architectures for the SAC\nalgorithms for all of our experiments. The hyperparameters and the network architecture are shown\nin Table 4."}, {"title": "C.2 Hyper-parameter Selection", "content": "Online\nFor the value of parameter of PER, we use as a reference for MuJoCo envi-\nronment where they show that a weight scale \u03b1 = 0.4 works best for the set of tasks. As to DM\nControl, we search over the set [0.1, 0.2, 0.4, 0.6, 0.8] for individual task. The final choice of the value\nfor each task is shown in Table 5.\nFor the value of parameter large batch of LaBER, we search over the set [768, 1024, 1280, 1536] for\nindividual task in both MuJoCo and DM Control environment. The final choice of the value for\neach task is shown in Table 5.\nFor our proposed method ROER, we have 5 parameters, namely convergence rate (\u03bb), gumbel loss\nclip (Grad Clip), loss temperature (\u03b2), immediate weight clip (Max Exp Clip), and minimum priority\nclip (Min Clip) that require tuning. However, we discover that only \u03b2 and the clip range requires\ntuning for each specific environment while we can use a set of values for the rest. We use the set\nof value used in as a reference and search over the set [0.005, 0.01, 0.05] for \u03bb,\n[5, 7, 10] for the Grad Clip, [0.4, 1, 4] for \u03b2, [25, 50, 100] for the Max Exp Clip, and [1, 5, 10] for the Min\nPriority Clip. The final choice of the value for each task is shown in Table 5. We take HalfCheetaah-\nv2 from MuJoCo and Hopper-Stand from DM Control as examples to show the hyper-parammeter\nablations for the online experiments. We show the effect of each parameter by fixing the rest as\nthe default values. The default parameter values for HalfCheetah-v2 is \u03bb= 0.01, Grad Clip = 7, \u03b2 = 4, Max Exp Clip = 50, Min Clip = 10. The default parameter values for Hopper-Stand is\n\u03bb = 0.01, Grad Clip = 7, \u03b2 = 1, Max Exp Clip = 100, Min Clip = 1. The performance comparisons\nof the two task for varying each parameter are plotted in Fig. 3 and Fig. 4 respectively.\nAccording to Fig. 3a and Fig. 4a, \u03bb= 0.01 achieves the best performance for the two environments.\nWe note that a too big \u03bb can results in divergence as in Fig. 4a where \u03bb = 0.05 achieves bad\nperformance due to the too quick priority update which results in numerical instability. A small \u03bb\ngives stable convergence but too small \u03bb may slow down the convergence in some cases as in Fig. 4a.\nWe discover that generally \u03b2 = 0.01 works well across a wide domain and we use this value for all\nenvironments in our study."}, {"title": "D Additional Results", "content": "In this section, we present the additional results and discussions of our experiments. The learn-\ning curves in the online setting are shown in Fig. 6 for tasks in MuJoCo and Fig. 7 for tasks in\nDM Control. Besides the better performance, we note that our proposed ROER also shows faster\nimprovement in Ant-v2, HalfCheetah-v2, Hopper-v2, Humanoid-v2, Fish-Swim and Hopper-Stand.\nThis implies that ROER can obtain better data efficiency than the baselines. We additionally\nevaluate our proposed ROER in comparison to baselines in MuJoCo over 3 million steps to better\nillustrate the advantage of our proposed method as shown in Fig. 8. ROER shows consistently\nbetter performance as in the evaluation over 1 million steps and outperforms baselines in Ant-v2,\nHalfCheetah-v2, and Humanoid-v2 with very little or without overlapping shaded region. We did\nnot include LaBER for this comparison due to the long training time it takes. We note that the per-\nformance of ROER gets worse in Hopper-v2 for longer steps as it reaches the reward saturation very\nearly and the extra training can be harmful for the policy update due to over-fitting and additional\nupdates on Q-functions with weights that causes loss explosion."}]}