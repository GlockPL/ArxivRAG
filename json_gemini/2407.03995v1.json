[{"title": "B.1 Derivation Details of ROER", "authors": ["Changling Li", "Zhang-Wei Hong", "Pulkit Agrawal", "Divyansh Garg", "Joni Pajarinen"], "abstract": "Experience replay serves as a key component in the success of online reinforcement\nlearning (RL). Prioritized experience replay (PER) reweights experiences by the\ntemporal difference (TD) error empirically enhancing the performance. However,\nfew works have explored the motivation of using TD error. In this work, we provide\nan alternative perspective on TD-error-based reweighting. We show the connec-\ntions between the experience prioritization and occupancy optimization. By using\na regularized RL objective with f-divergence regularizer and employing its dual\nform, we show that an optimal solution to the objective is obtained by shifting\nthe distribution of off-policy data in the replay buffer towards the on-policy op-\ntimal distribution using TD-error-based occupancy ratios. Our derivation results\nin a new pipeline of TD error prioritization. We specifically explore the KL di-\nvergence as the regularizer and obtain a new form of prioritization scheme, the\nregularized optimal experience replay (ROER). We evaluate the proposed priori-\ntization scheme with the Soft Actor-Critic (SAC) algorithm in continuous control\nMuJoCo and DM Control benchmark tasks where our proposed scheme outper-\nforms baselines in 6 out of 11 tasks while the results of the rest match with or do\nnot deviate far from the baselines. Further, using pretraining, ROER achieves\nnoticeable improvement on difficult Antmaze environment where baselines fail,\nshowing applicability to offline-to-online fine-tuning. Code is available at https:\n//github.com/XavierChanglingLi/Regularized-Optimal-Experience-Replay.", "sections": [{"title": "1 Introduction", "content": "Deep reinforcement learning (RL) have shown wide applications in various domains (Mnih et al.,\n2015; Levine et al., 2016; Koert et al., 2019; Li et al., 2022; Hong et al., 2024). One key factor\nfor its success is the integrated structure of experience replay (Zhang & Sutton, 2017). Experience\nreplay (Lin, 1992) allows RL algorithms to use collected experience to compute updates for the\ncurrent policy. It significantly increases the data efficiency and allows RL to be applied to fields\nwhere online data collection is expensive. On the other hand, sampling from experience replay\nbuffer breaks the temporal correlations among experiences and stabilizes the gradient update (Mnih\net al., 2013). However, past work shows that not all samples are equally informative in updating\npolicy (Katharopoulos & Fleuret, 2018). To enhance the performance, techniques of weighted ex-\nperience replay (Schaul et al., 2015; Kumar et al., 2020a; Liu et al., 2021; Sinha et al., 2022) are\nproposed to perform importance sampling and shape the distribution of the data in the replay buffer.\nAmong the proposed reweighting frameworks, prioritized experience replay (PER) is most commonly\nutilized for its simplicity and empirically good performance (Hessel et al., 2018). PER attempts to\naccelerate learning by assigning experiences with the temporal-difference (TD) error to enable higher\nsampling frequency for transitions with high error. However, PER inherits several shortcomings.\nFirst, experience replay reuses experiences from the past iterations to update the current policy. The\nresulted distribution shift between the data distribution of the replay buffer and the distribution of\nthe current policy can cause incorrect TD error estimations which is detrimental to the performance\nof PER. On the other hand, it has been empirically shown that staying on policy (Schulman et al.,\n2015) or maintaining an on-policy sample distribution can be beneficial to the performance (Sutton\n& Barto, 2018; Fu et al., 2019; Novati & Koumoutsakos, 2019). Second, even though the motivation\nof using TD error is intuitive, limited works have explored the theoretical foundation (Fujimoto\net al., 2020; Lahire et al., 2021).\nIn this work, we revisit the prioritization scheme and attempt to tackle the aforementioned problems.\nWe provide a new perspective on the TD error prioritization by making connection to the occupancy\noptimization. We leverage the dual function of the regularized RL objective with f-divergence\nregularizer between off-policy and on-policy data distributions (Nachum et al., 2019b) and show\nthat an optimal solution (occupancy ratio) to the objective is obtained by shifting the off-policy\ndistribution towards the on-policy optimal distribution which results in a TD error prioritization.\nThe form of TD error prioritization is closely associated with the regularized objective which implies\nthat using simple TD error alone may not work best for every RL objectives. On the other hand,\nintroducing regularizer into the objective penalizes TD-error estimation when the distribution of the\ndata from the replay buffer differs too much from the distribution induced by the current policy and\nthus, gives a smaller priority to mitigate the bias induced by the distribution shift. Together, our\nderivation provides an alternative perspective on PER and results in a new pipeline of TD-error-\nbased prioritization scheme whose form depends on the choice of the regularizer. Similar to PER,\nthe new framework can be easily integrated with existing RL algorithms by using an additional value\nnetwork with the regularized objective.\nWe specifically focus on KL-divergence as a regularizer and derive its corresponding objective. From\nthis objective, we obtain a new form of prioritized experience replay, the regularized optimal expe-\nrience replay (ROER). We combine our proposed ROER with Soft Actor-Critic (SAC) (Haarnoja\net al., 2018) algorithm and evaluate on continuous control MuJoCo and DM control benchmark\ntasks. ROER outperforms baselines in 6 out of 11 tasks while the rest match with or do not deviate\nfar from the best performance. Especially, ROER shows performance improvements on environments\nwhere PER and LaBER (Lahire et al., 2021) fails. Further evaluation on the value estimations shows\nthat the performance improvement of ROER attributes to the more accurate value estimation by\nmitigating the underestimation bias of SAC with double critics (Li et al., 2021; Zhou et al., 2022) and\nthus ROER can obtain or converge to the optimal solutions much faster than baselines. Further, we\nconsider the setting of online with pretraining and ROER achieves noticeable improvement on dif-\nficult Antmaze environment whereas the baselines fail, showing the applicability to offline-to-online\nfine-tuning."}, {"title": "2 Preliminaries", "content": "Online RL. Online reinforcement learning concerns optimizing an agent's policy in a Markov deci-\nsion process (MDP) (Puterman, 2014). The MDP is defined by a tuple $\\mathcal{M}=(\\mathcal{S},\\mathcal{A},\\mathcal{P},r,\\rho_0,\\gamma)$ where\n$\\mathcal{S}$ and $\\mathcal{A}$ represent the state space and action space respectively, $\\mathcal{P}(s'|s,a)$ denotes the dynamic\nmodel, $r(s,a)$ the reward function, $\\rho_0$ the initial state distribution, and $\\gamma \\in (0, 1)$ the discount factor.\nThe agent's behavior is described by its policy $\\pi : \\mathcal{S} \\rightarrow \\Delta_\\mathcal{A}$. The performance of a given policy can\nbe measured by the state-action value function $Q^{\\pi}(s, a) = \\mathbb{E}[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)|s_0 = s, a_0 = a, s_{t+1} \\sim\n\\mathcal{P}(\\cdot|s_t, a_t), a_t \\sim \\pi(\\cdot|s_t)]$. The corresponding value function is $V^{\\pi}(s) := \\mathbb{E}[Q^{\\pi}(s,a)|a \\sim \\pi(\\cdot|s)]$. The\ngoal is to learn a policy that maximizes the $\\gamma$-discounted expected cumulative return (Sutton &\nBarto, 2018):\n$\\max_{\\pi} J_P(\\pi) := (1 - \\gamma)\\mathbb{E}_{s_0\\sim\\rho_0, a_0\\sim\\pi(\\cdot|s_0)} [Q^{\\pi}(s_0, a_0)]$"}, {"title": "3 Experience Prioritization as Occupancy Optimization", "content": "The goal of using prioritization is to accelerate learning and obtain an optimal policy which induces\nan optimal on-policy distribution. We reverse this process and motivate our formulation by the\nproblem of obtaining an optimal policy by finding the optimal on-policy distribution $d^*$, given access\nto an off-policy distribution $d^\\mathcal{D}$. Here, $d^*$ is unknown and we assume to only have samples from\n$d^\\mathcal{D}$ which is the distribution of the experience replay buffer. For an MDP with a reward function\nr, there exists a unique $d^*$. We consider the following regularized objective with an f-divergence\nregularizer to include $d^\\mathcal{D}$ (Nachum et al., 2019a)\n$\\max_{d^\\mathcal{D}} I_\\mathcal{D}, f(d^*, d) := \\mathbb{E}_{(s,a)\\sim d^*} [r(s, a)] - \\beta D_f(d^* \\\\ d)$\nwhere $\\beta > 0$ and $D_f$ denotes the f-divergence induced by a convex function f:\n$D_f(d^*\\\\d^\\mathcal{D}) = \\mathbb{E}_{(s,a)\\sim d^\\mathcal{D}}[f(\\frac{d^*(s,a)}{d(s,a)})]$"}, {"title": "4 Regularized Optimal Experience Replay", "content": "In this section, we discuss our choice of using Kullback-Leibler (KL) divergence as the regularizer\nand proceed to the practical implementation of the prioritization scheme with the KL-divergence\nregularizer which forms our proposed method, the regularized optimal experience replay (ROER).\nOther forms of f-divergence can also be suitable candidates and we provide further discussions in\nAppendix B."}, {"title": "4.1 KL Divergence as Regularizer", "content": "f-divergence consists of numerous forms and past works have explored the application of it in the\npolicy update rules of RL (Belousov & Peters, 2017; Kumar et al., 2020b). Particularly, many"}, {"title": "4.2 Practical Implementation", "content": "$\\mathbf{Algorithm~1}$ Actor Critic with Regularized Optimal Experience Replay\n$\\mathbf{1}$: Initialize $Q_{\\theta}, \\pi_{\\psi}$, value network $V_{\\phi}$, training start step $\\tau$\n$\\mathbf{2}$: Let $\\mathcal{D}$ be the empty replay buffer or filled with offline data with $d(s, a) = 1$\n$\\mathbf{3}$: for step $t$ in $1, ..., N$ do\n$\\mathbf{4}$: ~~~~Update $(s, a, r, s')$ to $\\mathcal{D}$ with $d(s, a) = 1$\n$\\mathbf{5}$: ~~~~if $t > \\tau$ then\n$\\mathbf{6}$: ~~~~~~~~Update $d(s, a)$ with $d'$ from Eq. 16\n$\\mathbf{7}$: ~~~~~~~~Train $Q_{\\theta}$ with $J(Q_{\\theta})$ from Eq. 5 using $d(s, a)$ as $w(s, a)$\n$\\mathbf{8}$: ~~~~~~~~Train $V_{\\phi}$ with $\\mathcal{L}(V)$ from Eq. 15\n$\\mathbf{9}$: ~~~~~~~~Update $\\pi$\n$\\mathbf{10}$: ~~~~end if\n$\\mathbf{11}$: end for\n$\\mathbf{12}$: return $Q^*, \\pi^*$\n\nNote that the form of occupancy ratio is derived from a regularized objective which can be different\nfrom the objective of the applied algorithm. For smooth integration to the existing algorithms,\nwe propose to incorporate a separate value network using the regularized objective for TD error\nestimation and priority calculation. The above KL divergence gives the value network the following\nobjective of the ExtremeV loss (Garg et al., 2023)\n$\\mathcal{L}(V) = \\mathbb{E}_{(s,a)~d^\\mathcal{D}} [e^{(B^*Q(s,a)-V(s))/\\beta}] - \\mathbb{E}_{(s,a,s')~d^\\mathcal{D}} [Q(s, a) - V(s)] - 1.$\n\nWe then use the TD error obtained from the value network to calculate the priority. Since the\ndistribution of $d^\\mathcal{D}$ is changing, we consider a stable convergence and solve the optimization problem\nin many steps. We introduce a convergence parameter $\\lambda$ and formulate the following priority update\nfunction\n$d' = [\\lambda e^{\\delta q^*/\\beta} + (1 - \\lambda)] \\cdot d^\\mathcal{D}$ with $\\lambda \\in (0,1]$."}, {"title": "5 Experimental Evaluation", "content": "We combine our proposed prioritization scheme ROER with Soft-Actor Critic (Haarnoja et al., 2018)\nalgorithm for evaluation. We compare our method with two state-of-art prioritization schemes\nnamely uniform experience replay (UER) and the initial TD error prioritized experience replay\n(PER) (Schaul et al., 2015), and one additional baseline namely large batch experience replay\n(LaBER) (Lahire et al., 2021) across a wide set of MuJoCo continuous control tasks interfaced\nthrough OpenAI Gym (Brockman et al., 2016) and DM Control tasks (Tunyasuvunakool et al.,\n2020) in an online setting. Additionally, we consider a suite of more difficult environment Antmaze\nwith pretraining using the data from D4RL (Fu et al., 2020) to show that ROER can achieve good\nperformance in settings where both UER and PER fail. To allow for reproducibility, we use the orig-\ninal set of tasks without modification to the environment or rewards. For a fair comparison between\nbaselines and our approach, our implementations are all based on JAXRL (Kostrikov, 2021).\nCompared to the initial PER, even though our proposed method ROER has four more hyperparam-\neters namely the architecture of value network, loss temperature ($\\beta$), Gumbel loss clip (Grad Clip),\nand maximum exponential of TD-error clip (Max Exp Clip), we note that $\\beta$ and Grad Clip are not\nnew and they come from the objective of Extreme Q-Learning. Grad Clip is shown to affect the\nresults lightly and the value network can use the default parameters as the critic network. A set of\nvalues works well for multiple environments. We provide more details of implementation, ablations\nand hyperparameters in Appendix C."}, {"title": "5.1 Online", "content": "In the online setting, the empirical results demonstrate that our proposed ROER outperforms state-\nof-the-arts on 6 out of 11 continuous control tasks in terms of the average evaluation while do not\ndeviate far from the baselines for the rest 3 environments as shown in Table 1. We find that ROER"}, {"title": "5.2 Value Estimation Analysis", "content": "The better performance can be empirically confirmed by the faster convergence to the true value.\nBesides the derivation that shows our proposed prioritization scheme results in the optimal solution,"}, {"title": "5.3 Online with Pretraining", "content": "Our proposed ROER prioritization scheme can benefit from pretraining using offline data and show\nsignificant performance improvement over more difficult environment Antmaze-Umaze and Antmaze-\nMedium as revealed in Table 2. We recognize that using the average performance over the last 200\nevaluations may not be the most suitable metric here due to the sparsity of rewards and the difficulty\nof the environments. Thus, we also include the learning curves to illustrate the results as in Fig. 2.\nWe found that SAC with ROER can obtain good performance at a very early stage in Antmaze-\nUmaze environment using both Antmaze-Umaze-v2 dataset and Antmaze-Umaze-diverse-v2 dataset.\nEspecially with Antmaze-Umaze-diverse-v2 dataset, SAC with ROER achieved a significantly better\nperformance compared to the state-of-art prioritization schemes while PER and LaBER are shown to\nbe detrimental to the learning process. We also note that for a more difficult environment Antmaze-\nMedium, our proposed method can obtain rewards at an early stage and shows improvement over\ntraining steps as in Fig. 2c and Fig. 2d. In contrast, SAC with UER, SAC with PER and SAC with\nLaBER completely fail to obtain any reward signal. This implication is crucial to improve training\nefficiency and safety by using offline data and correct the distribution to obtain a good performance\nonline. It shows the potential applicability of our method in offline-to-online finetuning."}, {"title": "6 Related Work", "content": "Our approach builds upon regularized RL objective and weighted experience replay.\nRegularized RL. Regularization is commonly utilized in offline reinforcement learning to constrain\nthe behavior policy and action selection (Kumar et al., 2020b; Wu et al., 2019; Kumar et al., 2019).\nOther works have considered regularized Q-function of the behavior policy (Shi et al., 2023) and\nstate-action value offset (Kostrikov et al., 2021). Adapting such regularizers in online setting can\nachieve more stable performance (Fujimoto et al., 2019; Schulman et al., 2015). Maximizing the\nregularizer as a way to encourage exploration also shows improvement in performance and forms\nthe framework of max entropy RL (Ziebart et al., 2008; Haarnoja et al., 2017; 2018). Our work\nbuilds upon the line of work that utilizes the dual function of the regularized objective (Belousov &\nPeters, 2017; Nachum & Dai, 2020; Nachum et al., 2019b) which allows to express the max-return\noptimization by an expectation over an arbitrary behavior-agnostic and off-policy data distribution.\nWe extend this approach and formulate the prioritization scheme that allows the data distribution\nin replay buffer gradually converge to the optimal distribution which gives the optimal Q-function.\nTheoretical analysis of the regularized RL shows that despite its non-convexity, this problem has\nzero duality gap and can be solved exactly in the dual domain (Geist et al., 2019; Neu et al., 2017;\nPaternain et al., 2019).\nWeighted experience replay. Experience replay is crucial to the success of deep RL for improv-\ning the data efficiency by using off-policy data (Lin, 1992; Hessel et al., 2018). Various frameworks\nhave been proposed to change the sampling strategy to achieve superior performance than uniform\nsampling. Prioritized experience replay (PER) weights the experiences by their TD errors and\nshows empirical improvement when applying to deep RL (Schaul et al., 2015; Fujimoto et al., 2020).\nHowever, few works have explored the theoretical motivation of using TD-error based reweighting\nscheme. Lahire et al. (2021) suggest that PER can be considered as an importance sampling scheme\nusing approximated per-sample gradient norms and prioritizing stochastic gradient descent variance\nreduction. Our work, on the other hand, uses dual function of the regularized RL objective to\nprovide an alternative perspective on TD-error-based prioritization. Other considerations of priori-\ntization scheme include loss value (Hessel et al., 2018), accuracy of the TD-error estimation (Sinha\net al., 2022), regret minimization (Liu et al., 2021), and leveraging neural network for experience\nselection (Zha et al., 2019). We note that Kumar et al. (2020a) shares similarity to our work in\ncorrecting the replay buffer towards optimal distribution. However, they consider optimizing cor-\nrective feedback while our work builds on dual function of regularized RL objective. Another work\nthat shares slight similarity with our method is ReF-ER (Novati & Koumoutsakos, 2019) where\nthey ignore the updates from experiences that deviates significantly from the current policy. Our\nwork focuses on penalizing the TD errors of the samples that deviate from the current policy which\nleads to smaller priority instead of completely ignoring those experiences. In addition, our proposed\nmethod forms a new pipeline of TD-error-based prioritization scheme."}, {"title": "7 Conclusion", "content": "By leveraging the regularized RL objective and its dual function, we propose a new pipeline of TD-\nerror-based prioritization scheme that is more robust towards distribution shift between off-policy\ndata and current policy. By considering KL-divergence as the reuglarizer, we formulated a new\nprioritized experience replay, namely regularized optimal experience replay (ROER). Our proposed\nROER when applied to SAC empirically demonstrates the ability of mitigating the underestimation\nbias and shows faster convergence to the true value. It outperforms baselines in 6 out of 11 continu-\nous control tasks in the online setting and significantly improves the performance in Antmaze with\npretraining. However, we recognize the tuning of additional hyperparameters can limit the appli-\ncation. Future work can explore an adaptive loss temperature to dynamically adjust the strength\nof the regularization. Additionally, it would be valuable to extend the application of the proposed\nmethod to offline setting and further explore the applicability to offline-to-online fine tuning."}, {"title": "A Derivation Details", "content": "In this section", "f(\\pi)": "mathbb{E"}, {"f": "n$D_f(d^*\\\\d) = \\mathbb{E"}, {"w_{\\mathcal{D}}": "frac{d^*(s", "x": "mathcal{S"}, "times\\mathcal{A} \\rightarrow \\mathbb{R}$\nthat is bounded which gives the following expressions\n$\\hat{I}_{\\mathcal{D},f} (\\pi, x) : = \\min_{x} \\mathbb{E}_{(s,a)\\sim d^*} [r(s, a)] + \\beta\\cdot\\mathbb{E}_{(s,a)\\sim d^\\mathcal{D}}[f^*(x(s,a))] - \\beta\\cdot \\mathbb{E}_{(s,a)\\sim d^*} [x(s, a)]$\\\n    },\n    {\n      \"title\": \"B Other divergence\",\n      \"content\":", "In this section, we firstly show the connection between the dual objective and the actor-critc ob-\njective. Then we give another consideration of regularizer which results in a different form of\nprioritization."], "content": "KL-divergence has the form $f(x) = xlog(x)$ and its convex conjugate has the form $f^*(y) = e^y - 1$.\nLet $y = (B^*Q(s, a) - Q(s, a))/\\beta$, we follow the derivation in section 3 and obtain the following dual\nobjective:\n$\\min_{Q} \\mathbb{E}_{(s,a)~d^\\mathcal{D}} [e^{(B^*Q(s,a)-Q(s,a)/\\beta}] + (1 -\\gamma) \\mathbb{E}_{s_0~\\mu_0} [V^* (s_0)] - 1$\nwhich can be expanded to:\n$\\min_{Q} \\mathbb{E}_{(s,a)~d^\\mathcal{D}} [e^{(B^*Q(s,a)-Q(s,a))/\\beta}] + \\mathbb{E}_{(s,a,s')~d^\\mathcal{D}} [V^*(s) - V^* (s')] - 1.$\nRecall that $\\gamma V^*(s') = B^*Q(s, a) - r(s, a)$. We substitute the expression of $\\gamma V^*(s')$ back to the above\nobjective and obtain\n$\\min_{Q} \\mathbb{E}_{(s,a)~d^\\mathcal{D}} [e^{(B^*Q(s,a)-Q(s,a))/\\beta}] + \\mathbb{E}_{(s,a,s')~d^\\mathcal{D}} [V^*(s) - B^*Q(s, a) + r(s, a)] - 1,$\nand we can further simplify the expression and obtain\n$\\min_{Q} \\mathbb{E}_{(s,a)~d^\\mathcal{D}} [e^{(B^*Q(s,a)-Q(s,a))/\\beta}] - \\mathbb{E}_{(s,a,s')~d^\\mathcal{D}} [B^*Q(s, a) - Q(s,a)] - 1$\nwhich completes the derivation. This objective corresponds to ExtremeQ loss as in Garg et al.\n(2023). It has a corresponding ExtremeV loss in the following form:\n$\\mathcal{L}(V) = \\mathbb{E}_{(s,a)~d^\\mathcal{D}} [e^{(B^*Q(s,a)-V(s)/\\beta}] - \\mathbb{E}_{(s,a,s')~d^\\mathcal{D}} [Q(s, a) - V(s)] - 1$\nwhich is the objective of our value network."}, {"title": "B.2 Connection to Actor Critic", "content": "Recall that the dual function of the regularized RL objective with the change of variable has the\nfollowing form\n$\\mathcal{J}_{\\mathcal{D}, \\xi} (\\pi, Q) = \\min_{Q} \\beta\\cdot\\mathbb{E}_{(s,a) \\sim d^\\mathcal{D}}[f^* ((B^*Q(s, a) - Q(s, a))/\\beta)] + (1 -\\gamma) \\mathbb{E}_{s_0 \\sim \\mu_0, a_0 \\sim \\pi^*(s_0)} [Q (s_0, a_0)]$\nWe consider a convex function of the form $f(x) = \\frac{1}{2}x^2$. Its convex conjugate has the same form as\nitself $f^*(y) = \\frac{1}{2}y^2$. Let $y := B^*Q(s, a) - Q(s, a)$. The above function can be expressed as below:\n$\\mathcal{J}_{\\mathcal{D},f}(\\pi, Q) = \\min_{Q} \\frac{1}{2 \\beta} \\cdot\\mathbb{E}_{(s,a) \\sim d^\\mathcal{D}} [(B^*Q(s, a) - Q(s,a))^2] + (1 -\\gamma) \\mathbb{E}_{s_0 \\sim \\mu_0, a_0 \\sim \\pi^*(s_0)} [Q (s_0, a_0)]$\nwhich transforms the off-policy actor-critic to an on-policy actor-critic by introducing the second\nterm. This unifies the two separate objectives of value and policy into a single objective and both\nfunctions are trained with respect to the same off-policy objective (Nachum et al., 2019b)."}, {"title": "B.3 Pearson $\\chi^2$ Divergence", "content": "A variety of f-divergence can be suitable candidates for the dual objective and prioritization deriva-\ntion. Here, we provide a list of f-divergences f(x), its corresponding convex conjugates f*(y) and\nthe potential priority forms f'(y) in Table 3. We note that the forms presented are theoretical forms\nand they may vary when applying to the RL objectives.\nWe use Pearson $\\chi^2$ divergence as an example as the resulting objective has a particular implication.\nPearson $\\chi^2$ divergence has the form $f(x) = \\frac{1}{2} (x - 1)^2$ and its convex conjugate has the form $f^*(y) =$\n$\\frac{1}{2} y^2 + y$. Again, let $y := B^*Q(s, a) - Q(s, a)$. We can obtain the following dual objective:\n$\\min_{Q} \\frac{1}{2\\beta} \\cdot\\mathbb{E}_{(s,a) \\sim d^\\mathcal{D}} [(B^*Q(s, a) - Q(s, a))^2] +\\mathbb{E}_{(s,a) \\sim d^\\mathcal{D}} [B^*Q(s, a) - Q(s, a)]+(1-\\gamma) \\mathbb{E}_{s_0 \\sim \\mu_0} [V^* (s_0)]$\nUsing $V^*(s') + r(s,a) = B^*Q(s, a)$, the objective can be further simplified to:\n$\\min_{Q} \\frac{1}{2\\beta} \\cdot\\mathbb{E}_{(s,a) \\sim d^\\mathcal{D}} [(B^*Q(s, a) - Q(s, a))^2] +\\mathbb{E}_{(s,a) \\sim d^\\mathcal{D}} [V^*(s) - Q(s, a)]$\nWe note that this corresponds to the learning objective of conservative Q-learning (Kumar et al.\n2020b). This objective is optimized when $\\frac{d^*}{d^\\mathcal{D}} = f'(\\frac{\\delta_q}{\\beta}) = \\frac{\\delta_q}{\\beta} + 1$ which gives a new form of\npriority calculation. We address that even though this form is almost identical to PER, the source\nof $d$ is different. We derive this priority form from the conservative Q-learning objective and thus, it\nrequires the value network to use the corresponding loss function. In addition, the loss temperature\n$\\beta$ here also controls the scale of the TD error and the strength of the regularizer which we expect\nto give better performance than the naive PER."}, {"title": "C Experiments", "content": "In this section, we provide details of the implementation and the experiments with further discussion\non hyperparameter ablation and selection."}, {"title": "C.1 Experimental Details", "content": "Environment\nIn the online setting, our agents are evaluated in MuJoCo via OpenAI gym interface using the v2\nenvironments (Brockman et al., 2016) and DM Control tasks (Tunyasuvunakool et al., 2020). For\nthe MuJoCo environment, we do not modify or preprocess the state space, action space and reward\nfunction for easy reproducibility. For the DM Control tasks, we adapt to the gym environment\ninterface. In the online with pretraining settings, our agents are evaluated in the environment with\nD4RL datasets (Fu et al., 2020). We shaped the reward of Antmaze by subtracting 1 as suggested in\nKumar et al. (2020b) which shows to largely benefit the performance in Antmaze. Each environment\nruns for a maximum of 1000 time steps which is the default setting or until a termination state is\nreached.\nValue estimation\nValue estimates are averaged over mini-batches of 256 and sampled every 2000 iterations. The\ntrue value is estimated by sample 256 state-action pairs from the replay buffer and compute the\ndiscounted return by running the episode following the current policy until termination.\nReward Evaluation\nIn the online setting, we evaluate the current policy over 10 episodes for every 5000 training steps.\nThe evaluation reward takes the average over the 10 episodes. In the online with pretraining setting,\nwe evaluate the current policy over 100 episodes for every 10000 training steps due to the difficulty\nof the environments. The evaluation reward takes the average over the 100 episodes.\nAlgorithm implementation\nWe base our implementation of SAC off Kostrikov (2021). It uses one target critic, double critics,\na single actor network and a single network for temperature adjustment for maximum entropy. We\nadd an additional value network with extreme q-learning loss for ROER TD error estimation and\npriority calculation. We use the default hyperparameters and network architectures for the SAC\nalgorithms for all of our experiments. The hyperparameters and the network architecture are shown\nin Table 4."}, {"title": "B.1 Derivation Details of ROER", "content": "KL-divergence has the form $f(x) = xlog(x)$ and its convex conjugate has the form $f^*(y) = e^y - 1$.\nLet $y = (B^*Q(s, a) - Q(s, a))/\\beta$, we follow the derivation in section 3 and obtain the following dual\nobjective:\n$\\min_{Q} \\mathbb{E}_{(s,a)~d^\\mathcal{D}} [e^{(B^*Q(s,a)-Q(s,a)/\\beta}] + (1 -\\gamma) \\mathbb{E}_{s_0~\\mu_0} [V^* (s_0)] - 1$\nwhich can be expanded to:\n$\\min_{Q} \\mathbb{E}_{(s,a)~d^\\mathcal{D}} [e^{(B^*Q(s,a)-Q(s,a))/\\beta}] + \\mathbb{E}_{(s,a,s')~d^\\mathcal{D}} [V^*(s) - V^* (s')] - 1.$\nRecall that $\\gamma V^*(s') = B^*Q(s, a) - r(s, a)$. We substitute the expression of $\\gamma V^*(s')$ back to the above\nobjective and obtain\n$\\min_{Q} \\mathbb{E}_{(s,a)~d^\\mathcal{D}} [e^{(B^*Q(s,a)-Q(s,a))/\\beta}] + \\mathbb{E}_{(s,a,s')~d^\\mathcal{D}} [V^*(s) - B^*Q(s, a) + r(s, a)] - 1,$\nand we can further simplify the expression and obtain\n$\\min_{Q} \\mathbb{E}_{(s,a)~d^\\mathcal{D}} [e^{(B^*Q(s,a)-Q(s,a))/\\beta}] - \\mathbb{E}_{(s,a,s')~d^\\mathcal{D}} [B^*Q(s, a) - Q(s,a)] - 1$\nwhich completes the derivation. This objective corresponds to ExtremeQ loss as in Garg et al.\n(2023). It has a corresponding ExtremeV loss in the following form:\n$\\mathcal{L}(V) = \\mathbb{E}_{(s,a)~d^\\mathcal{D}} [e^{(B^*Q(s,a)-V(s)/\\beta}] - \\mathbb{E}_{(s,a,s')~d^\\mathcal{D}} [Q(s, a) - V(s)] - 1$\nwhich is the objective of our value network.", "authors": ["Changling Li", "Zhang-Wei Hong", "Pulkit Agrawal", "Divyansh Garg", "Joni Pajarinen"], "abstract": "Experience replay serves as a key component in the success of online reinforcement\nlearning (RL). Prioritized experience replay (PER) reweights experiences by the\ntemporal difference (TD) error empirically enhancing the performance. However,\nfew works have explored the motivation of using TD error. In this work, we provide\nan alternative perspective on TD-error-based reweighting. We show the connec-\ntions between the experience prioritization and occupancy optimization. By using\na regularized RL objective with f-divergence regularizer and employing its dual\nform, we show that an optimal solution to the objective is obtained by shifting\nthe distribution of off-policy data in the replay buffer towards the on-policy op-\ntimal distribution using TD-error-based occupancy ratios. Our derivation results\nin a new pipeline of TD error prioritization. We specifically explore the KL di-\nvergence as the regularizer and obtain a new form of prioritization scheme, the\nregularized optimal experience replay (ROER). We evaluate the proposed priori-\ntization scheme with the Soft Actor-Critic (SAC) algorithm in continuous control\nMuJoCo and DM Control benchmark tasks where our proposed scheme outper-\nforms baselines in 6 out of 11 tasks while the results of the rest match with or do\nnot deviate far from the baselines. Further, using pretraining, ROER achieves\nnoticeable improvement on difficult Antmaze environment where baselines fail,\nshowing applicability to offline-to-online fine-tuning. Code is available at https:\n//github.com/XavierChanglingLi/Regularized-Optimal-Experience-Replay.", "sections": [{"title": "1 Introduction", "content": "Deep reinforcement learning (RL) have shown wide applications in various domains (Mnih et al.,\n2015; Levine et al., 2016; Koert et al., 2019; Li et al., 2022; Hong et al., 2024). One key factor\nfor its success is the integrated structure of experience replay (Zhang & Sutton, 2017). Experience\nreplay (Lin, 1992) allows RL algorithms to use collected experience to compute updates for the\ncurrent policy. It significantly increases the data efficiency and allows RL to be applied to fields\nwhere online data collection is expensive. On the other hand, sampling from experience replay\nbuffer breaks the temporal correlations among experiences and stabilizes the gradient update (Mnih\net al., 2013). However, past work shows that not all samples are equally informative in updating\npolicy (Katharopoulos & Fleuret, 2018). To enhance the performance, techniques of weighted ex-\nperience replay (Schaul et al., 2015; Kumar et al., 2020a; Liu et al., 2021; Sinha et al., 2022) are\nproposed to perform importance sampling and shape the distribution of the data in the replay buffer.\nAmong the proposed reweighting frameworks, prioritized experience replay (PER) is most commonly\nutilized for its simplicity and empirically good performance (Hessel et al., 2018). PER attempts to\naccelerate learning by assigning experiences with the temporal-difference (TD) error to enable higher\nsampling frequency for transitions with high error. However, PER inherits several shortcomings.\nFirst, experience replay reuses experiences from the past iterations to update the current policy. The\nresulted distribution shift between the data distribution of the replay buffer and the distribution of\nthe current policy can cause incorrect TD error estimations which is detrimental to the performance\nof PER. On the other hand, it has been empirically shown that staying on policy (Schulman et al.,\n2015) or maintaining an on-policy sample distribution can be beneficial to the performance (Sutton\n& Barto, 2018; Fu et al., 2019; Novati & Koumoutsakos, 2019). Second, even though the motivation\nof using TD error is intuitive, limited works have explored the theoretical foundation (Fujimoto\net al., 2020; Lahire et al., 2021).\nIn this work, we revisit the prioritization scheme and attempt to tackle the aforementioned problems.\nWe provide a new perspective on the TD error prioritization by making connection to the occupancy\noptimization. We leverage the dual function of the regularized RL objective with f-divergence\nregularizer between off-policy and on-policy data distributions (Nachum et al., 2019b) and show\nthat an optimal solution (occupancy ratio) to the objective is obtained by shifting the off-policy\ndistribution towards the on-policy optimal distribution which results in a TD error prioritization.\nThe form of TD error prioritization is closely associated with the regularized objective which implies\nthat using simple TD error alone may not work best for every RL objectives. On the other hand,\nintroducing regularizer into the objective penalizes TD-error estimation when the distribution of the\ndata from the replay buffer differs too much from the distribution induced by the current policy and\nthus, gives a smaller priority to mitigate the bias induced by the distribution shift. Together, our\nderivation provides an alternative perspective on PER and results in a new pipeline of TD-error-\nbased prioritization scheme whose form depends on the choice of the regularizer. Similar to PER,\nthe new framework can be easily integrated with existing RL algorithms by using an additional value\nnetwork with the regularized objective.\nWe specifically focus on KL-divergence as a regularizer and derive its corresponding objective. From\nthis objective, we obtain a new form of prioritized experience replay, the regularized optimal expe-\nrience replay (ROER). We combine our proposed ROER with Soft Actor-Critic (SAC) (Haarnoja\net al., 2018) algorithm and evaluate on continuous control MuJoCo and DM control benchmark\ntasks. ROER outperforms baselines in 6 out of 11 tasks while the rest match with or do not deviate\nfar from the best performance. Especially, ROER shows performance improvements on environments\nwhere PER and LaBER (Lahire et al., 2021) fails. Further evaluation on the value estimations shows\nthat the performance improvement of ROER attributes to the more accurate value estimation by\nmitigating the underestimation bias of SAC with double critics (Li et al., 2021; Zhou et al., 2022) and\nthus ROER can obtain or converge to the optimal solutions much faster than baselines. Further, we\nconsider the setting of online with pretraining and ROER achieves noticeable improvement on dif-\nficult Antmaze environment whereas the baselines fail, showing the applicability to offline-to-online\nfine-tuning."}, {"title": "2 Preliminaries", "content": "Online RL. Online reinforcement learning concerns optimizing an agent's policy in a Markov deci-\nsion process (MDP) (Puterman, 2014). The MDP is defined by a tuple $\\mathcal{M}=(\\mathcal{S},\\mathcal{A},\\mathcal{P},r,\\rho_0,\\gamma)$ where\n$\\mathcal{S}$ and $\\mathcal{A}$ represent the state space and action space respectively, $\\mathcal{P}(s'|s,a)$ denotes the dynamic\nmodel, $r(s,a)$ the reward function, $\\rho_0$ the initial state distribution, and $\\gamma \\in (0, 1)$ the discount factor.\nThe agent's behavior is described by its policy $\\pi : \\mathcal{S} \\rightarrow \\Delta_\\mathcal{A}$. The performance of a given policy can\nbe measured by the state-action value function $Q^{\\pi}(s, a) = \\mathbb{E}[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)|s_0 = s, a_0 = a, s_{t+1} \\sim\n\\mathcal{P}(\\cdot|s_t, a_t), a_t \\sim \\pi(\\cdot|s_t)]$. The corresponding value function is $V^{\\pi}(s) := \\mathbb{E}[Q^{\\pi}(s,a)|a \\sim \\pi(\\cdot|s)]$. The\ngoal is to learn a policy that maximizes the $\\gamma$-discounted expected cumulative return (Sutton &\nBarto, 2018):\n$\\max_{\\pi} J_P(\\pi) := (1 - \\gamma)\\mathbb{E}_{s_0\\sim\\rho_0, a_0\\sim\\pi(\\cdot|s_0)} [Q^{\\pi}(s_0, a_0)]$"}, {"title": "3 Experience Prioritization as Occupancy Optimization", "content": "The goal of using prioritization is to accelerate learning and obtain an optimal policy which induces\nan optimal on-policy distribution. We reverse this process and motivate our formulation by the\nproblem of obtaining an optimal policy by finding the optimal on-policy distribution $d^*$, given access\nto an off-policy distribution $d^\\mathcal{D}$. Here, $d^*$ is unknown and we assume to only have samples from\n$d^\\mathcal{D}$ which is the distribution of the experience replay buffer. For an MDP with a reward function\nr, there exists a unique $d^*$. We consider the following regularized objective with an f-divergence\nregularizer to include $d^\\mathcal{D}$ (Nachum et al., 2019a)\n$\\max_{d^\\mathcal{D}} I_\\mathcal{D}, f(d^*, d) := \\mathbb{E}_{(s,a)\\sim d^*} [r(s, a)] - \\beta D_f(d^* \\\\ d)$\nwhere $\\beta > 0$ and $D_f$ denotes the f-divergence induced by a convex function f:\n$D_f(d^*\\\\d^\\mathcal{D}) = \\mathbb{E}_{(s,a)\\sim d^\\mathcal{D}}[f(\\frac{d^*(s,a)}{d(s,a)})]$"}, {"title": "4 Regularized Optimal Experience Replay", "content": "In this section, we discuss our choice of using Kullback-Leibler (KL) divergence as the regularizer\nand proceed to the practical implementation of the prioritization scheme with the KL-divergence\nregularizer which forms our proposed method, the regularized optimal experience replay (ROER).\nOther forms of f-divergence can also be suitable candidates and we provide further discussions in\nAppendix B."}, {"title": "4.1 KL Divergence as Regularizer", "content": "f-divergence consists of numerous forms and past works have explored the application of it in the\npolicy update rules of RL (Belousov & Peters, 2017; Kumar et al., 2020b). Particularly, many"}, {"title": "4.2 Practical Implementation", "content": "$\\mathbf{Algorithm~1}$ Actor Critic with Regularized Optimal Experience Replay\n$\\mathbf{1}$: Initialize $Q_{\\theta}, \\pi_{\\psi}$, value network $V_{\\phi}$, training start step $\\tau$\n$\\mathbf{2}$: Let $\\mathcal{D}$ be the empty replay buffer or filled with offline data with $d(s, a) = 1$\n$\\mathbf{3}$: for step $t$ in $1, ..., N$ do\n$\\mathbf{4}$: ~~~~Update $(s, a, r, s')$ to $\\mathcal{D}$ with $d(s, a) = 1$\n$\\mathbf{5}$: ~~~~if $t > \\tau$ then\n$\\mathbf{6}$: ~~~~~~~~Update $d(s, a)$ with $d'$ from Eq. 16\n$\\mathbf{7}$: ~~~~~~~~Train $Q_{\\theta}$ with $J(Q_{\\theta})$ from Eq. 5 using $d(s, a)$ as $w(s, a)$\n$\\mathbf{8}$: ~~~~~~~~Train $V_{\\phi}$ with $\\mathcal{L}(V)$ from Eq. 15\n$\\mathbf{9}$: ~~~~~~~~Update $\\pi$\n$\\mathbf{10}$: ~~~~end if\n$\\mathbf{11}$: end for\n$\\mathbf{12}$: return $Q^*, \\pi^*$\n\nNote that the form of occupancy ratio is derived from a regularized objective which can be different\nfrom the objective of the applied algorithm. For smooth integration to the existing algorithms,\nwe propose to incorporate a separate value network using the regularized objective for TD error\nestimation and priority calculation. The above KL divergence gives the value network the following\nobjective of the ExtremeV loss (Garg et al., 2023)\n$\\mathcal{L}(V) = \\mathbb{E}_{(s,a)~d^\\mathcal{D}} [e^{(B^*Q(s,a)-V(s))/\\beta}] - \\mathbb{E}_{(s,a,s')~d^\\mathcal{D}} [Q(s, a) - V(s)] - 1.$\n\nWe then use the TD error obtained from the value network to calculate the priority. Since the\ndistribution of $d^\\mathcal{D}$ is changing, we consider a stable convergence and solve the optimization problem\nin many steps. We introduce a convergence parameter $\\lambda$ and formulate the following priority update\nfunction\n$d' = [\\lambda e^{\\delta q^*/\\beta} + (1 - \\lambda)] \\cdot d^\\mathcal{D}$ with $\\lambda \\in (0,1]$."}, {"title": "5 Experimental Evaluation", "content": "We combine our proposed prioritization scheme ROER with Soft-Actor Critic (Haarnoja et al., 2018)\nalgorithm for evaluation. We compare our method with two state-of-art prioritization schemes\nnamely uniform experience replay (UER) and the initial TD error prioritized experience replay\n(PER) (Schaul et al., 2015), and one additional baseline namely large batch experience replay\n(LaBER) (Lahire et al., 2021) across a wide set of MuJoCo continuous control tasks interfaced\nthrough OpenAI Gym (Brockman et al., 2016) and DM Control tasks (Tunyasuvunakool et al.,\n2020) in an online setting. Additionally, we consider a suite of more difficult environment Antmaze\nwith pretraining using the data from D4RL (Fu et al., 2020) to show that ROER can achieve good\nperformance in settings where both UER and PER fail. To allow for reproducibility, we use the orig-\ninal set of tasks without modification to the environment or rewards. For a fair comparison between\nbaselines and our approach, our implementations are all based on JAXRL (Kostrikov, 2021).\nCompared to the initial PER, even though our proposed method ROER has four more hyperparam-\neters namely the architecture of value network, loss temperature ($\\beta$), Gumbel loss clip (Grad Clip),\nand maximum exponential of TD-error clip (Max Exp Clip), we note that $\\beta$ and Grad Clip are not\nnew and they come from the objective of Extreme Q-Learning. Grad Clip is shown to affect the\nresults lightly and the value network can use the default parameters as the critic network. A set of\nvalues works well for multiple environments. We provide more details of implementation, ablations\nand hyperparameters in Appendix C."}, {"title": "5.1 Online", "content": "In the online setting, the empirical results demonstrate that our proposed ROER outperforms state-\nof-the-arts on 6 out of 11 continuous control tasks in terms of the average evaluation while do not\ndeviate far from the baselines for the rest 3 environments as shown in Table 1. We find that ROER"}, {"title": "5.2 Value Estimation Analysis", "content": "The better performance can be empirically confirmed by the faster convergence to the true value.\nBesides the derivation that shows our proposed prioritization scheme results in the optimal solution,"}, {"title": "5.3 Online with Pretraining", "content": "Our proposed ROER prioritization scheme can benefit from pretraining using offline data and show\nsignificant performance improvement over more difficult environment Antmaze-Umaze and Antmaze-\nMedium as revealed in Table 2. We recognize that using the average performance over the last 200\nevaluations may not be the most suitable metric here due to the sparsity of rewards and the difficulty\nof the environments. Thus, we also include the learning curves to illustrate the results as in Fig. 2.\nWe found that SAC with ROER can obtain good performance at a very early stage in Antmaze-\nUmaze environment using both Antmaze-Umaze-v2 dataset and Antmaze-Umaze-diverse-v2 dataset.\nEspecially with Antmaze-Umaze-diverse-v2 dataset, SAC with ROER achieved a significantly better\nperformance compared to the state-of-art prioritization schemes while PER and LaBER are shown to\nbe detrimental to the learning process. We also note that for a more difficult environment Antmaze-\nMedium, our proposed method can obtain rewards at an early stage and shows improvement over\ntraining steps as in Fig. 2c and Fig. 2d. In contrast, SAC with UER, SAC with PER and SAC with\nLaBER completely fail to obtain any reward signal. This implication is crucial to improve training\nefficiency and safety by using offline data and correct the distribution to obtain a good performance\nonline. It shows the potential applicability of our method in offline-to-online finetuning."}, {"title": "6 Related Work", "content": "Our approach builds upon regularized RL objective and weighted experience replay.\nRegularized RL. Regularization is commonly utilized in offline reinforcement learning to constrain\nthe behavior policy and action selection (Kumar et al., 2020b; Wu et al., 2019; Kumar et al., 2019).\nOther works have considered regularized Q-function of the behavior policy (Shi et al., 2023) and\nstate-action value offset (Kostrikov et al., 2021). Adapting such regularizers in online setting can\nachieve more stable performance (Fujimoto et al., 2019; Schulman et al., 2015). Maximizing the\nregularizer as a way to encourage exploration also shows improvement in performance and forms\nthe framework of max entropy RL (Ziebart et al., 2008; Haarnoja et al., 2017; 2018). Our work\nbuilds upon the line of work that utilizes the dual function of the regularized objective (Belousov &\nPeters, 2017; Nachum & Dai, 2020; Nachum et al., 2019b) which allows to express the max-return\noptimization by an expectation over an arbitrary behavior-agnostic and off-policy data distribution.\nWe extend this approach and formulate the prioritization scheme that allows the data distribution\nin replay buffer gradually converge to the optimal distribution which gives the optimal Q-function.\nTheoretical analysis of the regularized RL shows that despite its non-convexity, this problem has\nzero duality gap and can be solved exactly in the dual domain (Geist et al., 2019; Neu et al., 2017;\nPaternain et al., 2019).\nWeighted experience replay. Experience replay is crucial to the success of deep RL for improv-\ning the data efficiency by using off-policy data (Lin, 1992; Hessel et al., 2018). Various frameworks\nhave been proposed to change the sampling strategy to achieve superior performance than uniform\nsampling. Prioritized experience replay (PER) weights the experiences by their TD errors and\nshows empirical improvement when applying to deep RL (Schaul et al., 2015; Fujimoto et al., 2020).\nHowever, few works have explored the theoretical motivation of using TD-error based reweighting\nscheme. Lahire et al. (2021) suggest that PER can be considered as an importance sampling scheme\nusing approximated per-sample gradient norms and prioritizing stochastic gradient descent variance\nreduction. Our work, on the other hand, uses dual function of the regularized RL objective to\nprovide an alternative perspective on TD-error-based prioritization. Other considerations of priori-\ntization scheme include loss value (Hessel et al., 2018), accuracy of the TD-error estimation (Sinha\net al., 2022), regret minimization (Liu et al., 2021), and leveraging neural network for experience\nselection (Zha et al., 2019). We note that Kumar et al. (2020a) shares similarity to our work in\ncorrecting the replay buffer towards optimal distribution. However, they consider optimizing cor-\nrective feedback while our work builds on dual function of regularized RL objective. Another work\nthat shares slight similarity with our method is ReF-ER (Novati & Koumoutsakos, 2019) where\nthey ignore the updates from experiences that deviates significantly from the current policy. Our\nwork focuses on penalizing the TD errors of the samples that deviate from the current policy which\nleads to smaller priority instead of completely ignoring those experiences. In addition, our proposed\nmethod forms a new pipeline of TD-error-based prioritization scheme."}, {"title": "7 Conclusion", "content": "By leveraging the regularized RL objective and its dual function, we propose a new pipeline of TD-\nerror-based prioritization scheme that is more robust towards distribution shift between off-policy\ndata and current policy. By considering KL-divergence as the reuglarizer, we formulated a new\nprioritized experience replay, namely regularized optimal experience replay (ROER). Our proposed\nROER when applied to SAC empirically demonstrates the ability of mitigating the underestimation\nbias and shows faster convergence to the true value. It outperforms baselines in 6 out of 11 continu-\nous control tasks in the online setting and significantly improves the performance in Antmaze with\npretraining. However, we recognize the tuning of additional hyperparameters can limit the appli-\ncation. Future work can explore an adaptive loss temperature to dynamically adjust the strength\nof the regularization. Additionally, it would be valuable to extend the application of the proposed\nmethod to offline setting and further explore the applicability to offline-to-online fine tuning."}, {"title": "A Derivation Details", "content": "In this section", "f(\\pi)": "mathbb{E"}, {"f": "n$D_f(d^*\\\\d) = \\mathbb{E"}, {"w_{\\mathcal{D}}": "frac{d^*(s", "x": "mathcal{S"}, "times\\mathcal{A} \\rightarrow \\mathbb{R}$\nthat is bounded which gives the following expressions\n$\\hat{I}_{\\mathcal{D},f} (\\pi, x) : = \\min_{x} \\mathbb{E}_{(s,a)\\sim d^*} [r(s, a)] + \\beta\\cdot\\mathbb{E}_{(s,a)\\sim d^\\mathcal{D}}[f^*(x(s,a))] - \\beta\\cdot \\mathbb{E}_{(s,a)\\sim d^*} [x(s, a)]$\\\n        },\n        {\n            \"title\": \"B Other divergence\",\n            \"content\":", "In this section, we firstly show the connection between the dual objective and the actor-critc ob-\njective. Then we give another consideration of regularizer which results in a different form of\nprioritization."]}, {"title": "B.2 Connection to Actor Critic", "content": "Recall that the dual function of the regularized RL objective with the change of variable has the\nfollowing form\n$\\mathcal{J}_{\\mathcal{D}, \\xi} (\\pi, Q) = \\min_{Q} \\beta\\cdot\\mathbb{E}_{(s,a) \\sim d^\\mathcal{D}}[f^* ((B^*Q(s, a) - Q(s, a))/\\beta)] + (1 -\\gamma) \\mathbb{E}_{s_0 \\sim \\mu_0, a_0 \\sim \\pi^*(s_0)} [Q (s_0, a_0)]$\nWe consider a convex function of the form $f(x) = \\frac{1}{2}x^2$. Its convex conjugate has the same form as\nitself $f^*(y) = \\frac{1}{2}y^2$. Let $y := B^*Q(s, a) - Q(s, a)$. The above function can be expressed as below:\n$\\mathcal{J}_{\\mathcal{D},f}(\\pi, Q) = \\min_{Q} \\frac{1}{2 \\beta} \\cdot\\mathbb{E}_{(s,a) \\sim d^\\mathcal{D}} [(B^*Q(s, a) - Q(s,a))^2] + (1 -\\gamma) \\mathbb{E}_{s_0 \\sim \\mu_0, a_0 \\sim \\pi^*(s_0)} [Q (s_0, a_0)]$\nwhich transforms the off-policy actor-critic to an on-policy actor-critic by introducing the second\nterm. This unifies the two separate objectives of value and policy into a single objective and both\nfunctions are trained with respect to the same off-policy objective (Nachum et al., 2019b)."}, {"title": "B.3 Pearson $\\chi^2$ Divergence", "content": "A variety of f-divergence can be suitable candidates for the dual objective and prioritization deriva-\ntion. Here, we provide a list of f-divergences f(x), its corresponding convex conjugates f*(y) and\nthe potential priority forms f'(y) in Table 3. We note that the forms presented are theoretical forms\nand they may vary when applying to the RL objectives.\nWe use Pearson $\\chi^2$ divergence as an example as the resulting objective has a particular implication.\nPearson $\\chi^2$ divergence has the form $f(x) = \\frac{1}{2} (x - 1)^2$ and its convex conjugate has the form $f^*(y) =$\n$\\frac{1}{2} y^2 + y$. Again, let $y := B^*Q(s, a) - Q(s, a)$. We can obtain the following dual objective:\n$\\min_{Q} \\frac{1}{2\\beta} \\cdot\\mathbb{E}_{(s,a) \\sim d^\\mathcal{D}} [(B^*Q(s, a) - Q(s, a))^2] +\\mathbb{E}_{(s,a) \\sim d^\\mathcal{D}} [B^*Q(s, a) - Q(s, a)]+(1-\\gamma) \\mathbb{E}_{s_0 \\sim \\mu_0} [V^* (s_0)]$\nUsing $V^*(s') + r(s,a) = B^*Q(s, a)$, the objective can be further simplified to:\n$\\min_{Q} \\frac{1}{2\\beta} \\cdot\\mathbb{E}_{(s,a) \\sim d^\\mathcal{D}} [(B^*Q(s, a) - Q(s, a))^2] +\\mathbb{E}_{(s,a) \\sim d^\\mathcal{D}} [V^*(s) - Q(s, a)]$\nWe note that this corresponds to the learning objective of conservative Q-learning (Kumar et al.\n2020b). This objective is optimized when $\\frac{d^*}{d^\\mathcal{D}} = f'(\\frac{\\delta_q}{\\beta}) = \\frac{\\delta_q}{\\beta} + 1$ which gives a new form of\npriority calculation. We address that even though this form is almost identical to PER, the source\nof $d$ is different. We derive this priority form from the conservative Q-learning objective and thus, it\nrequires the value network to use the corresponding loss function. In addition, the loss temperature\n$\\beta$ here also controls the scale of the TD error and the strength of the regularizer which we expect\nto give better performance than the naive PER."}, {"title": "C Experiments", "content": "In this section, we provide details of the implementation and the experiments with further discussion\non hyperparameter ablation and selection."}, {"title": "C.1 Experimental Details", "content": "Environment\nIn the online setting, our agents are evaluated in MuJoCo via OpenAI gym interface using the v2\nenvironments (Brockman et al., 2016) and DM Control tasks (Tunyasuvunakool et al., 2020). For\nthe MuJoCo environment, we do not modify or preprocess the state space, action space and reward\nfunction for easy reproducibility. For the DM Control tasks, we adapt to the gym environment\ninterface. In the online with pretraining settings, our agents are evaluated in the environment with\nD4RL datasets (Fu et al., 2020). We shaped the reward of Antmaze by subtracting 1 as suggested in\nKumar et al. (2020b) which shows to largely benefit the performance in Antmaze. Each environment\nruns for a maximum of 1000 time steps which is the default setting or until a termination state is\nreached.\nValue estimation\nValue estimates are averaged over mini-batches of 256 and sampled every 2000 iterations. The\ntrue value is estimated by sample 256 state-action pairs from the replay buffer and compute the\ndiscounted return by running the episode following the current policy until termination.\nReward Evaluation\nIn the online setting, we evaluate the current policy over 10 episodes for every 5000 training steps.\nThe evaluation reward takes the average over the 10 episodes. In the online with pretraining setting,\nwe evaluate the current policy over 100 episodes for every 10000 training steps due to the difficulty\nof the environments. The evaluation reward takes the average over the 100 episodes.\nAlgorithm implementation\nWe base our implementation of SAC off Kostrikov (2021). It uses one target critic, double critics,\na single actor network and a single network for temperature adjustment for maximum entropy. We\nadd an additional value network with extreme q-learning loss for ROER TD error estimation and\npriority calculation. We use the default hyperparameters and network architectures for the SAC\nalgorithms for all of our experiments. The hyperparameters and the network architecture are shown\nin Table 4."}, {"title": "C.2 Hyper-parameter Selection", "content": "Online\nFor the value of parameter of PER, we use Fujimoto et al. (2020) as a reference for MuJoCo envi-\nronment where they show that a weight scale $a = 0.4$ works best for the set of tasks. As to DM\nControl, we search over the set [0.1, 0.2, 0.4, 0.6, 0.8] for individual task. The final choice of the value\nfor each task is shown in Table 5.\nFor the value of parameter large batch of LaBER, we search over the set [768, 1024, 1280, 1536] for\nindividual task in both MuJoCo and DM Control environment. The final choice of the value for\neach task is shown in Table 5.\nFor our proposed method ROER, we have 5 parameters, namely convergence rate ($\\lambda$), gumbel loss\nclip (Grad Clip"}]