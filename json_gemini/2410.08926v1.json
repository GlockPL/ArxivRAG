{"title": "ZERO-SHOT PUPIL SEGMENTATION WITH SAM 2: A CASE STUDY OF OVER 14 MILLION IMAGES", "authors": ["Virmarie Maquiling", "Sean Anthony Byrne", "Diederick C. Niehorster", "Marco Carminati", "Enkelejda Kasneci"], "abstract": "We explore the transformative potential of SAM 2, a vision foundation model, in advancing gaze estimation and eye tracking technologies. By significantly reducing annotation time, lowering technical barriers through its ease of deployment, and enhancing segmentation accuracy, SAM 2 addresses critical challenges faced by researchers and practitioners. Utilizing its zero-shot segmentation capabilities with minimal user input\u2014a single click per video\u2014we tested SAM 2 on over 14 million eye images from diverse datasets, including virtual reality setups and the worlds largest unified dataset recorded using wearable eye trackers. Remarkably in pupil segmentation tasks, SAM 2 matches the performance of domain specific models trained solely on eye images, achieving competitive mean Intersection over Union (mIoU) scores of up to 93% without fine-tuning. Additionally, we provide our code and segmentation masks for these widely-used datasets to promote further research.", "sections": [{"title": "1 Introduction", "content": "The increasing integration of eye tracking into technologies like virtual reality (VR) devices and smart glasses[Byrne et al.(2024)] has amplified the demand for robust gaze estimation systems, where a key task is the accurate localization of the pupil within an image or video[Kim et al.(2019), Maquiling et al.(2024)]. Traditional methods for pupil localization\u2014including thresholding and center of mass calculations[Nystr\u00f6m et al.(2023), Shortis et al.(1994), Per\u00e9z et al.(2003)] and ellipse-fitting algorithms[Santini et al.(2018a), Santini et al.(2018b)]\u2014while effective in controlled environments, suffer catastrophic errors in the presence of noise such as occlusions or reflections, limiting their utility in real-world settings[Kothari et al.(2022)]. To overcome these limitations, deep learning-based approaches have emerged as powerful alternatives, addressing issues plaguing traditional methods like blinks or reflections[Kim et al.(2019)] and improving the robustness and accuracy of pupil detection under challenging conditions [Fuhl et al.(2016a)]. However, deploying these models requires vast amounts of annotated data and tech-"}, {"title": "2 Methodology", "content": "We evaluated SAM 2's performance on a diverse set of eye tracking datasets to test its generalizability across various domains. These datasets include both VR-based and mobile eye tracking environments, representing controlled and real-world settings. Specifically, we selected four VR-based datasets (including one synthetic) and three mobile eye tracking datasets captured in natural, uncontrolled environments. Two of these datasets are from the OpenEDS challenges[Garbin et al.(2019), Palmero et al.(2020)], which focus on creating generalizable and robust semantic segmentations within VR settings. The synthetic NVGaze[Kim et al.(2019)] dataset includes its own pupil segmentations for gaze estimation. For the remaining datasets, ground truth segmentations were sourced from TEyeD[Fuhl et al.(2021)], the world's largest unified public dataset of eye images taken with head-mounted devices.\nBelow is a brief description of the datasets used:\n1. OpenEDS2019 [Garbin et al.(2019)]: Contains 12,759 non-sequential images (400 \u00d7 640 pixels) acquired\nfrom 152 participants using a VR head-mounted display (HMD) with eye-facing cameras at 200 Hz under\ncontrolled lighting. Provides pixel-level annotations for the pupil, iris, and sclera.\n2. OpenEDS2020 [Palmero et al.(2020)]: Features eye-image sequences from 80 participants using a VR HMD\nat 100 Hz. The Eye Segmentation Dataset includes 200 sequences sampled at 5 Hz totalling to 29,500 images,\nof which 5% are manually annotated (640 \u00d7 400 pixels).\n3. NVGaze [Kim et al.(2019)]: Comprises two datasets for near-eye gaze estimation under infrared illumination.\nThe real-world dataset includes 264,279 images (640 \u00d7 480 pixels) from 14 participants in a VR setting; the\nsynthetic dataset contains 2 million images (1280 \u00d7 960 pixels). We evaluated SAM 2 on both.\n4. Labelled Pupils in the Wild (LPW) [Tonsen et al.(2016)]: Consists of videos from 22 participants recorded\nin everyday environments using a head-mounted eye tracker at 120 Hz (640 \u00d7 480 pixels), covering diverse\nlighting conditions and natural gaze distributions.\n5. Gaze-in-Wild (GW) [Kothari et al.(2020)]: Provides naturalistic recordings from 19 participants performing\neveryday tasks with a mobile eye tracker at 120 Hz (640 x 480 pixels), including eye and head movements,\ninfrared eye images, and scene imagery.\n6. Dikablis datasets: A combination of datasets from ElSe [Fuhl et al.(2016b)], ExCuSe [Fuhl et al.(2015)],\nPNET [Fuhl et al.(2016a)], and a driving study [Kasneci et al.(2014)], compiled in TEyeD [Fuhl et al.(2021)].\nRecorded at 25 Hz (384 \u00d7 288 pixels), it features eye recordings from 30 participants."}, {"title": "3 Results", "content": "Overall, SAM 2 [Ravi et al.(2024)] performed well on both VR and mobile eye tracking datasets, with VR datasets\nshowing higher performance likely due to more controlled environments. Although SAM 2 slightly underperformed on\nthe nonsequential OpenEDS2019 dataset\u2014composed of individual images rather than continuous video frames\u2014it still"}, {"title": "4 Discussion", "content": "The quality of the dataset significantly impacts SAM 2's performance. For instance, datasets with clear eye images,\nminimal noise and high resolution, such as OpenEDS and NVGaze, yielded higher IoU scores and lower pupil loss\nrates. However, SAM 2 encountered more difficulty in noisier datasets, like the Dikablis datasets, resulting in more\npupil tracking failures.\nIn terms of human interaction, the effort required is mostly limited to monitoring the quality of the predicted masks\nand adding additional prompts only when necessary. While we limited our evaluation to a single point prompt per\nvideo, SAM 2 supports various other prompt types, such as multiple positive or negative point prompts (where the\nsigns indicate areas that SAM 2 should and should not include in its segmentation) and bounding box prompts, offering\nflexibility for more complex and noisier datasets that require additional guidance. Below we highlight several practical\nlessons from conducting our study for researchers interested in implementing SAM 2 for eye tracking data segmentation\ntasks:"}, {"title": "4.1 Practical Lessons Learned", "content": "1. Significant Time Savings: SAM 2 significantly reduced the time necessary to annotate entire datasets. The\ndatasets in this study were annotated by two of the authors within a couple of days, demonstrating SAM\n2's efficiency in annotating large volumes of data quickly. Importantly, these authors spent most of the time\nwaiting for the model to finish producing its segmentation for the datasets, and only very little time setting up\nthe model and checking its output.\n2. Reduced Technical Barrier to Entry: SAM 2 not only saves time in obtaining segmentation masks but also\ngreatly simplifies the process for non-experts. Instead of developing custom pupil segmentation models, users\ncan run SAM 2 with just a few lines of code. This lowers the barrier to entry, enabling more people to develop\ngaze estimation pipelines.\n3. Minimal Human Interaction: The annotation process required minimal human involvement beyond preparing\nthe prompts and finding appropriate frames where the pupil is visible. SAM 2 handled the actual annotation,\nwhile the user only needs to perform quality checks and refine prompts when necessary.\n4. Dealing with Noisy Data While SAM 2 performed well in most cases, a decrease in performance was\nobserved in noisier datasets. To mitigate this, further refining of prompts (e.g. using a more appropriate prompt\nstrategy or adding prompts on more difficult frames) is necessary-although this still involves less human effort\ncompared to traditional annotation processes.\n5. Low Hardware Requirements SAM 2's low GPU requirements make it accessible for researchers with\nlimited computational resources. We used both a high-end NVIDIA A100 (80GB VRAM) and a GeForce RTX\n4090 (24 GB VRAM), achieving compute times of up to 40 frames per second (fps) for both. Impressively,\nSAM 2 also ran on more budget-friendly GPU's such as the RTX 4060 Ti (16 GB VRAM) delivering around\n12 fps, and was even functional on laptop-class GPU's, albeit at significantly lower frame rates of just a few\nfps. However, it is worth noting that inference performance of the model appeared to be limited by CPU and\nnot GPU performance in most of these cases.\n6. Strong Generalization SAM 2 demonstrated robust performance across a diverse set of eye tracking datasets\nincluding VR, mobile, and even synthetic data, despite not being specifically trained on eye tracking data."}, {"title": "4.2 Open Challenges, Limitations & Future Work", "content": "While SAM 2 excelled in pupil segmentation, challenges remain with segmenting less distinct eye features like the iris\nand sclera. To explore this, we evaluated SAM 2's performance on the OpenEDS2020 dataset, where it achieved an\nmIoU of 76.53% for the iris and only 7.36% for the sclera with a single box prompt. Fine-tuning SAM 2 on specific\neye features, especially under varying conditions like lighting, reflections, and noise, could improve performance by\nreducing its reliance on ideal conditions and simple prompts.\nAdditionally, alternative prompting strategies, such as using bounding boxes or multiple point prompts, may yield better\nresults in challenging cases. A limitation of our study is that we did not explore different prompt strategies, opting\nfor a single point prompt to highlight the simplicity of annotation with SAM 2. While a single prompt proved to be\neffective for pupil segmentation in many cases, other prompts may improve results in difficult cases or for less well\ndefined features.\nAnother consideration is the possibility that SAM 2 may have encountered similar images during training as all the\ndatasets we have used are open to the public. Future work should test SAM 2 on completely novel datasets to validate its\ngeneralization capabilities. Additionally, as eye tracking moves to consumer devices, a key challenge will be adapting\nSAM 2 for low-power hardware like smart glasses [Zhang et al.(2023)], making it crucial to balance performance with\nreduced computational requirements for real-time applications in VR, AR devices."}, {"title": "5 Conclusion", "content": "In this study, we assessed the practical segmentation capabilities of the SAM 2 Vision Foundation model. Using SAM\n2, we efficiently annotated over 14 million pupil images across multiple datasets with just a few click prompts per\ndataset, significantly streamlining traditional annotation workflows. Our findings show that foundation models like\nSAM 2 effectively address key challenges in eye tracking research: data annotation, domain adaptation, and reducing\ntraining data requirements. Notably, SAM 2 achieves robust performance without fine-tuning, offering a user-friendly\nand accurate solution compared to its predecessor, SAM. Its ability to annotate entire datasets with minimal human\ninput makes it suitable practically for large-scale applications. This work highlights the potential for general-purpose\nmodels to benefit other HCI fields where extensive labeled data is needed. As these models advance, we anticipate\ncontinued progress in both eye tracking research and broader human-computer interaction applications."}]}