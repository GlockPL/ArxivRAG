{"title": "Exploring the Stability Gap in Continual Learning: The Role of the Classification Head", "authors": ["Wojciech \u0141apacz", "Daniel Marczak", "Filip Szatkowski", "Tomasz Trzci\u0144ski"], "abstract": "Continual learning (CL) has emerged as a critical area in machine learning, enabling neural networks to learn from evolving data distributions while mitigating catastrophic forgetting. However, recent research has identified the stability gap a phenomenon where models initially lose performance on previously learned tasks before partially recovering during training. Such learning dynamics are contradictory to the intuitive understanding of stability in continual learning where one would expect the performance to degrade gradually instead of rapidly decreasing and then partially recovering later. To better understand and alleviate the stability gap, we investigate it at different levels of the neural network architecture, particularly focusing on the role of the classification head. We introduce the nearest-mean classifier (NMC) as a tool to attribute the influence of the backbone and the classification head on the stability gap. Our experiments demonstrate that NMC not only improves final performance, but also significantly enhances training stability across various continual learning benchmarks, including CIFAR100, ImageNet100, CUB-200, and FGVC Aircrafts. Moreover, we find that NMC also reduces task-recency bias. Our analysis provides new insights into the stability gap and suggests that the primary contributor to this phenomenon is the linear head, rather than the insufficient representation learning.", "sections": [{"title": "1. Introduction", "content": "Neural networks have been widely adopted across various domains, including computer vision, natural language processing, speech and audio processing, and control tasks [5, 10, 31]. However, most neural network applications are limited to offline settings with static data distributions, primarily due to the challenge of catastrophic forgetting [27]-a phenomenon where a model loses previously acquired knowledge when exposed to new data or distribution shifts. Adapting to and learning from evolving data distributions is crucial for many tasks, such as autonomous"}, {"title": "2. Related Works", "content": "Continual learning aims to enable learning from changing data streams and mitigate catastrophic forgetting [27] destructive overwriting of previously acquired knowledge that appears when learning from new data. Continual learning usually assumes that the data come in the form of sequential tasks, and once the learner finishes processing the task it can no longer access its data for training; however, it is still evaluated on the data from all the tasks seen so far. The main scenarios analyzed in the continual learning community are task-incremental learning, class-incremental learning, and domain-incremental learning [26, 40]. Task-incremental learning and class-incremental learning usually analyze a scenario where new tasks contain new classes, with task-incremental learning assuming access to task identity information during prediction and class-incremental learning assuming no access to such information, which makes it a more challenging scenario. Domain-incremental learning instead operates"}, {"title": "2.2. Stability Gap", "content": "Stability gap [19] is a phenomenon where a continually trained neural network experiences a significant forgetting on task transitions followed by a recovery phase in which the network recovers a significant fraction of its original performance. The pioneering work [19] identifies the stability gap in class-, task- and domain-incremental settings for image classification, while [8] identifies similar phenomenon in continual pre-training of Large Language Models (LLMs). [13] shows that the stability gap occurs even with incremental joint training ('full replay'). [15] goes a step further, demonstrating that it is also present in joint incremental learning of homogeneous tasks (all tasks are drawn from the same distribution). Carta et al. [4] try to overcome the stability gap and improve the network stability with temporal ensembling, but the scope of their work is limited to online continual learning. Harun and Kanan [11] also investigate the stability gap in a setting resembling out-of-distribution (OOD) detection and propose a method that combines multiple heuristics to address it. However, to the best of our knowledge, no prior work has studied the disentangled influence of the backbone and classification heads on the stability gap."}, {"title": "2.3. Nearest Mean Classifier (NMC) in CL", "content": "NMC is an alternative to a linear classifier that utilizes the prototypes (average representations of each class based on a set of samples) for classifying new samples based on the distance between their representations and the prototypes. NMC usually assigns the class of the closest prototype and as such is a non-parametric classifier that instead of weights requires storing class prototypes. NMC and its variants have been widely used in few-shot or zero-shot learn-"}, {"title": "3. Problem Setup", "content": ""}, {"title": "3.1. Incremental learning", "content": "We consider a continual learning scenario with a learner f, a neural network composed of the backbone \u0398, and a linear classification head g. Given input x, the model produces the prediction by applying the classifier on top of the features extracted from the backbone: f(x) = g(\u0398(x)).\nThe learner is trained on a sequence of T tasks, which correspond to separate, disjoint datasets $D_1, ..., D_T$. We refer to the model obtained after t tasks as $f_t$. For task t, $D_t$ contains inputs $x_t$ and corresponding ground truth labels $y_t$. The datasets are presented sequentially in an offline manner, meaning that the model can pass through the training data multiple times. During training on task t, the model can no longer access the full data corresponding to the previous tasks $t-1, t-2, ..., 1$. This differs from a scenario called joint incremental learning, where the model can continuously train on the union of all the datasets seen so far. Joint incremental learning is considered the upper bound on continual learning performance [26]. In practice, many continual learning methods store a small number of exemplars to rehearse previously learned knowledge, as this guarantees some degree of continual performance.\nWe consider a standard class-incremental scenario [26, 40], where each task contains new classes previously unseen by the model. Upon encountering a new task t, we create a dedicated task head $g_t$ that will predict the set of classes in this task. However, as the model cannot access the task identity in the class-incremental scenario, we use the concatenated output of heads for all the tasks seen so far to obtain the prediction.\nWe employ the Nearest-Mean Classifier (NMC) as introduced in iCaRL [30]. To predict label $y^*$, we compute prototype vectors $\u03bc_1, ..., \u03bc_C$ (mean features) of all C classes seen so far using exemplars for previous tasks classes, and training set for current task classes. Then, we compute distances between the new feature vector and all prototypes, assigning the class label of the most similar prototype:\n$y^* = argmin_{c=1,...,C} || f(x) \u2013 \u03bc_c||$"}, {"title": "3.2. Metrics", "content": "In this section, we describe the metrics we use to evaluate the performance of continual learners. We employ the standard average accuracy metric defined by Masana et al. [26] and average minimum accuracy and worst-case accuracy from the initial stability gap paper [19]. Finally, we define the stability gap as the normalized difference between the model performance at the end of the previous task and the lowest accuracy obtained through model learning on the current task. All metrics here are defined based on Task-Agnostic Accuracy, where the model does not have information about task ID. Additional study using Task-Aware Accuracy is presented in the Appendix. For the reader's convenience, we describe those metrics shortly below.\nAverage Accuracy. We define average incremental accuracy $ACC_t$ obtained after task t as:\n$ACC_t = \\frac{1}{t} \\sum_{i=1}^{t} A(f_t, Y_i)$\nwhere A($f_t, Y_i$) is the accuracy of the model $f_t$ on the i-th task data, with i < t.\nAverage minimum accuracy. We define average absolute minimum accuracy min-$ACC_t$ measured over previous evaluation tasks after they have been learned as:\nmin-$ACC_t = \\frac{1}{t-1} \\sum_{i=1}^{t-1} min_n A (f_n, Y_i)$,\nwhere i < n < t. Intuitively, this metric gives a worst-case estimation of knowledge preservation on the previously observed tasks.\nWorst-case accuracy. We define WC-$ACC_{t,k}$ as a trade-off between the accuracy on k-th iteration of task t and min-$ACC_t$ for previous tasks:\nWC-$ACC_{t,k} = \\frac{1}{2} A(f_k, Y_t) + (1-\\frac{1}{2})min-ACC_t$\nStability gap. We introduce stability gap $SG_{t,i}$, a metric to measure that we define as a maximum drop in accuracy on the task i < t throughout learning a new task t:\n$SG_{t,i} = \\frac{A(f_{t-1}, Y_i) - min_n A(f_t, y_i)}{A(f_{t-1}, Y_i)}$\nwhere $f^n_t$ refers to a model obtained after n gradient updates on a task t. This metric gives us an insight into how much the model degrades during training. However, note that"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Setup", "content": "We conduct experiments on common continual learning benchmarks CIFAR100 [17], ImageNet100 [39] and fine-grained classification datasets CUB-200-2011 Birds (Birds) [42] and FGVC Aircraft (Aircrafts) [24]. We create CIL scenarios by splitting the classes in each dataset into equally sized disjoint tasks.\nWe use the code built on top of FACIL [26] framework. We use ResNet18 [12] as a backbone and train from random initialization (except Sec. 4.3 where we start from the weights pre-trained on ImageNet). In the Appendix, we provide the ablation study of different CNN architectures used as backbones. We use the same hyperparameters for all variants within the single method unless stated otherwise and report our results averaged over three runs with different random seeds.\nIn every setup, we train the network on each new task for 100 epochs with a batch size of 128. We use an SGD optimizer with a linearly decaying learning rate. For the CIFAR100 and ImageNet100 datasets, we utilize a constant memory budget of 2000 exemplars. This approach ensures a balanced representation of past tasks while limiting the total memory usage. For experiments on fine-grained datasets, we use 10 exemplars per class. Exemplars are selected randomly at the end of each task. We compute all the metrics from Sec. 3.2 for the full test set at the end of each epoch."}, {"title": "4.2. Disentangling stability gap", "content": "To understand which part of the network contributes the most to the stability gap and further disentangle the performance of the classification head from the backbone, we consider an oracle NMC. It is trained in the same manner as fine-tuning (with exemplars) and NMC (on data from current task and from the memory buffer) but additionally it has access to all the training data seen so far when computing the prototypes. This setup gives us the upper bound on the non-parametric classification based on the network representations. By comparing the difference between the performance of the oracle NMC and the performance of the network using a linear head, we can isolate the portion of the stability gap that can be attributed to the classifier.\nWe present the results in Fig. 2 and observe that the oracle NMC greatly surpasses standard finetuning. Specifi-"}, {"title": "4.3. Realistic NMC scenarios", "content": "We build on the insights from the previous Section and utilize NMC's positive impact on stability in more realistic scenarios of limited memory buffer. We provide additional"}, {"title": "4.4. Impact of memory buffer size", "content": "NMC performs the classification using the class prototypes, and its performance will naturally be affected by the size of the exemplar set. Therefore, in this Section, we compare NMC and finetuning with varying numbers of exemplars and different types of memory. Specifically, we evalu-"}, {"title": "4.5. NMC in joint incremental learning", "content": "Recent work [15] showed that stability gap also happens in joint incremental learning. Therefore, it is valid for us to ask whether our previous findings on NMC's superior stability over the linear head also hold in this setting. To this end, we evaluate joint incremental learning models with standard head and NMC at Fig. 7 and examine their stability at the data from task 1, as well as average accuracy across all the tasks. Surprisingly, even in this scenario, NMC performs better than linear heads. This proves that non-parametric NMC improves learning stability and slightly helps with the stability gap even in scenarios with minimal changes in data distribution between the tasks."}, {"title": "4.6. Task Recency Bias", "content": "In this Section, we explore task recency bias of finetuning and NMC. To analyze this, we use the task confusion matrix, a variation of the classic confusion matrix, where"}, {"title": "4.7. Stability with classification head warm-up", "content": "Work from Kumar et al. [18] suggests that a warm-up protocol for linear heads improves the downstream performance in transfer learning by first training new linear heads in isolation before finetuning the full network. Such head initialization reduces the initial error when learning a new task and therefore reduces the magnitude of the subsequent gradient updates, overall leading to fewer changes in the backbone network. The proposed warm-up scheme promises to improve learning stability, so motivated by the weak results of standard heads in our previous experiments we extend this protocol to CL to see if it can partially mitigate the performance difference between standard head and NMC. At the beginning of the new task, we first freeze the backbone and train only the linear head for a few epochs; then, we finetune both the backbone and head simultaneously. We present the results in Fig. 10. We do not observe any benefits from employing the warm-up protocol in our"}, {"title": "5. Conclusions", "content": "In this paper, we explore the stability gap phenomenon in continual learning, focusing on its origins within different components of neural networks: feature extractor and classification head. To disentangle the impact of each part of"}, {"title": "A. Task-Aware Accuracy", "content": "Task-aware accuracy is a metric used in continual learning to evaluate model performance when task identity is known during inference. Task-aware accuracy requires the model only to distinguish classes within-task, as opposed to task-agnostic accuracy, which requires both within-task class separation and correct task classification. Therefore, task-aware accuracy is considered to be an easier setting. In this section, we show task-aware results corresponding to experiments from Sec. 4.2 and Sec. 4.3, and show that even when we evaluate task-specific linear head it performs worse than NMC, empowering our previous claims. The results can be seen in Fig. 11, Fig. 12, and Fig. 13."}, {"title": "B. Other approaches", "content": "We expand our investigation to regularization-based (LwF [22], SS-IL [1]), and parameter-isolation (EWC [16]) methods. We use a constant memory buffer for all the methods (2000 exemplars)."}, {"title": "B.1. Task-Agnostic results", "content": "More advanced methods, which usually perform better on CIL setups, provide better knowledge transfer while reducing forgetting and ultimately better representations, but still suffer from the linear multi-head classifier. Methods based on modified loss functions provide better latent representations, and NMC additionally mitigates the problems associated with the classifier head (see Tab. 4, Fig. 14, and Fig. 15)."}, {"title": "B.2. Latest-Task Prediction Bias", "content": "Experiment analogous to the one in Fig. 9. We observe that NMC reduces the LTB even in approaches that try to improve the linear heads of previous tasks, e.g. by knowledge distillation or other methods, more than simple finetuning with exemplars (see results in Fig. 15)."}, {"title": "C. Different CNN architectures", "content": "To further investigate the influence of the non-parametric NMC classifier, in addition to experiments with ResNet18, we test it with other standard convolutional neural networks. We evaluate finetuning with constant memory (2000 exemplars) on linear multi-head and NMC with MobileNetV2 [34], ResNet50 [12], EfficientNet-B4 [38], and VGG11 [37] as backbones. We use CIFAR100 split into 10 equally sized tasks and train the networks as described in Sec. 3.1. We still notice that using NMC constantly improves the results, regardless of the architecture we use."}]}