{"title": "Challenges and Best Practices in Corporate AI Governance:\nLessons from the Biopharmaceutical Industry", "authors": ["Jakob M\u00f6kander", "Margi Sheth", "Mimmi Gersbro-Sundler", "Peder Blomgren", "Luciano Floridi"], "abstract": "While the use of artificial intelligence (AI) systems promises to bring significant economic and social\nbenefits, it is also coupled with ethical, legal, and technical challenges. Business leaders thus face the\nquestion of how to best reap the benefits of automation whilst managing the associated risks. As a\nfirst step, many companies have committed themselves to various sets of ethics principles aimed at\nguiding the design and use of Al systems. So far so good. But how can well-intentioned ethical\nprinciples be translated into effective practice? And what challenges await companies that attempt to\noperationalize Al governance? In this article, we address these questions by drawing on our first-\nhand experience of shaping and driving the roll-out of AI governance within AstraZeneca, a\nbiopharmaceutical company. The examples we discuss highlight challenges that any organization\nattempting to operationalize AI governance will have to face. These include questions concerning\nhow to define the material scope of AI governance, how to harmonize standards across decentralized\norganizations, and how to measure the impact of specific AI governance initiatives. By showcasing\nhow AstraZeneca managed these operational questions, we hope to provide project managers, CIOs,\nAl practitioners, and data privacy officers responsible for designing and implementing AI governance\nframeworks within other organizations with generalizable best practices. In essence, companies\nseeking to operationalize AI governance are encouraged to build on existing policies and governance\nstructures, use pragmatic and action-oriented terminology, focus on risk management in development\nand procurement, and empower employees through continuous education and change management.", "sections": [{"title": "Introduction | The need for corporate AI governance", "content": "There are two main reasons why artificial intelligence (AI) might be giving business leaders around\nthe world sleepless nights. The first is sheer excitement at the possibilities of the technology. The\nsecond is abject fear about the implications of getting it wrong. The potential of autonomous and\nself-learning technologies to revolutionize industries is only just starting to be realized, with far-\nreaching implications for human development, economic prosperity, and the financial prospects of\nindividual firms (Floridi et al., 2018). Unsurprisingly, companies are scrambling to implement AI-\npowered solutions, conscious no doubt that their competitors will be doing the same. But in the rush\nto join the revolution, there is great danger of organizations making missteps that could lead them\ninto legal and ethical minefields, where they might not only suffer potentially fatal reputational\ndamage but also cause real-world harm (Silverman, 2021).\nConsider healthcare as an example of both the benefits and the risks associated with AI. Across the\nindustry, Al systems are already saving lives and increasing life quality by aiding medical\ndiagnostics, driving service improvements through better forecasting, and enabling more effective\ndrug discovery processes (Schneider, 2019; Topol, 2019). However, along with excitement and\nopportunity, the use of AI systems in healthcare is coupled with serious ethical challenges. Such\nsystems may leave users vulnerable to discrimination and privacy violations (Laurie et al., 2014).\nThey can also erode human self-determination and enable wrongdoing (Tsamados et al., 2021).\nRecently, public outcry against specific AI use cases (Holweg et al., 2022) and proposals for \"hard\"\nlegislation in both the EU and the U.S. have stressed the urgency of addressing these challenges. In\nthat context, defining and communicating ethical principles is a crucial first step toward\nimplementing effective AI governance. It is also a step that many organisations in both the private\nand the public sector have already taken. However, organisations still lack effective ways of\ntranslating those abstract principles into concrete actions that will enable them to achieve the benefits\nof AI in ways that are ethical, legal, and safe (Gianni et al., 2022; Morley et al., 2019)."}, {"title": "Case study | AstraZeneca's Al governance journey", "content": "As an R&D-driven organization, AstraZeneca's core business is to use science and innovation to\nimprove health outcomes through more effective treatment and the prevention of complex diseases.\nIn doing so, the company uses AI systems in many different ways. These include biological insight\nknowledge graphs to improve drug discovery processes, machine-learning powered image\nrecognition software for faster and more accurate medical analysis, and natural language processing\nmodels to prioritize adverse event reports (Crowe, 2020; Lea et al., 2021).\nAll these use cases change how the company collects, analyzes, and utilizes data. And as\ntechnologies and ways of working evolve, so must organizational governance. In November 2020,\nAstraZeneca's board moved toward addressing that need by publishing a set of Principles for Ethical\nData and AI. Those principles stipulate that the use of AI systems should be private and secure,\nexplainable and transparent, fair, accountable, human-centric, and socially beneficial.\nAstraZeneca's ethics principles aim to help employees and partners navigate the risks associated with\nAl systems. Yet principles alone cannot ensure that such systems are designed and used ethically\n(Mittelstadt, 2019), and their implementation is never straightforward (Ryan et al., 2021). Moreover,\nlike many other multinational corporations, AstraZeneca is a decentralized organization. Different\nbusiness areas were thus allowed to develop their own Al governance structures to reflect variations\nin objectives, digital maturity, and ways of working.\nTo support and unify local activities, the company launched four enterprise-wide initiatives:\n\u2022\nThe creation of overarching compliance- and guidance documents. The aim thereby was to\nbreak down each high-level principle into more tangible and actionable formulations.\n\u2022\nThe development of a Responsible AI playbook. The aim thereby was to provide detailed,\nend-to-end guidance on developing, testing, and deploying AI systems within AstraZeneca.\n\u2022\nThe establishment of an internal Responsible AI Consultancy Service, and an Al resolution\nBoard. These new organizational functions were established to (i) facilitate the sharing of best\npractices, (ii) educate staff, and (iii) monitor the governance of Al projects.\n\u2022\nThe commissioning of an Al audit conducted in collaboration with an independent party. By\nsubjecting itself to external review, AstraZeneca got valuable feedback on how to improve its\nexisting and emerging AI governance structures.\nThe above-listed initiatives may appear straight forward. But they only emerged out extended \u2013 and\nsometimes ad hoc \u2013 internal processes that came up against both conceptual difficulties and\norganizational tensions. In the remainder of this article, we highlight the challenges AstraZeneca\nfaced in its efforts to operationalize AI governance and discuss lessons learned, i.e., how these\nchallenges can be managed in pragmatic and constructive ways."}, {"title": "Practical implementation challenges | What to be prepared for?", "content": "Organizations seeking to operationalize AI governance face both conceptual and practical\ndifficulties. Our research and first-hand experience suggest that there are four main challenges:"}, {"title": "Balancing interests", "content": "The first challenge concerns the tension between risk management and innovation. The use of AI\nsystems in the pharmaceutical industry gives a stark illustration of that issue. Obviously, the industry\nmust put patients' safety first. Often, that means using available technologies to develop new drugs or\nto diagnose and intervene early in the course of a disease. To ensure that such drugs are safe,\nAstraZeneca trains AI systems to detect treatment response patterns (Nadler et al., 2021). But red\ntape related to Al governance could restrict the development of such potentially lifesaving\nprocedures. In such circumstances, how does a company \u201cerr on the safe side?\u201d There is no simple,\none-size-fits-all answer. Instead, organizations seeking to operationalize AI governance should\nprioritize defining and controlling the risk appetite of different projects."}, {"title": "Defining \u2018AI'", "content": "Second, every policy needs to define its scope, but how can you do this when there is no universally\naccepted definition of AI? Determining the scope of Al governance is especially difficult because\nthe technology is always embedded in larger sociotechnical systems, in which processes driven by\nhumans and by machines overlap (Lauer, 2020). That is why establishing the scope of AI governance\nis a balancing act. Make the scope overinclusive, and you create unnecessary administrative burdens.\nMake it underinclusive, and risks will go under the radar (Danks & London, 2017). In AstraZeneca's\ncase, countless meetings were spent on discussing how to best strike that balance. They key to move\nbeyond such discussion is to realize that there is a three-way trade-off between how precisely you\ndefine the scope of your Al governance, how easy it is to apply it, and how generalizable it is."}, {"title": "Harmonizing standards", "content": "Third, the same requirements must apply to all AI systems used by an organization. If not, corporate\nAI governance may simply persuade managers to outsource unethical or risky projects (Floridi,\n2019). But the drive to impose uniform requirements creates new tensions. Large organizations often\ncomprise distinct business areas that operate independently. The cycle of designing and training AI\nsystems often involves multiple organizations. For example, AstraZeneca collaborates with\nBenevolentAI, a British start-up, to identify treatments against chronic kidney disease by using the\nformer's rich datasets to build biological insight knowledge graphs. This is not an exception but the\nrule: AI systems result from supply chains spanning multiple actors and geographic regions\n(Crawford, 2021). Harmonizing standards means treating all AI systems equally, regardless of\nwhether they have been developed in-house or procured from third parties."}, {"title": "Measuring results", "content": "Fourth, ethics is hard to quantify, and it is not clear how organizations seeking to operationalize AI\ngovernance can measure and demonstrate their success. One option is assessing how Al systems\noperate in terms of fairness, transparency, and accountability. However, it is hard to find ways to\nquantify and measure these in practice (Kleinberg, 2018). And, as Goodhart's Law reminds us, when\na measure becomes a target, it ceases to be a good metric (Strathern, 1997). Alternatively,\norganizations could focus on designing process-based KPIs to capture the mechanisms in place to\nmitigate technology-related risks. Yet such checklists tend to reduce AI governance to a box-ticking\nexercise. Perhaps the solution here comes down to a question of mindset: In an Al governance\ncontext, the main purpose of KPIs is not to assess whether a specific system is \u201cethical\u201d but rather to\nspark debates about ethics that inform design choices."}, {"title": "Discussion | Best practices and lessons learned", "content": "While the challenges discussed above are real and important, they are not insurmountable. Our\nexperiences from coordinating and observing AstraZeneca's efforts to operationalize AI governance\ncan be condensed into four transferable best practices:"}, {"title": "Build on existing policies and governance structures", "content": "Al governance is most likely to be effective when integrated into existing governance structures and\nbusiness processes (Hodges, 2015). Policies that duplicate existing structures may be perceived as\nunnecessary by the people expected to implement them. Rather than adding steps to their software\ndevelopment processes, organizations can simply update them so that solution requirements align\nwith the objectives of AI governance. This makes is easier for employees to understand what is\nexpected of them and how any new measure related to AI governance impacts their daily tasks. For\nexample, AstraZeneca's software developers and clinical experts found that trying to implement the\ncompany's Al ethics principles pushed them to think about their projects in new ways that can help\norganizations improve their processes and workflows. In short, AI governance is most effectively\noperationalized when such advantages are clearly communicated."}, {"title": "Use pragmatic and action-oriented terminology", "content": "It is less important to define what AI is in abstract terms and more important to establish processes\nfor identifying those systems that require additional layers of governance. Rather than struggling to\npin down a precise definition of AI, AstraZeneca created a guidance document that describes the\ncharacteristics of the systems to which their ethics principles apply. A list of examples does not\nconstitute a definition of AI, but it can nevertheless help employees determine whether a specific use\ncase is in scope. Also, following the European Commission (2021), AstraZeneca adopted a risk-\nbased approach, classifying systems as either low-, medium- or high-risk with proportionate\ngovernance requirements attached to each level. Using a familiar concept as \u201crisk assessment\u201d helps\norganizations integrate AI governance into their existing quality management processes. A risk-based\napproach is also future-proof because it avoids the trap of committing to a definition that could\nbecome obsolete as the technology rapidly develops."}, {"title": "Focus on risk management in development and procurement", "content": "Distinguishing between compliance assurance and risk assurance helps to harmonize standards across\norganizations. While compliance assurance compares organizational procedures to existing laws and\nregulations, risk assurance asks open-ended questions about how different business areas work to\nidentify and manage risk. Because regulations vary across jurisdictions and sectors (Viljanen &\nParviainen, 2022), it is not always practically feasible to audit all parts of a large, multinational\norganization for compliance against the same substantive standards. In contrast, risk assurance can be\nadapted locally to reflect how different business areas understand risk. Because they leave space for\nmanagers in different regions and business areas to justify their governance design choices, it is both\npossible and desirable to subject all parts of an organisation to harmonized AI risk audits. Again, this\ndoes not necessarily require the creation of additional layers of governance. Organizations should\nsimply focus on finding any gaps in their existing development and procurement processes and filling\nthem by adding ethics-based evaluation criteria for AI systems."}, {"title": "Empower employees through continuous education and change management", "content": "Because corporate AI governance is about change management, internal communication and training\nefforts are key. In AstraZeneca's case, these efforts were continuous and happened simultaneously on\nseveral different levels. For example, the ethics principles were agreed upon through a bottom-up\nprocess that included extensive consultations with employees. An important aspect of this process\nwas anchoring the ethics principles with internal stakeholders. After all, ensuring that AI systems are\ndesigned and used legally, ethically, and safely requires organizations not only to have the right tools\nin place but also to make their employees aware of them and willing to use them.\nAdmittedly, change management is no easy task and the implementation of AI governance is no\nexemption: Humans have limited attention spans, and employees are frequently bombarded with\ninformation about different governance initiatives (Baldwin & Cave, 1999). That said, our first-hand\nexperiences suggest that much can be done to facilitate a successful implementation of AI\ngovernance. To start with, communication concerning AI governance is most effective when\nsupported by senior executives. AI governance is also more likely to be implemented when aligned\nwith incentives for individuals and business areas. Put differently, employees must be enabled and\nsupported to do the right thing. That includes training and education as well as channels through\nwhich employees can seek escalate issues without fear of being blamed. Finally, tools such as impact\nassessments and model testing protocols may be developed by individual teams but should be shared\nwidely to encourage the harmonization of practices and prevent the duplication of efforts."}, {"title": "Concluding remarks | Upfront investments vs. long-term benefits", "content": "Our case study of AstraZeneca shows that the most important step toward good corporate AI\ngovernance is to ensure procedural regularity and transparency. To do so, organizations do not need\nto invent or impose new corporate governance structures. For example, while many useful tools such\nas model cards (Mitchell et al., 2019) and datasheets (Gebru et al., 2018) and methods, like\nconformity assessments (Floridi et al., 2022), have already been developed, their use is typically\nneither coordinated nor enforced. That is why the immediate goal of corporate AI governance should\nbe to interlink existing structures, tools, and methods as well as to encourage and inform ethical\ndeliberation through all stages of the AI lifecycle (M\u00f6kander & Axente, 2021).\nEfforts to operationalize Al governance incur costs, both financial and administrative. To start with,\nformulating organizational values bottom-up is a time-consuming activity. In AstraZeneca's case, the\nprocess of drafting the ethics principles also included multiple consultations with internal executive\nleaders on strategy and with academic researchers offering external feedback. Since the publication\nof its ethics principles in 2020, approximately four full-time staff have been working on\nimplementing Al governance across AstraZeneca. Also, in Q4 2021, the company conducted an \u201cAI\naudit\" in collaboration with an independent third party. Added to the costs of procuring that service,\nAstraZeneca employees invested around 2,000 person-hours in the audit.\nTo put those numbers into perspective, consider the costs associated with certification and\ncompliance with hard legislation. According to the European Commission, obtaining certification for\nan AI system in line with the proposed EU legislation on AI will cost on average EUR 20.000,\ncorresponding to approximately 12% of the development cost (Renda et al., 2021). At the same time,\none of the main reasons why technology providers engage with auditors is that it is often cheaper and\neasier to address system vulnerabilities early in the development process. In addition, good AI\ngovernance can help organizations improve several business metrics, including data security, brand\nmanagement, and talent acquisition (EIU, 2020). This shows that \u2013 despite the associated costs \u2013\nbusinesses have clear incentives to implement effective corporate AI governance structures.\nOur discussion in this article has centered on lessons from the pharmaceutical industry. However,\nAstraZeneca's situation seems highly representative of the many firms that have recently adopted\nethics principles for the design and use of AI systems. That is why the challenges and best practices\ndiscussed above should be relevant to any organization seeking to operationalize AI governance. It is\nvital to remember that such governance will not, and should not, replace the need for the designers,\noperators, and users of AI systems to continuously reflect on the ethics of their actions. Nevertheless,\ngovernance that follows the best practices outlined in this article can help organizations manage the\nethical risks posed by AI systems while reaping the economic and social benefits of automation."}]}