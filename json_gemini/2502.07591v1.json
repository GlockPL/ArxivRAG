{"title": "DMWM: Dual-Mind World Model with Long-Term Imagination", "authors": ["Lingyi Wang", "Rashed Shelim", "Walid Saad", "Naren Ramakrishnan"], "abstract": "Imagination in world models is crucial for enabling agents to learn long-horizon policy in a sample-efficient manner. Existing recurrent state-space model (RSSM)-based world models depend on single-step statistical inference to capture the environment dynamics, and, hence, they are unable to perform long-term imagination tasks due to the accumulation of prediction errors. Inspired by the dual-process theory of human cognition, we propose a novel dual-mind world model (DMWM) framework that integrates logical reasoning to enable imagination with logical consistency. DMWM is composed of two components: an RSSM-based System 1 (RSSM-S1) component that handles state transitions in an intuitive manner and a logic-integrated neural network-based System 2 (LINN-S2) component that guides the imagination process through hierarchical deep logical reasoning. The inter-system feedback mechanism is designed to ensure that the imagination process follows the logical rules of the real environment. The proposed framework is evaluated on benchmark tasks that require long-term planning from the DMControl suite. Extensive experimental results demonstrate that the proposed framework yields significant improvements in terms of logical coherence, trial efficiency, data efficiency and long-term imagination over the state-of-the-art world models.", "sections": [{"title": "1. Introduction", "content": "Imagination is a core capability of world models that allows agents to predict and plan effectively within internal virtual environments by using real-world knowledge. By predicting future scenarios in latent spaces, agents can evaluate potential outcomes without frequent real-world interactions thus significantly improving data efficiency and minimizing trial-and-error costs. For complex tasks requiring long-term planning, imagination capabilities allow world models to evaluate the long-term consequences of diverse strategies and identify the optimal action plans. Consequently, the effectiveness of model-based decision-making approaches, such as model-based reinforcement learning (RL) and model predictive control (MPC), can heavily depend on the quality of their imagination abilities.\nOne of the most widely used frameworks for world models is the so-called recurrent state-space model (RSSM) and its variants that combine deterministic recurrent structures with stochastic latent variables to model environmental dynamics in a compact latent space. By doing so, RSSM models can capture the sequential dependencies and uncertainty of their target environment. However, RSSM faces challenges in providing reliable, long-term predictions over extended imagination horizons due to the accumulation of prediction errors and the limitations of statistical inference. In particular, although existing RSSM-based solutions can generate accurate short-term predictions in a single-step manner, small errors inevitably propagate over longer time horizons, gradually amplifying over each step and resulting in significant deviations between the imagined and actual states. Moreover, RSSM schemes often optimize state-space representations through reconstruction loss or regression. This approach can lead to overfitting to observed patterns and cannot properly capture latent dynamics, particularly in complex, dynamic environments.\nSeveral models have been proposed to enhance long-term planning. For instance, trajectory-based models learn long-term dynamics by directly predicting future states to reduce compounding errors compared to single-step prediction models. However, the absence of intermediate states impedes accurate action decisions, particularly in complex, high-precision tasks. Latent space-enhanced models incorporate abstracted long-term information, such as high-"}, {"title": "2. Proposed DMWM Framework", "content": "In this section, we introduce the proposed DMWM framework with long-term imagination capabilities inspired by the dual-process theory of human cognition. DMWM consists of RSSM-S1 and LINN-S2. First, we introduce RSSM-S1, which builds upon the RSSM architecture to learn the environment dynamics in a latent space and perform fast, intuitive state representations and predictions. Next, based on the LINN, we introduce a novel LINN-S2 to capture the intricate logical relationships between the state space and the action space. By employing a hierarchical deep reasoning framework, LINN-S2 facilitates structured reasoning and enforces logical consistency over extended horizons. Finally, we provide an inter-system feedback mechanism. The pipeline of the proposed DMWM framework is shown in Figure 1."}, {"title": "2.1. RSSM-based System 1", "content": "The RSSM-S1 component is based on DreamerV3.\n\nEncoder:  $z_t \\sim q_\\phi(z_t | h_t, o_t)$,Deterministic State: $h_t = f_\\phi(h_{t-1}, z_{t-1}, a_{t-1})$,Stochastic State: $\\hat{z_t} \\sim p_\\phi (z_t | h_t)$,Reward Predictor: $r_t \\sim p_\\phi (r_t | h_t, z_t)$,Decoder: $\\hat{o_t} \\sim p_\\phi (o_t | h_t, z_t)$,\nwith the deterministic state $h_t$, observation $o_t$, predicted observation $\\hat{o_t}$, stochastic state $z_t$, predicted stochastic state $\\hat{z_t}$, action $a_t$ and the predicted reward $\\hat{r_t}$ at time step t. RSSM-S1 achieves an effective balance between deterministic and stochastic states, enabling efficient data-driven prediction similar to the intuitive and automatic processes of System 1. The deterministic state captures data patterns, and the stochastic state models inherent uncertainty and variability for dynamic and complex environments.\nSystem 1 loss. RSSM-S1 is optimized by using the loss functions of DreamerV3."}, {"title": "2.2. Logic-integrated neural network-based System 2", "content": "Next, we introduce the LINN-S2 component whose deep logical reasoning is illustrated in Figure 2. In LINN-S2, the states and actions are encoded as logic vector inputs, enabling logical deduction through operations such as negation (\u00ac), conjunction (\u2227), disjunction (\u2228), and implication (\u2192). To establish these logical operations, the LINN framework serves as the foundational module for capturing and reasoning about structural logical relationships between the state embedding space and the action embedding space within the world model. To further enhance the logical reasoning capabilities, we incorporate a set of regularization rules specifically designed for implication-based operations.\nNeural modules for basic logic operations. In logical reasoning for world models, it is essential to uncover structural information and semantic relationships across different embedding spaces, specifically the action space and the state space. proposed a straightforward concatenation-based method for LINN but it limits that input vectors are from the same source space. Moreover, the straightforward concatenation-based method overlooks the semantic disparities between states and actions, risking the loss of critical information and failing to capture complex cross-space logical relationships.\nTo address the need for cross-space logical reasoning in world models, we propose to explore the action embeddings and apply the Kronecker product for cross-space feature alignment, which preserves logic integrity and captures second-order relationships. The state (imagination) logical embedding v and the state logical embedding m are obtained with multilayer perception (MLP) by\n$v = W_v f (W_1(z) \\oplus b_1), m = W_m f (W_1(z \\oplus a) + b_1),$"}, {"content": "where $\\oplus$ is the operation of vector concatenation, and $z \\oplus a$ aims to capture the action logic within the state context. The formulations of operations (AND, OR, NOT) are given as follows\n$AND(v, m) = W_2f (W_1(v \\oplus m) + b_1) + Conv2D(v \\otimes m, K_d) + b_d,$,\n$OR(v, m) = W_2f (W_1(v \\oplus m) + b_1) + Conv2D(v \\otimes m, K_o) + b_o,$,\n$NOT(v) = \\neg v + W_nf (W_1v + b_1),$"}, {"content": "where $v \\in R^d, m \\in R^d, W_1 \\in R^{d \\times 2d}, W_2 \\in R^{d \\times d}, b \\in R^{d}, b' \\in R^{d}$ are the parameters of the logical neural networks, $l \\in {d, o, n}$, Conv2D(\u00b7) represents the convolution neural network, $K$ is the convolution core, $\\otimes$ is the operation of Kronecker product, and $f(\u00b7)$ is the activation function.\nIt is clear that logical operations capture intricate logical correlations that cannot be fully encapsulated by the geometric properties of the embedding space. For instance, the logical independence in NOT(v), reflected in the relationship between v and \u00acv, does not correspond precisely to vector orthogonality. Logical operations are governed by logical"}, {"title": "2.3. Inter-System Feedback Mechanisms", "content": "Feedback from S1 to S2. During the real-environment interactions, LINN-S2 updates the domain-specific logical relationships by using the actual state transitions from RSSM-S1. In particular, the state-action sequence ${z_t, a_t, z_{t+1}}_{t=0}^T$ generated by RSSM-S1 based on the observations ${O_t}_{t=0}^T$ serves as the labeled data, which is fed into LINN-S2 with the objective of minimizing the inference loss (12).\nFeedback from S2 to S1. To embed LINN-S2-inspired logical reasoning into RSSM-S1, we propose a logic feedback that utilizes LINN-S2's logical consistency mechanisms to guide RSSM-S1. By unifying high-level logical structures with low-level representations, the approach fosters a more robust and coherent model for imagination. We particularly introduce the logical rule and rederive the variational evidence lower bound (ELBO) with a logic inference term.\n$\\begin{aligned}p_\\phi(o_{1:T}, z_{1:T} | a_{1:T}) = \\prod_{t=1}^T p_y(o_t | z_t)p_\\phi(z_t | z_{t-1}, a_{t-1})\\\\=  \\prod_{t=1}^T \\frac{p_y(o_t | z_t) p_\\phi(z_t | z_{t-1}, a_{t-1}) C(\\phi(z_t, z_{t-1}, a_{t-1}))}{q_\\phi(z_t | o_{\\leq t}, a_{\\leq t})},\\end{aligned}$"}, {"title": "3. Experimental Results and Analysis", "content": "In this section, we conduct extensive experiments to address the following key questions: (a) Can our model effectively capture logical relationships in dynamic environments?, (b) Does enhanced logical consistency enable our model to achieve higher task rewards under limited environment trials and data?, and (c) Over an extended horizon, can our model generate reliable long-term imagination?\nExperimental setup. The training environments consist of 20 continuous control tasks from DeepMind control (DMC) suite. We evaluate DMWM using two model-based decision-making approaches: An actor-critic reinforcement learning method and a gradient-based model predictive control (Grad-MPC) approach , referred to as DMWM-AC and DMWM-GD, respectively. Training details for DMWM-AC and DMWM-GD are provided in Appendix B. For comparison purposes and benchmarking, we include DreamerV3 and Dreamer-enabled Grad-MPC as baselines. To highlight the limitations of using a single RSSM for world cognition, we compare our method against two state-of-the-art RSSM variants: Hieros and HRSSM . HRSSM improves representation robustness via masking and bisimulation to mitigate visual noise interference, while Hieros enhances long-term modeling and exploration efficiency through S5WM and hierarchical strategies. Details on environment and model settings are provided in Appendix C.\nLogical consistency. Figure 3 shows the logical correlations between individual state-action pairs (si\u2227 aj) and the target state (s30). The diagonal shows strong correlations for one-step pairs si \u2227ai \u2192 s30, which is the key path of localized reasoning. The off-diagonal elements show inter-dependency across different state-action pairs (si\u2227aj, i \u2260 j) that propagate global logical information. The observed patterns highlight the need for deep logical reasoning to capture both short-term and long-term logical dependencies.\nTable 2 compares the logical consistency of our proposed DMWM against various RSSM baselines on DMC tasks. Logical consistency data for 20 tasks with different horizon sizes is provided in Appendix G.1. The proposed DMWM achieves state-of-the-art logical consistency in both mean logic loss and stability across all DMC environments. DMWM respectively achieves 14.3%, 2.6% and 3.3% improvement of logic consistency compared to Dreamer, Hieros, and HRSSM. For instance, while the masking and hierarchical strategies in Hieros and HRSSM can reduce single-step propagation errors by mitigating environment noise, they still have difficulty in addressing long-term imagination due to predictive deviation in statistical inference and error accumulation. This highlights the limitations of"}, {"title": "4. Conclusion and Limitations", "content": "Conclusion. In this paper, we have proposed DMWM, a novel world model framework for reliable long-term imagination that combines the fast, intuitive-driven RSSM-S1 with the structured, logic-driven inference of LINN. The feedback mechanisms between the two systems enhanced the logic coherence and adaptability. Extensive evaluations across diverse benchmarks demonstrated significant improvements in logical consistency, data-efficiency and reliable imagination over extended horizon size.\nLimitations and future work. A key limitation of our approach lies in its reliance on predefined simple domain-specific logical rules, which restricts its adaptability to environments where such rules are ambiguous or constantly evolving. This dependency limits the framework's ability to generalize to novel tasks. Future work could focus on enabling the model to autonomously learn and adapt logical rules from data by exploring causal relationships. This, in turn, can reduce the need for explicit manual definitions and enhance flexibility in dynamic and complex environments."}, {"title": "A. Background", "content": "A.1. Actor-Critic\nThe actor-critic model is a widely used algorithmic framework in Reinforcement Learning (RL), which combines the advantages of policy gradient methods and value function approximation. It addresses RL tasks through the collaborative learning of two primary components: the Actor (policy network) and the Critic (value network), given as follows:\nAction model: $\\alpha_\\tau ~ q_\\nu (a_\\tau | s_\\tau)$,\nValue model: $v_\\psi(s_\\tau) \\approx E_{q(t > \\tau)}[\\sum_{t=\\tau}^{H}r_t]$,\nIn the world Model, the target of the actor-critic model is to maximize the reward over the imagined trajectories with the horizon size H. Specifically, the action model seeks to maximize an estimated value, while the value model strives to accurately predict the value estimate, which evolves as the action model updates. The training target of the Actor and the Value are given as follows (Sutton & Barto, 2018; Hafner et al., 2019a; SV et al., 2023).\n$V^* = max_{a_{t:t+H}} E_{q_{\\phi,\\theta}} \\sum_{\\tau=t}^{t+H} r_\\tau$,\n$\\psi^* = min_{\\psi} E_{q_{\\phi,\\theta}} \\sum_{\\tau=t}^{t+H-1} [(\\sum_{\\tau=t}^{t+H} r_\\tau) - V_\\psi(s_\\tau)]^2$,\n$V_{\\lambda}(s_\\tau) = (1 - \\lambda) V_N(s_\\tau) + \\lambda V^*_{\\lambda}(s_\\tau)$,\n$V_N(s_\\tau) = E_{q_{\\phi,\\theta}} [\\sum_{n=1}^{h-1} \\lambda^{n-1}r_{\\tau+n} + \\lambda^{H-1} V^N_{\\lambda}(s_\\tau)]$,\n$V^*_{\\lambda}(s_\\tau) = E_{q_{\\phi,\\theta}} [\\sum_{n=1}^{h-n} \\lambda^{n-1}r_{\\tau+n} + \\lambda^{H-h} V^N_{\\lambda}(s_\\tau)]$,\nwhere h = min(r + k, t + H), v represents the parameters of the Actor psi represents the parameters of the Critic, and \u03bb is the discount factor."}, {"title": "A.2. Model Predictive Control", "content": "Model predictive control (MPC) is an optimization-based control method extensively applied in engineering and industrial control systems. By leveraging the system's dynamic model, MPC predicts future behavior through rolling optimization, generating optimal control inputs at each time step to achieve the desired system objectives. However, due to the fact that MPC strongly relies on the system model and the requirement for online optimization, it has difficulty in effective decision-making performance if the world model fails to provide stable and reliable imagined trajectories. The gradient-based MPC framework for world model is presented as\n$a_{t:t+H}^* = \\text{argmax}_{a_{t:t+H}} R(s_t^{(j)}), R(s_t^{(j)} = \\sum_{\\tau=t+1}^{t+H+1} E[q_{\\phi}(r_{\\tau}|s_t^{(j)})]$\n$a_{t:t+H}^{(j)} ~ N(\\mu_t, \\text{diag}(\\sigma))$"}, {"title": "B. Algorithms", "content": "B.1. DMWM With Actor-Critic\nThe training process of DMWM with actor-critic-based decision module is shown in Algorithm 1.\nB.2. DMWM With MPC\nThe training process of DMWM with Grad-MPC-based decision-making module is shown in Algorithm 2."}, {"title": "C. Hyperparameters", "content": "C.1. Environment Setting\nThe action repeat setting of different environments is presented in TABLE 2.\nC.2. Model Setting\nThe hyperparameters of models are presented in TABLE 3."}, {"title": "D. Further Related Work", "content": "D.1. World Model\nWorld models are fundamental building blocks for model-based intelligent systems to make decisions, learn, and reason in complex environments. It enables prediction, efficient behavior planning through environment simulation, and strategy optimization using virtual simulations, reducing reliance on real-world trial and error. Existing world modeling frameworks can be categorized as Recurrent State-Space Model (RSSM), Generative Mdoel , and Large Language Model (LLM).\nThe Dreamer series lays an important foundation for general model-based reinforcement learning (MBRL) by continuously improving sample efficiency and task generalization ability through dynamic modeling and planning in the latent space. To adapt the RSSM-based world model for real-world environments, optimized confidence and entropy regularization for the gradient to mitigate discrepancies between virtual simulations and real-world environments. studied an object-centric world model, and introduced an object-state recurrent neural network (OS-RNN) to follow the object states. extended multimodal RSSM to support joint text and visual inputs. explored the multiple tasks with RSSM-based world models. utilized SimNorm to sparse and normalize the potential states, which projected the latent representations to simplices of fixed dimensions, thus mitigating the gradient explosion and improving the training stability. introduced masking-based latent reconstruction with a dual branch structure to handle the exogenous noise in the complex environment. introduced the hierarchical imagination with parallel processing and proposed efficient time-balanced sampling.\nThe diffusion model-based world model offers high-quality and diverse generations with temporal smoothness and consistency, achieved through denoising and time inversion processes. However, its reliance on multi-step denoising significantly slows downsampling, inference, and computation compared to RSSM, making it less suitable for real-time tasks and challenging to maintain long-term consistency. Additionally, unlike RSSM's latent space modeling, diffusion models cannot extract task-relevant features and accurately capture environment dynamics, particularly in complex environments with long-tailed distributions. The LLM-based world model serves as a powerful tool for complex task planning and execution through its logical reasoning capability and dynamic controllability and is able to realize task decomposition and cross-domain knowledge transfer through natural language instructions. However, the model cannot perform in dynamic modeling, and the generated states and actions often cannot accurately reflect the dynamic changes in the environment. In addition, its high reliance on linguistic representation may lead to insufficient quality of modal alignment, posing the risk of information loss or misinterpretation, while the significant consumption of computational resources during training and inference limits the application of LLM-based world models in resource-constrained environments.\nUnlike the aforementioned work, this work focuses on the long-term imagination of world models and proposes to enable the logical consistency of state representations and predictions over an extended horizon for the first time. We retain the efficient sampling and representation capabilities of RSSM as System 1 and integrate a logic-integrated neural network (LINN) as System 2 to imbue the world model with logic reasoning capabilities. The proposed DMWM thus delivers reliable and efficient long-term imagination with logical consistency, addressing a critical research gap in existing approaches."}, {"title": "D.2. Logic Neural Reasoning", "content": "Logic neural reasoning enhances the logical consistency, long-term imagination, and generalization ability of world models by embedding logical rules and reasoning capabilities. By combining the interpretability of symbolic logic with the expressive power of neural networks, it provides robust and rapid reasoning for complex task planning. However, explicitly modeling the logical structure of the world presents significant challenges due to the"}]}