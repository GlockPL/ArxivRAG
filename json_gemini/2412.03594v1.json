{"title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix Sharing and Throughput-oriented Token Batching", "authors": ["Zhen Zheng", "Xin Ji", "Taosong Fang", "Fanghao Zhou", "Chuanjie Liu", "Gang Peng"], "abstract": "Large language models (LLMs) increasingly play an important role in a wide range of information processing and management tasks. Many of these tasks are performed in large batches or even offline, and the performance indictor for which is throughput. These tasks usually show the characteristic of prefix sharing, where different prompt input can partially show the common prefix. However, the existing LLM inference engines tend to optimize the streaming requests and show limitations of supporting the large batched tasks with the prefix sharing characteristic. The existing solutions use the LRU-based cache to reuse the KV context of common prefix between requests. The KV context that are about to be reused may prematurely evicted with the implicit cache management. Even if not evicted, the lifetime of the shared KV context is extended since requests sharing the same context are not scheduled together, resulting in larger memory usage. These streaming oriented systems schedule the requests in the first-come-first-serve or similar order. As a result, the requests with larger ratio of decoding steps may be scheduled too late to be able to mix with the prefill chunks to increase the hardware utilization. Besides, the token and request number based batching can limit the size of token-batch, which keeps the GPU from saturating for the iterations dominated by decoding tokens. We propose BatchLLM to address the above problems. BatchLLM explicitly identifies the common prefixes globally. The requests sharing the same prefix will be scheduled together to reuse the KV context the best, which also shrinks the lifetime of common KV memory. BatchLLM reorders the requests and schedules the requests with larger ratio of decoding first to better mix the decoding tokens with the latter prefill chunks, and applies memory-centric token batching to enlarge the token-batch sizes, which helps to increase the GPU utilization. Finally, BatchLLM optimizes the prefix-shared Attention kernel with horizontal fusion to reduce tail effect and kernel launch overhead. Extensive evaluation shows that BatchLLM outperforms vLLM by 1.1\u00d7 to 2.0\u00d7 on a set of microbenchmarks and two typical industry workloads.", "sections": [{"title": "1 INTRODUCTION", "content": "The modern information processing and management tasks are starting to leverage large language models (LLMs) to achieve better results, such as the search engine [35, 38], advertisement [23], and recommender systems [19, 43]. Due to the huge amount of data to be processed, many of these LLM tasks are performed in large batches or even offline (e.g., offline ranking), for which the most important performance indicator is throughput. Besides, different input (prompts) of these tasks can partially share the same prefix (e.g., the same web document in the search engine tasks). Given the high cost of LLM inference, how to serve these tasks efficiently is a critical problem for the information processing systems.\nThe recent LLM inference engines [20, 32] can flexibly support the execution of LLM inference tasks, especially with blocked KV memory management [20] and per-iteration token batching [42]. However, the design of existing inference engines tend to optimize streaming online services and show limitations in throughput-oriented large-batch processing. 1) Lack of global prefix sharing. The state-of-the-art (SOTA) LLM inference engine [20] supports to cache the KV context with LRU policy to enable prefix sharing be-tween prompts. However, from a global perspective of large batches, an LRU-based cache may prematurely evict KV contexts that are about to be reused and retain unused KV contexts for a long time, causing unnecessary recalculation. 2) Suboptimal token batching for throughput-oriented and prefix-shared tasks. The current LLM inference engines schedule different requests independently and do not cluster the requests with common prefix together to schedule. It can extend the lifetime of the common prefix and thus increase the memory usage, and may prematurely evict KV contexts that are about to be reused as discussed above. Besides, they usually schedule the requests in the first-come-first-serve or similar order to guarantee the fairness and latency of the streaming requests. The requests with larger ratio of decoding steps may be scheduled too late to be able to mix with the prefill chunks to increase the hardware utilization. They apply the mix of prefill and decoding tokens together according to the threshold of token number and request number. This limits the number of decoding tokens that can be batched and keeps the GPU\u00b9 from saturating for the itera-tions dominated by decoding tokens. 3) Room for improvement in Attention. The SOTA prefix-shared Attention optimization can be further optimized with horizontal kernel fusion of the computation on different KV chunks, which is effective to reduce the tail effect and kernel launch overhead."}, {"title": "2 LLM INFERENCE BACKGROUND", "content": "2.1 LLM Inference and Its Performance Factors\nThe auto-regressive LLM inference usually consists of two phases: the prefill phase to process the input prompt, and the decoding phase to generate the output.\nThe prefill phase is usually compute-intensive and bounded by the massive computation. This is because the input tokens are all known ahead, and the computation of these tokens can be per-formed in parallel. The decoding phase is performed differently from the prefill phase: the output tokens are generated one by one and cannot be performed in parallel\u00b3. As a result, the decoding phase can be easily bounded by the memory access of the weight due to the memory wall problem [36, 37]. Enlarging the batch size can help to push the decoding computation to compute-bound, but the batch size may be limited by the GPU memory capacity. In real practice, the batching of the decoding can be suboptimal due to either the lack of memory space or the inefficient token batching, leading to low hardware utilization.\nAttention can be an important performance factor when the sequence is long. According to its algorithm pattern, the basic At-tention computation in decoding phase is the matrix-vector multi-plication between the (batched) vector Q and matrix KV containing all history context, as tokens from different requests have different KV context. Due to this characteristic, enlarging the batch size usually can not increase the compute utilization of the GPU as it does not increase the compute-intensity."}, {"title": "2.2 KV Cache and Prefix Sharing", "content": "Based on the nature of Attention calculation [34], the processing of each token (in both prefill and decoding phase) will generate the corresponding KV context. The computation of latter tokens consume the KV context of earlier tokens to perform Attention calculation. During the processing of a request (i.e., a prompt), the KV context is usually cached in memory for each of the processed token (called KV cache) so that the latter tokens can reuse the history KV context in memory for Attention calculation, rather than recomputing the history KV.\nThe KV cache can consume a large amount of GPU memory, which can limit the batch size of LLM inference and thus hurt the GPU hardware utilization of the linear layers consist of matrix mul-tiplication (MatMul) calculation. The KV memory size of a single token is $2 \\times nbytes \\times Nlayers \\times Nkvhead \\times dhead$ Bytes. Take Mistral 7B model [15] with grouped-query attention [3] as an example, which is a small size LLM, each token takes 0.125MiB memory if it uses float16 datatype for KV, and a sequence of 2K length will take 256MiB memory for the KV cache. An NVIDIA A10 GPU with"}, {"title": "2.3 Per-iteration Token Batching", "content": "Due to the dynamic output length of LLM execution, batching the tasks at the granularity of request results in a waste of the GPU resource, as some requests may finish earlier. Instead, the current LLM inference engines batche the tasks at the granularity of tokens. Specifically, it forms the token-batch at each iteration and allows to add new tokens into the batch dynamically [42], which is also called continuous batching. During the execution, the linear operators of different tokens will be concatenated together into a larger linear operator for execution at each iteration.\nRecent works [1, 13] split the long prefill into chunks and mix the prefill chunks with decoding tokens into the same token-batch to conquer the memory-wall problem of pure decoding batches, called SplitFuse or chunked-prefill. By inserting the prefill chunks into the decoding batches, the token number of the batch is enlarged effectively without introducing a lot of memory consumption. As a result, the compute-intensity of the token-batches are increased and the hardware utilization becomes better. Mixing the decod-ing tokens with prefill chunks is an effective way to increase the throughput, but can potentially increase the latency of the decod-ing tasks, thus preventing existing LLM inference engines from enabling chunked-prefill by default."}, {"title": "3 OPPORTUNITIES AND INSIGHT", "content": "3.1 Emerging Scenarios: Large Batched Through Oriented Tasks with Shared Prefix\nLLM has been increasingly used in a broad range of information processing and management tasks [5, 7, 19, 23, 35, 38, 43]. Due to the large traffic of these tasks and the high cost of LLM inference, many of these tasks are processed in very large batch (or even offline) whose performance metric is mainly the throughput rather than latency. Take the snippet generation task [35, 38] in search engine as an example, which is to generate the snippet of the web document (web content) in the search result page according to both document and user query. An example prompt in a previous work [35] is: generate a short snippet based on the given Document to answer the given Query. Document: <...> Query: <...>. Due to that it is hard to meet the SLO when performing the massive LLM inferences online, a common practice is to run the snippet generation task offline for the high-frequency web documents and queries, and retrieve the offline results during online serving when the corresponding document-query pair occurs.\nMany of the LLM-based information processing and manage-ment tasks show the characteristic of the shared prefix between the prompts of different requests, which comes from the nature that different tasks can perform on the same content (e.g., web document). Take the search engine task as an example, it may ex-tract different information from the same web document so that the document can be a shared prefix in the prompt. Specifically, as for the snippet generation task, given that it is to extract the snippet of the document-query pair, the document can be the common prefix if it appears for different queries.\nThe shared prefix can be managed in multiple levels. For example, the first level prefix can be the global system information and in-structions, and the second level can be the web document. However, with the development of LLM and the using of task-specific model fine-tuning, the prefix of system information and instructions be-comes shorter and shorter, and the length of the prefix will finally be dominated by the web document itself. On the one hand, the LLM models are incorporating more and more reasoning abilities into themselves (like OpenAI 01 [25]) and will not rely on complex prompts. On the other hand, the task-specific fine-tuning can help to incorporate the complex prompts into the model weight. As a result, the prefix sharing of the long context rather than the other instructions can be the most important for many of the information processing and management tasks. This motivates the first-level prefix enlargement in Sec.4.2."}, {"title": "3.2 The New Optimization Demands", "content": "As discussed in Sec.2, the existing works have proposed solutions to support prefix sharing and per-iteration token batching to support LLM inference. However, the state-of-the-art solutions still lack specialized optimization for the increasingly import large batched scenarios described in Sec.3.1, showing three main limitations:\n1. The implicit prefix caching cannot lead to the optimal KV reuse for the large batched inference. The existing LLM serving systems [20, 44] use the prefix tree to maintain the KV blocks. The same prefix (identified by runtime hashing) can be"}, {"title": "3.3 Insight", "content": "To optimize the throughput oriented tasks described in Sec.3.1 and address the limitations described in Sec.3.2, by leveraging the char-acteristics of the large batch, BatchLLM has the following insights.\n1. Explicit prefix identification and sharing. Instead of hash-ing and matching the common prefix at runtime and manage the prefix with LRU cache, BatchLLM explicitly identifies the common prefix globally within the large batch ahead-of-time, which will not miss the opportunity of prefix sharing caused by the implicit cache. Besides, BatchLLM refactors the prefix tree with a Dynamic Programming algorithm to enlarge the first-level prefix to avoid the system complexity and kernel overhead of the multi-level prefix. Details are described in Sec.4.2.\n2. Grouped scheduling, request reordering and memory-centric token batching. BatchLLM schedules the requests at the granularity of a group of requests sharing the common prefix, which makes the prefix sharing convenient and shrinks the lifetime of the prefix's KV memory. BatchLLM reorders the requests according to the length of the prompt and the estimated decoding length. As indicated in Fig.1, BatchLLM will schedule the requests with larger ratio of decoding length to prompt length with higher priority. In"}, {"title": "4 DESIGN METHODOLOGY", "content": "4.1 Overview\nFig.3 shows the overview of BatchLLM optimizations. First, it ana-lyzes the large batch of input prompts and identifies the common prefix explicitly. This is done before the scheduling of the requests. It simplifies the multi-level prefix into a single level prefix with the insight that the prefixes of the information processing and manage-ment are usually dominated by a long context (e.g., web document), as discussed in Sec.3.1. The explicit prefix processing is illustrated in Sec.4.2. The requests will be organized into groups where each group corresponds to the quests sharing the same prefix. The group will be the basic unit of task scheduling. Before scheduling the groups, BatchLLM will also reorder the groups according to the ratio of prefill length and estimated decoding length and postpone the groups with longer prefill and shorter decoding. It then forms the token-batches with the consideration of the KV memory usage. This aims to better mix the decoding steps with the prefill chunks to increase the overall token-batch size. The throughput-oriented scheduling and token-batching optimization is described in Sec.4.3. BatchLLM also implements the horizontal fused Attention kernel to reduce the tail effect and kernel launch overhead, described in Sec.4.4. We have integrated the above optimization into the state-of-the-art LLM inference engine (vLLM [20])."}, {"title": "4.2 Explicit Global KV Reuse Identification", "content": "As discussed in Sec.3.1, different prompts may have multiple levels of prefixes. The multi-level prefixes can lead to system challenges and overhead of token batching and fused Attention kernel. For the token batching, it requires to manage the dependencies of the dif-ferent levels of prefixes. For the Attention kernel, it will introduce"}, {"title": "4.3 Throughput-oriented Token Batching", "content": "4.3.1 Prefix-sharing Group Based Scheduling. As described in Sec.4.2, BatchLLM identifies the common prefix explicitly ahead-of-time. To simplify the KV reuse and reduce the lifetime of the common prefix in the memory, BatchLLM schedules the requests sharing the same prefix all together. Specifically, the procedure described in Sec.4.2 will generate the prefix-sharing groups and BatchLLM will schedule the requests at the granularity of prefix-sharing group rather than each single request. A prefix-sharing group corresponds to the collection of requests sharing the same prefix. In this way, the requests sharing the same prefix can start at the same time and the memory of the common prefix can be released as soon as the group is completed. This differs from the LRU cache and reference counter based prefix management [20, 44] and can make sure all"}, {"title": "4.3.2 Request Groups Reordering", "content": "As discussed in Sec.3.3, Batch-LLM reorders the input requests to schedule the requests according to the ratio of prompt length to decoding length (R). We define the ratio R as:\n$R = \\frac{Ldecode}{Lprefill}$ (3)\nIn Eq.3, $Lprefill$ is the number of prefill tokens and $Ldecode$ is the length of the output tokens. The requests with larger R will be scheduled earlier. This helps to issue the requests with long decod-ing steps earlier and make the latter prompts's length larger and thus can be better mixed with the previous decoding tokens to en-large the token-batch size (example in Fig.1). The challenge is that the exact number of output tokens is not known before finishing the execution. We profiled the dataset of several typical information processing tasks (snippet generation, offline ranking, ads, etc.) and observed that the distribution of output length is relatively concen-trated for a task. We thus assume the output length is constant and normalize $Lprefill$ into 1 in Eq.3 for all requests uniformly. As we schedule the requests in the unit of prefix-sharing group, the ratio of the group (after normalizing the length of output) is\n$Rgroup = \\frac{1}{Lprefix + \\sum Ldistinct}$ (4)\nAccording to Eq.4, the request groups with larger Rgroup will be scheduled with higher priority."}, {"title": "4.3.3 Memory-centric Token Batching to Saturate GPU", "content": "The request group reordering described above helps to reduce the \"valleys\" in the timeline of the token numbers (\"valley\" example in Fig.2) by providing more chances for the decoding tokens to be able to find the prefill chunks to mix together. Another limiter of the size of token-batches is that, the existing works [20] use a fixed request number to limit the token batching, i.e., the request number cannot exceed a fixed threshold within a token-batch. For a token-batch dominated by the decoding tokens, it is easy to reach the upper bound of the request number while still have not achieved a large number of the total tokens. For example, vLLM sets the threshold of request number as 256 by default, if a token-batch already has 256 decoding tokens, it will have no chance to add more prefill"}, {"title": "4.4 Prefix-shared Attention Optimization", "content": "As described in Sec.3.2, the existing works use separate kernels to compute the partial Attention on the prefix and the distinct KV context. This will lead to the tail effect of each kernel and the launch overhead of these kernels. BatchLLM horizontally fuses the two partial Attention calculation into one kernel. Different parts of the computation are performed in different thread blocks. Note that the problem shape of the two parts are different. BatchLLM applies different tiling configuration for the two parts to achieve the best performance. It uses the common auto-tuning method to find the best configuration.\nThe horizontal fusion itself is not novel, but can effectively help to increase the performance of the prefix-shared Attention."}, {"title": "5 IMPLEMENTATION", "content": "We implement the prefix identification (Sec.4.2) in Python as a stan-dalone preprocessing script. It accepts a list of inputs and generate a list of prefix-sharing groups. The inputs can be the nested input ids (e.g., [<list of input 0 tokens>, <list of input 1 tokens>, ...]), and each prefix-sharing group is represented as a tuple of the prefix and the corresponding distinct prompts (e.g., tuple(<list of prefix tokens>, [<list of distinct 0 tokens>, <list of distinct 1 tokens>, ...])).\nWe integrate the token batching optimization (Sec.4.3) in vLLM [20] v0.5.4. Specifically, we customize the vLLM to accept the list of prefix-sharing group tuples generated by the preprocessing script, and implement the group-wised scheduling and token batching logic upon the vLLM token batching function.\nFinally, we integrate the prefix-shared Attention optimization (Sec.4.4) as the Attention kernel backend of the customized vLLM. We implement the Attention in BatchLLM through OpenAI Triton language [33]. The limitation of the Triton based implementation is that, the performance of Triton version of FlashAttention is still worse than that of CUDA version (Sec.6.3.3). However, the advantage is that Triton supports multiple platforms (both NVIDIA and AMD GPUs) so that BatchLLM implementation can run on both NVIDIA and AMD GPUs. We use the builtin autotuner of Triton to tune the tiling size of the Attention implementation in BatchLLM. Besides that, some ahead-of-time compilation methods are applied to reduce the launching overhead, including warm-up process on NVIDIA GPUs, and AOTriton on AMD GPUs."}, {"title": "6 EVALUATION", "content": "6.1 Setup\nBaseline Specification. We compare BatchLLM with vLLM, the state-of-the-art LLM inference engine with prefix sharing, for the end-to-end comparison to demonstrate the efficacy (Sec.6.2). We integrate all our design in vLLM v0.5.4 and the baseline for com-parison is also vLLM v0.5.4 (except as noted). We use FlashAtten-tionn [6] (based on v2.6.1) as the Attention backend of the vLLM baseline in NVIDIA GPU environments.\nWe enable the prefix-caching and chunked-prefill in vLLM base-line for the end-to-end comparison in Sec.6.2. The prefix-caching uses the implicit LRU-based caching policy. We use 2048 as the token-batch size when enabling the chunked-prefill of vLLM base-line, which is a recommended size to achieve its better throughput according to the vLLM community. Note that the prefix-caching and chunked-prefill are still not supported to be enabled together in v0.5.4. To make a fair comparison, we optimized the code of vLLM 0.5.4 baseline to make it able to enable the two techniques together, which makes the vLLM baseline have better performance in Sec.6.2.\nFor the kernel comparison in Sec.6.3.3, we compare BatchLLM with Cascade-Inference [41], which is the SOTA implementation of the prefix-shared Attention on NVIDIA platform. It is worth"}, {"title": "6.2 End to End Comparison", "content": "6.2.1 Microbenchmark Evaluation. Fig.5 shows the end-to-end through-put comparison between BatchLLM and the baselines. In these benchmark experiments, we compare BatchLLM with vLLM of dif-ferent configurations, under the FlashAttention backend [6], and with or without prefix-caching and chunked-prefill enabled.\nThe results show that BatchLLM outperforms the vLLM base-line under different configurations, for different models on differ-ent GPU platforms with different input data distributions. Specifi-cally, when comparing with vLLM + prefix-caching + chunked-prefill,"}, {"title": "6.2.2 Case Study 1: Long Common Prefix with Short Distinct Prompt", "content": "This industry workload is similar to the snippet generation task described in the recent work [35], which is also described in Sec.3.1. The original input has two levels of prefixes: the global shared instruction to extract the information from the document-query pair, and the shared document between different groups of queries. The first-level prefix is very short. With the optimization described in Sec.4.2, the first-level prefixes are enlarged (i.e., some of the second-level prefix of the document is merged into the first-level prefix). We have analyzed the token saving ratio according to Eq.2. It shows that the saving ratio are nearly the same between the multi-level and the enlarged single level prefixes, 56% for the former and 55% for the latter.\nWe synthesize the dataset for the evaluation according to the data distribution of the industry workload, where the average share-degree (i.e., the number of the distinct prompt that share the same prefix) is about 7, the average prefix length is about 1100 tokens, and the average distinct prompt length is about 400 tokens. We sample 3000 queries for this case study.\nWe conduct this experiment on the NVIDIA A100 GPU, using Mistral-7B model. Table.1 shows the effectiveness of BatchLLM of this workload (and another industry workload that will describe later), with about 1.3\u00d7 speedup over the best configuration of vLLM. BatchLLM better reuses the KV context than the vLLM baseline with the optimization in Sec.4.2, for which we have the breakdown analysis in Sec.6.3.1. BatchLLM also better mixes the decoding tokens with prefill tokens to increase the size of token-batches with the optimization in Sec.4.3, for which we have the breakdown analyze in Sec.6.3.2. It also benefits from the optimized Attention kernel described in this paper."}, {"title": "6.2.3 Case Study 2: Short Prefix with Long Distinct Prompt", "content": "We have evaluated another industry workload with different data dis-tribution to that in Sec.6.2.2. This is a ranking job that ranks a set"}, {"title": "6.3 Breakdown Analysis", "content": "6.3.1 Reusing Effect Comparison. We compare the saving ratio (refer to Eq.2) between vLLM's prefix-caching and BatchLLM on the industry task in Sec.6.2.2. The saving ratio of vLLM's prefix-caching is 44.2%, and BatchLLM achieves 54.9% for this metric, showing that the explicit prefix identification can help to reuse more KV context. Note that we only use 3000 requests to evaluate the saving ratio. With the increase of the requests number, it can be expected that the implicit LRU cache policy can suffer more from the eviction of the reusable KV context."}, {"title": "6.3.2 Token-batching Analysis", "content": "Fig.6 shows the per-iteration token number, using the same workload with Fig.2. It clearly shows that the token-batching optimization in BatchLLM successfully reduces the \"valleys\" of the iterations, thus can increase the overall GPU utilization.\nTo investigate the transferability of token-batching optimization in Sec.4.3, we conducted ablation experiments on the memory-centric token-batching and group-based reordering/scheduling, as"}, {"title": "6.3.3 Kernel Performance Comparison", "content": "We compare BatchLLM's prefix-shared Attention implementation with several baselines on both NVIDIA A100 GPU and AMD MI200 GPU. Fig.8 shows the performance comparison of different implementations, including Triton official FlashAttention implementation, official FlashAtten-tion, FlashinferCascade [41], and BatchLLM's kernels. We com-pare the performance among different kernels under two scenarios: the pure decoding scenario (request count 256) and the chunked-prefill scenario where a token-batch includes both prefill chunks and decoding tokens (7 prefill requests and 256 decoding requests). Besides that, how many requests one single prefix-group have, and various length setting of both shared prefix and non-shared tokens, are non-trivial. Our experiments are conducted based on these insights."}, {"title": "6.3.4 Global Prefix Identification Overhead", "content": "We have measured the time of the global prefix identification described in Sec.4.2. The time range of the overhead starts before adding the token ids of each prompt to form the basic prefix tree and ends after generating the final list of prefix-sharing groups. As for the industry workload in Sec.6.2.2, the total overhead of the 3000 requests is 0.28 seconds. While the time to process the 3000 requests with the input of prefix-sharing groups format is 233.46 seconds. The global prefix identification procedure nearly has no overhead compared with the end-to-end execution time."}, {"title": "7 RELATED WORK", "content": "Prefix sharing systems. The vLLM [20] proposes the PagedAt-tention to manage the KV memory in blocks, which enables to reuse the KV context both intra and inter requests in memory block level. Prompt Cache [12] and SGLang [44] propose the domain specific language (DSL) to define the shared prefix of prompt. Be-sides, SGLang proposes the radix tree based KV cache management with LRU policy for KV memory eviction. RAGCache [16] stud-ies the common KV caching for RAG optimization. Some recent works [10, 14, 17, 26, 28] study the distributed request scheduling with the consideration of prompt prefix reuse. None of these works identify the common prefix ahead-of-time for the large batch and enable the explicit reusing. Instead, BatchLLM manages the prefix reusing explicitly and can reuse the KV context the best for the large batched scenario.\nToken batching and serving optimization. Orca [42] pro-poses the continuous batching method for transformer models with the per-iteration token batching of auto-regressive models [11]. FastGen [13] and SARATHI [1] study the mix of of prefill chunks and decoding tokens into the same token-batch to increase the compute intensity of the batch. FlexGen [31] proposes the swizzled token computation and offloading to support the LLM inference with limited GPU memory. Some works [9, 30] study the request scheduling to achieve better SLO for online LLM serving. BatchLLM differs from these works as it targets the large batched inference, leveraging the static information to reorder the requests and using the memory-aware scheduling to enlarge the size of token-batch.\nThe vLLM [20] uses the multi-step scheduling method to reduce the token-batch scheduling overhead, which is orthogonal to Batch-LLM. Another orthogonal dimension to improve the token-batch performance is the pruning and quantization [8, 22, 36], which can be applied concurrently with the optimizations in this paper.\nAttention kernel optimization. Memory-efficient Attention [29] and FlashAttention [6] fuse the Attention operators into a single kernel to boost the performance, with Online Softmax [24] to tile the Attention computation. The recent prefix-shared Attention optimization (ChunkAttention [40], RelayAttention [45], Hydra-gen [18], and Cascade Inference [41]) compute the attention on the prefix and other part separately in different kernels, for which the former is transformed from matrix-vector multiplication between Q and K/V into MatMul, and reduce the results of different parts according to the Online Softmax algorithm. Different from these works, BatchLLM fuses the Attention computation of different parts into the same kernel to reduce tail latency and kernel launch over-head. The horizontal fusion has been proposed and used in the existing machine learning optimizers [21, 27]. BatchLLM borrows this idea and applies it in the prefix-shared Attention."}, {"title": "8 CONCLUSION", "content": "This paper presents BatchLLM, the holistic optimization techniques for large batched LLM-based information processing and manage-ment tasks. It identifies the limitations of existing methods, and optimizes the tasks with the global information of the large batch. It explicitly extracts the common prefix globally to avoid prefix's early eviction problem, and simplifies the prefix pattern by en-larging the first-level prefix with a DP algorithm on the tree to reduce the overhead of scheduling and Attention computation. It schedules the requests at the granularity of prefix-sharing groups, which enables the global prefix sharing the best and shrinks the lifetime of prefix's KV memory. It proposes the request reordering and memory-centric token batching method to better mix the prefill chunks into the decoding token-batches and thus better saturates the GPU. Finally, it presents the horizontal fused prefix-shared At-tention kernel to reduce the tail effect and kernel launch overhead. Extensive evaluation shows that BatchLLM outperforms vLLM by 1.1x to 2.0x on a set of microbenchmarks and two typical industry workloads."}]}