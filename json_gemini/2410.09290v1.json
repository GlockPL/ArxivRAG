{"title": "Ranking over Regression for Bayesian Optimization and Molecule Selection", "authors": ["Gary Tom", "Stanley Lo", "Samantha Corapi", "Al\u00e1n Aspuru-Guzik", "Benjamin Sanchez-Lengeling"], "abstract": "Bayesian optimization (BO) has become an indispensable tool for autonomous decision-making across diverse applications from autonomous vehicle control to accelerated drug and materials discovery. With the growing interest in self-driving laboratories, BO of chemical systems is crucial for machine learning (ML) guided experimental planning. Typically, BO employs a regression surrogate model to predict the distribution of unseen parts of the search space. However, for the selection of molecules, picking the top candidates with respect to a distribution, the relative ordering of their properties may be more important than their exact values. In this paper, we introduce Rank-based Bayesian Optimization (RBO), which utilizes a ranking model as the surrogate. We present a comprehensive investigation of RBO's optimization performance compared to conventional BO on various chemical datasets. Our results demonstrate similar or improved optimization performance using ranking models, particularly for datasets with rough structure-property landscapes and activity cliffs. Furthermore, we observe a high correlation between the surrogate ranking ability and BO performance, and this ability is maintained even at early iterations of BO optimization when using ranking surrogate models. We conclude that RBO is an effective alternative to regression-based BO, especially for optimizing novel chemical compounds.", "sections": [{"title": "Introduction", "content": "In the age of increasing data complexity and expensive computations, Bayesian optimization (BO) [1] emerges as a powerful tool to find optimal outcomes with minimal evaluations. By efficiently navigating complex search spaces, BO accelerates discovery and optimization across diverse scientific and engineering applications. In recent years, BO has been used in many applications in chemistry, such as in drug discovery [2-4] and materials design [5-10], accelerating the rate of discovery for useful molecules, materials, and pharmaceuticals. Leveraging machine learning models, information extracted from available observations is extrapolated to unseen portions of the search space. This is particularly useful when the search space is extremely large [2, 11], or when observations are expensive to evaluate and resource intensive, such as in high-quality chemical simulations, clinical trials, or laboratory experimentation. When coupled with automated high-throughput synthesis and characterization platforms, BO algorithms become a major component of experiment planning and decision-making in self-driving laboratories [12, 13].\nUnderlying the machinery of BO, machine learning (ML) and deep learning (DL) regression models act as surrogates for complex quantitative structure-property relationships (QSPR) [14\u201316]. Probabilistic models are typically preferred for the provided uncertainties for each prediction, and overall robustness to overfitting [17-19]. An acquisition function dictates the strategy of exploration or exploitation based on the surrogate results, and the candidates are evaluated in the subsequent iteration. However, in the early stages of BO and scientific discovery there is often a limited amount of high-quality data which does not work well with DL regression models that struggle to learn the QSPR in the low-data regime, especially when compared to traditional ML methods, such as boosted random forest, and Gaussian processes [20].\nAdditionally, when considering the optimization of the functional space of chemicals, the roughness of the space may present challenges to regression surrogate models. Extensive studies have been done looking at the roughness of chemical datasets, and the effect on the modellability of the QSPR using regression models [21-25]. Increasing functional space roughness, and the presence of activity cliffs- small changes in the molecular structure corresponding to high fluctuations in the property-reduces the predictive accuracy of regression models [26, 27]. Although smooth functional landscapes are best for regression modellability, many optimal compounds are often found at the activity cliffs [28-30], which are prevalent in many experimental data due to the complex physical effects that determine properties such as reactivity of a compound [31], or the potency of a drug compound [32].\nIn this study, we consider the use of DL ranking models as an alternative to regression models for surrogates in BO. Unlike regression models, ranking models are trained to learn the relative ordering of candidates, rather than the exact target values [33, 34]. Ranking models have often been used in the context of efficient information retrieval, recommendation systems, and preference optimization, in which the most relevant results are prioritized [35\u201338]. In Rank-based Bayesian Optimization (RBO) for chemical systems, the surrogate model does not need to accurately learn the QSPR and instead"}, {"title": "Methods", "content": "In RBO, we test the optimization performance of some common DL models as surrogates: multi-layer perceptron (MLP) [39, 40], bayesian neural network (BNN) [41], and graph neural network (GNN) [42]. Additionally, we compare the results with a Gaussian process (GP) [43] surrogate, commonly used for low-data BO. The molecules are represented using the Morgan extended-connectivity fingerprint (ECFP) representation [44], a 2048 dimensional bit-vector hashed from local structures of radius 3 in the molecular graph, as implemented in cheminformatics software RDKit [45]. For the GNN, the molecules are represented as graphs with atoms as nodes and bonds as edges, along with node and edge features as defined in the Open Graph Benchmark [46].\nThe GNN, BNN and the GP are all probabilistic models, while the MLP is deterministic. The GNN is based on the ChemProp architecture [47, 48], with two message-passing layers operating and a final variational inference Bayesian layer to produce an uncertainty in the prediction [49]. The same variational Bayesian layer is used to build the BNN, with two hidden layers each with 100 nodes and the rectified linear activation function. The MLP has the same architecture as the BNN, but is composed on fully connected feed-forward layers. The DL models are implemented in PyTorch [50], and the GNN in PyTorch Geometric [51, 52].\nA GP defines a distribution over functions, characterized by a mean and covariance kernel function. Here, we use the Tanimoto distance kernel, which is effective when using Morgan fingerprint representations of molecules [53, 20]. Uncertainty in predictions is inherent in GPs and is represented by the variance of the posterior distribution over predicted outputs. Unlike the deep Bayesian layers, the GP posterior is directly inferred, without variational inference or approximations. The GP is implemented using GPyTorch [54] and GAUCHE [55]."}, {"title": "Loss", "content": "For the regression models, the DL models are trained with a mean squared error (MSE) loss. The probabilistic GNN and BNN models have an additional regularization term from the KL divergence over the weight distributions of the variational layers. GPs are trained by minimizing the negative marginal log-likelihood (MLL).\nThe ranking loss used for the LTR task is the pairwise marginal ranking loss. Unlike a point-wise loss like MSE, which maps a scalar prediction and ground truth to a scalar loss value (R \u00d7 R \u2192 R), a pairwise loss maps a pair of predictions and a pair of ground truths to a scalar loss (R2 \u00d7 R2 \u2192 R). The pairwise marginal ranking loss has the form\n$L(Y_1, Y_2, \\hat{y}_1, \\hat{y}_2) = max (0, -sign(y_1 - Y_2) * (\\hat{y}_1 \u2212 \\hat{y}_2) + m),$\nbecomes a molecule-selector. At the same time, the BO loop retrieves the most relevant molecules for further experimentation (Figure 1). In addition, the relative order of fitness of compounds are not affected by outliers and activity cliffs, effectively reducing sharp changes in the functional space for ranking models.\nTo test the performance of the proposed RBO, we will simulate BO-guided molecular discovery on chemical datasets with varying roughnesses and compare the performance of the optimization between DL models trained with regression loss and ranking loss, as well as with traditionally used regression GP models. We will also compare the predictive and ranking abilities of the models throughout the campaign to the BO optimization performance. Our contributions and findings are summarized below:\n\u2022 Novel application of probabilistic ranking models in the BO of molecular datasets. All code and data provided here: https://github.com/gkwt/rbo\n\u2022 Demonstrate similar or improved BO performance of rough chemical datasets with deep ranking models when compared with regression models. In some cases, deep ranking models outperform GP surrogates.\n\u2022 Ranking performance is a more important metric of surrogate BO ability than regression performance.\n\u2022 Ranking loss enables DL models to achieve better ranking performance than regression loss, particularly at low-data regimes early in the optimization."}, {"title": "Datasets", "content": "The regression and ranking models are tested on the optimization of a variety of datasets. To test the robustness of optimization, we follow the steps of Aldeghi et al. [24] in generating a series of datasets with varying degrees of roughness. Samples of 2000 molecules are taken from the ZINC 250k dataset [5], a subset of the ZINC15 database [56] of drug-like molecules. The properties for optimization are derived from 12 oracles from the Therapeutics Data Commons (PyTDC) [57, 58], including 10 from the GuacaMol benchmark [59], the LogP solubility measure [60], and the Quantitative Estimate of Drug-likeness (QED) measure [61]. The roughness of the datasets is quantified by the Roughness Index (ROGI). ROGI measures how quickly the dispersion of a dataset changes as it is clustered by the pairwise distance between all molecules in the dataset across different distance thresholds [62, 63]. Although there is no definitive metric for roughness, this index has been shown to correlate well with out-of-sample ML model error better than other existing indices (i.e. Structure-Activity Landscape Index [22], Structure-Activity Relationship Index [21], Modellability Index [23]). Hence, we evaluate the RBO approach across chemistry datasets with varying roughness levels as measured by ROGI.\nAdditionally, we study the effects of activity cliffs on the BO performance. MoleculeACE [27] provides a curated set of datasets derived from the ChEMBL dataset (v29) [64] with highly rugged functional space and activity cliffs. This is particularly important when considering BO of unexplored chemical space, or for optimization of novel characterization methods with highly complex relationships between feature space and functional space. The curated datasets involve the minimization of the inhibitor constant Ki (the concentration required for half-maximal inhibition of a target protein) and the half-maximal effective concentration EC50 (the concentration required for half-maximal response of the drug), which both measure the effectiveness of drug molecules.\nFor targets that are not already normalized, we perform additional scaling by subtracting the median value and dividing by the inter-quartile range of the distribution of targets. This is similar to standard scaling, but does not assume a normal distribution, and is more robust to outliers."}, {"title": "Bayesian optimization", "content": "The different models are used as surrogates in Bayesian optimization of various chemical datasets (Figure 2) to find the molecule with the optimal property within the dataset with as few iterations as possible. The surrogates are trained on a subset (the measured set) of the dataset first, producing a series of predictions on the unseen part of the dataset. The candidates are then scored by an acquisition function and selected by $X_{next} = arg max a(\\hat{y}(x), \\hat{\\sigma}(x))$, where \u0177 and \u00f4 are the mean and standard deviation of the prediction on molecule x.\nFor probabilistic models, the predictions include an uncertainty, which are incorporated into an acquisition function. Here, we study the optimization with the upper-confidence bound\n$Q_{UCB}(\\tilde{y}, \\sigma) = \\tilde{y} + \\beta\\sigma,$\nwhere \u1e9e is a hyperparameter for controlling the exploration of the BO algorithm. We set \u03b2 = 0.3. The MLP is deterministic and does not produce a posterior distribution. The predictions are maximized directly in a greedy approach. Results on the expected improvement acquisition function are shown in the Supplementary Materials.\nThe BO campaigns start with 10 initial randomly sampled data points, followed by 100 additional evaluations. The surrogate models are re-trained every 5 evaluations for up to 100 epochs with early stopping, with the validation loss monitored on a 90/10 training/validation random split of the measured set. The optimization uses the Adam algorithm [65] with learning rate 0.001 for the DL models, and 0.01 for the GP. Statistics are obtained from 20 differently seeded BO runs. It takes at most 1 hour to complete 20 campaigns for a particular surrogate model on 1 GPU (NVIDIA GeForce RTX 2080), 4 CPUs (Intel Xeon Gold 5217), and 16GB of memory."}, {"title": "Results", "content": "To quantify the BO performance, we use the area under curve (AUC) of the fraction of top 100 molecules found as a function of BO evaluations, normalized by the budget. The BO-AUC metric rewards surrogate models that identify the most top-performing molecules in the earlier iterations of the campaign. Additionally, throughout the optimization campaigns, the surrogate model performance is evaluated on a held-out test set (15% of the entire dataset). For this, we measure the performance of the model through the ranking Kendall tau correlation coefficient, and the R\u00b2 coefficient of determination, at each BO iteration.\nRanking gives faster and better optimization"}, {"title": "The effect of dataset roughness on ranking and regression", "content": "Following the procedures of Aldeghi et al. [24], the BO performance can be correlated to the roughness of the dataset using the ROGI score. Higher ROGI scores correspond to rougher datasets. The constructed datasets are from the same chemical space, sampled from ZINC, and contain the same number of data points, making the comparison fair across the different properties. The BO-AUC of the BO campaigns compared to the ROGI of the ZINC datasets are shown in Figure 3, with additional comparison to the performance of a GP regression surrogate. We see a general trend of decreasing optimization performance with increasing ROGI for all surrogates. As observed before, the ranking models often perform better than the DL regression models. The optimization performance of the GP models suffers on datasets with higher ROGI, with DL models occasionally achieving similar BO-AUC. Still, for smoother datasets, the GPs are the surrogates of choice.\nWe also compare the relative performances of the BNN, GNN, MLP and GP surrogates on the MoleculeACE datasets by performing the Student's t-test on the achieved BO-AUC metrics (Figure 4). Individual BO-metrics achieved on each dataset is provided in the Supplementary Materials. On average, the DL ranking models serve as better surrogates when compared to the same model trained with MSE loss. While the GNN model struggles as a surrogate for all MoleculeACE datasets when compared to the GP model, the ranking model still readily improves upon the regression model. In the original benchmarking work of MoleculeACE, van Tilborg et al. [27] studied the regression of the same datasets using DL models, and found that GNNs had the worst performance on activity cliffs, while MLPs had the best performance. This corresponds well with our findings on the poor BO performance of GNN surrogates on the MoleculeACE datasets.\nFor the feed-forward BNN and MLP models, BO performance is similar to, if not better than, GP surrogates, for all MoleculeACE datasets. This is surprising, as traditional ML methods such as random forest and GPs are typically preferred over DL methods, which tend to be over-parameterized and overfit in the low-data regimes. This suggests that GPs struggle with the prediction of activity cliffs. However, between ranking and regression BNN and MLP, ranking model is still the model of choice, offering better performance on more datasets than both GPs and their regression counterparts. Similar to the results observed on the ZINC datasets, ranking models are more robust to rough functional space, activity cliffs, and outliers."}, {"title": "Ranking performance is better correlated to better selection", "content": "To elucidate the correlations between BO performance and the surrogate models' predictive and ranking abilities, the models are evaluated on a held-out test set at each BO iteration. The Pearson correlation coefficient is used to examine the relationship across the datasets between the fraction of top 100 molecules observed at the end of the campaigns and the final surrogate test performance. For regressive performance, even for the models trained with the MSE loss and the GP regressor, the R2 metric has a tenuous relationship to the BO performance, with the Pearson test failing to produce any correlations with p < 0.05.\nConversely, we find a positive correlation in the model ranking performance, evaluated by the test Kendall tau statistic, and the BO performance for both the regression and ranking surrogate models (Table 2). Stronger correlations are found across the ZINC datasets, while the MoleculeACE datasets exhibit lower, but positive correlation, likely an effect of the activity cliffs in these datasets. Additionally, with the exception of the GNN models on the ZINC datasets, the ranking surrogate models typically achieve a higher Pearson correlation between the test Kendall tau and the BO performance indicator. We conclude that the ranking performance of the surrogate model is a stronger indicator of performance as a BO surrogate.\nWe further examine the surrogate model test ranking performance at two different BO iterations, averaged across the three sets of datasets (Figure 5). As expected, the average test ranking statistic"}, {"title": "Discussion", "content": "Despite the promising results, there are several limitations and open questions that warrant further investigation:\n\u2022 Scope: Our study looks at synthetically generated datasets from ZINC with a diverse and well-utilized set of models and representations, aiming for a fair comparison. Model performance can change with carefully selected hyperparameters and alternative representations. More works needs to be done on larger datasets and non-synthetic properties.\n\u2022 Training complexity and other ranking losses: Ranking models require pairwise comparisons, which can increase training time. Future research could explore more efficient training methods, such as list-wise comparisons [66, 67]. There are also a variety of other ranking losses that can be studied.\n\u2022 Multi-task models: RBO leverages the ordinal nature of continuous numbers, which may not be the best strategy when considering multi-task optimization problems. Further study can be done on the optimization of multiple objectives through other scalarizations [68, 69], or pareto-optimal methods [70-72].\n\u2022 Broader applicability: While RBO showed success in the specific context of molecular selection, its applicability to other domains and types of data should be explored. For example, some success has been demonstrated in BO of ML model hyperparameter searches [73].\n\u2022 Uncertainty quantification: We do not study the calibration of uncertainties [74, 75], which is important to BO performance [76, 20, 77]. There is less work on estimating uncertainty of rank scores [78]. We also do not utilize experimental error of the properties which could be used to better evaluate performance of models.\n\u2022 Comparison to other selection strategies: a more comprehensive study can look at ranking in the context of other optimization strategies."}, {"title": "Conclusion", "content": "This study introduces RBO as an alternative to traditional regression-based BO methods, particularly in the context of molecular selection. Our investigation demonstrates that RBO can either match or outperform conventional BO surrogates, especially in environments characterized by noisy or rough data.\nThe comparative analysis between regression and ranking models revealed that ranking models offer significant advantages in prioritizing high-performing candidates. Specifically, the ranking models showed superior performance on datasets with rough structure-property landscapes and activity cliffs, where the exact predictive values are less critical than the relative ordering of candidates. This advantage is attributed to the ranking models' robustness against outliers and noise, which often impair the performance of regression models.\nOur findings suggest that the correlation between surrogate model performance and BO success is stronger for ranking models. The Pearson correlation analysis indicated a higher correlation between the test set's Kendall tau statistic and the BO performance for ranking models compared to regression models. This trend was consistent across various datasets, including ZINC and MoleculeACE, highlighting the efficacy of ranking models in identifying top-performing molecules early in the optimization process. By focusing on the relative ordering of candidate properties, RBO addresses key limitations of traditional regression-based models. Our results suggest that RBO is particularly effective in optimization of molecular datasets, making it a valuable tool for applications such as drug discovery and materials science.\nIn conclusion, RBO offers an alternative to traditional BO approaches, particularly in scenarios where the relative ranking of candidates is more critical than their exact predictive values. Future research should aim to address the identified limitations and expand the applicability of RBO to a wider range of optimization problems."}]}