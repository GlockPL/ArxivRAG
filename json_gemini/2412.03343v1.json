{"title": "Improving Linguistic Diversity of Large Language Models with Possibility Exploration Fine-Tuning", "authors": ["Long Mai", "Julie Carson-Berndsen"], "abstract": "While Large Language Models (LLMs) have made significant strides in replicating human-like abilities, there are concerns about a reduction in the linguistic diversity of their outputs. This results in the homogenization of viewpoints and perspectives, as well as the underrepresentation of specific demographic groups. Although several fine-tuning and prompting techniques have been suggested to tackle the issue, they are often tailored to specific tasks or come with a substantial increase in computational cost and latency. This makes them challenging to apply to applications that demand very low latency, such as chatbots and virtual assistants. We propose Possibility Exploration Fine-Tuning (PEFT), a task-agnostic framework that enhances the text diversity of LLMs without increasing latency or computational cost. Given the same prompt, models fine-tuned with PEFT can simultaneously generate multiple diverse responses, each corresponding with a controllable possibility number. Experiments on dialogue and story generation tasks demonstrate that PEFT significantly enhances the diversity of LLM outputs, as evidenced by lower similarity between candidate responses. Since PEFT emphasizes semantic diversity over lexical diversity, it can also notably reduce demographic bias in dialogue systems. The implementations and datasets are available in our repository\u00b9.", "sections": [{"title": "1 Introduction", "content": "LLMs represent a significant advancement in the field of artificial intelligence, specifically in natural language processing (NLP). These models are designed to perform various tasks, from text classification to question-answering and logical reasoning, through natural language prompts, even without task-specific training (OpenAI et al., 2024; Touvron et al., 2023; Jiang et al., 2023). The recipe for their success includes very large models trained on vast amounts of unfiltered internet data, which raises critical concerns about the perpetuation and amplification of biases (Gallegos et al., 2023). One of the primary concerns is that LLMs tend to be inherently conservative in their output. They are designed to predict the most likely words or sequences based on patterns observed in their training data. As a result, the generated text tends to closely align with the dominant narratives, ideas, and writing styles present in the datasets they were trained on. This can lead to a homogenization of content, where creative outliers and genuinely novel ideas are underrepresented. Studies by (Santurkar et al., 2023; Durmus et al., 2023) highlight that LLMs generate an unequal representation of views. Hence, future LLMs trained on such homogenized content may exacerbate the issue, perpetuating this cycle. The decline in diversity also presents significant challenges in other NLP areas, such as synthetic dataset production (Chung et al., 2023) or open-domain dialogue generation (Lee et al., 2023). Diversity in text generation has been extensively studied. Several approaches have been proposed, such as retraining the models on more balanced datasets (Zmigrod et al., 2019; Garimella et al., 2022; Solaiman and Dennison, 2021), or using a conditional variational inference framework (Bao et al., 2020). Post-editing approaches, such as modifying the decoding algorithms (Su et al., 2022; Holtzman et al., 2019; Fan et al., 2018) or optimizing the input prompts (Hayati et al., 2023; Lahoti et al., 2023; Mattern et al., 2022), can also be used to increase text diversity and do not require additional training. However, these methods either increase model complexity, failing to achieve a satisfactory level of diversity, or significantly increase inference latency and computational cost. This paper introduces Possibility Exploration Fine-Tuning (PEFT), a straightforward fine-tuning framework designed to enhance the text diversity of pre-trained LLMs. Our objective is to generate mul-"}, {"title": "2 Related work", "content": "Early methods to increase diversity involved modifying the conventional maximum likelihood training objective of text generation models. Shao et al. (2019) proposes a planning-based hierarchical variational model for generating long and diverse texts. Variational frameworks, employed by Du et al. (2022) and Bao et al. (2020), utilize randomly sampled latent variables to control the content of responses. These methods, however, significantly elevate training complexity and inference latency, and necessitate specific model architectures.\nA common strategy for enhancing text diversity modifies the decoding process. Techniques like diverse beam search (Vijayakumar et al., 2016), nucleus sampling (Holtzman et al., 2019), Top-K sampling (Fan et al., 2018), and logit suppression (Chung et al., 2023) aim to produce a broader set of outputs by not solely focusing on the most probable tokens. Contrastive search decoding (Su and Collier, 2022), in particular, has shown to improve both diversity and coherence. We demonstrate that models fine-tuned with PEFT can be combined with these decoding methods to further enrich diversity.\nRecent studies explore prompt optimization to improve diversity, including iterative prompting to uncover varied responses to the same input. Hay-"}, {"title": "3 Baselines", "content": null}, {"title": "3.1 Problem definition", "content": "Given the prompt P, our goal is to generate a list of candidate responses, L, where each response is semantically distinct from the others. This is crucial for applications such as brainstorming tools, creative writing assistants, or other prompting techniques that require reasoning over multiple solutions (Muralidharan and Thomas, 2024; Wang et al., 2022). In scenarios that require a single but creative response R, such as dialogue modeling, one can simply sample a response from the list L. If the list L is sufficiently diverse, then the response R will likely be unique. A proficient generation model should produce responses that are diverse and contextually relevant to the given prompt, while also maintaining low latency, which is critical for applications like real-time chatbots or interactive storytelling."}, {"title": "3.2 Decoding methods", "content": "Temperature sampling (Holtzman et al., 2019; Fan et al., 2018) adjusts the randomness of the generated text by modifying the distribution of predicted probabilities with a temperature parameter ; higher temperatures lead to more creative outputs. To generate N diverse responses for a single prompt, we can set a high temperature value and generate responses N times. Diverse Beam Search (DBS) (Vijayakumar et al., 2016), an alternative to beam search that decodes a list of diverse outputs by introducing mechanisms to explicitly encourage diversity among the candidates in the beam."}, {"title": "3.3 Prompting methods", "content": "Decoding methods, such as temperature sampling, do not account for semantic differences at the sentence level, as they generate responses independently. As a result, while the responses may vary in wording, their semantic meanings may remain similar. Inspired by recent work on diverse perspective"}, {"title": "3.4 Fine-tuning methods", "content": "We implement conditional variational frameworks (CVF) (Bao et al., 2021) to fine-tune LLMs for diversity enhancement. The fine-tuning process introduces latent variables to control the content of responses with the following objectives:\n$L_{NLL} = -E_{z~p(z|c,r)} log p(r|c, z)$\n$=\\ -E_{z~p(z/c,r)} \\sum_{t=1}^{T}logp(r_t|c, z, r_{<t})$\nwhere c is the context/prompt, r is the response, and $r_t$ is the t-th word in the response. z is a discrete latent variable with K possible values, each representing a specific latent speech act in the response. During training, z is first estimated using a latent recognition network given the context c and response r. The predicted latent variable is then used to condition the response in the response generation network. Note that both networks share"}, {"title": "4 Proposed method", "content": null}, {"title": "4.1 One-to-many dataset", "content": "Despite the inherent one-to-many (OTM) mapping nature of open-ended text generation, where each input prompt can yield multiple correct responses, current LLMs are predominantly fine-tuned on instruction-following or task-specific datasets that enforce a one-to-one (OTO) mapping. This means that each input prompt is accompanied by a single response. We refer to this approach as one-to-one fine-tuning (OTOFT). Although several studies have shown that OTOFT can improve the accuracy and performance of LLMs for specific tasks, its impact on the diversity of the output remains under-researched.\nTo address the one-to-many nature and potentially increase output diversity, we propose a method called one-to-many fine-tuning (OTMFT). OTMFT uses a OTM dataset to fine-tune LLMS for specific tasks. An OTM dataset is derived from a standard one-to-one mapping dataset. For each root pair of prompt-response (p, r), we generate N child samples $(p, r_1)$, $(p, r_2)$, ..., $(p, r_N)$, where each response $r_i$ is a valid reply to the prompt p and is semantically distinct from all other responses. This generation process can be conducted by human annotators or advanced LLMs. In this study, we utilize GPT-40 and List Prompting techniques to generate multiple distinct responses for the same prompt.\nOTMFT employs standard likelihood training, where all training samples corresponding to the same prompt are batched together. This fine-tuning process helps to flatten the probability distribution, allowing decoding techniques like temperature sampling to generate more diverse responses."}, {"title": "4.2 Possibility exploration dataset", "content": "Before presenting PEFT, we first introduce the Possibility Exploration (PE) dataset. We accompany each OTM training sample (p, ri) with a possibility number ki, indicating that the response $r_i$ is the $k_i$-th possible response out of all possible responses for prompt p. The inclusion of a possibility number in each prompt helps in the following ways: (1) It assists the model in understanding the reasons behind differences in responses, even when the same prompt is given; (2) It provides a degree of control over the inference process, allowing the possibility number k to be changed to elicit different responses; (3) It enables negative training (PEFT), which further enhances the dissimilarity between responses.\nGiven an OTM batch of training samples (p, $r_1$), (p, $r_2$), ...,(p,$r_n$), we construct a PE training batch by incorporating an additional instruction into the prompt p, as shown in Figure 2. Specifically, we instruct the model to contemplate all possible responses for the given prompt and then produce a response corresponding to possibility $k_i$, where $k_i$ is an integer randomly sampled from [1,.., M], with M being a hyper-parameter and M > N. Consequently, a PE batch of training samples will be (p, $k_1$, $r_1$), (p, $k_2$, $r_2$),..., (p,$k_n$,$r_n$).\nFigure 2 shows an example of PE training batch for open-domain dialogue generation task."}, {"title": "4.3 PEFT", "content": "We propose PEFT, which is based on unlikelihood training (Welleck et al., 2019). This approach aims to increase the dissimilarity between responses and enhance the impact of the possibility number. Unlikelihood training involves providing the model with both positive samples, which the model is encouraged to generate, and negative samples, which the model should avoid generating.\nWe use the PE batch of training samples $(p^+, k^+,r^+),..., (p^+,k^+,r^+)$ as described in"}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Tasks", "content": "We choose open-domain dialogue generation as the primary fine-tuning task because it necessitates both low latency and diverse outputs, which is the focus of this study. We also experiment with the story generation task to demonstrate the generalizability of our approach.\nMultiple responses generation. The task is to predict multiple possible responses for the next turn in a given dialogue context between two people. To create fine-tuning data for OTMFT and PEFT, we extract 1,000 dialogue contexts from Blended-SkillTask (Smith et al., 2020), ConvAI (Logacheva et al., 2018), TopicalChat (Gopalakrishnan et al., 2023), EmpatheticDialogues (Rashkin et al., 2018), and WizardOfWikipedia (Dinan et al., 2018), ensuring the number of samples per dataset is evenly distributed. For each dialogue context, we use GPT-40 and List Prompting to generate 4 different responses, resulting in a total of 4,000 context-response pairs. For CVF and OTOFT, 4,000 dialogue contexts are sampled, with each context accompanied by a single response that is also generated by GPT-40. Hence, the amount of training data for CVF, OTOFT, OTMFT, and PEFT is equivalent. For test set, 300 dialogue contexts are used.\nPersona generation. Aside from improving the diversity of generated texts, we are also interested in evaluating the effectiveness of PEFT in debiasing dialogue systems or LLMs in general. We designed a test called the persona-generation test, in which the chatbot is asked to play the role of a random individual and then engage in a conversation with another person. The persona attributes of the chatbot, such as age and gender, are gradually revealed throughout the conversation. Since the chatbot has the freedom to determine its own personality and demographic information, we want to analyze if there is significant bias in the generated personas. We conducted 300 conversations for each chatbot and then aggregated the results for final assessment. Details of the experiment can be found in the Appendix A.4. The chatbots used for this persona-generation test are the same as those used for the multiple responses generation task. However, we only sampled a single response from all generated responses at each turn.\nStory generation. Given a 4-sentence story as input, this task involves generating multiple diverse endings for the given story. We extract 1,000 train-"}, {"title": "5.2 Metrics", "content": null}, {"title": "5.2.1 Diversity", "content": "To measure lexical diversity, we utilize Distinct-1 and Distinct-2 scores (Liu et al., 2016), which account for the percentage of unique 1-grams or 2-grams in the entire collection of generated responses. For semantic diversity, we employ SBERT (Reimers and Gurevych, 2019) to compute the pairwise similarity between generated responses of each input prompt. The pairwise similarity is averaged across the test set, which is then used to calculate diversity as 1 \u2013 similarity.\nFor the persona generation test, we use Shannon entropy (Shannon, 1948) to measure the randomness/diversity of the generated personas. Assume we generate a set of N personas, denoted as P = {$P_1$, $P_2$, ..., $P_n$}. Each persona $P_i$ contains a set of attribute values $A_i$ = {a}, where $a^j_i$ represents a particular attribute value (such as female) corresponding to the j-th attribute (such as gender). Let A = {$a_1$, $a_2$, ..., $a_n$} be a collection of all values of the j-th attribute, extracted from P. Shannon entropy can be applied to measure the randomness score of the j-th attribute as follows:\nH(A') = -$\\sum_{k}^{K}$P(a)log(P(a))\nwhere H(A') represents the entropy of A, $a_k^j$ represents each possible value of A, P(a) represents the appearance ratio of the value a, and K is the number of distinct values of A\u00b9. This paper only focuses on evaluating specific attributes: age group, gender, current location, occupation sector, and highest education level. The extraction/normalization of these attributes from the generated conversations is done by GPT-40. See Appendix A.4.1 for details."}, {"title": "5.2.2 Coherence score", "content": "Given recent studies (Zheng et al., 2024) suggesting that LLMs can rival human performance in evaluating the quality of synthesized texts, we use GPT-40 and LLAMA 3 as coherence evaluators.\nPrevious studies often use the average rating (on a scale of 1 to 10) as the final measure of coher-"}, {"title": "5.3 Parameters settings", "content": "We use the Huggingface repository to conduct our experiments, employing Mistral 7B Instruct and LLAMA 2 7B Instruct as the pre-trained LLMs for fine-tuning. Each model is fine-tuned for one epoch using Qlora (Dettmers et al., 2024). The learning rate is set to 5e-5, with a batch size of 4 and a gradient accumulation of 2.\nThe number of possible target responses per input prompt, denoted as N, is set to 4 for all experiments. The maximum value for the possibility number in PEFT is set to 9. During inference and testing, each model is asked to generate 5 different responses per input prompt. We then calculate the diversity and coherence scores of these responses."}, {"title": "5.4 Comparison methods", "content": "Base model. We perform response generation using the original LLMs with zero-shot prompting and list prompting. For zero-shot prompting, we employ various decoding methods, including DBS, and temperature sampling. As we prioritize diversity, each decoding algorithm is configured with parameters that maximize output diversity without spoiling output coherence. For DBS, we employ hamming diversity as the objective term and set the diversity penalty to 5.0. For temperature sampling, we set the temperature value t to 1.5 for Mistral and 1.25 for LLAMA 2. We do not include contrastive search for comparison as the method is deterministic and can only generate a single response per prompt. The zero-shot prompt template can be found in Appendix A.3.1.\nOTOFT. We fine-tune the base model using a one-to-one dataset with a MLE objective.\nOTMFT. We fine-tune the base model using a one-to-many dataset with a MLE objective.\nPEFT. We fine-tune the base model using a possibility exploration dataset with both MLE and un-likelihood objectives.\nWhen comparing different fine-tuning techniques, we use temperature sampling as the decoding method with temperatures t = {0.5, 0.75, 1.0, 1.25}. For ease of comparing the diversity-coherence trade-offs between different methods, only optimal temperatures for each method are reported."}, {"title": "6 Experiment results", "content": "The experimental results for open-domain dialogue generation are reported in Tables 1-4. Results for the story generation task are reported in Table 5, Appendix A.1."}, {"title": "7 Conclusion", "content": "This paper investigates the degradation of diversity in LLMs through the lens of open-ended text generation. We found that instruction-following LLMs suffer from low diversity and exhibit bias when performing zero-shot generation. To address this issue, we propose and evaluate various fine-tuning techniques, including Conditional Variational, One-to-One, One-to-Many, and Possibility Exploration Fine-Tuning. Our results indicate that fine-tuning LLMs not only increases diversity but also enhances coherence scores, with PEFT achieving the"}, {"title": "Limitations", "content": "The main limitation of our work is the necessity for fine-tuning LLMs. This introduces two significant barriers: (1) the requirement to collect task-specific data, and (2) the fine-tuning of the original LLMs, which often demands substantial computational resources. Additionally, many off-the-shelf LLMs do not permit fine-tuning. As PEFT is task-agnostic, our future direction involves performing PEFT during the instruction tuning phase of LLMs. This approach entails extending the existing instruction-following datasets into a PEFT-like format and subsequently fine-tuning the base LLMs on this expanded dataset. By adopting this method, we aim to generate multiple diverse responses in a PEFT-style for any given task in a zero-shot setting."}, {"title": "Ethical considerations", "content": "Deploying Al responsibly requires a balance between creativity and safety in content generated by language models. Diversity is crucial to prevent monotonous and generic conversations, but it poses the risk of producing offensive or unsafe language when less common responses are chosen. This underscores the need for effective filtering of potentially harmful text. Advanced classifiers can be used to manage this careful filtration process by flagging and intercepting inappropriate content before it reaches the end user."}, {"title": "A Example Appendix", "content": null}, {"title": "A.1 Story generation results", "content": "Table 5 presents a comparison of results between PEFT and other baselines for the story generation task. Similar to the response generation task, PEFT significantly improves the diversity of the base model and outperforms Listing Prompting across all metrics, including diversity, coherence, and latency. PEFT also significantly outperforms CVF (without reranking) in both coherence and diversity. Although reranking can help CVF achieve a higher coherence score than PEFT, the same strategy could be applied to PEFT's responses to further improve coherence, though it would introduce some additional latency."}, {"title": "A.2 Examples of generated responses", "content": "Table 6 shows various examples of generated responses using different decoding and fine-tuning methods."}, {"title": "A.3 Prompt templates", "content": null}, {"title": "A.3.1 Zero-shot response generation with base LLMs", "content": "We convert the dialogue context into a conversation between people, Person A and Person B, where Person A always has the last turn. We then ask LLMs to generate the next response for Person B using the following template:\nGiven this conversation:"}, {"title": "A.3.2 PEFT response generation template", "content": "Given this conversation:\nPerson B:\nPerson A:\nImagine you are person B and act as if you were a real individual. Think about all the possibilities in which person B might respond next and then provide the response that corresponds to possibility number $k."}, {"title": "A.3.3 Coherence evaluation prompt template", "content": "Given this conversation:\nPerson B:\nPerson A:\nDoes this next response from Person B make coherent sense?\nPerson B: {response to be evaluated}\nBegin your evaluation by providing a short assessment. Then, rate the coherence of Person B's response on a scale from 1 to 10 by strictly following this example format: 'Coherence rating: [5]'\nCoherence assessment:"}, {"title": "A.4 Persona generate test", "content": "We ask the chatbot to mimic the role of a human and then conduct several conversations to evaluate if there is significant bias in the generated personas. Each conversation includes two roles: the persona revealer and the persona seeker. The chatbot under assessment will play the role of the persona revealer, who will disclose information about themselves throughout the conversation. The persona seeker's role is to guide the conversation toward extracting personal information from the persona revealer. The persona seeker can be a real human or another language model. In this study, we use ChatGPT as the persona seeker.\nWe use the following prompt template for the persona seeker:\nYou are an expert conversationalist acting as Person A. Your goal is to guide a conversation to gather Person B's demographic details: country of residence, age, occupation, level of education, and gender. Ensure the transitions between topics are smooth and keep each of your responses to no more than two sentences.\nConversation:\nPerson A:\nPerson B:\nTo ensure each conversation is unique, we seed each interaction with four different utterances from the test set. The conversation exchange between the persona revealer and the persona seeker will start from turn 5. An example of a conversation in persona generation is shown in Table 7."}, {"title": "A.4.1 Persona attribute extraction", "content": "After all conversations have taken place, we need to extract and standardize the persona attributes of the persona revealer. Here is a prompt template for attribute extraction from a conversation:\nGiven this conversation:\nPerson A:\nPerson B:\nPlease extract/infer information about Person B from the conversation and complete the following details. For any missing information, please fill in 'None'.\nAge:\nGender:\nPlace of birth (country):\nCurrent country of residence:"}]}