{"title": "FROM LOGITS TO HIERARCHIES: HIERARCHICAL CLUSTERING MADE SIMPLE", "authors": ["Emanuele Palumbo", "Moritz Vandenhirtz", "Alain Ryser", "Imant Daunhawer", "Julia E. Vogt"], "abstract": "The structure of many real-world datasets is intrinsically hierarchical, making the modeling of such hierarchies a critical objective in both unsupervised and supervised machine learning. Recently, novel approaches for hierarchical clustering with deep architectures have been proposed. In this work, we take a critical perspective on this line of research and demonstrate that many approaches exhibit major limitations when applied to realistic datasets, partly due to their high computational complexity. In particular, we show that a lightweight procedure implemented on top of pre-trained non-hierarchical clustering models outperforms models designed specifically for hierarchical clustering. Our proposed approach is computationally efficient and applicable to any pre-trained clustering model that outputs logits, without requiring any fine-tuning. To highlight the generality of our findings, we illustrate how our method can also be applied in a supervised setup, recovering meaningful hierarchies from a pre-trained ImageNet classifier.", "sections": [{"title": "1 INTRODUCTION", "content": "Modeling hierarchical structures in the data is a long-standing goal in machine learning research (Bengio et al., 2014; Jordan & Mitchell, 2015). In many real-world scenarios, data is inherently organized in hierarchies, such as phylogenetic trees (Linn\u00e6us, 1758; Sneath & Sokal, 1962; Penny, 2004), tumor subclasses (S\u00f8rlie et al., 2001) and social networks Ravasz & Barab\u00e1si (2003); Crockett et al. (2017). In unsupervised learning, hierarchical clustering can provide more accurate insights than flat (i.e. non-hierarchical) clustering methods by introducing multiple levels of granularity and alleviating the need for a fixed number of clusters specified a priori (Bertsimas et al., 2021; Chami et al., 2020). This aids scientific understanding and interpretability by providing a more informative representation (Lipton, 2018; Marcinkevi\u010ds & Vogt, 2020). The benefits of modeling a hierarchy in the data extend to supervised scenarios. For example, interpretable methods based on decision trees (Breiman, 2001; Tanno et al., 2019) hierarchically partition the data so that points in each split are linearly separable into classes. More recent work leverages hierarchies in the data to improve supervised methods (Bertinetto et al., 2020; Goren et al., 2024; Karthik et al., 2021) or for self-supervision (Long & van Noord, 2023).\nAmong classic algorithms for hierarchical clustering, agglomerative methods have been the most widely adopted. These methods compute pairwise distances between data points, often in a lower-dimensional representation space. Starting from the instance level, a hierarchy is then built based on the pairwise distances by recursive agglomeration of similar points or clusters together in a bottom-up fashion (Murtagh & Contreras, 2011). More recently, a revived interest in hierarchical clustering has sparked novel approaches using deep architectures (Mautz et al., 2020; Goyal et al., 2017; Shin et al., 2019; Vikram et al., 2019; Manduchi et al., 2023). However, despite their promising methodological contributions, recent approaches require specialized architectures and complex training schemes. Consequently, they cannot be applied to large-scale datasets due to their expensive computational requirements. Moreover, we find that they often exhibit a lower performance at the leaf level compared to non-hierarchical models."}, {"title": "2 RELATED WORK", "content": "Hierarchical clustering aims to learn clusters of datapoints that are organized in a hierarchical structure. The methods used can be broadly categorized into agglomerative and divisive approaches (Nielsen, 2016). The former tackles the problem with a bottom-up approach and iteratively agglomerates clusters into larger ones until a full hierarchy is built in the form of a dendrogram, starting with each datapoint being a separate cluster (Murtagh & Contreras, 2011). The similarity of datapoints is measured according to a distance function, which for high-dimensional data is often defined on a lower-dimensional representation space. Multiple linkage methods have been proposed to compute the distance between clusters of datapoints formed at a given step of the algorithm (Sneath, 1957; Ward, 1963). As examples, single, average, and complete linkage characterize the distance between two clusters as the minimum, average, and maximum distance between their datapoints, respectively. Since these algorithms can be costly, particularly in high-dimensional spaces, approximate versions have been developed for faster computation (Abboud et al., 2019; Cochez & Mou, 2015). Notably, linkage methods are still widely applied in many domains, for instance, in medical research (Nguyen et al., 2024; Senevirathna et al., 2023; Resende et al., 2023).\nOn the other hand, divisive algorithms start with all objects belonging to the same cluster and recursively split them into subclusters. While early approaches are mostly based on heuristics, Dasgupta. (2016) proposed the Dasgupta cost: an objective function for evaluating a hierarchical clustering, with a divisive approach to provide an approximately optimal solution. HypHC introduces a continuous relaxation of Dasgupta's discrete optimization problem with provable guarantees via hyperbolic embeddings that better reflect the geometry of trees compared to Euclidean representations Chami et al. (2020); Liu et al. (2019). More recently, research has focused on developing deep learning approaches for hierarchical clustering (Mautz et al., 2020; Goyal et al., 2017; Shin et al., 2019; Vikram et al., 2019; Manduchi et al., 2023). Among these, DeepECT learns a hierarchical clustering on top of the embedding space of a jointly optimized autoencoder (Mautz et al., 2020). Notably, TreeVAE not only learns a hierarchical clustering in the latent space but also provides a generative model that adheres to the learnt hierarchy, thereby enabling sample generation in a structured manner (Manduchi et al., 2023). However, all these approaches have mostly been tested on simple datasets, far from realistic settings. As shown in our experiments, they present relevant limitations when deployed on more challenging datasets, mainly due to their high computational complexity."}, {"title": "3 METHOD", "content": "In this work, we take a critical perspective on a recent line of research on hierarchical clustering: as an alternative to designing ad-hoc complex approaches, we focus on adapting pre-trained flat models to output a hierarchy with minimal overhead. To this end, we introduce a lightweight algorithm to leverage the information contained in the logits of a pre-trained flat clustering model to output a hierarchy of clusters. In the following, we describe the proposed procedure and also provide a graphical illustration as well as detailed pseudocode.\nLet $D = \\{x_1,...,x_N\\}$ be a dataset consisting of $N$ data points and $f_0$ be a non-hierarchical model trained to partition $D$ into $K$ clusters. We assume that $f_0$ outputs unnormalized logits, i.e. the cluster assignment $k^*$ for a datapoint $x$ is determined by computing $k^* = \\arg \\max_{k\\in \\{1,...,K\\}} \\text{softmax}_k(f_0(x))$. We define two functions\n$h_0(x) = \\arg \\max_{k\\in \\{1,...,K\\}} \\text{softmax}_k(f_0(x))$\n$g_0(x) = \\max_{k\\in \\{1,...,K\\}} \\text{softmax}_k (f_0(x))$\nof which $h_0$ computes the cluster assignment for a datapoint $x$, while $g_0$ computes the predicted probability of the cluster assignment for the datapoint $x$.\nA key idea behind our method is a simple yet effective way to determine the relatedness of clusters, or groups of clusters, by iteratively grouping them together to construct a hierarchy. Intuitively, to assess which group of clusters $G'$ is most related to a given group $G$, we propose the following strategy. For datapoints assigned to clusters in $G$, we determine which group $G'$ would have the majority of these datapoints reassigned to if clusters in $G$ were not available.\u00b9compute cluster assignments and corresponding predicted probabilities, restricting only to a subset of the total set of clusters.\nWe start with a masked version of the softmax function\n$\\text{m}\\_\\text{softmax}_k (v; G) = \\begin{cases} \\frac{\\exp(v_i)}{\\Sigma_{j\\notin \\{1,...,K\\}\\G} \\exp(v_j)} & \\text{if } i \\in G \\\\ 0 & \\text{if } i \\notin G \\end{cases}$\ngiven a $K$-dimensional vector $v$ and a set $G \\subset \\{1,...,K\\}$. This function restricts the softmax operation to the elements of $v$ at indexes in $\\{1, ...,K\\}\\G$. Next, we define functions\n$h^m (x; G) = \\arg \\max_{k\\in \\{1,...,K\\}} \\text{m}\\_\\text{softmax}_k (f_0(x); G)$\n$g^m (x; G) = \\max_{k\\in \\{1,...,K\\}} \\text{m}\\_\\text{softmax}_k(f_0(x); G)$\nNote that the $h^m$ and $g^m$ functions correspond to $h_0$ and $g_0$, except restricting the choice of viable clusters to $\\{1, ..., K\\}\\G$. In particular, the $h^m_0$ function computes the cluster assignment for a datapoint $x$ restricting to clusters in $\\{1, . . ., K\\}\\G$, and $g^m_0$ the corresponding predicted probability. Lastly, we define\n$D^c := \\{x \\in D | h_0(x) = c\\}$\ni.e. the subset of datapoints assigned to a given cluster $c \\in \\{1, ..., K\\}$. Similarly, we denote as $D^G = \\cup_{c\\in G}D^c$ the subset of datapoints assigned to a group of clusters $G \\subset \\{1, ..., K\\}$."}, {"title": "4 EXPERIMENTS", "content": "In this section we showcase the experimental results obtained with our proposed approach. In the first part, we focus on the task of hierarchical clustering. We demonstrate that existing specialized models for hierarchical clustering present major limitations when applied in realistic settings. In contrast, our proposed approach achieves convincing results in challenging vision datasets, markedly outperforming alternative methods. In the second part of this section, we present a case study where we discuss the application of the L2H algorithm on top of a pre-trained ImageNet classifier, demonstrating its value for model interpretability and the discovery of spurious correlations."}, {"title": "4.1 HIERARCHICAL CLUSTERING", "content": "In this section, we compare the performance of our proposed method for hierarchical clustering with recent specialized approaches on three challenging vision datasets: namely the CIFAR-10, CIFAR-100 (Lake et al., 2015) and Food-101 (Bossard et al., 2014) datasets. For each dataset, we implement our algorithm on top of two pre-trained flat clustering models, namely TURTLE (Gadetsky et al., 2024) and TEMI (Adaloglou et al., 2023). These are two state-of-the-art clustering methods (see Appendix B.2 for more details), both of which are not designed to produce a hierarchy of clusters. In our evaluation, we report both metrics to evaluate models at the flat level and metrics to evaluate the quality of the produced hierarchy. For comparing models at the flat level, we report NMI, ARI, Accuracy and Leaf Purity (LP). To assess the quality of the hierarchical clustering, we report two metrics: Dendrogram Purity (DP) and Least Hierarchical Distance (LHD). The former was introduced in Kobren et al. (2017) and extends the notion of purity, normally evaluated at the leaf level, to assess the quality of a tree clustering: higher purity corresponds to higher quality of the hierarchy. Note that this metric was recently adopted in Manduchi et al. (2023) to benchmark deep hierarchical clustering models. Least Hierarchical Distance, on the other hand, measures the average minimal log-distance in the hierarchy between any pair of data-"}, {"title": "5 CONCLUSION", "content": "In this work, we propose a lightweight yet effective procedure for hierarchical clustering based on pre-trained non-hierarchical models. Notably, our solution proves to be markedly more effective and significantly more computationally efficient than alternative methods. Different from existing models for hierarchical clustering, our method can successfully handle large datasets of many classes, taking an important step in deploying hierarchical clustering methods in challenging settings. Moreover, we show that the usefulness of our approach extends to supervised setups, by implementing it on top of a pre-trained classifier to recover a meaningful hierarchy of classes. A case study on ImageNet shows that this approach provides relevant insights for interpretability, and can reveal potential biases of the pre-trained model and ambiguities in existing categorizations.\nWhile we provide extensive results on image datasets, in this work we do not explore other data modalities. However, our procedure is general and may be applied to different data types, which we leave for future work. Hierarchical clustering presents important advantages over non-hierarchical clustering by simultaneously capturing the structure in the data at multiple levels of granularity. However, inspecting the hierarchy is still necessary to extract valuable insights. An important focus for future work is to investigate strategies to partly bypass this process, automatically selecting levels of the hierarchies that provide the most meaningful clustering."}, {"title": "APPENDIX", "content": "In Figure 4, we provide a Python implementation of the L2H algorithm proposed in this work using standard scientific computing libraries (NumPy, SciPy). As stated in Section 3, our algorithm only requires the logits as input. It can be executed on the CPU even for large datasets, e.g., with a runtime of less than a minute for ImageNet-1K. Note that our procedure can be applied to any pre-trained unsupervised model to perform hierarchical clustering. Further, it can also be applied to logits from a supervised model to infer a hierarchy of classes. The output of the algorithm is a list, that contains the two groups that are merged at each step, hence characterizing the hierarchy. The aggregation function for computing the score per group is a design choice (as described in Appendix B.2) that can be viewed as a hyperparameter."}, {"title": "B EXPERIMENTAL DETAILS", "content": "In this work, we run experiments on four challenging vision datasets, namely CIFAR-10 and CIFAR-100 (Lake et al., 2015), Food-101 (Bossard et al., 2014) and ImageNet-1K (Deng et al., 2009).\nCIFAR-10 and CIFAR-100 are well-established object classification datasets. The CIFAR-10 dataset consists of 60000 32x32 colored images divided in 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. The train/test splits contain 50000 and 10000 images respectively. Similarly, also the CIFAR-100 dataset consists of 60000 32x32 colored images. However, they are organized into 100 classes. In addition, the 100 classes are grouped into 20 superclasses. As for CIFAR-10, the train/test splits also contain 50000 and 10000 images respectively. The Food101 dataset is a fine-grained classification dataset of food images, consisting of 101000 images for 101 classes. Images are high-resolution, up to 512 pixels side length. Images are split between 75750 training samples and 25250 test images. The ImageNet-1K dataset, widely used in computer vision, consists of 1000 classes organized according to the WordNet hierarchy (Miller, 1995), with 1281167 training and 50000 test samples, respectively."}, {"title": "B.2 IMPLEMENTATION DETAILS", "content": "For our hierarchical clustering experiments, to train the TURTLE and TEMI models on all considered datasets, we use the official code provided by the authors with recommended choices for hyperparameters (Gadetsky et al., 2024; Adaloglou et al., 2023). In particular, TEMI employs CLIPVITL/14 representations of the data, while TURTLE employs both CLIPViTL/14 and DINOv2 ViT-g/14 representations. For more details on TURTLE trained using two representation spaces, see the original paper (Gadetsky et al., 2024). We train both TEMI and TURTLE with a number of clusters K equal to the true number of classes in each dataset. For each dataset, we train models on the training set, then report metrics on the test set. Note that the L2H algorithm takes as input logits from the training set to infer the hierarchy, while metrics that evaluate the quality of the hierarchy are computed on the test set. As the aggregation function A in the L2H algorithm (see Section 3) to score each group we employ\n$s(G) = \\sum_{D_c} \\sum_{c \\in G} \\frac{1}{|D_c|} \\sum_{x \\in D_c} g_0(x)$\nwhich we find to work well in practice. However, other choices are possiblle. We implement TreeVAE (Manduchi et al., 2023) with their contrastive approach using the provided PyTorch codebase with corresponding defaults. The splitting criterion is set to the number of samples, an inductive bias that benefits this baseline method, since all datasets are balanced (Manduchi et al., 2023; Vandenhirtz et al., 2024). We set the number of clusters to 10 for CIFAR-10 and to 20 for the rest, due to the computational complexity, as seen in Table 2, as well as memory complexity, since every additional leaf adds a new decoder. DeepECT (Mautz et al., 2020) is also implemented using their provided codebase with the augmented version. Note that similar to the results shown in Manduchi et al. (2023), for colored datasets, DeepECT fails to grow trees, as they always collapse, indicating that DeepECT fails to find meaningful splits. We implement agglomerative clustering using the scikit-learn library (Pedregosa et al., 2011), and fit the model using PCA embeddings of the datasets with 50 components and wards criterion (Ward, 1963) as the linkage method. Using the author's original codebase, we further train Hyperbolic Hierarchical Clustering (Liu et al., 2019) on CLIP embeddings of the respective datasets. The authors do not describe how to retrieve cluster assignments using their method, so we follow the agglomerative clustering procedure and assume the leaves of the last K tree nodes created to form a cluster, where K corresponds to the chosen number of clusters."}, {"title": "B.3 METRICS", "content": "Here we provide more details on the metrics reported in our experiments in Section 4.1. In our comparisons, we evaluate models both on flat and hierarchical clustering.\nFlat clustering To assess model performance in flat clustering, for each model we take the clustering at the level of the hierarchy where the number of clusters corresponds to the true number of"}]}