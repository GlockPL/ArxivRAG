{"title": "A MEASURE FOR LEVEL OF AUTONOMY BASED ON OBSERVABLE SYSTEM BEHAVIOR", "authors": ["Jason M. Pittman"], "abstract": "Contemporary artificial intelligence systems are pivotal in enhancing human efficiency and safety\nacross various domains. One such domain is autonomous systems, especially in automotive and\ndefense use cases. Artificial intelligence brings learning and enhanced decision-making to autonomy\nsystems' goal-oriented behaviors and human independence. However, the lack of clear understanding\nof autonomy system capabilities hampers human-machine or machine-machine interaction and\ninterdiction. This necessitates varying degrees of human involvement for safety, accountability, and\nexplainability purposes. Yet, measuring the level autonomous capability in an autonomous system\npresents a challenge. Two scales of measurement exist, yet measuring autonomy presupposes a\nvariety of elements not available in the wild. This is why existing measures for level of autonomy are\noperationalized only during design or test and evaluation phases. No measure for level of autonomy\nbased on observed system behavior exists at this time. To address this, we outline a potential\nmeasure for predicting level of autonomy using observable actions. We also present an algorithm\nincorporating the proposed measure. The measure and algorithm have significance to researchers and\npractitioners interested in a method to blind compare autonomous systems at runtime. Defense-based\nimplementations are likewise possible because counter-autonomy depends on robust identification of\nautonomous systems.", "sections": [{"title": "1 Introduction", "content": "Contemporary artificial intelligence (AI) systems play a pivotal role in enhancing human efficiency and safety\nacross various facets of daily life. The overarching objective remains to alleviate human burdens by entrusting artificial\nsystems with tasks deemed either undesirable, dull, or hazardous. Fields such as health, finance, automotive, national\ndefense, and criminal justice have experienced significant impact in this regard.\nOne area where Al is experiencing rapid innovation is in autonomous systems (Albrecht & Stone, 2018), particularly\nin the automotive (Yurtsever et al., 2020) and defense (Reis et al., 2021) industries. While AI is generally understood\nto be defined by learning and decision-making, autonomous systems (AS) add goal-oriented and independent (from\nhuman operators) behaviors (Wang et al, 2019) to the foundational AI definition. Indeed, at a minimum, AS are capable\nof self-direction and self-sufficiency (Bradshaw et al., 2013). Extended capabilities are of course desirable and, in a\nvariety of cases, necessary.\nHowever, according to Bradshaw et al., when then is a lack a clear understanding of highly autonomous system\ncapabilities, the overall system (i.e., human-machine or machine-machine) suffers. More specifically, not recognizing\na system's autonomous capabilities in a timely manner significantly limits the ability to interact with or interdict the\nsystem (Scharre & Horowitz, 2015; Longpre et al., 2022). Whether an AS is driving an automobile or selecting targets\nfor a weapon system, keeping a human in the loop or on the loop is required for safety, accountability, and explainability.\nIn AS development, the degree of human interaction and interdiction possible translates to a level of autonomy (Hudson\n& Reeker, 2007; Meakin, 2021). In simple terms, the higher the level of autonomy the less a human is needed.\nThere is a general problem here because, according to Antsaklis and Rahnama (2018), \u201c[m]easuring the degree\nof autonomy is non-trivial\" (pg 25). In simple terms, designers can specify the level of autonomy for an autonomous"}, {"title": "2 Related Work", "content": "There are four dimensions to the background for this work. The first dimension describes autonomy as a\nfoundational concept and establishes important definitions. The intersection of autonomy and AI (Figure 1) gives rise\nto autonomous intelligent systems. Here we again establish foundational concepts but also begin to introduce major\nchallenges in the field. For the third, we discuss existing measures for levels of autonomy. Then, lastly, we address\nexplainability and the potential, or lack thereof, for explainability to supplant measures for level of autonomy."}, {"title": "2.1 Autonomy", "content": "Autonomy is the first dimension related to this study. Autonomy, in this context, provides an operational framework\nfor autonomous intelligent systems and lethal autonomous systems. That is, without autonomy or autonomous systems,\nneither would exist. This matters because of the growing importance of autonomy in civilian and military contexts.\nA persistent issue throughout the literature is defining what is and what is not autonomy (Beer et al., 2014; Williams,\n2015; Saxon 2021). Several definitions exist, each providing a material component to our understanding of autonomy.\nMost commonly we found minor variations on the ability for a system to operate and make decisions (Vladeck 2014;\nWilliams, 2015; Saxon 2021). The variations appeared in how human involvement was articulated in the literature.\nThematically, we observed without continuous human involvement, without direct human involvement, and without\nhuman input. In all cases, definitions appeared to delimit at what point humans can choose to be on or out of the loop.\nThus, human involvement is implicit until it is no longer explicitly required. We conceptualize this definition as the\nengineering view of autonomy.\nWe also found the idea that autonomy is the ability of a system to learn and adapt its behavior (Kasabov & Kozma,\n1998; Palanisamy, 2020) which we think of as the computational view of autonomy. Here, the explicit inclusion of learn\nextends the notion of making decisions. Concurrently, adapt relates to navigating and interacting with an environment.\nIn some sense, this definition serves as bridge between fundamental decision-making and the tight coupling between\nautonomy and the area of operation for such autonomy. On the other side of the definition bridge then, the DoD\n(Williams, 2015; Saxon, 2021; Ness et al., 2023) positions autonomy as a system capable of executing a pre-planned\nmission and reacting to uncertainties in the environment without external control. Here, we take the navigation of"}, {"title": "2.2 Autonomous intelligent systems", "content": "In simple terms, applying artificial intelligence to autonomous system yields in an autonomous intelligent system\n(Reis et al., 2021; Chen, Sun, & Wang, 2022) or AIS. More specifically, autonomous intelligent systems are autonomous\nsystems capable of independently learning and adapting. In that sense, autonomous intelligent systems are the\ncomputational view of autonomy manifest. Moreover, alignment in definition of what an autonomous intelligent system\nis prevents the contention seen elsewhere with describing, comparing, and contrasting types of systems. This appears\ntrue across the plethora of AIS research areas such as swarms, robotics, automotive, and defense systems.\nThe lack of definitional contention may be related to the consensus on what differentiates AIS from mere autonomy.\nThat is, the center of AIS is the supervisory or control mechanism which is where artificial intelligence comes into\nplay. This component replaces the human in or on the loop to a degree commensurate with the level of autonomy. Here,\nTsamados and Taddeo (2023) outlined five steps for supervisory control in any autonomous intelligent system: planning,\nteaching, monitoring, intervening, and learning. Admittedly, the authors indicated one of the two agents involved were\nhuman. However, the five steps appear remarkably similar to self- * principles at the core of autonomic computing. The\nconnection is more than tenuous. Harel, Marron, and Sifakis (2020) claimed autonomic computing principles serve as\nthe foundation for autonomous intelligent systems.\nThere are a variety of engaging challenges to solve in AIS. Tyagi and Aswathy (2021) suggested three areas for\nfuture research to focus: security, trust, and performance. These problems align with the defense and engineering views\nof autonomy. In the computational view, Andresciani and Cingolani (2020) proposed accountability, transparency, and\nsafety. When we delimit challenges related to ethics and responsibility, the literature carries these challenges forward to\nthe very forefront of the field."}, {"title": "2.3 Measuring autonomy in autonomous systems", "content": "The need to measure the level of autonomy follows logically from the presence of levels of autonomy. Moreover, the\nneed transcends the type and implementation of autonomy. However, as Antsaklis and Rahnama (2018) demonstrated,\nmeasuring the level of autonomy in an AS or AIS is not a straightforward exercise. Nevertheless, there exists a variety\nof quantifiable methods to assess the level of autonomy in a system\nHrabia et al. (2015) laid groundwork for modern quantification of autonomy by detailing a multi-dimensional\nautonomy metric framework. The dimensions included adaptation, perception and acting, planning and goals, belief and\nreasoning, learning, and decision making. Each dimension has its own expression and output metric. Taken together\nthrough a relation function, the framework computes a normalized autonomy score. The score is a continuous value\nwhich lends a high degree of precision. Yet, Hrabia et al. do not map the individual dimensions or normalized score to\nscales of autonomy (e.g., Huang et al., 2007; Hudson & Reeker, 2007).\nAntsaklis (2020) consolidated the Hrabia et al. (2015) multi-dimensional framework by conceptualizing au-\ntonomous systems as control systems. Doing so led to exploring the integration of methods from operations research\nand AI to achieve higher levels of autonomy. This ultimately led to an expression capable of computing a level of\nautonomy on the ALFUS (Huang et al., 2007) scale. Notably, Antsaklis derived further expressions incorporating\nexternal interventions as well as system performance.\nMeakin (2021) described a method for quantifying the degree of autonomy of a system. The objective was to\nprovide a quantitative basis for comparing two or more systems. Where this method differs, but also connects to level\nof autonomy scales such as ALFUS (Huang et al., 2007), is in quantifying initiative. Meakin offered imitative as a\nproxy for independent decision-making. Overall, the measurement is functionally sound because of the reliance on\nindependent behavior. Furthermore, by mapping the measured Behavioural Indepdenence Level (BIL) to the ALFUS\nHuman Independence (HI), Meakin's method maps to a level of autonomy (Hudson & Reeker, 2007; Antsaklis, 2020).\nCertainly, there have been other proposed quantitative methods related to level of autonomy (Hudson & Reeker,\n2007; Seth, 2010; Nikitenko & Durst, 2016). Despite the variance in approaches to measuring autonomy, there are\nseveral commonalities. Such commonalities include knowledge of designed goals, capabilities, and behaviors.\nStill, none of the existing methods quantify autonomy based on observation in the wild. This appears true despite\na variety of measures including some form of behavioral dimension in the autonomy level calculation. Furthermore,\nall require a tight coupling to a test and evaluation apparatus. All also rely on knowing operational details such as\ngoals, mission parameters, and behavioral triggers. Yet, such information is not available when encountering potential\nautonomous systems in the wild. Yet, there are two reasons measuring level of autonomy based on observed behavior is\na critical need. On one hand, there is still a need to compare capabilities between two or more autonomous system.\nExisting measures provide this albeit during design or test and evaluation phases. Which leads to the need on the other\nhand- assessing to the degree or level of autonomy the autonomous system actually exhibits relative to mission goals\nand intended capability. Despite this gap in the research, we uncovered a potential clue in the literature.\nWolschke et al. (2017) determined evaluation of AS- vehicles, specifically- required knowledge of the systems\ngoals and capabilities. The authors found standard test-case creation based on goals and capabilities insufficient due to\nthe increasing levels of autonomy in such vehicles. As a response, Wolschke et al. demonstrated an observation-based\napproach to generating AS test-cases.\nThis approach uses observation of an action sequence as input, computes the difference between action sequence\ntime-steps using Damerau-Levenshtein string edit distance calculation, and then evaluates the action against the output\nrelative to the expected action sequence. While the output is not a measure of level of autonomy, we think the method\npoints to one potential way to use observed behavior as an input."}, {"title": "3 Measuring Observable Level of Autonomy", "content": "We propose the following theoretical framework for a measure for observable level of autonomy. The goal of the\nframework is to predict a normalized level of autonomy as described in the SAE scale. We designed the framework\nbased on two components: a human behavior lookup table and a foundational expression. Both components build upon\nexisting literature, chiefly Wolschke et al. (2017) and Meakin (2021)."}, {"title": "3.1 Human behavior lookup table", "content": "The measure requires an accessible lookup table of human behavior exists compatible with the observation. For\ninstance, a turn might consist of the turn direction (left or right), the velocity of the turn, the time spent in the turn\n(duration), and the angle of the turn. Each value is independent from others in the table. Value data types\nare floating points ($r \\in R$). Each individual value contributes necessarily to the overall behavior as subactions. Thus,\nturning left can be described as $T_{l} = {T_{v}, T_{d}, T_{a}}$.\nWe imagine such lookup tables to be application specific in early prototyping. Meaning, the action described in\nTable 2 is tightly coupled to turning. Moreover, the action would be specific to say an automobile turning. The same\ntable would not apply to a UAV. Therefore, complex behaviors would necessarily include a plethora of lookup tables.\nHowever, a generalized methodology for constructing lookup tables should be possible, thus reducing complexity in the\nmeasure and increasing operational efficiency."}, {"title": "3.2 Foundational expression", "content": "Whether the lookup table is tightly coupled to an action or generalized, the foundational expression embodies\na central idea: any given action at a specific time step can be treated as a discrete subaction. Further, each discrete\nsubaction can be converted into string element. Thus, we can let $O_{a}$ be the set of observed actions as {$o_{1}, o_{2}...o_{n}$}\nwhich are ingested through sensors. The set of human behaviors are ingested from a lookup table (e.g., Table 2) and\nencoded into $H_{a}$ as {$h_{1}, h_{2}, ...h_{n}$}. For clarity, each element in $O_{a}$ and $H_{a}$ is a $T_{i}$ action equivalent, not a subaction\nsuch as $T_{v}$.\nWe then describe the observed level of autonomy using the following expression\n\n$L = \\epsilon(  ( \\sum_{t=0}^{T}H_{a}), ( \\sum_{t=0}^{T}O_{a}))$     (1)\n\nThe level of autonomy L is equivalent to the edit distance function $ \\epsilon$. We envision the edit distance function\ninstantiated as equation 6 in Lopresti and Zhou (1996). Total time is T and t is a given time step where $t \\in T$ which\nbegins with a zeroed origin representing the initiation of observation. To differentiate, the T time is external whereas\nany similar symbol part of the human behavior lookup table is internal only."}, {"title": "3.3 Algorithmic implementation", "content": "We can describe the foundational expression in conceptual terms using an algorithmic implementation. Meaning,\nthe algorithmic implementation is intended to represent a sequential logic, in a loose narrative form, leading towards\nExpression 1. This implementation is not representative of how measuring level of autonomy based on observable\nsystem behavior ought to be practically implemented.\nIn simple terms, the algorithm would be triggered at the start of observing. Then, each discrete observation would\nbe checked against the human behavior lookup table. The observation would be added to the set of observations as\nlong as such is present in the human behavior lookup table. At this step, the subaction value (i.e., $T_{v}$ or turn velocity) is not"}, {"title": "3.4 Algorithm output and level of autonomy", "content": "The Observational Score output from Algorithm 1 is mapped to a minimum level of autonomy. A minimum is\nasserted because observation cannot accurately infer a maximum. Moreover, we opt for the five-point SAE level of\nautonomy scale for illustrative purposes. The Observational Score could be transposed to a range of zero to 10 if desired\nand mapped to the ALFUS ten-point scale."}, {"title": "4 Conclusions", "content": "Autonomy research is steadily incorporating AI (Albrecht & Stone, 2018) as a way to expand operational goals\ninto deeper fields of uncertainty. Such a transition is visible in the shift from AS to AIS (Reis et al., 2021; Chen, Sun, &\nWang, 2022). The motivation driving the AIS shift the need to design systems capable of higher levels of autonomy.\nThis appears to be in response to a selective pressure created greater amounts of uncertainty in operational environments\nsuch as driving and national defense.\nHowever, whether an AIS is driving an automobile or selecting targets for a weapon system, keeping a human in\nthe loop or on the loop is required for safety, accountability, and explainability. In this context, the extent of human\ninteraction and interdiction possible translates to a level of autonomy (Hudson & Reeker, 2007; Meakin, 2021). In\nsimple terms, the higher the level of autonomy the less a human is needed. Principally, there are two level of autonomy\nscales: a five-point scale (SAE, 2016) and a ten-point scale (Huang et al., 2007). Both scales allow designers to align\nintended system capabilities with discrete measures. Further, the scales allow for comparison between two or more\nsystems. Such implies a means to measure the level of autonomy.\nTherein exists a problem because according to Antsaklis and Rahnama (2018) because measuring a level of\nautonomy in an AIS is challenging. To this point, while several proposed measures exist, these share a\ncommon set of presuppositions and assumptions. Chiefly, the designed goals and capabilities are known to the measures.\nFurther, the measures rely upon access to internal system states. During design and testing, these presuppositions and\nassumptions are valid. Yet, in the wild, such become demonstrably false. Such inability to measure level of autonomy\nin the wild is a specific problem because emergent runtime behavior by an AIS may be less than or greater than the\ndesign-stated level of autonomy. Neither case is desirable. Accordingly, the purpose of this study was to develop a\nmeasure for predicting the level of autonomy in an observed AIS.\nTo that end, we developed such a measure. The measure attempted to calculate the similarity between observed\nactions and known human equivalent actions. This calculation is possible because we conceptualized the array of\nactions in both instances as sets of string data types. Then, we employed a variation an edit distance calculation outlined\nin Lopresti and Zhou (1996). The adapted calculation suggested the greater edit distance (i.e., less similar), the more"}]}