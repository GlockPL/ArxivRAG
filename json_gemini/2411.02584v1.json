{"title": "MULTI-AGENT DECISION TRANSFORMERS FOR DYNAMIC\nDISPATCHING IN MATERIAL HANDLING SYSTEMS LEVERAGING\nENTERPRISE BIG DATA", "authors": ["Xian Yeow Lee", "Haiyan Wang", "Takaharu Matsui", "Daisuke Katsumata", "Chetan Gupta"], "abstract": "Dynamic dispatching rules that allocate resources to tasks in real-time play a critical role in ensuring\nefficient operations of many automated material handling systems across industries. Traditionally, the\ndispatching rules deployed are typically the result of manually crafted heuristics based on domain\nexperts' knowledge. Generating these rules is time-consuming and often sub-optimal. As enterprises\nincreasingly accumulate vast amounts of operational data, there is significant potential to leverage\nthis big data to enhance the performance of automated systems. One promising approach is to use\nDecision Transformers, which can be trained on existing enterprise data to learn better dynamic\ndispatching rules for improving system throughput. In this work, we study the application of Decision\nTransformers as dynamic dispatching policies within an actual multi-agent material handling system\nand identify scenarios where enterprises can effectively leverage Decision Transformers on existing\nbig data to gain business value. Our empirical results demonstrate that Decision Transformers can\nimprove the material handling system's throughput by a considerable amount when the heuristic\noriginally used in the enterprise data exhibits moderate performance and involves no randomness.\nWhen the original heuristic has strong performance, Decision Transformers can still improve the\nthroughput but with a smaller improvement margin. However, when the original heuristics contain an\nelement of randomness or when the performance of the dataset is below a certain threshold, Decision\nTransformers fail to outperform the original heuristic. These results highlight both the potential\nand limitations of Decision Transformers as dispatching policies for automated industrial material\nhandling systems.", "sections": [{"title": "1 Introduction", "content": "Dynamic dispatching, the process of dispatching resources in real-time in response to system conditions, plays a\ncritical role in ensuring smooth and efficient operations in many industrial applications. Subsequently, this translates to\nadditional business value, such as cost savings and increased customer satisfaction. One area where the deployment of\ndynamic dispatching has a large impact is in material handling systems, where goods are typically transported between\nmultiple points under the constraint of limited resources.\nTraditionally, dynamic dispatching in these systems is often deployed using heuristic rules manually designed via\na trial-and-error process or by a domain expert. Dynamic dispatching and scheduling are also applied in a wide\nvariety of fields, not just material handling systems, and there have been multiple works that attempt to generate\ndispatching rules via domain knowledge and heuristics Dhurasevic and Jakobovic [2018], Branke et al. [2015]. These"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Material Dispatching Simulator", "content": "In this section, we describe an instantiation of an actual material handling system used in this study. Specifically, we\nstudy a material handling system with a conveyor belt commonly used in warehouse facilities. To explain the system, a"}, {"title": "2.2 Decision Transformers", "content": "Decision Transformers, introduced by Chen et al. Chen et al. [2021], reformulate the offline reinforcement learning\n(RL) problem into a sequence modeling problem, leveraging the auto-regressive architecture of GPT-2 Radford et al.\n[2019]. The model takes as input a sequence, of length k, of past states {$\\mathcal{S}_{t-k},..., S_t$}, actions {$\\mathcal{a}_{t-k},..., a_{t-1}$}, and\nreturns-to-go {$\\mathcal{R}_{t-k},..., R_t$}, where $R_t = \\sum_{t'=t}^{T}r_{t'}$, representing future cumulative rewards. At each time step t,\nthe input tuple $x_t = (s_t, a_{t-1}, R_t)$ is encoded into a sequence $X_t = [x_0,...,x_t]$, which is processed by a series of\ntransformation to produce hidden states $H_t =$ Transformer($X_t$). The model predicts the next action $\\hat{a}_t$ by applying a\nlinear transformation and softmax function to the final hidden state $h_t$, so that $\\hat{a}_t = softmax(Wh_t + b)$, where W and\nb are learned parameters. The model is trained using a supervised loss function, conventionally the mean squared error\nfor continuous actions:\n$L_{supervised} = E_{(s,a,R)\\sim D} \\sum_{t=0}^{T} ||a_t - \\hat{a}_t ||^2$"}, {"title": "3 Formulation and Methodology", "content": ""}, {"title": "3.1 Formulation", "content": "This section presents the formulation that allows us to apply Decision Transformers as dynamic dispatching policies.\nFormally, the problem of dynamic dispatching in material handling systems can be defined as selecting a sequence of\ndecisions at each incoming point such that the maximum throughput of the system is maximized. This, in turn, could be\nabstracted into an RL formulation, where an RL-based policy observes the state of the system and selects an action,\nrepresenting the dispatching decision, to maximize a specific reward, in this case, the total throughput. With the dynamic\ndispatching problem cast as an RL problem, we can then follow the conventional routine of collecting/generating a\nstatic dataset consisting of tuples of (state, action, reward, done) and train off-the-shelf offline RL algorithms on it. In\nthis study, we focus on Decision Transformers due to their powerful capabilities in attending to sequential data and\nrelative ease of implementation.\nAnother design choice we made in this formulation is the choice of representation. As there are multiple incoming\npoints where dispatching occurs, we face a choice of representing the dispatching policy using a single centralized\nDecision Transformer versus multiple decentralized Decision Transformers. Since dispatching at the incoming points\noccurs asynchronously, we hypothesize that representing the dispatching policies in a multi-agent paradigm will be\nmore beneficial for several reasons. The first reason is that if we use a centralized Decision Transformer, intuitively, the\nmodel would have to learn not just to model the joint distributions of states and actions but also to model the probability\nof an event occurring at a certain incoming point, which is significantly more challenging. Additionally, a centralized\nDecision Transformer limits the approach's scalability, as adding incoming points to the system's layout would require\nretraining the model to account for the change in action space. In contrast, using multiple Decision Transformers\nsimplifies the modeling task and potentially allows us to re-use the same trained model for new incoming points since"}, {"title": "3.2 Environment and Heuristics Details", "content": "Due to confidentiality restrictions preventing the use of actual enterprise data for publication, we developed an in-house\nhigh-fidelity simulator that utilizes data distributions obtained from actual enterprise data to replicate the actual material\nhandling system operations and generate the data for training Decision Transformers. We implemented multiple\nheuristics that have been tested on the actual system in the simulator and verified that the simulator's specifications and\nproperties, such as the processing times and throughput match the actual system. After validating that the simulator is\ncalibrated to the actual system faithfully, we used the simulator to generate data for training the Decision Transformers.\nAlthough we used simulated data in this work, we emphasize that the framework remains directly applicable to actual\ndata. To collect the simulation data in a usable form, we define the simulator's state as all the sensor information that is\navailable in real-time and used by the existing heuristics: 1) Number of pallets heading to each storage point, 2) Number\nof pallets at conveyor's junction going into each downstream direction, 3) Inventory level at each storage, which leads\nto a 44-dimensional state vector. The action space is defined as the dispatching decisions and the reward is the total\nthroughput of the system. The full details of the actual system's layout are shown in Table 1"}, {"title": "3.3 Data Generation", "content": "For each heuristic, we ran 4000 one-hour simulations, corresponding to 4000 episodes. A one-hour simulation time\nperiod was selected because it represents a time-frame with sufficient events occurring and is also a commonly used\ntime-frame used for comparing throughput in the industry. Specifically, at each incoming process point, we collected\nthe data in the form of (state, action, reward, done) whenever an event, i.e., a dispatching decision is needed, occurs\nat the specific point. Since the events occur asynchronously for each incoming point, the total number of events"}, {"title": "3.4 Training", "content": "We trained a Decision Transformer for each dispatching point in the material handling system on each generated dataset\nand trained each model for 50 epochs based on the observed convergence and plateauing of the loss function. Since the\ndynamic dispatching in this problem necessitates discrete actions, we trained the models using a standard cross-entropy\nloss. Our implementation is based on a modification of the implementation by Beeching and Simonini [2022]. All\ntraining was performed on a single NVIDIA RTX 4090 GPU and the hyperparameters we used in our experiments are\ndefault implementation parameters and are shown in Table 2"}, {"title": "4 Results and Discussion", "content": ""}, {"title": "4.1 Results", "content": "In this section, we present empirical results of our experiments and answer the questions we posed."}, {"title": "4.2 Discussions", "content": "In this section, we discuss the implications of this work from an industry point of view. Historical data owned by\nenterprises in the industrial sector holds substantial potential for training Decision Transformers for decision-making\nsystems in an offline manner. Our study highlights that while Decision Transformers hold promise, their effectiveness\nis significantly influenced by the randomness in the underlying dataset and sub-optimal data can severely limit their\neffectiveness. To this end, we posit that filtering/ranking techniques and statistical methods could potentially be applied\nto the dataset to obtain a subset of higher-performing data and to detect the presence of randomness in the absence of\ndomain experts knowledge of the enterprise data. This underscores the importance for industries to not only gather\nlarge volumes of data but also serves as a guideline to determine when training Decision Transformers will result in\neffective dynamic dispatching strategies.\nOne of the main benefits of Decision Transformers or any offline RL approach in general for decision-making problems\nis the ability to circumvent the requirement of developing a high-fidelity simulator or training the agent on an actual\nsystem. However, to validate the effectiveness of these methods in actual industrial applications, a simulator is still\noften required to test and refine the policies as it is often too costly and risky to directly deploy a dispatching policy\non the system. Despite this, enterprises with extensive historical data may still find this method beneficial. From an\nimplementation perspective, it is simpler to leverage historical data to train Decision Transformers to achieve marginal\nimprovements due to less hyper-parameter tuning required, provided that the data meets the required conditions, as\ncompared to an online RL approach. Nonetheless, deployment of Decision Transformer-based dispatching policies\non to an actual system is also susceptible to the challenges that are common to most data-driven machine learning\nmethods, such as integration with existing infrastructure, distribution drifts and the occurrence of edge cases. While\nthese challenges are non-trivial, we believe that they're not specific to Decision Transformers or dynamic dispatching\nproblems and could potentially be overcome by solutions such as a meticulous system integration plans, real-time data\nmonitoring platforms and careful design of rules for exception handling, e.g., data transformation pipelines that ensure\ninput and output data lies within an acceptable range.\nOur findings highlighted other limitations of the current implementation of Decision Transformers, particularly the\nlack of a strong correlation between the specified target return and the actual return achieved. This points to the need\nfor further experimentation to isolate the cause of these issues as well as a study into more sophisticated versions of\nDecision Transformers, such as those using reward prediction mechanisms, to address these shortcomings. We believe\nthat future studies focusing on how existing data can be better utilized or augmented with domain knowledge to enhance\nthe capabilities of Decision Transformers, would be beneficial to improve their application in industrial automation\nsettings."}, {"title": "5 Conclusion", "content": "We show that Decision Transformers have the potential to be deployed as dynamic dispatching policies in material\nhandling systems under the assumption that existing enterprise data can be transformed into the required format and\nthat the underlying method generating the data is not inherently random and has a good initial performance. Due to the\nrelatively simple implementation of Decision Transformers, this has the potential to democratize the deployment of\nthese models in actual industrial systems to improve business KPIs such as system throughput without the need to train\nonline RL-based methods. Future works will focus on complementary improvements such as methods that could better\nincorporate probabilistic data into the training process, better communication strategies between the multiple agents,\nperhaps through a centralized critic and also hyperparameter tuning to improve the existing implementation of Decision\nTransformers. Additionally, benchmarking the performance of Decision Transformers against other state-of-the-art\noffline RL methods and testing the generalization capability of multi-agent Decision Transformers to other dynamic\ndispatching systems are also important avenues of study."}]}