{"title": "Is larger always better? Evaluating and prompting large language models for non-generative medical tasks", "authors": ["Yinghao Zhu", "Junyi Gao", "Zixiang Wang", "Weibin Liao", "Xiaochen Zheng", "Lifang Liang", "Yasha Wang", "Chengwei Pan", "Ewen M. Harrison", "Liantao Ma"], "abstract": "Background: The deployment of Large Language Models (LLMs) in medical fields is increasing, but few research evaluate their ability to manage both structured Electronic Health Record (EHR) data and unstructured clinical notes. This study provides a comprehensive benchmark of various models, including GPT-based LLMs, BERT-based language models, and conventional clinical predictive models to determine their efficacy in non-generative medical tasks using widely recognized datasets.\nMethods: We evaluated 14 language models (9 GPT-based and 5 BERT-based) alongside 7 conventional clinical predictive models on the MIMIC dataset, which encompasses comprehensive ICU patient records, and the TJH dataset, which includes EHR data from early COVID-19 patients. The performance of these models was assessed through a series of both supervised and unsupervised tasks, including mortality and readmission predictions, disease hierarchy reconstruction, and biomedical sentence matching. The study focused on comparing zero-shot prediction capabilities of LLMs with conventional models and the effectiveness of finetuned BERT-based models in handling clinical texts.\nResults: LLMs demonstrated promising zero-shot predictive abilities on structured EHR data when equipped with well-designed prompting strategies, often surpassing traditional models trained with limited datasets. However, in tasks involving unstructured medical texts, LLMs did not outperform existing BERT-based models. Both in supervised and unsupervised scenarios, the performance of finetuned BERT models was superior, suggesting that the additional cost and complexity of implementing LLMs might not be justifiable for non-generative tasks.\nConclusion: While LLMs show significant potential in interpreting structured EHR data through zero-shot learning, their application in processing unstructured medical texts is less advantageous compared to finetuned BERT models. The findings underscore the importance of selecting appropriate models based on specific task requirements and the nature of the data, facilitating more efficient use of NLP technology in healthcare settings.", "sections": [{"title": "1 Introduction", "content": "In recent years, Large Language Models (LLMs), particularly those based on Generative Pretrained Transformers (GPT), have demonstrated outstanding capabilities across various domains. These models, characterized by a decoder-only architecture, mark a significant departure from traditional encoder-only or encoder-decoder architectures, such as BERT\u00b9. GPT-based models have achieved widespread application due to their scale and complexity, outperforming BERT-based models in tasks ranging from casual chatbots to complex scientific analyses. In the medical field, GPT-based LLMs\u00b9 trained on datasets like PubMed and clinical notes excel in generative tasks such as patient interaction\u00b2, complex medical query resolution\u00b3, and passing medical licensing exams4.\nDespite these advancements, challenges persist in non-generative prediction tasks involving structured Electronic Health Record (EHR) data and unstructured medical texts, such as patient mortality or readmission predictions5-7. These tasks typically involve transforming multivariate longitudinal EHR data and unstructured texts into patient embeddings, subsequently used with classifiers for predictions. Two prevalent assumptions within the research community are: (1) The next-token prediction training paradigm of generative models, which aims to predict the next word in a sequence based on the context of preceding words, is ill-suited for handling sparse, longitudinal, and multivariate structured data9,10. (2) The largest or most advanced models are presumed to inherently deliver superior performance, particularly in natural language generative tasks11,12. However, it remains uncertain whether LLMs are inferior to conventional prediction models in handling structured medical data, or if they can surpass conventional natural language process (NLP) models in non-generative medical NLP tasks solely based on their performance in generative tasks. Current research in medical LLMs primarily focuses on generative tasks, with few studies evaluating their efficacy on non-generative tasks, not to mention proposing a systematic and comprehensive benchmark evaluation13, 14\nTo address the identified research gap, our study conducts a comprehensive benchmarking of various models across representative non-generative medical tasks. This benchmarking work compares conventional machine learning (ML) and deep learning (DL) models designed for clinical prediction with EHR data, BERT-based NLP models for processing medical texts, and both general and medically specialized Large Language Models (LLMs). Our aim is to provide actionable insights to medical researchers on selecting optimal models for different clinical tasks. Our evaluation is structured around the following key research questions:\n\u2022 For structured EHR data in non-generative clinical prediction tasks: How do LLMs compare to conventional small expert models designed for specific prediction clinical predictive tasks in terms of performance? Can enhanced prompting strategies improve the understanding of structured medical data by LLMs and their predictive accuracy? Additionally, considering the groundbreaking generalizability of LLMs on generative tasks, can LLMs directly conduct predictions on new datasets without fine-tuning (i.e., the zero-shot or few-shot learning settings)? Such settings evaluate the generalizability of LLMs' clinical applications.\n\u2022 For unstructured medical free-text data in non-generative clinical NLP tasks: Do LLMs outperform conventional BERT-based models in extracting clinical semantics from clinical notes in supervised tasks? Additionally, do LLMs exhibit a deeper understanding of clinical concepts and better embedding capabilities in unsupervised tasks?\nThe benchmarking framework is depicted in Figure 1. Our evaluation encompasses four tasks,"}, {"title": "2 Methods", "content": "This section outlines the experimental setup and benchmarking approaches used in our study. We have chosen the following widely recognized clinical benchmarking tasks to assess our model's performance5, 7, 16,19.\n\u2022 In-hospital mortality prediction: This task involves predicting whether a patient will survive their hospital stay, based on EHR data or clinical notes from the admission.\n\u2022 30-day readmission prediction: This task predicts whether a patient will be readmitted within 30 days post-discharge, utilizing EHR data or clinical notes from the initial hospitalization.\nFollowing previous EHR benchmarking works5,7, both supervised tasks are evaluated using the Area Under the ROC Curve (AUROC) and the Area Under the Precision-Recall Curve (AUPRC). Both tasks are evaluated using structured EHR data and unstructured clinical notes separately. All data are sourced from publicly available datasets: Tongji Hospital COVID-19 dataset (TJH)25 and MIMIC dataset24. Our data preprocessing follows methodologies from established benchmarks5,7,19. Dataset statistics are detailed in Table 1.\nTo exploit the semantic capabilities of language models, we also include two unsupervised tasks to evaluate the model performance with clinical texts:\n\u2022 Medical sentence matching: This task evaluates the semantic similarity between pairs of medical sentences. Human experts rate the similarity of each pair, and models generate semantic embeddings for these sentences. Model performance is assessed by calculating the correlation between the similarity scores derived from the model-generated embeddings and the human ratings. Higher correlation scores indicate a better alignment with human judgments.\n\u2022 ICD code clustering: In this task, we explore the reconstruction of the hierarchical structure of ICD codes. The ICD coding system has a hierarchical structure that can be represented as a tree, with leaf nodes representing specific codes (e.g., E001, Congenital iodine-deficiency syndrome, myxedematous type) and parent nodes representing broader categories (e.g., E00, Congenital iodine-deficiency syndrome). By clustering the text embeddings of the disease names, we can evaluate the average hierarchical distance within clusters. A shorter average hierarchical distance suggests better preservation of the original ICD hierarchy in the embeddings.\nFor the medical sentence matching task, we utilize the BIOSSES dataset22, which is a benchmark resource comprising 100 sentence pairs specifically designed for evaluating semantic similarity in the biomedical domain. For the ICD code clustering task, we employ the 2023 ICD-10 data26, selecting 11,942 ICD-10 codes with fewer than five digits to focus on broader categories that encompass more general medical concepts."}, {"title": "3 Results", "content": "In this section, we report our benchmarking results, including: (1) supervised results comparing conventional EHR prediction models with LLMs using longitudinal EHR data; (2) supervised results comparing BERT-based models with LLMs using clinical notes data; (3) unsupervised results comparing BERT-based models with LLMs using disease hierarchy and biomedical text data.\n3.1 Benchmarking Results of Supervised Tasks Using Structured EHR Data\nIn this subsection, we discuss the benchmarking of supervised tasks (i.e., mortality and readmission prediction) using structured EHR data. As previously discussed, the decoder-only architecture of LLMs, trained primarily on unstructured natural language texts, encounters challenges with the structured, sparse, and longitudinal nature of EHR data9, 10. To bridge the gap between the complex nature of EHR data and unstructured text-oriented LLMs, we develop a prompting strategy to better deliver structured EHR data to LLMs. As shown in Figure 3, the proposed prompting strategy employs a feature-wise list-style format for inputting EHR data and provides LLMs with feature units and reference range. An in-context learning strategy was also used to provide examples that guide the LLMs toward the correct output format. The prompting templates for the prediction tasks"}, {"title": "3.2 Benchmarking Results of Supervised Tasks Using Clinical Notes Data", "content": "This section assesses mortality and readmission prediction tasks using the MIMIC dataset, as the TJH dataset lacks clinical notes data. The results are detailed in Table 4.\nThree settings are considered for model evaluation: \u2018freeze', 'finetune', and 'generation'. In the 'freeze' setting, we use the pretrained model to generate text embeddings and train a classifier on these embeddings to assess the out-of-the-box performance of the language models. The \u2018finetune' setting involves further training of the language model during classifier training. The \u2018generation' setting prompts the language model to directly generate prediction results from clinical notes, akin to the previous EHR prediction approach. We only evaluate the performance of GPT-3.5 and GPT-4 under the generation setting, as these models are closed-sourced. The templates for the prediction tasks with clinical notes data are shown in Appendix D.3.\nSurprisingly, in the \u2018freeze' setting, BERT-based models, such as ClinicalBERT and GatorTron, perform comparably to LLMs for mortality prediction, and slightly better for readmission prediction."}, {"title": "3.3 Benchmarking Results of Unsupervised Tasks Using Biomedical Text Data", "content": "To further assess the models' proficiency with clinical concepts and their embedding capabilities, we conducted unsupervised tasks using unstructured biomedical texts. The performance of various language models on medical sentence matching and ICD code clustering is illustrated in Figure 4. For clarity, we use the Pearson distance44 ($$\\sqrt{1 \u2013 r}$$, where r is the Pearson correlation) to represent performance in the medical sentence matching task (indicated by purple bars on the left y-axis). For both tasks, lower bars indicate better clustering performance. BioGPT slightly outperforms others in sentence matching, whereas Clinical-Longformer excels in the ICD code clustering task. Overall, no significant performance differences are observed between BERT-based and GPT-based LLMs in these tasks. This finding suggests that large-scale LLMs may not be necessary for general medical text embedding tasks. Detailed performance analyses of these tasks are available in Appendix B,"}, {"title": "4 Discussion", "content": "4.1 Model Selection Considerations in Real-world Clinical Tasks\nOur extensive analysis of various models across multiple tasks and datasets yields specific recommendations for model selection in clinical settings. For tasks using structured electronic health"}, {"title": "4.2 Obstacles and explorative strategies for the applications of clinical LLMs", "content": "Our experimental results and practical experiences have uncovered several research gaps between current large language model (LLM) research and their real-world clinical applications:\n(1) Lack of comprehensive training and evaluations across medical tasks: Most clinical LLMs are trained on medical literature and evaluated on generative tasks. Few explore non-generative applications, possibly due to challenges in accessing quality, large-scale EHR data or require-ments for deeper medical understanding. This gap significantly restricts LLM applications within decision-driven clinical workflows. Enhancing training and evaluation across diverse medical tasks could enable researchers to better gauge the capabilities and limitations of LLMs in clinical contexts.\n(2) Structural limitation in processing longitudinal EHR data: Despite promising zero-shot performance, LLMs lag behind traditional models when ample data is available. The inherent structure of LLMs struggles with longitudinal EHR data utilization. Integrating conventional predictive models using agent-based frameworks could leverage the accuracy and trustworthiness of clinical models with the expansive knowledge and adaptive capabilities of LLMs, potentially reducing prediction errors and hallucinations.\n(3) Deployment complexity, environmental impact, and privacy concerns: Deploying LLMs in secure healthcare environments presents challenges due to high computational costs and potential privacy issues. Our analysis, as detailed in Table 6, shows that larger models, especially generative ones, require substantial resources, increasing carbon emissions. The deployment in environments with limited GPU capacity and strict data privacy regulations poses further obstacles. Although solutions like Microsoft Azure's privacy-guaranteed LLM service exist, they may not align with all local data governance policies, particularly those restricting external internet access.\nIn conclusion, addressing these gaps calls for enhanced model training, evaluation, integration, and deployment strategies by clinical and NLP researchers, alongside better data governance from relevant institutions, to maximize the potential of LLMs in clinical settings."}, {"title": "4.3 Limitations and Future Works", "content": "Our study provides detailed benchmarking results for various models on non-generative medical tasks, yet it is not without limitations:\n\u2022 Limited task and dataset diversity: Our evaluations were confined to mortality and readmission predictions using the MIMIC and TJH datasets. Future research should broaden the scope to include diverse clinical tasks like diagnosis prediction, treatment recommendation, and adverse event forecasting, to provide a more thorough evaluation of model performance across various medical fields.\n\u2022 Model selection: Although our study included a comprehensive range of models, the rapid advancements in the field might have led to the development of newer models post our study period. Continuous incorporation of emerging state-of-the-art models will ensure that our benchmarks remain relevant and informative."}, {"title": "A Detailed Experimental Setups", "content": "This section details our experimental setups, highlighting tasks, datasets, evaluation metrics, problem formulation, baseline models, and implementation details for reproducibility.\nA.1 Tasks\nIn the context of deep learning, tasks can be broadly classified into two categories: generative and non-generative. Generative tasks involve creating or generating new content based on learned patterns and data, such as generating text summaries or translating languages. Non-generative tasks, typically are those discriminative tasks, which are our focus, involve classification, regression, or knowledge extraction without creating new content. These non-generative tasks analyze data to categorize, identify, or predict based on existing patterns46.\nWhile in the medical domain, non-generative diagnostic tasks are much more common and widely used, e.g., disease risk prediction47, mortality outcome prediction7, biomarker identification48, etc., are all non-generative tasks. Therefore, with a focus on non-generative medical tasks, we evaluate the performance of LLMs and conventional methods on the following non-generative tasks under two main data modalities in clinical practice: structured EHR and clinical notes38,49.\n(1) In-hospital mortality prediction: This task aims to predict patient mortality outcomes during their hospital stay, classifying whether a patient will survive (label 0) or die (label 1) based on various data modalities. Accurate mortality prediction models can assist healthcare providers in identifying high-risk patients, enabling timely interventions, optimizing resource allocation, and ultimately improving patient outcomes. Early identification of patients at risk of mortality allows for proactive care planning and targeted treatment strategies, potentially reducing mortality rates and enhancing the quality of care delivered.\n(2) 30-day readmission prediction: The goal of this task is to predict the likelihood of a patient being readmitted to the hospital within 30 days of their initial discharge, classifying patients into two categories: those not likely to be readmitted (label 0) and those with a higher probability of readmission (label 1). Accurate readmission prediction can help healthcare providers design targeted interventions and post-discharge care plans to reduce readmission rates, improve patient outcomes, and optimize healthcare resource utilization. By identifying patients at high risk of readmission, providers can implement personalized follow-up care, patient education, and care coordination to prevent avoidable readmissions and improve the continuity of care.\nThe two chosen non-generative diagnostic tasks serve as crucial references for patient health status in clinical decision-making, offering insights into outcomes, resource utilization, and care planning to aid personalized, high-quality care7,39,49, 50.\nIn addition to these conventional supervised prediction tasks, we also include two unsupervised prediction tasks to assess models' representation quality over unstructured clinical notes data.\n(1) Medical sentence matching: This task assesses the ability of models to determine the semantic similarity between pairs of sentences in the medical domain. Human experts evaluate and assign similarity scores to each pair of sentences, which the models then process to generate vector representations (embeddings) that capture the semantic information. The performance of the models is evaluated by comparing their predicted similarity scores against the human-labeled scores using metrics such as Pearson correlation, with higher correlations indicating better alignment with human judgments. The task enables healthcare professionals to group similar patient information, retrieve relevant literature, and support decision-making by identifying"}, {"title": "A.2 Datasets", "content": "This study utilizes two primary datasets: TJH25 (structured EHR data) and MIMIC24 (structured EHR data and unstructured clinical notes). The TJH dataset is chosen for its relevance to the early stages of the COVID-19 pandemic, a period characterized by a sudden surge in cases and significant pressure on healthcare services. As such, it serves as an ideal representation of an emerging disease scenario, aligning with our research scope of simulating the occurrence of a new emergency disease. The MIMIC dataset, on the other hand, is selected for its extensive nature and wide usage in healthcare research7, particularly for simulating ICU scenarios. Its inclusion of both structured EHR data and unstructured clinical notes allows for a comprehensive evaluation of model performance across multiple data modalities. Therefore, the MIMIC dataset provides a solid foundation for comparative analysis. It supports both mortality (classification) and readmission (classification) prediction tasks, aligning with our research goals.\n\u2022 TJH Dataset25: Derived from Tongji Hospital of Tongji Medical College, the TJH dataset consists of 485 anonymized COVID-19 inpatients treated in Wuhan, China, from January 10 to February 24, 2020. It includes 73 lab test features and 2 demographic features. The dataset is publicly available on GitHub (https://github.com/HAIRLAB/Pre_Surv_COVID_19, also available in original paper's supplementary information and data and code availability section25.).\n\u2022 MIMIC Dataset24: Sourced from the EHRs of the Beth Israel Deaconess Medical Center, the MIMIC dataset is extensive and widely used in healthcare research, particularly for simulating ICU scenarios. It is divided into \u201cMIMIC-EHR\u201d, which contains the structured EHR data, and \"MIMIC-Note\u201d, which contains the clinical notes. For MIMIC-EHR, 17 lab test features and 2 demographic features are extracted. To minimize missing data, we consolidate every consecutive 12-hour segment into a single record for each patient, focusing on the first 48 records. For MIMIC-Note, unstructured text data from clinical reports records are utilized."}, {"title": "A.3 Evaluation Metrics", "content": "We follow the benchmark preprocessing pipeline as established in the studies 7,51 for both datasets. For structured EHR data, we apply the Last Observation Carried Forward (LOCF) imputation strategy52 to handle missing values. In the preprocessing of clinical notes, we utilize the Clinical-Longformer approach42, which includes minimal but essential steps: removing all de-identification placeholders to protect Protected Health Information (PHI), replacing non-alphanumeric characters and punctuation marks, converting all letters to lowercase for consistency, and stripping extra white spaces. These standardized preprocessing techniques ensure uniformity and comparability in our data analysis. Moreover, following the benchmark's operation, we apply a stratified shuffled strategy with random selection to construct the training, validation and test set, among them, the test set is the same for all models to ensure fair comparison.\nFor the zero-shot setting with LLMs, we focus primarily on the test set, while the training and validation sets are specifically accessed by ML and DL models. In the instruction prompt where we have adopted the in-context learning53 strategy, we simulate the examples with randomly generated values in the designed instruction prompt to avoid leaking the dataset.\nThe unsupervised tasks, medical sentence matching and ICD code clustering, are evaluated based on the following datasets:\n(1) Medical sentence matching: For the medical sentence matching task, we utilize the BIOSSES (Biomedical Semantic Similarity Estimation System) dataset22, a benchmark resource specif-ically designed for evaluating semantic similarity in the biomedical domain. The BIOSSES dataset comprises 100 sentence pairs, each selected from the TAC (Text Analysis Conference) Biomedical Summarization Track Training Dataset, which contains articles from the biomedical literature. Each pair of sentences in the BIOSSES dataset was evaluated by five human experts, who assigned similarity scores ranging from 0 (indicating no relation) to 4 (indicating equiva-lence). The average of these scores across the five annotators serves as the gold standard for comparison.\n(2) ICD code clustering: For the ICD code clustering task, we utilize the 2023 ICD-10-CM (Inter-national Classification of Diseases, Tenth Revision, Clinical Modification) dataset. This dataset includes the latest updates effective from April 1, 2023, provided by the Centers for Disease Con-trol and Prevention's (CDC) National Center for Health Statistics (NCHS). The dataset contains a total of 96,795 ICD codes. For the purpose of clustering, we select 11,942 codes by extracting ICD codes with a length of four characters or fewer, aiming to capture broader categories that encapsulate more general medical concepts. The dataset can be downloaded at https://www.cms.gov/medicare/coding-billing/icd-10-codes/2023-icd-10-cm, specif-ically from the file named \u201c2023 Code Descriptions in Tabular Order - updated 01/11/2023 (ZIP)\" under the \u201cicd10cm_order_2023.txt\u201d file.\nWe employ following metrics for the binary classification task (including in-hospital mortality prediction and 30-day readmission prediction tasks):\n(1) AUROC: This is our primary metric in the binary classification task due to its widespread use in clinical settings and effectiveness in imbalanced datasets54.\n(2) AUPRC: AUPRC is particularly useful for evaluating performance in datasets with a significant imbalance between classes55.\n(3) Missing Rate: This metric is calculated as $$\\frac{n_{test}-n_{decoded}}{n_{test}} \u00d7 100%$$, where ntest represents the total"}, {"title": "A.4 Problem Formulation", "content": "A.4.1 Non-generative Supervised Medical Diagnostic Tasks\nLet $$\\mathcal{D}$$ = {$$(xi, yi)$$}i=1N denote a dataset containing N samples, where $$\\boldsymbol{x}_i$$ represents the input data (either structured EHR or unstructured clinical notes) for the i-th patient, and yi denotes the corresponding label or target value. The goal is to learn a mapping function $$\\boldsymbol{f}: x_i \\rightarrow y_i$$ that predicts the output $$\\boldsymbol{y}_i$$ given the input $$\\boldsymbol{x}_i$$.\nSpecifically, for the structured EHR data, each input $$\\boldsymbol{x}_i$$ consists of a sequence of visits $$\\boldsymbol{x}_i$$ = {$$(v_{i, 1}, v_{i,2},..., v_{i, T_i})$$}, where $$\\boldsymbol{v}_{i,t}$$ represents the data associated with the t-th visit of the i-th patient, and T\u00a1 denotes the total number of visits for the i-th patient. The data for each visit $$\\boldsymbol{v}_{i,t}$$ includes dynamic information such as lab tests and vital signs, as well as static information like demographics.\nIn the context of LLMs, we utilize pre-designed prompt templates that encapsulate the raw EHR data $$\\boldsymbol{x}_i$$, along with optional external information such as system prompts56, units, or reference ranges. The LLM is instructed to perform a specific task based on the input prompt. For binary"}, {"title": "A.4.2 Medical Sentence Matching", "content": "classification tasks, the LLM is expected to respond with a floating-point number representing the predicted probability.\nFormally, let $$\\mathcal{P}$$(xi, ei) denote the prompt template function that takes the input data $$\\boldsymbol{x}_i$$ and optional external information ei to generate the input prompt for the LLM. The LLM mapping function can be defined as:\nfLLM : LLM($$\\mathcal{P}$$(xi, ei)) \u2192 \u0177i\n(1)\nwhere \u0177i is the generated output of the LLM, also serving as the prediction result.\nFor conventional models, models are fine-tuned to predict the corresponding task label yi. To compare the representation quality of different models, we consider two fine-tuning approaches:\n(1) Integrating the pretrained backbone with a newly initialized prediction head and updating all parameters of the combined model during training.\n(2) Freezing the pretrained backbone parameters and training only the newly initialized prediction head.\nLet $$\\mathcal{D}MSM = {(\\boldsymbol{s}_i, \\boldsymbol{s}_j, score_{ij})}_1M$$ be a dataset containing M pairs of medical sentences, where si and sj are the sentences in the k-th pair, and scoreij is the human-annotated similarity score for the sentence pair (si, sj). The objective is to evaluate the correlation of the similarity derived from the model and the similarity annotated by humans, i.e., correlation : sim($$\\boldsymbol{s}_i$$, $$\\boldsymbol{s}_j$$) \u2192 scoreij.\nWe use embeddings e\u00a1 and ej for the sentences s\u012f and sj, respectively, derived from the pretrained language model. The predicted similarity score sc\u00f4reij is computed using a similarity function sim($$\\boldsymbol{e}_i$$, $$\\boldsymbol{e}_j$$), such as cosine similarity:\nsim(si, sj) = sim($$\\boldsymbol{e}_i$$, $$\\boldsymbol{e}_j$$) = $$\\frac{e_i e_j}{||e_i||||e_j||}$$\n(2)\nThe performance is evaluated using correlation metrics (Pearson, Spearman, Kendall) between the estimated similarity scores {sim($$\\boldsymbol{s}_i$$,$$\\boldsymbol{s}_j$$)} and the human-annotated scores {scoreij}."}, {"title": "A.4.3 ICD Code Clustering", "content": "Let DICD = {di}i=1L be a dataset containing L disease descriptions, where d\u012f represents the i-th disease description. Each description d\u012f is associated with an ICD code ICD\u012f. The objective is to evaluate models' representations e\u00a1 for each disease description d\u012f, where the similar diseases (according to the ICD hierarchy) have more similar embeddings is treated as better representation quality.\nWe perform clustering on the embeddings {e;} (computed from the pretrained language model) using a clustering algorithm (e.g., K-means). The quality of the clustering is evaluated by measuring the average intra-cluster distance distavg, where distavg is defined as the average pairwise distance between embeddings within the same cluster. The distance between two ICD codes is measured according to their positions in the ICD hierarchy:\ndistICD(ICDa, ICD\u266d) = dist(ICDa, LCA(ICDa, ICD\u266d))+dist(ICD\u266d,LCA(ICDa, ICD\u266d)) (3)\nwhere LCA(ICDa, ICD\u266d) is the lowest common ancestor of ICDa and ICD\u266d in the ICD tree, and dist(ICD, LCA(ICDa, ICD\u266d)) is the distance from ICDx to this lowest common ancestor."}, {"title": "A.5 Baseline Models", "content": "The average intra-cluster distance is given by:\ndistava = $$\\frac{1}{c} \u03a3_C \\frac{1}{|C|(|C|-1)} \u03a3_{i,jecij}  dist_{ICD} (ICD_i, ICD_j)$$\n(4)\nwhere & is the set of clusters, C is a cluster in 6, and |C| is the number of elements in cluster C.\nThe clustering performance reflects the quality of the embeddings in capturing the hierarchical structure of the ICD codes.\nIn summary, the unsupervised tasks of medical sentence matching and ICD code clustering evaluate the representation quality of the models, providing insights into their effectiveness in capturing semantic and hierarchical relationships in the medical domain.\nTo perform the above tasks, we comprehensively select conventional EHR-specific models and text-specific models to compare with large language models (LLMs). LLMs serve as general-purpose models capable of performing on diverse data modalities and tasks, whether supervised or unsupervised. We aim to explore if these LLMs consistently outperform conventional models in non-generative tasks.\nA.5.1 Clinical Predictive Models\nThe following models are applied on structured EHR prediction tasks, encompassing machine learning and deep learning methods, as well as models specifically designed for EHR data.\n(1) Decision Tree57: It splits data into branches at decision nodes represented visually as a tree structure. It utilizes simple rules inferred from features to progressively classify or predict outcomes, making it straightforward yet powerful for clinical decision-making tasks.\n(2) XGBoost35: It is an advanced implementation of gradient boosting, offering robust handling of various types of data and extensive support for fine-tuning and customization. Its efficiency and performance make it popular for predicting patient outcomes from EHR data.\n(3) GRU36: It efficiently captures dependencies in sequence data through gating mechanisms which regulate the flow of information. This capability is essential for processing time-series EHR data to predict future health events.\n(4) ConCare: It utilizes a multi-channel GRU with a time-aware attention mechanism to extract clinical features and re-encode the clinical information by capturing the interdependencies between features. It is specifically designed to handle the complexity of EHR data.\n(5) GRASP37: It is a generic framework for healthcare models, leveraging information extracted from patients with similar conditions to enhance cohort representation learning results. This approach improves the accuracy of patient outcome predictions.\n(6) M3Care38: It addresses the missing modality issue in EHR data by utilizing similar patients' existing modalities. Specifically, M3Care imputes task-related information in the latent space rather than generating raw missing data, enhancing stability and accuracy.\n(7) AICare39: It includes a multi-channel feature extraction module and an adaptive feature importance recalibration module. It learns personalized health status embeddings with both static and dynamic features, improving patient-specific predictions."}, {"title": "A.5.2 BERT-based Models", "content": "Building upon the BERT model, several variants are tailored for processing and extracting valuable information from unstructured clinical texts, enhancing the understanding and utility of medical documents.\n(1) BERT\u00b9: It leverages a transformer architecture with a focus on the encoder component, pro-cessing text bidirectionally to capture contextual information from both sides of each token across all layers. This makes BERT effective for tasks like medical document classification and information extraction.\n(2) ClinicalBERT19: It is an adaptation of BERT designed to effectively represent and extract information from unstructured clinical notes by capturing nuanced relationships between medical concepts. It is optimized for healthcare-specific language and tasks.\n(3) BioBERT40: It is a specialized version of BERT tailored for the biomedical field. It enhances the extraction of valuable information from biomedical literature, improving the effectiveness of biomedical text mining models.\n(4) GatorTron41: It is a clinical language model with 8.9 billion parameters, trained on over 90 billion words, including 82 billion de-identified clinical texts. It significantly improves performance on five clinical NLP tasks, including clinical concept extraction and medical question answering.\n(5) Clinical-Longformer42: It is a clinical knowledge enriched version of Longformer, pretrained using MIMIC-III clinical notes. It allows up to 4,096 tokens as the model input by utilizing a sparse attention mechanism."}, {"title": "A.5.3 Large Language Models", "content": "LLMs are basically general-purpose, designed to handle a wide range of tasks across various data types, including medical data, and can be adapted for more specific applications in the healthcare domain.\n(1) GPT-28: It is trained on a vast dataset of WebText and demonstrates the ability to perform NLP tasks. It improves log-linearly with increases in capacity and achieves competitive results on benchmarks, showing potential for medical text analysis.\n(2) Llama-327: It features significant upgrades to Llama-2 like a more efficient 128K token vocabulary and grouped query attention (GQA) in both 8B and 70B models. Additionally, Llama-3 offers improved reasoning and code generation capabilities, and has been trained on a substantially larger dataset of over 15 trillion tokens.\n(3) GPT-3.5 (ChatGPT)28: It employs Reinforcement Learning from Human Feedback (RLHF). The process begins with supervised fine-tuning and transitions to reinforcement learning, sub-stantially improving its conversational abilities through the use of a structured dialogue format, which can be beneficial for interactive medical applications.\n(4) GPT-429: It is a large-scale, latest one of the most advanced LLM developed by OpenAI. It achieves human-level performance on various benchmarks, including the United States Medical Licensing Examination (USMLE), indicating its potential for high-stakes medical decision support.\nMedical LLMs are specifically designed and fine-tuned for the medical domain from the general-purpose LLMs, enhancing their effectiveness in healthcare-related tasks."}, {"title": "A.6 Implementation Details", "content": "Our experimental phase spans from December 19"}, {"title": "Is larger always better? Evaluating and prompting large language models for non-generative medical tasks", "authors": ["Yinghao Zhu", "Junyi Gao", "Zixiang Wang", "Weibin Liao", "Xiaochen Zheng", "Lifang Liang", "Yasha Wang", "Chengwei Pan", "Ewen M. Harrison", "Liantao Ma"], "abstract": "Background: The deployment of Large Language Models (LLMs) in medical fields is increasing, but few research evaluate their ability to manage both structured Electronic Health Record (EHR) data and unstructured clinical notes. This study provides a comprehensive benchmark of various models, including GPT-based LLMs, BERT-based language models, and conventional clinical predictive models to determine their efficacy in non-generative medical tasks using widely recognized datasets.\nMethods: We evaluated 14 language models (9 GPT-based and 5 BERT-based) alongside 7 conventional clinical predictive models on the MIMIC dataset, which encompasses comprehensive ICU patient records, and the TJH dataset, which includes EHR data from early COVID-19 patients. The performance of these models was assessed through a series of both supervised and unsupervised tasks, including mortality and readmission predictions, disease hierarchy reconstruction, and biomedical sentence matching. The study focused on comparing zero-shot prediction capabilities of LLMs with conventional models and the effectiveness of finetuned BERT-based models in handling clinical texts.\nResults: LLMs demonstrated promising zero-shot predictive abilities on structured EHR data when equipped with well-designed prompting strategies, often surpassing traditional models trained with limited datasets. However, in tasks involving unstructured medical texts, LLMs did not outperform existing BERT-based models. Both in supervised and unsupervised scenarios, the performance of finetuned BERT models was superior, suggesting that the additional cost and complexity of implementing LLMs might not be justifiable for non-generative tasks.\nConclusion: While LLMs show significant potential in interpreting structured EHR data through zero-shot learning, their application in processing unstructured medical texts is less advantageous compared to finetuned BERT models. The findings underscore the importance of selecting appropriate models based on specific task requirements and the nature of the data, facilitating more efficient use of NLP technology in healthcare settings.", "sections": [{"title": "1 Introduction", "content": "In recent years, Large Language Models (LLMs), particularly those based on Generative Pretrained Transformers (GPT), have demonstrated outstanding capabilities across various domains. These models, characterized by a decoder-only architecture, mark a significant departure from traditional encoder-only or encoder-decoder architectures, such as BERT\u00b9. GPT-based models have achieved widespread application due to their scale and complexity, outperforming BERT-based models in tasks ranging from casual chatbots to complex scientific analyses. In the medical field, GPT-based LLMs\u00b9 trained on datasets like PubMed and clinical notes excel in generative tasks such as patient interaction\u00b2, complex medical query resolution\u00b3, and passing medical licensing exams4.\nDespite these advancements, challenges persist in non-generative prediction tasks involving structured Electronic Health Record (EHR) data and unstructured medical texts, such as patient mortality or readmission predictions5-7. These tasks typically involve transforming multivariate longitudinal EHR data and unstructured texts into patient embeddings, subsequently used with classifiers for predictions. Two prevalent assumptions within the research community are: (1) The next-token prediction training paradigm of generative models, which aims to predict the next word in a sequence based on the context of preceding words, is ill-suited for handling sparse, longitudinal, and multivariate structured data9,10. (2) The largest or most advanced models are presumed to inherently deliver superior performance, particularly in natural language generative tasks11,12. However, it remains uncertain whether LLMs are inferior to conventional prediction models in handling structured medical data, or if they can surpass conventional natural language process (NLP) models in non-generative medical NLP tasks solely based on their performance in generative tasks. Current research in medical LLMs primarily focuses on generative tasks, with few studies evaluating their efficacy on non-generative tasks, not to mention proposing a systematic and comprehensive benchmark evaluation13, 14\nTo address the identified research gap, our study conducts a comprehensive benchmarking of various models across representative non-generative medical tasks. This benchmarking work compares conventional machine learning (ML) and deep learning (DL) models designed for clinical prediction with EHR data, BERT-based NLP models for processing medical texts, and both general and medically specialized Large Language Models (LLMs). Our aim is to provide actionable insights to medical researchers on selecting optimal models for different clinical tasks. Our evaluation is structured around the following key research questions:\n\u2022 For structured EHR data in non-generative clinical prediction tasks: How do LLMs compare to conventional small expert models designed for specific prediction clinical predictive tasks in terms of performance? Can enhanced prompting strategies improve the understanding of structured medical data by LLMs and their predictive accuracy? Additionally, considering the groundbreaking generalizability of LLMs on generative tasks, can LLMs directly conduct predictions on new datasets without fine-tuning (i.e., the zero-shot or few-shot learning settings)? Such settings evaluate the generalizability of LLMs' clinical applications.\n\u2022 For unstructured medical free-text data in non-generative clinical NLP tasks: Do LLMs outperform conventional BERT-based models in extracting clinical semantics from clinical notes in supervised tasks? Additionally, do LLMs exhibit a deeper understanding of clinical concepts and better embedding capabilities in unsupervised tasks?\nThe benchmarking framework is depicted in Figure 1. Our evaluation encompasses four tasks,"}, {"title": "2 Methods", "content": "This section outlines the experimental setup and benchmarking approaches used in our study. We have chosen the following widely recognized clinical benchmarking tasks to assess our model's performance5, 7, 16,19.\n\u2022 In-hospital mortality prediction: This task involves predicting whether a patient will survive their hospital stay, based on EHR data or clinical notes from the admission.\n\u2022 30-day readmission prediction: This task predicts whether a patient will be readmitted within 30 days post-discharge, utilizing EHR data or clinical notes from the initial hospitalization.\nFollowing previous EHR benchmarking works5,7, both supervised tasks are evaluated using the Area Under the ROC Curve (AUROC) and the Area Under the Precision-Recall Curve (AUPRC). Both tasks are evaluated using structured EHR data and unstructured clinical notes separately. All data are sourced from publicly available datasets: Tongji Hospital COVID-19 dataset (TJH)25 and MIMIC dataset24. Our data preprocessing follows methodologies from established benchmarks5,7,19. Dataset statistics are detailed in Table 1.\nTo exploit the semantic capabilities of language models, we also include two unsupervised tasks to evaluate the model performance with clinical texts:\n\u2022 Medical sentence matching: This task evaluates the semantic similarity between pairs of medical sentences. Human experts rate the similarity of each pair, and models generate semantic embeddings for these sentences. Model performance is assessed by calculating the correlation between the similarity scores derived from the model-generated embeddings and the human ratings. Higher correlation scores indicate a better alignment with human judgments.\n\u2022 ICD code clustering: In this task, we explore the reconstruction of the hierarchical structure of ICD codes. The ICD coding system has a hierarchical structure that can be represented as a tree, with leaf nodes representing specific codes (e.g., E001, Congenital iodine-deficiency syndrome, myxedematous type) and parent nodes representing broader categories (e.g., E00, Congenital iodine-deficiency syndrome). By clustering the text embeddings of the disease names, we can evaluate the average hierarchical distance within clusters. A shorter average hierarchical distance suggests better preservation of the original ICD hierarchy in the embeddings.\nFor the medical sentence matching task, we utilize the BIOSSES dataset22, which is a benchmark resource comprising 100 sentence pairs specifically designed for evaluating semantic similarity in the biomedical domain. For the ICD code clustering task, we employ the 2023 ICD-10 data26, selecting 11,942 ICD-10 codes with fewer than five digits to focus on broader categories that encompass more general medical concepts."}, {"title": "3 Results", "content": "In this section, we report our benchmarking results, including: (1) supervised results comparing conventional EHR prediction models with LLMs using longitudinal EHR data; (2) supervised results comparing BERT-based models with LLMs using clinical notes data; (3) unsupervised results comparing BERT-based models with LLMs using disease hierarchy and biomedical text data.\n3.1 Benchmarking Results of Supervised Tasks Using Structured EHR Data\nIn this subsection, we discuss the benchmarking of supervised tasks (i.e., mortality and readmission prediction) using structured EHR data. As previously discussed, the decoder-only architecture of LLMs, trained primarily on unstructured natural language texts, encounters challenges with the structured, sparse, and longitudinal nature of EHR data9, 10. To bridge the gap between the complex nature of EHR data and unstructured text-oriented LLMs, we develop a prompting strategy to better deliver structured EHR data to LLMs. As shown in Figure 3, the proposed prompting strategy employs a feature-wise list-style format for inputting EHR data and provides LLMs with feature units and reference range. An in-context learning strategy was also used to provide examples that guide the LLMs toward the correct output format. The prompting templates for the prediction tasks"}, {"title": "3.2 Benchmarking Results of Supervised Tasks Using Clinical Notes Data", "content": "This section assesses mortality and readmission prediction tasks using the MIMIC dataset, as the TJH dataset lacks clinical notes data. The results are detailed in Table 4.\nThree settings are considered for model evaluation: \u2018freeze', 'finetune', and 'generation'. In the 'freeze' setting, we use the pretrained model to generate text embeddings and train a classifier on these embeddings to assess the out-of-the-box performance of the language models. The \u2018finetune' setting involves further training of the language model during classifier training. The \u2018generation' setting prompts the language model to directly generate prediction results from clinical notes, akin to the previous EHR prediction approach. We only evaluate the performance of GPT-3.5 and GPT-4 under the generation setting, as these models are closed-sourced. The templates for the prediction tasks with clinical notes data are shown in Appendix D.3.\nSurprisingly, in the \u2018freeze' setting, BERT-based models, such as ClinicalBERT and GatorTron, perform comparably to LLMs for mortality prediction, and slightly better for readmission prediction."}, {"title": "3.3 Benchmarking Results of Unsupervised Tasks Using Biomedical Text Data", "content": "To further assess the models' proficiency with clinical concepts and their embedding capabilities, we conducted unsupervised tasks using unstructured biomedical texts. The performance of various language models on medical sentence matching and ICD code clustering is illustrated in Figure 4. For clarity, we use the Pearson distance44 ($\\sqrt{1 \u2013 r}$$, where r is the Pearson correlation) to represent performance in the medical sentence matching task (indicated by purple bars on the left y-axis). For both tasks, lower bars indicate better clustering performance. BioGPT slightly outperforms others in sentence matching, whereas Clinical-Longformer excels in the ICD code clustering task. Overall, no significant performance differences are observed between BERT-based and GPT-based LLMs in these tasks. This finding suggests that large-scale LLMs may not be necessary for general medical text embedding tasks. Detailed performance analyses of these tasks are available in Appendix B,"}, {"title": "4 Discussion", "content": "4.1 Model Selection Considerations in Real-world Clinical Tasks\nOur extensive analysis of various models across multiple tasks and datasets yields specific recommendations for model selection in clinical settings. For tasks using structured electronic health"}, {"title": "4.2 Obstacles and explorative strategies for the applications of clinical LLMs", "content": "Our experimental results and practical experiences have uncovered several research gaps between current large language model (LLM) research and their real-world clinical applications:\n(1) Lack of comprehensive training and evaluations across medical tasks: Most clinical LLMs are trained on medical literature and evaluated on generative tasks. Few explore non-generative applications, possibly due to challenges in accessing quality, large-scale EHR data or require-ments for deeper medical understanding. This gap significantly restricts LLM applications within decision-driven clinical workflows. Enhancing training and evaluation across diverse medical tasks could enable researchers to better gauge the capabilities and limitations of LLMs in clinical contexts.\n(2) Structural limitation in processing longitudinal EHR data: Despite promising zero-shot performance, LLMs lag behind traditional models when ample data is available. The inherent structure of LLMs struggles with longitudinal EHR data utilization. Integrating conventional predictive models using agent-based frameworks could leverage the accuracy and trustworthiness of clinical models with the expansive knowledge and adaptive capabilities of LLMs, potentially reducing prediction errors and hallucinations.\n(3) Deployment complexity, environmental impact, and privacy concerns: Deploying LLMs in secure healthcare environments presents challenges due to high computational costs and potential privacy issues. Our analysis, as detailed in Table 6, shows that larger models, especially generative ones, require substantial resources, increasing carbon emissions. The deployment in environments with limited GPU capacity and strict data privacy regulations poses further obstacles. Although solutions like Microsoft Azure's privacy-guaranteed LLM service exist, they may not align with all local data governance policies, particularly those restricting external internet access.\nIn conclusion, addressing these gaps calls for enhanced model training, evaluation, integration, and deployment strategies by clinical and NLP researchers, alongside better data governance from relevant institutions, to maximize the potential of LLMs in clinical settings."}, {"title": "4.3 Limitations and Future Works", "content": "Our study provides detailed benchmarking results for various models on non-generative medical tasks, yet it is not without limitations:\n\u2022 Limited task and dataset diversity: Our evaluations were confined to mortality and readmission predictions using the MIMIC and TJH datasets. Future research should broaden the scope to include diverse clinical tasks like diagnosis prediction, treatment recommendation, and adverse event forecasting, to provide a more thorough evaluation of model performance across various medical fields.\n\u2022 Model selection: Although our study included a comprehensive range of models, the rapid advancements in the field might have led to the development of newer models post our study period. Continuous incorporation of emerging state-of-the-art models will ensure that our benchmarks remain relevant and informative."}, {"title": "A Detailed Experimental Setups", "content": "This section details our experimental setups, highlighting tasks, datasets, evaluation metrics, problem formulation, baseline models, and implementation details for reproducibility.\nA.1 Tasks\nIn the context of deep learning, tasks can be broadly classified into two categories: generative and non-generative. Generative tasks involve creating or generating new content based on learned patterns and data, such as generating text summaries or translating languages. Non-generative tasks, typically are those discriminative tasks, which are our focus, involve classification, regression, or knowledge extraction without creating new content. These non-generative tasks analyze data to categorize, identify, or predict based on existing patterns46.\nWhile in the medical domain, non-generative diagnostic tasks are much more common and widely used, e.g., disease risk prediction47, mortality outcome prediction7, biomarker identification48, etc., are all non-generative tasks. Therefore, with a focus on non-generative medical tasks, we evaluate the performance of LLMs and conventional methods on the following non-generative tasks under two main data modalities in clinical practice: structured EHR and clinical notes38,49.\n(1) In-hospital mortality prediction: This task aims to predict patient mortality outcomes during their hospital stay, classifying whether a patient will survive (label 0) or die (label 1) based on various data modalities. Accurate mortality prediction models can assist healthcare providers in identifying high-risk patients, enabling timely interventions, optimizing resource allocation, and ultimately improving patient outcomes. Early identification of patients at risk of mortality allows for proactive care planning and targeted treatment strategies, potentially reducing mortality rates and enhancing the quality of care delivered.\n(2) 30-day readmission prediction: The goal of this task is to predict the likelihood of a patient being readmitted to the hospital within 30 days of their initial discharge, classifying patients into two categories: those not likely to be readmitted (label 0) and those with a higher probability of readmission (label 1). Accurate readmission prediction can help healthcare providers design targeted interventions and post-discharge care plans to reduce readmission rates, improve patient outcomes, and optimize healthcare resource utilization. By identifying patients at high risk of readmission, providers can implement personalized follow-up care, patient education, and care coordination to prevent avoidable readmissions and improve the continuity of care.\nThe two chosen non-generative diagnostic tasks serve as crucial references for patient health status in clinical decision-making, offering insights into outcomes, resource utilization, and care planning to aid personalized, high-quality care7,39,49, 50.\nIn addition to these conventional supervised prediction tasks, we also include two unsupervised prediction tasks to assess models' representation quality over unstructured clinical notes data.\n(1) Medical sentence matching: This task assesses the ability of models to determine the semantic similarity between pairs of sentences in the medical domain. Human experts evaluate and assign similarity scores to each pair of sentences, which the models then process to generate vector representations (embeddings) that capture the semantic information. The performance of the models is evaluated by comparing their predicted similarity scores against the human-labeled scores using metrics such as Pearson correlation, with higher correlations indicating better alignment with human judgments. The task enables healthcare professionals to group similar patient information, retrieve relevant literature, and support decision-making by identifying"}, {"title": "A.2 Datasets", "content": "This study utilizes two primary datasets: TJH25 (structured EHR data) and MIMIC24 (structured EHR data and unstructured clinical notes). The TJH dataset is chosen for its relevance to the early stages of the COVID-19 pandemic, a period characterized by a sudden surge in cases and significant pressure on healthcare services. As such, it serves as an ideal representation of an emerging disease scenario, aligning with our research scope of simulating the occurrence of a new emergency disease. The MIMIC dataset, on the other hand, is selected for its extensive nature and wide usage in healthcare research7, particularly for simulating ICU scenarios. Its inclusion of both structured EHR data and unstructured clinical notes allows for a comprehensive evaluation of model performance across multiple data modalities. Therefore, the MIMIC dataset provides a solid foundation for comparative analysis. It supports both mortality (classification) and readmission (classification) prediction tasks, aligning with our research goals.\n\u2022 TJH Dataset25: Derived from Tongji Hospital of Tongji Medical College, the TJH dataset consists of 485 anonymized COVID-19 inpatients treated in Wuhan, China, from January 10 to February 24, 2020. It includes 73 lab test features and 2 demographic features. The dataset is publicly available on GitHub (https://github.com/HAIRLAB/Pre_Surv_COVID_19, also available in original paper's supplementary information and data and code availability section25.).\n\u2022 MIMIC Dataset24: Sourced from the EHRs of the Beth Israel Deaconess Medical Center, the MIMIC dataset is extensive and widely used in healthcare research, particularly for simulating ICU scenarios. It is divided into \u201cMIMIC-EHR\u201d, which contains the structured EHR data, and \"MIMIC-Note\u201d, which contains the clinical notes. For MIMIC-EHR, 17 lab test features and 2 demographic features are extracted. To minimize missing data, we consolidate every consecutive 12-hour segment into a single record for each patient, focusing on the first 48 records. For MIMIC-Note, unstructured text data from clinical reports records are utilized."}, {"title": "A.3 Evaluation Metrics", "content": "We follow the benchmark preprocessing pipeline as established in the studies 7,51 for both datasets. For structured EHR data, we apply the Last Observation Carried Forward (LOCF) imputation strategy52 to handle missing values. In the preprocessing of clinical notes, we utilize the Clinical-Longformer approach42, which includes minimal but essential steps: removing all de-identification placeholders to protect Protected Health Information (PHI), replacing non-alphanumeric characters and punctuation marks, converting all letters to lowercase for consistency, and stripping extra white spaces. These standardized preprocessing techniques ensure uniformity and comparability in our data analysis. Moreover, following the benchmark's operation, we apply a stratified shuffled strategy with random selection to construct the training, validation and test set, among them, the test set is the same for all models to ensure fair comparison.\nFor the zero-shot setting with LLMs, we focus primarily on the test set, while the training and validation sets are specifically accessed by ML and DL models. In the instruction prompt where we have adopted the in-context learning53 strategy, we simulate the examples with randomly generated values in the designed instruction prompt to avoid leaking the dataset.\nThe unsupervised tasks, medical sentence matching and ICD code clustering, are evaluated based on the following datasets:\n(1) Medical sentence matching: For the medical sentence matching task, we utilize the BIOSSES (Biomedical Semantic Similarity Estimation System) dataset22, a benchmark resource specif-ically designed for evaluating semantic similarity in the biomedical domain. The BIOSSES dataset comprises 100 sentence pairs, each selected from the TAC (Text Analysis Conference) Biomedical Summarization Track Training Dataset, which contains articles from the biomedical literature. Each pair of sentences in the BIOSSES dataset was evaluated by five human experts, who assigned similarity scores ranging from 0 (indicating no relation) to 4 (indicating equiva-lence). The average of these scores across the five annotators serves as the gold standard for comparison.\n(2) ICD code clustering: For the ICD code clustering task, we utilize the 2023 ICD-10-CM (Inter-national Classification of Diseases, Tenth Revision, Clinical Modification) dataset. This dataset includes the latest updates effective from April 1, 2023, provided by the Centers for Disease Con-trol and Prevention's (CDC) National Center for Health Statistics (NCHS). The dataset contains a total of 96,795 ICD codes. For the purpose of clustering, we select 11,942 codes by extracting ICD codes with a length of four characters or fewer, aiming to capture broader categories that encapsulate more general medical concepts. The dataset can be downloaded at https://www.cms.gov/medicare/coding-billing/icd-10-codes/2023-icd-10-cm, specif-ically from the file named \u201c2023 Code Descriptions in Tabular Order - updated 01/11/2023 (ZIP)\" under the \u201cicd10cm_order_2023.txt\u201d file.\nWe employ following metrics for the binary classification task (including in-hospital mortality prediction and 30-day readmission prediction tasks):\n(1) AUROC: This is our primary metric in the binary classification task due to its widespread use in clinical settings and effectiveness in imbalanced datasets54.\n(2) AUPRC: AUPRC is particularly useful for evaluating performance in datasets with a significant imbalance between classes55.\n(3) Missing Rate: This metric is calculated as $$\\frac{n_{test}-n_{decoded}}{n_{test}} \u00d7 100%$$, where ntest represents the total"}, {"title": "A.4 Problem Formulation", "content": "A.4.1 Non-generative Supervised Medical Diagnostic Tasks\nLet $$\\mathcal{D}$$ = {$$(xi, yi)$$}i=1N denote a dataset containing N samples, where $$\\boldsymbol{x}_i$$ represents the input data (either structured EHR or unstructured clinical notes) for the i-th patient, and yi denotes the corresponding label or target value. The goal is to learn a mapping function $$\\boldsymbol{f}: x_i \\rightarrow y_i$$ that predicts the output $$\\boldsymbol{y}_i$$ given the input $$\\boldsymbol{x}_i$$.\nSpecifically, for the structured EHR data, each input $$\\boldsymbol{x}_i$$ consists of a sequence of visits $$\\boldsymbol{x}_i$$ = {$$(v_{i, 1}, v_{i,2},..., v_{i, T_i})$$}, where $$\\boldsymbol{v}_{i,t}$$ represents the data associated with the t-th visit of the i-th patient, and T\u00a1 denotes the total number of visits for the i-th patient. The data for each visit $$\\boldsymbol{v}_{i,t}$$ includes dynamic information such as lab tests and vital signs, as well as static information like demographics.\nIn the context of LLMs, we utilize pre-designed prompt templates that encapsulate the raw EHR data $$\\boldsymbol{x}_i$$, along with optional external information such as system prompts56, units, or reference ranges. The LLM is instructed to perform a specific task based on the input prompt. For binary"}, {"title": "A.4.2 Medical Sentence Matching", "content": "classification tasks, the LLM is expected to respond with a floating-point number representing the predicted probability.\nFormally, let $$\\mathcal{P}$$(xi, ei) denote the prompt template function that takes the input data $$\\boldsymbol{x}_i$$ and optional external information ei to generate the input prompt for the LLM. The LLM mapping function can be defined as:\nfLLM : LLM($$\\mathcal{P}$$(xi, ei)) \u2192 \u0177i\n(1)\nwhere \u0177i is the generated output of the LLM, also serving as the prediction result.\nFor conventional models, models are fine-tuned to predict the corresponding task label yi. To compare the representation quality of different models, we consider two fine-tuning approaches:\n(1) Integrating the pretrained backbone with a newly initialized prediction head and updating all parameters of the combined model during training.\n(2) Freezing the pretrained backbone parameters and training only the newly initialized prediction head.\nLet $$\\mathcal{D}MSM = {(\\boldsymbol{s}_i, \\boldsymbol{s}_j, score_{ij})}_1M$$ be a dataset containing M pairs of medical sentences, where si and sj are the sentences in the k-th pair, and scoreij is the human-annotated similarity score for the sentence pair (si, sj). The objective is to evaluate the correlation of the similarity derived from the model and the similarity annotated by humans, i.e., correlation : sim($$\\boldsymbol{s}_i$$, $$\\boldsymbol{s}_j$$) \u2192 scoreij.\nWe use embeddings e\u00a1 and ej for the sentences s\u012f and sj, respectively, derived from the pretrained language model. The predicted similarity score sc\u00f4reij is computed using a similarity function sim($$\\boldsymbol{e}_i$$, $$\\boldsymbol{e}_j$$), such as cosine similarity:\nsim(si, sj) = sim($$\\boldsymbol{e}_i$$, $$\\boldsymbol{e}_j$$) = $$\\frac{e_i e_j}{||e_i||||e_j||}$$\n(2)\nThe performance is evaluated using correlation metrics (Pearson, Spearman, Kendall) between the estimated similarity scores {sim($$\\boldsymbol{s}_i$$,$$\\boldsymbol{s}_j$$)} and the human-annotated scores {scoreij}."}, {"title": "A.4.3 ICD Code Clustering", "content": "Let DICD = {di}i=1L be a dataset containing L disease descriptions, where d\u012f represents the i-th disease description. Each description d\u012f is associated with an ICD code ICD\u012f. The objective is to evaluate models' representations e\u00a1 for each disease description d\u012f, where the similar diseases (according to the ICD hierarchy) have more similar embeddings is treated as better representation quality.\nWe perform clustering on the embeddings {e;} (computed from the pretrained language model) using a clustering algorithm (e.g., K-means). The quality of the clustering is evaluated by measuring the average intra-cluster distance distavg, where distavg is defined as the average pairwise distance between embeddings within the same cluster. The distance between two ICD codes is measured according to their positions in the ICD hierarchy:\ndistICD(ICDa, ICD\u266d) = dist(ICDa, LCA(ICDa, ICD\u266d))+dist(ICD\u266d,LCA(ICDa, ICD\u266d)) (3)\nwhere LCA(ICDa, ICD\u266d) is the lowest common ancestor of ICDa and ICD\u266d in the ICD tree, and dist(ICD, LCA(ICDa, ICD\u266d)) is the distance from ICDx to this lowest common ancestor."}, {"title": "A.5 Baseline Models", "content": "The average intra-cluster distance is given by:\ndistava = $$\\frac{1}{c} \u03a3_C \\frac{1}{|C|(|C|-1)} \u03a3_{i,jecij}  dist_{ICD} (ICD_i, ICD_j)$$\n(4)\nwhere & is the set of clusters, C is a cluster in 6, and |C| is the number of elements in cluster C.\nThe clustering performance reflects the quality of the embeddings in capturing the hierarchical structure of the ICD codes.\nIn summary, the unsupervised tasks of medical sentence matching and ICD code clustering evaluate the representation quality of the models, providing insights into their effectiveness in capturing semantic and hierarchical relationships in the medical domain.\nTo perform the above tasks, we comprehensively select conventional EHR-specific models and text-specific models to compare with large language models (LLMs). LLMs serve as general-purpose models capable of performing on diverse data modalities and tasks, whether supervised or unsupervised. We aim to explore if these LLMs consistently outperform conventional models in non-generative tasks.\nA.5.1 Clinical Predictive Models\nThe following models are applied on structured EHR prediction tasks, encompassing machine learning and deep learning methods, as well as models specifically designed for EHR data.\n(1) Decision Tree57: It splits data into branches at decision nodes represented visually as a tree structure. It utilizes simple rules inferred from features to progressively classify or predict outcomes, making it straightforward yet powerful for clinical decision-making tasks.\n(2) XGBoost35: It is an advanced implementation of gradient boosting, offering robust handling of various types of data and extensive support for fine-tuning and customization. Its efficiency and performance make it popular for predicting patient outcomes from EHR data.\n(3) GRU36: It efficiently captures dependencies in sequence data through gating mechanisms which regulate the flow of information. This capability is essential for processing time-series EHR data to predict future health events.\n(4) ConCare: It utilizes a multi-channel GRU with a time-aware attention mechanism to extract clinical features and re-encode the clinical information by capturing the interdependencies between features. It is specifically designed to handle the complexity of EHR data.\n(5) GRASP37: It is a generic framework for healthcare models, leveraging information extracted from patients with similar conditions to enhance cohort representation learning results. This approach improves the accuracy of patient outcome predictions.\n(6) M3Care38: It addresses the missing modality issue in EHR data by utilizing similar patients' existing modalities. Specifically, M3Care imputes task-related information in the latent space rather than generating raw missing data, enhancing stability and accuracy.\n(7) AICare39: It includes a multi-channel feature extraction module and an adaptive feature importance recalibration module. It learns personalized health status embeddings with both static and dynamic features, improving patient-specific predictions."}, {"title": "A.5.2 BERT-based Models", "content": "Building upon the BERT model, several variants are tailored for processing and extracting valuable information from unstructured clinical texts, enhancing the understanding and utility of medical documents.\n(1) BERT\u00b9: It leverages a transformer architecture with a focus on the encoder component, pro-cessing text bidirectionally to capture contextual information from both sides of each token across all layers. This makes BERT effective for tasks like medical document classification and information extraction.\n(2) ClinicalBERT19: It is an adaptation of BERT designed to effectively represent and extract information from unstructured clinical notes by capturing nuanced relationships between medical concepts. It is optimized for healthcare-specific language and tasks.\n(3) BioBERT40: It is a specialized version of BERT tailored for the biomedical field. It enhances the extraction of valuable information from biomedical literature, improving the effectiveness of biomedical text mining models.\n(4) GatorTron41: It is a clinical language model with 8.9 billion parameters, trained on over 90 billion words, including 82 billion de-identified clinical texts. It significantly improves performance on five clinical NLP tasks, including clinical concept extraction and medical question answering.\n(5) Clinical-Longformer42: It is a clinical knowledge enriched version of Longformer, pretrained using MIMIC-III clinical notes. It allows up to 4,096 tokens as the model input by utilizing a sparse attention mechanism."}, {"title": "A.5.3 Large Language Models", "content": "LLMs are basically general-purpose, designed to handle a wide range of tasks across various data types, including medical data, and can be adapted for more specific applications in the healthcare domain.\n(1) GPT-28: It is trained on a vast dataset of WebText and demonstrates the ability to perform NLP tasks. It improves log-linearly with increases in capacity and achieves competitive results on benchmarks, showing potential for medical text analysis.\n(2) Llama-327: It features significant upgrades to Llama-2 like a more efficient 128K token vocabulary and grouped query attention (GQA) in both 8B and 70B models. Additionally, Llama-3 offers improved reasoning and code generation capabilities, and has been trained on a substantially larger dataset of over 15 trillion tokens.\n(3) GPT-3.5 (ChatGPT)28: It employs Reinforcement Learning from Human Feedback (RLHF). The process begins with supervised fine-tuning and transitions to reinforcement learning, sub-stantially improving its conversational abilities through the use of a structured dialogue format, which can be beneficial for interactive medical applications.\n(4) GPT-429: It is a large-scale, latest one of the most advanced LLM developed by OpenAI. It achieves human-level performance on various benchmarks, including the United States Medical Licensing Examination (USMLE), indicating its potential for high-stakes medical decision support.\nMedical LLMs are specifically designed and fine-tuned for the medical domain from the general-purpose LLMs, enhancing their effectiveness in healthcare-related tasks."}, {"title": "A.6 Implementation Details", "content": "Our experimental phase spans from December 19, 2023, to June 22, 2024. The experimental setup includes a server equipped with eight NVIDIA Tesla V100 GPUs, each with 32GB of memory, two Intel(R) Xeon(R) Gold 6146 CPUs (12 cores, 24 threads each) operating at 3.20GHz, and 384GB of RAM. The software environment comprises CUDA driver version 12.2, Python 3.11, PyTorch 2.1.1, PyTorch Lightning 2.1.2, and Transformers 4.40.2. This configuration is utilized to run the machine learning and deep learning model baselines presented in our benchmarked performance table.\nFor model training, we employ the AdamW optimizer across all experiments. Each model undergoes training for a maximum of 50 epochs using patient samples from the training set. To prevent overfitting, we implement an early stopping strategy monitored by the AUROC metric with a patience of 5 epochs. Notably, for most BERT-based model fine-tuning scenarios, optimal parameters are typically achieved within the first 3 epochs. The learning rate hyperparameter for training EHR prediction models is determined via grid search within the space of le-2, 1e-3, 1e-4. The hidden dimension for EHR prediction models is set to 128. The batch size in the TJH dataset is 64, while for the MIMIC dataset, it is 1024. The learning rate for fine-tuning BERT-based models is set to le-5, and for the freeze setting (training the MLP classifier), it is set to 1e-4. The batch size for BERT-based models is generally 32 on a single GPU. Due to GPU memory constraints, the batch size for GatorTron is 4 and for Clinical-Longformer is 16. The batch size for inferencing or obtaining embeddings from GPT-based LLMs is set to 1. BioGPT are run on a single GPU card, while other LLMs are run with eight GPUs in parallel.\nFor BERT-based models, we extract the embedding of the first token (CLS token) at the last layer in the backbone, and the last token at the last layer for GPT-based models. For clinical notes experiments, the max token count is set to 512 for all models (in both finetune and freeze settings). However, in the generation setting, we adjust the max token count to 4096 due to LLMs' longer context capacity. In addition, we use the default temperature in Ollama, and"}]}]}