{"title": "INFORMATION-THEORETIC FOUNDATIONS FOR NEURAL SCALING LAWS", "authors": ["Hong Jun Jeon", "Benjamin Van Roy"], "abstract": "Neural scaling laws aim to characterize how out-of-sample error behaves as a function of model and training dataset size. Such scaling laws guide allocation of a computational resources between model and data processing to minimize error. However, existing theoretical support for neural scaling laws lacks rigor and clarity, entangling the roles of information and optimization. In this work, we develop rigorous information-theoretic foundations for neural scaling laws. This allows us to characterize scaling laws for data generated by a two-layer neural network of infinite width. We observe that the optimal relation between data and model size is linear, up to logarithmic factors, corroborating large-scale empirical investigations. Concise yet general results of the kind we establish may bring clarity to this topic and inform future investigations.", "sections": [{"title": "1 Introduction", "content": "In recent years, foundation models have grown immensely, with some embodying trillions of trainable parameters. While larger models have in general produced better results, they also require much more compute to train. It has become impractical to perform hyperparameter sweeps at the scale of these modern models. This has required bypassing the practice of tuning hyperparameters via extensive trial and error, as was previously common in deep learning.\nAmong other things, hyperparameters control 1) the size, measured in terms of the parameter count $p$, of the neural network model and 2) the number $T$ of training tokens. If each parameter is adjusted in response to each token then the computational requirements of training scale will the product of these two quantities. For any compute budget $C$, one should carefully balance between $p$ and $T$. Too few training tokens leads to model estimation error, while too few parameters gives rise to mispecification error. As evaluating performance across multiple choices of $p$ and $T$ becomes computationally prohibitive at scale, alternative kinds of analysis are required to guide allocation of computational resources.\nKaplan et al. [2020] and Hoffmann et al. [2022] have proposed the following procedure for allocating a large compute budget: 1) Evaluate test errors of models produced using various small compute budgets $C$ with many different allocations to parameters $p$ versus training tokens $T$. 2) Extrapolate to estimate the relation between $p$ and $T$ for large $C$. 3) Extrapolate to estimate the relation between $p$ and $T$ for large $C$.\nTo give a sense of scales involved here, Hoffmann et al. [2022] evaluate test errors across \"small\" models for which $p \\times T$ ranges from around $10^{18}$ to $10^{22}$ and extrapolates out to \u201clarge\u201d models at around $10^{24}$. Kaplan et al. [2020] and Hoffmann et al. [2022] each extrapolate based on a hypothesized scaffolding function. Kaplan et al. [2020] guess a scaffolding function based on results observed in small scale experiments. Hoffmann et al. [2022] carry out an informal and somewhat speculative mathematical analysis to guide their choice (see their Appendix D).\nThe analysis of Hoffmann et al. [2022] is somewhat generic rather than specialized to the particular neural network architecture used in that paper. In this paper, building on the work of Jeon and Van Roy [2022a,b], we develop rigorous"}, {"title": "2 A Framework for Learning", "content": "information-theoretic foundations and use them to derive similar scaling laws. To keep things simple and concrete, we carry out the analysis with a particular data generating process for which neural networks are well-suited. The sorts of arguments developed by Hoffmann et al. [2022] are just as relevant to this context as they are to language models.\nHoffmann et al. [2022] suggest that the compute optimal trade-off between parameter count and number of training tokens is linear, though the authors expressed some doubt and considered other possibilities that are near-linear as well. We establish an upper bound on the minimal information-theoretically achievable expected error as a function of $p$ and $T$ and derive the relation required to minimize this bound for each compute budget. For large compute budgets, this relation is linear, as suggested by Hoffmann et al. [2022].\nOur main contributions include a first rigorous mathematical characterization of the compute-optimal efficient frontier for a neural network model and development of information-theoretic tools which enable that. A limitation of our analysis is in its simplified treatment of computational complexity as the product of the model and data set sizes; we do not assume any constraints on computation beyond those imposed by choices of $p$ and $T$. In particular, we analyze, algorithms which carry out perfect Bayesian inference with respect to a model that is misspecificified due to its restricted size. While this abstracts away the details of practical training algorithms, empirical evidence suggests that our idealized framework leads to useful approximations [Zhu et al., 2022]. In spite of these limitations, we hope our results set the stage for further mathematical work to guide hyperparameter selection when training large neural networks."}, {"title": "2.1 Probabilistic Framework", "content": "We define all random variables with respect to a common probability space $(\\Omega, \\mathcal{F}, P)$. Recall that a random variable $F$ is simply a measurable function $\\Omega \\to F$ from the sample space $\\Omega$ to an outcome set $F$.\nThe probability measure $P : \\mathcal{F} \\to [0, 1]$ assigns probabilities to the events in the $\\sigma$-algebra $\\mathcal{F}$. For any event $E \\in \\mathcal{F}$, $P(E)$ to denotes the probability of the event. For events $E, G \\in \\mathcal{F}$ for which $P(G) > 0$, $P(E|G)$ to denotes the probability of event $E$ conditioned on event $G$.\nFor realization $z$ of a random variable $Z$, $P(Z = z)$ is a function of $z$. We denote its value evaluated at $Z$ by $P(Z)$. Therefore, $P(Z)$ is a random variable (it takes realizations in $[0, 1]$ depending on the value of $Z$). Likewise for realizations $(y, z)$ of random variables $Y, Z$, $P(Z = z|Y = y)$ is a function of $(y, z)$ and $P(Z|Y)$ is a random variable which denotes the value of this function evaluated at $(Y, Z)$.\nIf random variable $Z : \\Omega \\to \\mathbb{R}^K$ has density $p_Z$ w.r.t the Lebesgue measure, the conditional probability $P(E|Z = z)$ is well-defined despite the fact that for all $z$, $P(Z = z) = 0$. If function $f(z) = P(E|Z = z)$ and $Y : \\Omega \\to \\mathbb{R}^K$ is a random variable whose range is a subset of $Z$'s, then we use the $\\leftarrow$ symbol with $P(E|Z \\leftarrow Y)$ to denote $f(Y)$. Note that this is different from $P(E|Z = Y)$ since this conditions on the event $Z = Y$ while $P(E|Z \\leftarrow Y)$ indicates a change of measure."}, {"title": "2.2 Data", "content": "We consider a stochastic process which generates a sequence $(X_t, Y_{t+1} : t \\in \\mathbb{Z}_+)$ of data pairs. For all $t$, we let $H_t$ denote the history $(X_0, Y_1, . . ., X_{t-1}, Y_t, X_t)$ of experience. We assume that there exists an underlying latent variable $F$ such that $(X_0, X_1, ...)|F$ and $F$ prescribes a conditional probability measure $P(\\cdot|H_t)$ to the next label $Y_{t+1}$. In the case of an iid data generating process, this conditional probability measure would only depend on $H_t$ via $X_t$. Note that the current pre-training objective of foundation models falls under this iid setting in which for all $t$, $X_t$ is a random segment of the training corpus and $Y_{t+1}$ is the subsequent token. As our framework is Bayesian, we represent our uncertainty about $F$ by modeling it as a random variable with prior distribution $P(F \\in \\mathcal{F})$."}, {"title": "2.3 A Learning Objective", "content": "We focus on a particular notion of error which facilitates analysis via Shannon information theory and reflects the objective of modern foundation models. For all $t \\in \\mathbb{Z}_+$, our algorithm is tasked with providing a predictive distribution $P_t$ of $Y_{t+1}$ which may depend on the history of data which it has already observed $H_t$. We express such an algorithm as $\\pi$ for which $P_t = \\pi(H_t)$. As aforementioned, an effective learning system ought to leverage data as it becomes available and perform well across all time. As a result, for any time horizon $T \\in \\mathbb{Z}_+$, we are interested in quantifying"}, {"title": "3 Error of Constrained Predictors", "content": "the cumulative expected log-loss:\n$L_{T, \\pi} = \\frac{1}{T} \\sum_{t=0}^{T-1} \\mathbb{E}_{\\pi} [- \\ln P_t (Y_{t+1})]$.\nNote that since we take all random variables to be defined with respect to a common probability space, the expectation $\\mathbb{E}$ integrates over all random variables which we do not condition on. We use the subscript $\\pi$ in $\\mathbb{E}$ to specify that all predictions $P_t$ for all $t$ are produced by $\\pi$. As $Y_{t+1}$ is the random variable which represents the next label that is generated by the underlying stochastic process, $P_t(Y_{t+1})$ denotes the probability that our algorithm's prediction $P_t$ assigns to label $Y_{t+1}$.\nIt is important to note that even for an omniscient algorithm, the minimum achievable log-loss is not 0. Consider the omniscient algorithm which produces for all $t$ the prediction $P^*_t = P(Y_{t+1} \\in \\cdot|F, H_t)$. Even this agent incurs a loss of:\n$\\frac{1}{T} \\sum_{t=0}^{T-1} \\mathbb{E}_{\\pi} [- \\ln P(Y_{t+1}|F, H_t)] = \\frac{1}{T} \\sum_{t=0}^{T-1} H(Y_{t+1}|F, H_t)$\nwhere our point follows from the fact that the conditional entropy (H) of a discrete random variable $Y_{t+1}$ is non-negative. As a result, we define the reducible error as:\n$L_{T, \\pi}^{red} = L_{T, \\pi} - \\frac{1}{T} \\sum_{t=0}^{T-1} H(Y_{t+1}|F, H_t)$\n$= \\frac{1}{T} \\sum_{t=0}^{T-1} \\mathbb{E} [d_{KL} (P^*(\\cdot)||P_t(\\cdot))]$.\nreducible error represents the error which is reducible via observing additional data and fitting a larger model. Therefore, we expect that this error will consist of two terms which reflect 1) the error due to estimation via finite data, 2) the error due to approximation with a finite parameter model.\nWe introduce a general upper bound on the reducible error of a constrained predictor. While the formulations remain abstract in this section, a useful running example is the following: Assume that $F$ is an infinite width neural network which generates the data and $F$ is a finite width network."}, {"title": "3.1 A Constrained Predictor", "content": "$F$ may exhibit endless complexity, likely beyond what can be represented with finite memory hardware. To represent the predictions made by a constrained predictor, we first define a random variable $\\tilde{F}$ whose range is a subset of $F$'s. As aforementioned, this random variable can be a lossy compression of $F$ i.e. if $F$ is represented by an infinite-width neural network, $\\tilde{F}$ could be a finite-width approximation. For all $t$, let the constrained predictor be:\n$P_t(\\cdot) = \\sum_{f} P(\\tilde{F} = f|H_t) \\cdot P(Y_{t+1} \\in \\cdot|F = f, X_t)$.\nThe predictor performs inference on $\\tilde{F}$ but performs predictions as if $F = \\tilde{F}$. We let\n$L_T(\\tilde{F}) = \\frac{1}{T} \\sum_{t=0}^{T-1} \\mathbb{E} [d_{KL} (P^*(\\cdot)||\\tilde{P}_t(\\cdot))]$."}, {"title": "3.2 Error of Constrained Predictor", "content": "Theorem 3.1. *For all* $T \\in \\mathbb{Z}_{++}$ *and random variables* $\\tilde{F} : \\Omega \\leftrightarrow \\mathcal{F}, \\hat{F} : \\Omega \\leftrightarrow \\mathcal{F}$ *for which* $\\hat{F} \\subseteq \\mathcal{F}$, *if* $((X_t, Y_{t+1}) : t \\in \\mathbb{Z}_+)$ *is iid conditioned on* $\\hat{F}$, *then*\n$\\tilde{L}_T(\\tilde{F}) \\le \\frac{I(\\hat{F}; \\tilde{F})}{T} + \\mathbb{E} [d_{KL} (P^*(\\cdot)||[\\tilde{P}_t(\\cdot)])]$,\n*where* $\\tilde{P}_t(\\cdot) = P(Y_{t+1} \\in \\cdot|\\hat{F} \\leftarrow \\tilde{F}, X_t)$."}, {"title": "3.3 Scaling Law", "content": "For a FLOP constraint $C \\approx p \\cdot T$, it is clear that there is a tension between $p$ and $T$ in minimizing the upper bound in Theorem 3.1. This can be seen by first fixing a FLOP count $C$ and substituting $T = C/p$. The upper bound becomes:\n$\\frac{p \\cdot I(\\hat{F}; \\tilde{F})}{C} + \\mathbb{E} [d_{KL} (\\tilde{P}(\\cdot)||\\hat{P}_t(\\cdot))]$.\nNote that the first term is increasing in $p$ whereas the second term is decreasing in $p$. Therefore, under a fixed FLOP budget, the designer ought to select a value of $p$ which effectively balances the two sources of error."}, {"title": "4 An Illustrative Example", "content": "The first term denotes the estimation error or the error which is reducible via access to more data. This is evident by the fact it decreases linearly in $T$ and the numerator reflects the complexity of $\\tilde{F}$. The more nats of information that $\\tilde{F}$ contains about the data stream, the more data will be required to arrive at a good predictor.\nThe second term denotes the misspecification error or the error which is reducible via a larger learning model. The closer that $\\tilde{F}$ approximates $F$, the smaller the KL divergence between $\\tilde{P}$ and $\\hat{P}_t$ will be. In the following section, we will use Theorem 3.1 to derive a concrete neural scaling law for an infinite-width neural network example."}, {"title": "4.1 Data Generating Process", "content": "The generating process is described by a neural network with $d$ inputs, a single asymptotically wide hidden layer of ReLU activation units, and a linear output layer. We denote by $F$ the associated mapping from input to output. Inputs and binary labels are generated according to $X_t \\overset{iid}{\\sim} N(0, I_d)$ and $P(Y_{t+1} = 1|F, X_t) = \\sigma(F(X_t))$ where $\\sigma$ denotes the sigmoid function.\nAs alluded to by the asymptotic width, $F$ is a nonparametric model which we will outline now. Let $\\theta$ be distributed according to a Dirichlet process with base distribution uniform($\\mathbb{S}^{d-1}$) and scale parameter $K$. Realizations of this Dirichlet process are probability mass functions on a countably infinite subset of $\\mathbb{S}^{d-1}$. Let $\\mathcal{W} = \\{w \\in \\mathbb{S}^{d-1} : \\theta_w > 0\\}$ denote this set. For all $w \\in \\mathcal{W}$,\n$\\theta_w = \\begin{cases}\\theta_w & \\text{with probability } 1/2, \\\\ -\\theta_w & \\text{otherwise}. \\end{cases}$\nFinally, we have that\n$F(X) = \\sqrt{K + 1} \\cdot \\sum_{w \\in \\mathcal{W}} \\theta_w \\text{ReLU} (w^T X)$.\nSince $\\mathcal{W}$ has countably infinite cardinality, $F$ is characterized by a neural network with infinite width. We let $\\theta = (\\theta_w : w \\in \\mathcal{W})$ and $W = (w : w \\in \\mathcal{W})$ denote the weights of such neural network and hence\n$F(X) = \\sqrt{K + 1} \\cdot \\theta \\text{ReLU}(WX)$.\nNote that the mean and variance structure satisfy\n$\\mathbb{E}[F(X)] = 0, \\qquad \\mathbb{E}[F(X)^2] = 1/2$.\nTherefore, this model remains nontrivial as $d$ and $K$ grow as all of the above quantities are invariant of $d$ and $K$."}, {"title": "4.2 Constrained Predictor", "content": "We will study the scaling law associated with a particular constrained predictor characterized by a neural network of width $n$. Let $w_1, w_2, . . ., w_n$ be distributed iid Categorical($\\delta$), where $\\mathcal{W}$ are the classes. For any $\\epsilon > 0$, let $\\mathbb{S}_\\epsilon^{d-1}$ be an $\\epsilon$-cover w.r.t $|\\cdot|_2$ and for all $i \\in [n]$, let\n$w_{i,\\epsilon} = \\arg \\min_{v \\in \\mathbb{S}\\_\\epsilon^{d-1}} ||w_i - v||_2$.\nFinally, let\n$F_{n,\\epsilon}(X_t) = \\frac{\\sqrt{K + 1}}{n} \\sum_{i=1}^{n} \\text{sign} (\\theta_{\\tilde{w}_i}) \\text{ReLU} (\\tilde{W}\\_{\\epsilon_i}^T X_t)$."}, {"title": "4.3 Error Bound", "content": "Let $\\theta \\in \\mathbb{R}^n$ is $(\\text{sign}(\\theta_{\\tilde{w}_i})/n : i \\in [n])$ and $W_\\epsilon \\in \\mathbb{R}^{n \\times d}$ is $(W_{i,\\epsilon} : i \\in [n])$. Therefore,\n$F_{n,\\epsilon}(X_t) = \\sqrt{K + 1} \\cdot \\theta \\text{ReLU} (W_\\epsilon X_t)$.\nWe consider the performance of a constrained agent which for all $t$, produces the prediction $P_t(\\cdot) =$\n$\\sum_{f} P(F_{n,\\epsilon} = f|H_t) \\cdot P(Y_{t+1} \\in \\cdot|F = f, X_t)$.\nNote that this agent performs inference on the constrained model $F_{n,\\epsilon}$ and produces predictions about $Y_{t+1}$ as if $F_{n,\\epsilon}$ were the function $F$ which produced the data.\nWe will now study the error incurred by the constrained predictor described above. We define\n$\\tilde{L}_{T,n,\\epsilon} = \\frac{1}{T} \\sum_{t=0}^{T-1} \\mathbb{E} [d_{KL} (P(Y_{t+1} \\in \\cdot|\\theta, X_t) ||P_t(\\cdot))]$"}, {"title": "5 Conclusion", "content": "Results follow after section 4.3"}]}