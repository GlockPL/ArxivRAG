{"title": "INFORMATION-THEORETIC FOUNDATIONS FOR NEURAL SCALING LAWS", "authors": ["Hong Jun Jeon", "Benjamin Van Roy"], "abstract": "Neural scaling laws aim to characterize how out-of-sample error behaves as a function of model and training dataset size. Such scaling laws guide allocation of a computational resources between model and data processing to minimize error. However, existing theoretical support for neural scaling laws lacks rigor and clarity, entangling the roles of information and optimization. In this work, we develop rigorous information-theoretic foundations for neural scaling laws. This allows us to characterize scaling laws for data generated by a two-layer neural network of infinite width. We observe that the optimal relation between data and model size is linear, up to logarithmic factors, corroborating large-scale empirical investigations. Concise yet general results of the kind we establish may bring clarity to this topic and inform future investigations.", "sections": [{"title": "Introduction", "content": "In recent years, foundation models have grown immensely, with some embodying trillions of trainable parameters. While larger models have in general produced better results, they also require much more compute to train. It has become impractical to perform hyperparameter sweeps at the scale of these modern models. This has required bypassing the practice of tuning hyperparameters via extensive trial and error, as was previously common in deep learning.\nAmong other things, hyperparameters control 1) the size, measured in terms of the parameter count $p$, of the neural network model and 2) the number $T$ of training tokens. If each parameter is adjusted in response to each token then the computational requirements of training scale will the product of these two quantities. For any compute budget $C$, one should carefully balance between $p$ and $T$. Too few training tokens leads to model estimation error, while too few parameters gives rise to mispecification error. As evaluating performance across multiple choices of $p$ and $T$ becomes computationally prohibitive at scale, alternative kinds of analysis are required to guide allocation of computational resources.\n[Kaplan et al., 2020] and Hoffmann et al. [2022] have proposed the following procedure for allocating a large compute budget: 1) Evaluate test errors of models produced using various small compute budgets $C$ with many different allocations to parameters $p$ versus training tokens $T$. 2) Extrapolate to estimate the relation between $p$ and $T$ for large $C$. 3) Extrapolate to estimate the relation between $p$ and $T$ for large $C$.\nTo give a sense of scales involved here, Hoffmann et al. [2022] evaluate test errors across \"small\" models for which $p \\times T$ ranges from around $10^{18}$ to $10^{22}$ and extrapolates out to \u201clarge\u201d models at around $10^{24}$. Kaplan et al. [2020] and Hoffmann et al. [2022] each extrapolate based on a hypothesized scaffolding function. Kaplan et al. [2020] guess a scaffolding function based on results observed in small scale experiments. Hoffmann et al. [2022] carry out an informal and somewhat speculative mathematical analysis to guide their choice (see their Appendix D).\nThe analysis of Hoffmann et al. [2022] is somewhat generic rather than specialized to the particular neural network architecture used in that paper. In this paper, building on the work of Jeon and Van Roy [2022a,b], we develop rigorous"}, {"title": "A Framework for Learning", "content": "information-theoretic foundations and use them to derive similar scaling laws. To keep things simple and concrete, we carry out the analysis with a particular data generating process for which neural networks are well-suited. The sorts of arguments developed by Hoffmann et al. [2022] are just as relevant to this context as they are to language models.\nHoffmann et al. [2022] suggest that the compute optimal trade-off between parameter count and number of training tokens is linear, though the authors expressed some doubt and considered other possibilities that are near-linear as well. We establish an upper bound on the minimal information-theoretically achievable expected error as a function of $p$ and $T$ and derive the relation required to minimize this bound for each compute budget. For large compute budgets, this relation is linear, as suggested by Hoffmann et al. [2022].\nOur main contributions include a first rigorous mathematical characterization of the compute-optimal efficient frontier for a neural network model and development of information-theoretic tools which enable that. A limitation of our analysis is in its simplified treatment of computational complexity as the product of the model and data set sizes; we do not assume any constraints on computation beyond those imposed by choices of $p$ and $T$. In particular, we analyze, algorithms which carry out perfect Bayesian inference with respect to a model that is misspecificified due to its restricted size. While this abstracts away the details of practical training algorithms, empirical evidence suggests that our idealized framework leads to useful approximations [Zhu et al., 2022]. In spite of these limitations, we hope our results set the stage for further mathematical work to guide hyperparameter selection when training large neural networks."}, {"title": "Probabilistic Framework", "content": "We define all random variables with respect to a common probability space $(\\Omega, \\mathcal{F}, P)$. Recall that a random variable $F$ is simply a measurable function $\\Omega \\rightarrow \\mathcal{F}$ from the sample space $\\Omega$ to an outcome set $\\mathcal{F}$.\nThe probability measure $P : \\mathcal{F} \\rightarrow [0, 1]$ assigns probabilities to the events in the $\\sigma$-algebra $\\mathcal{F}$. For any event $E \\in \\mathcal{F}$, $P(E)$ to denotes the probability of the event. For events $E,G \\in \\mathcal{F}$ for which $P(G) > 0$, $P(E|G)$ to denotes the probability of event $E$ conditioned on event $G$.\nFor realization $z$ of a random variable $Z$, $P(Z = z)$ is a function of $z$. We denote its value evaluated at $Z$ by $P(Z)$. Therefore, $P(Z)$ is a random variable (it takes realizations in $[0,1]$ depending on the value of $Z$). Likewise for realizations $(y, z)$ of random variables $Y, Z$, $P(Z = z|Y = y)$ is a function of $(y, z)$ and $P(Z|Y)$ is a random variable which denotes the value of this function evaluated at $(Y, Z)$.\nIf random variable $Z : \\Omega \\rightarrow \\mathbb{R}^K$ has density $p_z$ w.r.t the Lebesgue measure, the conditional probability $P(E|Z = z)$ is well-defined despite the fact that for all $z$, $P(Z = z) = 0$. If function $f(z) = P(E|Z = z)$ and $Y : \\Omega \\rightarrow \\mathbb{R}^K$ is a random variable whose range is a subset of $Z$'s, then we use the $\\leftarrow$ symbol with $P(E|Z \\leftarrow Y)$ to denote $f(Y)$. Note that this is different from $P(E|Z = Y)$ since this conditions on the event $Z = Y$ while $P(E|Z \\leftarrow Y)$ indicates a change of measure."}, {"title": "Data", "content": "We consider a stochastic process which generates a sequence $(X_t, Y_{t+1} : t \\in \\mathbb{Z}_+)$ of data pairs. For all $t$, we let $H_t$ denote the history $(X_0, Y_1, . . ., X_{t-1}, Y_t, X_t)$ of experience. We assume that there exists an underlying latent variable $F$ such that $(X_0, X_1, ...) | F$ and $F$ prescribes a conditional probability measure $F(\\cdot|H_t)$ to the next label $Y_{t+1}$. In the case of an iid data generating process, this conditional probability measure would only depend on $H_t$ via $X_t$. Note that the current pre-training objective of foundation models falls under this iid setting in which for all $t$, $X_t$ is a random segment of the training corpus and $Y_{t+1}$ is the subsequent token. As our framework is Bayesian, we represent our uncertainty about $F$ by modeling it as a random variable with prior distribution $P(F \\in \\cdot)$."}, {"title": "A Learning Objective", "content": "We focus on a particular notion of error which facilitates analysis via Shannon information theory and reflects the objective of modern foundation models. For all $t \\in \\mathbb{Z}_+$, our algorithm is tasked with providing a predictive distribution $P_t$ of $Y_{t+1}$ which may depend on the history of data which it has already observed $H_t$. We express such an algorithm as $\\pi$ for which $P_t = \\pi(H_t)$. As aforementioned, an effective learning system ought to leverage data as it becomes available and perform well across all time. As a result, for any time horizon $T \\in \\mathbb{Z}_+$, we are interested in quantifying"}, {"title": "Error of Constrained Predictors", "content": "We introduce a general upper bound on the reducible error of a constrained predictor. While the formulations remain abstract in this section, a useful running example is the following: Assume that $F$ is an infinite width neural network which generates the data and $\\tilde{F}$ is a finite width network."}, {"title": "A Constrained Predictor", "content": "$F$ may exhibit endless complexity, likely beyond what can be represented with finite memory hardware. To represent the predictions made by a constrained predictor, we first define a random variable $\\tilde{F}$ whose range is a subset of $F$'s. As aforementioned, this random variable can be a lossy compression of $F$ i.e. if $F$ is represented by an infinite-width neural network, $\\tilde{F}$ could be a finite-width approximation. For all $t$, let the constrained predictor be:\n$P_t(\\cdot) = \\sum_{\\tilde{f}}P(\\tilde{F} = \\tilde{f}|H_t) \\cdot P(Y_{t+1} \\in \\cdot|\\tilde{F} = \\tilde{f}, X_t)$.\nThe predictor performs inference on $\\tilde{F}$ but performs predictions as if $F = \\tilde{F}$. We let"}, {"title": "Error of Constrained Predictor", "content": "Theorem 3.1. For all $T \\in \\mathbb{Z}_{++}$ and random variables $\\tilde{F} : \\Omega \\leftrightarrow F$, $\\hat{F} : \\Omega \\leftrightarrow F$ for which $\\tilde{F} \\subseteq F$, if $((X_t, Y_{t+1}) : t \\in \\mathbb{Z}_+)$ is iid conditioned on $F$, then\n$\\mathcal{L}_T(\\tilde{F}) \\leq \\frac{I(F; \\tilde{F})}{T} + E \\Big[ d_{KL} \\big( P^*(\\cdot) || P_t(\\cdot) \\big) \\Big]$,\nwhere $P_t(\\cdot) = P(Y_{t+1} \\in \\cdot|\\hat{F} \\leftarrow \\tilde{F}, X_t)$."}, {"title": "Scaling Law", "content": "For a FLOP constraint $C \\approx p \\cdot T$, it is clear that there is a tension between $p$ and $T$ in minimizing the upper bound in Theorem 3.1. This can be seen by first fixing a FLOP count $C$ and substituting $T = C/p$. The upper bound becomes:\n$\\frac{p \\cdot I(F; \\tilde{F})}{C} + E \\Big[ d_{KL} \\big( P^*(\\cdot) || P_t(\\cdot) \\big) \\Big]$.\nNote that the first term is increasing in $p$ whereas the second term is decreasing in $p$. Therefore, under a fixed FLOP budget, the designer ought to select a value of $p$ which effectively balances the two sources of error."}, {"title": "An Illustrative Example", "content": ""}, {"title": "Data Generating Process", "content": "The generating process is described by a neural network with $d$ inputs, a single asymptotically wide hidden layer of ReLU activation units, and a linear output layer. We denote by $F$ the associated mapping from input to output. Inputs and binary labels are generated according to $X_t \\overset{\\text{ind}}{\\sim} \\mathcal{N}(0, I_d)$ and $P(Y_{t+1} = 1|F, X_t) = \\sigma(F(X_t))$ where $\\sigma$ denotes the sigmoid function.\nAs alluded to by the asymptotic width, $F$ is a nonparametric model which we will outline now. Let $\\theta$ be distributed according to a Dirichlet process with base distribution uniform($\\mathbb{S}^{d-1}$) and scale parameter $\\kappa$. Realizations of this Dirichlet process are probability mass functions on a countably infinite subset of $\\mathbb{S}^{d-1}$. Let $\\mathcal{W} = \\{w \\in \\mathbb{S}^{d-1} : \\theta_w > 0\\}$ denote this set. For all $w \\in \\mathcal{W}$,\n$\\theta_w = \\begin{cases}\n\\hspace{1em} \\theta_w & \\text{with probability } 1/2, \\\\\n-\\theta_w & \\text{otherwise}.\n\\end{cases}$\nFinally, we have that\n$F(X_t) = \\sqrt{\\kappa + 1} \\cdot \\sum_{w \\in \\mathcal{W}} \\theta_w \\text{ReLU}(w^T X_t)$.\nSince $\\mathcal{W}$ has countably infinite cardinality, $F$ is characterized by a neural network with infinite width. We let $\\theta = (\\theta_w : w \\in \\mathcal{W})$ and $W = (w : w \\in \\mathcal{W})$ denote the weights of such neural network and hence\n$F(X) = \\sqrt{\\kappa + 1} \\cdot \\theta \\text{ReLU}(W X_t)$.\nNote that the mean and variance structure satisfy\n$\\mathbb{E}[F(X)] = 0, \\qquad \\mathbb{E}[F(X)^2] = 1/2$.\nTherefore, this model remains nontrivial as $d$ and $\\kappa$ grow as all of the above quantities are invariant of $d$ and $\\kappa$."}, {"title": "Constrained Predictor", "content": "We will study the scaling law associated with a particular constrained predictor characterized by a neural network of width $n$. Let $\\tilde{w}_1, \\tilde{w}_2, . . ., \\tilde{w}_n$ be distributed iid Categorical($\\delta$), where $\\mathcal{W}$ are the classes. For any $\\epsilon > 0$, let $\\mathbb{S}_\\epsilon^{d-1}$ be an $\\epsilon$-cover w.r.t $||\\cdot||_2$ and for all $i \\in [n]$, let\n$\\tilde{w}_{i,\\epsilon} = \\underset{v \\in \\mathbb{S}_\\epsilon^{d-1}}{\\text{arg min}} ||\\tilde{w}_i - v||_2$.\nFinally, let\n$\\hat{F}_{n,\\epsilon}(X_t) = \\frac{\\sqrt{\\kappa + 1}}{n} \\sum_{i=1}^n \\text{sign} (\\theta_{\\tilde{w}_{i,\\epsilon}}) \\text{ReLU} (\\tilde{w}_{i,\\epsilon}^T X_t)$."}, {"title": "Error Bound", "content": "We will now study the error incurred by the constrained predictor described above. We define\n$\\mathcal{L}_{T,n,\\epsilon} = \\frac{1}{T} \\sum_{t=0}^{T-1} \\mathbb{E} \\Big[ d_{KL} \\big( P(Y_{t+1} \\in \\cdot|\\theta, X_t) || P_t(\\cdot) \\big) \\Big]$\nas the loss of interest.\nTheorem 4.1. For all $n, \\kappa, T \\in \\mathbb{Z}_{++}$ and $\\epsilon \\geq 0$, if for all $t \\in \\{0, 1, 2, . . ., T - 1\\}$, $(X_t, Y_{t+1})$ is generated by $F$, then\n$\\mathcal{L}_{T,n,\\epsilon} \\leq \\frac{\\frac{\\kappa}{2} \\ln \\big(1 + \\frac{n}{\\kappa}\\big) \\big( \\ln(2n) + d \\ln \\big(\\frac{3}{\\epsilon}\\big) \\big)}{T} + \\frac{\\frac{3\\kappa}{2}(1 + d\\epsilon^2)}{n}$.\nThe estimation error represents the error which is incurred in the process of learning $F$ from $H_T$. Notably, this error decays linearly in $T$, but only depends logarithmically in $n$. The misspecification error represents the error which persists due to the fact that we approximate $F$ via $\\hat{F}_{n,\\epsilon}$. As a result, this error decreases with greater $n$ and smaller $\\epsilon$, but is independent of $T$. If we let $\\mathcal{L}_{T,n} = \\inf_{\\epsilon > 0} \\mathcal{L}_{T,n,\\epsilon}$, then\nCorollary 4.2. For all $n \\geq 3, \\kappa > 2, T \\in \\mathbb{Z}_{++}$, if for all $t \\in \\{0, 1, 2, . . ., T - 1\\}$, $(X_t, Y_{t+1})$ is generated by $F$, then\n$\\mathcal{L}_{T,n} \\leq \\frac{d\\kappa \\ln \\big(1 + \\frac{n}{\\kappa}\\big) \\big( \\ln(e^{36TK}) + \\ln(2n) \\big)}{2T} + \\frac{\\frac{3\\kappa}{2}}{n}$."}, {"title": "Resulting Scaling Law", "content": "Corollary 4.2 provides an upper bound on loss. We conjecture that this upper bound is tight to within logarithmic factors. As such, we consider its use as an approximation of loss to guide allocation of compute resources in the following result:\nTheorem 4.3. (compute-optimal parameter count) For all $d, \\kappa \\in \\mathbb{Z}_{++}$ and FLOP counts $C \\in \\mathbb{Z}_{++}$, if $\\kappa > 2, d \\geq 3$, and $n^*$ minimizes the upper bound of Corollary 4.2 subject to $d \\cdot n \\cdot T \\leq C$, then\n$d \\cdot n^* = \\mathcal{O}(\\sqrt{C})$.\nA proof of Theorem 4.3 can be found in Appendix A.2. Recall that the FLOP count $C$ is the product of the parameter count ($n \\cdot d$) and the dataset size $T$. The above result states that for any FLOP budget $C$, we can minimize the loss upper bound from Corollary 4.2 by allocating $\\mathcal{O}(\\sqrt{C})$ to the parameter count. This would dictate that the optimal dataset size would also be $\\mathcal{O}(\\sqrt{C})$. This result is consistent with the insights of Hoffmann et al. [2022] that, up to logarithmic factors, the optimal parameter count grows linearly with the training dataset size."}, {"title": "Conclusion", "content": "Our results provide a first step in developing rigorous mathematics for the purposes of analyzing scaling laws for foundation models. We hope that this will inspire further theoretical research on the subject. Our analysis is based on an error upper bound and furthermore, our analysis restricts attention to single-hidden-layer feedforward neural networks. Generalizing the results to treat state-of-the-art architectures remains an open issue. Furthermore, we have only considered allocation of pretraining compute. State-of-the-art performance in modern application domains relies on subsequent fine-tuning (see, e.g., [Ziegler et al., 2019]) through reinforcement learning from human feedback. How best to allocate resources between pretraining and fine-tuning is another area that deserves attention. An information-theoretic framework that treats pretraining, fine-tuning, and decision making in a unified and coherent manner, perhaps in the vein of [Lu et al., 2023], might facilitate theoretical developments on this front."}, {"title": "Proofs of Theoretical Results", "content": "Theorem 3.1. For all $T \\in \\mathbb{Z}_{++}$ and random variables $\\tilde{F} : \\Omega \\leftrightarrow F$, $\\hat{F} : \\Omega \\leftrightarrow F$ for which $\\tilde{F} \\subseteq F$, if $((X_t, Y_{t+1}): t \\in \\mathbb{Z}_+)$ is iid conditioned on $F$, then\n$\\mathcal{L}_T(\\tilde{F}) \\leq \\frac{I(F; \\tilde{F})}{T} + \\mathbb{E} \\Big[ d_{KL} \\big( P^*(\\cdot) || P_t(\\cdot) \\big) \\Big]$,\nwhere $P_t(\\cdot) = P(Y_{t+1} \\in \\cdot|\\hat{F} \\leftarrow \\tilde{F}, X_t)$.\nProof.\n$\\mathcal{L}_T(\\tilde{F})$"}, {"title": "Proof of Dirichlet Process Results", "content": ""}]}