{"title": "Tree-of-Code: A Hybrid Approach for Robust\nComplex Task Planning and Execution", "authors": ["Yifan Li", "Ziyi Ni", "Daxiang Dong"], "abstract": "The exceptional capabilities of large language models (LLMs) have substantially\naccelerated the rapid rise and widespread adoption of agents. Recent studies have\ndemonstrated that generating Python code to consolidate LLM-based agents' ac-\ntions into a unified action space (CodeAct) is a promising approach for developing\nreal-world LLM agents. However, this step-by-step code generation approach often\nlacks consistency and robustness, leading to instability in agent applications, par-\nticularly for complex reasoning and out-of-domain tasks. In this paper, we propose\na novel approach called Tree-of-Code (ToC) to tackle the challenges of complex\nproblem planning and execution with an end-to-end mechanism. By integrating key\nideas from both Tree-of-Thought and CodeAct, ToC combines their strengths to\nenhance solution exploration. In our framework, each final code execution result is\ntreated as a node in the decision tree, with a breadth-first search strategy employed\nto explore potential solutions. The final outcome is determined through a voting\nmechanism based on the outputs of the nodes. Experimental results on complicated\ntask datasets demonstrate that our method provides more stable results compared\nto Tree-of-Thought and achieves higher accuracy than CodeAct.", "sections": [{"title": "Introduction", "content": "In recent years, the application of code generation techniques to complex task planning and execution\nhas garnered significant attention [11, 22, 24], particularly with the emergence of CodeAct [20]\napproaches. CodeAct has demonstrated remarkable efficiency in generating executable code for\ncomplex tasks, improving overall performance in terms of speed and accuracy. However, the lack\nof consistent reasoning in CodeAct leads to frequent interruptions during multi-step generation,\ncausing fragmented and stalled thinking. Additionally, this process accumulates significant model\nhallucinations [13], with increasing randomness, which ultimately undermines the robustness required\nfor reliably solving complex problems.\nTo address this challenge, we developed an end-to-end thought-code-execution pipeline that enables\nmodels to autonomously generate plans and decompositions for complex tasks, with clear reasoning.\nPost planning, the model produces implementation code that references earlier reasoning and leverages\nits innate code processing capabilities, making implicit reasoning explicit through code. Execution\nof this code yields results. Inspired by the Tree-of-Thought paradigm, our approach emphasizes\nstructured solution exploration using decision trees and incorporates continuous code reflection. The\nmain approach can be summarized into three parts:\n1. End-to-End Code Generation: By producing complete code solutions end-to-end, we mini-\nmize the need for intermediate reflection on execution results, thus enhancing stability, and\ndesign a long thought-code reasoning process. We introduce 'llm-function' to enable large\nmodels to generate prompts and summarize corresponding outcomes.\n2. Exploration of Incomplete Nodes: Leveraging the Tree-of-Thought methodology, we explore\nnodes with incomplete execution results by varying prompts, large language models, and\nmodel temperatures to improve result stability.\n3. Majority Voting for Final Results: large language models perform majority voting on all\nsuccessfully executed nodes to determine the outcome.\nThe contribution of this paper: 1) We propose the Tree-of-Code method, enhancing stability in\ncomplex task execution through efficient model integration. 2) Our approach can integrate various\nlarge language models without the need to do fine-tuning. 3) empirical validation of improved\nproblem-solving performance."}, {"title": "Related Work", "content": "LLM Reasoning: Enhancing reasoning capabilities is a crucial step toward improving general\nLLMs. The current mainstream approaches focus on training techniques\u2014such as pretraining [10],\nSFT [6], prompt-tuning [10], model editing [18], and reinforcement learning [17, 26]\u2014as well\nas the widely used method of prompt engineering to reduce output hallucinations and improve\nreasoning [5]. The latter primarily involves designing various frameworks for structured chains of\nthought. Longer thought chains seem to always yield better results [26, 21]. The recent success of\nGPT-01 has also highlighted the importance of extended chains of thought (CoT) [21]. Notably,\nexisting CoT method, even with simple prompts like \"let's think step by step,\" can achieve over a\n30% improvement. The Tree of Thoughts (ToT) framework [25] further enhances LLM reasoning\nby generating and evaluating intermediate steps, employing search algorithms to systematically\nexplore and backtrack thought paths, enabling more effective problem-solving. Other emerging\nframeworks, like Graph-of-Thought [4] and Algorithm-of-Thought [16], are also contributing to\nthis area of research. Code Generation: It is widely accepted that training LLMs with codes can\nsignificantly enhance their reasoning abilities [6, 1, 7, 9, 27]. Furthermore, research has shown that\ncode, as a formal programming language, closely mirrors logical structures, inherently containing\nflow control mechanisms that can stimulate reasoning capabilities. For instance, [20, 9, 19] argue that\ncode serves as an effective medium for representing reasoning processes. Recent works combining\ncode with agents have mainly focused on task completion in purely programming-related domains,\nsuch as software development [19, 14] and assisting humans in competitive programming tasks\n[23, 12]. However, these models do not use code as a scalable language, such as JSON, for direct\ncommunication in everyday tasks. CodeAct [20] attempted to use code directly to solve entire tasks,\nbut the granularity of each step was too small, leading to error accumulation during function calls."}, {"title": "Tree-of-Code: Code Generation Through Tree-Structured Reflection", "content": "The Tree-of-Code (ToC) method combines the structured exploration of Tree-of-Thought with\nthe efficient task planning of CodeAct. ToC introduces a tree-based generation and exploration\nmechanism that utilizes various code generation strategies and models, enhancing the robustness\nof execution results. By treating code as a form of reasoning, ToC capitalizes on the unique\ncharacteristics of code, such as consistency and determinism, to develop a more effective agent"}, {"title": "Overview the Tree-of-Code System", "content": "The Tree-of-Code (ToC) framework presents a comprehensive code reasoning process comprising\nfour key stages: thought, code generation, execution, and reflection. We represent this reasoning\nprocess as a tree $T = (N, L)$ In contrast to the thought chain series, where thought steps are\norganized into nodes (N), our approach emphasizes end-to-end generation. Here, we define nodes as\nthe combination of code generation and its corresponding execution. This tight coupling between\ncode and execution results ensures logical consistency and correctness while facilitating high-quality\noutcomes for subsequent model training. To enhance post-generation reflection through iterative\nrefinement, we model the reflection process as lines (L), which can encompass various strategies\nfor improvement: 1) System-level reflection: Sampling from diverse models to explore multiple\nsolutions. 2) Operation-level reflection: Modifying evaluation and reflection strategies to iterative\nenhance output. This structured approach guarantees that the final output is both accurate and diverse,\nstriking a balance between correctness and a variety of reasoning pathways to yield robust solutions."}, {"title": "Thought And Code Generator", "content": "Code generation is a critical component of code-as-reasoning. In our framework, the thought and code\ngeneration stage integrates interactions between the user, environment, and agent: The user provides\nthe task queries. The environment executes the generated code, providing execution feedback. The\nagent synthesizes information from the user, environment, and history, translating this into codes,\nwhich it then executes. The process of transforming thought into executable codes can be formalized\nas:\n$\\text{Execution}(i) = \\text{Code}(F_C(i), F_R(i), \\text{Thought}(i), \\text{Functions}) \\quad \\forall i \\in \\{1,..., n\\}$\nwhere $\\text{Thought}(i)$ represents the agent's internal reasoning at step i, and $\\text{Code} (F_C(i), F_R(i)$ and\n$\\text{Thought}(i), \\text{Functions})$ converts that reasoning into Python code for execution. $F_C(i)$ represents\nthe code of father node of the current node. $F_R(i)$ represents the code execution result of father node\nof the current node. $\\text{Functions}$ represent all the tools that current task can use.\nTo generate an end-to-end solution for current task, we add a llm-function tool into Functions set.\nWith Ilm-function, our code generation LLMs will generate prompt from the current context and call\nIlm-function to generate final results."}, {"title": "Tree Expansion and State Evaluation", "content": "We initialize from a root node and do tree expansion recursively based on termination criteria. Every\ntime a node's code is executed, state evaluation will be executed based on the code execution results.\nThe process is as follows: 1) The process starts at the root node, expanding the tree incrementally,\nwith each node representing generated thoughts, code, and execution results. 2) Generated codes of\neach leaf node will be executed in a Python environment. When errors are detected, child nodes are\nexpanded from the erroneous node to analyze specific issues and explore alternative solutions. 3)\nThis expansion continues until all leaf nodes execute successfully or a maximum depth is reached.\nThis strategy effectively manages complexity and ensures correct and efficient code execution results."}, {"title": "Final Result Generator", "content": "After candidate outputs from all executed nodes are collected, these results undergo a majority vote\nand tragic summarization process to determine the most likely single answer. Explanations of the\nfinal answer are not included, just the precise answer to the question is shown."}, {"title": "Experiment and Analysis", "content": "Our experiments evaluated the effectiveness of the ToC framework, comparing its performance\nprimarily against the CodeAct framework. We used the M\u00b3ToolEval, a newly curated benchmark\nfor evaluating performance across complex multi-scene tasks, the same used in CodeAct [20], to\nbenchmark ToC. We evaluated the input samples from M\u00b3 ToolEval. The context window was fixed at\n3k tokens, and the generation depth was set at 3 for consistency in computational costs. To highlight\nthe superiority of our method, we directly compared it to the best-performing model in CodeAct,\nwhich is based on GPT-4. In the tests, ToC achieved a 7.2% higher accuracy than CodeAct in tasks,\nand it reduced interaction steps significantly, demonstrating overall effectiveness and robustness in\ntask completion.\nTo ensure diverse outputs, we applied a multi-level sampling strategy: 1) Model Diversity: A range of\ngenerative models (e.g., GPT-4 [2], ERNIE-4.0-Turbo [15], DeepSeek Coder [8], and Claude 3.5 [3])\nwas utilized to introduce variability at the model level. 2) Temperature Variability: Adjustments\nto generation temperatures were made within specified bounds to encourage content variation. 3)\nPrompt Variation: Different role instructions and adaptive strategies were explored to further enhance\nadaptive diversity.\nFurther analysis compared ToC against both the JSON-based and thought-based modes of CodeAct.\nAcross all criteria, ToC consistently outperformed other traditional reasoning frameworks, particularly\nin managing complex reasoning tasks."}, {"title": "Conclusion", "content": "In this paper, we introduced the Tree-of-Code (ToC) method, which combines the strengths of\nTree-of-Thought and CodeAct to enhance model robustness and accuracy. Both thought and code"}]}