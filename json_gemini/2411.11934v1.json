{"title": "SpatialDreamer: Self-supervised Stereo Video Synthesis from Monocular Input", "authors": ["Zhen Lv", "Yangqi Long", "Congzhentao Huang", "Cao Li", "Chengfei Lv", "Hao Ren", "Dian Zheng"], "abstract": "Stereo video synthesis from a monocular input is a demanding task in the fields of spatial computing and virtual reality. The main challenges of this task lie on the insufficiency of high-quality paired stereo videos for training and the difficulty of maintaining the spatio-temporal consistency between frames. Existing methods primarily address these issues by directly applying novel view synthesis (NVS) techniques to video, while facing limitations such as the inability to effectively represent dynamic scenes and the requirement for large amounts of training data. In this paper, we introduce a novel self-supervised stereo video synthesis paradigm via a video diffusion model, termed SpatialDreamer, which meets the challenges head-on. Firstly, to address the stereo video data insufficiency, we propose a Depth based Video Generation module DVG, which employs a forward-backward rendering mechanism to generate paired videos with geometric and temporal priors. Leveraging data generated by DVG, we propose RefinerNet along with a self-supervised synthetic framework designed to facilitate efficient and dedicated training. More importantly, we devise a consistency control module, which consists of a metric of stereo deviation strength and a Temporal Interaction Learning module TIL for geometric and temporal consistency ensurance respectively. We evaluated the proposed method against various benchmark methods, with the results showcasing its superior performance.", "sections": [{"title": "1. Introduction", "content": "Stereo video synthesis from a monocular input aims to generate the target-view video based on the given view with geometric and spatio-temporal consistency, which has a wide range of applications in 3D model reconstruction [6], 3D movie production [18, 38], and Apple Vision Pro (AVP) [42] like virtual reality content.\nThe primary difficulties of this task stem from the lack of adequate high-quality paired stereo videos for training, and the challenge of preserving the geometric consistency between two views as well as maintaining the temporal consistency across generated frames. Traditional methods for"}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Single-image Novel View Synthesis", "content": "Early studies address the single-image novel view synthesis task without relying on explicit 3D representation, such as End-to-end Synthesis [67], Infinite-Nature [35] and Animatable 3D Characters [66]. These methods do not utilize explicit 3D representations for entire scenes, which limits the consistency of 3D views in the generated outputs and results in flickering and blurring, particularly in occluded regions. Layer-based methods, such as 3D-photography [54] and SLIDE [26] represent a 3D scene using discrete layers. Yet, the synthesized views may lack accuracy in texture and geometric consistency across different perspectives. SMPI [59], AdaMPI [21] and SinMPI [45] represent the 3D scene using multiple planes, while their rendered results may suffer from depth discretization artifacts and repetitive texture patterns. To reduce the reliance on large labeled datasets, Shi et.al [53], Wang et al. [64] and NVSVDE-Net [4] explore self-supervised learning for creating novel view images from single image. Although they are capable of producing photorealistic results, they struggle to maintain temporal and spatial consistency, particularly in dynamic scenes.\nAs for our approach, we design a depth-based video generation module that generates paired videos with geometric and temporal priors. Additionally, Our method ensures multi-view consistency for the entire scene by employing a consistency control module."}, {"title": "2.2. Multi-view Novel View Synthesis", "content": "Early approaches in this field frequently utilized geometric methods, such as multi-view stereo reconstruction [29, 52] and structure-from-motion techniques [44, 51], to generate novel viewpoints from multiple images captured at different angles. NeRF-based methods were proposed to generate novel views based on fewer input images, including SinNeRF [70], CoPoNeRF [24] and NViST [27]. 3D Gaussian Splatting [31] presented an explicit representation of the scene. Gaussian-based methods, such as pixelSplat [12] and MVSplat [14], can produce high-quality synthesis results from multi-view images. Recently, researchers have begun exploring the application of advanced diffusion models for novel view synthesis, driven by their remarkable success in this area. Notable approaches such as Photoconsistent-NVS [73], MultiDiff [40], GCD [61] and NVS-Solver [72] generate image sequences based on predefined camera trajectories using either single or multiple input images.\nWhile these methods have shown promising results, they often suffer from limitations such as dependence on dense scene geometry, handling complex scene dynamics, and being hard to generalize [31]. Moreover, the challenge of optimizing NeRF/GS with a limited number of input views hampers the ability of multi-view NVS to produce high-quality results.\nIn contrast, we explore a video diffusion model to achieve stereo video synthesis, while preserving geometric structures through a RefinerNet based framework."}, {"title": "2.3. Video Synthesis via Diffusion Models", "content": "For video synthesis, previous works mainly expand image models to video, exploiting the image generation capability in the pretrained image diffusion models [36]. The video diffusion model (VDM) [23] and Video LDM [8] are proposed to adopt a 3D U-Net structure for joint image and video training. Subsequent works [15, 19, 19, 25, 55] also follow this insight with temporal attention, and [32, 46, 63, 69] further apply cross-frame attention to improve the consistency of the generated video. Another approach to promote temporal consistency is encoding video into 2D images [3, 43], which has been mainly explored in the video editing area. Although these models produce consistent results, they often require per-video optimization, which takes a long time, and would result in degradation when dealing with large motion. Most recently, [10, 20, 33] employ a transformer [62] architecture to model the temporal and spatial relations, demonstrating excellent video generation capabilities. However, these models need very large scale video datasets for training, which can be difficult to collect.\nBeyond temporal information modeling, some works [13, 17, 65, 75] have introduced aligned spatial guidance to enhance the stability of the generated videos. The proposed framework also uses spatial guidance. Differing from those methods which require spatially aligned controls, a spatially unaligned reference image is used via a cross-attention mechanism. Furthermore, a metric of stereo deviation strength is introduced to strengthen the spatial constraint, resulting in more consistent results."}, {"title": "3. SpatialDreamer", "content": "Given an image sequence and a stereoscopic pose describing the scene depth information, the proposed method generates a stereo pair. In this section, we first provide a brief introduction about SVD [7]. Secondly, we offer detailed information of our proposed video generation module DVG, which can generate sufficient paired video with geometric and temporal consistency. Based on the DVG, we present the framework for self-supervised learning based video synthesis to support efficient and dedicate training. More importantly, to ensure the consistency between frames, we design a consistency control module for geometric and temporal consistency ensurance. Finally, we present the detailed training process."}, {"title": "3.1. Preliminaries", "content": "The challenge in video generation lies in accurately modeling the spatial-temporal dynamics, which involve understanding the spatial relationships within individual frames and the temporal relationships across frames. To address this challenge, Stable Video Diffusion (SVD) [7] introduces temporal convolution and attention layers. SVD follows the latent diffusion model (LDM) to encode video pixels into the latent space, enabling a more efficient denoising process. Given a video $I_v$, encoder $E$ encodes $I_v$ into the latent space as $z_0 = E(I_v)$. Gaussian noise $\\epsilon$ is then added in the Markov process:\n$z_t = \\sqrt{\\bar{a}_t}z_0 + \\sqrt{1 - \\bar{a}_t}\\epsilon_t$  (1)"}, {"title": "3.2. Depth based Video Generation", "content": "To create training pairs of images along with their corresponding occluded masks, the following self-supervised mechanism is employed utilizing real images. Firstly, the depth of the given video sample is estimated by MiDaS [47]. Secondly, the reference view image $x_1$ is rendered into a masked one under the target viewpoint $P_2$, and then the trained inpainting model [56] is used to fill these occluded regions to obtain the novel viewpoint $x_2$ [41]. Finally, $x_2$ is rendered back to the original viewpoint $P_1$ via a back rendering. After this, we can obtain the masked image $\\hat{x}_1$ with mask $M$ under viewpoint $P_1$, as well as the inpainted image $x_2$ under viewpoint $P_2$. The mask $M$ and masked image $\\hat{x}_1$ are considered to be the conditions, with the image $x_1$ serving as the ground truth for the diffusion model. This approach involves encoding the inpainted image $x_2$ into latent feature representations and feeding it into RefinerNet.\nHowever, the aforementioned approach encounters challenges in addressing inconsistencies among generated data frames, resulting in observable artifacts such as jittering and flickering. To tackle this issue, we leverage optical flow [58] to capture motion between successive video frames. Specifically, by leveraging the optical flow between the reference image $x_1$ at time $t$ and its adjacent frames, we establish the pixel correspondences between the image $x_t$ and the images $x_{t-1}$ and $x_{t+1}$, respectively. Particularly, these optical flows enable computing a confidence map via forward-backward consistency [16], which allows us to more accurately merge the stereo occlusions in the masks between adjacent frames [1]. Subsequently, the refined occlusion mask $m^t$ is generated by sampling from the occlusion maps $m^{t-1}$ and $m^{t+1}$, leveraging the pixel correspondences between consecutive frames under the reference view, as described by the following equation.\n$m^{t}(i, j) = \\begin{cases}\n1, & \\sum_{k \\in \\{t-1, t+1\\}} m^{k}(i+u, j + v) \\cdot C(i, j) \\ge 1 \\\\\nm^{t}(i, j), & else\n\\end{cases}$  (2)\nwhere $u$ and $v$ are the x and y components of the optical flow vector, and $C$ represents the optical flow confidence measure. By propagating information from occluded areas across frames, this inter-frame continuity facilitates a more precise refinement of occluded regions in paired view"}, {"title": "3.3. Self-Supervised Stereo Video Synthesis framework", "content": "Leveraging the generated paired view videos, we propose a self-supervised framework for stereo video synthesis, as illustrated in Fig 2. This framework commences with the rendering of the target view video that incorporates occlusions, followed by the application of a spatio-temporal method to effectively inpaint the missing regions, ultimately yielding a complete non-occluded target view video.\nTo learn the distribution of the feature space between the paired view images, we devise RefinerNet, which is modeled after the architecture of the denoising U-Net, excluding the temporal layer. RefinerNet adopts the weight initialization from the original SD2.1 model [48]. As depicted in Figure 2, a spatial attention layer is employed instead of the self-attention layer. Given a feature map $z_t \\in R^{t \\times h \\times w \\times c}$ from the denoising U-Net and another feature map $z_r \\in R^{t \\times h \\times w \\times c}$ from RefinerNet along with TIL module, they are concatenated along the h dimension, and then self-attention is applied. Subsequently, the first half of the feature map is retrieved as the output. One great advantage of this approach is that it enables RefinerNet to maintain the feature representation capability inherent in the original SD Unet, extracting reference view image information. In addition, the denoising U-Net can adaptively learn features from RefinerNet that are correlated in the same feature space, owing to the fundamentally similar network architectures between the denoising U-Net and RefinerNet. This not only ensures a robust initialization for the"}, {"title": "3.4. Consistency Control Module", "content": null}, {"title": "3.4.1 Temporal Interaction Learning module", "content": "While our RefinerNet-based approach providing spatial guidance, paired view features are still merged independently, posing challenges for temporal consistency and the generated images may exhibit discrepancies in texture or produce peculiar results, particularly between adjacent frames.\nInspired by previous studies [11, 22], we propose an attention-based temporal interaction learning module that learns the distribution of the feature space from adjacent frames, named TIL. Specifically, given latent codes of $N_r$ adjacent images under reference view $z^i_t$, where $i = 1, 2, ..., N_r$ and the latent features of the reference view image $z_t$ at the time step t, we augment the $z^i_t$ to $z^{aug}_t$ as:\n$z^{aug}_t = \\lambda \\cdot Attn_{self_t} + (1-\\lambda) \\cdot \\frac{1}{N_r} \\sum_{i=1}^{N_r} Attn_{cross_ti}$  (3)\nwhere $\\lambda \\in [0, 1]$ and $Attn_{i,j}$ defines attention between two latent features $z_i$ and $z_j$. This module blends the self-attention of $z_t$ with the cross-view attention between $z_t$ and each adjacent view $z^i_t$. The cross-view attention aligns the feature of all adjacent frames to the reference view while the self-attention helps each reference image retain distinctiveness.\nThe augmented reference feature $z^{aug}_t$ is then fed into spatial attention layer to assist U-Net network learning."}, {"title": "3.4.2 Stereo Deviation Strength", "content": "During the training, we noticed that different viewpoints impact the 3D impression of the scenes in the generated videos. However, relying solely on a given viewpoint as a guide for generating stereoscopic videos is insufficient. This is because videos with the same viewpoint may exhibit diverse depth ranges and variations, depending on the scene [68]. Therefore, pose alone cannot accurately control the 3D effect of a scene. Consequently, we introduce a metric called the stereo deviation strength, which quantitatively assesses the binocular disparity in a scene and facilitates the creation of controllable stereo vision:\n$S(z) = ||z_0 - z_{ref}||$  (4)\nwhere the stereo deviation strength quantifies the latent differences between reference view $z_{ref}$ and target view $z_0$. Similar to the time step, the deviation strength is projected into a positional embedding and added to each frame in the"}, {"title": "3.5. Training Strategy", "content": "The training process is divided into two stages. In the first training stage, we focus on individual frames from videos. The temporal layer in denoising U-Net is frozen, and TIL module is removed. The RefinerNet model and the denoising U-Net are initialized with pretrained weights from SD2.1 [48] and SVD [7] respectively. The weights of the variational autoencoder encoder and decoder, as well as the contrastive language-image pre-training (CLIP) image encoder, are all kept fixed. The aim of this stage is for the model to learn to synthesize new viewpoint images under the condition of a reference image and a new viewpoint pose. In the second stage, we train the temporal layer and TIL module with video sequences. This enables the model to capture temporal context information efficiently. The input for the model is the 8-frame video clip and N for TIL is set to 8. During this stage, only the denoising U-Net and TIL module are trained while keeping the weights of the remaining network unchanged."}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Datasets, Baselines and Metrics", "content": "We conduct our experiments on image and video levels with RealEstate10K [76] and self-collected video data. RealEstate10K is a large, widely used dataset with camera poses, corresponding to 10 million frames derived from about 80,000 video clips, gathered from about 10,000 YouTube videos. As there is not open-source stereo video benchmark, we collect 1500 monocular videos with 1400 for training, 100 for testing. For the image synthesis, we compared the proposed method with recent open-source SOTA methods: 3D-photography [54], SynSin [67], AdaMPI [21], SinMPI [45], Wang et al. [64], Photoconsistent-NVS(P-NVS) [73], NVSVDE-Net [4], CoPoNeRF [24] and MVSplat [14]. For a fair comparison, the method of 3D-photography, AdaMPI, SinMPI, and the proposed method all used MiDaS [47] for the depth estimation. All the methods were conducted with the same intrinsic matrices, source camera poses, and target camera poses. Following the method of 3D-photography [54], we measured the accuracy of the generated target views with regard to the ground-truth images using the three metrics of the learned perceptual image patch similarity (LPIPS), peak signal-to-noise ratio (PSNR), and structural similarity index measure (SSIM).\nFor the video synthesis, we picked several representative approaches, i.e., the method of 3D-photography [54], the open-source code repository webui-depthmap\u00b9, P-NVS [73], CoPoNeRF [24], NVS-Solver [72] and AVP [42]. Moreover, we conducted the same depth estimation method [47] for a fair comparison of 3D-photography [54], webui-depthmap and NVS-Solver [72]. We employ the FVD [60] score to measure the perceptual similarity between input videos and outputs and report the flow warping error $E_{warp}$ [34] to assess the temporal consistency of the resulting video sequences."}, {"title": "4.2. Implementation", "content": "We employed the AdamW [37] optimizer with a learning rate of 1 \u00d7 10-5 for training the model. All the experiments were conducted using two NVIDIA A800 GPUs. In the first training stage, individual video frames were sampled, resized, and center-cropped to a resolution of 1024 \u00d7 1024. We trained the model for 100,000 iterations with a batch size of 8. In the second training stage, the videos were first split according to the transitions to ensure that a scene only appeared in one video clip. We then formed each video clip for training with eight frames. Finally, the temporal layer and TIL module was trained for 10,000 steps with a batch size of 2. The stereo-aware loss scaling factor $\\lambda$ was set to 0.001 and $\\lambda$ in Eq. 3 was set as 0.6."}, {"title": "4.3. Qualitative Results", "content": null}, {"title": "4.3.1 Stereo Image Synthesis", "content": "We visually compared the outputs of the different methods by generating a new view image given the target viewpoint. We qualitatively compare the newly synthesized images in Figure 5. 3D-photography fails to resolve the image distortion problem, and the image appears to be enhanced only slightly by the rendering. SynSin generates the most blurry details and loses a lot of information, compared with the input view. Neither AdaMPI nor SinMPI can generate fine enough details, and the results contain noticeable artifacts. CoPoNeRF tends to produce discrepancies from the original image in terms of fine-grained details, and it struggles with effectively handling scenes involving motion, as well as MVSplat. In contrast, the proposed method shows a clean and detailed output, especially in the area of depth information discontinuity."}, {"title": "4.3.2 Stereo Video Synthesis", "content": "Moreover, we qualitatively compared the results of the stereo video synthesis with other methods. In addition to the quality issues of single-frame generation, the most important aspect in videos is the temporal consistency. We seek to generate videos that are free of flicker. The visualization of generated videos from different methods are illustrated in Figure 6, for each video, we show the scanline (the yellow line in figure) slice through the spatial-temporal volume. Our method achieve the smoothest trajectories, while other methods exhibit significant flickering. The most distinct regions are zoomed in and highlighted under the images to illustrate the details. In the first case, our method maintain the texture of the trees behind the woman. And the second case, only our method attain the smooth trajecto-"}, {"title": "4.4. Quantitative Results", "content": "Table 1 lists the SSIM, PSNR, and LPIPS scores for the newly synthesized images. The proposed method outperforms 3D-photography and SynSin in the major metrics by a wide margin. The method of Wang et al., CoPoNeRF and P-NVS perform slightly worse in terms of all metrics compared to the proposed method. Since the proposed method merges the spatial features and fills the occluded regions, it achieves the best LPIPS scores and shows competitive results in PSNR and SSIM when compared with AdaMPI and MVSplat. Overall, the results exhibit superior visual effects, which is consistent with the performance observed in the qualitative experiments. We further demonstrate the effectiveness of our method on video synthesis, the results are shown in Table 2. The proposed method achieves the best performance on both FVD and $E_{warp}$, which means our generated videos attain the best quality and temporal consistency."}, {"title": "4.5. Ablation Study", "content": null}, {"title": "4.5.1 Effectiveness of each components", "content": "We show the results in Table 3 on the RealEstate10K (stereo image) and collected stereo video datasets. CN means substituting our RefinerNet (RN) to ControlNet, SDS means the proposed stereo deviation strength. For image-level comparison, the results show that utilizing U-Net architecture only is not sufficient for stereo image synthesis and our RefinerNet is a better choice than ControlNet. Adding SDS further improves the geometric consistency. As visualized in Fig 7, solely relying on U-Net features results in distortion in the edge area and struggles with generating content properly. It also fails to preserve textural details near a plane boundary. Adding ControlNet often results in content inconsistency, presumably due to the failure of the paired view feature integration. RefinerNet handles this problem, showing it is a better architecture for stereo image generation. SDS further improves the consistency with the input and contain a more detailed image structure. As for video-level comparison, the quantitative results are shown in Table 3, adding TIL and DVG both attain the performance gain, and our final model achieves the best performance."}, {"title": "4.5.2 Data improvement by DVG", "content": "We show the generated stereo occlusion mask with inter-frame motion refinement in Fig 8. It can be observed that the occlusions appear more complete spatially and exhibit smoother in their temporal distribution. By employing DVG, we can acquire sufficient data to support the generation of spatio-temporally consistent videos."}, {"title": "4.5.3 Exploration of different depth estimation methods", "content": "We further compare different design choices of depth map including DepthAnything [71], Marigold [30], MiDaS [47] and ZoeDepth [5]. The results are available in supplementary materials. The proposed method can produce new viewpoint images that are realistic, seamless, and rich in detail, regardless of the depth estimation method used. This capability demonstrates the versatility and robustness of the proposed method, ensuring superior performance in various applications and environments."}, {"title": "5. Conclusion", "content": "In this paper, we introduce a new self-supervised stereo video synthesis approach using a video diffusion model, termed SpatialDreamer. This method addresses the issues of insufficient data and spatio-temporal inconsistency between frames. To solve the problem of insufficient data, we develop a Depth-based Video Generation (DVG) module that uses a forward-backward rendering mechanism to generate a rendered video with geometric and temporal priors. Additionally, we propose RefinerNet along with a self-supervised synthetic framework to enable efficient and dedicated training using data generated by DVG. Furthermore, we design a consistency control module to ensure geometric and temporal consistency. Our SpatialDreamer outperforms all other open-source stereo image and video synthesis methods and has the potential for future expansion into virtual reality applications."}]}