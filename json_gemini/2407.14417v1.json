{"title": "MIXTURE OF EXPERTS WITH MIXTURE OF PRECISIONS FOR\nTUNING QUALITY OF SERVICE", "authors": ["HamidReza Imani", "Abdolah Amirany", "Tarek El-Ghazawi"], "abstract": "The increasing demand for deploying large Mixture-of-Experts (MoE) models in resource-constrained\nenvironments necessitates efficient approaches to address their high memory and computational re-\nquirements challenges. Moreover, given that tasks come in different user-defined constraints and\nthe available resources change over time in multi-tenant environments, it is necessary to design an\napproach which provides a flexible configuration space. This paper presents an adaptive serving ap-\nproach for the efficient deployment of MoE models, capitalizing on partial quantization of the experts.\nBy dynamically determining the number of quantized experts and their distribution across CPU and\nGPU, our approach explores the Pareto frontier and offers a fine-grained range of configurations for\ntuning throughput and model quality. Our evaluation on an NVIDIA A100 GPU using a Mixtral\n8x7B MoE model for three language modelling benchmarks demonstrates that the throughput of\ntoken generation can be adjusted from 0.63 to 13.00 token per second. This enhancement comes\nwith a marginal perplexity increase of 2.62 to 2.80, 6.48 to 7.24, and 3.24 to 3.53 for WikiText2,\nPTB, and C4 datasets respectively under maximum quantization. These results highlight the practical\napplicability of our approach in dynamic and accuracy-sensitive applications where both memory\nusage and output quality are important.", "sections": [{"title": "Introduction", "content": "Utilizing Mixture-of-Experts (MoE) architectures [1, 2] in Large Language Models (LLMs) has significantly enhanced\nthe performance of Natural Language Processing (NLP) tasks [3, 4]. MoE-based transformer models employ multiple\nparallel feed-forward (FF) layers, known as experts, within each transformer block instead of a single FF layer. In this\narchitecture, a gating network is used to evaluate the input token generated by the attention layer and to assign weights\nto each expert to perform a weighted averaging. As a result, by scaling and adding more parameters, MoE models are\nable to achieve superior performance comparing to their dense counterparts.\nWith parameters reaching into the trillions, MoE models can expand to terabyte scale, making them challenging to\ndeploy for inference. Most of the available GPUs do not have sufficient resources to store the full model on their main\nmemory. Therefore previously proposed deployment approaches load the MoE model on GPU partially and consider\na swap space for the experts to be transferred between memories of CPU and GPU during inference which causes\nrecurring data movement and copy operations. Since these data movements take more time comparing to the actual\ncomputation on the GPU, they become bottleneck in the inference pipeline.\nMeanwhile, there is an increasing demand for deployment of customized LLMs in companies and research labs without\naccess to abundant resources or high-end GPUs. Additionally, in real-life scenarios, the computing systems available to\nthese entities are often shared among multiple users, with available resources fluctuating and user constraints changing\nover time. Therefore, it is important to design an adaptive serving systems that can accommodate deployment of large\nMoE models in dynamic resource-constrained settings."}, {"title": "Related Work", "content": "Prior work has covered different aspects to alleviate the inference process of MoE models. Model compression\ntechniques such as [5, 6, 7, 8] perform post-training quantization to directly reduce the size which can mitigate CPU-\nGPU communication overhead. However, these approaches reduce the quality of the output generated text considerably.\nMeanwhile, efficient serving approaches such as [5, 9] treat the memory available on the GPU as a cache and each\nexpert of the MoE model as a cache block therefore trying to minimize data movements by increasing the hit rate.\nAlthough these approaches improve the performance of MoE inference in throughput and output quality, they do not\nprovide the required flexibility to handle variable constraints.\nIn this paper, we focus on dynamic single-GPU settings and propose partial quantization of experts in MoE models.\nSince the main bottleneck of MoE deployment is data transfers between GPU and CPU and since the experts constitute\nmost of the model size, we only focus on experts to reduce the model's overall memory footprint. Moreover, to provide\nflexibility regarding to user-specific needs (task preference) and the available memory, we adaptively determines the\nnumber of 16-bit and 4-bit experts and their partitioning among CPU and GPU. This allows us to have fine-grained\ncontrol over the quality of the generated output, and throughput of token generation making it suitable for dynamic and\naccuracy sensitive applications. We evaluate our adaptive serving approach on an NVIDIA A100 GPU using a Mixtral\n8x7B MoE model [10]. The results show that the proposed approach is able to improve the throughput significantly\nwhile imposing negligible perplexity increase."}, {"title": "2.1 Mixture-of-Experts", "content": "The MoE architecture was first proposed in [1] to allow each FF network to specialize in a subset of the input space\nwhen dealing with clustered data. Later, in [3], the MoE architecture was employed in a machine translation task to\ninsert experts between stacked LSTM layers.\nTo apply the MoE concept to Transformer models, in [11, 12, 13, 14, 15, 16], the dense FF layers are replaced by sparse\nswitch FF layers in the Transformer block. To make these models computationally efficient, a top-k sparse gating\nmechanism is employed which selects only a subset of experts in each layer. This approach enables MoE models to\nscale linearly in size while imposing a minimal increase in computation, depending on the number of experts included\nin weighted averaging. This also makes training faster and more efficient compared to a dense model with the same\nnumber of parameters, since gradients are applied sparsely to the activated experts in the forward pass [17]."}, {"title": "2.2 Efficient Serving and Orchestration of MoE Models", "content": "To serve the MoE models efficiently, it is important to keep the model parameters as close to the GPU as possible.\nHowever, the model's large sizes make it impossible to store all the parameters on the limited accelerator memory.\nTherefore, as proposed in [18], maximum number of model parameters are stored on GPU and the remainder of them\nare transferred between GPU and CPU during inference.\nTo reduce the communication overhead caused by weight transfers, authors in [19], propose a dual-track execution flow.\nIn cases where the selected expert is not available on GPU, their approach transfers the activations from GPU to CPU\ninstead of bringing the expert to the GPU. This method allows computations for that specific expert to be performed on\na multi-core CPU, resulting in higher overall throughput. However, this approach necessitates a powerful many-core\nCPU to offset the expert transfer overhead.\nEliseev et. al. [5] utilize a table for tracking the experts and using the least recently used (LRU) policy they replace the\nexperts on GPU. Due to residual connections from each transformer block to the next, the authors also heuristically\nfeed the same activation from previous layers to the gating network of subsequent layers to predict potential experts for\nspeculative prefetching.\nIn [9], authors preprocess a specific dataset and design a data structure (table) to capture access patterns of experts for\neach prompt. All distinct access patterns are stored in a library, and during inference, a number of the most similar\ntables are selected based on accesses to experts in earlier layers, to determine which experts to prefetch."}, {"title": "2.3 Model Compression and Quantization", "content": "Model compression and quantization are highly effective approaches for efficient deployment of LLMs. These\napproaches can be categorized into two major groups: quantization aware training (QAT) and post training quantization\n(PTQ). QAT approaches [20, 21] refine and retrain the quantized model iteratively to improve the accuracy and therefore"}, {"title": "3 Adaptive Inference Partitioner and Planner", "content": "The motivation behind this partitioning and planning system is to adapt to the requirements of a multi-tenant environment.\nIn these environments, resources are shared among users, and job management system determines the amount of\nresources (GPU utilization, PCIe link bandwidth, and GPU memory) available at each point. Since these constraints\nchange over time, an adaptive system is required to adjust the configuration of the inference system accordingly. Here,\nwe focus the scope of the design on the case where the amount of available GPU memory imposes constraints on the\nsystem.\nIn this design, since all the non-expert layers together constitute a small portion of the model size (only 5% for the\nMixtral-8x7B model) and have a significant effect on the output quality, we load them onto the GPU in 16-bit format.\nTherefore, to meet the memory constraint, the partitioning system offers two options to control the amount of allocated\nmemory with higher granularity: 1) partial expert quantization, which determines the number of quantized vs. 16-bit\nexperts, and 2) deciding the number of experts that reside on the GPU and the number of experts that are transferred\nduring inference.\nAs depicted in Figure 1, we assume that tasks arrive at the inference system with a batch of prompts and a preference\nfor either quality or throughput. Based on the task's preference, one of the options mentioned above receives higher\npriority. If the preference is for throughput, the system must bring as many experts as possible into the GPU memory\nto reduce the amount of data transfers caused by an expert miss. Therefore, if the available memory (denoted as\n$Mem_{GPU}$) exceeds the total size of non-expert layers (denoted as $Size_{N.E.}$) in 16-bit and experts in 4-bit (denoted as\n$NUM_E \\times Size_{E,4}$), some of the experts will be selected to stay in 16-bit format (denoted by $Num_{E,16}$ and calculated"}, {"title": null, "content": "in equation 1) to utilize the excess resources for better quality. On the other hand, when the available memory is less,\nall of the experts will be assigned to the 4-bit format, and the system will have to perform offloading.\n$NUM_{E, 16} = \\begin{cases}\n    \\lfloor (Mem_{GPU} - Size_{N.E.} -\nNUM_E \\times Size_{E,4}) / (3 \\times Size_{E,4}) \\rfloor & \\text{if } Mem_{GPU} > Size_{N.E.} + NUM_E \\times Size_{E,4}\\\\\n    0 & \\text{otherwise}\n\\end{cases}$                                                                                       (1)\nMeanwhile, for tasks with a quality constraint, a range for selecting the number of quantized experts is provided. The\nupper bound of this range achieves the best output quality by setting all the experts to perform in 16-bit, while the lower\nbound keeps all the experts quantized. Therefore, the number of 4-bit experts ($Nume,4$), along with the available GPU\nmemory, directs the partitioner to distribute the model across CPU and GPU memories. If the available GPU memory is\nless than the size of the non-expert layers and $Num_E,4$ 4-bit experts, only a subset of quantized experts will be stored\non the GPU, and the rest of the experts will be transferred during inference. On the other hand, with a higher amount of\navailable memory, more experts will be loaded onto the GPU, which does not change the output quality but can improve\nthe throughput.\nBased on the partitioning mentioned above, we consider a table that has two boolean attributes for each expert. The\nfirst attribute determines if the expert is quantized or not, and the second attribute determines the location of the expert\n(CPU or GPU). First, the quantization attribute is assigned to experts randomly. This approach ensures that the method\nis not dependent on a specific dataset or task since MoE models are trained to have uniform access frequency among all\nexperts. Next, the location attribute is assigned. Since an expert not being available on the GPU stalls the computations\nin the pipeline, we give higher priority to 4-bit experts and assign them to the GPU first to increase the hit rate of the\ninference process.\nFor inference, the model is partially loaded onto the GPU, and the state of each expert is tracked using the described\ntable. Additionally, a swap space is allocated to transfer experts from the CPU when an expert miss occurs. Since user\nconstraints can change over time, the system is designed to adapt with minimal downtime. The planner recalculates the\nparameters based on the new constraints and partially reconfigures the system instead of reloading the model with the\nnew configuration as a whole. These partial reconfigurations involve offloading or downloading experts and switching\nbetween quantized and 16-bit formats."}, {"title": "4 Evaluation", "content": "Here, we first evaluate the effect of quantized experts on the performance of the MoE model. As shown in Figure\n2, increasing the number of 4-bit experts generally leads to a decrease in the quality of the generated output (higher\nperplexity), although this trend is not strictly monotonic. In the case of the WikiText2 and C4 datasets, there are points"}, {"title": "4.1 Experimental Setup", "content": "We develop and deploy our adaptive serving approach using Pytorch [27] and Huggingface [28] libraries. Also, we use\nthe Bitsandbytes library [25] for quantization of experts. We conduct our experiments on a server which has an AMD\n16-Core MILAN CPU and a single 80GB NVIDIA A100 GPU with PCIe Gen4 interconnect .\nModel and Datasets. We evaluate our approach using a Mixtral 8x7B MoE model [10], which consists of 32 layers\nand 8 experts per layer. The total size of the non-expert layers for this model adds up to 3.16 GB. Each expert occupies\n336 MB of memory, and transferring each expert between the CPU and GPU takes 27.35 ms on the described testbed.\nTo assess the effect of partial expert quantization in our approach, we evaluate the performance of the MoE model using\nthree language modeling benchmarks. We examine the model on three popular text datasets: WikiText2 [29], PTB [30],\nand C4 [31]. For each dataset, we measure the perplexity of the model's generated output using 128 samples of 2048\ntokens, as perplexity is known as a robust and widely adopted indicator of model capabilities [24].\nBaselines. Since our adaptive serving approach provides a design space for choosing between model quality and\nthroughput, our comparison against baseline approaches focuses on these two metrics. For perplexity, we compare our\napproach's performance against models with homogeneous weight formats: 16-bit, 8-bit, and 4-bit [25]. Regarding the\nefficiency of token generation, we compare our approach's throughput (tokens per second) against Mixtral-Offloading\n[5] under varying amounts of available GPU memory. The throughput is measured using a mix of prompts from the\nmentioned datasets, with input and output lengths limited to 16 to balance the prefill and decode stages' latencies."}, {"title": "4.2 Results", "content": "Here, we first evaluate the effect of quantized experts on the performance of the MoE model. As shown in Figure\n2, increasing the number of 4-bit experts generally leads to a decrease in the quality of the generated output (higher\nperplexity), although this trend is not strictly monotonic. In the case of the WikiText2 and C4 datasets, there are points"}, {"title": "5 Conclusion", "content": "In this paper, we propose an adaptive serving approach for efficient deployment of large MoE models in dynamic\nsingle-GPU constrained settings. Our approach enhances performance by reducing the overall memory footprint of\nMoE models through partial expert quantization. By employing partial expert quantization, our approach offers a"}]}