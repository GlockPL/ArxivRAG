{"title": "LOTUS: Enabling Semantic Queries with LLMs Over Tables of Unstructured and Structured Data", "authors": ["Liana Patel", "Carlos Guestrin", "Siddharth Jha", "Matei Zaharia"], "abstract": "The semantic capabilities of language models (LMs) have the potential to enable rich analytics and reasoning over vast knowledge corpora. Unfortunately, existing systems lack high-level abstractions to perform semantic queries at scale. We introduce semantic operators, a declarative programming interface that extends the relational model with composable AI-based operations for semantic queries over datasets (e.g., sorting or aggregating records using natural language criteria). Each operator can be implemented and optimized in multiple ways, opening a rich space for execution plans similar to relational operators. We implement our operators and several optimizations for them in LOTUS, an open source query engine with a Pandas-like API.\nWe demonstrate LOTUS' effectiveness across a series of real applications, including fact-checking, extreme multi-label classification, and search. We find that LOTUS' programming model is highly expressive, capturing state-of-the-art query pipelines with low development overhead across these diverse applications. Specifically, on the FEVER dataset for fact-checking application, LOTUS' programs can reproduce FacTool, recent state-of-the-art pipeline, in few lines of code, and implement a new pipeline with a simple change of operators that improves accuracy by 9.5%, while offering 7 - 34\u00d7 lower execution time. In the extreme multi-label classification task on the BioDEX dataset, LOTUS reproduces state-of-the art result quality with its join operator, while providing an efficient algorithm that runs 800\u00d7 faster than a naive join. In the search and ranking application, LOTUS allows a simple composition of operators to achieve 5.9 \u2013 49.4% higher nDCG@10 than the vanilla retriever and re-ranker, while also providing query efficiency, with 1.67 \u2013 10\u00d7 lower execution time than LM-based ranking methods used by prior works. LOTUS is publicly available at https://github.com/stanford-futuredata/lotus.", "sections": [{"title": "INTRODUCTION", "content": "The powerful semantic capabilities of modern language models (LMs) create exciting opportunities for building AI systems that reason over vast knowledge corpora. Many applications require complex reasoning over large amounts of data, including both unstructured and structured data. For example a researcher reviewing recent ArXiv [2] preprints may want to quickly obtain a summary of relevant papers from the past week, or find the papers that report the best performance for a particular task and dataset. Similarly, a medical professional may automatically extract biomedical characteristics and candidate diagnoses from many patient reports [32]. Likewise organizations may wish to automatically digest lengthy transcripts from internal meeting transcripts and chat histories to validate hypotheses about their business needs and productivity [5].\nEach of these tasks require a form of bulk semantic processing, where the Al system must process large amounts of data in often complex query patterns to perform the reasoning task at hand. Supporting the full generality of these applications with efficient and easily programmable query systems would have transformative impact. This prospect, however, raises two important and challenging questions: first, how should developers express semantic queries, and secondly, how should we design the underlying query system to achieve high efficiency and accuracy."}, {"title": "THE LOTUS PROGRAMMING MODEL", "content": "We now introduce the LOTUS programming model and show its expressive power in allowing developers to declaratively specify AI-based query pipelines that bulk process large datasets of structured and unstructured data. LOTUS extends the relational model with semantic operators, which we show in Table 1. These operators can be easily composed together with standard relational operators to build powerful programs that are transparently optimized (Section 3). In this section, we describe each semantic operator and our API for them in LOTUS, grounding each in concrete examples.\nWhile our current API implementation extends Pandas [12], LOTUS' semantic operators could be used with other existing relational query languages and APIs, such as SQL."}, {"title": "Datatypes", "content": "LOTUS' data model consists of tables with structured and unstructured text fields, and our current implementation extends Pandas [12]. Figure 2 shows an example LOTUS program that loads data about ArXiv papers and performs a summarization task, which finds relevant papers, filters according to whether the paper claims to outperform a specific baseline, and then summarizes the remaining papers into a single digest. We briefly describe two core components of LOTUS programming model: its data model (Section 2.1.1), and its parameterized natural language expressions (Section 2.1.2) for specifying semantic operations over the data."}, {"title": "Data Model", "content": "LOTUS is designed to seamlessly extend the relational model. Each row in the table, represents a logical entity. Table columns may contain either structured data or unstructured fields with free-form natural-language text. LOTUS' semantic-relational operators can take both of these data-types as inputs. Figure 3 shows an example table, where each rows represents an ArXiv paper document, with fields for the paper's title, ArXiv URL, abstract, ArXiv domain categories, and the publication date. These columns are then passed as parameters to semantic-relational operators, such as sem_search, sem_filter, sem_agg, as shown in Figure 2.\nAdditionally, LOTUS supports semantic similarity indices over natural-language text columns to provide optimized semantic query processing. These indices create embeddings over each document in the column to capture semantic similarity using embedding distance metrics. Semantic indexes can be created off-line and then loaded using sem_index and load_sem_index. Figure 2 provides an example program, where upon reading the ArXiv papers data from a CSV file, the programmer loads a semantic index over the abstract column. The program then repeatedly uses the semantically-indexed column in subsequent LOTUS operations, involving semantic search, filtering and aggregation."}, {"title": "Parameterized Natural Language Expressions (langex)", "content": "A core principle of LOTUS is to provide users with a declarative interface that separates the user-specified, logical query plan from its underlying implementation. As such, users program with LOTUS' semantic operators by writing parameterized natural language expressions (langex\u00b9), rather than directly prompting an underlying LM. Figure 2 shows an example of this, where the programmer provides a langex as the parameter to the sem_filter (line 18) and sem_agg (line 19) operations. Programmers write each langex in natural language text, parameterized by one or more data columns, which are indicated in the double brackets within the formatted string. The function of these expressions varies according to the semantic operator used and may represent a predicate, aggregation, comparator function, or projection in natural language. For instance, as shown in Figure 2, the langex signature of sem_filter."}, {"title": "Semantic Operators", "content": "We now overview each semantic operator and their corresponding LOTUS API. Table 1 provides a concise summary of each operator.\nSem_filter returns the subset of rows that pass the filtering condition, specified by the user's langex. As Figure 4 shows, the langex signature provides a semantic predicate over one or more table columns and can be answered by a binary \"True\" or \"False\" answer.\nSem_topk ranks a set of rows according to the user-defined criteria, and returns the $K$ rows that best match the ranking criteria. The signature of the langex provides a general ranking criteria according to one or more columns. The underlying system can use this langex to impose a ranking over any subset of rows, according to the chosen implementation. As Figure 5 shows, the programmer uses the langex to specify arbitrary reasoning-based ranking criteria, such as ranking paper abstracts by the most outrageous claim made.\nProgrammers can also optionally specify a group-by parameter to indicate a subset of columns to group over during ranking, as shown in 6. The groupings are defined using standard equality matches over the group-by columns. To use groupings according to semantic similarity, users can also perform a sem_cluster_by and pass the resulting cluster_id column as the group-by column parameter.\nSem_join combines data from two tables, evaluating the user's predicate to return the set of rows from the left and right table that pass. As Figure 7 shows, users specify the right join table, a langex, and optionally, the join key of the left and right tables. Here the langex contains two or more columns, and provides a predicate over the left and right tables. By default, the operator performs an inner join over the two tables, and can alternatively perform left, right or outer joins, if specified.\nSem_sim_join provides a variant of the semantic join, such that rows are matched according to their semantic similarity, rather than an arbitrary natural-language predicate. Akin to an equi-join in standard relational algebra, the semantic similarity join is a specialized semantic join. Figure 8 provides an example, where one table contains papers with an indexed column of abstracts, and the other table contains a list of research interests. The user specifies the left and right table join keys, and a parameter $K$. The left join key may or may not be indexed, whereas the the right join key must be a semantically indexed column with its index loaded. The operator performs a left join such that for each row in the left table, the output table will contain $K$ matching rows from the right table with the highest similarity scores. The programmer can also optionally specify a return column containing the semantic similarity scores of each joined row.\nSem_agg performs an aggregation over all rows of the table. As Figure 9 shows, the langex signature provides a commutative and associative aggregation function, which can be applied over any subset of rows to produce an intermediate results. Semantic aggregations can be useful for many tasks involving many-to-one reduction patterns, such as summarization or question-answering over multiple documents. Similar to sem_topk, users can also specify a group-by parameter to use. We also provide additional flexibility, by allowing the programmer to compose semantic aggregations with sem_partition_by, which provides finer-granularity control over how documents are grouped over in each LM invocation. We describe this further below, and show its use in Section 3 for overriding the default commutativity and associativity assumptions for some tasks.\nSem_partition_by creates a row partitioning over the table, which will be used by sucessive calls to sem_agg to decide which rows to group together within each LM invocation. We find in Section 3 that this can non-trivially affect the result quality of aggregation tasks, like summarization. As Figure 10 shows, sem_partition_by takes a function, which outputs a group-id for each row. LOTUS natively supports a semantic cluster function, which takes the number of clusters to create and a column, but users can also specify arbitrary partition functions. The group-ids output by the partition function indicate to the system which rows should be grouped together, in a best-effort manner, during LM invocations. The system will aggregate over documents within each group, before merging intermediate results across groups.\nSem_map performs a natural language projection over an existing column and outputs a new column in the table. As shown in Figure 11, the user's langex specifies what to project. This operator provides general functionality and simulates logically row-wise data-flow similar to prior works that use LLM-UDFs in relational languages [60, 61]. These semantic projections are broadly useful for a variety of tasks, such as row-wise summarization, classification, and entity-extraction.\nSem_extract provides similar functionality to sem_map but provides the answers by returning a list of sub-strings from the source text. This functionality is useful for applications, such as entity extraction or fact-checking, where snippet-finding or verified quotes may be preferable to synthesized LLM-based answers. As Figure 12, the user's langex signature specifies a projection, similar to sem_map.\nSem_index generates a semantic similarity index over the specified data column, as shown in Figure 13. To generate the semantic index, users first use the settings module to declare a retrieval model, which will be used for generating semantic embeddings and indexing the column data. The sem_index operator takes a column and a local directory path, to which the generated semantic index will be stored. As the figure shows, users can separately index multiple columns of the table.\nLoad_sem_index re-loads the stored semantic index upon reading the table data from disk, as shown in Figure 14. The user specifies the column corresponding to the semantic similarity index and the directory where the index was saved.\nSem_search performs a top-k semantic similarity search over a semantically-indexed column, as Figure 14 shows. The user specifies the table column to search over, a query in natural language, a target K of the number of results to return, and optionally indicates whether to return similarity scores as a column in the returned table.\nLOTUS also exposes advanced relevance-based re-ranking functionality for search. Users can specify a re-ranker model, as shown in Figure 15, and then set the n_rerank parameter during the semantic search. The semantic search in this case will first find the top-K most relevant documents according to the retriever model, and then re-rank the top-K found documents to return the top n_rerank ones using the re-ranker model.\nSem_cluster_by assigns a group to each row of the table according to semantic similarity. This operator is akin to a relational group_by, but uses semantic similarity to determine the groups, rather than equality over the specified column. As Figure 16 shows, the user specifies an indexed column to cluster, and the number of clusters to create. The returned table augments the input table with an additional column that specifies the assigned cluster_id of each row. Users can optionally also return the similarity scores between each row and its cluster centroid in an additional column of the returned table."}, {"title": "IMPLEMENTING AND OPTIMIZING SEMANTIC OPERATORS", "content": "Semantic operators create a rich design space, allowing for a diverse set of algorithmic decisions and optimizations which have significant consequences on system efficiency and accuracy. We identify multiple possible algorithms for these operators, noting their performance implications. LOTUS' query engine aims to automatically exploit this rich design space to provide an optimized implementation of each individual operator and composite pipelines.\nSpecifically, LOTUS employs a series of novel algorithms designed to leverage the semantics of each operator in order to maximize parallel batched-inference opportunities, use model cascades [28, 46, 48, 78, 83] with a lightweight scoring function unique to our setting, leverage the semantic similarity index, and perform algorithmic approximations for expensive operations (e.g. joins). We see in Section 4 that these decisions can significantly improve efficiency while maintaining high quality results for LOTUS' query pipelines.\nWhile our initial investigation studies several operator-specific design decisions unique to the semantic bulk processing setting, we envision a rich set of additional optimization opportunities. Several prior works demonstrate performance gains in both logical query plan optimizations (e.g. operator re-ordering [57, 58, 61, 64]) and other general LM-approximation techniques (e.g. code synthesis [24, 58] and prompt adaptation [28]), which we find to be promising opportunities for LOTUS' implementation, left to future work.\nLOTUS' current implementation extends the Pandas API [12]. We leverage VLLM [53] to perform efficient batched inference, and we use FAISS [43] for efficient vector similarity search."}, {"title": "sem_filter", "content": "LOTUS' semantic filter runs batched LLM calls over a set of rows, prompting the model to output a boolean value for each row to indicate whether the record passes the user's natural language predicate. Figure 18 shows an example of the operator-specific filtering instruction, which is managed transparently by the system."}, {"title": "Optimizations", "content": "We leverage model cascades, using an efficient scoring function unique to our setting, to further optimize the filter function. Many prior works have studied model cascades for ML tasks [28, 46, 48, 78, 83], leveraging a relatively cheap and inaccurate proxy model, along with an accurate but expensive oracle model in order to reduce the inference cost by routing easy queries to the small model, and resorting to the large model where necessary. Typically, a scoring function provides confidence scores over the proxy model's outputs, and allows the system to decide when to route examples to the larger model. Applying this paradigm to LMs introduces several challenges, including generating a reliable scoring function. Several works study this problem, suggesting to fine-tune a smaller LM to score each question along with the answer produced by the weak LM [28], or invoking multiple sampling paths from the small LM and evaluating self-consistency to build a confidence score [83]. While these methods provide general-purpose scoring functions, LOTUS' scoring function leverages the operator semantics to provide a lighter-weight scoring function, which avoids the execution time penalty of scoring with an additional model or re-sampling from the proxy-model.\nSpecifically, LOTUS' cascade scoring function leverages the binary-label output of each LM invocation. The system first batch-processes each record using the cheaper LM, then computes confidence scores by exponentiation the log-probabilities for the output LLM tokens corresponding to the True or False answer. For records with confidence scores below the user-defined threshold, the system then passes these records in batch to the larger LM. As we show, in Section 4, leveraging model cascades in this way for semantic filtering can significantly improves the system's efficiency while maintaining high quality results for some tasks."}, {"title": "sem_topk", "content": "Performing a semantic top-k ranking requires logically reasoning across rows, entailing joint reasoning over often large amounts of data. Implementing an efficient algorithm introduces many important design decisions pertaining to managing LM context length limits, grouping documents within each LM context window to achieve high quality LM-based comparisons over subsets of the data, and choosing an efficient top-k ranking algorithm to combine LM-based comparisons. These decisions have notable performance implications on efficiency and result quality, as we show in Section 4. While prior works have studied LM-based passage re-ranking [30, 33, 56, 65, 69-72, 74] and ranking with noisy comparisons [26, 73] with the goal of achieving high quality results in a modest number of total LM calls or comparisons, LOTUS' implementations provides a generalized ranking algorithms for arbitrary natural language expressions and aims to produce both high quality results and low query execution time. We leverage several algorithmic design decisions and optimizations to achieve this."}, {"title": "Algorithms", "content": "Prior LM-based sorting and ranking works suggest several methods for performing LM-based comparisons. Specifically, prior works primarily study three classes of methods: point-wise ranking methods [30, 33, 56, 72, 82], list-wise ranking methods [65, 69, 70, 74], and pair-wise ranking methods [71, 73]. Point-wise methods output a relevance score for each document independently; while these methods are fully parallelizable across rows, prior work [30] shows they provide poor accuracy due to difficulty in calibrating scores across prompts. We note that these methods can be implemented in LOTUS with a sem_map, but are not the focus of our semantic top-k operator due to their quality limitations. By contrast, list-wise methods feed a partial list of 10 to 20 documents to the LM and prompt it to output a ranking over these, which can be aggregated using a sliding window approach. Unfortunately, prior works shows these methods are often prone to prediction failures [74] and sensitive to document ordering in the prompt [71]. Pairwise-prompting methods instead offer a simple and effective approach that feeds a single pair of documents to the LM in each invocation, prompting the model to perform a comparison and output a binary label. This method has been shown to be an effective base unit for ranking with relatively high robustness to input order sensitivity [71].\nTo aggregate pairwise comparisons, we consider several ranking algorithms, including a quadratic sorting algorithm, a heap-based top-k algorithm and a quick-select-based top-k ranking algorithm. The quadratic ranking algorithm obtains a comparison between each pair of input documents, and uses a win rate to determine a ranking over all elements before selecting the k best. In contrast, the heap-based algorithm maintains a heap of size k, and makes a linear pass over the data, updating the heap when more promising elements are found. Each time a new element is inserted or removed from the heap, the algorithm performs a series of sequential LM comparisons to update the data structure. Lastly the quick-select based algorithm proceeds in successive rounds, each time choosing a pivot, and comparing all other remaining elements in the document set to the pivot item to determine the rank of the pivot. Because each round is fully parallelizable, we perform these LM-based comparisons efficiently in batch before recursing in the next round."}, {"title": "Discussion", "content": "As the core LM-based computation unit over subsets of the data, LOTUS' current implementation uses pair-wise rankings, following recent prior work [71], which demonstrates its effectiveness. We also find this choice useful for implementing further optimizations, such as model cascades, described below. Figure 19 shows an example task-instruction prompt, which we use to implement the LM-based comparison by instructing the model to select between two documents according to the user-defined sorting criteria.\nAdditionally, for aggregating comparisons, LOTUS' current implementation leverages the quick-select-based [39] top-k ranking algorithm, by default. Prior work [71] studies several alternative algorithms, including the heap-based ranking implementation and the quadratic sorting algorithm. We study these alternatives in Section 4, and find that the quick-select-based algorithm offers high accuracy while also offering an efficient implementation with at least an order magnitude fewer total calls then the quadratic sorting algorithm and more opportunities for batched calls leading to lower execution time compared to a heap-based implementation. We believe future work may leverage these multiple algorithms to allow the user to declaratively trade-off between performance metrics, like accuracy, execution time, and cost."}, {"title": "Optimizations", "content": "LOTUS' top-k algorithm is amenable to several optimizations. First, the top-k algorithm's pair-wise comparisons each invoke the LM to output a binary label, which is amenable to model cascades. To implement this, we use a simple an efficient scoring procedure based on log-probabilities of the generated tokens. We describe this procedure above for sem_filter (Section 3.1.1) and apply an equivalent procedure here. In Section 4, we demonstrate this optimization allows programmers to leverage small, cheap models in conjunction with large, expensive models to obtain high accuracy results at reduced cost.\nAdditionally, LOTUS can leverage the semantic index to optimize pivot selection for some queries, rather than resorting to random pivot selection. This optimization is useful when there exists correlation between the rankings imposed by the user's arbitrary sorting criteria and the rankings imposed by semantic similarity scores. In this case, LOTUS can sort the document set based on embedding distances to the user's query, and select the (k + \u20ac)-th item, rather than a random item, as the first pivot. This can reduce the number of LM comparisons required by subsequent rounds in the quick-select algorithm, leading to higher query efficiency at no accuracy loss. We believe fruitful future work will automatically estimate correlation between semantic similarity scores and the user-defined ranking criteria to transparently apply this optimization."}, {"title": "sem_join", "content": "Performing the semantic join involves evaluating the user's natural language predicate on each pair of rows in the left and right table. Implemented naively, this can be prohibitively expensive, incurring a quadratic number of LM calls with respect to to the size of the left and right join tables. LOTUS thus implements several algorithms suitable for a variety of settings, highlighting the rich design space, which we plan to further optimize over in future work. Here we describe the design patterns currently implemented, which we find to be useful for real-world tasks that we evaluate in Section 4."}, {"title": "Algorithms", "content": "The first join algorithm implements the nested-loop join pattern with efficient LM batch processing to maximize GPU utilization rather than naively looping over each pair of rows and invoking the LM. This yields an O(N1 N2) LM call complexity, where N1 and 2 are the table sizes of the left and right join tables respectively. As Figure 18 shows, each LM call instructs the model to output a boolean value after evaluating the user's natural language predicate. This quadratic join algorithm is suitable for small join tables.\nAlternatively, we implement a map-search-filter join pattern which can be used to approximate the quadratic join algorithm, while incurring fewer LM calls. This algorithm first performs a semantic mapping over the left join key to the domain of the right join key. In the example provided by Figure 20, which joins paper abstracts with the datasets they use, the map step would invoke the LM over each abstract, instructing the model to output the dataset used. This step is un-grounded in the sense that the LM is not given knowledge of right join table, but may optionally leverage user-specified demonstrations. Following the semantic projection, the algorithm then leverages the semantic index and performs a similarity search over the right join table to find candidates likely to pass the predicate. In the example, this search would retrieve a list of datasets from the right table for each paper, using semantic similarity of the datasets to the LM projection output in the first step. The semantic similarity search is parameterized by K, which is automatically set based on the user's specified LM call budget. Lastly, the algorithm performs a filter over the candidate pairs and outputs the joined table of tuples pairs that pass. This join algorithm partially mimics the information flow of prior work [31] and abstracts away intermediate steps by instead allowing users to specify an LM call budget for the join. The LM call complexity of this algorithm is O(N\u2081. K).\nLastly, we implement a search-filter join pattern. This algorithm first performs a semantic similarity join using a similarity index on the right join key, with the left join key embedded on the fly. In the example provided by Figure 20, this step would retrieve K semantically similar datasets from the right table for each paper abstract in the left table. The batched semantic-similarity search sets the search parameter K according to the user's specified LM call budget, similar to the map-search-filter algorithm. The search results provide candidate tuple pairs, which are then evaluated in batch using the filter operation to gather the final set of rows that pass the user's language predicate. This algorithm obtains an LM call complexity of O(N\u2081 K)."}, {"title": "Discussion", "content": "These join patterns offer performance tradeoffs suitable for different settings. The nested-loop algorithm offers an efficient solution when the join tables are sufficiently small, whereas the map-search-filter and search-filter patterns are suitable approximations that can apply efficiently over large tables, due to their linear LM call complexity in table size. The expected result quality of either approximation likely varies depending on the presence of predicate clustering and correlation, as defined by prior work [68] between the user's predicate and the semantic embeddings. The search-filter pattern is likely to produce high quality results under positive query correlation, where entities that are semantically similar are also likely to pass the predicate. On the other hand, the map-search-filter pattern is likely to produce higher quality results when the predicate-embedding correlation is low using the original left join key's embeddings as queries, but can be increased using the LM-projection over the left join key."}, {"title": "sem_agg", "content": "Performing semantic aggregations are inherently challenging because, similar to the semantic top-k, it requires logically reasoning across rows. Thus, the operator's implementation must efficiently orchestrate the LM over large amounts of data, while managing long context inputs, which may degrade result quality [59] or overflow the underlying model's context length. LOTUS aims to abstract away such low-level details from the user and provides an efficient implementation, designed to support high quality results and provide the programmer with flexibility."}, {"title": "Algorithms", "content": "LOTUS' implementation builds on and generalizes the LM-based summarization pattern studied by prior research works [19, 27, 81] and deployed systems [8, 14]. These implementations primarily leverage one of two aggregation patterns: either a fold pattern, which produces a sequential, linear pass over the data while iteratively updating an accumulated partial answer, or a hierarchical reduce pattern, which recursively aggregates the input data to produce partial answers until a single answer remains."}, {"title": "Discussion", "content": "By default, LOTUS' aggregation implements the hierarchical pattern, which allows for greater parallelism during query processing and has been shown to produce higher quality results for tasks like summarization in prior work [27]. However, LOTUS' partition function can be used to override the default functionality and generalizes the information flow of prior implementation by allowing users to specify arbitrary groupings over the data. Each unique group specified by the partition function will be aggregated with as few LM invocations as possible according to the model context length, before being merged with other groups. This allows the user to perform a fold pattern, as well as arbitrary user-defined patterns."}, {"title": "Optimizations", "content": "Preliminary results show that achieving high-quality semantic-aggregations is non-trivial and sensitive to the document's input ordering and grouping. We see qualitative evidence of this in a summarization task of 50 ArXiv paper abstracts, which we show in Figure 21. We contrast the summarization results of a naive semantic aggregation (Figure 22) performed over the paper abstracts with the semantic aggregation performed using a partitioning function based on semantic similarity (Figure 23). The semantic aggregation using the partitioner first clusters documents based on semantic similarity, then aggregates each cluster before performing aggregations across clusters. As the figures show the two methods result in significantly different summaries. While the latter demonstrates qualitatively more cohesion and effectively abstracts general themes across papers, the former omits details and tends to list low-level details from individual papers rather than capturing higher-level themes. We leave a quantitative study of this to future work and believe that semantic aggregations create a rich design space for optimization."}, {"title": "sem_map & sem_extract", "content": "Both LOTUS' semantic map and extract run batched LM calls over a set of rows, prompting the LM with the user's arbitrary natural-language expression to generate a new column. Both functions can be fully parallelized over rows, and LOTUS implements efficient batched inference with VLLM [53]. To implement sem_extract, LOTUS prompts the model to answer the user's langex with direct quotes, and the system then verifies that these snippets returned by the LM match the reference text."}, {"title": "Semantic Indexing", "content": "LOTUS supports several different algorithms for its semantic index. To generate the semantic similarity index, LOTUS' sim_index operator first batch processes the user-specified column to generate semantic embeddings with the configured retriever model, then constructs a vector index over the semantic embeddings. The current implementation uses FAISS' flat index by default and writes the index locally to disk. User's can specify the local file path to store the index, and optionally specify alternative vector indices, such as hierarchical navigable small worlds (HNSW) [66], inverted indices (IVF) [25, 35, 43, 44], and locality sensitive hash indices (LSH) [21-23, 36, 37, 40, 42, 55, 62, 63, 67, 75, 88]. In the future, we envision additionally supporting a wider variety of embeddings stores and indices [3, 10, 15, 17, 68, 79].\nLOTUS' semantic search, similarity join and cluster operations all leverage the similarity index in their implementation. sem_search first embeds the user's query string using the configured retriever model, then performs an efficient top-k search using the loaded FAISS index over the search column. sem_sim_join similarly performs a top-k search using the loaded FAISS index over the right key. Here the right-key may be a column of natural-language text, which LOTUS will embed using the retriever model on-the-fly, or an indexed column, for which LOTUS will load the previously-generated embeddings. The right-key embeddings then serve as the queries to perform batched search over the search column, given by the left key. Lastly, LOTUS implements sem_cluster_by using FAISS optimized kmeans library to cluster the user-specified column. Here, the column must be previously indexed, and LOTUS uses the generated embeddings to perform the clustering with the user-specified number of cluster centroids."}, {"title": "EVALUATION", "content": "We now evaluate LOTUS' programmability and efficiency through three diverse applications: fact-checking (Section 4.1), extreme multi-label classification (Section 4.2), and search and ranking (Section 4.3). For each of these applications, we see that state-of-the-art quality results are achievable with low development overhead, using LOTUS programs with a few lines of code and a few or even one semantic operator. In addition, our results demonstrate interesting implementation and optimization choices introduced by semantic operators. Specifically, we find that:\n\u2022 On the FEVER dataset [77] for fact-checking, LOTUS programs can reproduce FacTool [29], a recent state-of-the-art pipeline, in few lines of code, and implement a new pipeline with a simple change of operators that improves accuracy by 9.5%, while offering 7 - 34\u00d7 lower execution time.\n\u2022 On the BioDEX dataset [32] for the extreme multi-label classification task, LOTUS reproduces state-of-the art result quality [31] with it's join operator, while providing an efficient algorithm that runs up to 800x faster than a naive join, demonstrating the power of LOTUS' declarative interface.\n\u2022 On the SciFact dataset [76] and two newly-constructed paper datasets for the search and ranking application, LOTUS's semantic top-k operator achieves 5.9 - 49.4% higher nDCG@10 than a vanilla retriever and re-ranker, while also running 1.67 - 10x faster than alternative LM-based ranking methods used by prior works [71].\nUnless otherwise stated, we run our local model experiments with 4 A100 GPUs using Llama 3 models [7], with a batch size of 64 running on vLLM [53]. For our experiments that use OpenAI's GPT models [11], we run with 64-way thread parallelism."}, {"title": "Application: Fact-Checking", "content": "We evaluate on FEVER [77], a claim verification dataset. We use the development dataset, which contains about 38,000 total claims, of which we sample 500 for our evaluation. Each claim is labeled with one of three labels, \"Supported\", \"Refuted\", or \"NotEnoughInfo\", and the task is to correctly determine the label of each claim, leveraging evidence from a corpus of 5.5 million Wikipedia articles. We merge the latter two labels in to a single class, \"Not Supported\", following prior work [29] for our evaluation."}, {"title": "Baselines", "content": "FacTool [29] is a recent research work that proposes a multi-step pipeline for fact-checking involving, claim extraction, query generation, tool querying, evidence collection, and verification. We use FactTool's open source codebase [6] to measure its performance. FactTool's pipeline, by default, performs retrieval with a Google Search API [4]. We evaluate the pipeline with both the default retrieval API, and alternatively test with a ColBERT [52] index over the document corpus to perform retrieval. We find that the results are similar, and we report the results using ColBERT for retrieval to hold the retriever model constant with the implemented LOTUS programs."}, {"title": "LOTUS Programs", "content": "We compose several intuitive LOTUS programs, each in less than 50 lines of code. For each one, we use ColBERT as the retriever model for creating the semantic index, and we use Llama 70B as the primary LM, and Llama 8B and TinyLlama for cascade optimizations.\nFirst, we compose a pipeline designed to directly re-implement FacTool's information flow in LOTUS. Figure 24 shows thepseudocode for the LOTUS-FacTool pipeline. The two tables are shown by wiki_df, which stores the Wikipedia articles, and claim_df, which stores the claims from the FEVER dataset. After loading the semantic index to wiki_df, the pipeline first performs a semantic map over each claim to generate two search queries, which are then used to perform a semantic similarity join over the corpus of Wikipedia articles. The program then concatenates the context retrieved for each claim, and performs a sem_map to output whether the claim is true or false, along with a revised claim if the claim is false. We use the same prompts found in FacTool [6], which include 3 demonstrations for generating search queries in the first sem_map, and chain-of-thought prompting in the second sem_map.\nThe next program, LOTUS-fact-filter, makes a simple, single-operator modification to the LOTUS-FacTool program, replacing the semantic map at the end of the query pipeline, with LOTUS' semantic filter operation. We use 3 demonstrations for the filter. Figure 25 shows the pseudocode for this program. We evaluate this program with and without model cascades for the semantic filter operation.\nLastly, we compose an alternative pipeline, LOTUS-fact-join. As the pseudocode shows in Figure 26, the pipeline first performs a semantic map over each claim to obtain a set of sub-claims. From this, we create the claimed_facts_df, which separates each sub-claim into a different row. Next, the pipeline uses these sub-claims to perform a semantic similarity join over the Wikipedia corpus, then performs a semantic map over each retrieved article to generate the important facts in each one. Lastly, the program performs a semantic join between the sub-claims and the facts described in the retrieved passages. If the returned table contains a supporting fact for each sub-claim, the claim is labeled as \"Supported\". For the sem_map and join operations, we use 3 demonstrations each."}, {"title": "Results", "content": "Table 2 demonstrates the powerful abstraction that LOTUS provides, allowing programmers to quickly write and test programs that compose simple operators to obtain state-of-the-art results. We report the accuracy of each benchmarked method, an estimate of lines of code (LoC), and execution time in seconds both with and without batching. We see that FacTool's implementation offers strong accuracy performance on the FEVER datasets, however the full repository required several hundred lines of code, highlighting the development burden of building these applications without abstractions for semantic-bulk processing. By contrast, each LOTUS program offers comparable or higher accuracy, in relatively few lines of code. We also note that FacTool implements its method without batching, whereas LOTUS, by default, leverages batched LM execution for efficiency. To provide an apples-to-apples comparison, we compare FacTool's un-batched implementation to the LOTUS programs both with and without batching.\nFirst, we see from the Table 2 that LOTUS-FacTool is able to reproduce the result quality and efficiency of the original method's implementation, with 6.5 points higher accuracy and 1.2\u00d7 lower execution time without batching. The LOTUS-FacTool implementation with batching further decreases execution time compared to the original FacTool implementation by 10x. We see that the next LOTUS pipeline simply changes a single operation, switching the sem_map to a sem_filter, and maintains similar result quality to LOTUS-FacTool while further reducing execution time by 1.72\u00d7 in the batched implementation.\nLeveraging LOTUS' filter operation allows the programmer to further optimize the program using model cascades, which increases accuracy by 3 points and increases the batched execution time by 3.27\u00d7 compared to LOTUS-FacTool. Figure 27 highlights the diverse performance trade-offs presented the model cascade optimizations used in LOTUS' filter sem_filter. The plot shows the accuracy and execution time of the LOTUS-fact-filter pipeline using a single model, either Llama 8b, Llama 70B, or TinyLlama [86], shown by the circles. We compare this to the performance attainable using a pair of models, Llama 8B and Llama 70B, or TinyLlama and Llama70B to implement the filter casacade. We generate multiple cascade points, shown by the stars, by varying the confidence threshold used. The plot shows that this filter optimization can substantially reduce execution time, and offer diverse accuracy trade-offs, compared to implementing the pipeline with the oracle model, Llama 70B, alone.\nReturning to Table 2, we find that the last LOTUS pipeline (search-map-join), reduces accuracy and increases execution time. Notably the performance trade-offs of different LOTUS programs are non-obvious, highlighting the need for programmable, declarative abstractions so programmers can quickly explore and iterate on their query pipelines. Each LOTUS program we described can be implemented easily in relatively few lines of code, and the best performing one offers 9.5% higher accuracy and 34\u00d7 lower execution time than FacTool's original, un-batched implementation."}, {"title": "Application: Extreme Multi-label Classification", "content": "We evaluate on the Biodex Dataset [32], which consists of a corpus of 65, 000 biomedical articles, and expert-created drug safety reports constructed from each article. The task is to correctly label the drug reactions experience by the patient in each medical article. We sample 250 patient articles for our evaluation. Notably, there are approximately 24,000 possible drug-reaction labels, making this task an extreme multi-label classification task. Due to the large number of possible labels, leveraging an LM to perform inference is difficult, and this setting has been studied in prior works [31]. We show below that this task can be efficiently modeled and programmed using LOTUS' semantic join."}, {"title": "Baselines", "content": "We consider a simple retrieval baseline which uses an E5Model [80] as the retriever and performs a semantic-similarity join between the patient articles and the reaction labels. We show the pseudocode for this program in Figure 28."}, {"title": "LOTUS Programs", "content": "The proposed LOTUS program performs a semantic join over the drug reaction labels and the medical articles, as shown in Figure 32. We perform the semantic join using at most 7 map demonstrations and set an LM call budget of 10,000. We use Llama-70b as the LM."}, {"title": "Results", "content": "Table 3 shows that the proposed LOTUS program makes meaningful traction, obtaining high quality results on this task, while maintaining query efficiency. The table reports the rank-precision@5 (RP@5), rank-precision@10 (RP@10), following prior work [31], as well as execution time in seconds and the number of LM calls required for each program. Since the nested-loop join pattern is prohibitively expensive to run, we show an estimate of the execution time assuming linear scaling in the number of batched calls.\nFirst we compare performance of the LOTUS join program implemented with the map-search-filter pattern to the baseline method, the semantic similarity join. As expected, the LOTUS program offers substantially higher quality results, with 2.27\u00d7 and 2.15\u00d7 higher RP@5 and RP@10 respectively compared to the retrieval-based similarity join. This highlights the effectiveness of leveraging LMs over the data for complex reasoning-based tasks. We informally compare these accuracy results to recent work [31] which composes a multi-step DSPy [51] program compiled using Llama-2-7b-chat as the student LM and GPT-3.5-turbo as the teacher LM to perform a semantic mapping, followed by a re-ranking step using GPT-4 turbo. D'Oosterlinck et al. report 24.73 RP@5 and 27.67 RP@10 for the compiled program, representing comparable result quality to LOTUS' program, although notably the LOTUS program was not compiled with a prompt optimization system.\nWe now consider several interesting performance trade-offs presented in the semantic join algorithm. We compare LOTUS map-search-filter join pattern to the naive nested-loop join algorithm and the search-filter join pattern shown in Table 3. We see that nested-loop join pattern, which involves a quadratic LM budget of over 6 million LM calls, is untenable and prohibitively costly. By contrast, the search-filter and map-search-filter pattern substantially reduce the LM call budget of the naive algorithm by 800\u00d7, using an approximation. While these two approximation patterns have similar efficiency, according to execution time and the number of LM calls, interestingly, they offer substantially different result quality. Specifically, the map-search-filter pattern offers 55% higher RP@5 and 38% higher RP@10 compared to the retrieve-filter pattern on this task. These results highlight the unique opportunities bulk-semantic processing pipelines present for designing new algorithms and optimizations."}, {"title": "Application: Search & Ranking", "content": "We evaluate LOTUS' performance on the search and ranking task using three datasets, including BEIR's SciFact test set [76], a widely used benchmark for retrieval and re-ranking, as well as two new benchmarks, CIFAR-bench, and HellaSwag-bench, which we generate to evaluate more complex, reasoning-based ranking criteria over the data. For each, we report nDCG@10, as well as the execution time (ET) in seconds.\nThe SciFact dataset consists of a set of scientific claims and a corpus of articles, where the task is to rank articles by relevance given each scientific claim. We sample 300 scientific facts for our evaluation, and report the average ranking execution time across these samples.\nWhile the SciFact dataset provides a sorting task based on a simple relevance criterion, our newly proposed benchmarks provide a more complex sorting criteria over a corpus of paper abstracts. Specifically, the ranking task is to find the papers that report the highest accuracy on CIFAR-10 and HellaSwag in CIFAR-bench and HellaSwag-bench respectively. To generate CIFAR-bench, we took 100 abstracts from the Papers with Code Dataset [13] that state performance on CIFAR-10 in the abstract, and we and manually labeled their accuracy to obtain the top-10. We then synthetically generated HellaSwag-bench by prompting Llama-70B to create 200 paper abstracts, each with a specified accuracy value, randomly sampled from 0-100%. This setup allows us to evaluate LOTUS' LM-based sorting algorithms on a task with objective ground truth. We note that an alternative approach to these tasks could efficiently leverage sem_map to extract accuracy values on abstracts from either dataset, then perform a structured sort. However, our evaluation focuses on assessing the semantic ranking capabilities of LOTUS' top-k algorithms, and we find this benchmark useful for understanding performance trade-offs, which may likely generalize to a wider set of reasoning-based sorting queries, such as \"which paper makes the most outrageous claim\", for which ground truth is less objective to evaluate. For CIFAR-bench and HellaSwag-bench we report results for n = 20 trials of the ranking task, at temperature t = 0.7, similar to prior works [51]"}, {"title": "Baselines", "content": "We consider two simple baselines. The first baseline performs semantic search, as shown in Figure 30, using the E5Model [80] for retrieval. The second baseline performs search with re-ranking, as the pseudocode shows in Figure 31, using the E5Model for retrieval and the MixedBread cross-encoder [18] for re-ranking."}, {"title": "LOTUS Programs", "content": "The proposed LOTUS program performs a semantic top-k over the documents, as shown in Figure 32, which shows example pseudocode for the CIFAR-bench dataset. The langex for the semantic top-k on SciFact sorts based on relevance to the given claim, while the langex for the CIFAR-bench and HellaSwag-bench datasets sort abstracts by accuracy performance on the respective datasets, as shown in the figure. We note that for the SciFact dataset, we perform a semantic search using the E5Model as the retriever to obtain 100 articles, before ranking them with the sem_topk. We report results using both Llama-70B and GPT-40 as the LM."}, {"title": "Results", "content": "Tables 4 and 5 demonstrate the effectiveness of LOTUS' semantic top-k operator for complex search tasks. In addition, Table 6 and Figure 33 highlight the rich implementation design space that this task presents. We walk through several note-worthy findings.\nWe first turn our attention to Table 4, which presents the results for each bench-marked method on the SciFact dataset, which uses a relevance-based sorting criterion. As expected, both semantic search programs with and without re-ranking present strong baselines. Specifically, the re-ranker model, which is a supervised model trained specifically for the task of relevance-based ranking, increases nDCG@10 by 3 percentage points, while trading off query efficiency compared to the semantic search baseline. Notably, the unsupervised LM-based LOTUS programs outperform the supervised re-ranker's result quality. The table shows LOTUS semantic top-k program with Llama-70B and GPT-40, which outperform the semantic search baseline by 6 and 9 percentage points respectively. The LOTUS programs with Llama-70B and GPT-40 also outperform the re-ranker by 3 and 5 points respectively, improving upon the quality of the supervised baseline. As expected, the improved result quality comes at a trade-off to query efficiency due to the cost of LM calls. Notably, LOTUS' versatility allows programmers to easily compose each of these query pipeline and trade-off result quality and efficiency depending on application-specific requirements.\nTurning our attention to Table 5, we study LOTUS' generality in supporting arbitrary language-based ranking criteria over the dataset. On the CIFAR-bench and HellaSwag-bench datasets, which use a complex sorting criteria, we see that both semantic search baselines with and without re-ranking provide poor result quality with consistently low nDCG@10. The LOTUS program, using a semantic top-k with Llama 70B, acheives significant accuracy gains, with 49.4 and 44.8 points higher nDCG@10 than the best performing baseline on CIFAR-bench and HellaSwag respectively. These accuracy gains reflect the powerful reasoning capabilities of LMs efficiently orchestrated over the data. As expected, these significant accuracy gains come at an increase to execution time.\nWe now analyze the efficiency of LOTUS semantic top-k implementation along with it's proposed optimizations. Table 6 compares several semantic top-k algorithms, namely a quick-select top-k algorithm, a quick-select top-k that leverages the similarity index for pivot selection, a heap-based top-k algorithm, and a quadratic sorting algorithm. First, we see that the quadratic algorithm, which performs an LM comparison between each pair of input documents, offers consistently high result quality across each dataset. However, this method is prohibitively expensive, requiring 16 - 30\u00d7 more LM calls and over 10\u00d7 higher execution time than the alternative implementations. The heap top-k and quick-select top-k methods offer comparable result quality, but with interesting trade-offs in query efficiency. Notably the quick-select top-k method offers 1.67-2.24\u00d7 lower execution time than the heap-based sorting method across all datasets, despite requiring more LM calls in some cases. This is because the quick-select top-k implementation allows for efficient batch-processing in each round of the algorithm, whereas the heap-based top-k incurs sequential LM calls during heap updates. For this reason, our current implementation leverages the quick-select top-k algorithm, although we envision future iterations may leverage multiple algorithms and allow the user to declaratively trade-off accuracy, query throughput, and cost.\nIn addition to providing an efficient top-k algorithm, the table also demonstrates the use of LOTUS' similarity index for optimizing top-k query performance. The quick-select top-k algorithm optimized with the semantic similarity index for pivot selection demonstrates 1.2\u00d7 lower execution time at no accuracy loss on SciFact, where the ranking criteria correlates likely with semantic similarity. On the other hand, for the CIFAR-bench and HellaSwag-bench datasets, where the ranking criteria does not correlate with semantic similarity, we see that the similarity index has no significant impact on the accuracy or efficiency top-k performance.\nLastly, we analyze the impact of LOTUS' cascade optimization applied to the quickselect top-k implementation. Figure 33 compares the nDCG and execution time of implementing the operator with a single model, either Llama 8B or Llama 70B, to the implementation that leverages these models together using model cascades. We vary the confidence threshold of the cascade to generate several points in the trade-off space. We find that the cascade optimization offers diverse performance trade-offs, which can outperform the single-model oracle baseline, which uses Llama-70B. For instance, one cascade along the Pareto-frontier improves accuracy of the Llama-70B baseline by 3%, while reducing execution time by 1.8x, demonstrating substantial opportunities for automatically optimizing LOTUS' semantic query pipelines."}, {"title": "RELATED WORK", "content": "Specialized LLM-based Relational Extensions. Several prior works extend relational languages with a set of logically row-wise LM-based operations to serve specialized tasks or applications. Palimpzest [58] presents a declarative approach to data cleaning and extract-transform-load (ETL) tasks. The authors propose to automatically optimize relational operators with LM calls, and implement two row-wise relational operators, a newly proposed convert operator, which can be transparently optimized to perform entity extraction using LLMs, and an AI-based filter operation, logically similar to LOTUS' sem_filter. The system also implements several query optimizations, such as operator re-ordering, model selection, and code synthesis to implement user queries, and proposes several others as future work.\nSUQL [61] presents a SQL extension to support conversational agents with knowledge grounding over structured and unstructured data. Specifically, the system extends SQL with two new logically row-wise operators, answer, which prompts an LM to answer the user question over each row, and summary, which prompts an LM to provide a summary to the user over each row. The system can provide automatic optimizations, such as predicate re-ordering using a lazy evaluation approach and proposes to use retrieval to optimize some answer queries.\nZenDB [57] and EVAPORATE [24] tackle the task of automatically ingesting and extracting semi-structured documents into structured tables that can be queried using standard relational operators and languages. ZenDB extracts structure using a semantic hierarchical tree index, which is integrated with a SQL query engine to support efficient query processing over the extracted attribute values. The systems implements several optimizations, including predicate reordering, push-down, and projection pull-up. Additionally, EVAPORATE performs efficient entity extraction from semi-structured data using LM-based code generation and weak supervision to ensemble candidate functions.\nLOTUS, in contrast to these prior works, defines a general-purpose programming model designed to capture broad-ranging applications with a core set of composable semantic operators, including both logically row-wise ones and more complex ones, such as joins, aggregation, ranking and search functions. In LOTUS' current implementation, users can use sem_map to perform entity extraction over unstructured text fields, although future work may provide native functionality for entity extraction by integrating systems or optimizations of prior work. Additionally, we believe that several optimizations leveraged in prior work, such as lazy evaluation, operator reordering, model selection, and code synthesis, are worthwhile future work for LOTUS' optimizer.\nLLM UDFs Recent research work [60] and existing analytical database vendors, such as Google BigQuery [9], Databricks [1] and AWS Redshift [16], alternatively offer LLM user-defined functions (UDFs) to the programmer. The LLM UDF programming model provides a lower-level interface, which is limited to logically row-wise LLM execution over the data, equivalent to LOTUS' sem_map. In contrast, LOTUS' programming model is declarative and provides a rich set of semantic operators, allowing the system to automatically orchestrate the LM to serve a variety of complex query patterns, including aggregations, ranking, and joins.\nLiu et al. [60] study how to optimize LLM UDF functions, demonstrating performance gains with a de-duplication method and a prefix-sharing maximization method that reorders rows and reformulates parameterized prompts to maximize key-value (KV) cache reuse during query execution. We believe these methods could be effective optimizations in future work at LOTUS' batched execution layer.\nLM Programming Frameworks Recent LM programming frameworks, such as LangChain [8], LlamaIndex [14], and DSPy [51], have gained significant popularity. These systems provide a set of abstractions for programming with LMs, including utilities for handling prompts and post-processsing LM outputs, and support for common use cases, such as RAG, chat-bots, and function-calling. DSPy focuses on abstracting LM pipelines as programs and provides automatic prompt optimization. In contrast to these systems, LOTUS' programming model is designed for tasks involving bulk processing data with LLMs. While some of these systems support batched calls with multi-threading, support for bulk-processing is sparsely supported and largely un-optimized.\nML-based Query Processing Many prior works study the use of machine learning (ML) in databases, but do not focus on LLMs, which present unique opportunities for system design and optimization. MADLib [38] extends SQL with efficient abstractions for supervised learning, unsupervised learning, and descriptive statistics. Prior works such as NoScope [46], TASTI [49], SUPG [47], BlazeIt [45] and probabilistic predicates [64] propose methods to optimize queries involving expensive ML models over large datasets, typically in video analytics. Some optimizations, such as model cascades and predicate re-ordering, which were useful in these works are likewise useful for optimizing LOTUS pipelines with language models. However, our setting with natural-language reasoning tasks has new operators with significantly different semantics, such as sem_topk and sem_agg, requiring a new programming model as well as new query execution algorithms and optimizations."}, {"title": "CONCLUSION", "content": "In this work, we proposed semantic operators to provide the first declarative and general-purpose interface to serve bulk-semantic processing. We implement these operators in the LOTUS system to seamlessly extend the relational model and allow programmers to easily compose powerful reasoning-based query pipelines over vast corpora of structured and unstructured data. Our results across a diverse set of applications, including fact-checking, extreme multi-label classification, and search, demonstrate the generality and effectiveness of LOTUS' programming model as well as the efficiency and optimization opportunities of LOTUS' query engine. For each task, we find that LOTUS programs capture high quality and state-of-the-art query pipelines with low development overhead, and that they can be automatically optimized to achieve higher performance than existing implementations."}]}