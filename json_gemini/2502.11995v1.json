{"title": "Presumed Cultural Identity: How Names Shape LLM Responses", "authors": ["Siddhesh Pawar", "Arnav Arora", "Lucie-Aim\u00e9e Kaffee", "Isabelle Augenstein"], "abstract": "Names are deeply tied to human identity. They can serve as markers of individuality, cultural heritage, and personal history. However, using names as a core indicator of identity can lead to over-simplification of complex identities. When interacting with LLMs, user names are an important point of information for personalisation. Names can enter chatbot conversations through direct user input (requested by chatbots), as part of task contexts such as CV reviews, or as built-in memory features that store user information for personalisation. We study biases associated with names by measuring cultural presumptions in the responses generated by LLMs when presented with common suggestion-seeking queries, which might involve making assumptions about the user. Our analyses demonstrate strong assumptions about cultural identity associated with names present in LLM generations across multiple cultures. Our work has implications for designing more nuanced personalisation systems that avoid reinforcing stereotypes while maintaining meaningful customisation.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are increasingly being integrated into personalized applications like virtual assistants, where providing helpful suggestions requires tailoring responses to individual users. To build this understanding, models have to undergo a process of implicit personalisation, i.e., changing the answer based on implicit assumptions about the user (Jin et al., 2024). Popular platforms offering virtual assistants also have features where they store 'memories' about the user (OpenAI, 2024b) or mimic the writing style (Anthropic, 2024) to tailor the response to a specific user.\nNames carry deep cultural and personal identity, playing a central role in human communication. Sociological research indicates that names are imbued with culturally loaded meanings that can trigger stereotypes and discriminatory responses\u2014evidence of which is seen in field experiments, where individuals with ethnically distinctive names receive fewer opportunities (Bertrand and Mullainathan, 2003; Fryer Jr and Levitt, 2004). However, names do not always equate to a singular cultural identity. People may have names that reflect heritage from one culture while having grown up in a completely different cultural context, such as in cases of immigration, diaspora communities, or multicultural families. In human interaction, there is usually a larger context or other cues that provide a signal to a speaker about the other person's identity. However, such cues may be missing when a user is interacting with an LLM, making the limited information available in the prompts and stored in memory very important. Indeed, in analyzing LLM memory traces, OpenAI (2024a) found that the most common single memory is: \u201cUser's name is <NAME>\u201d], and that users often explicitly mention their own name in their interactions with models. Therefore, names could serve as a rich signal for personalisation to the models. However, erroneous assumptions about a name's associated identity can lead to biased or misleading personalisation, reinforcing stereotypes."}, {"title": "2 Background", "content": "LLM personalisation The recent uptake of chat interfaces for LLM has led to attempts to personalise LLM interactions by tailoring model outputs to individual user preferences and contexts (Zhang et al., 2024). Recent studies have explored various approaches to enhance LLM personalisation, such as reducing redundancy and creating more personalized interactions by remembering user conversations (Magister et al., 2024; Salemi et al., 2023). However, personalisation can also lead to over-simplifying user identity and reproduce or amplify model bias. This problem has been observed across various technical fields, e.g. Greene and Shmueli (2019) discusses how personalisation often reduces individuals to feature vectors, neglecting the complex facets of personal identity and potentially reinforcing biases present in the data. However, in the context of LLMs research on personalisation has just started. Previous work found that when LLMs are assigned personas, they exhibit bias, perpetuating stereotypes (Gupta et al., 2024), even when those identities are implicit (Kantharuban et al., 2024; Jin et al., 2024). In our work, we examine these implicit biases through the lense of names, i.e. the output of models being influenced by the addition of names across cultures.\nBias in LLMs Names are deeply intertwined with personal and cultural identity (Watzlawik et al., 2016; Dion, 1983). Tajfel (2010)'s Social Identity Theory posits that individuals derive a significant part of their self-concept from their membership in social groups, with names acting as identifiers of these affiliations. However, there can be a disconnect between one's name and cultural background, leading to complex implications for one's sense of belonging (DeAza, 2019). Names not always being a simple indicator of identity is exemplified by name assimilation, the adoption of common Western names by minority ethnic groups and immigrants (Carneiro et al., 2020).\nAs names can lead to simplified assumptions about user identity, names have been used across a variety of studies investigating bias in LLMs. For example, Haim et al. (2024) prompt LLMs with scenarios involving individuals with names associated with various racial and gender groups in the American cultural context. Their findings reveal that the models systematically disadvantage names commonly linked to racial minorities and women, with names associated with Black women receiving the least favorable outcomes. Names have been used as a proxy for gender Kotek et al. (2023); Wan et al. (2023) and ethnic identity bias (Nadeem et al., 2021), and cultural personas (Kamruzzaman and Kim, 2024). There has been a recent increase in work on cultural biases in LLMs (Pawar et al., 2024). OpenAI (2024a) evaluate the bias introduced by names in ChatGPT. They state that users often share their own names with chat assistants for"}, {"title": "3 Methodology", "content": "3.1 Names\nWe use a dataset from Facebook (Remy, 2021) to obtain names from across the world, based on names of Facebook users. It includes the most popular names, their gender, and the country from which the name was sourced. We only use first names for our task and select the top 30 names (based on popularity) from the dataset with an equal mix of male and female names (genders are marked in the dataset).\n3.2 Cultural information\nWe also need information about different cultures as ground truth to identify presumed cultures in LLM responses and to create information-seeking questions that require some cultural assumptions. We leverage assertions about cultures in the knowledge graph (KG) CANDLE (Nguyen et al., 2023) to do this. The KG has 1.1 million assertions about cultural common-sense knowledge across 5 facets of culture - food, drinks, tradition, clothing, and rituals. Qualitative analyses reveal that CANDLE contains numerous generic assertions about cultures that do not meaningfully contribute to our information-seeking setting, e.g. statements such as 'The Chinese civilization has been a long and enduring one.' To filter these out, we develop an LLM-based approach that identifies whether an assertion contains a concrete, distinctive cultural element (such as a specific food, tradition, ritual,\nor practice) rather than general claims about a culture's history, values or characteristics. More details can be found in subsection A.3.\n3.3 Cultures\nTo decide which cultures to use for our study, we take an intersection of the two data sources we list above, i.e. the source of names and the source of cultural information. We take the cultures with at least 30 names in the names dataset and at least 200 (filtered) assertions pertaining to the cultures in CANDLE-KG. Taking the intersection of the two, results in 30 countries, see Figure 3. For the scope of this study, we adopt a one-to-one mapping between cultures and countries to align with our names dataset and CANDLE's organization, while acknowledging that cultural identities often transcend national boundaries.\n3.4 Questions\nTo create the questions to probe LLMs, we use a semi-automatic approach. For a set of seed questions, the authors of this study manually crafted a list of pertaining to the categories used in the KG, i.e., clothing, food/drinks, tradition/rituals. This was done by qualitatively going through insights about what kind of questions are asked in real-world LLM interactions (Zhao et al., 2024; Ouyang et al., 2023).\nTo expand this set of seed questions and remove potential biases from manual curation, we add questions from a list of candidate questions generated by an LLM. For generating candidate questions that are related to the assertions, we prompt an LLM to generate candidate questions from clusters of assertions. Specifically, we remove country names (to ensure that clusters are about concepts"}, {"title": "3.6 Experimental setting", "content": "We outline our experimental setup in Figure 2 we generate responses to different questions using prompts with and without names in them. We then assess bias in responses in the form of cultural presumptions through two methodologies and compare their performance. The details of various parts of our pipeline are as follows.\n3.6.1 Response generation\nFor generating responses to probe LLMs, we add the name to the system prompt, in the format: \u201cMy name is <Name>. Help me with the following questions\". We add questions to the user prompt.\n3.6.2 Cultural presumption detection\nWe formulate a presumed culture, when responses to a question have an overt bias through particular cultural information included within them. As shown in Figure 2, we use two methodologies for cultural presumption detection. One using a pure LLM-as-a-judge approach where the model is tasked with detecting if the generated response is biased towards a given culture. The second, where an assertion is provided from CANDLE and the model is tasked with checking if that assertion is contained within the model response. The prompts used for both these tasks are provided in Figure 12 and Figure 13 in the Appendix. We evaluate both these approaches manually.\n3.6.3 Human evaluation\nFor analysing bias evaluation through our method, we conduct a human evaluation of the performance of the detection classifiers on 300 responses. Two PhD students are asked to (in tandem) annotate a randomly sampled set of model responses stratified by model type. We provide annotation guidelines and details in the Appendix (B.1). We evaluate both our approaches through the labeled set above. Our LLM-as-a-judge cultural presumption classifier has a 95% accuracy. For our entailment classifier, when compared against the second question, we achieved an 85.4% accuracy. This is because the labels for the second question are at times 'yes' even when the first one is 'no', due to the response being tailored towards several cultures, such as recommendations of dishes from around the world. While the assertion-based approach is grounded in real-world data, with assertions drawn from human generated text, the labels overpredict bias when measuring cultural presumption. For this reason, we report results with our LLM-as-a-judge approach in our paper.\n3.7 Robustness validation using CANDLE\nThough the results of our assertion-based approach overpredicts bias, as reported in the previous section, we conduct a correlation analysis between the response bias calculated through the two approaches. We calculate Pearson correlation and Spearman rank correlation between bias values of countries for each model and facet pair.\nWhile the overall correlations are moderate (Pearson = 0.218, Spearman = 0.423), a deeper examination shows stronger correlations between top-10 and bottom-10 values. For the highest-bias"}, {"title": "3.8 Bias calculation", "content": "We calculate cultural bias in model responses using LLM-as-a-judge (detailed in ref 3.6.2). We measure bias by calculating how frequently responses show cultural preferences for each combination of culture, model, and facet. These frequencies are then averaged across different names and questions to obtain a final bias score. We find that even prompts without names show cultural bias. To isolate the impact of names, we measure this \u2018default bias' in responses without names and subtract it from the bias found in responses with names. This gives us a clearer measure of the additional bias introduced by cultural names.\nMathematically, for each combination of culture $c$, model $m$, and facet $f$, the measured bias is defined as:\n$Bias(c_s, c, m, f) = \\frac{1}{N_{c_s,m,f}} \\sum_{i=1}^{N_{c_s,m,f}} I\\{r_i(c, m, f) = 1\\}$         (1)\nwhere $N_{c_s,m, f}$ is the number of responses associated with names sourced from culture $c_s$ for model $m$ and facet $f$ (across all questions of that facet), and $r_i(c, m, f)$ is a binary indicator (with respect to checking culture $c$) that equals 1 if the $i$th response is biased.\nFor responses without names, the default bias is computed as:\n$Bias_0(c, m, f) = \\frac{1}{N_{m,f}^{(0)}} \\sum_{i=1}^{N_{m,f}^{(0)}} \\{r_i^{(0)}(c, m, f) = 1\\}$            (2)\nwhere $N_{m,f}^{(0)}$ is the number of responses (without names) for model $m$ and facet $f$. Finally, the adjusted bias (which we report and analyse) is defined as:\n$Bias_{adj}(c_s, c, m, f) = Bias(c_s, c, m, f) - Bias_0(c, m, f)$    (3)"}, {"title": "4 Results", "content": "4.1 Default bias\nWe calculate default bias (see subsection 3.8) and observe that model responses show inherent bias towards certain cultures even without names in prompts. When prompted with open-ended information-seeking questions, models disproportionately generate suggestions drawing from East and South Asian cultural elements, with Japanese and Indian references appearing most frequently. This pattern aligns with recent studies (Khandelwal et al., 2023; Li et al., 2024) that show default responses disproportionately include culture-specific symbols from these regions. While this bias persists across all models, its magnitude varies significantly: DeepSeek shows the lowest average bias (0.035), while Mistral exhibits the highest (0.071), followed by Llama (0.068) and Aya (0.061).\n4.2 Cultural presumptions based on names\nTo understand how LLMs associate names with cultures, we analyse the difference between cultural bias (associations) in responses when prompts contain names and when no names are mentioned as discussed in subsection 3.8. The graph shown in Figure 4 represents the degree to which a model associates a particular culture to a name from that culture, over the case when no name is provided. For instance, both Korea and Russia show notably high positive differences in Llama3 (around 0.4-0.5), indicating that when presented with Korean or Russian names, the model generates significantly more Korean and Russian specific suggestions respectively, compared to when no name is mentioned. This suggests that names from these cultures lead"}, {"title": "5 Analysis", "content": "5.1 Cross-cultural bias evaluation\nTo study cross-cultural biases, we analyse potential bias in responses with respect to other cultures. Figure 6 shows cross-cultural bias for all countries above the default bias (averaged across models and facets). One observation across all countries is that mentions of names decrease the diversity of responses. For countries such as Japan, China, and India, this phenomenon is distinctly visible. The responses to questions without names, have predominance of suggestions from these countries."}, {"title": "5.2 Name-wise comparison", "content": "Not all names elicit biased responses from the models. In fact, the distribution is quite skewed. We show this in Figure 7. The distribution of biased responses per name is heavily skewed, with most names having relatively few biased responses and a smaller subset having substantially higher counts. We list the set of top biased names across all countries and their frequencies in Table 4."}, {"title": "5.3 Names present in more than one culture", "content": "To study cross-cultural associations, we consider the names present in more than one culture, grouping them based on Hanks et al. (2006). The cross-cultural names in our dataset fall into five broad clusters based on common countries: Anglophone, Hispanic/Latin, European, Middle Eastern/North African, and East Asian names with each cluster reflecting different patterns in country association as highlighted in Table 1.\nA key observation is that the models tend to flatten cultural identities by stereotyping names\u2014disproportionately linking them to one dominant country within each group. For instance, within the Anglophone group, names like Mark and James consistently receive suggestions biased towards the United States (typically 10\u201312%), while Canada, despite being an English-speaking country, is assigned very low values (below 1\u20131.5%). In the Hispanic/Latin cluster, although names such"}, {"title": "5.4 A closer look at the questions", "content": "We examine what words lead to the highest bias when a name is mentioned in the prompt (Figure 8). The plot reveals that the word 'tradition', when mentioned in the question, leads to disproportionally high bias in the responses compared to other words. We also consider bias elicited by the word for each country before and after the mention of the name in Figure 8. While the proportion of bias elicited by the word 'tradition' is extremely low with prompts without names, it becomes sizable when names are mentioned in the prompt."}, {"title": "6 Discussion", "content": "Through our experiments, we demonstrate that LLMs implicitly personalise their responses by inferring user background from names. Further, simple wording can further strengthen these influences. The mention of the word tradition, cultural or family along with a person's name in the query can lead to responses heavily biased towards some cultures over others. Relying solely on a name to determine cultural identity can be problematic as it can introduce biases in model responses towards underrepresented groups (Kantharuban et al., 2024; Das et al., 2023). We find that some names clearly introduce more bias than others, raising questions about how AI interaction is inadvertently influenced by a user's name. While we establish this in a template-based single-turn setting, how such response bias would manifest itself in a more naturalistic multi-turn setting remains to be explored. How LLMs should adapt output based on user names and assumed culture presents a complex interplay between beneficial customisation and the inadvertent reinforcement of biases. While personalisation aims to enhance user experience by tailoring interactions, it can also lead to the oversimplification of identities, resulting in the perpetuation of stereotypes (Kirk et al., 2024). The problem of implicit personalisation as a moral problem is defined by Jin et al. (2024), encouraging future discussions of the issues on a case-by-case basis. The distinction between beneficial and detrimental personalisation hinges on the model's ability to respect the multifaceted nature of individual identities. These considerations should particularly be made based on deployment context. Kirk et al. (2025) argue that as AI systems become more personalised and agentic, there is a pressing need for socioaffective alignment to ensure that AI behaviors support users' psychological and social well-being. Provided the anthropomorphic and relationship building behaviour (Ibrahim et al., 2025) that models are trained to interact with, above all, it is crucial for models to be transparent in the assumptions they are making and convey the implicit personalisation taking place. This provides the user with agency, which in the case of an error would allow the user to change the behaviour."}, {"title": "7 Conclusion", "content": "Our study establishes and quantifies the change in LLM responses and suggestions (to information seeking questions) when names are mentioned in the context. We find strong evidence of cultural identity assumptions, particularly for names from East Asian, Russian, and Indian cultures, while names from Ireland, Brazil, and the Philippines lead to more diverse and generic responses. We also find disparities between names themselves, with some leading to much more biased responses than others. Furthermore, a facet-based analysis indicates that clothing and tradition queries amplify bias most dramatically, especially when key terms such as 'tradition' are present. Our cross-cultural analysis highlights the issue of cultural flattening that models consistantly favour some cultures over others. We hope this study will serve as a useful reference for considerations on the utility vs. harms of names-based personalisation of LLMs."}, {"title": "8 Limitations", "content": "A limitation of our study is the methodological choice to equate countries with cultures, which is a simplification of complex cultural identities. This one-to-one mapping, while being the prevailing approach work on cultural NLP, fails to capture important nuances such as cultural groups that span multiple countries, multiple distinct cultures within a single country, diaspora communities, and regional cultural variations. While this simplification was necessary because of the nature of the names dataset and CANDLE, it potentially masks more nuanced cultural associations and biases in the models' responses.\nAnother limitation stems from our source of names and its inherent sampling bias. Countries with high internet penetration and digital presence are better represented both in our names dataset and in LLMs' training data. For instance, names from South Korea or Japan, countries with high internet usage, appear frequently in model responses with specific cultural suggestions, while names from regions with lower digital representation might elicit more generic responses. This data skew could explain why certain cultures consistently show stronger associations in model outputs, reflecting broader digital accessibility disparities rather than purely cultural biases."}, {"title": "9 Ethical Implications", "content": "In conducting this study, we carefully considered privacy implications by using only first names rather than full names, preventing potential identification of individuals while maintaining authenticity in our experiments. However, this methodological choice, while protective, still enables us to uncover significant ethical concerns about how LLMs make cultural assumptions based on names. These findings raise ethical concerns about the real-world impact of name-based cultural presumptions in LLMs. When models flatten cultural identities by linking certain names to specific cultural contexts, they risk stereotyping users and misrepresenting individual preferences. In applications like customer service and content recommendation, such misassumptions can lead to misguided personalization that not only reinforces cultural homogenization but also harms user sentiment\u2014potentially causing frustration, feelings of alienation, and even user dropout, particularly among underrepresented groups."}, {"title": "A Appendix", "content": "A.1 Model details and Experiment Details\nFor all our experiments, we use the vLLM library for efficient inference (Kwon et al., 2023). We use the hyperparameters, we provide specific model codes in Table 2.\nLlama: We used Meta-Llama-3.1-8B-Instruct available via HuggingFace1. We used vLLM for inference with parameters temperature=0.7, top_p=0.9, max_tokens=2048, dtype='half' and max_model_len=8096.\nAya: We used Aya-expanse-32b available via HuggingFace2. We used VLLM for inference with parameters temperature=0.8, top_k=50, max_tokens=2048, dtype='half' and max_model_len=8096.\nMistral: We used Mistral-Nemo-Instruct-2407 available via HuggingFace3. We used vLLM for inference with parameters temperature=0.6, top_p=0.8, max_tokens=2048, dtype='half' and max_model_len=8096.\nDeepSeek: We used DeepSeek-R1-Distill-Llama-8B available via HuggingFace4. We used VLLM for inference with parameters temperature=0.6, top_p=0.8, max_tokens=2048, dtype='half' and max_model_len=8096.\nFor generating responses (with and without names), we used the above four models, and total number of generations were around 90k per-model, which required around 1 day on 8 A100s. For calculating the bias, we ran LLM-as-a-Judge (using meta-llama/Llama-3.1-70B) to check for bais towards all 30 countries on the 360k responses, which required around 8 days on 8 Nvidia A100s. For robustness analysis, we carried out assertion-checking using meta-llama/Llama-3.1-8B which required around 10 days on 6 Nvidia H100s (as for each response, to check for bias towards a country, we checked on average 200 Assertions). Hyper-paramters for the LLM-as-a-judge were similar to the ones mentioned above. The names dataset used in the paper is released under Apache-2.0 license which is a permissive open-source license. allows anyone to freely use, modify, and distribute the licensed software. For the openweight models, we signed the terms of use on HuggingFace which allow to use the models to generate and analyze the data for publications."}]}