{"title": "MDSF: Context-Aware Multi-Dimensional Data Storytelling Framework based on Large language Model", "authors": ["Chengze Zhang", "Changshan Li", "Shiyang Gao"], "abstract": "The exponential growth of data and advancements in big data technologies have created a demand for more efficient and automated approaches to data analysis and storytelling. However, automated data analysis systems still face challenges in leveraging large language models (LLMs) for data insight discovery, augmented analysis, and data storytelling. This paper introduces the Multidimensional Data Storytelling Framework (MDSF) based on large language models for automated insight generation and context-aware storytelling. The framework incorporates advanced preprocessing techniques, augmented analysis algorithms, and a unique scoring mechanism to identify and prioritize actionable insights. The use of fine-tuned LLMs enhances contextual understanding and generates narratives with minimal manual intervention. The architecture also includes an agent-based mechanism for real-time storytelling continuation control. Key findings reveal that MDSF outperforms existing methods across various datasets in terms of insight ranking accuracy, descriptive quality, and narrative coherence. The experimental evaluation demonstrates MDSF's ability to automate complex analytical tasks, reduce interpretive biases, and improve user satisfaction. User studies further underscore its practical utility in enhancing content structure, conclusion extraction, and richness of detail.", "sections": [{"title": "I. INTRODUCTION", "content": "The explosive growth of data scale and the rapid development of big data technology have driven progress across various fields, but have also introduced the challenge of analyzing massive amounts of data. Efficient data analysis and precise data interpretation have thus become crucial tasks for data scientists and analysts. The traditional data analysis pipeline struggles to achieve comprehensive and accurate insights in big data scenarios. Intelligent data storytelling, as an important method of data analysis, is poised to reconstruct the data analysis process. This method is based on LLM agents and comprises various capabilities such as data visualization, storytelling techniques, and contextual analysis, bringing a new paradigm to the field of data analysis. According to Gartner, by 2025, intelligent data storytelling will become the primary mode of analytic consumption, with 75% of data stories predicted to be automatically generated using augmented intelligence and machine learning, rather than by data analysts.\nIntelligent Data Storytelling (IDS) holds significant advantages in big data environments by automating labor-intensive steps within the traditional data analysis pipeline, thereby establishing a data-driven and efficient analysis process. With the rapid advancement of large model technologies, the generalization and comprehension capabilities of these models present new opportunities for data storytelling. IDS leverages the extensive prior knowledge embedded in large models to automatically generate data reports during user interactions, significantly enhancing efficiency and reducing interpretative biases. For instance, in corporate sales analysis, the traditional analysis pipeline typically involves multiple stages: data extraction, cleaning, visualization creation, and report writing, each heavily dependent on the analyst's involvement, resulting in a lengthy process. In contrast, IDS automates data cleaning and dynamically generates narratives, presenting data insights directly in visual and natural language formats. It can also capture anomalies in real-time and provide explanations. This intelligent process eliminates the need for sequential tool switching, greatly optimizing the speed and quality of data interpretation.\nDespite the growing importance of data storytelling, current intelligent data storytelling still faces numerous challenges. First, most existing methods are typically limited to single-dimensional data analysis, making it difficult to effectively integrate and present complex multi-dimensional data. Second, generating accurate and engaging narratives remains a time-consuming process that requires specialized skills, which restricts the widespread application of data storytelling across various fields. Additionally, existing tools often lack sufficient flexibility and adaptability to meet the diverse needs of different industries and audiences.\nTo address these challenges, we propose the Multidimensional Data Storytelling Framework (MDSF). MDSF is based on Large Language Models (LLMs) and aims to provide a comprehensive, flexible, and intelligent solution to overcome the limitations of current data storytelling methods. It can simultaneously process and integrate multi-dimensional data, including numerical, categorical, time-series, and spatial data, thereby generating more comprehensive and in-depth data narratives. By leveraging the powerful natural language processing and generation capabilities of LLMs, MDSF can automatically create coherent and engaging stories, significantly reducing the need for manual intervention and increasing efficiency.\nOverall, the introduction of MDSF aims to advance the technology of data storytelling, providing more powerful and intelligent tools to various fields for more effectively extracting data value, promoting information dissemination, and facilitating decision-making. By addressing the limitations of existing methods, MDSF is expected to bring revolutionary changes to data analysis and communication, making data storytelling a more prevalent, efficient, and insightful practice. The major contributions of this paper are as follows:\n1) Development of the Multi-dimensional Data Storytelling Framework (MDSF): We propose an innovative framework based on Large Language Models (LLMs) for multi-dimensional data storytelling. This framework can handle complex datasets and generate easily understandable narrative content.\n2) Integration of LLMs into the Data Storytelling Process: We successfully integrate Large Language Models into the data storytelling process, ensuring that the generated narratives are more coherent and insightful. This approach not only improves the quality of the narratives but also significantly reduces the need for manual intervention.\n3) Empirical Validation of MDSF's Effectiveness and Advantages: We demonstrate the effectiveness and advantages of MDSF through a series of experiments and case studies. The results indicate that this framework performs excellently in data storytelling tasks across different fields, significantly enhancing the efficiency of data analysis and decision-making."}, {"title": "II. RELATED WORKS", "content": "In recent years, with the continuous growth of data scale and the increasing complexity of analysis, both academia and industry have been deeply researching efficient data analysis methods. Relevant work can be summarized into three main directions: automated data analysis, the application of large language models in data insights, and automated report generation based on agents. These studies provide the theoretical foundation and practical reference for the proposed Multi-dimensional Data Storytelling Framework (MDSF).\nAutomated data analysis and augmented analytics have gained significant attention due to their ability to streamline data-driven decision-making processes. Islam et al. [1] introduced DataNarrative, an innovative framework combining visualization and textual storytelling to enhance automated insights. Similarly, CoInsight by Li et al. [6] focuses on hierarchical table storytelling, emphasizing the utility of visual storytelling techniques to connect data insights.\nXie et al. [4] developed HAIChart, which pairs human and AI capabilities for visualization, bridging the gap between raw data and actionable insights. Additionally, Shao et al. [8] explored the impact of storytelling in data visualization, finding that it improves both the efficiency and effectiveness of insights comprehension. These works highlight the evolution of data analysis techniques, combining automation with human-centric designs to improve the interpretability and usability of data-driven narratives.\nLarge Language Models (LLMs) have been pivotal in enhancing data insight capabilities. Zhu et al. [5] investigated the statistical acumen of LLMs, providing a benchmark for their application in complex data analysis scenarios. Sui et al. [11] examined LLMs' understanding of structured table data, offering insights into their strengths and limitations in handling tabular information.\nHe et al. [12] conducted a comprehensive survey on leveraging LLMs for narrative visualization, emphasizing their role in crafting coherent and engaging data stories. Furthermore, InsightPilot by Ding et al. [15] exemplifies an LLM-powered system for automated data exploration, showcasing their potential in streamlining exploratory data analysis tasks. These studies underscore the transformative impact of LLMs on data insight generation and analysis, enabling more nuanced and scalable approaches to understanding complex datasets.\nAgent-based systems have emerged as a promising paradigm for automated reporting. DS-Agent, proposed by Guo et al. [7], integrates large language models with case-based reasoning to automate data science workflows, significantly reducing manual effort. Shen et al. [13] extended this concept with an LLM-based multi-agent system for creating animated data videos, pushing the boundaries of automated narrative generation.\nAdditionally, Singha et al. [9] explored semantically aligned question and code generation for insight generation, demonstrating the efficacy of agents in automating complex analytical processes. The Melody platform by Renda et al. [23] further exemplifies agent-based storytelling, combining linked open data visualization with curated narratives. These advancements highlight the growing role of agent-based frameworks in delivering dynamic and contextually relevant reports."}, {"title": "III. PRELIMINARIES", "content": "The data model D can be abstracted as an N \u00d7 M data table, where N represents the number of rows and M represents the number of dimensions. The dimensional attributes include time dimensions, categorical dimensions, and numerical dimensions, corresponding to time series data, categorical data, and numerical data, respectively.\nA breakdown dimension is a dimension on which enumeration operations are performed on the data model D. When a breakdown is specified, the indicators in the data model can be calculated by executing aggregation functions. An indicator is a function attribute that aggregates data. For example, an indicator measuring overall revenue is calculated by summing the individual revenue data points. A filter is a conditional selection applied to the data model D. For example, a filter can restrict the data domain to a specific time range or select a particular attribute with a specific value.\nFor instance, in a sales data model, the breakdown dimension could be the region, the indicator could be the sales amount, and the filter could be the time range. It is important to note that in practical business systems, the time dimension and time filters are often specified separately to optimize performance.\nA data insight is a specific pattern identified within a data model, discovered through our predefined types of insights and identification methods. An insight can be represented by the following components:\nInsights :=\nInsights :=\nInsights:=\nInsights:=\nInsights:=\nbreakdowns, indicators, type,\ndatamodel, details, score\nInsights are designed to encompass various types to describe specific patterns in the data model from different perspectives. These include describing distribution characteristics of fundamental attributes, year-over-year and month-over-month comparisons, detecting anomalies, periodicity, and trends in time series data, as well as more complex analyses such as root cause analysis and correlation breakdowns. Additionally, each insight includes a specific score, which quantifies the value of the insight. This score is crucial for large language models to understand and assess the significance of the insights.\nA data story is composed of multiple insights. By leveraging large language models, descriptions of various insights are generated and integrated into coherent paragraphs, along with visual components, to form a comprehensive data report.\nThe report includes a summary, detailed descriptions of the findings, a list of the source insights, and visual components such as charts and tables to present the data more intuitively."}, {"title": "IV. THE MDSF", "content": "We have designed a Multidimensional Data Storytelling Framework (MDSF) to automatically generate data insight text and provide real-time report generation capabilities based on context when users edit reports.\nData identification and pre-processing are the primary steps in the Multidimensional Data Storytelling Framework (MDSF). The system automatically extracts data from various sources (such as databases and file systems) and performs necessary preprocessing operations, including data cleaning, format conversion, and missing value imputation. This process ensures the accuracy and consistency of subsequent analyses. By utilizing automated tools, we significantly reduce manual intervention and increase data processing efficiency.\nFirstly, we clean the underlying multidimensional data and enumerate different subspaces based on filtering operations and dimension selection. Each subspace can derive multiple peer subspaces according to different filtering conditions. After grouping and aggregation, we obtain a finer granularity of the data domain. Data preprocessing involves subspace enumeration, pre-computation of data aggregation, and indicator trimming operations.\nTrimming primarily includes: dimension trimming based on correlation, and data filtering based on the null value rate and coverage rate of the indicators.\nIn the process of discovering insights, we employ various augmented analysis algorithms. For time series data, we use algorithms such as Prophet and SR-CNN. For non-time series data, we utilize a combination of the 3-sigma and iForest algorithms, as well as time series forecasting methods\nTo evaluate and rank insights, we design a multi-angle scoring mechanism and employ a Top-K algorithm for ranking. The scoring mechanism includes the following aspects:\nEvaluate the importance of the subspaces used to generate insights for the complete dataset. The greater the significance of a subspace within the collection, the higher its importance score. Based on practical data analysis experience, it is observed that long-tail distribution is a common phenomenon in datasets. The benefits of focusing on tail data are much less than those of focusing on head data. Therefore, the distribution characteristics are used here to quantify the importance of the data.\n$Imp = \\frac{\\sum_{i=0}^{n} |X_i - \\overline{X}|}{\\sum_{i=0}^{n} X_i} * \\frac{ASCRN}{n+1}$\n\nThe value score is a measure of the Significance of the insight. According to the evaluation of the insight judgment function, the degree of matching with the type, specifically, the significant degree of anomaly, the degree of fitting with the distribution, etc., varies with the different insight types.\nSurprise score is used to evaluate the degree to which users are surprised by the insight results. The degree of surprise mainly includes two aspects: on the one hand, whether the result is beyond the user's expectation of satisfactory results; On the other hand, whether the insight results can be easily found by the user drill-down. The purpose or expectation of an insight algorithm is to find insights that are difficult for analysts to find. In this scenario, the Insight surprise score that is easy to be found is lower than that of the Insight surprise score that is difficult to be found.\nFatigue refers to the user's tolerance for similar or identical insights. When the recommended insights in the recommendation pool are too similar and do not align with user preferences, user fatigue can occur, negatively impacting the effectiveness of the insights. Therefore, fatigue is used to score and rank insights according to specific rules. The design of fatigue consists of two components:\nLimiting the number of insights on similar topics.\nAdjusting the weight of different pattern types based on user feedback.\nInterpretability scoring represents the explainability of the model inputs and outputs, measuring the ability to justify why a certain data range is considered insightful.\nFor insights, interpretability is related to the significance of the insights. The more significant the insight, the higher its interpretability. Additionally, interpretability is associated with the prominence of pattern recognition."}, {"title": "B. Context Storytelling", "content": "Context-based data storytelling is one of the key components of the Multidimensional Data Storytelling Framework (MDSF). The context encompasses the user's editing history and modifications. By leveraging historical data and a large model Agent framework for reasoning, MDSF performs real-time analysis of the user's areas of interest, generating high-quality data reports and insightful analyses.\nAt the heart of the Big Model Agent framework is the use of pre-trained large language models (LLMS) such as GPT-4 for data analysis and text generation. The framework consists of the following key steps:\nThe model first needs to understand the context of the user input, including the structure, dimensions, and metrics of the data. Using natural language processing techniques, the model is able to identify key information in the data and generate initial insights.\nAfter understanding the context, the model performs inference to identify patterns and anomalies in the data. This step often involves complex statistical analysis and machine learning algorithms such as time series analysis, clustering analysis, and regression analysis.\nBased on the inference results, the model generates a detailed insight report. These reports not only contain descriptive statistics of the data, but also provide predictive analysis and decision making recommendations.\nFigure 2 (a) illustrates the overall architecture of our proposed Multiscenario Data Storytelling Framework (MDSF), which includes both offline and online computation components. In the offline computation part, we first collect metadata from multiple data sources (such as databases and file systems) and conduct preliminary analysis through an automated data discovery process. This process employs various data mining and statistical analysis techniques to extract potential insights from the raw data. These insights are presented in various forms, including time series analysis, scatter plot analysis, and histograms, providing a foundation for subsequent insight computation.\nAfter completing the preliminary automated insight discovery, we introduce human intervention to rank the insights. This manual ranking incorporates the knowledge and experience of domain experts, ensuring the high quality and relevance of the selected insights. The involvement of domain experts not only enhances the accuracy of the insights but also increases their practicality and interpretability. The output of this step is a preliminarily ranked list of insights, which serves as high-quality data input for subsequent model refinement.\nNext, we fine-tune the large language model (LLM) using the LLaMA-Factory framework to better understand and process domain-specific insights. Through this series of processes, we generate a fine-tuned model capable of accurately computing and ranking insights in subsequent online computations. The fine-tuned model not only improves the quality of generated insights but also enhances the model's adaptability to domain-specific content.\nThe online computation component primarily involves dynamically adjusting the ranking of insights based on the user's editing context and multi-scenario prompts. While the user is editing content, the MDSF framework parses the user's editing history, user profile, and current multi-scenario prompt information in real time. By comprehensively analyzing this information, the system can extract the most relevant insights from the precomputed insight library and re-rank these insights using the LLM. Specifically, the system first parses the data to identify potential insights in the current editing context. Then, it scores and ranks these insights using the LLM, generating descriptive text and visualizations. Finally, the system selects the optimal insights from the generated results, providing them to the user for further editing and utilization.\nIn this part, the combination of automation and human intervention ensures the high quality and relevance of the insights. Additionally, the dynamic adjustment mechanism in the online component allows the system to provide the most relevant insights in real time based on the user's editing context, thus enhancing the user's editing efficiency and content quality."}, {"title": "V. EXPERIMENTS", "content": "In this section, we evaluate MDSF on four classes of tasks. The specific research questions through the experimental study are as follows:\nCan MDSF understand insights and produce high-quality rankings?\nCan MDSF generate accurate and stable insight descriptions?\nCan MDSF create superior data stories in the data story generation task based on the above?\nIn this paper, we want to design a general framework that can be used on multiple models, so we refer to the latest open source and closed source models for the choice of large language model base. We selected GPT-3-turbo, GPT4, Gemini-1.5, LLama3-7B, and Qwen2-7B as the base models. Meanwhile, this paper also fine-tunes llama3-7B and qwen2-7B for the rank task and the generation task.\nIn the experiments, we use 2 NVIDIA V100 Gpus for finetuning, and the batch size of each GPU is 32. We used the Adam optimizer with a learning rate of 1e-4 and trained for 10 epochs.\nWe conducted experiments with the MDSF on one private dataset and three public datasets (Text2Analysis, InsightBench, and competition data) to validate its effectiveness and performance across different scenarios.\nThe private dataset is derived from the company's actual business data, encompassing information from multiple domains such as sales data, user behavior data, and advertisement data. Based on this dataset, we performed bulk data annotation"}, {"title": "Evaluation Metrics:", "content": "We used multiple evaluation metrics to assess the performance of MDSF. For the ranking tasks, we employed the Spearman Footrule Distance.\n$F = \\sum_{i=1}^{n} |r_i-P_i|$\n\nF is the Spearman Footrule, ri is the rank of the i-th item in the ground truth, and ri is the rank of the i-th item in the prediction. Bigger Spearman Footrule means worse performance.\nFor the insight description tasks, we used accuracy metrics, as we are more interested in whether the model can accurately represent the data without generating hallucinations.\nIn the accuracy evaluation, we focus on whether the generated descriptions accurately convey the type of insight, the content of the insight, and the value of the insight. Since this part of the evaluation is manually annotated, we conducted a sampled evaluation.\nFor the story generation task, we used BLEU and ROUGE as evaluation metrics.\n$BLEU = BP \\times exp( \\sum_{n=1}^{N} W_n log p_n)$ and ROUGE are commonly used evaluation metrics in the field of NLP for assessing the quality of generated text. BLEU evaluates the quality of generated text by calculating the overlap of n-grams between the generated text and the reference text. ROUGE, on the other hand, assesses the quality of generated text by computing the overlap between the generated text and the reference text.\nThe choice of BLEU and ROUGE as evaluation metrics is due to their effectiveness in reflecting the degree of overlap between the generated text and the reference text, thereby providing a better assessment of the quality of the generated text."}, {"title": "B. Experiment Results", "content": "We evaluated the performance of MDSF on four types of tasks, and we present the experimental results of each task separately below.\nIn the Insight Rank task, we compared the performance of the MDSF model with other mainstream models on both a private dataset and the InsightBench dataset. Table II presents the specific results of various models on the SFD metric, where a smaller SFD value indicates lower distance error and better performance.\nAs shown in the table, the manually annotated SFD value is 0.00, which serves as our benchmark. The GPT-4 model outperformed GPT-3.5 turbo on both datasets, achieving SFD values of 5.83 and 7.00, respectively. The Gemini 1.5 model also performed well, with SFD values of 7.21 on the private dataset and 8.25 on InsightBench.\nIn contrast, the Llama3-7B and ChatGLM2-6B models demonstrated relatively poorer performance, especially on the private dataset where Llama3-7B had an SFD value as high as 12.48. The Qwen-2-7B-Chat and Yi-1.5-9B models also underperformed, with SFD values exceeding 9.00.\nNotably, the MDSF model achieved SFD values of 6.82 on the private dataset and 7.25 on InsightBench. Although it did not reach the level of manual annotation, it showed significant improvement over other automated models. This indicates that the MDSF model possesses good accuracy and reliability in handling the Insight Rank task.\nIn summary, the MDSF model outperformed most existing mainstream models in the Insight Rank task, demonstrating its advantage in controlling distance error.\nIn the Insight Description task, we used the ACC (accuracy) metric to evaluate the performance of different models. Figure 3 shows the performance of each model on the ACC metric. A higher ACC value indicates greater accuracy of the model in the description task. As shown in the figure 3, there are significant differences in the ACC performance of the various models:\nThe specific results are as follows:\nLlama3-7B had an ACC value of 0.432, indicating the poorest performance. ChatGLM2-6B had an ACC value of 0.545, slightly higher than Llama3-7B. Qwen2-7B-Chat and Yi-1.5-9B had ACC values of 0.605 and 0.623, respectively, representing moderate performance. GPT-3.5 had an ACC value of 0.693, showing good performance. Gemini 1.5 had an ACC value of 0.742, demonstrating excellent performance. GPT-4 had an ACC value of 0.785, indicating outstanding performance. The MDSF model had an ACC value of 0.858, the best among the automated models, second only to the manually annotated (Manual) value of 1.0.\nFrom the results, it is evident that the MDSF model exhibited the most exceptional performance in the Insight Description task, achieving an ACC value of 0.858, significantly surpassing other automated models and approaching the accuracy of manual annotation. This indicates that the MDSF model has high accuracy and reliability in handling description tasks.\nIn the Data Story Generation task, we compared the performance of different models on a private dataset, Kaggle dataset, Text2Analysis dataset, and InsightBench dataset. Table III lists the scores of each model on the Rouge and BLEU metrics.\nFrom the table, it can be seen that GPT-4 and Gemini 1.5 outperformed other models on multiple datasets. Specifically, on the Kaggle dataset and InsightBench dataset, GPT-4 achieved Rouge and BLEU scores of 0.865 and 0.827, as well as 0.952 and 0.915, respectively. Gemini 1.5 also performed very close to these scores, with 0.871 and 0.832 on the Kaggle dataset, and 0.937 and 0.922 on InsightBench.\nIn contrast, Llama3-7B and Qwen2-7B performed relatively poorly, especially on the Kaggle dataset, where Llama3-7B had Rouge and BLEU scores of only 0.689 and 0.650. Even the fine-tuned versions, Llama3-7B-FT and Qwen2-7B-FT, did not significantly improve their performance.\nOur proposed MDSF model exhibited stable performance across all datasets. Particularly on the Text2Analysis dataset and InsightBench dataset, the MDSF model achieved Rouge and BLEU scores of 0.752 and 0.813, and 0.864 and 0.824, respectively. While it did not surpass GPT-4 and Gemini 1.5, it still demonstrated strong competitiveness.\nIn summary, although GPT-4 and Gemini 1.5 showed outstanding performance in the Data Story Generation task, the MDSF model also performed well across multiple datasets, indicating its potential in data story generation tasks.\nTo comprehensively evaluate the performance of the MDSF model, we conducted a user survey. The evaluation criteria for the user survey included three aspects: Structure, Conclusion Extraction, and Richness. Table IV presents the specific evaluation criteria and scoring details.\nIn the user survey, participants rated the outputs of each model based on the aforementioned criteria. Figure 4 presents the results of the user survey, showing the percentage scores of each model in terms of Structure, Conclusion Extraction, and Richness.\nThe observations from Figure 4 reveal the following key insights:\nThe MDSF model demonstrates a significant advantage over Qwen2 and GPT-4 in structural integrity and readability. The ratings predominantly cluster around scores of 2 and 3, underscoring the model's effectiveness in maintaining coherent and well-organized outputs.\nThe MDSF model performs notably well in extracting conclusions, with ratings concentrated around 2 and 3. This suggests a superior capability to distill and extract key insights and valuable information from the data.\nThe MDSF model excels in providing rich, multi-dimensional content, as reflected in the distribution of ratings around 4 and 5. This demonstrates its ability to offer comprehensive analyses supported by diverse data dimensions.\nOverall, the user study findings indicate that the MDSF model achieves exceptional performance in structure, conclusion extraction, and content richness, substantially outperforming the comparative models. These results further establish the model's reliability and practicality for augmented analysis in real-world scenarios."}, {"title": "VI. CONCLUSION", "content": "This paper presents the Multi-dimensional Data Storytelling Framework (MDSF), which leverages large language models to automate the complexities of data analysis and context-aware report generation. Traditional data analysis systems often struggle to extract actionable insights and maintain consistency across diverse data sources. MDSF addresses these challenges by integrating automated data exploration, insight prioritization, and fine-tuned model continuations into a unified solution.\nThe framework effectively extracts and ranks actionable insights from large datasets, enhancing the analytical relevance and depth of generated reports. A pre-labeled fine-tuning phase ensures the large language model is both accurate and context-sensitive, enabling precise and meaningful outputs. The agent-based architecture facilitates seamless continuation control, even in scenarios with complex data histories and diverse textual inputs, a critical capability for real-world applications characterized by data heterogeneity and contextual nuances.\nExperimental results on various real-world datasets demonstrate MDSF's superior accuracy, operational efficiency, and user satisfaction compared to existing methods. User feedback further underscores its practical value and potential for future refinement.\nIn conclusion, MDSF integrates advanced language models with innovative insight management strategies, addressing key challenges in data analysis while setting a solid foundation for more sophisticated and user-focused data storytelling solutions."}]}