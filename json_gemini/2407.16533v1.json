{"title": "HAPFI: History-Aware Planning based on Fused Information", "authors": ["Sujin Jeon", "Suyeon Shin", "Byoung-Tak Zhang"], "abstract": "Abstract-Embodied Instruction Following (EIF) is a task of planning a long sequence of sub-goals given high-level natural language instructions, such as \"Rinse a slice of lettuce and place on the white table next to the fork\". To successfully execute these long-term horizon tasks, we argue that an agent must consider its past, i.e., historical data, when making decisions in each step. Nevertheless, recent approaches in EIF often neglects the knowledge from historical data and also do not effectively utilize information across the modalities. To this end, we propose History-Aware Planning based on Fused Information(HAPFI), effectively leveraging the historical data from diverse modalities that agents collect while interacting with the environment. Specifically, HAPFI integrates multiple modalities, including historical RGB observations, bounding boxes, sub-goals, and high-level instructions, by effectively fusing modalities via our Mutually Attentive Fusion method. Through experiments with diverse comparisons, we show that an agent utilizing historical multi-modal information surpasses all the compared methods that neglect the historical data in terms of action planning capability, enabling the generation of well-informed action plans for the next step. Moreover, we provided qualitative evidence highlighting the significance of leveraging historical multi-modal data, particularly in scenarios where the agent encounters intermediate failures, showcasing its robust re-planning capabilities.", "sections": [{"title": "I. INTRODUCTION", "content": "The substantial progress in artificial intelligence has heightened expectations for embodied agents capable of interacting with real-world environments and executing in-teractive actions. Consequently, ongoing research has been focusing on the development of embodied agents, including robots, with the capacity to emulate human abilities in efficiently processing multifaceted, long-term information. One notable task of this is Visual-Language Navigation (VLN), where the agents are directed to specific destinations using natural language instructions. Recent attention has been on tackling the more complicated problem, Embodied Instruction Following (EIF), which aims to reflect real-world complexity better. For example, in Figure 1, an agent has to execute the instruction \"Rinse a slice of lettuce and place on the white table next to the fork\", which has a complexity level similar to real-world tasks. This task is more challenging due to the diversity of action spaces and the need to predict a sequence of sub-goals with a long-tern horizon.\nAs research in the field of EIF progresses, a variety of ap-proaches have emerged for predicting interactive sub-goals, including actions, objects, and receptacles, which the agent executes on the environment [1]-[12]. The first problem in"}, {"title": "II. RELATED WORK", "content": "Robotics has recently prioritized embodied agents that can integrate visual and linguistic data to interact with real-world environments, resulting in the progress of Visual-Language Navigation (VLN) [23]\u2013[29] and Embodied Instruction Fol-lowing (EIF) [1], [3], [5]\u2013[12]. Here, we focus on planning actions from the latter line of work, EIF, that aims to integrate both navigation and interaction.\nOne noteworthy benchmark in this regard is ALFRED [2]. Unlike earlier benchmarks [30]\u2013[32] marked by confined environments and limited object variety, ALFRED seeks to emulate real-world situations, infusing tasks with increased complexity and diversity.\nIn the early stages of research, there was a significant focus on studies in which agents executed actions by following step-by-step instructions to accomplish high-level goals [1], [2], [4], [10], [11]. However, recent methods have introduced approaches that provide only high-level goals rather than de-tailed step-by-step guidance [3], [5]-[9], [12]. These under-specified instructions referred to as high-level instructions, are insufficient for task completion, which demands a pro-found understanding of multifaceted temporal context. We intend to focus on the ability to plan sub-goals relying on high-level instructions, without step-by-step instructions.\nWhen considering existing action planning approaches from a modality perspective, it's worth noting that some of these methods rely on single-modal information [3], [5]. In most cases, the primary focus is natural language instructions. Even when other modalities (typically images or semantic information) are integrated into predictions, the integration process tends to be relatively weak [6]\u2013[8]. These modalities are processed individually through separate encoders, with integration occurring towards the end. Specifically, features from different modalities, each encoded by distinct encoders, do not mutually attend to each other.\nSeveral approaches have been attempted to address the complexity and long-term horizon in predicting subsequent actions. One of the most recent methods [3] involves incor-porating context into planning to memorize environmental changes. On the other hand, there have been approaches that focus on incorporating the history of sub-goals [6]-[8] to enhance prediction accuracy. Most existing methods either do not use historical information for sub-goal planning or rely on sub-goal history only.\nBuilding upon these insights, our proposed method, HAPFI, incorporates multi-modal historical information re-lated to RGB observations, bounding boxes, predicted sub-goals, and high-level instruction. This integration empowers our model to comprehend intricate scenarios and predict subsequent actions effectively, which proves particularly valuable in tasks characterized by extended time horizons. Furthermore, inspired by the visual grounding method GLIP [22], we adopt deep and early fusion mechanisms. This mechanism focuses on achieving more accurate action plan-ning by effectively fusing features from diverse modalities."}, {"title": "III. METHOD", "content": "We present HAPFI (History-Aware Planning based on Fused Information), a model designed to deeply fuse his-torical multi-modalities, enhancing the efficiency of action planning tasks. HAPFI comprises four essential phases: Historical Visual Feature Integration, Historical Linguistic Feature Integration, Mutually Attentive Fusion, and Sub-goal Classification. We proceed by providing an explanation of each phase. First, we elucidate the process of encoding and integrating visual features III-A and linguistic features III-B, respectively. Then, we detail the fusion of historical multi-modalities, emphasizing their mutual attention III-C. Finally, we explain HAPFI's subgoal inference mechanism in detail III-D.\n##### A. Historical Visual Feature Integration\nThis phase aims to capture a wide range of visual informa-tion obtained from the environment, including both bounding box and RGB observation. Initially, we separately encode the bounding box data and RGB observation of the current scene, combining them afterward. Following that, we employ the same process to accumulate historical visual features, subsequently integrating all visual features into a unified representation.\n##### Bounding Box Image. By using bounding box coordinates from the detection results, we create a mask that highlights the area within the bounding box. This is achieved by setting the pixels inside the bounding box to the corresponding object class in a zero-valued image. As a result, we obtain a single-channel image where '0' represents the absence of an object, and other numbers indicate the presence of a corresponding object. This image is labeled as \\(B_n\\), with n meaning the current step.\n##### RGB Image. As we predict the agent's actions by a step-by-step procedure, we consider the agent's viewpoint after the completion of the latest action as the current RGB observation, \\(O_n\\). For instance, if the agent has just picked up an apple, the image of holding the apple within the agent's view is treated as \\(O_n\\).\n##### Visual Modality Information Encoding. We employ pre-trained ViT [33] backbone as the RGB observation encoder. Passing \\(O_n\\) through this ViT backbone yields an embedding denoted as \\(E^O_n\\). Furthermore, we utilize the ViT trained from scratch as the bounding box encoder for processing \\(B_n\\), resulting in the embedding \\(E^B_n\\).\n##### History Accumulation. To ensure memory efficiency, we limit our usage of the historical information to the \\(l\\) most recent data points, resulting in: the bounding box image history, \\(B_{<n} = [B_{n-l},\u2026\u2026, B_{n-1}]\\), and the RGB observation history, \\(O_{<n} = [O_{n-l},\u2026\u2026, O_{n-1}]\\). To encode \\(B_{<n}\\) and \\(O_{<n}\\), we draw inspiration from the approach used in video-based research, which involves calculating the mean of frames [34]. In the encoding process for \\(O_{<n}\\), we individually encode each of the \\(O_k\\) (\\(k = n \u2212 l,\u2026\u2026, n - 1\\)) using the RGB observation encoder. Subsequently, we compute the mean of the resulting \\(l\\) embedding vectors, \\(E^O_k\\), aligning their size with that of \\(E^O_n\\). Similarly, for \\(B_{<n}\\), we separately encode each of the \\(B_k\\) (\\(k = n-l,\u2026\u2026\u2026, n - 1\\)) using the bounding box encoder. We then take the mean of the \\(l\\) resulting embedding vectors, \\(E^B_k\\), making their size consistent with \\(E^B_n\\).\n##### Feature Integration Ultimately, we obtain a visual feature information V that will be utilized in the Mutually Attentive Fusion layer. The procedure can be written as (1), where # denotes concatenation.\n\\[\nV = \\frac{1}{l}\\sum_{k=n-l}^{n-1}E^O_k + \\frac{1}{l}\\sum_{k=n-l}^{n-1}E^B_k + E^O_n \\quad (1)\n\\]\n##### B. Historical Linguistic Feature Integration\nIn this phase, we leverage two linguistic modalities of information: high-level instruction and sub-goal history. We encode each information separately and integrate the ob-tained features.\n##### Sub-goal History. We consolidate all prior actions, objects, and receptacles into a single string, which serves as our"}, {"title": "C. Mutually Attentive Fusion", "content": "Inspired by the GLIP [22] framework, we deeply fuse visual feature information V and linguistic feature informa-tion L through a multi-head cross-attention mechanism. The process unfolds as follows:\n\\[\nq = VW_q, k = LW_k ; \\quad (2)\n\\]\n\\[\nv_y = VW_v, V_l = LW_v ; \\quad (3)\n\\]\n\\[\nv = \\text{softmax} (q \\cdot k / \\sqrt{d}) v_y ; \\quad (4)\n\\]\n\\[\nv_l = \\text{softmax} (q \\cdot k / \\sqrt{d}) V_l ; \\quad (5)\n\\]\n\\[\nV_{f_1} = VW_P + V, L_{f_1} = v_l W_P + L ; \\quad (6)\n\\]\n\\[\n(V_{f_2}, L_{f_2}) = \\text{Fusion} (\\text{ViT} (V_{f_1}), \\text{BERTLayer} (L_{f_1})); \\quad (7)\n\\]\n\\[\nF = V_{f_2} + L_{f_2}. \\quad (8)    \n\\]\nInitially, we create query q and key k pairs by applying query matrix \\(W_q\\) and key matrix \\(W_k\\) to V and L respectively (2). Then we get two values \\(v_y\\) and \\(v_l\\) by multiplying different value matrices \\(W_v\\) and \\(W_l\\) to V and L (3). Attention weights are computed using scaled dot-product attention between the queries and keys. These attention weights are transformed into attention scores, which are then used to weight both the \\(v_y\\) and \\(V_L\\).\nThe score-weighted linguistic value \\(v_l\\) (5) is further pro-cessed by a projection matrix \\(W_p\\), resulting in the fused embedding. This embedding is added to V, generating fused visual feature \\(V_{f_1}\\) (6). Likewise, the score-weighted visual value v (4) is multiplied by another projection matrix \\(W_p\\) and added to L, producing fused linguistic feature \\(L_{f_1}\\) (6).\n\\(V_{f_1}\\) is reshaped into a 14 \u00d7 14 tensor and subsequently processed by a ViT, while \\(L_{f_1}\\) is fed into a Bert Layer. We then perform a second fusion step, resulting in \\(V_{f_2}\\) and \\(L_{f_2}\\), which are subsequently concatenated into F."}, {"title": "D. Sub-goal Classification", "content": "The concatenated feature F is fed into three separate 3-layer perceptron heads, each responsible for predicting actions, objects, and receptacles. In action prediction, we classify actions among 7 interactive actions ('Pick up', 'Put', 'Open', 'Close', 'Toggle on', 'Toggle off', and 'Slice'), 'Navigate', and 'Stop'. In object prediction, we determine which object the agent should interact with from a pool of 108 objects. If the action prediction is 'Navigate', we predict the destination object. For receptacle prediction, we identify potential receptacles from a set of 38 receptacles, including 'empty' to signify the absence of a receptacle requirement."}, {"title": "IV. EXPERIMENTS", "content": "##### A. Dataset\nWe evaluated our model using the ALFRED [2] bench-mark, a comprehensive dataset designed to assess EIF. It demands a set of abilities, including comprehending natural language instructions, planning a sequence of actions to attain the goal, and navigating to the intended location, thus making it a notably challenging task. The ALFRED dataset comprises 8,055 expert demonstration episodes and is divided into training, validation, and test datasets. The validation and test datasets can be categorized into two distinct groups: \"seen\" datasets, referring to those whose scenes have been part of the training dataset, and \"unseen\" datasets, denoting those whose scenes have not been included in the training dataset. Different scenes refer to variations in the types of objects, their shapes, colors, and their relative positions. The ALFRED dataset encompasses a total of 120 distinct scenes, posing a significant challenge for the agents to adapt to diverse environments.\nWithin the ALFRED [2] dataset, there are a total of 25,743 language instructions. Multiple language instructions form a set to describe a single expert demonstration episode. To illustrate, in a task like cooling a plate and placing it at a receptacle, the corresponding instructions might include phrases like \"Put a chilled plate on the counter left of the sink\" or \"Grab the plate from the corner, cool the plate in the refrigerator, put the plate by the sink\".\n##### B. Baseline Methods\nWe compared HAPFI with three distinct models: FILM [5], CAPEAM [3], and E.T. [4].\n*   FILM: FILM [5], which utilizes a template-based ap-proach, is widely regarded as a representative model within the ALFRED [2] benchmark. It categorizes in-structions into seven distinct templates and predicts the corresponding actions, objects, and receptacles for the template predicted to be appropriate for the task. For instance, in the task of cleaning and placing an object, the template is structured as follows: [(Obj, PickUp), (SinkBasin, Put), (Faucet, ToggleOn), (Faucet, ToggleOff), (Obj, PickUp), (Recep, Put)], with the ital-icized components representing the predicted elements. FILM relies solely on high-level instructions as its data input without utilizing other modalities or historical information.\n*   CAPEAM: Recognized as the current or state-of-the-art model on ALFRED [2], CAPEAM [3] harnesses an LSTM-based technique for sub-goal planning. The out-put of CAPEAM is a sequence of subgoals that includes actions, objects, and receptacles. This model exclusively"}, {"title": "C. Results", "content": "The evaluation results are presented in Table I. Evaluation metric is the percentage of cases where the predicted action, object, and receptacle match the ground truth, relative to the total number of data points. The table clearly illustrates that HAPFI, with all modalities and histories, outperforms other models in action planning of the ALFRED [2] benchmark.\nThe disparity between HAPFI and both FILM [5] and CAPEAM [3] is rooted in the fact that HAPFI operates as a step-by-step model, setting it apart from the others. This distinctive approach enables HAPFI to consider both its previous actions and prior visual observations, leading to more effective action planning. Furthermore, while FILM and CAPEAM rely solely on high-level language instructions, HAPFI incorporates visual inputs alongside corresponding bounding boxes, making its planning process environment-aware. This is advantageous in action planning, as it allows the model to discern valuable relationships between executed actions and objects, as well as their respective locations.\nOn the other hand, the distinction between E.T. [4] and HAPFI can be attributed to the method of modality integra-tion. While E.T. employs a single multi-modal transformer encoder focusing on uni-modal output, our approach utilizes a deep fusion based methodology, involving two fusion stages and harnessing outputs from both visual and linguistic features. It's worth noting that although E.T. incorporates an attention mechanism within its multi-modal transformer, performing self-attention within mixed modality settings may not effectively facilitate mutual attention, unlike our approach which utilizes cross-attention between distinct modalities.\nThe disparity in performance between E.T. and HAPFI highlights the substantial impact of our early and deep fusion technique in enhancing action planning. It is also important to note that E.T. incorporates step-by-step instructions, which is generally less complex than high-level instructions.\nAblation Study. We conducted experiments involving HAPFI that lacked specific modalities or histories. The modalities and histories employed in each model, along with the respective results, are indicated in Table I. The results demonstrated that HAPFI, equipped with all modalities and histories, exhibits superior performance in comparison. We will proceed to provide an explanation of the results, starting from the first result from the table and moving downward, comparing with the result of the HAPFI with all modalities and histories.\nTo delve into the details, the performance of the model lacking visual information is inferior to that of the model with all modalities and histories. This is primarily because the model, without access to visual information, remains unaware of the current environmental context, thereby in-creasing the likelihood of failure. This highlights the critical role played by our multi-modality approach in enhancing the action planning process.\nIt is evident that a model without visual and sub-goal history experienced a substantial decrease in accuracy. No-tably, the decrease in accuracy for validation of unseen data was more pronounced when compared to validation seen data, contrasting with the model's performance that benefits from the utilization of both history types. This implies that incorporating history into the model enhances its robustness against overfitting to the training data. The absence of history confines the agent's perception to only the current scene, which can significantly differ from the training data, particularly in the case of validation unseen data. This incongruity can lead to model confusion due to"}, {"title": "D. Qualitative Analysis.", "content": "Through qualitative analysis, we show that the model is capable of understanding the nature of failures and, consequently, executing appropriate re-planning in response to these failures. An illustrative example of re-planning is provided in Figure 3. In this specific scenario, the given instruction entails placing a knife in a bowl and relocating it to a side table. After completing step 5, the expected course of action is to navigate to the side table with the knife in a bowl. We considered two potential failure scenarios here.\nIn one scenario, a navigation error led the agent to the wrong location, e.g., in front of the microwave in Figure 3. Upon examining Figure 3, it becomes evident that the agent has adeptly re-planned its course, redirecting to the side table after ending up at the microwave. In the other scenario, a manipulation error resulted in the agent dropping the bowl with the knife, causing the bowl and knife to be out of the agent's view. Figure 2 illustrates how the agent addresses this failure by picking up the knife again after dropping it. Our experiments have confirmed the model's ability to comprehend the procedural context it has followed. Without the model's awareness of its history and the current context, the model provides identical plans for both scenarios, lacking the capability to discern how and where it went wrong.\nIn summary, our experiments not only validate the model's aptitude for accurate performance within the correct context but also highlight its capacity to address failures through re-planning and adaptive problem-solving. This implies that our model is also applicable in real-world scenarios where the behavior of the agent is not perfect."}, {"title": "V. CONCLUSION", "content": "Throughout this paper, we delved into the challenge of Embodied Instruction Following (EIF). In response, we in-troduced HAPFI, a framework that leverages various types of historical data to enhance the quality of planning. This approach uniquely integrates multiple sources of modali-ties, creating a more informed agent. By doing so, HAPFI not only demonstrates the potential for improved planning capabilities but also underscores its capacity to identify agent failures and rectify them through suitable re-planning strategies. This paves the way for more cognitively adept and adaptable robotic systems, poised to navigate and interact within complex real-world environments with heightened proficiency and understanding."}]}