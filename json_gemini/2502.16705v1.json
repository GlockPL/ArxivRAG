{"title": "Can ChatGPT Learn to Count Letters?", "authors": ["Javier Conde", "Gonzalo Mart\u00ednez", "Pedro Reviriego", "Zhen Gao", "Shanshan Liu", "Fabrizio Lombardi"], "abstract": "Despite their amazing capabilities on many different tasks, Large Language Models (LLMs) struggle on simple tasks such as counting the number of occurrences of a letter in a word. In this paper we explore if ChatGPT can learn to count letters.", "sections": [{"title": "Introduction", "content": "Since the introduction of ChatGPT two years ago, Large Language Model (LLM) based tools have shown impressive capabilities to solve mathematical problems or to answer questions on almost any topic [1]. In fact, evaluation benchmarks have to be revised frequently to make then harder as LLM performance improves continuously [2].\nThe development of LLMs has also been hectic with new models presented by large companies such as Google with Gemini or Gemma, Meta with Llama or x.AI with Grok. OpenAI has also released newer versions and improvements of their Generative Pre-trained Transformer (GPT) family such as GPT4 [3] and its variants GPT4o and GPT401. Those foundational models are then adapted to answer questions or interact with users and complemented with other functionalities to implement conversational tools like ChatGPT.\nDespite these astonishing results, there are some simple tasks that LLMs struggle with, for example arithmetic operations [4] or even counting the occurrences of a given letter in a word. For example, many LLMs failed to count the number of \"r\" in strawberry The failure to perform this task stems from how LLMs are designed as they internally they do not work with letters.\nOff the shelf LLMs can be fine-tuned using examples to adapt then to a given application or to produce the output in a given format [5]. Therefore, an interesting question is whether LLMs can learn to count letters by fine-tuning the models. In this paper we explore this idea and show how developing it provides insights into how LLMs work and learn. The starting point is to understand how LLMs operate and why they can fail to count letters."}, {"title": "WHAT ARE LETTERS? LLMS ONLY KNOW ABOUT TOKENS", "content": "For humans, the basic element of a language are its letters that we learn in primary school and that we use subsequently to form words and to learn the language. This means that we are able to identify and count letters from an early age. Instead, LLMs do not have the concept of a letter, they operate on units called tokens.\nA token is a sequence of letters that appears frequently in the text, for example let us consider the word \"strawberry\" and the tokenizer of GPT4\u1edf. This tokenizer splits the word in three \"str\", \"raw\" and \"berry\" tokens. It can be seen that tokens do not necessarily correspond to words, syllables or letters.\nTo understand how tokens are constructed we have first to understand how LLMs work. LLMs simply predict the next token based on the previous text. Then use the previous text and the generated token as the input to the model and so on."}, {"title": "LLMs and tokens", "content": "The use of tokens means that LLMs do not operate with letters which makes it hard for them to count something they do not use. For example, to count the number of \u201cr\u201d in \u201cstrawberry\u201d the LLM would have to know that tokens \u201cstr\", \"raw\" and \"berry\" have 1,1 and 2 \"r\" respectively. Clearly by design LLMs will find hard to count letters. To check that is the case we have randomly selected 7,790 words from a public list of English words with an even distribution of the number of letters per word and asked GPT40 to count the number of \"r\" letters on each of the words. It can be observed that even a complex and powerful LLM like GPT4o has a significant failure rate when counting letters."}, {"title": "CAN WE TEACH LLMS TO COUNT LETTERS", "content": "One possibility to have LLMs count letters is to fine-tune the existing models to this task. As discussed before fine-tuning adjusts the model parameters using a much smaller dataset that the one using used to train the model [5]. In our case, the dataset could be a few thousand words for which we have counted the number of \"r\" letters, so we provide the word and the number of \"r\" in it to fine-tune the model.\nIn order to evaluate the effectiveness of fine-tuning, another subset of 7,790 words is selected, so having two such subsets of words. Then the first subset is used to fine-tune the model to learn to count the number of \"r\" in each word and the second subset to measure the failure rates for both the original and the fine-tuned models. It can be observed that the fine-tuned model has a much lower failure rate, close to zero. Instead the original GPT4o has a much larger failure. Therefore, LLMs can learn to count letters by fine-tuning. Since LLMs work with tokens, during the fine-tuning process, it seems that the model learns how many \"r\" are in each token so that given a sequence of tokens that correspond to a word it can infer the number of \u201cr\u201d in that sequence.\nLet us consider a person that is taught how to count \"r\u201d letters in a word. If later we ask that person to count the number of \u201ca\u201d letters or the number of \u201cm\u201d letters in a word, the person will be able to count those letters too, being able to generalize as she has learned how to count letters in general, not only \u201cr\u201d. Would the same hold for LLMs? Will the fine-tuned model reduce the failure rate when counting other letters?\nTo try to answer that question, the original GPT40 and the version fine-tuned to count \"r\" letters have been asked to count the number of \"a\" and \"m\" letters in the 7,790 test words."}, {"title": "LLMs generalization", "content": "observed that the failure rates are reduced also when counting \"a\" and \"m\" letters on the model fine-tuned to count \"r\" letters. This suggest that to some extent the LLM is learning how to count letters in general and not only \"r\", similarly to what persons do.\nThe analysis of a simple problem, why LLMs fail to count the \"r\" letters in a word, has served us to illustrate some important aspects of how LLMs process text and also how they can learn to perform specific tasks with fine-tuning.\nInterestingly, this simple problem also illustrates the capabilities of LLMs to generalize beyond what they have been fine-tuned for, similar to what happens with persons."}, {"title": "Conclusion", "content": "The problem of counting letters has no interest from a practical point of view, as a traditional computer program can count letters much more efficiently and some LLM based tools have also been adapted to be able to count letters. In fact, some newer models like GPT401 (apparently named internally \"strawberry\" prior to its release) provide advanced reasoning capabilities that can solve this and much more complex problems. However, analyzing why LLMs can fail to count letters and how they can learn to do it provides an intuitive and simple case study to help understand some of the limitations and capabilities of LLMs."}]}