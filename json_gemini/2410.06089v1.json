{"title": "TOWER: Tree Organized Weighting for Evaluating Complex Instructions", "authors": ["Noah Ziems", "Zhihan Zhang", "Meng Jiang"], "abstract": "Evaluating the ability of large language models (LLMs) to follow complex human-written instructions is essential for their deployment in real-world applications. While benchmarks like Chatbot Arena use human judges to assess model performance, they are resource-intensive and time-consuming. Alternative methods using LLMs as judges, such as AlpacaEval, MT Bench, WildBench, and InFoBench offer improvements but still do not capture that certain complex instruction aspects are more important than others to follow.\nTo address this gap, we propose a novel evaluation metric, TOWER, that incorporates human-judged importance into the assessment of complex instruction following. We show that human annotators agree with tree-based representations of these complex instructions nearly as much as they agree with other human annotators. We release tree-based annotations of the InFoBench dataset and the corresponding evaluation code to facilitate future research.", "sections": [{"title": "1 Introduction", "content": "The ability of large language models (LLMs) to follow human-written instructions is crucial for real world applications. Despite this, evaluating and fairly comparing the ability of these LLMs to follow instructions remains a significant challenge, with many proposed approaches. For instance, Chatbot Arena employs pairwise comparison with crowd sourced human judges from a diverse user base to score the performance of language models (Chiang et al., 2024; Zheng et al., 2023). Ideally, a model is judged purely on human feedback as in the case of Chatbot Arena, but this is generally expensive, takes lots of time, and is difficult to reproduce. Further, results can differ based on the selection of the annotators and the distribution of instructions used for evaluation.\nSome approaches such as AlpacaEval and MT Bench use LLMs instead of humans as judges, which significantly reduces cost, improves reproducibility, and is much faster (Dubois et al., 2024; Zheng et al., 2023; Lin et al., 2024).\nHowever, in practice many real-world use cases involve instructions which are much more complex than the ones found in many instruction following benchmarks. Some recent promising work has been done creating evaluation benchmarks for these complex instructions, which contain many atomic aspects. One such benchmark, InFoBench, breaks 500 instructions into decomposed requirements, and propose Decomposed Requirements Following Ratio (DRFR) as a metric for measure the percentage of requirements in the benchmark followed by a language model (Qin et al., 2024).\nDespite these improvements it remains unclear how to best measure performance on complex instruction following, as some aspect of a complex instruction may be more important than others.\nIn this work, we aim to remedy this issue by proposing a new complex instruction following evaluation metric, TOWER, which weights com-"}, {"title": "2 Related Work", "content": "The development of language models capable of following human-written instructions has seen significant advancement in recent years. A large variety of training approaches have been proposed for training these models (Touvron et al., 2023; Ouyang et al., 2022; Jiang et al., 2024; Wang et al., 2023; Ivison et al., 2023; Luo et al., 2023, 2024; Xu et al., 2024)\nSignificant research attention has been recently paid to evaluating the ability of LLMs to follow instructions. Some approaches rely on crowd sourcing pairwise human feedback to allow humans to directly judge which model's generation is preferred (Chiang et al., 2024). Many works have aimed to automate this process by using LLM's instead of humans to judge whether an instruction has been properly followed (Chiang et al., 2024; Zheng et al., 2023; Dubois et al., 2024). Some work has been done to evaluate the ability of language models to follow complex instructions which are made of many atomic aspects (Qin et al., 2024; Lin et al., 2024).\nHowever, relatively little work has been done to evaluate the importance of each aspect question within the context of the entire instruction and how these should be measured or evaluated."}, {"title": "3 Benchmark", "content": "Existing approaches for complex instruction following evaluation such as InFoBench rely on prompting a language model with a complex instruction. Unlike many prior approaches for evaluation of instruction following, a judge is then asked a series of questions about the generated text, each addressing one aspect of the complex instruction. For instance, if the instruction asks for a letter from a parent to their child that is warm and supportive, two aspect questions may be \"Is the generated text suitable from a parent to their child?\" and \"Is the letter warm and supportive?\". These aspect questions are created by human annotators prior to evaluation and are held constant for each model being evaluated.\nExisting metrics for complex instruction following such as Decomposed Requirements Following Ratio (DRFR) from InFoBench weight all instruction aspects equally (Qin et al., 2024). However, this is not ideal. In many cases it may be important to weight adherence to higher level aspects such as system-level instructions stronger than adherence to lower level aspects. To that end, we propose a new instruction aspect weighting scheme which aims to weight the individual aspects of a complex instruction by their relative importance.\nAs manually annotating the importance of each aspect is a lengthy and prohibitively costly process, we instead opt to use automated annotation and measure agreement with human evaluators. This also gives us freedom to explore a variety of different weighting approaches with minimal extra cost and to ensure reproducibility. We propose three different candidate approaches for automated labeling the relative importance of complex instruction aspects: Direct Scoring, Ranking, and Tree-based Weighting. In the end we find Tree-based importance weighting, which we refer to as TOWER, agrees best with human evaluators. Details on our human annotation process are found in Section 4.4."}, {"title": "3.1 Direct Scoring", "content": "In Direct Scoring, a LLM is directly prompted to score the importance of each aspect on a scale of 1-5, where 5 is the most important and 1 is the least important. The aspects are all scored in the same generation to ensure that their importance is relative to one another. It is important to note that, unlike ranking, multiple aspects can have the same score, reflecting that they are of equal importance."}, {"title": "3.2 Ranking", "content": "Similar to Direct Scoring, in Ranking a LLM is prompted to rank the importance of each aspect relative to each other. Unlike Direct Scoring, no two aspects can have the same level of importance."}, {"title": "3.3 Tree-based Weighting", "content": "In Tree-Based Weighting, a tree is constructed by an LLM based on the original complex instruction and the provided aspect questions. At the root of the tree are the most important aspects with the child and grandchild nodes often being modifications to the original root aspect. For instance, the aspect questions \"Is the generated text a letter?\" may be the parent of \"Is the generated letter warm and supportive\". In this setting, the weight for an aspect question is derived from the level it occurs within the labeled tree, where a parent has higher importance than the children. Specifically the weight is calculated by:\n$\\frac{1}{level(v)}$\nwhere v is the node of the aspect question within the tree.\nA diagram of this can be found in Figure 1."}, {"title": "4 Experiments", "content": "Here we discuss our experiments showing that our proposed Tree-Based weighting metric, which we refer from here on as TOWER, strongly aligns with human preferences and provides insight into the complex instruction following capabilities of various models."}, {"title": "4.1 Dataset", "content": "We use the instructions and aspect questions from InFoBench, a dataset comprised of 500 instructions and 2250 aspect questions with an average of 4.5 aspect questions per instruction. InFoBench also proposes an evaluation metric, Decomposed Requirements Following Ratio (DRFR), which measures the percentage of aspects which are correctly addressed by a language model's generations."}, {"title": "4.2 Models", "content": "Following InFoBench, we use a LLM to judge whether a given aspect of an instruction is followed for a particular generation. We use GPT-4-Turbo for its strong performance, speed, and relatively low cost compared to GPT-4.\nWe also use GPT-4-Turbo for our experiments involving automated ranking, direct scoring, and tree-based labeling of aspect questions discussed in Section 4.4.\nA number of different prompts are used to extract the tree structure of the instruction. We find JSON decoding with a simple example leads to the best quality trees. Our full prompt can be found in the project repository on GitHub."}, {"title": "4.3 Human Annotators", "content": null}, {"title": "4.4 Aspect Importance Experiments", "content": "To measure the agreement between our LLM weighted aspect importance and human-ranked aspect importance, we use Spearman's rank correlation as follows:\n$\\rho=1-\\frac{6 \\Sigma d_i^2}{n(n^2-1)}$\nwhere $d_i$ is the difference between the ranks of each pair of instruction aspects and n is the number of instruction aspects.\nWe use 4 human annotators to independently rank the importance of decomposed questions within the individual instructions. The annotators are recruited students unfamiliar with the project. The agreement between annotators and the human weighted aspect importance is shown in Table 1. Our results show human annotators have an average Spearman correlation of 0.74 with each other."}, {"title": "4.5 Model Evaluation Results", "content": "We test a number of language models to compare TOWER with DRFR.\nThe model evaluation results show some models are substantially better at addressing all aspects of a complex instruction while others address only the root instruction and struggle to address the lower level aspects. GPT-4-Turbo has a difference of only 0.92 between the Tree-Based Weighting metric and DRFR, indicating that it addresses all aspects equally and does not tend to over emphasize aspects at the root or the leaves. Mixtral 8x7B on the other hand has a large difference of 6.86, indicating that it is much better at addressing aspects at the root of the complex instruction tree as opposed than those at the leaves.\nTo further directly compare DRFR and TOWER, we also experiment by sampling models multiple times with a temperature of 0.8 and use both DRFR and TOWER as the evaluation metric. We then find the instances where DRFR and TOWER have the largest gap and have human annotators pick which generation has the highest quality. We find human annotators prefer the same examples as TOWER 68% of the time, indicating that TOWER aligns much better with human preferences than DRFR."}, {"title": "4.6 Tree-Based Evaluation Analysis", "content": "Shown in Figure 2, we further explore the performance gap between DRFR and Tree-Based Weighting of complex instructions. We show the percentage of aspects correctly addressed for GPT-4-Turbo, Llama-3-70B, and Mixtral 8x7B at the various levels of the complex instruction tree. GPT-4-Turbo shows a relatively consistent performance across all levels of the decision tree, with performance degradation beginning around Level 5. Llama-3-70B shows a similar consistent performance, but with degradation beginning instead around Level 4. Surprisingly, Mixtral 8x7B performance degrades after only the first level, indicating that it does not reliably follow instruction aspects at the lower levels of the trees compared to the higher level ones."}, {"title": "5 Conclusion and Future Work", "content": "In this paper we explore a new metric for evaluating large language models' ability to follow complex instructions. Surprisingly, we find tree-based representations have nearly the same Spearman correlation with human annotators (0.72) that human annotators have with themselves (0.74), indicating that tree-based methods are a strong candidate for representing and evaluating complex instructions.\nBased on this observation our new proposed metric, TOWER, provides strong insights into which models are better at addressing the most important aspects of complex instructions.\nThere are numerous exciting directions for future work. While progress is being made on designing new metrics for evaluating complex instruction following, more benchmark datasets focused on complex instructions are needed as few currently exist in this domain. Additionally, finding ways to reduce evaluation costs is crucial, as evaluating a single model in InFoBench currently costs approximately $15."}, {"title": "Limitations", "content": "Despite solving many problems with existing evaluation metrics for complex instruction following, our approach is not without drawbacks. First and foremost, it is significantly more expensive than InFoBenches DRFR, as pairwise comparison requires many more tokens in the input than single point evaluation. Further, it is more expensive than AlpacaEval as each individual aspect question requires it's own pairwise evaluation, meaning each instruction will need many pariwise evaluations instead of just one. As with all evaluation approaches which rely on using LLMs as judges, our approach is susceptible to bias in the LLM, such as preferring longer generations or preferring it's own generations."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Aspect Evaluation Prompt", "content": "For consistency of LLM evaluation we use the same prompt used in InFoBench (Qin et al., 2024) where an optional input is provided along with the generated text and a final aspect question used for evaluation. The prompt template for this is shown in Table 3."}, {"title": "A.2 Tree Decomposition Prompt", "content": "For decomposing complex instructions into a tree structure, we test many different prompts and find the prompt shown in Table 4 with JSON decoding works well."}]}