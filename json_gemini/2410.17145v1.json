{"title": "Can General-Purpose Large Language Models Generalize to English-Thai Machine Translation?", "authors": ["Jirat Chiaranaipanich", "Naiyarat Hanmatheekuna", "Jitkapat Sawatphol", "Krittamate Tiankanon", "Jiramet Kinchagawat", "Amrest Chinkamol", "Parinthapat Pengpun", "Piyalitt Ittichaiwong", "Peerat Limkonchotiwat"], "abstract": "Large language models (LLMs) perform well on common tasks but struggle with generalization in low-resource and low-computation settings. We examine this limitation by testing various LLMs and specialized translation models on English-Thai machine translation and code-switching datasets. Our findings reveal that under more strict computational constraints, such as 4-bit quantization, LLMs fail to translate effectively. In contrast, specialized models, with comparable or lower computational requirements, consistently outperform LLMs. This underscores the importance of specialized models for maintaining performance under resource constraints.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have shown remarkable capabilities in Neural Machine Translation (NMT) and code-switching (CS), attributed to their robustness and generalization (Vaswani et al., 2013; Naveed et al., 2024; Radford et al., 2019). Recent studies indicate that NMT and CS are largely solved for LLMs in high-resource languages (Zhang and Zong, 2020; Hamed et al., 2017; Zhou et al., 2020). However, our research reveals that this performance fails to generalize to low-resource and low-computation settings, which is critical for real-world settings where computational resources are constrained.\nThis paper explores the generalization of LLMs through two research questions: (i) How do general-purpose LLMs and specialized translation models generalize to low-resource language translation? (ii) How do real-life computational constraints affect performance metrics? To address these questions, we experiment with Llama-3 in various quantization settings. Additionally, we compare LLMs with specialized translation models like NLLB (Team et al., 2022) to evaluate performance and efficiency trade-offs."}, {"title": "2 Experimental Setup", "content": "Datasets. We evaluated two translation datasets: (i) a proprietary medical CS translation dataset\u00b9, containing 63,982 English-Thai sentence pairs with retained English medical terms; and (ii) scb-mt-en-th-2020 (Lowphansirikul et al., 2021), a 1,001,752 sentence pair English-Thai translation dataset, from which we randomly selected 63,982 pairs to match the sample size of the CS dataset.\nModels Our evaluation focused on three models pertinent to our research questions: Llama-3 8B (Meta, 2024), NLLB-600M, and NLLB-3.3B (Team et al., 2022). For the Llama-3 model, we assessed both the pre-trained and finetuned versions, with the latter quantized to 2, 3, 4, and 8 bits using GPTQ (Frantar et al., 2022). For the NLLB models, we evaluated both pre-trained and finetuned versions. All were finetuned for 3 epochs with a learning rate of 2e-4 on an A100 GPU.\nMetrics We employed standard MT metrics for evaluation, such as BLEU3, METEOR, and CER. Additionally, we measured the CS boundary F1 score, which is the harmonic mean of precision and recall for correctly preserved English terms (Sterner and Teufel, 2023).\nLLM-as-a-judge Evaluation. To analyze performance degradation, we used GPT4-02 as a judge with 3-shot prompting to identify failure modes in each predicted translation. GPT4-o received the source, target, and predicted sentences. The LLM judge assigned a multiple-choice label to each translation, categorizing them as \"Forgot to translate,\" \"Meaning changed,\" \"Gibberish,\" or \"Excellent,\" with a \"Keywords not preserved\" category for the CS translation task."}, {"title": "3 Results", "content": "As illustrated in Table 1, NLLB-3.3B and NLLB-600M outperform Llama-3 8B on most metrics, despite using 2.35x and 10.81x less VRAM, respectively. This contrasts with prior studies indicating the superiority of general-purpose language models in specialized, low-resource tasks (Li et al., 2023; Nori et al., 2023; Naveed et al., 2024). Moreover, the average percentage difference between NLLB-3.3B and full-precision Llama-3 8B across BLEU and METEOR scores is ~23.39% and ~1.33% for the SCB and CS dataset, respectively. This minimal difference for the CS dataset suggests that NLLB's multilingual pre-training is not a significant advantage in translation-adjacent tasks.\nInterestingly, Llama-3-8B excels in the METEOR metric for CS translation, which accounts for word stems and synonyms. This suggests Llama-3-8B produces relevant but imprecise translations, affecting metrics that require exact matches but not METEOR."}, {"title": "4 Analysis", "content": "Failure Analysis As shown in Figures 1a and 1b, we observed a divergence in failure modes between the two datasets. For the SCB dataset, errors initially rise in the \"Meaning changed\" category (from 16 to 4 bits) and then in the \"Gibberish\" category (from 4 to 2 bits). In the CS dataset, errors first increase in the \"Meaning changed\" category while decreasing in \"Keywords not preserved\" (from 16 to 4 bits), followed by an increase in \"Gibberish\" errors (from 4 to 2 bits). Notably, the best-performing models (NLLB-3.3B and NLLB-600M) exhibit the highest number of \"Forgetting to preserve\" errors. This suggests an alternative failure mode in CS translation, where top models first lose the ability to preserve medical keywords, then to translate accurately, and finally to translate at all. Importantly, despite higher errors in the \"Forgetting to preserve\" category, NLLB models perform better on the CS-F1 metric, Table 1, highlighting the importance of task-specific metrics.\nImpact of Quantization Interestingly, CS results show greater resilience to quantization than SCB results. Across BLEU, CER, and METEOR metrics, CS translation results experience less degradation than SCB results when compared against the full-precision baseline. This may be due to the early loss of complex Thai vocabulary during quantization, while complex English vocabulary, rewarded in the CS task, is better preserved. The resilience of CS results suggests a novel approach for mitigating performance degradation in quantized multilingual models by leveraging CS outputs."}, {"title": "5 Conclusion", "content": "We study the performance of general-purpose and specialized language models on translation and translation-adjacent tasks. Our findings indicate that specialized translation models outperform general-purpose models, although the performance gap is smaller for CS translation. As models undergo increased quantization, the divergence in failure modes between SCB and CS datasets underscores the importance of task-specific metrics."}, {"title": "6 Appendix", "content": "6.1 Finetuning Prompts for Llama\nCode-switching (CS) Prompt\nYou are a helpful code switching English to Thai language translation assistant. Translate the given English texts to Thai while preserving the medical keywords.\nMachine translation (SCB) Prompt\nYou are a helpful English to Thai language translation assistant. Translate the given English texts to Thai.\n6.2 LLM Judge Prompts\nLLM Judge Code-switching Dataset Prompt\nYou will be given a user_text, model_answer, and system_translation trio. Your task is to provide a multiple choice answer, analyzing the cause of failure of the system's translation of the user's text when compared to the model_answer.Give your answer letter which can either be A, B, C, D, E.\nHere are the choices.\nA: The system_translation forgot to translate: missed translating a large part of the text\nB: The system_translation translated wrongly: adds additional information or hallucinates; changes the meaning in some significant way\nC: The system_translation is gibberish: it does not make sense and is just a jumble of words and characters\nD: The system_translation forgot to preserve the CS keyword: although the text is translated ; the meaning is quite well preserved; the keywords are translated amd not preserved in the orignal language\nE: The system_translation is excellent: preserves the keywords; has almost the meaning as the model answer; everything except for the keywords are translated\nYou MUST provide the answer letter. Do not provide anything else."}]}