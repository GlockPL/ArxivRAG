{"title": "Dual-Layer Training and Decoding of Large Language Model with Simultaneously Thinking and Speaking", "authors": ["Ningyuan Xi", "Xiaoyu Wang", "Yetao Wu", "Teng Chen", "Qingqing Gu", "Jinxian Qu", "Zhonglin Jiang", "Yong Chen", "Luo Ji"], "abstract": "Large Language Model can reasonably understand and generate human expressions but may lack of thorough thinking and reasoning mechanisms. Recently there have been several studies which enhance the thinking ability of language models but most of them are not data-driven or training-based. In this paper, we are motivated by the cognitive mechanism in the natural world, and design a novel model architecture called TaS which allows it to first consider the thoughts and then express the response based upon the query. We design several pipelines to annotate or generate the thought contents from prompt-response samples, then add language heads in a middle layer which behaves as the thinking layer. We train the language model by the thoughts-augmented data and successfully let the thinking layer automatically generate reasonable thoughts and finally output more reasonable responses. Both qualitative examples and quantitative results validate the effectiveness and performance of TaS. Our code is available at https://anonymous.4open.science/r/TadE.", "sections": [{"title": "1 Introduction", "content": "Large language model (LLM) has recently garnered significant recognition for their ability to generate contextually appropriate text, excelling across various NLP tasks like translation, summarization, and dialogue (Naveed et al., 2024). However, most of these models function as end-to-end systems, often bypassing the explicit reasoning processes integral to human communication. Incorporating a more human-like reasoning framework-where logical deliberation precedes expression-could enhance the sophistication, precision, and depth of AI-generated content.\nResearch has explored the thinking and reasoning behaviors of LLMs. For instance, the Chain-of-Thought (Jason Wei, 2022) helps LLMs generate intermediate reasoning chains, which greatly improves their few-shot performance on complex and multi-step problems. Similarly, SimToM (Alex Wilf, 2022) excels in Theory of Mind (ToM) (Premack and Woodruff, 1978) tasks by using perspective-taking to filter context and simulate a specific perspective. In addition, Think Twice (Yushan Qian, 2023) mimics human emotional reasoning by revising responses based on potential emotional reactions. STaR (Eric Zelikman, 2022a) bootstraps the reasoning process of LLM to solve math-like question-answering problems. Furthermore, Quiet-STaR (Eric Zelikman, 2022b) delineates reasoning tokens and utilizes the REIN-FORCE algorithm to train language models, ensuring they engage in deliberate and thoughtful reasoning before response generation.\nDespite these progresses, several shortcomings remain. First, most of these works are limited to"}, {"title": "2 Method", "content": "In this section, we first reformulate the instruction finetuning template, then propose a training framework that allows our LLM to generate thoughts."}, {"title": "2.1 Training Data Format", "content": "Starting from the OpenAI ChatCompletion prompt, we augment the original roles (system, user, response) with a new role called 'think'. Below is the resulting prompt format:"}, {"title": "2.2 Dual-Layer Fine-Tuning", "content": "Given the prompt-response samples, the loss of conventional Supervised Fine-Tuning (SFT) can be expressed as follows:\n$\\mathcal{L}_{S F T}=\\frac{1}{L} \\sum_{i=1}^{L} \\log \\left[P\\left(r_{i} | q, r_{1}, \\ldots, r_{i-1}\\right)\\right]$ (1)\nin which q is the query, ri is the i-th token of response, and L is the response length.\nAssume there are total K attention layers in the LLM. We first select the k-th layer as the thought-generating layer (0 < k < K), and denote the thought as the variable t. Before our training starts, we also implement the language heads in the k-th layer, and copy the weight values from the original"}, {"title": "3 Experiments", "content": "In this section, we first provide the experimental settings, then show the training process, and finally exhibit insightful qualitative and quantitative results."}, {"title": "3.1 Settings", "content": "We perform the training experiment in LlamaFactory (Zheng et al., 2024), running by 8 A100 GPUs. We use the AdamW optimizer with the cosine scheduler of learning rate and decay of 0.01. We first train the model with some open-domain dialogue and reasoning datasets then conduct some detailed downstream finetuning tasks. Details of training datasets are listed in the Appendix.\nIn this study we choose QWEN2-7B-Instruct (Qwen Team, 2024) as the base model, which has total K = 28 layers and we choose k = 20 and $f_T$ = 0.9. The total learning rate is 2.0e \u2013 6, the training batch size is 16 and the sequence window length is 2048."}, {"title": "3.2 Training Result", "content": "Figure 3 shows the training curves of the thinking loss and the expressing loss. Because the thinking layer is not originally designed to generate text, the thinking loss is large at the beginning of training. Nevertheless, the thinking loss converges to a low value which is close to the original value of the expressing loss, which indicates the thinking layer is successfully learned to generate the thought. Furthermore, the expressing loss also decays to a lower value, because the final layer switches from decoding from query to decoding from both query and"}, {"title": "3.3 Qualitative Examples", "content": "After Stage 1 of training, we observe both losses of expressing and thinking converge. Finally, not only the mid layer of LLM learn the text generating ability, but also we obtain reasonable thought and corresponding expression given the query. Table 1 provides some typical cases. Our TaS can also generate thoughts with some reasoning ability, with the corresponding case in the Appendix."}, {"title": "3.4 Results on Theory-of-Mind Capabilities", "content": "TOMI (Le et al., 2019) and BIGTOM (Gandhi et al., 2023) provide Sally-Anne false belief benchmarks which can verify the ToM capability. In this scenario, the LLM is provided with a multi-role scenario and also is assigned with a specific role. The LLM needs to study the case only from the ego-centric perspective and get the final conclusion."}, {"title": "4 Limitation", "content": "Compared with TaS, another natural idea is to utilize two LLMs to form a \u2018thinking and speaking' agent. The agent and training-based methodological comparison would be insightful. Also, more theoretical and qualitative comparisons with some recent progress (e.g. gpt-401) might shed more light on the development of future research."}, {"title": "5 Conclusion", "content": "In this paper, we propose a novel training paradigm called TaS, in which we synthetic human thoughts, and then use them to supervise the hidden layer of LLM to simultaneously generate thoughts and talks. TaS produces interesting thinking behavior and reasonable responses. The architecture of TaS might be the basis for implementing thinking modules with decoder-only models."}, {"title": "A.1 Training Dataset Details", "content": "Table 3 lists all the datasets we used during the first and the second stages, covering different tasks. Stage 1 means the task-mixing stage, and stage 2 corresponds the dataset-specific finetuning."}, {"title": "A.2 Synthesizing Thoughts of Multi-Turn Dialogue", "content": ""}, {"title": "A.2.1 Auto-Generation by LLM", "content": "The following prompt is utilized to generate the content of thought given a dialogue."}, {"title": "A.2.2 Rule-Based Generation", "content": "ESconv (Liu et al., 2021) is a multi-turn dialogue dataset with each turn annotated with user emotion and the support strategy (or skill) of response. Given the original dataset with the format of (Query, Emotion, Strategy, Response), we build the thought of each dialogue turn based on the following template:"}, {"title": "A.2.3 Human Annotation", "content": "We develop an annotation tool for human labelers to efficiently annotate human thoughts, as a complement of auto-generation of thoughts. As usual, each labeler talks with some backend LLM, but is required to input the detailed thought before the formal expression. It is asked that the thought should be content-related with the expression, and might imply more details that might not be suitable to speak directly. During the training, we shift the dialogue sample with one turn such that the LLM plays as the 'user' and the human labeler plays as the 'assistant'."}, {"title": "A.3 Rule-Based Generation of COT Thoughts", "content": "COT usually encourages LLM to generate rationale content before output the final answer, by some prompt such as 'Let's think step by step'. Here we transform the COT sample (Query, Rational, Answer) into the TaS data by the following template:"}, {"title": "A.4 Rule-Based Generation of ToM Thoughts", "content": "The original TOMI and BIGTOM are in the format of (Story, Character, Question, Answer). We first"}, {"title": "A.5 Typical Cases of Multi-Step Reasoning and Planning", "content": "Table 4 provides a typical case, in which TaS can first make the scratchpad consideration within the thought content, and then generate the final multi-step planning result."}, {"title": "A.6 Analysis", "content": "Figure 4 shows the model parameter differences between the original Base and Chat model, and Figure 5 shows the model parameter differences between the original Chat model and TaS.\nIt is evident that after undergoing Supervised Fine-Tuning, the base model and the chat model exhibit differences in parameters across all layers, although the magnitude of these differences is not uniformly distributed across layers. In contrast, the parameter differences between the TaS model and the chat model are primarily concentrated within the first 20 layers (since we select k = 20). This indicates that our supervised fine-tuning of the intermediate layers has been effective, successfully altering the output logic of the intermediate layers, as reflected in the parameter differences from the base model."}]}