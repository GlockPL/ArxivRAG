{"title": "STTS-EAD: Improving Spatio-Temporal Learning Based Time Series Prediction via Embedded Anomaly Detection", "authors": ["Yuanyuan Liang", "Tianhao Zhang", "Tingyu Xie"], "abstract": "Handling anomalies is a critical preprocessing step in multivariate time series prediction. However, existing approaches that separate anomaly preprocessing from model training for multivariate time series prediction encounter significant limitations. Specifically, these methods fail to utilize auxiliary information crucial for identifying latent anomalies associated with spatiotemporal factors during the preprocessing stage. Instead, they rely solely on data distribution for anomaly detection, which can result in the incorrect processing of numerous samples that could otherwise contribute positively to model training. To address this, we propose STTS-EAD, an end-to-end method that seamlessly integrates anomaly detection into the training process of multivariate time series forecasting and aims to improve Spatio-Temporal learning based Time Series prediction via Embedded Anomaly Detection. Our proposed STTS-EAD leverages spatio-temporal information for forecasting and anomaly detection, with the two parts alternately executed and optimized for each other. To the best of our knowledge, STTS-EAD is the first to integrate anomaly detection and forecasting tasks in the training phase for improving the accuracy of multivariate time series forecasting. Extensive experiments on a public stock dataset and two real-world sales datasets from a renowned coffee chain enterprise show that our proposed method can effectively process detected anomalies in the training stage to improve forecasting performance in the inference stage and significantly outperform baselines.", "sections": [{"title": "I. INTRODUCTION", "content": "Multivariate time series (MTS) forecasting focuses on accurately predicting time series data comprising multiple interrelated variables and plays a vital role in various industries, including power consumption forecasting [1], stock price prediction [2], and sales forecasting [3]. MTS comprises multiple univariate time series, each representing a metric from a specific entity, and involves temporal dependencies within each series and spatial dependencies among series [4]. The future of a variable depends on its own history and the combined histories of others. Compared to univariate forecasting, MTS forecasting is more complex due to spatio-temporal dependencies, dynamic changes, and noise. Temporal dependency captures patterns over time, reflecting how history shapes the future, improving trend and fluctuation modeling. Spatial dependency reveals interactions and corre-"}, {"title": "II. RELATED WORK", "content": "A. Time Series Forecasting\nTime Series Forecasting involves univariate and multivariate methods. Univariate methods analyze and forecast individual time series separately, offering simplicity and interpretability. ARIMA[8], a widely used univariate method, combines au-toregression and moving averages, performing well on sta-tionary data. However, for more complex forecasting tasks with multiple variables, univariate methods fail to capture complex relationships and may not provide accurate results. These methods cannot explore the correlations between vari-ables. In contrast, multivariate methods consider multiple time series simultaneously, modeling both intra- and inter-metric dependencies. This allows for better prediction accuracy by capturing the dynamic relationships between series. Research on multivariate time series prediction is growing, with neu-ral network-based methods like RNNs, CNNs, Transformers, MLPs, and GNNs emerging.\nRNNs are designed for sequence data, retaining historical information for time series prediction. LSTM models improve on RNNs by using memory cells and gates to manage infor-mation flow, addressing issues like vanishing gradients. GRU models simplify LSTMs by combining memory cells with reset and update gates. Bi-RNNs [9] process sequences in both directions, capturing dependencies from past and future data .\nCNNs, initially for image processing, are now adapted for time series forecasting. Dilated convolutions improve sequence pattern recognition. TCN [10] and WaveNet [11] use dilated convolutions and residual connections to capture long-term dependencies. LSTNet [12] and ConvLSTM [13] combine CNNs and RNNs for both short- and long-term dependen-cies, while TimesNet [14] uses Fourier transforms for multi-periodic modeling.\nTransformers [15], designed for NLP, are applied to time series forecasting. Models like Autoformer [16], Preformer [17], and FEDformer [18] enhance learning with decomposition and improved attention mechanisms. Autoformer introduces self-correlation, Preformer uses Multi-Scale Segment Corre-lation (MSSC), and FEDformer employs Fourier transforms. ETSformer improves accuracy with Exponential Smoothing Attention (ESA), while Informer [19] reduces complexity with ProbSparse attention. Pyraformer [20] uses pyramid-style attention for hierarchical transmission.\nRecently, [21] proposes using simple models like MLPs to approximate the complexity of Transformer-based mod-els for time series prediction. MLP-based models, including NLinear, which combines time series decomposition with linear layers, and DLinear, which uses linear layers with simple data normalization, have recently achieved state-of-the-art performance [21]. LightTS [22] introduces continuous and interval sampling for capturing short and long-term patterns, especially in lengthy sequences. MTS-Mixer [23] decomposes time and channel mixing to capture dependencies separately. TiDE [24] employs MLPs to encode past series and covariates, then decode for future predictions.\nLastly, GNNs enhance spatial understanding in multivariate time series data, improving predictions. MTGNN [25] models time series as graphs, using GNNs to learn dynamic rela-tionships for accuracy. StemGNN [26] combines time series with graphs, capturing frequency and temporal relationships. Graph WaveNet [27] integrates GNNs and temporal convolu-tional networks for precise predictions. ST-GCN [28] performs convolutions in spatial and temporal dimensions, suitable for various prediction tasks."}, {"title": "B. Time Series Anomaly Detection", "content": "Time Series Anomaly Detection methods can be divided into two categories according to the purpose of detection [29]. Research is active on the purpose of the event of interest, and this family is divided into prediction-based and reconstruction based approaches. Hundman et al. [30] used an LSTM based model for spacecraft anomaly detection, introducing an adaptive non-parametric dynamic thresholding method. Munir et al. [31] proposed DeepAnT, employing a CNN based model to detect various anomalies in multivariate time series. CHMM [32] captures time dependencies and variable correlations incrementally. Shipmon et al. [33] combined deep learning and statistics for anomaly detection in streaming data, utilizing rules based on predicted and actual values, tailored to each data stream. While AE [34] and VAE [35] are widely employed in reconstruction-based anomaly detection. EncDec-AD [34] utilizes an LSTM-based encoder-decoder architecture tailored for multivariate time series anomaly de tection. It learns normal time series reconstruction patterns and detects anomalies through reconstruction errors. MSCRED [36] combines multi-scale feature matrices, ConvLSTM net works, and convolutional decoders with residual matrices for anomaly detection, root cause analysis, and severity inter pretation. OmniAnomaly [35] uses techniques like random variable coupling and normalization flow to reconstruct inputs, capturing normal patterns and detecting anomalies with recon struction probabilities for robust explanations. MTAD-GAT [37] jointly optimizes predictive and reconstructive models, improving multivariate time series representations for anomaly detection. Alternative methods include AnoGAN [38], which uses DCGAN for unsupervised anomaly detection in medical images, and MAD-GAN [39], which adapts AnoGAN for time series data with an LSTM-based GAN and a novel DR score for anomaly detection. TAnoGAN [40] maps time series data into latent space and reconstructs it through adversarial training. USAD [41] trains an encoder-decoder architecture with adversarial training to amplify reconstruction errors in input data containing anomalies, offering higher stability than GAN-based methods. DAEMON [42] employs adversarial training with two discriminators on an autoencoder to detect anomalies through reconstruction errors, ensuring robustness.\nCurrent methods for preprocessing anomalies in training data are limited, often relying on statistical techniques like z score [43] or box plots [6] to assess deviations from the overall distribution. However, these methods may not be effective for time series data due to its seasonal and cyclical nature. For time series anomaly detection, most methods focus on individual time series. Simple approaches use constant or piecewise constant models with sliding windows and median based references [44], [7]. Smoothing techniques like B-spline [45], EWMA (Exponentially Weighted Moving Average) vari ants [46], and methods such as SCREEN [47] and those by Zhang et al. [48] employ slope constraints for streaming data, allowing for rapid anomaly detection and correction."}, {"title": "III. METHOD", "content": "A. Problem Statement and Overall Architecture\nAs shown in Figure 2, the raw MTS data is represented as $S \\in R^{N \\times T}$, consists of N individual time series ${S_i | i = 1,...,N}$. The N denotes the number of entities, also the feature dimension of the MTS data, and T denotes the length of the MTS data. Typically, N remains constant, but in certain scenarios, the number of entities dynamically changes over time. For instance, in the context of forecasting daily sales, each store represents an entity, and the daily sales of all stores form the MTS data. As the number of stores varies over time, the value of N is not fixed. To accommodate the dynamic evolution of the number of entities and harness inter-metric spatial information, we transform the raw MTS data into an input representation denoted as $X = {X_{i,t} | 1 \\le i \\le N, 1 \\le t \\le T}$. $X_{i,t} \\in R^{N \\times P}$ is processed as a sliding window input sample, where P is the sliding window length, and t and i denote the timestamp and target prediction entity index, respectively. To construct $X_{i,t}$, the window data of the target prediction series i is placed in the first dimension, with the remaining series concatenated as auxiliary series in subsequent dimensions to provide auxiliary spatial information.\nIf prior entity information is available, it can be used to create a spatial graph $G \\in R^{N \\times N}$ as an adjacency matrix, representing relationships among entities. For example, in sales forecasting, entities can be connected based on features like store category, city, or region. However, due to the flexibility and scalability of the proposed method, the model performs well even without prior spatial information. Thus, constructing G is beneficial but not essential.\nThe above formulation of the data aims to accomplish two goals as following:\n\u2022 Time Series Forecasting: works on both training and inference phase, given X (and G), aims to build a mapping $f(X, G, \\phi)$ to predict Y , the value of target time series at the next timestamp, $\\phi$ is the model parameters. The formal representation is as follows:\n$f(X, G, \\phi) \\rightarrow Y$ (1)\n\u2022 Anomaly Detection: only works during training to cor rect anomalies in the training data, given the input X (and G), the real target value Y , the goal is to detect anomalies through a mapping $g(X, G, Y, \\phi)$. In the detection phase, the model parameters $\\phi$ are constant.\n$g(X, G, Y, \\phi) \\rightarrow C$ (2)\nDuring each iteration, only the training data is optimized for anomaly detection, with fixed model parameters. Conversely, when optimizing the prediction model, only the model param eters are updated, while the training data remains unchanged. The STTS-EAD model consists of two main components: the Spatio-Temporal Learning-based Time Series prediction model (STTS) and the Embedded Anomaly Detection module (EAD), as shown in Figure 2. The EAD module improves the STTS model by detecting and handling anomalies using"}, {"title": "B. Model Architecture of STTS", "content": "The right part of Figure 2 shows the STTS model for time series prediction, consisting of key components: Spatiotempo-ral Embedding Construction, Auxiliary Series Selection, Spa-tiotemporal Feature Learning, Integrated Feature Learning, and a Predictor and Reconstructor. The Spatiotemporal Embedding Construction module combines temporal and spatial embed-dings for each series. The Auxiliary Series Selection module uses these embeddings to select relevant time series, reducing computation and adapting to dynamic MTS data. The Spa-tiotemporal Feature Learning module captures temporal and spatial dependencies, while the Integrated Feature Learning module refines these features. The high-level features are then used by the Predictor for predictions and the Reconstructor for reconstruction, with errors provided to the EAD module for anomaly detection. Overall, STTS captures spatio-temporal patterns and produces accurate predictions.\n1) Spatiotemporal Embedding Construction Module: The spatiotemporal embedding construction module constructs spatiotemporal embedding for each time series, which aims to learn the features of entities and ensures similar or related time series have similar embedding, while unrelated time series have distinct embedding. The spatiotemporal embedding is concatenated by the temporal embedding $E_{time} \\in R^{d_1}$ learned from historical time series data and the spatial embedding $E_{spat} \\in R^{d_2}$ learned from prior graph information. $d_1$ and $d_2$ are dimensions of the corresponding embedding. For the temporal embedding $E_{time}$ of each entity, a momentum encoder is employed to update it as follows:\n$E_{time} = \\gamma E_{time} + (1 - \\gamma) f(X)$ (3)\nThe former term uses the temporal embedding before iteration to represent the overall temporal characteristics and the latter uses an MLP layer f to map local time window data X to the local temporal characteristics and then adds the two parts with the fixed weight y to update the temporal embedding. The weighted summation of local and global information"}, {"title": "2) Auxiliary Series Selection Module", "content": "allows the temporal embedding to not only incorporate short-term information from the current window but also retain long-term global sequential information accumulated from historical windows. And the incremental update approach en-hances the stability and robustness of the temporal embedding construction. The $E_{time}$ is randomly initialized at first.\nAs for the spatial embedding construction, if prior graph knowledge G is available, $E_{spat}$ is encoded through a GCN [49] network. The network aggregates the information of the connected neighbor entities, therefore spatial embedding can learn the dependency of similar and related entities. After the construction of temporal embedding and spatial embedding, the spatiotemporal embedding denoted as $E = [E_{time}, E_{spat}] \\in R^{d}, d = d_1 + d_2$ which formed by concatenating the tem-poral and spatial embedding. In addition, when prior graph knowledge G is not provided, the spatiotemporal embedding will solely consist of temporal embedding: $E = E_{time}$ and $d = d_1$. Excluding the calculation method of spatiotemporal embedding, the subsequent module operations will be identical regardless of whether G is provided or not.\n2) Auxiliary Series Selection Module: The auxiliary series selection module selects relevant time series as external infor-mation for target series prediction and anomaly detection for three reasons. First, when the number of time series changes dynamically, this module selects a fixed number of series as input, allowing the model to adapt to dynamic MTS data. Sec-ond, when MTS data dimensionality is too high, predicting all series simultaneously becomes difficult. A common solution, as in [30], is to model each series independently, but this approach doesn't leverage spatio-temporal information from external series. Instead, we calculate the correlation between series using spatiotemporal embeddings and select the most relevant series to reduce computational costs and better utilize external spatio-temporal information. Finally, for the anomaly detection module in III-C, the single target series prediction approach eliminates the need for anomaly attribution across multiple time series.\nThis module calculates the correlation between other series j and target series i based on spatiotemporal embedding as follows:\n$r_{ij} = \\frac{E_i \\cdot E_j}{||E_i|| \\cdot ||E_j||}, j \\in {1, 2, ..., N}$ (4)\nThe $r_{ij}$ is the correlation between two series. $E_i$ and $E_j$ are spatiotemporal embeddings of series i and j respectively, and $|| ||$ denotes vector norm. The module outputs the time series data of M series (including the target prediction series itself) with the highest similarity to the target series i, represented as $X' \\in R^{M \\times P}$ based on the similarity $r_{ij}$. M is a manually selected hyperparameter."}, {"title": "3) Spatiotemporal Feature Learning Module", "content": "3) Spatiotemporal Feature Learning Module: The spa-tiotemporal feature learning module comprises a temporal attention block and a spatial attention block, designed to effectively capture and model both intra-metric and inter-metric dependencies with attention mechanisms. After filtered by the auxiliary series selection module, X' is input into the spatiotemporal feature learning module. For the input $X'$, it can be considered as a matrix of shape $M \\times P$, where each row corresponds to the time series window data of a series (M series in total) and each column represents cross-sectional features of a timestamp (P timestamps in total). We denote the i-th row as $X_{i,:} \\in R^{P}$, representing the i-th time series, and the i-th column represents the i-th timestamp feature denoted as $X_{:,i} \\in R^{M}$. For Spatiotemporal Feature Learning, GATv2 [50] is employed to learn the temporal representations $H_{time} \\in R^{M \\times P}$ and spatial representation $H_{spat} \\in R^{M \\times P}$. These representations are obtained via weighted summation with attention scores aij.\nIn the temporal attention block, each computing unit corre-sponds to a column vector $X_{:,j}$, as the equations (5).\n$H_{time} = (\\sum_{j=1}^{P} a_{ij}X_{:,j})$\n$p_{ij} = a^T \\cdot LeakyReLU(W \\cdot (X_{:,j} \\oplus X_{:,i}))$\n$a_{ij} = \\frac{exp(p_{ij})}{\\sum_{k=1}^{P}(exp(p_{ik}))}$ (5)\nIn the representations calculation, $a \\in R^{d'}$ and $W \\in R^{d' \\times 2d_u}$ are learned parameters, LeakyReLU is the activate function, and $\\oplus$ is the concatenate operation. And d' is the manually set dimension of the intermediate hidden state, $d_u$ corresponds to the dimension of the attention computing unit. In temporal attention calculation, $d_u = M$. While in spatial attention calculation, $d_u = P + d$. Because each computing unit is a concatenation of a row vector $X_{i,:}$ and the corresponding spatiotemporal embedding $E_i$ as described by the equations (6). The spatiotemporal embedding contains rich spatiotem-poral information about the entity, enriching the learning of spatial representations.\n$H_{spat} = (\\sum_{j=1}^{M} a_{ij} (X'_{j,:} \\oplus E_j))$\n$p_{ij} = a^T \\cdot LeakyReLU(W \\cdot ((X'_{i,:} \\oplus E_i) + (X'_{j,:} \\oplus E_j)))$\n$a_{ij} = \\frac{exp(p_{ij})}{\\sum_{k=1}^{M}(exp(p_{ik}))}$ (6)\nTo prevent information loss, the selected series X' is concatenated with the two representations $H_{time}$ and $H_{spat}$, resulting in $X \\in R^{3M \\times P}$ for input into subsequent modules."}, {"title": "4) Integrated Feature Learning Module", "content": "4) Integrated Feature Learning Module: The integrated feature learning module consists of a feature-wise transformer block and an LSTM block. After integrating the selected se-ries, temporal, and spatial representations into X, the feature-wise transformer block uses self-attention to learn complex relationships between the representations and auxiliary series. This step enhances the blending of information and better captures spatial dependencies. The output from the transformer block is then passed to the LSTM layer, which learns temporal patterns and characteristics. The resulting hidden state captures important high-level spatio-temporal information."}, {"title": "5) Predictor & Reconstructor", "content": "5) Predictor & Reconstructor: STTS-EAD consists of two output layers: a predictor and a reconstructor. By jointly optimizing these two layers, STTS-EAD is able to accurately predict target values while providing residual information to the EAD module for anomaly detection in training samples. The predictor, which is a fully connected layer is designed to predict target values and generate prediction errors for the EAD module. The reconstructor is an LSTM decoder that aims to reconstruct the target sequence within the sliding window, providing reconstruction errors for EAD. The overall loss function is the weighted sum of two optimization targets:\n$L = \\beta L_{pred} + (1 - \\beta) L_{rec}$\n$L_{pred} = \\sqrt{(Y - \\hat{Y})^2}$ \n$L_{rec} = \\sqrt{\\sum_{t=1}^{P} (X_{target,t} - \\hat{X}_{target,t})^2}$ (7)\nHere, \\\\ serves as the weight coefficient for two parts of losses. $L_{pred}$ represents the loss from predicting the target sequence, quantifying the discrepancy between the predicted $\\hat{Y}$ and the actual prediction label Y. $L_{rec}$ denotes the reconstruction loss for the target series window, reflecting the deviation between the reconstructed sliding window of the target prediction series $\\hat{X}_{target,t:t+P}$ and the raw window data $X_{target,t:t+P}$. Both losses are computed using the Root Mean Square Error metric."}, {"title": "C. Improve Training with EAD module", "content": "C. Improve Training with EAD module"}, {"title": "IV. EXPERIMENTS AND ANALYSIS", "content": "A. Experimental Setup\nDatasets: Three datasets are used to validate the effectiveness of the proposed model: Coffee-Bean (espresso roast coffee bean sales), Coffee-Cream (cream sales in coffee drinks), and Stock-SP500 (closing prices of the S&P 500). Detailed statistics for each dataset are provided in Table I, including the number of series, timestamp length, sample counts for the training, validation, and testing sets, and whether the dataset includes prior spatial information G.\nBaseline Methods: We compared STTS-EAD with nine time series forecasting baseline models of six categories, i.e., tra-ditional univariate model: ARIMA [8], RNN-based model: LSTM [51], three transformer-based models: Informer [19], Autoformer [16] and Preformer [17], CNN-based model: TCN [10], MLP-based model: DLinear and NLinear [21] and GNN-based model: MTGNN [25].\nEvaluation: Prediction accuracy is evaluated using RMSE and MAE metrics.\nB. Prediction Performance Evaluation\nThe evaluation results in Part 1 of Table II show that bold entries correspond to the best results, achieved by the STTS-EAD method. These results demonstrate that STTS-EAD significantly outperforms all strong baseline methods. Compared to the baselines, STTS-EAD shows improvements of 5.6%, 3.8%, and 8.1% in RMSE on three datasets, re-spectively. Additionally, STTS without the EAD anomaly detection module still improves by 3.6%, 1.6%, and 7.5%. These findings highlight that STTS leverages spatiotemporal"}, {"title": "C. Ablation Study for STTS", "content": "C. Ablation Study for STTS\nWe conduct ablation experiments on the STTS method to assess the role of each component. The second part of Table II shows that removing any component increases prediction errors, proving their importance. Without the auxiliary se-ries selection module, errors increase by 3.6% and 5.4%, highlighting its role in selecting relevant series. Removing the spatial attention module increases errors by 0.9% and 2.1%, and removing the temporal attention module increases them by 0.9% and 1.0%, showing the value of both attention mechanisms. Removing the Feature-wise Transformer and LSTM layers causes RMSE to increase by 1.8% and 2.1%, and MAE by 2.4% and 3.5%, emphasizing their necessity."}, {"title": "D. EAD Evaluation and Analysis", "content": "D. EAD Evaluation and Analysis\n1) Comparison of EAD with other anomaly detection ap-proaches: To further evaluate EAD, we replaced it with traditional \"two-stage\" anomaly preprocessing methods while keeping the STTS model unchanged. We used different anomaly detection methods for data preprocessing, and the processed data was then used to train and evaluate the STTS model. We compared two statistical methods-3\\sigma and Expo-nentially Weighted Moving Average (EWMA)\u2014and two deep learning-based methods\u2014USAD and LSTM-NDT. The results in Part 3 of Table II show that the model with EAD outper-forms all baselines, proving its ability to handle anomalies and improve forecasting. Some preprocessing methods improve performance, while others degrade it, indicating they fail to distinguish between anomalous and normal data. In contrast, the STTS-EAD model optimizes both anomaly detection and training, leading to better results.\n2) Impact of Anomaly Score Weight \\\\: In anomaly de-tection, the anomaly score measures the abnormality of data points, helping algorithms identify potential anomalies. Higher scores indicate greater abnormality. In Algorithm 1, the score is calculated as the weighted sum of prediction error ep and reconstruction error er. To explore their impact on the score, we adjust the \\\\ parameter to change the weights of these errors."}, {"title": "V. CONCLUSION", "content": "V. CONCLUSION\nWe propose a novel approach, STTS-EAD, designed to improve the accuracy of time-series prediction based on spatio-temporal learning using an embedded anomaly detection mod-ule that dynamically detects anomalies during the training phase to optimize the training process. Our experiments demonstrated the superior performance of our STTS-EAD model and the significant improvement in prediction accuracy achieved by using the EAD module."}]}