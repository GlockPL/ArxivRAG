{"title": "Client-Side Patching against Backdoor Attacks in Federated Learning", "authors": ["Borja Molina Coronado"], "abstract": "Federated learning is a versatile framework for training models in decentralized environments. However, the trust placed in clients makes federated learning vulnerable to backdoor attacks launched by malicious participants. While many defenses have been proposed, they often fall short when facing heterogeneous data distributions among participating clients. In this paper, we propose a novel defense mechanism for federated learning systems designed to mitigate backdoor attacks on the clients-side. Our approach leverages adversarial learning techniques and model patching to neutralize the impact of backdoor attacks. Through extensive experiments on the MNIST and Fashion-MNIST datasets, we demonstrate that our defense effectively reduces backdoor accuracy, outperforming existing state-of-the-art defenses, such as LFighter, FLAME, and RoseAgg, in i.i.d. and non-i.i.d. scenarios, while maintaining competitive or superior accuracy on clean data.", "sections": [{"title": "1 Introduction", "content": "In recent years, Federated Learning (FL) has emerged as the main paradigm for privacy-preserving machine learning. FL enables collaborative training of a model across multiple clients under the orchestration of a central server [1]. The particularity of FL lies in the fact that training is done without sharing the private data stored on the participating clients. This not only ensures data privacy, but reduces data transfers and enables the use of vast amounts of decentralized data to train a model. As a consequence, FL has become the standard learning framework in critical areas such as connected and autonomous vehicles [2], medical diagnosis [3], and information security [4], where strong privacy and security guarantees are required.\nHowever, the decentralized nature of FL makes it susceptible to various threats, including backdoor attacks, which compromise the reliability of the trained models. In backdoor attacks, the attacker is able to manipulate one or more clients of the federation to introduce a hidden functionality into the final"}, {"title": "2 Background", "content": "This section introduces basic knowledge about the Federated Learning framework and backdoor attacks."}, {"title": "2.1 Federated Learning", "content": "The concept of Federated Learning (FL) was firstly described in [1]. It consist of an iterative learning process where each client updates a shared model using its private data and sends only the model updates (i.e., gradients or parameters) to a central server. The server then aggregates the updates to refine the global model, which is subsequently shared back with the clients. Mathematically, the process is defined as follows:\nLet $D_k = \\{(x_k,y_k)\\}$ denote the local dataset of client k, where $x_k \\in \\mathbb{R}^d$ represents the samples on client k, and $y_k$ the corresponding labels of samples $x_k$.\nIn traditional centralized training, the model is optimized over the combined dataset $D = \\bigcup_{k=1}^{K} D_k$. In FL, however, on each iteration every client k trains a model locally using its dataset $D_k$, and sends the model update $\\Delta\\theta_k$ to the central server. The model update is computed as:\n$\\Delta\\theta_k = \\theta_k \u2013 \\theta,$\nwhere $\\theta_k$ is the locally updated model parameters and $\\theta$ refers to the parameters of the global model.\nAfter each local training performed on the clients, the server aggregates the updates from all K clients using the Federated Averaging (FedAvg) algorithm to refine the global model. The FedAvg update rule is given by:\n$\\theta_{t+1} = \\theta_t - \\eta \\frac{1}{K} \\sum_{k=1}^{K} \\Delta\\theta_k,$\nwhere $\\eta$ is the global learning rate, t is the current training round, and $\\Delta\\theta_k$ is the model update from the k-th client."}, {"title": "2.2 Backdoor Attacks on FL", "content": "The assumption that all clients are trustworthy makes FL inherently vulnerable to poisoning attacks, especially to backdoor attacks. In such attacks, an adversary introduces a malicious behavior into the model, that is linked to a specific"}, {"title": "2.2.1 Stealthy Backdoor Strategies", "content": "To avoid detection by defense mechanisms, attackers may craft their updates to resemble those of benign clients. Since benign updates are inaccessible to the attacker, the global model update between rounds $t-1$ and $t$ can serve as a proxy for benign behavior [5]. The loss function of the attacker then combines a stealth term with the poisoning objective:\n$L_{attack} = \\lambda \\cdot L_{poisoned}(\\theta) + (1 - \\lambda) \\cdot L_{stealth}(\\theta),$\nwhere $L_{poisoned}(\\theta)$ ensures accuracy on both clean and poisoned data, $L_{stealth}(\\theta)$ measures the dissimilarity between benign and poisoned gradients (e.g., using L2 [12, 13] or cosine distance [5]), and $\\lambda \\in [0, 1]$ is a hyperparameter that controls the trade-off between effectiveness and stealth."}, {"title": "3 Defense Mechanism", "content": "As we have seen, the malicious behavior embedded into models by backdoor attacks forces the model to associate a specific trigger pattern $\\delta$ with an attacker-chosen target class t. Our defense aims to detect and neutralize these backdoor triggers by iteratively optimizing potential triggers for source-target class pairs and then patching the models of benign clients with the identified triggers to resist backdoor behavior. To do so, our approach incorporates gradient-based optimization techniques inspired by the Projected Gradient Descent (PGD) [14, 15], along with adversarial training [16] to enhance the robustness of the final models."}, {"title": "3.1 Finding Potential Source-Target Class Pairs", "content": "In real-world black-box scenarios, the backdoor triggers and source-target class pairs chosen by the attacker are not explicitly known. Consequently, our defense must identify the $\\alpha$ of plausible source-target pairs and the trigger patterns associated with them. To address this, we propose using an iterative optimization approach, similar to PGD, to generate a candidate trigger pattern for each source-target class pair.\nLet D denote the local dataset of a benign client of the federation, and $D_s$ represent the subset of samples belonging to a specific class s. The set of poisoned samples for a source class s targeting a class t is defined as:\n$D_{st}^{p} = \\{(x + d_{s\\rightarrow t}, t) : x \\in D_s\\},$\nwhere $d_{s\\rightarrow t}$ is the trigger pattern that induces the model to misclassify samples from class s as class t. We parameterize the trigger pattern as:\n$\\delta_{st} = m \\gamma,$\nwhere m is a binary mask matrix that specifies the shape and location of the trigger, and $\\gamma$ is a perturbation matrix determining the intensity of the trigger. This parameterization ensures the trigger remains localized and minimally invasive, as suggested by prior work [17, 18].\nFor each candidate source-target pair (s,t), where $s \\neq t$, we optimize $d_{s\\rightarrow t}$ to minimize the following combined loss function:\n$\\mathcal{L}(\\delta_{st}) = \\mathcal{L}_{bd} + \\mathcal{L}_{clean} + \\mathcal{L}_{sparsity},$\nwhere:\n$\\mathcal{L}_{bd} = \\mathbb{E}_{(x,t) \\sim D_{st}^{p}}l(f_{\\theta}(x^p), t),$\n$\\mathcal{L}_{clean} = \\mathbb{E}_{(x,s) \\sim D_{s}}l(f_{\\theta}(x), s),$\n$\\mathcal{L}_{sparsity} = w||d_s||_1,$"}, {"title": "3.2 Model Patching", "content": "At the end of the FL training process, a patching step is applied to the global model using the triggers identified and optimized by each client during the FL process. This patching mechanism is designed to neutralize backdoor triggers by disrupting the malicious associations learned by the model that enable the backdoor attack.\nLet $T_k = \\{d_{st}\\}$ denote the set of optimized trigger patterns identified during training on client k. We use the triggers in $T_k$ to construct a patching dataset $D^P$ on clients. Specifically, we apply each trigger $\\{d_{s\\rightarrow t}\\} \\in T_k$ only to clean samples from its corresponding source class s in $D_s$.\nNext, we use the samples in the patching dataset $D^P$, along with their original labels y, and the clean dataset D to fine-tune the global model $f_\\theta$ by minimizing the following objective function:\n$L_{patch}(\\theta) = \\frac{1}{|D|}\\sum_{(x,y) \\in D} l(f_\\theta(x), y) + \\frac{1}{|D^P|}\\sum_{(x,y) \\in D^P}l(f_\\theta(x^p), y),$\nwhere, l represents a loss function (e.g., the cross-entropy loss). By minimizing $L_{patch}(\\theta)$, we adjust the parameters of the model to reduce the sensitivity to backdoor triggers while maintaining the performance on clean samples.\nTherefore, this patching step ensures that the global model $f_\\theta$ becomes robust to backdoor attacks without introducing additional computational overhead during the standard FL training process."}, {"title": "4 Evaluation", "content": "In this section, we describe the experimental setup used to evaluate our proposal.\nWe then present the results obtained by our defense and compare them with five"}, {"title": "4.1 Setup and Methodology", "content": "Datasets To assess the effectiveness of our defense, we experiment with two widely used benchmark datasets from the FL literature: the MNIST and Fashion-MNIST. The MNIST dataset [19] contains 70,000 grayscale images of handwritten digits (0-9). The Fashion-MNIST dataset comprises 70,000 grayscale images of fashion items from 10 categories, such as T-shirts, trousers, and shoes.\nAttack Setup We evaluate our proposal against three state-of-the-art backdoor attacks: the Model Replacement Attack (MRA) [5], the Distributed Back-door Attack (DBA) [20], and Neurotoxin [10]. For each attack, we select a random trigger pattern and assign the target class for each dataset as indicated in the rightmost column of Table 1.\nEvaluation Metrics The performance of our defense mechanism is measured using two standard metrics commonly used in the FL literature: (1) the main task accuracy (MTA), which reflects the performance of the model on the task it is designed for and, (2) the backdoor accuracy (BA), that evaluates the effectiveness of the attack on the model. The MTA is computed as the proportion of correctly classified clean samples out of the total predictions made by the model, whereas the BA value is calculated as the proportion of poisoned samples with the backdoor trigger that are misclassified into the target class selected for the attack.\nFL Setup To simulate the FL framework, each training dataset is divided into K local datasets, each corresponding to a node in the federation. For the i.i.d scenario, local datasets are balanced in size and class distribution. In contrast, non-i.i.d local datasets are generated by sampling the main dataset according to a Dirichlet distribution [21] with $\\alpha = 0.5$. The federation consists of 100 nodes in the i.i.d scenario and 20 nodes in the non-i.i.d scenario, with 40% of the nodes acting as adversaries and sending poisoned updates to the server. The overall FL process spans 50 global training (aggregation) rounds. Within each FL round, between 60% and 90% of clients are selected at random to perform three local training epochs and send their gradients to the server."}, {"title": "4.2 Attack Mitigation Results", "content": "The results of experiments conducted on the MNIST and Fashion-MNIST datasets for the i.i.d. scenario are shown in Tables 2 and 3, respectively. As observed, the accuracy of the models on clean data (MTA) remains stable across most scenarios, whereas BA values for MRA and Neurotoxin attacks vary depending on the defense mechanism applied. Specifically, for the MNIST dataset, MRA and Neurotoxin attacks achieved high BA values (above 90%) against defenses such as MedianKrum, FoolsGold, FLAME, and RoseAgg. In contrast, the DBA attack was effectively neutralized by all defenses in this dataset. Remarkably, the most effective defenses were LFighter, which completely neutralized the effects of all attacks, with BA values of 0%, and our proposed defense mechanism, which significantly reduced the BA of MRA and DBA attacks to 0.6% and 2.2%, respectively, while completely eliminating the impact of the Neurotoxin attack.\nFor the Fashion-MNIST dataset, the inability of defenses such as Fools Gold and RoseAgg to stop the DBA and MRA attacks is evidenced by the high BA values (above 97%). FLAME was vulnerable to the Neurotoxin attack, with a BA of 70% and a reduced MTA of 74%. Again, LFighter demonstrated robust performance, with stable values of 86% for MTA and 0% BA for all attacks. Strong results were also achieved by our defense, with reduced BA values for MRA and DBA attacks of 3.9% and 0.7%, respectively, and complete effectiveness in neutralizing the Neurotoxin attack. Overall, while LFighter achieved immunity to all attacks, our defense mechanism substantially reduced BA values to an average of 3%, while maintaining comparable or superior accuracy on clean data with respect to the baseline model.\nWhen evaluating defenses with non-i.i.d. data (see Tables 4 and 5), our experiments reveal key differences in the performance of the defenses. All evaluated defenses, except for our proposal, have shown to be vulnerable to at least two of the three backdoor attacks tested, with an average BA of 95%. Among existing methods, RoseAgg was found to produce models highly susceptible to all considered backdoor attacks. Notably, LFighter, the most effective defense under i.i.d. settings, proved particularly vulnerable to MRA and DBA attacks in this non-i.i.d. scenario, with BA values of 98%. In contrast, our proposed defense has shown to be robust against all tested attacks. Specifically, in the"}, {"title": "5 Related Work", "content": "Among existing defenses aiming to mitigate the impact of backdoor attacks, techniques such as differential privacy [7, 26, 27] and secure aggregation [8,"}, {"title": "6 Conclusions", "content": "In this work, we proposed a novel patching mechanism for the client-side of federated learning. Specifically, we introduced an iterative optimization process for candidate trigger patterns using adversarial learning techniques, enabling effective patching and neutralization of highly sophisticated backdoor attacks. Our experimental results demonstrate that the proposed method is highly effective"}]}