{"title": "S-SYNTH: Knowledge-Based, Synthetic Generation of Skin Images", "authors": ["Andrea Kim", "Niloufar Saharkhiz", "Elena Sizikova", "Miguel Lago", "Berkman Sahiner", "Jana Delfino", "Aldo Badano"], "abstract": "Development of artificial intelligence (AI) techniques in medical imaging requires access to large-scale and diverse datasets for training and evaluation. In dermatology, obtaining such datasets remains challenging due to significant variations in patient populations, illumination conditions, and acquisition system characteristics. In this work, we propose S-SYNTH, the first knowledge-based, adaptable open-source skin simulation framework to rapidly generate synthetic skin, 3D models and digitally rendered images, using an anatomically inspired multi-layer, multi-component skin and growing lesion model. The skin model allows for controlled variation in skin appearance, such as skin color, presence of hair, lesion shape, and blood fraction among other parameters. We use this framework to study the effect of possible variations on the development and evaluation of AI models for skin lesion segmentation, and show that results obtained using synthetic data follow similar comparative trends as real dermatologic images, while mitigating biases and limitations from existing datasets including small dataset size, lack of diversity, and underrepresentation.", "sections": [{"title": "1 Introduction", "content": "Robust and generalizable artificial intelligence (AI) for medical applications requires large datasets representative of intended populations and their subgroups, relying on a time-consuming and laborious annotation process by clinical experts (e.g. labeling pixel-level segmentation masks or identifying findings). In this work, we explore the potential of a skin simulation model and the practical utility of resulting synthetic images in development and evaluation of AI-based techniques.\nSegmenting and labeling dermatologic images is time-consuming and challenging, and thus public dermatologic datasets generally contain few examples"}, {"title": "2 Related Work", "content": "A particular challenge in practical development and evaluation of dermatologic AI is the lack of labeled datasets. Of the few public labeled datasets that exist, only a fraction have fine grained annotations, such as pixel-level segmentation levels for lesions or hair. The annotation process is error-prone and can have large variations across truthers (particularly when datasets are collected for different purposes and no consistent labeling protocol is established). In part due to these annotation difficulties, there is a lack of transparency and potential bias within the datasets used for dermatologic AI studies. A recent investigation found that from a total of 70 unique studies that involved AI models for dermoscopic imaging, only about 7 (10%) included skin tone information for at least one dataset used. Synthetically generated data has been explored to address these challenges. Generative models have been used to create synthetic skin lesions to tackle the class imbalance problem and improve the performance of the skin lesion classifiers or enhance lesion segmentation. Rezk et al."}, {"title": "3 S-SYNTH: Skin Simulation Framework", "content": "Our approach for generating synthetic skin involves the construction of a 3D digital object model comprising of skin tissue (epidermis, dermis, hypodermis), blood network, hair, and a lesion. This process is implemented and automated in Houdini, a software for 3D modeling, animation, and visual effects, via a Python API. Once created, each model is subsequently processed through a research-oriented rendering system, Mitsuba 3, to generate each synthetic rendering. The approach can be automated to create large databases with controlled variation for various AI analysis applications. In the following sections, we describe the digital skin model and rendering process."}, {"title": "3.1 Digital Skin Model", "content": "Skin Tissue Each skin sample is created from a multi-layer model, with each layer representing a component of skin tissue. Thicknesses of the topmost two layers are based on values reported in : epidermis (20-150 \u00b5m) and dermis (1-4 mm). To introduce geometric variability, we incorporated surface roughness (noise) into the top surfaces of epidermis, hypodermis, and papillary dermis (located between the epidermis and dermis). Roughness values were randomized within predefined constraints, enhancing the naturalistic appearance and diversity of the generated skin models.\nBlood Network and Hair The blood network model is created by solving a shortest path problem on randomly distributed points within a tetrahedral mesh generated from a primitive cube. Points from the bottom of the cube, corresponding to the lower blood network capillaries, were designated as start points, while those from top of the cube, representing the upper capillaries, were designated as end points. To generate each blood network model, the numbers and positions of starting and ending points were randomized. Hair models were constructed by manipulating hair properties (density, length, distribution, thickness, and curvature) in Houdini to generate a diverse range of hair apprearances.\nGrowing Lesion Model We developed a probabilistic, growing lesion model that generates a 3D volume with stochastic growth of lesion shape and size, based on [20]. Growth starts with one active cell (i.e., voxel). We define active cells as those cells that exist and can grow at a given time point. Within each time point, we iterated over the active cells c and selected them for growth based on probabilities (see Fig. 2). To control the growing direction, the probability of an active cell to grow outside the skin is set to extremely low, while the probability for inwards growing is set to be high. Probabilities were based on a Gaussian distribution G, and are updated in each time point for added variability.\nTo control for lesion shape irregularity, we set an irregularity probability for each cell $C_p$, which starts $C_i$ number of recursive iterations of the same growing algorithm on that cell's location, independent of the regular growth. Irregular cells can themselves trigger another recursive growth up to a maximum of $C_r$ recursions. We generated both regular ($C_p = 0.0001$) and irregular lesions ($C_p = 0.001$). Sample images of lesions and growth evolution can be seen in Fig 1. Additional details can be found in the Supplementary Material. Once created,"}, {"title": "3.2 Rendering and Image Formation", "content": "Optical Material Properties Melanosomes are the primary contributors to optical absorption in the epidermis. In the dermis, optical absorption is mainly attributed to blood, while, in the hypodermis, lipid and fat components are the predominant sources of optical absorption. Thus, each skin layer is assigned an optical material, containing the index of refraction (IOR) and the spectral distribution of the absorption and scattering coefficients. Following the methodology established by Jacques et al. [22], we calculated the optical absorption properties for each the four distinct tissue types: epidermis, dermis, hypodermis, and blood. We model reduced scattering of tissue using $\\mu' = a(\\lambda/500)^{-b}$, where $\\lambda$ is the light wavelength [23]. Finally, for each skin layer material, we define a surface scattering model using a bidirectional scattering distribution function (BSDF), based on [24].\nLighting and Camera To account for lighting variations, we render each skin model using a collection of High Dynamic Range Imaging (HDRI) images capturing various environmental lighting conditions from [25]. The Volumetric Path Tracer is a rendering algorithm that simulates the paths taken by light as it interacts with the 3D scene. It is particularly effective for scenes with volumetric effects or materials with complex light interactions. Spectral multiple importance sampling is a technique used to address the spectral nature of light. In scenes with materials that exhibit variations in spectral properties (e.g., spectral absorption), this method accounts for the complex interactions of light with materials across different wavelengths, such as those encountered in biological tissues. A perspective sensor with Mitsuba's HDR Film is placed 1.5cm above the skin with a field of view (FOV) of 75 degrees, facing perpendicularly to the top surface of the epidermis. The Integrator (Mitsuba 3's plugin for the rendering technique) is set to Volumetric Path Tracer with spectral multiple importance sampling. We chose this specific configuration due to its ability to simulate spectrally-varying optical properties across varying skin tones in simulated lighting conditions. We use 124 samples per pixel (SPP) and render images at resolution 1024x1024 pixels. Each image and mask take about three minutes to generate on a GPU. Sample generated images can be seen in Fig. 3."}, {"title": "3.3 AI Device Description", "content": "To demonstrate the usage of S-SYNTH for augmenting real patient data, we experiment with real and synthetic examples on the task of skin lesion segmentation. which is important for timely treatment decisions [26]. We rely on DermoSegDiff [27], a state of the art diffusion-based skin segmentation model,"}, {"title": "4 Results and Discussion", "content": "We present segmentation performance based on three aspects: impact of training set composition comprising different ratios of real and synthetic images, impact of various physiological or rendering parameters of the synthetic images when used as test set, and impact of specific characteristics of synthetic images that are measurable on real images.\nSynthetic Data for Training We systematically evaluated the effect of the training data composition on the real test set performance (similar to the popular Train-Synthetic-Test-Real (TSTR) protocol [31]) on both ISIC and HAM. As"}, {"title": "Limitations", "content": "S-SYNTH is the first skin simulation framework that generates synthetic skin images with controllable variations. There are a number of limitations to our work. First, S-SYNTH does not model any specific disease presentation, and additional work needs to be performed to evaluate the realism of the generated examples. Second, the current rendering techniques utilizes an RGB camera setup, and additional modifications allowing for multispectral imaging, when validated, might be useful for more advanced algorithms that use a broader variety of wavelengths."}, {"title": "5 Conclusion and Future Work", "content": "We presented S-SYNTH, a novel, knowledge-based simulation pipeline for synthetic generation of dermoscopic images and skin lesions. S-SYNTH procedurally generates multi-layer 3D skin models, with consideration to optical skin characteristics, and digitally renders synthetic images under realistic lighting conditions. Using S-SYNTH, we generated realistic and varied skin surface models for sub-surface scattering simulation in the context of skin lesion segmentation, and compared the resulting synthetic images to public dermatologic benchmarks. We showed that examples created using S-SYNTH can be used to augment limited real datasets and identify performance trends."}, {"title": "6 Supplementary Materials", "content": ""}]}