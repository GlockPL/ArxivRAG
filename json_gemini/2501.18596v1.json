{"title": "DELTALLM: Compress LLMs with Low-Rank Deltas between Shared Weights", "authors": ["Liana Mikaelyan", "Ayyoob Imani", "Mathew Salvaris", "Parth Pathak", "Mohsen Fayyaz"], "abstract": "We introduce DELTALLM, a new post-training compression technique to reduce the memory footprint of LLMs. We propose an alternative way of structuring LLMs with weight sharing between layers in subsequent Transformer blocks, along with additional low-rank difference matrices between them. For training, we adopt the progressing module replacement method and show that the lightweight training of the low-rank modules with approximately 30M-40M tokens is sufficient to achieve performance on par with LLMs of comparable sizes trained from scratch.\nWe release the resultant models, DELTALLAMA and DELTAPHI, with a 12% parameter reduction, retaining 90% of the performance of the base Llama and Phi models on common knowledge and reasoning benchmarks. Our method also outperforms compression techniques Joint-Drop, LaCo, ShortGPT and SliceGPT with the same number of parameters removed. For example, DeltaPhi 2.9B with a 24% reduction achieves similar average zero-shot accuracies as recovery fine-tuned SlicedPhi 3.3B with a 12% reduction, despite being approximately 400M parameters smaller with no fine-tuning applied.\nThis work provides new insights into LLM architecture design and compression methods when storage space is critical.", "sections": [{"title": "1. Introduction", "content": "Transformer-based architectures (Vaswani, 2017) have become the cornerstone of modern language modeling (Abdin et al., 2024; Dubey et al., 2024; Team et al., 2024; Jiang et al., 2024; Bai et al., 2023; Brown et al., 2020). While scaling laws indicate that performance improves as these models grow in size and training data (Kaplan et al., 2020; Hoffmann et al., 2022), many model families are also trained in smaller sizes due to deployment constraints. In addition, multiple approaches\u2014such as distillation (Hinton, 2015; Gu et al., 2024), prompt and KV-cache compression (Zhang et al., 2023; Cai et al., 2024; Liu et al., 2023; Pan et al., 2024), speculative decoding (Leviathan et al., 2023; Li et al., 2024), and quantization (Ashkboos et al., 2024b; Frantar et al., 2022; Xu et al., 2024) have been proposed to meet resource limitations. For extremely low-resource scenarios such as edge device deployments, these techniques are often combined to make deployment feasible.\nModel compression has emerged as a prominent strategy to reduce both the model size and the computational overhead. Most previous works on compressing large language models (LLMs) rely on traditional pruning methods, which do not fully account for the multi-layered Transformer architecture at the core of these models (Singh & Alistarh, 2020; Frantar & Alistarh, 2023b; 2022; Sun et al., 2024; Mishra et al., 2021; Ma et al., 2023), resulting in suboptimal performance. Recently, several studies have proposed compression techniques more tightly tailored to LLMs (Ashkboos et al., 2024a; Men et al., 2024; Yang et al., 2024; He et al., 2024)."}, {"title": "2. Related Work", "content": "The lottery ticket hypothesis posits that a dense, randomly initialized model contains subnetworks (or \"winning tickets\") which, when trained in isolation, can match or exceed the accuracy of the original model (Frankle & Carbin, 2019). This hypothesis has also been validated in reinforcement learning and natural language processing (Yu et al., 2020; Frankle et al., 2020). Recent studies further reveal substantial redundancy in Transformer architectures (Men et al., 2024), with efforts focusing on removing entire blocks or selectively dropping attention and MLP layers using similarity metrics (He et al., 2024; Shoaib Ahmed Siddiqui, 2024; Bian et al., 2021). Additionally, there is evidence that redundancy tends to concentrate in the middle to later layers, while the first and last layers play more specialized roles (Men et al., 2024; Ma et al., 2023).\nPruning has emerged as a widely used model compression technique, removing less important parameters to reduce computational and memory footprints (Han et al., 2015b). Early efforts often relied on unstructured pruning (Han et al., 2015a; Michael H. Zhu, 2018; Gale et al., 2019; Frantar & Alistarh, 2022; 2023a), which did not necessarily improve inference speed. Consequently, more recent approaches explored structured pruning-removing entire filters or attention heads for predictable speedups (Hoefler et al., 2021; Mishra et al., 2021; Sun et al., 2024).\nIn large Transformer-based models, specialized strategies typically apply structured pruning followed by continued training. For example, movement pruning (Sanh et al., 2020) dynamically drops redundant connections during finetuning; Ashkboos et al. (2024a) uses principal component analysis to remove trailing rows and columns of weight matrices; Ma et al. (2023) selects non-critical coupled structures based on gradient information; Men et al. (2024) eliminates whole layers based on input-output similarity; and LaCo (Yang et al., 2024) collapses outermost layers into their neighbors. MINITRON (Muralidharan et al., 2024) further compares various pruning and re-training approaches, demonstrating superior efficiency over training small language models from scratch. An essential difference between MINITRON and DELTALLM is the training data requirement: while DELTALLM recovers performance with as few as 37M tokens, MINITRON requires billions.\nWeight sharing, which offers flexible ways to reduce computational and memory overhead, has recently attracted significant attention. Two widely adopted weight-sharing approaches are Mixture of Experts (MoE) and cross-layer weight sharing. MoE, originally introduced to machine learning as a method similar to ensemble techniques (Jacobs et al., 1991), has since been applied to various architectures (Fedus et al., 2022; Eigen et al., 2014; Shazeer et al., 2017) including Transformers (Jiang et al., 2024; Dubey et al., 2024; Abdin et al., 2024). Cross-layer weight sharing, introduced for Transformer-based language modeling in (Dehghani et al., 2019) as a way to reduce memory usage and improve performance, was subsequently adopted by multiple works for training Transformer models from scratch (Wang & Li, 2024; Dehghani et al., 2019; Lan et al., 2020; Liu et al., 2024) or for shrinking existing models (Bae et al., 2024).\nLow-Rank Adaptation\nLow-rank approximations (LoRA) have been extensively applied to various stages of machine learning for diverse purposes. Aside from tasks that inherently rely on low-rank structures (Cai et al., 2010; Li et al., 2016; 2018), LoRA-based methods have been proposed to improve the generalization of over-parameterized models (Oymak et al., 2019), enable efficient model fine-tuning (Hu et al., 2022; Hyeon-Woo et al., 2021; YEH et al., 2024; Mahabadi et al., 2021), and reduce computational costs by integrating low-rank layers directly into model architectures (Lan et al., 2020) or as a post-training compression step (Ben Noach & Goldberg, 2020; Tukan et al., 2020; Denton et al., 2014)."}, {"title": "3. Methodology", "content": null}, {"title": "3.1. Preliminaries", "content": "For a weight $W \\in \\mathbb{R}^{D \\times D}$ the LoRA technique introduces two additional low-rank matrices $A \\in \\mathbb{R}^{D \\times R}$ and $B \\in \\mathbb{R}^{R \\times D}$, where R is the rank of the matrices. These matrices are then updated during the fine-tuning process and are added to the weight, while the rest of the network remains frozen."}, {"title": "3.2. DELTALLM", "content": "DELTALLM introduces low-rank differences between layers in consecutive Transformer blocks that share weights, called deltas.\nA model weight W of layer l + i can be restructured as a function of the previous layer l and a delta between the two weights as follows:\n$W_{l+i}^{M \\times N} = W_{l}^{M \\times N} + \\delta_{l+i} \tlabel{eq:deltaW}$\nwhere $\\delta_{l+i}$ is the low-rank approximation of\n$\\delta_{l+i} = W_{l+i}^{M \\times N} - W_{l}^{M \\times N} \tlabel{eq:delta}$\nWe define the delta matrices in the same manner as the A and B matrices in the LoRA setup. The low-rank approximations can be obtained using Singular Value Decomposition (SVD) or other approximation methods. The low-rank deltas allow to restore diversity and adaptiveness that get affected due to layer replication.\nRefer to figure 2 for the architecture of a DELTALLM model. We present two strategies for organizing the model structure: a single Transformer block with delta modules creating each subsequent block (left) and alternating Transformer blocks with delta modules in between (right). Within blocks, we can compress attention and/or multi-layer perceptron (MLP) layers.\nWe can further extend this to allow any previous layer's weight $W_{l-k}$ to be used to initialize the current weight $W_{l}$. We refer to these layers as anchor layer and the corresponding Transformer blocks as anchor blocks.\nRefer to the ablation studies in Section 5.7 for the best practices on the choice of the blocks and layers."}, {"title": "3.3. Delta-Model Training", "content": "While with the right initialization the deltas may be sufficient to retain the desired performance, our method relies on further training, albeit on a small number of tokens.\nWe explore two strategies for training a DELTALLM model:\nDelta-tuning only with Progressive Module Replacement: delta-layers are progressively replaced with original layers. Only the delta-layers are trained while the rest of the model weights remain fixed.\nDelta-layer tuning with LoRA fine-tuning: LLM weights are fine-tuned jointly with the delta weights using parameter-efficient training methods.\nDelta-tuning only\nFollowing (Xu et al., 2020), we progressively replace original LLM weights with the corresponding DELTALLM weights according to a probability scheduler. That is, at the beginning of training, modules are replaced with the given probability rate which gradually increases until convergence to 1.0. Unlike (Xu et al., 2020), after probability rate convergence, we continue training for an additional number of epochs, as determined by the hyperparameter search we conduct.\nWe additionally extend the progressive module replacement (PMR) method to allow uneven replacement rates across the blocks. This is motivated by our preliminary experiments that later layers contain more redundancy than earlier layers and that it may be beneficial to replace the later layers first for smoother training.\nThe total loss is computed as\n$L = (1 - \\alpha)L_{CE} + \\alpha L_{logits}, \tlabel{eq:loss}$\nwhere $L_{CE}$ is the cross entropy of the student model (with potentially some layers replaced with the teacher model), $L_{logits}$ is the distillation loss between the final logits of the teacher and the student models. Following (Muralidharan et al., 2024), we choose Kullback-Leibler (KL) divergence as the distillation loss as it is shown to outperform the mean squared error (MSE) and the cosine similarity, $\\alpha$ is the distillation weight.\nFull model Fine-tuning\nIn the second approach for DELTALLM model training, we train the delta-layers using the progressing module replacement along with the fine-tuning of the rest of the model. We adopt parameter-efficient fine-tuning methods."}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Experiment Settings", "content": "We use Phi 3.5 (Abdin et al., 2024) and Llama 3.2 (Dubey et al., 2024) as the teacher models to obtain DELTAPHI and DELTALLAMA models respectively using the procedure outlined in Section 4.\nWe use Alpaca (Rohan Taori & Hashimoto, 2013) and Ultrachat (Ding et al., 2023) for all training experiments with DELTAPHI and DELTALLAMA. Alpaca is separated into train (80%), validation (10%) and test (10%) portions. The train set is used to conduct the hyperparameter search, for which the test set is used as an optimization metric. The Delta-Model is then trained on the Ultrachat train set with the best set of the hyperparameters."}, {"title": "4.5. Ablation Study: Which blocks and layers to compress?", "content": "In all our experiments we keep the first and the last two blocks unchanged since they are highly specialized and cause significant degradation if removed (Men et al., 2024; Ma et al., 2023).\nWe explore three strategies of the construction of the delta-blocks:\nSequential: A sequential set of delta-blocks. That is, weights of a single anchor Transformer block get shared with the next few blocks with deltas.\nAlternating: A set of blocks where a standard and delta block are alternating. In this setup multiple anchor blocks share weight with other blocks and have deltas between them.\nJointDrop: The layers obtained by the JointDrop method (He et al., 2024). The JointDrop method computes the importance of layers using a similarity-based metric to capture redundancy. This metric is then used to eliminate those layers completely. In our setup, instead of removing them, we replicated those layers with the addition of deltas.\nIn each of these options, we can further choose the layers to compress within the blocks: MLP, attention or a combination of both. When choosing which layers to replicate the weights from, we rely on prior work suggesting that the changes between the outputs of consecutive layers tends to be less significant (Yang et al., 2024). Hence, we always choose the anchor layer to be layer prior to the delta-layer.\nRefer to Table 6 for the ablations on the Transformer blocks chosen for Phi-3.5. Note that the JointDrop method always outputs the attention layers. This happens due to the importance computation of the inputs and outputs of the layers.\nIn the sequential and alternating cases, the MLP choice tends to results in lower perplexities for the models with more layers removed.\nWe hypothesize that the compression of MLP layers tends to perform better due to the nature of the DELTALLM architecture. Attention layers are highly sensitive to the input tokens and their replication may lead to the loss of layer-specific toke-to-token relationships, hurting effective language modeling. MLP layers transform tokens independently and contribute to most of the model parameters, suggesting that they may exhibit more redundancy. Our method does not remove the MLP layers as pruning methods do, preserving the number of computations performed and the trained low-rank deltas allow to diversify the replicated MLPs.\nIn addition, given the same number of blocks, compressing MLP layers helps to reduce the storage requirements more significantly due to the large number of parameters. Hence, in the subsequent experiments we apply our method to the MLP layers."}, {"title": "4.6. Baselines", "content": "In this section, we compare our method to state-of-the-art compression methods JointDrop, SliceGPT, ShortGPT and LaCo. The results are given in the Tables 2 and 3."}, {"title": "5. Limitations and Future Work", "content": null}, {"title": "Delta-layer Initialization", "content": "Compared with training-free compression methods, our method heavily relies on the training of the low-rank deltas. This implies a dependency on the quality of the training data as well as the need for additional compute resources, although light. We hypothesize that the right initialization of deltas may reduce the need for additional training. We plan to explore this as a future research direction.\nShared Layer Operations\nIn the current setup, the subsequent layers are initialized with full weight replication of the anchor weights. We hypothesize that the shared weights can also be initialized using other operations such as a weighted average of the weight matrices. This may facilitate a better knowledge transfer between the layers.\nInference latency\nThe DELTALLM currently focuses on restructuring an LLM to save disk space on-device while preserving comparable accuracy. Our method does not focus on reducing the inference time of the model running on-device. We plan to investigate ways to reduce the inference time in future work. For example, a potential approach is to combine attention pruning or removal methods with MLP weight sharing, which can contribute to both space and computation efficiency."}, {"title": "6. Conclusion", "content": "In this paper, we present DELTALLM, an alternative approach to structuring Transformer-based models, which optimize for space efficiency. In our setting, MLP and attention layers share weights with the corresponding anchor layers earlier in the network. The shared weights have additional low-rank delta-layers between them, which are trained to preserve the knowledge and capabilities of the model. We show that DELTALLM models compressed from open-source LLMs can achieve comparable performance with other models of similar sizes trained from scratch. This new structure also outperforms many post-training compression methods such as JointDrop, SliceGPT, ShortGPT and LaCo. We hope that our method inspires new efficient designs of LLM architectures that can be trained from scratch as well as serve as both a post-training compression technique."}, {"title": "7. Impact Statements", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}]}