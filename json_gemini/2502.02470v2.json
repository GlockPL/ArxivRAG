{"title": "Modular Training of Neural Networks aids Interpretability", "authors": ["Satvik Golechha", "Maheep Chaudhary", "Joan Velja", "Alessandro Abate", "Nandi Schoots"], "abstract": "An approach to improve neural network interpretability is via clusterability, i.e., splitting a model into disjoint clusters that can be studied independently. We define a measure for clusterability and show that pre-trained models form highly enmeshed clusters via spectral graph clustering. We thus train models to be more modular using a \"clusterability loss\" function that encourages the formation of non-interacting clusters. Using automated interpretability techniques, we show that our method can help train models that are more modular and learn different, disjoint, and smaller circuits. We investigate CNNs trained on MNIST and CIFAR, small transformers trained on modular addition, and language models. Our approach provides a promising direction for training neural networks that learn simpler functions and are easier to interpret.", "sections": [{"title": "1. Introduction", "content": "Interpretability is an active area of research that aims to solve both high-stake deployment constraints for fairness and robustness (McClure et al., 2020) and AI safety and trustworthiness concerns (Bereska & Gavves, 2024). Several breakthroughs in the subdomain of mechanistic interpretability, have helped us understand the inner workings of deep networks, both via circuits (Wang et al., 2022; Olah et al., 2020; Elhage et al., 2021) and representation spaces (Zou et al., 2023; Bricken et al., 2023).\nOne approach to mechanistic interpretability is to identify features at the level of neurons, alternatively we can identify 'subskills' at the level of subnetworks or circuits. While sparse auto-encoders (SAEs) (Huben et al., 2024) extract a model's features over different neurons, we nudge the model to divide its computation into disentangled components."}, {"title": "2. Related Work", "content": "Modularity metrics inspired by research in neuroscience incorporate transfer entropy (Novelli et al., 2019; Ursino et al., 2020) or spatial metrics (Liu et al., 2023a;b). Models are sometimes trained or pruned using modularity metrics (Patil et al., 2023). Tsitsulin et al. (2023) find that Graph Neural Networks node pooling methods do not work well to cluster graphs and introduce an unsupervised clustering method. Their method first uses a graph convolutional network to obtain soft clusters for each node and then optimizes this assignment based on the first modularity metric in Section 3. Salha-Galvan et al. (2022) add a regularization term for modularity in Graph Autoencoders to show performance gains. In this work, we focus on how modularity can help us train models to be more interpretable.\nMixture of experts is a form of modularity, where each module is an entire network. In our set-up each module is a component of a layer, i.e. the partitioning is at a lower level. Mixture of experts sometimes use routing policies to decide which expert to activate (e.g. based on features of the input). Gradient routing (Cloud et al., 2024) is a method that trains task-specialized experts by hard-coding what tasks each expert specializes on, one of their stated benefits is that this allows a user to selectively remove ability on a particular task. MONET (Park et al., 2024) trains monosemantic experts, which learn to route different types of inputs to specific experts during training, without human-specified task assignments. Instead, we encourage the emergence of task-specialized layer components during training.\nClustering of neural network weights is typically done either using graph properties of the weights (structural) (Watanabe et al., 2018; Filan et al., 2021; Patil et al., 2023) or using correlations between neuron activations (functional) (Hod et al., 2022; Lange et al., 2022). MoEfication groups feedforward neurons in a pre-trained language model into clusters of 'experts' and at inference only activates the most clusters neurons (Zhang et al., 2022). In this paper, we consider both weight-based and gradient-based clustering, and show that they do not help with modularity across clusters.\nCircuit Discovery is an active field of research (Wang et al., 2022; Olah et al., 2020; Elhage et al., 2021; Conmy et al., 2023), which aims to uncover subnetworks that perform a specific functionality. Wortsman et al. (2020) use a randomly initialized, fixed base network and for each task find a subnetwork that achieves good performance on this task. These overlapping subnetworks can be thought of as circuits. In this work, we evaluate and optimize for structural connectedness or global functionality without considering specific functionalities. We evaluate the resulting modules by assessing the extent to which they have specialized on a class level, i.e. the extent to which they correspond to class-specific performance."}, {"title": "3. Introducing a Measure of Modularity", "content": "First, we discuss existing modularity metrics used in other contexts and their drawbacks in our setting. We then motivate our choice of the clusterability metric (and the clusterability loss) and discuss its benefits as an optimization metric for neural network modularity.\nNon-differentiable metrics to measure modularity that rely on conditional entropy (Ursino et al., 2020), sampling, or discrete computation (Veniat et al., 2021) have been introduced. While they capture the essence of modularity, we cannot use gradient descent to optimize for them during training. For unweighted graphs, 'community structure' is a commonly used modularity metric (Newman, 2006; Salha-Galvan et al., 2022; Tsitsulin et al., 2023; Bhowmick et al., 2024), which is based on the difference between an edge value (0 or 1) and the expected number of edges (see Appendix H.1 for an explicit formula).\nOur metric, inspired by community structure, is differentiable and better tailored to weighted graphs. In particular, in our metric large weights are disproportionally punished, and when we slot in real numbers for the weights, the terms can not cancel out."}, {"title": "4. Neural Networks trained on Cross-Entropy are not Modular", "content": "First, we would like to evaluate (based on our clusterability metric) how modular neural networks trained using the cross-entropy loss are by default. Directly searching for clusters that maximize our metric leads to a combinatorial explosion, so we use a clustering algorithm to split a component into clusters. For our clustering algorithm, we modify the methodology of Filan et al. (2021) and use normalized spectral clustering to split any component of a neural network into k different clusters, taking into account that a network layer is bipartite.\nOur clustering method, called Bipartite Spectral Graph Clustering (BSGC) is shown in Algorithm 1. The similarity matrix for BSGC can be created by either the weights of the model or the accumulated gradients.\nWeight-based BSGC. Here, we use the weight matrix of a layer as the similarity matrix between neurons of adjacent hidden layers, based on the idea that neurons with strong weights connecting them can be expected to cluster well.\nGradient-based BSGC. Analyzing the gradients of each parameter during training gives us another way to cluster models. The idea is that weights that update together are likely to be part of the same circuit and connect neurons that cluster well together. We set the similarity matrix in Algorithm 1 to the average cosine similarity of the gradients of each parameter."}, {"title": "5. Methodology", "content": "Across a range of models and tasks, we optimize for modularity by training to maximize the clusterability (as defined in Section 3), and use various automated interpretability methods to evaluate the interpretability gains of the clusters thus obtained. This section details our methodology, and in Section 6 we share our results."}, {"title": "5.1. Training for Modularity", "content": "We train models to be modular by following a training pipeline that comprises the following steps:\n1. Train the original model to minimize cross-entropy for the first t steps. This can help to form simpler features before we begin clustering and allow \"winning tickets\" (as defined in the lottery ticket hypothesis (Frankle & Carbin, 2018)) to emerge. In practice, we found that our results do not vary with t, and default to t = 0 for most results.\n2. For the set U of model components to cluster, use a clustering method to cluster them and calculate the clusterability loss for each component. Here, we find that gradient-based BSGC (see Algorithm 1) leads to slightly better initializations (see Figure 2), but the gains do not remain after our training, which is why we use arbitrary clustering instead. Without loss of generality, we make contiguous clusters of the same size for our experiments for better visualization.\n3. Calculate the effective loss function $L_{eff} = L_{CE} + \\lambda \\sum_{U \\in \\mathcal{U}} L_e$, where $\\lambda$ is a hyperparameter that controls the trade-off between performance and modularity. We share results for training on $\\lambda = 40$, but have found results to be stable across various values of $\\lambda$.\n4. Complete the rest of the training for t steps to minimize $L_{eff}$ to promote the clusters to be modular (see Section 3 for more details).\nWe experiment with vanilla MLPs on MNIST (Deng, 2012), and CNNs on the CIFAR-10 dataset (Krizhevsky et al., 2009), transformers on modular addition, and fine-tuning of large language models (LLMs). We compare the performance, clusterability, and interpretability gains of our clustered models against models trained without the clusterability loss. Our results are available in Section 6 and all the hyperparameters we use are given in Appendix A."}, {"title": "5.2. Evaluating Interpretability Gains", "content": "We use various automated interpretability metrics to evaluate our clustered models and their interpretability gains. In this section, we define these metrics: class-wise accuracy (with and without each individual cluster), and Effective Circuit Size (ECS) for each label. (See Section 6 for our results on these metrics.)"}, {"title": "5.2.1. INTERVENTIONS", "content": "We perform two types of interventions on clusters to evaluate their contribution toward a model's computation:\n1. Type 1 / ON-intervention: Here, we turn off (zero-ablate) the activations of all the other clusters, and run the model's forward pass using just the activations of a single cluster. This gives us a measure of the contribution of a cluster on its own toward predicting any given class or datapoint, which tells us whether a cluster is sufficient.\n2. Type 2/ OFF-intervention: Here, we switch off a given cluster, and keep the activations of all the other clusters. This gives us a measure of how well all the other clusters combined can predict any given class or datapoint, which tells us whether a cluster is necessary."}, {"title": "5.2.2. EFFECTIVE CIRCUIT SIZE (ECS)", "content": "We use automated pruning to recursively remove edges that do not contribute (see Automated Circuit DisCovery (Conmy et al., 2023)) to extract the 'effective circuit' for each label and define the ratio of the number of parameters in it to the number of parameters in the whole model to be the effective circuit size (ECS) for that label and model. A lower effective circuit size indicates that the model has learned a smaller task-specific circuit, and reduction in ECS is a proxy for interpretability gains. Since recursively removing edges is expensive, we only show results on simpler models, and leave more efficient pruning techniques for future work."}, {"title": "6. Results", "content": "6.1. Task-Specialization of Clusters in a Network Trained to be Modular\n6.1.1. CLUSTERS SPECIALIZE IN CLASS-LEVEL FEATURES FOR MLPS AND CNNS\nFigure 6 compares class-wise accuracy of the model on the CIFAR-10 dataset with individual clusters turned OFF and ON respectively in a clustered model with an accuracy > 95% for each label. We find individual clusters learning near-complete circuits for various labels for both kinds of interventions described in Section 5.2.1. In other words, clusters specialize on certain labels."}, {"title": "6.1.2. TASK SPECIALIZATION IN MODULAR LANGUAGE MODEL (LLM) FINE-TUNING", "content": "We also do an ON-intervention investigation on GPT2-small fine-tuned on wiki data. In Figure 7a, we see the fraction of samples (left) that need k clusters for correct prediction, for k\u2208 [1,2,3,4] for a clustered model with 4 clusters. Note that in a modular model, most datapoints can be solved by 1-2 clusters, while 3-4 are required for a large fraction in a non-modular model fine-tuned on the same data. See Appendix F for a more elaborate explanation of these values.\nFigure 7b on the right shows the number of datapoints each cluster contributes to, indicating that a modular model reduces the number of clusters involved and keeps them all similarly useful."}, {"title": "6.2. Modularity Encourages Smaller Circuits", "content": "Figure 8 compares the Effective Circuit Size (ECS) for each label for the clustered and unclustered models on CIFAR-10 (see Section 5). We show that an unclustered model has, on average, 61.25% more parameters in its effective circuits. In Appendix B, we share similar results for the MNIST dataset (Deng, 2012)."}, {"title": "7. Theoretical Analysis of the Effects of Modularity", "content": "7.1. Polytopes in a Modular Network\nWe show that the partitioning of neural networks into polytopes on which the network is linear (Sudjianto et al., 2020) becomes coarser when we make a model modular. In other words, the simplicity of the function space of a network increases by adding modularity constraints. In this section, we restrict to fully connected ReLU feed-forward neural networks."}, {"title": "Definition 7.1 (ReLU Network)", "content": "A ReLU network $N: \\mathbb{R}^n \\to \\mathbb{R}^m$, is a composition of $L \\in \\mathbb{N}$ hidden layers given by $x^{(l)} = \\sigma(W^{(l)}x^{(l-1)} + b^{(l)})$, where $\\sigma$ is an element-wise ReLU activation function, $\\sigma(x_i) = \\max{0, x_i}$. We define $x^{(0)} = x$ and the output layer is given by\n$N(x) = W^{(L+1)}x^{(L)} + b^{(L+1)}$.\nThe number of neurons in each layer is given by a vector $N = [n_1, n_2, ..., n_L]$, and all activations are in the positive reals, i.e. $x^{(l)} \\in \\mathbb{R}^N$ for all $l \\in {1, ..., L} = [L]$. We stress the dependence on x by writing $x^{(l)}(x)$."}, {"title": "Proposition 7.2", "content": "[Sudjianto et al. (2020)] For a ReLU network $N: \\mathbb{R}^n \\to \\mathbb{R}^m$ there is a finite partition $\\Omega$ of $\\mathbb{R}^n$ of cardinality $p := #\\Omega$ such that for each part $\\omega \\in \\Omega$ there exists a piece-wise linear function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, and its restriction on $\\omega$, denoted $f|\\omega$, can be described by a linear function:\n$f_{\\omega}(x) = \\alpha_{\\omega}^T x + \\beta_{\\omega}$.\nMoreover, each part is a polytope, given by the intersection of a collection of half-spaces. We write the minimum set of"}, {"title": "Proposition 7.3", "content": "(Johnson-Lindenstrauss lemma (Johnson & Lindenstrauss, 1984)). Let $0 < \\epsilon < 1$ and let $X$ be a set of $m$ points in $\\mathbb{R}^N$ and $n > \\frac{8(\\ln(m))}{\\epsilon^2}$, then there exists a linear map $f: \\mathbb{R}^N \\to \\mathbb{R}^n$ such that for all $u, v \\in X$ we have\n$(1 - \\epsilon)||u - v||^2 \\leq ||f(u) - f(v)||^2 \\leq (1 + \\epsilon)||u - v||^2$.\nIn other words, a set of $m$ nearly orthogonal points in $\\mathbb{R}^N$ can be projected onto nearly orthogonal directions in a much smaller dimensional space $\\mathbb{R}^n$, as long as $n > \\frac{8(\\ln(m))}{\\epsilon^2}$ or equivalently $e^n > m$. This means that the number of points that can be projected orthogonally onto $\\mathbb{R}^n$ is exponential in n.\nGiven a network with neurons $N = [n_1, n_2, ..., n_L]$ where the weight matrix $W^{(l)}$ in layer $x^{(l)}$ is modular and contains k modules such that $n_{l-1}$ and $n_l$ can be divided into k sets $[n_{l-1}^1, ..., n_{l-1}^k]$ and $[n_l^1, ..., n_l^k]$ such that the only non-zero weights in $W^{(l)}$ are between $n_{l-1}^i$ and $n_l^i$.\nThen the number of nearly orthogonal points that can be represented in layer $n_l$ becomes $e^{n_1^1} + ... + e^{n_l^k}$ as opposed to $e^{n_l^1 + ... + n_l^k}$. This means that as a result of modularizing the weight matrix $W^{(l)}$, the model learns to perform computation by using features from an exponentially smaller set of subspaces. Sparse Autoencoders (SAEs) (Cunningham et al., 2023) extract linear subspaces as latents for human-interpretable features, and modularity helps reduce the search space for SAEs exponentially. Training SAEs on modular models and clusters is an interesting direction for future work."}, {"title": "8. Conclusion", "content": "We show that a simple regularizer is effective at splitting a neural network layer into simpler, more interpretable clusters. We show that the average circuit size and the search space improve with more clusters without a decrease in overall performance (for the models and tasks that we investigated).\nFuture work could study the Pareto frontier (clusterability versus performance) as we scale to harder tasks and larger models. We are also interested in using our insights to train and align language models with modularity and see if it leads to better control for harmful behaviors. We hope that modularity can help with a number of mechanistic interpretability goals, and a scaled-up exploration into this (such as clustering attention heads and training SAEs) is an interesting direction for future work."}, {"title": "9. Acknowledgments and Disclosure of Funding", "content": "We would especially like to thank Dylan Cope, Daniel Filan, Sandy Tanwisuth, Siddhesh Pawar, Niels uit de Bos, Matthew Wearden, and Henning Bartsch for valuable discussions, feedback, and support. SG,NS would like to thank the ML Alignment Theory & Scholars (MATS) Program, the organizers, funders, and staff. SG was supported by independent research grants from AI Safety Support and the Long-Term Future Fund. This research was supported by the Center for AI Safety Compute Cluster. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors."}]}