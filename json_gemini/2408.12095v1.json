{"title": "uMedSum: A Unified Framework for Advancing Medical Abstractive Summarization", "authors": ["Aishik Nagar", "Yutong Liu", "Andy T. Liu", "Viktor Schlegel", "Vijay Prakash Dwivedi", "Arun-Kumar Kaliya-Perumal", "Guna Pratheep Kalanchiam", "Yili Tang", "Robby T. Tan"], "abstract": "Medical abstractive summarization faces the challenge of balancing faithfulness and informativeness. Current methods often sacrifice key information for faithfulness or introduce confabulations when prioritizing informativeness. While recent advancements in techniques like in-context learning (ICL) and fine-tuning have improved medical summarization, they often overlook crucial aspects such as faithfulness and informativeness without considering advanced methods like model reasoning and self-improvement. Moreover, the field lacks a unified benchmark, hindering systematic evaluation due to varied metrics and datasets. This paper addresses these gaps by presenting a comprehensive benchmark of six advanced abstractive summarization methods across three diverse datasets using five standardized metrics. Building on these findings, we propose uMedSum, a modular hybrid summarization framework that introduces novel approaches for sequential confabulation removal followed by key missing information addition, ensuring both faithfulness and informativeness. Our work improves upon previous GPT-4-based state-of-the-art (SOTA) medical summarization methods, significantly outperforming them in both quantitative metrics and qualitative domain expert evaluations. Notably, we achieve an average relative performance improvement of 11.8% in reference-free metrics over the previous SOTA. Doctors prefer uMedSum's summaries 6 times more than previous SOTA in difficult cases where there are chances of confabulations or missing information. These results highlight uMedSum's effectiveness and generalizability across various datasets and metrics, marking a significant advancement in medical summarization.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have shown exceptional performance in generative tasks, including zero-shot and out-of-the-box applications in specialized areas like summarization (Lei et al. 2023; Van Veen et al. 2024). In the medical field, document summarization holds promise for greatly improving the efficiency of medical staff in reviewing lengthy documents, such as medical exam reports or patient histories. However, the stochastic nature of LLMs and their lack of formal guarantees (Li et al. 2023; Schlegel et al. 2023) often lead to summaries that deviate from input documents, limiting their practical usability.\nThis is particularly problematic in the medical domain, where accurate and complete information is crucial for effective decision-making. Doctors rely on summaries that capture all relevant details without introducing erroneous information, emphasizing two critical aspects of medical summarization: faithfulness and informativeness. Lack of faithfulness results in confabulations, where parts of the summary contain information not present in the input document (Maynez et al. 2020). Insufficient informativeness leads to omitting relevant details from the input document (Mao et al. 2020). Such summaries can provide doctors with incomplete evidence or inaccurate information, potentially leading to misdiagnoses or inappropriate treatment decisions, ultimately impacting patient outcomes.\nCurrent efforts in enhancing faithfulness and informativeness for summarization face several limitations: (i) Many techniques address only specific sub-problems (e.g., faithfulness or informativeness) in isolation; (ii) Most approaches focus on reference-based confabulation detection, while reference-free confabulation removal in summarization remains an open problem. Moreover, overzealous removal of confabulated content may lead to summaries with missing information; (iii) Existing methods often rely on either purely abstractive or extractive techniques, without leveraging both; and (iv) Hybrid exceptions to the previous point, such as Constrained Abstractive Summarization (CAS) (Mao et al. 2020), which aim to add missing information into abstractive summaries, inherit limitations from their constituent parts. Specifically, confabulations in the initial abstractive summary will persist in the final summary.\nAdvancements in medical summarization are further impeded by the lack of standardized benchmarks, driven by inconsistent metric and dataset choices in prior studies, and insufficient evaluations of faithfulness and informativeness. Recent approaches rely on task adaptation such as in-context learning (ICL) (Van Veen et al. 2024) and parameter-efficient fine-tuning like QLORA (Dettmers et al. 2024), but neglect recent model reasoning advancements (Chang et al. 2024). Secondly, by mainly showing improvement on reference-based metrics, they neglect important summary aspects not captured by those, such as faithfulness and informativeness (Maynez et al. 2020).\nIn light of these considerations, we present a comprehensive medical summarization benchmark and a large-scale"}, {"title": "2 Related Work", "content": "Summarization is usually approached by extractive and abstractive approaches (Nenkova, McKeown et al. 2011; Luo, Xue, and Niu 2024). Extractive summarization selects key sentences or phrases directly from the input document. For example, more recent extractive summarization approaches explore semantic matching (Zhong et al. 2020), domain-specific term extraction using BERT embeddings (Sammet and Krestel 2023), and advancements in keyphrase extraction with pre-trained language models (Song, Feng, and Jing 2023). Abstractive summarization, conversely, aims to rephrase content for more concise and readable summaries. More recent advancements over traditional sequence-to-sequence approaches (Lewis et al. 2020; Zhang et al. 2020) include element-aware steering of summary content (Wang, Zhang, and Wang 2023), coherence assessment for long documents (Chang et al. 2024), and reinforcement learning for factual consistency (Roit et al. 2023). Notably, most existing work applies either extractive or abstractive techniques in isolation, potentially limiting their effectiveness. Furthermore, challenges remain with confabulations (Maynez et al. 2020) and incompleteness (Mao et al. 2020).\nConfabulated Information Detection. Confabulation detection, or identifying information not grounded in the input document, is a major challenge in medical summarization. While existing methods typically provide a general factuality score for generated summaries (Maynez et al. 2020; Liu et al. 2024; Ji et al. 2023), they often fail to systematically remove confabulated information. Some techniques attempt to address epistemic uncertainty by leveraging internal logit-level data (Yadkori et al. 2024; Manakul, Liusie, and Gales 2023; Chen et al. 2024; Farquhar et al. 2024), but these are generally limited to question-answering scenarios with clear ground truths and require access to inter-"}, {"title": "3 uMedSum: Faithfulness and Informativeness in Medical Summarization", "content": "Summarization Benchmark. To address the lack of a systematic benchmark for summarization methods in medical summarization, we first evaluate four recent methods: Standard Prompting (baseline), Element-Aware Summarization with Large Language Models (Wang, Zhang, and Wang 2023), Chain of Density (Addams et al. 2023), and Hierarchical Summarization (Chang et al. 2024). Each technique offers distinct benefits and drawbacks. For instance, Element-Aware Summarization enhances content relevance by targeting domain-specific elements, while Chain of Density produces information-dense but less readable summaries. Hierarchical Summarization effectively addresses the \"lost-in-the-middle\" issue in long contexts (Ravaut et al. 2023). We then combine the top-performing methods with task adaptation strategies, particularly In-Context Learning, which outperforms QLORA for similar tasks (Van Veen et al. 2024). This step ensures the highest possible quality for the initial summary, laying a strong foundation for uMedSum.\nuMedSum. The uMedSum pipeline is designed to produce high-quality, faithful, and comprehensive medical summaries through a three-stage process, visualized in Figure 1: Initial Summary Generation, Confabulation Removal, and Missing Information Addition. Each stage of uMedSum is a modular, self-contained component that can be independently updated and tuned, providing flexibility and efficiency with minimal computational overhead. This modularity allows for seamless integration with both open and closed-source models during inference."}, {"title": "3.1 Stage 1: Initial Summary Generation", "content": "The combination of best-performing methods and models from the benchmark is selected for further evaluation and enhancement using the stages of uMedSum. In the first stage, we generate an initial abstractive summary given the input document."}, {"title": "3.2 Stage 2: Confabulation Removal\n(Faithfulness)", "content": "The uMedSum pipeline takes a novel approach by repurposing Natural Language Inference (NLI) models to not just evaluate the factuality of generated summaries but to directly detect and remove discrete confabulated information from generated summaries. This approach differs from existing methods, which typically focus on sentence-level entailment or entity-based splitting (Lei et al. 2023), by introducing a more granular decomposition based on atomic facts (Thirukovalluru, Huang, and Dhingra 2024; Nawrath et al. 2024; Stacey et al. 2023). Specifically, we propose a two-step process: (1) summary decomposition into smaller, manageable units, and (2) pairwise NLI-based confabulation detection and removal.\nSummary Decomposition. We begin by decomposing the summary generated in Stage 1 into smaller units called Summary Content Units (SCUs) (Nawrath et al. 2024) or Decomposed Summary Units (DSUs). We propose Recursive Threshold-based Text Segmentation to further split sentences into clause-level atomic facts. Unlike previous works that stop at sentence-level decomposition or rely on entity-based splitting for further decomposition (Lei et al. 2023), our approach aims to create self-contained units that encapsulate atomic facts. Atomic facts encapsulate the smallest meaningful statements that can stand alone as true or false propositions. This aligns well with the NLI task, where the goal is to determine the logical relationship (entailment, contradiction, or neutrality) between two statements. This atomic view of facts allows us to detect confabulations precisely.\nFormally, let $D_k$ represent a decomposed summary unit (DSU) from summary $S_i$, where k indexes the specific unit. The NLI model computes the entailment score $E(D_k)$ for each DSU, where the score $E(D_k)$ represents the probability distribution over entailment labels (entailment, neutral, contradiction).\nRecursive Threshold-Based Text Segmentation (RTB-TS). The algorithmic details are described as follows:\n\u2022 Initial Segmentation: We begin by decomposing the summary $S_i$ into DSUs $D_k$ using a sentence boundary disambiguation technique: $S_i \\rightarrow \\{D_1, D_2,..., D_k\\}$. This initial step provides a coarse segmentation based on sentence boundaries.\n\u2022 Pairwise NLI Scoring: For each DSU $D_k$, we compute the entailment score $E(D_k)$ using a fine-tuned NLI model:\n$E(D_k) = P(entailment | I, D_k)$, (1)\n$N(D_k) = P(neutral | I, D_k)$, (2)\n$C(D_k) = P(contradiction | I, D_k)$. (3)"}, {"title": "3.3 Stage 3: Missing Information Addition\n(Informativeness)", "content": "Hybrid methods like (Mao et al. 2020) risk confabulated initial summaries. Our approach separates confabulation removal (Stage 2) before adding missing key information (Stage 3), reducing the chance of new confabulations in the final summary. To capture key information from the input document, we identify key sentences in the document and key phrases in the Stage 2 summary. We introduce a novel approach to measure coverage of key information in the summary and integrate missing information into the appropriate sections of the summary to maintain consistency and readability.\nKey Information Extraction. Our extracted key information from either the input document or the summary is described as follows. Let $K_{doc} = \\{k_{doc}^i | i < top_m\\}$ for the source document, and $K_{summ} = \\{k_{summ}^i | i < top_n\\}$ for the generated summary. Where $K_{doc}$ are key sentences from the input document; $K_{summ}$ are key phrases from our summary; $top_m$ and $top_n$ are the thresholds for the number of key sentences and key phrases, respectively.\nFor the input document, we use sentences as the minimum unit of granularity for extraction. For the summary generated in Stage 2, we apply a key phrase extraction method, such as the one used by Grootendorst (2020), which extracts n-grams as key phrases. We then iteratively rank the sentences or phrases using MMR (Bennani-Smires et al. 2018) and select the top-K as key sentences or key phrases. The complete algorithm for this process is described in the Appendix.\nMissing Key Information Detection. Given $K_{doc}$ and $K_{summ}$ extracted from the input document and generated summary respectively, we calculate coverage scores $cov_{score}^i$ for each $k_{doc}^i$ based on $K_{summ}$. Specifically, we compute the embedding matrices for key sentences and key phrases (Reimers and Gurevych 2019). Let $Embed_{doc}$ represent the matrix formed by stacking the embeddings of the key sentences from the input document, and $Embed_{summ}$ represent the matrix formed by stacking the embeddings of the key phrases from the Stage 2 summary. Here, $Embed_{doc}$ is of size $m \\times d$, where m is the number of key sentences in the input document, and d is the embedding dimension. Similarly, $Embed_{summ}$ is of size $n \\times d$, where n is the number of key phrases in the Stage 2 summary.\nThe similarities between $K_{doc}$ and $K_{summ}$ are computed as the dot product of the document and summary embedding matrices, yielding a similarity matrix $[sim^{i,j}]_{m \\times n}$. Coverage scores for key sentences in the document are then determined by taking the maximum similarity for each sentence across the key phrases from summary, resulting in a vector $Cov_{score}$ of size $m \\times 1$.\nWe define the coverage score of the i-th sentence as $COV_{score}^i = max_{j < n}\\{sim^{i,j}\\}$ and introduce a threshold parameter $cov_{min}$. Any $k_{doc}$ with a coverage score below $COV_{min}$ is considered missing information. The set of potential missing information is represented as:\n$K_{missing} = \\{k_{doc}^i | i \\leq m, cov_{score}^i < COV_{min}\\}$.\nMerging Missing Information to Summary. We use perplexity (PPL) to select the best location to insert a missing key sentence $k_{missing}$  $K_{missing}$ into our summary (Sharma et al. 2024):\n$l^* = argmin_{l \\epsilon locs} PPLLM(k_{missing}, summary, l)$, (9)\nwhere summary is the summary obtained from Stage 2. We employ a greedy algorithm to dynamically insert the missing information. The complete algorithm will be provided in the appendix."}, {"title": "4 Evaluation", "content": "We compare state-of-the-art approaches to medical summarization and improve the best-performing ones using uMedSum. We demonstrate the improvements both through quantitative measurements and qualitative insights from a study conducted by domain experts.\nFigure 2 describes the datasets, models and techniques chosen for our experimental setup. We make use of three biomedical datasets for summarization tasks: MIMIC III for Radiology Report Summarization (Johnson et al. 2016), MeQSum for Patient Question Summarization (Abacha and Demner-Fushman 2019), and ACI-Bench for doctor-patient dialogue summarization (Yim et al. 2023). These provide a diverse range of biomedical summarization task settings, with varying document lengths, requirement for background knowledge as well as the need for domain-specific vocabulary and understanding."}, {"title": "Evaluation Metrics", "content": "Maynez et al. (2020) found that reference-based metrics by themselves do not align with human perception of faithfulness and factuality in abstractive summarization tasks and should be combined with reference-free metrics. We thus make use of two reference-based metrics, ROUGE-LSum (Lin 2004) and BERTScore (Zhang et al. 2019) which assess content overlap and semantic similarity with the reference summary, in combination with three reference-free metrics, SummaC (Laban et al. 2022), QuestEval (Scialom et al. 2021), and Entailment Scores (Liu et al. 2024) which evaluate factual consistency, informativeness, and entailment relative to the source document for purposes of our evaluation."}, {"title": "Experiment Setup", "content": "We benchmark the performance of four models: LLaMA3 (8B) (Meta 2024), Gemma (7B) (Team et al. 2024), Meditron (7B) (Chen et al. 2023a,b), and GPT-4 (Achiam et al. 2023).\nIn Stage 2, we use two NLI models for our experiments, DeBERTa v3 finetuned on NLI datasets (He et al. 2021; Laurer 2023), as well as biomedical finetuned PubMedBERT NLI model (Gu et al. 2021; lighteternal 2023). To obtain the DSU's, sentence level decomposition is performed using PySBD (Sadvilkar and Neumann 2020) and atomic fact decomposition is obtained from the model used in Stage 1. Additionally, the modular setup of uMedSum allows us to compare the confabulation detection of dedicated NLI models with LLM based techniques such as Self-Reflection (Ji et al. 2023) as an ablation study.\nFor Stage 3, we use all-MiniLM-L6-v2 (Wang et al. 2020) as the encoder for key information extraction and missing information detection, following the Sentence-BERT (Reimers and Gurevych 2019) framework. Additionally, we set $cov_{min}$ to 0.4; this parameter can be tuned based on the specific coverage metrics or models employed. When merging missing information, we use GPT-2 (Radford et al. 2019) to calculate the perplexity and rearrange the added sentences to improve fluency and coherence (Sharma et al. 2024).\nFormally, let $S_i$ represent the summary generated by the i-th summarization technique. The quality of this summary is evaluated using a set of metrics $M_i$, where j denotes the specific metric used (e.g., ROUGE-L, BERTScore, SummaC). The score for a given metric $M_j$ applied to a summary $S_i$ is denoted as $M_j(S_i)$. The final evaluation score for a summary $S_i$ generated by a specific method is determined by aggregating its rank across all metrics:\n$Rank = \\sum_{j=1}^n Rank(M_j(S_i))$, (10)\nwhere $Rank(M_j(S_i))$ is the rank of the method based on metric $M_i$, and n is the total number of metrics used. The method with the lowest Rank, is considered the most effective. Since NLI is used directly in confabulation detection in Stage 2, we provide two separate rankings for a more objective comparison: one considering entailment and one without."}, {"title": "5 Results and Discussion", "content": "Figure 3 presents the benchmark results of different summarization techniques. Table 1 presents the full set of results for the benchmark, as well as results of uMedSum based experiments. The benchmark results are structured into three key areas: the performance of summarization methods, the influence of datasets, and the comparative evaluation of models."}, {"title": "5.1 Summarization Techniques Benchmark", "content": "Methods. The Standard Prompting method was selected as the baseline. We first compare the different summarization techniques in zero-shot settings without task adaptation. Chain of Density particularly improves the performance of the QuestEval metric, which aims to measure the factual information retention between input documents and summaries. This can be attributed to its nature of creating the most information-dense summaries, albeit at the cost of readability and conciseness. The Hierarchical method demonstrates noticeable performance in summarizing longer documents, effectively mitigating the \"lost-in-the-middle\" effect (Ravaut et al. 2023). This is evidenced by consistent improvements across all models when employing hierarchical summarization, particularly for lengthy inputs like those in ACI Bench. The approach enhances faithfulness to the input document of summaries by decomposing documents into manageable blocks. While these methods excel in specific areas, Element Aware Summarization outperforms them by leveraging model reasoning to extract the most relevant information, summarize it effectively, and achieve the best rank across all models.\nTask adaptation using ICL consistently improves both reference-based and reference-free metrics across all datasets. Our findings align with Van Veen et al. (2024), confirming GPT-4 Standard Prompting with ICL as the previous SOTA for medical summarization. However, our benchmark reveals that Element Aware Summarization with ICL-based task adaptation surpasses this previously established SOTA.\nThis demonstrates that task adaptation complements model reasoning techniques in enhancing summary quality.\nDatasets. Figure 3 suggests that for MIMIC-III, models perform worse on phrase overlap metrics such as ROUGE-LSum while maintaining relatively high scores in reference-free and reference-based semantic metrics such as BERTScore and SummaC, indicating that the models tend to paraphrase or compress the information in the input document while staying consistent and faithful to the inputs for MIMIC-III.\nMeQSum contains the shortest input documents and involves summarizing patient questions, which requires less background knowledge but a clear understanding of the query. The models perform the best on MeQSum across most metrics, particularly in reference-free metrics like QuestEval and Entailment, reflecting the models' ability to handle content low on domain-specific jargon. Notably, the shorter and less technical nature of MeQSum allows smaller models like Gemma 7B and Llama 3 8B to perform competitively with GPT-4, as the task requires a clear understanding of short queries rather than extensive domain knowledge or information extraction capabilities.\nDue to its conversational nature and length, ACI Bench requires the summarization of long context documents that might include more redundant or less structured information. Task adaptation using ICL particularly helps in this dataset, where giving the models examples of the kind of information to focus on in the input significantly improves"}, {"title": "Models", "content": "As shown in Table 1, Meditron 7B exhibits the lowest performance across most metrics and datasets when using Standard Prompting. Due to its limited instruction-following capabilities, we only consider Meditron for Standard Prompting. Gemma 7B shows weak performance in metrics like RougeLSum and SummaC. It particularly struggles with Entailment across all datasets, showing poor ability to maintain logical consistency and faithfulness in summaries. Llama 3 8B performs the best among the open-source models, often showing competitive performance to GPT-4 for the given tasks. Llama 3 8B also benefits more from ICL than Gemma, which highlights its strong ability to adapt to tasks. Consistent with the findings of Van Veen et al. (2024), we find that unsurprisingly, GPT-4 performs best across all summarization tasks. Overall, we find that GPT-4 performs best, with Llama3 8B being the best-performing open-source model. Based on these findings, we select the two models: Llama 3 8B and GPT-4, and two methods: the previously established SOTA of Standard Prompting as well as the best-performing technique based on our experiments, Element Aware Summarization, for the next stage of experiments using uMedSum."}, {"title": "5.2 Analysis of uMedSum Results", "content": "Table 1 demonstrates that uMedSum consistently outperforms the above-mentioned benchmark results, with seven out of the top ten ranked methods utilizing uMedSum. We especially see significant improvement in reference-free metrics that assess the factual consistency and completeness of the summaries, such as SummaC, QuestEval, and Entailment, while being competitive or improving performance in reference-based metrics. This indicates that uMedSum improves faithfulness and informativeness of the summaries, while staying grounded to the input document. uMedSum's impact is most pronounced when combined with Element Aware Summarization and ICL, suggesting that it can be used in combination with methods leveraging model reasoning as well as task adaptation techniques to produce summaries that utilize the key benefits of all the methods.\nFor all datasets, uMedSum helps improve ROUGE-LSum, particularly with Llama 3 8B. uMedSum also maintains a high BertScore across datasets, particularly with GPT-4. This suggests that the additional stages of confabulated information removal and missing information addition preserve and even enhance semantic similarity between generated and reference summaries by focusing on error correction and gap-filling. Notably, SummaC and Entailment scores significantly improve for all models when using uMedSum. These metrics directly benefit from the confabulation detection and removal stage, as they ensure that the final summary is factually consistent and faithful to the source information. QuestEval scores show marked improvements as well. The missing information addition stage (Stage 3) proves particularly beneficial, ensuring comprehensive coverage of key aspects of the input document. Lastly, we point out that uMedSum significantly improves summarization quality for smaller models. For instance, Llama3 8B with uMedSum and ICL outperforms GPT-4's Standard Prompting baseline and remains competitive with GPT-4 across all metrics, despite starting from a significantly lower baseline performance.\nAblation Studies. We conducted ablation studies by removing individual stages and comparing performance against the complete framework. Results show that Stages 2 and 3 complement each other, with net gains across both reference-based and reference-free metrics, leading to more comprehensive and faithful summaries. Additional ablations explored different NLI models (Gu et al. 2021; Laurer 2023) and LLM-based hallucination removal methods like self-reflection (Ji et al. 2023). Stage 2 using a DeBERTa-based finetuned NLI model (Laurer 2023) performed best across datasets and models. Full ablation results are provided in the technical appendix."}, {"title": "5.3 Clinician Evaluation", "content": "We perform a human evaluation by two orthopaedic surgeons for the radiology report summarization task, who are provided with related summaries generated using the previous SOTA (Standard Prompting ICL + GPT-4), and our best performing method (Element Aware + ICL uMedSum + GPT-4). Doctors performed pairwise selections based on overall summary quality and annotated difficult cases with confabulations or missing key information, without knowing the methods which generated the summaries. Our results show that when there were no confabulations or missing information, doctors showed equal preference between the previous SOTA and uMedSum. However, in difficult cases involving confabulations or missing key information, doctors preferred uMedSum 46% of the time, citing its effectiveness in resolving issues, compared to only 8% for the previous SOTA. Both summaries were considered inadequate 23% of the time, acceptable 15%, and undecidable in 8% of cases. This preference underscores the critical importance of uMedSum in minimizing errors in medical summaries, as the impact of resolving confabulations or missing information far outweighs the benefit of matching previous methods in straightforward cases when considering patient care. Full clinical study details are provided in the appendix."}, {"title": "6 Conclusion", "content": "We introduce uMedSum, a novel framework for accurate and informative medical summarization. We conduct a comprehensive benchmark and integrate the findings with uMedSum to surpass recent SOTA on medical summarization. We achieve a significant 11.8% improvement in reference-free metrics which focus on faithfulness and informativeness without sacrificing reference-based performance. The human evaluation shows doctors preferred uMedSum six times more than the previous SOTA in the presence of confabulations or the absence of key information. Our approach sets a new standard for faithful and informative medical summarization."}, {"title": "A.1 Algorithm 1", "content": "In Algorithm 1, topk represents the threshold for the number of key sentences or key phrases to extract. K is a list of key sentences extracted from input document or key phrases extracted from Stage 2 summary, and len(K) represents the number of extracted key sentences or key phrases in the K."}, {"title": "A.2 Algorithm 2", "content": "In Algorithm 2, topk represents the thresholds for the number of missing information to merge, PPLLM(kmissing, summout, l) represents the perplexity of the text formed by inserting i-th missing information kmising after l-th sentence of (updated) Stage 2 summary summout, and insert(kmissing, summout,l*) represents inserting i-th missing information kmising after l-th sentence of (updated) Stage 2 summary summout."}, {"title": "B Clinical Evaluation", "content": "Our evaluators include two orthopaedic surgeons, who are provided with related summaries generated using 2 methods:\nStandard Prompting ICL + GPT-4 (Previous SOTA)\nElement Aware + ICL uMedSum + GPT-4 (overall best performing method)\nWe specifically selected summaries relevant to the clinicians in order to fully utilize their expertise during evaluation. Thus, from the subset of MIMIC-III (Johnson et al."}, {"title": "C Hyperparameter Search", "content": "For Stage 2 and Stage 3 of uMedSum, we perform a qualitative analysis of threshold values. For Stage 2, we perform grid search for Entailment (Te), Contradiction (Tc), Atomic Fact Entailment (Ta), while for stage 3, we perform grid search for Number of Key Sentences to Extract (top) and Minimum Coverage Score (covmin) thresholds. This analyis is performed on a subset of all the datasets used for evaluation, but sample non-intersecting, separate data points from each dataset for fixing thresholds so as to not overfit on the test sample. 40 such data points from each dataset are used for fixing the thresholds. We used summaries generated by GPT-4 + ICL + uMedSum for the threshold selection process.\nFor Stage 2, in order to optimize the process of selecting the optimal thresholds, we first start with Te and Te fixed to their most extreme values to maximise the condition for \"uncertain\" DSU's to be split into atomic fact. In this setting, we then find the optimal Ta which can be reasonably used without overzealous removal or retention of information presented in atomic facts. Next, We fix Te and Ta and find the optimal values for Te, and finally use the same process to find the optimal values for Te. The final thresholds"}, {"title": "D Prompts", "content": "Table 3 provides the dataset specific prompts used for the experiments. These prompts were used as-is for the Standard Prompting experiments. For other methods, these prompts were combined with method specific instructions and logic, which can be found in the code."}, {"title": "E Ablation Study", "content": "Table 2 presents the full ablation study performed. We start from the best performing summarisation method from the benchmark - Element Aware Summarisation, and first add each stage of uMedSum separately. Finally, we test on separate configurations of confabulation removal on the end-to-end uMedSum pipeline. We conduct first conduct an ablation study using Standard Prompting + ICL which was established as the previous SOTA, and comparing the impact of LLM based techniques such as self-reflection with our proposed Stage-2 using DeBERTa. We find that for standard prompting, self-reflection performs better.\nWe then conduct a full ablation study using our best performing technique (Element Aware Summarisation). In this ablation, we begin with the base Element Aware method, and incrementally add different stages of uMedSum to the generated summary. Thus, in the next step, we implement Stage 2 with DeBERTa as well as Self-reflection, followed by task adaptation using ICL. Since the results suggest that NLI based Stage 2 performs better than self-reflection based stage 2, we use DeBERTa based stage 2 and evaluate the impact of Element Aware + ICL with stage 2 and stage 3 separately.\nFinally, we implement ablations on the final framework, where we again implement 2 different NLI models as well as self-reflection and evaluate the impact on the final uMedSum results. The ablation results suggest that the final uMedSum gives the most balanced results, with all 5 ablations using the full framework achieving 5 out of the top 6 ranks. Additionally, using Stage 2 and Stage 3 together complements performance, as seen from the fact that there is an improvement in both reference based as well as reference free metrics in Element Aware + ICL + Stage 2 (DeBERTa) + Stage 3 as compared to Element Aware + ICL + Stage 2 (DeBERTa) and Element Aware + ICL + Stage 3.\nWe also confirm that Element Aware outperforms Standard Prompting (Previous SOTA) for both NLI based as well as self-reflection based methods for Stage 2, further reinforcing the benchmark results.\nThe results show that the best rank is obtained by uMedSum using DeBERTa as the NLI module for confabulation detection in Stage 2 achieves the best performance. The experiments with Stage 3 are ranked high since they directly optimize for the reference based metrics, which might not always necessarily lead to better abstractive summaries but do improve the reference based metrics.\nConsidering a holistic improvement across both reference based and reference free metrics, the end-to-end pipeline for uMedSum still performs the best."}]}