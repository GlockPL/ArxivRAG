{"title": "uMedSum: A Unified Framework for Advancing Medical Abstractive Summarization", "authors": ["Aishik Nagar", "Yutong Liu", "Andy T. Liu", "Viktor Schlegel", "Vijay Prakash Dwivedi", "Arun-Kumar Kaliya-Perumal", "Guna Pratheep Kalanchiam", "Yili Tang", "Robby T. Tan"], "abstract": "Medical abstractive summarization faces the challenge of balancing faithfulness and informativeness. Current methods often sacrifice key information for faithfulness or introduce confabulations when prioritizing informativeness. While recent advancements in techniques like in-context learning (ICL) and fine-tuning have improved medical summarization, they often overlook crucial aspects such as faithfulness and informativeness without considering advanced methods like model reasoning and self-improvement. Moreover, the field lacks a unified benchmark, hindering systematic evaluation due to varied metrics and datasets. This paper addresses these gaps by presenting a comprehensive benchmark of six advanced abstractive summarization methods across three diverse datasets using five standardized metrics. Building on these findings, we propose uMedSum, a modular hybrid summarization framework that introduces novel approaches for sequential confabulation removal followed by key missing information addition, ensuring both faithfulness and informativeness. Our work improves upon previous GPT-4-based state-of-the-art (SOTA) medical summarization methods, significantly outperforming them in both quantitative metrics and qualitative domain expert evaluations. Notably, we achieve an average relative performance improvement of 11.8% in reference-free metrics over the previous SOTA. Doctors prefer uMedSum's summaries 6 times more than previous SOTA in difficult cases where there are chances of confabulations or missing information. These results highlight uMedSum's effectiveness and generalizability across various datasets and metrics, marking a significant advancement in medical summarization.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have shown exceptional performance in generative tasks, including zero-shot and out-of-the-box applications in specialized areas like summarization (Lei et al. 2023; Van Veen et al. 2024). In the medical field, document summarization holds promise for greatly improving the efficiency of medical staff in reviewing lengthy documents, such as medical exam reports or patient histories. However, the stochastic nature of LLMs and their lack of formal guarantees (Li et al. 2023; Schlegel et al. 2023) often lead to summaries that deviate from input documents, limiting their practical usability.\nThis is particularly problematic in the medical domain, where accurate and complete information is crucial for effective decision-making. Doctors rely on summaries that capture all relevant details without introducing erroneous information, emphasizing two critical aspects of medical summarization: faithfulness and informativeness. Lack of faithfulness results in confabulations, where parts of the summary contain information not present in the input document (Maynez et al. 2020). Insufficient informativeness leads to omitting relevant details from the input document (Mao et al. 2020). Such summaries can provide doctors with incomplete evidence or inaccurate information, potentially leading to misdiagnoses or inappropriate treatment decisions, ultimately impacting patient outcomes.\nCurrent efforts in enhancing faithfulness and informativeness for summarization face several limitations: (i) Many techniques address only specific sub-problems (e.g., faithfulness or informativeness) in isolation; (ii) Most approaches focus on reference-based confabulation detection, while reference-free confabulation removal in summarization remains an open problem. Moreover, overzealous removal of confabulated content may lead to summaries with missing information; (iii) Existing methods often rely on either purely abstractive or extractive techniques, without leveraging both; and (iv) Hybrid exceptions to the previous point, such as Constrained Abstractive Summarization (CAS) (Mao et al. 2020), which aim to add missing information into abstractive summaries, inherit limitations from their constituent parts. Specifically, confabulations in the initial abstractive summary will persist in the final summary.\nAdvancements in medical summarization are further impeded by the lack of standardized benchmarks, driven by inconsistent metric and dataset choices in prior studies, and insufficient evaluations of faithfulness and informativeness. Recent approaches rely on task adaptation such as in-context learning (ICL) (Van Veen et al. 2024) and parameter-efficient fine-tuning like QLORA (Dettmers et al. 2024), but neglect recent model reasoning advancements (Chang et al. 2024). Secondly, by mainly showing improvement on reference-based metrics, they neglect important summary aspects not captured by those, such as faithfulness and informativeness (Maynez et al. 2020).\nIn light of these considerations, we present a comprehensive medical summarization benchmark and a large-scale"}, {"title": "2 Related Work", "content": "Summarization is usually approached by extractive and abstractive approaches (Nenkova, McKeown et al. 2011; Luo, Xue, and Niu 2024). Extractive summarization selects key sentences or phrases directly from the input document. For example, more recent extractive summarization approaches explore semantic matching (Zhong et al. 2020), domain-specific term extraction using BERT embeddings (Sammet and Krestel 2023), and advancements in keyphrase extraction with pre-trained language models (Song, Feng, and Jing 2023). Abstractive summarization, conversely, aims to rephrase content for more concise and readable summaries. More recent advancements over traditional sequence-to-sequence approaches (Lewis et al. 2020; Zhang et al. 2020) include element-aware steering of summary content (Wang, Zhang, and Wang 2023), coherence assessment for long documents (Chang et al. 2024), and reinforcement learning for factual consistency (Roit et al. 2023). Notably, most existing work applies either extractive or abstractive techniques in isolation, potentially limiting their effectiveness. Furthermore, challenges remain with confabulations (Maynez et al. 2020) and incompleteness (Mao et al. 2020).\nConfabulated Information Detection. Confabulation detection, or identifying information not grounded in the input document, is a major challenge in medical summarization. While existing methods typically provide a general factuality score for generated summaries (Maynez et al. 2020; Liu et al. 2024; Ji et al. 2023), they often fail to systematically remove confabulated information. Some techniques attempt to address epistemic uncertainty by leveraging internal logit-level data (Yadkori et al. 2024; Manakul, Liusie, and Gales 2023; Chen et al. 2024; Farquhar et al. 2024), but these are generally limited to question-answering scenarios with clear ground truths and require access to inter-"}, {"title": "3 uMedSum: Faithfulness and Informativeness in Medical Summarization", "content": "Summarization Benchmark. To address the lack of a systematic benchmark for summarization methods in medical summarization, we first evaluate four recent methods: Standard Prompting (baseline), Element-Aware Summarization with Large Language Models (Wang, Zhang, and Wang 2023), Chain of Density (Addams et al. 2023), and Hierarchical Summarization (Chang et al. 2024). Each technique offers distinct benefits and drawbacks. For instance, Element-Aware Summarization enhances content relevance by targeting domain-specific elements, while Chain of Density produces information-dense but less readable summaries. Hierarchical Summarization effectively addresses the \"lost-in-the-middle\" issue in long contexts (Ravaut et al. 2023). We then combine the top-performing methods with task adaptation strategies, particularly In-Context Learning, which outperforms QLORA for similar tasks (Van Veen et al. 2024). This step ensures the highest possible quality for the initial summary, laying a strong foundation for uMedSum.\nuMedSum. The uMedSum pipeline is designed to produce high-quality, faithful, and comprehensive medical summaries through a three-stage process, visualized in Figure 1: Initial Summary Generation, Confabulation Removal, and Missing Information Addition. Each stage of uMedSum is a modular, self-contained component that can be independently updated and tuned, providing flexibility and efficiency with minimal computational overhead. This modularity allows for seamless integration with both open and closed-source models during inference."}, {"title": "3.1 Stage 1: Initial Summary Generation", "content": "The combination of best-performing methods and models from the benchmark is selected for further evaluation and enhancement using the stages of uMedSum. In the first stage, we generate an initial abstractive summary given the input document."}, {"title": "3.2 Stage 2: Confabulation Removal\n(Faithfulness)", "content": "The uMedSum pipeline takes a novel approach by repurposing Natural Language Inference (NLI) models to not just evaluate the factuality of generated summaries but to directly detect and remove discrete confabulated information from generated summaries. This approach differs from existing methods, which typically focus on sentence-level entailment or entity-based splitting (Lei et al. 2023), by introducing a more granular decomposition based on atomic facts (Thirukovalluru, Huang, and Dhingra 2024; Nawrath et al. 2024; Stacey et al. 2023). Specifically, we propose a two-step process: (1) summary decomposition into smaller, manageable units, and (2) pairwise NLI-based confabulation detection and removal.\nSummary Decomposition. We begin by decomposing the summary generated in Stage 1 into smaller units called Summary Content Units (SCUs) (Nawrath et al. 2024) or Decomposed Summary Units (DSUs). We propose Recursive Threshold-based Text Segmentation to further split sentences into clause-level atomic facts. Unlike previous works that stop at sentence-level decomposition or rely on entity-based splitting for further decomposition (Lei et al. 2023), our approach aims to create self-contained units that encapsulate atomic facts. Atomic facts encapsulate the smallest meaningful statements that can stand alone as true or false propositions. This aligns well with the NLI task, where the goal is to determine the logical relationship (entailment, contradiction, or neutrality) between two statements. This atomic view of facts allows us to detect confabulations precisely.\nFormally, let Dk represent a decomposed summary unit (DSU) from summary Si, where k indexes the specific unit. The NLI model computes the entailment score E(Dk) for each DSU, where the score E(Dk) represents the probability distribution over entailment labels (entailment, neutral, contradiction).\nRecursive Threshold-Based Text Segmentation (RTB-TS). The algorithmic details are described as follows:\n\u2022 Initial Segmentation: We begin by decomposing the summary Si into DSUs Dk using a sentence boundary disambiguation technique: Si \u2192 {D1,D2,...,Dk}. This initial step provides a coarse segmentation based on sentence boundaries.\n\u2022 Pairwise NLI Scoring: For each DSU Dk, we compute the entailment score E(Dk) using a fine-tuned NLI model:\n$E(D_k) = P(entailment | I, D_k),$\n(1)\n$N(D_k) = P(neutral | I, D_k),$\n(2)\n$C(D_k) = P(contradiction | I, D_k).$\n(3)"}, {"title": "3.3 Stage 3: Missing Information Addition\n(Informativeness)", "content": "Hybrid methods like (Mao et al. 2020) risk confabulated initial summaries. Our approach separates confabulation removal (Stage 2) before adding missing key information (Stage 3), reducing the chance of new confabulations in the final summary. To capture key information from the input document, we identify key sentences in the document and key phrases in the Stage 2 summary. We introduce a novel approach to measure coverage of key information in the summary and integrate missing information into the appropriate sections of the summary to maintain consistency and readability.\nKey Information Extraction. Our extracted key information from either the input document or the summary is described as follows. Let Kdoc = {kdoc | i < topm} for the source document, and Ksumm = {ksumm | i < topn} for the generated summary. Where Kdoc are key sentences from the input document; Ksumm are key phrases from our summary; top\u2122 and tops are the thresholds for the number of key sentences and key phrases, respectively.\nFor the input document, we use sentences as the minimum unit of granularity for extraction. For the summary generated in Stage 2, we apply a key phrase extraction method, such as the one used by Grootendorst (2020), which extracts n-grams as key phrases. We then iteratively rank the sentences or phrases using MMR (Bennani-Smires et al. 2018) and select the top-K as key sentences or key phrases. The complete algorithm for this process is described in the Appendix.\nscore\nMissing Key Information Detection. Given Kdoc and Ksumm extracted from the input document and generated summary respectively, we calculate coverage scores covec for each koc based on Ksumm. Specifically, we compute the embedding matrices for key sentences and key phrases (Reimers and Gurevych 2019). Let Embeddoc represent the matrix formed by stacking the embeddings of the key sentences from the input document, and Embedsumm represent the matrix formed by stacking the embeddings of the key phrases from the Stage 2 summary. Here, Embeddoc is of size m \u00d7 d, where m is the number of key sentences in the input document, and d is the embedding dimension. Similarly, Embedsumm is of size n \u00d7 d, where n is the number of key phrases in the Stage 2 summary.\nThe similarities between Kdoc and Ksumm are computed as the dot product of the document and summary embedding matrices, yielding a similarity matrix [sim\u00b2,1]mxn. Coverage scores for key sentences in the document are then determined by taking the maximum similarity for each sentence across the key phrases from summary, resulting in a vector Covscore of size m \u00d7 1.\ni\nscore\nWe define the coverage score of the i-th sentence as $COV_i = max_{j<n}{sim_{ij}}^\text{score}$ and introduce a threshold parameter covmin. Any kdoc with a coverage score below COVmin is considered missing information. The set of potential missing information is represented as:\nKmissing = {kdoc | i \u2264 m, cove <COVmin}.\nscore\nMerging Missing Information to Summary. We use perplexity (PPL) to select the best location to insert a missing key sentence komis missing \u2208 Kmissing into our summary (Sharma et al. 2024):\nl* = argmin PPLLM(kmissing, summary, 1), (9)\nlelocs\nwhere summary is the summary obtained from Stage 2. We employ a greedy algorithm to dynamically insert the missing information. The complete algorithm will be provided in the appendix."}, {"title": "4 Evaluation", "content": "We compare state-of-the-art approaches to medical summarization and improve the best-performing ones using uMedSum. We demonstrate the improvements both through quantitative measurements and qualitative insights from a study conducted by domain experts.\nDataset and Tasks Figure 2 describes the datasets, models and techniques chosen for our experimental setup. We make use of three biomedical datasets for summarization tasks: MIMIC III for Radiology Report Summarization (Johnson et al. 2016), MeQSum for Patient Question Summarization (Abacha and Demner-Fushman 2019), and ACI-Bench for doctor-patient dialogue summarization (Yim et al. 2023). These provide a diverse range of biomedical summarization task settings, with varying document lengths, requirement for background knowledge as well as the need for domain-specific vocabulary and understanding."}, {"title": "5 Results and Discussion", "content": "5.1 Summarization Techniques Benchmark\nFigure 3 presents the benchmark results of different summarization techniques. Table 1 presents the full set of results for the benchmark, as well as results of uMedSum based experiments. The benchmark results are structured into three key areas: the performance of summarization methods, the influence of datasets, and the comparative evaluation of models."}, {"title": "5.2 Analysis of uMedSum Results", "content": "Table 1 demonstrates that uMedSum consistently outperforms the above-mentioned benchmark results, with seven out of the top ten ranked methods utilizing uMedSum. We especially see significant improvement in reference-free metrics that assess the factual consistency and completeness of the summaries, such as SummaC, QuestEval, and Entailment, while being competitive or improving performance in reference-based metrics. This indicates that uMedSum improves faithfulness and informativeness of the summaries, while staying grounded to the input document. uMedSum's impact is most pronounced when combined with Element Aware Summarization and ICL, suggesting that it can be used in combination with methods leveraging model reasoning as well as task adaptation techniques to produce summaries that utilize the key benefits of all the methods.\nFor all datasets, uMedSum helps improve ROUGE-LSum, particularly with Llama 3 8B. uMedSum also maintains a high BertScore across datasets, particularly with GPT-4. This suggests that the additional stages of confabulated information removal and missing information addition preserve and even enhance semantic similarity between generated and reference summaries by focusing on error correction and gap-filling. Notably, SummaC and Entailment scores significantly improve for all models when using uMedSum. These metrics directly benefit from the confabulation detection and removal stage, as they ensure that the final summary is factually consistent and faithful to the source information. QuestEval scores show marked improvements as well. The missing information addition stage (Stage 3) proves particularly beneficial, ensuring comprehensive coverage of key aspects of the input document. Lastly, we point out that uMedSum significantly improves summarization quality for smaller models. For instance, Llama3 8B with uMedSum and ICL outperforms GPT-4's Standard Prompting baseline and remains competitive with GPT-4 across all metrics, despite starting from a significantly lower baseline performance.\nAblation Studies. We conducted ablation studies by removing individual stages and comparing performance against the complete framework. Results show that Stages 2 and 3 complement each other, with net gains across both reference-based and reference-free metrics, leading to more comprehensive and faithful summaries. Additional ablations explored different NLI models (Gu et al. 2021; Laurer 2023) and LLM-based hallucination removal methods like self-reflection (Ji et al. 2023). Stage 2 using a DeBERTa-based finetuned NLI model (Laurer 2023) performed best across datasets and models. Full ablation results are provided in the technical appendix."}, {"title": "5.3 Clinician Evaluation", "content": "We perform a human evaluation by two orthopaedic surgeons for the radiology report summarization task, who are provided with related summaries generated using the previous SOTA (Standard Prompting ICL + GPT-4), and our best performing method (Element Aware + ICL uMedSum + GPT-4). Doctors performed pairwise selections based on overall summary quality and annotated difficult cases with confabulations or missing key information, without knowing the methods which generated the summaries. Our results show that when there were no confabulations or missing information, doctors showed equal preference between the previous SOTA and uMedSum. However, in difficult cases involving confabulations or missing key information, doctors preferred uMedSum 46% of the time, citing its effectiveness in resolving issues, compared to only 8% for the previous SOTA. Both summaries were considered inadequate 23% of the time, acceptable 15%, and undecidable in 8% of cases. This preference underscores the critical importance of uMedSum in minimizing errors in medical summaries, as the impact of resolving confabulations or missing information far outweighs the benefit of matching previous methods in straightforward cases when considering patient care. Full clinical study details are provided in the appendix."}, {"title": "6 Conclusion", "content": "We introduce uMedSum, a novel framework for accurate and informative medical summarization. We conduct a comprehensive benchmark and integrate the findings with uMedSum to surpass recent SOTA on medical summarization. We achieve a significant 11.8% improvement in reference-free metrics which focus on faithfulness and informativeness without sacrificing reference-based performance. The human evaluation shows doctors preferred uMedSum six times more than the previous SOTA in the presence of confabulations or the absence of key information. Our approach sets a new standard for faithful and informative medical summarization."}, {"title": "A Algorithms", "content": "A.1 Algorithm 1\nIn Algorithm 1, topk represents the threshold for the number of key sentences or key phrases to extract. K is a list of key sentences extracted from input document or key phrases extracted from Stage 2 summary, and len(K) represents the number of extracted key sentences or key phrases in the K.\nAlgorithm 1: Key Information Extraction\n1: input \u2190 document or Stage 2 summary\n2: K \u2190 {}\n3: if input = document then\n4: candidates \u2190 each sentence in document\n5: else\n6: candidates \u2190 each phrase in summary\n7: end if\n8: while len(K) < topk do\n9: candidate* := argmaxx\u2208candidates MMR(input, K, x)\n10: K \u2190 K + candidate*\n11: end while\n12: return K\nA.2 Algorithm 2\nIn Algorithm 2, topk represents the thresholds for the number of missing information to merge, PPLLM(kmissing, summout, 1) represents the perplexity of the text formed by inserting i-th missing information kmising after l-th sentence of (updated) Stage 2 summary summout, and insert(kmissing, summout,l*) represents inserting i-th missing information kmising after l-th sentence of (updated) Stage 2 summary summout.\nAlgorithm 2: Merge Missing Information to Summary\n1: summout \u2190 Stage 2 summary\n2: i \u2190 0\n3: while i < topk do\n4: s \u2190 number of sentences in summout\n5: locs := {1 | 0 < 1 <s}\n6: l* := argmint\u2208locs PPLLM(kmissing, summout, l)\n7: summout \u2190 insert(kmissing, summout, l*)\n8: i \u2190 i + 1\n9: end while\n10: return summout"}, {"title": "B Clinical Evaluation", "content": "Our evaluators include two orthopaedic surgeons, who are provided with related summaries generated using 2 methods:\n1. Standard Prompting ICL + GPT-4 (Previous SOTA)\n2. Element Aware + ICL uMedSum + GPT-4 (overall best performing method)\nWe specifically selected summaries relevant to the clinicians in order to fully utilize their expertise during evaluation. Thus, from the subset of MIMIC-III (Johnson et al. 2016) which was used for the experiments, we selected a subset of 60 samples filtered using the fillowing keywords:\nArthritis, Bone, Clavicle, Deformity, Dislocation, Femur, Fibula, Fracture, Humerus, Intervertebral Disc, Joint, Ligament, Malunion, Non-union, Osteophyte, Patella, Radius, Sacrum, Scapula, Scoliosis, Sondylolisthesis, Spondylosis, Spine, Spur, Tibia, Ulna, Union.\nThe clinicians were asked to select the preferred summary in pairwise fashion. This meant that for each input document, they would be provided 2 summaries: A summary generated by method 1 and another by method 2. The clinicians would not be aware of the model or technique which generated the summaries, in order to avoid any bias. The clinicans were asked to evaluate both the summaries according to the following criteria:\n1. Which summary do they prefer between the two summaries?\n2. Is there any information which should be removed from either of the summaries?\n3. Is there any key information missing from either of the summaries?\nThis allowed us to measure the general quality of the summary, confabulations in generated summaries, missing key information in the generated summaries. We only selected cases where the clinicians had a consensus on the preference between both the summaries. We classify difficult cases as cases where either of the summaries contains confabulations or missing information according to any of the clinicians based on their annotations. If not, the summaries are considered straightforward."}, {"title": "C Hyperparameter Search", "content": "For Stage 2 and Stage 3 of uMedSum, we perform a qualitative analysis of threshold values. For Stage 2, we perform grid search for Entailment (Te), Contradiction (Tc), Atomic Fact Entailment (Ta), while for stage 3, we perform grid search for Number of Key Sentences to Extract (top) and Minimum Coverage Score (covmin) thresholds. This analyis is performed on a subset of all the datasets used for evaluation, but sample non-intersecting, separate data points from each dataset for fixing thresholds so as to not overfit on the test sample. 40 such data points from each dataset are used for fixing the thresholds. We used summaries generated by GPT-4 + ICL + uMedSum for the threshold selection process.\nFor Stage 2, in order to optimize the process of selecting the optimal thresholds, we first start with Te and Te fixed to their most extreme values to maximise the condition for \"uncertain\" DSU's to be split into atomic fact. In this setting, we then find the optimal Ta which can be reasonably used without overzealous removal or retention of information presented in atomic facts. Next, We fix Te and Ta and find the optimal values for Te, and finally use the same process to find the optimal values for Te. The final thresholds"}, {"title": "D Prompts", "content": "Table 3 provides the dataset specific prompts used for the experiments. These prompts were used as-is for the Standard Prompting experiments. For other methods, these prompts were combined with method specific instructions and logic, which can be found in the code."}, {"title": "E Ablation Study", "content": "Table 2 presents the full ablation study performed. We start from the best performing summarisation method from the benchmark - Element Aware Summarisation, and first add each stage of uMedSum separately. Finally, we test on separate configurations of confabulation removal on the end-to-end uMedSum pipeline. We conduct first conduct an ablation study using Standard Prompting + ICL which was established as the previous SOTA, and comparing the impact of LLM based techniques such as self-reflection with our proposed Stage-2 using DeBERTa. We find that for standard prompting, self-reflection performs better."}, {"title": null, "content": "We then conduct a full ablation study using our best performing technique (Element Aware Summarisation). In this ablation, we begin with the base Element Aware method, and incrementally add different stages of uMedSum to the generated summary. Thus, in the next step, we implement Stage 2 with DeBERTa as well as Self-reflection, followed by task adaptation using ICL. Since the results suggest that NLI based Stage 2 performs better than self-reflection based stage 2, we use DeBERTa based stage 2 and evaluate the impact of Element Aware + ICL with stage 2 and stage 3 separately.\nFinally, we implement ablations on the final framework, where we again implement 2 different NLI models as well as self-reflection and evaluate the impact on the final uMedSum results. The ablation results suggest that the final uMedSum gives the most balanced results, with all 5 ablations using the full framework achieving 5 out of the top 6 ranks. Additionally, using Stage 2 and Stage 3 together complements performance, as seen from the fact that there is an improvement in both reference based as well as reference free metrics in Element Aware + ICL + Stage 2 (DeBERTa) + Stage 3 as compared to Element Aware + ICL + Stage 2 (DeBERTa) and Element Aware + ICL + Stage 3.\nWe also confirm that Element Aware outperforms Standard Prompting (Previous SOTA) for both NLI based as well as self-reflection based methods for Stage 2, further reinforcing the benchmark results.\nThe results show that the best rank is obtained by uMedSum using DeBERTa as the NLI module for confabulation detection in Stage 2 achieves the best performance. The experiments with Stage 3 are ranked high since they directly optimize for the reference based metrics, which might not always necessarily lead to better abstractive summaries, but do improve the reference based metrics.\nConsidering a holistic improvement across both reference based and reference free metrics, the end-to-end pipeline for uMedSum still performs the best."}]}