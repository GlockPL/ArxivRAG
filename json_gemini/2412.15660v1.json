{"title": "Adaptable and Precise: Enterprise-Scenario LLM Function-Calling Capability Training Pipeline", "authors": ["Guancheng Zeng", "Wentao Ding", "Beining Xu", "Chi Zhang", "Wenqiang Han", "Gang Li", "Jingjing Mo", "Pengxu Qiu", "Xinran Tao", "Wang Tao", "Haowen Hu"], "abstract": "Enterprises possess a vast array of API assets scattered across various functions, forming the backbone of existing business processes. By leveraging these APIs as functional tools, enterprises can design diverse, scenario-specific agent applications, driven by on-premise function-calling models as the core engine. However, generic models often fail to meet enterprise requirements in terms of computational efficiency, output accuracy, and stability, necessitating scenario-specific adaptation. In this paper, we propose a training pipeline for function-calling capabilities tailored to real-world business scenarios. This pipeline includes the synthesis and augmentation of scenario-specific function-calling data, model fine-tuning, and performance evaluation and analysis. Using this pipeline, we generated 1,260 fully AI-generated samples and 1,035 augmented manually-labeled samples in digital HR agent scenario. The Qwen2.5-Coder-7B-Instruct model was employed as the base model and fine-tuned using the LoRA method on four GPUs with 24GB VRAM. Our fine-tuned model demonstrated outstanding performance in evaluations and practical applications, surpassing GPT-4 and GPT-40 in accuracy on the test set. These results validate the reliability of the proposed pipeline for training scenario-specific function-calling models.", "sections": [{"title": "1 Introduction", "content": "In the era of generative AI, AI agents are defined as autonomous systems capable of handling tasks independently [1, 2]. These agents can not only respond directly to user queries but also perform complex task decomposition and execution by invoking various functional tools [3, 4]. With a vast knowledge base and the ability to interact with external environments, AI agents demonstrate significant potential for deep integration with existing enterprise systems [5]. Across industries, enterprises are actively exploring the integration of AI agent systems into real business scenarios [6]. This is driven by the abundance of API assets within enterprise systems, which, when leveraged as functional tools, enable AI agents to deeply integrate with existing systems, automate workflows across specialized business scenarios, and enhance operational efficiency.\nAI agents require LLMs (Large Language Models) as their core reasoning engines to generate instructions for invoking functions and APIs [7]. While both open-source and commercial LLMs perform well in general function-calling tasks given the extensive training data [8, 9], these models often struggle to provide accurate and stable function-calling instructions in specialized enterprise scenarios. Errors in instruction structure, function selection, and parameter input still occur due to the lack of domain-specific training data. Furthermore, to prevent data leakage [10], enterprises typically opt to deploy on-premise LLMs rather than utilizing LLMs served on public cloud platform. However, due to the limited computational resources and lack of experience in training LLMs, it is difficult to implement AI agent effectively for Small and Medium Enterprises. Therefore, there is an urgent need to identify an efficient training pipeline suitable for developing small-scale models in enterprise scenarios, so that SMEs can easily train and deploy their own agent models based on their needs.\nIn this work, we designed a specialized training pipeline for function-calling capabilities tailored to professional scenarios. This pipeline includes data synthesis and augmentation for scenario-specific function-calling task, SFT (Supervised Fine-Tuning) generic function calling models using LoRA [11], and comprehensive evaluation and analysis of model performance. Specifically, we used 14 workflow sets and generated 1,260 fully AI-synthesis and 1,035 manually augmented function-calling training samples in a digital HR intelligent agent system, allowing the model to better align with actual business preferences in tool selection and parameter extraction. The Qwen2.5-Coder-7B-Instruct model [12] was used as the base model and fine-tuned using LoRA within approximately five hours on four A10 GPUs (24GB VRAM each). The fine-tuned model exhibited exceptional performance, surpassing GPT-4 and GPT-40 in function calling output structural completeness, tool selection accuracy, and parameter input accuracy, both on the test set and in real-world usage. These results demonstrated the effectiveness of our pipeline in enhancing the function-calling capabilities of moderate-size LLMs for professional scenarios under limited computational resources.\nIn summary, our main contributions are as follows:\n\u2022 We developed a function-calling training pipeline, enabling data synthesis, model fine-tuning, and evaluation for various enterprise-specific tools, significantly improving the performance of generic models in professional business scenario.\n\u2022 We trained a 7B function-calling LLM for the digital HR scenarios. The model exhibited outstanding performance in both evaluations and real-world usage, surpassing GPT-4 and GPT-40 in test set accuracy.\nThe remainder of the paper is organized as follows: Sec.2 refers to related works of data synthesis and model evaluation on Function Calling; Sec.3 describes the pipeline of our work, including initial setting, data synthesis, model training and evaluation; Sec.4 contains our experiments as well as results; ablation study and analysis are in Sec.5."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Function-Calling Data Synthesis", "content": "The introduction of Large Language Models (LLMs) has catalyzed a significant paradigm shift in the field of deep learning [13]. Their ability to produce text that rivals human quality renders them invaluable for generating synthetic data, thereby alleviating the long-standing dilemma of data quality and quantity in Natural Language Processing (NLP) [14, 15, 16]. LLMs have also demonstrated remarkable proficiency in generating function-calling data.Toolformer [17], for instance, enhances an LLM's ability to identify retrievable information through function calls, by annotating examples and using augmented data for training, thus enabling the model to effectively determine tool usage. Similarly, ToolLLaMA [18] leverages over 16,000 API instances and datasets generated by ChatGPT to enhance its handling of both simple and complex multi-tool instructions, showcasing robust generalization with novel API documentation. To further simulate real user instructions and enhance the richness and complexity of commands, ToolACE [19] and ToolAlpaca [20] employ multiple LLM agents to simulate multi-turn interactions, generating tool usage examples. Additionally, to ensure data quality, both ToolACE and APIGen [21] implement multiple validation processes to filter data that adhere to correct invocation formats and align the semantic consistency between instruction goals and API outputs.\nWhile LLMs with general Function-calling capabilities are emerging in abundance, there is still no standardized process for fine-tuning Function-calling LLMs in specific professional domains. To address this, we provide a comprehensive workflow for data synthesis, model fine-tuning, and model"}, {"title": "2.2 Function-Calling Evaluation Metrics", "content": "During model training, the loss function indicates the error for each forward pass, guiding the gradient updates in back-propagation. However, relying solely on loss often fails to capture the model's task-specific performance. Researchers therefore employ discrete evaluation metrics like code execution success, multiple choice and integer scores to assess performance. The field has developed task-specific evaluation benchmarks such as MMLU, CMMLU, C-Eval, GSM8K, ELM, and OpenCompass [22, 23, 24, 25, 26, 27], enhancing model assessment across diverse tasks like code synthesis and commonsense reasoning.\nIn function-calling studies, numerous evaluation methodologies have been proposed, leading to the emergence of both open-source and proprietary benchmarks. These benchmarks are designed to assess a model's function-calling abilities, often relying on synthetic methods to construct datasets due to the scarcity of function-calling data compared to regular types of data. For instance, APIBench [28] builds its evaluation data using APIs from platforms such as TorchHub, TensorHub, and Hugging Face to assess models' accuracy and hallucination rates in function calls. For certain task types, there is a greater emphasis on multi-turn and multi-step function-calling. ToolBench (ToolEval) [29] collects tool APIs from the RapidAPIHub platform and uses ChatGPT to generate diverse instructions for utilizing these APIs. Similarly, AgentBoard [30] provides nine categories of tasks and evaluates models' multi-turn agent interaction capabilities using ToolOperation and ToolQuery. The BFCL [31] framework introduced a Live Dataset in its V2 version, built from real-time user-contributed function documentation and queries, to evaluate models' ability to operate effectively in dynamic environments. Its V3 version further incorporated multi-step and multi-turn evaluation logic.\nFor evaluating results, multiple assessment methods have been developed. ToolBench employs GPT-series models to evaluate model performance based on pass rates. On the other hand, benchmarks like APIBench and BFCL utilize Abstract Syntax Trees (AST) to parse the structure of model-generated function calls. This approach enables a multi-faceted evaluation without actually executing the call instructions, allowing for more efficient assessments. Additionally, the BFCL framework offers the Exec method, which evaluates the results of executing the model-generated function-calling instructions, making it more suitable for multi-turn dialogue evaluation. In practical use, researchers typically choose evaluation methods based on their specific needs and available resources."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Overview", "content": "In enterprise environments, characterized by relatively high labor costs, it is crucial to minimize manual involvement [32]. Therefore, we designed a highly automated training pipeline for function-calling capabilities in enterprise models tailored to such scenarios, as shown in Figure 1.\nThis pipeline consists of three main modules:\n\u2022 Data Synthesis Module: This module generates user questions based on function tool information and creates corresponding function-calling instructions. It enhances and filters the generated questions to further improve the quantity and quality of the data [33]. The resulting data is then assembled and divided into fine-tuning training sets and evaluation sets [34].\n\u2022 Model Fine-Tuning Module: Using the training set, this module fine-tunes a generic function-calling model via LoRA [11] and integrates the resulting weights back into the original model [35].\n\u2022 Model Evaluation Module: After training, the model undergoes AST-based evaluation to assess its performance [36]. Additionally, the module performs a confusion matrix analysis of the model's function selection distribution, identifying its precision and areas of confusion for different function tools [37]."}, {"title": "3.2 Initial Settings", "content": null}, {"title": "3.2.1 Generic Model", "content": "In enterprise scenarios, computational resources are often limited, and small to medium-sized enterprises typically struggle to support the fine-tuning of large-parameter models [38, 39]. As a result, smaller-parameter models are required. Furthermore, these models need to possess general function-calling capabilities to enable the transfer of their abilities from general scenarios to enterprise-specific contexts [40]. Previous research has demonstrated that models with 7B parameters can effectively handle general function-calling tasks, achieving high accuracy levels in real-world applications [41]."}, {"title": "3.2.2 Scenario API Tools", "content": "Depending on the task, models can utilize various types of tools, including APIs, algorithms, code workflows, operational pipelines, and even other models. The model must be able to select these tools correctly based on their format and provide the appropriate input parameters [42]. To enable stable data synthesis and tool usage, users need to supply the following information about each function tool: Tool Name, Tool Description, Parameter Names, Parameter Descriptions, Parameter Data Types, Parameter Necessity [15]. Furthermore, for parameters that require high accuracy, users can enhance the stability of tool invocation by providing specific examples and default values for the input parameters [34]. Descriptions of function tools and their parameters must be provided by the user, which should clearly convey the tool's functionality and parameter requirements. The higher the quality of these descriptions, the better the quality of the data generated, which in turn enhances the outcomes of model training [43]."}, {"title": "3.2.3 Human-Annotated Seed Data", "content": "To ensure that the data closely align with real-world scenarios, the pipeline could initially incorporate a small amount of manually annotated data as seed data for the data synthesis module. Compared to fully AI-generated data, seed data annotated by business experts is better aligned with actual question-and-answer patterns, improving the quality of the generated data. This, in turn, enhances the model's stability and accuracy [44].\nDuring manual annotation, several key aspects must be considered:\n\u2022 Diversity: Questions should exhibit diversity in expression, including variations in sentence structure, content, and phrasing [45].\n\u2022 Uniqueness: Each data entry should be unique in terms of questions and parameters, avoiding repetition of names (e.g., people, places, departments, or projects) [46].\n\u2022 Scale: The seed data should be of sufficient size, with its volume proportionally allocated according to the importance of the function [41].\n\u2022 Consistency: The internal logic of the seed data must remain consistent, with uniform output formats and standardized parameter names for the same functionality [47].\nBy adhering to these principles, the seed data ensures a high-quality foundation for data synthesis and model training, optimizing overall performance in specific application scenarios."}, {"title": "3.3 Data Synthesis", "content": "In this process, questions are first generated based on the descriptions of scenario-specific function tools[21, 45]. These questions, along with a small number of manually annotated question-answer pairs, are used as seed data for data augmentation [46]. The augmented data are then utilized as the training set to fine-tune the model [48]."}, {"title": "3.3.1 Generating Seed Questions Based on API Description", "content": "During model fine-tuning, it has been observed that the diversity, quality, and quantity of the dataset directly influence training outcomes [21]. Diversity is critical for the model's performance on unseen tasks or instructions [34, 49], quality impacts its accuracy on known tasks [48], and quantity effectively prevents the model from over-fitting to specific tasks. When synthesizing function-calling training datasets, the design of seed questions plays a pivotal role in determining the model's final performance. These seed data must cover as many task scenarios as possible while adhering to the logical boundaries of the tool's functionality. The generated questions must not deviate from the purpose described in the tool [34, 48].\nTo achieve this, we designed various customization prompt templates in the pipeline, incorporating elements such as role settings, generation rules, and output formats. These templates must meet the following requirements:\n\u2022 Role Setting: Define the identity of the user asking the question, such as a salesperson, domain expert, or engineer, ensuring that the roles are diverse, clearly defined, and logically aligned with the scenario [50].\n\u2022 Generation Rules: Specify requirements for data synthesis, such as the number of questions generated per prompt, question length, required content, content to avoid, and language of the generated questions [49, 51].\n\u2022 Output Format: Standardize the output format to facilitate automated data counting and dataset assembly. Unnecessary prefixes and suffixes in the generated output should be minimized [48, 50].\nWhile research indicates that small-parameter models can produce more data with the same resource consumption, potentially enhancing training outcomes [52, 53], their limited instruction-following capabilities make them unsuitable for complex prompt templates required in specialized scenarios. As a result, such models often produce data with formatting issues and high redundancy. Therefore, for generating initial seed data in professional contexts, high-performance models remain essential [51, 53]."}, {"title": "3.3.2 Data Augmentation Based on Seed Questions", "content": "The pipeline, starting from seed questions, broadens scenario coverage through data augmentation. This method, compared to direct annotation, lowers resource consumption. It also proves more stable than generating all data simultaneously and reduces the chance of producing duplicates. Data augmentation, like data synthesis, depends on LMs and prompt templates. However, the diversity required in augmentation directions necessitates a broader array of templates. Overall, our pipeline employs four data augmentation strategies: replacement, rewriting, simplification, and error introduction. The definition and function of these four augmentation strategies are shown in Table 1. Samples of augmentation prompt templates in digital HR scenario can be found in Appendix B.\nIn practice, replacement and rewriting are the most commonly used augmentation methods, while simplification and error introduction are employed less frequently. Multiple augmentation methods can be applied to the same seed question. Note that error introduction should be based on already-augmented questions to avoid excessive similarity to the original seed questions."}, {"title": "3.3.3 Generating Function Calling Instructions Based on Questions", "content": "After generating the questions, function-calling instructions can be created based on these questions and included in the dataset as labels and answers. The generation of instructions primarily relies on the extraction of key parameters [17]. At this stage, it is generally necessary to use high-performance models for parameter extraction to ensure consistency between the extracted function-"}, {"title": "3.3.4 Data Validation and Assembly", "content": "Since certain parameters in specific problems are generated synthetically by the model and may not accurately reflect real-world conditions, the final augmented training data must undergo a validation process [55]. This involves checking their structural integrity and verifying the accuracy of parameter assignments.\nAfter data validation, the remaining dataset is divided into training and validation sets while ensuring that all usage scenarios of the tools are covered [56]. Additionally, the data format must be adjusted to align with the requirements of the training and evaluation frameworks. Typically, each data entry should include the user's question, the model's tool-calling instructions, and a list of available tools, formatted in JSON. Commonly used formats include shareGPT, Alpaca, and OpenAI formats.\nRegarding the list of available tools, it must align with the task coverage of the scenarios and provide all the required tools [57]. The number of available tools and the length of their descriptions should be kept within a certain range to avoid exceeding the model's context window size and to reduce the complexity of tool selection [58]. During data assembly, the order of the tool list in the training data should be randomized to prevent the model from over-fitting to specific sequences of tool combinations [59]."}, {"title": "3.4 Model Fine-tuning", "content": null}, {"title": "3.4.1 LORA", "content": "To prevent over-parametrization during scenario-specific task adaptation and to save computational resources, the process employs the LoRA (Low-Rank Adaptation) fine-tuning approach [11]. This reduces the number of parameters required for adaptation while effectively preserving the base model's general capabilities. During training, the model's context length must always exceed the length of the training data, avoiding truncation of longer data entries to prevent structural disruption, which could affect training performance [58]. The number of training epochs should not be excessively short or long, as this may result in under-fitting or over-fitting on the training dataset."}, {"title": "3.4.2 Merging LoRA Adapters", "content": "After completing LoRA fine-tuning, the LoRA adapters are merged to the base model to facilitate further fine-tuning tasks. Additionally, the pipeline includes a multi-adapter merging strategy, which encompasses various processing methods such as linear combination, SVD [61], and parameter concatenation [62]. These strategies enable the merging of weight matrices from different LoRA adapters prior to integration, addressing training scenarios such as multi-task fine-tuning, and data feedback fine-tuning, supports continuous updates to the model after deployment. This is crucial for business scenario as data often follows a life-cycle with continuous replacement of updated data. Furthermore, with this multi-LoRA adapter merging approach, the influence of different training data can be encapsulated within separate LoRA adapters. By selecting specific LoRA adapters, it becomes easier to mitigate the impact of certain data cycles or tasks on the model's performance, offering enhanced flexibility in managing the model's behavior and adaptability."}, {"title": "3.5 Model Evaluation", "content": null}, {"title": "3.5.1 AST Evaluation", "content": "To effectively verify the correctness of the function-calling instructions generated by the model, evaluation must address multiple aspects, including structural completeness of the instructions, accuracy of function selection, and accuracy of parameter inputs. The AST (Abstract Syntax Tree) evaluation method enables a step-by-step parsing of the model's output structure, allowing the correctness of function-calling instructions to be assessed without actually executing the function tools [63]. This capability makes AST-based evaluation particularly useful for assessing AI-generated function-calling instructions that contain fictitious parameters, a scenario where Exec (Execution) methods often struggle. The Exec method requires actual execution of the model's output instructions and evaluates correctness based on the returned results [64]. However, instructions with fictitious parameters\u2014such as nonexistent IDs, names of people, places, or companies\u2014cannot be executed, especially for query-based API tools. In addition, since AST evaluation does not involve actual API execution, it is not constrained by API response times, enabling faster verification of model outputs [31].\nIn our pipeline, we adopted the open-source BFCL evaluation framework as the primary benchmark for our practical experiments, modifying it to enhance its suitability for assessing scenario-specific function-calling ability evaluation [31]. This framework provides a rapid and comprehensive evaluation system, including AST-based instruction parsing methods and error analysis features. These features facilitated advanced analyses of the model's function-calling outputs, enabling deeper insights into the causes of errors and refining the evaluation process."}, {"title": "3.5.2 Confusion Matrix for Tool Selection", "content": "In addition to exploring the causes for errors in function-calling instructions, tool selection can be evaluated as a multi-class classification task, given that the model selects a specific tool from a list of available tools. This evaluation employs a confusion matrix to analyze the accuracy of tool selection [65]. Specifically, each function tool category is treated as an independent class, and a confusion matrix is constructed where rows represent the actual tool categories and columns represent the model's predicted categories. This approach facilitates the computation of the number of instances where the model accurately predicted a tool for each real tool's usage scenario [37].\nUsing the confusion matrix, the usage of a target tool is categorized as a positive instance, whereas the usage of alternative tools is categorized as a negative instance. This enables the calculation of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN) for each tool in its respective usage scenario. These metrics, along with their definitions, are listed in Table 2."}, {"title": "Precision", "content": "\u2022  Precision: The proportion of correct predictions among all instances where the model predicts the use of a specific tool.\n$$Precision = \\frac{TP}{TP + FP}$$\n(1)"}, {"title": "Recall", "content": "\u2022  Recall: The proportion of correct tool selections in the total number of instances where the tool is actually used in the scenario.\n$$Recall = \\frac{TP}{TP + FN}$$\n(2)"}, {"title": "F1 Score", "content": "\u2022  F1 Score: The harmonic mean of precision and recall, providing a comprehensive assessment of the model's performance in selecting a specific tool.\n$$F_1 = 2. \\frac{Precision \\cdot Recall}{Precision + Recall}$$\n(3)\nThese metrics reflect the model's capability to select tools and can be employed to compare its performance before and after fine-tuning, as well as against high-performance models.\nFurthermore, conducting confusion matrix analysis with evaluation results from high-performance generic models in zero-shot setting can provide insights into the correspondence between tool descriptions and user queries, as well as the confusion degree between tools. Tools with low precision rate are more likely to interfere with the selection of other tools, while tools with low recall rate are more likely to be affected by other tools. Based on these observations, practitioners can optimize the functional scope and descriptive information of tools prior to data synthesis. This ensures that each tool has clearly differentiated functionality and that its description aligns with its logical use case within the given scenario, which helps reduce tool confusion and improves the overall accuracy of tool selection by the model."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experimental Background", "content": "In our experiment, we tested our pipeline in a digital HR intelligent agent scenario within a large corporate group, involving over 8,000 employees. Users could interact with the intelligent agent in Chinese, inquiring about information related to the company's employees and departments. The experiment provided a total of 14 specialized workflows with distinct functionalities, each encapsulated in the format of function tools. The model needs to interpret user's queries accurately, pass them as parameters to the workflows. Workflow will automatically query the relevant data from the database, summarize the results, and then deliver them back to the users."}, {"title": "4.2 Experimental Setup", "content": null}, {"title": "4.2.1 Data Synthesis", "content": "In this experiment, the training data were synthesized in two stages: seed question generation and data augmentation. These tasks were completed using GPT-4 equipped with appropriate prompt"}, {"title": "4.2.2 Model Fine-tuning", "content": "In this experiment, we selected Qwen2.5-Coder-7B-Instruct as the base model and performed LoRA fine-tuning with four NVIDIA A10 GPUs (24GB VRAM each). Under the default hyper-parameter settings, the batch size was set to 1, with a gradient accumulation step of 16 to simulate large-batch gradient updates under limited computational resources. The learning rate warm-up ratio was set to 0.1, with a peak learning rate of 8.0 \u00d7 10\u22125, followed by a cosine learning rate decay to gradually reduce the learning rate after reaching its peak.\nThe training process used bf16 precision and ran for 10 iterations. For testing, checkpoints were typically selected between the 7th and 10th iterations. During ablation experiments, the same number of data iterations was maintained to ensure consistency. In the LoRA configuration, the rank size r"}, {"title": "4.2.3 Model Evaluation", "content": "In this experiment, we evaluated the function-calling instructions generated by the model using the AST method. A total of 207 manually annotated seed questions were selected as the  DHR_test_A test set to compare the effects of different hyper-parameter settings and mixed data on model fine-tuning. Additionally, we designed 135 distinct questions as the DHR_test_B test set to prevent evaluation bias caused by potential over-fitting to seed data.\nAfter obtaining the model's output and its overall accuracy score, we further analyzed the distribution of error types in certain test scenarios. These error types included Structure Errors (SE), Tool Errors (TE), Parameter Errors (PE). These errors are calculated in certain orders during evaluation: Function-calling instructions with structural parsing errors could not be evaluated for tool selection or parameter input accuracy, while instructions with tool selection errors could not be evaluated for parameter input accuracy. The calculation formulas are expressed in the equations below:\n$$P(Structural Completeness Rate) = \\frac{N(Test Set) - N(SE)}{N(Test Set)}$$\n(4)\n$$P(Tool Selection Acc.) = \\frac{N(Test Set) \u2013 N(SE) \u2013 N(TE)}{N (Test Set) - N(SE)}$$\n(5)\n$$P(Parameter Filling Acc.) = \\frac{N(Test Set) - N(SE) \u2013 N(TE) \u2013 N(PE)}{N(Test Set) \u2013 N(SE) \u2013 N(TE)}$$\n(6)\nIn addition, we conducted confusion matrix evaluations for the fine-tuned model trained on the DHR_train_3 dataset. We calculated Precision, Recall, and F1 scores for each workflow invoked by the models, providing detailed insights into the model's performance in workflow selection."}, {"title": "4.3 Experimental Results", "content": "Tab.3 presents the training results of our experiments. The models DHR_train_1_ft, DHR_train_2_ft, and DHR_train_3_ft were trained using their respective datasets. As shown, DHR_train_1_ft and DHR_train_3_ft significantly outperformed the baseline models Qwen2.5-Coder-7B-Instruct and GPT-40 across both test sets. To further analyze the experimental results, we categorized the error types for different models on the DHR_test_A test set, with the findings summarized in Tab.3. The analysis revealed substantial improvements in all aspects after fine-tuning, including the completeness of function-calling structures, the accuracy of tool selection, and the accuracy of parameter filling. After fine-tuning, the models achieved complete accuracy in output structure completeness and parameter filling accuracy. In terms of tool invocation accuracy, DHR_train_1_ft surpassed both the base model and GPT-40, DHR_train_2_ft reached the performance level of GPT-4, and DHR_train_3_ft exceeded GPT-4. These findings underscore the effectiveness of fine-tuning for specific scenarios, demonstrating that even small-parameter models can be effectively optimized for specialized tasks, thereby narrowing the performance gap with larger, resource-intensive LLMs."}, {"title": "4.4 Confusion Matrix Evaluation Results", "content": "To verify the rationality of these workflow designs and test sets, we also analyzed the performance of the Qwen2.5-Coder-7B-Instruct model after fine-tuning on the DHR_test_A test set. The results were represented using confusion matrices in Fig.3 and relevant performance metrics in Fig.4."}, {"title": "5 Ablation Study", "content": "To further investigate the factors affecting the model's training performance, we conducted a series of in-depth ablation experiments. These experiments involved adjustments to the base model, the composition of training data, data length, and the selection of training hyper-parameters. The tests were conducted on the DHR_test_A test set, with the results and analyses detailed below:"}, {"title": "5.1 Comparison of Base Models after Training", "content": "First, we explored the impact of different base models on training performance. The experiments focused primarily on the Qwen series models, which were trained using the DHR_train_3 dataset.\nTab.4 shows that Qwen2.5-Coder-7B-Instruct performed the best and is the most suitable base model for this scenario. The Coder model excels in generating function-calling data formats, as its training set includes a higher proportion of code and structured data, which enhances its performance in the function-calling tasks. Qwen1.5-7b-Chat also achieved very good results, likely due to the DPO (Direct Preference Optimization) training it underwent in later stages, making it more adaptable for fine-tuning in downstream tasks [66]. In contrast, Qwen2.5-7B, as a pre-trained model without fine-tuning on tasks, did not perform as well as the instruction-tuned models."}, {"title": "5.2 Comparison Between AI-Generated and Manually Annotated Seed Data", "content": "We used Qwen2.5-Coder-7B-Instruct as the base model for subsequent ablation experiments. To compare the impact of different training datasets on model performance, we conducted fine-tuning tasks using the DHR_train_1, DHR_train_2, and DHR_train_3 datasets. Each dataset was randomly sampled to include only 1,000 data instances for fine-tuning. The results, shown in Table 5, indicate that the DHR_train_3_1000_ft model achieved particularly significant improvements in both the DHR_test_A and DHR_test_B benchmark tests. This demonstrates that training the model using a combination of AI-generated and manually annotated seed data yields better results. One possible explanation is that the AI-generated data expanded the coverage of workflows that were underrep-resented in the manually annotated seed data, thereby improving the model's performance in those areas, and preventing the model from over-fitting to several high-frequency tools. The proportion of corresponding data has been increased, which has played a role in balancing the data set."}, {"title": "5.3 Comparison of Training Data Cutoff Length", "content": "In fine-tuning tasks involving unstruc-tured natural language, the fine-tuning framework offers a data cutoff method. By setting a maximum input length for training data, this method processes the data by truncating the parts that ex-ceed the defined length, retaining only the portion within the maximum input limit [54]. This approach helps pre-vent excessive consumption of GPU memory. For tasks that use natural lan-guage as training data, truncating text is a common practice, as truncating some content typically does not sig-nificantly affect model performance. However, since function-calling tasks rely heavily on structured data, trun-cating training data could have a more substantial impact. To investigate the effect of cutoff length on structured data, we trained models with various cutoff lengths. The results are shown in Figure 5. The results indicate that as the cutoff length decreases, the training performance deteriorates progressively. Given that the average length of our training data exceeds 3,000 tokens, truncation occurs when the cutoff_len is set to 3,000 or less. The results show that the greater the degree of truncation to the origin data, the more significant the negative impact on the model's final performance. Therefore, when training models for tasks involving structured data, it is crucial to ensure that the truncation length exceeds the average length of the training data to preserve training effectiveness."}, {"title": "5.4 Comparison of Tool Description Length", "content": "The length of tool descriptions plays a critical role in data synthesis, model training, and evaluation. Detailed descriptions are highly beneficial for data synthesis and generic model invocation. However, they also lead to longer context lengths, increasing GPU memory consumption and making it more challenging for the model to pay attention to key information. In our experiment, we explored the impact of workflow description length on training performance. Specifically, we categorized description lengths into three types:\n\u2022 Long Description: The original training data containing lengthy and detailed descriptions.\n\u2022 Short Descriptions: Simplified versions of long descriptions, manually reduced to retain only essential functions and key concepts.\n\u2022 No Descriptions: Tool descriptions were completely omitted."}, {"title": "5.5 Comparison of Multi-LoRA Adapter Merging", "content": "To explore a training-free method, we studied the impact of merging multiple LoRA weights, aiming to enable the model to continuously incorporate new data while mitigating the effects of outdated data. Results are shown in Table 7. Compared to continual learning, LoRA merging provides greater flexibility as a plug-in mechanism, allowing the model to adapt to different scenarios by selecting data from appropriate cycles. For this experiment, the DHR_train_3 dataset was randomly split into two parts (split by seed groups) to simulate new data generated at different time periods. These were used to train two separate LoRA models (LoRA_a and LoRA_b) using identical training methods. After training, we attempted various merging methods to combine the two LoRA weights and tested their effectiveness on the DHR_test_A test set. The merging methods included linear, cat, dare_linear, svd, ties, and ties_svd [62]. The results, shown in the Table 7, indicate that the cat(weight=1)"}, {"title": "6 Conclusion", "content": "In this paper, we introduced a specialized training pipeline for function-calling capabilities in professional scenarios. This pipeline encompasses the generation and augmentation of scenario-specific function-calling data, LoRA fine-tuning of models, and comprehensive evaluation"}]}