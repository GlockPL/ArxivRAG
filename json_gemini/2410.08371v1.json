{"title": "Merging in a Bottle: Differentiable Adaptive Merging (DAM) and the Path from Averaging to Automation", "authors": ["Thomas Gauthier-Caron", "Shamane Siriwardhana", "Elliot Stein", "Malikeh Ehghaghi", "Charles Goddard", "Mark McQuade", "Jacob Solawetz", "Maxime Labonne"], "abstract": "By merging models, AI systems can combine the distinct strengths of separate language models, achieving a balance between multiple capabilities without requiring substantial retraining. However, the integration process can be intricate due to differences in training methods and fine-tuning, typically necessitating specialized knowledge and repeated refinement. This paper explores model merging techniques across a spectrum of complexity, examining where automated methods like evolutionary strategies stand compared to hyperparameter-driven approaches such as DARE, TIES-Merging and simpler methods like Model Soups. In addition, we introduce Differentiable Adaptive Merging (DAM), an efficient, adaptive merging approach as an alternative to evolutionary merging that optimizes model integration through scaling coefficients, minimizing computational demands. Our findings reveal that even simple averaging methods, like Model Soups, perform competitively when model similarity is high, underscoring each technique's unique strengths and limitations. We open-sourced DAM, including the implementation code and experiment pipeline, on GitHub\u00b9.", "sections": [{"title": "1 Introduction", "content": "As the demand for versatile and powerful AI systems grows, the need to merge Large Language Models (LLMs) with specialized capabilities, such as multilingual skills or domain-specific knowledge, has become increasingly pressing. Effective model merging enables systems to leverage the unique strengths of individual models without necessitating extensive retraining. Merging also offers the potential to reduce catastrophic forgetting, a significant advantage in maintaining learned knowledge from each model (Sukhbaatar et al., 2024; Siriwardhana et al., 2024; Labrak et al., 2024). However, model merging remains inherently complex due to differences in training and fine-tuning processes, often requiring deep expertise and iterative tuning to achieve a balanced integration of the models' contributions.\nModel merging techniques can be divided into two primary categories: manual and automated, and further distinguished by whether they are data-free or data-informed. Manual, data-free methods such as Model Soups (Wortsman et al., 2022), Trim, Elect, Sign, & Merge (TIES-Merging) (Yadav et al., 2024) or Spherical Linear intERPolation (SLERP) focus on merging model parameters directly without any reliance on data, making them computationally efficient but requiring manual tuning, which can limit scalability.\nAutomated, data-informed methods like AdaMerging (Yang et al., 2023) and evolutionary model merging (Akiba et al., 2024) utilize representative data to inform and optimize parameter adjustments. This approach supports fine-grained control, such as per-layer or per-feature adjustments, reducing the need for manual tuning and improving performance on complex tasks. However, these automated methods typically demand more computational resources and may be impractical in scale. To gain deeper insight into the strengths and weaknesses of these approaches, we performed an in-depth comparative analysis of model merging techniques, spanning from basic averaging methods to more sophisticated automated approaches.\nBuilding on these insights, we introduce Differentiable Adaptive Merging (DAM), a new approach developed as a more efficient alternative to compute-heavy evolutionary strategies. In this paper, we compare DAM with existing established techniques, including DRops And REscales with TIES sign election (DARE-TIES) (Yu et al., 2024), and evolutionary merging (Akiba et al., 2024), and"}, {"title": "2 Related Work", "content": "The process of model merging can broadly be split into pre-merge model alignment (if necessary), followed by a merging method which either uses the model weights alone, or some representative data samples to inform the merging process. We refer to these steps as Model Alignment, Data-Free Merging and Data-Informed Merging respectively.\n2.1 Model Alignment\nA fundamental issue in any model merging scenario is ensuring alignment. Alignment refers to the process of mapping functionally equivalent features to the same relative positions in the weight matrices of the models being merged. Without alignment, merging operations might combine disparate features, leading to interference and performance degradation.\nMerging models trained from different initializations, or even on entirely different tasks, compounds this difficulty. Significant contributions to the field include methods like ZipIt (Stoica et al., 2023), Git Re-Basin (Ainsworth et al., 2022), and Optimal Transport (OT) Fusion (Singh and Jaggi, 2020). Git Re-Basin generalizes the merging process by permuting model parameters into a common space before averaging them, an approach that works well for models trained on similar tasks but struggles when tasks diverge significantly. OT Fusion employs Optimal Transport to achieve a similar goal, introducing soft matching for increased flexibility over strict permutations. ZipIt relaxes the constraints on merging, allowing for partial merging both across layers-merging up to a specified depth-and within layers by combining correlated features within models, as well as between models. This method excels in multi-task scenarios by accounting for unshared features, outperforming conventional permutation methods, especially when models are trained on disjoint tasks. Though even at their most successful, no current method has been shown to reliably merge a set of models trained from different initializations on different tasks and achieve high multi-task performance. This remains an open problem in the field.\nFortunately, when merging models fine-tuned from the same base or 'parent' model, alignment is naturally preserved through the fine-tuning process. The merging methods discussed in subsequent sections assume this inherent alignment, enabling more effective integration of models.\n2.2 Data-Free Merging\nA significant subset of model merging methods focuses on combining model parameters through linear operations, often accompanied by pre-merging strategies to mitigate parameter interference. These methods consider only the model's parameters, making them highly computationally efficient compared to any LLM operation that involves data and inference. One of the pioneering methods in this space is Model Soups (Wortsman et al., 2022), which merges model weights via simple averaging. This approach first demonstrated the feasibility of weight-space merging for pre-trained LLMs and remains a reliable baseline due to its simplicity. However, despite its efficiency, the simplicity of Model Soup can lead to performance degradation caused by unresolved parameter conflicts.\nA more sophisticated alternative to linear interpolation is SLERP, which interpolates between two models along the curved path connecting them on the surface of a sphere, with the base model serving as the center. SLERP is effective and widely adopted but limited by the fact that it can only merge two models at a time. Methods such as TIES-Merging (Yadav et al., 2024) and DELLA-"}, {"title": "2.3 Data-Informed Merging", "content": "Recognizing the limitations of manual parameter tuning, researchers have explored automated model merging techniques. These techniques leverage representative data during inference to better balance the trade-offs between the input models being merged. By utilizing the additional insights gained during inference, they facilitate the automatic selection of hyperparameters that would otherwise need to be approximated in data-free methods. Given these methods are fully automated, they can optimize more granular hyperparameters than the data-free methods, such as per-layer or even per-feature weighting parameters.\nAdaMerging (Yang et al., 2023) is an advanced technique that automates model merging by focusing on minimizing the entropy associated with both the input models and their respective datasets. The process begins by calculating task vectors, which represent the difference in weights between the input models and the base model. Coefficients are then assigned and optimized for each task vector, determining the impact each input model has on the final merged model. AdaMerging adjusts these coefficients in an unsupervised manner, minimizing entropy on multi-task, unlabeled test data, which serves as a surrogate objective for minimizing loss on the full dataset. This optimization not only balances task vectors but also ensures the merged model's predictions are more deterministic and robust. While AdaMerging has shown success in image classification tasks using transformer models, our work focuses on adapting this concept to merge LLMs for a broader range of tasks, including multilingual capabilities and domain-specific knowledge.\nZipLoRA (Shah et al., 2023) is a technique developed to merge independently trained Low-Rank Adaptations (LoRAs) (Hu et al., 2021) by optimizing coefficients to reduce interference between content and style representations. They demonstrate that orthogonality between the columns of the LoRAs is strongly correlated with successful merging that minimizes interference. Primarily applied in the image generation domain with diffusion models, ZipLoRA preserves subject fidelity and style accuracy without requiring extensive manual adjustments.\nEvolutionary Model Merging (Akiba et al., 2024) leverages evolutionary algorithms to optimize merging parameters across both the parameter space and the data flow (layer) space. Unlike AdaMerging, which assigns coefficients to task vectors, evolutionary techniques define merging coefficients on a per-layer, per-weight basis. The process involves initializing a population of potential solutions, each representing a different set of merging coefficients, and iteratively refining these through selection, mutation, and recombination based on task-specific performance metrics. Although evolutionary merging can produce highly effective merged models, it is computationally ex-"}, {"title": "3 Differentiable Adaptive Merging\n(DAM)", "content": "In this section, we introduce DAM, a novel approach designed to merge multiple LLMs efficiently and effectively. DAM leverages a datainformed methodology to learn cost-efficient scaling coefficients for each model prior to merging, optimizing the integration process. This method can be applied to all linear layers, embedding layers, and layer normalization layers within the models.\n3.1 Mathematical Formulation\nThe core idea of DAM is to find the optimal scaling coefficients for multi-task merging. By scaling the columns of the source models' weight matrices, we can effectively adjust the input features, ensuring that the merged model leverages the strengths of each individual model. For a given layer l in the model, let $W^i$ represent the weight matrix of the i-th model. The goal is to find optimal scaling coefficients $c_{ij}$ for each column j in the weight matrix of each model such that the merged weight matrix $W^l$ is given by:\n$W^l = \\sum_{i=1}^{N} W^i C^i$\nwhere N is the number of models being merged, and $C^i$ is a diagonal matrix with the scaling coefficients $c_{ij}$ on the diagonal. This ensures that each column of the weight matrix is scaled individually.\n3.2 Column-wise Scaling\nTo provide a more intuitive understanding, consider a weight matrix $W^i$ of layer l in model i with dimensions M \u00d7 N. Each column j in this weight matrix has a corresponding scaling factor $c_{ij}$. The merged weight matrix $W^l$ can then be expressed as:\n$W^l = \\sum_{i=1}^{N} W^i diag(c_{i1}, c_{i2}, ..., c_{iN})$\nwhere $diag(c_{1}, c_{2}, ..., c_{N})$ is a diagonal matrix with the scaling factors on the diagonal. These factors are learned to optimally scale the input features, effectively adjusting the contribution of each model's weight matrix to the merged model."}, {"title": "3.3 Objective Function for DAM", "content": "The objective function for DAM is designed to optimize the scaling coefficients for each model in a way that ensures the best possible merge. The core idea is to balance task performance, regularization, and similarity constraints between coefficients to achieve a robust and efficient merging process. The objective function consists of the following components:\n3.3.1 Kullback-Leibler (KL) Divergence Loss\nGiven three models and their corresponding datasets $D_1, D_2, D_3, .., D_n$, the idea is to minimize the KL divergence between the logits of the merged model and the logits of each individual model on their respective datasets. Let KL(P||Q) denote the KL divergence between distributions P and Q. The KL divergence loss for the merged model is given by:\n$L_{KL} = \\sum_{i=1}^{N} KL(logits_{merged}(D_i)||logits_{i}(D_i))$\nwhere $logits_{merged}(D_i)$ are the logits of the merged model on dataset $D_i$, and $logits_{i}(D_i)$ are the logits of the i-th model on its corresponding dataset. This ensures that the merged model's predictions are aligned with those of the individual models on their respective datasets.\n3.3.2 Cosine Similarity Loss\nInspired by ZipLoRA, we add a constraint to reduce the cosine similarity between the scaling coefficients of different models for each layer. This encourages the models to scale the feature space in unique ways, promoting diversity in the merged model. The cosine similarity loss is given by:\n$L_{cosine} = \\lambda_{cosine} \\sum_{l} \\sum_{i<j} cos(c_{ij}, c_{ik})$\nwhere $\\lambda_{cosine}$ is the regularization coefficient for the cosine similarity loss. This term helps to ensure that the scaling coefficients for different models are not too similar, which can improve the robustness of the merged model.\n3.3.3 L1 and L2 Regularization\nTo ensure stable training and add sparsity to the coefficients, we apply L1 and L2 regularization to the scaling coefficients:\n$L_{reg} = \\lambda_1 \\sum_{i=1}^{N} \\sum_{j=1}^{d} ||c_{ij}||_1 + \\lambda_2 \\sum_{i=1}^{N} \\sum_{j=1}^{d} ||c_{ij}||_2^2$\nwhere $\\lambda_1$ and $\\lambda_2$ are regularization coefficients. L1 regularization encourages sparsity in the scaling coefficients, while L2 regularization ensures that the coefficients remain small and stable during training.\n3.3.4 Overall Objective Function\nThe overall objective function for training the scaling coefficients in DAM combines the KL divergence loss, cosine similarity loss, and regularization terms:\n$L = L_{KL} + L_{cosine} + L_{reg}$\nThis comprehensive objective function ensures that the scaling coefficients are optimized for task performance, regularization, and similarity, leading to a more robust and efficient merging process."}, {"title": "4 Experiment Design", "content": "4.1 Benchmarking Different Merging Techniques against DAM\nTo thoroughly benchmark model merging approaches, we designed experiments focusing on two primary model families, Mistral (Jiang et al., 2023a) and Llama 3 (Dubey et al., 2024). In these experiments, we compare the effectiveness of various merging methods across models with diverse"}, {"title": "4.1.1 Case Study 1: Japanese Language\nProcessing and Mathematical Reasoning", "content": "In this case study, we applied Mistral-based models specialized in Japanese language processing and mathematical reasoning to evaluate the effectiveness of various merging techniques in preserving and integrating these distinct capabilities. Our experiments focus on assessing the adaptability and compositional generalization potential of DAM by examining its performance on mathematical reasoning tasks in Japanese. To benchmark DAM's capabilities, we compared it against other merging techniques, including DARE-TIES, Model Soups, and evolutionary merging methods.\nSelected Models: For this case study, we selected the shisa-gamma-7b model (augmxnt, 2023), a Japanese language model trained specifically on Japanese language tasks, demonstrating proficiency in linguistic comprehension and expression in Japanese. Additionally, we utilized the WizardMath-7B-V1.1 model (Luo et al., 2023) and Abel-7B-002 model (Chern et al., 2023), both of which are trained on datasets oriented towards mathematical reasoning, equipping them with capabilities in numerical and logical problem-solving. Each of these models is derived from the Mistral-7B-v0.1 base model (Jiang et al., 2023b), and our merged models retain the embedding and RMSNorm weights from Mistral-7B-v0.1 to maintain consistency.\nRepresentative Datasets: For DAM training, we used the Ichikara Japanese instruction tuning dataset (Sekine et al., 2024), covering the same kind of broad general-purpose Japanese language chat conversations used to train shisa-gamma-7b. For mathematical reasoning, we used the MetaMathQA (Yu et al., 2023) and Orca-Math (Mitra et al., 2024) datasets, covering basic arithmetic, algebraic operations, and logical"}, {"title": "4.1.2 Case Study 2: SQL Coding, German,\nand Korean Language Processing", "content": "In this case study, we expanded our experiments to encompass multilingual and domain-specific models in German, Korean, and SQL, aiming to further evaluate DAM's effectiveness in managing diverse languages and tasks. In this context, we conducted comparative evaluations of DARE-TIES, Model Soups, against DAM, assessing their respective performances across these varied domains.\nSelected Models: For this case study, we used the following Llama-3-based models: Llama-3-SauerkrautLM-8b-Instruct, a German language model fine-tuned for various linguistic tasks in German; Llama-3-Open-Ko-8B, specialized in Korean language processing; and llama-3-sqlcoder-8b, tailored for SQL coding tasks. Each of these models is based on the Meta-Llama-3-8B model, with the German and SQL models being initialized from the Instruct variant.\nRepresentative Datasets: For German, we sourced diverse linguistic samples from corpora, using"}, {"title": "5 Ablation Studies on the DAM Method", "content": "The overall objective function for DAM, as shown in Equation 3.3.4, comprises multiple components, each designed with a specific purpose, as outlined in Section 3.3. Here, we analyzed the impact of each component and examined alternative approaches, based on the results in Table 3.\n5.1 Comparison of Different Output Distribution Loss Functions\nIn place of KL divergence (Equation 3.3.1), we experimented with mean square error (MSE) and entropy loss as potential alternatives for output distribution loss.\n5.1.1 MSE Loss\nMSE loss is an intuitive alternative to KL divergence as a loss term to minimize the difference between the output distribution of the merged model and the selected input models. This loss is calculated as shown below:\n$L_{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (logits_{merged}(D_i) - logits_{i}(D_i))^2$\nwhere N is the total number of datasets,\n$logits_{merged}(D_i)$ are the logits of the merged model on dataset $D_i$, and logits_(i)(D_i) are the logits of the input model i on dataset Di.\n5.1.2 Entropy Loss\nFollowing AdaMerging (Yang et al., 2023), we also considered entropy loss as an alternative for output distribution loss. The objective is to encourage more confident predictions from the merged model. This loss is calculated on the merged model's logits across all datasets:\n$L_{entropy} = \\sum_{i=1}^{N} H (logits_{merged}(D_i))$\nwhere N is the total number of datasets, H(\u00b7) is the entropy function, and $logits_{merged}(D_i)$ are the logits of the merged model on dataset Di.\nMinimizing this entropy loss encourages the merged model to make more decisive predictions across all datasets, potentially improving its overall performance and generalization capabilities. Unlike KL divergence or MSE, entropy loss minimizes the entropy of the merged model's output distribution, without reference to the input models\n5.2 Effectiveness of Weight Regularization\nIn this experiment, we first evaluated the impact of incorporating cosine similarity loss into the objective function on the performance of the merged model. Specifically, we compared model performance trained using KL divergence or entropy loss functions, both with and without the additional cosine similarity regularization term.\nIn addition, we assessed the influence of L1 and L2 regularization term within both the KL-only and KL + Cosine Similarity loss settings to examine their effect on model accuracy and consistency.\nThese experiments enabled us to examine how each regularization technique influences the effectiveness and accuracy of the merged model across"}, {"title": "6 Results and Discussion", "content": "This section presents a detailed analysis of our results, highlighting the comparative performance of DAM, Evolutionary Merging, DARE-TIES, and Model Soups. We discuss insights from the case studies on Japanese and Mathematics domains (Table 1) and the German, SQL, and Korean domains (Table 2), as well as our findings from ablation studies (Table 3) on DAM's objective function components.\n6.1 Performance Analysis of DAM Compared to Different Merging Methods\n6.1.1 Case study 1: Japanese Language Processing and Mathematical Reasoning\nOur experiments in the Japanese and Mathematics domains reveal clear distinctions among merging methods in handling language and reasoning capabilities.\nAs shown in Table 1, DAM outperforms other methods on average, effectively balancing performance across tasks involving both Japanese comprehension (e.g., JAQKET, JComQA), and mathematical reasoning (MGSM). Evolutionary Merging, while competitive with a 0.62 average score, requires considerable computational resources, highlighting DAM's advantage in balancing performance with efficiency.\nDARE-TIES and Model Soups both show moderate effectiveness, with DARE-TIES achieving higher scores in specific tasks such as JComQA (0.82) and MGSM (0.62). However, their average scores (0.55 and 0.61, respectively) remain below that of DAM, underscoring DAM's adaptability across diverse tasks without intensive hyperparameter tuning. Model Soups, which is a simple linear averaging approach, performs relatively well, suggesting that linear merging can sometimes rival more complex methods when the merged mod-"}, {"title": "6.1.2 Case Study 2: SQL Coding, German,\nand Korean Language Processing", "content": "In the case study on German, SQL, and Korean domains, DAM's effectiveness extends across multilingual and structured data processing tasks, as shown in Table 2.\nDAM again leads in performance, particularly in German and SQL tasks, with average scores of 0.4434 and 0.6125, respectively. The DAM model performs well across German linguistic tasks with different complexity levels, showing a slight edge over DARE-TIES and Model Soups. In SQL generation tasks, DAM's performance also surpasses Model Soups, which scores 0.5969. DARE-TIES achieves higher effectiveness in Korean tasks (0.6307), likely attributed to differences in model initialization; unlike the other two models, the Korean model was initialized from the Llama 3 base model rather than the Llama 3 Instruct variant. DARE-TIES's sparsification strategy appears beneficial for minimizing interference in cases where task vectors diverge significantly.\n6.1.3 DAM's Adaptability in Multilingual Settings\nThese observations confirm that DAM's columnwise scaling effectively balances task-specific representations in multilingual settings. DAM's flexibility to optimize activation coefficients within individual weight columns proves advantageous in maintaining the distinct linguistic nuances required for each language task.\n6.1.4 Effectiveness of DAM Compared to Evolutionary Merging\nDAM provides a more practical and efficient alternative to Evolutionary Merging. Evolutionary methods require iterative searches through potential parameter configurations, which, while producing competitive performance, demand substantial computational power. In contrast, DAM's gradientbased optimization achieves similar or superior results with significantly fewer resources. By directly optimizing activation coefficients with a relatively compact dataset, DAM bypasses the extensive com-"}, {"title": "6.1.5 Insights on Model Soups and Simplicity\nin Model Averaging", "content": "One surprising finding is the effectiveness of simple averaging methods like Model Soups. Although it lacks the fine-grained control of DAM or DARETIES, Model Soups achieves competitive results in both the Japanese and multilingual domains (e.g., 0.61 average in Table 1 and 0.5969 for SQL in Table 2). This aligns with previous findings on linear interpolation, suggesting that in cases where models share significant similarities, simple averaging may yield satisfactory performance at minimal computational cost. Model Soup's low resource demand and ease of implementation make it a viable option for scenarios where computational efficiency outweighs the need for nuanced control.\n6.2 Impact of Objective Function\nComponents on DAM's Performance\nThe ablation study (Table 3) reveals the influence of each component in DAM's objective function on model performance. KL divergence serves as the primary driver, with configurations including KL achieving the highest average scores. The cosine similarity constraint, proposed by ZipLoRA to promote feature diversity among scaling coefficients, was shown to be less effective in this case. Entropy minimization, as demonstrated in AdaMerging's application, shows potential as a surrogate objective in language modeling tasks, confirming its applicability beyond image classification. While not outperforming KL outright, these results suggest it offers a promising alternative if the domain of the training data is not known. Importantly, the entropy minimization objective also has a lower computational burden, as it does not require logits from the individual input models.\nOverall, the combined objective function enables DAM to balance task performance, regularization, and coefficient diversity, leading to a well-"}, {"title": "7 Conclusion", "content": "This paper provides a comprehensive analysis of model merging techniques, spanning from simple averaging methods to automated, data-informed approaches such as evolutionary merging. We introduce DAM as an efficient alternative to evolutionary merging, significantly reducing computational overhead while achieving competitive performance. Our findings challenge the traditional assumption that more complex methods are inherently superior, showing that straightforward techniques like linear averaging can perform just as well, especially when merged models share similar characteristics. Future work could expand these merging strategies to cover a broader range of languages, domains, and modalities, enabling the creation of merged models with effective multi-task capabilities. Additionally, exploring the scalability of these techniques in realworld applications and examining their suitability for resource-constrained environments could further extend their practical impact across the AI landscape."}]}