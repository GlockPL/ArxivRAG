{"title": "FRAGFM: EFFICIENT FRAGMENT-BASED MOLECULAR GENERATION VIA DISCRETE FLOW MATCHING", "authors": ["Joongwon Lee", "Seonghwan Kim", "Wou Youn Kim"], "abstract": "We introduce FragFM, a novel fragment-based discrete flow matching framework for molecular graph generation. FragFM generates molecules at the fragment level, leveraging a coarse-to-fine autoencoding mechanism to reconstruct atom-level details. This approach reduces computational complexity while maintaining high chemical validity, enabling more efficient and scalable molecular generation. We benchmark FragFM against state-of-the-art diffusion- and flow-based models on standard molecular generation benchmarks and natural product datasets, demonstrating superior performance in validity, property control, and sampling efficiency. Notably, FragFM achieves over 99% validity with significantly fewer sampling steps, improving scalability while preserving molecular diversity. These results highlight the potential of fragment-based generative modeling for large-scale, property-aware molecular design, paving the way for more efficient exploration of chemical space.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep generative models, such as diffusion and flow matching, have demonstrated remarkable success across domains like images (Nichol et al., 2021; Rombach et al., 2022; Ho et al., 2020), text (Li et al., 2022), and videos (Hu & Xu, 2023; Ho et al., 2022). Recently, their application to molecular graph generation has gained attention, where they aim to generate chemically valid molecules by leveraging the structural properties of molecular graphs (Jo et al., 2022; Vignac et al., 2022; Qin et al., 2024).\nHowever, existing atom-based generative models face scalability challenges, particularly in generating large and complex molecules. The quadratic growth of edges as graph size increases results in computational inefficiencies. At the same time, the inherent sparsity of chemical bonds makes accurate edge prediction more complex, often leading to unrealistic molecular structures or invalid connectivity constraints (Qin et al., 2023; Chen et al., 2023). Additionally, graph neural networks (GNNs) struggle to capture topological features such as rings and loops, leading to deviations from chemically valid structures. While various methods incorporate auxiliary features (e.g., spectral, ring, and valency information) to mitigate these issues, they do not fully resolve the sparsity and scalability bottlenecks (Vignac et al., 2022).\nFragment-based molecular generation has been explored as an alternative approach, inspired by its long-standing role in medicinal chemistry (Hajduk & Greer, 2007; Joseph-McCarthy et al., 2014; Kirsch et al., 2019). Instead of generating molecules atom by atom, fragment-based methods construct molecules using functional groups, ring systems, or chemically meaningful substructures, reducing complexity while preserving structural validity. This approach leverages established domain knowledge and significantly improves scalability by representing molecules as coarse-grained graphs. Within molecular generative frameworks, fragment-based methods enable more efficient exploration of the chemical space while maintaining structural coherence, offering better control over"}, {"title": "2 FRAGFM FRAMEWORK", "content": "We propose FragFM, a novel molecular generative framework that utilizes discrete flow matching (DFM) at the fragment level graph. In this approach, fragments and their connections are represented as nodes and edges respectively. This enables a discrete flow matching procedure on the resulting fragment-level graph. Because a single fragments arrangement can correspond to multiple molecular structures depending on how fragments junctions are permuted, we bridge fragment- and atom-level representations via a KL-regularized autoencoder that reconstructs the atom-level graph from its fragment-level graph with a latent variable. The learned latent variable contains the missing information during the fragmentation procedure, and it is generated through the flow matching model in conjunction with the fragment-level graph."}, {"title": "2.1 FRAGMENT GRAPH NOTATION", "content": "We represent a molecule at the atom level as a graph G = (V, E), where V is the set of atoms, and E represents chemical bonds between them. Each node \\(v_k \\in V\\) corresponds to a distinct atom, while an edge \\(e_{ki} \\in E\\) denotes a bond (including non-bond interactions) between atoms \\(v_k\\) and \\(v_i\\). At the fragment level, we define a coarse-grained representation of the molecule as a graph \\(G = (X, \\mathcal{E})\\). Here, \\(x_i \\in F\\) corresponds to a fragment, while each edge \\(\\mathcal{E}_{ij} \\in \\mathcal{E}\\) corresponds to the connectivity of the fragments. Each fragment is interpreted as an atom-level graph. Specifically, \\(\\{X_i\\}_i = \\{(V_i, E_i)\\}_i\\) are disjoint sub-graphs of G = (V, E), where \\(V_i \\subseteq V\\) and \\(E_i \\subseteq E\\), with \\(V_i \\cap V_j = \\emptyset\\) for different fragment indices i, j. The edges in the fragment-level graph \\(\\mathcal{E}\\) are induced from E, meaning that two fragments \\(F_i, F_j \\in X\\) are connected if at least one bond exists between their corresponding atoms, i.e.,\n\\[\\mathcal{E}_{ij} \\in \\mathcal{E} \\text{ if } e_{kl} \\in E \\text{ such that } v_k \\in V_i, v_l \\in V_j.\\quad\\quad\\quad(1)\\]"}, {"title": "2.2 MOLECULAR GRAPH COMPRESSION BY COARSE-TO-FINE AUTOENCODER", "content": "Recent advances in hierarchical generative models (Razavi et al., 2019; Rombach et al., 2022; Qiang et al., 2023) have demonstrated the effectiveness of learning structured latent representations through autoencoding, enabling efficient perceptual compression and reconstruction of complex data distributions. Motivated by this, we extend discrete generative modeling to molecular graphs by incorporating a coarse-to-fine autoencoding framework, where a fragment-level graph serves as a compressed representation of an atom-level graph. The fragment-level graph provides a higher-level"}, {"title": "2.3 DISCRETE FLOW MATCHING FOR COARSE GRAPH", "content": "We aim to model the joint distribution over the fragment-level graph and its latent representation p(G, z) with the flow-matching formulation. Because the connectivity variable \\(\\mathcal{E} \\in E\\) is binary, and the latent z is a real-valued vector with low dimension, we follow the DFM approach from Campbell et al. (2024) for \\(\\mathcal{E}\\) and adopt the Lipman et al. (2022) for z. Although the fragment \\(x \\in F\\) is also a discrete variable, the potentially large number of fragment types required to span the molecular space makes the transition rate matrix of continuous time Markov chain (CTMC) in the DFM approach prohibitively large. To address this, we introduce a stochastic bag selection strategy; for a fragment type variable \\(x_1\\), we set a fragment type where the stochastic path \\(\\{x_t\\}_{t\\in[0,1]}\\) can take the values. For a given data \\(x_1\\):D, we sample a type bag \\(B\\) that includes the types that are in present following a distribution, \\(B \\sim Q(\\cdot|x]:D)\\), where D is the dimension of the discrete variables. With a clean data \\(X_1\\) and B, we define temporal marginal conditioned to B, based on an linear interpolation with a prior distribution:\n\\[p_{t|1}(x_t | x_1, B) = t\\delta_B(x_t, x_1) + (1 - t)p_0(x_t | B),(2)\\]\nwhere \\(\\delta_B(\\cdot,\\cdot) = \\mathbb{I}_B(\\cdot)(\\cdot,\\cdot)\\) represents the Kronecker delta multiplied by the indicator function restricted on B and \\(p_0(\\cdot|B)\\) is uniform over B.\nDiscrete flow matching is learning a denoising process meeting the marginal distribution using a CTMC formulation. For a given fragments bag B, sampling \\(x_t \\sim B\\) begins with an initial distribution \\(p_0(\\cdot|B)\\) and propagates through a CTMC with transition rate \\(R_t(\\cdot,\\cdot|B)\\) which only allows the transition between the states in the B. The evolution of the process follows the Kolmogorov forward equation:\n\\[p_{t+dt|t}(y|x_t, B) = \\delta_B(x_t, y) + R_t(x_t, y|B)dt.(3)\\]\nIn the sampling phase, \\(x_1\\) is sampled based on eq. (3), which requires B. Thus, the sampling phase begins with sampling B from Q and \\(x_0\\) from \\(p_0(\\cdot|B)\\), where the unconditional bag distribution Q obtained by marginalizing out \\(x_1, E_{x_1 \\sim P_{data}} [Q(\\cdot|X_1)]\\). The neural network model approximates the distribution of \\(x_1\\) given \\(x_t\\) and B, which is utilized in computing the transition rate \\(R_t\\). For the generalization of the neural network to diverse molecules, we incorporated a fragment embedding strategy, allowing the model to make predictions on novel fragment compositions. More details about coarse graph sampling and training algorithms are described in Appendix B."}, {"title": "3 RESULTS", "content": ""}, {"title": "3.1 MOLECULAR GRAPH GENERATION", "content": "We evaluate FragFM using the MOSES (Polykovskiy et al., 2020) and GuacaMol (Brown et al., 2019) benchmark datasets, following the dataset splits and evaluation metrics from Vignac et al. (2022). Across both MOSES  and GuacaMol , FragFM consistently outperforms existing diffusion- and flow-based models. While denoising-based models have traditionally lagged behind auto-regressive models, FragFM is the first to surpass them, achieving state-of-the-art Fr\u00e9chet ChemNet Distance (FCD) on MOSES and KL divergence on GuacaMol. Notably, FragFM achieves over 99% validity while also demonstrating strong property-based performance (MOSES Filters, GuacaMol KL divergence), performing on par with JT-VAE and GraphINVENT. Furthermore, FragFM maintains uniqueness and novelty, highlighting its ability to assemble fragments into diverse molecular structures. We provide non-curated samples of generated molecules in Figures 6 and 7."}, {"title": "3.2 NATURAL PRODUCT MOLECULE GENERATION", "content": "Understanding natural products is crucial as they serve as a rich source of bioactive compounds and provide valuable insights for drug discovery (Atanasov et al., 2021; Newman & Cragg, 2020). Additionally, the COCONUT dataset includes a hierarchical classification scheme (pathway, superclass, class) that captures structural and biosynthetic relationships, enabling a more in-depth evaluation of generative models on complex molecular categories. Details on the dataset and evaluation metrics are provided in Appendix C.2.2.\nWe compared FragFM with DiGress on the COCONUT benchmark, as summarized in Table 2. While both models achieve high validity, validity alone does not guarantee that generated molecules resemble natural products. FragFM outperforms DiGress, by achieving lower KL divergence across pathway, superclass, class, and NP-likeness scores, indicating a closer alignment with the training set natural products. This suggests that fragment-based modeling more effectively captures molecular structural characteristics, leading to the generation of more biologically relevant molecules. Example molecules generated by FragFM and DiGress are shown in Figures 8 to 10."}, {"title": "3.3 SAMPLING EFFICIENCY", "content": "Generative models based on the denoising process required multiple iterative steps, making sampling inefficient. Therefore, reducing the number of denoising steps while maintaining generation quality is crucial for improving efficiency and scalability in molecular generation. Figure 1 and Table 5 present the MOSES benchmark results across different denoising steps for various generative models. As expected, reducing the number of denoising steps generally leads to a decrease in generation quality. However, FragFM exhibits a significantly lower decline in quality and consistently outperforms other models, achieving over 95% validity and an FCD of 0.66 with just 10 steps, exceeding other models. This is likely due to the fragment-based discrete flow matching approach, which reduces the number of edges that need to be predicted and allows for more stable intermedi-"}, {"title": "3.4 CONDITIONAL GENERATION", "content": "Conditional generation is crucial in molecular design, enabling precise control over molecular properties. We integrate classifier-free guidance (CFG) (Ho & Salimans, 2022; Nisonoff et al., 2024) to FragFM (details in Appendix B.6) to guide the generation process towards desired property values. We conduct conditional molecular generation on the MOSES dataset to evaluate its effectiveness, targeting logP, number of rings, QED, and TPSA.\nFrom Figures 2 and 5, we observe that FragFM achieves a lower condition MAE while maintaining a lower FCD compared to DiGress, positioning our method on the Pareto-optimal frontier in the FCD-Condition MAE trade-off. The improvement can be attributed to the structured generative process of FragFM, where molecules are assembled from semantically meaningful fragments, allowing for better preservation of structural patterns and improved property control. We provide further details on the CFG and experiments in Appendix D.5."}, {"title": "4 CONCLUSION", "content": "We introduce FragFM, a fragment-based discrete flow matching framework for molecular graph generation. By leveraging fragment-level representations with a coarse-to-fine autoencoder and a fragment bag selection approach, FragFM enables efficient and accurate molecular generation while preserving structural diversity and validity. Extensive benchmarks on MOSES, GuacaMol, and natural product datasets demonstrate that FragFM consistently outperforms existing diffusion and flow-based models across multiple metrics, including validity and Fr\u00e9chet ChemNet Distance (FCD), while requiring fewer denoising steps."}, {"title": "A RELATED WORKS", "content": ""}, {"title": "A.1 DENOISING GRAPH GENERATIVE MODELS", "content": "Denoising-based generative models have become fundamental for molecular graph generation by iteratively refining noisy graphs into structured molecular representations. Diffusion methods (Ho et al., 2020; Song et al., 2020), which have been successful in a variety of domains, have been extended to graph structure data (Jo et al., 2022; Niu et al., 2020), demonstrating the advantages of applying diffusion in graph generation. This approach was further extended by incorporating discrete stochastic processes (Austin et al., 2021), addressing the inherently discrete nature of molecular graphs (Vignac et al., 2022). The discrete diffusion modeling is reformulated by the continuous time Markov chain (CTMC), which has been introduced (Xu et al., 2024; Siraudin et al., 2024; Kim et al., 2024), allowing more flexible and adaptive generative processes. More recently, flow-based models have been explored for molecular graph generation. Continuous flow matching (Lipman et al., 2022) has been applied to structured data (Eijkelboom et al., 2024), while discrete flow models (Campbell et al., 2024; Gat et al., 2024) have been extended to categorical data generation, with recent methods demonstrating their effectiveness in modeling molecular distributions (Qin et al., 2024; Hou et al., 2024)."}, {"title": "A.2 FRAGMENT BASED MOLECULE GENERATION", "content": "Fragment-based molecular generative models construct new molecules by assembling existing molecular substructures, known as fragments. This strategy enhances chemical validity and facilitates the efficient exploration of novel molecular structures. Several works have employed fragment-based approaches within variational autoencoders (VAEs). Jin et al. (2020); Kong et al. (2022); Maziarz et al. (2021) generate molecules using VAEs by learning to assemble fragments in a chemically meaningful way. Jin et al. (2018) adopts a stepwise generation approach, first constructing a coarse fragment-level graph before refining it into an atom-level molecule through substructure completion. Seo et al. (2023); Jin et al. (2020) construct molecules by assembling fragments sequentially, enabling better control over molecular properties during generation. Noutahi et al. (2024); Lee et al. (2025; 2024) generates and optimizes molecules in modified SMILES representation, showing the strength of fragments based on goal-guided molecular generation.\nFragment-based approaches have also been explored in diffusion-based molecular graph generation. Levy & Rector-Brooks (2023) proposed a method that utilizes a fixed set of frequently occurring fragments to generate drug-like molecules, ensuring chemical validity but limiting exploration beyond predefined structures. Since enumerating all possible fragment types is infeasible, the method operates solely within a fixed fragment vocabulary. In contrast, Chen et al. (2024) introduced an alternative fragmentization strategy depending on the dataset based on byte-pair encoding, offering a more flexible molecular representation. However, this approach cannot still incorporate chemically"}, {"title": "\u0412 \u041c\u0415\u0422\u041dOD DETAILS", "content": ""}, {"title": "B.1 DETAILS OF COARSE-TO-FINE AUTOENCODER", "content": "We adopted a KL-regularized autoencoder for coarse-to-fine graph conversion. The coarse-grained graph representation G can be interpreted as a compressed version of atom-level fine graph G. In the fragmentation procedure, atom-level graph loose the fine-grained connection information. For reconstructing the original atom-level graph, the fragment-level graph and the missing information is required, which is encoded in the latent variable z. Formally, the encoding and decoding process is defined as:\n\\begin{aligned}\n&\\mathcal{G} = Fragmentation(G), \\\\\n&z \\sim q_e = \\mathcal{N}(Encoder(G; \\theta), \\sigma), \\\\\n&\\hat{E} = Decoder(\\mathcal{G}, z; \\theta),\n\\end{aligned} (4)\nwhere the decoder reconstructs only those atom-level edges \\(\\hat{E}\\) corresponding to the fragment connectivity in the coarse representation.\nTo ensure that the reconstructed graph faithfully preserves the original molecular structure, we optimize the autoencoder using a reconstruction loss. Additionally, we introduce a small KL regularization term to the training loss for latent variable to enforce a well-structured and unscaled latent space:\n\\[\\mathcal{L}_{VAE}(\\theta) = \\mathbb{E}_{G \\sim p_{data}} \\big[\\mathcal{L}_{CE} (E, \\hat{E}(\\theta)) + \\beta D_{KL} \\big(q_e(z|G) \\|\\| p(z)\\big)\\big].(5)\\]\nWe set a low regularization coefficient of \\(\\beta = 0.0001\\) to maintain high-fidelity reconstruction.\nWe discretize the decoded edges \\(\\hat{E}\\) during the fine-graph sampling procedure. In this process, we employ the blossom algorithm detailed in Appendix B.4, which yields robust sampling performance."}, {"title": "B.2 FRAGMENT DENOISING FLOW MATCHING", "content": "For sampling, we now want to design a stochastic process that meets the temporal marginal distributions of eq. (2) along with the Kolmogorov forward equation eq. (3). Campbell et al. (2024) proposed such a transition rate conditioned on \\(x_1\\) that aligns with the linear interpolated distributions, and we modified the equation in the B conditioned form, which is as:\n\\[R(x_t, y | x_1, B) = \\frac{\\text{ReLU} \\big[Q_t p_{t|1}(y | x_1, B) - p_{t|1}(x_t | x_1, B)\\big]}{Zp_{t|1}^{\\phi}(x_t | x_1, B)} \\text{ for } x_t \\neq y,(6)\\]\nwhile \\(Z^{\\phi}_{t} = \\big|\\{z_t: p_{t|1}(x_t | x_1, B) > 0\\}\\big|\\). The entries for \\(z_t = y\\) are calculated by normalization. For a D dimensional case, where D is the number of dimensions we model, Campbell et al. (2024)"}, {"title": "B.3 NEURAL NETWORK PARAMETERIZATION", "content": "First, we model the coarse-to-fine autoencoder with simple MPNN. We model \\(p_{1|t} (G_1|G_t; \\phi)\\) using a fragment embedding message passing neural network (fragment encoder) and a graph transformer (GT) to facilitate message passing in the fragment-level graph. Through fragment MPNN, each fragment type in F is represented as a 1-dimenstional latent vector:\n\\[h_i = \\text{FragmentEncoder}(x_i; \\phi), \\text{ for } x_i \\in \\mathcal{F}.(9)\\]\nUsing these embedded fragment representations, we construct a fragment-level graph where nodes correspond to individual fragment embeddings, edges representing fragment-level connectivity, and global features z, a latent variable from the coarse-to-fine autoencoder. We incorporate the graph transformer backbone from Vignac et al. (2022); Qin et al. (2024) to propagate information across the fragment graph. After l layers of graph transformer, we obtain node embeddings, edge embeddings, and the global embedding for the fragment graph, denoted as \\(h_k^{(l)}, e_{ij}^{(l)}, g^{(l)}(t)\\). For fragment edge types \\(e_{ij}\\) and the continuous latent variable z, FragFM utilizes simple linear layers. For fragment type prediction, we perform a softmax operation over the fragment bag, where the logit score is computed as the inner product between the fragment embeddings and the node embeddings:\n\\[P_i = \\text{Softmax} \\Big(\\big\\{h_k^{(l)}, h^{(0)}_{x \\in B}\\big\\}_i\\Big).(10)\\]"}, {"title": "B.4 \u0410\u0422\u041eM-LEVEL GRAPH RECONSTRUCTION FROM FRAGMENT GRAPHS", "content": "We utilize the Blossom algorithm (Edmonds, 1965) to determine the optimal matching in the atom-level connectivity given coarse-to-fine decoder output. The Blossom algorithm is an optimization"}, {"title": "B.5 SAMPLING TECHNIQUES", "content": ""}, {"title": "B.5.1 TARGET GUIDANCE", "content": "Diffusion and flow models are typically designed to predict the clean data Gf,1 = {Vf,1, Ef,1} from noisy input. Building on this, (Qin et al., 2024) proposed a modified sampling method for DFM by adjusting the rate matrix toward the predicted clean data. Specifically, the rate matrix is redefined as\n\\[R_t(x_t, y | x_1) = R^{\\phi}_t(x_t, y | x_1) + R_t^{\\omega}(x_t, y | x_1)(13)\\]\nfor \\(x_t \\neq y\\), where\n\\[R^{\\omega}_t(x_t, y | x_1) = \\omega \\cdot \\frac{\\delta(y, x_1)}{Z p_{t|1}^{\\phi}(x_t | x_1)}(14)\\]\nThis modification introduces a slight O(w) violation of the Kolmogorov equation. However, empirical findings suggest that a small w improves sample quality without significantly distorting the learned distribution. For our results, we use a small value of w = 0.002."}, {"title": "B.5.2 DETAILED BALANCE", "content": "The space of valid rate matrices extends beyond the original formulation of \\(R^{\\phi}(Z_t, Z_{t+dt} | z_1)\\), meaning that alternative formulations can still satisfy the Kolmogorov equation. Building on this, Campbell et al. (2024) explored this space and demonstrated that any rate matrix \\(R^{DB}\\) satisfying the detailed balance condition,\n\\[p_{t|1}(x_t | X_1)R^{DB}(x_t, y | x_1) = p_{t|1}(y | x_1)R^{DB}(y, x_t | x_1),(15)\\]\ncan be used to construct a modified rate matrix:\n\\[R = R^{\\phi} + \\eta R^{DB}, \\eta \\in \\mathbb{R}+,(16)\\]\nwhich remains consistent with the Kolmogorov equation. Increasing \\(\\eta\\) introduces additional stochasticity into CTMC, enabling more transition pathways between states. We integrate this stochasticity into our FragFM framework, enabling variability in fragment-type transitions while maintaining valid generative pathways. We set \\(\\eta = 0.1\\) in our experiments."}, {"title": "B.6 CLASSIFIER-FREE GUIDANCE", "content": "Classifier-Free Guidance (CFG) allows for controllable molecular generation by interpolating between conditioned and unconditioned models, eliminating the need for explicit property classifiers (Ho & Salimans, 2022). This approach, widely used in continuous diffusion models, was recently extended to discrete flow matching (Nisonoff et al., 2024). We adopt this technique to enhance the controllability of FragFM while maintaining sample diversity.\nDuring training, FragFM learns both conditional rate matrix \\(R^{\\phi}(x_t, y | c)\\) and an unconditional rate matrix \\(R^{\\phi}(x_t, y)\\) simultaneously by conditioning on property labels c for 90% of samples and conditioning with the masked label \\(\\phi\\) for the remaining 10%. During sampling, the rate matrix is adjusted using the guidance level \\(\\gamma\\):\n\\[R(x_t, y | c) = R^{\\phi}(x_t, y | c)^{\\gamma} + R^{\\phi}(x_t, y)^{\\big(1-\\gamma\\big)}.(17)\\]\nSetting \\(\\gamma = 0\\) corresponds to purely unconditional generation while increasing \\(\\gamma\\) strengthens class adherence, biasing transitions toward the desired property distribution. However, excessively high \\(\\gamma\\) values can constrain exploration, generating molecules that diverge from the overall unconditional data distribution. Ho & Salimans (2022) demonstrated that this trade-off follows a characteristic pattern: as guidance strength increases, sample fidelity (e.g., lower FID) improves at the cost of class adherence (e.g., lower IS)."}, {"title": "C EXPERIMENTAL DETAILS", "content": ""}, {"title": "C.1 BASELINES", "content": "Our experiments compare FragFM with several state-of-the-art graph and molecule generative models. For non-denoising methods, we include JT-VAE (Jin et al., 2018), GraphINVENT (Mercado et al., 2021), NAGVAE (Kwon et al., 2020), and MCTS (Jensen, 2019). For diffusion-based models, we evaluate DiGress (Vignac et al., 2022), DisCo (Xu et al., 2024), and Cometh (Siraudin et al., 2024). In addition, we compare with DeFog (Qin et al., 2024) for flow-based approaches."}, {"title": "C.2 DATASET AND METRICS", "content": ""}, {"title": "C.2.1 MOSES AND GUACAMOL", "content": "MOSES and GuacaMol provide standardized molecular-generation benchmarking frameworks, offering predefined training, test datasets, and automated evaluation metrics. Validity refers to the percentage of generated molecules that adhere to fundamental valency constraints, ensuring chemically plausible structures. Uniqueness quantifies the proportion of generated molecules with distinct SMILES representations, indicating non-isomorphism. Novelty measures the number of generated molecules that do not appear in the training dataset, assessing the model's ability to create new structures.\nThe filter score evaluates the percentage of molecules that satisfy the same chemical constraints applied during test set construction. Fr\u00e9chet ChemNet Distance (FCD) quantifies the similarity between training and test molecules based on learned neural network embeddings. SNN (Similarity to Nearest Neighbor) captures how closely generated molecules resemble their closest counterparts in the training set based on Tanimoto similarity. Scaffold similarity assesses how well the distribution of Bemis-Murcko scaffolds in the generated molecules aligns with that of actual molecules. Finally, KL divergence compares the distributions of various physicochemical properties."}, {"title": "C.2.2 NATURAL PRODUCT GENERATION BENCHMARK", "content": "While understanding natural products is crucial, their intrinsically complex structures and large molecular sizes pose significant challenges for molecular design. To address this, we designed a natural product generation benchmark to evaluate the ability of generative models to capture and reproduce the biochemical characteristics of natural products.\nWe first preprocessed the COCONUT dataset (Sorokina et al., 2021; Chandrasekhar et al., 2025) , the most extensive open-access collection of natural products by filtering out molecules, including charges, retaining only natural compounds. We also excluded metal-containing molecules and"}, {"title": "D ADDITIONAL RESULTS", "content": ""}, {"title": "D.1 RESULTS ON GUACAMOL DATASET", "content": ""}, {"title": "D.2 ANALYSIS OF COARSE-TO-FINE AUTOENCODER", "content": ""}, {"title": "D.3 SAMPLING STEP", "content": "A key challenge in generative models based on stochastic processes is the need for multiple iterative refinement steps, which can significantly impact computational efficiency. While increasing the number of denoising steps generally improves the sampling of diffusion and flow-based models, it also extends the time required for sampling, making large-scale generation impractical. To evaluate this trade-off, we analyzed how different generative models behave under varying step conditions, focusing on their ability to maintain validity, uniqueness, and structural diversity.\nAs shown in Table 5, reducing the number of denoising steps leads to a general decline in molecular quality across all models. However, the extent of this degradation varies considerably depending on the model architecture. DiGress, for example, suffers a catastrophic performance drop, achieving only 6.3% validity and an FCD of 9.30 at 10 steps, highlighting its heavy reliance on many iterative refinements. Continuous-time models, such as DeFoG and Cometh, exhibit better robustness but still experience a significant decline in performance when operating with a low number of steps.\nIn contrast, FragFM maintains high validity even with significantly fewer denoising steps. In fact, at just 10 steps, FragFM achieves 95.8% validity while preserving other key MOSES benchmark metrics. This performance surpasses other models, typically requiring at least 50 to 100 steps to reach comparable results. This improvement can be attributed to the fragment-based discrete flow matching approach, which reduces the number of nodes and edges in the molecular graph, lowering the computational complexity of edge prediction. Since molecular fragments inherently capture larger structural motifs, they provide a more structured and stable generative process, allowing for efficient sampling with fewer denoising steps while preserving overall molecular validity."}, {"title": "D.4 SAMPLING TIME ANALYSIS", "content": "While graph representations exhibit a quadratic increase in edge dimensions as the number of nodes grows, fragment-level graphs contain significantly fewer nodes and edges, resulting in lower computational complexity than atom-level graphs. A detailed analysis of sampling time is provided in Table 6, where FragFM demonstrates the fastest sampling time across all datasets with 500 sampling steps. Furthermore, as shown in Table 5, FragFM achieves comparable or superior performance to baseline models even with just 50 sampling steps. This suggests that its sampling time can be further optimized, enabling speeds \u00d710 to \u00d730 times faster than other models while maintaining the generative quality."}, {"title": "D.5 CONDITIONAL GENERATION", "content": ""}, {"title": "D.6 VISUALIZATION OF GENERATED MOLECULES", "content": ""}]}