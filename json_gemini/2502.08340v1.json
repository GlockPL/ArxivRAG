{"title": "Hierarchical Learning-based Graph Partition for Large-scale Vehicle Routing Problems", "authors": ["Yuxin Pan", "Ruohong Liu", "Yize Chen", "Zhiguang Cao", "Fangzhen Lin"], "abstract": "Neural solvers based on the divide-and-conquer approach for Vehicle Routing Problems (VRPs) in general, and capacitated VRP (CVRP) in particular, integrates the global partition of an instance with local constructions for each subproblem to enhance generalization. However, during the global partition phase, misclusterings within subgraphs have a tendency to progressively compound throughout the multi-step decoding process of the learning-based partition policy. This suboptimal behavior in the global partition phase, in turn, may lead to a dramatic deterioration in the performance of the overall decomposition-based system, despite using optimal local constructions. To address these challenges, we propose a versatile Hierarchical Learning-based Graph Partition (HLGP) framework, which is tailored to benefit the partition of CVRP instances by synergistically integrating global and local partition policies. Specifically, the global partition policy is tasked with creating the coarse multi-way partition to generate the sequence of simpler two-way partition subtasks. These subtasks mark the initiation of the subsequent K local partition levels. At each local partition level, subtasks exclusive for this level are assigned to the local partition policy which benefits from the insensitive local topological features to incrementally alleviate the compounded errors. This framework is versatile in the sense that it optimizes the involved partition policies towards a unified objective harmoniously compatible with both reinforcement learning (RL) and supervised learning (SL). Additionally, we decompose the synchronized training into individual training of each component to circumvent the instability issue. Furthermore, we point out the importance of viewing the subproblems encountered during the partition process as individual training instances. Extensive experiments conducted on various CVRP benchmarks demonstrate the effectiveness and generalization of the HLGP framework. The source code is available at https://github.com/panyxy/hlgp_cvrp.", "sections": [{"title": "INTRODUCTION", "content": "The Vehicle Routing Problem (VRP) is a widely-studied NP-hard problem which has many real-world applications including transportation [10], logistic [2], and digital e-commerce [25]. Exact methods for solving VRP often use mixed integer linear programming (MILP) techniques and employ MILP solvers to generate optimal solutions with theoretical guarantees [22]. However, these methods so far are not computationally efficient enough to handle large-scale instances, particularly for the applications with time-sensitive and dynamically changing VRP scenarios. In contrast, heuristic methods such as LKH3 [12] and HGS [38] aim to generate high-quality solutions quickly. They commonly improve the quality of the existing solution incrementally by local search techniques. However, in addition to the heavy reliance on the quality of handcrafted local operators, these methods are not robust and often need to start from scratch for the problem instances with slight variations.\nMore recently, there has been much work on neural network-based solvers for VRP. Experimentally they have been shown capable of inferring near-optimal efficiently solutions for instances which fall within the training data distribution. These learning-based solvers typically use one of the following methods: constructive, iterative, and divide-and-conquer. The constructive method, as a pioneering paradigm, incrementally deduces a complete solution starting from an empty state [19\u201321, 26, 33, 39, 41]. However, challenges arise when dealing with out-of-distribution instances due to the limited expressivity of neural network and the intricate search landscape. To mitigate this performance degradation, the iterative method merges a neural network-based policy with heuristic local operators to progressively refine the current solution [3, 13, 27, 30, 31, 42]. Yet, this approach relies on numerous improvement steps with well-crafted local operators for satisfactory solutions. By comparison, the divide-and-conquer approach embraces either a heuristic-based partition policy [4, 7, 8, 18, 24, 47] or a neural partition policy [14, 34, 43, 45] to globally divide the entire graph into subgraphs and employ a local construction policy to solve subproblems. However, a failure in either component policy may lead to a significant performance drop. Moreover, heuristic-based partition rules often result in local optima, and neural partition policies may be vulnerable to distribution shifts. Hence, there is a pressing need for a more generalizable and meticulous partition policy, which is the focus of this paper."}, {"title": "RELATED WORKS", "content": "Learning-based methods for solving combinatorial optimization problems (COPs) typically fall into three main categories: constructive methods, iterative methods, and divide-and-conquer methods. Constructive methods aim to progressively infer complete solutions using the autoregressive mechanism [1, 9, 11, 15, 19\u201321, 32, 33, 36, 37, 39, 46]. Impressively, SL-driven constructive policies, such as BQ [5], LEHD [28] and SIL [29] can mitigate the high GPU memory demands associated with gradient backpropagation by eliminating the need for delayed rewards in the training of RL algorithms. Iterative methods offer the benefit of consistently improving a given solution until convergence [3, 13, 27, 30, 31, 42] by integrating local improvement operators into RL policies. The divide-and-conquer paradigm can exploit local topological features that remain insensitive to distribution or scale shifts, thus alleviating performance degradation. Some methods harness heuristic rules for the partitioning process [4, 8, 18, 24, 47]. In contrast, H-TSP [34], TAM [14], GLOP [43], and UDC [45] opt to use a learning-based policy to globally divide the entire instance into subproblems, which are then addressed by a pretrained local construction policy."}, {"title": "PRELIMINARIES", "content": "CVRP Formulation. A CVRP instance $I$ is defined as a tuple, represented by $I = (G, D, N_{\\text{max}})$. The graph $G$ consists of a depot node $v_0$ and $N_o$ customer nodes $v_i$ ($1 \\leq i \\leq N_o$). $D$ and $N_{\\text{max}}$ denote the vehicle capacity and maximum allowable number of times vehicles returning to the depot, respectively. Each node is associated with its coordinates $(a_i, b_i)$ and each customer node is further associated with a demand $d_i$. $N_{\\text{max}}$ is accordingly defined as $\\lceil \\sum_{i=1}^{N_o} d_i / D \\rceil + 1$. The distance between any pair of nodes can be measured by the Euclidean distance. In the CVRP, the vehicle needs to visit each customer exactly once, fulfills their demands without exceeding $D$, and returns to the depot to reload goods if necessary. A feasible solution $T \\in S$ can be described as a node permutation where the depot node can occur multiple times, while each customer node appears only once. Furthermore, the feasible solution $T$ also can be decomposed into $N_t$ feasible subtours. In each subtour $\\tau_i$ ($1 \\leq i \\leq N_t$), the starting and ending nodes are the depot, and the intermediate nodes are customers. The travel cost $e(\\tau_i)$ associated with $\\tau_i$ is the sum of Euclidean distances along this subtour. Thus, the objective is to minimize $e(T) = \\sum_{i=1}^{N_t} e(\\tau_i)$. $S$"}, {"title": "HL Formulation of Partition Problem", "content": "In this section, we begin by introducing the feasible cost function $f(C)$ for a feasible partition solution $C$ as defined in Definition 1. Following this, various forms of $f (C)$ will be presented in the subsequent sections to align with both RL and SL objectives for training the partition policies.\nDEFINITION 1. Let $\\pi_{\\text{part}}$ denote the optimal partition policy obtained by optimizing the objective in Equation 1. Given a cost function $f(C) : S_c \\rightarrow \\mathbb{R}$, if $\\pi_{\\text{part}}$ can be derived by optimizing the objective $\\min_{\\pi_{\\text{part}}} \\mathbb{E}_{C \\sim \\pi_{\\text{part}}} [f(C)]$, then $f (C)$ is a feasible cost function.\nBy leveraging this well-defined feasible cost function $f (C)$, the goal of the partition problem is to minimize $\\mathbb{E}_C [f (C)]$. Then, we reformulate the partition problem using a multi-level HL framework. In this framework, the global partition policy $\\pi_{\\text{Gpart}}$ and the local partition policy $\\pi_{\\text{Lpart}}$ work together in synergy to execute the partition task"}, {"title": "RL-driven HLGP", "content": "In the HLGP framework, the imperative task at hand involves the joint optimization for the global and local partition policies, as illustrated in Equation 6. To address this intricate optimization challenge through RL algorithms, a rigorous formulation utilizing a multi-level Markov Decision Process (MDP) is required. However, Equation 6 essentially revolves around evaluating $C^{(K)}$ at the $K$-th level. The absence of direct evaluations for $C^{(k)}, k < K$, primarily contributes to the instability concern during the joint training via RL. We thus equivalently convert it to one involving direct evaluations at each level, as outlined in Theorem 5.\nTHEOREM 2. Let $g(c_i)$ denote $\\mathbb{E}_{T_i \\sim \\pi_{\\text{perm}}(\\cdot|c_i)} (e(T_i))$. It is clear that $f(C) = \\sum_{i=1}^{N_c} g(c_i)$ acts as a feasible cost function. Then, the optimization problem defined in Equation 6 can be transformed equivalently as follows:\n$\\min_{\\pi_{\\text{Gpart}}, \\pi_{\\text{Lpart}}} \\mathbb{E}_{C^{(0)}} [f(C^{(0)})] + \\mathbb{E}_{C^{(0)}} \\mathbb{E}_{C^{(1)}} [f(C^{(1)}) - f(C^{(0)})]+$ \n$\\cdots + \\mathbb{E}_{C^{(0)}} \\mathbb{E}_{C^{(1)}} \\cdots \\mathbb{E}_{C^{(K)}} [f(C^{(K)}) - f(C^{(K-1)})].$\\ (7)\nThe evaluation for $C^{(k)}, k \\geq 1$, can further be derived as:\n$f(C^{(k)}) - f(C^{(k-1)}) = \\sum_{j=1}^{\\lceil \\frac{N_c}{2} \\rceil} [h(C^{(k)}, k, m) - h(C^{(k-1)}, k, m)];$\n$h(C^{(k)}, k, m) = g(c_{(m+k-1)\\%N_c+1}^{(k)})+g(c_{(m+k)\\%N_c}^{(k)}),$\nwhere $m = 2(j - 1).$\nPlease see Appendix-C.2 for proofs. Theorem 5 breaks down the objective described in Equation 6 into $K + 1$ components, with each component associated with the direct evaluation of the respective partition solution"}, {"title": "HL Formulation of Partition Problem", "content": "In this section, we begin by introducing the feasible cost function $f(C)$ for a feasible partition solution $C$ as defined in Definition 1. Following this, various forms of $f (C)$ will be presented in the subsequent sections to align with both RL and SL objectives for training the partition policies.\nDEFINITION 1. Let $\\pi_{\\text{part}}$ denote the optimal partition policy obtained by optimizing the objective in Equation 1. Given a cost function $f(C) : S_c \\rightarrow \\mathbb{R}$, if $\\pi_{\\text{part}}$ can be derived by optimizing the objective $\\min_{\\pi_{\\text{part}}} \\mathbb{E}_{C \\sim \\pi_{\\text{part}}} [f(C)]$, then $f (C)$ is a feasible cost function.\nBy leveraging this well-defined feasible cost function $f (C)$, the goal of the partition problem is to minimize $\\mathbb{E}_C [f (C)]$. Then, we reformulate the partition problem using a multi-level HL framework. In this framework, the global partition policy $\\pi_{\\text{Gpart}}$ and the local partition policy $\\pi_{\\text{Lpart}}$ work together in synergy to execute the partition task"}, {"title": "RL-driven HLGP", "content": "In the HLGP framework, the imperative task at hand involves the joint optimization for the global and local partition policies, as illustrated in Equation 6. To address this intricate optimization challenge through RL algorithms, a rigorous formulation utilizing a multi-level Markov Decision Process (MDP) is required. However, Equation 6 essentially revolves around evaluating $C^{(K)}$ at the $K$-th level. The absence of direct evaluations for $C^{(k)}, k < K$, primarily contributes to the instability concern during the joint training via RL. We thus equivalently convert it to one involving direct evaluations at each level, as outlined in Theorem 5.\nTHEOREM 2. Let $g(c_i)$ denote $\\mathbb{E}_{T_i \\sim \\pi_{\\text{perm}}(\\cdot|c_i)} (e(T_i))$. It is clear that $f(C) = \\sum_{i=1}^{N_c} g(c_i)$ acts as a feasible cost function. Then, the optimization problem defined in Equation 6 can be transformed equivalently as follows:\n$\\min_{\\pi_{\\text{Gpart}}, \\pi_{\\text{Lpart}}} \\mathbb{E}_{C^{(0)}} [f(C^{(0)})] + \\mathbb{E}_{C^{(0)}} \\mathbb{E}_{C^{(1)}} [f(C^{(1)}) - f(C^{(0)})]+$ \n$\\cdots + \\mathbb{E}_{C^{(0)}} \\mathbb{E}_{C^{(1)}} \\cdots \\mathbb{E}_{C^{(K)}} [f(C^{(K)}) - f(C^{(K-1)})].$\\ (7)\nThe evaluation for $C^{(k)}, k \\geq 1$, can further be derived as:\n$f(C^{(k)}) - f(C^{(k-1)}) = \\sum_{j=1}^{\\lceil \\frac{N_c}{2} \\rceil} [h(C^{(k)}, k, m) - h(C^{(k-1)}, k, m)];$\n$h(C^{(k)}, k, m) = g(c_{(m+k-1)\\%N_c+1}^{(k)})+g(c_{(m+k)\\%N_c}^{(k)}),$\nwhere $m = 2(j - 1).$\nPlease see Appendix-C.2 for proofs. Theorem 5 breaks down the objective described in Equation 6 into $K + 1$ components, with each component associated with the direct evaluation of the respective partition solution."}, {"title": "Training Details", "content": "During the training phase for both RL-driven and SL-driven HLGP, we strictly adhere to the standard problem settings used in GLOP [43]. In these settings, each CVRP instance consists of 1000 nodes uniformly distributed within a unit square area. The vehicle capacity, denoted as D, is fixed at 200. The maximum permissible number of times the vehicle returns to the depot is accordingly calculated as $\\lceil \\sum d_i/D \\rceil +1$, where $N_o$ represents the number of customer nodes and $d_i$ signifies the demands associated with each customer node. The demand $d_i$ is uniformly sampled from the range {1, ..., 9}.\nThe RL-driven HLGP adopts the same isomorphic GNN architecture [17, 36] as employed in GLOP [43] for both the global and local partition policies to generate partition heatmaps. These heatmaps, specific to each CVRP instance, aid in progressively decoding feasible partition solutions that adhere to the predefined CVRP constraints. Subsequently, the same pretrained local permutation policy, as utilized in GLOP [43], is applied to resolve the subproblems derived from these feasible partition solutions. The resulting subsolutions to these subproblems constitute the overall CVRP solution. Consequently, the cost of the CVRP solution corresponds to the evaluation of the feasible partition solution.\nThe training phase of the RL-driven HLGP comprises 20 epochs, with each epoch consisting of 256 iterations. During each iteration, 5 CVRP instances are randomly generated to train both the global and local partition policies. For detailed training procedures, please refer to the Algorithm Pseudocodes. Consistent with the approach in GLOP [43], 20 solutions are simultaneously generated for both global and local partition policies to calculate the baseline, thereby reducing gradient variance. The Adam Optimizer is employed with an initial learning rate of 3\u00d710\u22124 for the involved partition policies. Additionally, a Cosine Annealing scheduler is utilized to gradually decrease the learning rate as the training epoch progresses. Similarly, we impose a constraint to maintain the maximum L2 norm of gradients below 1. The RL-driven HLGP is trained on a NVIDIA RTX 3090 GPU and an INTEL(R) XEON(R) GOLD 5218R CPU@2.10GHz.\nThe SL-driven HLGP leverages the same variant Transformer architecture utilized in BQ [5] for both the global and local partition policies to incrementally generate the partition solution. Although BQ [5] was initially designed to serve as a neural solver for producing the CVRP solution directly, it can also be repurposed to generate the partition solution. This adaptability arises from the fact that each subtour in the CVRP solution can be extracted by rearranging the node sequence within the subgraph of the partition solution. Consequently, the decoded node sequence from BQ [5] can be interpreted as the partition solution, regardless of the specific order of decoding"}, {"title": "Evaluation Details", "content": "During the evaluation phase, our focus is on assessing the generalization capabilities of various models. Therefore, we utilize diverse datasets with varying scales and distributions. Each evaluation dataset can specify the number of customer nodes as 1000, 2000, 5000, 7000, or 10000. The customer nodes in each dataset are distributed according to a Uniform distribution, a Gaussian distribution, an explosion pattern, or a rotation pattern. Except for the dataset with 1000 customer nodes setting capacity as 200, the capacity for the other datasets is set to 300. Each dataset comprises 128 instances. The process of generating problem instances aligns with the methodologies outlined in [20, 46]. In addition, for comparison, the metrics include the average solution costs (Avg.), the standard deviation of solution costs (Std.), and average inference time (Time). For both RL-driven and SL-driven HLGP, the number of levels, denoted as K is set as 5 across various evaluation settings. Additionally, the beam size in SL-driven HLGP for the evaluation is set as 16, 16, 8, 4, and 4 for the CVRP datasets with node counts of 1000, 2000, 5000, 7000, and 10000, respectively."}, {"title": "Hyperparameter Studies", "content": "We begin by examining the influence of the number of levels in the multi-level HL framework, denoted as K, on the performance of both RL-driven and SL-driven HLGP. In Table 3, we assess RL-driven HLGP using the CVRP1K dataset with Gaussian distributed nodes (CVRP1K+G) and the CVRP2K dataset with uniformly distributed nodes (CVRP2K+U). Here, K ranges from 1 to 10. It is evident that the average costs gradually converge to 34.51 when K = 7 for the CVRP1K+G dataset and to 59.50 when K = 8 for the CVRP2K+U dataset. Concurrently, the time consumption increases as the number of levels rises. Table 4 illustrates the performance variations of SL-driven HLGP on the CVRP1K+G and CVRP2K+U datasets as K varies. Notably, for the CVRP1K+G dataset, the average cost has already converged to 32.55 when K = 5, while for the CVRP2K+U dataset, the convergent average cost is 57.66 at K = 6. Likewise, the time taken for computation escalates with the increase in the number of levels. In practice, to trade off the performance against efficiency, we select K = 5 for both RL-driven and SL-driven HLGP."}, {"title": "Time Overheads Analysis", "content": "In this section, we analyze the time overhead contributions of the global partition process and the local partition process for both RL-driven HLGP and SL-driven HLGP, as illustrated in Table 9. In the RL-driven HLGP, it is evident that the local partition process contributes significantly more to the average time overhead compared to the global partition process. This disparity arises due to the local partition process typically involving multiple levels of partitioning. Conversely, in the SL-driven HLGP, we observe a similar average time overhead between the global and local partition processes. This similarity can be attributed to the nature of the global partition process in the SL-driven HLGP. In this process, with each step, a new encountered subproblem is derived for the current node selection, necessitating each encountered subproblem to be processed by the policy network. As the number of encountered subproblems is linearly proportional to the graph size, the time overhead of the global partition process increases accordingly. In contrast, in the global partition process of the RL-driven HLGP, only an encountered subproblem is formed and fed to the policy network when a new subgraph is constructed. Consequently, the time overhead associated with the global partition process is less significant compared to that of the local partition process."}, {"title": "Implementation details of baselines", "content": "In this section, we provide the implementation details of the baseline methods, including both both non-learning and learning-based approaches, selected for comparison with our proposed approaches. LKH3. We adhere to the settings outlined in Omni-POMO [46] to reproduce the results of LKH3. For solving each CVRP instance with varying scales and distributions, we set the maximum number of trials to 10000. HGS. In order to decrease the runtime, we adjust the number of iterations of HGS for the CVRP datasets with varying scales. Specifically, we set the number of iterations to 20000, 5000, 2000, 1500, and 1000 for the CVRP instances with node counts of 1000, 2000, 5000, 7000, and 10000, respectively. AM. We follow the implementation guidelines of AM as outlined by Ye et al. [43], where the checkpoint trained on CVRP100 is adapted for comparison purposes. In the evaluation stage, to enhance the quality of solutions generated by AM, we employ a sampling decoding strategy with 1280 solutions for each instance. The temperature of the softmax function in the output layer is set to 0.1 to prevent solutions from diverging towards lower-quality outcomes. POMO. To benchmark against POMO, we generalize the checkpoint trained on CVRP100 to generate results across different CVRP datasets. The POMO size is adjusted to align with the number of nodes in each CVRP instance. Furthermore, data augmentation is implemented for each CVRP instance with an augmentation factor of 8. Sym-POMO. We stick to the original implementations of Sym-POMO, where the POMO size is adjusted to match the number of nodes, and each CVRP instance is augmented by a factor of 8. The checkpoint trained on CVRP100 is generalized for comparisons. AMDKD. AMDKD aims to improve the generalization capacity of neural solvers on CVRP datasets with scale and distribution shifts through knowledge distillation. We adhere to the original evaluation settings with the model trained via knowledge distillation on CVRP100 datasets with various distributions. The default POMO size and augmentation factor are employed in our evaluations. Omni-POMO. Omni-POMO aims to transfer neural solvers to problem instances with diverse scales and distributions through meta-learning techniques. Therefore, we strictly follow the evaluation configurations by utilizing the provided model trained via MAML with 250000 epochs to replicate the results of Omni-POMO across various CVRP datasets. The POMO size is configured to match the number of nodes in each CVRP instance. Additionally, data augmentation is applied to every CVRP instance with an augmentation factor of 8. ELG-POMO. ELO-POMO integrates both a global neural solver and a local neural solver to leverage insensitive local topological features, thereby enhancing the overall system's generalization capability."}]}