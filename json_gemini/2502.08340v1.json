[{"title": "Hierarchical Learning-based Graph Partition for Large-scale Vehicle Routing Problems", "authors": ["Yuxin Pan", "Ruohong Liu", "Yize Chen", "Zhiguang Cao", "Fangzhen Lin"], "abstract": "Neural solvers based on the divide-and-conquer approach for Vehicle Routing Problems (VRPs) in general, and capacitated VRP (CVRP) in particular, integrates the global partition of an instance with local constructions for each subproblem to enhance generalization. However, during the global partition phase, misclusterings within subgraphs have a tendency to progressively compound throughout the multi-step decoding process of the learning-based partition policy. This suboptimal behavior in the global partition phase, in turn, may lead to a dramatic deterioration in the performance of the overall decomposition-based system, despite using optimal local constructions. To address these challenges, we propose a versatile Hierarchical Learning-based Graph Partition (HLGP) framework, which is tailored to benefit the partition of CVRP instances by synergistically integrating global and local partition policies. Specifically, the global partition policy is tasked with creating the coarse multi-way partition to generate the sequence of simpler two-way partition subtasks. These subtasks mark the initiation of the subsequent K local partition levels. At each local partition level, subtasks exclusive for this level are assigned to the local partition policy which benefits from the insensitive local topological features to incrementally alleviate the compounded errors. This framework is versatile in the sense that it optimizes the involved partition policies towards a unified objective harmoniously compatible with both reinforcement learning (RL) and supervised learning (SL). Additionally, we decompose the synchronized training into individual training of each component to circumvent the instability issue. Furthermore, we point out the importance of viewing the subproblems encountered during the partition process as individual training instances. Extensive experiments conducted on various CVRP benchmarks demonstrate the effectiveness and generalization of the HLGP framework.", "sections": [{"title": "1 INTRODUCTION", "content": "The Vehicle Routing Problem (VRP) is a widely-studied NP-hard problem which has many real-world applications including transportation [10], logistic [2], and digital e-commerce [25]. Exact methods for solving VRP often use mixed integer linear programming (MILP) techniques and employ MILP solvers to generate optimal solutions with theoretical guarantees [22]. However, these methods so far are not computationally efficient enough to handle large-scale instances, particularly for the applications with time-sensitive and dynamically changing VRP scenarios. In contrast, heuristic methods such as LKH3 [12] and HGS [38] aim to generate high-quality solutions quickly. They commonly improve the quality of the existing solution incrementally by local search techniques. However, in addition to the heavy reliance on the quality of handcrafted local operators, these methods are not robust and often need to start from scratch for the problem instances with slight variations.\nMore recently, there has been much work on neural network-based solvers for VRP. Experimentally they have been shown capable of inferring near-optimal efficiently solutions for instances which fall within the training data distribution. These learning-based solvers typically use one of the following methods: constructive, iterative, and divide-and-conquer. The constructive method, as a pioneering paradigm, incrementally deduces a complete solution starting from an empty state [19\u201321, 26, 33, 39, 41]. However, challenges arise when dealing with out-of-distribution instances due to the limited expressivity of neural network and the intricate search landscape. To mitigate this performance degradation, the iterative method merges a neural network-based policy with heuristic local operators to progressively refine the current solution [3, 13, 27, 30, 31, 42]. Yet, this approach relies on numerous improvement steps with well-crafted local operators for satisfactory solutions. By comparison, the divide-and-conquer approach embraces either a heuristic-based partition policy [4, 7, 8, 18, 24, 47] or a neural partition policy [14, 34, 43, 45] to globally divide the entire graph into subgraphs and employ a local construction policy to solve subproblems. However, a failure in either component policy may lead to a significant performance drop. Moreover, heuristic-based partition rules often result in local optima, and neural partition policies may be vulnerable to distribution shifts. Hence, there is a pressing need for a more generalizable and meticulous partition policy, which is the focus of this paper.\nIn the divide-and-conquer paradigm, the local construction policy agent benefits from the local topological features within sub-problems insensitive against distribution and scale shifts, contributing to the (near-)optimality of solutions of subproblems [6, 9, 16]. However, during the multi-step decoding process of the learning-based partition policy for Capacitated VRP (CVRP) instances [14, 43], the decoding of clustered nodes in each step relies on the partial partition solution from the preceding step. This implies that errors in the clustering from earlier steps have a tendency to propagate and result in a chain of misclusterings in subsequent steps, called as compounded errors. Consequently, even with an optimal local construction policy, deficiencies in the partition task lead to substantial deviations from the ideal policy in the overall system. Therefore, we argue that the partition task holds a critical position in the overall decomposition-based system for solving CVRP. Furthermore, the success of the local construction policy inspires us to introduce a local partition policy which aims to progressively alleviate compounded errors by harnessing local topological features in the partition task. We thus consider to implement a hierarchical learning (HL) framework specifically designed for the partition task in CVRP, which is capable of seamlessly integrating both global and local partition policies. In prevailing HL frameworks, a high-level policy is adopted to derive a series of simpler sub-tasks which are then delegated to the low-level policy, with the aim of facilitating exploration [35]. These frameworks predominantly focus on reinforcement learning and undergo joint training of the associated policies [23]. Yet, these HL frameworks have not been extensively explored in addressing compounded errors within the graph partition task of large-scale CVRP. In contrast, our study extends the HL framework to the partition task of CVRP and demonstrates its efficacy in mitigating compounded errors.\nIn this paper, we present a versatile Hierarchical Learning-based Graph Partition (HLGP) framework specifically tailored for the partition task in CVRP, which synergistically integrates both global and local partitioning policies. To be specific, our method formulates the partition problem of CVRP using a multi-level HL framework. At the global partition level, the global partition policy is responsible for initiating a coarse multi-way partition to create a series of simpler 2-way partition subtasks. These subtasks stand as the starting point for the subsequent K local partition levels. At each local partition level, a tailored sequence of subtasks is derived from the partition solution of the preceding level. These subtasks are then fed into the local partition policy. Such a setup allows the local partition policy to mitigate the potential misclustering arising from the previous level by leveraging the insensitive local topological features inherent in the subtask. By enabling the local partition policy to traverse through the subtasks at each local partition level, the compounded misclusterings can be mitigated progressively across levels as a consequence.\nOur proposed HLGP framework is versatile, featuring a unified objective that effortlessly accommodates both reinforcement learning (RL) and supervised learning (SL) for training the partition policies. It is worth noting that unlike prior approaches that utilized SL to directly train the neural solver, our method explores the application of SL for training the partition policy, usually omitting the need for permutation information. Additionally, the joint training of the involved policies is disentangled to mitigate training instability. Moreover, by conducting in-depth analyses in both the RL and SL settings, we shed light on the importance of viewing the subproblems encountered during the partition process as individual training instances. Empirically, the proposed HLGP framework convincingly demonstrates its superiority through extensive experiments on various CVRP benchmarks over previous SOTA methods. In particular, our method can scale up to CVRP10K instances with around 10% performance improvement over current literature."}, {"title": "2 RELATED WORKS", "content": "Learning-based methods for solving combinatorial optimization problems (COPs) typically fall into three main categories: constructive methods, iterative methods, and divide-and-conquer methods. Constructive methods aim to progressively infer complete solutions using the autoregressive mechanism [1, 9, 11, 15, 19\u201321, 32, 33, 36, 37, 39, 46]. Impressively, SL-driven constructive policies, such as BQ [5], LEHD [28] and SIL [29] can mitigate the high GPU memory demands associated with gradient backpropagation by eliminating the need for delayed rewards in the training of RL algorithms. Iterative methods offer the benefit of consistently improving a given solution until convergence [3, 13, 27, 30, 31, 42] by integrating local improvement operators into RL policies. The divide-and-conquer paradigm can exploit local topological features that remain insensitive to distribution or scale shifts, thus alleviating performance degradation. Some methods harness heuristic rules for the partitioning process [4, 8, 18, 24, 47]. In contrast, H-TSP [34], TAM [14], GLOP [43], and UDC [45] opt to use a learning-based policy to globally divide the entire instance into subproblems, which are then addressed by a pretrained local construction policy. For a more detailed review of related algorithms used to solve VRPs, please refer to the Appendix-D."}, {"title": "3 PRELIMINARIES", "content": "CVRP Formulation. A CVRP instance $I$ is defined as a tuple, represented by $I = (G, D, N_{max})$. The graph $G$ consists of a depot node $v_{0}$ and $N_{0}$ customer nodes $v_{i}$ ($1 \\le i \\le N_{0}$). $D$ and $N_{max}$ denote the vehicle capacity and maximum allowable number of times vehicles returning to the depot, respectively. Each node is associated with its coordinates $(a_{i}, b_{i})$ and each customer node is further associated with a demand $d_{i}$. $N_{max}$ is accordingly defined as $\\lceil \\frac{\\sum_{i=1}^{N} d_{i}}{D} \\rceil + 1$. The distance between any pair of nodes can be measured by the Euclidean distance. In the CVRP, the vehicle needs to visit each customer exactly once, fulfills their demands without exceeding $D$, and returns to the depot to reload goods if necessary. A feasible solution $T \\in S_{T}$ can be described as a node permutation where the depot node can occur multiple times, while each customer node appears only once. Furthermore, the feasible solution $T$ also can be decomposed into $N_{t}$ feasible subtours. In each subtour $\\tau_{i}$ ($1 \\le i \\le N_{t}$), the starting and ending nodes are the depot, and the intermediate nodes are customers. The travel cost $e(\\tau_{i})$ associated with $\\tau_{i}$ is the sum of Euclidean distances along this subtour. Thus, the objective is to minimize $e(T) = \\sum_{i=1}^{N_{t}} e(\\tau_{i})$. $S_{T}$"}, {"title": "4 HIERARCHICAL LEARNING-BASED GRAPH PARTITION", "content": "Our proposed HLGP framework is built upon the GPLC paradigm. Likewise, we assume the optimal local permutation policy $\\pi_{perm}^{*}$ is obtainable by leveraging LKH3 [12] or the neural solver used in GLOP [43]. It is evident from the partition policy expression in Equation 2 that decoding the nodes at each step hinges on the partial partition solution obtained in the preceding steps. Consequently, inaccuracies in clustering from earlier steps tend to propagate, resulting in a chain of misclustering in subsequent steps. These misclusterings in the partition policy exacerbate notably when confronted with substantial distribution or scale shifts. This empirical challenge in CVRP thus motivates us to develop a HL framework for solving the partition problem in CVRP. We anticipate that this HL framework can progressively mitigate compounded errors by incorporating both global and local partition policies."}, {"title": "4.1 HL Formulation of Partition Problem", "content": "In this section, we begin by introducing the feasible cost function $f(C)$ for a feasible partition solution $C$ as defined in Definition 1. Following this, various forms of $f(C)$ will be presented in the subsequent sections to align with both RL and SL objectives for training the partition policies.\nDEFINITION 1. Let $\\pi_{part}^{*}$ denote the optimal partition policy ob-tained by optimizing the objective in Equation 1. Given a cost function $f(C) : S_{C} \\rightarrow \\mathbb{R}$, if $\\pi_{part}^{*}$ can be derived by optimizing the objective $\\min_{\\pi_{part}} E_{C \\sim \\pi_{part}} [f(C)]$, then $f(C)$ is a feasible cost function.\nBy leveraging this well-defined feasible cost function $f(C)$, the goal of the partition problem is to minimize $E_{C} [f(C)]$. Then, we reformulate the partition problem using a multi-level HL framework. In this framework, the global partition policy $G_{part}$ and the local partition policy $L_{part}$ work together in synergy to execute the partition task, as depicted in Figure 1. At the global partition level, $G_{part}$ creates an initial coarse feasible partition $C^{(0)} = \\{ c_{1}^{(0)}, ..., c_{N_{c}}^{(0)} \\}$, where $c_{j}^{(0)}$ denotes the subgraph at this level. In this partition solution $C^{(0)}$, each pair of subgraphs $(c_{i}^{(0)}, c_{i+1}^{(0)})$ (where $1 \\le i \\le N_{c}$) is stipulated as neighboring subgraphs as defined by a specific heuristic rule. For instance, a simple heuristic involves rearranging subgraphs in $C^{(0)}$ based on the polar angles of their centroids in a Polar coordinate system centered at the depot node. This coarse multi-way partition $C^{(0)}$ serves as the entry point of the subsequent $K$ local partition levels. At each local partition level $k \\in \\{ 1, ..., K \\}$, the subproblems are sequentially formed by reuniting pairs of neighboring subgraphs from $C^{(k-1)}$. Each subproblem $I_{j}^{(k-1)}$ ($1 \\le j \\le \\lceil \\frac{N_{c}}{2} \\rceil$) is defined as:\n$I_{j}^{(k-1)} = (G_{j}^{(k-1)}, D, 2);$\n$G_{j}^{(k-1)} = c_{(m+k-1)\\%N_{c}+1}^{(k-1)} \\cup c_{(m+k)\\%N_{c}+1}^{(k-1)},$ (3)\nwhere $m = 2(j-1)$. There are $\\lceil \\frac{N_{c}}{2} \\rceil$ subproblems in each local partition level. For each subproblem, the vehicle is only allowed to return twice to the depot by subproblem definition. Please note that each pair of consecutive subproblems $I_{j}^{(k-1)}$ and $I_{j+1}^{(k-1)}$ do not overlap in terms of the subgraphs they contain. Additionally, this technique for creating subproblems can be described as initially left-shifting the subgraphs in $C^{(k-1)}$ by $k - 1$ places and then merging the neighboring subgraphs without overlaps. At each local partition level $k \\ge 1$, the subproblem sequence is directed to the local partition policy $L_{part}$. This allows the local partition policy $L_{part}$ to address potential misclusterings from the preceding level by utilizing the robust local topological features. As a result, the local partition policy can traverse through subproblems at each local partition level, gradually reducing accumulated misclusterings across levels. Moreover, upon completion of solving the subproblem $I_{j}^{(k-1)}$, the pair of subgraphs $(c_{(m+k-1)\\%N_{c}+1}^{(k-1)}, c_{(m+k)\\%N_{c}+1}^{(k-1)})$ is transitioned to the corresponding subgraph pair $(c_{(m+k-1)\\%N_{c}+1}^{(k)}, c_{(m+k)\\%N_{c}+1}^{(k)})$. Consequently, the resolution of the subproblem sequence results in an update from $C^{(k-1)}$ to $C^{(k)}$.\nWithin the overall HLGP framework, the global partition policy $G_{part}$ is formulated identical to the partition policy in the GPLC method, written as:\n$G_{part} (C^{(0)} |I) = \\prod_{n=1}^{N_{sol}} G_{part} (C^{(0)} [n]|C^{(0)} [0 : n - 1], I),$ (4)\nwhere $C^{(0)} [n]$ and $C^{(0)} [0 : n - 1]$ denote the $n$-th selected node and the partial solution in $C^{0}$, respectively. In contrast, the local partition policy addresses the series of subproblems produced from the previous partition solution $C^{(k-1)}$ to construct the partition solution $C^{(k)}$. Let $C_{j}^{(k-1)}$ denote the partition solution for the sub-problem $I_{j}^{(k-1)}$. Again, the partition solution $C_{j}^{(k-1)}$ can be either represented as a node list where $C_{j}^{(k-1)} [n]$ and $C_{j}^{(k-1)} [0 : n - 1]$ indicate the $n$-th node and partial solution within it respectively, or decomposed into two subgraphs $c_{(m+k-1)\\%N_{c}+1}^{(k)}, c_{(m+k)\\%N_{c}+1}^{(k)}$, both of which also belong to $C^{(k)}$. Thus, it can be expressed as:\n$\\begin{aligned}\nL_{part} (C^{(k)}|C^{(k-1)}, k) & = \\prod_{j=1}^{\\lceil \\frac{N_{c}}{2} \\rceil} L_{part}(C_{j}^{(k-1)}|I_{j}^{(k-1)}) \\\\\n& = \\prod_{j=1}^{\\lceil \\frac{N_{c}}{2} \\rceil} \\prod_{n=1}^{N_{sol}} L_{part} (C_{j}^{(k-1)} [n]|C_{j}^{(k-1)} [0 : n - 1], I_{j}^{(k-1)}).\n\\end{aligned}$ (5)\nPlease note that in the LHS of Equation 5, the parameter $k$ representing the level is included as an input to the local partition policy. This inclusion is necessary as the parameter $k$ governs the construction of different subproblem sequences for each level. As a result, the objective of HLGP framework is to minimize the expected cost by optimizing both $G_{part}$ and $L_{part}$, written as:\n$\\min_{G_{part}, L_{part}} E_{C^{(0)}} E_{C^{(1)}} \\cdots E_{C^{(K)}} [f(C^{(K)})],$ (6)\nwhere $C^{(0)}$ and $C^{(k)}$ ($k \\ge 1$) are sampled from $G_{part} (C^{(0)} |I)$ and $L_{part} (C^{(k)} |C^{(k-1)}, k)$, respectively."}, {"title": "4.2 RL-driven HLGP", "content": "In the HLGP framework, the imperative task at hand involves the joint optimization for the global and local partition policies, as illustrated in Equation 6. To address this intricate optimization challenge through RL algorithms, a rigorous formulation utilizing a multi-level Markov Decision Process (MDP) is required. However, Equation 6 essentially revolves around evaluating $C^{(K)}$ at the $K$-th level. The absence of direct evaluations for $C^{(k)}$, $k < K$, primarily contributes to the instability concern during the joint training via RL. We thus equivalently convert it to one involving direct evaluations at each level, as outlined in Theorem 5.\nTHEOREM 2. Let $g(c_{i})$ denote $E_{\\tau_{i} \\sim \\pi_{perm}(\\cdot|c_{i})} (e(\\tau_{i}))$. It is clear that $f(C) = \\sum_{i=1}^{N_{c}} g(c_{i})$ acts as a feasible cost function. Then, the optimization problem defined in Equation 6 can be transformed equivalently as follows:\n$\\begin{aligned}\n\\min_{G_{part}, L_{part}} E_{C^{(0)}} [f(C^{(0)})] &+ E_{C^{(0)}} E_{C^{(1)}} [f(C^{(1)}) - f(C^{(0)})] + \\cdots \\\\\n& + E_{C^{(0)}} E_{C^{(1)}} \\cdots E_{C^{(K)}} [f(C^{(K)}) - f(C^{(K-1)})].\n\\end{aligned}$ (7)\nThe evaluation for $C^{(k)}$, $k \\ge 1$, can further be derived as:\n$\\begin{aligned}\nf(C^{(k)}) - f(C^{(k-1)}) & = \\sum_{j=1}^{\\lceil \\frac{N_{c}}{2} \\rceil} [h(C^{(k)}, k, m) - h(C^{(k-1)}, k, m)]; \\\\\nh(C^{(k)}, k, m) & = g(c_{(m+k-1)\\%N_{c}+1}) + g(c_{(m+k)\\%N_{c}+1}),\n\\end{aligned}$ (8)\nwhere $m = 2(j - 1)$.\nPlease see Appendix-C.2 for proofs. Theorem 5 breaks down the objective described in Equation 6 into $K + 1$ components, with each component associated with the direct evaluation of the respective partition solution. Notably, except for the evaluation of $C^{(0)}$ which solely considers its own cost $f(C^{(0)})$, the evaluation of $C^{(k)}$, $k \\ge 1$ hinges on the difference between its own cost $f(C^{(k)})$ and the cost $f(C^{(k-1)})$ from the preceding level. At each local partition level $k \\ge 1$, the local partition policy is responsible for resolving each subproblem $I_{j-1}^{(k-1)}$, leading to the modification of each pair of subgraphs $(c_{(m+k-1)\\%N_{c}+1}^{(k-1)}, c_{(m+k)\\%N_{c}+1}^{(k-1)})$ to the respective subgraph pair $(c_{(m+k-1)\\%N_{c}+1}^{(k)}, c_{(m+k)\\%N_{c}+1}^{(k)})$. We are thus allowed to proceed with the derivation as indicated in Equation 8. Given the optimization problem stated above, we present the formulation utilizing a multi-level MDP in Proposition 2.\nPROPOSITION 1. In the multi-level MDP framework, at the global partition level, for t \u2265 1, the state x(0) \u2208 X(0) comprises problem instance I and the partial partition solution C(0) [0 : t \u2212 1] (C(0) [0] = 0). The initial distribution \u00b5(0) aligns with the problem instance distribution pr. The action u(0) \u2208 U(0) involves selecting a node denoted as C(0) [t], from unvisited customer nodes and the depot node. Let it index subgraphs such that at timestep t, the agent is constructing it-th subgraph c. If the subgraph c0 is created, then the reward rt0 is set as \u2212g(cit0 ); otherwise, it remains at 0. The global partition policy, parameterized by \u03b8G, is thus specified as \u03c0\u03b8G (u(0) |x0 ).\nAt each local partition level k \u2265 1, the local partition policy is tasked with solving the sequence of subproblems obtained from C(k\u22121). In this context, we use jt as an index for subproblems, indicating that the jt-th subproblem denoted as I(k\u22121), is currently being addressed but remains incomplete at timestep t. The state x(k) \u2208 X(k) consists of the subproblem sequence and the partial solution of I(k\u22121). The initial state distribution \u00b5(k) corresponds to the distribution of the subproblem sequence. The action u(k) \u2208 U(k) involves selecting a node for solving Ik\u22121 . When I(k\u22121) is successfully solved, the index jt will proceed to the next subproblem, and the reward rt(k) is set as \u2212(h(C(k), k, m) \u2013 h(C(k-1), k, m)) (where m = 2(jt \u2212 1)). Otherwise, the reward remains at 0. Thus, the local partition policy parameterized by \u03b8L, is defined as \u03c0\u03b8L (u(k) |x(k)). The objective is to maximize the sum of expected returns across levels, as defined below:\n$J(\\theta_{G}, \\theta_{L}) = E_{\\omega^{(0)}}[\\sum_{t=1}^{T(0)} r_{t}^{(0)}] + \\cdots + E_{\\omega^{(0)}} \\cdots E_{\\omega^{(K)}}[\\sum_{t=1}^{T(K)} r_{t}^{(K)}],$ (9)\nwhere $T^{(k)}$ and $\\omega^{(k)}$ denote the horizon and the trajectory at level k.\nNotably, although Equation 16 isolates the evaluation exclusively for $\\omega^{(k)}$, the evaluation impacted by the trajectories $\\omega^{(k+1)}, ..., \\omega^{(K)}$ still remains. This implies that the underlying MDP at level k remains nonstationary. We thus take the following optimization problem as an approximation:\n$\\hat{J}(\\theta_{G}, \\theta_{L}) = L(\\theta_{G}, A_{G}, 0) + \\sum_{k=1}^{K} L(\\theta_{L}, A_{L}, k);$ (10)\n$L(\\theta, A, k) = E_{\\hat{\\mu}^{(k)}, \\pi_{\\theta}}[\\sum_{t=1}^{T(k)} r_{t}^{(k)}] + A H(\\pi_{\\theta}),$\nwhere $\\hat{\\mu}^{(k)}$ is a surrogate initial state distribution at level k, $H(\\pi_{\\theta})$ is the entropy term, and $A$ denotes the hyperparameter. The entropy term is typically defined to minimize the KL divergence between the policy and a uniform distribution. In Equation 16, $\\omega^{(k)}, k \\ge 1$ is drawn from the initial distribution $\\mu^{(k)}$ and the local partition policy $\\pi_{\\theta}$. However, $\\mu^{(k)}$ heavily relies on preceding partition solutions derived from both the global and local partition policies. Therefore, in Equation 10, the surrogate initial distribution $\\hat{\\mu}^{(k)}$ is introduced to eliminate this dependency. Please note that $\\hat{\\mu}^{(0)} = \\mu^{(0)}$. As a result, the optimization for $\\pi_{\\theta_{G}}$ and $\\pi_{\\theta_{L}}$ is decoupled.\nIn the context of RL-driven HLGP, we incorporate the surrogate objective defined in Equation 10 into the REINFORCE algorithm [40] to update $\\pi_{\\theta_{G}}$ and $\\pi_{\\theta_{L}}$. In each iteration $n \\ge 0$ of REINFORCE, the existing global partition policy denoted as $\\pi_{\\theta_{G}}^{n}$ is employed to sample $\\omega^{(0)}$ for the update of $\\pi_{\\theta_{G}}^{n}$. At each local partition level $k \\ge 1$, the current local partition policy denoted as $\\pi_{\\theta_{L}}^{n}$ is additionally leveraged to sample the partition solution $C^{k-1}$, crucial for $\\hat{\\mu}^{(k)}$. Following this, $\\omega^{(k)}$ is sampled to update $\\pi_{\\theta_{L}}^{n}$. Please refer to Appendix-B for the pseudocode.\nFurthermore, in the standard theoretical analysis of REINFORCE algorithm conducted in [44], the upper bound of regret includes the term represented by $||d - \\mu||_{\\infty}$, where d and u stand for the stationary state distribution and the initial state distribution. However, the existing method using REINFORCE algorithm for whatever types of problems (permutation or partition) in the context of CVRP ignores the potential risks highlighted in the regret bound. We exemplify the partition problem as a case study to elucidate this issue. The support set of $\\mu$ consists solely of the problem instances I. Let $N_{s}(t)$ denote the number of customer nodes selected before timestep t. In contrast, during the partition process, at each step t > 1, the partition policy is indeed responsible to solve the subproblem denoted as $I_{N_{s}(t)}$ in which the graph comprises depot and unvisited customers. Let $c_{i_{s}}$ denote the subgraph under construction. The capacity in $I_{N_{s}(t)}$ is accordingly subtracted from the total demand of the visited node in $c_{i_{s}}$, reverting back to D once $c_{i_{s}}$ is fully formed. The support set of d thus includes the subproblems $I_{N_{s}(t)}$. This significant discrepancy in support sets inevitably results in an infinite $||d - \\mu||_{\\infty}$ in the regret bound. This observation inspires us to incorporate the subproblems $I_{N_{s}(t)}$ encountered during the partition process into the training of involved partition policies to reduce the mismatch between support sets.\nIn the practical implementation, a problem instance I is initially generated from the instance distribution $p_{I}$, which is used to train $\\pi_{\\theta_{G}}$ via RL. If a new subgraph $c_{i_{s}}$ is formed at timestep t, then the subproblem $I_{N_{s}(t)}$ is treated as an individual problem instance, denoted as $I \\leftarrow I_{N_{s}(t)}$, for the training of $\\pi_{\\theta_{G}}$. This procedure continues until G = 0 in I, and reverts back to $p_{I}$ for a new instance I. For efficiency reasons, we do not include all subproblems. In inference, the partition solution C(0) is formed by sequentially replacing the partial partition solution with the corresponding complete partition solution of the subproblem. An example is shown in Figure 2(a). The training and inference procedure utilizing the encountered subproblem can similarly be applied to $\\pi_{\\theta_{L}}$. Additionally, we utilize the isomorphic Graph Neural Netwok (GNN) as presented in GLOP [43] to serve as the backbones of $\\pi_{\\theta_{G}}$ and $\\pi_{\\theta_{L}}$ correspondingly."}, {"title": "4.3 SL-driven HLGP", "content": "In this section", "1(C[0": "hat{C"}, [0], "C[N_{sol}"], "minimize": "n$L(\\theta_{G"}, {"minimize": "n$\\min_{\\theta_{G"}, {"2(b))": "At the global partition level", "as": "n$J(\\theta_{G}, \\theta_{L}) = E_{(I,\\hat{C})\\sim p_{I},\\pi_{part}} [L(\\theta_{G}, \\theta_{L},\\hat{C})] + r e g(\\theta_{G},\\theta_{L}),$ (13)\nwhere $r e g(\\theta_{G}, \\theta_{L}) = \\lambda_{G} ||\\theta_{G}||^{2} + \\lambda_{L} ||\\theta_{L}||^{2}$, with hyperparameters $\\lambda_{G}$ and $\\lambda_{L}$. Therefore, this loss function is incorporated into the imitation learning algorithm to iteratively optimize $\\pi_{\\theta_{G}}$ and $\\pi_{\\theta_{L}}$. In each iteration $n \\ge 0$, the algorithm deploys $\\pi_{part}^{n}$ (which relies on $\\pi_{\\theta_{G}}^{n}$ and $\\pi_{\\theta_{L}}^{n}$) and gathers the labeled instance $(I, \\hat{C})$. Online gradient updates are then executed based on estimated gradients to update $\\theta_{G}^{n}$ and $\\theta_{L}^{n}$. Please refer to Appendix-B for the pseudocode.\nHere, let us delve deeper into illustrating the training process for the global partition policy $\\pi_{\\theta_{G}}$ as a case study to elucidate the intricacies of the SL algorithm for the partition problem. A similar analysis can be conducted for the local partition policy $\\pi_{\\theta_{L}}$. The global partition policy $\\pi_{\\theta_{G}}$ requires to imitate the whole trajectory induced by the behavioral policy $\\pi_{part}$. Following the formulation in Equation 4, the global partition policy can directly output the node sequence. Subsequently, the log-probability of this node sequence in $\\hat{C}$ is maximized to update $\\theta_{G}$. This log-probability of the node sequence contains the product of a series of conditional probabilities, represented as $\\log \\prod_{t="}]