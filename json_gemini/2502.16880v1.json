{"title": "CORAL: Learning Consistent Representations across Multi-step Training with Lighter Speculative Drafter", "authors": ["Yepeng Weng", "Dianwen Mei", "Huishi Qiu", "Xujie Chen", "Li Liu", "Jiang Tian", "Zhongchao Shi"], "abstract": "Speculative decoding is a powerful technique that accelerates Large Language Model (LLM) inference by leveraging a lightweight speculative draft model. However, existing designs suffers in performance due to misalignment between training and inference. Recent methods have tried to solve this issue by adopting a multi-step training strategy, but the complex inputs of different training steps make it harder for the draft model to converge. To address this, we propose CORAL, a novel framework that improves both accuracy and efficiency in speculative drafting. CORAL introduces Cross-Step Representation Alignment, a method that enhances consistency across multiple training steps, significantly improving speculative drafting performance. Additionally, we identify the LM head as a major bottleneck in the inference speed of the draft model. We introduce a weight-grouping mechanism that selectively activates a subset of LM head parameters during inference, substantially reducing the latency of the draft model. We evaluate CORAL on three LLM families and three benchmark datasets, achieving speedup ratios of 2.50x-4.07x, outperforming state-of-the-art methods such as EAGLE-2 and HASS. Our results demonstrate that CORAL effectively mitigates training-inference misalignment and delivers significant speedup for modern LLMS with large vocabularies.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), such as GPT (OpenAI, 2023) and Llama series (Touvron et al., 2023a,b; Grattafiori et al., 2024), have demonstrated exceptional capabilities in various natural language processing tasks. However, achieving stronger model performance often depends on increasing the number of model parameters (Kaplan et al., 2020; Hoffmann et al., 2022), which leads to higher costs in both training and inference. Thus, achieving strong performance while maintaining quick response is a crucial part in LLM implementations. Under common hardware conditions, transformer decoder-based LLMs are memory-bound (Dao et al., 2022), which means that the generation speed is mainly determined by memory access and bandwidth, rather than arithmetic computations. This allows for the acceleration of generation using speculative decoding (Chen et al., 2023; Leviathan et al., 2023). The general idea of speculative decoding is to utilize one or multiple lightweight draft models to predict the output of target LLM for several upcoming timesteps, and then verify the drafted predictions in parallel using the target model. The memory-bound characteristic guarantees that the parallel verification of multiple tokens does not incur a significant increase in latency compared to generating a single token.\nRecently, autoregressive draft models, such as EAGLE (Li et al., 2024b), have received widespread attention for their excellent speedup performance. For training, EAGLE uses not only the output tokens but also the last hidden states from target LLM as input to the draft model, while during the drafting phase, the draft model uses its own hidden states from the previous timestep,"}, {"title": "2 Preliminaries", "content": "In this section, we provide some background information related to speculative decoding and review some existing methods, including EAGLE and HASS."}, {"title": "2.1 Speculative Decoding", "content": "Speculative decoding (Chen et al., 2023; Leviathan et al., 2023) aims to accelerate the generation speed of autoregressive LLMs. Vanilla speculative decoding employs a lightweight model (draft model) to generate a chain of candidate tokens for the next y timesteps, which are then verified in parallel by the original LLM (target model) and decide whether to accept them or not. Since the latency of LLM generation mainly lies in the memory access, parallel verification of multiple tokens does not significantly impact the latency of the target LLM, although the computational cost is multiplied.\nThe acceleration capability of speculative decoding is typically evaluated using two metrics: average acceptance length \\(\\overline{\\tau}\\) and the actual Speedup Ratio (SR). A drafting-verification cycle consists of one token provided by the target model and multiple candidates generated by the draft model over y time steps. The average acceptance length \\(\\overline{\\tau}\\) is defined as the number of new tokens generated in a single drafting-verification cycle.\nIdeally, we can estimate the speedup ratio using \\(\\overline{\\tau}\\) and the latencies of draft and target model:\n\\[SR \\approx \\overline{\\tau} \\times \\frac{L'_t}{y \\times L_d + L_t'},\\]\nwhere \\(L_t\\) and \\(L_d\\) denote the latency of the target model and draft model, respectively. \\(L'_t\\) denotes the latency for evaluating multiple tokens one time, it could be slightly different from \\(L_t\\) depending on the hardware. Some additional overheads might also contribute to latency, such as comparing the probabilities of tokens from draft and target models to determine acceptance. However, since these overheads typically do not dominate the overall latency, it is a good choice to ignore them when estimating the speedup ratio.\nFrom Equation (1) we can see the speedup ratio is primarily influenced by two factors: the alignment between the draft model and the target model, which mainly influences \\(\\overline{\\tau}\\), and the ratio of their latencies. Specifically, the lower the latency of the draft model and the better alignment between the two models, the higher the speedup ratio will be achieved by speculative decoding."}, {"title": "2.2 EAGLE", "content": "EAGLE (Li et al., 2024b) is a lightweight autoregressive draft model that leverages a single transformer layer identical to that of the target model. The LM head of draft model is reused directly from the target model, with its parameters frozen. EAGLE discovers that utilizing the feature (i.e., the last hidden states) of the target model can effectively enhance the alignment between the draft and target model. For training, the input of the draft model at position s is the current token \\(t_s\\) and the feature of the target model at position s - 1. The token \\(t_s\\) will first be transformed into embedding \\(e_s\\), and then concatenated with the feature. A linear layer is adopted to reduce the dimensions before"}, {"title": "2.3 HASS", "content": "HASS (Zhang et al., 2024) addresses the inconsistency between the training and inference phases of EAGLE by introducing a multi-step training strategy. As demonstrated in Figure 3, EAGLE uses the feature of the target model for training, whereas in inference, the draft model uses its own feature. HASS solves this problem by feeding the output feature of draft model back into itself for multiple times. To expose the draft model to inference-time conditions during training, attention masks from different training steps require careful adjustment. HASS also incorporates other improvements on EAGLE, but they are orthogonal to multi-step alignment. In this paper, we focus mainly on HASS alignment, and all references to HASS in the remainder of this paper denote HASS alignment unless otherwise specified.\nWhile HASS improves the accuracy of draft models in autoregressive generation, we argue that there are still unresolved issues due to the discrepancies between representations from multiple training steps (i.e., \\(f_d, f_{d'}\\) and \\(f_{d''}\\) in Figure 3). It is harder for the draft model to adapt to more complex inputs and the conflicting gradients from multiple steps may hinder convergence speed."}, {"title": "3 Method", "content": "In this section, we first introduce Cross-Step Representation Alignment, a method designed to strengthen the alignment between the draft model and the target model. We then analyze the speedup ratio and identify the LM head of the draft model as a bottleneck. To address this issue, we propose the LM head router, a novel solution that aims to reduce the latency of the draft model."}, {"title": "3.1 Cross-Step Representation Alignment", "content": "Cross-Step Representation Alignment (CSRA) leverages the idea of contrastive learning (Chopra et al., 2005; Schroff et al., 2015). Specifically, in multi-step training, we treat the output features at the same position in a sentence as positive views of the same sample, while all other features are considered negative samples.\nAssuming current training step is t, the output features of current step are \\(F_t \\in \\mathbb{R}^{B \\times S \\times D}\\), where B, S, and D represent the batch size, sequence length, and hidden dimension, respectively. Naturally, we regard them as B \u00d7 S samples, and each sample has t positive views, while all other features are considered negative samples.\nFor each output feature f in current training step, our objective is to minimize its distance to other positive views while maximizing the distance to negative samples. To achieve this, we normalize the features and compute the InfoNCE loss (van den Oord et al., 2018) as the objective function, which encourages the feature to be closer to its positive views and away from negative samples:\n\\[\\mathcal{L}_{CSRA} = -log \\frac{exp(sim(q, f^+)/\\tau)}{\\sum_{f \\in \\mathbb{F}} exp(sim(q, f)/\\tau)}, \\]"}, {"title": "3.2 Estimation of Speedup Ratio", "content": "As discussed in Section 2.1, the generation speed is primarily constrained by memory bandwidth. Therefore, the theoretical latency \\(L_{\\text{theo.}}\\) in generation phase is proportional to the LLM\u2019s parameter count \\(W_{LLM}\\):\n\\[L_{\\text{theo.}} \\propto W_{LLM}. \\]\nHowever, this estimation is not always accurate due to the following factors: 1) Not all operators and computing graphs are fully optimized. 2) The latency of some element-wise operators (e.g., activation, norm) is not reflected in the parameter count. This issue is particularly noticeable for PyTorch, because it is not a framework optimized for inference.\nLuckily, the draft model and target one share the same transformer structure, and the extra latency caused by the aforementioned factors is relatively consistent in both models. This allows us to estimate the wall time and speedup ratio of speculative decoding based on the parameters of draft model and target model:\n\\[\\frac{L_d}{L_t} \\approx \\frac{W_d}{W_t} \\]\n\\[ SR \\approx \\overline{\\tau} \\times \\frac{W_t}{y \\times W_d + W_t} \\]\nwhere \\(W_d, W_t\\) and \\(L_d, L_t\\) denote the parameter counts and latency of draft and target model, respectively. Note that the embedding layer does not participate in general matrix multiplication (GEMM), therefore its parameters should not be included in"}, {"title": "3.3 LM Head Router", "content": "As mentioned in Section 3.2, for draft models with large vocabularies, LM head constitutes the major part of drafting latency. We propose the LM head router, aiming to group the LM head and then activate only a subset of LM head parameters during drafting, as demonstrated in Figure 5.\nAssuming a LLM with a vocabulary size V, we divide the LM head equally into N groups, each with a vocabulary size of v = V/N. We utilize a router to select which group to activate. The output of router can be outlined as follows:\n\\[P_{router} = Softmax(W_2(act(W_1h) + h)),\\]\n\\[W_2 \\in \\mathbb{R}^{N \\times d}, W_1 \\in \\mathbb{R}^{d \\times d},\\]\nwhere h denotes the hidden states of draft model, d is the hidden size.\nLet p(x), q(x) denote the predicted and target distribution, and \\(p_{group}(x_n)\\) denote the probability distribution within a specific group n. After selecting a particular group, the softmax probability is calculated by logits in this group, independent of the logits in other groups.\nThen the final distribution with router should be\n\\[p(x) = \\sum P_{router}(n) \\cdot p_{group}(x_n).\\]\nFor each group, \\(\\sum p_{group}(x_n) = 1\\), and for router we have \\(\\sum p_{router}(n) = 1\\). Therefore, the final p(x) is normalized.\n\\[\\mathcal{L}_{router} = \\sum q_{router}(n) log P_{router}(n). \\]"}, {"title": "4 Experiments", "content": "In this section, we first introduce the experimental setup, then discuss the overall effectiveness of our method, and finally present the ablation studies on CSRA and LM head router."}, {"title": "4.1 Experimental Setup", "content": "Target LLMs. We choose Llama3-Instruct-8B/70B(Grattafiori et al., 2024), Llama2-chat-7B/13B(Touvron et al., 2023b) and Qwen2.5-Instruct-7B/14B(Yang et al., 2024) as our target models.\nTasks. We choose multiple datasets covering three tasks, including MT-Bench(Zheng et al., 2023) for multi-turn dialogue, GSM8K(Cobbe et al., 2021) for mathematical reasoning, and HumanEval(Chen et al., 2021) for code generation. For 7B/14B models, experiments are conducted with batch size of 1 on a single NVIDIA A6000 48G GPU. For Llama3-70B, we use 4\u00d7A6000 GPUs due to memory requirements.\nMetrics. Since CORAL is a lossless speculative decoding strategy, it is not necessary to measure the generation quality. For acceleration, we use two metrics to evaluate the performance:\n\u2022 Speedup Ratio: the actual speedup ratio compared to vanilla decoding.\n\u2022 Acceptance Length \\(\\overline{\\tau}\\): the average number of new tokens generated per drafting-verification cycle.\nComparisons. We use vanilla decoding as the baseline (1.00\u00d7) to measure the speedup ratio. We primarily compare CORAL with the latest lossless speculative decoding methods, including EAGLE, EAGLE-2, and HASS. Since EAGLE is already one of the fastest speculative decoding methods, we choose EAGLE as the speculative decoding baseline and do not compare with other methods with lower speedup ratios.\nImplementation. Our implementation is based on the open source repositories of HASS\u00b9 and EAGLE-2\u00b2, and the settings are primarily identical to those of them. All models are trained with ShareGPT dataset for 20 epochs with batch size of 2 per GPU. For HASS and CORAL, the default step for training is set to 3. Our system prompt for Llama3 is slightly different from that of EAGLE, please refer to Appendix E for detailed discussion. For inference, we employ a tree depth of 6 and select 60 candidate tokens for all models."}, {"title": "4.2 Effectiveness and Ablation Studies", "content": "We present the acceptance lengths \\(\\overline{\\tau}\\) and speedup ratios of three datasets in Table 2. The results show that CSRA achieves the best performance in both \\(\\overline{\\tau}\\) and speedup ratio (SR) in all experiments we have tested, surpassing EAGLE, EAGLE-2, and HASS. The advantages of CSRA are more pronounced for"}, {"title": "4.2.2 Ablation Study on CSRA", "content": "We adjust the number of training steps and make a more detailed comparison with HASS. Since CSRA and HASS employ the same draft model, the inference overheads are identical, we therefore compare the acceptance length only. The results in Table 3 show that CSRA consistently outperforms HASS under different training steps.\nTo provide a more intuitive measure of the alignment between the draft model and the target model, we compare the acceptance rates \\(\\alpha\\) of HASS and CSRA at different timesteps during inference, as shown in Figure 6. The results show that CSRA generally outperforms HASS at different timesteps."}, {"title": "4.2.3 Ablation Study on LM Head Router", "content": "The LM head router has two hyperparameters: the total number of groups N, and the number of top-"}, {"title": "5 Related Work", "content": "There has been a significant amount of work in accelerating LLMs. Some methods focus on reducing the number of parameters, such as low-bit quantization (Dettmers et al., 2022; Frantar et al., 2023; Xiao et al., 2023; Lin et al., 2024), and model distillation (Gu et al., 2024; Ko et al., 2024; Zhong et al., 2024). Recently, some studies have also explored activating only a subset of model parameters during inference to reduce memory access cost (Du et al., 2022; Fedus et al., 2022). Speculative decoding"}, {"title": "6 Conclusion", "content": "This paper proposes CORAL, an efficient speculative decoding method. We introduce Cross-Step Representation Alignment, which effectively mitigates training-inference misalignment and improves the accuracy of speculation. Additionally, we propose the LM head router, a plug-and-play module designed to reduce the latency of the draft model. We compare CORAL with other state-of-the-art methods on various LLMs and datasets, and the results show that CORAL achieves the best speedup performance."}, {"title": "Limitations", "content": "There are mainly two limitations in this work. Firstly, the introduction of CSRA loss may lead to a slight increase in regression loss, which results in a decrease in the acceptance length if the draft model is trained with single step. This issue can be addressed by multi-step training. Secondly, adopting a large vocabulary is a trend in the development of modern LLMs, and our LM head router is specifically designed for LLMs with large vocabularies. It might not be suitable for models with small vocabularies, as the computational overhead of LM head is limited in the overall wall time of speculative decoding. In this case, the time saved by the draft model cannot compensate for the loss in acceptance length."}, {"title": "E Discussion on System Prompt", "content": "EAGLE utilizes the system prompt from the official Llama2-chat example5:\nsys_p1 = You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\nThe same system prompt is also used in Llama3 drafter training. However, it appears that Llama3 does not have a default system prompt. Nevertheless, we find the system prompt in the official Llama3.3 example is simpler and also widely adopted:\nsys_p2 = You are a helpful assistant\nThe system prompt has a certain impact on the acceptance length and speedup ratio. To investigate this, we compared the open-source Llama3-8B-Instret draft model in EAGLE official repository (trained with sys_p1) and draft models trained by ourselves using sys_p1 and sys_p2. Our results in Table 6 show that switching between different system prompts might lead to a decrease in speedup and acceptance length on the MT-Bench and HumanEval datasets, while GSM8K is an exception.\nUpon closer inspection of the GSM8K results, we find that when using sys_p1, most responses start with a sentence similar to \"Let's break this down step by step\", whereas when using sys_p2, the beginning if outputs will be more diverse. This suggests that the speedup ratio using sys_p1 might be artificially inflated in some cases.\nFurthermore, since longer system prompts provide the draft model with more context, we suppose that detailed prompts and increased information could potentially improve the performance of draft model when the system prompt of training and inference is aligned. However, when the system prompts are not consistent, training the model with a more detailed system prompt may lead to greater performance degradation.\nTo obtain a more generalizable draft model, we use sys_p2 in all experiments with Llama3-Instruct 8B/70B. We believe a more general and simple system prompt would reflect the draft model's true capabilities more accurately."}]}