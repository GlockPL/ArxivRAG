{"title": "Mitigating Hallucinations in Multimodal Spatial Relations through Constraint-Aware Prompting", "authors": ["Jiarui Wu", "Zhuo Liu", "Hangfeng He"], "abstract": "Spatial relation hallucinations pose a persistent challenge in large vision-language models (LVLMs), leading to generate incorrect predictions about object positions and spatial configurations within an image. To address this issue, we propose a constraint-aware prompting framework designed to reduce spatial relation hallucinations. Specifically, we introduce two types of constraints: (1) bidirectional constraint, which ensures consistency in pairwise object relations, and (2) transitivity constraint, which enforces relational dependence across multiple objects. By incorporating these constraints, LVLMs can produce more spatially coherent and consistent outputs. We evaluate our method on three widely-used spatial relation datasets, demonstrating performance improvements over existing approaches. Additionally, a systematic analysis of various bidirectional relation analysis choices and transitivity reference selections highlights greater possibilities of our methods in incorporating constraints to mitigate spatial relation hallucinations.", "sections": [{"title": "1 Introduction", "content": "In recent years, large vision-language models (LVLMs) have been widely adopted for tasks such as image captioning and visual question answering (VQA). While these models have demonstrated remarkable capabilities, hallucination remains a persistent challenge in multimodal systems. Even state-of-the-art models occasionally generate hallucinated responses (Chang et al., 2024). In this study, we focus on mitigating the hallucination in multimodal spatial relations, a challenging task that requires the cognition and reasoning ability of LVLMs about objects in the image.\nExisting research has explored various methods to enhance the performance of LVLMs in spatial"}, {"title": "2 Constraint-Aware Prompting", "content": "This section provides a description of our proposed methods. The demonstrated methods are designed for spatial relation binary VQA, where the input is typically an image-question pair. The underlying constraint-aware approach has the potential to be applied to a broader range of spatial relation tasks.\nSkeleton As illustrated in Figure 2, our methods follow the structure: Instructions + Output Format + Question. Instead of relying on a few-shot prompt, we use a zero-shot prompt with step-by-step instructions to reduce costs and specify an output format to facilitate validation and evaluation.\nTo minimize hallucination in intermediate steps and enhance LVLM reasoning, we incorporate various techniques. We leverage zero-shot chain-of-thought (CoT) prompting (Wei et al., 2022) to enable LVLMs to reason effectively based on detected spatial relations. In the output format, we adopt a reasoning structure (Zhou et al., 2024), explicitly instructing LVLMs to analyze horizontal, vertical, and depth relations between objects. This approach ensures that models generate comprehensive spatial relations and engage in thorough reasoning. A detailed analysis of these techniques can be found"}, {"title": "3 Experiments", "content": "3.1 Experimental Settings\nDatasets We utilize three datasets containing spatial relation data to evaluate our proposed methods. ARO (Yuksekgonul et al., 2023) consists of 50K real-world image-caption pairs, with data sourced from Visual Genome (Krishna et al., 2017), MSCOCO (Lin et al., 2014), and Flickr30k (Young et al., 2014). GQA (Hudson and Manning, 2019)\nMethods With the hypothesis that the combination of bidirectional and transitivity constraints can yield improved performance, we introduce the combined constraint. It integrates two constraints and performs the relation analysis in the AC + BC + BA + AB order. In the experiment, we use the above three constraint-aware methods against two baseline methods. The first baseline uses vanilla prompts, directly asking LVLMs to answer questions with either \"yes\" or \"no.\" The other baseline is based on the vanilla prompts but leverages prompting techniques, such as CoT and structured reasoning output (CoT+structure), which are also incorporated into our methods.\nModel Settings We use GPT-402 (Radford, 2018) as the LVLM in all experiments. The temperature and top-p are both set to a small number 1 \u00d7 10-15, and a fixed seed is used to get more deterministic responses.\n3.2 Results\nAs shown in Table 1, all of our proposed methods significantly outperform two baselines in both accu-"}, {"title": "4 Analysis", "content": "4.1 Bidirectional Relation Analysis\nWhen utilizing the bidirectional constraint, the key factor is ensuring that LVLMs analyze the converse relation (BA). In our experiments, we prompted GPT to analyze spatial relations in the BA + AB order. However, alternative approaches exist, such as analyzing in the AB + BA order or focusing solely on BA.\nWe compare the performance of these variants under bidirectional and combined constraints.\nHowever, no single method consistently outperformed the others across datasets, with results varying between them. Additionally, we observed that within the group employing bidirectional constraints, the AB and AB + BA performed similarly,\n4.2 Reference Selection\nThe reference object in transitivity constraints plays a crucial role. Ideally, it should not introduce new hallucinations and must be tactically positioned to challenge the originally hallucinated relation. Thus, selecting a reliable reference object is essential.\nIn this analysis, we evaluate different reference selection strategies using transitivity and combined"}, {"title": "5 Conclusion", "content": "We have proposed two powerful constraint-aware methods, bidirectional and transitivity constraints, based on the inter-constraint relations among structured variables. These methods and their combinations and variants can be easily implemented to enhance the performance of LVLMs in visual spatial relations. We hope our proposed methods and experimental results can inspire further exploration in multimodal spatial relation tasks.\nLimitations\nThe proposed methods are inspired by constraints found in structured variables. This insight was examined and evaluated using binary VQA, and we did not evaluate our proposals with other visual tasks. Besides that, in our current evaluation, we primarily focus on regular spatial relationships, such as horizontal and depth relations. However, real-world data encompasses a broader range of spatial relations. They will be solid aspects for us to extend in future experiments.\nFor future research, we aim to develop a more deterministic automatic reference object selection mechanism to replace the current random selection used in the transitivity constraint."}, {"title": "A Appendix", "content": "A.1 Data Preprocessing\nWe preprocess the datasets to align with our experimental objectives. To focus on more challenging data, GPT-40-mini\u00b3 is used to filter the data, ensuring that all candidate images contain at least five objects. Non-spatial relations and vague spatial relations, such as \"sitting on\" and \"nearby,\" are eliminated, leaving only those with clear spatial definitions in the images. Additionally, we format the annotations so that the questions follow the structure: Is there  in the image? This makes image-question pair have uniform formats and allows us to focus on evaluating the feasibility of the proposed method in spatial relation tasks.\nIn details, the preprocessing of the ARO and GQA datasets is more complex than that of MM-Rel, as the annotations of ARO are primarily image captions rather than VQA questions, and ARO and GQA include non-spatial relations. Our approach mainly relies on keyword filtering and manual review to ensure the sampled data contains only spatial relations. ARO features image captions with clear lexical structures:  is . This structure enables us to easily reconstruct the captions and convert them into VQA format.\nA.2 Analysis of Prompting Techniques\nIn this section, we present experiments that demonstrate the effectiveness of the prompting techniques discussed in Section 2. We do not assert that these prompting techniques alone will significantly improve the accuracy of LVLMs on spatial relation tasks. Instead, we highlight how our constraint-aware methods can effectively integrate these techniques to enhance overall performance.\nCoT In this analysis, we compare GPT-40's performance using bidirectional, transitivity, and combined constraints, both with and without CoT prompting. To control for CoT, we either include or omit the phrase \"think step by step\" in the prompt.\nReasoning Structure Similarly, we evaluate the effectiveness of the reasoning structure by comparing the performance of GPT-40 with and without this structure. To create a scenario without the reasoning structure, we remove the output format from the prompt and provide additional descriptive instructions that convey the information originally included in the structured reasoning output, such as analyzing horizontal, vertical, and depth relations.\nA.3 Tradeoff between Accuracy and Cost\nCompared to the vanilla prompt and the prompt incorporating CoT and reasoning structure, the prompts used in our method are longer. While they improve the accuracy of model performance on tasks, they also increase the API cost.\nA.4 Results in Other LVLMS\nIn addition to GPT-40, we also use other models to evaluate our methods."}]}