[{"title": "Towards Robust Alignment of Language Models:\nDistributionally Robustifying Direct Preference\nOptimization", "authors": ["Junkang Wu", "Yuexiang Xie", "Zhengyi Yang", "Jiancan Wu", "Jiawei Chen", "Jinyang Gao", "Bolin Ding", "Xiang Wang", "Xiangnan He"], "abstract": "This study addresses the challenge of noise in training datasets for Direct Prefer-\nence Optimization (DPO), a method for aligning Large Language Models (LLMs)\nwith human preferences. We categorize noise into pointwise noise, which includes\nlow-quality data points, and pairwise noise, which encompasses erroneous data\npair associations that affect preference rankings. Utilizing Distributionally Robust\nOptimization (DRO), we enhance DPO's resilience to these types of noise. Our\ntheoretical insights reveal that DPO inherently embeds DRO principles, confer-\nring robustness to pointwise noise, with the regularization coefficient \u1e9e playing a\ncritical role in its noise resistance. Extending this framework, we introduce Dis-\ntributionally Robustifying DPO (Dr. DPO), which integrates pairwise robustness\nby optimizing against worst-case pairwise scenarios. The novel hyperparameter\n\u03b2' in Dr. DPO allows for fine-tuned control over data pair reliability, providing a\nstrategic balance between exploration and exploitation in noisy training environ-\nments. Empirical evaluations demonstrate that Dr. DPO substantially improves the\nquality of generated text and response accuracy in preference datasets, showcasing\nenhanced performance in both noisy and noise-free settings.", "sections": [{"title": "1 Introduction", "content": "Aligning Large Language Models (LLMs) [32, 41, 1, 8] with human preferences is critical for their\nimplementation in real-world scenarios. Central to the alignment is the fine-tuning of LLMs using\nhuman feedback [33], ensuring they adhere to human values and mitigate safety risks. Among\nthe alignment methods, Reinforcement Learning from Human Feedback (RLHF) [33] is becoming\na widely adopted technology. It initially learns a reward model on pairwise preference data, and\noptimizes LLMs using the Proximal Policy Optimization (PPO) [37] method. However, its inherent\nreinforcement learning nature poses significant challenges to computational efficiency and training\nstability [35, 46]. Addressing these, Direct Preference Optimization (DPO) [35] eschews the explicit\nreward model learning, using human preferences to train the LLMs directly. It achieves the same\nobjectives [2] as RLHF by learning an optimal proxy for each pointwise instance and simultaneously\nranking preferences in a pairwise manner, offering greater simplicity and training stability [21]."}, {"title": "2 Preliminaries", "content": "Bradley-Terry Model. Given a context x within a finite space of contexts X, we employ the\npolicy \\(\\pi(y|x)\\) to independently generate a pair of actions (y1, y2). These actions are presented to\nhuman raters, who then indicate their preference, with the preferred action labeled as yw and the less\npreferred as y\u0131, satisfying yw \u2265 y\u03b9. Although we cannot directly observe the latent reward model\nr* (x, y) that underlies these preferences, the Bradley-Terry (BT) model [7] offers a well-established\napproach for modeling pairwise comparisons, which is given as:\n\n\n\\(p^*(Y_1 \\succ Y_2|x) = \\frac{\\exp (r^* (x, y_1))}{\\exp(r^*(x, y_1) + \\exp(r^*(x, y_2)))}\\).\n\n\nGiven the dataset \\(O = \\{(x^{(i)}, y_w^{(i)}, y_\\iota^{(i)})\\}_{i=1}^N\\) sampled from p*, we can parametrize a reward model\nr(x, y) and estimate the parameters by optimizing the following logistic regression loss:\n\n\n\\(L_{LR}(r_{\\phi}, O) = -E_{(x,y_w,y_\\iota)\\sim O}[\\log \\sigma(r_{\\phi}(x,y_w) \u2013 r_{\\phi}(x,y_\\iota))].\\)\n\n\nwhere \\(\\sigma(\\cdot)\\) is the sigmoid function. As the size of dataset O grows, the empirical distribution of the\ndataset O converges to the underlying distribution p*, and the reward model \\(r_{\\phi}\\) converges to the true\nreward model r*.\nReinforcement Learning from Human Feedback (RLHF) [33]. The standard RLHF paradigm is\ncomposed of three phases: i) supervised fine-tuning, ii) reward modeling, and iii) RL fine-tuning.\nUsing the reward model \\(r_{\\phi}\\) learned from the reward modeling, we can then fine-tune the policy \\(\\pi_{\\theta}\\) by\noptimizing the following objective:\n\n\n\\(\\max_{\\pi_{\\theta}} E_{x\\sim D,y\\sim \\pi_{\\theta} (y|x)} [r_{\\phi}(x, y)] \u2013 \\beta D_{KL}[\\pi_{\\theta}(y|x)||\\pi_{ref}(y|x)].\\)\n\n\nIn practice, both the language model policy \\(\\pi_{\\theta}\\) and the reference policy \\(\\pi_{ref}\\) are typically initialized to\nthe same supervised fine-tuning (SFT) model \\(\\pi_{SFT}\\). Here, \u03b2 is a parameter that controls the strength\nof the regularization term, and \\(D_{KL}\\) represents the KL divergence penalty used to regularize the policy\n\\(\\pi_{\\theta}\\) to be close to \\(\\pi_{ref}\\).\nDirected Preference Optimization (DPO) [35]. DPO offers an alternative approach to the RL\nparadigm described above. It establishes a functional mapping between the reward model and the\noptimal policy under a KL divergence constraint with the following formulation:\n\n\n\\(r(x,y) = \\beta \\log \\frac{\\pi_{\\theta} (y|x)}{\\pi_{ref}(y|x)} + \\beta \\log Z(x),\\)\n\n\nwhere \\(Z(x) = \\sum_y \\pi_{ref}(y|x) \\exp(r(x, y)/\\beta)\\) is the partition function. By incorporating this reward\ninto the BT model, the DPO objective enables the comparison of response pairs, facilitating the\ndiscrimination between preferred and dispreferred actions, given by:\n\n\n\\(L_{DPO}(\\pi_{\\theta}; \\pi_{ref}) = -E_{(x,y_w,y_\\iota)\\sim O}[\\log \\sigma(\\beta \\log \\frac{\\pi_{\\theta}(y_w | x)}{\\pi_{ref}(y_w | x)} - \\beta \\log \\frac{\\pi_{\\theta}(y_\\iota | x)}{\\pi_{ref}(y_\\iota | x)})].\\)\n\n\nDistributionally Robust Optimization (DRO) [22, 24, 43]. DRO provides a strategic framework to\neffectively mitigate the uncertainty inherent in training data. It achieves this by optimizing for the\nworst-case expected loss across a set of potential distributions Q. These distributions are confined\nwithin a robustness radius \u03b7 anchored around the empirical training distribution Qo, and are bounded\nby a prescribed divergence metric D. The formal formulation of DRO can be succinctly expressed\nas follows:\n\n\n\\(L_{DRO} = \\max_{Q \\in Q} E_Q[L(x; \\theta)],\\qquad s.t. D_{\\varphi}(Q, Q_0) \\leq \\eta,\\)"}, {"title": "3 Analyzing DPO's Pointwise Robustness", "content": "In this section, we examine the resilience of DPO to pointwise noise. We analyze how DPO responds\nto noise, highlighting its strengths and vulnerabilities."}, {"title": "3.1 Pointwise Noise Impairs DPO Performance", "content": "We start by investigating the impact of pointwise noise on DPO by conducting experiments on the\nIMDB sentiment dataset [27]. Following the setup in [18], we fine-tune the GPT-2-large [34] model\nand employ SiEBERT [17], a specialized variant of ROBERTa-large [26], for reward calculation.\nWe introduce pointwise noise by integrating an additional proportion of low-quality pairs \\((z_\\omega, z_\\iota)\\)\ngenerated by the unrefined GPT-2-large model to the training set. To quantitatively measure DPO's\nrobustness against pointwise noise, we evaluate the performance of each algorithm by examining the\ntrade-off between the achieved reward and the KL divergence from the reference policy."}, {"title": "3.2 Pointwise Robustness in Reward Modeling", "content": "In Section 3.1, we investigate the negative impact of pointwise noise on the reward of individual\ninstances. To address this issue and enhance the robustness of LLMs, we propose integrating DRO\nduring the reward modeling stage. We define the Reward Modeling DRO (RM-DRO) objective, which\noptimizes the expected reward under the worst-case noise distribution within a specified ambiguity\nset as follows:\n\n\n\\(\\max_{\\pi_{\\theta}} E_{x\\sim O,y\\sim \\pi_{\\theta}(y|x)} [r_{\\phi}(x, y)] \\qquad s.t. D_{\\varphi}(\\pi_{\\theta}(y|x), \\pi_{ref}(y|x)) \\leq \\eta.\\)\n\n\nThe direct consequence of pointwise noise is the resultant unreliability of the reference model (SFT).\nBy adopting RM-DRO, we aim to maximize a surrogate objective that accounts for various potential\ndistributions within a robustness radius \u03b7 around the reference distribution \\(\\pi_{ref}(y|x)\\), measured by\nthe distance metric D. With this formulation, we provide a fresh perspective on DPO."}, {"title": "A. DPO is Implicitly a Pointwise DRO.", "content": "Thm 3.1 (Optimal Reward Function under KL Divergence). Let the Kullback-Leibler (KL) divergence\nbetween policy \\(\\pi_{\\theta}\\) and reference policy \\(\\pi_{ref}\\) be defined as: \\(D_{KL}(\\pi_{\\theta} \\| \\pi_{ref}) = \\int \\pi_{\\theta}(x) \\log \\frac{\\pi_{\\theta}(x)}{\\pi_{ref}(x)} dx\\)."}, {"title": "B. The Optimal Value of \u1e9e Reflects the Noise within the SFT Model.", "content": "Lemma 3.2. [15, Lemma 5] The optimal \\(\\beta^* (\\eta)\\) in DPO is monotonically decreasing with respect to\n\u03b7 and obeys the following relationship:\n\n\n\\(\\beta^* (\\eta) = \\sqrt{\\mathbb{V}_{ref}[r(x, y)]/2\\eta},\\)\n\n\nwhere \\(\\mathbb{V}_{ref}[r(x, y)]\\) denotes the variance of the reward model r(x, y) under the reference distribution\n\\(\\pi_{ref}\\).\nLemma 3.2 elucidates the inverse correlation between the parameter \u1e9e and the robustness radius \u03b7.\nThe value of n delineates the boundaries of the search space; to wit, an increase in noise necessitates\nan expansion of the required search space, thereby mandating a larger \u03b7. We empirically validate this\nrelationship by conducting experiments on the IMDB dataset, training models with varying \u1e9e values\nunder different levels of pointwise noise."}, {"title": "4 Dr. DPO: Toward Pairwise Robustness", "content": "In this section, we investigate the impact of pairwise noise and introduce Dr. DPO as a mitigation\nstrategy. We conclude with a theoretical examination of its robustness against such noise."}, {"title": "4.1 Pairwise Noise Impairs DPO Convergence and Performance", "content": "We previously explored DPO's pointwise robustness, while recent work [10] has examined its re-\nsilience to pairwise noise. However, methods that rely on explicit noise estimation may overlook"}, {"title": "4.2 Distributionally Robustifying DPO", "content": "Building upon the principles of DRO, we introduce the Distributionally Robustifying DPO (Dr. DPO)\nframework, designed to enhance DPO's resilience to pairwise noise while preserving its inherent\nrobustness to pointwise noise. The Dr. DPO objective is formulated as follows:\n\n\n\\(\\max_{O'} E_{(x,y_w,y_\\iota)\\sim O'} [h(x, y_w, y_\\iota)] \\qquad s.t. D_{\\varphi}(O', O) \\leq \\eta'.\\)\n\n\nHere, h(x, yw, Y\u0131) = log(ro(x,yw) \u2013 ro(x, y)) denotes the log-likelihood objective of dataset\npoint. The -divergence, denoted as \\(D_{\\varphi}(O', O)\\), quantifies the discrepancy between the hypothetical\ndistribution O' and the dataset distribution O. Additionally, \u03b7' signifies the robustness radius, which\nquantifies the degree to which the model can withstand perturbations.\nThm 4.1. Consider the scenario where the KL divergence is employed to measure the discrepancy\nbetween the hypothetical distribution O' and dataset distribution O, we derive the ultimate loss\nfunction for Dr. DPO as follows:\n\n\n\\(L_{Dr. DPO}(\\pi_{\\theta}; \\pi_{ref}) = -\\beta' \\log E_{O} [\\exp(- \\frac{h_{DPO}(x, y_w, y_\\iota)}{\\beta'})].\\)\n\n\nwhere \\(h_{DPO}\\) represents the log-likelihood in the DPO framework, defined as:\n\n\n\\(h_{DPO}(x, y_w, y_\\iota) = \\log \\sigma(\\beta \\log \\frac{\\pi_{\\theta}(y_w | x)}{\\pi_{ref}(y_w | x)} - \\beta \\log \\frac{\\pi_{\\theta}(y_\\iota | x)}{\\pi_{ref}(y_\\iota | x)}),\\)\n\n\nwith \u1e9e and \u1e9e' being regularization coefficient respectively.\nPlease check Appendix D.2 for detailed proofs. In contrast to the objective function described in\nEquation (7), our Dr. DPO method specifically targets the pairwise noise present within the dataset.\nRather than assigning a uniform weight of to each instance \\((x^{(i)}, y_w^{(i)}, y_\\iota^{(i)})\\) [35], Dr. DPO seeks\nto reweight these instances by an optimal distribution O'. This approach is driven by the intention to\ncapture the incorrect pairs in the data, thereby enhancing the robustness of the resulting policy.\nThm 4.2 (Upper Bound for Dr. DPO). Let \\(h_{DPO} \\in [a, b]\\) and \\(L_{Dr. DPO}\\) represents the Dr. DPO loss on\nN samples. Given a hypothetical distribution O' satisfying \\(D_{KL}(O', O) \\leq \\eta'\\) to dataset distribution\nO, we have that with probability at least 1 \u03b4:\n\n\n\\(L_{O'} \\leq L_{Dr. DPO} + B(\\delta, N, \\beta'),\\)\n\n\nwhere:\n\n\n\\(B(\\delta, N, \\beta') = \\frac{2b \\exp ((b \u2212 a)/\\beta')}{N \u2212 1 + \\exp ((b \u2212 a)/\\beta')} \\sqrt{\\frac{N}{2} \\ln \\frac{1}{\\delta}}.\\)\n\n\nPlease check Appendix D.3 for detailed proofs. In scenarios involving pairwise noise, consider O'\nas the \"ideal\" distribution that discerns the correct ranking between pairwise instances accurately.\nTheorem 4.2 suggests the ideal loss relative to O' is upper bounded by the proposed Dr. DPO loss.\nThis bound is achieved when B(\u03b4, N, \u03b2') approaches zero, or in other words, the number of samples"}, {"title": "4.3 Why is Dr. DPO Robust to Pairwise Noise?", "content": "Our approach extends the analysis presented in Rafailov et al. [35]. To understand the resilience of\nDr. DPO to pairwise noise, we examine the gradient of its loss function, denoted \\(\\nabla_{\\theta} L_{Dr. DPO}\\):\n\n\n\\(\\nabla_{\\theta} L_{Dr. DPO}(\\pi_{\\theta}; \\pi_{ref}) = -\\beta E_{(x,y_w,y_\\iota)\\sim O}[ w(x, y_w, y_\\iota) \\sigma(r_{\\theta}(x, y_\\iota) \u2013 r_{\\theta}(x, y_w)) (\\nabla_{\\theta},y_w - \\nabla_{\\theta},y_\\iota)],\\)\n\n\nwhere \\(r_{\\theta}(x, y) = \\beta \\log \\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)}\\) represents the reward function implicitly learned by the policy \\(\\pi_{\\theta}\\),\nrelative to a reference policy \\(\\pi_{ref}\\). In this framework, \\(\\nabla_{\\theta},y_w\\) and \\(\\nabla_{\\theta},y_\\iota\\) are gradients increasing the\nprobability of the \"chosen\" action yw and decreasing it for the \"rejected\" action y\u0131, respectively. The\nfactor \\(\\sigma(r_{\\theta} (x, y_\\iota) \u2013 r_{\\theta}(x, y_w))\\) serves to amplify the gradient contributions from mismatched action\npairs, which is a principal aspect of the Dr. DPO's design aimed at enhancing learning from compara-\nexp(h(x,yw,y1)/\u03b2\u0384)\ntive feedback. Conversely, the function w(x, Yw, y\u0131), defined as \\(w(x, y_w, y_\\iota) = \\frac{\\exp(h(x,yw,y1)/\u03b2\u0384)}{E_O [\\exp(h(x,y_w,y_\\iota)/\\beta') ]}\\)\n(cf. Appendix D.4), acts to mitigate the influence of these incorrect pairings. It achieves this by\npreferentially weighting correct action pairs over incorrect ones, thus refining the policy update\nmechanism. Moreover, the parameter \u1e9e' does not require intensive tuning; setting it to a default value\nof 1 typically yields stable enhancements. Remarkably, Dr. DPO is straightforward to implement,\nrequiring only an additional line of code with negligible computational overhead."}, {"title": "5 Experiments", "content": "In this section, we conduct an empirical assessment of Dr. DPO to evaluate its ability to mitigate noise\nimpacts in preference datasets and to improve performance in noise-free environments. We outline our\nexperimental design, including the datasets used, evaluation metrics, and comparative benchmarks.\nOur results underscore Dr. DPO's effectiveness, supporting its utility in relevant scenarios."}, {"title": "5.1 How Well can Dr. DPO Resist the Pairwise Noise?", "content": "Datasets and Setup. We conduct experiments on two datasets: IMDB [27] and Anthropic HH [3].\nThe IMDB dataset is widely utilized for sentiment analysis tasks. The Anthropic HH dataset consists\nof approximately 170,000 dialogues between humans and automated assistants. The objectives of\nthese experiments were twofold: firstly, to evaluate the robustness of the proposed Dr. DPO against\npairwise noise; and secondly, to investigate whether Dr. DPO exhibits superior performance on\nnoise-free datasets. To achieve the first objective, we introduce random inversions between selected"}, {"title": "5.2 Comparing Dr. DPO with Baselines on MT-Bench", "content": "To evaluate the generation quality of DPO and its variants, we conduct pairwise comparisons using\nthe MT-Bench framework [47]. This framework, grounded in GPT-4, reliably aligns with human\nevaluative preferences, exhibiting an agreement rate exceeding 80% on the quality of outputs from\nLLMs. Adhering to the established MT-Bench guidelines [47] 4, our approach involves generating\nmodel responses at a controlled temperature of 0.7 and restricting the token count to a maximum of"}, {"title": "5.3 Ablation Studies on Dr. DPO", "content": "We conduct ablation studies to investigate the impact of the $-divergence and \u03b2' on the performance\nof Dr. DPO, and provide the convergence analysis.\nEvaluating the Impact of $-divergence on Dr. DPO. Figure 4 (3) explores Dr. DPO's performance\nwith various -divergences, including Jensen-Shannon (JS) and a-divergence. Demonstrated results\nindicate that Dr. DPO consistently outperforms the baseline DPO when \u03b2' is set to 1.0, serving as\na viable default without requiring further adjustments. This is in contrast to the baseline \u03b2 = 0.1\nsetting, where although improvements can be realized by manually tuning \u03b2*, the process becomes\ntime-consuming and impractical for regular use.\nEvaluating the Impact of \u03b2'. Figure 4 (4) illustrates how varying \u03b2' across different noise levels\naffects preference accuracy. The experimental results reveal a trend wherein increased noise levels\ncorrespond to a reduced optimal value for \u03b2', which is consistent with our theoretical analysis\nprovided in Section 3. Consequently, we propose a default setting of \u03b2' = 1.0 for balancing accuracy\nand robustness in the presence of noise.\nConvergence Analysis. Figure 5 shows that Dr. DPO not only converges faster but also surpasses\nDPO in the early training stage, attributed to its superior management of flipped noisy pairs. This\nenhancement meets our goal of boosting DPO's robustness in noisy environments."}, {"title": "6 Conclusion", "content": "In this study, we analyze DPO's robustness from a DRO perspective, highlighting its resilience to\npointwise noise. We establish a link between DPO's regularization and DRO's robustness, showing\nthat a smaller regularization parameter B enhances stability against uncertain data. Our experiments\nconfirm the crucial role of \u1e9e in noise resistance but uncover DPO's weakness against pairwise noise.\nTo address this, we introduce a novel Distributionally Robustifying DPO (Dr. DPO) framework with\nan additional parameter \u1e9e' that balances data pair importance in training to enhance model robustness.\nThe Dr. DPO's fine-tuning of exploration and exploitation could markedly improve the alignment of\nlanguage models, assuring reliable performance in the presence of real-world noise."}, {"title": "A Related Work", "content": "Reinforcement Learning from Human Feedback. RLHF [11, 3, 41, 33] has emerged as a key\nmethod for aligning language models with human values and preferences, mitigating the generation\nof biased or factually incorrect outputs. Compared to supervised learning, RLHF is rather complex,\nless stable, and requires more memory resources. These challenges have motivated the development\nof alternatives to the RLHF pipeline. For example, RAFT [13] uses an existing reward model to\nselect the best set of training samples based on the model outputs, while RRHF [44] leverages a much\nsimpler ranking loss to align human preferences and retain the performance of PPO. DPO [35] is\nanother alternative to RLHF that uses a preference loss function to directly optimize the LLMs, and\nhas been shown to be more stable and less computationally intensive than RLHF. Despite these efforts,\nall the methods ignore the noise in the training data, which can lead to suboptimal performance.\nDistributionally Robust Optimization. DRO differs from traditional robust optimization methods\n[22, 24, 43] by minimizing the worst-case error within an uncertainty set defined by constraints\nlike -divergence [30, 14], Wasserstein distance [38, 40, 20], and shape [23, 9]. [28, 29] introduced\nparametrization to the uncertainty set for greater architectural flexibility. Separately, [45] addressed\nsensitivity to outliers in DRO, diverging from the other studies."}, {"title": "B Limitations", "content": "The current work introduces Dr. DPO, an enhancement to Direct Preference Optimization (DPO) that\naddresses label flipping noise in training datasets through an additional hyperparameter \u03b2'. Despite\nthe robust performance indicated by empirical results with a default \u03b2' value of 1.0, the need for\nparameter tuning in different applications remains. The sensitivity of \u03b2' to data and task specifics\nmay necessitate a search process to fully leverage Dr. DPO's potential.\nMoreover, the experimental foundation of this study is built upon a 2.8B model. The scalability\nand generalization of Dr. DPO on larger models, such as 7B or greater, have not been explored.\nUnderstanding Dr. DPO's effectiveness across various model sizes is a critical aspect for future\ninvestigation."}, {"title": "C Broader Impacts", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many\npotential societal consequences of our work, none of which we feel must be specifically highlighted\nhere."}, {"title": "D Appendix of Proofs", "content": "D.1 Proof of Theorem 3.1\nThm 3.1 (Optimal Reward Function under KL Divergence). Let the Kullback-Leibler (KL) divergence\nbetween policy \\(\\pi_{\\theta}\\) and reference policy \\(\\pi_{ref}\\) be defined as: \\(D_{KL}(\\pi_{\\theta} \\| \\pi_{ref}) = \\int \\pi_{\\theta}(x) \\log \\frac{\\pi_{\\theta}(x)}{\\pi_{ref}(x)} dx\\).\nOptimizing the RM-DRO objective as defined in Equation (7) yields an optimal reward rKL(x, y)\ngiven by:\n\n\n\\(r_{KL}(x, y) = \\beta^*(\\eta) \\log \\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)} - \\alpha.\\)\n\n\nHere, a, \u1e9e are Lagrange multipliers, \\(\\beta^* (\\eta)\\) denotes the optimal value of \u1e9e that minimizes Equation\n(7), acting as the regularization coefficient in DPO. By deriving the optimal value of a, given by:\n\n\n\\(\\alpha^* = -\\beta \\log E_{x\\sim D,y\\sim \\pi_{ref}}[\\exp(\\frac{r_{\\phi}(y|x)}{\\beta})],\\)\n\n\nEquation (8) can be re-expressed to match the ultimate form of the reward function in Equation (4).\nProof.\nDefinition D.1 ($-divergence [31]). For any convex function \u00a2 with \\(\\phi(1) = 0\\), the $-divergence\nbetween Q and Qo is:\n\n\n\\(D_{\\phi}(\\pi_{\\theta}, \\pi_{ref}) := E_{\\pi_{ref}} [\\phi(\\frac{\\pi_{\\theta}}{\\pi_{ref}})].\\)\n\n\nwhere \\(D(Q, Q_0) = \\infty\\) if Q is not absolutely continuous with respect to Q0. Specially, when\n\\(\\phi(x) = x \\log x \u2212 x + 1\\), 4-divergence degenerates to the well-known KL divergence.\nDefinition D.2 (Convex conjugate [19]). We consider a pair (A, B) of topological vector spaces and\na bilinear form (\u00b7, \u00b7) \u2192 R such that (A, B, (\u00b7, \u00b7)) form a dual pair. For a convex function f : R \u2192 R,\ndom f := {x \u2208 R : f(x) < \u221e} is the effective domain of f. The convex conjugate, also known as\nthe Legendre-Fenchel transform, of f : A\u2192 R is the function f* : B \u2192 R defined as\n\n\n\\(f^*(b) = \\sup_{a}{ab \u2212 f(a)}, \\qquad b\\in B\\)\n\n\nThm D.3 (Interchange of minimization and integration [4]). Let (\u03a9, F) be a measurable space\nequipped with \u03c3-algebra F, LP(\u03a9, F, P) be the linear space of measurable real valued functions\nf : \u03a9 \u2192 R with \\(||f||_p < \\infty\\), and let X := LP(\u03a9, F, P), p \u2208 [1, +\u221e]. Let g : R \u00d7 \u03a9 \u2192 R be a\nnormal integrand, and define on X. Then,\n\n\n\\(\\min_{x\\in X} \\int_{\\Omega} g(x(\\omega), \\omega) dP(\\omega) = \\min_{s\\in R} \\int_{\\Omega} g(s, \\omega) dP(\\omega)\\)\n\n\nTo ease the derivation, we denote the likelihood ratio \\(L(y|x) = \\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)}\\). Note that the\n-divergence between \\(\\pi_{\\theta}\\) and \\(\\pi_{ref}\\) is constrained, and thus L(.) is well-defined. For brevity, we usually\nshort L(yx) as L. And in terms of Definition D.1 of 6-divergence, the expression of RM-DRO (cf.\nEquation (7)) becomes:\n\n\n\\(L_{CRM-DRO} = \\max E_{x\\sim D,y\\sim \\pi_{ref}} [r_{\\theta}(y|x)L] \\qquad s.t. E_{\\pi_{ref}} [\\phi(L(y|x))] \\leq \\eta\\)\n\n\nNote that \\(E_{\\pi_{ref}} [r_{\\theta}(y|x)L]\\) and \\(E_{\\pi_{ref}}[\\phi(L(y|x))]\\) are both convex in L. We use the Lagrangian function\nsolver:\n\n\n\\(L_{CRM-DRO} = \\min_{\\beta \\geq 0, \\alpha} \\max_{L(y|x)} {E_{x\\sim D,y\\sim \\pi_{ref}}[r_{\\theta}(y|x)L(y|x)] \u2013 \\beta[E_{\\pi_{ref}}[\\phi(L(y|x))] \u2013 \\eta] + \\alpha(E_{\\pi_{ref}}[L(y|x)] \u2013 1)}\\)\n\n\n\\(= \\min_{\\beta \\geq 0, \\alpha} {\\beta \\eta - \\alpha + \\beta \\max_{L(y|x)} {E_{x\\sim D,y\\sim \\pi_{ref}} [\\frac{r_{\\theta} (y|x) + \\alpha}{\\beta}L(y|x) \u2013 \\phi(L(y|x)) ]}}\\)\n\n\n\\(= \\min_{\\beta \\geq 0, \\alpha} {\\beta \\eta - \\alpha + \\beta E_{x\\sim D,y\\sim \\pi_{ref}} [\\max_{L(y|x)} {(\\frac{r_{\\theta}(y|x) + \\alpha}{\\beta} L(y|x) \u2013 \\phi(L(y|x)) ]}}\\)\n\n\n\\(= \\min_{\\beta \\geq 0, \\alpha} {\\beta \\eta - \\alpha + \\beta E_{D,y\\sim \\pi_{ref}}[\\phi^* (\\frac{r_{\\theta}(y|x) + \\alpha}{\\beta}) ]}\\)"}, {"title": "D.2 Proof of Theorem 4.1", "content": "Thm 4.1. Consider the scenario where the KL divergence is employed to measure the discrepancy\nbetween the hypothetical distribution O' and dataset distribution O", "follows": "n\n\n\\(L_{Dr. DPO"}, "pi_{\\theta}; \\pi_{ref}) = -\\beta' \\log E_{O'} [\\exp(- \\frac{h_{DPO}(x, y_w, y_\\iota)}{\\beta'})"], "as": "n\n\n\\(h_{DPO"}, {"function": "n\n\n\\(O(O'", "problem": "n\n\n\\(O''^* = \\arg \\max_{O'"}, {"follows": "n\n\n\\(O_{KL"}, {"follows": "n\n\n\\(O(O', \\beta') = \\beta' \\log E_{O} [\\exp( \\frac{h(x, y_w, y_\\iota)}{\\beta'})]\\)\n\n\nIn order to attain a Distributionally Robustifying DRO objective that encompasses both pointwise\nand pairwise robustness, we consider the previously established fact that the DPO approach confers\npointwise robustness. Consequently, by"}]