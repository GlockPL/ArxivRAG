{"title": "Towards Robust Alignment of Language Models:\nDistributionally Robustifying Direct Preference\nOptimization", "authors": ["Junkang Wu", "Yuexiang Xie", "Zhengyi Yang", "Jiancan Wu", "Jiawei Chen", "Jinyang Gao", "Bolin Ding", "Xiang Wang", "Xiangnan He"], "abstract": "This study addresses the challenge of noise in training datasets for Direct Prefer-\nence Optimization (DPO), a method for aligning Large Language Models (LLMs)\nwith human preferences. We categorize noise into pointwise noise, which includes\nlow-quality data points, and pairwise noise, which encompasses erroneous data\npair associations that affect preference rankings. Utilizing Distributionally Robust\nOptimization (DRO), we enhance DPO's resilience to these types of noise. Our\ntheoretical insights reveal that DPO inherently embeds DRO principles, confer-\nring robustness to pointwise noise, with the regularization coefficient \u03b2 playing a\ncritical role in its noise resistance. Extending this framework, we introduce Dis-\ntributionally Robustifying DPO (Dr. DPO), which integrates pairwise robustness\nby optimizing against worst-case pairwise scenarios. The novel hyperparameter\n\u03b2' in Dr. DPO allows for fine-tuned control over data pair reliability, providing a\nstrategic balance between exploration and exploitation in noisy training environ-\nments. Empirical evaluations demonstrate that Dr. DPO substantially improves the\nquality of generated text and response accuracy in preference datasets, showcasing\nenhanced performance in both noisy and noise-free settings.", "sections": [{"title": "1 Introduction", "content": "Aligning Large Language Models (LLMs) [32, 41, 1, 8] with human preferences is critical for their\nimplementation in real-world scenarios. Central to the alignment is the fine-tuning of LLMs using\nhuman feedback [33], ensuring they adhere to human values and mitigate safety risks. Among\nthe alignment methods, Reinforcement Learning from Human Feedback (RLHF) [33] is becoming\na widely adopted technology. It initially learns a reward model on pairwise preference data, and\noptimizes LLMs using the Proximal Policy Optimization (PPO) [37] method. However, its inherent\nreinforcement learning nature poses significant challenges to computational efficiency and training\nstability [35, 46]. Addressing these, Direct Preference Optimization (DPO) [35] eschews the explicit\nreward model learning, using human preferences to train the LLMs directly. It achieves the same\nobjectives [2] as RLHF by learning an optimal proxy for each pointwise instance and simultaneously\nranking preferences in a pairwise manner, offering greater simplicity and training stability [21]."}, {"title": "2 Preliminaries", "content": "Bradley-Terry Model. Given a context x within a finite space of contexts X, we employ the\npolicy \u03c0(y|x) to independently generate a pair of actions (y1, y2). These actions are presented to\nhuman raters, who then indicate their preference, with the preferred action labeled as yw and the less\npreferred as y\u03b9, satisfying yw \u2265 y\u03b9. Although we cannot directly observe the latent reward model\nr* (x, y) that underlies these preferences, the Bradley-Terry (BT) model [7] offers a well-established\napproach for modeling pairwise comparisons, which is given as:\np*(Y1Y2x) = \\frac{exp (r* (x, y1))}{exp(r*(x, y\u2081) + exp(r*(x, y2)))}.\nGiven the dataset O = {(x(i), yu), y(i))1}N sampled from p*, we can parametrize a reward model\nr(x, y) and estimate the parameters by optimizing the following logistic regression loss:\nL_{LR}(r_{\\phi}, O) = -E_{(x,y_w,y_\\iota)\\sim O}[log\\sigma(r_{\\phi}(x,y_w) \u2013 r_{\\phi}(x,y_\\iota))],\nwhere \u03c3(\u00b7) is the sigmoid function. As the size of dataset O grows, the empirical distribution of the\ndataset O converges to the underlying distribution p*, and the reward model r\u00f8 converges to the true\nreward model r*.\nReinforcement Learning from Human Feedback (RLHF) [33]. The standard RLHF paradigm is\ncomposed of three phases: i) supervised fine-tuning, ii) reward modeling, and iii) RL fine-tuning.\nUsing the reward model r\u00f8 learned from the reward modeling, we can then fine-tune the policy \u03c0\u03b8 by\noptimizing the following objective:\nmax_{\\pi_{\\theta}} E_{x\\sim O,y\\sim\\pi_{\\theta} (y/x)} [r_{\\phi}(x, y)] \u2013 \\beta D_{KL}[\\pi_{\\theta}(y|x)||\\pi_{ref}(y|x)].\nIn practice, both the language model policy \u03c0\u03b8 and the reference policy Tref are typically initialized to\nthe same supervised fine-tuning (SFT) model \u03c0SFT. Here, \u03b2 is a parameter that controls the strength\nof the regularization term, and DKL represents the KL divergence penalty used to regularize the policy\n\u03c0\u03b8 to be close to ref.\nDirected Preference Optimization (DPO) [35]. DPO offers an alternative approach to the RL\nparadigm described above. It establishes a functional mapping between the reward model and the\noptimal policy under a KL divergence constraint with the following formulation:\nr(x,y) = \\beta log \\frac{\\pi_{\\theta} (y|x)}{\\pi_{ref}(y|x)} + \\beta log Z(x),\nwhere Z(x) = \u03a3y \u03c0ref(y|x) exp(r(x, y)/\u03b2) is the partition function. By incorporating this reward\ninto the BT model, the DPO objective enables the comparison of response pairs, facilitating the\ndiscrimination between preferred and dispreferred actions, given by:\nL_{DPO}(\\pi_{\\theta}; \\pi_{ref}) = -E_{(x,y_w,y_\\iota)\\sim O}[log \\sigma(\\beta log \\frac{\\pi_{\\theta}(y_w | x)}{\\pi_{ref}(y_w | x)} - \\beta log \\frac{\\pi_{\\theta}(y_\\iota | x)}{\\pi_{ref} (y_\\iota|x)})].\nDistributionally Robust Optimization (DRO) [22, 24, 43]. DRO provides a strategic framework to\neffectively mitigate the uncertainty inherent in training data. It achieves this by optimizing for the\nworst-case expected loss across a set of potential distributions Q. These distributions are confined\nwithin a robustness radius \u03b7 anchored around the empirical training distribution Qo, and are bounded\nby a prescribed divergence metric D. The formal formulation of DRO can be succinctly expressed\nas follows:\nL_{DRO} = max_{Q} E_{Q}[L(x; \\theta)], \\quad s.t. D_{\\zeta}(Q, Q_0) \\leq \\eta,"}, {"title": "3 Analyzing DPO's Pointwise Robustness", "content": "In this section, we examine the resilience of DPO to pointwise noise. We analyze how DPO responds\nto noise, highlighting its strengths and vulnerabilities."}, {"title": "3.1 Pointwise Noise Impairs DPO Performance", "content": "We start by investigating the impact of pointwise noise on DPO by conducting experiments on the\nIMDB sentiment dataset [27]. Following the setup in [18], we fine-tune the GPT-2-large [34] model\nand employ SiEBERT [17], a specialized variant of ROBERTa-large [26], for reward calculation.\nWe introduce pointwise noise by integrating an additional proportion of low-quality pairs (\u03b6\u03c9, \u03c8\u03b9)\ngenerated by the unrefined GPT-2-large model to the training set. To quantitatively measure DPO's\nrobustness against pointwise noise, we evaluate the performance of each algorithm by examining the\ntrade-off between the achieved reward and the KL divergence from the reference policy."}, {"title": "3.2 Pointwise Robustness in Reward Modeling", "content": "In Section 3.1, we investigate the negative impact of pointwise noise on the reward of individual\ninstances. To address this issue and enhance the robustness of LLMs, we propose integrating DRO\nduring the reward modeling stage. We define the Reward Modeling DRO (RM-DRO) objective, which\noptimizes the expected reward under the worst-case noise distribution within a specified ambiguity\nset as follows:\nmax_{\\pi_{\\theta}} E_{x\\sim O,y\\sim\\pi_{\\theta} (y|x)} [r_{\\phi}(x, y)] \\quad s.t. D_{\\zeta}(\\pi_{\\theta}(y|x), \\pi_{ref}(y|x)) \\leq \\eta.\nThe direct consequence of pointwise noise is the resultant unreliability of the reference model (SFT).\nBy adopting RM-DRO, we aim to maximize a surrogate objective that accounts for various potential\ndistributions within a robustness radius \u03b7 around the reference distribution \u03c0ref(y|x), measured by\nthe distance metric D. With this formulation, we provide a fresh perspective on DPO.\nA. DPO is Implicitly a Pointwise DRO.\nThm 3.1 (Optimal Reward Function under KL Divergence). Let the Kullback-Leibler (KL) divergence\nbetween policy \u03c0\u03b8 and reference policy Tref be defined as: D_{KL}(\\pi_{\\theta}|\\pi_{ref}) = \\int \\pi_{\\theta}(x) log \\frac{\\pi_{\\theta}(x)}{\\pi_{ref}(x)} dx."}, {"title": "4 Dr. DPO: Toward Pairwise Robustness", "content": "In this section, we investigate the impact of pairwise noise and introduce Dr. DPO as a mitigation\nstrategy. We conclude with a theoretical examination of its robustness against such noise."}, {"title": "4.1 Pairwise Noise Impairs DPO Convergence and Performance", "content": "We previously explored DPO's pointwise robustness, while recent work [10] has examined its re-\nsilience to pairwise noise. However, methods that rely on explicit noise estimation may overlook"}, {"title": "4.2 Distributionally Robustifying DPO", "content": "Building upon the principles of DRO, we introduce the Distributionally Robustifying DPO (Dr. DPO)\nframework, designed to enhance DPO's resilience to pairwise noise while preserving its inherent\nrobustness to pointwise noise. The Dr. DPO objective is formulated as follows:\nmax_{O'} E_{(x,y_w,y_\\iota)\\sim O'} [h(x, y_w,y_\\iota)] \\quad s.t. D_{\\zeta}(O', O) \\leq \\eta'.\nHere, h(x, yw, Yi) = log(r(x,yw) \u2013 ro(x, y)) denotes the log-likelihood objective of dataset\npoint. The -divergence, denoted as D\u2084(O', O), quantifies the discrepancy between the hypothetical\ndistribution O' and the dataset distribution O. Additionally, \u03b7' signifies the robustness radius, which\nquantifies the degree to which the model can withstand perturbations.\nThm 4.1. Consider the scenario where the KL divergence is employed to measure the discrepancy\nbetween the hypothetical distribution O' and dataset distribution O, we derive the ultimate loss\nfunction for Dr. DPO as follows:\nL_{Dr. DPO}(\\pi_{\\theta}; \\pi_{ref}) = -\\beta' log E_{O} \\left[ exp\\left( -\\frac{h_{DPO}(x, y_w, y_\\iota)}{\\beta'}\\right)\\right],\nwhere hppo represents the log-likelihood in the DPO framework, defined as:\nh_{DPO}(x, y_w, y_\\iota) = log \\sigma\\left(\\beta log \\frac{\\pi_{\\theta}(y_w | x)}{\\pi_{ref}(y_w | x)} - \\beta log \\frac{\\pi_{\\theta}(y_\\iota | x)}{\\pi_{ref}(y_\\iota|x)}\\right),\nwith \u03b2 and \u03b2' being regularization coefficient respectively.\nPlease check Appendix D.2 for detailed proofs. In contrast to the objective function described in\nEquation (7), our Dr. DPO method specifically targets the pairwise noise present within the dataset.\nRather than assigning a uniform weight of to each instance (x(i), yu), y(i)) [35], Dr. DPO seeks\nto reweight these instances by an optimal distribution O'. This approach is driven by the intention to\ncapture the incorrect pairs in the data, thereby enhancing the robustness of the resulting policy.\nThm 4.2 (Upper Bound for Dr. DPO). Let hDPO \u2208 [a, b] and LDr., DPO represents the Dr. DPO loss on\nN samples. Given a hypothetical distribution O' satisfying DKL(O', O) \u2264 n' to dataset distribution\nO, we have that with probability at least 1 \u03b4:\nL_{O'} \\leq L_{Dr. DPO} + B(\\delta, N, \\beta'),\nwhere:\nB(\\delta, N, \\beta') = \\frac{2b \\exp ((b \u2212 a)/\u03b2')}{N \u2212 1 + \\exp ((b \u2212 a)/\u03b2')} \\sqrt{\\frac{N}{2} \\ln {\\frac{1}{\\delta}}}.\nPlease check Appendix D.3 for detailed proofs. In scenarios involving pairwise noise, consider O'\nas the \"ideal\" distribution that discerns the correct ranking between pairwise instances accurately.\nTheorem 4.2 suggests the ideal loss relative to O' is upper bounded by the proposed Dr. DPO loss.\nThis bound is achieved when B(\u03b4, N, \u03b2') approaches zero, or in other words, the number of samples"}, {"title": "4.3 Why is Dr. DPO Robust to Pairwise Noise?", "content": "Our approach extends the analysis presented in Rafailov et al. [35]. To understand the resilience of\nDr. DPO to pairwise noise, we examine the gradient of its loss function, denoted VoLDr. DPO:\n\\nabla_{\\theta}L_{Dr. DPO}(\\pi_{\\theta}; \\pi_{ref}) = -\\beta E_{(x,y_w,y_\\iota)\\sim O}\\left[ w(x, y_w, y_\\iota) \\sigma(r_{\\theta}(x, y_\\iota) \u2013 r_{\\theta}(x, y_w)) (\\nabla_{\\theta,y_w} - \\nabla_{\\theta,y_\\iota})\\right],\nwhere r\u03b8(x, y) = Blog \u03c0\u03b8(yx) Tref (y) represents the reward function implicitly learned by the policy \u03c0\u03b8,\nrelative to a reference policy #ref. In this framework, V0,y and Vey are gradients increasing the\nprobability of the \"chosen\" action yw and decreasing it for the \"rejected\" action y\u0131, respectively. The\nfactor \u03c3(\u0390\u03b8 (x, y\u03b9) \u2013 fo(x, Yw)) serves to amplify the gradient contributions from mismatched action\npairs, which is a principal aspect of the Dr. DPO's design aimed at enhancing learning from compara-\ntive feedback. Conversely, the function w(x, Yw, y\u0131), defined as w(x, y_w, y_\\iota) = \\frac{exp(h(x,y_w,y_\\iota)/\\beta')}{E_O \\left[ exp(h(x,y_w,y_\\iota)/\\beta')\\right]}\n(cf. Appendix D.4), acts to mitigate the influence of these incorrect pairings. It achieves this by\npreferentially weighting correct action pairs over incorrect ones, thus refining the policy update\nmechanism. Moreover, the parameter \u1e9e' does not require intensive tuning; setting it to a default value\nof 1 typically yields stable enhancements. Remarkably, Dr. DPO is straightforward to implement,\nrequiring only an additional line of code with negligible computational overhead."}, {"title": "5 Experiments", "content": "In this section, we conduct an empirical assessment of Dr. DPO to evaluate its ability to mitigate noise\nimpacts in preference datasets and to improve performance in noise-free environments. We outline our\nexperimental design, including the datasets used, evaluation metrics, and comparative benchmarks.\nOur results underscore Dr. DPO's effectiveness, supporting its utility in relevant scenarios."}, {"title": "5.1 How Well can Dr. DPO Resist the Pairwise Noise?", "content": "Datasets and Setup. We conduct experiments on two datasets: IMDB [27] and Anthropic HH [3].\nThe IMDB dataset is widely utilized for sentiment analysis tasks. The Anthropic HH dataset consists\nof approximately 170,000 dialogues between humans and automated assistants. The objectives of\nthese experiments were twofold: firstly, to evaluate the robustness of the proposed Dr. DPO against\npairwise noise; and secondly, to investigate whether Dr. DPO exhibits superior performance on\nnoise-free datasets. To achieve the first objective, we introduce random inversions between selected"}, {"title": "6 Conclusion", "content": "In this study, we analyze DPO's robustness from a DRO perspective, highlighting its resilience to\npointwise noise. We establish a link between DPO's regularization and DRO's robustness, showing\nthat a smaller regularization parameter B enhances stability against uncertain data. Our experiments\nconfirm the crucial role of \u1e9e in noise resistance but uncover DPO's weakness against pairwise noise.\nTo address this, we introduce a novel Distributionally Robustifying DPO (Dr. DPO) framework with"}, {"title": "A Related Work", "content": "Reinforcement Learning from Human Feedback. RLHF [11, 3, 41, 33] has emerged as a key\nmethod for aligning language models with human values and preferences, mitigating the generation\nof biased or factually incorrect outputs. Compared to supervised learning, RLHF is rather complex,\nless stable, and requires more memory resources. These challenges have motivated the development\nof alternatives to the RLHF pipeline. For example, RAFT [13] uses an existing reward model to\nselect the best set of training samples based on the model outputs, while RRHF [44] leverages a much\nsimpler ranking loss to align human preferences and retain the performance of PPO. DPO [35] is\nanother alternative to RLHF that uses a preference loss function to directly optimize the LLMs, and\nhas been shown to be more stable and less computationally intensive than RLHF. Despite these efforts,\nall the methods ignore the noise in the training data, which can lead to suboptimal performance.\nDistributionally Robust Optimization. DRO differs from traditional robust optimization methods\n[22, 24, 43] by minimizing the worst-case error within an uncertainty set defined by constraints\nlike -divergence [30, 14], Wasserstein distance [38, 40, 20], and shape [23, 9]. [28, 29] introduced\nparametrization to the uncertainty set for greater architectural flexibility. Separately, [45] addressed\nsensitivity to outliers in DRO, diverging from the other studies."}, {"title": "B Limitations", "content": "The current work introduces Dr. DPO, an enhancement to Direct Preference Optimization (DPO) that\naddresses label flipping noise in training datasets through an additional hyperparameter \u03b2'. Despite\nthe robust performance indicated by empirical results with a default \u1e9e' value of 1.0, the need for\nparameter tuning in different applications remains. The sensitivity of \u1e9e' to data and task specifics\nmay necessitate a search process to fully leverage Dr. DPO's potential.\nMoreover, the experimental foundation of this study is built upon a 2.8B model. The scalability\nand generalization of Dr. DPO on larger models, such as 7B or greater, have not been explored.\nUnderstanding Dr. DPO's effectiveness across various model sizes is a critical aspect for future\ninvestigation."}, {"title": "C Broader Impacts", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many\npotential societal consequences of our work, none of which we feel must be specifically highlighted\nhere."}, {"title": "D Appendix of Proofs", "content": "D.1 Proof of Theorem 3.1\nThm 3.1 (Optimal Reward Function under KL Divergence). Let the Kullback-Leibler (KL) divergence\nbetween policy \u03c0\u03b8 and reference policy tref be defined as: D_{KL}(\\pi_{\\theta}|\\pi_{ref}) = \\int \\pi_{\\theta}(x) log \\frac{\\pi_{\\theta}(x)}{\\pi_{ref}(x)} dx.\nOptimizing the RM-DRO objective as defined in Equation (7) yields an optimal reward rKL(x, y)\ngiven by:\nr_{KL}(x, y) = \\beta^*(\\eta) log \\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)} - \\alpha.\nHere, a, \u1e9e are Lagrange multipliers, \u03b2* (n) denotes the optimal value of \u1e9e that minimizes Equation\n(7), acting as the regularization coefficient in DPO. By deriving the optimal value of a, given by:\n\\alpha^* = -Blog E_{x\\sim D,y\\sim \\pi_{ref}}[exp(-\\frac{r_{\\phi}(y|x)}{\\beta})],\nEquation (8) can be re-expressed to match the ultimate form of the reward function in Equation (4).\nProof.\nDefinition D.1 ($-divergence [31]). For any convex function \u00a2 with $(1) = 0, the $-divergence\nbetween Q and Qo is:\nD_{\\phi}(\\pi_{\\theta}, \\pi_{ref}) := E_{\\pi_{ref}} [\\phi(\\frac{\\pi_{\\theta}}{\\pi_{ref}})]\nwhere D(Q, Qo) = \u221e if Q is not absolutely continuous with respect to Q0. Specially, when\n\u03c6(x) = x log x \u2212 x + 1, 4-divergence degenerates to the well-known KL divergence.\nDefinition D.2 (Convex conjugate [19]). We consider a pair (A, B) of topological vector spaces and\na bilinear form (\u00b7, \u00b7) \u2192 R such that (A, B, (\u00b7, \u00b7)) form a dual pair. For a convex function f : R \u2192 R,\ndom f := {x \u2208 R : f(x) < \u221e} is the effective domain of f. The convex conjugate, also known as\nthe Legendre-Fenchel transform, of f : A\u2192 R is the function f* : B \u2192 R defined as\nf^*(b) = sup_{a}{ab \u2212 f(a)}, b\u2208 B\nThm D.3 (Interchange of minimization and integration [4]). Let (\u03a9, F) be a measurable space\nequipped with \u03c3-algebra F, LP(\u03a9, F, P) be the linear space of measurable real valued functions\nf : \u03a9 \u2192 R with ||f||p < \u221e, and let X := LP(\u03a9, F, P), p \u2208 [1, +\u221e]. Let g : R \u00d7 \u03a9 \u2192 R be a\nnormal integrand, and define on X. Then,\nmin_{X \\in X} \\int_{\\Omega} g(x(\\omega), \\omega) dP(\\omega) = \\int_{\\Omega} min_{s \\in R} g(s, \\omega) dP(\\omega)\nTo ease the derivation, we denote the likelihood ratio L(y|x) = \u03c0\u03b8(y|x)/\u03c0ref(y|x). Note that the\n-divergence between \u03c0\u03b8 and Tref is constrained, and thus L(.) is well-defined. For brevity, we usually\nshort L(yx) as L. And in terms of Definition D.1 of 6-divergence, the expression of RM-DRO (cf.\nEquation (7)) becomes:\nL_{CRM-DRO} = max E_{x\\sim D,y\\sim \\pi_{ret}} [r_{\\theta}(y|x)L] \\quad s.t. E_{\\pi_{ref}} [\\phi(L(y|x))] \\leq \\eta\nNote that Eref [ro(y|x)L] and E\u03c0ref[$(L(y|x))] are both convex in L. We use the Lagrangian function\nsolver:\nL_{CRM-DRO} = min_{\\beta\\geq 0,\\alpha} max_{L(y|x)} \\{E_{x\\sim D,y\\sim \\pi_{ref}}[r_{\\theta}(y|x)L(y|x)] \u2013 \\beta[E_{\\pi_{ret}}[\\phi(L(y|x))] \u2013 \\eta] + \\alpha(E_{\\pi_{ref}}[L(y|x)] \u2013 1)\\}\n= min_{\\alpha\\geq 0,\\beta} \\{ \\beta\\eta - \\alpha + \\beta max_{L(yx)} \\{E_{x\\sim D,y\\sim \\pi_{ref}}[\\frac{r_{\\theta}(y|x) + \\alpha}{\\beta} L(y|x) \u2013 \\phi(L(y|x))]\\}\\}\n= min_{\\alpha\\geq 0,\\beta} \\{ \\beta\\eta - \\alpha + \\beta E_{x\\sim D,y\\sim \\pi_{ref}}[ max_{L(yx)}\\{\\frac{r_{\\theta}(y|x) + \\alpha}{\\beta} L(y|x) \u2013 \\phi(L(y|x))\\}]\\}\n= min_{\\alpha\\geq 0,\\beta} \\{ \\beta\\eta - \\alpha + \\beta E_{x\\sim D,y\\sim \\pi_{ref}}[\\phi^* (\\frac{r_{\\theta}(y|x) + \\alpha}{\\beta})]\\}"}, {"title": "D.2 Proof of Theorem 4.1", "content": "Thm 4.1. Consider the scenario where the KL divergence is employed to measure the discrepancy\nbetween the hypothetical distribution O' and dataset distribution O, we derive the ultimate loss\nfunction for Dr. DPO as follows:\nL_{Dr. DPO}(\\pi_{\\theta}; \\pi_{ref}) = -\\beta' log E_{v}\\left[ exp\\left( -\\frac{h_{DPO}(x, y_w, y_\\iota)}{\\beta'}\\right)\\right].\nwhere hppo represents the log-likelihood in the DPO framework, defined as:\nh_{DPO}(x, y_w, y_\\iota) = log \\sigma\\left(\\beta log \\frac{\\pi_{\\theta}(y_w | x)}{\\pi_{ref}(y_w | x)} - \\beta log \\frac{\\pi_{\\theta}(y_\\iota | x)}{\\pi_{ref}(y_\\iota|x)}\\right),\nwith \u03b2 and \u03b2' being regularization coefficient respectively.\nProof. To solve the optimization problem in Equation (11), we first introduce the Lagrangian function:\nO(O', \u03b2', \u03b1') = E_{(x,y_w,y_\\iota)\\sim O'} [h(x, y_w, y_\\iota)] + \u03b2' (D_{\\zeta}(O', O) \u2013 n') + a'(Eo[8] \u2212 1).\nThen, we can obtain the optimal distribution O''* by solving the following saddle-point problem:\nO''^* = arg \\underset{O'}{max} \\underset{\\beta',\\alpha'}{min} O(Q, \u03b2', a').\nSpecifically, when the KL divergence is selected as the measure of $-divergence, that is, KL(O', 0) =\n\u03a3Olog(0/0), the optimal distribution Ok can be derived as follows:\nO''_{KL} = \\frac{1}{Z^*} \\frac{1}{N} exp \\left( \\frac{h(x, y_w, y_\\iota)}{\\beta'}\\right)\nwhere Z* = E(x,yw,y1)~0 [exp(h(x, Yw, Y\u0131)/\u03b2')] denotes the partition function. In this case, we can\nderive a closed-form expression of the ultimate objective O(O\u045c, 1') as follows:\nO(O', \u03b2') = \u03b2' log Eo [exp(\\frac{h(x, \u03c8\u03c9, \u03c8\u03b9)}{\u03b2'})]\nIn order to attain a Distributionally Robustifying DRO objective that encompasses both pointwise\nand pairwise robustness, we consider the previously established fact that the DPO approach confers\npointwise robustness. Consequently, by substituting the term h(x, yw, y\u0131) in Equation (31) with\nthe DPO objective from Equation (5), we can derive a comprehensive objective that integrates the\nstrengths of both methods:\nL_{Dr. DPO} (\\pi_{\\theta}; \\pi_{ref}) = -\\beta' log E_O [exp(h_{DPO} (x, y_w, y_\\iota)/\\beta')].\nHere hppo = log \u03c3(\u03b2log \u03c0\u03bf) - Blog \u03c0\u03bf(\u03b9)) denotes the optimal policy using in DPO."}, {"title": "D.3 Proof of Theorem 4.2", "content": "Thm 4.2 (Upper Bound for Dr. DPO). Let hppo \u2208 [a, b] and LDr. DPO represents the Dr. DPO loss on\nN samples. Given a hypothetical distribution O' satisfying DKL(O', O) \u2264 n' to dataset distribution\nO, we have that with probability at least 1 \u03b4:\nL_{O'} \\leq L_{Dr. DPO} + B(\\delta, N, \\beta'),\nwhere:\nB(\\delta, N, \\beta') = \\frac{2b \\exp ((b \u2212 a)/\u03b2')}{N \u2212 1 + \\exp ((b \u2212 a)/\u03b2')} \\sqrt{\\frac{N}{2} \\ln {\\frac{1}{\\delta}}}.\nProof. Firstly, we assume that the optimal policy O' satisfies the following constraint:\nO' \u2208 {Q | DKL(O', 0) \u2264 \u03b7'}\nUnder this assumption, the loss function Lo, can be bounded as:\nL_{O'} = E_{O'} [h(x, y_w, y_\\iota)] \\leq \\underset{DKL(0,0)\u2264n'}{max} E_{O'} [h(x, y_\u03c9, \u03b6\u03b9)]\n= \\beta' log E_{v} \\left[ exp\\left( -\\frac{h(x,y_w,y_\\iota)}{\\beta'}\\right)\\right] = L_{Dr. DPO}.\nWe now introduce McDiarmid's inequality as a foundational result:\nThm D.4 (McDiarmid's Inequality). Let X1, ..., XN \u2208 XN be a set of N > 1 independent random\nvariables and assume that there exists c1, ..., CN > 0 such that f : X\u00d1 \u2192 R satisfies:\n|f(x1, ..., Xi, ..., XN) \u2212 f (x1, ..., Xi, ..., XN)| \u2264 Ci.\nFor all i \u2208 1, 2, ...N and any points x1, ...XN, x \u2208 X. Let f(S) denote f(X1, ..., XN), then for all\n\u20ac > 0, the following inequalities hold:\nP[f(S) - E{f(S)} \u2265 \u20ac] < exp \\left( \\frac{-2\u20ac^{2}}{\\sum_{i=1}^{N} C_i^{2}}\\right)\nGiven a dataset with N samples, for any pair of samples: (x, yw, y\u0131), (x', yw, y\u00ed), we have:\n|w(x, yw, y\u0131)h((x, yw, y\u0131)) \u2013 w(x', Yw, Y\u00ed)h((x', Yw, h))|\\leq\n<2 sup|w(x, yw, Y\u0131)h(x, Yw, Y\u0131)|\\leq\n<\\frac{2b \\exp ((b \u2212 a)/\u03b2')}{N -1+ \\exp ((b \u2212 a)/\u03b2')},\nwhere the second inequality holds as w(x, Yw, Y\u0131) = \\frac{exp(h(x,yw,y1)/\\beta')}{E_O [exp(h(x,y\u03c9,\u03b6\u03b9)/\u03b2')]} and hDPO \u2208 [a, b].\nBy applying McDiarmid's inequality, we obtain:\nP \\left(L_{Dr. DPO} - L_{DI. DPO} \\geq \u03b5 \\right) \\leq exp\\left(  \\frac{-2\u03b5^{2}}{\\frac{N}{(N - 1 + exp ((b \u2212 a)/\u03b2'))}} \\right)\nSetting:\n\u03b4 = exp(- \\frac{2\u03b5^{2}}{\\frac{N}{(N - 1 + exp ((b \u2212 a)/\u03b2'))}})\nwe can solve for & as:\n\u03b5 = \\sqrt{ \\frac{N}{(N - 1 + exp ((b \u2212 a)/\u03b2'))} \\frac{1}{2} ln \\frac{1}{\u03b4}}\nThus, for any \u03b4 \u2208 (0, 1), we conclude that with probability at least 1 \u2013 \u03b4:\nL_{O'}  \\leq L_{Dr. DPO} \\frac{N\\sqrt{\\frac{N}{(N - 1 + exp ((b \u2212 a)/\u03b2'))} \\frac{1}{2} ln \\frac{1}{\u03b4}}"}, {"title": "D.4 Proof of w(x, Yw, Y\u0131)", "content": "In this section, we present the derivation of the gradient for the Dr. DPO objective function as follows:\n\\nabla_{\\theta}L_{Dr. DPO} (\\pi_{\\theta}; \\pi_{ref}) = \\nabla_{\\theta}\\beta' log E_v \\left[ exp\\left( -\\frac{h_{DPO}(x, y_w, y_\\iota)}{\\beta'}\\right)\\right]\n\nVe\u00df' log Eo exp\nThe right-hand side of Equation (42) can be rewritten as:\nVe\u00df' log Eo exp [exp (\\frac{hDPO (X, Yw, Yl)}{\\beta'})] = hDPO(1)Blog Eo [exp (\\frac{hDPO (X, Yw, Yl)}{\\beta'})]] VohDPO\nConsidering the gradient with respect to the function hDPO (X, Yw, y\u0131) yields:\nVhowe' 3'log Eo [exp (ho(2-))] = 3'Eo [exp()]VhDPO\nFocusing on the gradient with respect to 0, we have:\nVe hDPO = Velog \u03c3\u03b2log - Blog = \u03c3'(\u03ba) Vou,\n\u03c3(\u03b9)\nwhere u = Blog \u03c0\u03bf- Blog \u03c0\u03bf\u03c0\u03b9\u03bf\u03bf. Leveraging the properties of the sigmoid function, where\n\u03c3'(x) = \u03c3(x)(1 \u2212 \u03c3(x)) and \u03c3'(\u2212x) = 1 \u2212 \u03c3(x), we derive the final gradient expression:\nVe\u00df' logEo [exp (hpo/B\nB' [Velog \u03c0\u03bf(X) - Velog \u03c0\u03bf.\nHere, a crucial indicator that distinguishes Dr. DPO from traditional DPO is encapsulated by the\nweight term:\nw(x, Y\u03c9, \u03c8\u03b9) =\nHDPO (X,Yw,Y1) \u03b9)\n[exp (h\nEo exp\nwhich gravitates towards a uniform distribution as the parameter \u03b2' approaches infinity. In such a\nscenario, the gradient of Dr. DPO aligns with that of the standard DPO. This relationship furnishes a\ndeeper insight into how Dr. DPO can be linked and differentiated from DPO through the incorporation\nof a dynamic tuning parameter \u03b2', enhancing the adaptability of policy optimization in varied\nenvironments."}, {"title": "E Analysis", "content": "E.1 Analysis about general -divergence.\nLemma E.1 (Optimal Reward Function under General $-Divergence). [42, Theorem 1] Given a\n$-divergence D$ with corresponding derivative \u00a2', the optimal reward function r$(x, y) under the\nRM-DRO framework is defined by:\nr_{\\phi}(x,y) = \\beta^* (\\eta)\\phi' (\\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)}) - \\alpha,\nwhere a is Lagrange multiplier.\nProof. To determine the optimal expression for r\u00f8(x, y), one must identify the optimal L(y|x). As\nestablished in Theorem 3.1, the optimal L(y|x) is given by:\n\\beta E_{x\\sim D,y\\sim \\pi_{ret}} \\text{arg} \\underset{L(yx)}{\\text{max}} \\left[  \\frac{r_{\\theta}(y|x) + \\alpha}{\\beta}L(y|x) \u2013 \\phi(L(y|x))\\right]\n= \\beta E_{x\\sim D,y\\sim \\pi_{ret}} \\text{arg} \\underset{L(yx)}{\\text{max}} \\left[ \\frac{r_{\\theta}(y|x) + \\alpha}{\\beta}L(y|x) \u2013 \\phi(L(y|x))\\right]\nTo find this maximum, we differentiate the convex function w.r.t. L(yx) and equate the derivative to\nzero:\n\\frac{\\partial}{\\partial L} \\{\\frac{r_{\\phi} (y|x) + \\alpha}{\\beta} -\\phi(L(y|x))\\} = 0.\nSolving for re (y|x) yields the optimal expression:\nr_{\\phi}(y|x) = \\beta \\phi'(L(y|x)) - \\alpha.\nGiven that a is a constant, the critical component of the expression is \u03b2\u03c6'(L(y|x)). While this result\naligns with Theorem 1 from [42], our approach is grounded in a comprehensive DRO framework,\nproviding a more direct and complete theoretical justification. Thus, the lemma is substantiated.\nComparison with Wang et al. [42]. Since a is a constant that does not affect the optimization\nprocess, Lemma E.1 reveals that the reward function r(x, y) is influenced not only by the choice\nof -divergence but also by the parameter B. This observation suggests an intuitive understanding\nthat various -divergences enforce constraints with unique geometric characteristics, which, in\nturn, dictate the optimal value of \u03b2* (\u03b7) within DRO. Interestingly, our findings contradict the\nconclusions presented in Wang et al. [42], which suggest that various -divergences lead to different\nalignment accuracy. Instead, our results demonstrate that by fine-tuning the parameter \u1e9e, comparable\nperformance can be achieved across different divergences (cf. Table 2). This underscores the critical\nrole of the robust radius in determining the efficacy of DRO frameworks. For detailed experimental\nsettings, please refer to Section 5.1. Moreover, a thorough analysis of the behavior of diverse\n-divergences can be found in Appendix F."}, {"title": "F Appendix of Experiments", "content": "1 # pi_logps\n: policy logprobs, shape (B,)\n2 # ref_logps\n: reference model logprobs, shape (B,)\n3 # yw_idxs\n: preferred completion indices in [0, B-1], shape (T,)\n4 # yl_idxs\n: dispreferred completion indices in [0, B-1], shape (T,)\n5 # beta\n: regularization coefficient\n6 # beta_1\n: regularization coefficient for pairwise robustness\n7\n8 pi_yw_logps, pi_yl_logps = pi_logps [yw_idxs], pi_logps [yl_idxs]\n9 ref_yw_logps, ref_yl_logps = ref_logps [yw_idxs], ref_logps[yl_idxs]\n10 pi_logratios = pi_yw_logps pi_yl_logps\n11 ref_logratios = ref_yw_logps ref_yl_logps\n12 losses = -F.logsigmoid ( beta * (pi_logratios ref_logratios))\n13\n14 #DPO\n15 DPO_loss = losses.mean()\n16\n17 #Dr. DPO\n18 DrDP0_loss = beta_1 * torch.log(torch.mean(torch.exp( losses / beta_1)))\nFigure 7 presents a PyTorch-style pseudocode comparison between the standard objective and our\nproposed Dr. DPO objective. The implementation simplicity of the Dr. DPO loss is highlighted, as\nit necessitates no additional lines of code beyond what is required for the standard objective. This\nease of integration underscores the practicality of adopting Dr. DPO in existing machine learning\nworkflows without the need for extensive code modifications.\nF.1 Experiments Setup on HH\nFor our preliminary research, we conducted experiments on the Anthropic HH dataset [3], which\ncomprises 170,000 human-automated assistant dialogues. Each dialogue concludes with two large\nlanguage model-generated responses and an accompanying preference label denoting the human's\nfavored choice. Our training regimen was in line with the DPO-established protocol [35]. We built\nupon the Pythia 2.8B model, as described in [5], to develop our Supervised Fine-Tuning (SFT)\nmodel. The SFT model was fine-tuned on the Anthropic HH dataset over the course of one epoch,\nemploying a batch size of 64 and a learning rate of 5 \u00d7 10-7. In addition, we further refined the\nmodel using the Anthropic HH dataset and the DPO loss function (or other baseline approaches)\nthrough an additional epoch of fine-tuning. To test the model's resilience to noise, we introduced\nrandom inversions between selected and rejected responses in the training data with probabilities of\n10%, 20%, 30%, and 40%. Throughout these experiments, we consistently set the \u1e9e parameter to 0.1\nand adopted the Kullback-Leibler (KL) divergence as the metric for $-divergence. We carried out all\ncomputational tasks on a suite of four 80GB A100 GPUs.\nF.2 Experiments on Reddit TL;DR Dataset\nFor a fair comparison with DPO, we maintained the parameters \u03b2 = 0.5 and lr = 1e \u2013 6, and chose\n\u03b2' = 1.0 without extensive tuning. This approach ensures that our evaluation of the proposed Dr.\nDPO framework is consistent and comparable to the existing baseline.\nFinally, the table below presents the win-rate comparison on the TL;DR dataset under various\nsampling temperatures, further supporting our claims:"}, {"title": "F.3 Experiments on Ambiguous Datasets", "content": "To incorporate the feedback regarding the evaluation of our approach on datasets with ambiguity-\ninduced noise, we conducted additional experiments. These were aimed at understanding how\nperforms under varying conditions of data perturbation, specifically through token masking and\nsubstitution. The comparative analysis between the traditional DPO and Dr. DPO was carried out\nunder consistent experimental conditions to ensure the validity and reliability of the results.\nExperimental Setup. To simulate ambiguous datasets, we introduced randomness in the form of\ntoken masking and substitution at different ratios, thereby increasing the difficulty of the dataset. The\nintention was to assess the resilience and adaptability of our Dr. DPO method under challenging\nconditions that are akin to real-world scenarios. The experiments were conducted using the HH\ndataset, known for its complexity and relevance in evaluating data processing algorithms.\nThe configurations for both DPO and Dr. DPO were kept consistent with previous experiments\nto maintain comparability. Specifically, we set \u03b2 = 0.1 and the learning rate lr = 5e 7 for\nboth approaches. For Dr. DPO, an additional hyperparameter, \u03b2', was introduced and set to 1.0.\nNotably, we did not undertake extensive hyperparameter tuning, opting instead for a straightforward\ncomparison.\nExperimental Results. The results of our experiments are summarized in the table below, illustrating\npreference accuracies under varying noise conditions:"}, {"title": "F.4 Reward", "content": "Reward. The reward metric is computed on the IMDB dataset, which is selected for the availability\nof a ground-truth reward function provided by a sentiment classifier. Figure 8 demonstrates that\nthe Dr. DPO algorithm achieves enhanced stability and superior reward performance under varying\npairwise noise and different B. Additionally, by setting \u1e9e' to a fixed value of 1, we address the issue\nof DPO's sensitivity to the parameter B. This consistent setting of \u1e9e' eliminates the need for extensive\nparameter tuning, which significantly benefits practical applications."}, {"title": "F.5 Impact of Varying B", "content": "Concomitantly, we have carried out an evaluation of the performance enhancement of Dr. DPO\nrelative to DPO under diverse beta values, as shown in Figure 9. The aforementioned Dr. DPO assures\na stable performance augmentation regardless of beta selection, further attesting to the efficacy of the"}, {"title": "F.7 Discussion", "content": "Comparison with LDR [48]: Unlike LDR, which applies DRO for robust multi-class classification,\nDr. DPO is tailored for preference learning tasks. While LDR offers pointwise robustness by\nadjusting weights for individual class labels per instance x, Dr. DPO provides pairwise robustness by\noptimizing weights for each pair of responses (yw, y\u0131) within dataset O. Furthermore, LDR seeks\nto reduce overfitting by decreasing the weights of selected instances, whereas Dr. DPO counters\nmismatched pair effects by up-weighting chosen response pairs."}]}