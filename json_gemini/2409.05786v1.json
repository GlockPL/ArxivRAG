{"title": "Leveraging Object Priors for Point Tracking", "authors": ["Bikram Boote", "Anh Thai", "Wenqi Jia", "Ozgur Kara", "Stefan Stojanov", "James M. Rehg", "Sangmin Lee"], "abstract": "Point tracking is a fundamental problem in computer vision with numerous applications in AR and robotics. A common failure mode in long-term point tracking occurs when the predicted point leaves the object it belongs to and lands on the background or another object. We identify this as the failure to correctly capture objectness proper- ties in learning to track. To address this limitation of prior work, we propose a novel objectness regularization approach that guides points to be aware of object priors by forcing them to stay inside the the boundaries of object instances. By capturing objectness cues at train- ing time, we avoid the need to compute object masks during testing. In addition, we leverage contextual attention to enhance the feature repre- sentation for capturing objectness at the feature level more effectively. As a result, our approach achieves state-of-the-art performance on three point tracking benchmarks, and we further validate the effectiveness of our components via ablation studies. The source code is available at: https://github.com/RehgLab/tracking_objectness", "sections": [{"title": "1 Introduction", "content": "Point tracking, which is the estimation of point correspondences across mul- tiple frames in a video sequence, is a fundamental problem in computer vi- sion. The estimation of point correspondences is fundamental for many tasks in AR/VR [26,37,56], SfM/SLAM [7,23,38], and autonomous driving [17,25,31]. Point tracking also can play a crucial role in instance-level recognition. By ac- curately tracking points belonging to specific object instances across frames, we can concretely understand instance behavior over time, which can be leveraged in robotics applications involving object manipulation [1,15]. Point tracking in extended video sequences [10, 13, 33, 35, 49, 57] is extremely challenging because 1) the appearance of points can change dramatically due to viewpoint, lighting, and shape changes, and 2) points can become occluded and disoccluded over"}, {"title": "2 Related Works", "content": "Optical flow aims to precisely estimate the continuous motion of every pixel between two consecutive images, providing a detailed map of movement across the entire scene. Prior works in this domain can be categorized into two main streams: classical variational approaches [2-4, 8, 12, 16, 29] and recently deep learning-based techniques [11, 18\u201320, 41, 46, 53]. Classical approaches, based on assumptions like color constancy and motion smoothness within localized pixel neighborhoods, faced challenges such as the aperture problem and the inability to handle substantial displacements within the scene. In contrast, Teed et al. [46] introduced RAFT, a deep learning paradigm for optical flow estimation. RAFT leverages a 4D correlation volume to compute pixel feature similarity across frames, followed by an iterative update process to estimate the flow. This 4D cost volume approach pioneered by RAFT has been adopted by many subsequent"}, {"title": "2.1 Optical Flow", "content": "time. Recent particle video-style methods such as PIPS++ [57] address these challenges by leveraging multi-frame temporal context windows to improve the robustness of appearance modeling and leverage temporal continuity in tracking individual target points. In contrast, optical flow-based methods [30,44,46] esti- mate the motion vectors of all pixels between a pair of frames, and then establish point tracks by chaining flow vectors together over multiple frames. This often leads to significant accumulation of error and tracking failure due to occlusions. However, one potential advantage of flow based methods is that they leverage the spatial continuity of motion, which arises from the fact that points on the same object often move in a similar way. Recently, CoTracker [22] presented a method to jointly track multiple points and demonstrated that leveraging ad- ditional support points in the vicinity of a target point can improve extended point tracking performance.\nThe central thesis of this paper is that the performance of particle video- style point trackers can be significantly improved by leveraging spatial continuity through the inclusion of an objectness prior, leading to effective instance-level awareness. Moreover, we show that this can be accomplished by introducing an objectness loss only at training time, which obviates the need for object segmen- tation at testing time. This allows us to directly incorporate spatial continuity without incurring substantial run-time computational cost, which is extremely beneficial in many applications like AR and robotics. Although prior methods like GAFlow [30] and CoTracker [22] have incorporated neighborhood informa- tion to learn better feature representations, they do not explicitly capture ob- jectness properties in an efficient manner.\nThe intuition behind our approach is illustrated in Figure 1. Figure 1 (a) shows that SOTA trackers frequently predict point correspondences that leave the target object (in this case the figure behind the wall). Once a predicted point deviates from the object it belongs to, it is very difficult for subsequent"}, {"title": "3 Method", "content": "The point tracking problem can be formulated as follows: Given an input video\n$V\\in R^{T\\times H\\times W\\times 3}$ with $T$ frames and an initial point denoted as $p_1 \\in R^2$ on\nthe first frame, our goal is to predict the corresponding point trajectory $P =$\n$\\left\\{P_t\\right\\}_{t=1}^T \\in R^{T\\times 2}$ throughout the entire video. In this section, we first address the\npreliminary framework of persistent independent particles (PIPs) [13,57] that\nour method builds upon. We then present our novel objectness regularization\nscheme that encourages points to adhere to object boundaries, followed by a\ncontextual attention module that enhances object awareness at the feature level\nfor improved tracking. Figure 2 shows the overview of our proposed approach."}, {"title": "3.1 Preliminary", "content": "The Persistent Independent Particles (PIPs) framework [13,57] tackles the prob- lem of estimating dense point trajectories over a video sequence. The key idea is to track each point independently by leveraging a learned temporal prior and an iterative inference mechanism to refine the trajectory estimates. PIPs++ [57], which our work mainly builds upon, consists of two stages: initialization and iterative updates.\nIn the initialization stage, a 2D CNN encoder is used to extract a feature map $F_t$ for each frame $I_t$. The feature representing the appearance of the initial"}, {"title": "3.2 Objectness Regularization", "content": "target point $p_1$, denoted as $f_1$, is obtained through bilinear sampling from the\nfirst frame feature map $F_1$, at the position corresponding to the point. All point\nlocations and features in the subsequent $T-1$ frames are then initialized with\nthe first target point location and feature, $\\left\\{(P_1, f_1)\\right\\}$.\nThe iterative update stage aims to refine the trajectory $P \\in R^{T\\times 2}$ of $p_1$ over\n$K$ iterations. At each iteration $k$, for each frame $t$, PIPs++ initially extracts\nlocal spatial feature crops, around the current estimated point position $p_t$ from\nthe frame feature $F_t$ at multiple scales. Correlation features between the initial\npoint feature $f_1$ and each feature crop are computed via the dot product. After\nobtaining correlation features from multiple feature crops, they are concatenated\nwith motion vectors $(p_t-p_{t-1})$, and then passed through a 1D ResNet to\npredict position updates $\\Delta p_t^k$. The new position estimates for the next iteration\n$k + 1$ are then obtained as $p_t^{k+1} = p_t^k + \\Delta p_t^k$. To adapt to appearance changes,\nafter the first iteration, PIPs++ also correlates feature crops with the recently\ntracked point features $f_{t-2}$ and $f_{t-4}$, in addition to the initial point feature $f_1$.\nConsequently, using the initial point feature $f_1$ preserves the initial appearance\nof the target point while incorporating $f_{t-2}$ and $f_{t-4}$ contributes to considering\nrecent appearance features, which enables effective tracking despite occlusions\nand appearance changes.\nEach point either belongs to a specific object or the background. Points asso- ciated with the same object typically exhibit similar movement patterns and should consistently remain within the object mask. Hence, we propose inte- grating this underlying object property to mitigate the common issue of points drifting toward different objects or the background while missing the target ob- ject. As shown in Figure 1, despite both predictions Pred1 and Pred2 being at an equal distance from the ground truth point location, Pred1 yields a bet- ter prediction due to its placement within the object boundary. Anchoring the predicted points onto the correct object helps avoiding drift towards unrelated objects, which results in more effective long-term tracking.\nTo this end, we enforce objectness prior in our model via our novel objectness regularization with a training loss $L_{obj}$, enhancing instance-aware point tracking. We leverage the ground truth object masks from [57] for training. In an object mask map, different objects are represented by different values. Specifically, we penalize the model when the predicted point does not belong to the same object mask as the ground-truth point. The loss $L_{obj}$ for objectness regularization is formulated as:"}, {"title": "3.3 Contextual Attention", "content": "Feature maps used for matching in motion estimation tasks, such as optical flow and point tracking, need to exhibit two key characteristics. Firstly, they should be locally discriminative. Secondly, they should promote smoothness in motion within the neighborhood, which stems from the observation that nearby points on an object tend to exhibit similar motions. A primary reason for failure in classical optical flow methods [29] is the reliance on limited context information, which often results in challenges such as the aperture problem [32]. CNN-based feature extractors [14, 42] employing standard pooling layers to reduce spatial dimension might fail to capture local object boundaries effectively.\nTo enhance the feature representation for objectness in tandem with the objectness regularization, we leverage a contextual attention module inspired by the optical flow work [55]. The contextual attention encodes neighborhood contexts for local feature regions. As a result, the enhanced feature maps pro- duce sharper peaks in correspondence matching, facilitating the distinction of individual objects even when they have similar visual patterns.\nWe first extract feature maps from the 2D CNN encoder with d channels and partition them into non-overlapping patches of size M \u00d7 M (red squares in the module of Figure 2 indicates patches). Each feature patch then attends to neighborhood 3 \u00d7 3 patches, including itself. For each attention head j, we project the vectorized feature patch $Q \\in R^{M^2\\times d}$ and the surrounding 3 \u00d7 3 vectorized feature patches $V \\in R^{9M^2\\times d}$ to dimension $d_{proj}$ using learnable linear projection layers, resulting in $Q_{proj}^j$ and $V_{proj}^j$. We then compute the attention\n$h_j$, with $Q_{proj}^j$ serving as the query and $V_{proj}^j$ as both the key and value. The\noutputs of n attention heads $h_j$ are then concatenated and fed through a linear"}, {"title": "4 Experiments", "content": "We train our model with the videos of the Point Odyssey training set and evaluate it on the Point Odyssey test set, TAP-Vid-DAVIS, and CroHD datasets following the experimental setting of PIPs++ [57]. Following are further details about the point tracking datasets we used:\nPoint Odyssey. PointOdyssey [57] dataset is a synthetic benchmark for long- term tracking. This dataset involves around 100 videos with several thousand frames consisting of scenes with both camera and object motion. The test set consists of 12 videos ranging from 884 to 4325 frames in duration.\nTAP-Vid-DAVIS. TAP-Vid-DAVIS [9] is a real-world dataset consisting of 30 videos each around 100 frames long with points queried on random objects at random times and during evaluation. TAP-Vid-DAVIS has uses two evaluation protocols, namely \"queried first\" and \"queried strided\". In the \"queried first\" protocol each point is queried only once which is at the first frame where they become visible, and in the \"queried strided\" protocol points are queried every five frames with tracking being bidirectional. We evaluate our method on the \"query-first\" protocol following [57].\nCroHD. CroHD [50] is a real-world dataset consisting of surveillance-like videos of crowds with tracks annotated on all human heads, with videos varying in length from around 500 frames to a few thousand frames. For evaluation, videos longer than a thousand frames are broken down into thousand frame sequences, yielding a total of 12 sequences."}, {"title": "4.1 Datasets", "content": "projection layer to produce a feature vector $H \\in R^{M^2\\times d}$, where $d_{proj}=d/n$.\nThese procedures are formulated as follows:\n$h_j = softmax(Q_{proj}^j V_{proj}^{j^T}/\\sqrt{d_{proj}}). V_{proj}^j (4)$\n$H = Linear(concat([h_1,..., h_n])). (5)$\nThen, H replaces the corresponding feature patch region of Q. By applying this process in a sliding window manner, we eventually obtain an improved feature map $F_t$ that is aware of the neighborhood context. By incorporating the contextual attention module, our approach enhances the feature representation, making it more effective in distinguishing individual objects based on contextual information."}, {"title": "4.2 Implementation", "content": "Model Architecture. We use the same 2D CNN encoder and 8-block 1D Resnet block for position update estimation as PIPs++ [57].The 2D CNN en- coder is based on a modified ResNet architecture consisting of one convolutional layer with 64 kernels followed by 4 layers consisting of 2 residual blocks each, where each layer has 64, 96, 128, 128 kernels respectively. The output from each of these residual layers are concatenated and passed through two more convo- lutional layers with 256 and 128 kernels respectively, thus producing a feature map with 128 channels and resolution downsampled by a factor of 8. We use the ReLU [34] activation and Instance normalization [48] in our encoder. The feature maps from the CNN encoder is further passed through 6 layers of the contextual attention module, with each layer having 8 attention head and using 7 x 7 patches. To compute correlation maps for feature similarity, we compute dot product between the reference features and feature maps at every timestep at 4 different scales in coarse-to-fine manner. Finally to get the correlation vec- tors for each point, we sample the correlation maps in a 3 x 3 neighborhood of the estimated point location. The 1D ResNet module to compute the position"}, {"title": "4.3 Performance Evaluation", "content": "updates consists of 1 convolutional layer followed by 8 1D residual blocks and finally a densely connected layer to produce the required position updates for each track.\nTraining Details. We train our model on 140K clips of 24 frames generated from the Point Odyssey [57] train dataset. Each clip has a resolution of 384 x 512 and consists of 128 point tracks. Our model is trained for 300K iterations with a batch-size of 2 using the AdamW [27] optimizer and a learning rate of 0.005 with the 1cycle learning rate policy [43]. We use the same \u03b3 = 0.8 in $L_{seq}$ with [57] and \u03b1 = 0.15 as weight for our objectness regularlization at training time. Training the model on two RTX 4090 GPUs takes around 2.5 days.\nEvaluation metrics. We use the same evaluation metrics used by Zheng et al. [57], namely average position accuracy $d_{avg}$, Survival and Median Trajectory"}, {"title": "4.4 Qualitative Results", "content": "Error (\u039c\u03a4\u0395). $\u03b4_{avg}$ was proposed in TAP-Vid [9] and computed as the average over the percentage of trajectories within a threshold of 1, 2, 4, 8, 16 pixels to the ground truth, in a normalized resolution of 256 x 256. Survival is defined as the ratio of the average number of frames until tracking failure over the video length and failure happens when the L2 distance between the predicted and ground truth trajectory exceeds 50 pixels in the normalized resolution of 256 x 256. The MTE metric measures the median of the distance between the estimated and ground truth tracks. We evaluate our method with videos at a resolution of 512 x 896 for Point Odyssey dataset [57] and Tap-Vid-DAVIS dataset [9], and we use a resolution of 768 x 1280 for CroHD dataset [50].\nCompared Methods. We use point-trackers like PIPS [13], TAP-Net [9], PIPs++ [57], CoTracker [22], an optical flow based method RAFT [46] (where tracks are generated by chaining estimated flows together for consecutive frames) and a feature-matching method DINO [5] to compare our method against. For RAFT and DINO pretrained weights are used for evalutation while all the other methods are trained with clips from the Point Odyssey training split. We ob- tain the numbers for the different metrics for PIPS, RAFT, DINO from the PointOdyssey [57] paper and for CoTracker from its respective paper. Following CoTracker for fair comparison, the PIPs++ numbers are obtained by using their publicly released official weights and code.\nPerformance comparisons. Table 1, 3 and 4 shows tracking performance comparison with $d_{avg}$, Survival, and MTE metrics on the PointOdyssey, TAP- Vid-DAVIS, and CroHD datasets, respectively. Our method overall outperforms existing prior methods, showing the effectiveness of our proposed designs. Table 2 shows the comparison results with specific \u03b4 metrics (i.\u0435., $\u03b4\u03b1\u03c5\u03b1, \u03b4\u03c5\u03bd\u03b1 \u03b4\u03b1\u03c2, \u03b4\u03c5$), where the later two are similar to $d_{avg}$, but with only visible or occluded points. We outperform both methods on the first two metrics and achieve competitive performance for $doce$.\nFigure 3 shows qualitative results with visualized points from PIPs++ and ours. As shown in the first two rows of images, the animated humanoid (circled in blue) becomes occluded by a wall of similar color in the initial frame, making it a very challenging scenario. In the following frames, predicted points from PIPs++ fails to stay within the target humanoid but drift away to the wall. In contrast, our approach prevents such drift and keeps tracking the underlying object they belong to. For the below example, our approach tracks the points on the shoe (circled in red) well while PIPs++ fails to do so. In the case of PIPs++, many points leave the object (i.e., shoes) and fail to return to the correct target object."}, {"title": "4.5 Effect of Proposed Designs", "content": "the paraglider (a) even under sudden change of orientation of the person and can even track the single point on the very thin rope. (b) and (c) are demostrations of effective tracking by our method under fast motion and also under complete change of viewpoint in the case of the biker. In (d) we can see our model can track points consistently even under motion blur and over come occlusions as the cyclist crosses the tree-barks in the view. These examples show the effectiveness of our method in tracking points in real-world videos with diverse motion.\nTable 5 shows the effect of our \"Objectness Regularization\" and \"Contextual At- tention\" with the baseline model, PIPs++. As shown in the table, both object- ness regularization and contextual attention components properly contributes to the point tracking performances (i.\u0435., \u03b4avg, Survival, MTE). As a result, we achieve the best performances on all the evaluation metrics with our final model including both objectness regularization and contextual attention. Note that the objectness regularization does not require any computational overhead. It is only"}, {"title": "5 Discussion", "content": "Our work demonstrates the effectiveness of learning object priors in point track- ing by utilizing object masks which are readily available in synthetic environ- ments. Synthetic data is mostly used in practice for training point tracking meth- ods due to the ease of obtaining point correspondence labels. However, there can be a domain gap between synthetic data and real-world data, thus training with further real-world data could be beneficial for practical real-world applications. Future work could explore the use of object masks generated by foundation mod- els like Segment Anything [24] to extend our method to real-world data training, potentially bridging this domain gap."}, {"title": "4.6 Effect of Regularization Weight", "content": "applied at training time. In addition, the contextual attention module only re- quires a small number of network parameters. As a result, the parameter number of our baseline is 17.6M while the parameter number of our proposed method is 18.6M. This gap is quite marginal, but the performances are significantly improved by the proposed designs.\nTable 6 shows Survival performances based on the weight \u03b1 for our objectness regularization (please refer to equation (3) in Section 3.2). Note that \u03b1 adjust the weight of objectness regularization compared to the typical distance loss. As shown in the table, we could obtain higher performance than the existing methods with any weight \u03b1 values. In particular, we achieve the best result when using \u03b1 = 0.15."}, {"title": "6 Conclusion", "content": "In this work, we introduce a novel object-aware approach for point tracking that encourages tracked points to stay within the boundaries of object instances. Our key ideas include an objectness regularization scheme that penalizes points drift- ing outside their associated objects during training, and a contextual attention module that enhances feature representations to better distinguish individual ob- jects. Extensive experiments on the PointOdyssey, TAP-Vid-DAVIS, and CroHD benchmarks demonstrate the effectiveness of our approach, with state-of-the-art performance across multiple evaluation metrics. Ablation studies confirm the complementary benefits of the objectness regularization and contextual atten- tion components. Our approach substantially improves tracking robustness and accuracy without sacrificing efficiency."}]}