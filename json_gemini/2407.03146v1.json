{"title": "Enhancing Class Fairness in Classification with A Two-Player Game Approach", "authors": ["Yunpeng Jiang", "Paul Weng", "Yutong Ban"], "abstract": "Data augmentation is widely applied and has shown its benefits in different machine learning tasks. However, as recently observed in some downstream tasks, data augmentation may introduce an unfair impact on classifications. While it can improve the performance of some classes, it can actually be detrimental for other classes, which can be problematic in some application domains. In this paper, to counteract this phenomenon, we propose a FAir Classification approach with a Two-player game (FACT). We first formulate the training of a classifier with data augmentation as a fair optimization problem, which can be further written as an adversarial two-player game. Following this formulation, we propose a novel multiplicative weight optimization algorithm, for which we theoretically prove that it can converge to a solution that is fair over classes. Interestingly, our formulation also reveals that this fairness issue over classes is not due to data augmentation only, but is in fact a general phenomenon. Our empirical experiments demonstrate that the performance of our learned classifiers is indeed more fairly distributed over classes in five datasets, with only limited impact on the average accuracy.", "sections": [{"title": "1 Introduction", "content": "Data augmentation is a popular technique used in the field of computer vision to improve both training efficiency and performance of models [1][2]. It involves creating new training samples by applying semantically-preserving transformations, such as random shift[3] [4], random convolution [5] and random resized cropping, to the original data. The aim is to increase the diversity of the training dataset and prevent overfitting, leading to better generalizability to the testing dataset.\nHowever, recent research [6] has highlighted a potential issue with data augmentation: although it is beneficial in improving the overall performance, data augmentation can lead to disparities in performance across different classes. This means that while there are significant improvements on the performance of some classes, others may experience a decline or minimal gains. This disparity raises concerns about fairness in classification tasks. If certain classes are consistently under-performing, it can lead to biased outcomes and a lack of equitable treatment for all classes[7][8].\nAs a result, there is growing interest in developing methods to achieve equitable performance among all classes within classification tasks. The goal of fair classification problems is to ensure that the"}, {"title": "2 Related Work", "content": "Our fair classification problem is different from, although not completely unrelated, the usual fair machine learning research direction [13]. Here, our focus is on obtaining a classifier whose performance has a fair distribution over classes. In the realm of addressing fairness in classification tasks, several approaches have been proposed, which can be categorized in two themes: relabelling and specialized loss functions.\nOne strategy to mitigate the unfairness of per-class accuracy is through the process of relabelling. Kirichenko et al. [14] demonstrate that by leveraging the more precise multi-label annotations available in ImageNet, the typically negative impact of data augmentation on individual class accuracies is substantially reduced. A related method is proposed by Jung et al. [8], who suggest a method for assigning group labels based on confidence scores. Further, Anchlia and Choi [15] introduce a \"data clean-up\" technique aimed at replacing biased labels with more equitable ones. Additionally, they propose a reweighting method that estimates statistical fairness with respect to the inferred fair labels."}, {"title": "3 Background", "content": "Before recalling the GGF function and the multiplicative weight method, we introduce a few notations: An denotes the (n - 1)-simplex, i.e., the set of non-negative n-dimensional vectors w that sums to one. For a vector v, vector v\u2191 is obtained by sorting v in a non-decreasing order (i.e, v\u2081 < ... < ).\nGeneralized Gini Social Welfare Function (GGF) GGF [11] extends the concept of Gini coeffi-cient, often used to measure income equality, to a measure of social welfare including fairness:\n$GGF_{w\u2193}(u) = \\sum_{j=1}^{n} w_{j}^{\u2193} u_{j}^{\u2191},$                                                       (1)\nwhere w\u2193 \u2208 \u0394n such that w\u2081 > w > ... > w and u is a utility vector. As can be seen from its definition, GGF encodes fairness by assigning higher weights to worse-off components. When comparing two vectors, a higher GGF value indicates either overall utility and/or more equitable distribution. Interestingly, by setting w\u2193 close to (1,0, . . ., 0), GGF can encode the classic max-min fairness. One difficulty with exploiting GGF in applications is that it is not always clear how to choose this weight vector w\u2193, which actually controls a trade-off between efficiency and equity.\nMultiplicative Weight Method This method [24] is a learning algorithm, which can be applied by a player in adversarial two-player games. It updates the probabilities of the player's strategy based on the outcomes of previous timesteps. Without loss of generality, we assume that the player is the min player. The player initializes a mixed strategy, i.e., probability distribution w \u2208 An over all her strategies (e.g., with a uniform distribution). Then at each timestep t, the player chooses a strategy i sampled according to her current mixed strategy wt and incurs loss vi according to the other player's choice and the payoff matrix. The player then updates her mixed strategy using the multiplicative weight method. The probability of strategy i is updated as:\n$w_{i}^{t+1} \\leftarrow \\frac{w_{i}^{t} e^{-\\tau v_{i}}}{\\mathrm{Z}_{t}}, \\quad \\text { with } \\mathrm{Z}_{t}=\\sum_{i=1}^{n} w_{i}^{t} e^{-\\tau v_{i}}$ ,                                                       (2)\nwhere \u03c4 > 0 is a hyperparameter. This process is repeated for multiple rounds until convergence. The hyperparameter \u03c4 controls the update rate of the probabilities. Freund and Schapire [24] have"}, {"title": "4 Methodology", "content": "In this section, we introduce our proposed approach, FAir Classification with Two-player game (FACT), where the solution should satisfy two generally contradictory objectives. Firstly, it should achieve a high overall performance, the average accuracy over all classes should be maximized. Secondly, it should achieve fairness, the classification results should not favor any particular class over others. For this purpose, we formulate the fair classification problem as an adversarial two-player game, which motivates our adaptation of the multiplicative weights method. We finish this section with a theoretical proof of its convergence to an optimal fair classifier.\n4.1 Fair Classification as An Adversarial Two-Player Game\nTo motivate our formulation of the fair classification problem (see Equation (5)), we first recall the standard classification problem. Traditionally, an n-class classification problem is solved by simply maximizing the average performance over all classes:\n$\\max _{\\theta} \\frac{1}{n} \\sum_{i=1}^{n} v_{i}(\\theta)$ ,                                                       (3)\nwhere @ is the parameter of the classification model (e.g., a neural network) and vi(0) is the accuracy achieved by the model for the ith class. As usual in deep learning, this optimization problem is solved approximately by training the model (e.g., minimizing an empirical risk expressed with a convex loss such as the cross-entropy loss). Training is usually performed with the assistance of data augmentation, which can improve significantly the sample efficiency.\nHowever, this standard approach often results in a skewed performance distribution over classes, favoring some over others. To counteract this imbalance, a naive approach would be to resort to the standard minmax fairness2, which can be interpreted as a two-player game. This alternative"}, {"title": "perspective shifts the focus from the average performance to the performance of the worse classes:", "content": "$\\max _{\\theta} \\min _{\\omega \\in \\Delta_{n}} \\sum_{i} w_{i} v_{i}(\\theta)$ .                                                       (4)\nThis formulation aims to enhance the worst-class performance by assigning them greater weights. This two-player game could be solved as follows. At the end of training epoch t, the min player chooses w, a probability distribution over classes. With the knowledge of w, the max player computes her best response: she optimizes the weighted objective function \u2211i wivi(0) by updating 0. After the model parameter update at epoch t + 1, the loss of the min player is computed by the class accuracy weighted by w. Meanwhile, due to the nature of the zero-sum game, the loss of the max player is the negative of the loss incurred by the min player.\nNonetheless, this max-min approach raises another issue. The min player may finally converge (e.g., by applying the multiplicative weights method) to a w that assigns all probability weight to one worst class and zero probability to all the others. By focusing solely on the worst class, the performance of the other classes is neglected, potentially leading to sub-optimal outcomes of the classification model as a whole. To avoid this extreme case, we instead a constrained max-min formulation:\n$\\max _{\\theta} \\min _{\\omega \\in \\Lambda} \\sum_{i} w_{i} v_{i}(\\theta)$ ,                                                       (5)\nwhere A is a strict subset of An excluding its extreme points. Depending on applications, different subsets could possibly be defined. This refinement ensures that the maximization of the worst-class performance occurs with a controlled set of weights. Then the efforts to enhance the accuracies of the worst classes do not come at the cost of substantial reductions in the performance of other classes. This offers a more balanced approach to improving the worst-class performance while minimizing the impact on the other classes. This constrained max-min formulation defines our fair classification problem. Before explaining how it can be solved by adapting the multiplicative weight method, we further discuss how it is related to fair optimization as modeled by the GGF.\nThis refined problem shares a close relationship with the GGF (Equation (1)). Indeed, to enforce fairness via GGF, the objective function in Equation (3) can be changed to:\n$GGF_{w \\downarrow}(v(\\theta))=\\sum_{i=1}^{n} w_{i}^{\u2193} v_{i}^{\u2191}(\\theta)$                                                        (6)\nwhere w\u2193 \u2208 \u2206n is a strictly decreasing weight vector (i.e, w > > w). By maximizing this function during the training process, the model is not only optimized for higher overall accuracy but also for fairness among classes, leading to an increase of the accuracies of the worst classes. Interestingly, GGF can be rewritten as follows:\n$GGF_{w \\downarrow}(v(\\theta)) = \\min _{s \\in S_{n}} \\sum_{i} w_{s(i)} v_{i}(\\theta)$                                                       (7)\nwhere Sn is the symmetric group of order n, i.e., set of all permutations of n elements. Using this rewriting, optimizing GGF corresponds to the following max-min problem:\n$\\max _{\\theta} \\min _{s \\in S_{n}} \\sum_{i} w_{s(i)} v_{i}(\\theta)$                                                        (8)\nThis problem formulation indicates that when we train the model to optimize the GGF-based objective function, the min player chooses a permutation s, while the max player tune the weights @ to maximize the weighted sum defined by the min player. Therefore, our fair classification problem defined in Equation (5) includes the GGF-based formulation by setting the constrained simplex to only contain weights that corresponds to reorderings of weights w\u00b9. However, our generic fair classification problem is more flexible since it does not require to define precisely weight vector wt, which is hard to choose in practice.\n4.2 Fair Optimization via Multiplicative Weight Algorithm\nAs defined above, the min player has a mixed strategy w over classes while the max player maximizes the sum of weighted accuracies by optimizing 0. This adversarial two-player game is a well studied"}, {"title": "5.1 Baselines, Experimental Setup, and Evaluation Metrics", "content": "In order to show the effectiveness of our proposed method, we perform a set of experiments aimed at investigating whether our method alleviate unfairness in classification tasks. We compare our method with four baselines across five classification tasks. The objective is to demonstrate the efficacy of our method in comparison to these existing techniques.\nBaselines Except for normal cross entropy loss, we further select four baseline methods, which are specifically designed for training a fair classifier. These include focal loss [7], tilted cross entropy loss (TCE) [16], performance weighted loss (PW) [17] and GGF-enhanced cross entropy loss [12]. The detailed descriptions and hyperparameters of these baseline methods are listed in Appendix B.\nExperimental Setup To evaluate the performance of our method versus these baselines, we run experiments on five classification tasks: CIFAR-10, CIFAR-100 [26], Fashion-Mnist [27], Mini-ImageNet and ImageNet [28]. Following the setup of [6], we train a Resnet-50 model without loading of pretrained weights and apply random resized cropping as the data augmentation technique during the training process. The crop lower bound 0 < \u03b2 < 1 is a hyperparameter that determines the minimum possible size of the original image remaining in the cropped image. A smaller \u1e9e indicates a stronger data augmentation. We conduct experiments using 8 distinct crop lower bounds, ranging from 0.48 to 1. These experiments are carried out for each method across various tasks. Detailed hyperparameters of our method can be found in Appendix D.\nEvaluation Metrics To assess the effectiveness of our approach and compare it with the baselines, we employ the commonly used metric in classification tasks, namely the overall evaluation accuracy."}, {"title": "5.2 Results and Analysis", "content": "Through these experiments, we aim to answer the following questions in detail.\n\u2022 Does our method learn a classifier that is more fairly-distributed over classes?\n\u2022 Does our method improve the worst class performance?\n\u2022 Does our method degrade the overall performance a lot?\nDoes our method learn a classifier that is more fairly-distributed over classes? To effectively address whether our method learns a more equitably distributed classifier across classes, we analyze the results from five datasets. As detailed in Table 1, the results of each datasets are assessed using three key fairness metrics: standard deviation, coefficient of variation and range. These metrics are fundamental in evaluating the fairness of the learned classifiers. Our analysis reveals that, consistently across all five datasets, our method leads to a remarkable reduction in the standard deviation of class accuracies. This reduction indicates a narrower spread of accuracies among different classes, which is a positive indicator for fairness. The coefficient of variation further corroborates the trend by demonstrating a reduced relative variability in class accuracies. This decrease implies that the distribution of class accuracy is more uniform, which ensures fairness in learned classifiers. The range of class accuracies also shows a diminished gap between the best and worst class accuracies, indicating a more even performance across all classes. In summary, the fairness metrics from five datasets strongly substantiate the assertion that our method learns a classifier that exhibits a more balanced and thus fairer distribution of accuracies across classes.\nDoes our method improve the worst class performance? Here we focus on the improvement in worst-class performance. One overarching goal of our method is to ensure that the least accurate classes receive enhancement. To examine the efficacy of our method in improving worst class performance, we hereby specifically present the accuracies of the worst 10% classes across five diverse datasets"}, {"title": "6 Conclusion", "content": "In this paper, we addressed the critical issue of fairness in classification tasks. To tackle the chal-lenge, we propose FACT, a novel approach that conceptualizes the fair classification problem as an adversarial two-player game. With this novel formalization, we propose a multiplicative weight method that strategically adjusts class weights to achieve a balance between fairness and overall accuracy. We validate our method both empirically and theoretically. Consistent with our theoretical analysis, the experimental results show that our method successfully mitigates unfairness, improves the worst class performance, and has a limited impact on overall accuracy. Importantly, with very few hyperparameters, our method is simple yet effective. This simplicity makes our method easily adaptable to various machine-learning applications. For future work, our work lays a foundation for further exploration in fair machine learning. There is potential to extend our method to other domains and to combine it with additional techniques that promote fairness."}, {"title": "J Broader Impacts", "content": "Positive societal impacts By proposing and validating a novel method that balances accuracy with fairness, we contribute to the development of a more equitable machine learning system. This kind of system have the potential to positively affect various domains, such as healthcare and education, where unbiased decisions can lead to more just outcomes for all individuals.\nNegative societal impacts To the best of our knowledge, we don't see any negative societal impacts of our work."}]}