{"title": "Zero-Shot Interactive Text-to-Image Retrieval via Diffusion-Augmented Representations", "authors": ["Zijun Long", "Kangheng Liang", "Gerardo Aragon-Camarasa", "Richard Mccreadie", "Paul Henderson"], "abstract": "Interactive Text-to-Image Retrieval (I-TIR) has emerged as a transformative user interactive ways for applications in domains such as e-commerce and education. Yet, current methodologies predominantly depend on finetuned Multimodal Large Language Models (MLLMs), which face two critical limitations: (1) Finetuning imposes prohibitive computational overhead and long-term maintenance costs. (2) Finetuning narrows the pretrained knowledge distribution of MLLMs, reducing their adaptability to novel scenarios. These issues are exacerbated by the inherently dynamic nature of real-world I-TIR systems, where queries and image databases evolve in complexity and diversity, often deviating from static training distributions. To overcome these constraints, we propose Diffusion Augmented Retrieval (DAR), a paradigm-shifting framework that bypasses MLLM finetuning entirely. DAR synergizes Large Language Model (LLM)-guided query refinement with Diffusion Model (DM)-based visual synthesis to create contextually enriched intermediate representations. This dual-modality approach deciphers nuanced user intent more holistically, enabling precise alignment between textual queries and visually relevant images. Rigorous evaluations across four benchmarks reveal DAR's dual strengths: (1) Matches state-of-the-art finetuned I-TIR models on straightforward queries without task-specific training. (2) Scalable Generalization: Surpasses finetuned baselines by 7.61% in Hits@10 (top-10 accuracy) under multi-turn conversational complexity, demonstrating robustness to intricate, distributionally shifted interactions. By eliminating finetuning dependencies and leveraging generative-augmented representations, DAR establishes a new trajectory for efficient, adaptive, and scalable cross-modal retrieval systems.", "sections": [{"title": "1 INTRODUCTION", "content": "Interactive Text-to-Image Retrieval (I-TIR) seeks to identify relevant images for a user through a turn-by-turn dialogue with a conversational agent. This approach allows users to progressively refine their queries with the agent's guidance, facilitating the retrieval of highly specific results, even when initial queries are vague or simplistic [11], as shown in the top-left of Figure 1. This conversational paradigm is increasingly popular, as it is well-suited to content exploration use cases, such as fashion shopping or searching for long-tail content, where the agent can help the user refine their search [11, 20, 23, 25, 30].\nCurrent state-of-the-art solutions to I-TIR typically rely on finetuning Multimodal Large Language Models (MLLMs) for the retrieval task, aiming to bridge the domain gap between MLLM pretraining and retrieval objectives [10, 11]. However, we argue that this finetuning is not always needed. First, the one-to-one mapping between text and images that MLLM fine-tuning aims to establish is neither sufficient nor feasible for complex and diverse quires in I-TIR. Second, by restricting the pretrained distribution, finetuning compromises the broader generalizability that MLLMs acquire from pretraining, causing such models to underperform for I-TIR whenever they encounter dialogues outside the finetuning distribution. This motivates our key research question: How can we enhance the generalizability of I-TIR frameworks without additional training?\nTo address this research question, we begin by revisiting the motivations for fine-tuning MLLMs and examining its associated drawbacks. MLLM pretraining typically uses large-scale, noisy text-image pairs from internet or crowdsourced data to learn a joint embedding space. However, this approach often leads to sparse"}, {"title": "2 RELATED WORK", "content": "2.1 Visual Generative Models\nBefore Diffusion Models (DMs), the most successful visual generative models were Generative Adversarial Networks (GANs) [1, 5, 6], which introduced adversarial training for image synthesis [6]. While GANs achieved notable success in tasks like image generation and style transfer [6], they suffer from limitations such as mode collapse, training instability, and the need for carefully tuned architectures, making them less robust for diverse tasks [7].\nDMs address these challenges by using a noise-adding and noise-removal process, enabling stable and high-quality generation [2]. Initial approaches like Denoising Diffusion Probabilistic Models laid the groundwork [8], followed by advancements such as Stable Diffusion [18] and Imagen [19], which enhance efficiency and scalability [2, 19]. Diffusion models excel in generating high-fidelity, diverse outputs without mode collapse and are adaptable to multimodal tasks like text-to-image generation [17]. In this work, we aim to leverage the prior knowledge of DMs to bridge the domain gap between the pretraining tasks of Multimodal Large Language Models (MLLMs) and retrieval objectives, achieving superior zero-shot performance and better handling of unseen samples.\n2.2 Interactive Text-to-Image Retrieval\nInteractive text-to-image retrieval (I-TIR) overcomes the limitations of conventional single-turn methods by iteratively clarifying a user's information need across multiple dialog turns. This iterative process is especially effective for image retrieval, as a single initial query often fails to capture the fine details of target images. By incorporating user feedback over successive turns, I-TIR progressively aligns with user preferences, improving both retrieval accuracy and user satisfaction. However, this flexibility also adds complexity; the representation of a user's information need becomes more elaborate due to the diversity of multi-turn dialogs.\nRecent studies show the potential of LLMs and MLLMs for I-TIR. For instance, ChatIR [11] employs LLMs to simulate dialogues between users and answer bots, compensating for the scarcity of specialized text-to-image datasets tailored to I-TIR. Although this approach opens a promising direction for research, it does not address the unique training hurdles posed by I-TIR-specifically, handling highly diverse dialogue inputs. PlugIR [10] further advances I-TIR by improving the diversity of top-k results using k-means clustering to group similar candidate images and identify representative exemplars. However, this technique incurs additional computational overhead. Moreover, ChatIR and PlugIR, as well as other I-TIR strategies, rely on fine-tuning on small, curated datasets to achieve good results on a specific benchmark [13, 29, 32], limiting their ability to generalize to the broad range of real-world dialogues. Consequently, they often fail on out-of-distribution queries, leading to diminished performance in practical settings, as shown in Section 4.3.\nTo address these challenges, we propose the DAR framework, which prioritizes zero-shot I-TIR performance. By avoiding dataset-specific fine-tuning altogether, DAR avoids the reduced distribution space introduced by smaller, finetuned datasets, thereby achieving superior generalizability."}, {"title": "3 DAR FOR INTERACTIVE TEXT-TO-IMAGE RETRIEVAL", "content": "In this section, we present the Diffusion Augmented Retrieval Framework (DAR), illustrated in Figure 2, which comprises three main steps: dialogue reformulation, imagination (image generation), and matching.\nTo bridge the domain gap between the pretraining tasks of multimodal large language models (MLLMs) and Interactive Text-to-Image Retrieval (I-TIR) without additional training, DAR employs two generative models to imagine user intentions and generate intermediate representations for retrieval:\n\u2022 Large Language Model (LLM): An LLM is utilized to adapt the dialogue context, ensuring it closely aligns with the retrieval model's input requirements and user's intent. This adaptation enhances the relevance of retrieval results by reducing ambiguities in dialogues.\n\u2022 Diffusion Model (DM): DMs provide valuable prior knowledge about text-to-image mappings-information that unfinetuned MLLMs lack. By generating multiple images based on LLM refined prompts, DMs create multifaceted representations of the user's intent, thereby bridging the domain gap and eliminating the need for fine-tuning MLLMs.\nThe integration of these intermediate representations offers a richer and more robust foundation for bridging the text-to-image domain gap than finetuned MLLMs, leading to more accurate identification of semantically and visually related images."}, {"title": "3.1 Preliminary", "content": "Interactive text-to-image retrieval is formulated as a multi-turn task that begins with an initial user-provided description, Do. The objective is to identify a target image through an iterative dialogue between the user and the retrieval framework. At each turn t, the retrieval framework generates a question Qt to clarify the search, and the user responds with an answer At. This interaction updates the dialogue context Ct = (Do, Q1, A1, ..., Qt, At), which is processed-such as by concatenating all textual elements-to form a unified search query St for that round. The retrieval framework then matches images I \u2208 I in the image database against St, ranking them based on a similarity score s(I, St). This process iterates until the target image I* is successfully retrieved or the maximum number of turns is reached. Formally, this process can be defined as:\n$I^* = \\arg \\max_{I \\in I} s (I, S_T)$\nwhere Sy is the final search query after T dialogue turns."}, {"title": "3.2 Dialog Context Aware Reformulation", "content": "While multi-turn interactions help capture user intent, raw dialogue data can introduce noise and complexity that degrades retrieval performance. Both encoders and diffusion models struggle with lengthy or ambiguous dialogue context, particularly because DMs have limited capacity for long or complex descriptions. Nevertheless, the quality of images generated by DMs is crucial for DAR to achieve superior performance. Moreover, discrepancies between the training distributions of encoders and DMs make it difficult to generate images that accurately reflect user intentions. To address these challenges, we propose two targeted approaches: refining the dialogue for textual representations used in retrieval; and optimizing the prompts for the DM generation process:\n(1) Dialogue Context Aware Reformulation: This pipeline adapts the dialogue context to better align with the input expectations of encoders and the user's intent. Instead of directly using the raw dialogue context Ct = {Do, Q1, A1, ..., Qt, At} as textual representations, we follow a multi-step process to refine the input.\n(a) Summarizing the Dialogue: We first ask an LLM to summarize the entire dialogue context Ct, providing a coherent and concise overview of the conversation up to turn t.\n(b) Structuring the Input: The summarized dialogue is then reformulated into a specific format that clearly distinguishes the initial query (Do) from the subsequent elaborations in the dialogue. This ensures that the LLM understands the progression of the conversation and the relevance of each turn.\n(c) Generating the Refined Query: Using this structured input, we prompt the LLM to generate a refined query St that adheres to the encoders' input distribution. This ensures that the generated query captures the user's intent in a way that facilitates accurate retrieval.\nThe reformulation process can be expressed as:\n$S_t = R_1 (C_t)$\nwhere R\u2081 denotes the reformulation function utilizing LLMs. An example prompt for R\u2081 is: \"The reconstructed [New Query] should be concise and in an appropriate format to retrieve a target image from a pool of candidate images.\" This approach allows the dialogue context to be transformed into a more structured and relevant form for the retrieval pipeline, optimizing the alignment between user intent and the model's output.\n(2) Diffusion Prompt Reformulation: This pipeline generates multiple prompts Pt,k for use by the subsequent diffusion models based on the reformulated dialogue St. By producing diverse prompts, we ensure that the generated images align with the diffusion model's training distribution, capturing various linguistic patterns and semantic nuances. The reformulation process follows these steps:\n(a) Structuring the Prompt Template: We begin by structuring a prompt template that captures key elements from the reformulated dialogue St, including the primary subject, setting, and important details. This structured format helps guide the diffusion model to generate images that are semantically coherent with the user's intent.\n(b) Generating Diverse Prompts: Using the structured template, we generate multiple distinct prompts Pt.k by varying linguistic patterns, modifiers, and details, ensuring a variety of interpretations that reflect the full scope of the dialogue. This diversity helps cover different possible details in the image generation process.\n(c) Adapting to the DM's Distribution: The generated prompts are further adjusted to align with the diffusion model's training distribution. This adaptation ensures that the prompts match the model's expectations and improve the relevance of the generated images.\nThe reformulation process is expressed as:\n$P_{t.k} = R_2(S_t, k) \\text{ for } k = 1, 2, . . ., K$\nwhere R2 denotes the prompt generation function, and K is the number of prompts generated per turn. An example template prompt for R2 is: \"[Adjective] [Primary Subject] in [Setting], [Key Details]. Style: photorealistic.\"\nBy generating diverse prompts, this approach ensures that the diffusion models produce images that act as multiple intermediate representations of the user's information need, which are both semantically rich and better aligned with the user's query."}, {"title": "3.3 Diffusion Augmented Multi-Faceted Generation", "content": "Following the reformulation step, we obtain a refined textual dialog for retrieval (Section 3.2) and multiple prompts that capture different aspects of the user's intent. We now proceed to the imagine step, where these prompts are used to generate images that augment the retrieval process. We term this approach Diffusion Augmented Retrieval (DAR).\nThe core idea of DAR involves utilizing DMs to generate synthetic images that serve as multiple intermediate representations of the user's information needs, thereby enhancing the retrieval process. DMs excel at producing high-quality, visually realistic images from textual descriptions, enabling them to closely align with the user's intent. By leveraging the prior visual knowledge embedded in DMs through image generation, DAR establishes many-to-one mappings between queries and target images instead of one-to-one mappings. This multi-faceted representation of queries addresses the challenges posed by incomplete or ambiguous textual inputs, offering a richer and more diverse set of visual representations for retrieval. Consequently, DAR achieves robust zero-shot performance, particularly in I-TIR scenarios where labeled data is scarce.\nSpecifically, for text-guided diffusion generation, the reverse process is conditioned on a text embedding to from the diffusion text encoder:\n$p_\\theta(x_{t-1} | x_t, t_d) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t,t_d), \\Sigma(t)),$\nwhere the function \\( \\mu_\\theta(x_t, t, t_d) \\) represents the learned denoiser, which predicts the mean of the posterior distribution for \\( x_{t-1} \\) given the noisy input \\( x_t \\), timestep t, and additional context \\( t_d \\).\nLet\n$\\lbrace P_{t,k}\\rbrace_{k=1}^{K}$\nbe the set of K diffusion-ready prompts produced by the reformulation process. We feed each prompt Pt.k into the diffusion model D(.) to generate a corresponding set of images:\n$\\lbrace \\hat{I}_{t,k}\\rbrace_{k=1}^{K} = \\lbrace D(P_{t,k})\\rbrace_{k=1}^{K}$\nEach generated image It,k reflects one possible interpretation of the user's intent based on the prompt Pt,k. By leveraging the large-scale pretraining of the diffusion model and generating multiple images per turn, DAR captures diverse visual features relevant to the query."}, {"title": "3.4 Retrieval Process", "content": "3.4.1 DAR Encoding and Feature Fusion. In the DAR framework, an MLLM is employed to encode both the reformulated textual dialogue St and the generated images {It,k}. Although MLLMs may be based on unified or two-tower architectures for handling text and images, their exact design does not affect the overall flow of DAR, as we only utilize them as encoders. Consequently, we do not delve into internal implementation details. Instead, we focus on four key steps: dialog encoding, generated image encoding, candidate image encoding, and feature fusion.\n(1) Dialog Encoding. Let Et (.) denote the text encoder of the MLLM for dialogue inputs. Given the reformulated textual dialogue St at turn t, we obtain its embedding as:\n$\\mathbf{t_t}=E_t(S_t)$,\nwhere tt \u2208 Rd is the resulting textual embedding and d is the embedding dimensionality.\n(2) Generated Image Encoding. Let Ev() denote the image encoder of the MLLM. For each generated image \u00cetk, where k = 1, 2, ..., K, its embedding is:\n$\\mathbf{i_{t,k}}=E_v(\\hat{I}_{t,k})$,\nso that {it,k} represents the set of embeddings corresponding to the K synthetic images generated at turn t.\n(3) Image Encoding for Candidate Images. Let I = {I1, I2, ..., IN} be the set of N candidate images in the database. We encode each candidate image Ij using the same image encoder Ev() that we used for generated images:\n$\\mathbf{i_j}=E(I_j), j = 1, 2, . . ., N.$"}, {"title": "4.  EXPERIMENTAL RESULTS", "content": "4.1 Experimental Settings\nWe evaluate our proposed DAR framework in interactive retrieval settings using four well-established benchmarks. Specifically, we employ the validation set of Visual Dialog (VisDial) [3] dataset and three dialog datasets\u00b9 constructed by [11], named ChatGPT_BLIP2, HUMAN_BLIP2, and Flan-Alpaca-XXL_BLIP2. For the latter three dialog datasets, the first part of each name indicates the questioner model used to generate the dataset, and the second part refers to the answer model. Note that human refers to professional human assessors. Further details on these three datasets can be found in [11]. All four dialog datasets consist of 2,064 dialogues, each with 10 dialogue turns.\nFollowing previous work in the interactive cross-modal retrieval domain, we use BLIP [12] as our default MLLM encoder, given its established zero-shot performance and to ensure a fair comparison with prior studies. Unless otherwise specified, we report Hits@10 as our primary evaluation metric.\nWe adopt the Stable Diffusion 3 model (SD3) [4] as the default diffusion model (DM) in our experiments. Additionally, BLIP-3 [28] is employed as the Large Language Model (LLM) for reformulating textual content. Using specially designed prompts, BLIP-3 operates under two distinct reformulation pipelines: one for adapting the dialogue and another for producing aligned prompts for the DM, as described in Section 3.2. We empirically set the weighting factors for textual and visual content to 0.7 and 0.3, respectively, for the first two dialogue turns. Starting from turn 3, the weights are both set to 0.5. This strategy balances the influence of textual vs. visual embeddings as the dialogue becomes more dynamic. In all experiments, we fix the number of generated images per turn at three. Code for all experiments is available at https://anonymous.4open.science/r/Diffusion-Augmented-Retrieval-7EF1/README.md.\nBaselines and DAR variants. We compare our proposed DAR framework with three baselines, namely ChatIR, ZS, and COCOFT:\n\u2022 ChatIR: We adopt the BLIP-based variant of ChatIR [11] as our baseline. Since it is finetuned on the Visual Dialog dataset, ChatIR represents a finetuned model that helps us compare both the effectiveness and efficiency benefits of DAR.\n\u2022 ZS: The zero-shot (ZS) baseline uses BLIP with its original, publicly available pretrained weights 2, without any fine-tuning on retrieval datasets. This setup captures the common scenario where researchers or practitioners directly rely on publicly released weights without task-specific training.\n\u2022 COCOFT: The COCOFT baseline denotes the BLIP model finetuned on the popular MSCOCO [14] retrieval dataset. Although it benefits from MSCOCO-specific training, it remains non-finetuned for interactive text-to-image retrieval (I-TIR). Consequently, dialogues including questions and answers are applied directly as queries for retrieval. This serves as an indicator of how previously finetuned single-turn retrieval models perform in an interactive retrieval setting."}, {"title": "4.2 Zero-Shot I-TIR Performance", "content": "We first investigate how our proposed DAR framework performs under zero-shot I-TIR conditions. Specifically, BLIP_zs denotes the baseline where BLIP is used with its original, publicly available pretrained weights-without any fine-tuning on retrieval datasets. Our DAR_zs setup similarly employs pretrianed BLIP as the encoder, thereby reflecting a common scenario in which researchers or practitioners rely solely on publicly released weights.\nEach subfigure (a-d) in Figures 3 presents the performance of DAR variants and baseline models across the four benchmarks outlined in Section 4.1, evaluated over multiple conversational turns.\nIn this section, we compare the dashed blue lines (baseline BLIP_zs) with the solid blue line (DAR_zs) across the four subfigures (a-d) in Figures 3. We can tell that DAR_zs consistently outperforms BLIP_zs across all four evaluated benchmarks. Notably, the FLAN_BLIP2 dataset exhibits the largest improvement, with DAR_zs achieving a 7.61% increase in Hits@10 after 10 dialog rounds. Indeed, the performance gap between DAR_zs and BLIP_zs consistently widens as the dialogue extends over multiple turns.\nThese findings demonstrate that DAR effectively boosts zeroshot performance in interactive retrieval tasks where our \"query\" is derived from a complex and diverse dialog, and it can do this without incurring additional tuning overhead."}, {"title": "4.3 Robustness to Complex and Diverse Dialogue Queries", "content": "Since our proposed DAR framework is aimed at tackling the challenges of complex and diverse dialogue queries in I-TIR, evaluating its robustness under such conditions is important. Therefore, we focus our analysis on the most challenging benchmark among the four we evaluated-specifically the one where the best-performing"}, {"title": "4.4 Gains and losses of DAR compared to finetuned models", "content": "In this section, we investigate both the effectiveness and efficiency of the zero-shot, non-finetuned version of our proposed DAR framework, DAR_zs, by comparing it against the finetuned ChatIR baseline. Thus, we compare the solid blue line (DAR_zs) with the dash green line (ChatIR) across the four subfigures (a-d) in Figures 3. Overall, DAR_zs demonstrates competitive performance on all evaluated benchmarks.\nAs analyzed in Section 4.3, FLAN_BLIP2 is the most challenging dataset in our experiments, owing to its particularly complex and dynamic interactive dialogues. Notably, DAR_zs (solid blue line) outperforms the finetuned ChatIR (dash green line) model by 4.22% in Hits@10 at turn 10 (Figure 3.d), supporting our hypothesis that fine-tuning MLLMs on limited I-TIR datasets can undermine their ability to handle out-of-distribution queries. Finetuned models like ChatIR perform best on the VisDial dataset (see Figure 3.a), given their extensive fine-tuning on 123k VisDial training samples. Even in this favorable setting, the worst-case of non-finetuned DAR model, DAR_zs, lags behind ChatIR by only 3.01% in Hits@10, all while saving 100% of the fine-tuning time.\nThis suggests that ChatIR's narrower finetuned distribution makes it more prone to out-of-distribution errors during inference, leading to underperformance. In contrast, DAR excels in these failure scenarios and remains competitive even on ChatIR's own finetuned dataset."}, {"title": "4.5 Compatibility with finetuned Models", "content": "Our proposed DAR can also be combined with various encoders, including those already finetuned for dialogue-based text. In this context, the main comparison is between DAR_chatir and ChatIR, where DAR_chatir employs the finetuned ChatIR model as its encoder. Thus, we compare the solid green line (DAR_chatir) with the dash green line (ChatIR) across the four subfigures (a-d) in Figures 3."}, {"title": "4.6 Does Fine-Tuning on Single-Turn Retrieval Datasets Improve Performance?", "content": "Given that single-turn and interactive text-to-image retrieval share some similarities at turn 0, this section investigates whether finetuning on a single-turn retrieval dataset-specifically, the widely used MSCOCO dataset [14]-enhances performance in interactive retrieval tasks.\nAs shown in all four subfigures of Figure 3, the benefit of fine-tuning on a single-turn retrieval dataset is relatively limited. While finetuned models such as BLIP_cocoft (dash red lines) and DAR_cocoft (solid red lines) exhibit noticeable performance improvements at earlier turns compared to their zero-shot counterparts (BLIP_zs and DAR_zs), this advantage diminishes as the dialogue progresses. By turn 8, the zero-shot model DAR_zs even surpasses the COCO finetuned model DAR_cocoft in performance, as illustrated in Figure 3.d.\nThese findings show that conventional finetuning strategies on single-turn datasets are insufficient to address the increasing query diversity and complexity inherent in interactive text-to-image retrieval. This underscores the limitations of fine-tuning-based approaches in handling evolving multi-turn interactions. In contrast, our proposed DAR framework effectively bridges this research gap by enhancing zero-shot performance without requiring additional fine-tuning, making it a more scalable and adaptive solution for real-world retrieval scenarios."}, {"title": "5 ANALYSIS", "content": "5.1 Qualitative Analysis of DAR-Generated Images\nBeyond the numerical results presented in Section 4, we conduct an in-depth analysis of the images generated by DAR and uncover several key insights. As illustrated in Figure 4, the initial generated images tend to lack fine details and are easily distinguishable as synthetic. For instance, as the dialogue progresses and additional contextual information is incorporated, the generated images become increasingly photorealistic, with details aligning more closely with the target image-such as the accurate color of the player's clothing. A similar trend is observed in the second-row example,"}, {"title": "5.2 Compatibility of DAR with Different Encoders and Generators", "content": "To assess the compatibility of DAR with different MLLMs as encoders and DMs as visual generators, we evaluate its performance using two widely adopted MLLMs-CLIP [16] and BEiT-3 [24]-as encoders, as well as Stable Diffusion v2-1 [18] as an alternative visual generator.\nThe CLIP model, constrained by its maximum input length of 77 tokens, struggles to handle complex and lengthy dialogue-based queries compared to simpler caption-based queries in single-turn cross-modal retrieval tasks. Consequently, CLIP achieves a peak performance of 56.64% Hits@10 on the Visual Dialog benchmark. Despite this limitation, incorporating CLIP as the encoder within DAR still yields an improvement of 5.31% Hits@10 after 10 dialogue turns, demonstrating the robustness of our framework even with models that have constrained input capacity.\nBEIT-3, a stronger encoder than CLIP and BLIP models, further enhances performance. When employed as the encoder within DAR, the framework achieves a 6.28% improvement in Hits@10 after 10 turns on the Visual Dialog benchmark, compared to the standalone pre-trained BEiT-3 model, highlighting the adaptability of DAR in leveraging stronger backbone encoders.\nIn addition to evaluating different encoders, we also assess the impact of using Stable Diffusion v2-1 as the visual generator. While DAR continues to yield notable improvements, the performance gain is slightly lower compared to using Stable Diffusion 3 (SD3), aligning with SD3's superior generative capabilities. Specifically, DAR achieves a 6.37% Hits@10 with Stable Diffusion v2-1, compared to 7.61% Hits@10 with SD3."}, {"title": "5.3 Impact of the Number of Generated Images on Performance", "content": "In our evaluation, even when generating only a single image within our framework, we observe a 6.43% improvement in Hits@10 on the FLAN_BLIP2 benchmark compared to the ChatIR baseline. When increasing the number of generated images to three, performance further improves to 7.61%. However, beyond this point, we observe diminishing returns, with Hits@10 reaching saturation as the number of generated images continues to increase.\nConsidering the trade-off between effectiveness and computational efficiency, we set three generated images as the default configuration in DAR to achieve an balance between performance and inference cost."}, {"title": "5.4 Generation Overhead", "content": "While DAR eliminates the need for expensive fine-tuning, it introduces additional inference-time overhead due to the reformulation and generation of visual content. However, this trade-off is small compared with the benefits in retrieval performance and adaptability. In our experiments using a single Nvidia RTX 4090 GPU, the image generation process takes approximately 5 seconds, while the query reformulation step incurs only 0.5 seconds of additional processing time.\nTo put this into perspective, ChatGPT-01 and other 'reasoning' LLMs use additional reasoning steps, which enhances response quality at the cost of increased inference time (often exceeding 10 seconds). Analogously, DAR achieves substantial gains in zero-shot retrieval while entirely eliminating training costs, making it an efficient and scalable alternative. Consequently, the modest inference overhead is a worthwhile trade-off, particularly in applications where adaptability and retrieval quality are crucial."}, {"title": "6 CONCLUSION", "content": "In this work, we introduce DAR, a novel framework that eliminates the need for fine-tuning multimodal large language models (MLLMs) for Interactive Text-to-Image Retrieval (I-TIR), thereby preserving their generalizability. Extensive experiments show that DAR performs competitively with existing I-TIR models, whether they are fine-tuned or pretrained. Our analysis reveals that finetuning MLLMs on limited retrieval datasets compromises their ability to handle out-of-distribution queries. Furthermore, our results illustrate that generating multiple intermediate, multi-faceted representations of user intent enables a many-to-one mapping between text and images, effectively accommodating the diverse and complex queries characteristic of I-TIR. This work highlights the potential of diffusion-augmented retrieval and suggests avenues for future exploration in optimizing efficiency, supporting multimodal queries, and extending to real-world applications."}]}