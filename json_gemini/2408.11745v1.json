{"title": "FocusLLM: Scaling LLM's Context by Parallel Decoding", "authors": ["Zhenyu Li", "Yike Zhang", "Tengyu Pan", "Yutao Sun", "Zhichao Duan", "Junjie Fang", "Rong Han", "Zixuan Wang", "Jianyong Wang"], "abstract": "Empowering LLMs with the ability to utilize useful information from a long context is crucial for many downstream applications. However, achieving long context lengths with the conventional transformer architecture requires substantial training and inference resources. In this paper, we present FocusLLM, a framework designed to extend the context length of any decoder-only LLM, enabling the model to focus on relevant information from very long sequences. FocusLLM processes long text inputs by dividing them into chunks based on the model's original context length to alleviate the issue of attention distraction. Then, it appends the local context to each chunk as a prompt to extract essential information from each chunk based on a novel parallel decoding mechanism, and ultimately integrates the extracted information into the local context. FocusLLM stands out for great training efficiency and versatility: trained with an 8K input length with much less training cost than previous methods, FocusLLM exhibits superior performance across downstream long-context tasks and maintains strong language modeling ability when handling extensive long texts, even up to 400K tokens.", "sections": [{"title": "1 Introduction", "content": "The importance of extending the context length of large language models (LLMs) cannot be overstated. In numerous applications, ranging from complex document analysis to generating coherent long-form text, the ability to effectively utilize extended context is critical. For instance, in tasks such as document summarization and question answering over lengthy articles, a more extensive context allows for a more comprehensive understanding and accurate responses. However, leveraging long contexts in LLMs presents several formidable challenges. (1) The computational complexity of transformers grows quadratically with the sequence length, rendering the training process prohibitively expensive. (2) LLMs exhibit poor extrapolation performance for longer sequences, even after additional fine-tuning. (3) Acquiring high-quality long-text datasets, which are essential for training and fine-tuning, is exceedingly difficult.\nTo circumvent the substantial costs of directly scaling the window length by fine-tuning on longer inputs, many approaches have attempted to modify the attention mechanism or compress tokens to theoretically achieve infinite length. While these methods can maintain lower perplexity over extended texts, the loss of information from earlier parts of the text hampers the model's ability to perform precise understanding tasks such as information verification or question answering."}, {"title": "2 Methodology", "content": "In this section, we introduce the design methodology of FocusLLM. First, we describe how we construct FocusLLM based on the architecture of LLM to enable it to handle extremely long text contexts. Then, we explain the training process of FocusLLM."}, {"title": "2.1 Architecture", "content": "As illustrated in Figure 2, the standard model architecture has a quadratic complexity and a corresponding limited context length. This limitation restricts the model's application to longer texts, and FocusLLM is designed to address these drawbacks.\nThe overall framework of FocusLLM is simple and intuitive. Each decoder in Figure 3 shares the same model (e.g. LLaMA-2). Besides, for the decoder handling each chunk, we augment the original decoder with a small set of additional parameters."}, {"title": "2.1.1 Notations.", "content": "Given a long sequence with S tokens $\\{x_1,...,x_S\\}$, we segment them into memory tokens $\\{x_1,...,x_m\\}$ and local tokens $\\{x_{m+1},...,x_S\\}$, with the length of local tokens not exceeding the model's default context length, denoted as $L$. Concurrently, we divide the memory into chunks, labeled as $C_1, C_2, ..., C_k$, with each chunk's size also not exceeding $L$. These chunks can represent distinct documents or constitute a single long document.\nWe define the original decoder model as $F_{dec}$ and its hidden dimension $d_{dec}$. When processing the memory, to endow the model with the capability to generate candidate tokens, we introduce a small set of new parameters, resulting in the modified model $F'_{dec}$. The candidate token is denoted as the trainable hidden states corresponding to the last local token $x_S$ in each chunk, serving as a signal indicating whether this chunk contains information relevant to the local context, as well as facilitating the prediction of the next token based on the current chunk."}, {"title": "2.1.2 Local Context Injection.", "content": "The aim of our FocusLLM is to distribute the burden of understanding long texts across each chunk. Therefore, unlike previous approaches that encode memory tokens and save them as cache, we append a small fragment of local tokens (we refer to it as the 'prompt' in the figure) behind each chunk and perform parallel decoding within each chunk, as shown in Figure 3. This strategy allows for a more efficient handling of long sequences by focusing the computational effort on relevant segments of the text, while maintaining the global information necessary for accurate decoding. We can formally define this process as follows:\n$\\hat{C_i} \\leftarrow \\{C_i; x_{m+j}, ..., x_S \\} \\quad i=1, ..., k; 1 \\leq j \\leq S-m $\nHere j is a hyperparameter that determines the number of local tokens appended to each chunk. To reduce computational overhead, there is no need to append all local tokens. For instance, if the local context length is 2K, we could concatenate only 512 tokens or just a short instruction. In experiments, we adopt a default length of 512 tokens for inference, which is sufficient to encapsulate the necessary local contextual information."}, {"title": "2.1.3 Parallel Decoding.", "content": "FocusLLM transforms the next token prediction process based on long sequences into the simultaneous generation of candidate tokens from different chunks, followed by aggregating these candidate tokens as memory to produce the final token. We refer to this process as parallel decoding.\nDifferences from previous encoding methods. Here, we will elucidate the meaning of the term 'decoding' and explain the logical differences from previous encoding methods. Many previous methods involve encoding the context into some form of memory and retaining the key-value (KV) cache for subsequent use. For instance, CEPE divides long texts into chunks and then encodes each chunk with a smaller language model. FocusLLM also divides long texts into chunks, but within each chunk, FocusLLM undergoes a decoding process.\nSpecifically, the purpose of encoding is to transform a segment of text into hidden states for later use, whereas decoding is about generating the next token based on the current text. From this perspective, our method can be viewed as a form of implicit decoding. This is because our objective is not to acquire representations for entire chunks of text but rather to maintain information about the next token (based on the current chunk $\\hat{C_i}$) via a special candidate token. Although we do not explicitly map the hidden states of the special candidate tokens through a softmax layer to the corresponding tokens in the vocabulary, our approach is fundamentally aligned with the concept of decoding. For further discussion, please refer to Appendix A.\nHow to obtain the representations of candidate tokens. Inspired by , in order to preserve the generalizability of the original model as much as possible, we only add a new set of trainable parameters to the linear projection matrices of each layer:\n$\\mathcal{Q} = \\{W_q^l, W_k^l, W_v^l, W_o^l\\}$ \nwhere $W_q^l, W_k^l, W_v^l,$ and $W_o^l$ represent the new parameters for the query, key, value, and output matrices, respectively, and $l$ denotes the layer number. The modified decoder model are denoted as $F'_{dec}$. We apply parallel decoding on each chunk and obtain one candidate token from every chunk:\n$e_i = F'_{dec}(\\hat{C_i})$\nwhere $e_i$ consists of key-value hidden states $K_e$ and $V_e$ of the last token in each layer. More formally, the output of the candidate token in the self-attention module is calculated similarly to:\n$Q_eHW \\leftarrow HW_q^l \\quad K_e \\leftarrow HW_k^l\\quad V_e \\leftarrow HW_v^l$\n$A \\leftarrow \\text{softmax} (Q_e (K + K_e)^T)$\n$O_e \\leftarrow V_eW_o^l \\quad V \\leftarrow A (V \\oplus V_e^T)$\nwhere $H \\in \\mathbb{R}^{d_{dec}}$ is the input hidden states of the candidate token, $\\oplus$ represents the concatenation of matrices, and $K, V$ correspond to the representations of the normal tokens in one chunk. Finally, the generated candidate tokens are concatenated with the local tokens and are subsequently processed by a frozen decoder. To optimize the candidate tokens, we design two loss functions of language modeling in Section 2.3. Note that the process of obtaining the candidate token from each chunk is independent, which allows us to perform parallel forwarding for each chunk. This is the reason we refer to this process as 'parallel decoding'."}, {"title": "2.1.4 Efficiency.", "content": "The parallel decoding mechanism of FocusLLM effectively reduces the computational complexity of the standard architecture. Specifically, when dealing with very long sequences, the primary computational burden in the transformer architecture lies in the attention mechanism, which has a complexity of $O(L^2)$, where $L$ represents the total sequence length. By dividing the sequence into n chunks, the complexity within each chunk becomes $O((L/n)^2)$. Therefore, when we process chunks in parallel, the time complexity can be reduced to $O((L/n)^2)$. And the space complexity of n chunks becomes approximately $O((L/n)^2 * n) = O(L^2/n)$. This means that compared to a standard transformer, FocusLLM can reduce the computational complexity to a fraction, $1/n$ or even more of the original theoretically, where n is the number of chunks into which the sequence is divided. In experiments, the longer the sequence length, the more apparent the improvement in efficiency."}, {"title": "2.2 Data", "content": "To ensure the generalizability of our method and to maintain fairness in comparison with the baselines, we leverage the same training corpus as Activation Beacon, with sequence lengths varying between 3K and 8K tokens. Specifically, we utilized 70K samples from the pre-training dataset RedPajama and 10K samples from the instruction-following dataset LongAlpaca. RedPajama is an open-source pre-training dataset for LLaMA-1, while LongAlpaca is an instruction-following dataset for question answering. Both datasets are widely utilized in previous work."}, {"title": "2.3 Training", "content": "We primarily conduct experiments on the LLaMA2-7B-Chat model. After incorporating the parameters mentioned in Section 2.1, the additional parameters amount to only 2B approximately. We train the model using an auto-regressive approach:\nAuto-Regressive Loss. FocusLLM is trained using a natural auto-regressive method. Specifically, we train the model to predict the next token, which encourages the candidate token to aggregate useful information from each chunk. Therefore, the loss is only applied to the local tokens:\n$\\min_{F'_{dec}} \\sum_{i=2}^{L} \\text{log}(p(x_i | e_1,..., e_k, x_1,..., x_{i-1}; F'_{dec}))$ \nIn Section 2.1.1, we noted that local tokens are positioned subsequent to the memory tokens. However, to explicitly train the model to harness its ability to recover information from the memory tokens, we also set the local tokens to be a substring of the memory tokens. Specifically, based on the different selection methods for local tokens, we design two types of loss functions for joint training. i) Firstly, if the last L tokens from a long document are selected as local tokens, with the remainder serving as memory tokens, we term this loss the 'Continuation loss', as it trains the model's ability to naturally continue generating new tokens based on the context. ii) Alternatively, if we take the entire long document as memory and then randomly select L continuous tokens from it as local tokens, we define this loss the 'Repetition loss', because it trains the model's ability to repeat when clear information from the context is already available. Subsequent experiments have proven that both types of loss are important.\nGeneralizing Chunk Size. To ensure the model exhibits robust generalizability across various chunk sizes and number of candidate tokens, we maintain a constant local context size of 2048, while the chunk size is randomly selected from the set {64, 128, 256, 1024, 2048} during training."}, {"title": "3 Experiments", "content": "In this section, we will conduct a comprehensive evaluation of the effectiveness of FocusLLM, spanning both language modeling and a variety of downstream tasks."}, {"title": "3.1 Experimental Details", "content": "We aligned most of our experimental settings with those of Activation Beacon to ensure comparable results. Specifically, we conducted training on a Linux server equipped with 8\u00d7A100 GPUs, each with 40GB of memory. The training was carried out for 10,000 steps, equivalent to one epoch of the entire training dataset, using a batch size of 8 and a learning rate of 5e-5 with a linear scheduler. To conserve GPU memory, we employed deepspeed's zero2_offload optimizing stage. The training process was completed in approximately 20 hours.\nFor hyper-parameters, during training, the chunk size was randomly selected from the set {64, 128, 256, 1024, 2048}. For the length of tokens injected into each chunk, we set a default of 512 tokens for inference. And we ensured this length did not exceed the chunk size in the training procedure. As a result, the length of injected tokens was min{512, chunk size}."}, {"title": "3.2 Long-context Language Modeling", "content": "In this section, we evaluate the fundamental capabilities of FocusLLM on long-context language modeling benchmarks, with text lengths ranging from 4K to 128K tokens.\nDatasets. We perform the evaluation on three datasets: PG19 , Proof-Pile , and CodeParrot . These three datasets encompass 100 long test cases related to books, arXiv papers, and code repositories, respectively. The results of baseline models are token from for comparison. Following the setting of , as FocusLLM relies on the last decoder to perform generation, we calculate the perplexity on the last 256 tokens of each sequence, and for the 128K length, we filter out documents exceeding 128K tokens and evaluate 10 samples due to data scarcity and computational cost.\nModel. FocusLLM is based on LLaMA-2-7B (chat), hence the models for comparison are all on the same scale, 7B. The baseline models can be categorized into the following types: i) Methods focusing on the modification of positional encoding, including Positional Interpolation , the NTK-Aware Scale ROPE, and the training-free method StreamingLLM"}, {"title": "3.3 Downstream Tasks", "content": "Datasets. To assess the capabilities of FocusLLM in real-world scenarios, we select two widely used datasets: Longbench and \u221e-Bench. Longbench offers an evaluation on a variety of tasks including question answering, summarization, few-shot learning, mathematical counting, and code completion. The average and 95% percentile length are 12.8K and 31K tokens, respectively. \u221e-Bench is designed to test a model's ability to understand and reason over super long contexts. The average length and the 95% percentile length are 145.1K and 214K tokens. For more detailed dataset statistics, please refer to Appendix B. We believe that these two benchmarks can comprehensively reflect the capabilities of the model on downstream tasks. For evaluations on the Longbench, we adopt a larger local context size of 3,500 tokens for FocusLLM, consistent with the official setting.\nModels. The purpose of our paper is to enable LLMs with limited context length to understand extremely long sequences at a very minimal cost. Therefore, in addition to comparing FocusLLM with the three types of baselines mentioned in Section 3.2, we mainly focused on comparing FocusLLM with recent proposed models capable of processing extremely long streaming inputs. Specifically, StreamingLLM utilizes a sliding window mechanism; InfLLM stores processed context into memory units and retrieves it using attention scores; Activation Beacon compresses the preceding text to maintain a smaller context length. CEPE adopts a small encoder to process long inputs chunk by chunk and feeds the memory to a decoder by cross-attention."}, {"title": "4 Further Exploration", "content": "In this section, we conduct further explorations of FocusLLM. Initially, we investigate the upper limits of the sequence lengths that FocusLLM can effectively handle. Subsequently, we perform a quantitative analysis of several key parameters within the framework. Ultimately, we undertake ablation studies on the training loss functions to discern their impacts on the model's overall performance."}, {"title": "4.1 Scaling to 400K Context", "content": "We contend that FocusLLM is capable of processing extremely long sequences. To validate this, we first conduct experiments on the passkey retrieval task. The results, as illustrated in Figure 1, demonstrate that FocusLLM maintains nearly 100% effectiveness at lengths of up to 400K, outperforming all other models. In addition to this, we extended the language modeling experiments introduced in Section 3.2 to 400K, a length at which most models fail to manage effectively.\nTo conclude, FocusLLM's exceptional performance in both passkey retrieval and language modeling tasks, even at the substantial length of 400K tokens, indicates that it can achieve good performance in the vast majority of real-world scenarios, which underscores the effectiveness and versatility of FocusLLM."}, {"title": "4.2 Memory Footprint and Inference Time", "content": "For models that focus on long-form text, aside from training costs, another critical aspect is the memory footprint and inference time, especially as sequence lengths increase. In this section, we compare FocusLLM with several previous long-context methods capable of retaining global information by preserving the cache of all context: Standard (PI/NTK), LongLlama, and CEPE. As for models like Activation Beacon and StreamingLLM, although they maintain a constant memory footprint by only retaining cache for a fixed window, they struggle with the precise understanding of extremely long texts. Therefore, they are not the primary subjects of comparison.\nIn the Figure, \"FocusLLM with or without parallel\" signifies that we process each chunk either concurrently or sequentially. The findings indicate that: (1) Serial processing is good enough in terms of memory usage and inference time. When ample memory resources are available, parallel processing is more efficient. And in practice, the parallel level is also controllable, making our model robust and applicable under most scenarios.(2) Although FocusLLM splits long texts into numerous chunks, resulting in a slightly longer inference time compared to the standard approach, it holds a significant advantage over other long-context methods."}, {"title": "4.3 Chunk Size", "content": "We conduct an investigation into the impact of different chunk sizes on performance. In theory, larger chunk sizes, as long as they do not exceed the model's default context length (e.g., 4K for LLaMA-2), are preferable because they allow for processing the memory with a smaller number of forward passes. However, smaller chunk sizes may enable more precise processing.\nIn experiments, we maintain a total sequence length of 8K, testing the perplexity using different chunk sizes on the same samples of PG19. We select {256, 512, 1024, 2048} as our test sizes. We observe that there is no consistent trend in perplexity as the chunk size increases; it remains relatively stable. This confirms our hypothesis that we can employ larger chunk sizes on models with longer default context lengths (e.g. LLaMA-2-32K). We will explore this direction in our future work."}, {"title": "4.4 Local Context Size", "content": "While the parallel decoding mechanism theoretically allows the model to attend to all tokens, we aim to investigate whether the principle that a larger local context size equates to better performance holds true for FocusLLM as well. As we reduce the local context size from 3.5K to 1K, it can be observed that performance for the majority of tasks experiences a slight decline, with the exception of the passkey retrieval task, which has very clear and concise instructions. This indicates that a larger local context size indeed provides higher quality local information."}, {"title": "4.5 Ablation Studies", "content": "We employ both Continuation Loss and Repetition Loss for the training of FocusLLM. The motivation behind this is to equip the model with the natural language modeling capability while also enhancing its ability to recover global information. Ablation studies on both losses, as detailed in Table 5, reveal that relying solely on the Continuation Loss enables the model to manage some tasks effectively. Nonetheless, for tasks with substantial dependencies on the preceding context, like HotpotQA and Retrieve.PassKey, the model's efficacy deteriorates. Similarly, while employing the Repetition Loss ensures accurate restatement of the preceding context, the lack of generalizability of generating new tokens leads to a considerable decrease in performance. Therefore, the combined use of both loss functions is crucial for enhancing the performance and generalizability of FocusLLM."}, {"title": "5 Related Work", "content": "Long-context language models\nRecent advancements in long-context modeling have seen a surge in innovative approaches that aim to transcend the limitations of transformer architectures. One research direction involves length extrapolation in transformers, where methods like positional interpolation help models adapt to longer sequences . However, these techniques often fail to address the distraction issue caused by noisy content within extended texts . Another research branch focus on modifying the attention mechanism or employing compression techniques to maintain long texts within manageable lengths . For instance, discovered that retaining \u2018sink tokens' in conjunction with a sliding window can achieve smooth streaming output. expanded the context dramatically through compression. However, these methods share a common limitation: they cannot utilize information from all tokens."}, {"title": "5.2 Memory-enhanced Model", "content": "The integration of memory layers within transformer architectures has become a pivotal strategy for enhancing long-context comprehension. Common methodologies in memory-enhanced models often employ recurrent strategies that iteratively integrate information from the current window into a persistent memory ."}, {"title": "6 Conclusion", "content": "In this work, we introduced FocusLLM, a novel framework that significantly extends the context length of LLMs. The core innovation lies in the parallel decoding strategy, which distribute the burden of understanding long texts across each chunk and effectively aggregating global information. FocusLLM stands out due to its remarkable training efficiency, allowing us to achieve substantial gains in context comprehension with minimal computational and memory cost. Compared to existing methods, FocusLLM not only exhibits superior performance across downstream tasks but also maintains low perplexities when handling extensive texts, up to 400K tokens. We hope FocusLLM can be an inspiring work for the community, driving further exploration of long-context models."}, {"title": "A More discussion", "content": "In this section, we provide an in-depth explanation of the parallel decoding utilized by FocusLLM. As we mentioned in section 2.1.3, the processing of each chunk in FocusLLM can be considered a form of implicit decoding. This is because our goal is not to obtain representations for entire chunks of text, but rather to preserve information about the next token (based on the current chunk) through a special candidate token.\nFurthermore, we can observe that the overall structure of FocusLLM shares similarities with the recently proposed concept of the decoder-decoder model architecture , which differs from the traditional encoder-decoder model. Specifically, encoder-decoder models, like T5 , utilize a bidirectional encoder for input encoding and a unidirectional decoder for output generation. These models necessitate the re-encoding of the entire set of input and output tokens for each subsequent generation step, making them less suitable for autoregressive generation. In contrast, the decoder-decoder model (also FocusLLM), by caching the previously computed key/value vectors, enables the model to reuse them in the current generation step."}, {"title": "A.2 Loss Design", "content": "In the loss design of FocusLLM, in addition to using the commonly employed Continuation Loss, we designed Repetition Loss to explicitly train the model to maintain attention to the context. We believe that Repetition Loss is the reason why FocusLLM can achieve good results with a relatively small number of training tokens. Previous work has demonstrated that synthetic data can effectively enhance model capabilities or reduce costs. This has inspired us to design different types of synthetic data in the future to help FocusLLM quickly learn specific abilities."}, {"title": "B Details of Benchmarks", "content": "LongBench includes 14 English tasks, 5 Chinese tasks, and 2 code tasks, with the average length of most tasks ranging from 5K to 15K. In experiments, we only utilize the English tasks.\n comprises 12 unique tasks, each crafted to assess different aspects of language processing and comprehension in extended contexts. Detailed statistics of the tasks used in our paper are shown in Table 7."}, {"title": "C Details of the effective lengths of models in Table 3 and 4.", "content": "Not all models are capable of processing infinite text lengths. Therefore, we provide a clear explanation of the effective input length for each method in Table 3 and Table 4. Specifically: (i) For models with a finite context length, we truncate the inputs by only preserving the system prompts and the tail of inputs to simulate real-world applications with streaming inputs like . For instance, in Table 3, these models include Original (4K), LChat (32K), Vic-16K (16K), Yarn (128K), PI (128K), and NTK (128K). (ii) For other models, including StreamingLLM, InfLLM, LongLlama, CEPE, Activation Beacon, and our FocusLLM, the input can theoretically be of any length. So we input the entire sequence on the two benchmarks."}]}