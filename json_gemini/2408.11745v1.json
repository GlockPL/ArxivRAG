{"title": "FocusLLM: Scaling LLM's Context by Parallel Decoding", "authors": ["Zhenyu Li", "Yike Zhang", "Tengyu Pan", "Yutao Sun", "Zhichao Duan", "Junjie Fang", "Rong Han", "Zixuan Wang", "Jianyong Wang"], "abstract": "Empowering LLMs with the ability to utilize useful information from a long context is crucial for many downstream applications. However, achieving long context lengths with the conventional transformer architecture requires substantial training and inference resources. In this paper, we present FocusLLM, a framework designed to extend the context length of any decoder-only LLM, enabling the model to focus on relevant information from very long sequences. FocusLLM processes long text inputs by dividing them into chunks based on the model's original context length to alleviate the issue of attention distraction. Then, it appends the local context to each chunk as a prompt to extract essential information from each chunk based on a novel parallel decoding mechanism, and ultimately integrates the extracted information into the local context. FocusLLM stands out for great training efficiency and versatility: trained with an 8K input length with much less training cost than previous methods, FocusLLM exhibits superior performance across downstream long-context tasks and maintains strong language modeling ability when handling extensive long texts, even up to 400K tokens. Our code is available at https://github.com/leezythu/FocusLLM.", "sections": [{"title": "1 Introduction", "content": "The importance of extending the context length of large language models (LLMs) cannot be overstated. In numerous applications, ranging from complex document analysis to generating coherent long-form text, the ability to effectively utilize extended context is critical. For instance, in tasks such as document summarization and question answering over lengthy articles, a more extensive context allows for a more comprehensive understanding and accurate responses (Li et al., 2024a). However, leveraging long contexts in LLMs presents several formidable challenges. (1) The computational complexity of transformers (Vaswani et al., 2017) grows quadratically with the sequence length, rendering the training process prohibitively expensive. (2) LLMs exhibit poor extrapolation performance for longer sequences, even after additional fine-tuning (Chen et al., 2023a; Peng et al., 2023). (3) Acquiring high-quality long-text datasets, which are essential for training and fine-tuning, is exceedingly difficult (Xiong et al., 2023; Wang et al., 2022).\nTo circumvent the substantial costs of directly scaling the window length by fine-tuning on longer inputs, many approaches have attempted to modify the attention mechanism (Xiao et al., 2023; Han et al., 2023) or compress tokens (Zhang et al., 2024a; Chevalier et al., 2023; Ge et al., 2023) to theoretically achieve infinite length. While these methods can maintain lower perplexity over extended texts, the loss of information from earlier parts of the text hampers the model's ability to perform precise understanding tasks such as information verification or question answering (Zhang"}, {"title": "2 Methodology", "content": "In this section, we introduce the design methodology of FocusLLM. First, we describe how we construct FocusLLM based on the architecture of LLM to enable it to handle extremely long text contexts. Then, we explain the training process of FocusLLM."}, {"title": "2.1 Architecture", "content": "As illustrated in Figure 2, the standard model architecture has a quadratic complexity and a corresponding limited context length. This limitation restricts the model's application to longer texts, and FocusLLM is designed to address these drawbacks.\nThe overall framework of FocusLLM is simple and intuitive. Each decoder in Figure 3 shares the same model (e.g. LLaMA-2). Besides, for the decoder handling each chunk, we augment the original decoder with a small set of additional parameters."}, {"title": "2.1.1 Notations.", "content": "Given a long sequence with S tokens {x1,...,xs}, we segment them into memory tokens {x1,...,xm} and local tokens {xm+1,...,XS}, with the length of local tokens not exceeding the model's default context length, denoted as L. Concurrently, we divide the memory into chunks, labeled as C1, C2, ..., Ck, with each chunk's size also not exceeding L. These chunks can represent distinct documents or constitute a single long document.\nWe define the original decoder model as Fdec and its hidden dimension ddec. When processing the memory, to endow the model with the capability to generate candidate tokens, we introduce a small set of new parameters, resulting in the modified model Flec. The candidate token is denoted as the trainable hidden states corresponding to the last local token xs in each chunk, serving as a signal indicating whether this chunk contains information relevant to the local context, as well as facilitating the prediction of the next token based on the current chunk."}, {"title": "2.1.2 Local Context Injection.", "content": "The aim of our FocusLLM is to distribute the burden of understanding long texts across each chunk. Therefore, unlike previous approaches that encode memory tokens and save them as cache, we append a small fragment of local tokens (we refer to it as the 'prompt' in the figure) behind each chunk and perform parallel decoding within each chunk, as shown in Figure 3. This strategy allows for a more efficient handling of long sequences by focusing the computational effort on relevant segments of the text, while maintaining the global information necessary for accurate decoding. We can formally define this process as follows:\n\u0108i \u2190 {Ci; xm+j, ..., xs } i = 1, ..., k; 1 \u2264 j \u2264 S\u2212m (1)\nHere j is a hyperparameter that determines the number of local tokens appended to each chunk. To reduce computational overhead, there is no need to append all local tokens. For instance, if the local context length is 2K, we could concatenate only 512 tokens or just a short instruction. In experiments, we adopt a default length of 512 tokens for inference, which is sufficient to encapsulate the necessary local contextual information."}, {"title": "2.1.3 Parallel Decoding.", "content": "FocusLLM transforms the next token prediction process based on long sequences into the simultaneous generation of candidate tokens from different chunks, followed by aggregating these candidate tokens as memory to produce the final token. We refer to this process as parallel decoding.\nDifferences from previous encoding methods. Here, we will elucidate the meaning of the term 'decoding' and explain the logical differences from previous encoding methods. Many previous methods involve encoding the context into some form of memory and retaining the key-value (KV) cache for subsequent use (Yen et al., 2024; Tworkowski et al., 2024; Zhang et al., 2024a). For instance, CEPE (Yen et al., 2024) divides long texts into chunks and then encodes each chunk with a smaller language model. FocusLLM also divides long texts"}, {"title": "2.1.4 Efficiency.", "content": "The parallel decoding mechanism of FocusLLM effectively reduces the computational complexity of the standard architecture. Specifically, when dealing with very long sequences, the primary computational burden in the transformer architecture lies in the attention mechanism, which has a complexity of O(L2), where L represents the total sequence length. By dividing the sequence into n chunks, the complexity within each chunk becomes O((L/n)\u00b2). Therefore, when we process chunks in parallel, the time complexity can be reduced to O((L/n)2). And the space complexity of n chunks becomes approximately O((L2/n) * n) = O(L2/n). This means that compared to a standard transformer, FocusLLM can reduce the computational complexity to a fraction, 1/n or even more of the original theoretically, where n is the number of chunks into which the sequence is divided. In experiments, the longer the sequence length, the more apparent the improvement in efficiency."}, {"title": "2.2 Data", "content": "To ensure the generalizability of our method and to maintain fairness in comparison with the baselines, we leverage the same training corpus as Activation Beacon (Zhang et al., 2024a), with sequence lengths varying between 3K and 8K tokens. Specifically, we utilized 70K samples from the pre-training dataset RedPajama (Together, 2023b) and 10K samples from the instruction-following dataset LongAlpaca (Chen et al., 2023b). RedPajama is an open-source pre-training dataset for LLaMA-1 (Touvron et al., 2023a), while LongAlpaca is an instruction-following dataset for question answering. Both datasets are widely utilized in previous work. The detailed statistics of our training data are reported in Table 1."}, {"title": "2.3 Training", "content": "We primarily conduct experiments on the LLaMA2-7B-Chat model. After incorporating the parameters mentioned in Section 2.1, the additional parameters amount to only 2B approximately. We train the model using an auto-regressive approach:\nAuto-Regressive Loss. FocusLLM is trained using a natural auto-regressive method. Specifically, we train the model to predict the next token, which encourages the candidate token to aggregate useful information from each chunk. Therefore, the loss is only applied to the local tokens:\nminF'dec L\u2211i=2 log(p(xi | e1,..., ek, X1,..., Xi\u22121; Fdec)) (7)\nIn Section 2.1.1, we noted that local tokens are positioned subsequent to the memory tokens. However, to explicitly train the model to harness its ability to recover information from the memory tokens, we also set the local tokens to be a substring of the memory tokens. Specifically, based on the different selection methods for local tokens, we design two types of loss functions for joint training. i) Firstly, if the last L tokens from a long document are selected as local tokens, with the remainder serving as memory tokens, we term this loss the 'Continuation loss', as it trains the model's ability to naturally continue generating new tokens based on the context. ii) Alternatively, if we take the entire long document as memory and then randomly select L continuous tokens from it as local tokens, we define this loss the 'Repetition loss', because it trains the model's ability to repeat when clear information from the context is already available. Subsequent experiments have proven that both types of loss are important.\nGeneralizing Chunk Size. To ensure the model exhibits robust generalizability across various chunk sizes and number of candidate tokens, we maintain a constant local context size of 2048, while the chunk size is randomly selected from the set {64, 128, 256, 1024, 2048} during training."}, {"title": "3 Experiments", "content": "In this section, we will conduct a comprehensive evaluation of the effectiveness of FocusLLM, spanning both language modeling and a variety of downstream tasks."}, {"title": "3.1 Experimental Details", "content": "We aligned most of our experimental settings with those of Activation Beacon (Zhang et al., 2024a) to ensure comparable results. Specifically, we conducted training on a Linux server equipped with 8\u00d7A100 GPUs, each with 40GB of memory. The training was carried out for 10,000 steps, equivalent to one epoch of the entire training dataset, using a batch size of 8 and a learning rate of 5e-5 with a linear scheduler. To conserve GPU memory, we employed deepspeed's zero2_offload optimizing stage. The training process was completed in approximately 20 hours.\nFor hyper-parameters, during training, the chunk size was randomly selected from the set {64, 128, 256, 1024, 2048}. For the length of tokens injected into each chunk, we set a default of 512 tokens for inference. And we ensured this length did not exceed the chunk size in the training procedure. As a result, the length of injected tokens was min{512, chunk size}."}, {"title": "3.2 Long-context Language Modeling", "content": "In this section, we evaluate the fundamental capabilities of FocusLLM on long-context language modeling benchmarks, with text lengths ranging from 4K to 128K tokens.\nDatasets. We perform the evaluation on three datasets: PG19 (Rae et al., 2019), Proof-Pile (Azerbayev et al., 2023), and CodeParrot (Tunstall et al., 2022). These three datasets encompass 100 long test cases related to books, arXiv papers, and code repositories, respectively. The results of baseline models are token from (Zhang et al., 2024a) for comparison. Following the setting of (Yen et al., 2024), as FocusLLM relies on the last decoder to perform generation, we calculate the perplexity on the last 256 tokens of each sequence, and for the 128K length, we filter out documents exceeding 128K tokens and evaluate 10 samples due to data scarcity and computational cost.\nModel. FocusLLM is based on LLaMA-2-7B (chat), hence the models for comparison are all on the same scale, 7B. The baseline models can be categorized into the following types: i) Methods focusing on the modification of positional encoding, including Positional Interpolation (Chen et al., 2023a), the NTK-Aware Scale ROPE\u00b9, and the training-free method StreamingLLM (Xiao"}, {"title": "3.3 Downstream Tasks", "content": "Datasets. To assess the capabilities of FocusLLM in real-world scenarios, we select two widely used datasets: Longbench (Bai et al., 2023) and\u221e-Bench (Zhang et al., 2024b). Longbench offers an evaluation on a variety of tasks including question answering, summarization, few-shot learning, mathematical counting, and code completion. The average and 95% percentile length are 12.8K and 31K tokens, respectively. \u221e-Bench is designed to"}, {"title": "4 Further Exploration", "content": "In this section, we conduct further explorations of FocusLLM. Initially, we investigate the upper limits of the sequence lengths that FocusLLM can effectively handle. Subsequently, we perform a quantitative analysis of several key parameters within the framework. Ultimately, we undertake ablation studies on the training loss functions to discern their impacts on the model's overall performance."}, {"title": "4.1 Scaling to 400K Context", "content": "We contend that FocusLLM is capable of processing extremely long sequences. To validate this, we first conduct experiments on the passkey retrieval task (Mohtashami and Jaggi, 2024). The results, as illustrated in Figure 1, demonstrate that FocusLLM maintains nearly 100% effectiveness at lengths of up to 400K, outperforming all other models. In addition to this, we extended the language modeling experiments introduced in Section 3.2 to 400K, a length at which most models fail to manage effectively. The outcomes of these extended experiments are presented in Figure 4.\nTo conclude, FocusLLM's exceptional performance in both passkey retrieval and language modeling tasks, even at the substantial length of 400K tokens2, indicates that it can achieve good performance in the vast majority of real-world scenarios, which underscores the effectiveness and versatility of FocusLLM."}, {"title": "4.2 Memory Footprint and Inference Time", "content": "For models that focus on long-form text, aside from training costs, another critical aspect is the memory footprint and inference time, especially as sequence lengths increase. In this section, we compare FocusLLM with several previous long-context methods capable of retaining global information by preserving the cache of all context: Standard (PI/NTK), LongLlama, and CEPE. The results are shown in Figure 5 and Figure 6. As for models like Activation Beacon and StreamingLLM, although they maintain a constant memory footprint by only retaining cache for a fixed window, they struggle with the precise understanding of extremely long texts. Therefore, they are not the primary subjects of comparison.\nIn the Figure, \"FocusLLM with or without parallel\" signifies that we process each chunk either"}, {"title": "4.3 Chunk Size", "content": "We conduct an investigation into the impact of different chunk sizes on performance. In theory, larger chunk sizes, as long as they do not exceed the model's default context length (e.g., 4K for LLaMA-2), are preferable because they allow for processing the memory with a smaller number of forward passes. However, smaller chunk sizes may enable more precise processing.\nIn experiments, we maintain a total sequence length of 8K, testing the perplexity using different chunk sizes on the same samples of PG19. We select {256, 512, 1024, 2048} as our test sizes. The results are shown in Figure 7. We observe that there is no consistent trend in perplexity as the chunk size increases; it remains relatively stable. This confirms our hypothesis that we can employ larger chunk sizes on models with longer default context lengths (e.g. LLaMA-2-32K). We will explore this direction in our future work."}, {"title": "4.4 Local Context Size", "content": "While the parallel decoding mechanism theoretically allows the model to attend to all tokens, we aim to investigate whether the principle that a larger"}, {"title": "4.5 Ablation Studies", "content": "We employ both Continuation Loss and Repetition Loss for the training of FocusLLM. The motivation behind this is to equip the model with the natural language modeling capability while also enhancing its ability to recover global information. Ablation studies on both losses, as detailed in Table 5, reveal that relying solely on the Continuation Loss enables the model to manage some tasks effectively. Nonetheless, for tasks with substantial dependencies on the preceding context, like HotpotQA and Retrieve.PassKey, the model's efficacy deteriorates. Similarly, while employing the Repetition Loss ensures accurate restatement of the preceding con-"}, {"title": "5 Related Work", "content": "5.1 Long-context language models\nRecent advancements in long-context modeling have seen a surge in innovative approaches that aim to transcend the limitations of transformer architectures. One research direction involves length extrapolation in transformers (Peng et al., 2023; Jin et al., 2024), where methods like positional interpolation help models adapt to longer sequences (Chen et al., 2023a). However, these techniques often fail to address the distraction issue caused by noisy content within extended texts (Tworkowski et al., 2024). Another research branch focus on modifying the attention mechanism or employing compression techniques to maintain long texts within manageable lengths (Chevalier et al., 2023; Zhang et al., 2024a). For instance, (Xiao et al., 2023) discovered that retaining \u2018sink tokens' in conjunction with a sliding window can achieve smooth streaming output. (Zhang et al., 2024a) expanded the context dramatically through compression. However, these methods share a common limitation: they cannot utilize information from all tokens."}, {"title": "5.2 Memory-enhanced Model", "content": "The integration of memory layers within transformer architectures has become a pivotal strategy for enhancing long-context comprehension (Bertsch et al., 2024; Tworkowski et al., 2024; Fang et al., 2024). Common methodologies in memory-enhanced models often employ recurrent strategies that iteratively integrate information from the current window into a persistent memory (Munkhdalai"}, {"title": "6 Conclusion", "content": "In this work, we introduced FocusLLM, a novel framework that significantly extends the context length of LLMs. The core innovation lies in the parallel decoding strategy, which distribute the burden of understanding long texts across each chunk and effectively aggregating global information. FocusLLM stands out due to its remarkable training efficiency, allowing us to achieve substantial gains in context comprehension with minimal computational and memory cost. Compared to existing methods, FocusLLM not only exhibits superior performance across downstream tasks but also maintains low perplexities when handling extensive texts, up to 400K tokens. We hope FocusLLM can be an inspiring work for the community, driving further exploration of long-context models."}, {"title": "A More discussion", "content": "A.1 Architecture\nIn this section, we provide an in-depth explanation of the parallel decoding utilized by FocusLLM. As we mentioned in section 2.1.3, the processing of each chunk in FocusLLM can be considered a form of implicit decoding. This is because our goal is not to obtain representations for entire chunks of text, but rather to preserve information about the next token (based on the current chunk) through a special candidate token.\nFurthermore, we can observe that the overall structure of FocusLLM shares similarities with the recently proposed concept of the decoder-decoder model architecture (Sun et al., 2024), which differs from the traditional encoder-decoder model. Specifically, encoder-decoder models, like T5 (Raffel et al., 2020), utilize a bidirectional encoder for input encoding and a unidirectional decoder for output generation. These models necessitate the re-encoding of the entire set of input and output tokens for each subsequent generation step, making them less suitable for autoregressive generation. In contrast, the decoder-decoder model (also FocusLLM), by caching the previously computed key/value vectors, enables the model to reuse them in the current generation step."}, {"title": "A.2 Loss Design", "content": "In the loss design of FocusLLM, in addition to using the commonly employed Continuation Loss, we designed Repetition Loss to explicitly train the model to maintain attention to the context. We believe that Repetition Loss is the reason why FocusLLM can achieve good results with a relatively small number of training tokens. Previous work has demonstrated that synthetic data can effectively enhance model capabilities or reduce costs (Wu et al., 2024; Li et al., 2022, 2023b, 2024b). This has inspired us to design different types of synthetic data in the future to help FocusLLM quickly learn specific abilities."}]}