{"title": "REL: WORKING OUT IS ALL YOU NEED", "authors": ["Toby Simonds", "Jey Han Lau", "Chaithanya Bandi"], "abstract": "Recent developments, particularly OpenAI's O1 model, have demonstrated the remarkable potential\nof Large Language Models (LLMs) for complex reasoning tasks. Through analysis of Ol's outputs\nand provided sample Chain-of-Thought (CoT) demonstrations, we observe that it approaches problem-\nsolving in a distinctly human-like manner-systematically brainstorming ideas, testing hypotheses,\nverifying results, and planning comprehensive solutions. These sophisticated reasoning capabilities\nremain notably absent in other state-of-the-art language models. In this paper, we hypothesize\nthat this performance gap stems from the limited availability of high-quality reasoning process\ndata in current training sets. We demonstrate that by constructing a specialized dataset focused\non explicit problem-solving workflows (\u201cworked solutions\u201d), we can elicit substantially improved\nplanning capabilities from existing models. Additionally, we propose Reasoning Enhancement\nLoop (REL), a method for generating synthetic worked solutions. All code and data are released at:\nhttps://github.com/tamassimonds/REL", "sections": [{"title": "Introduction", "content": "OpenAI's O1 [7] represents a step-function improvement in language model capabilities, particularly in its approach\nto problem-solving. Analysis of Ol's Chain-of-Thought (CoT) samples reveals a fundamental shift in reasoning\nstrategy: while traditional LLMs typically produce direct solutions akin to textbook answers, O1 exhibits a more\nexploratory problem-solving approach that mirrors advanced human cognition-systematically investigating problem\nspaces, considering multiple approaches, and iteratively refining solutions.\nWe propose that the direct, solution-focused reasoning of traditional LLMs stems from their training data composition.\nCurrent datasets predominantly consist of problem-solution pairs rather than detailed problem-solving processes. While\nmethods like rejection sampling and quality filtering have successfully generated additional high-quality synthetic\ntraining data, they fundamentally fail to capture the sophisticated reasoning processes exhibited by O1 [11]. Our\nempirical testing reveals that current synthetic data generation approaches, despite producing correct solutions, cannot\nreplicate Ol's characteristic exploratory problem-solving behavior\u2014its ability to brainstorm approaches, test hypotheses,\nand iteratively refine solutions [5]. This limitation persists even when applying state-of-the-art filtering techniques to\nselect the highest quality samples, suggesting that the ability to generate intermediate reasoning steps cannot be easily\nbootstrapped from existing models trained primarily on final solutions.\nIn this paper, we demonstrate that by constructing a high-quality dataset of fully \u201cworked solutions\u201d\u2014including\nbrainstorming, hypothesis testing, and solution refinement\u2014we can elicit fundamental reasoning capabilities in\nlanguage models. This initial dataset, though resource-intensive to create, proves crucial for inducing basic planning\nbehaviors that can then be amplified through our Reasoning Enhancement Loop (REL), a critic-generator pipeline that\nautonomously produces additional high-quality worked solutions. Our empirical investigation reveals that attempting to\ndirectly apply self-improvement techniques to models without this foundational working out capacity fails to produce\nsophisticated planning behavior. However, by first finetuning a base LLM on expert demonstrations that explicitly\nshowcase problem exploration and solution planning, we establish the necessary scaffolding for our REL pipeline to\nthen iteratively enhance these capabilities through autonomous generation and refinement of worked solutions."}, {"title": "Background and Related Work", "content": ""}, {"title": "Large Language Models and Reasoning", "content": "Recent advances in Large Language Models (LLMs) have shown remarkable capabilities in complex reasoning tasks\n[6]. Traditional approaches to improving LLM reasoning have focused on techniques like Chain-of-Thought prompting\n[13] and Self-Taught Reasoning [16], which guide models to break down problems into intermediate steps. While these\nmethods have shown success in mathematical and logical reasoning tasks, they typically produce linear solution paths\nrather than the exploratory problem-solving demonstrated by human experts.\nThe release of O1 [7] represents a significant departure from this paradigm. Unlike previous models that generate direct\nsolutions, Ol demonstrates a more sophisticated approach to problem-solving, actively exploring solution spaces and\ntesting multiple hypotheses before arriving at conclusions. This capability suggests a fundamental advancement in how\nLLMs can approach complex reasoning tasks that can not be replicated with prompting."}, {"title": "Self-Improvement and Synthetic Data Generation", "content": "Recent approaches to enhancing model capabilities have explored both self-improvement and synthetic data generation.\nThe Self-Taught Reasoner (STaR) method [16] pioneered the use of bootstrapping techniques, demonstrating that\nmodels can enhance their reasoning abilities by iteratively solving problems and learning from their own solutions.\nThis concept has been extended by methods like Tree of Thoughts [15] and Least-to-Most [18], which generate more\nstructured reasoning paths.\nHowever, these approaches have primarily focused on generating either direct solutions or simple step-by-step reasoning.\nOur work bridges this gap by introducing a more sophisticated pipeline that explicitly focuses on generating and\nvalidating complex problem-solving demonstrations. We combine the iterative improvement aspects of STaR with\nstructured quality criteria to generate synthetic data that captures the full breadth of human-like problem exploration\nand solution refinement."}, {"title": "Test Time Compute and Solution Space Exploration", "content": "Recent work has demonstrated the significant impact of test-time compute scaling on model performance [10],\nparticularly for complex reasoning tasks. While methods like Monte Carlo Tree Search (MCTS)[14] have proven\neffective in domains with well-defined state spaces and clear evaluation metrics (e.g., chess and Go), applying similar\nsystematic search strategies to language model reasoning is less straightforward. This is because the solution space\nfor mathematical or logical reasoning problems is often less structured and more difficult to systematically explore\ncompared to game trees.\nChain-of-Thought (CoT) prompting has emerged as a more fluid alternative for scaling test-time compute in language\nmodels. Rather than attempting to implement rigid tree search algorithms, CoT allows models to naturally explore\nthe solution space through verbalized reasoning steps. This approach can be viewed as a form of \"soft\" search"}, {"title": "Method", "content": "We first developed a high-quality dataset comprising detailed worked solutions from the American Invitational Mathe-\nmatics Examination (AIME).\nAIME was selected as our source material because its problems typically do not demand advanced mathematical\ntechniques, but they require sophisticated logical reasoning and problem-solving skills. This combination makes AIME\nproblems particularly suitable for studying mathematical reasoning capabilities in LLMs. All human-annotated data in\nour dataset was sourced exclusively from AIME 2023 and before. AIME 2024 was used as our test set. Our results\nrepresent the average performance across three complete runs of all 30 questions on AIME 2024.\nQuestion: Lilypads 1, 2, 3, . . . lie in a row on a pond. A frog makes a sequence of jumps starting on pad 1.\nFrom any pad k the frog jumps to either pad k + 1 or pad k + 2 chosen randomly with probability $\\frac{1}{2}$ and\nindependently of other jumps. The probability that the frog visits pad 7 is $\\frac{p}{q}$, where p and q are relatively prime\npositive integers. Find p + q.\nIntial Human Solution: First let's understand the question. 123 lilly pads in a row on a lake. Okay, one two\nthree lillpyads and a frog makes a sequence of jumps starting at pad one. From any pad k the frog can jump to k\n+ one or k plus two. Ahh I just made a slight mistake it's one two three ... so there can be more than 3 lilly\npads otherwise it wouldn't make sense because where would he jump for state 2? So there's basically a real\ninteger number of pads, okay. So he starts at number one and you can jump 1 forward or 2 forward, okay. So\nessentially every pad you can jump over one step forward or two steps forward.\nProbability visits pad 7 is P over Q relative prime integers. Find P over Q. Okay so essentially we want to see\nwhat is the probability of him reaching 7, right? So based on this this looks like some kind of like Markov\nchain problem where we can do some kind of backtracking recursion. That's a little complicated, is there like a\nsimple way of solving this? Looking at it doesn't really seem like that. Initial brain strom and relating to what\nwe know It looks like a markov chain problem. Maybe there's some way to do it with like a binomial of simpler\nway than just backtracking. Back tracking seems like a pretty easy method though..."}, {"title": "Creating Worked Solutions", "content": "To develop high-quality worked solutions, we developed a multi-stage process combining the expertise of graduate-level\nmathematics students with large language model capabilities.\u00b9 The process begins with these expert solvers verbalizing\ntheir complete thought process while solving mathematical problems, captured through speech-to-text technology.\nWe found this method particularly effective at documenting authentic problem-solving processes, including crucial\nelements such as initial brainstorming, strategy selection, and the development of mathematical intuition.\nA significant advantage of speech-to-text capture was its ability to record authentic problem-solving techniques that\nmight be omitted in written solutions, such as trial-and-error approaches and the strategy of solving simpler cases to\nbuild intuition. However, this initial stage proved challenging, as participants often struggled to fully articulate their\nimplicit problem-solving steps and mathematical reasoning. An example of the raw working out can be seen in Figure\n3.\nWe then experimented with experts using LLMs to transform their conceptual approaches into detailed solutions. Im-\nportantly, we made a deliberate decision to minimize human computational errors in this initial dataset. Our preliminary"}, {"title": "Reasoning Enhancement Loop (REL)", "content": "We first finetune an LLM (generator) on the human-created worked solutions created in the previous section.4 We\nfocused primarily on finetuning GPT-40 and GPT-40-mini as they provide the most direct comparison to O1 and\nO1-mini's performances. We used OpenAI's fine-tuning API for fine tuning.\nAfter establishing baseline reasoning capabilities through initial finetuning, we next introduced Reasoning Enhancement\nLoop (REL), a self-improvement pipeline designed to generate more worked solutions for new problems. The novelty\nof REL is that it involves both exploratory reasoning and self-verification (mimicking natural problem-solving), where\nerrors are discovered and corrected during the solution process itself.\nREL consists of two primary components: our finetuned model serving as the solution generator, and a verification\nmodel that identifies mathematical or logical errors. Unlike traditional approaches that aim to eliminate errors entirely,\nREL intentionally preserves and enhances the model's ability to detect and correct its own mistakes during problem-\nsolving(Figure 1). For verifier model we used GPT 40 prompted with the correct solution and the generators working\nout.\nWhen errors are found by the verifier, instead of discarding the work and starting over, we implement an iterative\nhint-based correction process. The verifier first identifies where the solution went wrong and provides a targeted hint\nabout the error. The verifier has access to the provided sample AIME solutions. The hint does not give the full solution\nbut just a hint to the model of how to fix the issue in the solution. The generator is then passed it's previous working out\nand the hint and asked to rewrite to keep it's initial working out the same but to acknowledging its mistake (e.g., \"Ah, I\nsee I've made an error...\") before proceeding with the correction. If the solution remains incorrect after two rounds\nof hints, we provide the complete correct solution and tell the generator to naturally intergrate this into into solution\nas if it discovered it. For example, if a geometry problem contained an incorrect angle calculation, the verifier might\nhint \"Check your calculation of the supplementary angle in step 3,\" and the generator would append to its existing\nsolution: \"Ah, I see I've made an error in calculating the supplementary angle. Let me revise my calculation...\" This\napproach maintains solution continuity while teaching the model to naturally incorporate feedback and corrections into\nits reasoning process.\nSolutions generated through REL are incorporated into subsequent training iterations. We then finetune the generator on\nthis new dataset and repeat the process. To summarise, for our experiments we first finetuned the model of the dataset\nof 100 human-created worked solutions and then a generated 100 samples for each iteration of the REL process.\nNote that when training GPT-40-mini and Llama 3.2 3B, we found it better to train using the outputs of GPT-40 from\nREL. We noticed that the worked solutions generated by smaller models are of poor quality, implying ultimately we\nstill need a model of a certain capacity for generating synthetic worked solutions in REL (this also means the training of\nour smaller models can be interpreted as a form of knowledge distillation)."}, {"title": "Results", "content": ""}, {"title": "Human Generation Solutions vs AIME Solutions", "content": "Our initial experiments focused on comparing two approaches to fine-tuning GPT-40 mini: using human-generated\nworked solutions versus the provided AIME solutions. The aim is to show that any performance improvement isn't\nsimply due to domain specialization, but that the nature of the fine-tuning data is important. For clarity, we refer to these\nmodels as Human FT GPT-40 mini (fine-tuned on human-generated solutions described in Section 3.1) and AIME FT\nGPT-40 mini (fine-tuned on sample solutions provided by AIME). Sample solutions provided for the AIME problems\ngave direct step by step to the final solution, and so are very different to the worked solutions.\nWe evaluated these approaches on AIME problems from 2023 and earlier; results in Figure 2. For these results, we vary\nthe number of fine-tuning data (x-axis), from 1 sample to 1K samples. Note that we only have results for Human FT\nGPT-40 mini for up to 100 samples-that's because that's the maximum number of worked solutions we have created\nin Section 3.1."}, {"title": "REL Process Evaluation", "content": "We investigate whether iterative refinement through our Reasoning Enhancement Loop (REL) can further improve\nperformance beyond the gains achieved with human-annotated solutions alone. Our analysis examines the performance\nprogression of GPT-40 through multiple iterations of the REL process, comparing the final REL-enhanced model\n(REL GPT-40) against GPT-40 fine-tuned using only human-created worked solutions (Human GPT-40) and OpenAI's\nOl model. Results are shown in Figure 3, with the iterative progression displayed in the left plot and final model\ncomparisons in the right plot."}, {"title": "Synthetic Data Comparison", "content": "We investigate whether synthetic data generated through our REL process can match or exceed the effectiveness of\ntraditional synthetic data generation approaches. Our analysis compares two methods of creating training examples\nusing GPT-40. We compare using GPT 40 with rejection sampling and rationalization (if answer incorrect given correct\nanswer and told to rationalize) to using data generated from our REL GPT 40 using the REL process. Both methods are\nthen used to fine-tune GPT-40 mini, resulting in two models for comparison: REL data finteuned (REL_Data_FT_mini)\nand rejection sampling data finetuned (RS_Data_FT_mini).\nResults of this comparison are shown in Figure 5. REL_Data_FT_mini demonstrated superior scaling properties,\nachieving 21.11% accuracy with 1000 training examples. In contrast, Baseline RS_Data_FT_mini showed significantly\nslower improvement despite equivalent dataset sizes. The performance gap became particularly pronounced as dataset\nsize increased, with REL-generated data maintaining consistent improvement while baseline-generated data showed\ndiminishing returns.\nQualitative analysis of solution patterns reveals distinct behavioral differences between the models. REL_Data_FT_mini\nconsistently produced detailed, structured solutions with clear progression through problem-solving stages. These\nsolutions typically began with explicit strategy consideration, followed by systematic exploration of solution paths,\nand included self-correction when necessary. In contrast, Baseline RS_Data_FT_mini tended to generate more\nlinear solutions that, while technically correct, lacked the metacognitive elements and strategic depth observed in\nREL-generated solutions.\nThese results reveal broader insights about effective methods for generating synthetic training data for language models.\nBoth synthetic data approaches\u2014REL-generated and baseline-generated-significantly outperformed training on raw\nAIME solutions, demonstrating the value of detailed solution rationales in training data. However, the rapid emergence\nof sophisticated reasoning patterns in models trained on just 100 REL-generated examples, compared to their slower\ndevelopment in baseline-generated examples, suggests that our REL process provides a more efficient method for\nsynthetic data generation. By encoding complex problem-solving strategies more effectively, REL offers a promising\napproach for creating high-quality training data that can efficiently improve mathematical reasoning capabilities in\nlanguage models. Additionally, this approach demonstrates a more effective method for leveraging our limited corpus\nof high-quality mathematical problems and solutions. By using existing question-answer pairs as seeds for generating\nrich, pedagogically valuable training data, REL provides a scalable way to amplify the impact of rare but valuable\nmathematical content."}, {"title": "REL FT GPT 40", "content": "First Thoughts:\n\u2022 Symmetry? The condition involves reciprocals and squares, which suggests\nsymmetry in the polynomial.\n\u2022 Roots Idea: Maybe if $p (\\frac{1}{k}) = k^2$, then\n$\\frac{1}{k}$ are roots of some related equation.\n\u2022 Polynomial Transformation: Perhaps\ntransforming the polynomial could help.\n\u2022 Using Vieta's Formulas: Since it's\nmonic, the sum and product of roots\nmight play a role.\nPossible Strategies:\n1. Create a New Polynomial: Define a\nnew polynomial based on the given con-\ndition and see what its roots are.\n2. Leverage Symmetry: Exploit any sym-\nmetrical properties in the polynomial.\n3. Factor the Polynomial: Try to factor\np(x) using the given conditions.\n4. Use Polynomial Identities: Apply\nknown identities or theorems related to\nreciprocals.\n....Working Out...\nWait a minute, $p (\\frac{1}{x})$ is in terms of $\\frac{1}{x}$, so q(x)\nmight have a higher degree when expressed in\nterms of x.\nNeed to Re-express q(x): Let's re-express q(x)\nas a polynomial in x.\nNew Approach:\nWait a Second: Since n is even and n \u2265 2, all k\nfrom 1 to n and their negatives are included. So,\nx = \u00b11 are already accounted for..."}, {"title": "01", "content": "In other words, we are to find all real numbers x\nsuch that p(1/x) = x\u00b2.\nFirstly, note that:\n\u2022 The monic polynomial p(x) is of degree\n2n.\n\u2022 We have 2n data points: for k = \u00b11 to\n\u00b1n, p(1/k) = k2.\nThus, we have 2n distinct values of $\\frac{1}{k}$ (since k\nranges over integers \u00b11 to \u00b1n). Therefore, we\nhave 2n points (x, y) of the form ($\\frac{1}{k}$, k\u00b2).\nThus, p is a polynomial of degree 2n that satisfies\np($\\frac{1}{k}$) = $k^2$ for 2n values of$\\frac{1}{k}$.\n.Working Out...\nWait, but p(1/x) \u2013 x2 = 0.\nOur equation is p(1/x) \u2013 x\u00b2 = 0.\nLet me define q(x) = p(1/x) \u2013 x\u00b2.\n.Working Out...\nWait, but $x^{2n}p(1/x)$ is the \"reciprocal polyno-\nmial\" of p(x), evaluated at x.\nBut since p(x) is monic of degree 2n, its recipro-\ncal polynomial is monic of degree 2n as well.\nThus, let t(x) = $x^{2n}p(1/x)$ = $P*(x)$..."}, {"title": "Problem Statement", "content": "Prompt: How many R's are in 'strawberry'?"}, {"title": "01-Llama 3.2 3B Output", "content": "First Thoughts\nOkay, so the question is: How many R's are in\n\"strawberry\"?\nBrainstorming:\n\u2022 Option 1: I could write out the word and\ncount each letter individually.\n\u2022 Option 2: Perhaps there's a pattern\nor rule in English words regarding the\nplacement of R's.\n\u2022 Option 3: Maybe I can use a dictionary\nor online tool to verify.\nStep 1: Writing Out the Word\nLet me start by writing out the word \"strawberry\"\nto see it clearly:\nS-T-R-A-W-B-E-R-R-Y\nStep 2: Counting the R's\nNow, I'll focus specifically on the R's:\nR's: R-R-Y\n\u2022 Count: There are 2 R's in the word\n\"strawberry\".\n\u2022 Check: Let me double-check... S-T-R-\nA-W-B-E-R-R-Y-yes, I see two R's.\nStep 3: Final Check\n\u2022 Quick Check: Let me glance back at the\nword: S-T-R-A-W-B-E-R-R-Y\n\u2022 Final Answer: There are 2 R's in the\nword \"strawberry\"."}, {"title": "Llama 3.1 3B Output", "content": "There are 2 R's in Strawberry"}, {"title": "Analysis", "content": "O1-Llama 3.2 3B demonstrates extensive step-by-step reasoning and self-verification, though both models\narrive at the same answer. Note O1-Llama 3.2 3B's multiple verification steps and consideration of alternative\napproaches, showing significantly more sophisticated reasoning even for this simple task."}, {"title": "Discussion", "content": "Our findings suggest that small amounts of expertly crafted data can be surprisingly effective at unlocking latent model\ncapabilities, challenging the conventional focus on expanding model scale and pretraining data size. The dramatic\nperformance difference between models trained on thoroughly worked solutions versus larger sets of simple solutions\n(10x size advantage) demonstrates that quality of reasoning demonstrations matters more than quantity.\nWhile our REL methodology enhanced reasoning patterns, the fundamental problem-solving strategies traced back to\nhuman demonstrations, suggesting that advancing AI capabilities may depend more on effectively capturing human\nexpertise than on scaling data collection. This points to a future direction focused on creating comprehensive libraries\nof expert human reasoning processes across different fields, where the key challenge lies in surfacing and refining\npretrained models' latent abilities through exposure to sophisticated human problem-solving examples.\nThe effectiveness of targeted post-training with a limited dataset aligns with recent findings on pretraining and post-\ntraining in language models ([2]; [3]). Our results with 100 human demonstrations build on previous small-dataset\nsuccesses ([19]; [9]) and show that specific capabilities can be enhanced through focused interventions ([1], [8]). This\nresource-efficient approach ([17]) suggests opportunities for making AI development more accessible, with future work\nfocused on collecting more diverse human demonstrations of expert reasoning.\nThe initial success of our REL methodology with AIME problems led us to develop ReasonSet, a comprehensive\ndataset of 2,000 question-solution pairs spanning multiple reasoning domains. Using our REL FT GPT-40 model, we\ngenerated synthetic data encompassing AIME, GPQA, MATH dataset problems, and novel general reasoning questions\nderived from our prompt template (Appendix A). Notably, this expansion required no additional human demonstrations\nbeyond the original AIME examples. We are releasing ReasonSet as an open-source resource to help the broader\ncommunity develop reasoning capabilities in language models. These experiments revealed important insights about\nthe transferability of reasoning capabilities across model scales. When fine-tuning Llama 3.2 3B on ReasonSet, we\nobserved interesting patterns in the model's adoption of reasoning behaviors. While the model successfully incorporated\nsurface-level planning elements like explicit brainstorming, strategy enumeration, and verification attempts (Figure 6),\nit struggled with deeper reasoning tasks. The significant performance gap between O1-Llama 3.2 3B and larger models\nlike GPT-40 and GPT-40 mini suggests that although smaller models can learn to emulate sophisticated reasoning\npatterns, they may lack the computational capacity to fully execute complex logical reasoning chains. Nevertheless,\nour open-sourced O1-Llama 3.2 3B model demonstrates how readily structured reasoning behaviors can be induced\nthrough targeted fine-tuning, even in relatively small language models.\nThe success of our approach raises broader implications for language model training methodologies. While significant\nresearch effort has focused on developing increasingly sophisticated training techniques and architectures, our results\nsuggest that certain advanced behaviors might be more readily obtained simply through exposure to high-quality\nhuman demonstrations. The apparent absence of sophisticated reasoning capabilities in standard language models may\nstem not from architectural limitations, but from the nature of their training data which typically consists of final\nsolutions and conclusions rather than detailed problem-solving processes. Just as our models learned to mimic human\nproblem-solving processes from detailed mathematical demonstrations, other advanced capabilities might emerge more\nnaturally if models were trained on data that better captures human expert thinking. This insight may extend beyond\nmathematical reasoning to other domains, including agent behavior and decision-making, where current training data\nsimilarly lacks explicit documentation of human decision-making processes. Rather than developing elaborate training\ntechniques, the key challenge may be collecting high-quality demonstrations that make expert thinking processes\nexplicit. Our findings suggest that addressing this gap in training data composition could be as impactful as architectural\ninnovations in advancing AI capabilities."}, {"title": "Conclusion", "content": "In this paper, we presented several contributions to enhance reasoning capabilities in language models. First, we\nintroduced a novel data creation methodology combining human expertise with AI assistance to efficiently generate\nhigh-quality worked solutions. This hybrid approach proved crucial for capturing authentic problem-solving processes\nwhile maintaining scalability and data quality.\nSecond, we developed the Reasoning Enhancement Loop (REL), a systematic pipeline for generating additional\nhigh-quality worked solutions through iterative refinement and validation. Our empirical results demonstrated that REL\nsuccessfully enhanced model performance on AIME problems, achieving a 27.78% accuracy compared to the 12%\nbaseline of GPT-40, while exhibiting many of the sophisticated reasoning behaviors characteristic of 01."}]}