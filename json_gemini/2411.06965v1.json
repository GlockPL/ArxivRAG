{"title": "IMITATION FROM DIVERSE BEHAVIORS: WASSERSTEIN QUALITY DIVERSITY\nIMITATION LEARNING WITH SINGLE-STEP ARCHIVE EXPLORATION", "authors": ["Xingrui Yu", "Zhenglin Wan", "David Mark Bossens", "Yueming Lyu", "Qing Guo", "Ivor W. Tsang"], "abstract": "Learning diverse and high-performance behaviors from a limited set of demonstrations is a grand challenge.\nTraditional imitation learning methods usually fail in this task because most of them are designed to learn one\nspecific behavior even with multiple demonstrations. Therefore, novel techniques for quality diversity imitation\nlearning are needed to solve the above challenge. This work introduces Wasserstein Quality Diversity Imitation\nLearning (WQDIL), which 1) improves the stability of imitation learning in the quality diversity setting with\nlatent adversarial training based on a Wasserstein Auto-Encoder (WAE), and 2) mitigates a behavior-overfitting\nissue using a measure-conditioned reward function with a single-step archive exploration bonus. Empirically,\nour method significantly outperforms state-of-the-art IL methods, achieving near-expert or beyond-expert QD\nperformance on the challenging continuous control tasks derived from MuJoCo environments.", "sections": [{"title": "1 INTRODUCTION", "content": "Imitation Learning (IL) aims to mimic an expert's behavior by\nlearning from the demonstrations. IL has achieved great suc-\ncess in many real-world applications, such as robotics [51], au-\ntonomous driving [6], and drone control [37]. However, most\nof the traditional imitation learning methods were designed to\nlearn one specific behavior, even given multiple demonstra-\ntions.\nLearning diverse and high-quality policies is the ultimate goal\nof many real-world applications like robotic locomotion tasks\n[4]. Recent literature has demonstrated that quality diver-\nsity reinforcement learning (QDRL) [4, 44] is a promising\ndirection for achieving this goal. Prior methods that com-\nbine Differentiable Quality Diversity (DQD) [14] with off-\npolicy RL achieved diverse and relatively high-performance\npolicies. However, the performance gap between standard RL\nand QDRL still exists [4]. More recently, Batra et al. [4] miti-\ngate this gap by leveraging the on-policy RL method PPO [39]\nwith DQD. PPO estimates the gradients of diversity and perfor-\nmance objectives from the online collected data. Then, the esti-\nmated gradients are used by DQD methods like CMA-MAEGA\n[15] that maintain a single search point and move through the\nbehavior space by filling new regions. The synergy between\nPPO and DQD results in a state-of-the-art QDRL method, Prox-\nimal Policy Gradient Arborescence (PPGA) [4], that achieves\nthe ultimate goal of robotic locomotion. Generally, this kind of\nQDRL method finds a diverse archive of high-performing loco-\nmotion behaviors for an agent by combining PPO gradient ap-\nproximations with Differentiable Quality Diversity algorithms.\nHowever, the success of QDRL heavily relies on high-quality\nreward functions, which can be intractable in practice. Qual-\nity Diversity Imitation Learning (QDIL) offers a more flexi-\nble strategy for learning diverse and high-quality policies from\ndemonstrations with diverse behaviors. In the literature, ad-\nversarial IL methods such as GAIL [23] have achieved great\nsuccess in learning specific behaviors for robotic locomotion\ntasks. Therefore, a naive solution for QDIL is to apply ad-\nversarial IL to estimate rewards from the given demonstrations\nand then leverage the estimated rewards for learning diverse\nand high-quality policies. Unfortunately, adversarial IL meth-\nods suffer from the training instability issue, which usually re-\nsults in worse-than-demonstrator performance [23]. Moreover,\nwhen the demonstrations only contain a few behaviors, the re-\nwards learned by adversarial IL techniques will be behavior-\noverfitted and unable to guide the agent to learn more diverse\nbehaviors beyond the demonstrations.\nThe training instability issue and the behavior-overfitted reward\nissue heavily limit the adversarial QDIL to learning diverse and\nhigh-quality policies with limited demonstrations. In this work,\nwe propose two synergic strategies to overcome these two chal-\nlenging issues. The first strategy aims to stabilize the training of\nthe reward model, and the second strategy focuses on encour-\naging the agent to do behavior space exploration. To develop\nthe first strategy, we propose to stabilize the reward learning\nby applying Wasserstein adversarial training within the latent\nspace of the Wasserstein Auto-Encoder (WAE) [46]. Similar\nto VAE [26], WAE keeps the good properties of stable training\nand a nice latent manifold structure while generating higher-\nquality images than GAN [46]. Therefore, we propose to ap-\nply WAE to enable a more stable training of reward model in\nadversarial QDIL. In addition, we propose latent Wasserstein\nadversarial training to further improve the consistency of the\nreward training stability. For the second strategy, we introduce\na bonus that enables the agent to collect data with more diverse\nbehaviors via single-step archive exploration. We call the re-\nsulting method Wasserstein Quality Diversity Imitation Learn-\ning (WQDIL) with Single-Step Archive Exploration (SSAE).\nFigure 1 illustrates the two issues of the Adversarial QDIL (i.e.,\ntraining instability and behavior-overfitted reward) and the cor-\nresponding solutions (i.e. WQDIL and Single-Step Archive Ex-\nploration). The synergy between WQDIL and SSAE adherently"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "This section provides background and related works for Imita-\ntion Learning (Section 2.1), Quality Diversity Reinforcement\nLearning (Section 2.2), Quality Diversity Imitation Learning\n(Section 2.3), Training Stability (Section 2.4), and Exploration\n(Section 2.5)."}, {"title": "2.1 Reinforcement Learning and Imitation Learning", "content": "Reinforcement Learning (RL) searches for policies that maxi-\nmize cumulative reward in an environment, typically assuming\nthe discrete-time Markov Decision Process (MDP) formalism\n(S, A, r, P, \u03b3). Here, S and A are the state and action spaces,\nr(s, a) is the reward function, P(s'|s, a) defines state transition\nprobabilities, and y is the discount factor. The traditional RL\nobjective is to maximize the discounted episodic return of a\npolicy \u0395 [\u03a3_{k=0}^{T-1} \u03b3^{k}r(sk, ak)] where T is the episode length.\nImitation learning (IL) trains an agent to mimic expert be-\nhaviors from demonstrations [50]. Behavior Cloning (BC)\nuses supervised learning to imitate expert behavior but suffers\nfrom severe error accumulation [36]. Inverse Reinforcement\nLearning (IRL) seeks to recover a reward function from the\ndemonstrations, and use reinforcement learning (RL) to train\na policy that best mimics the expert behaviors [1]. Early IRL\nmethods estimate rewards in the principle of maximum en-\ntropy [52, 48, 13]. Adversarial Inverse Reinforcement Learn-\ning (AIRL) [18] learns a robust reward function by training the\ndiscriminator via logistic regression to classify expert data from\npolicy data.\nAdversarial IL methods treat IRL as a distribution-matching\nproblem. Generative Adversarial Imitation Learning (GAIL)\n[23] trains a discriminator to differentiate between the state-\naction distribution of the demonstrations and the state-action\ndistribution induced by the agent's policy, and output a reward\nto guide policy improvement. More recently, Primal Wasser-\nstein Imitation Learning (PWIL) [12] introduces an offline re-\nward function based on an upper bound of the Wasserstein dis-\ntance between the expert and agent's state-action distributions,\navoiding the instability of adversarial IL methods. This paper\nincludes max-entropy IRL (MaxEntIRL), the classic adversar-\nial IL (i.e., GAIL) and the state-of-the-art PWIL as our base-\nlines in QDIL with limited demonstrations."}, {"title": "2.2 Quality Diversity Reinforcement Learning", "content": "Quality Diversity Optimization. Distinct from traditional op-\ntimization which aims to find a single solution to maximize the\nobjective, Quality Diversity (QD) optimization aims to find a\nset of high-quality and diverse solutions in an n-dimensional\ncontinuous space R\". Given an objective function f : R\" \u2192 R\nand k-dimensional measure function m : R\" \u2192 R^k, the goal is\nto find solutions \u03b8 \u2208 R\u201d for each local region in the behavior\nspace B = m(R\"). QD algorithms discretize B into M cells,\nforming an archive A. Formally, the objective is to find a set of\nsolutions {0_i}_{i=1}^{M} which maximises f(0_i) for each i = 1, ..., M.\nEach solution 0_i corresponds to a cell in A via its measure\nm(0_i), forming an archive of high-quality and diverse solutions\n[10, 34].\nPrevious Quality Diversity optimization methods integrate Evo-\nlution Strategies (ES) with MAP-Elites [31], such as Covari-\nance Matrix Adaptation MAP-Elites (CMA-ME) [16]. CMA-\nME uses CMA-ES [21] as an ES algorithm generating new so-\nlutions inserted into the archive, and uses MAP-Elites to retain\nthe highest-performing solution in each cell. CMA-ES adapts\nits sampling distribution based on archive improvements from\noffspring solutions. However, traditional ES faces low sam-\nple efficiency, especially for high-dimensional parameters such\nas neural networks. Differentiable Quality Diversity (DQD)\nenhances exploration and optimization by leveraging gradient\ninformation from both the objective and measure functions.\nThe state-of-the-art DQD algorithm, CMA-MAEGA [15], uti-\nlizes CMA-ES to maintain a distribution over coefficient sets\nc_i, rather than directly generating solutions 0. The objective\nfunction is g(0) = |c_0|f(0) + \u03a3_{j=1}^{k} c_jm_j(0), where coefficients c_i\nweight the gradients of the objective f and measures m_j. CMA-\nMAEGA maintains a search policy and samples multiple coef-\nficient sets after computing gradients of f and m_i to generate\nbranched offspring solutions \u03c0\u03bf_1, ..., \u03c0\u03bf_\u03bb. The archive improve-\nment from each offspring guides the update of the coefficient\ndistribution, which is then used to refine the search policy.\nBuilt on the QD optimization paradigm, the Quality Diversity\nReinforcement Learning (QDRL) problem can be viewed as\nmaximizing f(0) = \u0395_{\u03c0\u03bf} [\u03a3_{k=0}^{T-1} \u03b3^{k}r(sk, ak)] with respect to di-\nverse @ in a policy archive defined by measure m [11]. In\nQDRL, both the objective and measure are non-differentiable,\nrequiring approximations by DQD approaches. The state-of-\nthe-art QDRL algorithm, Proximal Policy Gradient Arbores-\ncence (PPGA), employs a vectorized PPO (VPPO) architec-\nture to approximate the gradients of the objective and measure\nfunctions [5]. PPGA introduced the Markovian Measure Proxy\n(MMP), a surrogate measure function that correlates strongly\nwith the original measure and allows gradient approximation\nvia policy gradient by treating it as a reward function. Specif-\nically, MMP decomposes trajectory-based measures m into in-\ndividual steps by computing:\nm_i(0) = \\frac{1}{T} \\sum_{t=0}^{T-1} \u03b4_i(s_t),\nwhere \u03b4_i(st) represents the single-step measure dependent on\nthe state st. By breaking down the measure in this way, it\nbecomes state-dependent and adheres to the Markov property.\nThen PPGA uses k + 1 parallel environments with distinct re-"}, {"title": "2.3 Quality Diversity Imitation Learning", "content": "Definition 1 (Quality-Diversity Imitation Learning (QDIL))\nGiven expert demonstrations D = {(Si, ai, d(Si))}_{i=1}^{N}, where\ns,a,d(s) represent state and action and the Markovian mea-\nsure proxy of state s. QDIL aims to learn an archive of\ndiverse policies {o_i}_{i=1}^{M} that collectively maximizes f(0_i) (i.e.,\ncumulative reward) without access to the true reward. The\narchive is defined by a k-dimensional measure function m(0),\nrepresenting behavior patterns. After dividing the archive into\nM cells, the objective of QDIL is to find M solutions, each\noccupying one cell, to maximize:\nmax_{{0_i}} \\sum_{i=1}^{M} f(0_i).\nIn this paper, we focus on Adversarial QDIL, in which we\napply adversarial IL, based on GAIL [19] in particular, to es-\ntimate rewards from the given demonstrations D. This adver-\nsarial training scheme considers the policy as an adversary and\nan additional neural network, the disciminator D, which dis-\ncriminates between samples of the policy and samples of the\ndemonstrations. The resulting reward model is based on the\ndiscriminator, and indicates the likelihood of corresponding to\ndemonstration behaviours. This reward model then is used for\nthe objective of QDIL."}, {"title": "2.4 Improving Stability of Adversarial IL", "content": "The methodology of adversarial IL comes from Generative Ad-\nversarial Networks (GAN) [19]. Although adversarial IL meth-\nods like GAIL have achieved great success in low-dimensional\nenvironments, they also inherit the training instability issue of\nGAN that limits its generalization to more diverse tasks [28].\nWasserstein GAN (WGAN) [2] was proposed to stabilize the\ntraining of GAN by minimizing the Wasserstein distance in-\nstead of the Jensen-Shannon divergence between true data dis-\ntribution and generated data distribution. However, the original\nWGAN with weight clipping can lead to undesired behaviors\n[20]. Gradient penalty (GP) was later proposed to address this\nissue, and has become a widely-used strategy for improving\nthe stability of GAN and WGAN [38, 33, 20, 30, 43, 35]. We\nhave applied these stabilization methods in adversarial QDIL,\nand conduct preliminary investigation experiments on contin-\nuous control tasks derived from MuJoCo. However, we only\nobserved negligible effects of these methods on stabilizing ad-\nversarial QDIL.\nWasserstein Auto-Encoder (WAE) [46] minimizes the opti-\nmal transport cost Wc (Px, PG) based on the novel auto-encoder\nformulation. In the resulting optimization problem, the decoder\ntries to accurately reconstruct the encoded training examples as\nmeasured by the cost function fo. Similar to VAE [26], the\nencoder tries to simultaneously achieve two conflicting goals:"}, {"title": "2.5 Encouraging exploration in RL and IL", "content": "Exploration bonuses have a long history in bandits and rein-\nforcement learning as part of the \"optimism in face of uncer-\ntainty\" principle, and include but are not limited to visitation\ncount based approaches. In bandits, upper confidence bound\n(UCB, e.g. [3]) computes a bonus based on Vn(s_t,a_t) = \\sqrt{\\frac{2ln(t)}{n_t(a)}} for action\na, at time t. In tabular reinforcement learning, UCRL tech-\nniques (e.g. [24]) provide upper confidence bounds to the re-\nward based on a bonus proportional to \\sqrt{\\frac{2ln(t)}{n(a)}}. R-MAX [8]\nand E3 [25] explicitly distinguish between known and unknown\nstates, where in unknown states R-MAX assigns the maximal\nreward of the MDP and E3 performs balanced wandering (i.e.\ntaking the action with lowest visitation count for the current\nstate). Related concepts have also been applied in deep re-\ninforcement learning [42, 22, 29, 40]. Related to exploration\nbonuses, recent techniques in imitation learning also consid-\nered curiosity-based exploration [32, 9, 49], which relates to\ndiscrepancy between the transition model's prediction and the\ntrue transition. For instance, GIRIL [49] computes rewards of-\nfline by pretraining a reward model using a conditional VAE\n[41].\nOur measure bonus differs from these approaches in that it is\ndesigned to explore the behavior space rather than the (state-)\naction space. Further, contrasting to UCRL type bonuses, due\nto dividing by the proportion of visits, rather than the count, our\nmeasure bonus does not shrink to zero such that underexplored\nbehaviors will continue to receive a bonus. This is somewhat\ncomparable in aim to the balanced wandering of E3 but dif-\nfers in that it is a reward bonus rather than a policy definition\n(since we do not have control over the measures) and that it\nis maintained throughout the entire optimization process as we\nare interested in diversity."}, {"title": "3 WASSERSTEIN QUALITY DIVERSITY IMITATION\nLEARNING (WQDIL)", "content": "This section introduces our proposed framework-WQDIL. We\nfirst begin with two solutions for the behavior overfitting prob-\nlem which are called Single-Step Archive Exploration Bonus\nand Measure Conditioning (Section 3.1). We then intro-\nduce Wasserstein Auto-encoders (Section 3.2), and propose a\nWQDIL instance WAE-WGAIL that applies latent Wasserstein\nadversarial training for WAE optimization (Section 3.3)."}, {"title": "3.1 Mitigating behavior overfitting", "content": "Compared to the vast behavior space, expert behaviors tend\nto be limited in diversity. This poses a problem for learning\ndiverse behaviors, as the reward model in the traditional IL\nparadigm focuses solely on shaping rewards around specific\nexpert behaviors while neglecting other behavior patterns. We\nrefer to this type of reward as a \"behavior-overfitted reward.\"\nConsequently, this issue significantly inhibits the learning of di-\nverse behaviors, since any RL method inherently converges to\nhigh-reward behaviors. To address this problem, we introduce\na single-step archive-exploration reward bonus and measure-\nconditioning, which together ensure exploration across the be-\nhavior space as well as the sensitivity of the reward function to\nthe local measure information.\nSingle-Step Exploration Bonus and Measure Conditioning\nIn both the bonus and the measure conditioning, we are primar-\nily interested in single-step measures building on the Marko-\nvian Measure Proxy mentioned in Section 2.2 to make the mea-\nsure information more fine-grained. This is not merely for\nconvenience purposes but also as this will yield richer data\nfrom the demonstrations, which would otherwise be sparse (e.g.\nonly a few episodic measures since we focus on limited-expert-\ndemonstrations in our setting). Meanwhile, Focusing on sin-\ngle step measure will allow exploring behaviours with similar\nepisodic measure but different single-step measure for poten-\ntially yielding higher fitness within a particular behavioral re-\ngion. This is due to the episodic measure is the average (or\nsum) of each single-step measure and is less fine-grained.\nFor the exploration bonus, we establish the single-step archive\nAsingle, which corresponds to the state-dependent measure d(s).\nSimilar to the behavior archive A, we partition Asingle into\nnumerous cells for discretization. Notably, instead of merely\nrecording whether a cell is occupied, we track the visitation\ncount n_i for each cell C_i in Asingle. The exploration reward\nbonus is defined as:\nr_{exp}(s, a, d(s)) = \\frac{1}{1 + p(d(s))},\nwhere\np(d(s)) = \\frac{n(C(d(s)))}{\\sum_i n(C_i)}.\nIn these equations, Ci denotes the i-th cell in Asingle, C(d(s))\nrepresents the cell corresponding to the single-step measure\nproxy d(s), and n signifies the visitation count. Each time a\nstate s activates a cell in Asingle, the visitation count of that cell\nis incremented by one. This mechanism allows the single-step\narchive Asingle to be dynamically updated during training.\nThe exploration bonus assigns higher rewards to regions in\nAsingle that are less frequently visited, thereby promoting the\nagent to explore unseen behavior patterns. Additionally, once\na region within the single-step behavior space has been suffi-\nciently explored, the bonus decreases, facilitating the exploita-\ntion of that region to discover high-performing policies. How-\never, note that the bonus is defined relative to the exploration\nof other measures such that the bonus never shrinks to zero for\na particular measure. The form in Eq. 3 also avoids extreme\nvalues due to the +1 offset in the denominator. With these\nfeatures together, the reward bonus can effectively mitigate the"}, {"title": "3.2 Wasserstein auto-encoders", "content": "The Wasserstein distance is a formulation that occurs within the\ncontext of optimal transport [47]. Kantorovich's formulation of\nthe problem is given by\nWc(Px, PG) := inf_{\u0393\u2208P(x~Px,y~PG)} \u0415_{(x,y)~\u0433}[fc(x, y)],\nwhere fc(x, y): X \u00d7 X \u2192 R+ is the cost function and P(x, y) is\nthe set of joint distributions of (x, y) with marginals Px and PG\nrespectively. We are interested in the set X = S\u00d7A\u00d7 M, where\nM is the measure space.\nAuto-encoders aim to represent input data in a low-dimensional\nlatent space. Modern generative models like variational auto-\nencoders (VAEs) [26] and generative adversarial networks\n(GANs) [19] do so by minimizing a discrepancy measure be-\ntween the data distribution Px and the generative model PG. To\nformulate a Wasserstein based generative model, the form in\nEq. 5 is intractable; however, following [7], one can equate the\nWasserstein distance to the following WAE objective [46]:\nDwAE(Px, PG) := inf_{Q(zx)\u2208Q} Ep_x EQ(z|x) [fc(x, G(z))]\n+ \u03bb\u00b7 Dz(Qz, Pz),\nwhere P\u2082 is the latent distribution, D\u2082 is a divergence measure,\nand PG is the decoder (i.e. generative model). We restrict our\ncost function to the squared Euclidian distance, i.e. f(x, y) =\n||x \u2212 y||2, resulting in the 2-Wasserstein distance.\nWAE-GAN [46] is an instance of WAE when choosing\nDz(Qz, Pz) = DJs(Qz, Pz) and using adversarial training to es-\ntimate Dz. Specifically, WAE-GAN uses an adversary and dis-\ncriminator in the latent space Z trying to separate \u201ctrue\u201d points\nsampled from P\u2082 and \u201cfake\u201d ones sampled from Qz [19]. In the\nimitation learning setting, P\u2082 corresponds to the distribution of\nlatent data obtained from the encoded demonstrations while Qz\ncorresponds to the distribution of latent data obtained from the\nencoded trajectory data from the policy.\nAnalogously, we propose WAE-WGAN, which is equivalent to\nWAE-GAN except that it sets the divergence measure to the 1-\nWasserstein distance, i.e. D\u2082(Qz, Pz) = W1(Qz, Pz). We choose\nthis option based on results on the improved stability during\nadversarial training [2]. Due to strong duality with p = 1, the\nKantorovic-Rubinstein duality can be applied, i.e.\nW1(Qz, Pz) = sup_{||f||_L\u22641} Ez~Q[f(z)] \u2013 Ez~P\u2082[f(z)],\nwhere f is a 1-Lipschitz function f : X \u2192 R."}, {"title": "3.3 WQDIL: WAE Meets Adversarial QDIL", "content": "This section introduces our Wasserstein Quality Diversity Im-\nitation Learning (WQDIL) framework. We illustrate the pro-\ncesses of WQDIL in Algorithm 1, which is built on top of the\nProximal Policy Gradient Arborescence (PPGA) algorithm [5].\nThe blue color indicates the WQDIL's operations that are dis-\ntinct from the PPGA algorithm."}, {"title": "Algorithm 1 Wasserstein Quality Diversity Imitation Learning\n(WQDIL)", "content": "1: Input: Initial policy 00, VPPO instance to approximate \u2207f,\nVm and move the search policy, number of QD iterations\nNo, number of VPPO iterations to estimate the objective-\nmeasure functions and gradients N1, number of VPPO iter-\nations to move the search policy N2, branching population\nsize A, and an initial step size for xNES \u03c3g. Initial reward\nmodel R, Expert demonstrations D.\n2: Initialize the search policy \u03b8\u03bc = 00. Initialize NES parame-\nters \u03bc, \u03a3 = \u03c3\u03bf\u0399\nIn WQDIL, we input the initial reward model R and the expert\ndemonstrations D. In each iteration, we first approximate the\ngradients Vf and m (in step 4) using the rewards estimated\nfrom R (Algorithm 2). Steps 5-15 are QD optimization options\nused in PPGA. In step 16, we update the search policy using the\nreward model R. Step 17 is a key step, in which we update the\nreward model R using the input expert demonstrations D. We\nhave explored a few algorithms in the following for updating\nthe reward model R in WQDIL.\nWAE-GAIL. The first reward updating algorithm is WAE-\nGAIL, which is a naive adaptation of WAE-GAN in the QDIL\nframework. Algorithm 5 in Appendix A summarizes the details\nof WAE-GAIL."}, {"title": "ALGORITHM 2 Reward Estimation", "content": "1: Initialize: Reward model R\n2: Method: Reward estimation in Algorithm 1 (step 4)\n3: def get_episode_reward(episode, current archive A):\n4: $1, A1, \u03b4(81), S2, A2, 8(S2) ..., Sk, ak, d(sk) \u2190 episode\n5: r1, 2, ..., rk \u2190\u2190 R(s, a, d(s)) batch reward\n6: compute exploration bonuses r1,exp, 12,exp, ..., rk,exp\nbased on equation (3) and (4)\n7: For i = 1 k\n8: riri + ri,exp calculate total reward for each step\n9: return 1, 2,..., rk\nWAE-WGAIL. Although WAE-GAIL achieves better stability\nand higher QD performance than GAIL in some tasks, the im-\nprovements are not consistent. To further improve WAE-GAIL,\nwe propose to apply Wasserstein adversarial training in the la-\ntent space of WAE, which is analogous to the WAE-WGAN\nproposed in Section 3.2. The loss function of the auto-encoder\nin WAE-WGAIL is given by\nL(4,4) = Lrecon + Lrecon + ADz(ze, z", "fc(x": "G\u2084(z", "mCWAE-WGAIL.\n1": "Input: Expert emonstrations D, Current policy \u03c0\u03b8, Regu-\nlarization coefficient a > 0.\n2: Initialize the parameters of the encoder Q4,\ndecoder Gy, and latent Wasserstein discriminator Dw.\n3: Method: Update reward model in Algorithm 1 (step 17)\n4: def update(D, \u03c0\u03bf):\n5: Sample expert data (s, a, d(s)),\u00b7\u00b7\u00b7, (s, an, d(s))}\nfrom the expert demonstration D\n6:\n7: Sample trajectories {(s1, a, d(s)),\u00b7\u00b7\u00b7, (s, a, d(s))}\nfrom the policy \u03c0\u03bf\nSample z from Q\u2084(z|s, a, d(s)) for i = 1, ..., n\n8:\nSample z from Q\u2084(z|s, a, d(s)) for i = 1,...,n\n9: Update Dw by ascending:\nlin\\n\\sum_{i=1}^{n} Dw(2) - Dw(27)\n10: Update Q and G by descending:\n11:\n12:\n\\frac{1}{n} \\sum_{i=1}^{n} (fc((s, a, d(s)), G\u2084(z)) + fc((s, a, d(s)), G(z)))\nRepeat until the model converges or the number of\nepochs is reached\nreturn Updated Q, Gy and Dw\nmCWAE-WGAIL. Compared to GAIL, WAE-WGAIL has\nsignificantly better stability and consistency across different"}, {"title": "3.1 Mitigating behavior overfitting", "content": "tasks. However, the latent space of WAE-WGAIL is learned\nfrom the state-action pairs. Since the demonstrations only con-\ntain very limited behaviors, the learned latent space is not adap-\ntive to more diverse behaviors that the agent may encounter. To\nimprove the adaptivity of the latent space and the reward model\nR, we propose measure-conditioned WAE-WGAIL (mCWAE-\nWGAIL) that learns latent space by reconstructing state-action-\nmeasure triplet, i.e. formulating x = (s, a, d(s)) in Eq. 8. Algo-\nrithm 3 summarizes the details of mCWAE-WGAIL, our pro-\nposed algorithm for WQDIL."}, {"title": "4 EXPERIMENTS", "content": null}, {"title": "4.1 Experiment Setup", "content": "We evaluate our framework on three popular MuJoCo [45] en-\nvironments: Halfcheetah, Humanoid, and Walker2d. The mea-\nsure function is the number of times a leg contacts with the\nground, divided by the trajectory length. We implement all the\nmethods based on the PPGA implementation, which utilizes the\nBrax simulator [17] with QDax wrappers for measure calcula-\ntion [27]. All Experiments were conducted on A40 GPUs for\nthree runs per task."}, {"title": "4.2 Demonstrations", "content": "We use a policy archive obtained by PPGA to generate ex-\npert demonstrations. To follow real-world scenario with limited\ndemonstrations, we first sample the top 500 high-performance\nelites from the archive as candidate pool. Then from this pool,\nwe select a few demonstrations such that they are as diverse\nas possible. This process results in 4 diverse demonstrations\n(episodes) per environment. Figure 3 visualizes the demonstra-\ntors in the policy archive, and Table 6 in Appendix C provides\nthe statistical properties of the demonstrations."}, {"title": "4.3 Performance", "content": "To validate the effectiveness of WQIL with Single-Step Archive\nExploration, we compare our mCVAE-WAIL-Bonus against\nstate-of-the-art IL methods within QDIL framework. These\nmethods includes our WAE-WGAIL (Algorithm 6, Appendix\nA), GAIL [23], PWIL [12], AIRL [18], MaxEntIRL [52] and\nGIRIL [49]. We apply a gradient penalty to all the methods\nthat require the online update of the reward model. Appendix\nB summarizes the hyperparameters.\nWe compare the performance under four QD metrics [5]: 1)\nQD-Score, the sum of scores of all nonempty cells in the"}, {"title": "4.4 Ablations", "content": "We conduct two ablations to study the effects of each compo-\nnent of our method mCWAE-WGAIL-Bonus. The first ablation\nin Section 4.4.1 studies the effects of latent Wasserstein adver-\nsarial training in the QDIL framework. On the other hand, Sec-\ntion 4.4.2 focuses on studying the effects of single-step archive\nexploration bonus and measure conditioning."}, {"title": "4.4.1 Ablation on Latent Wasserstein Adversarial Training", "content": "To study the effect of latent Wasserstein adversarial training,\nwe additionally introduce mCWAE-GAIL-Bonus, which rein-\nforces WAE-GAIL with measure conditioning and a single-step\narchive exploration bonus. Table 1 compares the QD perfor-\nmance of our mCWAE-WGAIL-Bonus with mCWAE-GAIL-\nBonus and WAE-WGAIL. As we can see, applying Wasserstein\nadversarial training improves the QD-Score of WAE-GAIL\nand mCWAE-GAIL-Bonus by 27.5% and 23.7% respectively\non HalfCheetah. In Walker2d, the latent Wasserserstein ad-\nversarial training enhances the QD-Score of WAE-GAIL and\nmCWAE-GAIL-Bonus by 74.3% and 62.4%, which is sig-\nnificant. Moreover in Humanoid, mCWAE-WGAIL-Bonus\nachieves 2x QD-Score of mCWAE-GAIL-Bonus, impressively\noutperforming the expert (i.e., PPGA-trueReward) by 12%.\nThe learning curves are shown in Figure 6 in the Appendix D.2."}, {"title": "4.4.2 Ablations on Single-Step Archive Exploration and\nMeasure Conditioning", "content": "Table 2 compares the effects of single-step archive exploration\nbonus"}]}