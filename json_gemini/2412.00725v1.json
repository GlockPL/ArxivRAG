{"title": "Decision Transformer vs. Decision Mamba: Analysing the Complexity of Sequential Decision Making in Atari Games", "authors": ["Ke Yan"], "abstract": "This work analyses the disparity in performance between Decision Transformer (DT) and Decision Mamba (DM) in sequence modelling reinforcement learning tasks for different Atari games. The study first observed that DM generally outperformed DT in the games Breakout and Qbert, while DT performed better in more complicated games, such as Hero and Kung Fu Master. To understand these differences, we expanded the number of games to 12 and performed a comprehensive analysis of game characteristics, including action space complexity, visual complexity, average trajectory length, and average steps to the first non-zero reward. In order to further analyse the key factors that impact the disparity in performance between DT and DM, we employ various approaches, including quantifying visual complexity, random forest regression, correlation analysis, and action space simplification strategies. The results indicate that the performance gap between DT and DM is affected by the complex interaction of multiple factors, with the complexity of the action space and visual complexity (particularly evaluated by compression ratio) being the primary determining factors. DM performs well in environments with simple action and visual elements, while DT shows an advantage in games with higher action and visual complexity. Our findings contribute to a deeper understanding of how the game characteristics affect the performance difference in sequential modelling reinforcement learning, potentially guiding the development of future model design and applications for diverse and complex environments.", "sections": [{"title": "1 Introduction", "content": "The field of reinforcement learning (RL) has seen significant advancements in recent years, particularly in the domain of sequential decision-making tasks. One notable approach has emerged as a powerful tool for these challenges: the Decision Transformer (DT) [1]. Recently, Ota introduced Decision Mamba (DM), which replaces DT's causal self-attention blocks with the new Mamba architecture [2, 3]. Both DT and DM have shown promise in various RL tasks. However, to the best of our knowledge, a comprehensive analysis of their relative performance across different environments remains an open question. This study aims to fill this gap by thoroughly comparing DT and DM across a wide range of Atari games.\nOur investigation begins with an initial observation: while DM outperforms DT in some games, such as Breakout, DT demonstrates superior performance in others, notably Hero and KungFuMaster. This gap in performance across different games raises questions about the factors that influence the effectiveness of these models. We seek to understand whether certain"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Transformer", "content": "The Transformer introduced by Vaswani et al. revolutionized the field of sequence modelling [4]. The architecture consists of an encoder and decoder, each of which contains multiple layers of self-attention and fully connected neural networks, with a cross-attention connecting the encoder and decoder [4]. Compared to Recurrent Neural Networks (RNNS), Transformer can process the entire sequence simultaneously, and without suffering long-range dependency problems. This is achieved through the self-attention mechanism. It allows the model to weigh the importance of different parts of the input sequence, captures complex relationships inside the data, and can be computed in parallel on modern GPUs. The Transformer show better performance on various tasks while significantly reducing training time [4, 5]. However, they may encounter difficulties when dealing with very long sequences during inference due to their quadratic computational complexity, which leads to a large video RAM cost. [3]."}, {"title": "2.2 State Space Models", "content": "State Space Models (SSMs) have emerged as a promising alternative for long sequential modelling tasks. An SSM, or state space model, utilises a mathematical framework to convert a one- dimensional input into an n-dimensional latent state, which is then projected back to a one- dimensional output [3, 6]. The process can be expressed by the following equations:\n\\begin{equation}\nh'(t) = Ah(t) + Bx(t)\n\\end{equation}\n\\begin{equation}\ny(t) = Ch(t)\n\\end{equation}\nwhere h(t) represents the N-D latent state, x(t) is the 1-D input, and y(t) is the 1-D output [3, 6].\nThe equations of SSMs are similar to those of RNNs, but there are differences in how the matrix A is initialised. In traditional recurrent neural networks (RNNs), these matrices are commonly initialised randomly and then updated via backpropagation. In contrast, SSMs utilise HiPPO (High-order Polynomial Projection Operators) theory for initialization. The HiPPO theory initializes the A matrix to represent the projection of the input sequence onto polynomial bases [7]. For instance, one variant of the HiPPO matrix, known as HiPPO-LegT:"}, {"title": "2.2.1 Mamba", "content": "Despite the benefits of structured HiPPO initialization, there are limitations to the original SSM due to its time-invariant nature, which means that the matrices A, B, and C are constant throughout time. The Mamba architecture, introduced by Gu et al., addresses this problem by incorporating a selection mechanism that introduces linear time-varying [3]. Inspired by gating mechanisms in Long Short-Term Memory (LSTM) networks, which have been successful in selectively updating and forgetting information in sequential data processing [8]. In Mamba, this selection mechanism allows the model to adaptively focus on or ignore specific inputs based on their relevance, making it capture long-range dependencies and selectively copy from the inputs [3]."}, {"title": "2.3 Reinforcement Learning", "content": ""}, {"title": "2.3.1 Traditional Reinforcement Learning", "content": "Reinforcement learning (RL) is a paradigm in machine learning. In traditional RL, the agent (model) learns to make decisions by interacting with the environment. One of the basic frameworks of RL is the Markov decision process (MDP), which assumes that the future state of the environment depends only on the current state and behaviour, not on the previously recorded"}, {"title": "2.3.2 Sequence Modeling in Reinforcement Learning", "content": "In recent years, there has been a paradigm shift in solving reinforcement learning tasks. Motivated by the success of sequence models across several domains, the new method redefines the reinforcement learning task as a sequence modelling problem. The Decision Transformer (DT), introduced by Chen et al., is a prime example of this paradigm shift in reinforcement learning [1]. Utilizing the power of the Transformer in sequence modelling, DT processes trajectories of return-to-go, states, and actions to predict the next optimal actions. DT reframes the RL problem as a sequence prediction task, freeing itself from the Markov property assumption by considering the entire trajectory rather than just the immediate state-action pairs. Therefore this approach does not require and explicit value-function (Bellman equation) approximation, which is fundamentally different from the traditional RL. The larger context allows DT to consider longer sequences of states, actions, and returns, enabling it to directly learn the relationship between returns and optimal actions. The key advantage of this approach is that it can consider a wider range of contexts, thus capturing long-term dependencies more effectively than traditional RL methods. Moreover, DT uses offline data for training, avoiding the inefficiency caused by the agent continuously interacting with the environment in traditional RL.\nDecision Transformer represent a promising direction in reinforcement learning research. Based on the Decision Transformer, several variants have been proposed to solve specific challenges or improve performance in different domains. Decision ConvFormer replaces the causal-self attention layer of DT with a convolutional layer. For state, action, and return-to-go in the game trajectory, it applies three separate convolutional filters: The state filter, action filter, and RTGfilter to improve the model's ability to capture the inherent local correlations [10]. The Decision S4 model focuses on continuous control tasks such as HalfCheetah. It uses a structured state space sequence (S4) model to improve the efficiency of modelling long-range dependencies. With fewer parameters and training time, it achieves better performance than Decision Transformer on most tasks. [11]. Recently, Ota introduced Decision Mamba (DM), replacing DT's causal self-attention mechanism with the Mamba blocks [2]. This modification aims to utilise the efficiency and effectiveness of the Mamba architecture in handling long sequences, showing competitive performance with DT models.\nWhile these studies show promising results, there remains a gap in understanding how different sequence modelling architectures apply to various reinforcement learning tasks. To the best of our knowledge, most existing work has focused on limited environments. For instance, Decision"}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Dataset", "content": "This study utilizes the DQN-replay dataset introduced by Agarwal et al., comprising the replay experience of a DQN agent across Atari games [12]. This dataset contains approximately 50 million trajectories, each composed of (state, action, reward, next state) tuples. States are represented as 84\u00d784 pixel frame stacks of four consecutive images, providing temporal context for the agent's decision-making process. The action space and reward structure depend on the specific game mechanics."}, {"title": "3.1.1 Data Processing", "content": "The first step of implementing Decision Transformer and Decision Mamba is the preparation of input data. This process involves transforming raw game experiences into a format suitable for sequence modelling. Basically, we followed the method used in the original Decision Transformer paper [1].\nThe primary innovation in data representation for these models is the use of return-to-go (RTG) instead of immediate rewards. Return-to-go at any given timestep is defined as the sum of all future rewards from that point until the end of the episode [1, 2]. For a timestep t in an episode of length T, the return-to-go $R_t$ is calculated as:\n\\begin{equation}\nR_t = \\sum_{t'=t}^{T}r(t')\n\\end{equation}\nwhere r(t') is the reward at timestep t'.\nIn the data preparation process, each trajectory is represented as a sequence of return-to-go values, states, and actions. The sequence takes the form:\n\\begin{equation}\n(R_1, s_1, a_1, R_2, s_2, a_2, \u2026, R_T, s_T, a_T)\n\\end{equation}\nwhere $s_t$ represents the state at timestep t, and $a_t$ is the action taken at that timestep t.\nWith data prepared, the training set is then created by sampling trajectories from the DQN-replay dataset. To reduce computational overhead, we followed the approach of Chen et al., that is, for each game, randomly sampling 1% of the total gameplay experience [1]. Consequently, the model is trained offline using this pre-collected gameplay experience, allowing for efficient training without requiring real-time interaction with the game environment. Following the offline training, we conduct the online evaluation. During evaluation, we do not use a separate test set from the dataset. Instead, we evaluate the trained model directly in the Atari game environment using the Arcade Learning Environment (ALE), which is specifically focused on Atari games and"}, {"title": "3.2 Model Architecture", "content": "The architecture of the Decision Transformer, as illustrated in Figure 2 (left), consists of several key components. The input to the model is a sequence of tokens representing returns-to-go, states, and actions. These inputs are first processed through embedding layers and combined with positional embeddings to provide temporal context. The core of the model is a stack of transformer layers, each containing a causal self-attention mechanism and a feedforward neural network. This structure allows the model to capture complex dependencies between different elements of the input sequence [1].\nDecision Mamba builds on the Decision Transformer by replacing the causal self-attention mechanism with the Mamba block. As shown in Figure 2 (right), Decision Mamba's overall structure is similar to the Decision Transformer, with the key difference being the replacement of the attention mechanism [2].\nBoth DT and DM operate on similar principles when it comes to training and inference."}, {"title": "3.3 Experimental Setup", "content": "Our experimental design aimed to evaluate and compare the performance of Decision Transformer (DT) and Decision Mamba (DM) across various Atari games. We initially selected four games: Breakout, Qbert, Hero, and KungFuMaster. Breakout and Qbert were chosen as they were previously examined in both the Decision Transformer and Decision Mamba papers, providing a baseline for comparison. Hero and KungFuMaster were added to expand the scope of our analysis, as they potentially present greater challenges for the models due to their more complex action spaces and game dynamics.\nTo ensure a comprehensive evaluation, we conducted experiments with different context lengths: 10, 30, and 50, where 30 is the default value in the original Decision Transformer paper. For all experiments, we maintained consistency in hyperparameters across both models, the detailed settings of all hyperparameters can be found in Appendix A.\nIn the extended experiments, we broadened our game selection to include eight additional Atari games, bringing the total to twelve, which aims to provide a more complete view of how DT and DM perform across a wider range of game characteristics and complexities. For these additional experiments, we focused on the context length of 10 for computational efficiency while still providing valuable insights."}, {"title": "3.3.1 Loss function", "content": "Atari games, which are considered discrete action control problems, differ from continuous control tasks like those in robotics. The Atari games have a fixed set of possible actions such as UP, Down, Left, Right, and Fire. For these discrete action predictions, the cross-entropy loss is particularly well-suited and widely used in sequence modelling [1]. The equation of cross-entropy loss can be defined as:\n\\begin{equation}\nL=\\sum_{a=0}^{M}y_{s,a} \\log(p_{s,a})\n\\end{equation}\nwhere M is the number of actions, y is 1 or 0, depends on if action label a is correct or not for state s, and p is the predicted probability of action a at state s."}, {"title": "3.3.2 Evaluation", "content": "The model is trained offline using the DQN-replay dataset and subsequently assessed online using the Arcade Learning Environment (ALE) [13]. This methodology accelerates the training process by avoiding continuous interaction with the game environment. Furthermore, in the original paper of Decision Transformer, it has been shown to achieve results comparable to online learning [1]."}, {"title": "4 Experiments and Analysis", "content": ""}, {"title": "4.1 Initial Experiments", "content": "The experimental design aims to compare the performance of Decision Transformer (DT) and Decision Mamba (DM) across four Atari games: Breakout, Qbert, Hero, and Kung Fu Master . These games were chosen to represent a range of complexities and challenges, allowing for an evaluation of the models' capabilities.\nOur experiments also explored the effect of different context lengths (10, 30, and 50), with 30 being the default setting in the original Decision Transformer paper."}, {"title": "4.2 Extended Experiments and Analysis", "content": "While the initial experiments revealed interesting performance differences between Decision Transformer (DT) and Decision Mamba (DM) across four Atari games, we recognized the need for a more comprehensive analysis to understand the factors influencing these disparities. Therefore, we expanded our analysis to a broader range of Atari games. We added eight new games, carefully selected to represent a variety of action space complexities. This expanded dataset allows us to examine how the performance difference between DT and DM varies across a more diverse set of game environments. To maintain computational efficiency, we conducted these additional experiments using a context length of 10.\nTo better understand these performance differences, we conducted an in-depth analysis of various game characteristics. We report the number of actions, average trajectory length, and average steps to the first non-zero reward for each game as before. To further capture the visual features of each game, we introduced three additional metrics: Image Entropy, Compression Ratio, and Feature Count:\n\u2022 Image Entropy\nImage Entropy is a measure of randomness or unpredictability in the image, by quantifying the amount of information contained in an image [16]. Higher entropy generally indicates more complex, information-rich images. It is calculated using the Shannon entropy formula [17]:\n\\begin{equation}\nH = - \\sum(P_i \\log_2(P_i))\n\\end{equation}\nwhere $p_i$ is the probability of pixel intensity i occurring in the image. We used OpenCV's calcHist function to compute each frame's pixel intensities histogram. The histogram was"}, {"title": "Compression Ratio", "content": "Compression Ratio is the ratio of the uncompressed size to the compressed size of data [18]. Calculated as:\n\\begin{equation}\nCompression Ratio = \\frac{Uncompressed Size}{Compressed Size}\n\\end{equation}\nMore complex images are typically less compressible, resulting in lower compression ratios, while a simple image will have a larger ratio. In our implementation, we use zlib which applies the DEFLATE algorithm to ensure lossless compression and preserve all original data [19]."}, {"title": "4.2.1 Regression Analysis with Random Forest", "content": "To quantify the relative importance of various game characteristics in determining performance differences between DT and DM, we employed a random forest regression analysis [21]. By using the game metrics from table 6 as input Xand the difference in performance between DT and DM (DT norm score DM norm score) as the target variable y. To ensure robust results given our limited sample size of 12 games, we applied a 6-fold cross validation. The process starts by initializing the random forest regressor and setting up cross-validation. It then iterates through the folds, training the model on each subset and calculating the Mean Squared Error (MSE). After all folds are completed, the overall Root Mean Square Error (RMSE) and its standard deviation are calculated. Finally, the model is fitted to the entire dataset and the feature importance score is calculated. After performing the random forest regression, we get a root mean square error (RMSE) of 17.57 and a standard deviation of 12.35."}, {"title": "4.2.2 Correlation Analysis", "content": "To further investigate the relationships between game characteristics and performance differences, we conducted a correlation analysis. This analysis provides insights into the linear relationships between variables, complementing the non-linear insights gained from the Random Forest regression.\nSimilar to random forest regression, the performance difference for each game was calculated as the normalized score of Decision Transformer (DT) minus the normalized score of Decision Mamba (DM). A positive value indicates better performance by DT, while a negative value suggests DM outperformed DT. We then computed the Pearson correlation coefficient between this performance difference and each game metric, as well as between the metrics themselves [23]. The Pearson correlation coefficient r between two variables X and Y is defined as:\n\\begin{equation}\nr = \\frac{\\sum_{i=1}^{n}(X_i \u2013 \\bar{X})(Y_i \u2013 \\bar{Y})}{\\sqrt{\\sum_{i=1}^{n}(X_i - \\bar{X})^2} \\sqrt{\\sum_{i=1}^{n}(Y_i - \\bar{Y})^2}}\n\\end{equation}\nwhere $X_i$ and $Y_i$ are the individual game metrics and the performance difference indexed with i. The coefficient r ranges from -1 to 1, where -1 indicates a perfect negative linear relationship, 0 indicates no linear relationship, and 1 indicates a perfect positive linear relationship [23].\nThe resulting correlation matrix is presented in Figure 5. In interpreting these results, we follow the guidelines where absolute values of correlation coefficients are categorized as: 0.00-0.10 \"negligible\", 0.10-0.39 \u201cweak\", 0.40-0.69 \"moderate\", 0.70-0.89 \"strong\", and 0.90-1.00 \"very strong\" [24].\nExamining the correlations with the performance difference, we observe several noteworthy relationships. The number of actions shows a moderate positive correlation (0.43) with the performance difference, which aligns with our earlier observation that DT tends to outperform DM in games with more complex action spaces. This correlation supports the hypothesis that action space complexity is indeed a significant factor in determining the relative performance of these two models. Image entropy exhibits a weak positive correlation (0.30) with the performance difference. This suggests that as the visual complexity of the game increases, there is a slight tendency for DT to perform better relative to DM. The compression ratio, which was identified as the second most important feature in our random forest analysis, shows a weak negative correlation (-0.28) with the performance difference. This negative correlation implies that as games become more compressible (i.e., visually simpler), there is a tendency for DM to outperform DT. This finding is consistent with our earlier observation that DM seems to excel in visually simpler environments. The average trajectory length shows weak positive correlations (0.29) with the performance difference. This suggests that DT might have a slight advantage in games with longer episodes. The average steps to the first non-zero reward and the feature count show a very weak positive correlation (0.15 and 0.12 respectively) with the performance difference, suggesting that these metrics may not significantly influence the relative performance of DT and DM.\nIt's also worth noting the correlations between the game metrics themselves. For instance, the number of actions shows a strong positive correlation (0.68) with image entropy, suggesting that games with more complex action spaces tend to have higher visual complexity as well."}, {"title": "4.3 Analysis of the Effect of Action Space Complexity", "content": "The Random Forest and correlation analyses provided insights into the factors influencing the performance differences between DT and DM. While these analyses highlighted the importance of both action space complexity and visual complexity, we recognized the potential for further investigation into the specific impact of action space complexity. To isolate and investigate the effect of action space complexity in these two games: Hero and Kung Fu Master, we explored a method to simplify the action space while preserving the full range of possible actions during evaluation. This led us to implement Action Fusion."}, {"title": "4.3.1 Action Fusion", "content": "Action fusion is an approach that allows for the combination of primitive actions into fused actions, allowing the agent to perform tasks like firing and moving simultaneously in a single timestep. This method preserves the full range of possible actions during evaluation while simplifying the decision-making process during training and prediction.\nTo implement action fusion, we explored two methods. The first, Simple Action Fusion, combines basic movement actions with the fire action into single composite actions. The second method, Frequency-based Action Fusion, focuses on combining actions that occur less frequently in the training data. This strategy is designed to minimize the impact on essential game mechanics while still reducing the complexity of the action space.\nSimple Action Fusion As shown in Figure 6. Simple action fusion keeps NOOP and FIRE unfused, then fuse move actions and fire actions into action combinations, therefore the agent is still capable of performing the original full actions during evaluation, training and making predictions in reduced actions space. Consequently, the action space complexity for Hero is reduced from 18 to 10, and for Kung Fu Master from 14 to 9."}, {"title": "4.3.2 Results of Action Fusion", "content": "Table 7 and Table 8 present the original results and the outcomes of both action fusion methods, respectively. To directly compare the original and action-fused results, we maintain consistent values (Random Walk scores and Human Players soirees) when calculating normalized scores. Although action fusion simplifies the game's action space, it preserves the core game mechanics and objectives. The fused actions still enable the agent to perform all original game actions, with the reward assignment mechanism controlled by ALE remaining unchanged. While this method may introduce slight bias, it ensures comparability across experiments.\nIn the game Hero, the normalized score of Decision Transformer dropped from the original 30.37 to 18.72 for Simple Action Fusion and 16.06 for Frequency-based Action Fusion, while the Decision Mamba's performance in Hero remained relatively stable, with only slight variations across different fusion strategies. For KungFuMaster, both DT and DM experienced significant performance drops with action fusion, but the relative performance between the two models shifted. In the original setup, DT substantially outperformed DM (29.41 vs 5.29). However, with action fusion applied, DM slightly edged out DT in both fusion strategies.\nThe results suggest that simplifying the action space, even when preserving the full range of actions during evaluation, impacts the models' ability to learn optimal strategies. The performance drops observed with both fusion methods, particularly for DT, indicate that action space complexity is indeed a significant factor in model performance. However, the persistent performance gaps between DT and Decision Mamba (DM), especially in games like Hero, suggest that action space complexity alone cannot fully account for the disparities. This finding points to the need for more investigation into other potential factors influencing performance. Given that both models process game frames as input, visual complexity emerges as a promising avenue for further exploration. Consequently, we propose a new hypothesis: the performance gap between DT and DM in games like Hero and KungFuMaster is likely influenced by multiple factors, with action space complexity being just one component of a more complex interplay of game characteristics."}, {"title": "5 Future Work", "content": "The above analysis extends our understanding of the relationship between game characteristics and performance differences in Decision Transformer (DT) and Decision Mamba (DM). The analysis confirms the importance of action space complexity, it also reveals the key role of visual complexity in it. These findings emphasise the need for further research. One potential direction is to investigate the fundamental mechanisms in visual processing capabilities. This may need a theoretical analysis of attention matrices in DT and state space representations in DM of games with different visual complexities. Moreover, future research could explore hybrid architectures that combine the advantages of DT and DM, potentially yielding more robust performance in a wider range of environments."}, {"title": "6 Conclusion", "content": "In conclusion, this study has conducted a comprehensive evaluation of Decision Transformer (DT) and Decision Mamba (DM) across selected 12 Atari games. This research started by noting the differences in performance between DT and DM in the games Hero and Kung Fu Master, leading to a systematic analysis of the game characteristics influencing these discrepancies. The random forest regression analysis highlights the importance of action space complexity, as well as visual complexity, by calculating the feature importance and SHAP values. Our correlation analysis also supports these findings, showing a moderate positive correlation between performance differences and game characteristics such as the number of actions and some weak correlation with image entropy. In addition, to further investigate the effect of action space complexity, we implemented action fusion, including simple action fusion and frequency-based action fusion, further highlighting the important role of action space complexity. Overall, our analysis suggests that action space complexity and visual complexity are the main factors that influence the performance gap between Decision Transformer and Decision Mamba. DM tended to perform better with simpler action space and visual environments, and DT showed an advantage in games with complicated actions and visual components. For future research, it can focus on theoretical analysis of the attention patterns in DT and the state space representations in DM. Alternatively, researchers can focus on developing hybrid models that combine the advantages of DT and DM to improve the model's ability to handle complex visual elements."}]}