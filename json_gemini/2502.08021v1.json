{"title": "Model Selection for Off-policy Evaluation: New Algorithms and Experimental Protocol", "authors": ["Pai Liu", "Lingfeng Zhao", "Shivangi Agarwal", "Jinghan Liu", "Audrey Huang", "Philip Amortila", "Nan Jiang"], "abstract": "Holdout validation and hyperparameter tuning from data is a long-standing problem in offline reinforcement learning (RL). A standard framework is to use off-policy evaluation (OPE) methods to evaluate and select between different policies, but OPE methods either incur exponential variance (e.g., importance sampling) or have hyperparameters of their own (e.g., FQE and model-based). We focus on model selection for OPE itself, which is even more under-investigated. Concretely, we select among candidate value functions (\"model-free\") or dynamics (\u201cmodel-based\") to best assess the performance of a target policy. Our contributions are two fold. We develop: (1) new model-free and model-based selectors with theoretical guarantees, and (2) a new experimental protocol for empirically evaluating them. Compared to the model-free protocol in prior works, our new protocol allows for more stable generation and better control of candidate value functions in an optimization-free manner, and evaluation of model-free and model-based methods alike. We exemplify the protocol on a Gym environment, and find that our new model-free selector, LSTD-Tournament, demonstrates promising empirical performance.", "sections": [{"title": "1. Introduction", "content": "Offline reinforcement learning (RL) is a promising paradigm for applying RL to important application domains where perfect simulators are not available and we must learn from data [LKTF20; JX24]. Despite the significant progress made in devising more performant training algorithms, how to perform holdout validation and model selection-an indispensable component of any practical machine learning pipeline-remains an open problem and has hindered the deployment of RL in real-life scenarios. Concretely, after multiple training algorithms (or instances of the same algorithm with different hyperparameter settings) have produced candidate policies, the primary task (which contrasts the secondary task which we focus on) is to select a good policy from these candidates, much like how we select a good classifier/regressor in supervised learning. To do so, we may estimate the performance (i.e., expected return) of each policy, and select the one with the highest estimated return.\nUnfortunately, estimating the performance of a new target policy based on data collected from a different behavior policy is a highly challenging task, known as off-policy evaluation (OPE). Popular OPE algorithms can be roughly divided into two categories, each with their own critical weaknesses: the first is importance sampling [PSS00; JL16; TB16], which has elegant unbiasedness guarantees but suffers variance that is exponential in the horizon, limiting applicability beyond short-horizon settings such as contextual bandits [LCLW11]. The second category includes algorithms such as Fitted-Q Evaluation (FQE) [EGW05; LVY19; Pai+20], marginalized importance sampling [LLTZ18; NCDL19; UHJ20], and model-based approaches [VJY21], which avoid the exponential variance; unfortunately, this comes at the cost of introducing their own hyperparameters (choice of neural architecture, learning rates, etc.). While prior works have reported the effectiveness of these methods [Pai+20], they also leave a chicken-and-egg problem: if these algorithms tune the hyperparameters of training, who tunes their hyperparameters?\nIn this work, we make progress on this latter problem, namely model selection for OPE algorithms themselves, in multiple dimensions. Concretely, we consider two settings: in the model-based setting, evaluation algorithms build dynamics models to evaluate a target policy. Given the uncertainty of hyperparameters in model building, we assume that multiple candidate models are given, and the task is to select one that we believe evaluates the performance of the target policy most accurately. In the model-free setting, evaluation algorithms only output value functions. Similar to above, the task is to select a value function out of the candidate value functions."}, {"title": "2. Preliminaries", "content": "Markov Decision Process (MDP). An MDP is specified by $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma, d_0)$, where $\\mathcal{S}$ is the state space, $\\mathcal{A}$ is the action space, $P : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{S})$ is the transition dynamics, $R : \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, R_{\\text{max}}]$ is the reward function, $\\gamma\\in [0, 1)$ is the discount factor, and $d_0$ is the initial state distribution. A policy $\\pi : \\mathcal{S} \\rightarrow \\Delta(\\mathcal{A})$ induces a distribution over random trajectories, generated as $s_0 \\sim d_0$, $a_t \\sim \\pi(\\cdot|s_t)$, $r_t = R(s_t, a_t)$, $s_{t+1} \\sim P(\\cdot|s_t, a_t)$, $\\forall t \\geq 1$. We use $\\text{Pr}^\\pi[\\cdot]$ and $\\mathbb{E}^\\pi[\\cdot]$ to denote such a distribution and the expectation thereof. The performance of a policy is defined as $J(\\pi) := \\mathbb{E}^\\pi[\\sum_{t=0}^\\infty \\gamma^t r_t]$, which is in the range of $[0, V_{\\text{max}}]$ where $V_{\\text{max}} := R_{\\text{max}}/(1 - \\gamma)$.\nValue Function and Bellman Operator. The Q-function $Q^\\pi \\in \\mathbb{R}^{\\mathcal{S} \\times \\mathcal{A}}$ is the fixed point of $T^\\pi : \\mathbb{R}^{\\mathcal{S} \\times \\mathcal{A}} \\rightarrow \\mathbb{R}^{\\mathcal{S} \\times \\mathcal{A}}$, i.e., $Q^\\pi = T^\\pi Q^\\pi$, where for any $f \\in \\mathbb{R}^{\\mathcal{S} \\times \\mathcal{A}}$, $(T^\\pi f)(s,a) := R(s, a) + \\gamma \\mathbb{E}_{s'\\sim P(\\cdot|s,a)} [f(s', \\pi)]$. We use the shorthand $f(s', \\pi)$ for $\\mathbb{E}_{a'\\sim\\pi(\\cdot|s')} [f(s', a')]$.\nOff-policy Evaluation (OPE). OPE is about estimating the performance of a given target policy $\\pi$ in the real environment denoted as $\\mathcal{M}^* = (\\mathcal{S}, \\mathcal{A}, P^*, R, \\gamma, d_0)$, namely $J_{\\mathcal{M}^*}(\\pi)$, using an offline dataset $\\mathcal{D}$ sampled from a behavior policy $\\pi_b$. For simplicity, from now on we may drop the $\\mathcal{M}^*$ in the subscript when referring to properties of $\\mathcal{M}^*$, e.g., $J(\\pi) = J_{\\mathcal{M}^*}(\\pi)$, $Q^\\pi = Q_{\\mathcal{M}^*}^\\pi$, etc. As a standard simplification, our theoretical derivation assumes that the dataset $\\mathcal{D}$ consists of $n$ i.i.d. tuples $(s, a, r, s')$ generated as $(s, a) \\sim \\mu$, $r = R(s,a)$, $s' \\sim P^*(\\cdot|s, a)$. We use $\\mathbb{E}[\\cdot]$ to denote the true expectation under the data distribution, and $\\mathbb{E}_{\\mathcal{D}}[\\cdot]$ denotes the empirical approximation from $\\mathcal{D}$.\nModel Selection. We assume that there are multiple OPE instances that estimate $J(\\pi)$, and our goal is to choose among them based on the offline dataset $\\mathcal{D}$. Our setup is agnostic w.r.t. the details of the OPE instances, and view them only through the intermediate objects (dynamics model or value function) they produce. Concretely, two settings are considered:\n\u2022 Model-based: Each OPE instance produces an MDP $\\mathcal{M}_i$ and uses $J_{\\mathcal{M}_i}(\\pi)$ as an estimate of $J(\\pi)$; w.l.o.g. we assume $\\mathcal{M}_i$ only differs from $\\mathcal{M}^*$ in the transition $P_i$. The task is to select $\\mathcal{M}$ from $\\mathcal{M} := \\{\\mathcal{M}_i\\}_{i\\in[m]}$, such that $J_{\\mathcal{M}_i}(\\pi) \\approx J(\\pi)$. We assume that at least one model is close to $\\mathcal{M}^*$, and in theoretical analyses we make the simplification that $\\mathcal{M}^* \\in \\mathcal{M}$; extension to the misspecified case ($\\mathcal{M}^* \\notin \\mathcal{M}$) is routine in RL theory [AJS23; ACK24] and orthogonal to the insights of this work.\n\u2022 Model-free: Each OPE instance provides a Q-function that approximates $Q^\\pi$. The validation task is to select $Q$ from the candidate Q-functions $\\mathcal{Q} := \\{Q_i\\}_{i\\in[m]}$, such that $\\mathbb{E}_{s\\sim d_0} [Q(s, \\pi)] \\approx J(\\pi)$. Similar to the model-based case, we will assume $Q^\\pi \\in \\mathcal{Q}$ in the derivations."}, {"title": "3. New Model-free Selector", "content": "In this section we introduce our new model-free selector, LSTD-Tournament. To start, we review the difficulties in model-free selection and the idea behind BVFT [XJ21; ZJ21] which we build on."}, {"title": "3.1. Challenges of Model-free Selection and BVFT", "content": "To select $Q^\\pi$ from $\\mathcal{Q} = \\{Q_1, ..., Q_m\\}$, perhaps the most natural idea is to check how much each candidate function $Q_i$ violates the Bellman equation $Q^\\pi = T^\\pi Q^\\pi$, and choose the function that minimizes such a violation. This motivates the Bellman error (or residual) objective:\n$\\mathbb{E}_\\mu[(Q_i - T^\\pi Q_i)^2]$.\nUnfortunately, this loss cannot be estimated due to the infamous double-sampling problem [Bai95; SB18; CJ19], and the na\u00efve estimation, which squares the TD error, is a biased estimation of the Bellman error (Eq.(1)) in stochastic environments:\n$\\mathbb{E}_\\mu[(Q_i(s, a) - r - \\gamma Q_i(s', \\pi))^2]$.\nCommon approaches to debiasing this objective involves additional \"helper\" classes, which we show can be naturally induced in the model-based setting; see Section 4 for details.\nBVFT. The idea behind BVFT [XJ21] is to find an OPE algorithm for learning $Q^\\pi$ from a function class $\\mathcal{F}$, such that to achieve polynomial sample-complexity guarantees, it suffices if $\\mathcal{F}$ satisfies 2 assumptions:\n1. Realizability, that $Q^\\pi \\in \\mathcal{F}$.\n2. Some structural (as opposed to expressivity) assumption on $\\mathcal{F}$, e.g., smoothness, linearity, etc.\nStandard learning results in RL typically require stronger expressivity assumption than realizability, such as the widely adopted Bellman-completeness assumption ($T^\\pi f \\in \\mathcal{F}, \\forall f \\in \\mathcal{F}$). However, exceptions exist, and BVFT shows that they can be converted into a pairwise-comparison subroutine for selecting between two candidates $\\{Q_i, Q_j\\}$, and extension to multiple candidates can be done via a tournament procedure. Crucially, we can use $\\{Q_i, Q_j\\}$ to automatically create an $\\mathcal{F}$ needed by the algorithm without"}, {"title": "3.2. LSTD-Tournament", "content": "We now provide a theoretical analysis of LSTDQ (which is simplified from the literature [MPW23; PKBK23]), and show how to transform it into a selector via the BVFT recipe. In LSTDQ, we learn $Q^\\pi$ via linear function approximation, i.e., it is assumed that a feature map $\\phi : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}^d$ is given, such that $Q^\\pi (s, a) = \\phi(s, a)^\\top \\theta^*$, where $\\theta^* \\in \\mathbb{R}^d$ is the groundtruth linear coefficient. Equivalently, this asserts that the induced linear class, $\\mathcal{F}_\\phi := \\{\\phi^\\top \\theta : \\theta \\in \\mathbb{R}^d\\}$ satisfies realizability, $Q^\\pi \\in \\mathcal{F}$.\nLSTDQ provides a closed-form estimation of $\\theta^*$ by first estimating the following moment matrices:\n$\\Sigma := \\mathbb{E}_\\mu[\\phi(s, a)\\phi(s, a)^\\top], \\quad \\Sigma^\\pi := \\mathbb{E}_\\mu[\\phi(s,a)\\phi(s', \\pi)^\\top],$\n$A := \\Sigma - \\gamma \\Sigma^\\pi, \\quad b := \\mathbb{E}_\\mu[\\phi(s,a)r].$\nAs a simple algebraic fact, $A\\theta^* = b$. Therefore, when $A$ is invertible, we immediately have that $\\theta^* = A^{-1}b$. The LSTDQ algorithm thus simply estimates $A$ and $b$ from data, denoted as $\\hat{A}$ and $\\hat{b}$, respectively, and estimate $\\theta^*$ as $\\hat{A}^{-1}\\hat{b}$. Alternatively, for any candidate $\\theta$, $||A\\theta - b||_\\infty$ can serve as a loss function that measures the violation of the equation $A\\theta^* = b$, which we can minimize over $\\Theta$. Its finite-sample guarantee is given below. All proofs of the paper can be found in Appendix B.\nTheorem 1. Let $\\Theta \\subset \\mathbb{R}^d$ be a set of parameters such that $\\theta^* \\in \\Theta$. Assume $\\text{max}_{s,a}||\\phi(s,a)||^2 \\leq B$ and $\\text{max}_{\\theta\\in\\Theta} ||\\theta||^2 \\leq 1$. Let $\\hat{\\theta} := \\text{arg} \\text{min}_{\\theta\\in\\Theta} || A\\theta -b||_\\infty$. Then, with probability at least $1 - \\delta$,\n$||Q^\\pi - \\phi^\\top \\hat{\\theta}||_\\infty \\leq \\frac{6 \\text{max}\\{R_{\\text{max}}, B\\}^2}{\\sigma_{\\text{min}}(A)} \\sqrt{\\frac{d \\text{log}(2d|\\Theta|/\\delta)}{n}}$,\nwhere $\\sigma_{\\text{min}}(\\cdot)$ is the smallest singular value."}, {"title": "4. Model-based Selectors", "content": "We now turn to the model-based setting, i.e., choosing a model from $\\{\\mathcal{M}_i\\}_{i\\in[m]}$. This is a practical scenario when we have structural knowledge of the system dynamics and can build reasonable simulators, but simulators of complex real-world systems will likely have many design choices and knobs that cannot be set from prior knowledge alone. In some sense, the task is not very different from system identification in control and model learning in model-based RL, except that (1) we focus on a finite and small number of plausible models, instead of a rich and continuous hypothesis class, and (2) the ultimate goal is to perform accurate OPE, and learning the model is only an intermediate step.\nExisting Methods. Given the close relation to model learning, a natural approach is to simply minimize the model prediction loss [Jia24]: a candidate model $\\mathcal{M}$ is scored by\n$\\mathbb{E}_{(s,a,s')\\sim \\mu, \\tilde{s} \\sim P(\\cdot|s,a)}[d(s', \\tilde{s})]$,\nwhere $s'$ is in the data and generated according to the real dynamics $P^* (.|s, a)$, and $\\tilde{s}$ is generated from the candidate model $\\mathcal{M}$'s dynamics $P$. $d(\\cdot, \\cdot)$ is a distance metric that measures the difference between states.\nDespite its wide use and simplicity [NKFL18], the method has major caveats: first, the distance metric $d(\\cdot, \\cdot)$ is a design"}, {"title": "4.1. Regression-based Selector", "content": "Recall that the difficulty in estimating the Bellman error $\\mathbb{E}_\\mu[(Q_i - T^\\pi Q_i)^2]$ is the uncertainty in $T^\\pi$. To overcome this, we leverage the following observation from [ASM08], where for any $f : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$,\n$T^\\pi f \\in \\underset{g:\\mathcal{S}\\times\\mathcal{A}\\rightarrow \\mathbb{R}}{\\text{arg} \\text{min}} \\mathbb{E}_\\mu[(g(s, a) - r - \\gamma f(s', \\pi))^2]$,\nwhich shows that we can estimate $T^\\pi Q_i$ by solving a sample-based version of the above regression problem with $f = Q_i$. Statistically, however, we cannot afford to minimize the objective over all possible functions $g$; we can only search over a limited set $\\mathcal{G}_i$ that ideally captures the target $T^\\pi Q_i$. Crucially, in the model-based setting we can generate such a set directly from the candidates $\\{\\mathcal{M}_i\\}_{i\\in[m]}$:\nProposition 3. Let $\\mathcal{G}_i := \\{T_{\\mathcal{M}_j}Q_i : j \\in [m]\\}$. Then if $\\mathcal{M}^* \\in \\{\\mathcal{M}_i\\}_{i\\in[m]}$, it follows that $T Q_i = T^{\\pi_*}Q_i \\in \\mathcal{G}_i$."}, {"title": "4.2. Sign-flip Average Bellman Error", "content": "We now present another selector that leverages the information of $\\mathcal{G}_i = \\{T_{\\mathcal{M}_j}Q_i : j \\in [m]\\}$ in a different manner. Instead of measuring the squared Bellman error, we can also measure the absolute error, which can be written as (some $(s, a)$ argument to functions are omitted):\n$\\mathbb{E}_\\mu[|Q_i - T^\\pi Q_i |]$\n$= \\mathbb{E}_\\mu[\\text{sgn}(Q_i(s, a) - (T^\\pi Q_i)(s, a))(Q_i - T^\\pi Q_i)]$\n$= \\mathbb{E}_\\mu[\\text{sgn}(Q_i - T^\\pi Q_i)(Q_i(s, a) - r - \\gamma Q_i(s', \\pi))]$\n$\\leq \\underset{g\\in\\mathcal{G}_i}{\\text{max}} \\mathbb{E}_\\mu[\\text{sgn}(Q_i - g)(Q_i(s, a) - r - \\gamma Q_i(s', \\pi))].$\nHere, the $\\mathcal{G}_i$ from Proposition 3 induces a set of sign functions $\\text{sgn}(Q_i - g)$, which includes $Q_i - T^\\pi Q_i$, that will negate any negative TD errors. The guarantee is as follows:\nTheorem 5. Let $Q_i$ be the minimizer of the empirical estimate of Eq.(8), and $C := \\text{max}_{s,a} \\frac{d^\\pi (s,a)}{\\mu(s,a)}$. W.p. $\\geq 1 - \\delta$,\n$J(\\pi) - \\mathbb{E}_{d_0} [Q(s, \\pi)] \\leq 4 \\cdot C_\\mathcal{V} \\frac{V_{\\text{max}}}{\\sqrt{n}} \\text{log}(2m/\\delta)$"}, {"title": "5. A Model-based Experiment Protocol", "content": "Given the new selectors, we would like to evaluate and compare them empirically. However, as alluded to in the introduction, current experiment protocols have various caveats and make it difficult to evaluate the estimators in well-controlled settings. In this section, we describe a novel model-based experiment protocol, which can be used to evaluate both model-based and model-free selectors."}, {"title": "5.1. The Protocol", "content": "Our protocol consists of experiment units defined by the following elements:\n1. Groundtruth model $\\mathcal{M}^*$.\n2. Candidate model list $\\mathcal{M} = \\{\\mathcal{M}_i\\}_{i\\in[m]}$.\n3. Behavior policy $\\pi_b$ and offline sample size n.\n4. Target policies $\\Pi = \\{\\pi_1,..., \\pi_l\\}$.\nGiven the specification of a unit, we will draw a dataset of size n from $\\mathcal{M}^*$ using behavior policy $\\pi_b$. For each target policy $\\pi\\in \\Pi$, we apply different selectors to choose a model $\\mathcal{M\\in \\mathcal{M}$ to evaluate $\\pi$. Model-free algorithms will access $\\mathcal{M}$ only through its Q-function, $Q_{\\mathcal{M}}$, effectively choosing from the set $\\mathcal{Q} = \\{Q_{\\mathcal{M}} : \\mathcal{M} \\in \\mathcal{M}\\}$. Finally, the prediction error $|J_{\\mathcal{M}}(\\pi) - J_{\\mathcal{M}^*}(\\pi)|$ is recorded and averaged over the target policies in $\\Pi$. Moreover, we may gather results from multiple units that share the same $\\mathcal{M}^*$ but differ in $\\mathcal{M}$ and/or the behavior policy to investigate issues such as robustness to misspecification and data coverage, as we will demonstrate in the next section.\nLazy Evaluation of Q-values via Monte Carlo. While the pipeline is conceptually straightforward, practically accessing the Q-function $Q_{\\mathcal{M}}$ is nontrivial: we could run TD-style algorithms in $\\mathcal{M}$ to learn $Q_{\\mathcal{M}}$, but that invokes a separate RL algorithm that may require additional tuning and verification, and it can be difficult to control the quality of the learned function.\nOur innovation here is to note that, for all the model-free algorithms we are interested in evaluating, they all access $Q_{\\mathcal{M}}$ exclusively through the value of $Q_{\\mathcal{M}}(s, a)$ and $Q_{\\mathcal{M}}(s', \\pi)$ for $(s, a, r, s')$ in the offline dataset $\\mathcal{D}$. That is, given n data points in $\\mathcal{D}$, we only need to know $2n$ scalar values about $Q_{\\mathcal{M}}$. Therefore, we propose to directly compute these values without explicitly representing $Q_{\\mathcal{M}}$, and each value can be easily estimated by averaging over multiple Monte-Carlo rollouts, i.e., $Q_{\\mathcal{M}}(s, a) = \\mathbb{E}^\\pi[\\sum_{\\tau=0}^\\infty \\gamma^\\tau r_\\tau|s_0 = s, a_0 = a]$ can be approximated by rolling out multiple trajectories starting from $(s, a)$ and taking actions according to $\\pi$.\nMoreover, for the model-based estimators proposed in Section 4, we need access to quantities in the form of $(T_{\\mathcal{M}_j}Q_{\\mathcal{M}_i})(s, a)$. This value can also be obtained by Monte-Carlo simulation: (1) start in $(s, a)$ and simulate one step in $\\mathcal{M}_j$, then (2) switch to $\\mathcal{M}_i$, simulate from step 2 onwards and rollout the rest of the trajectory."}, {"title": "5.2. Computational Efficiency", "content": "Despite not involving neural-net optimization, the experiment can still be computationally intensive due to rolling out a large number of trajectories. In our code, we incorporate the following measures to reduce the computational cost:\nQ-caching. The most intensive part of the pipeline is to roll-out Monte-Carlo trajectories for Q-value estimation. In contrast, the cost of running the actual selection algorithms is often much lower and negligible. Therefore, we generate these Monte-Carlo Q-estimates and save them to files, and retrieve them during the selection period. This makes it efficient to experiment with new selection algorithms or add extra baselines, and also enables fast experiment that involves a subset of the candidate models (see Section 6.2).\nBootstrapping. To account for the randomness due to $\\mathcal{D}$, we use bootstrapping to sample (with replacement) multiple datasets and run the algorithms on each dataset, and report the mean performance across these bootstrapped samples with 95% confidence intervals. Using bootstrapping maximally reuses the cached Q-values and avoids the high computational costs of sampling multiple datasets and performing Q-caching in each of them, which is unavoidable if we were to repeat each experiment verbatim multiple times."}, {"title": "6. Exemplification of the Protocol", "content": "In this section we instantiate our protocol in the Gym Hopper environment to demonstrate its utility, while also providing preliminary empirical results for our algorithms."}, {"title": "6.1. Experiment Setup and Main Results", "content": "Our experiments will be based on the Hopper-v4 environment. To create a variety of environments, we add different levels of stochastic noise in the transitions and change the gravity constant (see Appendix C.1). Each environment is then parameterized by the gravity constant g and noise level n. We consider arrays of such environments as the set of candidate simulator $\\mathcal{M}$: in most of our results, we consider a \"gravity grid\" (denoted using MF.G in the figures) $\\mathcal{M}_g := \\{\\mathcal{M}_0,..., \\mathcal{M}_{14}\\}$ (fixed noise level, varying gravity constants from -51 to -9) and a \"noise grid\" (MF.N) $\\mathcal{M}_n := \\{\\mathcal{M}_0,..., \\mathcal{M}_{14}\\}$ (fixed gravity constant, varying noise level from 10 to 100). Each array contains 15 environments, though some subsequent results may only involve"}, {"title": "6.2. Subgrid Studies: Gaps and Misspecifications", "content": "We now demonstrate how to extract additional insights from the Q-values cached earlier. Due to space limit we are only able to show representative results in Figure 4, and more"}, {"title": "6.3. Data Coverage", "content": "In the previous subsection, we have seen how multiple experiment units that only differ in $\\mathcal{M}$ can provide useful insights. Here we show that we can also probe the methods' sensitivity to data coverage by looking at experiment units that only differ in the dataset $\\mathcal{D}$. In Figure 4R, we take a previous experiment setting ($\\mathcal{M}_g$) and isolate a particular target policy $\\pi$; then, we create two datasets: (1) $\\mathcal{D}_\\pi$ sampled using $\\pi$; (2) $\\mathcal{D}_{off}$ sampled using a policy that is created to be very different from the target policies and offer very little coverage (see Appendix C.2). Then, we run the algorithm with $\\lambda$ fraction of data from $\\mathcal{D}_\\pi$ combined with $(1-\\lambda)$ from $\\mathcal{D}_{off}$; as predicted by theory, most methods perform better with better coverage (large $\\lambda$), and performance degrades as $\\lambda$ goes to 0."}, {"title": "A. Other Related Works", "content": "Here we review some existing works on model selection in offline RL. Most of them are not concerned about new selection algorithms with theoretical guarantees (apart from [XJ21; ZJ21; ZDMAK23; LNPW23] which are already discussed in the main text) or experiment protocol for OPE model selection (see [VLJY19; KKKKNS23] for experiment protocol and benchmarks of OPE itself), so their focus is different and often provides insights complementary to our work. For example, [NFBJSB22] discuss data splitting in offline model selection; this is a question we avoid by assuming a fixed holdout dataset for OPE model selection. An exception is [UKNST23] who studies the model selection problem for OPE itself, but focuses on the bandit case and makes heavy use of the importance sampling estimator, which we do not consider due to the focus on long-horizon tasks.\n[FMPNG22] challenge the idea of using Bellman errors for model selection due to their surrogacy and poor correlation with actual objective; despite the valid criticisms, there are no clear alternatives that address the pain points of Bellman errors, and the poor performance is often due to lack of data coverage, which makes the task fundamentally difficult for any algorithms. We still believe that Bellman-error-like objectives (defined in a broad sense, which includes our LSTD-Tournament) are promising for model selection, and the improvement on OPE error is the right goal to pursue instead of correlation (which we know could be poor due to the surrogacy).\nAs mentioned above and demonstrated in our experiments, the lack of data coverage is a key factor that determines the difficulty of the selection tasks. [LTND22] propose feature selection algorithms for offline contextual bandits that account for the different coverage effects of candidate features. On a related note, ideas from offline RL training, such as version-space-based pessimism [XCJMA21], can also be incorporated in our method. This will unlikely improve the accuracy of OPE itself, but may be helpful if we measure performance by how OPE can eventually lead to successful selection of performant policies, which we leave for future investigation."}, {"title": "B. Proofs", "content": ""}, {"title": "B.1. Proof of Theorem 1", "content": "Proof. Define the following loss vectors,\n$l(\\theta) := A\\theta - b \\in \\mathbb{R}^d,$\n$\\hat{l}(\\theta) := \\hat{A}\\theta - \\hat{b} \\in \\mathbb{R}^d$\nand recall that we select as the estimator\n$\\hat{\\theta}:= \\underset{\\theta\\in\\Theta}{\\text{arg min}} ||\\hat{l}(\\theta) ||_\\infty$.\nSince $\\theta^* = A^{-1}b$, we can write the desired bound as a function of $l(\\theta)$ as follows,\n$|| Q^* (\\cdot) - \\phi^\\top (\\cdot)\\hat{\\theta}||_\\infty = ||\\phi^\\top (\\cdot)(\\hat{\\theta}-\\theta^*) ||_\\infty$\n$= ||\\phi^\\top (\\cdot)A^{-1}(A\\hat{\\theta} - b)||_\\infty$\n$= ||\\phi^\\top (\\cdot)A^{-1}l(\\hat{\\theta}) ||_\\infty$\n$= \\underset{s,a}{\\text{max}}|\\phi^\\top (s, a) A^{-1}l(\\hat{\\theta})|$\n$\\leq (\\underset{s,a}{\\text{max}} ||\\phi^\\top (s,a)||_{L^2}) \\cdot ||A^{-1}||_2 ||l(\\hat{\\theta}) ||_2$\n$\\leq \\sqrt{dB} ||A^{-1}||_2 \\cdot ||l(\\hat{\\theta}) ||_\\infty$\nNext, we control the $l(\\hat{\\theta})$ term. In the sequel we will establish via concentration that\n$||l(\\theta) - \\hat{l}(\\theta)||_\\infty < \\text{Estat} := 3 \\cdot \\text{max}\\{R_{\\text{max}}, B\\}^2 \\sqrt{\\frac{\\text{log}(2d|\\Theta|\\delta^{-1})}{n}}, \\quad \\forall \\theta \\in \\Theta$."}, {"title": "B.3. Proof of Theorem 4", "content": "We bound\n$J(\\pi) - \\mathbb{E}_{d_0} [Q(s, \\pi)", "a)": "n$= \\frac{1}{1-\\gamma} \\mathbb{E}_{d_0, \\pi} [Q^* (s, a) - R(s, a) - \\gamma Q^* (s', \\pi) - Q(s, a) + R(s, a) + \\gamma Q(s', \\pi)", "T^*Q^*": "s, a) - Q(s, a) + [T Q", "s,a)": "n$= \\frac{1}{1-\\gamma} \\mathbb{E}_{d_0, \\pi} [[TQ", "Q(s,a)": "n$\\leq \\frac{C^*}{1-\\gamma} \\cdot \\mathbb{E}_\\mu [|[TQ", "Q(s,a)|": "n$\\leq \\frac{C^*}{1-\\gamma} \\cdot \\mathbb{E}_\\mu [\\underset{g\\in G_Q}{\\text{max}} \\text{sgn} (Q(s, a) - g(s,a)) (Q(s, a) - r - \\gamma g(s', \\pi))", "R_{\\text{max}}": "n$\\text{sgn}(Q(s, a) - g(s, a))(Q(s, a) - r - \\gamma g(s', \\pi)) \\in [-V_{\\text{max}}, V_{\\text{max}}"}]}