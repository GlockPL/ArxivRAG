{"title": "MindBench: A Comprehensive Benchmark for Mind Map Structure Recognition and Analysis", "authors": ["Lei Chen", "Feng Yan", "Yujie Zhong", "Shaoxiang Chen", "Zequn Jie", "Lin Ma"], "abstract": "Multimodal Large Language Models (MLLM) have made significant progress in the field of document analysis. Despite this, existing benchmarks typically focus only on extracting text and simple layout information, neglecting the complex interactions between elements in structured documents such as mind maps and flowcharts. To address this issue, we introduce the new benchmark named MindBench, which not only includes meticulously constructed bilingual authentic or synthetic images, detailed annotations, evaluation metrics and baseline models, but also specifically designs five types of structured understanding and parsing tasks. These tasks include full parsing, partial parsing, position-related parsing, structured Visual Question Answering (VQA), and position-related VQA, covering key areas such as text recognition, spatial awareness, relationship discernment, and structured parsing. Extensive experimental results demonstrate the substantial potential and significant room for improvement in current models' ability to handle structured document information. We anticipate that the launch of MindBench will significantly advance research and application development in structured document analysis technology. MindBench is available at: https://miasanlei.github.io/MindBench.github.io/.", "sections": [{"title": "Introduction", "content": "The rise of Multimodal Large Language Models (MLLM) [61] has marked the pivotal turning point in the development of artificial intelligence technology. These models, by integrating multiple modalities such as text, vision, and speech, have demonstrated exceptional capabilities in understanding and generating complex content [67, 29, 27, 3, 59, 28, 8], particularly in the field of document analysis [64, 57, 58, 16, 32], where they significantly enhance the accuracy of information extraction and content comprehension. However, the benchmarks currently used to evaluate these models often focus primarily on extracting text [31, 10, 62] and simple layout information [40, 22, 21, 47, 46], such as positional relationships in tables [9, 33] and invoices [43], yet frequently overlook the complex interactions between elements in structured documents. This limitation in evaluation hinders our ability to fully understand and assess models in complex real-world scenarios."}, {"title": "Related Work", "content": "Visual Document Understanding (VDU) aims to comprehend text-rich images covering a wide range of types including documents [37, 47, 46, 36], tables [41, 5, 66], charts [23, 38, 35, 24, 49, 18], natural images [44, 45, 17], and screenshots [48, 6]. The tasks of VDU are diverse, encompassing visual question answering [37, 36, 41, 35, 48, 45], image captioning [44, 24, 49], information extraction [47, 46] and natural language inference [5]. However, the tasks of extraction and understanding for complex structured documents, such as mind maps, have not been taken into consideration. Models designed for VDU can be broadly categorized into two types: OCR-model-driven methods and OCR-free methods. OCR-model-driven methods [56, 55, 20, 50, 2, 54] use the models to integrate visual data with detected text and layout information from off-the-shelf OCR models. OCR-free methods [25, 11, 26, 34, 53] learn text-layout recognition with a high-resolution image encoder in an end-to-end manner. Both of these VDU methods require fine-tuning for specific tasks.\nMultimodal Large Language Models (MLLM) have recently been developed for general visual language understanding [67, 29, 27, 3, 59, 28, 8], leveraging the powerful language comprehension and general capabilities of Large Language Models (LLM) [4, 51, 52, 65]. These approaches utilize a common architectural paradigm that connects a visual encoder, e.g., ViT [13, 42] to a Large Language Model through a visual-to-text module, e.g., linear layers [30, 29] or a Q-Former [3]/Resampler [1]/Abstractor [59, 60] with learnable queries. To facilitate the comprehension of text-rich images by MLLMs, several research efforts [64, 57, 15] have explicitly conducted tuning instructions on visual text understanding datasets. To handle high-resolution document images, some methods [58, 19, 12, 7] employ shape-adaptive cropping modules to segment images into resolutions suitable for ViT models. Additionally, to enhance the understanding of document text and structured information, various tasks such as text reading [3, 58, 12, 7], text grounding [3, 16, 14, 19, 32], and table parsing [19, 32] have been designed. However, these tasks primarily focus on learning text recognition and simple layout information, overlooking the complex interactions among elements in structured documents. In this paper, we introduce a comprehensive benchmark called MindBench for structured document parsing and understanding. This benchmark allows for the evaluation of various capabilities of existing models, encompassing document text recognition, layout information perception, and complex interaction understanding."}, {"title": "The MindBench Dataset", "content": null}, {"title": "Data Generation", "content": "Data preparation. Given the limited availability of labeled mind map data online, we synthesize additional mind map data using a multi-step process. Firstly, we randomly sample textual content of the nodes. Then, we generate mind maps in various shapes by randomly sampling the number of nodes, node children, and depths. These structured mind maps are then rendered into images using the Graphviz tool. To ensure diversity, we incorporate various layout engines and a wide range of properties for nodes and edges. Furthermore, we randomly place 0 to multiple background images and apply Gaussian noise to bring background diversity. The synthetic examples are shown in Fig. 5. While synthesizing data, we also recognize the importance of validating the models on real-world data. Hence, we make efforts to download a limited number of mind map source files from open-source mind map websites, including XMind\u00b9, Biggerplate\u00b2, and Zhixi\u00b3."}, {"title": "Data parsing", "content": "In order to obtain unified structured annotations for training and evaluation, we parse the raw files of two types of data, preserving the textual and structural information while removing redundant information. Fig. 2 illustrates the parsing process for the crawled data. First, we use the XMind software to automate the export of PNG images and HTML tag files of the source files. The HTML file contains structured information about the mind map. Then, we employ BeautifulSoup to parse the HTML, maintaining the tree structure and relationships among nodes, and convert the mind map into a nested JSON format. In the JSON structure, the node's children were represented as a list, allowing for nested nodes. For training, we convert the JSON data into a token sequence,"}, {"title": "Task Definition", "content": "Fig. 1 illustrates five OCR-free tasks we designed, focusing on mind map structure parsing and understanding, which are elaborated in the following:\nFull parsing. As indicated by the red rectangle in Fig. 1, the task requires the model to return the full parsing results of the input mind map image, specifically the final token sequence discussed in the previous subsection. Mind map images, as depicted in Fig. 3a, often have significantly higher resolutions than typical document images, with some exceeding 10,000 pixels. This demands models capable of processing high-resolution images. However, most existing MLLMs handle only up to 1000 pixels, and even advanced models [12] supporting up to 4k pixels struggle to clearly display text in many nodes. Furthermore, higher resolution mind maps contain more information, resulting in longer structured data, which presents a significant challenge for existing models. We utilize all crawled data and the majority of the synthetic data to perform this task.\nPart parsing. This task involves returning a subgraph centered around a specific node, resulting in shorter token output. This can alleviate pressure on models that struggle with insufficient processing length. However, it also poses new challenges, requiring the model to accurately identify the central theme node from the question and return its corresponding subgraph based on a thorough understanding of the mind map structure. Additionally, this task addresses the tendency of models to parse from the beginning, similar to the rationale behind continue reading task. However, this task does not provide preceding texts but prompts only with the theme name, posing a greater challenge.\nPosition-related parsing. Similar to part parsing, this task also returns a subgraph of the mind map. The difference is that this task emphasizes spatial positioning, requiring the model to integrate capabilities in text recognition, spatial awareness, and relational parsing. Since the crawled data's exported HTML lacks coordinate information, this task is conducted on synthetic data, where we can extract the bounding boxes of each node from Graphviz source files. As in previous works [3, 19, 32], we describe the bounding box as \u201c<bbox>x1,y1,x2,y2</bbox>\", normalizing the coordinates to integers between 0 and 999.\nStructured VQA. Besides the parsing tasks, we design multiple VQA sets to enable explicit learning of the components of mind maps and their interrelationships. For instance, we craft prompts such as, \"Describe the central theme of the mind map.\" Typically, the central theme of a conventional mind map is easily identifiable, often located at the center or along the middle of an edge. However, in some layouts, such as the image in Fig. 5c, identifying the central theme is challenging. An initial misprediction of the central node can lead to subsequent structural confusion and parsing failures. Thus, explicitly retrieving the central theme is crucial. We also design VQA tasks related to node kinship and hierarchical relationships, with specific prompts provided in Appendix \u00a7 B.\""}, {"title": "Statistic", "content": "Table la displays the data downloaded from multiple websites, segmented into training and testing sets. To accurately assess the model's ability to handle mind maps of varying complexities, we select a subset of simpler data (test*) from the test set based on the crucial metric of node number, which serves as our default validation set. Our research indicates that using large mind maps with a higher number of nodes during the training phase greatly benefits structural parsing learning; therefore, our training set encompasses data of various complexities. Table 1b lists the volume of synthetic data used for each task, with the key full parsing task utilizing a larger number of samples, and all synthetic data evenly distributed between English and Chinese. It should be noted that due to the non-uniqueness of node content and the absence of coordinate information in the crawled data, we primarily use this data for full parsing tasks to ensure high data quality. In the future, part parsing and VQA tasks could also consider utilizing this data for further research.\nResolution. The sizes of images are crucial for model processing capabilities, hence we conduct a detailed analysis of the resolution distribution of the crawled data. As depicted in Fig. 3a, we present the length of the longest side of images from various sources alongside their corresponding numbers. Among these, BXMind and BMManager feature relatively low resolutions, typically ranging from 1000 to 3000 pixels, while the resolution distribution of XMind exhibits a normal distribution pattern. Notably, Zhixi has higher resolutions, usually between 7000 to 8000 pixels, posing significant challenges to existing MLLMs: when these high-resolution images are scaled down to the input resolutions of the models, the texts often become illegible. As for the synthetic data, its resolution is influenced by the layout engine and the number of nodes. During synthesis, we uniformly sample these two parameters to ensure a consistent resolution distribution across all tasks.\nToken length. Token length is another crucial metric determining the processing capabilities of models. As illustrated in Fig. 3b and Fig. 3c, we conduct a detailed analysis of the token length distribution in both crawled and synthetic data. In the crawled data, the token lengths exhibit a"}, {"title": "Experiments", "content": null}, {"title": "Experimental Setup", "content": "Model. We evaluate several visual document understanding models [39, 25, 58, 32, 12] on the proposed benchmark. The criteria to select a baseline model are as follows: models are pre-trained on an extensive corpus of OCR and document data, they can possess a sufficiently high input resolution, and is capable of handling documents of substantial length. For implementation details of each model, please consult the respective original publications. In this paper, all models use unified structure learning and perform different tasks depending on the prompt. Due to the limited quantity of the crawled data, it is up-sampled 10 times during training to balance the quantity between the two data types. Table 3 provides the comparison of model settings. We employ GPT4V [39] for two-shot inference to examine whether the existing commercial models have the capability of structural graphical parsing. We then utilize one domain-specific model Donut [25] and three large document models [58, 32, 12] for SFT on our dataset. The training details, largely in line with the original paper, can be found in Appendix \u00a7 A.\nMetric. For parsing task, following Donut, we evaluate the models using two metrics: field-level F1 score and Tree Edit Distance (TED) based accuracy. We first convert the predicted token sequence to JSON format to recover the tree structure of the graph. The F1 metric flattens the nested JSON into a non-nested format, and then calculates F1 score at each field. F1 can efficiently evaluate the extracted field information, but it cannot exactly measure the structure of the tree. The TED-based metric is appropriate for evaluating tree-structured documents. Specifically, it uses the Zhang-Shasha (ZSS) algorithm [63] to calculate the nTED between the prediction tree and the answer tree, where n represents the size of the answer tree. The accuracy based on nTED is then computed using the formula $\\max(1 - \\frac{nTED}{n}, 0)$. For VQA task, we simply evaluate the models with F1 score."}, {"title": "Comparison with SOTA MLLMS", "content": "We conduct the performance comparison of existing visual document understanding models on the MindBench benchmark, as detailed in Table 4. GPT4V exhibits mediocre performance, indicating challenges for commercial models in parsing complex structured documents such as mind maps. Donut ranks second in parsing performance, significantly outperforming UReader and TextMonkey, and closely approaching the performance of IXC2-4KHD. This underscores the advantages of domain-specific models for parsing tasks. Although MLLMs are versatile, their capability in structured document understanding is not yet exceptional. IXC2-4KHD delivers the best performance, likely due to extensive OCR data pre-training, higher resolution input, and the capability to handle longer token lengths. Additionally, we conduct evaluations on challenging test samples. There is a notable accuracy discrepancy between complex samples with over 60 nodes and simpler ones. This highlights that the capabilities of current MLLMs are still limited when it comes to analyzing complex mind maps, particularly in processing high-resolution complex graphical images and ultra-long structured document information. There is an urgent need for further improvement of MLLM technology."}, {"title": "Ablation Study", "content": null}, {"title": "Unified structure learning", "content": "We conduct ablation experiments to analyze the impact of unified structure learning, as presented in Table 6. To expedite the experiments, we use half of the data for this ablation study. Initially, we fine-tune the UReader model on 50% of the crawled data and evaluate its performance on the XMind test set as well as the synthetic test set. Due to the disparity in graph style, the model struggles on the synthetic test set. Subsequently, we introduce the full parsing task with synthetic data during training, resulting in improvements on both the XMind and synthetic test sets. This indicates that incorporating synthetic datasets can significantly aid in parsing real mind maps, even in the presence of substantial style differences. Lastly, we integrate all tasks for unified structure learning. We train the model using 50% of the full parsing task data and 50% of other task data, maintaining the same total quantity of synthetic data as in the previous experiment. It can be observed that the model continues to show improvements on the XMind test set, highlighting the effectiveness of explicitly learning inter-node relationships and spatial information for comprehensive structure parsing. However, the model's performance slightly decreases on the synthetic test set, which may be attributed to the reduced quantity of synthetic data in the full parsing task."}, {"title": "Qualitative Results", "content": "We first investigate the structured parsing capability of existing MLLMs through zero-shot inference, as depicted in Fig. 4. It is evident that GPT4V exhibits superior parsing ability. However, when confronted with closely positioned nodes, it tends to assign child nodes to incorrect parent nodes. This behavior can be attributed to the model's inclination to rely on layout information rather than inter-node interactions for determining node relationships. On the other hand, IXC2-4KHD demonstrates weaker zero-shot parsing ability. While the model comprehends the markdown format in the prompt, it can only generate flat prediction results with incomplete texts.\nNext, we present the prediction results of UReader and IXC2-4KHD tuned on the MindBench, as depicted in Fig. 5. It is evident that IXC2-4KHD outperforms UReader across all four tasks, showcasing its strengths in comprehending node interactions, spatial perception, and structure parsing. In Fig. 5b, IXC2-4KHD can successfully correlate spatial information with subgraph structure; however, it still faces challenges in parsing details, such as recognizing small text and accurately determining parent-child relationships."}, {"title": "Conclusion", "content": "In this paper, we introduce MindBench, the first comprehensive benchmark designed for structured document. MindBench stands out due to two primary features: 1) abundant structured document images with detailed annotations and evaluation metrics, providing a standardized research tool; 2) unified structure learning of five mind map understanding and parsing tasks that comprehensively assess the model's ability to text recognition, spatial awareness, relationship discernment, and structured parsing. We empirically investigate multiple visual document understanding baseline methods on the MindBench dataset. Experimental results demonstrate that there is significant room for improvement in current models' performance, particularly in handling high-resolution complex images and processing lengthy structured documents.\nFuture work. This paper primarily focuses on establishing a benchmark for structured document parsing of mind maps. Although the data sources include various styles such as tables, relationship diagrams, and posters, mind map data predominates. In the future, we aim to expand structured document parsing to encompass a wider range of graphical types, enabling the understanding of information in any graphical document."}]}