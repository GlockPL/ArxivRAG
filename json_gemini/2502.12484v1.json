{"title": "LocalEscaper: A Weakly-supervised Framework with Regional Reconstruction for Scalable Neural TSP Solvers", "authors": ["Junrui Wen", "Yifei Li", "Bart Selman", "Kun He"], "abstract": "Neural solvers have shown significant potential in solving the Traveling Salesman Problem (TSP), yet current approaches face significant challenges. Supervised learning (SL)-based solvers require large amounts of high-quality labeled data, while reinforcement learning (RL)-based solvers, though less dependent on such data, often suffer from inefficiencies. To address these limitations, we propose LocalEscaper, a novel weakly-supervised learning framework for large-scale TSP. LocalEscaper effectively combines the advantages of both SL and RL, enabling effective training on datasets with low-quality labels. To further enhance solution quality, we introduce a regional reconstruction strategy, which mitigates the problem of local optima, a common issue in existing local reconstruction methods. Additionally, we propose a linear-complexity attention mechanism that reduces computational overhead, enabling the efficient solution of large-scale TSPs without sacrificing performance. Experimental results on both synthetic and real-world datasets demonstrate that LocalEscaper outperforms existing neural solvers, achieving state-of-the-art results. Notably, it sets a new benchmark for scalability and efficiency, solving TSP instances with up to 50,000 cities.", "sections": [{"title": "1. Introduction", "content": "The Traveling Salesman Problem (TSP) is a classic combinatorial optimization (CO) problem with widespread applications in fields such as transportation (Wang & Tang, 2021), logistics (Castaneda et al., 2022), and circuit design (Alkaya & Duman, 2013). Due to its NP-hard nature, finding the optimal solution for large-scale TSP instances remains a significant challenge.\nOver the past few decades, researchers have focused on mathematical programming and heuristic methods to find locally optimal solutions (Applegate et al., 2009; Lin & Kernighan, 1973; Helsgaun, 2000). However, these methods tend to be time-consuming, making them unsuitable for real-world, large-scale applications.\nRecently, neural combinatorial optimization (NCO) methods have shown considerable promise in solving TSP (Kwon et al., 2020; Jin et al., 2023; Drakulic et al., 2024). Current NCO solvers typically rely on either supervised learning (SL) (Joshi et al., 2019; Hottung et al., 2021; Luo et al., 2023) or reinforcement learning (RL) (Bello et al., 2016; Kwon et al., 2020; Jin et al., 2023). SL-based methods require large datasets of labeled instances, where labels represent high-quality solutions typically obtained from exact solvers or heuristics. While SL-based methods are efficient in training, they depend on large amounts of labeled data, and generating high-quality labels for large-scale instances is computationally expensive. On the other hand, RL-based methods do not rely on labeled data, learning to generate solutions through reward signals. However, RL methods face issues such as sparse rewards and a tendency to get stuck in local optima (Bengio et al., 2021; Min et al., 2024), which can reduce learning efficiency.\nIn addition to these challenges, scalability is a major concern for both SL- and RL-based NCO solvers, as most solvers plateau at problem sizes of around 500 to 1,000 nodes due to memory constraints. Moreover, most NCO solvers employ Transformer network architectures (Vaswani, 2017), which exhibit quadratic computational complexity as the problem scale increases. This makes it particularly difficult to train models capable of generating high-quality solutions for large-scale TSP.\nTo address the scalability issue, a number of NCO solvers have adopted divide-and-conquer strategies based on the Partial Optimization Metaheuristic Under Special Intensification Conditions (POPMUSIC) framework (Taillard & Helsgaun, 2019), improving the solution quality by dividing the TSP tour into smaller, non-overlapping subsequences and reconstructing each subsequence independently (Cheng et al., 2023; Luo et al., 2023; Ye et al., 2024; Zheng et al., 2024). While these methods perform local search using"}, {"title": "2. Related Work", "content": "In this section, we provide literature review from the following three perspectives on NCO Solvers."}, {"title": "2.1. SL-based NCO Solvers", "content": "Supervised learning (SL)-based NCO solvers rely on large labeled datasets to train models. Early work in this domain was pioneered by Vinyals et al. (2015), who proposed the Pointer Network. Their approach used SL techniques to solve small-scale TSP problems, employing a recurrent neural network (RNN) with attention mechanisms to iteratively construct solutions. Joshi et al. (2019) advanced this by incorporating graph convolutional networks (GCNs) (Kipf & Welling, 2016) to predict the probability of each edge being part of the optimal solution. This helped the model better capture the underlying graph structure of the TSP.\nSun & Yang (2023) utilized an anisotropic graph neural network (Bresson & Laurent, 2018) to construct a diffusion model, which iteratively denoises the solution and predicts a heatmap to guide the Monte Carlo Tree Search (MCTS) algorithm. Luo et al. (2023) introduced the Light Encoder and Heavy Decoder (LEHD) model, which combines a lightweight encoder and a heavy decoder, making it particularly suitable for SL-based training. These SL-based methods have shown success for small-scale problems but still face challenges when scaling to larger TSP instances due to their reliance on large labeled datasets."}, {"title": "2.2. RL-based NCO Solvers", "content": "Reinforcement learning (RL)-based NCO solvers, unlike SL-based methods, do not require labeled instances for training. Instead, they rely on reward signals to guide learning. Kool et al. (2018) introduced a self-attention-based (Vaswani, 2017) NCO solver, trained using RL, and demonstrated that it outperforms earlier SL-based methods in solving TSP instances. Building on this, Kwon et al. (2020) introduced the POMO solver, which generates multiple trajectories for a single TSP instance by starting from different nodes. During inference, POMO augments the input data with techniques like flipping and folding, enabling the model to produce diverse solutions and select the optimal one."}, {"title": "2.3. Search-based NCO Solvers", "content": "Search-based NCO solvers typically begin with an initial feasible solution and iteratively refine it to improve the outcome. Neural network models often play a direct or indirect role in guiding this refinement process. For instance, Xin et al. (2021) and Zheng et al. (2023) integrated SL and RL into the classical heuristic Lin-Kernighan-Helsgaun (LKH) solver (Helsgaun, 2000; 2009; 2017), improving its efficiency by using learning-based methods to construct candidate node sets.\nSome search-based approaches leverage MCTS to enhance solution quality (Fu et al., 2021; Qiu et al., 2022; Sun & Yang, 2023; Xia et al., 2024). These methods typically use GCNs to generate heatmaps that guide MCTS in finding better results. However, they are heavily reliant on MCTS, and constructing solutions greedily based solely on these heatmaps often leads to suboptimal performance. Additionally, MCTS is computationally expensive, making it difficult to scale for large TSP instances.\nOther search-based solvers use neural-based heuristics to directly refine solutions. For instance, Luo et al. (2023) applied subsequence reconstruction on top of a constructive model to improve solutions. They also employed this approach in a self-improved learning solver (Luo et al., 2024), where the model is trained on a dataset with low-quality labels and then refines these labels using subsequence reconstruction. Cheng et al. (2023) and Ye et al. (2024) generated initial solutions using random insertion and applied subsequence reconstruction via RL-based models to enhance them. However, subsequence reconstruction methods have limited ability to modify the global relationships between nodes, often leading to the solver getting stuck in local optima. This limitation is a significant challenge for search-based NCO solvers and motivates our proposed approach, which aims to improve solution quality by addressing global relationships and escaping local optima."}, {"title": "3. Preliminaries", "content": ""}, {"title": "3.1. The TSP Problem", "content": "The focus of current NCO research is on the classic 2D Euclidean Traveling Salesman Problem (TSP). In this problem, we are given an undirected, fully connected graph G(V, E) with n nodes. The node set V = {v_i|1 \\leq i \\leq n} represents the cities and the edge set E = {e_{ij}|1 < i, j < n} represents all possible connections between the cities. The cost, denoted as cost(v_i, v_j), represents the Euclidean distance between nodes v_i and v_j.\nThe objective of the TSP is to find a Hamiltonian circuit (or tour) \\mathcal{T} = (\\tau_1, \\tau_2, ..., \\tau_n), which minimizes the total cost L_{total}(\\mathcal{T}). The total cost is the sum of the Euclidean distances between consecutive nodes in the tour, as well as the distance from the last node back to the first, formulated as:\n\nL_{total}(\\mathcal{T}) = cost(\\tau_n, \\tau_1) + \\sum_{i=1}^{n-1} cost(\\tau_i, \\tau_{i+1}), \\qquad (1)\n\nwhere \\tau_n is followed by \\tau_1 to form the closed circuit."}, {"title": "3.2. Subsequence Reconstruction Task", "content": "In the subsequence reconstruction task, we are given a subsequence \\mathcal{T}' = (\\tau'_1, \\tau'_2,..., \\tau'_m) of length m, where \\tau' \\subseteq \\mathcal{T} and m \\leq n. The goal is to reorder the intermediate nodes of the subsequence to form a new sequence \\mathcal{T}'' = (\\tau''_1, \\tau''_2, ..., \\tau''_m), while keeping the endpoints fixed: \\tau''_1 = \\tau'_1 and \\tau''_m = \\tau'_m\nThe objective of subsequence reconstruction is to minimize the subsequence cost L_{sub}(\\mathcal{T}''), which is given by:\n\nL_{sub}(\\mathcal{T}'') = \\sum_{i=1}^{m-1} cost(\\tau''_i, \\tau''_{i+1}), \\qquad (2)\n\nwhere the goal is to reorder the subsequence to reduce the total cost, improving the overall tour quality."}, {"title": "3.3. Regional Reconstruction Task", "content": "The regional reconstruction task aims to improve the overall TSP solution by focusing on local regions within the tour. Given a tour \\mathcal{T}, the corresponding set of edges E_{\\mathcal{T}} = {\\tau_i, \\tau_{i+1}|1 \\leq i \\leq n - 1} \\cup {e_{\\tau_n,\\tau_1}} represents the set of connections in the tour.\nFor each city \\tau_i, we consider a 2D coordinate c = (x, y) and define the set of k-nearest neighbors \\mathcal{V}_c in graph G as the nodes closest to c. The edges in E_{\\mathcal{T}} that have nodes from \\mathcal{V}_c as their predecessors are defined as E_{\\mathcal{T}}' = {e_{\\tau_i,\\tau_{i+1}}|\\tau_i \\in \\mathcal{V}_c}. Specifically, define \\tau_{n+1} = \\tau_1 to denote the successor of \\tau_n."}, {"title": "4. Methodology", "content": "In this section, we introduce a weakly-supervised learning framework to address the challenges of large-scale TSP discussed in the Introduction. An overview of the proposed framework is presented in Figure 1. Our framework is designed to operate on a dataset consisting of three main tasks: Task 1 for dataset improvement, Task 2 for training reconstruction models, and Task 3 for training the constructive model. These tasks can be executed in parallel. For practical purposes, we adopt a sequential execution strategy where Task 1 and Task 2 alternate, while Task 3 runs continuously."}, {"title": "4.1. Weakly-supervised Learning Framework", "content": "We define the dataset as \\mathcal{D}_t(G, \\mathcal{T}_t), where G represents the set of TSP graphs, \\mathcal{T}_t corresponds to the set of tour labels, with t \\in {0,1,..., T} indicating the iteration counter. The initial label set, \\mathcal{T}^0, is generated using a random insertion algorithm based on the graph set G.\nWe employ a weakly-supervised learning approach, where the dataset \\mathcal{D} is used to train a constructive model parameterized by \\theta, following the \"learning to construct partial solutions\" paradigm proposed in Luo et al. (2023). This model generates TSP solutions by sequentially selecting the next node to visit.\nAt each training step, we sample a subsequence \\tau' from a"}, {"title": "4.2. Improver for Escaping Local Optima", "content": "A crucial component of our framework is the improver I, designed to help escape local optima by refining solutions through regional reconstruction. The improver operates both during training (to improve the label quality of the dataset) and during inference (to enhance the quality of the generated solutions).\nThe improver follows a three-step process: Subsequence Reconstruction, 2-opt, and Regional Reconstruction.\nSubsequence Reconstruction: Building upon the approach in Ye et al. (2024), we randomly decompose a TSP solution of length n into [m] subsequences, each of length m. To improve the homogeneity of the input, we apply a coordinate transformation to the x and y coordinates of the nodes: x\\leftarrow \\frac{x - x_o}{\\sigma}, y \\leftarrow \\frac{y - y_o}{\\sigma}, where \\sigma is the maximum absolute value among the x and y coordinates. Each subsequence is then reconstructed using a neural model parameterized by \\psi. The model replaces the original subsequence \\tau' with a newly reconstructed subsequence \\tau'', if L_{sub}(\\mathcal{T}'') < L_{sub}(\\mathcal{T}').\nOur model follows the multiple trajectories approach of POMO (Kwon et al., 2020), with the distinction that we employ a non-autoregressive neural model. Specifically, the encoder generates a heatmap of edge selection probabilities in a single pass, and the decoder constructs new subsequences iteratively based on the heatmap. Non-autoregressive models offer fast construction speeds. When the subsequence length m is small (e.g., less than 100), the drawback of such coarse construction (Ye et al., 2024) is minimal.\n2-opt: We apply the classical 2-opt algorithm (Lin & Kernighan, 1973), which iteratively checks all possible edge swaps to improve the solution. If a swap leads to a shorter solution, it is executed, and the process continues until no further improvements are possible. This helps fine-tune the"}, {"title": "Regional Reconstruction:", "content": "In this step, we remove a set of edges E' from the original TSP solution E_{\\mathcal{T}}. Let |E'| = k, the remaining edges E_{\\mathcal{T}} \\setminus E' can compose k subsequences S = {\\tau_1, \\tau_2, ..., \\tau_k}, where each subsequence is independent of the direction of traversal.\nTo ensure that the reconstructed solution is a valid Hamiltonian circuit, we transform the reconstruction process into a permutation problem. We permute the subsequences in S to form a subsequence list \\mathcal{X} = (\\tau_1, \\tau_2, ..., \\tau_k). During the permutation, we have the option to reverse the direction of any subsequence. For instance, for a subsequence \\tau' = (\\tau_1, \\tau_2,..., \\tau_m), its reverse is denoted as \\bar{\\tau'} = (\\tau_m, \\tau_{m-1},..., \\tau_1).\nOnce the subsequences are permuted and reversed as necessary, they are concatenated to form a new Hamiltonian circuit \\mathcal{T}_{new}. This reconstructed solution provides a refined path that potentially escapes local optima by incorporating a broader set of possible configurations.\nThis permutation process can be modeled as a Markov Decision Process (MDP), detailed in the next subsection."}, {"title": "4.3. The Permutation as an MDP", "content": "We model the permutation task as a Markov Decision Process (MDP) to optimally select subsequences and reconstruct the final solution. In this framework, the subsequence list \\mathcal{X} is gradually built by appending subsequences from a set of available subsequences C.\nThe MDP formulation consists of the following elements: states, actions, transition, and rewards.\nStates: The state is characterized by the current subsequence list \\mathcal{X} and the set of available subsequences C. Initially, \\mathcal{X} is an empty list, and C contains the subsequences as well as their reverses: C = {\\tau'_1, \\bar{\\tau_1}, \\tau'_2, \\bar{\\tau_2}, ..., \\tau'_k, \\bar{\\tau_k}}. At each step, subsequences are selected from C and added to \\mathcal{X}.\nActions: An action is the selection of a subsequence \\tau_{act} from the available set C to be appended to the subsequence list \\mathcal{X}.\nTransition: After performing an action, the subsequence \\tau_{act} is appended to \\mathcal{X}, and both \\tau_{act} and its reverse \\bar{\\tau_{act}} are removed from C. The process continues until all subsequences are appended, and C becomes empty, signaling the completion of the reconstruction.\nRewards: The reward function is designed to evaluate the quality of the reconstructed solution. Once the reconstruction is complete, the total reward is defined as the negative of the solution's cost, which is the sum of the edge costs"}, {"title": "4.4. Linear Attention for TSP", "content": "The vanilla attention mechanism (Vaswani, 2017) is computationally expensive, especially for large-scale problems like the TSP, where pairwise attention calculations between nodes can become prohibitively costly. To address this issue, we propose a linear attention mechanism that significantly reduces computational complexity while still effectively capturing the relevant information for TSP.\nOur attention mechanism operates by aggregating the global features of the graph into representative nodes, which are then broadcast to all other nodes. This method reduces the need for pairwise attention between all nodes. The representative nodes consist of anchor nodes, the starting node, and the destination node.\nEach node v \\in V (city node) is assigned a coordinate"}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Experimental Setup", "content": "Problem Settings. We adopt the standard data generation process from the previous work (Kool et al., 2018). The training set consists of 8192 instances, each with 1000 nodes randomly sampled from a uniform distribution. The test set includes both synthetic data and real-world data from TSPLIB (Reinelt, 1991). The synthetic data includes instances with three problem sizes: 1000, 5000, and 10000 nodes, referred to as TSP-1000, TSP-5000, and TSP-10000, respectively. The TSP-1000 and TSP-10000 test sets use the same instances as in Fu et al. (2021), with 128 and 16 instances, respectively. The TSP-5000 test set is generated similarly, with 16 instances. For subsequence reconstruction problems, each subsequence length is set to m = 100. For regional reconstruction problems, the center point c is sampled from the [0, 1]^2 space, and the number of removed edges is set to k = 60.\nModel Settings. The constructive model consists of L = 6 decoding layers with A = 5 \\times 5 anchor nodes. Both the subsequence reconstruction model \\psi and the regional reconstruction model \\phi have 6 encoding layers. We evaluate LocalEscaper in three modes: (1) LocalEscaper greedy: A greedy construction using the constructive model;"}, {"title": "5.2. Results and Analysis", "content": "The main experimental results on the uniform distribution instances are presented in Table 1. In comparison with constructive solvers, LocalEscaper demonstrates fast inference speed under greedy decoding and achieves the lowest gap across all three instance sizes. Our constructive model \\theta, based on LEHD, performs similarly to LEHD on TSP-1000 but outperforms it on instances with more than 1000 nodes. While LEHD is trained on small-scale instances (with 100 nodes), our model uses a lightweight linear attention mechanism that reduces computational overhead, enabling training on larger instances (up to 1000 nodes), resulting in better generalization to large-scale instances.\nCompared to search-based solvers, LocalEscaper also achieves the lowest gap across all instance sizes. LocalEscaper Rec100 delivers competitive solutions in a short amount of time, while LocalEscaper Rec500 further improves the"}, {"title": "5.3. Ablation Study", "content": "Weakly-supervised Learning vs. Supervised Learning. To assess the difference between weakly-supervised learning and fully-supervised learning for training the constructive model, we present a comparison in Table 3. Both methods were trained on the same dataset of 1000-node instances, with LKH-3 generating the labels for SL. Although weakly-supervised learning slightly outperforms supervised learning on TSP-1000, the difference is only 0.05%, indicating that the performance is nearly identical. For instances with"}, {"title": "Effect of the Improver's Components.", "content": "We conducted an ablation study on TSP-1000 to investigate the impact of each component of the improver. Figure 3 shows the solution improvement process over 500 iterations, with one or two components of the improver removed. Removing either regional reconstruction or both regional reconstruction and 2-opt significantly reduces performance, indicating that relying solely on subsequence reconstruction leads to a fast convergence to low-quality local optima."}, {"title": "5.4. Performance on Larger Scale Instances", "content": "Table 4 presents the evaluation results of LocalEscaper on TSP-20000 and TSP-50000, each with 16 instances. We removed the 2-opt step from the improver in TSP-50000 to adapt to the scale of these instances. The results show that while the generalization of the constructive model decreases on TSP-50000, the improver still efficiently enhances solution quality."}, {"title": "6. Conclusion", "content": "In this work, we propose LocalEscaper, a weakly-supervised framework with regional reconstruction for scalable neural TSP solvers. Our method leverages RL-based heuristics to refine solutions and iteratively enhance low-quality training labels, thereby reducing reliance on expert-annotated data. For solution improvement, we introduce a regional reconstruction strategy that enhances subsequence reconstruction by improving node reordering and escaping local optima. Furthermore, we introduce a TSP-specific linear attention mechanism that reduces computational complexity, enabling our model to efficiently handle large-scale instances. The LocalEscaper framework achieves state-of-the-art performance among neural combinatorial optimization solvers, outperforming previous methods in solution accuracy, inference speed, and generalization to large-scale TSP instances."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to Explore approaches to combine deep learning techniques in solving combinatorial optimization problems in large scale, that could be applicable for real world applications. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}]}