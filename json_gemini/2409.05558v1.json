{"title": "Seeing Through the Mask:\nRethinking Adversarial Examples for CAPTCHAS", "authors": ["Yahya Jabary", "Andreas Plesner", "Turlan Kuzhagaliyev", "Roger Wattenhofer"], "abstract": "Modern CAPTCHAs rely heavily on vision tasks that are supposedly hard for\ncomputers but easy for humans. However, advances in image recognition models\npose a significant threat to such CAPTCHAs. These models can easily be fooled\nby generating some well-hidden \"random\" noise and adding it to the image, or\nhiding objects in the image. However, these methods are model-specific and thus\ncan not aid CAPTCHAs in fooling all models. We show in this work that by\nallowing for more significant changes to the images while preserving the semantic\ninformation and keeping it solvable by humans, we can fool many state-of-the-art\nmodels. Specifically, we demonstrate that by adding masks of various intensities\nthe Accuracy @ 1 (Acc@1) drops by more than 50%-points for all models, and\nsupposedly robust models such as vision transformers see an Acc@1 drop of 80%-\npoints. These masks can therefore effectively fool modern image classifiers, thus\nshowing that machines have not caught up with humans \u2013 yet.", "sections": [{"title": "Introduction", "content": "Not surprisingly, CAPTCHAs are currently threatened by advanced image recognition models.\nPlesner et al. [2024] has recently shown that the most popular CAPTCHA environment (reCAPTCHA\nby Google [6sense, 2023]) can be solved equally well by machines and humans. If CAPTCHAS\nare to have a future, a new approach is needed. Adversarial machine learning is closely related to\nCAPTCHAs, as researchers try to build samples where the machine fails to recognize the image\nwhile the human does not register any manipulation happening. On the one hand, these imperceptible\nmanipulations are more ambitious than CAPTCHAs since even the earliest CAPTCHAs did not\nbother to hide the manipulation of the input. On the other hand, adversarial image generation is not\nrobust enough for automatic bot detection, as it often tailors the attack to a specific model. We want\nimages that can effectively fool any machine learning model, but we do not mind having a visible\nmanipulation. However, the manipulation should be easy for humans to filter out. In other words,\nwe do not mind if many pixels are changed a lot, as long as the image is still easily recognizable to\nhumans. This is easily achieved if the image manipulation is somehow predictable, for instance by\noverlaying the original with a periodic signal like a grid. A promising new form of CAPTCHAS,\nknown as hCaptcha, is doing exactly that, and in this work, we want to get a clearer understanding of\nwhat this approach can and cannot do."}, {"title": "Related Work", "content": "Deep learning models have achieved unprecedented performance in computer vision tasks, frequently\nexceeding human-level accuracy on image classification benchmarks [He et al., 2015, Russakovsky\net al., 2015]. State-of-the-art architectures such as Vision Transformers (ViT) [Dosovitskiy et al.,\n2021], ConvNeXt [Liu et al., 2022], and EVA-02 [Fang et al., 2024] now form the foundation of\nnumerous critical applications, ranging from autonomous vehicles [Yurtsever et al., 2020] to medical\nimaging [Chen et al., 2022, Shamshad et al., 2023]. However, the robustness of these models against\nadversarial attacks remains a pressing concern for their deployment in real-world scenarios, which\ncould compromise their reliability and security [Serban et al., 2020].\nThe field of adversarial examples in machine learning has seen significant advances in recent\nyears [Hendrycks et al., 2021]. Our work on geometric masks for CAPTCHAs builds on the"}, {"title": "Methodology", "content": "In this section, we will go over the data that we used for the analysis along with the model choices. We\nhave selected multiple models, which we will evaluate on the datasets to demonstrate the effectiveness\nof the masks we have constructed.\nModels We selected several models to evaluate the performance of, namely: \u201cCon-\nvNeXt_XXLarge\" [Liu et al., 2022], Open CLIP's \u201cEVA01-g-14-plus\" [Fang et al., 2023b] and\n\"EVA02-L-14\" [Fang et al., 2024], \"DFN5B-CLIP-ViT-H\" by Apple [Fang et al., 2023a], the original\n\u201cViT-L-14-378\u201d and \u201cViT-H-14-378-quickgelu\u201d [Dosovitskiy et al., 2021], \u201cResNet50x64\u201d [He et al.,\n2015], and ROBERTa-B and RoBERTa-L [Conneau et al., 2020]; the RoBERTa models are selected\nas they are supposed to be robust against adversarial attacks. Due to time constraints, we were not\nable to test the method presented recently by Fort and Lakshminarayanan [2024]; we leave this for\nfuture work. The models were selected to represent landmark architectures in both convolutional and\ntransformer-based approaches. This selection allows us to evaluate the effectiveness of our masks\nacross different model paradigms.\nData We conducted our experiments using both the enriched ImageNet dataset with 1,000 entries\nprovided by \"visual-layer\u201d on HuggingFace and the reduced ImageNette dataset [Howard, 2019].\nThe ImageNette dataset, consisting of approximately 10,000 images evenly distributed across 10\ncategories, was chosen to make the computations more feasible. To accommodate the need for\nmultiple iterations on each image, we created three smaller datasets: SubSet200, SubSet500, and\nResizedAll. SubSet200 and SubSet500 contain 2,000 and 5,000 images, respectively, maintaining\nthe full resolution of ImageNette. ResizedAll includes all ImageNette images scaled down to\n128x128 pixels, a standard size for CAPTCHAs, to speed up image processing. Note that this resizing\nmay result in a slight performance drop compared to full-resolution images. The models generally\nachieve Acc@1 accuracy in the high 80% to low 90% range, with Acc@5 accuracy in the high 90%\nrange; see Appendix A.1 for details."}, {"title": "Results", "content": "We perform three experiments, one per dataset, with the range of models mentioned earlier. We only\nshow the key partial results here with full tables in the Appendix."}, {"title": "Experiment 1 - SubSet500", "content": "We evaluate how the rank of the correct class changes when applying the masks by measuring the\nrank (the position after sorting) of the ground-truth class before and after applying a mask to an image.\nIn addition, we measure the perceptual quality of the images. We then look at the mean change in\nranks across models and images, and report the results for each combination of mask and opacity.\nThe results of our experiment are visualized in Figure 2 (the specific values can be found in Table 4 in\nthe Appendix). The figure reveals a clear trend in the trade-off between adversarial effectiveness and\nperceptual quality. The plot shows a clear inverse relationship between these two factors, as indicated\nby the polynomial regression curve of degree 2. This relationship suggests that as the effectiveness\nof the adversarial attack increases (lower \u2206 Accuracy Rank), the perceptual quality of adversarial"}, {"title": "Experiment 2 \u2013 SubSet200", "content": "This experiment measures the drop in Acc@1 and Acc@5 for the subset of images in SubSet200\nthat all models correctly classify. Thus, for the images used in this experiment, Acc@1 (and Acc@5)\nis 100% before applying the masks. We show in Table 1 the change in accuracy observed in the\nexperiment. The table shows that the circle mask is very effective in confusing models, and even\nwith a relatively low opacity the Acc@1 drops by almost 20%-points for ResNet. We also see that\nROBERTa, as a supposedly robust model, is worse than ViT for masks and opacity levels. Based on\nthe results, we see that diamond-shaped masks pose the least threat to the models at any opacity, but\nthe square masks are almost as effective as the circle masks. In an extension of this, we also looked\nat the confidence scores, the results of which are Appendix A.8."}, {"title": "Experiment 3 \u2013 ResizedAll", "content": "In this experiment, we used the ResizedAll dataset to measure the drop in Acc@1 and Acc@5\nof the models for CAPTCHA-sized images. We see the result of this in Table 2, and an important\nconclusion regarding the combination of masks and resolution changes is that while the drops in\nAcc@1 are similar to earlier, the drops in Acc@5 are larger. Compared to the results from the\nprevious experiment, it is evident that in this setting, masks at much lower opacity ratios are more\nsuccessful in distorting models' performance. Based on these results, the scaling of images combines\nvery well with masks. In closer analysis, it is also evident that EVA02 is the one that suffers the least\nfrom circular masks at opacity values >30% in both datasets, but that it comes at a trade-off of being\nmore sensitive to diamond-shaped masks."}, {"title": "Conclusion", "content": "In this study, we have demonstrated the high effectiveness of geometric masks in fooling state-of-\nthe-art vision models, and the experiments leverage the gaps between human and machine abilities.\nThis suggests potential new directions for developing more robust vision models over the long term\nwhile creating secure visual challenges in the short term. We show that there is a clear trade-off in\nthe perceptual quality of images for them to be effective against vision models. However, while the\nperceptual quality decreases, the accuracy of the models also drops, often with more than 50%-points.\nThis highlights vulnerabilities in advanced vision systems and underscores the continued capability\nof CAPTCHA-style challenges in differentiating humans from machines.\nAlthough our study focused on specific mask types and datasets, one could easily expand into\nother masks or determine how effectively models can be fine-tuned on images with masks applied.\nFurthermore, one could try the methods on the recently published DeepMind model which is supposed\nto be very robust against adversarial examples [Fort and Lakshminarayanan, 2024]. In addition, a\ndetailed human evaluation of the masks should be performed.\nOverall, this study contributes to the ongoing discussion on AI safety and reliability, highlighting the\npersistent challenge of creating truly robust vision systems that can match human-level adaptability\nin visual perception tasks."}, {"title": "Appendix / supplemental material", "content": null}, {"title": "Acc@1 and Acc@5 accuracy of the tested models.", "content": null}, {"title": "Hyperparameter Optimization", "content": "In our hyperparameter optimization phase, we focused on classification models because of their\ninterpretability advantages over segmentation models. Our initial dataset comprised 1600 scraped and\nannotated hCaptcha samples, which we used to benchmark several state-of-the-art closed-vocabulary\nclassification models. The \u201cEVA01-g-14 model\u201d, trained on \u201cLAION-400M\", emerged as the top\nperformer with Acc@1 of 94.39% and Acc@5 of 98.93%. Other models like \u201cConvNeXt-XXLarge\"\nand \"ViT-H-14\" also showed strong performance, although none achieved 100% accuracy, a notable\ndeparture from the results typically seen with reCAPTCHAv2 [Plesner et al., 2024].\nUpon analysis of the misclassified images, we observed a combination of imperceptible perturbations\nand perceptible geometric masks. We identified four distinct geometric mask types for reconstruction\nand added a novel \u201cknit\u201d mask, essentially a modified \"diamond\" mask allowing for overlapping\nshapes. We intentionally left out word-level adversarial attack masks, as they have been proven\nto be easy to mitigate [Zhang et al., 2023, Dong et al., 2023, Shayegani et al., 2023]. For each\nmask, we parameterized three variables: \u201copacity\u201d (alpha value of the overlay), \u201cdensity\u201d (shapes\nper row/column and nesting, ranging from 0-100), and \u201cepsilon\" (for white-box FGSM attacks with\nCLIP-ViT on ImageNet).\nWe conducted a hyperparameter grid search using the visual-layer/imagenet-1k-vl-enriched\ndataset on HuggingFace, testing 5-20 examples per combination on the validation set. We chose the\nCLIP ViT model for this phase due to its superior adversarial robustness, as noted by Wang et al.\n[2024]. Our optimization metric combined the difference in model accuracy pre- and post-mask\napplication with an average of three perceptual quality metrics. To identify optimal parameters, we\nselected examples with the highest perceptual quality for each level of accuracy difference and per-\nformed a linear regression. We then focused on samples above the regression line in multidimensional\nspace. This approach proved to be more tractable than our attempts with multi-objective optimization\nwith multiple variables.\nOur findings revealed that FGSM perturbations generally degraded the results when combined with\nmasks. We determined that the optimal density value was consistently 70, while the most effective\nopacity range was 50-170 (equivalent to 19%-66% alpha). These insights allowed us to isolate the\nbest-performing masks for a comprehensive benchmark against the latest models.\nThis rigorous optimization process, grounded in semantic computer vision research, enabled us to\nsystematically explore the parameter space and identify the most effective adversarial techniques\ninspired by hCaptcha challenges. The results, visualized in Figure 1, provide a quantitative basis for\ncomparing the masks."}, {"title": "Generalizability of Masks \u2013 Table", "content": "The table with values plotted in Figure 2 can be found in Table 4."}, {"title": "Acc@1 and Acc@5 accuracy for SubSet500.", "content": "In Tables 5 and 6 we show the full tables with drops in accuracy for all the tested models. We see that\nthe circle mask is very aggressive against all models."}, {"title": "Acc@1 and Acc@5 accuracy for SubSet200.", "content": "In the following we show the full tables with Acc@1 and Acc@5 in Tables 7 and 8 when evaluating on\nSubSet200 as done in Experiment 2. Noticeably, RoBERTa-B performs much worse than RoBERTa-\nL as its accuracy drops much more. As mentioned in the main results, we see in general that the\nmodels have a harder time dealing with the \"circles\" mask."}, {"title": "Acc@1 and Acc@5 accuracy for ResizedAll.", "content": "Table 9 and Table 10 show the full tables with the drops in Acc@1 and Acc@5, respectively when\napplying the masks to the resized images in ImageNette (ResizedAll)."}, {"title": "Ground Truth Confidence for SubSet200", "content": "In extension to Acc@1 and Acc@5 results, then it is useful to compare the results on the confidence\nof the ground truth for all the same masks, cf. Table 11, as it provides a better idea of how stable the\nAcc@5 scores are. Initially, the confidence in ground truth is very high and stands far from the next"}, {"title": "Ground Truth Confidence for ResizedAll", "content": "We see that the confidence of the ground truth drops further for many instances Table 12 which\nindicates that it can be easier to be combined with an FGSM-like attack and target Acc@5 specifically.\nThe table also demonstrates that VIT-L-14 (not presented in the main sections of the paper) is more\nresistant to masks of circular shape than ViT-H-14 at opacity levels >40%, but more sensitive to the\nother shapes in both datasets."}]}