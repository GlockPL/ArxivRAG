{"title": "Flow-guided Motion Prediction with Semantics and Dynamic Occupancy Grid Maps", "authors": ["Rabbia Asghar", "Wenqian Liu", "Lukas Rummelhard", "Anne Spalanzani", "Christian Laugier"], "abstract": "Accurate prediction of driving scenes is essential for road safety and autonomous driving. Occupancy Grid Maps (OGMs) are commonly employed for scene prediction due to their structured spatial representation, flexibility across sensor modalities and integration of uncertainty. Recent studies have successfully combined OGMs with deep learning methods to predict the evolution of scene and learn complex behaviours. These methods, however, do not consider prediction of flow or velocity vectors in the scene. In this work, we propose a novel multi-task framework that leverages dynamic OGMs and semantic information to predict both future vehicle semantic grids and the future flow of the scene. This incorporation of semantic flow not only offers intermediate scene features but also enables the generation of warped semantic grids. Evaluation on the real-world NuScenes dataset demonstrates improved prediction capabilities and enhanced ability of the model to retain dynamic vehicles within the scene.", "sections": [{"title": "I. INTRODUCTION", "content": "Predicting driving scenes holds significant importance in enhancing road safety, optimizing traffic flow, and advancing autonomous driving technologies. By accurately anticipating various elements within a driving environment such as pedestrian movements, and vehicle trajectories, predictive models enable proactive decision-making for both human drivers and autonomous systems. This foresight aids in mitigating risks, preventing accidents, and ultimately saving lives.\nDynamic Occupancy Grid Maps (DOGMs) represents static and dynamic elements within the environment in a bird's-eye-view (BEV) grid. One of the key advantages lies in their flexibility regarding sensor dependency, as they can be generated from various types of sensors including Lidar, radars, and cameras. In our research, we leverage probabilistic DOGMs [1], which further enhances predictive capabilities by incorporating uncertainty estimation. Moreover, by integrating deep learning techniques with such probabilistic DOGMs, we can effectively learn complex interactions within the driving environment, tackle the intrinsic spatiotemporal problems, and lead to more robust and precise predictions.\nIn the domain of autonomous vehicles, Occupancy Grid Map (OGM) predictions are often ego-centric ([2], [3]), meaning they are fixed relative to the ego-vehicle frame. Dependent on the motion of the ego-vehicle, the future predicted grids suffer from the disappearance of static and dynamic scenes over time and loss of scene structure, especially during turns. To address these shortcoming, Asghar et al. [4] proposed making allo-centric DOGM predictions by representing the scene evolution in a fixed reference frame. This approach successfully preserves scene integrity independent of ego-vehicle motion and focuses on the behavior and motion prediction of dynamic agents in the scene.\nAsghar et al. [5] extended this work to consider availability of semantic information with the input and to predict agents motion in the scene as sequential BEV semantic grids. The framework demonstrated improved abilities to predict dynamic agents as well as the possibility to evaluate with the ground truth.\nIn this work, we build further on the existing approaches to formulate a multi-task framework by learning the flow of the scene to improve future semantic grids. For our input we consider both occupancy probabilities and estimated velocities of the dynamic scene components from DOGMs, along with the semantic information from the camera images. Our future predictions are allo-centric, fixed with respect to the latest ego position. We generate predictions for both the future semantic grids and the flows. The predicted flows also serve as intermediate features, capturing to the spatial and temporal changes in the semantic information within the scene. By forecasting the flows, we anticipate how the semantic content of the scene will evolve over time. Subsequently, we iteratively warp the current semantic grid with the future flows to obtain the sequence of future semantic grids. A brief overview of this work can be accessed online.\nThe main contribution of this paper is a novel multi-task framework that combines DOGM and semantics to predict multi-step vehicle semantic grids, as well as the multi-step flow of the complete scene. Evaluation on real-world NuScenes dataset [6] shows how the proposed framework improves prediction capabilities and learns to recognize vehicle behaviors within the scene without prior maps."}, {"title": "II. RELATED WORK", "content": "In the state-of-the-art literature, the challenge of predicting ego-centric future OGM (Occupancy Grid Map) involves incorporating spatio-temporal deep-learning methods ([7], [8], [9], [10], ). Asghar et al. [4] proposed a novel approach for future DOGM prediction within a fixed reference frame, preserving scene integrity particularly during ego-vehicle maneuvers.\nStudies by Toyungyernsub et al. [2] and Schreiber et al. [11] proposed separate predictions of the static and dynamic\nDifferent from these methods, we employ probabilistic version of DOGMs [1] as our input source, which provide static and dynamic occupancy states of the scene in a BEV grid. Our input DOGMs not only offer probabilistic information even when only partial data is available, but also facilitate seamless integration of various sensor inputs and configurations without retraining the entire network."}, {"title": "III. SYSTEM OVERVIEW", "content": "Our methodology builds upon the overall pipeline proposed in [5], which addresses vehicle motion forecasting into semantic grids from OGMs. This work specifically focuses on incorporating scene flow to further improve the vehicle motion predictions as semantic grids. We discuss in detail here our proposed approach, and the pipeline is summarized in Fig. 1."}, {"title": "A. Problem Formulation", "content": "We formally define the task of vehicle motion prediction, see Fig. 1. Let $X_t \\in \\mathbb{R}^{6 \\times w \\times h}$ be the t-th frame of the input grid where w and h denote the width and height respectively. $X_t$ comprises of three occupancy state channels, two velocity channels, and one semantic grid channel. Let $Z_t \\in \\mathbb{R}^{3 \\times w \\times h}$ be the t-th frame of the output grid that contains a single vehicle semantic grid channel, $Y_t \\in \\mathbb{R}^{1 \\times w \\times h}$, and two flow grid channels, $F_t \\in \\mathbb{R}^{2 \\times w \\times h}$.\nGiven a set of input sequence $X_{t-N:t}$, the task of our proposed multi-head network is to predict 1) the vehicle grid $\\hat{Y}_t$ at the current time step, and 2) the sequence of the future grids $\\hat{Z}_{t+1:t+P}$. This sequence encompasses both the future vehicle semantic grids $Y_{t+1:t+p}$, and the future flow grids $F_{t+1:t+p}$, with P representing the prediction horizon.\nNext, we utilize the predicted sequence of flow grids $\\hat{F}_{t+1:t+p}$ to warp the vehicle grid $\\hat{Y}_t$ recursively, and obtain an additional sequence of warped future vehicle semantic grids $W_{t+1:t+p}$. This warping process can be expressed by Eq. 1,\n$W_{t+1} = f_W(W_t, F_{t+1})$\nwhere $f_W$ is the warping operation and $W_t = \\hat{Y}_t$ initially."}, {"title": "B. Input Scene Representation", "content": "1) Dynamic occupancy grid maps: DOGMs provide a grid-based representation of the environment in a bird's-eye view. Each cell in the grid is estimated in parallel within the system and contains information about its occupancy state, as well as associated dynamics. Unlike classic object detection-and-tracking methods, such approaches mitigates risks linked to uncautious thresholding.\nWe incorporate Bayesian dynamic occupancy grid filter [1] to generate DOGMs from LiDAR sensor data. This approach is used to estimate probabilities associated with four possible occupancy states for each cell in the grid: i) free, ii) occupied and static, iii) occupied and dynamic, and iv) unknown occupancy. Additionally, the framework provides velocity estimates for each cell, represented by mixtures of grids and particle sets.\nIn the proposed network, the input $X_t \\in \\mathbb{R}^{6 \\times w \\times h}$ comprises six channels, with five channels sourced from the output of a DOGM. These encompass information pertaining to three DOGM occupancy states (the definitive visualization represented in Fig. 2) and two-channel DOGM velocities along the x and y-axis. Figure 1 showcases the DOGM state grids and the velocity grids depicted below as two-dimensional flow illustrations in the input. An additional layer of semantic information is overlaid on top of the DOGM state grids, depicted in white. The inclusion of semantic information is discussed in the section below.\n2) Semantic information: A semantic grid contains the probability assigned to each cell for being occupied by a specific semantic category, such as vehicles in this case. The approach outlined in the Lift, Splat, Shoot (LSS) [19] is re-purposed in this work to prepare the semantic information for input into our proposed network. LSS extracts feature maps from multiple input camera images in BEV and decodes them into vehicle occupancy prediction represented as a semantic grid. Our adaptation involves concatenating corresponding DOGMs alongside the feature maps extracted from camera images and decoding the combined information into semantic grids. This fusion of DOGMs with BEV features results in a more comprehensive understanding of the scene. This modified pipeline is illustrated in Fig 2.\nNote that the semantic information can be incorporated in this framework via alternative methods as well. For example, vehicle semantic information from camera images"}, {"title": "C. Flow-guided Predictions", "content": "The network is equipped with two decoder heads: one is dedicated to predicting a semantic grid for current time step, while the other forecasts sequential future grids and flows simultaneously.\nThe flow is represented as a two-channel grid that models the velocity of vehicles along the x and y dimensions. We utilize backward flow to capture vehicle motion from their future positions back to their current positions. At each time step, the backward flow contains velocity vectors for each cell, indicating the change in occupancy from the previous time step. Refer to Fig. 3 for a visual illustration.\nTo generate a sequence of flow-guided semantic grid, we start with the current semantic grid $\\hat{Y}_t$. This grid is initially warped using the flow $F_{t+1}$ to obtain a warped semantic grid $W_{t+1}$. Subsequently, $W_{t+1}$ is further warped with the next flow $F_{t+2}$. This process is repeated recursively for all the subsequent time steps to yield a sequence of warped vehicle semantic grids. Worth noting, our method not only predicts the flow of occupancy but also incorporates the potential flow in free spaces.\nAlongside predicting the flow, the network also forecasts a sequence of future semantic grids. This simultaneous prediction aids the network in correlating the occupancy of the semantic grid with the corresponding flow vectors."}, {"title": "D. Model Architecture", "content": "The model combines spatio-temporal predictions with a conditional variational approach.\n1) Endoder: Each frame in the input sequence, denoted as $X_{t-N:t}$, is passed through the spatial encoder. Then the generated features are sequentially fed into a ConvLSTM (Convolutional Long Short Term Memory) block, enabling the simultaneous capture of spatial and temporal information. To achieve this, we use 4 ConvLSTM units, each with 128 hidden features, ensuring the extraction of spatio-temporal features that encompass the past evolution of the scene.\n2) Probablistic modeling: To allow for multimodal pre-dictions, we adopt a conditional variational approach [20]. This approach allows the network to learn two distributions that capture the evolution of the scene in latent space: i)"}, {"title": "E. Losses", "content": "The network is trained to learn vehicle motion forecasting, as semantic grids and their corresponding flow representations. All vehicle semantic grids are trained with binary cross-entropy loss $L_{BCE}$ with a positive sample weight of 5. This includes loss for $\\hat{Y}_t$, $\\hat{Y}_{t+1:t+P}$, and $W_{t+1:t+p}$. The loss for warped semantic grids $W$ acts as self-supervised loss for the flow grid $F$.\n$L_{BCE.sum} = \\lambda_aL_{BCE_{\\hat{Y}_t}} + \\lambda_bL_{BCE_{\\hat{Y}_{t+1:t+P}}} + \\lambda_wL_{BCE_{W_{t+1:t+P}}}$\nwhere $\\lambda_a$, $\\lambda_b$ and $\\lambda_w$ are the loss weights for $\\hat{Y}_t$, $\\hat{Y}_{t+1:t+P}$, and $W_{t+1:t+p}$ respectively.\nFor the supervised loss of the flow grid f, we define an L1 loss, only for the cells that are occupied by the vehicles. Additionally, we introduce Kullback-Leibler divergence loss $L_{KL}$ to encourage alignment between the present and the future distributions, described in III-D.2.\nThe final loss is weighted sum of all these losses.\n$L = L_{BCE.sum} + \\lambda_fL_{flow} + \\lambda_kL_{KL}$\nwhere $\\lambda_f$ and $\\lambda_k$ are the respective loss weights."}, {"title": "IV. EXPERIMENTS", "content": "We evaluate our proposed model on the real-world NuScenes dataset [6]. The original dataset consists of 700 and 150 scenes for training and validation respectively. Each scene has a duration of 20s, with lidar data available at 10Hz and annotated keyframes at frequency of 2Hz. The lidar based DOGMs are generated comprising of state grids and velocity grids, with a resolution of 0.1 m. For semantic grid prediction, all 6 camera images at each keyframes are considered along with respective DOGM state grids.\nThe ground truth annotations for semantic grids are prepared based on the vehicle bounding box annotations provided in the dataset. The velocity or speed of the vehicles is unavailable. Therefore, the ground truth flow of the vehicles is estimated by calculating the displacement of vehicle centroids on the grid, which is then normalized by the grid dimensions.\nAll grids are initially generated ego-centrically, with the ego-vehicle positioned at the center and facing upward. Future predictions in each sequence are made relative to the ego-vehicle frame of reference at the latest timestep. The past and future grids are subsequently transformed to facilitate allo-centric predictions in a fixed frame, using the available odometry data from the dataset, and then cropped to cover an area of 60x60m.\nIn total, we have 23K training and 5K validation sequences respectively."}, {"title": "B. Training", "content": "The input sequence $X_{t-2:t}$ consists of 3 frames, spanning over 1.0s. The network is trained to make predictions for present $\\hat{Y}_t$, and 4 future frames $\\hat{Y}_{t+1:t+4}$, for instances 0.5s to 2.0s, with steps of 0.5s. For training, the grid images are resized to 240x240 pixels, thus each pixel has a resolution of 0.25m. The loss coefficients are set to $\\lambda_a = 1.0$, $\\lambda_w = 1.0$ $\\lambda_f = 0.05$ and $\\lambda_k = 0.005$. Our network is implemented on Pytorch and trained on 8 Nvidia Tesla V100 GPUs. Adam Optimizer is employed at the learning rate of 3x10-4 and weight decay of 1x10-7. Dataset is trained for 20 epochs with the batch size of 18. This takes approximately 5 hours."}, {"title": "V. RESULTS", "content": "The network's flow grid predictions focus on warping the semantic grid from one timestep to the next, rather than specifically capturing the flow of occupancy. This allows for predicting both the movement of dynamic vehicles and the resulting free space. In the flow images, white spaces dominate the static scene structure observed in the input state grids. Regions with no observed occupancies, whether in the input or the predicted horizon, contain random values in the flow grid as they do not impact the predicted grids.\nCollision Avoidance: A notable attribute in the flow-warped semantic grids is their effective collision avoid-ance capability during predictions. The motion prediction of dynamic vehicles consistently demonstrates a tendency to navigate away from static occupancies within the scene, regardless of whether they are static vehicles or unrecognized categories. As illustrated in Fig. 4, multimodal predictions avoid the static obstacles within the scene, as highlighted in the magnified sections of grids in the rightmost column. For instance, in the first example, bus predictions exhibit cautious behavior in the face of static cells ahead. Similarly, in the third scenario, the multimodal predictions steer clear of small static obstacles within the space."}, {"title": "B. Quantitative Evaluation", "content": "We conduct an ablation study with dif-ferent configurations of the proposed network to investigate the role of semantic flow. We consider 3 different variations for the future prediction decoder: i) only predict semantic grids $\\hat{Y}_{t+1:t+P}$, ii) only predict flows $F_{t+1:t+p}$, and iii) predict both together. These variations are trained with and without the velocity grid in the input.\nWe evaluate the performance of the semantic and warped grids using binary classification evaluation metrics: Inter-section of Union (IoU) and Area under the precision-recall curve (AUC). IoU is computed with a threshold of 0.5, while AUC is calculated across 100 linearly spaced thresholds.\nTable I shows the average IoU and AUC values over a 2.0s prediction horizon for six cases. The IoUs for our"}, {"title": "VI. CONCLUSION", "content": "In this work, we introduce a novel forecasting approach that leverages DOGMs and semantic information to pre-dict scene evolutions as flow-guided semantic grids. Our framework is capable of multi-modal predictions, learning patterns and behaviours based on the observable static scene and motion of dynamic agents. By predicting flow alongside semantic grids, this methods tends to improve prediction results, especially in dynamic parts. While only vehicles were considered in the flow-guided agent prediction, such method should in the future be expanded to the prediction of other types of agents, and unidentified grid-level components."}]}