{"title": "Temporal Binding Foundation Model for Material Property Recognition via Tactile Sequence Perception", "authors": ["Hengxu You", "Tianyu Zhou", "Jing Du"], "abstract": "Robots engaged in complex manipulation tasks require robust material property recognition to ensure adaptability and precision. Traditionally, visual data has been the primary source for object perception; however, it often proves insufficient in scenarios where visibility is obstructed or detailed observation is needed. This gap highlights the necessity of tactile sensing as a complementary or primary input for material recognition. Tactile data becomes particularly essential in contact-rich, small-scale manipulations where subtle deformations and surface interactions cannot be accurately captured by vision alone. This letter presents a novel approach leveraging a temporal binding foundation model for tactile sequence understanding to enhance material property recognition. By processing tactile sensor data with a temporal focus, the proposed system captures the sequential nature of tactile interactions, similar to human fingertip perception. Additionally, this letter demonstrates that, through tailored and specific design, the foundation model can more effectively capture temporal information embedded in tactile sequences, advancing material property understanding. Experimental results validate the model's capability to capture these temporal patterns, confirming its utility for material property recognition in visually restricted scenarios. This work underscores the necessity of embedding advanced tactile data processing frameworks within robotic systems to achieve truly embodied and responsive manipulation capabilities.", "sections": [{"title": "INTRODUCTION", "content": "The ability of robots to recognize and adapt to different material properties is a crucial component in developing versatile and intelligent manipulation systems [1-4]. Traditionally, visual sensors have been the primary means of perceiving and identifying objects, allowing robots to make informed decisions based on texture, shape and spatial orientation [5, 6]. However, reliance solely on visual data can be limiting, particularly in scenarios where visibility is obstructed or when detailed tactile information is needed to distinguish between materials with similar visual characteristics. This gap underscores the importance of tactile sensing as a complementary modality, enabling robots to physically interact with their environment in a manner akin to human touch [7, 8].\nTactile sensing is especially valuable in contact-rich, small-scale manipulation tasks, such as handling soft or deformable materials. Studies have shown that tactile sensors, when integrated with machine learning models, can significantly enhance a robot's perception and handling capabilities by providing detailed information about texture, hardness, and compliance [9-12]. Tactile data also supports adaptive force feedback control, enabling robots to dynamically adjust their grip and manipulation strategy based on the material's properties, thus contributing to safer and more effective interactions [13, 14].\nRecent advancements in tactile sensor design have led to the development of high-resolution fingertip sensors that push robotic tactile sensing closer to human-level sensory capabilities [15-17]. These sensors capture fine-grained details, allowing robots to perceive subtle differences in texture and pressure distribution with improved accuracy. However, interpreting this high-resolution tactile input as a new sensory modality presents unique challenges, requiring models capable of handling complex, data-rich inputs. Foundation models, with their strong transferability and adaptability, provide an effective approach for analyzing tactile data, capturing detailed and temporally correlated patterns essential for comprehensive material recognition [18, 19]. The necessity of foundation models in tactile-based learning has become increasingly evident as robots acquire more sophisticated and data-intensive sensory inputs [9, 20].\nFurthermore, human tactile perception is inherently temporal and incorporates memory mechanisms that facilitate continuous learning and adaptation. As noted in [21], human touch relies on past sensory experiences to inform current responses. This insight underscores the value of incorporating similar temporal learning mechanisms in robotic tactile systems to achieve human-like processing. By embedding memory-based structures into tactile learning models, robots can better replicate human sensory behaviors, leading to more adaptive and precise material property recognition. Advanced architectures, such as those utilizing transformers and temporal networks, provide a robust framework for processing sequential high-resolution data [22].\nInspired by human tactile memory and supported by the development of foundation model on perception, this letter presents a temporal binding foundation model, TeBi-Llama (Llama model with temporal information binding), that processes tactile data as time-sequenced input. Our model captures the nuanced temporal patterns embedded in tactile interactions, enhancing the robot's ability to recognize and adapt to different material properties."}, {"title": "METHODS", "content": "Our proposed TeBi-Llama model is based on the model proposed in [20], which applies LlaMa-Adapter and ImageBind to align the tactile and RGBD as multi-modality inputs with LlaMa2 model. On top of the original model, we designed a LSTM-based encoder to process the sequential image-tactile input pairs. To effectively excavate the embedded information in the sequential sensory inputs, we develop a structure-aware cross-modality feature fusion method that gradually insert the hidden states into different stages of the attention layers in LlaMa2. Fig. 1 gives the overall structure of our designed model including the two major modules: LSTM Encoder and Structure-Aware Temporal Fusion."}, {"title": "LSTM Encoder", "content": "Fig. 2 gives the detailed design of the LSTM encoder. Let $T$ to be the sequence length. $I_m$ and $Tact$ denote the image and tactile frame at time step $t$. We apply the pretrained visual encoder from OpenCLIP [23] as the image encoder and the Vision Transformer [24] (ViT) as the tactile encoder, which align with the usage in [20]. The encoded feature pairs ($f_{image}$, $f_{tactile}$) would be separately fed to two LSTM modules with similar structures. Let $h_{image}$ and $h_{tactile}$ denote the hidden state output at $t$ for image and tactile respectively."}, {"title": "Structure-aware Temporal Fusion", "content": "We design a structure-aware feature fusion method that insert the temporal features from LSTM into different attention layers in the foundation model illustrated in Fig. 3. We apply the opensource Llama2-7b as the main model.\nInstead of using the last-step LSTM output as Llama2 input once, we utilized each hidden to capture the evolution of the tactile-visual sequence, as each state contains valuable intermediate information that contributes to the final estimation. We apply ImageBind [25] to fuse the LSTM-generated hidden states with the LLaMA-2 model's attention layers. Given that LLaMA-2 consists of n attention layers, we map each hidden state outputs $h^{t}:=< h_{image}, h_{tactile} >$ to a distinct subset of the attention layers. Assuming T dived evenly into 32, we assign each $h^{t}$ to a set of 32/T consecutive attention layers. Specifically, for the 32/T, we fuse $h^{1}$ (separately on $h_{image}$ and $h_{tactile}$) with the output of each layer; for the next 32/T layers, we fuse $h^{2}$; continuing this process until the last 32/T layers are fused with $h^{T}$. This design aligns each LSTM hidden state with the LLaMA-2 layers, reflecting a gradual refinement of the multi-modal features.\nAligning each LSTM hidden state with successive blocks of LLaMA-2's attention layers leverage its hierarchical processing structure. Visual-tactile input, with its changing sensory details, benefits from this approach. Early LSTM states, which tends to capture transient or noisy information, pair with the initial LLaMA-2 layers that focus on broad features. As the sequence progresses, more refined LSTM states align with deeper LLaMA-2 layers, which interpret high-level patterns like consistent textures. This structured flow enables clearer, interpretable multi-modal feature extraction. The progressive alignment from $h^{1}$ to $h^{T}$ across the 32 layers supports the sequential nature of visual-tactile data, enhancing the model's ability to track both short-term variations and long-term dependencies crucial for manipulation tasks."}, {"title": "RESULTS AND ANALYSIS", "content": "The training dataset is Human Collected Tactile (HCT) Dataset from [20]. The HCT dataset consists of 39,154 visual-tactile image pairs, collected from five human subjects across 847 trajectories (a total of 20 hours). Each trajectory captures four stages of interaction: approaching, contacting, sliding, and withdrawing, where participants use a GelSight sensor to interact with various objects.\nTo align with the LSTM training setup, we reorganized the original dataset to create the HCT Temporal training set. For each trajectory, we sampled frames starting from the first frame to the T-th frame from the end, where $T$ is the chosen sequence length. Given a trajectory length of $L_{traj}$, this setup yields $L_{traj} - T + 1$ temporal training examples per trajectory. Each example consists of T consecutive tactile-RGB pairs, paired with a text description of the object's material properties (e.g., \"solid,\" \"soft\"). Since only one object is used in each trajectory, the material description remains consistent across all frames in a single LSTM sample $Tem^{t}as$:"}, {"title": "Dataset", "content": "$Tem^{t} :=< [Im^{t},Tac^{t}],[keyword labels] >, t \\in [1,T]$\nWe employed a two-stage training workflow consisting of LSTM encoder pretraining and temporal-binding finetuning, following a setup similar to that in [20].\nIn the pretraining phase, the task was framed as a 402-class classification problem, using the text description of each data sample as the ground truth. We evaluated model performance through top-1 and top-5 accuracy metrics for both tactile-text and tactile-visual modalities. The visual-text group was omitted, as it relies on a pre-trained feature extractor from CLIP, which is not directly relevant to the current model's design analysis. Here, top-1 accuracy represents instances where the ground truth label matches the model's highest-confidence prediction, while top-5 accuracy reflects cases where the label appears within the top five predictions by confidence. The LSTM encoder for both tactile and visual modalities consist of two unidirectional LSTM layers, followed by two MLP layers to align the feature output size with the LlaMa2 input requirements.\nIn finetuning phase, we applied the proposed temporal binding method, integrating the pre-trained LSTM encoder features into the LLaMA-2 7B model's 32 multi-head self-attention layers. This approach enables the progressive alignment of sequential features from the LSTM encoder with LLaMA-2's hierarchical processing layers.\nThe training process used 8 A100 GPUs and 96 CPU cores on the University of Florida's HiPerGator computing platform, supported by NVIDIA. This setup is used to train both the LSTM encoder and the binding model, ensuring efficient utilization of computational resources."}, {"title": "Training Setup", "content": "To illustrate the improvements achieved by applying the temporal binding structure, we compared results in two areas: the LSTM encoder classification results after pretraining and the overall model performance on material property recognition."}, {"title": "Results", "content": "Our results demonstrate that tactile data has strong potential to offer sensory details for material property recognition. However, a model must be specifically designed to handle the temporal flow and structured fusion of tactile features."}, {"title": "CONCLUSION", "content": "In this work, we explored the potential of tactile data for material property recognition, presenting a model that combines LSTM-based sequential processing with structure-aware feature fusion into the LLaMA-2 attention layers. Our approach leverages both temporal dynamics and hierarchical alignment within the model, resulting in significant improvements in recognizing material properties over baseline methods. The results demonstrate that tactile data, with its ability to capture unique sensory information, can substantially enhance robotic perception when integrated with models specifically designed to handle its temporal and structural complexities.\nFor future work, we plan to investigate the influence of different time steps on model performance to optimize temporal sensitivity. Additionally, recognizing that transitions between contact and non-contact frames may further reflect material properties, we aim to include integrated sequences of both frame types in training to capture these dynamic properties. We also intend to experiment with alternative binding methods, such as cross-attention, to deepen our exploration of temporal-structural fusion mechanisms."}]}