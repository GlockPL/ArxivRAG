{"title": "MEMORAG: MOVING TOWARDS NEXT-GEN RAG VIA MEMORY-INSPIRED KNOWLEDGE DISCOVERY", "authors": ["Hongjin Qian", "Peitian Zhang", "Zheng Liu", "Kelong Mao", "Zhicheng Dou"], "abstract": "Retrieval-Augmented Generation (RAG) leverages retrieval tools to access external databases, thereby enhancing the generation quality of large language models (LLMs) through optimized context. However, the existing retrieval methods are constrained inherently, as they can only perform relevance matching between explicitly stated queries and well-formed knowledge, but unable to handle tasks involving ambiguous information needs or unstructured knowledge. Consequently, existing RAG systems are primarily effective for straightforward question-answering tasks. In this work, we propose MemoRAG, a novel retrieval-augmented generation paradigm empowered by long-term memory. MemoRAG adopts a dual-system architecture. On the one hand, it employs a light but long-range LLM to form the global memory of database. Once a task is presented, it generates draft answers, cluing the retrieval tools to locate useful information within the database. On the other hand, it leverages an expensive but expressive LLM, which generates the ultimate answer based on the retrieved information. Building on this general framework, we further optimize MemoRAG's performance by enhancing its cluing mechanism and memorization capacity. In our experiment, MemoRAG achieves superior performance across a variety of evaluation tasks, including both complex ones where conventional RAG fails and straightforward ones where RAG is commonly applied. MemoRAG is under intensive development, whose prototypes and resources will be constantly published at our project repository.", "sections": [{"title": "1 INTRODUCTION", "content": "Although the fundamental capabilities of large language models (LLMs) have improved rapidly over time Brown et al. (2020); OpenAI (2023); Touvron et al. (2023), they still face significant challenges in practice due to their inherent limitations. For instance, LLMs are likely to generate hallucinations or out-dated contents because of the lack of proper knowledge. They also struggle to manage overloaded historical interactions with users due to their limited context windows Bai et al. (2023); Zhang et al. (2024). Retrieval-augmented generation (RAG) has emerged as a promising paradigm for LLMs to address these challenges. It employs specialized retrieval tools that bring in useful knowledge from external databases, therefore, it enables LLMs to generate factual responses based on a knowledge-grounded context Izacard & Grave (2021); Gao et al. (2024).\nTraditional RAG systems often require clearly stated information needs and well-formed knowledge. As a result, their applications are mostly constrained to straightforward question answering tasks Nogueira & Cho (2020); Lewis et al. (2020); Gao et al. (2024). However, it is not the case for many real-world problems where the information needs are ambiguous and the external knowledge is unstructured Edge et al. (2024); Qian et al. (2024). For example, a reader of a book might want to understand the mutual relationships between the main characters. To solve this problem, the system would need to first identify the main characters' names and then locate the sections where the corresponding names co-exist, from which their mutual relationships can be inferred. In other words, it calls for the comprehension of information needs based on the contextual knowledge of the book before relevant information can be effectively retrieved.\nTo address the above challenge, we propose a novel framework called MemoRAG, as shown in Figure 1. This framework introduces an smart interface that connects tasks with the relevant knowl-edge from a database. For each presented task, MemoRAG prompts its memory module to generate retrieval clues. These clues are essentially drafted answers based on a compressed representation of the database, i.e. the memory. Despite the possible existence of false details, the clues explicitly reveal the underlying information needs for the presented task. Furthermore, they can also corre-spond directly to the source information in reality. By using these clues as queries, MemoRAG can effectively retrieve the necessary knowledge from the database.\nAccording to the above mechanism, the memory module is expected to be 1) retentive: memorizing the global information of the entire database, and 2) instructive: providing useful clues based on which all needed knowledge can be retrieved comprehensively. Therefore, we propose the following designs to optimize the performance of MemoRAG. First, we introduce a dual-system architecture, with a light LLM to serve as the memory and a heavy LLM to perform retrieval-augmented genera-tion. The light LLM must be cost-effective and lengthy-in-context, being able to accommodate the whole database in a limited computation budget. Second, we perform fine-tuning of the memory, such that the generated clues can achieve the optimized retrieval quality.\nTo evaluate the effectiveness of MemoRAG, we have developed a comprehensive benchmark called ULTRADOMAIN, which consists of complex RAG tasks with long input contexts drawn from di-verse domains (e.g., law, finance, education, healthcare, programming). ULTRADOMAIN includes tasks characterized by queries that: (1) involve implicit information needs, (2) require distributed evidence gathering, and (3) demand a high-level understanding of the entire database. A common challenge among these queries is that the relevant knowledge cannot be directly retrieved through simple searches. While conventional RAG methods struggle with such complex tasks, MemoRAG"}, {"title": "2 MEMORAG", "content": "The standard RAG framework can be concisely expressed as:\n$Y = \\Theta(q, C | \\theta)$, $C = \\Gamma(q, D|\\gamma)$,\n(1)\nwhere $\\Theta(\\cdot)$ and $\\Gamma(\\cdot)$ denote the generation model and the retrieval model, respectively. q represents the input query, C is the context retrieved from a relevant database D and Y is the final answer. In many practical scenarios, the input query q often carries implicit information-seeking intents that can be challenging for a standard retriever, which typically relies on lexical or semantic matching, to fully comprehend. This limitation underscores the necessity of designing an intermediate module to bridge the semantic gap that arises in many practical scenarios.\nIn this paper, we propose MemoRAG, which leverages a memory model $\\Theta_{mem}(\\cdot)$ to function as a semantic bridge between the input query q and the relevant database D. Formally, this process can be represented as:\n$Y = \\Theta(q, C | \\theta)$, $C = \\Gamma(y, D | \\gamma)$, $y = \\Theta_{mem} (q, D | \\Theta_{mem})$.\n(2)\nHere, y represents a staging answer that may be incomplete or lack detail, serving as a set of answer clues that guide the retrieval of the most relevant context from D. The memory model $\\Theta_{mem}(\\cdot)$ is designed to establish a global memory of the database D. In practice, any language model capable of efficiently processing super-long contexts can serve as the memory model. For example, a 7B language model incorporating key-value compression techniques could be an appropriate choice. While such a model might not generate a fully detailed and accurate answer, it can produce a rough outline that facilitate in locating the correct answers.\nThe form of the staging answer y is tailored to the specific requirements of each task. For example, in a question-answering task where the input query is implicit, the staging answer y may comprise intermediary steps, such as generating surrogate queries that are more explicit and disambiguated, along with specific text evidence from the database that contributes to the final answer. In addition, for tasks that do not involve explicit queries, such as summarization, the staging answer might consist of key points or concepts extracted from the context, which are crucial for assembling a coherent and accurate summary.\nSystem Implementation The system implementation of MemoRAG is available at this reposi-tory. Currently, we have released two memory models: memorag-qwen2-7b-inst and memorag-mistral-7b-inst, which are based on Qwen2-7B-Instruct and Mistral-7B-Instruct-v0.2, respectively. In MemoRAG, the memory module $\\Theta_{mem}(\\cdot)$ can be any model designed to handle very long contexts efficiently, while currently the system employs a token compression technique, enabling efficient processing of extended contexts, which will be explained in detail later.\nThe two memory models support compression ratios ranging from 2 to 16, allowing them to manage different context lengths. For example, memorag-qwen2-7b-inst can process up to 128K \u00d7 16 tokens when a compression ratio of 16 is applied. In practice, memorag-mistral-7b-inst performs well with context lengths up to 150K tokens, and memorag-qwen2-7b-inst performs well with context lengths up to 600K tokens. When processing longer context, their performances would degrade slightly.\nMemoRAG can integrate various retrieval methods into the system, including sparse retrieval, dense retrieval, and reranking. The current implementation uses dense retrieval by default. In future releases, we aim to provide flexible retrieval methods with easy-to-use interfaces.\nSimilarly, MemoRAG can integrate any generative language model as the generator. The current implementation supports initializing the generator from HuggingFace generative models or through commercial APIs (e.g., Azure and OpenAI). Since the memory models are dependent on their un-derlying models, MemoRAG uses the underlying models of the memory module as the default generator. For instance, when using memorag-mistral-7b-inst as the memory model, MemoRAG defaults to Mistral-7B-Instruct-v0.2 as the generation model."}, {"title": "2.1 \u039c\u0395\u039cORY MODULE", "content": "In this paper, we propose a flexible model architecture specifically designed to facilitate memory formation. The memory model progressively compresses the raw input tokens into a significantly smaller set of memory tokens, while preserving the essential semantic information. Specifically, suppose the input X comprises n tokens, $X = \\{x_1,\\cdots, X_n\\}$, and is processed by a transformer-based model $\\Theta(\\cdot)$. The attentive interaction of each layer can be formulated as:\n$Q = XW_Q, K=XW_K, V = XW_V,$\n(3)\n$\\mathrm{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V, \\quad \\Theta(X)=\\operatorname{Attention}(Q, K, V)$,\n(4)\nwhere $W_Q, W_K$, and $W_V$ are the weight matrices for the query, key, and value projections, respec-tively, and $d_k$ is the dimension of the key vectors. The deep attentive interactions among X after multiple Transformer layers lead to a comprehensive understanding of the input sequence X. This is akin to human's short-term memory that is accurate but can only contain recent perceived content. We denote this process as $\\tilde{X} = \\Theta(X)$ where $\\tilde{X}$ refers to the hidden states of the input sequence X and $\\Theta(\\cdot)$ can be any pretrained LLMs.\nTo enable the conversion from short-term memory to long-term memory, we introduce memory to-kens $x_m$ to serve as the information carriers of long-term memory in LLMs. Specifically, supposing the underlying LLM $\\Theta(\\cdot)$ has a working context window length of l, after each context window, we append k memory tokens, that is:\n$X = \\{x_1,\\cdots,x_l,x_m^1,\\cdots,x_m^k,x_{l+1},\\cdots\\}, k\\ll l$.\n(5)\nDuring the attentive interactions defined by Eq. (4), we initialize another set of weight matrices $W_{Qm}, W_{Km}$ and $W_{Vm}$ on the special purpose of memory formation. Therefore, we have:\n$Q_m = XW_{Qm}, K_m= XW_{Km}, V_m = XW_{Vm},$\n(6)\n$\\mathrm{Attention}(Q, K, V) = \\operatorname{softmax}\\left(\\frac{\\left[Q ; Q_{m}\\right]\\left[K ; K_{m} ; K_{m}^{\\text {cache }}\\right]^{T}}{\\sqrt{d_{k}}}\\right)\\left[V, V_{m}, V_{m}^{\\text {cache }}\\right]$,\n(7)\nwhere $Q_m, K_m$, and $V_m$ refer to the query, key, and value for the memory tokens $x_m$. $K_{m}^{\\text {cache}}$ and $V_{m}^{\\text {cache}}$ refer to the KV cache of previous memory tokens. We denote the memory tokens as $x_m$ and the conversion process as $x_m = \\Theta_{mem}(X)$. For $l$ raw tokens $\\{x_1,\\ldots,x_l\\}$, after multiple attentive processes defined by Eq. (7), they are encoded into hidden states $\\tilde{X}^{[0: l]} = \\{\\tilde{x}_1,\\ldots,\\tilde{x}_l,\\tilde{x}_m^1,\\ldots,\\tilde{x}_m^k\\}$, where $\\{\\tilde{x}_1,\\ldots,\\tilde{x}_l\\}$ represent the raw tokens' hidden states and $\\{\\tilde{x}_m^1,\\ldots,\\tilde{x}_m^k\\}$ represent the memory tokens' hidden states. After the formation of memory, the KV cache of the l raw tokens is discarded, similar to the forgetting process in human memory. After n context windows, MemoRAG progressively converts all raw tokens in X into memory tokens. Thus, we have $\\Theta(X) \\rightarrow \\Theta_{mem}(X) = \\{\\tilde{x}_m^1,\\ldots,\\tilde{x}_m^{k}\\}$, which represents the global memory $X_m$ formed from the input X."}, {"title": "2.2 TRAINING FOR MEMORY MODULE", "content": "As mentioned above, we initialize another set of weight matrices $W_{Qm}, W_{Km}$, and $W_{Vm}$ for the special purpose of mapping the memory tokens $x_m$ into query, key, and value vectors (as formulated in Eq. 7). The newly initialized weight matrices are updated during the training process, while the parameters of the underlying LLM remain frozen.\nWe train the newly initialized parameters in two stages: (1) Pre-training: we use randomly sampled long contexts from the RedPajama dataset to pre-train the model Soboleva et al. (2023), allowing MemoRAG's memory module to learn how to form memory from raw context; (2) Supervised Fine-tuning (SFT): we use task-specific SFT data to enable MemoRAG to generate task-specific clues based on the formed memory.\nWhile challenging, accurately remembering details from memory remains the ultimate goal of any human memory enhancement training. The training objective of the memory model in MemoRAG also pursues this goal, which can be formulated as:\n$\\max _{\\Theta_{\\text {mem }}} P\\left(x_{i, j} \\mid x_{1, \\ldots, 1, k_{i-1}}, x_{i, 1}, \\cdots, x_{i, j-1}\\right)$.\n(8)\nThe objective in Eq. (8) aims to maximize the generation probability of the next token given the KV cache of the previous memory tokens $\\{x_{1, \\ldots, 1, k_{i-1}}\\}$ and recent raw tokens $\\{x_{i, 1}, \\cdots, x_{i, j-1}\\}$."}, {"title": "2.3 THE MEMORAG FRAMEWORK", "content": "In the previous section, the input sequence X was transformed into a compact memory repre-sentation $X_m$, which encapsulates high-level semantic information from a global perspective. A straightforward way to utilize this memory $X_m$ is to prompt it to generate task-specific answers, i.e., y = @mem(Xm, q|0), where q represents the task description (e.g., a query or a summarization instruction). While this approach is feasible, it may lack information accuracy since $X_m$ is a highly condensed form of the raw input sequence X. This is analogous to how humans may struggle to recall detailed information from memory but can generate a draft answer, which can then be refined by revisiting and finding relevant evidence.\nIn MemoRAG, the global memory $X_m$ is used to generate task-specific clues y. These clues help outline the expected answers Y, effectively bridging the gap between the raw input context and the ground-truth answer. Based on these memory-generated clues, any stand-alone retriever can be employed to locate the precise evidence text within the input sequence, as defined in Eq. (2).\nSubsequently, the final answer Y is generated based on the retrieved evidence text, $Y = \\Theta_{gen} (\\tilde{X}, q|\\theta)$, where $\\tilde{X}$ includes the input query or task instruction q and the retrieved context \u0106. The generation model can be any generative LM, and by default, MemoRAG utilizes the underlying LM of the memory model for generation, avoiding the need to load additional model parameters."}, {"title": "2.4 APPLICABILITY", "content": "In the following sections, we demonstrate how global memory can expand the applicability of stan-dard RAG, making it suitable for more general-purpose tasks.\nAmbiguous Information Needs Information seeking with ambiguous information needs is chal-lenging for standard RAG, as the user's intent is often not explicitly stated, requiring deeper contex-tual understanding and inferential reasoning. To address this, MemoRAG creates a global memory across the relevant database, enabling it to infer the underlying intent of implicit queries. By gen-erating staging answers, such as more specific answer clues, MemoRAG bridges the gap between implicit information needs and the retrieval process. This significantly broadens the applicability of standard RAG systems to a wider range of tasks involving implicit queries.\nIn Table 1, we illustrate how MemoRAG handles implicit queries. For instance, the input query \"How does the book convey the theme of love?\" lacks a direct semantic connection with the content in the relevant database, as \u201cthe theme of love\" is not explicitly stated in the raw text. MemoRAG forms a global memory across the entire database and generates key answer clues that facilitate the retrieval of relevant content from the database.\nInformation Seeking with Distributed Evidence Query Information seeking with queries that require distributed evidence gathering introduces additional complexity for standard RAG systems, as these queries require integrating knowledge across multiple steps or unstructured database. Most retrieval methods struggle with these queries because they demand a coherent understanding of interrelated data points, often spanning different contexts.\nTo address this challenge, MemoRAG utilizes its global memory capabilities to connect and inte-grate relevant information across multiple steps within the database. By generating staging answers that guide the retrieval of interconnected data points, MemoRAG effectively manages the complex-ity of multi-hop queries. This approach allows MemoRAG to significantly enhance the performance of standard RAG systems in tasks that involve multi-hop reasoning.\nIn Table 2, we demonstrate how MemoRAG handles such a query. For instance, the input query \"Which year had the peak revenue in the past three years?\u201d requires analyzing financial data across multiple years. MemoRAG forms a global memory from the past ten years' financial reports of a large company, reformulates the multi-hop query into several specific query, and integrates this information to determine the peak revenue year.\nInformation Aggregation Information aggregation tasks, such as summarizing long documents, require the ability to condense large amounts of unstructured data into concise and coherent outputs. Standard RAG systems often struggle with these tasks because they rely on retrieving discrete pieces of information without a mechanism to effectively combine and summarize them into a comprehen-sive overview.\nMemoRAG addresses this challenge by leveraging its global memory to capture and synthesize key points from the entire dataset. Through this process, MemoRAG is able to generate intermediate staging answers that represent the essential elements of the content, which are then used to retrieve detailed information from the raw content. All of these information are aggregated to produce a final summarization.\nIn Table 3, we illustrate how MemoRAG handles an information aggregation task. For instance, the task is to summarize a government report on city construction. MemoRAG first extracts key points from the report, such as infrastructure development, budget allocations, and future planning, then retrieve detailed content, and aggregates these information to produce a comprehensive summary of the report.\nThe current version of MemoRAG primarily targets the aforementioned application scenarios. Be-low, we outline our planned application scenarios for MemoRAG, which could be achieved by fur-ther training on task-specific training data.\nPersonalized Assistant Personalized assistant tasks, such as recommending a song based on a user's preferences, require a deep understanding of the user's unique characteristics and history. This is because personalized information needs are often ambiguous, heavily influenced by the user's individual persona. Standard RAG systems may struggle with such tasks because they typically rely on general relevance matching rather than personalizing results based on specific user data.\nMemoRAG enhances personalization by leveraging global memory to analyze and understand the user's dialogue history. This allows MemoRAG to identify and utilize key clues such as the user's music preferences, knowledge background, age, and other relevant factors that can be inferred from past interactions. By synthesizing this information, MemoRAG can generate highly personalized recommendations that align closely with the user's tastes.\nIn Table 4, we demonstrate how MemoRAG handles a personalized recommendation query. For example, when asked, \"Can you recommend a song for me?\" MemoRAG analyzes the dialogue history of the user, identifying preferences for certain music genres, artists, or eras, and uses this information to suggest a song that fits the user's profile.\nLife-Long Conversational Search Conversational search frequently involves queries with omit-ted semantics, where the user's intent relies on the context of prior interactions. Query rewriting is a widely adopted technique to address this challenge. In life-long conversational search, the semantic dependencies of the current query may extend to much earlier interactions, making it essential to accurately identify the relevant context within an extremely long interaction history. Standard RAG systems may struggle with this task due to the ambiguity in the information need presented by the query. This is primarily because standard RAG systems lack the ability to effectively track and in-corporate the evolving conversational context, often resulting in incomplete or inaccurate retrievals.\nMemoRAG addresses this challenge by leveraging its global memory to maintain and utilize the full context of the conversational history. This enables the system to interpret queries with omitted semantics by referencing relevant previous exchanges and filling in the gaps in meaning. As a result, MemoRAG can accurately interpret and respond to follow-up queries that depend on prior conversation context.\nFor instance, consider the query, \"Does it have any weaknesses?\" posed after a discussion about a particular research paper. A standard retrieval system might struggle to understand what \"it\" refers to without explicit context, potentially retrieving irrelevant information. In contrast, MemoRAG would refer back to the previous conversation, recognize that \"it\" refers to the research paper being discussed, and then retrieve information about the paper's weaknesses.\nIn Table 5, we illustrate how MemoRAG handles a query with omitted semantics in a conversational search context. By drawing on the full conversational history, MemoRAG ensures accurate and contextually appropriate responses."}, {"title": "3 EXPERIMENT", "content": "The standard RAG system primarily focuses on QA tasks that involve explicit information needs. To assess the effectiveness of MemoRAG compared to the standard RAG system in such tasks, we evaluate both MemoRAG and baseline models using 13 existing benchmark datasets, including:\n(1) Single-Doc QA: NarrativeQA (Ko\u010disk\u00fd et al., 2017), Qasper (Dasigi et al., 2021), and Multi-FieldQA (Bai et al., 2023). (2) Multi-Doc QA: HotpotQA (Yang et al., 2018), 2WikiMQA (Ho et al., 2020), and MuSiQue (Trivedi et al., 2022). (3) Summarization: GovReport (Huang et al., 2021), MultiNews (Fabbri et al., 2019), and En.SUM Zhang et al. (2024). (4) Long-book QA: En.QA Zhang et al. (2024). For summarization tasks, we use the task instruct as a fake query.\nIn practical scenarios, not all user queries have explicit information needs. Most queries require a comprehensive understanding of the full context and the aggregation of multiple pieces of in-formation to obtain a final answer. To evaluate MemoRAG and standard RAG systems across a broad range of applications, we have constructed the ULTRADOMAIN benchmark. The benchmark comprises tasks with long context and high-level query on multiple specialized domains.\nFirst, we utilize contexts from datasets representing specialized areas of knowledge, focusing on two specialized datasets. The first is the Fin dataset, derived from financial reports. This dataset tests MemoRAG's ability to process and interpret complex financial data, ensuring that the system can handle the intricacies of financial language and reporting. The second is the Legal dataset, constructed from legal contracts. This dataset challenges MemoRAG to understand and navigate the complex and nuanced language of legal documents, where precision is critical.\nIn addition to these specialized datasets, we collected a diverse set of 428 college textbooks spanning 18 distinct domains, such as natural sciences, humanities, and social sciences, from this repository. These textbooks are used to test MemoRAG's versatility and adaptability across a wide range of topics that may not directly relate to the specialized datasets. By evaluating MemoRAG on these varied contexts, we gain a deeper understanding of its potential for broader applications beyond specific domains such as finance and law.\nLastly, we construct a dataset comprising of mixed context from above datasets, namely Mix. This mixed dataset is intended to evaluate how well MemoRAG can generalize its understanding across different types of context."}, {"title": "3.1 DATASET", "content": "The statistical details of the specialized datasets are provided in Table 8, while the details of the textbook-based datasets are shown in Table 9. Together, these datasets form a comprehensive bench-mark that rigorously tests MemoRAG's effectiveness in handling both domain-specific challenges and broader, cross-disciplinary tasks.\nFurther details on the datasets, their construction, and the evaluation metrics used in this study can be found in Appendix A."}, {"title": "3.2 BASELINES", "content": "We compare MemoRAG with the following baselines: (1) Full: Directly feeding the full context into LLMs to fit the maximum length of the LLMs. (2) BGE-M3 Chen et al. (2023): A general-purpose retriever, with which we perform standard RAG. (3) Stella-en-1.5B-v5\u00b9: This model ranks top-3 on the MTEB leaderboard at the time of writing this paper, and we perform standard RAG with it. (4) RQ-RAG Chan et al. (2024): RQ-RAG prompt LLMs to resolve the input query into several queries that are better for searching regarding explicit rewriting, decomposition, and disambiguation. The supporting passages are retrieved by both the input query and refined queries. (5) HyDE Gao et al. (2022): Directly prompts LLMs to produce fake documents by providing only a query and then retrieves passages using the fake documents, then producing the final answer generation according to the retrieved passages. For more comprehensive comparison, we use three popular LLMs as the generators: Llama3-8B-Instruct-8K 2; Mistral-7B-Instruct-v0.2-32K Jiang et al. (2023) and Phi-3-mini-128K Abdin et al. (2024)."}, {"title": "3.3 EXPERIMENTS ON ULTRADOMAIN", "content": "To evaluate MemoRAG's ability to generalize across diverse and complex tasks, we evaluate Mem-ORAG on the UltraDomain benchmark. Most queries in UltraDomain involve either ambiguous information needs or unstructured knowledge retrieval challenges. UltraDomain consists of two types of datasets. The first type includes three datasets with context sizes under 100K tokens which are in the same distributions of our training datasets. We refer to these three datasets as the in-"}, {"title": "3.4 EXPERIMENTS ON ALL BENCHMARKS", "content": "Table 7 shows the experiment results on three benchmarks, from which we can conclude that Mem-ORAG generally surpasses all baselines on all datasets, except for a single outlier. First, for open-domain QA tasks, MemoRAG improves performance over all baselines on all datasets except for en.qa with Llama3 as generator. This verifies that in the comfortable zone of standard RAG, where most queries have explicit information needs, MemoRAG can better locate the expected evidence within the original context, thanks to the memory-generated clues. Second, most previous RAG methods struggle with tasks that do not involve queries, such as summarization tasks (e.g., Multi-News, GovReport, and en.sum)\u00b3. Our MemoRAG enables the RAG system to generate key points from the input context and retrieve more details to form a comprehensive summary. Third, for domain-specific tasks (e.g., Financial and Legal), MemoRAG shows significant improvement, indi-cating MemoRAG's superiority in handling complex tasks with long contexts. In summary, the re-sults demonstrate that MemoRAG significantly enhances performance over standard RAG methods and other baselines across various datasets and query types. MemoRAG's ability to handle complex and long-context tasks effectively highlights its advantages, particularly in scenarios where standard RAG systems struggle. This consistency across different generators underscores the robustness and general applicability of MemoRAG."}, {"title": "4 CONCLUSION", "content": "In this paper, we introduce MemoRAG, a novel Retrieval-Augmented Generation (RAG) system that integrates global context-awareness to address the challenges posed by complex tasks involving long input contexts. MemoRAG features a memory module that constructs a compact global memory across the entire database, facilitating the generation of context-dependent clues that effectively link the knowledge database to the precise information required for accurate answers. Extensive experiments across knowledge-intensive QA, summarization, and real-world applications involving lengthy documents demonstrate that MemoRAG significantly outperforms traditional RAG systems. It excels in tasks requiring advanced information aggregation and exhibits exceptional robustness and versatility in managing large-scale texts, such as textbooks, financial reports, and legal contracts, handling contexts of up to one million tokens and resolving complex queries with superior accuracy."}]}