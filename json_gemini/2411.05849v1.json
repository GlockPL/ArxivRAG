{"title": "INPUT-DRIVEN DYNAMICS FOR ROBUST MEMORY RETRIEVAL\nIN HOPFIELD NETWORKS", "authors": ["Simone Betteti", "Francesco Bullo", "Giacomo Baggio", "Sandro Zampieri"], "abstract": "The Hopfield model provides a mathematically idealized yet insightful framework for understanding\nthe mechanisms of memory storage and retrieval in the human brain. This model has inspired four\ndecades of extensive research on learning and retrieval dynamics, capacity estimates, and sequential\ntransitions among memories. Notably, the role and impact of external inputs has been largely\nunderexplored, from their effects on neural dynamics to how they facilitate effective memory retrieval.\nTo bridge this gap, we propose a novel dynamical system framework in which the external input\ndirectly influences the neural synapses and shapes the energy landscape of the Hopfield model. This\nplasticity-based mechanism provides a clear energetic interpretation of the memory retrieval process\nand proves effective at correctly classifying highly mixed inputs. Furthermore, we integrate this\nmodel within the framework of modern Hopfield architectures, using this connection to elucidate\nhow current and past information are combined during the retrieval process. Finally, we embed both\nthe classic and the new model in an environment disrupted by noise and compare their robustness\nduring memory retrieval.", "sections": [{"title": "1 Introduction", "content": "Since the beginning of the 80s, the words \"Associative Memory Network\" have closely echoed with \"Hopfield Network\"\n[Hopfield, 1982, 1984], and a plethora of subsequent works have endeavored to provide a detailed picture of the\nproperties of such networks [Amit et al., 1987a, Crisanti et al., 1986, Treves and Amit, 1988]. Drawing from the toolbox\nof statistical mechanics, Hopfield networks provided a convincing explanation for the multi-stability of memories as\nfunction of the neurons couplings, and therefore a plausible, dynamic retrieval mechanism over an energy landscape.\nRecently, in a machine-learning driven Renaissance for associative memory networks, the original framework has\nbeen generalized to higher order interactions [Krotov and Hopfield, 2016] and to multi-layered architectures [Krotov\nand Hopfield, 2020, Chaudhry et al., 2023], thus endowing the model with both a significantly improved capacity\n[Demircigil et al., 2017] and a direct bridge to state-of-the-art transformer models and their attention mechanism [Hubert\net al., 2021]. Moreover, the new framework has paved the way for new hypotheses on how neurons and astrocytes could\ninteract [Kozachkov et al., 2023], at the functional level, to support cognitive processes. The effort to bridge formal\napproaches and neuroscience is of paramount importance for the advancement of both fields. As proposed in [Treves\nand Rolls, 1992], attractor dynamics may be a key component of hippocampal functioning, where the signal relayed by\ncortical areas is sparsified and orthogonalized in the CA3-CA1 regions [Yassa and Stark, 2011, Rolls, 2013]. In addition,\nsimple attractor models provide a viable tool to study global cortical dynamics in the brain [Russo and Treves, 2012,\nNaim et al., 2018], by partitioning the surface in interacting patches of cortex each idealized by Hopfield like networks.\nIn classic treatments on computational neuroscience [Amit, 1989, Dayan and Abbott, 2005, Gerstner et al., 2014],\nmemory retrieval in the Hopfield model is implicitly described as a two-step process. First, a noisy or incomplete input\nis presented as a cue and adopted as an initial condition. Then, driven by an energy landscape, the network state flows\ntowards the closest energy minimum representing the prototypical memory. The literature however lacks an explanation\nfor how an external input becomes an initial condition in the neural dynamics; it is worth noting that external inputs\nand initial conditions play distinct roles in the behavior of a dynamical system. Most importantly, while this classic\ntwo-step process is natural within an algorithmic paradigm, it fails to explain how neuronal circuits continuously react\nand adapt in real time to external inputs.\nIn light of these limitations, we advocate for a paradigm shift from a two-step mechanism, akin to a standard algorithmic\napproach, to a input-driven dynamic mechanism, aligned with the principles of online algorithms and continual learning\n[Zenke et al., 2017, Hadsell et al., 2020, Lesort et al., 2020]. To this extent, we propose a novel version of the Hopfield\nmodel that is driven by external inputs. A key feature of this model is that the input shapes the energy landscape and\naffects the resulting gradient descent flow (see Fig. 1). Furthermore, our model admits a simple representation as a\nmodern Hopfield network [Krotov and Hopfield, 2020, Hubert et al., 2021]; this representation provides a conceptual\nbridge with the recent literature on transformer models and machine learning. Finally, the addition of noise reveals\nthe advantageous integration of past and present information by our model, thereby reducing misclassification errors\ninduced by inconsistent or 'glitchy' inputs."}, {"title": "2 Results", "content": ""}, {"title": "2.1 Primer on Hopfield Networks", "content": "Hopfield networks are a fundamental tool in the study of high level, distributed memories retrieval [Hopfield, 1982].\nThey significantly simplify neural dynamics into the interplay of two components: a dissipative flow, constantly\npolarizing the network state towards its resting value, and a synaptic flow, which takes into account the weighted sum of\nthe incoming activity from other neurons in the network. Namely,\n$$\\begin{cases}\nx(t) = -x(t) + W\\Psi(x(t)) \\\\\nx(0) = x_0 \\in \\mathbb{R}^N\n\\end{cases}$$\nwhere the prototypical memories {$\\xi^{\\mu}$}$_{\\mu=1}^{P}$, $\\xi^{\\mu} \\in \\{-1,+1\\}^N$ are assumed to be orthogonal or with entries that are\nindependent and identically distributed. They are stored in the synaptic matrix $W$ through one-shot Hebbian learning as\n$$W = \\frac{1}{N} \\sum_{\\mu=1}^{P} \\xi^{\\mu} {\\xi^{\\mu}}^T$$\nUnder suitable assumptions on the activation function $\\Psi$ [Krotov and Hopfield, 2020], convergence to any of the stored\nmemories is guaranteed [Hopfield, 1984] by the existence of the energy function (see Fig. 1(d))\n$$E(x; W) = - \\frac{1}{2} x^T W \\Psi(x) + \\sum_{i=1}^{N} \\int_0^{x_i} {\\Psi(x)} dx$$\ngiven an appropriate initial condition $x_0$.\nThe model, first proposed as a discrete time dynamic system [Hopfield, 1982], has been particularly successful and\ncaptivating at explaining pattern reconstruction starting from a partial or corrupted cue [Gerstner et al., 2014, Dayan and\nAbbott, 2005]. In its simplicity, the original theory effectively framed memory retrieval in the cascade of reactions that\nlead a network of elementary computational units to fix in a meaningful collective state, i.e. the prototypical memory.\nFurthermore, the geometric constraints placed on the prototypical memories and on the synaptic matrix allowed for\na detailed study of the network capacity. In the original paper and subsequent works [Hopfield, 1982, Amit et al.,\n1987b] it was estimated at around 0.14N, and only later was refined [Petritis, 1996] at $\\frac{N}{6 log(N)}$ for exact convergence in\nprobability. These estimates represent the number of patterns that can be recovered in the asymptotic limit of infinite\nnetwork size without the endogenous noise disrupting their stability."}, {"title": "2.2 The Input-Driven Plasticity (IDP) Hopfield Model", "content": "The aim of this work is that of studying a plausible mechanism that, given a mixed input, first maintains fixation for\nshort transients on what was previously retrieved, that is past information, and then gradually merges it with the current\ninput, that is present information. This gradual integration should, at a certain point, favor the present information\nand retrieve the new correct memory. We thereby introduce a novel, externally modulated Hopfield model, named\ninput-driven plasticity (IDP) Hopfield model, that tries to capture this exact phenomenology. The new IDP Hopfield\nmodel is defined as\n$$\\dot{x}(t) = -x(t) + W(u(t))\\Psi(x(t))$$\nwhere for a generic input $u(t)$ the novel input modulated synaptic matrix is\n$$W(u(t)) = \\frac{1}{N} \\sum_{\\mu=1}^{P} \\langle \\xi^{\\mu}, u(t) \\rangle \\xi^{\\mu} {\\xi^{\\mu}}^T = \\frac{1}{N} \\sum_{\\mu=1}^{P} \\alpha_{\\mu}(t) \\xi^{\\mu} {\\xi^{\\mu}}^T$$\nwhere we call the $\\alpha_{\\mu}(t) := {\\xi^{\\mu}}^T u(t)$ saliency weights in accordance with previous literature [Blumenfeld et al., 2006,\nTang et al., 2010]. These previous studies have explored the role of saliency weights and their effect on the dynamics (7)\nwithin the context of the morphing problem, focusing on how these weights drive transitions between memories as they"}, {"title": "2.3 A Modern Interpretation", "content": "As we have mentioned in the introduction, the IDP model naturally lends itself to a description through the modern\nformalism outlined in [Krotov and Hopfield, 2020, Hubert et al., 2021]. By means of this formalism, a recurrent neural\nnetwork such as the Hopfield model can be effectively deconstructed into two interacting layers void of intralayer\nconnections.\nThe following tripartite architecture characterizes a more general model that captures the interaction of the activity in\nthe memory layer $y \\in \\mathbb{R}^P$ and in the saliency layer $\\alpha \\in \\mathbb{R}^P$. The new combined information is then exploited to drive\nthe retrieval in the feature layer $x \\in \\mathbb{R}^N$. In summary, the modern Hopfield reformulation of the IDP model is\n$$\\begin{aligned}\n\\tau_x \\dot{x}(t) &= -x(t) + M_x (y(t) \\odot \\alpha(t)) \\\\\n\\tau_y \\dot{y}(t) &= -y(t) + M_y \\Psi_y(x(t)) \\\\\n\\tau_\\alpha \\dot{\\alpha}(t) &= -\\alpha(t) + M_\\alpha \\Psi_\\alpha(u(t))\n\\end{aligned}$$\nwhere the symbol $\\odot$ is the Hadamard entrywise product, namely $(y(t) \\odot \\alpha(t))_i = y_i(t) \\alpha_i(t)$, and $\\Psi_\\alpha$ is an activation\nfunction implementing either a linear or non-linear processing of the input. Notice that when $M_x = M$, and $M_y = M_\\alpha =$\n$M^T$ with $M = \\frac{1}{\\sqrt{N}} [\\xi^1 ... \\xi^P] \\in \\frac{1}{\\sqrt{N}} \\{-1,1\\}^{N \\times P}$, and $\\Psi, \\Psi_\\alpha$ are identity functions, the equations (14),(15),\n(16) reduce to the IDP Hopfield model (7) in the limits $\\tau_x \\rightarrow 0$ and $\\tau_\\alpha \\rightarrow 0$. Unraveling the activity of the IDP Hopfield\nnetwork into the distinct components allows for a better qualitative understanding of the layers' contribution (see Fig. 4\nfor a block representation). The memory layer serves the function of pooling layer, projecting the activity of the feature\nlayer into a similarity space. The pooled activity is then modulated by the input decomposition. It is clear then that the\ncombination of pooling and modulation implements a natural trade-off between past internal activity and externally\nincoming information. In the model discussed so far we have the input processing $M_\\alpha \\Psi_\\alpha(u(t)) = Mu(t)$, because it"}, {"title": "2.4 IDP Induces Resiliency to Noise", "content": "Both the classic (5) and the IDP Hopfield (7) models are fully capable of retrieving the correct memory when subjected\nto the same well-mixed input (see Fig. 2(c,d)). This may mislead the reader to conclude that the two models are in fact\nequivalent. However, both are still ideal models, where neural units are perfectly insulated. Thus, they fail to capture\nthe essence of real neural phenomena in settings with ubiquitous background noise. Thus, we consider the associated"}, {"title": "3 Discussion", "content": ""}, {"title": "Achieving Robust Behavior in Multistable Systems", "content": "Multistability is a distinctive feature of the classical Hopfield model, required for its associative memory functionality.\nYet, multistability is typically avoided in controlled engineering systems because it potentially leads to fragile and\nunpredictable behavior. While complex systems, such as the electric power grid, may in fact exhibit multiple stability\nregions, only one of them corresponds to proper grid functioning. It is the responsibility of the grid control infrastructure\nto steer the system away from undesired stability regions. The proposed IDP Hopfield model fulfills this same\nresponsibility in the context of memory retrieval: when the input is not ambiguous, the neural state is reliably driven to\nthe correct stability region, achieving a robust and predictable memory system."}, {"title": "Energy Shaping in the IDP Hopfield Model", "content": "The IDP Hopfield model presents a simple yet effective explanation of how a direct input-driven modulation of the\nsynapses can enrich the dynamic range of recurrent neural networks. The input driven adjustments of the synaptic\ncouplings between neurons enforce a clear memory hierarchy, with single memories existing only if sufficiently\nstimulated. Furthermore, the input decomposition changes the stability properties of single memory patterns. Through\nthe existence of the threshold $\\alpha_{stability}$, a memory that was stable at a certain time instant can suddenly become a\nsaddle point, and thus allow the network dynamics to roll towards another memory, as proposed by Karuvally et\nal. in [Karuvally et al., 2023]. During the retrieval process, the saliency attributed to individual memories via input\ndecomposition reshapes the energy landscape of the model. This process deepens the wells associated with the most"}]}