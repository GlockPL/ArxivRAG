{"title": "MIRAGE-BENCH: Automatic Multilingual Benchmark Arena for Retrieval-Augmented Generation Systems", "authors": ["Nandan Thakur", "Suleman Kazi", "Ge Luo", "Jimmy Lin", "Amin Ahmad"], "abstract": "Traditional Retrieval-Augmented Generation (RAG) benchmarks rely on different heuristic-based metrics for evaluation, but these require human preferences as ground truth for reference. In contrast, arena-based benchmarks, where two models compete each other, require an expensive Large Language Model (LLM) as a judge for a reliable evaluation. We present an easy and efficient technique to get the best of both worlds. The idea is to train a learning to rank model as a \"surrogate\u201d judge using RAG-based evaluation heuristics as input, to produce a synthetic arena-based leaderboard. Using this idea, We develop MIRAGE-BENCH, a standardized arena-based multilingual RAG benchmark for 18 diverse languages on Wikipedia. The benchmark is constructed using MIRACL, a retrieval dataset, and extended for multilingual generation evaluation. MIRAGE-BENCH evaluates RAG extensively coupling both heuristic features and LLM as a judge evaluator. In our work, we benchmark 19 diverse multilingual-focused LLMs, and achieve a high correlation (Kendall Tau ($\\tau$) = 0.909) using our surrogate judge learned using heuristic features with pairwise evaluations and between GPT-40 as a teacher on MIRAGE-BENCH leaderboard using the Bradley-Terry framework. We observe proprietary and large open-source LLMs currently dominate in multilingual RAG. MIRAGE-BENCH is available: https://github.com/vectara/mirage-bench.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have recently gained popularity for information-seeking queries leading to the widespread adoption of Retrieval-Augmented Generation (RAG) (Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Borgeaud et al., 2022). The naive RAG setup traditionally includes a retrieval and a generation stage, conducted sequentially. RAG systems such as Bing Search (Microsoft, 2023) provide grounded responses, i.e., statments include citations to one or more retrieved passages. The citations reduce factual hallucinations with easy verifiability and improve support or faithfulness to passages provided within context (Khandelwal et al., 2020; Lewis et al., 2020; Gao et al., 2023a,b; Liu et al., 2024). However, existing RAG benchmarks are English-centric, due to uneven and scarce data available across multiple languages (Thakur et al., 2024b). So far, it is unclear how well existing LLMs perform in multilingual RAG, i.e., where queries and passages are non-English and the LLM generates a response in the same language. An example of RAG in Hindi language (hn) is shown in Figure 1. Existing RAG benchmarks can be broadly classified as either (i) heuristic-based, where benchmarks design multiple evaluation metrics to evaluate systems across multiple dimensions (Gao et al.,"}, {"title": "2 Related Work", "content": "Prior work on RAG evaluation has been conducted exclusively in English. For example,benchmarks such as ALCE (Gao et al., 2023a), FreshLLM (Vu et al., 2024), ClapNQ (Rosenthal et al., 2024), HAGRID (Kamalloo et al., 2023) and CRAG (Yang et al., 2024b), all include long-form answers for English-only queries and are based on collections containing documents from either the English Wikipedia, MS MARCO (Bajaj et al., 2016) or NQ (Kwiatkowski et al., 2019). Similarly, TREC 2024 RAG, an ongoing TREC competition for RAG evaluation is focused on English.\nMultilingual RAG. On the multilingual side, RAG has not been well studied in prior literature. The RGB benchmark (Chen et al., 2024c), is limited in language scope as it covers only one additional language: Chinese (zh). NeuCLIR (Mayfield et al., 2024) evaluates long-form report generation from participants in the upcoming 2024 track; but is limited to three languages. A concurrent work is BERGEN (Chirkova et al., 2024), which evaluates multilingual open-domain QA settings across 13 languages. In contrast, MIRAGE-BENCH evaluates the generation task in the multilingual RAG pipeline on 18 languages, provides multilingual instruction-tuned data for RAG fine-tuning, and evaluates on high-quality queries in MIRACL.\nLearning to rank. It is a supervised learning technique (Liu, 2010), where models are trained to provide an ordering between items in each list. The goal of constructing the ranking model is to rank new, unseen lists in a similar way to rankings in the training data (Turnbull, 2017). Models are trained"}, {"title": "3 MIRAGE-BENCH: A Multilingual RAG Benchmark", "content": "We select 18 languages in MIRAGE-BENCH as the starting point, representing an appropriate cross-section of the diversity of the languages spoken worldwide at this point. MIRAGE-BENCH serves as a comprehensive multilingual RAG benchmark focusing on the generation task evaluation. As shown in Table 1, MIRAGE-BENCH includes 11,195 evaluation pairs and 39,763 training pairs across 18 languages. We discuss the MIRAGE-BENCH evaluation and training datasets and highlight differences from the MIRACL dataset below:\n3.1 MIRAGE-BENCH Evaluation Dataset\nMIRACL introduced in Zhang et al. (2023), is a monolingual retrieval dataset, i.e., queries and passages are both in the same language for passage retrieval. Queries are high-quality and generated by native speakers (Zhang et al., 2023). The annotation procedure in MIRACL is identical to TyDI-QA (Clark et al., 2020). The passage collection is constructed from language-specific Wikipedia corpora and parsed using WikiExtractor. The MIRAGE-BENCH evaluation dataset is constructed re-using the queries and oracle-judged passages available in the MIRACL development split.\nWe incorporate two changes: (i) In Arabic (ar), we randomly sample a smaller subset of 1,501 out of 2,896 queries for uniformity in the number of pairs available across other languages. (ii) We filter out queries with zero non-relevant passages, i.e., we always include non-relevant passages, i.e., hard negatives, from MIRACL to make the MIRAGE-BENCH evaluation task challenging.\n3.2 MIRAGE-BENCH Training Dataset\nThe MIRAGE-BENCH training dataset is developed from the MIRACL (Zhang et al., 2023) training dataset. MIRACL only contains information on queries and relevant and non-relevant passages. In MIRAGE-BENCH, we reuse all the MIRACL training pairs available in 16 languages, except German (de) and Yoruba (yo). We convert the training dataset using a simple recipe into a multilingual instruction-tuned RAG dataset (Zhang et al., 2024; Niu et al., 2024). In our recipe, we filter the relevant passages, and keep them along with the input query to generate a zero-shot RAG output using strong teachers available including GPT-40 (OpenAI, 2023), Llama 3 (70B) (Dubey et al., 2024) and Mixtral (8x22B) (Jiang et al., 2024). After generation, we include non-relevant passages within our prompt as \"distracting and noisy information\", to help improve the quality of the MIRAGE-BENCH training dataset. Note that, since we convert a retrieval dataset, we do not have human-annotated answers for questions in MIRAGE-BENCH.\n3.3 Distinction and Extension from MIRACL\nMIRACL introduced in Zhang et al. (2023) is a monolingual retrieval dataset, which evaluates the \"retrieval task\", i.e. given a user query and a Wikipedia corpus, MIRACL contains human-annotated relevance judgements to evaluate for query-passage level relevancy using retrieval models, e.g., sparse models like BM25 (Robertson and Zaragoza, 2009) or bi-encoders like mDPR (Karpukhin et al., 2020), or late-interaction models like ColBERT (Khattab and Zaharia, 2020).\nIn contrast, MIRAGE-BENCH evaluates the \"generation task\" in RAG, requiring LLMs to generate a summarized answer given the query and context available from retrieved passages. In our work, we re-use the queries and oracle relevance-judgments from MIRACL and focus solely on evaluating the"}, {"title": "4 Multilingual RAG Evaluation", "content": "4.1 Heuristic-based Evaluation\nMultilingual generation in RAG requires the evaluation of multiple dimensions. For example, whether a system's response provides the correct final answer or cites the relevant documents, a single metric alone is not sufficient to capture the comprehensive evaluation required for RAG systems. Inspired by other recent works (Kiela et al., 2021; Santhanam et al., 2023; Gao et al., 2023a), we introduce five deterministic features and two LLM as a judge features for evaluation. We additionally provide details about each feature in Appendix A.\nLanguage detection. We compute the probability of a system's response in the required target language with langid (Lui and Baldwin, 2012). We compute two metrics: language detection (target language) and English detection.\nCitation quality. Using passage-level relevance judgments for all queries (or qrels) information available in MIRACL, we evaluate whether the system's response cites the relevant passages, crucial for measuring faithfulness. We compute and evaluate: Recall@k and MAP@k, where k = 10.\nSupport. Grounding is necessary to avoid hallucinations in the system's response. Support evaluation (Gao et al., 2023a) checks whether each sentence is supported by cited passages using a multilingual NLI model. We compute the probability of the entailment and neutral score, macro-averaged across the sentence-citation pairs.\nReranker score. The reranker score measures the average similarity (can be greater than 1.0) between the query and the passages cited within the system's response. We compute the reranker score using a multilingual reranker model, macro-averaged across the query-passage pairs.\nAnswer overlap. Having the correct answer is crucial in the RAG system's response. Since MIRAGE-BENCH does not include a human-labeled answer, we use the generated answer from GPT-4 (OpenAI, 2023) as the gold truth. We compute two traditional answer overlap metrics: SacreBLEU\n4.2 Arena-based Evaluation\nHeuristic-based evaluation metrics often rely on a gold standard for evaluation. Tasks such as text"}, {"title": "4.3 Learning to Approximate Rankings", "content": "There is no predefined way to aggregate the heuristic features to provide an overall leaderboard ranking in MIRAGE-BENCH. Averaging the scores is too simplistic, as features measure different aspects of RAG evaluation. On the other hand, arena-based evaluations provide ranked leaderboards but are computationally expensive to compute with a strong teacher model. To avoid computational costs, smaller models as teachers have been proposed (Thakur et al., 2024a; Ni et al., 2024). Motivated by similar observations, we train a learning to rank model as a surrogate judge to approximate the Bradley-Terry model coefficients (Hunter, 2004) learned from an arena-based evaluation that uses GPT-40 for pairwise judgments.\nRegression model. While the heuristic RAG features introduced in Section 4.1 can be computed efficiently and without the reliance on proprietary models, inducing a ranking from pairwise comparisons via a Bradley-Terry model is computationally expensive and requires access to a high-performance LLM. Furthermore, in Section 6.3, we demonstrate that the ranking accuracy, measured by the Kendall-Tau ($\\tau$) coefficient, degrades rapidly when subsampling tournament matches. Therefore we investigate whether a regression model can be successfully trained to approximate the Bradley-Terry logits using heuristic features.\nThe procedure, detailed in Algorithm 1, simulates $N_t$ tournaments, each involving a total of $N_i$ models and $N_q$ queries. For each query, judgments are obtained for all $\\binom{N_i}{2}$ pairings of models. We employ bootstrapping on the query selection process to estimate the variance in the $R^2$ metric in the regression models' approximations of the Bradley-Terry coefficients over a randomly-sampled holdout set, $\\text{LLM}_{\text{predict}}$.\nWe randomly selected two models, Gemma 1.1 (2B) as Llama-3 (70B) as holdout models. For English, we observed an average $R^2 = 0.971$ with a 95% confidence interval of [0.905, 0.999], while for Bengali, we observed $R^2 = 0.937$ with a 95% confidence interval of [0.766, 0.998]. All scores with 95% confidence intervals are listed in Table 5. Taken together, these results indicate that the training procedure is fairly robust with $N_q = 100$."}, {"title": "5 Experimental Settings", "content": "5.1 Multilingual Baselines\nExisting state-of-the-art LLMs are either English-only or support a limited set of languages, predominantly due to the curse of multilinguality for large models (Conneau et al., 2020), i.e., it is unclear how well-existing LLMs perform on RAG across a wide variety of languages, due to scarce availability of multilingual instruction tuning datasets. We experiment with models from seven different families, containing proprietary and open-sourced models. Wherever possible, we benchmark the instruction-tuned version if available. Please refer to Appendix B for more details on baselines.\n OpenAI: We evaluate the GPT-3.5-turbo, GPT-4, and GPT-40 models (OpenAI, 2023) using Azure OpenAI service.\n Mistral: We evaluate the Mistral-Instruct-v0.2 and v0.3 (7B) (Jiang et al., 2023), Mixtral (8x7B) and Mixtral (8x22B) (Jiang et al., 2024).\n Cohere: We evaluate the Command-R (35B), Command-R+ (104B) and Aya-23 (35B) models (Aryabumi et al., 2024)."}, {"title": "5.2 Evaluation Details", "content": "Prompt template. We internally optimized the ChatQA prompt template (Liu et al., 2024), to include in-text citations of the context passages following the IEEE format (Kamalloo et al., 2023). In MIRAGE-BENCH, we have about 10 passages annotated in the oracle setting. Therefore, we trim each passage available and take the first 800 tokens to fit all passages within a fixed context length of 8192 tokens. following prior work in Shi et al. (2023), the prompt requires the LLM to explain the multilingual generation task starting with \u201c##Reason\u201d and the answer itself starting with \u201c##Answer\u201d. Utilizing this output format has its advantages in easily parsing the generated answer and the rationale behind the answer. The prompt template is shown in Figure 10. For GPT-40 as a judge pairwise evaluation, we modified the prompt template available in RAGEval (Rackauckas et al., 2024). The prompt template is shown in Figure 13."}, {"title": "6 Experimental Results", "content": "6.1 Heuristic-based Results\nFigure 3 shows lollipop plots indicating the average heuristic-feature value (y-axis) distribution across all languages (x-axis). In English detection, smaller LLMs such as Gemma-1.1 (2B) do not generate output in the required target language, but rather rely on English. Next, for citation quality and support evaluation, OpenAI models achieve better Recall@10 and Entailment scores (except Llama-3 (70B) for a few languages), indicating baseline responses include grounded citations from relevant passages. In contrast, models such as Qwen-2 or Gemma-1.1 tend to under-cite in their response. Similar trends are observed for the reranker score.\nFurthermore, we observe OpenAI models achieve a higher word overlap in the ROUGE-L metric (GPT-4 used as ground truth) and for Llama-3 (8B) as a judge, we observe rather less variance in scores across models. In Fluency, we observe a majority of the baselines are rather fluent, except Bengali (bn), Telugu (te), and Yoruba (yo)."}, {"title": "6.2 Arena-based Results", "content": "Figure 4 (left) shows the arena-based leaderboard using bootstrapping and Bradley-Terry modeling after conducting 200 tournaments and sampling 100 matches per tournament on a subset of 100 queries using GPT-40 pairwise comparisons. We observe that proprietary models such as GPT-40 and GPT-4, and larger models such as LLAMA-3 (70B) and Mixtral (8x22B), are slightly better than other baselines on MIRAGE-BENCH. Baseline rankings across languages are usually stable; with a few notable exceptions such as Gemma-1.1 (7B) which achieves a rank of 4 in Telugu. Command R (35B) performs poorly in low-resource languages such as Bengali (rank 13) or Swahili (rank 14). The complete bradley-terry model coefficient logits and 95% confidence intervals across all 18 languages in MIRAGE-BENCH are provided in Table 7 and Table 8 in the Appendix.\nSynthetic rankings using the learning to rank model. Figure 4 (right) plots the overall synthetic average rankings on all queries using heuristic-based features trained with a random forest learning to rank model in MIRAGE-BENCH. The learned-ranking leaderboard highly correlates to the GPT-40 as a judge leaderboard, achieving an average Kendall-Tau ($\\tau$) rank correlation = 0.909, by training on 17 models during training and 2 models as holdout for every language. Individual language-specific Kendall-Tau rank correlation scores are listed in Table 2. This provides evidence of the efficacy in training a random forest model as a surrogate judge on heuristic features with bootstrapping to approximately learn the Bradley-Terry logits.\nHeuristic feature importance. In Figure 5, we plot the average feature importance achieved by our Random Forest regression model across all 18 languages. We observe using Llama-3 (8B) as a"}, {"title": "6.3 Ablations & Discussion", "content": "To better understand the gaps observed during training of the regression model, we conduct further ablations on a subset of seven languages including Arabic (ar), Bengali (bn), Finnish (fi), Japanese (ja), Korean (ko), Russian (ru) and Telugu (te):\nRegression model choice. We compare several learning to rank models as choices for learning the Bradley-Terry model coefficients. We conduct our experiments on the train set, where models contain pairwise judgments, and on a randomly sampled holdout set, a realistic scenario, with no available training data. We evaluate Random Forest, Linear Regression, MLP Regressor, XGB Regressor, and SVR. All learning to rank models are implemented via scikit-learn. The results are depicted in Table 3. Random forest achieves the best $R^2$ value on the holdout set for 4/7 languages. SVR also achieves a $R^2$ value on the holdout subset, however, underperforms random forest on the training dataset. Other baselines, such as XGB Regressor and MLP Regressor show signs of overfitting on the training subset, thereby underperforming random forest on the holdout subset in MIRAGE-BENCH."}, {"title": "7 Conclusion", "content": "We present MIRAGE-BENCH, a multilingual RAG benchmark for 18 languages aimed towards accelerating research on multilingual RAG. In MIRAGE-BENCH, we evaluate the multilingual generation part within RAG and aggregate traditional heuristic-based features to train a lightweight learning to rank model as a surrogate judge to learn a Bradley Terry model with GPT-40 pairwise judgments. Our results indicate a strong correlation between our surrogate judge and original LLM as a judge arena-based leaderboard demonstrating the effectiveness of our random forest model trained using RAG-based heuristic features to emulate a much expensive LLM as a judge pariwse rankings.\nUsing our arena-based leaderboard, we observed a majority of proprietary and open-sourced larger models currently dominate on the MIRAGE-BENCH benchmark. Instruction tuning on MIRAGE-BENCH training data helps improve the performance of open-sourced and smaller models on MIRAGE-BENCH. Instruction-tuned Mistral-v0.2 (7B) is able to outperform the Llama 3 (70B) baseline on MIRAGE-BENCH."}, {"title": "8 Limitations", "content": "MIRAGE-BENCH is one of the first holistic multilingual RAG benchmarks. Although not perfect, we below discuss a set of limitations in our work:\n In MIRAGE-BENCH, we focused on benchmarking the generation task in RAG, we did not evaluate the multilingual retrieval task and its error propogating on the multilingual generation task.\n In the future, we wish to evaluate diverse LLMs as teachers such as Claude-3.5 (sonnet) (Anthropic, 2024) or Gemini Pro (Anil et al., 2023), currently we only included a single teacher, GPT-40, as a judge for evaluation. This can cause self-enhancement bias in our experiments towards GPT-40 and similar models in the family (GPT-4 and GPT-3.5).\n In our heuristic evaluation, we only considered a small subset of features. We did not explore more recent RAG evaluation techniques such as nugget-based evaluation (Lin and Demner-Fushman, 2005).\n Lastly, MIRAGE-BENCH dataset does not provide human-labeled answers for queries across all languages and is limited to Wikipedia, unlike recent RAG benchmarks which use the human answer for RAG-based evaluation or cover a wider variety of domains in English."}]}