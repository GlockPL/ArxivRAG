[{"title": "Generalization capabilities of MeshGraphNets to unseen geometries for fluid dynamics", "authors": ["Robin Schm\u00f6cker", "Alexander Henkes", "Julian Roth", "Thomas Wick"], "abstract": "This works investigates the generalization capabilities of MeshGraphNets (MGN) [Pfaff et al. Learning Mesh-Based Simulation with Graph Networks. ICML 2021] to unseen geometries for fluid dynamics, e.g. predicting the flow around a new obstacle that was not part of the training data. For this purpose, we create a new benchmark dataset for data-driven computational fluid dynamics (CFD) which extends DeepMind's flow around a cylinder dataset by including different shapes and multiple objects. We then use this new dataset to extend the generalization experiments conducted by DeepMind on MGNs by testing how well an MGN can generalize to different shapes. In our numerical tests, we show that MGNs can sometimes generalize well to various shapes by training on a dataset of one obstacle shape and testing on a dataset of another obstacle shape.", "sections": [{"title": "Introduction", "content": "For many engineering applications, we need to have fast and accurate simulations of fluid dynamics. In this work we focus on the modeling of fluids by the incompressible Navier-Stokes equations [28, 66, 55, 30, 68, 31, 25, 40]. With regard to numerical modeling, various spatial discretization techniques are known such as the finite volume method [50] and the finite element method [30]. Temporal discretization is often based on finite difference schemes [31, 68] or space-time discretizations [49, 11, 5]. The arising nonlinear and linear systems are often solved with iterative techniques or multigrid schemes. Implementations on CPUs can be computationally expensive and can take a long time to run. To overcome this, we can use parallelization to multiple processors or GPUs [67, 39, 6, 5] or only refine the mesh in regions where the solution is not accurate enough [10, 14, 11, 56]. Nevertheless, even though this can speed up the simulation, fluid simulations can still be prohobitively expensive for multi-query problems, e.g. different material parameters, different geometries, or different boundary conditions. Therefore, for real-time simulations or multi-query problems, we need to find a way to speed up the simulation even further, e.g. by means of data-driven methods. A popular method to reduce the computational cost is to collect a large dataset of fluid simulations and project the dynamics onto a small set of eigen modes using the proper orthogonal decomposition [58]. On the one hand, using this kind of reduced order model is well-studied and we have error bounds for the reduced basis approximation [57]. On the other hand, proper orthogonal decomposition is an intrusive method, which makes it difficult to implement for nonlinear problems [9, 19, 20, 24, 7, 51, 4] and changing meshes [33].\nAnother approach is to use machine learning methods to predict the fluid dynamics [44]. This kind of approaches are appealing since they are generally non-intrusive and easier to implement, but they can be difficult to interpret [26, 70, 63, 15], can be sensitive to the training data and can be difficult to generalize to unseen data [52]. Albeit all these limitations, neural networks are frequently used for regression in the latent space [37], superresolution of coarse solutions [48] or physics-informed approaches [18]. More recently, convolutional neural networks have been used to learn the full solution field of fluid simulations [42]. Despite their success in computer vision tasks [43], convolutional neural networks work best on cartesian grids and not for complex geometries, e.g. random obstacles in the flow field, where parts of the neural network input that is not part of the domain needs to be masked [34]. To overcome this restriction of architectures that are based on cartesian grids, like convolutional neural networks or fourier neural operators, graph neural networks [17, 16, 60] can be used to directly work on the finite element mesh [47].\nIn this work, we will focus on a graph neural network-based solution introduced by Google DeepMind [59, 54]. The method is based on the idea of encoding the system state into a mesh graph, applying a graph neural network to this graph to predict the change of the system, and then applying this change to the simulation state. This method has found wide adoption for physics simulations with applications in weather forecasting (GraphCast) [45] and multiscale simulations for better efficiency [27]. The application of these neural networks is usually to inverse problems [2] or for fast fluid simulations on unseen geometries [21, 69]. The latter is the focus of this work. We numerically investigate the generalization capabilities [64] of MeshGraphNets to unseen geometries for fluid dynamics by predicting the flow around a new obstacle that was not part of the training data. For this purpose we create new benchmark datasets [13] for the instationary incompressible Navier-Stokes equations that extend DeepMind's flow around a cylinder dataset by including different shapes and multiple objects.\nThe outline of this paper is as follows. In Section 2, we introduce neural networks, MeshGraphNets method and the Navier-Stokes equations. Next, in Section 3, we present the experiment setup and discuss the generalization of MeshGraphNets that have been trained on one obstacle shape to another obstacle shape. Our work is summarized in Section 4."}, {"title": "MeshGraphNets", "content": "We briefly recapitulate the MeshGraphNets (MGN) method introduced by Pfaff et al. in [54]. MGNs are a deep learning approach for predicting the evolution of complex physics systems by applying graph neural networks to a mesh graph. The method is based on the idea of encoding the system state into a mesh graph, applying a graph neural network to this graph to predict the change of the system, and then applying this change to the simulation state. The method is autoregressive, meaning that the previous output can be used as the new input, allowing for the simulation to be rolled out to obtain the next states. The method is trained using a differentiable loss function and noise is added to the training data to force the MGN to correct any mistakes before they can accumulate in future states.\nWe first give an abstract definition of neural networks and then introduce the concept of MGNs for fluid dynamics simulations governed by the Navier-Stokes equations."}, {"title": "Neural Networks", "content": "An artificial neural network (ANN) is a parametrized, nonlinear function composition. By the universal function approximation theorem [38], arbitrary Borel measurable functions can be approximated by ANN. There are several different formulations for ANN, which can be found in standard references such as [12, 32, 1, 29, 22]. Following [35], most ANN formulations can be unified. An ANN $N$ is a function from an input space $\\mathbb{R}^{d_x}$ to an output space $\\mathbb{R}^{d_y}$, defined by a composition of nonlinear functions $h^{(l)}$, such that\n$N: \\mathbb{R}^{d_x} \\to \\mathbb{R}^{d_y}$\n$x \\to N(x) = h^{(n_L)} \\circ ... \\circ h^{(0)} = y$, $l=1,...,n_l$.   (1)\nHere, $x$ denotes an input vector of dimension $d_x$ and $y$ an output vector of dimension $d_y$. The nonlinear functions $h^{(l)}$ are called layers and define an $l$-fold composition, mapping input vectors to output vectors. Consequently, the first layer $h^{(0)}$ is defined as the input layer and the last layer $h^{(n_L)}$ as the output layer, such that\n$h^{(0)} = x \\in \\mathbb{R}^{d_x}$,   $h^{(n_L)} = y \\in \\mathbb{R}^{d_y}$.   (2)\nThe layers $h^{(l)}$ between the input and output layer, called hidden layers, are defined as\n$\\mathbf{h}^{(l)}< {\\mathbf{h}^{(l)}_\\eta, \\eta = 1,...,n_u},   \\mathbf{h}^{(l)}_\\eta = \\phi^{(l)}_\\eta \\circ \\tilde{\\phi}^{(l)} (\\mathbf{W}^{(l)}_\\eta \\cdot \\mathbf{h}^{(l-1)})$,   (3)\nwhere $\\mathbf{h}^{(l)}_\\eta$ is the $\\eta$-th neural unit of the $l$-th layer $\\mathbf{h}^{(l)}$ and $n_u$ is the total number of neural units per layer, while $\\cdot$ denotes a product. Following the notation in [41], the symbol $<$ denotes an abbreviation of a tuple of mathematical objects $(O_1, O_2, ...)$, such that $O < (O_1, O_2, ...)$. In (3), the details of type-specific layers $\\mathbf{h}^{(l)}$ are gathered in general layers $\\mathbf{h}^{(l)}$ from (1). The specification follows from the $\\cdot$-operator, which denotes the operation between the weight vector $\\mathbf{W}^{(l)}_\\eta$ of the $\\eta$-th neural unit in the $l$-th layer $\\mathbf{h}^{(l)}$ and the output of the preceding layer $\\mathbf{h}^{(l-1)}$, where the bias term is absorbed [1]. If $\\cdot$ is the ordinary matrix multiplication $\\cdot = \\cdot$, then the layer $\\mathbf{h}^{(l)}$ is called dense layer. In the context of GNNs, the choice of the operator as $\\cdot = \\oplus$, where $\\oplus$ is a permutation invariant aggregation operator, yields the so-called message passing network [16].\nFurthermore, $\\phi^{(l)} : \\mathbb{R} \\to \\mathbb{R}$ is a nonlinear activation function and $\\tilde{\\phi}^{(l)}$ is a function of the previous layer, such that $\\tilde{\\phi}^{(l)} : \\mathbf{h}^{(l-1)} \\mapsto \\tilde{\\phi}^{(l)} (\\mathbf{h}^{(l-1)})$. If $\\tilde{\\phi}^{(l)}$ is the identity function, the layer $\\mathbf{h}^{(l)}$ is called a feedforward layer. All weight vectors $\\mathbf{W}^{(l)}$ of all layers $\\mathbf{h}^{(l)}$ can be gathered in a single expression, such that\n$\\theta= {W^{(l)}_\\eta}$.   (4)\nwhere $\\theta$ inherits all parameters of the ANN $N(x)$ from (1). Consequently, the notation $N(x;\\theta)$ emphasises the dependency of the outcome of an ANN on the input on the one hand and the current realization of the weights on the other hand. The specific combination of layers $\\mathbf{h}^{(l)}$ from (3), neural units $\\mathbf{h}^{(l)}_\\eta$ and activation functions $\\phi^{(l)}$ from (3) is called topology of the ANN $N(x;\\theta)$. The weights $\\theta$ from (4) are typically found by gradient-based optimization with respect to a task-specific loss function [32]."}, {"title": "Mesh Graph Networks", "content": "In this section, we introduce MGN, a deep learning approach for predicting the evolution of complex physics systems by applying graph neural networks to a mesh graph. This is based on the paper [54] by Pfaff et al. which is an extension of their previous paper on graph network based simulations [59]. In the following and overall in this paper, we introduce and test MGNs for the Flow around the Cylinder benchmark (abbreviated as Cylinder Flow). However, we notice that our developments can be applied to other computational fluid dynamics numerical tests as well."}, {"title": "Incompressible Navier-Stokes equations", "content": "Let $\\Omega \\subset \\mathbb{R}^2$ be some domain with sufficiently smooth boundary $\\partial \\Omega$. Moreover, let $(0, T)$ be the time interval with end time value $T > 0$. To generate training data, we model incompressible, viscous fluid flow with constant density and temperature by the Navier-Stokes equations\n$\\partial_t v - \\nabla_x \\cdot \\sigma + (v \\cdot \\nabla_x) v = 0 \\text{ in } \\Omega \\times (0, T)$,\n$\\nabla_x \\cdot v = 0 \\text{ in } \\Omega \\times (0, T)$,\n$v = v_D \\text{ on } \\partial \\Omega \\times (0, T)$,\n$v(0) = v^0 \\text{ in } \\Omega \\times {0}$,\nwhere for the stress $\\sigma$ we use the unsymmetric stress tensor\n$\\sigma := \\sigma_p = -pI + \\nu \\nabla_x v$.\nPlugging the definition of the unsymmetric stress tensor into the Navier-Stokes equations leads to the following formulation:\nFormulation 2.1 (Incompressible Navier-Stokes equations). Find the vector-valued velocity $v : \\Omega \\times (0, T) \\to \\mathbb{R}^2$ and the scalar-valued pressure $p : \\Omega \\times (0, T) \\to \\mathbb{R}$ such that\n$\\partial_t v + \\nabla_x p - \\nu \\Delta_x v + (v \\cdot \\nabla_x) v = 0 \\text{ in } \\Omega \\times (0, T)$,\n$\\nabla_x \\cdot v = 0 \\text{ in } \\Omega \\times (0, T)$,\n$v = v_D \\text{ on } \\partial \\Omega \\times (0, T)$,\n$v(0) = v^0 \\text{ in } \\Omega \\times {0}$.\nHere, $\\nu > 0$ is the kinematic viscosity, $v^0$ is the initial velocity and $v_D$ is a possibly time-dependent Dirichlet boundary value. In the following, we directly address the time-discrete problem. To this end, $s_k = (v_k, p_k) : \\Omega \\to \\mathbb{R}^3$ denotes the system state (i.e. the velocity and pressure field) with domain $\\Omega \\subset \\mathbb{R}^2$ at time $\\Delta t_k$ with the time step size $\\Delta t > 0$ and the time step index $k \\in \\mathbb{N}$. Additionally, we use $\\tilde{s}_k = (\\tilde{v}_k, \\tilde{p}_k) : \\Omega \\to \\mathbb{R}^3$ to denote our prediction for $s_k$. To create the dataset, we employ the finite element method using an incremental pressure correction scheme (IPCS) [46] which is based on Chorin's method [23]."}, {"title": "MGN idea", "content": "On a high level, the MGN approach can be explained as follows: First, we triangulate the domain to represent it as a mesh graph. We then encode the current system state $s_k$ into the graph's nodes and edges. Next, we use a graph neural network to predict quantities which can be used to directly compute how $s_k$ evolves to $s_{k+1}$. In Cylinder Flow, we will use an MGN to directly predict the pressure field $p_{k+1}$, and the change in velocity $\\delta v = v_{k+1} - v_k$."}, {"title": "System state encoding", "content": "In this step, we want to encode the system state $s_k$ into a graph. As a first step, the domain $\\Omega$ has to be triangulated to obtain a mesh graph $G = (V, E)$ where $V \\subseteq \\mathbb{N}$ are the vertices and $E \\subseteq {e \\in \\mathcal{P}(V) : |e| = 2}$ are the edges.\nNext, we encode the position information into the graph: Let $e = {v_1, v_2} \\in E$ and $x_{v_1}, x_{v_2} \\in \\Omega$ be the domain coordinates associated with these two vertices. We assign the distance $||x_{v_1} - x_{v_2}||_2$ and the relative displacement $x_{v_1} - x_{v_2}$ as features to $e$. It would also be possible to encode the absolute positions in the nodes. Experiments showed, however, that this is not a viable option [54]. To better capture the structure of the mesh, the node type is added as a feature to each vertex. In the case of Cylinder Flow there are four types of nodes:\n*   fluid nodes,\n*   wall (top, bottom, and cylinder) nodes,\n*   inflow nodes,\n*   outflow nodes.\nBesides the node type we also add the values of the fields of $s_k$ needed to predict $s_{k+1}$ as features to each vertex. For example, in the case of Cylinder Flow, each vertex $\\nu \\in V$ with associated coordinates $x \\in \\Omega$ gets the value of the velocity field $v_k$ at position $x$. Though pressure is also part of $s_k$, it is not encoded as $v_k$ already determines the pressure values hence they are not needed to predict $s_{k+1}$. The resulting graph and its associated features now approximately model $s_k$."}, {"title": "Graph processing", "content": "In this step, we use the graph-encoding of the previous step to predict $s_{k+1}$. For the incompressible Navier-Stokes equations, we directly predict the pressure $p_{k+1}$ and the change in velocity $\\delta v = v_{k+1} - v_k$. The processing is done in three different steps.\n1.  First, to prepare the features for processing, we use a feedforward-network to encode the edge features and another feedforward-network to encode the vertex features at each edge/vertex. We say that the features are embedded into latent space. In theory, this step is not needed. However, in practice without this step, the network would not perform well. This is because in the next step, these features will iteratively be updated to finally arrive at the prediction for the quantity change. If we do not encode the features in this first step, the information captured in these intermediate states is always restricted to the dimension of the input features. Therefore, this step can be viewed as widening the information bottleneck.\n2.  Next, we process the previously encoded features. First, we apply a neural network $N^M$ to all edges to update their edge features. Let $e = {v_1, v_2} \\in E$ and $F(v_1), F(v_2), F(e)$ be the features associated with this edge/nodes. The updated edge features $F'(e)$ are given as follows:\n$F'(e) = N^M (F(e), F(v_1), F(v_2))$.   (5)\nUsing $F'(e)$, we update the node-features using another neural network $N^V$. Let $v \\in V$ and $N_v \\subset E$ be the set of edges connected to $v$. Again, we denote their associated features with $F(v)$ and $F'(e)$ for $e \\in N_v$. The updated features $F'(v)$ are defined as follows:\n$F' (v) = N^V \\Big( F(v), \\sum_{e \\in N_v} F'(e) \\Big)$.   (6)\nAfter this, we simply replace $F(v)$ by $F'(v)$ and $F(e)$ by $F'(e)$ for each vertex/edge. This is called one message passing block (MPB). In total, we apply $L$ identical message passing blocks to the features (i.e. $N^M, N^V$ are fixed).\nNote that within each message passing block, information can flow only from one vertex to all its neighbors, because its information first passes to its edges by $N^M$ and then to its neighbors by $N^V$. Hence with $L$ MPBs only vertices with a distance of less than $L$ steps can be reached which means that even though in theory nodes can influence each other no matter their distance, this is not captured by this processing step.\n3.  In this last step, we apply a final feedforward-net (called the decoder) to the latent features of each vertex obtained after the last MPB step. This output will then be used to directly update the system state as will be described in the following."}, {"title": "Updating the system state", "content": "Assume that $q_{k+1} : V \\to \\mathbb{R}^3$ is the output of our MGN with input $s_k$. We interpret $q_{k+1}$ as the derivative of the velocity field $v_k$ and the pressure $p_{k+1}$, i.e. $q_{k+1} = (q_{1,k+1}, q_{2,k+1}) \\approx (\\partial_t v_k, p_{k+1})$ where $\\partial_t v_k$ denotes the time derivative of the velocity field at time $\\Delta t_k$. Our prediction $\\tilde{s}_{k+1}$ at node $\\nu \\in V$ with coordinates $x \\in \\Omega$ is then given by\n$\\tilde{s}_{k+1}(x) = (v_k(x) + \\Delta t \\cdot q_{1,k+1}(\\nu), q_{2,k+1}(\\nu))$.   (7)"}, {"title": "Training MGNs", "content": "In this section, we discuss the training details of MGNs. First, note that each step as detailed in the previous section is differentiable. Hence, the predicted quantities at each vertex are differentiable with respect to the parameters of all neural networks that were used. Therefore, we can define a differentiable loss $\\mathcal{L}$ at each node $v \\in V$ with coordinates $x \\in \\Omega$ by taking the mean-squared-error of the MGN prediction and the ground truth field values. Hence, the loss $\\mathcal{L}$ is\n$\\mathcal{L}(\\nu) = \\frac{1}{2} || \\tilde{s}_{k+1}(x) - s_{k+1}(x) ||^2$.   (8)\nThe MGN can then be trained using gradient descent on $\\mathcal{L}$ averaged on each vertex. Note that this requires access to the ground truth values $s_k$ at the vertices or approximations of these which might not always be efficiently available.\nThough this training setup works in theory, experiments [54] showed that this does not suffice to predict states far in the future and not just the next time step. This problem is addressed by adding noise to the training data and thus forcing the MGN to correct any mistakes before they can accumulate in future states. For all simulations in the training data, at every time step $k$ and at each vertex $\\nu \\in V$ with coordinates $x \\in \\Omega$, we independently sample normally distributed noise $\\epsilon = (\\epsilon_1, \\epsilon_2)$ with $\\epsilon_1, \\epsilon_2 \\sim \\mathcal{N}(0, \\sigma)$ with zero mean and a fixed variance $\\sigma \\in \\mathbb{R}^+$. This noise is then added to the velocity field $v_{\\nu}(x)$ to obtain the noisy velocity field $v_\\nu^{noisy}(x) = v_k(x) + \\epsilon$. Then, instead of $v_k$, the noisy velocity field $v_\\nu^{noisy}$ is fed as the input to the MGN whose next state prediction we then denote by $s_{k+1}^{noisy}$ which is then used in Equation (8) instead of $s_{k+1}$. The target $s_{k+1}$ remains unchanged. This modification allows the MGN to learn how to denoise the system state thereby learning how to reduce error accumulation as shown experimentally by DeepMind [54]."}, {"title": "Generalization Capabilities of MeshGraphNets and Numerical Simulations", "content": "In this section, we will present experimental results on the generalization of MGNs to unseen datasets. First, in Section 3.1 we will describe our experiment setup. Then, in Section 3.2 we will present the datasets of the generalization experiments, followed by the results in Section 3.3 and the runtimes in Section 3.4. The code used for these experiments is publicly available [65]."}, {"title": "Setup of the experiments", "content": "In this section, we will describe the exact setup for the MGNs used for the generalization experiments. We will also describe which evaluation metrics for the MGNs were used for the subsequent experiments."}, {"title": "Implementation", "content": "For the generalization experiments, we use the implementation contained in NVIDIA's Modulus, a machine learning framework for physics [36, 53]. More specifically, the exact implementation contained in examples/cfd/vortex_shedding_mgn\u00b9 from NVIDIA Modulus [53] was used for experimentation with modifications only affecting logging and the learning rate schedule which is now only updated after every epoch such that the learning rate in epoch $l$ is $\\eta_l = \\eta_1 \\gamma^{l-1}$ for some initial learning rate $\\eta_1$ and learning rate decay $\\gamma$."}, {"title": "MGN architecture details", "content": "We highlight some notable architectural choices of the MGNs.\n*   Input and target normalization: The training data set is used to compute the mean and variance of all edge features and of the velocity and pressure for the node features. The features of all inputs and targets are then normalized using their corresponding mean and variance.\n*   Residual connections: The MGN will be extremely deep, therefore residual connections are added such that the input of each MBP is directly added to its output.\n*   Layer normalization: Since we will be using a batch size of one but still want to normalize the hidden-layer outputs to reduce the covariate shift (i.e. gradients in layer $i$ tend to highly depend on the outputs of layer $i-1$), we employ layer normalization [8] at the output of the edge/node encoder and after each MPB (before the residual connection). Layer normalization transforms an output $x \\in \\mathbb{R}^n$ as follows:\n$y = \\frac{x - \\mathbb{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}} \\cdot \\alpha + \\beta$   (9)\nwhere $\\mathbb{E}[x]$ is subtracted componentwise and\n$\\mathbb{E}[x] = \\frac{1}{n} \\sum_{i=1}^{n} x_i,   \\text{Var}[x] = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mathbb{E}[x])^2$   (10)\nwhere $\\alpha, \\beta \\in \\mathbb{R}$ are learnable parameters and $\\epsilon > 0$ for numerical stability. We used $\\epsilon = 10^{-5}$."}, {"title": "Hyperparameters", "content": "The exact hyperparameters from examples/cfd/vortex_shedding_mgn\u00b2 from NVIDIA Modulus [53] were used for the subsequent experiments. The only exception is the learning rate decay factor $\\gamma$ since we used a slightly different implementation of the learning rate decay schedule as described in Subsection 3.1.1."}, {"title": "Evaluation criteria", "content": "For each experiment, we use at least 10% of the total dataset for evaluation. The exact evaluation datasets will be mentioned in the corresponding experiment. Let $K \\in \\mathbb{N}$ denote the number of simulations, let $J \\in \\mathbb{N}$ denote the number of time steps in each simulation, and let $I_k \\in \\mathbb{N}$ denote the number of mesh nodes in the $k$-th simulation with $k \\le K$ of the evaluation dataset. For evaluation, we measure eight quantities. The first four are derived from the root-mean-squared-error (RMSE) of the velocity. Let $v_{i,j,k} \\in \\mathbb{R}^2$ denote the velocity vector at mesh node $i \\le I_k$ and time step $j \\le J$ in the $k$-th ($k \\le K$) simulation of the evaluation dataset obtained by performing the IPCS (more details on the dataset generation in the next section). Furthermore, we use $\\tilde{v}_{i,j,k}$ to denote the MGN velocity prediction from $v_{i,j-1,k}$ if $j > 1$ and $v_{i,1,k}$ if $j = 1$. Lastly, by $\\hat{v}_{i,j,k}$ we denote the MGN velocity prediction from $\\tilde{v}_{i,j-1,k}$ if $j > 1$ and $v_{i,1,k}$ if $j = 1$. From which MGN the predictions come will be specified every time one of the following error quantities is reported.\n*   Velocity 1-step error $\\epsilon_1^{(v)}$: This error is the RMSE for all the next state velocity predictions of all timesteps of all simulations which is given by\n$\\epsilon_1^{(v)} = \\sqrt{\\frac{\\sum_{k=1}^{K} \\sum_{j=1}^{J} \\sum_{i=1}^{I_k} || v_{i,j,k} - \\tilde{v}_{i,j,k} ||^2}{2 I_k J K}}$.   (11)\n*   Velocity 50-steps error $\\epsilon_{50}^{(v)}$: Each simulation is rolled out for 50 timesteps. The RMSE is then taken for the rollout velocity at each of the first 50 time steps compared to the IPCS result for all simulations. This given by\n$\\epsilon_{50}^{(v)} = \\sqrt{\\frac{\\sum_{k=1}^{K} \\sum_{j=1}^{50} \\sum_{i=1}^{I_k} || v_{i,j,k} - \\hat{v}_{i,j,k} ||^2}{2 I_k \\cdot 50 \\cdot K}}$.   (12)\n*   Velocity all-steps error $\\epsilon_{\\text{all}}^{(v)}$: Each simulation is fully rolled out. The RMSE then compares the rollout velocity prediction at each time step to the IPCS velocity for all simulations. Formally, this is\n$\\epsilon_{\\text{all}}^{(v)} = \\sqrt{\\frac{\\sum_{k=1}^{K} \\sum_{j=1}^{J} \\sum_{i=1}^{I_k} || v_{i,j,k} - \\hat{v}_{i,j,k} ||^2}{2 I_k J K}}$.   (13)\n*   Velocity all-steps median $\\epsilon_{\\text{all,median}}^{(v)}$: We first calculate the all-steps RMSE for each individual simulation $k \\le K$ as\n$\\epsilon_{\\text{all}, k}^{(v)} = \\sqrt{\\frac{\\sum_{j=1}^{J} \\sum_{i=1}^{I_k} || v_{i,j,k} - \\hat{v}_{i,j,k} ||^2}{2 I_k J}}$.   (14)\nThe error $\\epsilon_{\\text{all,median}}^{(v)}$ is then given by the median of $(\\epsilon_{\\text{all}, 1}^{(v)}, ..., \\epsilon_{\\text{all}, K}^{(v)})$.\nWe ran each experiment with three different seeds. For each of the above error quantities, we report the mean and the maximum deviation from the mean across all three seeds. For example, we would report the three values {0,3,12} as 5 \u00b1 7.\nThe remaining four error quantities $\\epsilon_1^{(p)}$, $\\epsilon_{50}^{(p)}$, $\\epsilon_{\\text{all}}^{(p)}$ and $\\epsilon_{\\text{all,median}}^{(p)}$ come from doing the same with the pressure predictions."}, {"title": "Generalization experiments: Datasets", "content": "The experiments in this section have the aim of expanding DeepMind's MGN generalization experiments. DeepMind claims that their MGN has strong generalization capabilities due to using a relative encoding on the mesh graphs", "54": "."}, {"54": "assuming that uniform distributions were used. One notable difference to the DFG3 2D-2 benchmark problem [62", "0.41)": "backslash C \\subseteq \\mathbb{R"}, 2, "tandard_cylinder", 0.5], "cylinder_stretch": "Same as in standard_cylinder except that the circle may now be an ellipse with height $h \\sim \\mathcal{U"}, [0.02, 0.08], "and width $w \\sim \\mathcal{U", [0.02, 0.08], ".", "n*   cylinder_tri_quad: There is a $\\frac{1}{3}$ chance that a circle as in standard_cylinder is chosen. There is another $\\frac{1}{3}$ chance that a square with a midpoint sampled the same way as the ellipse's and side length $s \\sim \\mathcal{U}[0.02\\sqrt{2}, 0.08\\sqrt{2}]$ and a random orientation with a uniformly sampled angle is chosen. And another $\\frac{1}{3}$ chance that an equilateral triangle with side length $s \\sim \\mathcal{U}[0.078, 0.182]$ and a"]