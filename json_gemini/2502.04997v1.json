{"title": "Aligning Black-box Language Models with Human Judgments", "authors": ["Gerrit J. J. van den Burg", "Gen Suzuki", "Wei Liu", "Murat Sensoy"], "abstract": "Large language models (LLMs) are increasingly used as automated judges to evaluate recommendation systems, search engines, and other subjective tasks, where relying on human evaluators can be costly, time-consuming, and unscalable. LLMs offer an efficient solution for continuous, automated evaluation. However, since the systems that are built and improved with these judgments are ultimately designed for human use, it is crucial that LLM judgments align closely with human evaluators to ensure such systems remain human-centered. On the other hand, aligning LLM judgments with human evaluators is challenging due to individual variability and biases in human judgments. We propose a simple yet effective framework to align LLM judgments with individual human evaluators or their aggregated judgments, without retraining or fine-tuning the LLM. Our approach learns a linear mapping between the LLM's outputs and human judgments, achieving over 142% average improvement in agreement across 29 tasks with only a small number of calibration examples used for training. Notably, our method works in zero-shot and few-shot settings, exceeds inter-human agreement on four out of six tasks, and enables smaller LLMs to achieve performance comparable to that of larger models.", "sections": [{"title": "Introduction", "content": "Recent improvements to the reasoning capabilities of large language models (LLMs) have increased their use for judgment and evaluation tasks that would previously have been addressed with human evaluators (Zheng et al., 2023b; Chiang and Lee, 2023). For example, LLMs have been used for judging the relevance in information retrieval systems (Faggioli et al., 2023), the coherence of written discourse (Naismith et al., 2023), or the quality of translations (Kocmi and Federmann, 2023), among many others. A common theme in these settings is that LLMs are performing a grading task where the output label is on an ordinal scale. However, recent work has shown that the performance of LLMs on such judgment tasks varies widely depending on the specific task and the specific LLM used (Bavaresco et al., 2024).\nWe argue that the performance of LLMs on such judgment tasks does not automatically align well with that of human evaluators. Moreover, LLMs can exhibit their own response styles on these judgment tasks, which can be overly positive, in part due to the nature of supervised fine-tuning during their training regime. Indeed, as we show in Figure 1a, LLMs can avoid negative judgments entirely on tasks where humans show more diverse judgments. Thus, an additional step is needed to align the judgment labels of the LLM with that of a human evaluators. In this work, we propose a simple but effective approach to align the LLM outputs with those of human annotators using only a small set of labeled data. We show on 29 tasks that this additional alignment step significantly improves the performance of LLM judgments on the test set. Figure 1b clearly indicates that the judgment distributions of LLMs becomes much closer to that of the human evaluators after our alignment approach. Specifically, we propose to learn a mapping of the LLM outputs to human judgments to better align with those on a training set. In contrast to previous work on LLM calibration (Zhao et al., 2021; Han et al., 2023; Fei et al., 2023; Zhou et al., 2024), our technique is applicable without access to the model logits, which makes it much easier to use with general-purpose black-box LLMs available to the public. Moreover, we show that with our alignment approach we can bring the performance of smaller LLMs on par with that of larger ones, which can significantly reduce the cost of using LLM judges at scale.\nOur key contribution is a simple yet effective approach to learn a linear mapping between a lan-"}, {"title": "Methodology", "content": "Our approach aims to align the judgments provided by an LLM with those given by human evaluators, even when their respective output spaces differ. This alignment process is done without modifying the underlying LLM, making it efficient and easy to apply in practice."}, {"title": "Problem Formulation", "content": "Consider a judgment task where, for a given input instance $x_i$, the goal is to predict an output label $y_i \\in \\mathcal{Y}$, where $\\mathcal{Y}$ is the set of possible labels given by human evaluators. For instance, in a book review evaluation task, human judgments may take on values from $\\mathcal{Y} = \\{\\text{bad}, \\text{good}, \\text{average}\\}$, implying $|\\mathcal{Y}| = 3$. We assume we have a small training dataset $\\{(x_i, y_i)\\}_{i=1}^{N}$ containing instances $x_i$ and their corresponding human judgments $y_i$.\nOn each task instance $x_i$ we prompt an LLM to provide its own judgment, potentially with a more nuanced set of options, denoted by $\\mathcal{Z}$. For example, the LLM might use labels $\\mathcal{Z} = \\{\\text{bad}, \\text{neutral}, \\text{good}, \\text{excellent}\\}$ implying $|\\mathcal{Z}| = 4$ or stars between 1 and 5 implying $|\\mathcal{Z}| = 5$. Thus,"}, {"title": "One-hot Encoding Representation", "content": "To facilitate alignment, we convert both human and LLM judgments into one-hot encoded vectors. Specifically, for a judgment $y_i \\in \\mathcal{Y}$ by a human evaluator, we represent it as a one-hot vector $\\mathbf{y}_i \\in \\{0, 1\\}^n$, where $n = |\\mathcal{Y}|$. Similarly, for a judgment $z_i \\in \\mathcal{Z}$ by the LLM, we have $\\mathbf{z}_i \\in \\{0, 1\\}^m$, with $m = |\\mathcal{Z}|$. For a dataset of $N$ instances, we denote the matrices of one-hot representations as:\n$\\mathbf{Y} = [\\mathbf{y}_1, \\mathbf{y}_2, ..., \\mathbf{y}_N]^T \\in \\{0, 1\\}^{N \\times n}$\n$\\mathbf{Z} = [\\mathbf{z}_1, \\mathbf{z}_2, ..., \\mathbf{z}_N]^T \\in \\{0, 1\\}^{N \\times m}$."}, {"title": "Learning the Mapping", "content": "Our goal is to learn a mapping $\\phi: \\mathcal{Z} \\rightarrow \\mathcal{Y}$, from the LLM's output space $\\mathcal{Z}$ to the human judgment space $\\mathcal{Y}$, such that the transformed LLM judgments align closely with human labels. To achieve this, we define a linear transformation matrix $\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$, which maps the one-hot encoded LLM judgments to the human ones. The optimal matrix $\\mathbf{W}$ is obtained by solving the following regularized least squares problem:\n$\\mathbf{W} = \\underset{\\mathbf{W}}{\\text{arg min}} ||\\mathbf{Z}\\mathbf{W} - \\mathbf{Y}||_F^2 + \\lambda ||\\mathbf{W}||_F^2, \\quad(1)$ where $||\\cdot||_F$ denotes the Frobenius norm, and $\\lambda > 0$ is a regularization parameter to prevent overfitting and to avoid a singular system, as in ridge regression (Hoerl and Kennard, 1970). The closed-form solution to this optimization problem is given by:\n$\\mathbf{W} = (\\mathbf{Z}^T\\mathbf{Z} + \\lambda \\mathbf{I})^{-1}\\mathbf{Z}^T\\mathbf{Y}, \\quad (2)$\nwhere $\\mathbf{I} \\in \\{0, 1\\}^{m \\times m}$ is the identity matrix."}, {"title": "Aligned Judgment Inference", "content": "Once we have the learned transformation matrix $\\mathbf{W}$, we can align one-hot representation of an LLM's judgment $\\mathbf{z}$ for a new instance by:\n$\\phi(\\mathbf{z}) = \\underset{j=1,...,n}{\\text{arg max}}\\{\\mathbf{z}\\mathbf{W}\\_j\\}, \\quad (3)$ where $\\{\\mathbf{z}\\mathbf{W}\\_j\\}$ represents the transformed value for each human judgment. The aligned LLM judgment is determined by selecting the human judgment corresponding to the highest transformed value, reflecting the human judgment that the alignment process associates most closely with the LLM's original output.\nThis approach allows us to flexibly map LLM judgments to human judgments, regardless of differences in output space sizes or interpretations. For example, if the LLM output space consists of 4 labels while the human output space has only 3, the learned transformation matrix $\\mathbf{W} \\in \\mathbb{R}^{4 \\times 3}$ informs a projection that aligns LLM outputs with human annotations. This flexibility remains crucial even when the output spaces of LLMs and humans are identical, as the interpretation of the same labels can differ significantly, particularly in highly subjective judgment tasks."}, {"title": "Experiments", "content": "To evaluate the effectiveness of our proposed alignment approach, we conduct a series of experiments across 29 tasks with three widely-adopted large language models: Claude-3 Sonnet (Anthropic, 2024), Mixtral 8x7B Instruct (Jiang et al., 2024), and Llama-3 70B Instruct (Dubey et al., 2024). With the exception of the Feedback-QA dataset (Li et al., 2022), each of the datasets and prompts used to query LLMs are obtained from Judge-Bench, a benchmark dataset proposed recently by Bavaresco et al. (2024) to evaluate LLMs as judges. The datasets we consider from Judge-Bench include LLMBar (Zeng et al., 2024), Medical-Safety (Abercrombie and Rieser, 2022), Newsroom (Grusky et al., 2018), SummEval (Liu et al., 2023), WMT-20-EnDe (Freitag et al., 2021), and the ROSCOE metrics (Golovneva et al., 2023) on COSMOS-QA (Huang et al., 2019), DROP (Dua et al., 2019), ESNLI (Camburu et al., 2018), and GSM8K (Cobbe et al., 2021). Since many datasets have multiple metrics for evaluation, this results in a total of 29 tasks.\nIn our evaluations, we primarily focused on the case where both LLMs and human evaluators operate within the same judgment space, implying that $\\mathcal{Z} = \\mathcal{Y}$. This setup allows us to directly assess and discuss the impact of alignment on the agreement between LLM and human judgments. Notably, if $\\mathcal{Z} \\neq \\mathcal{Y}$, the agreement metric would not be straightforward to calculate without an alignment step. In our experiments, we set $\\lambda = 10^{-6}$ in eq. (2), as it is primarily used to avoid issues with matrix inversion.\nOur primary objective was to assess the agree-"}, {"title": "Zero-shot Judgments", "content": "In Table 1, we present the accuracy of LLM judgments compared to human ones, both before and after the alignment process. The results indicate a significant increase in agreement post-alignment, demonstrating the effectiveness of our approach in enhancing LLM performance in alignment tasks.\nOverall, our approach significantly improves"}, {"title": "Judgments with In-Context Learning", "content": "Our findings in the zero-shot setting motivated us to investigate whether the sources of misalignment between LLMs and humans could be addressed through in-context learning (Brown et al., 2020), by providing examples of human judgments for each output label. Specifically, for each human judgment label we include in the LLM prompt an example on which human evaluators agreed, aim-"}, {"title": "Transferring alignments", "content": "For judgment tasks that use grading scales with the same number of categories and the same meaning it is possible to evaluate whether an alignment learned on one task can be used on a different task. To test this, we learn an alignment on the coherency task of the Roscoe-Cosmos dataset, and test it on the coherency task of the other Roscoe datasets (each of which evaluates the coherence of GPT-3's response on reasoning tasks). The results in Ta-ble 4 indicate that using the transferred alignment gives better performance than the non-aligned LLM judges in Table 1, and is only slightly worse than when using a task-specific alignment."}, {"title": "Related Work", "content": "The use of large language models as automated judges has been gaining traction due to their potential for scalability and efficiency (Zheng et al., 2023b; Bavaresco et al., 2024). However, as these models are increasingly relied upon to evaluate recommendations, search results, and other subjective tasks, it is crucial that their judgments align closely with human evaluators to ensure evaluations remain human-centered and useful.\nGilardi et al. (2023) investigated the capabilities of ChatGPT compared to human annotators across tweet annotation tasks including relevance, stance, topics, and frame detection. The results show that ChatGPT's zero-shot accuracy exceeds that of crowd workers and echoes the findings on annotating political affiliation in tweets by T\u00f6rn-"}, {"title": "Discussion", "content": "Our method provides a simple yet powerful framework to align LLM judgments with human evaluations, offering substantial improvements without the need for model retraining or fine-tuning. By learning linear mappings between LLM outputs and human judgments, we achieve significant improvement in agreement, a result that is both significant and practical for real-world applications where human labeled data is scarce. Furthermore, in four out of six tasks where there are multiple human annotators, the LLM performance after our alignment exceeds the inter-human agreement.\nThe simplicity and scalability of our approach is a key advantage. With only minimal calibration examples, we allow smaller models to perform at levels comparable to much larger, more resource-intensive LLMs. This shows that our method may greatly extend the usefulness of smaller models in judgment tasks, opening up opportunities for their deployment in a wider range of applications. This is especially valuable in resource-constrained settings, where using large models may be impractical or too costly.\nAlthough this extension is beyond the scope of the current paper, we note that when model logits are available, the one-hot encoded vectors representing LLM judgments can be replaced with the judgment probabilities derived from logits values. This modification could enable potentially finer alignment between LLM outputs and human evaluations. Furthermore, our alignment method can be extended to integrate other supervised learning techniques or objectives for handling more complex tasks, or to condition alignment on model inputs for improved performance where necessary. We"}, {"title": "Limitations", "content": "While our proposed alignment approach demonstrates significant improvements in aligning LLM judgments with human evaluations, there are some limitations to consider.\nFirst, the effectiveness of our method may vary depending on the specific characteristics of the domain and tasks involved. Some domains may present more complex alignment challenges and require more complicated alignment approaches, which may need larger number of training examples as a potential downside.\nSecond, while we compare our approach with in-context learning, we did not comprehensively explore the prompt space to see if we can improve the agreement and alignment through prompt optimisation. Instead, we used the prompts provided along with the task datasets from Judge-Bench"}, {"title": "Limited Training Examples", "content": "In Table 5, we demonstrate the performance of our approach when the number of training examples are limited to 20 samples while 300 samples are used for testing. We used 25% of the dataset as training examples if the total number of examples in the dataset less then 320 samples. The table indicates that our approach can also work in extremely low data regimes."}, {"title": "Prompt Example", "content": "Figure 3 shows an example of the prompt we used in both our zero-shot and few-shot experiments for the medical safety (response type) task."}]}