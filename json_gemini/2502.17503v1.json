{"title": "Doctor-in-the-Loop: An Explainable, Multi-View Deep Learning Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer", "authors": ["Alice Natalina Caragliano", "Filippo Ruffini", "Carlo Greco", "Edy Ippolito", "Michele Fiore", "Claudia Tacconi", "Lorenzo Nibid", "Giuseppe Perrone", "Sara Ramella", "Paolo Soda", "Valerio Guarrasi"], "abstract": "Non-small cell lung cancer (NSCLC) remains a major global health challenge, with high post-surgical recurrence rates underscoring the need for accurate pathological response predictions to guide personalized treatments. Although artificial intelligence models show promise in this domain, their clinical adoption is limited by the lack of medically grounded guidance during training, often resulting in non-explainable intrinsic predictions. To address this, we propose Doctor-in-the-Loop, a novel framework that integrates expert-driven domain knowledge with explainable artificial intelligence techniques, directing the model toward clinically relevant anatomical regions and improving both interpretability and trustworthiness. Our approach employs a gradual multi-view strategy, progressively refining the model's focus from broad contextual features to finer, lesion-specific details. By incorporating domain insights at every stage, we enhance predictive accuracy while ensuring that the model's decision-making process aligns more closely with clinical reasoning. Evaluated on a dataset of NSCLC patients, Doctor-in-the-Loop delivers promising predictive performance and provides transparent, justifiable outputs, representing a significant step toward clinically explainable artificial intelligence in oncology.", "sections": [{"title": "1. Introduction", "content": "Non-small cell lung cancer (NSCLC) is the most common subtype of lung cancer, constituting approximately 85% of lung cancer cases [20]. Currently, the main treatment for early-stage and resecatable locally advanced NSCLC is surgery, despite a notable number of patients experiencing post-surgery recurrence. Neoadjuvant therapy (NAT) has shown potential in improving overall survival rates and reducing the risk of distant disease recurrence [14, 2]. For patients undergoing pre-operative treatments, achieving a complete pathological response, indicating the absence of tumor cells in all specimens, may have a potential prognostic role and serve as a surrogate endpoint of survival [17]. Evaluating the complete pathological response before surgical resection could tailor the type of treatment to the needs of patients, ensuring that aggressive, surgery-based interventions are reserved only for patients who are most likely to benefit from them. However, while complete pathological response is prognostically significant, it is relatively rare to achieve in NSCLC patients [22]. In contrast, major pathological response, defined as the presence of no more than 10% viable tumor cells, is observed in a larger proportion of NSCLC patients and has been associated with significant clinical benefits, including improved progression-free survival [15]. Additionally, major pathological response's higher prevalence enables the identification of a broader cohort of patients who may benefit from NAT [22]. Notably, considering a slightly higher threshold for viable tumor cells also helps address potential inter and intra-observer variability in pathological evaluations [23], effectively acting as a margin of error. Consequently, major pathological response represents a more achievable, reliable and clinically relevant endpoint for assessing NAT efficacy in NSCLC [9]."}, {"title": null, "content": "For this reason, in this study we selected major pathological response as the main predicting endpoint, using the term pathological response (pR) to collectively refer to both complete and major pathological response.\nState-of-the-art reports that specific biomarkers, such as tumor mutational burden or tumor infiltrating lymphocytes [5, 13], as well as radiomics features extracted from Computed Tomography (CT) scans [1, 6, 11, 25], correlate with pR. However, both approaches have limitations. Biomarkers are typically assessed after the biopsy, which not only exposes patients to potential complications from the invasive procedure, but may also fail to fully capture tumor heterogeneity. Similarly, the radiomics approach, while non-invasive, relies on hand-crafted feature definition and selection methods, which may not be sufficient to account for the complexity and variability within the tumor [21].\nA powerful solution to address these issues is to leverage Deep Learning (DL), which has demonstrated remarkable success in cancer-related tasks, such as tumor segmentation, diagnosis, and classification [4]. Despite its success, an aspect which is gaining increasing importance is the explainability of the results generated by deep models because trust and transparency are essential for their adoption, especially in a critical field like healthcare [10]. Although recent studies have exploited the combination of DL and medical images to predict pR [12, 19, 16, 24], only a few have incorporated eXplainable Artificial Intelligence (XAI) techniques to analyze the obtained results [19, 24]. Specifically, these studies relied only on post-hoc methods applied after training. Such methods are generated separately from the trained models, which limits their faithfulness to the model's actual decision-making process. In contrast, intrinsic explainability, achieved by integrating explainability directly into the model architecture during training, inherently offers faithful and transparent explanations of models' outcomes [10]. To the best of our knowledge, in the context of pR prediction, none has yet focused on incorporating intrinsic explainability to achieve not only accurate but also explainable results. This lack of explainability represents a significant limitation in healthcare applications, where understanding what the model focuses on is as important as the predictions themselves.\nThe challenge of explainability is closely tied to how neural networks are trained. The training process of a neural network typically focuses on minimizing a loss function to optimize performance. In imaging-based classification tasks, this often leads the network to identify patterns or regions in the data that effectively discriminate between classes. However, the medical"}, {"title": null, "content": "relevance of such patterns is not inherently considered. Consequently, the model may prioritize features that improve classification performance but hold no meaningful connection to the underlying medical context. This disconnection between model learning and domain-specific insights emphasizes the need for XAI systems that align decision-making processes with clinical knowledge. In medical contexts, prior domain knowledge, such as clinically relevant anatomical regions, is often available and can guide the model toward medically meaningful insights. However, existing methods rarely incorporate mechanisms to direct learning toward these known areas of interest, leaving a gap in the literature.\nMoreover, a significant challenge in the clinical field is dealing with small datasets, as medical annotated data are typically scarce and difficult to collect. This complicates the training of deep models, which generally perform better when large datasets are available. Typically, models trained on large datasets can autonomously learn which features to focus on to provide accurate predictions. However, when dealing with small datasets and complex tasks, such as pR prediction, the model may struggle to identify the most relevant features on its own. In this context, it becomes crucial to guide the model towards the relevant areas that are most likely to influence the prediction. This results in a method, called Doctor-in-the-Loop, which leverages XAI techniques to integrate domain knowledge from clinicians in the training process, guiding the model's focus to clinically relevant regions. Thus, our goal is to bridge the gap in generating both accurate and explainable results in the context of pR prediction and to develop a solution capable of effectively integrating prior clinical knowledge in the training process while handling the complexity of the task. Additionally, our approach progressively refines the model's focus from broader views, such as the entire lung region area, to more detailed views, like the lesion itself. This multi-view approach distinguishes our work from state-of-the-art studies, which typically analyze only a single view (the lesion region), relying on features extracted from this area to predict pR [1, 6, 11, 25, 12, 19, 16, 24].\nThe main contributions of this work are the following:\n\u2022 We propose a technique that leverages the interaction between DL and XAI on CT imaging to achieve a non-invasive, robust and explainable prediction of pR in NSCLC patients undergoing NAT therapy.\n\u2022 We propose the Doctor-in-the-Loop method to guide the model's focus through domain knowledge, ensuring that the model focuses on"}, {"title": null, "content": "clinically relevant regions, addressing the challenges of a complex task.\n\u2022 We introduce a gradual multi-view approach that integrates different views, allowing the model to focus both on broad anatomical regions and specific sites, thus providing a more comprehensive analysis.\n\u2022 We compare our approach with state-of-the-art approaches for the prediction of the pR achieving higher predictive accuracy and clinical relevance.\nThe paper is organized as follows: Section 2 presents the state-of-the-art of pR estimation techniques; Section 3 describes the proposed method; Section 4 details the dataset used for this study, the image-preprocessing, and the experiments performed; Section 5 shows and discusses the results obtained; Section 6 provides the final conclusions."}, {"title": "2. Background", "content": "Accurately predicting pR in NSCLC patients undergoing NAT is of utmost importance, as pR serves as a critical endpoint for assessing treatment efficacy. Since a lower proportion of residual tumor cells after surgery correlates with better prognoses [15], achieving pR provides clinicians with valuable insights into tumor sensitivity to the administered therapy, allowing for more tailored treatment strategies. Given the high rates of post-surgery recurrence in NSCLC, patients often receive NAT to reduce the risk of recurrence by decreasing the tumor size, downstaging the tumor stage, and minimizing the need for extensive surgery [14, 2]. This approach can lead to organ preservation and an improved quality of life.\nSeveral studies have investigated the role of biomarkers in predicting the treatment efficacy in NSCLC. For instance, tumor mutational burden or tumor infiltrating lymphocytes, have been shown to correlate with pR [5, 13]. However, these biomarkers are typically measured through invasive biopsies, which carry a significant morbidity risk due to its invasive nature and may not provide a complete picture of the tumor heterogeneity. These limitations underscore the need for non-invasive approaches for the pR prediction.\nIn response to the limitations of biomarker-based methods, other studies exploited the potential of radiomics as a promising non-invasive technique for extracting quantitative features from medical images that correlate with"}, {"title": null, "content": "treatment response. Studies have demonstrated that radiomics-based models can predict pR by analyzing textural, shape, and intensity features of the tumor region [1, 6, 11, 25]. Agrawal et al. [1] utilized a private dataset of 101 NSCLC patients treated with preoperative chemoradiation to investigate the association between quantitative CT-based tumor volume measurements and pR. They reported that the reduction in tumor volume following treatment achieved an Odds Ratio of 1.06, while the pre-treatment tumor volume achieved an Odds Ratio of 0.99. Similarly, Coroller et al. [6] investigated the value of radiomics data, extracted from pre-treatment CT scans, in predicting pR. They used a private dataset of 85 NSCLC patients undergoing neoadjuvant chemoradiation and their findings indicated that 3 hand-crafted radiomics features, related to sphericity of the primary tumor and homogeneity of the lymph nodes, were significantly predictive of pR, achieving an Area Under the Curve (AUC) of 0.68 on the test set. Jiang et al. [11] analyzed a private dataset of 130 NSCLC patients who underwent neoadjuvant chemotherapy, leveraging hand-crafted intratumoral and peritumoral radiomics features to predict pR. Their model achieved an AUC value of 0.87 on the test set. Ye et al. [25] studied a private dataset of 178 NSCLC patients from four centers who underwent neoadjuvant immunochemotherapy. By analyzing the intensity and texture radiomics features extracted from four tumor subregions, they demonstrated the correlation between the tumor's internal heterogeneity and pR prediction, achieving an AUC of 0.78 in an external validation cohort. Despite the relevance of these findings, radiomics has some limitations, as it often struggles to capture complex patterns within the data that DL approaches can handle more effectively [21].\nMore recently, some studies have highlighted the feasibility of DL approaches to predict pR using CT images, as these models can automatically learn relevant features without the need for manual intervention [12, 19, 16, 24]. Lin et al. [12] conducted a retrospective analysis on a private dataset of 62 NSCLC patients treated with neoadjuvant immunotherapy. They extracted radiomics (first-order, shape and intensity features) and DL features using a ResNet-50 architecture, achieving an Accuracy (ACC) of 0.70 and 0.62, respectively. She et al. [19] used a private dataset of 274 NSCLC patients from four centers who underwent neoadjuvant chemoimmunotherapy. They applied a 3D-ShuffleNetv2x05 architecture to extract features from pre-treatment CT scans to predict pR, achieving AUC values of 0.73 and 0.72 in the internal validation and external validation cohorts, respectively. Similarly, Qu et al. [16] analyzed a private dataset of 248 NSCLC patients from"}, {"title": null, "content": "three centers treated with neoadjuvant immunotherapy. They employed a ResNet-152 architecture for feature extraction to predict pR, obtaining an AUC of 0.77 and 0.74 in the validation set and external cohort, respectively. Ye et al. [24] evaluated a private dataset of 225 NSCLC patients from four centers treated with immunochemotherapy. They used a foundation model (FM-LCT) to extract features from both non-contrast-enhanced and contrast-enhanced pre-treatment CT scans to predict pR, achieving an AUC of 0.86 on a test set encompassing patients from three different centers.\nWhile Lin et al. [12] and Qu et al. [16] did not include any explainability analysis in their studies, She et al. [19] and Ye et al. [24] incorporated a post-hoc Grad-CAM analysis to visualize the areas of the CT scans that the model focused on for its predictions, providing some level of explainability. Although post-hoc explanations offer some insights about the model's functioning, they are generated separately from the trained model, which may limit their faithfulness to the model's actual decision-making process. These limitations of post-hoc XAI approaches are particularly problematic in the healthcare context, where clinicians require a detailed and trustworthy understanding of how the models reach their predictions. In contrast, intrinsic explainability integrates explainability directly into the model during training, ensuring that the reasoning behind predictions aligns with the model's internal mechanisms. This approach inherently provides faithful and transparent explanations of model outcomes. In the broader domain of chest imaging, only one study [3] explored intrinsic explainability using a loss-guided attention approach. However, this study focused on disease classification rather than outcome prediction and did not incorporate a gradual multi-view framework as the one we proposed. In the specific context of pR, intrinsic explainability has, to the best of our knowledge, been overlooked, leaving a significant gap in the literature.\nTo address these issues, we present a method that exploits intrinsic explainable approaches within the training loop to develop a model that uses pre-treatment CT scans to predict pR in NSCLC patients treated with chemoradiation. This approach ensures that the model not only predicts pR, but also provides faithful insights into its decision-making process."}, {"title": "3. Methods", "content": "In many applications, particularly in medical imaging, the accurate interpretation of images is central, necessitating not only high predictive per-"}, {"title": null, "content": "formance but also a deep level of model explainability. Traditional neural network training paradigms often optimize for specific loss functions, aimed at improving accuracy, without providing insights into their decision-making processes, potentially leading to trust issues, especially in critical fields like healthcare. While neural networks trained on large datasets can autonomously learn which features to focus on to provide accurate predictions, when datasets are small, as is typical of medical contexts, and the task is complex, the network may struggle to identify the most relevant features independently. Hence, it becomes crucial to guide the model's focus through domain-specific insights, ensuring that the model focuses on clinically relevant areas, provided by domain experts.\nAs shown in Figure 1, to address these challenges, we introduce the Doctor-in-the-Loop training paradigm, which utilizes a Gradual Learning (GL) process to progressively integrate multiple CT views during the training phase, enhancing both the performance and transparency of the model. These views range from the broad global image to more detailed areas defined by expert-provided segmentation masks, effectively incorporating domain knowledge throughout the training process. We iteratively refine the model's focus by aligning it with these expert-driven masks, transitioning from general anatomical regions to specific, clinically relevant areas.\nThe Doctor-in-the-Loop paradigm represents a shift from traditional training methods by adapting to the task's complexity over time and ensuring that the model's learning trajectory is continuously guided by expert insights. This approach not only enhances the model's accuracy in critical regions but also boosts explainability and adaptability in real-world applications where expert knowledge plays a crucial role.\nThe following sections detail the model formulation, loss functions, and the comprehensive training procedure."}, {"title": "3.1. Model Formulation", "content": "Let $X \\subset \\mathbb{R}^{H\\times W\\times D}$ represent the input image space, where each image $x \\in X$ has dimensions $H$ (height), $W$ (width), and $D$ (depth), with $D$ indicating the number of slices in the image. The corresponding label space $Y$ consists of discrete class labels, represented as one-hot encoded vectors for a set of classes $\\{1, 2, ..., C\\'\\}$.\nThe convolutional neural network is characterized by its parameters $\\theta$, which include the weights and biases across its layers, and is designed to perform image classification. The network consists of a fully convolutional"}, {"title": "3.2. Loss Functions", "content": "To train our model effectively, we employ a combination of loss functions that target both prediction accuracy and explainability. Our primary objective is to correctly classify the input images based on the learned features, achieved through a classification loss $L_{cls}$, detailed in Section 3.2.1. However, given the critical importance of explainability in medical applications, we integrate an XAI loss $L_{xai}$, detailed in Section 3.2.2, which encourages the model to align its focus with expert-provided regions of interest. During training, the network optimizes a weighted composite loss function $L$ that combines the classification loss and the XAI loss, defined as:\n$L(\\theta; x, y, M_i) = L_{cls}(\\theta; x, y) + \\lambda L_{xai}(\\theta; x, M_i)$                                                                                                                     (1)\nwhere $\\lambda$ is a hyperparameter that balances the contribution of the XAI in the overall training objective.\nInitially, the model is trained using only $L_{cls}$ $(\\lambda = 0)$. Once the model reaches a convergence criterion based on its validation performance, $L_{xai}$ is introduced $(\\lambda = 1)$. The composite loss function $L$ balances both classification performance and explainability, ensuring the network not only provides"}, {"title": "3.2.1. Classification Loss", "content": "The classification loss $L_{cls}$ is a fundamental component used for training the network to correctly predict the class labels of input images. $L_{cls}$, computed using the cross-entropy loss function, measures the discrepancy between the predicted probabilities and the actual class labels:\n$L_{cls} (\\theta; x, y) = -\\sum_{c=1}^{C} y_c \\log(\\hat{y}_c)$                                                                                                                               (2)\nwhere $\\theta$ and $x$ are the network parameters and the input image already defined, $y$ is the one-hot encoded class vector, with $y_c$ being the indicator for class $c$. The vector $\\hat{y}$ corresponds to the predicted class probabilities obtained via the softmax function, with $\\hat{y}_c$ representing the posterior probability for class $c$."}, {"title": "3.2.2. \u03a7\u0391\u0399I Loss", "content": "The XAI loss $L_{xai}$ enhances the model's transparency by encouraging alignment between the network-generated heatmaps and the expert-provided masks: it utilizes the mean squared error (MSE) to quantify the difference between the Grad-CAM heatmap $H(x; \\theta)$ and the expert mask $M_i$. The heatmap $H(x; \\theta)$, generated based on the gradients of the target class with respect to the feature maps of the last convolutional layer of the network, highlights important pixels influencing the network's prediction. $L_{xai}$ is defined as:\n$L_{xai} (\\theta; x, M_i) = \\frac{1}{N}\\sum_{n=1}^{N}(M_{in} - H_n(x; \\theta))^2$                                                                                                               (3)\nwhere $N$ is the total number of pixels in the image, $M_{in}$ is the value at pixel $n$ in the expert mask $M_i$, and $H_n(x; \\theta)$ represents the value at pixel $n$ in the heatmap.\nGrad-CAM [18] is used to generate heatmaps $H(x; \\theta)$ that visually indicate which parts of the input image are most important for the predictions made by the neural network. The process for computing $H(x; \\theta)$ is as follows [18]:"}, {"title": null, "content": "1. Forward Pass: Input image $x$ is passed through the neural network to obtain the class scores $z$ before the softmax layer, which represent the unnormalized predictions for each class.\n2. Target Class: The class $c$ for which the heatmap is to be generated is the true class of the input image $x$.\n3. Compute Gradients: Calculate the gradients of the logits $z_c$ (corresponding to class $c$) with respect to the feature map $A^k$ of a chosen convolutional layer. These gradients $\\frac{\\partial z_c}{\\partial A_{ij}^k}$ indicate how much each feature map $A^k$ contributes to the increase or decrease of the class score.\n4. Pooling of Gradients: Perform global average pooling of the gradient over the width and height dimensions of the feature map $A^k$ to obtain the neuron importance weights $a_c^k$:\n$a_c^k = \\frac{1}{Z} \\sum_{i}\\sum_{j} \\frac{\\partial z_c}{\\partial A_{ij}^k}$                                                                                                                                          (4)\nwhere $Z$ is the total number of pixels in the feature map, and $A_{ij}^k$ denotes the pixel intensity at position $(i, j)$ in feature map $k$. These weights $a_c^k$ indicate the importance of each feature map $A^k$ for the target class $c$.\n5. Weighted Combination of Feature Maps: Compute a weighted sum of the feature maps using $a_c^k$, followed by a ReLU to obtain the heatmap:\n$H(x; \\theta) = ReLU(\\sum_{k} a_c^k A^k)$                                                                                                                                             (5)\nThis operation ensures that only the features with a positive influence on the class score are visualized, enhancing explainability.\nIt is worth noting that all the steps involved in computing $H(x; \\theta)$, including the gradient computation and the subsequent weighted sum of feature maps, consists of differentiable operations with respect to the model parameters $\\theta$. This makes it possible to compute the gradients of $L_{xai}$ with respect to $\\theta$, allowing for its effective inclusion in backpropagation-based neural network training."}, {"title": "3.3. Training Procedure", "content": "The Doctor-in-the-Loop paradigm combines two stages of training: first, training the model using only $L_{cls}$; second, refining the model's explainability"}, {"title": null, "content": "and accuracy by providing regions identified by experts that guide the network's focus through a combination of $L_{cls}$ and $L_{xai}$. As shown in Algorithm 1, the key passages of the training process are those detailed below.\n\u2022 Initialization: The algorithm starts by random initializing the model parameters $\\theta$, which include weights and biases, and the learning rate $\\eta$ to 0.001. Initially, only $L_{cls}$ is used by setting $\\lambda = 0$, i.e., the network exclusively minimizes the classification loss.\n\u2022 Initial Training: The model is trained using $L_{cls}$, iterating over batches of data. This phase uses stochastic gradient descent to update $\\theta$, minimizing $L_{cls}$ until the early stopping criterion on the validation loss is met, ensuring the model does not overfit.\n\u2022 Explainability-guided Training: After the Initial Training, $L_{xai}$ is introduced by setting $\\lambda = 1$. The training now minimizes the composite loss function $L$, thereby improving explainability while maintaining prediction accuracy. In this phase, the model undergoes a GL strategy, iterating over multiple views $\\{M_1, M_2, . . ., M_t\\}$, which progressively refine the model's focus on increasingly specific and relevant areas of the image. The process is repeated until the early stopping criteria is met."}, {"title": "4. Experimental Setup", "content": "To validate the proposed Doctor-in-the-Loop approach we conducted a series of experiments using an in-house dataset of NSCLC patients on the pR task. This section outlines the dataset and pre-processing steps, describes the experimental configurations, and details the evaluation metrics used to assess our methodology."}, {"title": "4.1. Dataset", "content": "The experiments were conducted using a dataset of 64 NSCLC patients with TNM stage II-III [7], collected at Fondazione Policlinico Universitario Campus Bio-Medico of Rome. All patients underwent neoadjuvant chemoradiation therapy followed by surgical resection. Among these patients, 27% achieved a pR, defined as having no more than 10% viable tumor cells in all specimens (primary tumors and lymph nodes). In-house pathologists con-ducted the pR analysis for cases from Fondazione Policlinico Universitario"}, {"title": "4.2. Pre-processing", "content": "To standardize the data and optimize the model's performance, a series of pre-processing steps were applied. First, all CT images were resampled to a uniform resolution of 1 \u00d7 1 \u00d7 1 mm\u00b3 via a nearest-neighbor interpolation, ensuring consistency across the dataset. The images were then clipped using a lung window setting (mean: -300 HU; width: 1200 HU) to minimize the impact of extreme values and enhance the relevant anatomical features. Next, linear normalization was applied to standardize the pixel intensity distribution, scaling the values to the [0, 1] range. Finally, CT scans were cropped using a fixed rectangular bounding-box that encompassed the lung region, ensuring inputs of uniform size (324 \u00d7 324 pixels). To enhance model robustness and mitigate overfitting, we applied data augmentation without altering the underlying anatomical structures by spatial shifts (\u00b13 pixels) and vertical flips, which introduced variability into the dataset."}, {"title": "4.3. Experimental Configuration", "content": "To assess the effectiveness of our method, we conducted a series of experiments that include the Doctor-in-the-Loop approach and comparisons with three ablation studies designed to evaluate the impact of GL and XAI on model performance and two additional competitors based on state-of-the-art methodologies. Each experiment is detailed below, and a general summary of the ablation studies is illustrated in Table 1.\nDoctor-in-the-Loop. This experiment tests the Doctor-in-the-Loop approach, integrating both GL and XAI guidance through a three-stage progressive refinement:\n\u2022 : In the first step, the model is trained on the global image using only $L_{cls}$."}, {"title": null, "content": "\u2022 : In the second step, the training is refined by introducing the lung segmentation mask as a guide for the model's focus, applying $L$.\n\u2022 In the third step, the model's focus is further refined by using the lesion segmentation as a guide, again applying $L$.\nXAI-guide. The goal of this ablation experiment is to evaluate the performance when only $L_{xai}$ is applied, without the advantages of GL. Hence, the model does not adjust its learning trajectory over time through the progression of different stages of focus, views, but the models are trained independently guided by a fixed segmentation mask.\n\u2022 The first model is trained on the global image view using only $L_{cls}$.\n\u2022 : The second model is independently trained using the lung segmentation view as a guide for the model's focus, applying $L$.\n\u2022: The third model is independently trained using the lesion segmentation view as a guide for the model's focus again applying $L$.\nIf Doctor-in-the-Loop outperforms this ablation study, it would indicate that explainability guidance alone is insufficient to fully exploit the information in the data, highlighting the importance of the gradual progression provided by GL to achieve better results.\nGradual Learning. This ablation experiment isolates the effect of GL without the influence of the $L_{xai}$. The models are trained using only $L_{cls}$ in each step, without explicitly guiding their focus through XAI techniques.\n\u2022 : In the first step, the model is trained on the global image using only $L_{cls}$.\n\u2022 : In the second step, the training is refined by introducing the masked lungs as model input, applying only $L_{cls}$.\n\u2022: In the third step, the training is further refined using the masked lesion as model input, again applying only $L_{cls}$.\nIf Doctor-in-the-Loop outperforms this ablation study, it would suggest that the explicit guide on clinically relevant regions, provided by XAI, is critical in complementing the gradual refinement of GL to improve the performance."}, {"title": null, "content": "Segmentation. In this ablation experiment, we evaluate the models' performance when directly trained on the expert-provided segmentation masks (lung and lesion), bypassing both XAI and GL. Each model is trained independently on masked inputs with only $L_{cls}$.\n\u2022: The first model is trained on the global image view using only $L_{cls}$.\n\u2022 : The second model is independently trained using the masked lungs view as model input, applying only $L_{cls}$.\n\u2022: The third model is independently trained using the masked lesion view as model input, again applying only $L_{cls}$.\nIf Doctor-in-the-Loop outperforms this ablation study, it would indicate that relying solely on expert-provided segmentation inputs is not sufficient to achieve optimal performance, hence highlighting the importance of incorporating insights from the global image while progressively directing model's focus toward lung and lesion views."}, {"title": null, "content": "Competitors. To provide an exhaustive evaluation of the proposed method, we implemented a benchmarking analysis against approaches commonly employed in the state-of-the-art for pR prediction. As highlighted in Section 2, the use of radiomics features as well as the extraction of deep features, both"}, {"title": null, "content": "combined with machine learning classifiers, have emerged as the main approaches. They primarily focus on the tumor region for feature extraction and analysis.\nHowever, direct replicating individual state-of-the-art methods was not feasible due to two constraints. First, the lack of open-source code for most of these approaches made exact replication impractical. Second, they used feature extraction pipelines tailored to their specific datasets. Furthermore, such approaches were all tested on private datasets, preventing us from applying our method to their data. Thus, we opted to implement the following generalized frameworks of these two approaches to ensure a fair and meaningful comparison:\n\u2022 Deep Feature Extraction + Machine Learning: This method involves extracting 1664 deep features from a model trained on lesion segmentation. Extracted features were standardized before being input into machine learning classifiers for pR prediction.\n\u2022 Radiomics Feature Extraction + Machine Learning: This approach computes radiomics features from the tumor region, including shape descriptors, first-order statistics, and texture-based metrics, resulting in 107 features. Image pre-processing steps, such as interpolation, clipping, and normalization, were made consistent with those used for the deep models to ensure comparability. These features were standardized before being passed to machine learning classifiers.\nFor both approaches, we employed three widely used machine learning models as classifiers: Support Vector Machines (SVM), a robust model for high-dimensional data and small sample sizes; eXtreme Gradient Boosting (XGBoost), a gradient-boosting framework known for its high performance in tabular data; and Multilayer Perceptron (MLP), a simple neural network architecture capable of capturing non-linear feature relationships. These models were selected for their relevance to the task and their established use in the literature on pR."}, {"title": "4.3.1. Training", "content": "For all the described experiments, we used a DenseNet169, a 3D convolutional neural network, as it demonstrated successful results in lung cancer-related tasks [26]. The dataset was split into training (60%), validation (20%) and test (20%) sets, via a 5-fold stratified cross-validation scheme. For all"}, {"title": "5. Results and Discussion", "content": "In this Section, we present and discuss the results obtained from the experimental configurations outlined in Section 4.3. Quantitative performance metrics are detailed in Table 2 and Table 5, which reports ACC, AUC, TPR, and TNR, as the mean and standard error, calculated across the different folds, for ablation studies and competitors, respectively. For each metric, the best performing value is highlighted in bold. Visual and intuitive comparisons of performance metrics between the Doctor-in-the-Loop approach and each ablation study are provided in Figure 3, Figure 4, and Figure 5."}, {"title": "5.1. Doctor-in-the-Loop", "content": "The Doctor-in-the-Loop approach demonstrates progressive improvements across its learning stages (Table 2), achieving the best performance in Step 3, which combines GL with explainability-driven guidance in the lesion area. As shown in Table 2, Step 1, which is trained on the global image, yields an ACC of 59.36%, AUC of 52.82%, TPR of 28.33%, and TNR of 69.78%. Incorporating the lung segmentation as a guide for the model's focus in Step 2 improves its performance, resulting in higher metrics, with an ACC of 73.33%, AUC of 60.11%, TPR of 45.00%, and TNR of 82.44%. Finally, Step"}, {"title": "5.2. Doctor-in-the-Loop vs. XAI-guide", "content": "The comparison between Doctor-in-the-Loop and XAI-guide highlights the importance of incorporating GL. Quantitatively, the Doctor-in-the-Loop method outperforms XAI-guide across all steps and metrics, as shown in both Table 2 and Figure 3. For example, in Step 3, Doctor-in-the-Loop achieves an AUC of 62.93%, significantly higher than the 47.96% of XAI-guide. Similarly, Doctor-in-the-Loop achieves a TPR of 46.67%, compared to XAI-guide's 38.33%. These results suggest that XAI-guide, while incorporating the segmentation guidance provided by expert annotations, lacks the benefit of gradual refinement in learning stages, resulting in lower performance. Moreover, these findings support the hypothesis that GL enhances the expert-driven guidance provided by XAI, emulating a physician's approach of progressively and more accurately focusing on detailed regions of clinical interest. Statistical analysis (Table 3) confirms that the differences in performance between Doctor-in-the-Loop and XAI-guide are significant, with p-value < 0"}]}