{"title": "OVERCOMING SLOW DECISION FREQUENCIES IN CONTINUOUS CONTROL: MODEL-BASED SEQUENCE REINFORCEMENT LEARNING FOR MODEL-FREE CONTROL", "authors": ["Devdhar Patel", "Hava Siegelmann"], "abstract": "Reinforcement learning (RL) is rapidly reaching and surpassing human-level control capabilities. However, state-of-the-art RL algorithms often require timesteps and reaction times significantly faster than human capabilities, which is impractical in real-world settings and typically necessitates specialized hardware. Such speeds are difficult to achieve in the real world and often requires specialized hardware. We introduce Sequence Reinforcement Learning (SRL), an RL algorithm designed to produce a sequence of actions for a given input state, enabling effective control at lower decision frequencies. SRL addresses the challenges of learning action sequences by employing both a model and an actor-critic architecture operating at different temporal scales. We propose a \"temporal recall\" mechanism, where the critic uses the model to estimate intermediate states between primitive actions, providing a learning signal for each individual action within the sequence. Once training is complete, the actor can generate action sequences independently of the model, achieving model-free control at a slower frequency. We evaluate SRL on a suite of continuous control tasks, demonstrating that it achieves performance comparable to state-of-the-art algorithms while significantly reducing actor sample complexity. To better assess performance across varying decision frequencies, we introduce the Frequency-Averaged Score (FAS) metric. Our results show that SRL significantly outperforms traditional RL algorithms in terms of FAS, making it particularly suitable for applications requiring variable decision frequencies. Additionally, we compare SRL with model-based online planning, showing that SRL achieves superior FAS while leveraging the same model during training that online planners use for planning. Lastly, we highlight the biological relevance of SRL, showing that it replicates the \u201caction chunking\u201d behavior observed in the basal ganglia, offering insights into brain-inspired control mechanisms.", "sections": [{"title": "1 Introduction", "content": "Biological and artificial agents must learn behaviors that maximize rewards to thrive in complex environments. Reinforcement learning (RL), a class of algorithms inspired by animal behavior, facilitates this learning process [Sutton and Barto, 2018]. The connection between neuroscience and RL is profound. The Temporal Difference (TD) error, a key concept in RL, effectively models the firing patterns of dopamine neurons in the midbrain [Schultz et al., 1997, Schultz, 2015, Cohen et al., 2012]. Additionally, a longstanding goal of RL algorithms is to match and surpass human performance in control tasks [OpenAI et al., 2019, Schrittwieser et al., 2020, Kaufmann et al., 2023a, Wurman et al., 2022a, Vinyals et al., 2019, Mnih et al., 2015]"}, {"title": "2 Necessity of Sequence Learning: Frequency, Delay and Response Time", "content": "To perform any control task, the agent requires the following three components: Sensor, Processor/Computer, Actuator. In the traditional RL framework, all three components act at the same frequency due to the common timestep. However, this is not the case in biological agents that have different sensors of varying frequencies that are often faster than the compute frequency or the speed at which the brain can process the information [Borghuis et al., 2019]. Additionally,"}, {"title": "3 Related Work", "content": ""}, {"title": "3.1 Model-Based Reinforcement Learning", "content": "Model-Based Reinforcement Learning (MBRL) algorithms leverage a model of the environment, which can be either learned or known, to enhance RL performance [Moerland et al., 2023]. Broadly, MBRL algorithms have been utilized to:\n1. Improve Data Efficiency: By augmenting real-world data with model-generated data, MBRL can significantly enhance data efficiency [Yarats et al., 2021, Janner et al., 2019, Wang et al., 2021].\n2. Enhance Exploration: MBRL aids in exploration by using models to identify potential or unexplored states [Pathak et al., 2017, Stadie et al., 2015, Savinov et al., 2018].\n3. Boost Performance: Better learned representations from MBRL can lead to improved asymptotic performance [Silver et al., 2017, Levine and Koltun, 2013].\n4. Transfer Learning: MBRL supports transfer learning, enabling knowledge transfer across different tasks or environments [Zhang et al., 2018, Sasso et al., 2022].\n5. Online Planning: Models can be used for online planning with a single-step policy [Fickinger et al., 2021]. However, this approach increases model complexity as each online planning step requires an additional call to the model, making it nonviable for energy and computationally constrained agents like the brain and robots.\nCompared to online planning, our algorithm maintains a model complexity of zero after training, eliminating the need for any model calls post-training. This significantly reduces the computational and energy requirements, making it more suitable for practical applications in constrained environments. Additionally, model-based online planning is less biologically plausible than SRL. Wiestler and Diedrichsen [2013] demonstrated that the activations in the motor cortex reduce after skill learning, suggesting that the brain gets more efficient at performing the task after learning. In contrast, model-based online planning does not reduce in the compute and model complexity, but rather might increase in complexity as we perform longer sequences. SRL, on the other hand, has a model complexity of zero after training and thus is biologically plausible based on this observed phenomenon."}, {"title": "3.2 Model Predictive Control", "content": "Similar to model-based reinforcement learning, Model Predictive Control (MPC) utilizes a model of the system to predict and optimize future behavior. In the context of modern robotics, MPC has been effectively applied to trajectory planning and real-time control for both ground and aerial vehicles. MPC has been applied to problems like autonomous driving [Gray et al., 2013] and bipedal control [Galliker et al., 2022]. However, MPC requires an accurate dynamics model of the system. This limits its practicality to systems that already been sufficiently modeled. In contrast, SRL does not start with any knowledge of the environment.\nAdditionally, similar to current RL, MPC requires very fast operational timesteps for practical application. For example, Galliker et al. [2022] implemented walker at 10 ms, Farshidian et al. [2017] implemented a four legged robot at 4 ms and Di Carlo et al. [2018] implemented the MIT Cheetah 3 at 33.33 ms."}, {"title": "3.3 Macro-Actions", "content": "Reinforcement Learning (RL) algorithms that utilize macro-actions demonstrate many benefits, including improved exploration and faster learning [McGovern et al., 1997]. However, identifying effective macro-actions is a challenging problem due to the curse of dimensionality, which arises from large action spaces. To address this issue, some approaches have employed genetic algorithms [Chang et al., 2022] or relied on expert demonstrations to extract macro-actions [Kim et al., 2020]. However, these methods are not scalable and lack biological plausibility.\nIn contrast, our approach learns macro-actions using the principles of RL, thus requiring little overhead while combining the flexibility of primitive actions with the efficiency of macro-actions."}, {"title": "3.4 Action Repetition and Frame-skipping", "content": "To overcome the curse of dimensionality while gaining the benefits of macro-actions, many approaches utilize frame-skipping and action repetition, where macro-actions are restricted to a single primitive action that is repeated. Frame-skipping and action repetition serve as a form of partial open-loop control, where the agent selects a sequence of actions to be executed without considering the intermediate states. Consequently, the number of actions is linear in the number of time steps [Kalyanakrishnan et al., 2021, Srinivas et al., 2017, Biedenkapp et al., 2021, Sharma et al., 2017, Yu et al., 2021].\nFor instance, FiGaR [Sharma et al., 2017] shifts the problem of macro-action learning to predicting the number of steps that the outputted action can be repeated. TempoRL [Biedenkapp et al., 2021] improved upon FiGaR by conditioning the number of repetitions on the selected actions. However, none of these algorithms can scale to continuous control tasks with multiple action dimensions, as action repetition forces all actuators and joints to be synchronized in their repetitions, leading to poor performance for longer action sequences."}, {"title": "4 Sequence Reinforcement Learning", "content": "Based on the insights presented in Section 6, we introduce a novel reinforcement learning model capable of learning sequences of actions (macro-actions) by replaying memories at a finer temporal resolution than the action generation, utilizing a model of the environment during training.\nComponents\nThe Sequence Reinforcement Learning (SRL) algorithm learns to plan \"in-the-mind\" using a model during training, allowing the learned action-sequences to be executed without the need for model-based online planning. This is achieved using an actor-critic setting where the actor and critic operate at different frequencies, representing the observation/computation and actuation frequencies, respectively. Essentially, the critic is only used during training/replay and can operate at any temporal resolution, while the actor is constrained to the temporal resolution of the slowest component in the sensing-compute-actuation loop. Denoting the actor's timestep as t' and the critic's timestep as t, our algorithm includes three components:\nModel: $s_{t+1} = m(s_t, a_t)$\nCritic : $q_t = q(s_t, a_t)$\nActor : $a_{t'} = a_{t'}, a_{t'+t}, a_{t'+2t}... ~ \\pi_{\\omega}(s_{t'})$\nWe denote individual actions in the action sequence generated by actor using the notation $\\pi_{\\omega}(s_{t'})_t$ to represent the action $a_{t'+t}$\n1. Model: Learns the dynamics of the environment, predicting the next state $s_{t+1}$ given the current state $s_t$ and primitive action $a_t$.\n2. Critic: Takes the same input as the model but predicts the Q-value of the state-action pair.\n3. Actor: Produces a sequence of actions given an observation at time t'. Observations from the environment can occur at any timestep t or t', where we assume t' > t. Specifically, in our algorithm, $t' = Jt$ where $J > 1; J \\in \\mathbb{Z}$.\nEach component of our algorithm is trained in parallel, demonstrating competitive learning speeds.\nWe follow the Soft-Actor-Critic (SAC) algorithm [Haarnoja et al., 2018] for learning the actor-critic. Exploration and uncertainty are critical factors heavily influenced by timestep size and planning horizon. Many model-free algorithms, like DDPG [Lillicrap et al., 2015] and TD3 [Fujimoto et al., 2018] explore by adding random noise to each action during training. However, planning a sequence of actions over a longer timestep can result in additive noise, leading to poor performance during training and exploration. The SAC algorithm addresses this by maximizing the entropy of each action in addition to the expected return, allowing our algorithm to automatically lower entropy for deeper actions farther from the observation.\nLearning the Model\nThe model is trained to minimize the Mean Squared Error of the predicted states. For a trajectory $T = (s_t, a_t, s_{t+1})$ drawn from the replay buffer D, the predicted state is taken from $s_{t+1} ~ m_{\\phi}(s_t, a_t)$. The loss function is:\n$L_{\\phi} = E_{\\tau \\sim D}(s_{t+1} - \\hat{s}_{t+1})^2$\nFor this work, the model is a feed-forward neural network with two hidden layers. In addition to the current model $m_{\\phi}$, we also maintain a target model $m_{\\phi^-}$ that is the exponential moving average of the current model.\nLearning Critic\nThe critic is trained to predict the Q-value of a given state-action pair $\\hat{q}_t = q_{\\psi}(s_t, a_t)$ using the target value from the modified Bellman equation:\n$\\hat{Q}_t = r_t + E_{a_{t+1} \\sim \\pi_{\\omega}(s_{t+1})} [q_{\\psi^-} (s_{t+1}, a_{t+1}) - \\alpha log \\pi_{\\omega}(a_{t+1}|s_{t+1})]$\nHere, $q^-$ is the target critic, which is the exponential moving average of the critic. Following the SAC algorithm, we train two critics and use the minimum of the two q\u2013 values to train the current critics. The loss function is:\n$L_{\\psi} = E_{\\tau \\sim D}[(\\hat{Q}_t^k - \\hat{q}_t)^2] \\forall k \\in 1,2$\nBoth critics are feed-forward neural networks with two hidden layers. It should be noted that while the actor utilizes the model during training, the critic does not train on any data generated by the model, thus the critic training is model-free and grounded on the real environment states.\nLearning Policy\nThe SRL policy utilizes two hidden layers followed by a Gated-Recurrent-Unit (GRU) [Cho et al., 2014] that takes as input the previous action in the action sequence, followed by two linear layers that output the mean and standard deviation of the Gaussian distribution of the action. This design allows the policy to produce action sequences of arbitrary length given a single state and the last action.\nA naive approach to training a sequence of actions would be to augment the action space to include all possible actions of the sequence length. However, this quickly leads to the curse of dimensionality, as each sequence is considered a"}, {"title": "5 Experiments", "content": "Overview\nWe evaluate our SRL approach on several continuous control tasks, comparing it against the SAC baseline. Our focus is on environments with multi-dimensional actions, ranging from the simple LunarLanderContinuous (2 action dimensions) to the complex Humanoid environment (17 action dimensions). This allows us to highlight the benefits of SRL over traditional action repetition approaches. We utilize the OpenAI gym [Brockman et al., 2016] implementation of the MuJoCo environments [Todorov et al., 2012].\nExperiemental Setup\nWe train SRL with four different action sequence lengths (ASL), $J = 2,4,8,16$, referred to as SRL-J. During training, SRL is evaluated based on its J value, processing states only after every J actions. All hyperparameters are identical between SRL and SAC, except for the actor update frequency: SRL updates the actor every 4 steps, while SAC updates every step. Thus, SAC has four more actor update steps compared to SRL. Additionally, SRL learns a model in parallel with the actor and critic.\nWe presents the learning curves of SRL and SAC across 10 continuous control tasks in the appendix. We observe that SRL outperforms SAC in six out of ten tasks. Notably, SRL-16 achieves competitive performance on LunarLander, Hopper, Reacher and Swimmer tasks, showcasing the algorithm's capability to learn long action sequences from scratch. Surprisingly, SRL also outperforms SAC in the Humanoid environment with fewer inputs and actor updates while concurrently learning a model, demonstrating the efficacy of the algorithm on environments with higher action dimensions. It should be noted that the learning curves presented for SRL-J take in states every J steps, thus SRL is evaluated on a disadvantage in the learning curves.\nFrequency-Averaged Score\nTransitioning from simulation to real-world implementation (Sim2Real) in control systems is challenging because deployment introduces computational stochasticity, leading to variable sensor sampling rates and inconsistent end-to-end delays from sensing to actuation [Sandha et al., 2021]. This gap is not captured by the mean reward or return that is the norm in current RL literature. To address this, we introduce Frequency-Averaged Score (FAS) that is the normalized area under the curve (AUC) of the performance vs. decision frequency plot. Figure 2 demonstrates the plot over which FAS is calculated for the Hopper environment. We provide similar plots for all environments in the Appendix. The FAS captures the overall performance of the policy at different decision frequencies, timesteps or macro-action lengths. A High FAS indicates that the policy performance generalizes across:"}, {"title": "6 Neural Basis for Sequence Learning", "content": "Unlike artificial RL agents, learning in the brain does not stop once an optimal solution has been found. During initial task learning, brain activity increases as expected, reflecting neural recruitment. However, after training and repetition, activity decreases as the brain develops more efficient representations of the action sequence, commonly referred to as muscle memory [Wiestler and Diedrichsen, 2013]. This phenomenon is further supported by findings that sequence-specific activity in motor regions evolves based on the amount of training, demonstrating skill-specific efficiency and specialization over time [Wymbs and Grafton, 2015].\nThe neural basis for action sequence learning involves a sophisticated interconnection of different brain regions, each making a distinct contribution:\n1. Basal ganglia (BG): Action chunking is a cognitive process by which individual actions are grouped into larger, more manageable units or \"chunks,\u201d facilitating more efficient storage, retrieval, and execution with reduced cognitive load [Favila et al., 2024]. Importantly, this mechanism allows the brain to perform extremely fast and precise sequences of actions that would be impossible if produced individually. The BG plays a crucial role in chunking, encoding entire behavioral action sequences as a single action [Jin et al., 2014, Favila et al., 2024, Jin and Costa, 2015, Berns and Sejnowski, 1996, 1998, Garr, 2019]. Dysfunction in the BG is associated with deficits in action sequences and chunking in both animals [Doupe et al., 2005, Jin and Costa, 2010, Matamales et al., 2017] and humans [Phillips et al., 1995, Boyd et al., 2009, Favila et al., 2024]. However, the neural basis for the compression of individual actions into sequences remains poorly understood.\n2. Prefrontal cortex (PFC): The PFC is critical for the active unbinding and dismantling of action sequences to ensure behavioral flexibility and adaptability [Geissler et al., 2021]. This suggests that action sequences are not merely learned through repetition; the PFC modifies these sequences based on context and task requirements. Recent research indicates that the PFC supports memory elaboration [Immink et al., 2021] and maintains temporal context information [Shahnazian et al., 2022] in action sequences. The prefrontal cortex receives inputs from the hippocampus.\n3. Hippocampus (HC) replays neuronal activations of tasks during subsequent sleep at speeds six to seven times faster. This memory replay may explain the compression of slow actions into fast chunks. The replayed trajectories from the HC are consolidated into long-term cortical memories [Zielinski et al., 2020, Malerba et al., 2018]. This phenomenon extends to the motor cortex, which replays motor patterns at accelerated speeds during sleep [Rubin et al., 2022]."}, {"title": "7 Discussion, Limitations and Future Work", "content": "We introduce the Sequence Reinforcement Learning (SRL) algorithm, a biologically plausible model for sequence learning. It represents a significant step towards achieving robust control at brain-like speeds. The key contributions of SRL include its ability to generate long sequences of actions from a single state, its resilience to reduced input frequency, and its lower computational complexity per primitive action.\nThe current RL framework encourages synchrony between the environment and the components of the agent. However, the brain utilizes components that act at different frequencies and yet is capable of robust and accurate control. SRL provides an approach to reconcile this difference between neuroscience and RL, while remaining competitive on current RL benchmarks. SRL offers substantial benefits over traditional RL algorithms, particularly in the context of autonomous agents such as self-driving cars and robots. By enabling operation at slower observational frequencies and providing a gradual decay in performance with reduced input frequency, SRL addresses critical issues related to sensor failure and occlusion, and energy consumption. Additionally, SRL generates long sequences of actions from a single state, which can enhance the explainability of the policy and provide opportunities to override the policy early in case of safety concerns. SRL also learns a latent representation of the action sequence, which could be used in the future to interface with large language models for multimodal explainability and even hierarchical reinforcement learning and transfer learning.\nFuture Work\nWe believe the SRL model contributes to both artificial agents and the study of biological control. Future work will incorporate biological features like attention mechanisms and knowledge transfer. Additionally, SRL can benefit from existing Model-Based RL approaches as it naturally learns a model of the world. In the Appendix, we demonstrate"}, {"title": "8 Conclusion", "content": "In this paper, we introduced Sequence Reinforcement Learning (SRL): a model based action sequence learning algorithm for model free control. We demonstrated the improvement of SRL over existing framework by testing it over various control frequencies. Furthermore, we introduces the Frequency-Averaged-Score (FAS) metric to measure the robustness of a policy across different frequencies. Our work is the first to achieve competitive results on continuous control environments at low control frequencies and serves as a benchmark for future work in this direction. Finally, we demonstrated directions for future works including comparison to model-based planning, generative replay and connections to neuroscience."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 SRL Algorithm", "content": "Algorithm 1: Sequence Reinforcement Learning\nInput: \u03a6, 41, 42, W. Initial parameters\n1 \u03a6 \u2190 \u03c6, \u03c81 \u2190 \u03c81, \u03c82 \u2190 \u03c82;\n2 D0;\n3 for each iteration do\n4\n{at, at+1,..., at+J\u22121} ~ \u03c0\u03c9({at, at+1,...,At+J\u22121}|St);\n5\nfor each action at in the sequence do\n6\nSt+1~ P(St+1|St, at);\n7\nD \u2190 DU {(st, at, r(St, at), St+1)};\n8\nend\n9\nfor each gradient step do\n10\n\u2190 - m\u2207\u00a2\u00a3\u00a2 ;\n11\nfor i \u2208 {1,2} do\n12\n| \u03c8\u03af \u2190 \u03c8\u03b9 \u2013 \u03bb\u03bf\u2207\u03c8\u2081L\u03c8;;\n13\nend\n14\n{at,at+1,...,at+J\u22121} ~ \u03c0\u03c9({at, at+1,..., at+J\u22121}|St); \n15\nif iteration mod actor_update frequency == 0 then\n16\nfor j \u2208 {1, ..., J} do\n17\n| Sj+1 ~ m\u1ef9(sj+1|8j,aj);\n18\nend\n19\n\u03c6\u03b5 \u03c9 \u2013 \u03bb\u03c0\u03bd\u03c9L\u03c9;\n20\nend\n21\na \u2190 a \u2013 V\u00e2L(a);\n22\nfor i \u2208 {1, 2} do\n23\n| Vi\u2190 TVi + (1 \u2212 \u315c)Vi ;\n24\nend\n25\n\u2190 \u0442\u0444 + (1 \u2212 \u0442)\u0444;\n26 end\n27 end\nOutput: \u03c6, \u03c8\u03b9, \u03c82, \u03c9;\nHyperparameters\nThe table below lists the hyperparameters that are common between every environment used for all our experiments for the SAC and SRL algorithms:"}, {"title": "A.2 Implementation Details", "content": "Due to its added complexity during training, SRL requires longer wall clock time for training when compared to SAC. We performed a minimal hyperparameter search over the actor update frequency parameter on the Hopper environment (tested values: 1, 2, 4, 8, 16). All the other hyperparamters were picked to be equal to the SAC implementation. We also did not perform a hyerparameter search over the size of GRU for the actor. It was picked to have the same size as the hidden layers of the feed forward network of the actor in SAC. The neural network for the model was also picked to have the same architecture as the actor from SAC, thus it has two hidden layers with 256 neurons. Similarly the encoder for the latent SRL implementation was also picked to have the same architecture. For the latent SRL implementation we also add an additional replay buffer to store transitions of length 5, to implement the temporal consistency training for the model. This was done for simplicity of the implementation, and it can be removed since it is redundant to save memory.\nAll experiments were performed on a GPU cluster the Nvidia 1080ti GPUs. Each run was performed using a single GPU, utilizing 8 CPU cores of Intel(R) Xeon(R) Silver 4116 (24 core) and 16GB of memory."}, {"title": "A.3 Plots for Frequency Averaged Scores", "content": "Figure 6 shows the plots for FAS. The ASL of 1 in the figure represents the performance of each policy in the standard reinforcement learning setting. We can see that SRL is competitive with SAC on ASL of 1 on all environments tested. Larger H results in better robustness at longer ASLs but it often comes at the cost of lower performance at shorter ASLS.\nAdditionally, as the FAS reflects, SRL is also significantly more robust across different frequencies than standard RL (SAC)."}, {"title": "A.4 Plots for FAS vs. Stochastic Timestep Performance", "content": "In Figure 7, we present the plots for FAS vs performance for all environments. For all environments except InvertedDoublePendulum-v2, we see a high correlation. InvertedDoublePendulum-v2 is a difficult problem at slow frequency and demonstrates poor performance of less than 200, thus it does not correlate to FAS."}, {"title": "A.5 Generative Replay in Latent Space", "content": "Previous studies have shown that generative replay benefits greatly from latent representations [Van de Ven et al., 2020]. Recently, Simplified Temporal Consistency Reinforcement Learning (TCRL) [Zhao et al., 2023] demonstrated that learning a latent state-space improves not only model-based planning but also model-free RL algorithms. Building on this insight, we introduced an encoder to encode the observations in our algorithm.\nFollowing the TCRL implementation, we use two encoders: an online encoder ee and a target encoder ee-, which is the exponential moving average of the online encoder:"}, {"title": "A.6 Clarification Figure", "content": "Thus, the model predicts the next state in the latent space. Additionally, we introduce multi-step model prediction for temporal consistency. Following the TCRL work, we use a cosine loss for model prediction. The model itself predicts only a single step forward, but we enforce temporal consistency by rolling out the model H-steps forward to predict et+1:t+1+H.\nSpecifically, for an H-step trajectory \u03c4 = (zt, at, zt+1)t:t+H drawn from the replay buffer D, we use the online encoder to get the first latent state et = eo(ot). Then conditioning on the sequence of actions at:t+H, the model is applied iteratively to predict the latent states \u0113t+1 = m(\u0113t, at). Finally, we use the target encoder to calculate the target latent states \u00eat+1:t+H+1 = eo- (Ot+1:t+1+H). The Loss function is defined as:\n$L_{o,\u03c4} = E_{\u03c4~D}[\u03a3_H  ||   -   ||]^2$\nWe set H = 5 for our experiments. Both the encoder and the model are feed-forward neural networks with two hidden layers.\nWe provide preliminary results for the Walker environment. Utilizing the latent space for generative replay significantly improved performance, making it competitive even at 16 steps (128ms) (Figure 8).\nWe also provide the TempoRL [Biedenkapp et al., 2021] algorithm as a benchmark as it is an algorithm that successfully reduces the number of decisions per episodes. TempoRL is designed to dynamically pick the best frameskip (for performance), therefore we report the avg. action sequence length for TempoRL."}]}