{"title": "General Uncertainty Estimation with Delta Variances", "authors": ["Simon Schmitt", "John Shawe-Taylor", "Hado van Hasselt"], "abstract": "Decision makers may suffer from uncertainty induced by limited data. This may be mitigated by accounting for epistemic uncertainty, which is however challenging to estimate efficiently for large neural networks.\nTo this extent we investigate Delta Variances, a family of algorithms for epistemic uncertainty quantification, that is computationally efficient and convenient to implement. It can be applied to neural networks and more general functions composed of neural networks. As an example we consider a weather simulator with a neural-network-based step function inside \u2013 here Delta Variances empirically obtain competitive results at the cost of a single gradient computation.\nThe approach is convenient as it requires no changes to the neural network architecture or training procedure. We discuss multiple ways to derive Delta Variances theoretically noting that special cases recover popular techniques and present a unified perspective on multiple related methods. Finally we observe that this general perspective gives rise to a natural extension and empirically show its benefit.", "sections": [{"title": "1 Introduction", "content": "Decision makers often need to act given limited data. Accounting for the resulting uncertainty (epistemic uncertainty) may be helpful for active learning (MacKay 1992a), exploration (Duff 2002; Auer, Cesa-Bianchi, and Fischer 2002) and safety (Heger 1994).\nHow to measure epistemic uncertainty efficiently for large neural networks is active research. Computational efficiency is important because even a single evaluation (e.g. a forward pass through a neural network) can be expensive. Popular approaches compute an ensemble of predictions using bootstrapping or MC dropout and incur a multiplicative computational overhead. Other approaches are faster but require changes to the predictors architecture and training procedure (Van Amersfoort et al. 2020).\nIn this paper we propose the Delta Variance family of algorithms which connects and extends Bayesian, frequentist and heuristic notions of variance. Delta Variances require no changes to the architecture or training procedure while incurring the cost of little more than a gradient computation. We present further appealing properties and benefits in Table 1.\nThe approach can be applied to neural networks or functions that contain neural networks as building blocks to compute a quantity of interest. For instance we could learn a step-by-step dynamics model and then use it to infer some utility function for decision making."}, {"title": "2 Notation", "content": "We consider a function $f_\\theta(x)$ with parameters $\\theta$ trained on a dataset $D$ of size $N$. We strive to estimate the uncertainty introduced by training on limited data. To admit Bayesian interpretations we assume that $f_\\theta(x)$ is a density model that is trained with log-likelihood (or a function that is trained with a log-likelihood-equivalent loss, such as L2 regression or cross-entropy). Unless otherwise specified we assume that $\\theta$ has been trained until convergence \u2013 i.e. equals the (local) maximum likelihood estimate $\\bar{\\theta}$. Under appropriate conditions $\\bar{\\theta}$ converges to the true distribution parameters $\\theta_{True}$. Let $H_f$ be the Hessian of the log-likelihood of all $D$ evaluated at $\\bar{\\theta}$. When adopting a Bayesian view with prior belief $p(\\theta)$ the posterior over parameters is defined as $p(\\theta | D) \\propto p(D | \\theta)p(\\theta)$. $\\mathbb{E}_{z \\sim p(z)}[\\cdot]$ refers to the expectation with respect to random variable $z$ with distribution $p(z)$ \u2013 which we shorten to $\\mathbb{E}_z$ or $\\mathbb{E}$ when the distribution is clear. Similarly let $V[X] := \\mathbb{E}[X^2] - \\mathbb{E}[X]^2$.\nLet $F_f := \\mathbb{E}_{x \\sim f_\\theta}[\\nabla_\\theta \\log f_\\theta(x) \\nabla_\\theta \\log f_\\theta(x)^\\intercal |_{\\theta = \\theta_{True}}]$ be the Fisher information matrix and let $\\hat{F}_f := \\mathbb{E}_{x \\in D}[\\nabla_\\theta \\log f_\\theta(x) \\nabla_\\theta \\log f_\\theta(x)^\\intercal |_{\\theta = \\bar{\\theta}}]$ be the empirical Fisher information. Note that $H_f^{-1}$ and $F_f^{-1}/N$ are both $O(N^{-1})$. When strong conditions are met (see van der Vaart 1998, for details) the Bernstein-von Mises theorem ensures that the Bayesian posterior converges to the Gaussian distribution $\\mathcal{N}(\\theta, F_f^{-1})$ in total variation norm independently of the choice of prior as the number of data-points $N$ increases. In Definition 1 we consider Quantities of Interest that we denote $u_\\theta$. In practice $u_\\theta(z)$ may be a utility function that depends on some context provided by $z$. For a simpler exposition but without loss of generality we assume that $u_\\theta(z)$ is scalar valued. We require $f_\\theta$ and $u_\\theta$ to have bounded second derivatives wrt. $\\theta$ in order to perform first order Taylor expansions. To simplify notations we assume that $\\theta$ is evaluated at the learned parameters $\\bar{\\theta}$ unless specified otherwise: in particular we write $\\nabla_\\theta f_\\theta(x)$ in place of $\\nabla_\\theta f_\\theta(x)|_{\\theta = \\bar{\\theta}}$ and $\\Delta f(x) := \\nabla_\\theta f_\\theta(x)|_{\\theta = \\bar{\\theta}}$."}, {"title": "3 Epistemic Variance of Quantities of Interest", "content": "Sometimes we use a neural network $f_\\theta$ to learn predictions, that are then used to compute a downstream quantity of interest $u_\\theta$ \u2013 see motivational examples below. Given limited training data neither the neural network's prediction nor the downstream quantity of interest will be exact. This motivates our research question:\nIf we estimate the parameters $\\theta$ of $u_\\theta$ by learning $f_\\theta(x)$, how can we quantify the epistemic uncertainty of $u_\\theta(z)$?\nFor a simpler exposition and without loss of generality we assume that $u_\\theta(z)$ predicts scalar quantities. The prototypical example is a utility function that depends on some context provided by $z$ and internally uses $f_\\theta$ to compute a utility value. The derivations carry over naturally to the multi-variate case. Note that $f_\\theta$ and $u_\\theta$ may have different input spaces. Our research focuses on the general case where $f_\\theta \\neq u_\\theta$ which has received little attention. This naturally includes the case where $f_\\theta = u_\\theta$.\nDefinition 1. We call the real-valued function $u_\\theta(z)$ quantity of interest if it depends on the same parameters $\\theta$ as a related parametric model $f_\\theta(x)$."}, {"title": "3.1 Motivational Examples", "content": "We consider three motivational examples for training on $f_\\theta$ but evaluating a different quantity of interest $u_\\theta$. We will see that training $f_\\theta$ is straightforward while training a predictor for $u_\\theta(z)$ is inefficient, impractical, or even impossible.\n1.  As a simple motivation let us consider estimating the 10-year survival chance using a neural network predictor $f_\\theta(x)$ of 1-year outcomes given patient features $x$:\n$u_\\theta(x) = f_\\theta(x)^{10}$\nThis example illustrates that it may be impossible to train $u_\\theta(x)$ directly unless we collect data for 9 more years, hence we train $f_\\theta(x)$ and evaluate $u_\\theta(z)$.\n2.  Distinct input spaces: $u_\\theta(z)$ might aggregate predictions of $f_\\theta(x)$ for sets $z = \\{x_1, ..., x_k\\}$: E.g. the survival chance of everyone in set of patients $z$ via $u_\\theta(z) := \\prod_{x_i \\in z} f_\\theta(x_i)$, or the average value of some basket of items $z$, or the chance of any advertisement from a presented set being clicked. Here training $f_\\theta$ may be more convenient than training $u_\\theta$.\n3.  Multiple derived quantities: In Section 6 we compute multiple quantities of interest using the GraphCast weather forecasting system (Lam et al. 2023). Training a separate $u_\\theta$ for each of them would be cumbersome and expensive."}, {"title": "3.2 Epistemic Variance", "content": "Here we define Epistemic Variance first from a Bayesian and then from a frequentist perspective. This allows us to formalize and quantify how parameter uncertainty from training $f_\\theta$ translates to uncertainty of any quantity of interest $u_\\theta(z)$ that also depends on $\\theta$.\nBayesian Definition Epistemic uncertainty can be formalized with a Bayesian posterior distribution over parameters given training data: $p(\\theta | D)$. The Epistemic Variance of a function evaluation $u_\\theta(z)$ is then defined to be the variance induced by the posterior over $\\theta$:\nDefinition 2. Given any function $u_\\theta$ and a posterior over parameters $p(\\theta | f, D)$ resulting from training $f_\\theta$ on data $D$ the Epistemic Variance of $u_\\theta(z)$ is defined as\n$\\mathbb{V}_{\\theta \\sim p(\\theta | f, D)}[u_\\theta(z)]$\nwhere $\\mathbb{V}[X] := \\mathbb{E}[X^2] - \\mathbb{E}[X]^2$.\nFrequentist Definition Leave-one-out cross-validation (Quenouille 1949) is a frequentist counterpart to Epistemic Variance. It computes the variance of $u_\\theta(z)$ induced by removing a random element from the training data and re-estimating the parameters $\\theta$.\nDefinition 3. Let $\\theta_{\\setminus i}$ be the leave-one-out parameters resulting from training $f_\\theta$ on data $D \\setminus \\{x_i\\}$, then the Leave-one-out Variance is defined as\n$\\mathbb{V}_{\\theta \\sim LOO}[u_\\theta(z)] := \\mathbb{V}_{i \\sim \\mathcal{U}(1, ..., N)}[u_{\\theta_{\\setminus i}}(z)]$\nwhere $\\mathcal{U}(1, ..., N)$ is the uniform distribution over indices."}, {"title": "4 Delta Variance Approximators", "content": "Delta Variance estimators are a family of efficient and convenient approximators of epistemic uncertainty. They can be used to compute the Epistemic Variance of a quantity of interest $u_\\theta(z)$ where the parameters $\\theta$ are obtained by learning $f_\\theta$ with limited data. Given any quantity of interest $u_\\theta(z)$ they approximate both the Bayesian Epistemic Variance as well as the frequentist leave-one-out analogue:\n$\\mathbb{V}_{\\theta \\sim p(\\theta | f, D)}[u_\\theta(z)] \\approx \\Delta u(z)^\\intercal \\Sigma \\Delta u(z) \\approx \\mathbb{V}_{\\theta \\sim LOO}[u_\\theta(z)]$\nHere the Delta $\\Delta u(z) := \\nabla_\\theta u_\\theta(z)$ is the gradient vector of $u_\\theta$ evaluated at the input $z$. $\\Sigma$ is a suitable matrix for which the canonical choice is an approximation of the scaled inverse Fisher Information matrix of $f_\\theta$."}, {"title": "4.1 How to choose $\\Sigma$", "content": "Principled choices of $\\Sigma$ Theory suggests three principled choices for the covariance matrix, which all scale as $\\Sigma \\propto \\frac{1}{N}$. Each choice can be derived in at least two ways using statistics or using influence functions (see Section 5 for details and Table 2 for an overview).\n1. The inverse Fisher Information $F_f^{-1}$ divided by N.\n2. The inverse Hessian of the training loss $H_f^{-1}$.\n3. The sandwich $H_f^{-1}F_f H_f^{-1}$ times N."}, {"title": "5 Analysis", "content": "In this section we will investigate multiple ways to derive and motivate Delta Variances. Broadly speaking they can be separated into three classes:\n1.  In Section 5.1 we begin with the easiest derivations, which approximate the Bayesian posterior and make strong assumptions that may not always apply to neural networks.\n2.  In Section 5.2 we consider the frequentist analogue of Epistemic Variance, that is compatible with neural networks and does not make assumptions about any posterior.\n3.  In Section 5.3 and 5.4 we consider alternative derivations that are based on adversarial robustness and out-of-distribution detection and rely on even fewer assumptions.\nAll of the considered derivations yield Delta Variances with principled covariance matrices. For an overview consider Table 5.1, where we can observe that assuming a Bernstein-von Mises Posterior is computationally equivalent to performing OOD Detection. Interestingly the former makes strong assumptions about $f_\\theta$ which typically do not apply to neural networks, while the later only requires that the covariance of gradients is finite and that $\\theta$ converges locally. Due to their milder assumptions the frequentist interpretations can serve as fall-back interpretations if the stricter conditions on the Bayesian interpretations are not met."}, {"title": "5.1 Bayesian Interpretation", "content": "We begin with a derivation that gives rise to a bound on the approximation error. While requiring strong assumptions, it serves as a motivation and introduction. The error diminishes with the number of observed data-points N:\n$\\mathbb{V}_{\\theta \\sim p(\\theta | f, D)}[u_\\theta(z)] = \\Delta u(z)^\\intercal \\Sigma \\Delta u(z) + O(N^{-1.5})$\nBernstein-von Mises Motivation As a motivational introduction we will derive the approximation error when the Bernstein-von Mises conditions are met (e.g. differentiability and unique optimum \u2013 see van der Vaart (1998) for details). Under such conditions the posterior converges to a Gaussian distribution centered around the maximum likelihood solution with a scaled inverse Fisher Information as covariance matrix.\np(\\theta | D) \\rightarrow \\mathcal{N}(\\bar{\\theta}, \\frac{1}{N}F_f^{-1})\nThe Epistemic Variance can then be computed using the Delta Method resulting in Proposition 1.\nProposition 1. For a normally distributed posterior with mean $\\bar{\\theta}$ and a covariance matrix $\\Sigma$ proportional to $\\frac{1}{N}$ it holds:\n$\\mathbb{V}_{\\theta \\sim p(\\theta | f, D)}[u_\\theta(z)] = \\Delta u(z)^\\intercal \\Sigma \\Delta u(z) + O(N^{-1.5})$\nwhere $\\Delta u(z) := \\nabla_\\theta u_\\theta(z)|_{\\theta = \\bar{\\theta}}$ as usual.\nProof. See appendix.\nIf the Bernstein-von Mises conditions are met Proposition 1 holds with $\\Sigma = F_f^{-1}/N.\nFurther Bayesian Interpretations Other Gaussian posterior approximations can be considered by plugging their respective posterior covariance matrix into Proposition 1: The misspecified Bernstein-von Mises theorem (see Kleijn and van der Vaart 2012) states that we obtain the sandwich covariance $H_f^{-1}F_f H_f^{-1} \\times N$ if the model $f_\\theta$ is misspecified (i.e. does not represent the data well). Proponents advocate that the sandwich estimate is more robust to heteroscedastic noise while others argue against it (Freedman 2006). Similarly a Laplace approximation (Laplace 1774; MacKay 1992b; Ritter, Botev, and Barber 2018) can be made resulting in a Delta Variance with $H_f^{-1}$. Again those choices of $\\Sigma$ are $\\propto \\frac{1}{N}$."}, {"title": "5.2 Frequentist Interpretation", "content": "To better cater to complex function approximators such as neural networks this section discusses a frequentist derivation of the Delta Variance, which relies on milder assumptions: As it is frequentist it does not consider posterior distributions. This allows us to side-step any questions about the shape and tractability of posterior distributions for neural networks. It does not require global convexity or a unique optimum. Convergence of the parameters to some local optimum together with locally bounded second derivatives is sufficient. In Proposition 2 we observe that the Delta Variance computes an infinitesimal approximation to the leave-one-out variance (see Definition 5) for choice of $\\Sigma = H_f^{-1}F_f H_f^{-1}$:\n$\\Delta u(z)^\\intercal \\Sigma \\Delta u(z) = \\mathbb{V}_{IJ}[u_\\theta(z)] \\approx \\mathbb{V}_{\\epsilon \\sim LOO}[u_\\theta(z)]$\nThe infinitesimal approximation to the leave-one-out variance (also known as the infinitesimal jackknife (Jaeckel 1972)) is defined as follows:\nDefinition 4. Let $\\theta_i$ be the parameters resulting from training $f_\\theta$ on data $D$ with $x_i$ down-weighted by $\\epsilon$ (i.e. from weight 1 to $1 - \\epsilon$), then the $\\epsilon$-Leave-One-Out Variance is defined as\n$\\mathbb{V}_{\\theta \\sim \\epsilon-LOO}[u_\\theta(z)] := \\frac{N}{\\epsilon^2} \\mathbb{V}_{i \\sim \\mathcal{U}(1, ..., N)}[u_{\\theta_i}(z)]$\nDefinition 5. With slight abuse of notation we define the Infinitesimal LOO Variance as the limit of the $\\epsilon$-Leave-One-Out Variance:\n$\\mathbb{V}_{IJ}[u_\\theta(z)] := \\lim_{\\epsilon \\rightarrow 0} \\mathbb{V}_{\\theta \\sim \\epsilon-LOO}[u_\\theta(z)]$\nProposition 2. The Delta Variance equals the infinitesimal LOO Variance for $\\Sigma = H_f^{-1}F_f H_f^{-1} \\times N$:\n$\\mathbb{V}_{IJ}[u_\\theta(z)] = \\Delta u(z)^\\intercal \\Sigma \\Delta u(z)$\nProof. See appendix."}, {"title": "5.3 Adversarial Data Interpretation", "content": "Sometimes it is of interest to quantify how much a prediction changes if the training dataset is subject to adversarial data injection. Intuitively this is connected to epistemic uncertainty: one may argue that predictions are more robust the more certain we are about their parameters and vice versa. In Section A.1 we show that this intuition also holds mathematically. In particular we observe that:\n1.  The Delta Variance with $\\Sigma = H_f^{-1} H_f^{-1}$ computes how much a quantity of interest $u_\\theta(z)$ changes if an adversarial data-point is injected.\n2.  This adversarial interpretation is technically equivalent to the Laplace Posterior approximation (from Section 5.1) even though interestingly both start with different assumptions and objectives."}, {"title": "5.4 Out-of-Distribution Interpretation", "content": "We show that a large Delta Variance of $u_\\theta(z)$ implies that its input $z$ is out-of-distribution with respect to the training data. This relates to epistemic uncertainty intuitively: a model is likely to be uncertain about data-points that differ from its training data. The derivation in Section A.2 is based on the Mahalanobis Distance (Mahalanobis 1936) \u2013 a classic metric for out of distribution detection. It accounts for the possibility that $f_\\theta \\neq u_\\theta$ and relies on minimal assumptions only requiring existence of gradients and that the training of $f_\\theta$ has converged."}, {"title": "6 Experiments", "content": "To empirically study the Delta Variance we build on the state-of-the-art GraphCast weather forecasting system (Lam et al. 2023) which trains a neural network $f_\\theta(x)$ to predict the weather 6 hours ahead. This $f_\\theta(x)$ is then iterated multiple times to make predictions up to 10 days into the future. We define various quantities of interest $u_\\theta$ such as the average rainfall in an area or the expected power of a wind turbine at a particular location and compute their Epistemic Variance.\nWe assess the Epistemic Variance predictions on 5 years of hold-out data using multiple metrics such as the correlation between predicted variance and prediction error and the likelihood of the quantities of interest. Empirically Delta Variances with a diagonal Fisher approximation yield competitive results at lower computational cost \u2013 see Figure 3. Next we give an overview on the experimental methodology \u2013 please consider the appendix for more technical details."}, {"title": "6.1 Weather Forecasting Benchmark", "content": "GraphCast Training We build on the state-of-the-art GraphCast weather prediction system. It trains a graph neural network to predict the global weather state 6 hours into the future. This step function $x_{t+1} = f_\\theta(x_t)$ is then iterated to predict up to 10 days into the future. The global weather state $x$ is represented as a grid with 5 surface variables and and 6 atmospheric variables at 37 levels of altitude (see Lam et al. 2023, for details). The authors consider a grid-sizes of 0.25 degrees. To save resources we retrain the model for a grid size of 4 degrees and reduce the number of layers and latents each by factor a of 2. Finally we skip the fine-tuning curriculum for simplicity. Besides the graph neural network we also consider a standard convolutional neural network. Training data ranges from 1979-2013 with validation data from 2014-2017 and holdout data from 2018-2021 resulting in about 100 GB of weather data.\nQuantities of Interest First we define 126 different quantities of interest $u_\\theta$ based on 4 topics that we evaluate on the hold-out data (2018-2021) for two different neural network architectures: 1) Precipitation at various times into the future. 2) Inspired by wind turbine energy yield we measure the third power of wind-speed at various times into the future. 3) Inspired by flood risk we measure precipitation averaged over areas of increasing size five days into the future. 4) Inspired by health emergencies we predict the maximum temperature maximized over areas of increasing size five days into the future. The first two quantities are predicted 1,..., 5 days"}, {"title": "7 Illustrations and Extensions", "content": "To highlight the generality of our approach we illustrate two extensions in this section.\n1.  By learning $\\Sigma$ to represent uncertainty well, we generalize the parametric from of Delta Variances beyond Fisher and Hessian matrices and observe improved results in the GraphCast benchmark \u2013 see Figure 3.\n2.  We consider an example where $u_\\theta$ is not an explicit function but maps to a fixed-point of an iterative algorithm. We observe that it is possible to compute the Delta Variance of fixed-points using the implicit function theorem. Applied to an eigenvalue solver we observe empirically that the Delta Variance variance yields reasonable uncertainty estimates \u2013 see Figure 4."}, {"title": "7.1 Learning $\\Sigma$", "content": "In Section 5 we observed that Delta Variances with special $\\Sigma$ such as the Fisher Information approximate theoretically established measures of uncertainty. In this section we observe that $\\Sigma$ may also be learned or fine-tuned. In an illustrative example we differentiate the Delta Variances with respect to $\\Sigma$ and use gradient descent to obtain an improved $\\Sigma$. This may be helpful to improve the uncertainty prediction or to improve a downstream use-case if the variance is used in a larger system.\nFine-Tuning $\\Sigma$ Example We present a simple instance of fine-tuning a few parameters of $\\Sigma$, which empirically yields improved results \u2013 see Figure 3. Note that $\\Sigma$ is approximated block-diagonally in most practical cases to limit the computational requirements \u2013 with one block for each weight vector in each neural network layer. Hence the Delta Variance splits into a sum of per-block Delta Variances derived from per-block gradients $\\Delta_i$:\n$\\Delta(x)^\\intercal \\Sigma \\Delta f(x) = \\sum_i \\Delta_i^\\intercal \\Sigma_i \\Delta_i$\nIn this example we introduce a factor to rescale $\\Sigma$ within each block. Intuitively this adjusts the importance of each layer. Since only a few parameters need to be estimated we only need little fine-tuning data. This is applicable in situations where there is a small amount of training data for $u_\\theta$. In our experiments we optimize the coefficients of this linear combination using gradient descent to improve the log-likelihood or correlation on a small set of held-out validation data. Note that the per-layer variances can be cached which reduces the optimization problem significantly."}, {"title": "7.2 Epistemic Variance of Iterative Algorithms and Implicit Functions", "content": "So far we considered quantities of interest $u_\\theta$ that are explicit functions of the parameters $\\theta$. Here we consider an example where the quantity of interest is an implicit function: $u_\\theta$ maps to the fixed-point (solution) of an iterative algorithm for which there is no closed-form formula that we could differentiate to obtain its gradient.\nGiven some initial point $w_0$ the iteration $w_{k+1} = F_\\theta(w_k)$ may converge to a fixed-point $w^*$ that depends on the parameters $\\theta$. To estimate $\\mathbb{V}[w^*]$ we need to define $u_\\theta$ as follows, which can not be differentiated with regular back-propagation due to the limit\n$u_\\theta(w_0) := \\lim_{k \\rightarrow \\infty} F_\\theta^\\circ ... \\circ F_\\theta(w_0) = w^*$\nImplicit Epistemic Variance Calculation To compute the Delta Variance of an implicitly defined $u_\\theta$ we need its gradient $\\nabla_\\theta u_\\theta$. This can be obtained under mild conditions using the implicit function theorem. Let us denote $w_{k+1} = F_\\theta(w_k)$ any fixed-point iteration converging to $w^*$ with the corresponding non-linear equation $G_\\theta(w) := F_\\theta(w) - w = 0$. The implicit function theorem yields the gradient of $u_\\theta$ by considering the Jacobian of G at the fixed-point $w^*$:\n$\\Delta w^* := \\nabla_\\theta u_\\theta = -(\\nabla_{w^*} G_\\theta(w^*))^{-1} \\nabla_\\theta G_\\theta(w^*)$\nwhenever G is continuously differentiable and the inverse of $\\nabla_{w^*} G_\\theta(w^*)$ exists. Now we can compute the Epistemic Variance as $\\mathbb{V}[w^*] \\approx \\Delta w^* \\Sigma \\Delta w^*$.\nEigenvalue Example Eigenvalues are a quantity of interest in structural engineering. As an illustrative example we consider the eigenvalues $\\lambda_i(A_\\theta)$ of a finite element model matrix $A_\\theta = M_0^{-1}K_\\theta$ that indicates the stability of a physical structure. If the parameters $\\theta$ are uncertain the eigenvalues will be uncertain as well. Recall that they are the solutions to $\\det(A_\\theta - \\lambda I) = 0$. We can estimate the Epistemic Variance of an eigenvalue $\\mathbb{V}[\\lambda_i(A_\\theta)]$ using Delta Variances if we obtain the gradient of the eigenvalue $\\nabla_\\theta \\lambda_i(A_\\theta)$. To this extend we need the implicit function theorem as $\\lambda_i(A_\\theta)$ is an implicitly defined function \u2013 please consider the appendix for technical details."}, {"title": "A Further Analysis", "content": "Sometimes it is of interest to quantify how much a prediction changes if the training dataset is subject to adversarial data injection. Intuitively this is connected to epistemic uncertainty: one may argue that predictions are more robust the more certain we are about their parameters and vice versa. In this section we will observe that this intuition also holds mathematically. In particular we will observe that:\n1.  The Delta Variance with $\\Sigma = H_f^{-1}$ computes how much a quantity of interest $u_\\theta(z)$ changes if an adversarial data-point is injected.\n2.  This adversarial interpretation is technically equivalent to the Laplace Posterior approximation (from Section 5.1) \u2013 even though interestingly both start with different assumptions and objectives.\nAt first we need to generalize the notion of adversarial robustness to the general case of $f \\neq u$: We consider the hypothetical scenario where an adversarial fine-tuning-like step on $u_\\theta$ is included into the regular training of $f_\\theta$. We then quantify the worst-case error this may introduce. The hypothetical worst-case training scenario then includes an additional data-point $z$ with adversarially selected target value $y$. Then $\\theta$ is optimized to minimize prediction error on the training data $x \\in D$ as well as the $\\epsilon$-weighted L2 loss for $u_\\theta$: I.e. the term $(u_\\theta(z) - y)^2$ is added to the training loss. We consider two corruptions where an adversary injects a bad data-point at the very input $z$ that we are interested to evaluate $u_\\theta$ at. First adding a data-point $z$ with adversarially selected value offset and second adding a data-point $z$ with noisy value:"}, {"title": "A.1 Adversarial Data Interpretation", "content": "Definition 6. We call a data-point $(z, y)$ $\\delta$-adversarial for $u_\\theta(z)$ if its target value $y$ deviates from the current prediction $u_\\theta(z)$ by some arbitrarily large value $\\delta$.\nDefinition 7. Similarly we call an data-point $(z, y)$ $\\sigma$-noise-adversarial for $u_\\theta(z)$ if its target value $y$ deviates from the current prediction $u_\\theta(z)$ by some zero-mean random variable $\\delta$ with variance $\\sigma^2$.\nProposition 3. Let $\\theta_{adv}$ be the parameters trained after including an $\\delta$-adversarial data-point (Definition 6) at $z$ with $\\epsilon$-weighted L2 loss into the training. The prediction then changes from $u_\\theta(z)$ to $u_{\\theta_{adv}}(z)$. Its difference can be described with the Delta Variance with $\\Sigma = H_f^{-1}$:\n$|(u_\\theta(z) - u_{\\theta_{adv}}(z))| = |\\delta| \\Delta u(z)^\\intercal \\Sigma \\Delta u(z) + O(\\delta^{1.5})$\nIn particular:\n$(\\Delta u(z)^\\intercal \\Sigma \\Delta u(z))^2 = \\frac{1}{\\epsilon^2} \\lim_{\\delta \\rightarrow 0} \\frac{1}{\\delta^2} (u_\\theta(z) - u_{\\theta_{adv}}(z))^2$\nProof. See Section D.\nProposition 4. Let $\\theta_{noisy}$ be the parameters trained after including a $\\sigma$-noise-adversarial data-point (Definition 7) at $z$ with $\\epsilon$-weighted L2 loss into the training. Then the expected error can be described with the Delta Variance with $\\Sigma = H_f^{-1}$:\n$\\mathbb{E}_\\delta [(u_\\theta(z) - u_{\\theta_{noisy}}(z))^2] = \\epsilon \\sigma^2 (\\Delta u(z)^\\intercal \\Sigma \\Delta u(z)) + O(\\epsilon^3)$\nIn particular:\n$(\\Delta u(z)^\\intercal \\Sigma \\Delta u(z))^2 = \\frac{1}{\\epsilon^2} \\lim_{\\sigma^2 \\rightarrow 0} \\mathbb{E}_\\delta [(u_\\theta(z) - u_{\\theta_{noisy}}(z))^2]$\nProof. See Section D."}, {"title": "A.2 Out-of-Distribution Interpretation", "content": "We show that a large Delta Variance of $u_\\theta(z)$ implies that its input $z$ is out-of-distribution with respect to the training data. This relates to epistemic uncertainty intuitively: a model is likely to be uncertain about data-points that differ from its training data. The derivation below is based on the Mahalanobis Distance (Mahalanobis 1936) \u2013 a classic metric for out of distribution detection. It accounts for the possibility that $f_\\theta \\neq u_\\theta$ and relies on minimal assumptions only requiring the existence of gradients and that the training of $f_\\theta$ has converged."}, {"title": "B Epistemic Variance of Eigenvalues", "content": "To illustrate the Epistemic Variance computation with the implicit function theorem, we consider the eigenvalue problem and compute the uncertainty of an eigenvalue.\nEigenvalue Derivation Recall that the eigenvalues $\\lambda_i$ of matrix A are the solutions to the characteristic polynomial $\\det(A - \\lambda I) = 0$, which is typically solved using iterative algorithms. If some entries of A are uncertain \u2013 or more generally if $A_\\theta$ is a function of learned parameters $\\theta$ \u2013 then its eigenvalues will be a non-trivial function of $\\theta$.\nWe can estimate the Epistemic Variance $\\mathbb{V}[\\lambda_i(A_\\theta)]$ using Delta Variances if we obtain the gradient of the eigenvalue $\\nabla_\\theta \\lambda_i(A_\\theta)$. To this extend we need the implicit function theorem as the function $\\lambda_i(A_\\theta)$ admits no closed form. We refer to the derivation in Magnus (1985) which applies to any eigenvalue of multiplicity one: $\\nabla_\\theta \\lambda_i(\\theta) = e_i^T (\\nabla_\\theta A_\\theta) \\hat{e}_i + \\mathbb{E}[e_i^T \\hat{e}_i]$ where $e_i$ and $\\hat{e}_i$ are the unit-norm left and right eigenvectors corresponding to $\\lambda_i$ of $A_\\theta$ evaluated at the learned parameters $\\bar{\\theta}$. For convenience we can also write $\\nabla_\\theta \\lambda_i(\\theta) = \\nabla_\\theta e_i A_\\theta \\hat{e}_i e_i^T \\hat{e}_i $ because $e_i$ and $\\hat{e}_i$ only depend on $\\theta$ not on $\\bar{\\theta}$."}, {}]}