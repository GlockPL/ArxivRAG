{"title": "The Scaling Law for LoRA Base on Mutual Information Upper Bound", "authors": ["Jing Zhang", "Hui Gao", "Peng Zhang", "Shuzhen Sun", "Chang Yang", "Yuexian Hou"], "abstract": "LORA (Low-Rank Adaptation) is a widely used model fine-tuning method. In fine-tuning, the law among model performance, model parameters, and data complexity has been a focal issue in the field. Existing methods often leverage external metrics (such as cross-entropy or perplexity) to evaluate model performance. In the fine-tuning process for large models, two types of knowledge are typically involved: the frozen, general knowledge acquired by the model during pre-training and the new knowledge learned through the LoRA module from the current data. Generally, the less LoRA's learned knowledge relies on the large model, the more it captures the specific knowledge of new data, thereby enhancing its adaptability to new tasks. However, external metrics do not readily capture the dependency relationship between these two types of knowledge. Therefore, we designed an internal metric based on the Mutual Information Upper Bound (MIUB) theory to investigate the scaling law of large-model LORA fine-tuning. In our experiments, we validated this approach on benchmark datasets, using the Llama3-8B and Phi3-3B models. The results show that the proposed MIUB metric aligns more accurately and stably with the scaling law of LORA fine-tuning compared to cross-entropy and perplexity.", "sections": [{"title": "1 Introduction", "content": "Pre-trained on vast amounts of data, large language models like GPT-X (Achiam et al., 2023) and LLaMA3 (Dubey et al., 2024) have achieved remarkable results in general domains. However, to address various personalized needs, especially under the pressure of inference deployment costs, fine-tuning serves as an effective method for model compression, enhancing the model's personalization and multi-tasking capabilities with relatively small datasets (Kim et al., 2024; Wang et al., 2023; Ge et al., 2023). Among these, LoRA (Low-Rank Adaptation) (Hu et al., 2021; Yang et al., 2024) fine-tuning leverages the idea of low-rank approximation. By freezing the parameters of the large model, it only uses a small number of newly added low-rank parameter matrices to learn the structured knowledge in the new data.\nIn order to reduce the blindness of large model pre-training, some work has proposed that there is a scaling law for large model pre-training (Kaplan et al., 2020; Wei et al., 2024), that is, as the size of the large model increases and the amount of pre-training data increases, the effect of pre-training usually changes regularly. However, there is currently no systematic research on the scaling law of large model LoRA fine-tuning, which will undoubtedly increase the computational cost and trial-and-error cost of fine-tuning.\nThe factors that affect the effect of model fine-tuning mainly include large model size, the size of LORA parameters, and data size. However, how to accurately quantify the impact of these factors on the effect is still an urgent problem to be solved. From a general perspective, the effect after fine-tuning is mainly related to two parts of knowledge, one is the meta-knowledge relied on from the large language model, and the other is the generalized knowledge learned by the newly added parameters (Mao et al., 2024; Jovanovic and Voss, 2024). Some work has shown that when fine-tuning large models, there will be conflicts between new and old knowledge (Shi et al., 2024).The existing evaluation metrics, such as test set PPL (perplexity) and cross-entropy, primarily focus on assessing the overall distribution of the model. However, they have yet to fine-grainedly quantify the relationship between the feature distribution of the base LLM and the feature distribution introduced by the LORA module under the fine-tuning paradigm. Therefore, some work also shows that evaluations based on indicators such as Cross-Entropy and PPL are some-\ntimes not accurate and stable (Wei et al., 2024).\nIn order to measure the relationship between the feature distribution space of the large model and the feature distribution space of LoRA, we propose to use a mutual information metric with an upper bound to accurately measure the impact of model size and data scale on the LLM effect. This paper adds the LoRA structure to the attention layer and the FFN layer. Relying on the structural advantages of LoRA, it concisely and effectively calculates the Mutual Information Upper Bound (MIUB) between the output distribution of the large model and the output distribution of LoRA after residual connection.\nIn general, the Mutual Information Upper Bound (MIUB) between the hidden representation of the large model and that of the LoRA module can serve as a quality indicator for evaluating the fine-tuning effect, particularly in the context of model compression and personalization. Experimental results show that the MIUB decreases with the increase of the size of the large model, decreases with the increase of LoRA rank, and also decreases with the increase of data size (i.e., length or complexity). Furthermore, the upper bound of mutual information not only conforms to the scaling laws, but is also more stable and more in line with the changing trend of the actual effect of the model (such as Accuracy) than traditional outward evaluation indicators. In addition, compared with the prompt template used for fine-tuning, this article also shows the size rules of MIUB under different prompt templates.\n\u2022 Building on the LoRA fine-tuning framework, we propose an evaluation metric based on the upper bound of mutual information between the hidden distributions of the large model and the LoRA model, aimed at quantifying the performance of large models under model compression and personalized fine-tuning requirements.\n\u2022 Building on the upper bound of mutual information, this paper systematically studies the scaling laws of LoRA fine-tuning based on the analysis of internal model relationships for the first time.\n\u2022 Experimental results show that the upper bound of mutual information not only conforms to scaling laws in terms of model size and data complexity, but its results are more accurate and stable than traditional Cross-Entropy and PPL indicators."}, {"title": "2 Related Works", "content": "2.1 Low-Rank Adaptation\nLow-Rank Adaptation (LoRA) is a method for effectively fine-tuning pre-trained models by introducing low-rank matrices while keeping the large model parameters frozen. In recent years, it has become a primary technique for fine-tuning large models, offering high flexibility and efficiency. (Hu et al., 2021) was the first to propose the application of LoRA in large models, with experiments showing that it can achieve good fine-tuning results with a small number of additional parameters, reducing both training parameters and computational overhead. The core idea of this method is to decompose the weight updates of the model into low-rank matrices, significantly reducing computational costs while maintaining model performance. In recent years, various works have focused on reducing the cost of model fine-tuning and enhancing its generalization capabilities in the design of LoRA structures. (Ding et al., 2023) further reduced the computational cost of LoRA by using gating units to dynamically adjust the intrinsic rank. Additionally, combining LoRA with MoE techniques has also provided assurance for enhancing its generalization ability. LoRAHub (Huang et al., 2023) selects different LoRA combinations for task generalization. (Dou et al., 2024) proposed MoELORA, which utilizes both LoRA and MoE for specific task adjustment and multi-task processing. (Liu et al., 2023) introduced the multimodal learning capabilities of multimodal expert models.\n2.2 Scaling Law\nScaling laws have been a persistent topic in both nature and science (Gan et al., 2021), and in recent years, they have also shown strong guiding capabilities in the field of Large Language Models (LLMs). In the field of neural networks, scaling laws critically demonstrate how model performance scales with increases in computational resources, data, and model parameters. (Hu et al., 2021) first introduced the \"scaling law\" for neural language models, indicating that larger models trained on more data tend to perform better. (Zhang et al., 2024) comprehensively tested the scaling laws of fine-tuning frameworks under existing evaluation metrics. In the information retrieval domain, (Fang et al., 2024)"}, {"title": "3 Methodology", "content": "3.1 The Scaling Law of LORA\nFor newly added fine-tuning data, without disrupting the feature space of the large model itself (i.e., by freezing the parameters of the large model), LORA relies on some of the meta-knowledge of the LLM and learns new specific features by adding low-rank parameter weights. Therefore, as shown in Figure 1, there is a natural dependence and generalization relationship between the LLM and LORA modules. Furthermore, we model the dependency relationship between them as mutual information.\nAssumption 1. In neural networks, the dependency between large language models (LLM) and LoRA is manifested through the intertwining of their feature spaces, which can be quantified by mutual information. This mutual information not only measures the information obtained about the distribution of LORA from the LLM variables but also reveals the extent of their overlap in feature space. Under this dependency, the feature distribution of LoRA can be viewed as conditional, co-evolving with the distribution of the LLM in the hidden space.\nDefinition 2. Formally, let O represent the hidden states from the feature space of the LLM, and L represent the hidden states from the feature space of LoRA. The mutual information I(O; L) between O and L is defined as:\n$I(O; L) = \\iint_{OXL} p(o, l) \\log (\\frac{p(o, l)}{p(o)p(l)}) dodl,$\nwhere p(o,l) is the joint probability density function of O and L, and p(o) and p(l) are the marginal probability density functions of O and L, respectively. This double integral defines mutual information over the continuous domain, with every point in the feature space contributing to this information measure. Furthermore, a higher value of I(O; L) indicates a stronger dependency between O and L and a greater degree of shared information.\nThe mutual information measure based on the LORA architecture quantifies the role of the large model's feature space information in reducing the uncertainty of the LoRA distribution's feature space, thereby capturing the dependency between the two. However, a strong dependency does not necessarily align with the actual performance of the model, and it is difficult to find a consistent pattern across models of different scales. Therefore, this paper introduces an upper bound on mutual information to measure the distance between the large model distribution and the LoRA distribution.\nTheorem 3 (Upper Bound of Mutual Information Based on JS Divergence). Let O and L be random variables in the original LLM and LoRA probability spaces with joint distribution POL and marginal distributions Po and PL, respectively. Let $D_{JS}(P_O||P_L)$ denote the Jensen-Shannon divergence between Po and PL. The mutual information I(O; L) satisfies the following upper bound:\n$I(O; L) \\le log(2) \\cdot D_{JS}(P_{OL}||P_OP_L)$\nwhere the Jensen-Shannon divergence between two distributions P and Q is defined as:\n$D_{JS}(P||Q) = \\frac{1}{2} D_{KL}(P||M) + \\frac{1}{2} D_{KL}(Q||M)$,\nand $D_{KL}$ denotes the Kullback-Leibler divergence, with $M = \\frac{1}{2}(P + Q)$, where P and Q are the distributions being compared.\nProof. We begin by noting that mutual information I(O; L) can be expressed as the Kullback-Leibler divergence between the joint distribution POL and the product of the marginal distributions Po and PL:\n$I(O; L) = D_{KL}(P_{OL}||P_OP_L)$.\nThe Jensen-Shannon divergence between the joint distribution $P_{OL}$ and the product of the marginal distributions $P_OP_L$ is defined as:\n$D_{JS}(P_{OL}||P_OP_L) = \\frac{1}{2}D_{KL}(P_{OL}||M) + \\frac{1}{2}D_{KL}(P_OP_L||M)$,\nwhere $M = \\frac{1}{2}(P_{OL} + P_OP_L)$. This form is symmetric, and the divergence is bounded below by the"}, {"title": "3.2 The FrameWork of Fine-tuning and Evaluation", "content": "The paper adds LoRA structures to all the Dense Linear layers in the Attention and FFN modules of a large model. The original parameters of the large model are frozen, and only the LoRA components"}, {"title": "4 Experiments", "content": "In this section, we will evaluate the proposed model structure on natural language tasks and verify the effectiveness of various measures we use. All experiments were performed on NVIDIA A-800 GPU."}, {"title": "4.1 Model and Hyperparameters", "content": "We used Llama3 (Dubey et al., 2024) and Phi3-3B (Abdin et al., 2024) as our testing model, Llama3 has 8B parameters and 32 layers and Phi3 has 3B parameters and 32 layers, we use them to test the best model settings on models of different sizes. We use Adam as the optimizer with a learning rate of 4 \u00d7 10-5 for fine-tuning downstream tasks and set the batch size to 32."}, {"title": "4.2 Dataset", "content": "We used our proposed structure on five popular zero-shot generation tasks, including PIQA (Bisk et al., 2020), ARC-Challenge (Clark et al., 2018), ARC-Easy (Clark et al., 2018), Winogrande (Sakaguchi et al., 2021), and HellaSwag (Zellers et al., 2019), with higher accuracy, indicating that Mooe has a stronger parameter fine-tuning ability to handle downstream tasks.\nFor perplexity verification, we chose Wiki2 (Merity et al., 2016) and PTB (Marcus et al., 1994) as our verification data sets. Lower perplexity indicates that the compressed model has a stronger ability to maintain the output distribution of the original model."}, {"title": "4.3 Metrics", "content": "The test metrics used in this paper are as follows:\nCross Entropy Loss (CE) measures how well the predicted probability distribution q approximates the true distribution p. Lower cross-entropy indicates better predictive performance.\n$H(p,q) = - \\sum_{i=1}^{N} p(w_i) \\log q(w_i)$\nwhere p(wi) is true probability distribution of the i-th event, q(wi) is predicted probability distri-\nbution of the i-th event, and N is number of events or classes.\nPerplexity (PPL) is the exponentiated average negative log-likelihood of a sequence. It measures how well a language model predicts a sequence, with lower values indicating better performance.\nPPL = exp(\\frac{1}{N} \\sum_{i=1}^{N} log q(w_i))\nwhere q(wi) is the predicted probability of the i-th word and N represents number of words in the sequence.\nMutual information measures the amount of information shared between LLM space and LoRA Space."}, {"title": "4.4 Scaling Law Setting", "content": "\u2022 For the scaling settings of the LoRA components, we primarily adjusted the rank to different sizes, specifically 32, 128, 512.\n\u2022 For the large model, we applied a parameter-sharing compression method to adjust the scaling of the model. To ensure that the model's basic performance is not unfairly affected or that abnormal experimental results do not occur due to compression, we fixed the first 16 layers of the Phi3 and llama3 models and applied different parameter-sharing strategies to the last 16 layers: sharing every eight layers (shares), every four layers (share4), every two layers (share2), and no sharing (share1)."}, {"title": "4.5 Main Results", "content": "We conducted experiments on five benchmark datasets, where AVG refers to the average value of all datasets. The experimental results show that the indicator proposed in this paper has two conclusions:\nMIUB changes regularly with the change of model size. The bolded text in Table 1 indicates that MIUB follows the pattern of decreasing as the model size increases. From the results of AVG (the average results of ARCE, ARCC, Hel, PIQA and Win), the ranks of LoRA are set to 32, 128 and 512 respectively. As the ranks increase, that is, the size of LoRA increases, MIUB gradually decreases, which means that LoRA relies less on the features of the large model and has stronger generalization. In the right half of the table, the sizes of the large model are set to share8, share4, share3, share\u2081, which means that the parameters of every eight layers of the large model are shared, every four layers of parameters are shared, every two layers of parameters are shared, and the source large model. As the size of the large model changes, MIUB also decreases. It is worth noting that the change of the rank of the LoRA part has little effect on the model size, so the change of MIUB is small, while the change of the large model size is large, so the change of MIUB is larger. Additionally, as shown in Figure 2, we present the MIUB and PPL with respect to the size of the large model for the PTB and Wiki2 language modeling tasks. The experimental results indicate that, with the increase in the number of parameters, MIUB exhibits a significant decreasing trend, demonstrating that the scaling law holds.\nCompared to traditional metrics, MIUB not only better reflects the changing trend of actual effects but also exhibits greater stability in adhering to the scaling law.\nWe analyze this from two perspectives: first, by comparing the trends of CE, MIUB, and actual performance (ACC), and second, by conducting a comparative analysis of the scaling laws based on PPL, CE, and MIUB. As shown in Table 1, in the AVG test based on the Phi3 model, an abnormal increase in CE was observed (indicated by the red arrow) even as ACC improved. In contrast, MIUB consistently decreased as ACC increased, indicating that during fine-tuning, the model's dependency on the larger model weakened, leading to stronger learning of generalized knowledge. Regarding the stability of the scaling law, compared to CE, which exhibited a significant abnormal increase as the rank increased, MIUB consistently maintained a steady decline. As the size of the larger model increased, calculations revealed that the CE value at shares was 571 times that of share1, while the size of the larger model increased by less than twice. In comparison, MIUB's change was more stable, decreasing by 17%. This effect is more pronounced in Figure 2. The change in large model size is not linear; the increase becomes more significant. Additionally, under limited data conditions, although the model tends to learn more generalized knowledge, the complexity of the large model inevitably increases. Therefore, while MIUB still shows a decreasing trend, the rate of decrease will correspondingly diminish."}, {"title": "4.6 Data Complexity", "content": "To study the data size scaling law based on MIUB, we conducted experiments analyzing data complexity. As described in the Scaling Law Setting section, the data was divided according to length, and the experimental results are shown in Figure 3. The results indicate that as the data length increases, the actual performance (ACC) of the large model improves, suggesting that the large model acquires more comprehensive information from the prompt. At the same time, MIUB exhibits a systematic decrease, indicating that as the new data becomes more complex (larger in scale), the dependency on the large model during fine-tuning diminishes, and there is a greater need for the LoRA module to learn more generalized knowledge."}, {"title": "4.7 MI VS MIUB", "content": "To compare the differences between Mutual Information (MI) and its upper bound (MIUB), this paper tests these two evaluation metrics within the LORA fine-tuning framework. As shown in Table 2, with the increase in the rank setting of the LORA module and the size of the large model, both MI and MIUB exhibit a downward trend, and their values are relatively close. However, as MIUB is the upper bound of MI, it consistently remains higher than MI. In certain special cases, such as small sample sizes, high model complexity, or high-dimensional data, the gap between MI and MIUB may be larger. These factors can cause MIUB to be overestimated or MI to be underestimated, thus widening the difference between the two. Therefore, MIUB is more suited for generalized scenarios."}, {"title": "4.8 Prompt Learning Analysis", "content": "In the Methodology section, this paper uses four types of prompt templates, as shown in Table 3. \"Main\" refers to the zero-shot prompt used for training, while \"Prompt1,\" \"Prompt2,\" and \"Prompt3\" are one-shot templates, few-shot templates with positive and negative examples, and output control templates, respectively.\nThe experimental results show that regardless of the template used, MIUB decreases with the increase in LoRA and the size of the large model, demonstrating the stability of MIUB as a scaling law effectiveness metric. Comparing the four shampfompts, the order is generally: MIUB (Prompt1) 0.880MIUB (Main) > MIUB (Prompt3) > MIUB 1561(BHgmpt2). Prompt1 has the highest MIUB because 1561it35corporates data from the training set, which enhances the dependency on the large model during the knowledge learning process. In contrast, Prompt2, due to the inclusion of negative examples, has greater uncertainty and thus a smaller mutual information upper bound."}, {"title": "5 Conclusion", "content": "This paper proposes an upper bound metric for mutual information (MIUB) used for evaluating general LoRA frameworks and systematically explores the patterns of MIUB with respect to changes in large model sizes, LoRA rank sizes, and data sizes. Specifically, LoRA fine-tuning is typically a technique for enhancing the multifaceted capabilities of large models using a small amount of data and rela-"}]}