{"title": "AppVLM: A Lightweight Vision Language Model for Online App Control", "authors": ["Georgios Papoudakis", "Thomas Coste", "Zhihao Wu", "Jianye Hao", "Jun Wang", "Kun Shao"], "abstract": "The utilisation of foundation models as smartphone assistants, termed app agents, is a critical research challenge. These agents aim to execute human instructions on smartphones by interpreting textual instructions and performing actions via the device's interface. While promising, current approaches face significant limitations. Methods that use large proprietary models, such as GPT-40, are computationally expensive, while those that use smaller fine-tuned models often lack adaptability to out-of-distribution tasks. In this work, we introduce AppVLM, a lightweight Vision-Language Model (VLM). First, we fine-tune it offline on the AndroidControl dataset. Then, we refine its policy by collecting data from the AndroidWorld environment and performing further training iterations. Our results indicate that AppVLM achieves the highest action prediction accuracy in offline evaluation on the AndroidControl dataset, compared to all evaluated baselines, and matches GPT-40 in online task completion success rate in the AndroidWorld environment, while being up to ten times faster. This makes AppVLM a practical and efficient solution for real-world deployment.", "sections": [{"title": "1. Introduction", "content": "The development of smartphone assistants using foundation models is an open research challenge. These assistants, which we refer to as app agents, should be capable of executing human instructions on a smartphone, interacting with apps through the same interface as a human user. The user provides a textual description of a goal, and the app agent must take a sequence of actions to successfully complete the task. Such technology has the potential to revolutionise smartphone interactions, providing significant business value by enabling automation for productivity tools, customer service, and accessibility features. Moreover, it could enhance smartphone accessibility for a wider range of users, including individuals with disabilities or those less familiar with digital interfaces.\nTwo primary approaches have been explored for developing app agents. The first relies on large foundation models, such as GPT-4, combined with prompt engineering methods to solve tasks. While flexible, this approach is expensive, both in terms of financial resources and execution time; making real-world deployment impractical. The second approach focuses on fine-tuning smaller models (e.g., Bai et al., 2024; Ma et al., 2024; Christianos et al., 2024; Wang et al., 2024c), typically using an offline dataset and, in some cases, incorporating online-collected trajectories. While these methods have demonstrated promising results, many evaluations are limited to offline action predictions or online tasks drawn from the same distribution as the training dataset. However, findings from Chen et al. (2024) suggest that when these models are tested in out-of-distribution (OOD) settings, their success rates drop significantly. This highlights a critical challenge in generalising beyond the training distribution.\nIn this work, we propose AppVLM, an app agent designed to overcome these challenges by achieving both efficiency and strong generalisation to tasks OOD compared to the original offline dataset. Our model is lightweight, enabling fast and cost-effective inference for real-time execution, and capable of adapting to OOD tasks, unlike standard offline-trained models. To achieve this, we assume access to an offline dataset of near-optimal human trajectories of phone interactions, which we use for Supervised Fine-Tuning (SFT) as an initial step on top of a pretrained vision-language model (VLM). This allows the model to become familiar with the observations and actions required for interacting with an Android smartphone. We then introduce a Reinforce Fine-Tuning (RFT) pipeline, consisting of data collection, utilising a distributed client-server architecture to balance resources and enable efficient data collection, followed by offline fine-tuning, where the collected data is used to refine the agent's decision-making capabilities. Using this pipeline, we iteratively fine-tune our model, which we refer to as AppVLM."}, {"title": "Related Work", "content": ""}, {"title": "2.1. Prompt Engineering Agents", "content": "Several recent works focus on developing agents that execute actions in smartphone or desktop environments in order to complete textual commands. With the advancement of foundation models, the research community has been exploring ways to leverage the general cross-domain knowledge of these pretrained models for app control. Yang et al. (2023); Wang et al. (2024b) were some of the first works that utilised large foundation models to perceive smartphone observations and generate human-like actions. To successfully solve more complex tasks requiring long-term planning and history awareness, several frameworks were proposed with dedicated prompt-engineering components for steps like planning, reflection, etc. (Wang et al., 2024a; Wang & Liu, 2024; Song et al., 2024). Although these added reasoning steps improved performance considerably, they significantly increased the computational cost and wall-time of each interaction. Other works tried to obtain app-specific knowledge utilising memory (Wen et al., 2023; Lee et al., 2024), which stores past interaction between the agent and specific apps."}, {"title": "2.2. Fine-Tuned Agents", "content": "To address the gap between the general capabilities of foundation models and the specific needs of smartphone environments, as well as to reduce the cost of querying general foundation models, several works have focused on fine-tuning to implement more specialised app agents. Wang et al. (2024d); Gou et al. (2024) use large foundation models for the high-level proposal of actions or plans, while they fine-tune a smaller VLM to ground this action. Ma et al. (2024) proposed CoCoAgent, a small foundation model that aims to predict actions for app control in smartphones by decomposing the actions into action type prediction and optionally the target UI element that this action will be applied to. Similarly, LiMAC (Christianos et al., 2024) introduced a small action transformer to predict the action type and the target UI element, while integrating a fine-tuned VLM for text completion. InfiGUIAgent (Liu et al., 2025) proposed a two-stage fine-tuning process, which first focuses on learning details about the screenshot observations, such as predicting the text of specific UI elements, and then learns how to generate actions based on user's instructions.\nPrevious research has also investigated online optimisation of app agents to overcome the limitations of trajectory diversity in static datasets. DigiRL (Bai et al., 2024) introduced an online RL framework that simulates app control tasks, training a policy that is first fine-tuned on an offline dataset. DistRL (Wang et al., 2024c) enhanced the training efficiency with asynchronous online learning. However, both methods depend on online tasks that follow the same distribution as the offline dataset. In contrast, our work aims to enable agents to tackle tasks beyond those encountered during the initial SFT within the offline dataset."}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Problem Formulation", "content": "We define the app control task as a Goal-conditioned Partially Observable Markov Decision Process (GPOMDP), represented as $(S, A, O, G, R, T, \\Omega)$. Here, $S$ is the set of states, $A$ is the set of actions, $O$ is the set of observations, and $G$ is the set of goals. The function $T$ describes the state transition dynamics, and $\\Omega$ represents the observation probability distribution. The reward function is denoted by $R$. We assume an agent with a parameterized policy $\\pi_\\theta$, where $\\theta$ represents the policy parameters. Our objective is to optimize the following expression:\n$\\max E_{\\tau ~ G}\\left[ \\sum_{t=0}^{H-1} \\gamma^t r_t \\right]$,\nwhere $r_t$ is the reward at time step $t$ of the episode and $H$ is the horizon of the episode. For simplicity, we assume $\\gamma = 1$ in this task. The reward function returns 1 when the episode terminates successfully, and 0 otherwise. To run our experiments, we specifically use the AndroidWorld environment (Rawles et al., 2024), which consists of parametrised tasks to be solved in an online fashion. For example, the task of adding a contact might be described as \"Create a new contact for Sofija Alves. Their number is +17168349367.\""}, {"title": "3.2. Supervised Fine-Tuning", "content": "Before initiating any online interactions within the AndroidWorld environment, we first perform SFT on a VLM using the AndroidControl dataset (Li et al., 2024), to allow the model to learn essential Android phone interactions. We use the Paligemma-3B-896 (Beyer et al., 2024) as our base model for several reasons. First, with 3 billion parameters, it offers a good balance of performance and efficiency, making it lightweight enough for mobile device deployment, especially when quantised to lower precision. Furthermore, Paligemma-3B-896 downscales images from their original resolution of 2400x1080 pixels to 896x896 pixels. This preserves important visual details, such as legible text, while supporting higher accuracy in tasks that require visual comprehension. In contrast, many CLIP-based (Radford et al., 2021) vision transformers typically downscale images to 224x224 pixels, a reduction that results in the loss of fine-grained details, making it difficult to retain important visual details and hindering task success. Paligemma-3B-896 has been fine-tuned for computer vision tasks, and is therefore not inherently capable of executing app control commands based on textual instructions. As such, the SFT step in this work is essential for adapting the model to execute app-specific tasks within the AndroidWorld environment.\nThe input for Paligemma is constructed as follows: For each observation, we use a screenshot annotated with bounding boxes and a label indicating the UI element number for each clickable item. This information is available in the UI tree of the observation, which can be extracted from"}, {"title": "3.3. Reinforce Fine-Tuning", "content": "After fine-tuning the agent on the AndroidControl dataset, we deploy it within an interaction and fine-tuning pipeline using the AndroidWorld environment. We refer to this procedure as Reinforce Fine-Tuning (RFT), also known as Reinforced Self-Training (ReST) (Gulcehre et al., 2023), or iterative SFT with rejection sampling. Please note, that RFT should not be confused with the on-policy REINFORCE algorithm (Willams, 1992). Collecting data and fine-tuning in this way is an essential step to enable the agent to adapt to tasks in AndroidWorld, which differ from the training data provided by AndroidControl. The RFT pipeline consists of two steps, executed sequentially: (1) data collection and preprocessing and (2) fine-tuning our model, AppVLM. These two steps of data collection and policy improvement using the SFT loss correspond to the \"grow\" and \"improve\" steps of the ReST algorithm (Gulcehre et al., 2023). In contrast to ReST, our RFT does not use the original offline dataset during the policy improvement."}, {"title": "3.3.1. DATA COLLECTION AND PREPROCESSING", "content": "To facilitate efficient and scalable data collection, we implement a distributed client-server architecture"}, {"title": "3.3.2. RFT POLICY IMPROVEMENT", "content": "After collecting a dataset that contains successful AndroidWorld trajectories, we fine-tune AppVLM to enhance the agent's ability to solve a broader range of tasks. The optimisation objective follows the standard maximum likelihood objective weighted by a return term, where successful trajectories are assigned a return of one, and unsuccessful ones receive a return of zero. In practice, trajectories that have received zero return are not included in the fine-tuning dataset. The RFT optimisation objective is:\n$L_{rft} = -E_{x,a \\sim D_{on}} \\sum_i log \\pi(a_i | x, a_{<i})$,\nwhere $D_{on}$ is the gathered preprocessed dataset."}, {"title": "3.4. The Training Architecture", "content": "Having outlined the individual training steps, this section provides a summary of the entire training pipeline, presented in Algorithm 1. As described in Section 3.2, the process begins with SFT on the pretrained Paligemma model. The resulting model, referred to as AppVLM-base, can interact with Android emulators and generate actions in the correct format. However, it faces challenges in online interactions. To address this, the model undergoes further refinement using RFT on data generated within the AndroidWorld environment. This pipeline alternates between collecting trajectories and fine-tuning the model by maximising the likelihood of actions that led to successful task completions. The policy improvement phase of the RFT is conducted offline to allow recalibrating the dataset by removing duplicate observations and to ensure a more balanced distribution of data across tasks. This approach prevents over-representation of simpler tasks, which could cause the model to overfit. In our experiments, we use three iterations of RFT. As a final step, all data collected throughout the entire RFT procedure is used to fine-tune the agent of the initial SFT stage, AppVLM-base, using the standard maximum likelihood objective. In Section 4.6, we present an ablation study demonstrating that this approach achieves superior performance in both online and offline settings compared to fine-tuning directly the output of the RFT pipeline."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Android World Environment", "content": "AndroidWorld (Rawles et al., 2024) offers a benchmark of 116 unique tasks with randomised parametrisation, leading"}, {"title": "4.2. Android Control Dataset", "content": "Before the RFT in AndroidWorld, we perform an SFT step on AndroidControl (Li et al., 2024), due to its similarities with AndroidWorld. AndroidControl is an open-source app control dataset that contains a large number of human demonstrations for a wide range of phone navigation tasks, from setting alarms to adding items to a shopping basket.\nImportantly, AndroidControl episodes present themselves similarly to AndroidWorld tasks. Each episode contains a textual goal along with a list of observations and corre-"}, {"title": "4.3. Evaluated Baselines", "content": "GPT-40 methods: T3A and M3A (Rawles et al., 2024) were introduced alongside AndroidWorld and are widely used as evaluation baseline. They are based on the same two-step prompting method: summarising the previous action in one step, and generating an action based on the current observation and the summary. T3A is text-only, receiving observations as a list of UI elements and descriptions based on the UI tree, while M3A also receives screenshots of the phone screen annotated with UI element bounding boxes and labels. In addition to these two agents, we include SeeAct (Zheng et al., 2024), another popular two-step GPT-prompting method. Specifically, we use the SeeActchoice variant, as in Rawles et al. (2024), since this has been found to be the best-performing (Zheng et al., 2024). In this variant, GPT-40 is given the goal and screenshot and prompted to produce a high-level description of the proposed action. The next step is an action grounding step, where a multiple-choice list of UI elements is provided, along with the action proposal and details about expected action formats, and GPT-40 is tasked with producing the final action output.\nFine-Tuned Models: We also include smaller models, fine-tuned on the AndroidControl dataset, as evaluation baselines. First, we evaluate Llama-3 (Dubey et al., 2024) with 8B parameters. We fine-tuned Llama-3 using a similar observation format as AppVLM, but instead of using screenshots as input, we provide a condensed textual form of the UI-tree. To reduce computational requirements, we fine-tune Llama-3 using LoRA adapters (Hu et al., 2021). In addition, for AndroidControl, we include the action prediction accuracy of the LT-all-r64 model as reported by Li et al. (2024). LT-all-r64 model is a fine-tuned version of PALM-2S using LORA adapters, which achieved the highest accuracy among all evaluated models in Li et al. (2024). To the best of our knowledge, it has achieved the highest reported accuracy to this day in AndroidControl. It is important to note that this comparison may not be entirely consistent. While we have made every effort to faithfully reproduce their evaluation protocol, minor differences could impact the comparison with AppVLM. Since the LT-all-r64 model is unavailable,"}, {"title": "4.4. Experimental Setup", "content": "Our experiments focus on two evaluations: an offline evaluation of the action prediction accuracy in AndroidControl, and an online evaluation of the success rate in the AndroidWorld environment tasks. Details about these can be found in Sections 4.1 and 4.2 respectively. Here we discuss specifically how we conduct evaluation in these settings.\nAndroidControl: Each timestep is a datapoint, composed of a goal, observation, and an action. Models are tasked with generating an action, which will be compared against the ground truth. Fine-tuned models are trained to provide the appropriate action format, while GPT-40 methods are provided with a large prompt detailing the format, as in Rawles et al. (2024). A relaxed action prediction accuracy is reported for all methods, whereby a click target is considered correct as long as its bounding box is within the target element, following previous works (Li et al., 2024).\nAndroidWorld: Online evaluation is performed, where agents are tasked with taking steps until a task is either solved or the maximum number is steps is reached. Task success is evaluated at every step using the provided reward signal, and a task is considered unsuccessful if the maximum number of steps is reached. In addition to overall success rate, we report per-difficulty success rates, using the task difficulty information provided by the benchmark. Due to the nature of our agents, action space, and evaluation process, certain tasks are omitted from the evaluation, notably verification and Q&A tasks. Discussed further in Appendix A.3, our final benchmark consists of 82 tasks, with a harder difficulty distribution than the full 116-task benchmark. In our AndroidWorld experiments, we perform evaluation across three different seeds, leading to different"}, {"title": "4.5. Results and Analysis", "content": "Table 1 shows the action accuracy of all methods on the four splits of the AndroidControl test set. We find that AppVLM-base, which is fine-tuned only on AndroidControl, outperforms all baselines as well as AppVLM. It is important to highlight that AppVLM-base achieves the best action accuracy on this task, surpassing the previous benchmark achieved by LT-all-r64. Even AppVLM achieves comparable accuracy to LT-all-r64 in IDD test set, and higher accuracy in OOD test splits.\nThe decline in action accuracy of AppVLM compared to AppVLM-base is expected, as the final SFT step relies solely on new data from the AndroidWorld environment. This shift reduces model accuracy in AndroidControl. A key factor in this decline is the rigidity of AndroidControl action accuracy evaluation. For example, in AndroidControl, the trajectory typically includes a wait action after performing open-app. In contrast, AndroidWorld introduces an automatic two-second delay between actions, eliminating the need for an explicit wait action. During the online dataset preprocessing, these wait actions are usually removed, as they do not affect the phone's screenshot. Finally, AppVLM also outperforms Llama-3 in AndroidControl, which may indicate that for the specific task of predicting actions that match the ground truth, the image may be more informative. A similar pattern is observed when comparing M3A with T3A, providing further evidence that visual information plays a crucial role in action prediction.\nTable 2 presents the online evaluation success rate of AppVLM and related baselines in the AndroidWorld environment across three different difficulty levels. AppVLM achieves performance comparable to M3A/T3A while requiring significantly fewer resources, both in financial cost and computation time. Indeed, it exceeds both SeeAct and M3A's performance, while coming only 4% short of T3A's"}, {"title": "4.6. Additional Studies", "content": "In this section, we provide additional studies to further explain the design choices of AppVLM. First, in Table 3, we present the action accuracy and success rate in AndroidControl and AndroidWorld respectively for different iterations of RFT. We observe that RFT plays a crucial role in improving AppVLM's success rate in AndroidWorld, with a linear increase over three iterations. However, further iterations of RFT showed that the improvement in success rate was lower compared to performing SFT in AppVLM-base, as evidenced in Tables 1 and 4. Additionally, the action accuracy in AndroidControl drops across RFT iterations, as previously discussed in Section 4.5.\nWe also provide an analysis of how the final SFT step influences both offline and online performance (see Table 4). We compare AppVLM, against AppVLM-RFT_4, and AppVLM-AWO (AndroidWorld Only), which skips SFT on AndroidControl entirely, performing SFT on top of the pretrained Paligemma model using the collected AndroidWorld dataset. Our results show that AppVLM-RFT_4 suffers from lower action prediction accuracy in AndroidControl compared to AppVLM. This follows the downward trend observed in Table 3 over successive RFT training steps. Similarly, its AndroidWorld success rate is lower than that of AppVLM. We hypothesise that AppVLM-RFT_4's performance saturates as it starts to overfit to the simpler tasks. AppVLM-AWO, on the other hand, performs poorly in offline evaluations since it has not been fine-tuned on AndroidControl. Its online success rate is also relatively low, because it has not learned basic Android interactions that would have been acquired through SFT on AndroidControl, which also represents a much more significant amount of training data than the collected AndroidWorld dataset. By applying the final SFT step on AppVLM-base, we retain high action prediction accuracy on AndroidControl while achieving the highest success rate on AndroidWorld tasks compared to the fine-tuning baselines."}, {"title": "4.7. Case Study and Failure Analysis", "content": "For illustration purposes, we show examples of AndroidWorld trajectories"}, {"title": "5. Conclusion", "content": "In this work, we introduced AppVLM, the first lightweight VLM capable of successfully solving online tasks in AndroidWorld. We present a complete pipeline for fine-tuning a pretrained VLM to efficiently tackle goal-based tasks on Android smartphones. Our results demonstrate that AppVLM-base achieves the highest AndroidControl action prediction accuracy, compared to all baselines. Moreover, in online evaluations within AndroidWorld, AppVLM delivers performance comparable to, and in some cases exceeding, agents that rely on GPT-40, while requiring significantly less time and computational resources. Notably, AppVLM can compute actions up to ten times faster than GPT-40 agents, making it an efficient alternative for app control.\nDespite its strong performance, AppVLM has limitations stemming primarily from the constraints of its training data. The model struggles with tasks involving operations it has never encountered, such as using the phone's \u201cclipboard\". Since it has not been exposed to the concept of a clipboard, it fails to recognise and execute related actions. Addressing these gaps requires expanding the scope of training data to better capture app control tasks. A promising direction is integrating a broader range of mobile interactions during pretraining, such as UI element detection, UI tree generation, etc. Existing datasets such as AndroidControl and AitW (Rawles et al., 2023) provide valuable benchmarks, but they lack a unified format. For example, AitW does not include UI trees and focuses more on generalisation across Android versions. To advance this field, the community should prioritise the creation of a large-scale, standardised dataset tailored specifically for app control.\nAnother crucial challenge is data generation. Currently, most datasets rely on human demonstrations, a process that is expensive, time-consuming, and impractical at scale. However, automatically generating trajectories is limited by the lack of reward functions for such tasks. AndroidWorld is the only app control environment that provides an internal reward function. Other approaches leverage large foundation models (e.g., GPT-40) to evaluate trajectories, but these methods are slow, costly, and highly sensitive to prompt variations, making them unreliable for systematic evaluation. To overcome these challenges, we believe that the development of dedicated reward models for app control is necessary. Recent studies have explored using models as reward functions (Ma et al., 2022; Chan et al., 2023), yet no robust, and app control-specific, reward model has been proposed. Such a model would enable scalable evaluation and unlock new possibilities for RL in app control."}, {"title": "A. Datasets and Environment", "content": "This section presents additional information and examples about our datasets and environment."}, {"title": "A.1. Action Space", "content": "As introduced in Section 4.1, AppVLM has a fixed action space. This helps standardise actions for training and grounding the model's outputs into valid actions. Our action space is presented in Table 5, along with example actions as they are expected to be generated by AppVLM. The action spaces of AndroidWorld and AndroidControl are very similar, with only minor naming differences, as well as a couple action alterations. AndroidWorld includes a keyboard-enter action which we omit, since it is not present in AndroidControl and thus our initial fine-tuning. AndroidControl also includes a click target as part of its input-text action, while we choose to keep these as separate actions as in AndroidControl."}, {"title": "A.2. Observation Space", "content": "Sections 4.1 and 4.2 introduced the observation processing performed on AndroidWorld and AndroidControl respectively. An example observation from AndroidWorld is illustrated in Figure 3, though an observation from AndroidControl would be essentially identical. As previously described, this observation contains both a visual input, the annotated screenshot, and a textual input, composed of the goal and history of actions.\nThe history of actions provides crucial context for the current state and offers options for error recovery and mitigation. To reduce computational costs, with the objective of creating a lightweight agent, we limit the size of this history to only the five most recent actions. Additionally, the target element index component of click and long-press actions is not very informative as part of this history once the timestep's screenshot is no longer observable. Therefore, the agent stores an alternate representation of actions in its history, sourced from the UI tree data. A condensed textual representation of the target element is used, containing information such as the type of object and its textual content or description, as can be seen in the textual input of Figure 3."}, {"title": "A.3. Android World Benchmark Set", "content": "While the full AndroidWorld benchmark consists of 116 tasks, we use a reduced subset of 82 tasks for our experiments. Firstly, we remove the verification tasks, such as ClockStopWatchPausedVerify, because we check whether tasks have been successfully completed at each timestep and these tasks would automatically succeed. We also remove all Q&A"}, {"title": "B. Implementation Details", "content": "In this section we discuss the implementation details of AppVLM. As we already discussed we use Paligemma-3b-pt-896\nas our base model. All fine-tuning rounds, both for the initial SFT, the RFT, and the last SFT steps use the AdamW optimiser\nwith 3 \u00d7 10-6 learning rate. The learning rate is gradually reduced to zero during the course of\nthe training. Additionally we fine-tune always for three epochs and we use effective batch size of 64. We perform full\nfine-tuning of the model without using any adapters."}, {"title": "C. Case Studies", "content": "Sample AndroidWorld trajectories from our final model are illustrated in the following figures. Figure 4 shows our agent\ncorrecting the audio recorder task from Figure 2. In this example, AppVLM successfully deletes the existing text before\ntyping the filename. This is particularly impressive because the model has learned to generate a long-press action to\ndo so, an action which is extremely rare in the initial AndroidControl dataset, featuring less than 1% of the time. It shows\nthe merit of our RFT pipeline, which enables the model to teach itself behaviour it does not have initially. This happens\nwhen an agent successfully explores during the data collection phase and the advantageous interaction is reinforced by the\nrejection sampling and subsequent training."}]}