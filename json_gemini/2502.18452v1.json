{"title": "FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in Object-Based Common Sense Reasoning for Disaster Response", "authors": ["Mollie Shichman", "Claire Bonial", "Austin Blodgett", "Taylor Hudson", "Francis Ferraro", "Rachel Rudinger"], "abstract": "Large Language Models (LLMs) have the potential for substantial common sense reasoning. However, these capabilities are often emergent in larger models. This means smaller models that can be run locally are less helpful and capable with respect to certain reasoning tasks. To meet our problem space requirements, we fine-tune smaller LLMs to disaster domains, as these domains involve complex and low-frequency physical common sense knowledge. We introduce a pipeline to create Field Ready Instruction Decoding Agent (FRIDA) models, where domain experts and linguists combine their knowledge to make high-quality seed data that is used to generate synthetic data for fine-tuning. We create a set of 130 seed instructions for synthetic generation, a synthetic dataset of 25000 instructions, and 119 evaluation instructions relating to both general and earthquake-specific object affordances. We fine-tune several LLaMa and Mistral instruction-tuned models and find that FRIDA models outperform their base models at a variety of sizes. We then run an ablation study to understand which kinds of synthetic data most affect performance and find that training physical state and object function common sense knowledge alone improves over FRIDA models trained on all data. We conclude that the FRIDA pipeline is capable of instilling general common sense, but needs to be augmented with information retrieval for specific domain knowledge.", "sections": [{"title": "1 Introduction", "content": "Which of the following would be most dangerous if it collapsed? This question is fairly trivial for humans to answer, but requires several types of semantic knowledge. First, one must know what objects are capable of collapsing. One must also know the general size of these items and the item's other functions to assess the danger. A collapse is also a change of state that fundamentally shifts the use of these objects; a collapsed chair is more likely to cut, scrape, or be carried if the chair folds. All of this knowledge is needed to answer this question, and all of it is embedded in our semantic understanding of things that can cause danger and things that can collapse.\nAs LLMs have improved exponentially, their abilities at reasoning about objects have improved as well. LLMs have long proven to encode physical world knowledge (Petroni et al., 2019), and their embeddings can improve physical understanding of an environment and its objects both within and beyond a fine-tuned domain (Cohen et al., 2024). However, much of this improvement only emerges in larger models trained on more data (Wei et al., 2022a). This makes these essential semantic capabilities less accessible, particularly for running LLMs locally on edge devices. We thus wanted to answer: how can we imbue all the physical common sense and semantics needed for smaller models to be more capable at understanding the physical world?\nTo answer this question, we turned to improving small LLMs in the disaster relief domain. The first reason for this was because disasters require a lot of both general and domain-specific semantic knowledge: one needs to know both the standard objects in daily life and the specialized tools required for search and rescue, such as hydraulic pumps and concrete saws. Secondly, the specific knowledge (and to a lesser extent, the general knowledge) needed for reasoning varies by disaster, which requires a high level of adaptability in any potential pipeline. Finally, most publicly available data on disasters is social media-based reactions (Godinho, 2024), which does not pertain much to our subdomain of the objects used during these events. This is due to the writer's assumption of latent semantic knowledge (e.g., everyone knows that counters don't collapse, so no one makes a point to tweet that) (Gordon and Van Durme, 2013). This makes the task more novel for LLMs which likely have not seen explicit reasoning in this domain during pre-training.\nWe present a pipeline to create Field Research and Instruction Decoding Agent (FRIDA) models. For FRIDA, we leveraged both disaster and linguistic expertise to create gold-standard instructions that in turn are used as a basis for synthetic data generation, as seen in Figure 2. This synthetic data is then used to fine-tune smaller models to increase Like its rescue dog eponym,\u00b9 our FRIDA models were initially developed and tested for earthquake disaster relief, based on expert knowledge pertaining to the February 6th, 2023 earthquakes in Turkiye and Syria (Arranz et al., 2023).\nWe found our FRIDA models out-performed their base models overall in both exact match and embedding vector similarity (Aynetdinov and Akbik, 2024), regardless of model size or architecture. Knowing that our pipeline improved performance, we wanted to investigate which synthetic data were most influential in that improvement. Do accomplish this, we ran an ablation study where we fine-tuned a variety of small LLMs on subsets of our synthetic data corresponding to a specific type of object-based common sense. We call these resulting models the ablation FRIDA (aFRIDA) models. We found that aFRIDA models trained on general common sense significantly outperformed models trained on the domain-specific common sense knowledge and the full FRIDA models themselves. We posit that FRIDA succeeds in improving object-related general common sense and is a strong basis for improving domain-specific general common sense as well.\nOur contributions are as follows:\n1.  An expert-in-the-loop pipeline (Figure 2) for generating specific and high-quality synthetic data that can be used for fine-tuning when man-made data are not feasible to obtain, as well as the resulting gold-standard datasets\n2.  A synthetic dataset of 25,000 instructions relating to common sense and earthquake with accompanying analysis\n3.  The FRIDA 1B, 3B, Minstal 8B, and LLaMa 8B models, trained on the above synthetic data.\n4.  A series of ablation FRIDA (aFRIDA) models trained on subsets of the synthetic dataset\nAn anonymous github containing code and a complete example of the FRIDA pipeline is currently available.2"}, {"title": "2 Background", "content": "2.1 Synthetic Data Generation\nSynthetic data, or data generated by an LLM, has become increasingly popular as an inexpensive and relatively proficient method of data collection. While cyclically fine-tuning LLMs on the synthetic data they generate denigrates the models' performance (Alemohammad et al., 2023), fine-tuning on synthetic data has nevertheless improved short term performance in instruction following and social common sense (Eldan and Li, 2023; Wang et al., 2022).\nThis paper is inspired in particular by the pipeline used in Wang et al. (2022). Wang et al., 2022 hand crafted 175 instructions that were used for 8-shot learning to prompt GPT's text-davinci-001 model to generate more than 50,000 instructions for a generic and ungrounded AI assistant. These instructions were then used to fine-tune text-davinci-001. The authors found that their method and resulting fine-tuned model performed comparably to OpenAI's GPTInstruct (Wang et al., 2022). Taori et al., 2023 innovated on"}, {"title": "2.2 Data Creation", "content": "We developed an expert-in-the loop pipeline to generate high-quality seed data that leverages disaster-relief and linguistic expertise. The purpose of this pipeline is to enable quick and efficient fine-tuning of small LLMs capable of addressing critical informational needs in specific disasters. The details of the pipeline are described in a previous work, here we provide a brief overview. We developed a series of templates that can be filled in with vocabulary from an affordance ontology, based on the Rich Event Ontology (Kazeminejad et al., 2018). This affordance ontology is extended to serve as an ontology of disaster-related objects and their functionalities, as defined by the objects' PropBank semantic roles labels (Palmer et al., 2005).\nTo fill in these templates with proper data, a disaster expert first provides information about the relevant objects and situations a they would encounter in their work. For this paper, the authors simulated this step by gathering existing resources authored by experts on the Turkiye-Syria Earthquake recovery efforts (Arranz et al., 2023). After gathering domain-specific data, linguists go through a template-filling pipeline. Summarily, the linguists select the relevant vocabulary from the expert knowledge to add to the aforementioned affordance ontology. They then use this ontology and template-specific generation instructions to fill in these templates to create gold-standard data, our \"seed instructions\". These templates are formatted as multiple choice questions with semantically distinct answers. Some examples of this process, as well as some synthetic instructions that result from 5-shot prompting, can be seen in Table 1.\nIn total, we created 26 templates, which were then grouped by topic or type of common sense. For example, the first example in Table 1 describes a change in state, which is often a change in the shape, size, or weight of an object. It therefore was put in the category Relative Sizes, the category concerned with how the object exists in space. The second example in table 1 describes how a specific"}, {"title": "3 Methods", "content": "3.1 Synthetic Dataset Generation and Analysis\nThe dataset we use in this work is about the Turkiye-Syria Earthquake. Our data is formatted as a user-assistant chat in order to more closely align our data to the general instruction tuning all our base models received. In addition to the seed data, we also developed a separate gold-standard evaluation dataset. The final result is that each template has 5 seed instructions for synthetic data generation (130 total instructions) and at least 4 evaluation instructions (119 examples).\nFor each template, we used 5-shot prompting with Gemini-1.5-flash to generate 980 examples of said template (Team, 2024a). We chose Gemini for its accessible and affordable API, as well as its high scores on our evaluation (0.725 exact match accuracy, 0.94 average Semscore, see section 3.3). We prompted Gemini to return 40 instructions per API call. To ensure our synthetic data were robust, we used ROUGE scoring (Lin, 2004) to ensure Gemini was not giving us duplicates of previously generated instructions. Depending on the template, the cut-off ROUGE score went from 0.8 for templates with more varied language to 0.97 for templates with very structured wording. We also increased model temperature for the more structured templates to increase diversity of responses.\nWe get a sense of the resulting synthetic dataset from the topmost chart in table 2. We automatically evaluated for instruction length and each instruction's maximum ROUGE score with the other instructions in the dataset. We found we had substantial average instruction length, and reasonable ROUGE scores given our data is template-based. However, there was a large range in both metrics across the different template categories. We attribute this to the overall complexity of the individual templates, which vary substantially.\nWe then randomly selected 190 synthetic instructions, with 4-5 examples from each template and had 2 authors examine them. The authors rated these instructions on 2 binary metrics: reasonableness and informativeness. An instruction is marked as \"Reasonable\" if the model sufficiently followed the template's general format and intent. An instruction marked as \u201cInformative\" means the question is non-trivial. For instance, the instruction \"Which would be most likely to cause a serious injury if it fell from a roof? A leaf, a small pebble, a large tile, a piece of paper, or a feather\" follows the injury template \"Which of the following would be most likely to cause an injury in [SCENARIO]?\". However, the four wrong answers are so harmless the instruction becomes too easy to be meaningful. It follows that an instruction can only be informative if it is reasonable. As seen in table 2, 80% of sampled instructions were reasonable, but only 50% were informative. The 2 authors had Cohen's K values of 0.28 for reasonableness and 0.35 for informativeness, meaning the authors had fair agreement for both metrics."}, {"title": "3.2 FRIDA Model construction", "content": "We used our synthetic dataset to fine-tune the 1 Billion, 3 Billion, and 8 Billion parameter Instruct models from the LLaMa-3 herd (Team, 2024b) as well as the Ministral 8B Instruction tuned model (Team, 2024c). We chose to use the LLaMa suite due to its strong performance on its open source small LLMs, as well as it having multiple small instruction tuned models of different sizes (Team, 2024b). We chose Ministral 8B to serve as a comparison, since it is trained with sliding window attention, unlike the LLaMa models (Team, 2024c).\nIt also was released after the LLaMa 3 herd and outperformed it in many metrics (Team, 2024c). We chose to fine-tune the instruct variations of these models because our task is based in answering questions. Fine-tuning specifics can be found in Appendix section C."}, {"title": "3.3 Evaluation", "content": "We implement 2 different evaluations to compare FRIDA output to the gold-standard evaluation. We first evaluate with an Exact Match (EM) comparison between the gold standard and FRIDA's output. This is a strict metric, as formatting mistakes (i.e. not predicting the letter corresponding to the answer, adding punctuation) lead to an incorrect mark. We do provide system instructions to help the FRIDA models respond in the correct format, but all prompts are zero-shot. We feel this is a reasonable metric since the instructions are multiple choice, so the model sees the full answer in proper format before responding.\nTo get a better idea of how close the answers are from a semantic perspective, we also used SemScore (Aynetdinov and Akbik, 2024; Geronimo and Lera, 2024). SemScore is a scoring metric that uses cosine similarity to compare the a tokenizer's embedding vectors of the gold standard and FRIDA responses. Aynetdinov and Akbik (2024) compared human rankings of 252 LLM instruction responses collected by Wang et al. (2022), and found that SemScore most closely resembled human rankings when compared to other automatic semantic evaluations. Additionally, SemScore was tested with a variety of general instruction based models (Aynetdinov and Akbik, 2024), making it the best silver-standard metric for understanding how semantically close the FRIDA models were to the gold standard."}, {"title": "3.4 Ablation Study", "content": "As we developed the templates used to generate seed instructions, we wanted to explore which types of data improved model performance the most. To better understand the effectiveness of our data, we ran an ablation study where we fine-tuned all base models on subsets of the synthetic fine-tuning data, which can be seen in Table 2.\nWe made an ablated model for each category, where each model is fine-tuned only on the synthetic data generated by templates in said category. For example, the Relative Sizes ablation model is trained on data generated from 4 templates testing size, weight, objects fitting in containers, and objects changing state, respectively. We refer to these ablated models as ablated-FRIDA (or aFRIDA) models.\nThe resulting name for a FRIDA model trained only on data from the Relative Size category would thus be, \"aFRIDA 3B: relative sizes\" where the \"3B\" represents the number of parameters in the base model and \"relative sizes\" refers to the subset of data used for fine-tuning (see Appendix Table 4 for data categories). Since we fine-tuned a LLaMa and Mistral model which both had 8 Billion parameters, we the fine-tuned LLaMa model was named aFRIDA 8B, and the fine-tuned Mistral model was named MaFRIDA 8B (Minstral ablated FRIDA, 8B). The ablated models were tuned with the same hyper-parameters and hardware as the FRIDA models trained on the entire synthetic dataset.\nA model suite for a given base model contains FRIDA, trained on the full dataset, as well as 8 aFRIDA models trained on the categorical subsets of the data: relative sizes and state, object function, object differences, specialized equipment, objects causing harm, non-function object facts, earthquake knowledge, and instruction understanding. Examples of data for each category can be found in 4 in the appendix."}, {"title": "4 Results", "content": "As seen in Table 3, all FRIDA models larger than 1 Billion parameters scored better in 0-shot exact match scoring than their base models. Surprisingly, the aFRIDA models for the Relative Size and Object Functions categories outperformed their corresponding FRIDA models and pre-trained baselines. The top scoring ablation models consistently outperformed their full FRIDA models by 4-9 points. Minstral-FRIDA 8B not only outperformed LLaMa-FRIDA 8B; it and MaFRIDA 8B: relative sizes also outperformed Gemini-1.5-flash's Exact Match score of 0.725 in a 0 shot setting.\nWe also assessed each model's capability on each subset of templates within the evaluation dataset, and we show the Semscore (cosine similarity) results for the two best fine-tuned models in Figure 3. Overall, models fine-tuned only on objects' basic physical characteristics and functionality data performed more strongly across all categories. This was despite these categories being the \"easiest\" in our evaluation, receiving the highest exact match scores and SemScores across all models. It is also clear from both heatmaps that the most difficult topic is equipment-related questions.\nAnother observation from Figure 3 is that the base models get more consistent scores across the board, while the ablation models are more varied across categories. This is especially true for MaFRIDA 8B, where fine-tuning caused it to perform worse than the base model when measuring by SemScore. Additionally, the highest subset accuracy scores for the aFRIDA models were not always for the data on which they were fine tuned. An example of this is that the model \u201cMaFRIDA 8B: relative size\" scored lower on the relative size evaluation templates than the model \u201cMaFRIDA 8B: specialized equipment\", as seen in the leftmost heatmap for M-FRIDA 8B in Figure 3."}, {"title": "5 Discussion", "content": "We were surprised by the level of improvement the FRIDA pipeline imbued, especially when our sample for data informativeness was judged to be less than ideal by the authors. We were especially surprised that the aFRIDA relative size and object function models outperformed all other models across the board. It's possible that clarifying the basic properties and affordances of objects provided a better basis for the model to have better reasoning. Another contributing factor is likely that our synthetic data on specific objects and tasks tended to be longer, more diverse, and less informative according to our dataset analysis. The sample size of these more specific synthetic data may have been too small for major improvement in those areas.\nAn additional unexpected observation was that the Instruction Understanding ablation models performed so poorly in the exact match metric, but comparably to the other aFRIDA models with SemScore. We believe this is because the templates for instruction understanding included punctuation in the answer choices, but none of the other templates did. Thus the models fine-tuned only of instruction understanding would add punctuation and miss the Exact Match, reinforcing the need for multiple types of evaluation on this task.\nOverall, the FRIDA pipeline is effective at improving small LLMs on its data. Another positive is that they are very lightweight, with comparable performance to a much larger Gemini model. The data relating to objects function and physicality seem to improve model performances on both the Exact Match and the SemScore metrics. It is clear, however, that FRIDA models could improve with different data distributions more heavily favoring object size and functions (which helped it improve) and specialized equipment (which all models struggled on). The base models clearly have enough access to this disaster to correctly answer the more fact-based templates regarding disaster response, while physical common sense-related questions help to improve real-world knowledge for practical interactions."}, {"title": "6 Related Work", "content": "6.1 LLMs Reasoning about the World\nThere a wide variety of methods for leveraging LLMs for reasoning in a physical environment based on Chain of Thought prompting (Wei et al., 2022b). These include variants like re-prompting (Raman et al., 2022), which prompts the LLM to regenerate a plan if certain criteria aren't met at each steps, or Tree of Thought (Yao et al., 2023), which generates a tree of potential steps and evaluates each potential path via either a breadth-first or depth-first search.\nThere are also methods that take allow the LLM to take in environmental feedback in response to its output. For Inner-Monologue (Huang et al., 2023), the LLM is given the option to ask for more scene descriptors from a human handler, which it then incorporates into its prompts, improving task completion and decreasing hallucination. Another example is SayPlan (Rana et al., 2023), which uses 3D scene plans to iterate on proposed strategies until an effective path is discovered. Xie and Zou (2024) get feedback from LLMs themselves by using a wide variety of LLM agents to do various sub-tasks for planning, including generating a general outline, using external tools to gain information, and evaluating which plan is best.\nOne resource for improving LLM understanding object affordances specifically is Adak et al. (2024), who curate a dataset of naturally occurring sentences and corresponding images, then transform them into inference, probing, and text and visual masking tasks. They further prove that even Visual Language Models (VLMs) do not have straightforward understandings of affordances, but few-shot fine-tuning improved LLM and VLM performance on identifying object affordances."}, {"title": "6.2 Disaster Work and Natural Language Processing", "content": "Godinho (2024) completed a systematic search and analysis of over 100 peer-reviewed papers relating to Natural Language Processing (NLP) tools being applied to disasters. 85 of the 107 papers found were for analyzing social media, with 67 of the papers analyzing twitter data specifically. Over half of the total papers had a sentiment analysis component to their work, and the 2nd and 3rd most common tools used were text classification and information extraction. 87.8% of papers focused on natural disasters, with only 3.7% being solely about man-made disasters (the rest pertained to both) (Godinho, 2024). Godinho (2024) and (Wang et al., 2024) together showcase that while there is much research on LLMs as agents and much research on NLP analysis of disasters, there is not much overlap in these spaces."}, {"title": "7 Future Work", "content": "Even with strong performance, we still feel there are several ways we can further improve the FRIDA pipeline. Firstly, we want to improve our prompting for synthetic data to make them less trivial. We want to refine our common sense related templates and make sub-templates with different phrasing to make our synthetic data more reflective of real world natural language. We also hope implementing the strategies in other work (Ge et al., 2024; Ding et al., 2023; Mukherjee et al., 2023) for diversifying synthetic data will improve quality and make data generation more time-efficient. Finally, we plan to test the pipeline on a variety of specific disasters with disaster experts who can provide feedback on the feasibility of our process."}, {"title": "8 Conclusion", "content": "We introduce a pipeline to create expert-knowledge-based synthetic data that is then used for fine-tuning to create FRIDA models. We found our pipeline substantially improved performance over 3 different instruction-tuned LLaMa models and 1 instruction-tuned Ministral model. We then performed an ablation study and found that data generated from templates based in physical common sense reasoning about objects improved performance most; ablation models trained on those data scored higher than FRIDA models trained on all synthetically generated data. This pipeline is an important step in understanding and improving LLM object reasoning for practical use."}, {"title": "9 Limitations, Risks, and Ethics", "content": "One limitation is that we train and evaluate on template-generated data rather than naturally occur ring language; there could be linguistic or stylistic differences between template-generated data and naturally occurring instructions. Though our approach still relies on access to expert input and non-trivial computational power for fine-tuning to counter these shortcomings, we outline solutions in Section 7 which we believe are ripe avenues for future work.\nWe note that multiple choice questions can be different and less complicated than an unconstrained turn between a user and an AI assistant Nevertheless, we believe this work is an important step towards our goal of imbuing smaller language models with physical common sense. This is because we prove the feasibility and capability of small LLMs to complete this more constrained task. We argue that FRIDA should be seen as a proof-of-concept for LLM physical common sense understanding, which sets the stage for increasingly challenging training data and evaluations.\nFRIDA is built by biasing an LLM to a specific domain. While this is important for our work, this could be misused to bias models in harmful ways, especially when considering applications involving social common sense. When modifying our seed data and templates, we took care to reduce gender bias as much as possible. This was fairly trivial since all questions pertained to objects and events, not people. We acknowledge that many objects from the ontology we used were annotated with a Western perspective, and that other cultures likely have additional uses for these objects."}]}