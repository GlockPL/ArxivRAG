{"title": "BECAUSE:\nBilinear Causal Representation for Generalizable\nOffline Model-based Reinforcement Learning", "authors": ["Haohong Lin", "Wenhao Ding", "Jian Chen", "Laixi Shi", "Jiacheng Zhu", "Bo Li", "Ding Zhao"], "abstract": "Offline model-based reinforcement learning (MBRL) enhances data efficiency by\nutilizing pre-collected datasets to learn models and policies, especially in scenarios\nwhere exploration is costly or infeasible. Nevertheless, its performance often\nsuffers from the objective mismatch between model and policy learning, resulting in\ninferior performance despite accurate model predictions. This paper first identifies\nthe primary source of this mismatch comes from the underlying confounders\npresent in offline data for MBRL. Subsequently, we introduce BilinEar CAUSal\nrEpresentation (BECAUSE), an algorithm to capture causal representation for both\nstates and actions to reduce the influence of the distribution shift, thus mitigating\nthe objective mismatch problem. Comprehensive evaluations on 18 tasks that vary\nin data quality and environment context demonstrate the superior performance of\nBECAUSE over existing offline RL algorithms. We show the generalizability and\nrobustness of BECAUSE under fewer samples or larger numbers of confounders.\nAdditionally, we offer theoretical analysis of BECAUSE to prove its error bound\nand sample efficiency when integrating causal representation into offline MBRL.", "sections": [{"title": "1 Introduction", "content": "Offline Reinforcement Learning (RL) has shown great promise in learning directly from historically\ncollected datasets, especially in scenarios where active interaction is expensive or infeasible [1].\nSpecifically, offline model-based reinforcement learning (MBRL) [2, 3, 4], learning policies with an\nestimated world model, generally perform better than their model-free counterparts in long-horizon\ntasks such as self-driving vehicles [5], robotics [6], and healthcare [7]. However, offline RL suffers\nfrom distribution shift since the rollout data could sample from some unknown behavior policies that\nare sub-optimal or from slightly different environments compared to the deployment time [8].\nAlthough identifying distribution shift issues, many\nof the current offline MBRL works fail to model the\nshift in environment dynamics, which is ubiquitous\nand could cause catastrophic failure of trained policy\nat a slightly different deployment stage. Furthermore,\nsince the learning objectives of the world models and\npolicies are isolated from each other, a significant\nchallenge in offline MBRL is objective mismatch [9,\n10] problem (shown in Figure 1): models that achieve\na lower training loss are not necessarily better for\ncontrol performance. For example, in long-horizon\nplanning tasks, the reward is sparse yet the prediction accuracy of the model may decay as the horizon"}, {"title": "2 Problem Formulation", "content": "To alleviate the objective mismatch problem and the degraded performance caused by the spurious\ncorrelation, we first provide our novel formulation of learning the underlying causal structures of\nMarkov Decision Process (MDP) under the bilinear MDP setting, then introduce the causal discovery\nfor MDP with confounders."}, {"title": "2.1 Preliminary: MDP and Bilnear MDP", "content": "We denote an episodic finite-horizon MDP by $\\mathcal{M} = {\\mathcal{S}, \\mathcal{A}, \\mathcal{T}, H, r}$, which is composed of state\nspace $\\mathcal{S}$, action space $\\mathcal{A}$, a set of transition functions $\\mathcal{T}$, planning horizon $H$ and reward function $r$\nassociated with task preferences. Without loss of generality in many real-world practices, we assume\nthat the reward function is bounded by $r_h \\in [0, 1], \\forall h \\in [H]$. Specifically, we are interested in a\ngoal-conditioned reward setting, where $\\forall g \\in \\mathcal{S}, r(s, a; g) = 1$ if and only if $s = g$.\nGiven a policy $\\pi$ and the state-action pair $(s, a) \\in \\mathcal{S} \\times \\mathcal{A}$, we then define the state-action value\nfunction in the timestep $h$ as $Q^{\\pi}_h(s,a) = \\mathbb{E}_{\\pi} \\left[ \\sum_{i=h}^{H} r_i(S_i, a_i) | S_h = s, a_h = a \\right]$, and the value func-\ntion $V^{\\pi}_h(s) = \\mathbb{E}_{\\pi} \\left[ \\sum_{i=h}^{H} r_i(S_i, a_i) | S_h = s \\right]$. The expectation $\\mathbb{E}_{\\pi}$ here is integrated into randomness\nthroughout the trajectory, which is essentially induced by the random action of the policy $a_i \\sim \\pi(\\cdot | s_i)$\nand the time-homogeneous transition dynamics of the environment $s_{i+1} \\sim T(\\cdot | s_i, a_i), \\forall i \\in [h, H]$.\nIn the offline dataset, the data rollouts can be seen as generated by some (mixed) behavior policy $\\pi_\\beta$,\nresulting in a dataset $\\mathcal{D}$ with in total $n$ samples $\\{s_i, a_i, s'_i, r_i\\}_{1 \\leq i \\leq n}$."}, {"title": "2.2 Action State Confounded MDP", "content": "We consider the existence of confounders in\nthe MDP to represent the offline data collec-\ntion process, and define action-state confounded\nMDP (ASC-MDP):\nDefinition 2 (ASC-MDP). Besides the compo-\nnents in standard MDPs $\\mathcal{M} = {\\mathcal{S}, \\mathcal{A}, \\mathcal{T}, H, r}$,\nwe introduce a set of unobserved confounders\n$\\boldsymbol{u}$. In ASP-MDP, confounders are factorized as\n$\\boldsymbol{u} = {\\boldsymbol{u}_{\\pi}, \\boldsymbol{u}_{c}}_{1 \\leq h \\leq H}$, where $\\boldsymbol{u}_{\\pi} \\in \\mathcal{U}$ denotes\nthe confounders between $s$ and $a \\sim \\pi_\\beta(s)$ in-\nduced by behavior policies, and $\\boldsymbol{u}_{c} \\in \\mathcal{U}$ denotes the confounders within the state-action pairs of the\nenvironment transition, that is, the inherent structure between $(s, a)$ and $s'$. Here we assume a time-\ninvariant confounder distribution $\\boldsymbol{u} \\sim P_{\\boldsymbol{u}}(\\cdot), \\forall h \\in [H]$, which is a common assumption [15, 16, 17]\nThe resulting causal relationship of ASC-MDP is demonstrated in Figure 2. Originating from\nthe original MDP, ASC-MDP is different from the Confounded MDP [18] and State-Confounded\nMDP (SC-MDP) [19] in that it models both the spurious correlation between the current state $s$ and\nthe current action $a$, as well as those between the next state $s'$ and $(s, a)$. Yet, confounded MDP and\nSC-MDP only model part of the possible confounders between states and actions. The factorization\nof the confounder in ASC-MDP aligns with the source of spurious correlation in offline MBRL."}, {"title": "3 Proposed Method: BECAUSE", "content": "We propose BECAUSE, our core methodology for modeling, learning, and applying our causal\nrepresentations for generalizable offline MBRL. Section 3.1 models the basic format of causal\nrepresentations and analyzes their properties. Section 3.2 gives a compact way to learn the causal\nrepresentation $\\phi(s, a)$ and $\\mu(s')$, as well as the core mask estimation $M$. Section 3.3 utilizes these\nlearned causal representations in both world model learning and MBRL planning from offline datasets."}, {"title": "3.1 Causal Representation for ASC-MDP", "content": "In the presence of a hidden confounder $\\boldsymbol{u}$, we model the confounder behind the transition dynamics\nas a linear confounded MDP [18]:\n$T(s'|s, a, \\boldsymbol{u}) = \\phi(s, a, \\boldsymbol{u})^\\top \\mu(s'),$ (2)\nwhere $\\boldsymbol{u} \\sim P_{\\boldsymbol{u}}(\\cdot)$. Inspired by the Bilinear MDP in Definition 1, we decompose $\\phi(s, a, \\boldsymbol{u})$ into a\nconfounder-aware core matrix $M(\\boldsymbol{u})$ and a feature mapping $\\phi(s, a)$, which factorize the influence of\nthe confounders. Given the factorization of confounder $\\boldsymbol{u} = {\\boldsymbol{u}_c, \\boldsymbol{u}_\\pi}$ in Definition 2, we derive via\n$\\mathit{d}$-separation in the graphical model in Figure 2 that $s' \\perp \\hspace{-1.2ex} \\perp \\boldsymbol{u}_\\pi | \\{s, a, \\boldsymbol{u}_c\\}$. As a result, we only need to\nconsider the confounder $\\boldsymbol{u}_c$ from the environment when decomposing the transition model:\n$T(s'|s, a, \\boldsymbol{u}) = T(s'|s, a, \\boldsymbol{u}_c) = \\phi(s, a)^\\top M(\\boldsymbol{u}_c) \\mu(s').$\n(3)\nDefinition 3 (Construction of causal graph $G$). In ASC-MDP, $G = \\begin{bmatrix}\n O_{d \\times d} & O_{d \\times d'}\\\\\n M & O_{d' \\times d'}\n\\end{bmatrix}$ for all (sparse)\ncore matrix $M$, the causal graph $G$ is bipartite, thus $\\forall G, G \\in \\mathcal{DAG}$."}, {"title": "3.2 Learning Causal Representation from Offline Data", "content": "We first learn the causal world model $T(s'|s, a)$\nin the presence of confounders $\\boldsymbol{u}$ in the offline\ndatasets. As formulated in ASC-MDP 2, there\nare two sets of confounders: $\\boldsymbol{u}_\\pi$ and $\\boldsymbol{u}_c$. To\nestimate an unconfounded transition model and\nremove the effect of confounder, we first remove\nthe impact of $\\boldsymbol{u}_c$ which comes from the dynam-\nics shift by estimating a batch-wise transition\nmatrix $M(\\boldsymbol{u}_c)$, then we apply a reweighting for-\nmula to deconfound $\\boldsymbol{u}_\\pi$ induced by the behavior\npolicies and mitigate the model objective mis-\nmatch.\nAs discussed in Definition 3, we only need to\noptimize the part of the parameters of the causal\ngraph $G$, i.e. $M$. Thus, we can remove the constraints in (4), then transform the original causal\ndiscovery problem into a regularized MLE problem as follows:\n$\\begin{aligned}\n&\\min_M \\mathcal{L}_{\\text{mask}} (M) = \\min_M \\left( - \\log p(\\mathcal{D}; \\phi, \\mu, M) + \\lambda |M| \\right)\\\\\n&\\qquad = \\min_M \\left(\\mathbb{E}_{(s, a, s') \\in \\mathcal{D}} ||\\mu^\\top (s')K_\\mu^{-1} - \\phi^\\top (s, a) M||_2^2 + \\frac{\\lambda ||M||_0}{Sparsity ~ Regularization} \\right).\\tag{5}\n\\end{aligned}$\nwhere $K_\\mu := \\sum_{s' \\in \\mathcal{S}} \\mu(s')\\mu(s')^\\top$ is an invertible matrix. The derivation of equation (5) is elaborated\nin Appendix A.3. In practice, we use the $\\chi^2$-test for discrete state and action space and the fast\nConditional Independent Test (CIT) [27] for continuous variables to estimate each entry in the core\nmatrix $M$. We regularize the sparsity of $M$ by controlling the $p$-value threshold in CIT and provide a\nmore detailed implementation in Appendix C.1.\nEstimating the core mask provides a more accurate relationship between state and action represen-\ntations, and we further refine the state action representation function $\\phi$ and $\\mu$ to help capture more\naccurate transition dynamics. We optimize them by solving the following problem, according to the\ntransition model loss and spectral norm regularization [28] to satisfy the regularity constraints of the\nfeature in Assumption 3:\n$\\min_{\\phi, \\mu} \\mathcal{L}_{\\text{rep}} (\\phi, \\mu) \\triangleq \\min_{\\phi, \\mu} \\mathbb{E}_{(s, a, s') \\in \\mathcal{D}} \\left[ ||\\mu^\\top (s')K_\\mu^{-1} - \\phi^\\top (s, a) M||_2^2 + \\lambda_\\phi ||\\phi||_*^2 + \\lambda_\\mu ||\\mu||_*^2 \\right].\\tag{6}$\nThe world model learning process is illustrated in Figure 3. The estimation of individual $M(\\boldsymbol{u}_c)$\nmitigates the spurious correlation brought by $\\boldsymbol{u}_c$. To further deal with the spurious correlation in"}, {"title": "3.3 Causal Representation for Uncertainty Quantification", "content": "To avoid entering OOD states in the\nonline deployment, we further design\na pessimistic planner according to the\nuncertainty of the predicted trajecto-\nries in the imagination rollout step to\nmitigate objective mismatch.\nWe use the feature embedding from\nbilinear causal representation to help\nquantify the uncertainty, denoted as\n$\\mathbb{E}_\\theta(s, a)$. As we have access to the\noffline dataset, we learn an Energy-\nbased Model (EBM) [30, 31] based\non the abstracted state representation\n$\\phi$ and core matrix $M$. A higher out-\nput of the energy function $\\mathbb{E}_\\theta(\\cdot, \\cdot)$ in\ndicates a higher uncertainty in the cur-\nrent state as they are visited by the be-\nhavior policies $\\pi_\\beta$ less frequently. In\npractice, the energy-based model usu-\nally suffers from a high-dimensional\ndata space [32]. To mitigate this over-\nhead of training a good uncertainty\nquantifier, we first embed the state\nsamples through the abstract representation $\\mu(s')$, and the state action pair via $\\phi(s, a)$.\n$\\begin{aligned}\n\\mathcal{L}_{\\text{EBM}}(\\theta) = \\mathbb{E}_{f(\\boldsymbol{s}, \\boldsymbol{a})} \\mathbb{E}_\\theta [\\mu(\\boldsymbol{s}^+) | \\phi(\\boldsymbol{s}, \\boldsymbol{a})] - \\mathbb{E}_{q(\\boldsymbol{s}, \\boldsymbol{a})} \\mathbb{E}_\\theta [\\mu(\\boldsymbol{s}^-) | \\phi(\\boldsymbol{s}, \\boldsymbol{a})] + \\lambda_{EBM} ||\\theta||^2,\n\\end{aligned}$ (8)\nwhere $\\mu(\\boldsymbol{s})^+$ refers to the positive samples from the approximated transition dynamics $T(\\cdot | s, a)$,\nand $\\mu(\\boldsymbol{s})^-$ refers to the latent negative samples via the Langevin dynamics [30]. Additionally, we\nregularize the parameters of EBM to avoid overfitting issues. We attach more training details and\nresults of EBMs in Appendix C.2 The learned energy function $\\mathbb{E}_\\theta(s, a)$ is used to quantify the\nuncertainty based on the offline data.\nDuring the online planning stage, we use the learned EBM to adjust the reward estimation based on\nModel Predictive Control (MPC) [33]. At timestep $h$, we basically subtract the original step return\nestimation $r_h(s, a)$ by its uncertainty $\\mathbb{E}_\\theta(s, a)$:\n$Q_h(s, a) = Q_h(s, a) - \\mathbb{E}_\\theta(s, a) = r_h(s, a) - \\mathbb{E}_\\theta(s, a) + \\sum_{s' \\in \\mathcal{S}} \\widehat{T}(s' | s, a)V_{h+1}(s').$\n(9)"}, {"title": "3.4 Theoretical Analysis of BECAUSE", "content": "Then we move on to develop the theoretical analysis for the proposed method BECAUSE. Based\non two standard Assumption 2 and 3 on the feature's existence and regularity, we achieve the\nfinite-sample complexity guarantee  an upper bound of the suboptimality gap as follows, whose\nproof is postponed to Appendix B.\nTheorem 1 (Performance guarantee). Consider any $0 < \\delta < 1$ and any initial state $\\bar{s} \\in \\mathcal{S}$. Under\nthe Assumption 2, 3 and that the transition model $\\mathcal{T}$ is an SCM (defined in 4), for any accuracy level\n$0 \\leq \\xi \\leq 1$, with probability at least $1 - \\delta$, the output policy $\\pi$ of BECAUSE (Algorithm 1) based on\nthe historical dataset $\\mathcal{D}$ with $n = \\sum_{(s, a) \\in \\mathcal{S} \\times \\mathcal{A}} n(s, a)$ samples generated from a behavior policy $\\pi_\\beta$\nsatisfies:\n$\\begin{aligned}\n V^*_1(\\bar{s}) - V^{\\pi}_1(\\bar{s}) \\leq &~\\min \\left\\{ C_1 \\log \\left(\\frac{||M||_0}{\\xi}\\right) \\sqrt{|S|}, C_s \\sigma  \\sqrt{\\frac{||M||_0}{n}}\\right\\} \\sum_{h=1}^H \\mathbb{E}_{\\pi^{\\star}}  \\left[ \\sqrt{\\frac{\\log(1/\\delta)}{n(s_h, a_h)} } \\bigg| s_1 = \\bar{s} \\right\\},\n\\end{aligned}$\nwhere $C_1$, $C_s$ are some universal constants, $\\sigma$ is SCM's noise level (see Definition 4), and $M \\in \\mathbb{R}^{d \\times d'}$\nis the optimal ground truth sparse transition matrix to be estimated.\nThe error bound shrinks as the offline sample size $n$ over all state-action pairs increase. It also grows\nproportionally to the planning horizon $H$, SCM's noise level $\\sigma$, and the $l_0$ norm of the ground true\ncausal mask $M$, which describes the intrinsic complexity of the world model.\nConsequently, with Proposition 1 in the Appendix, we can achieve $\\xi$-optimal policy $(V^*_1(\\bar{s})-V^{\\pi}_1(\\bar{s}) \\leq\n\\xi)$ as long as the historical dataset satisfies the following conditions: $\\forall 0 \\leq \\xi \\leq 1$,\n$\\begin{aligned}\n&\\min_{(s, a, h) \\in \\mathcal{S} \\times \\mathcal{A} \\times [H]} \\mathbb{E}_{\\pi^\\star} [n(s_h, a_h) | s_1 = \\bar{s}] \\geq  \\frac{\\min \\left\\{C_1 \\log^2 \\left(\\frac{||M||_0}{\\xi}\\right) |S|, C_s^2 \\sigma^2 ||M||_0 \\right\\} \\cdot H^2 \\log(1/\\delta)}{\\xi^2}.\n\\end{aligned}$"}, {"title": "4 Experiment Results", "content": "In this section, we conduct a comprehensive empirical evaluation of BECAUSE's generalization\nperformance in a diverse set of environments, covering different decision-making problems in the\ngrid world, manipulation, and autonomous driving domains, shown in Appendix Figure 8."}, {"title": "4.1 Experiment Setting", "content": "Environment Design We design 18 tasks in 3 representative RL environments. Agents need to\nacquire reasoning capabilities to receive higher rewards and achieve goals.\n\u2022 Lift: Object manipulation environment in RoboSuite [34]. We designed this environment for the\nagent to lift an object with a specific color configuration on the table to a desired height. In the\nOOD environment Lift-O, there is an injected spurious correlation between the color of the cube\nand the position of the cube in the training phase. During the testing phase, the correlation between\ncolor and position is different from training.\n\u2022 Unlock: We designed this environment for the agent to collect a key to open doors in Minigrid [35].\nIn the OOD environment Unlock-O, there will be a different number of goals (doors to be opened)\nin the testing environments from the training environments.\n\u2022 Crash: Safety is critical in autonomous driving, which is reflected by the collision avoidance\ncapability. We consider a risky scenario where an AV collides with a jaywalker because its view is\nblocked by another car [36]. We design such a crash scenario based on highway-env [37], where\nthe goal is to create crashes between a pedestrian and AVs. In the OOD environment Crash-O, the\ndistribution of reward (number of pedestrians) is different in online testing environments.\nFor all three different environments, we set a specific subset of the state space as the goal $g \\in \\mathcal{S}$, and\nthe reward is defined as the goal-reaching reward $r(s, a, g) = \\mathbb{I}(r = g)$. When the episode ends in\nthe goal state within the task horizon $H$, the episode is considered a success. We then use the average\nsuccess rate as the general evaluation metrics for our BECAUSE and all baselines.\nIn each environment, we collect three types of offline data: random, medium, and expert based on\nthe different levels of $\\boldsymbol{u}_\\pi$ in the behavior policies. In Unlock environments, we collect 200 episodes"}, {"title": "4.2 Experiment Results Analysis", "content": "We empirically answer the following research questions.\n\u2022 RQ1: How is the generalizability of BECAUSE in the online environments (which may be unseen)?\nSpecifically, how does BECAUSE perform under diverse qualities of demonstration data (different\nlevel of $\\boldsymbol{u}_\\pi$), and different environment contexts (different $\\boldsymbol{u}_c$)?\n\u2022 RQ2: How does the design in BECAUSE contribute to the robustness of its final performance\nunder different sample sizes or spurious levels?\n\u2022 RQ3: How does BECAUSE achieve the aforementioned generalizability by mitigating the objective\nmismatch problem in offline MBRL?"}, {"title": "4.3 Ablation Studies", "content": "We conducted ablation studies with three variants of BECAUSE and report the average success\nrate across nine in-distribution and nine out-of-distribution tasks in Table 6. The Optimism variant\nconducts optimistic planning instead of pessimistic planning in equation (9), which uses uniform\nsampling in the planner module. The Linear variant assumes a full connection to the causal matrix\n$M$, then directly uses linear MDP to parameterize the dynamics model $\\mathcal{T}$, which removes the causal\ndiscovery module in BECAUSE. The Full variant learns from the full batch of data to estimate the"}, {"title": "5 Related Works", "content": "Objective Mismatch in MBRL The objective mismatch in MBRL [43, 44] refers to the fact that\npure MLE estimation of the world model does not align well with the control objective. Previous\nworks [29, 45] propose reweighting during model training to alleviate this mismatch, [46] proposes\na goal-aware prediction by redistributing model error according to their task relevance. These works\nessentially reweight loss for the entire model training, while our work conducts reweighting just\nover the estimated causal mask more efficiently. More recently, [9, 10] proposed a joint training\nbetween the world model and policies. Although joint optimization improves performance, they do\nnot address the generalizability of the learned model under the distribution shift setting. In the offline\nsetting, Model-based RL [2, 3, 4] employs model ensemble, pessimistic policy optimization or value\niteration [47, 48], and an energy-based model for planning [49] to quantify uncertainty and improve\ntest performance. To the best of our knowledge, no previous work explored or modeled the impact of\ndistribution shift on the objective mismatch problem in MBRL.\nCausal Discovery with Confounder Most of the existing causal discovery methods [50] can\nbe categorized into constraint-based and score-based. Constraint-based methods [51] start from\na complete graph and iteratively remove edges with statistical hypothesis testing [52, 53]. This\ntype of method is highly data-efficient but not robust to noisy data. As a remedy, score-based\nmethods [54, 55] use metrics such as the likelihood or BIC [56] as scores to manipulate edges in\nthe causal graph. Recently, researchers have extended score-based methods with RL [57], order\nlearning [58] or differentiable discovery [22, 59, 60]. To alleviate the non-identifiability under\nhidden confounders, active intervention methods have been explored [61], aiming to break spurious\ncorrelations in an online fashion. With extra assumptions on confounders, some recent works detect\nsuch correlations [62, 63, 64] so that models can effectively identify elusive confounders.\nCausal Reinforcement Learning Recently, many RL algorithms have incorporated causality\nto improve reasoning capability [65] and generalizability. For instance, [66] and [67] explicitly\nestimate causal structures with the interventional data obtained from the environment in an online\nsetting. These structures can be used to constrain the output space [19] or to adjust the buffer\npriority [68]. Building dynamic models in model-based RL [24, 69, 70] based on causal graphs is\nwidely studied. Most existing causal MBRL works focus on estimating the causal world model by\npredicting transition dynamics and rewards. Existing methods learn this causal world model via\nsparsity regularization [23, 71], conditional independence test [24, 69, 72], variational inference [73,\n12], counterfactual data augmentation [74, 75], hierarchical skill abstraction [76, 77], uncertainty\nquantification [40], reward redistribution [24, 78], causal context modeling [79, 80] and structure-\naware state abstraction [12, 13, 81, 82] based on the controllability and task or reward relevance.\nHowever, the presence of confounders during data collection can skew the learned policy, making it\nsusceptible to spurious correlations. Deconfounding solutions have been proposed either between\nactions and states [39, 83, 84] or among different dimensions of state variables [19, 85]."}, {"title": "6 Conclusion", "content": "In this paper, we study how to mitigate the objective mismatch problem in MBRL, especially under\nthe offline settings where distribution shift occurs. We first propose ASC-MDP and the bilinear\ncausal representation associated with it. Based on the formulation, we proposed how to learn this\ncausal abstraction by alternating between causal mask learning and feature learning in fitting the\nworld dynamics. In the planning stage, we applied the learned causal representation to an uncertainty\nquantification module based on EBM, which improves the robustness under uncertainty in the\nonline planning stage. We theoretically justify BECAUSE's sub-optimality bound induced by\nthe sparse matrix estimation problem and offline RL. Comprehensive experiments on 18 different\ntasks show that given a diverse level of demonstration as the offline dataset, BECAUSE has better\ngeneralizability than baselines in different online environments, and it robustly outperforms baselines\nunder different spurious levels or sample sizes. We empirically show that BECAUSE mitigates the\nobjective mismatch with causal awareness learned from offline data. One limitation of BECAUSE\nlies in its simplified assumption of time-homogeneous causal structure, which may not always hold in\nlong-horizon or non-stationary settings. Besides, the current implementation is still based on vector\nobservations. It will be interesting to scale up the causal reasoning framework into high-dimensional\nobservations to discover concept factors in long-horizon visual RL settings."}, {"title": "A Auxiliary Details of BECAUSE Framework", "content": "We illustrate all the notations used in the main paper and appendix in Table 1."}, {"title": "A.2 Derivation of Definition 3", "content": "The node of this causal graph $G = \\begin{bmatrix}\n O_{d \\times d} & O_{d \\times d'}\\\\\n M & O_{d' \\times d'}\n\\end{bmatrix}$ contains two groups of entities: (1) The state\naction abstraction $\\phi(s, a)$, and (ii) the next state abstraction $\\mu(s')$. We denote $\\phi(\\cdot, \\cdot)^{(i)}$ as the $i$th\nfactor in the abstracted state action representations, and $\\mu(\\cdot, \\cdot)^{(j)}$ for the $j$th factor in the abstracted\nstate representations."}, {"title": "A.3 Derivation of equation (5)", "content": "Definition 4 (Structured Causal Model). An SCM $\\theta := (\\mathcal{S}, \\mathcal{E})$ consists of a collection $\\mathcal{S}$ of $d$\nfunctions [11],\n$S_j := f_j(PA(s_j), e_j), j \\in [d]$, (11)\nwhere $PA(s_j) \\subseteq \\{s_1, ..., s_d\\} \\setminus \\{s_j\\}$ are called parents of $x_j$ in the Directed Acyclic Graph (DAG) $G$,\nand $\\mathcal{E} = \\{e_j\\}_{j=1}^d$ are jointly independent. For instance, in continuous state and action space, we\nparameterize the world model with joint Gaussian Distribution, i.e. $\\epsilon \\sim \\mathcal{N}(0, \\sigma \\mathbb{I}_{dd'})$.\nWe then use bilinear MDP to approximate the original likelihood function in equation (4), i.e.\n$p(\\mathcal{D}; \\phi, \\mu, M) \\propto \\prod_{(s, a, s') \\in \\mathcal{D}} \\exp\\left(-||\\mu^\\top (s')K_\\mu^{-1} - \\phi^\\top (s, a) M||_2^2 \\right),$ (12)\nwhere $K_\\mu := \\sum_{s' \\in \\mathcal{S}} \\mu(s')\\mu(s')^\\top$ is an invertible matrix. Then we can apply an MLE in equation (5).\nIn our BECAUSE algorithm, the optimization of the causal world model is conducted by solving\nthe regularized MLE problem in equation (13). The biggest difference between BECAUSE and the\noffline version of [14, 15] is that it aims to apply $l_0$ regression instead of ridge regression to estimate\nmatrix $M$:\n$\\begin{aligned}\n&M_n = \\arg \\max_M \\left[\\log p(\\mathcal{D}; \\phi, \\mu, M) - \\lambda |M|\\right] \\\\\n&\\quad = \\arg \\min_M \\left[\\sum_{(s, a, s') \\in \\mathcal{D}} ||\\mu^\\top (s')K_\\mu^{-1} - \\phi^\\top (s, a) M||_2^2 + \\frac{\\lambda ||M||_0}{Sparsity ~ Regularization} \\right].\\tag{13}\n\\end{aligned}$"}, {"title": "A.4 Proof of equation (7)", "content": "The derivation depends on the following re-weighting formula in [18]:\n$T(s' | s, a) = \\frac{\\mathbb{E}_{P_{\\boldsymbol{u}}}\\left[T(s' | s, a, \\boldsymbol{u}) \\pi_\\beta(a | s, \\boldsymbol{u}_\\pi)\\right]}{\\mathbb{E}_{P_{\\boldsymbol{u}}}\\left[\\pi_\\beta(a | s, \\boldsymbol{u}_\\pi)\\right]}.$ (14)\nThen we apply equation (14) to the decomposition in equation (2) and equation (3), which yields\n$\\begin{aligned}\n T(s' | s, a) &= \\frac{\\mathbb{E}_{P_{\\boldsymbol{u}}}\\left[T(s' | s, a, \\boldsymbol{u}) \\pi_\\beta(a | s, \\boldsymbol{u}_\\pi)\\right]}{\\mathbb{E}_{P_{\\boldsymbol{u}}}\\left[\\pi_\\beta(a | s, \\boldsymbol{u}_\\pi)\\right]} \\\\\n &= \\frac{\\mathbb{E}_{P_{\\boldsymbol{u}}}\\left[\\phi(s, a)^\\top M(\\boldsymbol{u}_c) \\mu(s') \\pi_\\beta(a | s, \\boldsymbol{u}_\\pi)\\right]}{\\mathbb{E}_{P_{\\boldsymbol{u}}}\\left[\\pi_\\beta(a | s, \\boldsymbol{u}_\\pi)\\right]} \\\\\n &= \\phi(s, a)^\\top \\frac{\\mathbb{E}_{P_{\\boldsymbol{u}}}\\left[ M(\\boldsymbol{u}_c) \\pi_\\beta(a | s, \\boldsymbol{u}_\\pi)\\right]}{\\mathbb{E}_{P_{\\boldsymbol{u}}}\\left[\\pi_\\beta(a | s, \\boldsymbol{u}_\\pi)\\right]} \\mu(s') \\qquad (15) \\\\\n &= \\phi(s, a)^\\top M(\\boldsymbol{u}) \\mu(s'),\n\\end{aligned}$\nwhere the last equality holds by letting $M(\\boldsymbol{u}) := \\frac{\\mathbb{E}_{P_{\\boldsymbol{u}}}\\left[ M(\\boldsymbol{u}_c) \\pi_\\beta(a | s, \\boldsymbol{u}_\\pi)\\right]}{\\mathbb{E}_{P_{\\boldsymbol{u}}}\\left[\\pi_\\beta(a | s, \\boldsymbol{u}_\\pi)\\right]}$."}, {"title": "B Proof of Theorem 1", "content": "In this section, we provide the proof of the sub-optimality upper bound in Theorem 1. We first show\nsome useful definitions and lemmas in Section B.1.  Armed with them, we provide the theoretical\nresults tailored for the causal discovery setting in Section B.2. Furthermore, we give a detailed proof\nof the uncertainty set form in our causal discovery problems in Section B.3."}, {"title": "B.1 Preliminary", "content": "In this subsection, we first define the $\\delta$-uncertainty quantifier $\\Gamma$, then we refer to the lemmas in the\nprevious literature to construct a suboptimality bound based on the defined uncertainty quantifier $\\Gamma$.\nFirst, we define the Bellman operator $\\mathcal{B}_h$, for some value function $V: \\mathcal{S} \\rightarrow \\mathbb{R}$, the Bellman operator\ncan be defined as:\n$(\\mathcal{B}_h V)(s, a) = \\mathbb{E}[r_h(s_h, a_h) + V(s_{h+1}) | s_h = s, a_h = a].$ (16)\nSimilarly, we denote the approximate Bellman operator of the empirical MDP constructed from the\noffline dataset $\\mathcal{D}$ as $\\mathcal{B}_h$ for any $h \\in [H]$.\nDefinition 5 ($\\delta$-Uncertainty Quantifier). We let {$\\Gamma_h$}$_{h=1}^H$, $\\Gamma_h: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ to be a $\\delta$-uncertainty\nquantifier with respect to data distribution $\\mathbb{P}_D$ if the event:\n$\\mathcal{E} = \\left\\{ |(\\mathcal{B}_h V_{h+1})(s, a) - (\\widehat{\\mathcal{B}}_h V_{h+1})(s, a) | \\leq \\Gamma_h(s, a), \\forall (s, a, h) \\in \\mathcal{S} \\times \\mathcal{A} \\times [H] \\right\\}$\nsatisfies $\\mathbb{P}_D(\\mathcal{E}) \\geq 1 - \\delta$.\nAs we consider the offline model learning and planning, we define the model evaluation error at each\nstep $h \\in [H]$ as\n$\\forall (s, a) \\in \\mathcal{S} \\times \\mathcal{A}:  \\tau_h(s, a) = (\\mathcal{B}_h V_{h+1})(s, a) - Q_h(s, a),$ (17)\nwhere $\\tau_h$ is the error induced by the approximate Bellman operator, especially the transition ker-\nnel based on $\\mathcal{D}$. We then identify the source of sub-optimality in our offline MBRL setting by\ndecomposing the sub-optimality error in Lemma 1.\nLemma 1 (Decomposition of Suboptimality [86]).\n$\\begin{aligned}\n \\forall s \\in \\mathcal{S}: V^*_1(s) - V^{\\pi}(s) =  \\sum_{h'=h}^H \\mathbb{E}_{\\pi} [\\tau_{h'}(S_{h'}, a_{h'}) | s_h = s] +  \\sum_{h'=h}^H \\mathbb{E}_{\\pi^{\\star}} [t_{h'}(S_{h'}, a_{h'}) | s_h = s] \\\\\n  +\\sum_{h'=h}^H  \\mathbb{E}_{\\pi^{\\star}} [(Q_{h'}(S_{h'}, \\cdot), \\pi^{\\star}(\\cdot, S_{h'}) - \\pi(\\cdot, S_{h'}))_\\mathcal{A} | s_h = s],\n \\end{aligned}$ (18)\nwhere $\\pi$ is any learned policy, $\\pi^{\\star}$ is the optimal policy that maximizes the cumulative return as below:\n$\\pi^{\\star} = \\arg \\max_{\\pi} \\mathbb{E}_{\\pi} \\left[ \\sum_{h'=1}^H r(s_{h'}, a_{h'}; g) \\right].$\nBased on this decomposition, we will get the basic form of sub-optimality error bound for general\noffline RL settings in Lemma 2:\nLemma 2 (Suboptimality in standard MDP [86]). Suppose we have {$\\Gamma_h$}$_{h=1}^H$ as $\\delta$-uncertainty\nquantifier. Under $\\mathcal{E}$ defined in equation (5), the suboptimality error bound by conservative planning\nsatisfies:\n$\\forall s \\in \\mathcal{S}: V^*_1(s) - V^{\\pi}(s) \\leq 2 \\sum_{h'=h}^H \\mathbb{E}_{\\pi^{\\star}} [\\Gamma_{h'}(S_{h'}, a_{h'}) | s_1 = s].$\nThe basic form of sub-optimality bound in Lemma 2 involves an uncertainty quantifier $\\Gamma_h$, which in\nour case will be further replaced by an exact bound in our sparse matrix estimation problem of causal\ndiscovery algorithms."}, {"title": "B.2 Proof of Theorem 1", "content": "The main results hold under the following two assumptions:\nAssumption 2 (Existence of a core matrix given the feature embedding). For each $(s, a) \\in \\mathcal{S} \\times \\mathcal{A}$,\nfeature vectors $\\phi(s, a) \\in \\mathbb{R}^{d}$, $\\mu(s) \\in \\mathbb{R}^{d'}$ are approximated as a priori. Given a specific confounder\nset $\\boldsymbol{u}$, there exists an unknown matrix $M(\\boldsymbol{u})^{\\star} \\in \\mathbb{R}^{d' \\times d}$ such that,\n$\\mathcal{T}(s' | s, a, \\boldsymbol{u}) = \\phi(s, a)^\\top M(\\boldsymbol{u}) \\mu(s').$ (19)\nAssumption 3 (Feature regularity). We assume feature regularity [14, 15] for the following compo-\nnents of the confounded bilinear MDP:\n\u2022 $\\forall \\boldsymbol{u}$, $||M(\\boldsymbol{u})|| \\leq C_{\\text{md}},$\n\u2022 $\\forall (s, a) \\in \\mathcal{S} \\times \\mathcal{A}$, $||\\phi(s, a)||_3 \\leq C_{\\phi \\text{d}},$\n\u2022 $\\forall s' \\in \\mathbb{R}^{|\\mathcal{S}|}$, $||\\mu^\\top s'||_2 \\leq C_\\mu ||s'||_\\infty, ||\\mu K^{-1}_\\mu||_{2, \\infty} \\leq C_\\mu,$\n\u2022 $\\forall s, a, s' \\in \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S}$, $|| \\phi(s, a) \\mu(s')^\\top ||_1 \\leq C_\\mu$.\nwhere $C_M, C_\\phi, C_\\mu, C_\\phi$ are some universal constants.\nHere, for any matrix $X$, $||X||_{2, \\infty} := \\max_i \\sqrt{\\sum_j X_{ij}^2}$ represents the operator $2 \\rightarrow \\infty$ norm.\nProof pipeline. Armed with the above assumptions, we turn to the bilinear MDP setting, which this\nwork focuses on. We shall develop the finite-sample analysis by specifying the main error term  $\\Gamma$\n($\\delta$-uncertainty quantifier (see Lemma 2)) for our time-homogeneous core matrix estimation problem\nin the following lemma.\nLemma 3 (Uncertainty bound for Bilinear Causal Representation). Under the Assumption 2, 3 and\nthat $\\mathcal{T}$ is an SCM (defined in 4), for the BECAUSE algorithm, for the $\\xi$-optimal policy $(V^*_1(\\bar{s}) -$\n$V^{\\pi}_1(\\bar{s}) \\leq \\xi)$, $\\forall 0 \\leq \\xi \\leq 1$, we have the $\\delta$-uncertainty set as:\n$\\begin{aligned}\n&\\mathcal{E}_{\\text{BECAUSE}} = \\left\\{ |(\\mathcal{B}_h V_{h+1})(s, a) - (\\widehat{\\mathcal{B}}_h V_{h+1})(s, a)|\\\\n&\\qquad \\leq \\min \\left\\{C_1 \\log \\left(\\frac{||M||_0}{\\xi}\\right) \\sqrt{|S|}, C_s \\sigma  \\sqrt{\\frac{||M||_0}{n}} \\right\\} \\frac{\\sqrt{\\log(1/\\delta)}}{n(s, a)}, \\forall (s, a, h) \\in \\mathcal{A} \\times \\mathcal{S} \\times [H] \\right\\},\n\\end{aligned}$\nwhere $C_1$ is some universal constants.\nArmed with the above lemma, we complete the proof of Theorem 1 by showing that\n$\\begin{aligned}\nV^*_1(\\bar{s}) - V^{\\pi}_1(\\bar{s}) &\\leq 2 \\sum_{h=1}^H \\mathbb{E}_{\\pi^{\\star}} [\\Gamma_{h'}(S_{h'}, a_{h'}) | s_1 = \\bar{s}] \\\\\n& \\leq 2 \\sum_{h=1}^H \\mathbb{E}_{\\pi^{\\star}} \\min \\left\\{C_1 \\log \\left(\\frac{||M||_0}{\\xi}\\right) \\sqrt{|S|}, C_s \\sigma  \\sqrt{\\frac{||M||_0}{n}} \\right\\} \\frac{\\sqrt{\\log(1/\\delta)}}{n(s_h, a_h)} \\bigg| s_1 = \\bar{s}.\n\\end{aligned}$ (20)\nThis concludes the proof of Theorem 1."}, {"title": "B.3 Proof of Lemma 3", "content": "The key to proving Theorem 1 is to prove Lemma 3. The proof pipeline of Lemma 3 is illustrated\nbelow. In Step 1, we derive the estimation of the causal transition matrix $M$ in BECAUSE as a\nsparsity regression problem. In Step 2, we decompose the error terms within $\\delta$-uncertainty set into\ntwo parts: (a) error due to the under-explored dataset, (b) error due to optimization error in the\nstructured causal model. Then we bound both error terms in Step 3 and Step 4, respectively. Finally,\nin Step 5, we sum up all the results and derive the form of $\\delta$-uncertainty quantifier which will lead to\nour final results in Theorem 1."}, {"title": "Step 1: deriving the output model of BECAUSE.", "content": "Recalling the original optimization problem in\nequation (13) to estimate the core matrix:\n$\\begin{aligned}\n&M = \\arg \\max_M \\left[\\log p(\\phi, \\mu, M) - \\lambda |M|\\right] \\\\\n&\\qquad = \\arg \\min_M \\left[\\sum_{(s, a, s') \\in \\mathcal{D}} ||\\mu^\\top (s')K_\\mu^{-1} - \\phi^\\top (s, a) M||_2^2 + \\frac{\\lambda ||M||_0}{Sparsity ~ Regularization} \\right].\\tag{21}\n\\end{aligned}$\nThis part of derivation aims to transform the above estimation problem into a linear regression\nproblem, with the regression data pairs $(\\mathcal{X}, T)$ and some unknown parameters $\\beta$ associated with\nmask $M$ to be estimated. Eventually, we'll derive the representation of each part of $\\beta_M, \\mathcal{X}, T$, and\neventually reach the following form:\n$\\min_{\\beta_M} \\sum_{(\\mathcal{S}_i, a_i, \\mathcal{S}_i) \\in \\mathcal{D}} \\left[T_{\\pi_{\\beta}} (s' | s_i, a_i) - \\mathcal{X}_i \\beta_M ||^2 + \\frac{\\lambda ||\\beta_M||_0}{\\text{Sparsity}}\\right].$ (22)\nWe define each component of this target form of $l_0$ regression as follows:\n\u2022 For unknown parameters $\\beta_M$: We first define $\\beta_M \\in [0, 1]^{dd'}$ as a column dimensional vector\nconsisting of all the entries in time-homogenous causal matrix $M$, where $\\beta_M^i$ denotes the $i$-th\nentry of $\\beta_M$. Besides, we define $\\beta^{\\star}_M$ as the true core matrix given some offline dataset $\\mathcal{D}$ and\ncorresponding data pairs $\\mathcal{T}_{data}, \\mathcal{X}_{data}$ that satisfies $\\mathcal{T}_{data} = \\beta^{\\star}_M \\mathcal{X}_{data} + \\epsilon$.\n\u2022 For dataset $\\mathcal{D}$: Recall the transition pairs in the offline dataset $\\mathcal{D} = \\{s_i, a_i, s'_i\\}_{1<i<n}$. Here, $n$\nrepresents the sample size over certain state-action pairs in the rollout data by some behavior\npolicy $\\pi_\\beta$. For simplicity, we denote $n \\eqq n(s, a)$ in the following derivation, which is mentioned\nin Section 2.1.\n\u2022 For regression target $T_{\\pi_\\beta}$: Then, we introduce the following transition targets $T_{\\pi_{\\beta}}$ induced by\nthe offline dataset $\\mathcal{D}$ sampled with behavior policy $\\pi_\\beta$:\n$\\mathcal{T}_{\\pi_\\beta}(s' | s, a) := \\frac{1}{n(s, a)} \\sum_{(\\mathcal{S}_i, a_i, s'_i) \\in \\mathcal{D}} 1(s_i = s, a_i = a, s'_i = s').$ (23)\nUnder the n finite samples in the offline dataset, we assume that $T_{\\pi_\\beta} \\sim \\mathcal{N}(\\mathbb{E}[\\mathcal{T}_{\\pi_\\beta}], \\sigma^2 \\mathbb{I}_n)$. The\nabove definition specifies the regression target in the $l_0$ regression problem, and we denote\n$\\mathcal{T}_{\\pi_\\beta} = [\\mathcal{T}_{\\pi_\\beta} (s_1|s_1, a_1), ..., \\mathcal{T}_{\\pi_\\beta}(s_n|s_n, a_n)]^T \\in \\mathbb{R}^n$ as the empirical transition probabilities of\ncertain transition pairs in the offline data $\\mathcal{D} = \\{s_i, a_i, s'_i\\}_{1<i<n}$.\n\u2022 For regression data $\\mathcal{X}$: Next, we need to specify the data $\\mathcal{X}$ in the regression problem. We\ndenote the $i$-th row of $\\mathcal{X}$ as the $i$-th sample in the offline transition pairs $\\mathcal{X}_i \\in \\mathcal{D}$, which is a\nvector of Kronecker product between $\\phi(s_i, a_i) \\in \\mathbb{R}^d$ and normalized $\\frac{\\mu(s')}{C_{\\mu}} \\in \\mathbb{R}^{d'}$ (without loss\nof generality, we assume $C_\\phi = 1$ and only need to normalize $\\mu(s')$ by $C_\\mu$):\n$\\begin{aligned}\n&\\mathcal{X}_i = \\phi(s_i, a_i) \\otimes \\frac{\\mu(s')}{C_{\\mu}} \\\\\n& = \\left[\\phi(s_i, a_i)^{(1)} \\frac{\\mu(s')^{(1)}}{C_{\\mu}}, \\phi(s_i, a_i)^{(1)} \\frac{\\mu(s')^{(2)}}{C_{\\mu}}, ..., \\phi(s_i, a_i)^{(d)} \\frac{\\mu(s')^{(d')}}{C_{\\mu}}\\right]^T\n\\end{aligned}$ (24)\nAs a result, $\\mathcal{X}_i \\in \\mathbb{R}^{dd'}$, since there are in all $n$ samples in offline dataset, $\\mathcal{X} \\in \\mathbb{R}^{n \\times dd'}$ is the\ndataset-dependent matrix with all $n$ rows of samples, and $d$ and $d'$ are the latent dimension\nof $\\phi$ and $\\mu$, respectively. Based on the feature regularity criteria in Assumption 3, we have\n$|| \\mathcal{X}_i ||_2 \\leq || \\mathcal{X}_i ||_1 \\leq 1$, $||\\mathcal{X}||_\\infty \\leq 1$.\nThe prior work [14] estimate the transition kernel of a bilinear MDP using the following ridge\nregression:\n$\\begin{aligned}\n\\min_M \\mathbb{E}_{(\\mathcal{S}, a, s') \\in \\mathcal{D}} ||\\mu(s')^\\top K_\\mu^{-1} - \\phi(s, a)^\\top M||^2 + \\lambda ||M||_*\n\\end{aligned}$ (25)"}, {"title": "decomposing the term of interest. To begin with, recalling the definition of Bellman", "content": "decomposing the term of interest. To begin with, recalling the definition of Bellman\noperator $\\mathcal{B}_h$ in equation (16) and applying H\u00f6lder's inequality, the term of interest for any time step"}, {"title": "the world", "content": "0 \u2264 h < H and state-action pair (s, a) \u2208 S \u00d7 A can be controlled as\n|(BhVh+1)(s, a) \u2212 (BhVh+1)(s, a)| \u2264 |(T(\u00b7|s, a) \u2212 T(\u00b7|s, a), Vh+1)|\n\u2264 ||T(\u00b7|s, a) \u2212 T(\u00b7|s, a)||\u221e||Vh+1||1\n\u2264 ||T(\u00b7|s, a) \u2212 T(\u00b7|s, a)||\u221e, (29)\nwhere the first inequality is held given our goal-conditioned reward formulation in section 2.1. To\ncontinue, we have\n|(BhVh+1)(s, a) \u2212 (BhVh+1)(s, a)| \u2264 ||T(\u00b7|s, a) \u2212 T(\u00b7|s, a)||\u221e\n(30)\n= ||X(\u00b7|s, a) \u2212 X(\u00b7|s, a)BM||\u221e\nHere, we recall BM represents the parameter vector in the estimated causal masks based on the offline\ndataset D sampled by \u03c0\u03b2. Similarly, we denote BM as the estimated causal mask outputted from\nequation (26) based on the infinite dataset D* generated by the behavior policy \u03c0\u03b2. Then, we can\nfurther control equation (30) as\n||X(\u00b7|s, a) \u2212 X(\u00b7|s, a)BM||\u221e = ||X(\u00b7|s, a) \u2212 X(\u00b7|s, a) + X(\u00b7|s, a) \u2212 X(\u00b7|s, a)BM||\n\u2264 ||X(\u00b7|s, a)[ - \u221e + ||X(\u00b7|s, a)[ 8 M]|\n\u2264 ||X(\u00b7|s, a) ||\u221e|| - \u221e + ||X(\u00b7|s, a) ||\u221e|| - B100\n8 M8\n\u2212||B1\u2212BM1 +200|| \u2212 ||BM8 \u2212BM8||\n\u2264-+\n\u221e +\n(a)\n(b)\n(31)\nHere the last inequality comes from the fact that\n||X(\u00b7|s, a) ||\u221e = max \u2211 |X(s, alij = max ||X(\u00b7|s, a)i||1\ni\u2208 S\ni\u2208[dd\u2019]\n= max | \u2297 ]|1 \u2264 = 1 (d02) iE|S| C\u03bc|\u03bc(s\u2019)| i.s.t.4\nC\u03bc\nbased on the definition of X in equation (24) and assumption 3. Here (a) comes from the mismatch\nerror between the demonstrated offline dataset and some optimal rollout datasets. And (b) comes\nfrom the error of the l0 optimization of causal masks given the existence of exogenous noise \u03c3\ndefined by SCM in Definition 4. We will control them separately in the following.\nStep 3: Controlling term (a). We need to consider the optimization process in the original\nregression problem in equation (22) to fully understand the difference between BM and B , B0, where\nthe only difference is that the latter uses a perfect dataset with infinite samples. The optimization\nproblem we target (cf. equation (26)) can be solved by the iterative hard thresholding algorithm (IHT)\nproposed by [87] IHT offers an iterative solution for the l0 regression problem, armed with a hard\nthresholding operator as below:\n|| (j)\n= 0 ( ) . dd=1 [ 20, || 8M] | \u2264 \u03bb/ j if otherwise || \u2264 ,0 max, \u03b2; \nWe denote (i) as the estimated causal mask parameters after i-th iterations with dataset D.32\nSimilarly, we denote M (i) as the estimation after i-th iterations with dataset D*. We initialize theM\nthe graph to be a full graph regardless of the datasets (D* or D) used in the optimization process, leading\nto (0) = (0) = 1 \u2208 Rdd .BM2M8\n(i)(i-1)+T - X (i-1)] .D .X \u03b7T E X (i - (i(i-1+XT | )- XM) 2 .E T \u2212 X ] . )D (BM= 2 .7/ 2 .Recall that X T\n, E \u2208 Rand is0| B) ) ( ( \u2208 M\u2208is - \u03c0\u03c0 812\u03c0 5 BM E 0. In7B BM03 BM. ThenBM 1263 ( , ( ) 1] -BM\u2208 - \u03c0\u03c0 =2\u03c0(10-\u2212X[ T-X T ( -X ( )E \u22127T25\n - 137E X ]+ (\u03c0M+ XM| i.e .e , for ( . ( ))3 XM, M.e\n XM, M02eE -7 E XM E8< || E [ E8 [||E\ntr BM = 0 and XM is2 .E>M < B417E \nX\u03c0 BMM740 ( (BM0 EBM 10+\u03c0MBM<127EE |0, where: ||(D1085)8 ( )| \u03c02 ( -X| \u03c0 - \u03c0( )8 TBM\u2212, we ( ( ()+470-E7=>>=<<"}, {"title": "B.32 Controlling", "content": " (BM. D and (BM> (27) ) = T\u2212 ,223E X M.12 BM E17<><1- B 18 8 = .T .0728XM ( -BM - (8 M >M (M0\u03c0 = 82+ ,2,81\u2212 22 .7 \u2212||D ,B2>+ \u03c0E||872 E212E < | -| 1E| < \u03c0 , , | -| \u03c08 \u03c0\u03c0 1-T\u2212M0 . \u03c0T+2BME |: 02 + >0 T>+70"}, {"title": "44\u03c0(BM <.E XM BM E ( (B 4 XM44\u03c0XM8. (5E22EE . . 2.B7 .0E274TBM E5", "content": "3- - - = > > + E\u03c0 E\u03c0 = E2 | [E188+4 < > [ > \u03c08 +E < \u2212275= B82B.2 BM08= - .BM) 4 =144\u03c08( )+|| |:B"}, {"title": "M8+EMB7+3 .2E+BE71.E (6 (4\u03c0", "content": "2 ( .2 8471 B74>1E <||E( . T+T+ 2BM= 2BM\u03c0\u03c0BM.7 BM5.4<<|| -\u03c0 E||E2:28=0==0=0 == ="}, {"title": "\u03a0\u039c4 B 8477+86 \u03a00+\u03c08 + 2 + ||BM3+72\u03c02E ||4\u0395M + B3 - ( 36 TBE - EE ( <M4", "content": "3441 BM BM.==+= =1+= -||M4++ = 0 > = 154:B .237\u03a0E1(541<1M14 ( 2+ ) ||EM< 82 8 + ==-+++"}]}