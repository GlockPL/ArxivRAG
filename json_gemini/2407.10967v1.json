{"title": "BECAUSE:\nBilinear Causal Representation for Generalizable\nOffline Model-based Reinforcement Learning", "authors": ["Haohong Lin", "Wenhao Ding", "Jian Chen", "Laixi Shi", "Jiacheng Zhu", "Bo Li", "Ding Zhao"], "abstract": "Offline model-based reinforcement learning (MBRL) enhances data efficiency by\nutilizing pre-collected datasets to learn models and policies, especially in scenarios\nwhere exploration is costly or infeasible. Nevertheless, its performance often\nsuffers from the objective mismatch between model and policy learning, resulting in\ninferior performance despite accurate model predictions. This paper first identifies\nthe primary source of this mismatch comes from the underlying confounders\npresent in offline data for MBRL. Subsequently, we introduce BilinEar CAUSal\nrEpresentation (BECAUSE), an algorithm to capture causal representation for both\nstates and actions to reduce the influence of the distribution shift, thus mitigating\nthe objective mismatch problem. Comprehensive evaluations on 18 tasks that vary\nin data quality and environment context demonstrate the superior performance of\nBECAUSE over existing offline RL algorithms. We show the generalizability and\nrobustness of BECAUSE under fewer samples or larger numbers of confounders.\nAdditionally, we offer theoretical analysis of BECAUSE to prove its error bound\nand sample efficiency when integrating causal representation into offline MBRL.", "sections": [{"title": "1 Introduction", "content": "Offline Reinforcement Learning (RL) has shown great promise in learning directly from historically\ncollected datasets, especially in scenarios where active interaction is expensive or infeasible [1].\nSpecifically, offline model-based reinforcement learning (MBRL) [2, 3, 4], learning policies with an\nestimated world model, generally perform better than their model-free counterparts in long-horizon\ntasks such as self-driving vehicles [5], robotics [6], and healthcare [7]. However, offline RL suffers\nfrom distribution shift since the rollout data could sample from some unknown behavior policies that\nare sub-optimal or from slightly different environments compared to the deployment time [8].\nAlthough identifying distribution shift issues, many\nof the current offline MBRL works fail to model the\nshift in environment dynamics, which is ubiquitous\nand could cause catastrophic failure of trained policy\nat a slightly different deployment stage. Furthermore,\nsince the learning objectives of the world models and\npolicies are isolated from each other, a significant\nchallenge in offline MBRL is objective mismatch [9,\n10] problem: models that achieve\na lower training loss are not necessarily better for\ncontrol performance. For example, in long-horizon\nplanning tasks, the reward is sparse yet the prediction accuracy of the model may decay as the horizon\nenlarges and compounding error accumulates. Previous works [9, 10] have attempted to reduce such\nobjective mismatch by jointly learning the model and policy. However, the performance is suboptimal\nin the absence of the underlying cause of objective mismatch [8].\nIn this work, we identify that the objective mismatch between model estimation and policy learning\ncomes from two sources of distribution shift in offline MBRL: (1) shift between the online optimal\npolicy and offline sub-optimal behavior policies, and (2) shift between the data collection environment\nand online testing environments. Unlike humans, who make decisions based on reasoning over task-\nrelevant factors, models in offline RL memorize correlations without learning the causality. The\nsub-optimal behavior policies introduce spurious correlations [11] between actions and states, making\nthe model memorize specific actions. When online testing environments differ from the data collection\nenvironment, the model could overfit spurious correlations in the state and fail to generalize to unseen\nstates. Based on the analysis of the above mismatch, our work differs from previous work of causal\nmodel-based RL [12, 13] in that we model causality in both model and policy learning. We aim to\navoid spurious correlation by discovering underlying structures between abstracted states and actions.\nTo alleviate objective mismatch and generalize well, we introduce the BilinEar CAUSal rEpresenta-\ntion (BECAUSE) that integrates the causal representation in both world model learning and planning\nof MBRL agents. Inspired by preliminary works that use bilinear MDPs to capture the structural\nrepresentation in MBRL [14], we first approximate the causal representation to capture the low-rank\nstructure in the world model, then use this learned representation to facilitate planning by quantifying\nthe uncertainty of sampled transition pairs. Consequently, we factorize the spurious correlations and\nlearn a unified representation for both the world model and planner.\nIn summary, the contribution of this paper is threefold:\n\u2022 We formulate offline MBRL into the causal representation learning problem, highlighting the tight\nconnection between structural causal models and low-rank structures in MDPs. To the best of\nour knowledge, this is the first work that systematically reveals the connection between causal\nrepresentation learning and Bilinear MDPs.\n\u2022 We propose BECAUSE, an empirical causal representation framework, based on the above formu-\nlation. BECAUSE first learns a causal world model, then fosters the generalizability of offline RL\nagents by quantifying the uncertainty of the state transition, which facilitates conservative planning\nto mitigate the objective mismatch.\n\u2022 We provide extensive empirical studies and performance analysis in tasks of multiple domains to\ndemonstrate the superiority of BECAUSE over existing baselines, which illustrates its potential to\nimprove the generalizability and robustness of offline MBRL algorithms."}, {"title": "2 Problem Formulation", "content": "To alleviate the objective mismatch problem and the degraded performance caused by the spurious\ncorrelation, we first provide our novel formulation of learning the underlying causal structures of\nMarkov Decision Process (MDP) under the bilinear MDP setting, then introduce the causal discovery\nfor MDP with confounders."}, {"title": "2.1\nPreliminary: MDP and Bilnear MDP", "content": "We denote an episodic finite-horizon MDP by M = {S, A, T, H, r}, which is composed of state\nspace S, action space A, a set of transition functions T, planning horizon H and reward function r\nassociated with task preferences. Without loss of generality in many real-world practices, we assume\nthat the reward function is bounded by rh \u2208 [0,1],\u2200h \u2208 [H]. Specifically, we are interested in a\ngoal-conditioned reward setting, where \u2200g \u2208 S, r(s, a; g) = 1 if and only if s = g.\nGiven a policy \u03c0and the state-action pair (s, a) \u2208 S \u00d7 A, we then define the state-action value\nfunction in the timestep h as $Q_\u03c0^h(s,a) = \\mathbb{E}_\u03c0[\\sum_{i=h}^{H} r_i(s_i, a_i) | s_h = s, a_h = a]$, and the value func-\ntion $V_\u03c0^h(s) = \\mathbb{E}_\u03c0[\\sum_{i=h}^{H} r_i(s_i, a_i) | s_h = s]$. The expectation En here is integrated into randomness\nthroughout the trajectory, which is essentially induced by the random action of the policy $a_i \\sim \u03c0(\\cdot | s_i)$\nand the time-homogeneous transition dynamics of the environment $s_{i+1} \\sim T(\\cdot | s_i, a_i), \\forall i \\in [h, H]$.\nIn the offline dataset, the data rollouts can be seen as generated by some (mixed) behavior policy \u03c0\u03b2,\nresulting in a dataset D with in total n samples ${s_i, a_i, s'_i, r_i}_{1 \\leq i \\leq n}$."}, {"title": "2.2 Action State Confounded MDP", "content": "We consider the existence of confounders in\nthe MDP to represent the offline data collec-\ntion process, and define action-state confounded\nMDP (ASC-MDP):\nDefinition 2 (ASC-MDP). Besides the compo-\nnents in standard MDPs M = {S, A, T, H, r},\nwe introduce a set of unobserved confounders\nu. In ASP-MDP, confounders are factorized as\n$u = {u_\u03c0,u_c}_{1\\leq h \\leq H}$, where $u_\u03c0 \\in U$ denotes\nthe confounders between s and $a \\sim \u03c0_\u03b2(s)$ in-\nduced by behavior policies, and $u_c \\in U$ denotes the confounders within the state-action pairs of the\nenvironment transition, that is, the inherent structure between (s, a) and s'. Here we assume a time-\ninvariant confounder distribution $u \\sim P_u(\\cdot), \\forall h \\in [H]$, which is a common assumption [15, 16, 17]\nThe resulting causal relationship of ASC-MDP is demonstrated in Figure 2. Originating from\nthe original MDP, ASC-MDP is different from the Confounded MDP [18] and State-Confounded\nMDP (SC-MDP) [19] in that it models both the spurious correlation between the current state s and\nthe current action a, as well as those between the next state s' and (s, a). Yet, confounded MDP and\nSC-MDP only model part of the possible confounders between states and actions. The factorization\nof the confounder in ASC-MDP aligns with the source of spurious correlation in offline MBRL."}, {"title": "3 Proposed Method: BECAUSE", "content": "We propose BECAUSE, our core methodology for modeling, learning, and applying our causal\nrepresentations for generalizable offline MBRL. Section 3.1 models the basic format of causal\nrepresentations and analyzes their properties. Section 3.2 gives a compact way to learn the causal\nrepresentation $(s, a)$ and $\u00b5(s')$, as well as the core mask estimation M. Section 3.3 utilizes these\nlearned causal representations in both world model learning and MBRL planning from offline datasets."}, {"title": "3.1 Causal Representation for ASC-MDP", "content": "In the presence of a hidden confounder u, we model the confounder behind the transition dynamics\nas a linear confounded MDP [18]:\n$T(s'|s, a, u) = \u03c6(s,a,u)^T\u00b5(s'),$\\\n      ", "model": "n$T(s'|s, a, u) = T(s'|s, a, u_c) = \u03c6(s, a)^T M(u_c)\u00b5(s').$\\"}, {"title": "3.2 Learning Causal Representation from Offline Data", "content": "We first learn the causal world model T(s'|s, a)\nin the presence of confounders u in the offline\ndatasets. As formulated in ASC-MDP 2, there\nare two sets of confounders: un and uc. To\nestimate an unconfounded transition model and\nremove the effect of confounder, we first remove\nthe impact of uc which comes from the dynam-\nics shift by estimating a batch-wise transition\nmatrix M(uc), then we apply a reweighting for-\nmula to deconfound un induced by the behavior\npolicies and mitigate the model objective mis-\nmatch.\nAs discussed in Definition 3, we only need to\noptimize the part of the parameters of the causal\ngraph G, i.e. M. Thus, we can remove the constraints in (4), then transform the original causal\ndiscovery problem into a regularized MLE problem as follows:\n$\\begin{aligned}\n\\min \\mathcal{L}_{\\text{mask}} (\\mathbf{M}) & = \\min_{\\mathbf{M}} \\big( - \\log p( \\mathcal{D}; \\phi, \\mu, \\mathbf{M}) + \\lambda |\\mathbf{M}|_0 \\big) \\\\ & = \\min_{\\mathbf{M}} \\big( \\mathbb{E}_{(s,a,s') \\in \\mathcal{D}} \\|\\mu^\\top(s') \\mathbf{K}_\\mu^{-1} - \\phi^\\top (s,a) \\mathbf{M} \\|_2 + \\frac{\\lambda \\|\\mathbf{M}\\|_0}{\\text{Sparsity Regularization}} \\big). \\label{loss:m}\n\\end{aligned}$$\\\n      \"(5)\"\nwhere $K_\u00b5 := \u2211_{s'\u025bS}\u00b5(s')\u00b5(s')^T$ is an invertible matrix. The derivation of equation (5) is elaborated\nin Appendix A.3. In practice, we use the \u03c7\u00b2-test for discrete state and action space and the fast\nConditional Independent Test (CIT) [27] for continuous variables to estimate each entry in the core\nmatrix M. We regularize the sparsity of M by controlling the p-value threshold in CIT and provide a\nmore detailed implementation in Appendix C.1.\nEstimating the core mask provides a more accurate relationship between state and action represen-\ntations, and we further refine the state action representation function \u03c6 and u to help capture more\naccurate transition dynamics. We optimize them by solving the following problem, according to the\ntransition model loss and spectral norm regularization [28] to satisfy the regularity constraints of the\nfeature in Assumption 3:\n$\\begin{aligned}\n\\min_{\\phi, \\mu} \\mathcal{L}_{\\text{rep}} (\\phi, \\mu) \\triangleq  \\min_{\\phi, \\mu}  \\mathbb{E}_{(s,a,s') \\in \\mathcal{D}} \\|\\mu^\\top (s') \\mathbf{K}_\\mu^{-1} - \\phi^\\top (s,a) \\mathbf{M} \\|_2^2 + \\lambda_\\phi \\|\\phi \\|_\\ast + \\lambda_\\mu \\|\\mu \\|_\\ast, \\label{loss:rep}\n\\end{aligned}$$\\\n      \"(6)\nThe world model learning process is illustrated in Figure 3. The estimation of individual M(uc)\nmitigates the spurious correlation brought by uc. To further deal with the spurious correlation in\""}, {"title": "3.3 Causal Representation for Uncertainty Quantification", "content": "To avoid entering OOD states in the\nonline deployment, we further design\na pessimistic planner according to the\nuncertainty of the predicted trajecto-\nries in the imagination rollout step to\nmitigate objective mismatch.\nWe use the feature embedding from\nbilinear causal representation to help\nquantify the uncertainty, denoted as\nEo(s,a). As we have access to the\noffline dataset, we learn an Energy-\nbased Model (EBM) [30, 31] based\non the abstracted state representation\n\u03c6 and core matrix M. A higher out-\nput of the energy function Eo(\u00b7, \u00b7) in-\ndicates a higher uncertainty in the cur-\nrent state as they are visited by the be-\nhavior policies \u03c0\u03b2 less frequently. In\npractice, the energy-based model usu-\nally suffers from a high-dimensional\ndata space [32]. To mitigate this over-\nhead of training a good uncertainty\nquantifier, we first embed the state\nAlgorithm 1: BECAUSE Training and Planning\nInput: Offline dataset D, causal discovery frequency k\nOutput: Causal mask M, feature function \u03c6, \u00fb, policy \u03c0\n// Causal world model learning\nMo\u2190 [1]d'\u00d7d'\nfor i \u2208 [K] do\nUpdate \u03c6\u03b7, \u03bc\u03b7 by Lrep(\u03c6, \u03bc) in (6)\nif i mod k = 0 then\nUpdate Mn with Lmask(M) in (5)\nWeighted average Mn with (7)\n// Uncertainty quantifier learning\nFit Ee(s, a) with LEBM (8)\nInitialize VH+1(s) = 0, \u2200(s, a)\n// Pessimistic planning\nwhile h < H do\nEstimate the uncertainty with score Eo(s, a)\nCompute Qh(s, a) with (9)\nVh(s, a) = maxaQ(s, a)\na \u2190 arg max Q(s, a)\ns', r \u2190 env.step (a, g)\nsamples through the abstract representation \u00b5(s'), and the state action pair via \u03c6(s, a).\n$L_{EBM}(\u03b8) = \\mathbb{E}_{f(s,a)} \\mathbb{E}_\u03b8[\\mu(s^+)|\\phi(s, a)] - \\mathbb{E}_{q(s,a)} \\mathbb{E}_\u03b8[\\mu(s^-)|\\phi(s, a)] + \\lambda_{EBM}\\|\u03b8\\|^2,$$\\\n      ", "a)": "n$Q_h(s, a) = Q_h(s, a) - E_\u03b8(s, a) = r_h(s,a) - E_\u03b8(s, a) + \\sum_{s' \\in \\mathcal{S}} \\hat{T}(s'|s, a)V_{h+1}(s').$\\"}, {"title": "3.4 Theoretical Analysis of BECAUSE", "content": "Then we move on to develop the theoretical analysis for the proposed method BECAUSE. Based\non two standard Assumption 2 and 3 on the feature's existence and regularity", "\u03c0\u03b2\nsatisfies": "n$\\begin{aligned"}, 1, 0], "bigg": "label{eq:0}\n\\end{aligned}$$\nwhere C1, Cs are some universal constants, o is SCM's noise level (see Definition 4), and M \u2208 Rd\u00d7d'\nis the optimal ground truth sparse transition matrix to be estimated.\nThe error bound shrinks as the offline sample size n over all state-action pairs increase. It also grows"}