{"title": "SMARTRAG: JOINTLY LEARN RAG-RELATED TASKS\nFROM THE ENVIRONMENT FEEDBACK", "authors": ["Jingsheng Gao", "Linxu Li", "Ke Ji", "Weiyuan Li", "Yixin Lian", "Yuzhuo Fu", "Bin Dai"], "abstract": "RAG systems consist of multiple modules to work together. However, these mod-\nules are usually separately trained. We argue that a system like RAG that incor-\nporates multiple modules should be jointly optimized to achieve optimal perfor-\nmance. To demonstrate this, we design a specific pipeline called SmartRAG that\nincludes a policy network and a retriever. The policy network can serve as 1) a de-\ncision maker that decides when to retrieve, 2) a query rewriter to generate a query\nmost suited to the retriever, and 3) an answer generator that produces the final\nresponse with/without the observations. We then propose to jointly optimize the\nwhole system using a reinforcement learning algorithm, with the reward designed\nto encourage the system to achieve the best performance with minimal retrieval\ncost. When jointly optimized, all the modules can be aware of how other modules\nare working and thus find the best way to work together as a complete system.\nEmpirical results demonstrate that the jointly optimized SmartRAG can achieve\nbetter performance than separately optimized counterparts.", "sections": [{"title": "1 INTRODUCTION", "content": "Although large language models(LLMs) (Chowdhery et al., 2023; Touvron et al., 2023; Chung et al.,\n2024) have demonstrated exceptional capabilities across various domains, addressing knowledge-\nrelated issues beyond model parameters remains a challenging task (Mallen et al., 2023b; Min et al.,\n2023). Retrieval-augmentation generation(RAG) effectively enhances model performance in these\nscenarios by retrieving additional information from external tools (Ram et al., 2023).\nRAG systems usually consist of multiple modules including at least a retriever and a generator.\nSome systems may have other modules like a reranker (Glass et al., 2022), a decision maker deciding\nwhen to retrieve (Jeong et al., 2024; Wang et al., 2023a), a query rewriter (Ma et al., 2023; Tan et al.,\n2024) or a verifier (Lewis et al., 2020; Izacard et al., 2023). These modules are often hand-designed\nand separately optimized. One of the issues is that the golden answer of the intermediate modules\nare usually not accessible. What is worse, sometimes the golden answer is model-dependent or\nretriever-dependent. For example, Asai et al. (2024) uses the result of GPT4 (Achiam et al., 2023)\nas the ground truth for the decision maker, which can be suboptimal. A question that GPT4 can\nanswer without retrieval may need retrieval for other base models, meaning that the golden answer\nfor the decision maker is model-dependent. In this sense, optimizing each module separately will\nobviously be suboptimal.\nWe argue that a system like RAG that incorporates multiple modules should be jointly optimized in\nan end-to-end manner. For this purpose, we design a RAG system, namely SmartRAG, and jointly\noptimize it using reinforcement learning, with environment feedback as supervision.\nSpecifically, SmartRAG includes two core building blocks, a policy network, and a retriever, as\nshown in Figure 1. The policy network can serve as three different roles. Firstly, it should decide"}, {"title": "2 RELATED WORK", "content": "Retrieval augmentation techniques have been utilized to acquire external knowledge, which aids\nlanguage models in achieving superior performance across a broad spectrum of tasks (Guu et al.,\n2020; Gao et al., 2023; Hu & Lu, 2024). Previous research has primarily concentrated on \u201cwhat\nto retrieve\" (Khandelwal et al., 2019; Ram et al., 2023) and \"how to utilize the retrieved informa-.\ntion\"(Khattab et al., 2022).\nRecently, some studies have begun to explore when to use retrieval to meet the varying requirements\nof diverse tasks (Jiang et al., 2023). For instance, Mallen et al. (2023b) assesses the popularity"}, {"title": "2.1 RETRIEVAL-AUGMENTED GENERATION", "content": "of a query based on the frequency of its entities and recommends activating retrieval modules only\nwhen the entity frequency drops below a predetermined threshold. Wang et al. (2023a) enhances\nthe model's retrieval performance by using self-knowledge to decide what they know and do not\nknow. Self-Rag (Asai et al., 2024) leverages special tokens to adaptively retrieve external knowl-\nedge and confirm the output's relevance or efficacy. Additionally, some researchers incorporated\nsupplementary modules or classifiers to determine whether a query necessitates additional knowl-\nedge for resolution (Liu et al., 2024; Jeong et al., 2024). Wang et al. (2024) create a compositional\nunknown dataset (CuQA), and utilize the confidence words or scores to decide whether to retrieve.\nUnlike previous methods, we have implemented an end-to-end approach to jointly optimize all the\nrelated tasks such that the whole system can achieve better performance."}, {"title": "2.2 LEARNING FROM FEEDBACK", "content": "Training LLMs from feedback (Ouyang et al., 2022; Ji et al., 2023) has proven effective in enabling\nLLMs to understand the impact of their actions and adapt their behavior accordingly, which effec-\ntively aligns the model's behavior with human intentions. Based on the different forms of explicit\nguidance, it can be categorized into four types: label-based (Guerdan et al., 2023), reward-based (Wu\net al., 2024), demonstration-based (Dasari et al., 2023), and comparison-based approaches (Ouyang\net al., 2022). One of the most effective training paradigms among them is reinforcement learn-\ning (Ge et al., 2024). The significant reason is that it is relatively easier to collect and evaluate the\nquality of responses compared to expert answers (Rafailov et al., 2024), improving proficiency in\ntranslation (Kreutzer et al., 2018), summarization (Liu et al., 2020), and instruction-following Ra-\nmamurthy et al. (2023). In retrieval augmentation, Ma et al. 2023 employs Proximal Policy Opti-\nmization (PPO) (Schulman et al., 2017b) to refine the search query, using the match score between\nthe predicted answer and the gold standard answer as the reward. Furthermore, this use of gold\nanswers as feedback enables the model to learn when to utilize tools to expand its capabilities and\ninteract with the real world (Qiao et al., 2024).\nDifferent from most of the previous methods, which are solving a context bandit problem, our pro-\nposed SmartRAG contains multiple steps of interactions between the policy and the environment\nand learns from the environment feedback rather than human feedback."}, {"title": "3 METHODOLOGY", "content": "Let x be the input question and y be the golden answer. For RAG systems, there is usually a\ncompanion retriever R that takes a query q as input and returns an observation o, i.e. o = R(q).\nThe core part of SmartRAG is a policy network $\\pi_{\\theta}$, which is an LLM with the parameters being $\\theta$.\nThe policy network takes the state s as input, which is a combination of the original question and\nthe optional observations, i.e.\na ~ $\\pi_{\\theta}(s = [x, o_s])$, (1)\nwhere $o_s$ is the concatenation of all the historical observations returned from the retriever while a\nis the action sampled from the output of the policy network. In SmartRAG, there are two different\ntypes of actions: answer and retrieve. Let $a_0$ be the first token of a. Then $a_0$ is a special token that\ncan be either [ANSWER] or [RETRIEVE]. If $a_0$ = [RETRIEVE], it means that the policy thinks\nit necessary to retrieve with query $a_{1:}$, which is the rest part of a. In this case, the retriever will be\ncalled to obtain the new observation $R(a_{1:})$ that will be appended to $o_s$. Then the policy network\nwill be called again to produce the next action by (1). On the other hand, if $a_0$ = [ANSWER], it\nindicates that the policy network thinks there is no need for an extra retrieve and it is able to produce\nthe right answer with $a_{1:}$. Thus we obtain the final answer y = $a_{1:}$.\nThe SmartRAG pipeline is shown in Algorithm 1. We can set the quota for retrieval as N. When\nthe number of retrieves has already met the quota, we can force the policy network to generate the\nanswer by forcing $a_0$, which is the first output token, to be [ANSWER]. Thus we can avoid the\npolicy network to keep retrieving if no satisfying observation is obtained from the retriever."}, {"title": "3.1 WARM-UP USING SUPERVISED FINETUNING", "content": "Since our policy network is initialized using a base LLM, the output of which does not necessarily\nfollow our design that the first output token is either [RETRIEVE] or [ANSWER]. To achieve the\noutput format requirements, we first warm-up the base LLM with our constructed dataset.\nFor each question x, we construct three different types of input-output pairs for the warm-up step.\nThe first pair is a direct answer. In this case, the input is the original question x while the output is\nthe golden answer prefixed with the special token [ANSWER]. Thus we have the first data point,\ni.e.\ns = [x, $o_s$ = []], a = [[ANSWER], y]. (2)\nThe second data point encourages the policy network to apply retrieval action. To achieve a diverse\ninitial state for the future RL algorithm, we ask GPT-3.5 \u00b9 to rewrite the original question x to\nproduce a diverse query for the retriever. The second data point then becomes\ns = [x, $o_s$ = []], a = [[RETRIEVE], GPT-Rewriter(x)]. (3)\nThe third data point enables the initial policy network to know how to answer with the observations,\nit is constructed as\ns = [x, $o_s$ = R(GPT-Rewriter(x))], a = [[ANSWER], y]. (4)\nAfter finetuning the base LLM with the above-constructed dataset, we can obtain an initial policy\nnetwork $\\pi_{\\theta_0}$ that can output with our desired format and generate reasonable results. It should be\nnoticed that the warm-up step only provides a reasonable initial policy. It does not necessarily mean\nthat the constructed dataset from (2) to (4) is the \u201cgolden\" answer since the behavior of the policy\nnetwork will be further optimized in the following reinforcement learning step.\nWe can further enhance the initial policy by slightly modifying the warm-up dataset. For the data\npoints that the base LLM can already correctly answer, we remove the constructed SFT data points\ndefined in (3) and (4). Otherwise, we remove the constructed SFT data point defined in (2). By\ndoing so, the warm-up dataset already contains the information about whether the base LLM has the\ncorresponding knowledge for the question, leading to a better initial policy, denoted as $\\pi_{\\theta^*}$."}, {"title": "3.2 JOINTLY OPTIMIZATION USING REINFORCEMENT LEARNING", "content": "After obtaining the initial policy $\\pi_{\\theta_0}$ in the SFT warm-up stage, we then start jointly optimizing the\nwhole framework using reinforcement learning. Following Algorithm 1, we can generate a trajectory\n($s^{(0)}$, $a^{(0)}$, $r^{(0)}$, ..., $s^{(T)}$, $a^{(T)}$, $r^{(T)}$), where $s^{(t)}$, $a^{(t)}$ and $r^{(t)}$ represents the state, action and reward\nat the t-th step respectively. T stands for the trajectory length and we then define the details of the\nreward $r^{(t)}$."}, {"title": "5 CONCLUSION", "content": "We introduce SmartRAG, a novel framework that can decide when to retrieve, what to retrieve,\nand how to answer with/without the retrieved observations. All these functions are accomplished\nby a policy network and a companion retriever. SmartRAG jointly optimizes the whole system\nusing reinforcement learning. Empirical results show that SmartRAG outperforms previous methods\nwhose modules are separately optimized. For systems comprising multiple modules, it is necessary\nto optimize them from a holistic perspective-a challenge we reserve for future research endeavors."}, {"title": "ETHICAL CONCERNS", "content": "This paper primarily focuses on an end-to-end RAG framework that addresses the questions of when\nto retrieve, what to retrieve, and how to answer within a single LLM. Our method utilizes open-\nsource datasets and Bing retrieval for training and testing, with all data being publicly accessible.\nWhile our approach demonstrates the ability to effectively enhance the model's decision-making\ncapabilities and improve the accuracy of retrieved information, as well as the final answers, we hope\nthat this design-more aligned with real-world applications-can enhance the efficiency of RAG\npipelines. Furthermore, we aspire for this promising framework to be applied across various LLM\ndecision-making scenarios."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "The details of experiment settings are provided in Section 4, and a more detailed description and\nimplementation setting can be found in Appendix A. All datasets and models used in our work are\npublicly available. Additionally, we have uploaded our code in the supplementary materials and will\nopen-source it upon acceptance of the paper."}, {"title": "BANALYSIS", "content": "Our training process includes both warm-up training and reinforcement learning training. The figure\nillustrates the changes in iteration reward, KL reward, policy loss, and value loss during the rein-\nforcement learning phase. The iteration reward, corresponding to the reward in the equation, shows\nan upward trend throughout the training process, indicating that the model is exploring in the right\ndirection. The increase in KL divergence reward indicates that the model's strategy is being updated,\nwith growing divergence from the previous strategy. At the same time, the loss trends of the policy\nmodel and value model are almost the opposite of the reward trend, which aligns with the typical\ntraining process of oscillating convergence followed by stable oscillations."}, {"title": "B.1 TRAINING PROCESS", "content": "Our training process includes both warm-up training and reinforcement learning training. The figure\nillustrates the changes in iteration reward, KL reward, policy loss, and value loss during the rein-\nforcement learning phase. The iteration reward, corresponding to the reward in the equation, shows\nan upward trend throughout the training process, indicating that the model is exploring in the right\ndirection. The increase in KL divergence reward indicates that the model's strategy is being updated,\nwith growing divergence from the previous strategy. At the same time, the loss trends of the policy\nmodel and value model are almost the opposite of the reward trend, which aligns with the typical\ntraining process of oscillating convergence followed by stable oscillations."}, {"title": "B.2 RETRIEVAL NUMBER", "content": "In the retrieval-augmented framework, the number of retrieved texts also influences the model's\nperformance. Generally, when the number of retrieved documents is not large, an increase in the\nnumber of retrieved documents leads to greater gains for the model. This is because when the\nretrieved information falls within the model's capacity to process, a larger set of documents is more\nlikely to contain useful information. Therefore, we investigated the model's performance when\nthe retrieval size is K=1 and K=4 based on initial policy 1. When K=4, it represents a retrieval of\nmore documents and a stronger retrieval capacity, allowing us to explore the impact of varying-value\ndatabase on our method's training results. The results, as shown in the figure, indicate that retrieving"}, {"title": "B.3 DATASET TRANSFER", "content": "Given that our framework emphasizes the importance of making the model aware of the retrieval\ndatabase, we tested the model trained with our framework on the TriviaQA dataset in a data mi-\ngration scenario. The goal was to explore whether this database awareness could transfer to other\ndatasets. We categorized the TriviaQA test set into three types: the first type includes questions\nthat can be answered correctly after supervised fine-tuning (SFT) without retrieval; the second type\nconsists of questions that can only be answered correctly after retrieval; and the third type contains\nquestions that cannot be answered correctly even after retrieval. Ideally, the model should avoid\nretrieval for the first and third types and opt for retrieval only for the second type. We tested the\nretrieval proportion for these three data types, and the results are presented in Table 8. It is evident\nthat the second type of data maintains the highest retrieval percentage, while the first type has a rel-\natively lower retrieval percentage. This indicates that in transfer testing scenarios, our model retains\na certain level of database awareness."}, {"title": "B.4 ADAPTIVE RETRIEVAL", "content": "Section 4.1 presents the F1 score results under the initial policy2 state. Here, we provide more\ncomprehensive results with a different initial state, where the results for both initiate policy2 and\ninitiate policy1 are shown in Tables 6 and 7, respectively. Nevertheless, we can observe that our\napproach enables the model to learn the concept of when to retrieve, resulting in improved final\nanswers, which stem from the optimization of what to retrieve and how to answer."}, {"title": "B.5 QUALITATIVE CASES", "content": "Sections 4.2 and 4.3 have already demonstrated that SmartRAG significantly improves both the\n'what to retrieve' and 'how to answer' tasks through quantitative and qualitative analyses. Table 9\nand 10 presents additional cases from three datasets. Similarly, it can be observed that the reformu-\nlated queries are more effective in retrieving relevant results, and under the same retrieval conditions,\nthe model trained with PPO also demonstrates superior performance"}]}