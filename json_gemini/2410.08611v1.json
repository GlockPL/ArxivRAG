{"title": "Conjugated Semantic Pool Improves OOD Detection with Pre-trained Vision-Language Models", "authors": ["Mengyuan Chen", "Junyu Gao", "Changsheng Xu"], "abstract": "A straightforward pipeline for zero-shot out-of-distribution (OOD) detection involves selecting potential OOD labels from an extensive semantic pool and then leveraging a pre-trained vision-language model to perform classification on both in-distribution (ID) and OOD labels. In this paper, we theorize that enhancing performance requires expanding the semantic pool, while increasing the expected probability of selected OOD labels being activated by OOD samples, and ensuring low mutual dependence among the activations of these OOD labels. A natural expansion manner is to adopt a larger lexicon; however, the inevitable introduction of numerous synonyms and uncommon words fails to meet the above requirements, indicating that viable expansion manners move beyond merely selecting words from a lexicon. Since OOD detection aims to correctly classify input images into ID/OOD class groups, we can \"make up\" OOD label candidates which are not standard class names but beneficial for the process. Observing that the original semantic pool is comprised of unmodified specific class names, we correspondingly construct a conjugated semantic pool (CSP) consisting of modified superclass names, each serving as a cluster center for samples sharing similar properties across different categories. Consistent with our established theory, expanding OOD label candidates with the CSP satisfies the requirements and outperforms existing works by 7.89% in FPR95.", "sections": [{"title": "1 Introduction", "content": "The efficacy of machine learning models typically diminishes on out-of-distribution (OOD) data, thereby underscoring the significance of flagging OOD samples for caution [35]. Traditional visual OOD detection methods are typically driven by a single image modality, leaving the rich information in textual labels untapped [21, 34, 63, 56, 11]. As pre-trained vision-language models (VLMs) develop, employing textual information in visual OOD detection has become a burgeoning paradigm [15, 12, 40, 58, 64, 43, 29]. A straightforward pipeline [29] is to select potential OOD labels from a semantic pool and leverages the text-image alignment ability of a pre-trained VLM. Specifically, potential OOD labels are selected from WordNet [39] based on their similarities to the In-distribution (ID) label space, and then CLIP [47] is employed to classify input images into ID/OOD class groups."}, {"title": "2 Preliminaries", "content": "Task setup. OOD detection leveraging pre-trained vision-language models (VLMs), also termed as zero-shot OOD detection [15, 12, 40, 58, 64, 43, 29], aims to identify OOD images from ID ones with only natural-language labels of ID classes available. Formally, given the testing image set X = Xin \u222a Xout, where Xin \u2229 Xout = (), and ID label (class name) set yin = {y1,...,\u0423\u043a}, where K is the number of ID classes, our target is to obtain an OOD detector G(x; yin) : X \u2192 {in, out}, where x \u2208 X denotes a test image. It is noteworthy that the zero-shot setting does not require that there be no overlap between the pre-training data of VLMs and the testing data X, but only stipulates that no ID images are available for model fine-tuning. In other words, the split of ID and OOD data completely depends on how users manually preset the ID label set Vin.\nOOD detection with a pre-trained VLM and a semantic pool. A straightforward pipeline of this task is to select potential OOD labels from a semantic pool and leverages the text-image alignment ability of a pre-trained VLM to perform zero-shot OOD detection [29]. Specifically, there are three steps: (1) Fetching numerous words from a semantic pool like WordNet [39] as OOD label candidates; (2) Selecting a portion of OOD label candidates most dissimilar to the entire ID label space; (3) Employing a pre-trained VLM like CLIP [47] to obtain similarities between testing images and ID/OOD labels and then performing OOD detection with a designed OOD score.\nThe OOD detection performance of this pipeline can be modeled as follows [29]. Given the selected OOD label set yout = {21,...,zm}, 0 < m < M, where m is the number of selected OOD labels, and M is the size of the semantic pool. By applying a threshold 4, we can naturally define p = P(si \u2265 4|f, zi, Xin) as the probability of classifying ID input images x \u2208 Xin as positive for the given label zi, where si = sim(f(x), f(zi)) is the similarity score given by the pre-trained model f. To derive an analytic form for the model's OOD detection performance, we employ a straightforward OOD score function S(x), aka the total positive count across categories for a sample x. Specifically, S(xin) = \u2211is, where sin is a Bernoulli random variable with parameter pin, i.e., the probability of sin = 1 is pin and the probability of sn = 0 is 1 \u2013 pin. Consequently, S(xin) follows a Poisson binomial distribution with parameters {pi, ..., pm }. put and S(xout) are defined similarly. Based on the Lyapunov central limit theorem (CLT) [1], we can obtain the following lemma:\nLemma 1. Given independent Bernoulli random variables {81, ..., sm} with parameters {P1, ..., Pm }, where 0 < pi < 1, as m goes to infinity, the Poisson binomial random variable C = \u2211i=1 Si converges in distribution to a normal random variable with distribution $N(\\sum_{i=1}^m P_i, \\sum_{i=1}^m P_i(1 - P_i))$.\nAccording to Lemma 1, proved in Appendix A.1, the distribution of Cin can be approximated as Cin ~ \u039d (\u03a31,\u03a31(1-p)), and the distribution of Cout can be approximated similarly. By denoting q1 = Ei[p], v\u2081 = Vari [p], q2 = Ei [put], v2 = Vari [put], we have\n$C^{in} \\sim N (mq_1, mq_1(1 - q_1) - mv_1), C^{out} \\sim N (mq_2, mq_2(1 - q_2) - mv_2) .$\nThereafter, with the derivation provided in Appendix A.2, we can obtain the closed-form expression of one of the most commonly adopted OOD performance metric, aka the false positive rate (FPR) when the true positive rate (TPR) is \u03bb \u2208 [0, 1], denoted by FPR\u5165, as\n$FPR_\\lambda = \\frac{1}{2} + \\frac{1}{2} erf (\\frac{\\Phi^{-1}(2\\lambda - 1) \\sqrt{q_1(1-q_1) - \\frac{v_1}{m}} + (q_1-q_2)}{\\sqrt{2(q_2(1-q_2) - \\frac{v_2}{m})}})$, where $\\Phi(z)$ is the CDF of a standard normal distribution.\nContrary to the monotonic trend suggested by Eqn. 2, the actual performances in experiments exhibit an inverted-V trend as the ratio of selected OOD labels in the semantic pool increases. Therefore, we further optimize the mathematic model by incorporating finer-grained variable relationships, seeking theoretical guidance for performance enhancement."}, {"title": "3 Methodology", "content": "Since selecting OOD labels is typically based on the reverse-order of similarities to the ID label space to minimize semantic overlap, i.e., the most dissimilar OOD label candidates are most likely"}, {"title": "3.1 A Theoretical Scheme for Performance Enhancement", "content": "to be selected, the expected probability, q1, of OOD labels being activated by ID images is not static. Specifically, as the ratio of selected OOD labels r = m/M increases, the expected activation probability q1 = Ei [pr] of existing OOD labels for ID images will monotonically increase, since more OOD labels with higher affinities to ID labels are selected. When all labels in the semantic pool are finally selected, q1 will achieve q2, which means the expected probabilities of OOD labels being activated by ID and OOD images are close. Meanwhile, q2 is considered as a constant when the semantic pool is fixed and the ratio r varies, since whether an element in the pool (excluding ID labels) corresponds to a potential OOD sample is independent of its similarity to the ID label space. Formally, defining qo as the lower bound of q1, we model the accumulated increase in q\u2081 as the ratio r increases from zero with the function u(r), which exhibits following properties:\n$u(r) = q_1 (r) - q_0, \\qquad u(r = 0|q_0, q_2) = 0, \\qquad u(r = 1|q_0, q_2) = q_2 - q_0 > 0, \\qquad u'(r) \\geq 0.$\nBesides, we assume that the absolute value of the curvature of u is constrained within a specific range, thereby preventing abrupt changes in the trend of u, which facilitates subsequent analysis. With u(r), we set x = 0.5 in Eqn. 2 for convenience and then explore the properties of\n$FPR_{0.5} = \\frac{1}{2} + \\frac{1}{2} erf (\\frac{\\sqrt{m} (\\frac{q_0 - q_2 + u(r|q_0, q_2)}{\\sqrt{q_2(1-q_2) - v_2}})}{2})$.\nDenote z = $\\frac{\\sqrt{m}}{2}(\\frac{q_0 - q_2 + u(r|q_0, q_2)}{\\sqrt{q_2(1-q_2) - v_2}})$, from Eqn. 4, we can derive that the first-order derivative of FPR0.5, denoted as G(r), can be expressed as\n$G(r|q_0, q_2, u, M) = \\frac{\\partial FPR_{0.5}}{\\partial r} = \\frac{M e^{-z^2}}{2\\sqrt{2\\pi}} \\frac{\\frac{\\partial u}{\\partial r}}{\\sqrt{m/q_2(1-q_2) - v_2}}$,\nwhich can be further proved to monotonically increase over the interval (0, 1] with respect to r with the above assumptions. Besides, according to Eqn. 5, we can obtain that\n$\\lim_{r \\rightarrow 0^+} G(r) = \\lim_{r \\rightarrow 0^+} \\frac{\\kappa (q_0 - q_2)}{2\\sqrt{r}} = -\\infty,  \\qquad \\lim_{r \\rightarrow 1^-} G(r) = \\kappa u'(r = 1) \\geq 0,$"}, {"title": "3.2 A Closer Look at the Inefficacy of Simple Lexicon Expansion", "content": "Therefore, it is time to consider how to expand the original semantic pool, which already includes most common words, while ensuring the increase of q2 and low mutual dependence. The most straightforward strategy for expansion, adopting larger existing lexicons, fails to consistently yield satisfactory outcomes, as shown in Fig. 2. Subsequently, we analyze that the inefficacy of simple lexicon expansion is attributed to the following reasons.\nOn the one hand, larger lexicons bring numerous uncommon words, whose expected probability of being activated by OOD images are minimal, thus resulting in a reduction of q2. As derived in Section 3.1, the decrease of q2 attenuates the performance improvements yielded by enlarging M. There are two potential reasons for the activation probability put of an uncommon OOD label z\u00ee being minimal:\n(1) Pre-trained VLMs lack semantic matching capability for label zi.\nThis issue is particularly pronounced when zi pertains to concepts such as highly abstract notions (e.g., \"idealism\", \"metaphysics\"), complex mathematical concepts (e.g., \"Lyapunov condition\", \"central limit theorem\"), or specific knowledge of individuals (e.g., personal names excluding celebrities). Pre-trained VLMs are unable to recognize the corresponding content of these text inputs, resulting in put remaining close to zero.\n(2) The set Xout lacks testing samples similar to label zi. For instance, when the test dataset primarily consists of images of everyday items, new labels constructed from astronomical terms are likely to maintain put close to zero. The above scenarios are much more prevalent in lexicons of uncommon terms than those of common words. Thereafter, a larger proportion of labels with minimal activation probability pout will diminish q2 = E\u00bf [put], thus attenuating the performance improvement.\nOn the other hand, larger lexicons introduce plenty of synonyms and near-synonyms for existing OOD label candidates, leading to a high degree of functional overlap with little additional benefit."}, {"title": "3.3 Expanding Label Candidates with Conjugated Semantic Pool", "content": "The above analysis suggests that viable strategies for semantic pool expansion require moving beyond the paradigm of simply selecting labels from a lexicon. Since the goal of OOD detection is to correctly classify input images into ID/OOD class groups, we can freely \"make up\" OOD label candidates which are not standard class names but beneficial for the process. Inspired by the fact that the original semantic pool is comprised of unmodified specific class names, each of which serves as a cluster center for samples from the same category but with varying properties, we correspondingly construct a conjugated semantic pool (CSP) consisting of specifically modified superclass names, each of which serves as a cluster center for samples sharing similar properties across different categories.\nWe notice that a class name inherently encompasses a broad semantic range. As shown in the bottom right of Fig. 3, when an image is attached with the class label \"cat\", it actually depicts one of various more specific situations, such as a \"white cat\", \"tabby kitten\", \"gray cat\", \"yawning cat\", or \"cat on a mat\", etc. Considering all feature points that correspond to more specific descriptions of cats as a cluster within the feature space, the feature point of \"cat\" can be regarded as its cluster center.\nThereafter, it naturally occurs to us that we should construct more suitable cluster centers to attract such \"homeless\" OOD images. This is why we expand the original semantic pool by constructing the CSP as follows: Instead of specifying concrete category names (e.g., \"cat\", \"wallet\", \"barbershop\"), we utilize superclass names to encompass a wider range of categories (e.g., \"creature\", \"item\", \"place\"); Instead of leaving category names undecorated, we using adjectives from a lexicon as modifiers to attract objects sharing similar properties. As a result, we obtain numerous label candidates of random combinations of adjectives and superclasses (e.g., \"white creature\", \"valuable item\", \"communal place\"). As Fig. 3 shows, in the feature space, \"white creature\" can be considered as the cluster center of all feature points corresponding to creatures modified by \"white\", such as \"white cat\", \"white butterfly\", \"white polar bear\", etc. Note that the semantic scopes of label candidates in the CSP may"}, {"title": "4 Related works", "content": "Traditional visual OOD detection. Traditional visual OOD detection methods, driven by the single image modality, can be broadly categorized into four distinct types: (1) Output-based methods, which aims to obtain improved OOD scores from network output, can be further classified into post-hoc methods [21, 34, 25, 54, 55, 44, 38] and training-based ones [10, 23, 57, 26, 71, 65, 30]. (2) Density-based methods [37, 49, 53, 67, 11] explicitly model the ID data with probabilistic models and identify test data located in regions of low density as OOD. (3) Distance-based methods [32, 51, 63, 56, 41, 7, 70, 59, 24, 18] originate from the core idea that OOD samples should be relatively far away from ID prototypes or centroids. (4) Reconstruction-based methods [76, 69, 28, 33], which employ an encoder-decoder framework trained on ID data, leverage the performance discrepancies between ID and OOD samples as indicators for anomaly detection. Besides, uncertainty estimation methods [36, 4, 5, 16, 3, 17] can also function as OOD detection methods. Furthermore, numerous studies [52, 72, 14, 42, 6] offer theoretical contributions.\nOOD detection leveraging pre-trained VLMs. By adopting pre-trained VLMs, employing textual information in visual OOD detection has become a burgeoning paradigm with remarkable performance [15, 12, 40, 58, 64, 43, 29]. Fort et al. [15] propose to feed the names of potential outlier classes to image-text pre-trained transformers like CLIP [47] for OOD detection. ZOC [12] extends CLIP with a text-based image description generator to output OOD label candidates for testing. MCM [40] simply adopts maximum predicted softmax value as the OOD score, which is an effective and representative post-hoc OOD detection method based on vision-language pre-training. Based on MCM, NPOS [58] generates artificial OOD training data and facilitates learning a reliable decision boundary between ID and OOD data. CLIPN [64] trains a text encoder to teach CLIP to comprehend negative prompts, effectively discriminating OOD samples through the similarity discrepancies between two text encoders and the frozen image encoder. Also based on CLIP, LSN [43] constructs negative classifiers by learning negative prompts to identify images not belonging to a given category. NegLabel [29] proposes a straightforward pipeline, that is, selecting potential OOD labels from an extensive semantic pool like WordNet [39], and then leveraging a pre-trained VLM like CLIP to classify input images into ID/OOD class groups. In this study, we explore the theoretical requirements for performance enhancement in this pipeline, and thus construct a conjugated semantic pool to expand OOD label candidates, which achieves performances improvements as expected.\nFurther discussion about NegLabel. NegLabel [29] undertakes a rudimentary theoretical analysis of the correlation between OOD detection performance and the quantity of adopted potential labels, concluding that an increase in selected labels correlates with enhanced performance. However, this conclusion contradicts the observed actual trend. The contradiction arises from that [29] simply assume a constant higher similarity between OOD labels and OOD images compared to ID images, neglecting that this similarity discrepancy originates from the strategy of reverse-order selection of OOD labels based on their similarity to the ID label space. As the set of selected OOD labels transitions from \"a small subset of labels with the lowest similarity to the entire ID label space\" to \"the whole semantic pool, which is unrelated to the setting of ID and OOD labels\", the discrepancy in similarity of ID images to OOD labels versus OOD images to OOD labels will progressively diminish"}, {"title": "5 Experiments", "content": "We mainly evaluate our method on the widely-used ImageNet-1k OOD detection benchmark [26]. This benchmark utilizes the large-scale ImageNet-1k dataset as the ID data, and select samples from iNaturalist [60], SUN [66], Places [73], and Textures [8] as the OOD data. The categories of the OOD data have been manually selected to prevent overlap with ImageNet-1k. Furthermore, we conduct experiments on hard OOD detection tasks, or with various ID datasets. Besides, we access whether our method generalizes well to different VLM architectures, including ALIGN [27], GroupViT [68], EVA [13], etc. More details of datasets can be found in Appendix B.\nImplementation details. Unless otherwise specified, we employ the CLIP ViT-B/16 model as the pre-trained VLM and WordNet as the lexicon. The superclass set for constructing the conjugated semantic pool is {area, creature, environment, item, landscape, object, pattern, place, scene, space, structure, thing, view, vista}, which nearly encompasses all real-world objects. The ablation in Appendix C.5 shows that numerous alternative selections can also yield significant performance improvements. All hyper-parameters are directly inherited from [29] without any modification, including the ratio r which is set to 15%. Additionally, we adopt the same NegMining algorithm, OOD score calculation method, and grouping strategy as described in [29]. All experiments are conducted using GeForce RTX 3090 GPUs.\nPrompt ensemble. We use the following prefixes to construct prompts for labels in the original semantic pool: the, the good (nice), a photo of (with) the nice, a good (close-up) photo of the nice. For labels in the conjugated semantic pool, we apply the prefixes: a nice (good, close-up) photo of. The baseline results reported in the ablation study (see Table 3) also utilize this technique.\nComputational cost. The prompt ensemble is constructed over the embedding space to avoid any additional inference cost. Similar to NegLabel, our method is a post hoc OOD detector with negligible extra computational burden, which introduces < 1% network forward latency.\nFollowing previous works [40, 29, 64], we adopt the following metrics: the area under the receiver operating characteristic curve (AUROC), and the false positive rate of OOD data when the true positive rate of ID data is 95% (FPR95) [46]."}, {"title": "5.2 Evaluation on OOD detection benchmarks", "content": "We compare our method with existing OOD detection methods on the ImageNet-1k benchmark organized by [26] in Table 1. The methods listed in the upper section of Table 1, ranging from MSP [21] to VOS [11], represent traditional visual OOD detection methods. Conversely, the methods in the lower section, extending from ZOC [12] to NegLabel [29], employ pre-trained VLMs like CLIP. It is evident that the integration of textual information through VLMs has increasingly become the predominant paradigm. Our method outperforms the baseline method NegLabel with a considerable improvement of 1.55% in AUROC and 7.89% in FPR95, which underscores the efficacy of our method. The reported results are averaged from runs of 10 different random seeds, whose results are provided in Appendix C.2.\nFollowing [40], we also evaluate our method on the hard OOD detection tasks in Table 2. The results of NegLabel [29] are reproduced with its released setting. Our method shows consistently high performances on various hard ID-OOD dataset pairs.\nWe also experiment on various ID datasets, including Stanford-Cars [31], CUB-200 [61], Oxford-Pet [45], Food-101 [2], ImageNet-Sketch [62], ImageNet-A [22], ImageNet-R [20], ImageNetV2 [48], etc. Refer to Appendix C.3 for details."}, {"title": "5.3 Empirical evidence supporting our assertions", "content": "In Fig. 1 and Table 8 (Appendix C.4), we present the FPR performances of our method and NegLabel against a progressively increasing ratio r, which represents the proportion of selected OOD labels in the whole semantic pool. The color gradations displayed in the table clearly illustrate an initial improvement in model performance followed by a subsequent decline as the ratio r increases. This trend aligns with our derivation in Section 3.1.\nIn Fig. 2 and Table 9 (Appendix C.4), we assess whether adopting larger lexicons enhances performances. Our findings indicate that it does not always hold. When the semantic pool covers the vast majority of common words, further expansion will introduce an excessive number of uncommon words and (near-)synonyms, thus failing to meet the derived requirements for theoretical performance enhancement. The inefficacy of simple lexicon expansion indicates that viable expansion manners move beyond merely selecting words from existing lexicons."}, {"title": "5.4 Ablation study", "content": "As shown in Table 3, we explore the effect of three semantic pool components: (1) the original semantic pool, which consists exclusively of specific class names; (2) simple adjective labels employed by NegLabel; and (3) our conjugated semantic"}, {"title": "6 Conclusion", "content": "In this paper, we propose that enhancing the performance of zero-shot OOD detection theoretically requires: (1) concurrently increasing the semantic pool size and the expected activation probability of selected OOD labels; (2) ensuring low mutual dependence among the label activations. Furthermore, we analyze that the inefficacy of simply adopting larger lexicons is attributed to the introduction of numerous uncommon words and (near-)synonyms, thus failing to meet the above requirements. Observing that the original semantic pool is comprised of unmodified specific class names, we correspondingly construct a conjugated semantic pool consisting of specifically modified superclass names, each serving as a cluster center for samples sharing similar properties across different categories. Consistent with the established theory, expanding OOD label candidates with the conjugated semantic pool satisfies the requirements and achieves considerable improvements.\nOur method has following limitations worth further exploration: (1) The effectiveness of CSP depends on the implicit assumption that the OOD samples exhibit a variety of distinct visual properties. When this assumption does not hold, i.e., OOD samples most share similar visual properties, such as the plant images in iNaturalist, the addition of CSP results in a slight performance decline, since most newly added labels are not likely to be activated. Reducing dependency on this assumption is a valuable direction for future research. (2) In this work, we primarily focus on analyzing and optimizing the activation status of OOD labels while making no modification to the ID label set. However, there is a possibility that selecting additional labels from the semantic pool, including the CSP, to expand the ID label set could enhance the identification of difficult ID samples. We consider this a promising direction for future exploration."}, {"title": "A Proof and Derivation", "content": "The proof is partially adapted from the appendix of [29].\nSuppose ${s_1, ..., s_m, ...}$ is a sequence of independent random variables, each with finite expected value $\u03bc_i$ and variance $\u03c3_i^2$. Define $p_m^2 = \\sum_{i=1}^m \u03c3_i^2$. If for some \u03b4 > 0, Lyapunov's condition\n$lim_{m \\rightarrow\\infty} \\frac{1}{p_m^{2+\\delta}} \\sum_{i=1}^m E[|s_i - \u03bc_i|^{2+\\delta}] = 0$ is satisfied, then a sum of $\\frac{s_i-\u03bc_i}{p_m}$ converges in distribution to a standard normal random variable, as m goes to infinity:\n$\\frac{1}{p_m} \\sum_{i=1}^m (s_i \u2013 \u03bc_i) \\longrightarrow N(0, 1)$.\nGiven a sequence of independent Bernoulli random variables ${s_1, ..., s_m}$ with parameters ${p_1, ..., p_m}$, where 0 < pi < 1, as m goes to infinity, the Poisson binomial random variable $C = \\sum_{i=1}^m s_i$ converges in distribution to the normal random variable:\n$C \\xrightarrow{d} N(\\sum_{i=1}^m p_i, \\sum_{i=1}^m p_i(1-p_i))$"}]}