{"title": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems", "authors": ["Robert Friel", "Masha Belyi", "Atindriyo Sanyal"], "abstract": "Retrieval-Augmented Generation (RAG) has become a standard architectural pattern for incorporating domain-specific knowledge into user-facing chat applications powered by Large Language Models (LLMs). RAG systems are characterized by (1) a document retriever that queries a domain-specific corpus for context information relevant to an input query, and (2) an LLM that generates a response based on the provided query and context. However, comprehensive evaluation of RAG systems remains a challenge due to the lack of unified evaluation criteria and annotated datasets. In response, we introduce RAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k examples. It covers five unique industry-specific domains and various RAG task types. RAGBench examples are sourced from industry corpora such as user manuals, making it particularly relevant for industry applications. Further, we formalize the TRACe evaluation framework: a set of explainable and actionable RAG evaluation metrics applicable across all RAG domains. We release the labeled dataset at https://huggingface.co/datasets/rungalileo/ragbench. RAGBench explainable labels facilitate holistic evaluation of RAG systems, enabling actionable feedback for continuous improvement of production applications. Thorough extensive benchmarking, we find that LLM-based RAG evaluation methods struggle to compete with a finetuned ROBERTa model on the RAG evaluation task. We identify areas where existing approaches fall short and propose the adoption of RAGBench with TRACe towards advancing the state of RAG evaluation systems.", "sections": [{"title": "1 Introduction", "content": "Despite remarkable reasoning and conversational abilities, out-of-the-box pre-trained Large Language Models (LLMs) struggle to reason about out-of-domain, knowledge-intensive queries [21, 14]. In response, Retriever-Augmented Generation (RAG) systems [21, 20] are becoming increasingly popular in user-facing dialogue applications [35]. Generally, RAG systems comprise a retriever component that queries relevant documents from an in-domain corpus and a downstream LLM generator model that incorporates the retrieved documents along with the original user query to output an informed response. The additional context helps ground the LLM in factual information and has been shown to boost performance on knowledge-intensive tasks [21].\nStill, when used in production settings, RAG systems are prone to hallucinations as the generator model struggles to retrieve relevant information from the context [1, 31, 7]. In the absence of a one-fits-all approach, application-specific RAG systems must be fine-tuned for optimal performance on domain-specific tasks. However, the choice of retriever and generator models for each application is complex and has serious implications on overall system quality and costs. With numerous"}, {"title": "2 Related Work", "content": "RAG benchmarks Numerous general LLM evaluation benchmarks, such as ChatbotArena [46] have been proposed in past work. However, human preference datasets, constructed through pairwise comparisons, have limitations. While these data are appropriate for fine-tuning general purpose LLM judges, they are insufficient for building RAG evaluation systems because preference judgements under-represent important RAG dimensions like factuality and completeness of the response [13].\nHOWEVER, CHATRAGBENCH only contains ground truth responses and lacks the granular component-specific labels that we release with RAGBench. As future work, we can consider annotating CHATRAGBENCH with the schema proposed in this paper, to further scale RAGBench.\nRAGTruth [41] is another recent effort at a RAG Benchmark. RAGTruth combines QA, Data-toText, and Summarization RAG data with human annotated hallucinated spans in the response. While it is an excellent benchmark for hallucination detection, it does not offer the level of granularity we present with RAGBench that is necessary to understand the RAG system as a whole.\nRAG evaluation Recently, several parallel efforts have proposed approaches to automated RAG evaluation. In RAGAS [9], the authors query an LLM-judge (GPT-3.5) with a curated prompt to evaluate context relevance, answer relevance and faithfulness of a RAG response. Next, Saad-Falcon et al. [33] propose ARES, a framework for fine-tuning smaller NLI models to predict the same metrics. This approach benefits from fine-tuning, though domain-specific annotated validation sets are required for each domain adaptation. In parallel, Chen et al. [4] develop a heuristic system to probe LLM's robustness to noisy and irrelevant context documents, and Adlakha et al. [1] explore heuristic algorithms to estimate RAG correctness and faithfulness. The lack of established RAG"}, {"title": "3 RAGBench Construction", "content": "RAGBench is a collection of real-world datasets that span different domains and RAG task types. We source data from open-book Question-Answer (QA) datasets (CovidQA [27], PubmedQA [15],\nHotpotQA [42], MS Marco [29], CUAD [12], EManual [28], TechQA [3], FinQA [5], TAT-QA [47],\nExpertQA [26], HAGRID [16]), as well one that was specifically adapted for RAG (DelucionQA [34]).\nWe transform all 12 component datasets to a standardized RAG format with consistent annotations.\nTo best represent real-world RAG scenarios, we vary a number parameters to construct the benchmark:\nthe source domain, number of context documents, context token length, and the response generator\nFigure 1 illustrates where these variable parameters fall in the RAG pipeline.\nRAGBench comprises five distinct domains: bio-medical research (PubmedQA,\nCovidQA), general knowledge (HotpotQA, MS Marco, HAGRID, ExperQA), legal contracts (CuAD),\ncustomer support (DelucionQA, EManual, TechQA), and finance (FinBench, TAT-QA). We select\nthese specific domains based on availability of data, and applicability to real-world RAG applications\nacross different industry verticals. For detailed descriptions of each component data source, refer to\nAppendix 9.2.\nContext token length in RAGBench ranges from 100 to 11k tokens, which\nwe report in Table 1. Notably, CUAD documents feature long contexts of up to 11k tokens each,\ncompared to the relatively short context in PubMedQA.\nWe curate RAGBench to inlcude a variety of difficult RAG task types. Customer\nsupport datasets simulate a common application of RAG in industry settings. FinQA and TAT-QA\nrequire numerical reasoning over hybrid tabular and text data. HotpotQA, CovidQA, and PubMedQA"}, {"title": "3.2 TRACe Evaluation Framework", "content": "We propose a suite of four comprehensive metrics to evaluate the quality of the retriever and the\nresponse generator components of RAG. An optimal RAG system must balance accuracy and\nefficiency. The retriever should precisely return all the necessary information to address the user\nquery, avoiding any superfluous data. The generator must effectively utilize the retrieved information,\nensuring the response is strictly based on the provided context without introducing any hallucinations\nin the output.\nTowards comprehensive evaluation of the abovementioned criteria, we introduce the TRACe evalua-\ntion framework to measure uTilization, Relevance, Adherence, and Completeness of a RAG system.\nUtilization, Adherence, and Completeness measure the quality of the generator. Adherence here is\nsynonymous with previously proposed answer faithfullness, groundednes, and attribution, all terms\nused in literature to measure how well an LLM output adheres to a source of factual information.\nRelevance measures the quality of the retriever output with respect to the query. Below we formalize\nthe definition of each metric.\nLet D be a set of context documents \\{d\u2081...dn\\} retrieved for a RAG input query. We\ndefine a set of relevant tokens in di as Ri = \\{t1, ...tr\\}. Ri encodes information in context document\ndi that is useful for answering the query. Similarly, we define Ui = \\{t1, ...tu\\} as the set of utilized\ntokens in document di, which reflect information that the generation model is using to produce\na response. Refer to Figure 3 for a visual representation of relevant and utilized spans. Len(x)\nmeasures the length of strings in x, which can be interpreted as character length, token length, or\nsentence length. For calculating ground-truth metrics, we employ sentence-length, since it aligns best\nwith our annotation schema (Section 3.3). However, token or character length may also be suitable\nfor other use cases.\nContext Relevance Context Relevance is defined in [9, 33] as the fraction of the retrieved context\nthat is relevant to the input query. Low relevance points to an inefficient retriever that supplies excess"}, {"title": "Context Relevance", "content": "Context Relevance is defined in [9, 33] as the fraction of the retrieved context\nthat is relevant to the input query. Low relevance points to an inefficient retriever that supplies excess\ninformation to the generation model. Long context inputs into the generator may accrue unnecessary\ncosts, as well as compromise the quality of the generated output. We measure relevance of context\ndocument di as:\ndocument relevance = $\\frac{Len(R_i)}{Len(d_i)}$ (1)\nExample-level relevance can be aggregated over all context documents in the example as:\ni=1\nexample relevance = $\\frac{\\sum_{i=1}^{n} Len(R_i)}{\\sum_{i=1}^{n} Len(d_i)}$ (2)\nContext Utilization Context Utilization is a new metric introduced in TRACE. We aim to measure\nthe the fraction of the retrieved context that is used by the generator to produce the response. Low\nUtilization in combination with low Relevance points to a greedy retriever, while low Utilization\nalone points to a weak generator that fails to leverage the provided context efficiently. Document-level\nand example-level Utilization are defined as:\ndocument utilization = $\\frac{Len(U_i)}{Len(d_i)}$   example utilization = $\\frac{\\sum_{i=1}^{n} Len(U_i)}{\\sum_{i=1}^{n} Len(d_i)}$ (3)\nCompleteness Completeness is another new metrics we introduce to measure how well the response\nincorporates all the relevant information in the context. Note that this is different from Utilization; it\nis possible to have high Relevance and high Utilization, but low Completeness when the generator\nutilizes irrelevant information in the context to produce a low quality response. Completeness for\ndocument di is calculated as the fraction of utilized substrings among all relevant substrings:\ncompleteness = $\\frac{Len(R_i \\cap U_i)}{Len(R_i)}$ (4)\nAnd can be extended to example-level by considering all relevant and utilized substrings across all\ncontext documents.\nAdherence Adherence is designed to detect hallucinations in RAG responses. Our definition of\nAdherence is synonymous with answer faithfullness [9, 33], groundednes [37], and attribution [32].\nFor alignment with existing hallucination detection approaches, we define example-level adherence\nas a boolean indicating whether or not all parts of the response are grounded in the context. However,\nin our annotation schema (Section 3.3) we also define A\u2081 = \\{t1, ...ta\\} as the set of response tokens\nthat are supported by the context to enable granular Adherence evaluation."}, {"title": "3.3 LLM annotator", "content": "We prompt GPT-4 (gpt-4-0125-preview) to produce ground truth Adherence, Relevance, and\nUtilization labels for input (documents, query, response) tuples in RAGBench. Completeness is\neasily derived from span-level Relevance and Utilization annotations, thus we don't request explicit\nannotations for it.\nFor high quality labels, we use proven techniques like chain of thought [40] that have been shown\nto maximize the correlation between GPT-4 and human judgements [43, 46]. For relevance and\nutilization we request the LLM-annotator to directly identify relevant and utilized sub-strings in the\ninput documents. For adherence, we instruct the LLM to identify which response sentences, if any,\nare supported by the provided context. We can then derive an example-level boolean adherence label\nby checking if all response sentences are supported. The exact prompt used for annotation is provided\nin Appendix 9.4. We apply post-processing steps to ensure high quality, reliable annotations from\nour GPT-labeler, which we outline in Appendix 9.5. We further validate our annotation approach in\nSection 4, and discuss the limitations of using an LLM-annotator in Section 8.\nRAGBench raw annotations contain token-level labels for utilization and relevance, which are\nconverted to TRACe metrics using equations in Section 3.2. We encourage future work on automated\nevaluators to predict the raw token-level labels, like relevant and utilized spans, rather than predicting\nthe example-level scores directly which are less interpretable for the end user."}, {"title": "5 Experiments", "content": "We benchmarks a few LLM evaluators on RAGBench: (1) zero-shot GPT-3.5-judge, where we query\nGPT-3.5 with our annotation prompt, (2) RAGAS [9], and (3) TruLens [37]. RAGAS employs a\nseries of few-shot prompts to GPT-3.5 to measure answer groundedness (Adherence) and Context\nRelevance metrics. Trulens is another zero-shot prompting approach that measures answer faithfulness\n(Adherence) and Context Relevance.\nWe fine-tune a DeBERTa-v3-Large [10] NLI checkpoint\u00b3 from Laurer et al. [19] with one key\narchitecture modification: we add a shallow prediction head for each of the output RAG metrics,\nwhich allows us to compute all TRACe metrics in a single forward pass. This is both cost-effective\nand enables transfer learning from head to head through back-propagation down to the shared base\nlayers. Each prediction head is a single layer feed-forward net that acts on the token-level output of\nthe last DeBERTa layer.\nWe attach two heads on the context tokens to estimate Relevance and Utilization probabilities, and\nanother head on the response tokens to estimate Adherence. For training, we broadcast sentence-level\nannotations to tokens, and tune to maximize token-level probabilities of Relevant, Utilized, and"}, {"title": "5.3 Evaluation", "content": "Our granular annotation schema allows for various evaluation setups. For example, we could\nevaluate either span-level or example/response-level predictions. For easy comparison with existing\nRAG evaluation approaches that are less granular, we report area under the receiver-operator curve\n(AUROC) on the response-level hallucination detection task, and root mean squared error (RMSE)\nfor example-level context Relevance and Utilization predictions."}, {"title": "6 Discussion", "content": "Table 3 reports results on test splits of each RAGBench component dataset. We compare baseline\nLLM methods with a finetunes DeBERTA encoder that trained on the full RAGBench train split.\nWe observe that the finetuned DeBERTa model\noutperforms the few/zero-shot LLM-judge baselines on most datasets. While GPT-3.5 demonstrates\ncompetitive performance with DeBERTa on a few metrics, DeBERTa consistently achieves superior\nperformance metrics across all evaluations. Despite the versatility of LLM judges across various\ntasks, their lack of specialization necessitates finetuning for optimal results. Future work may focus\non finetuning LLM judges to close the gap between DeBERTA and GPT-4 evaluation performance.\nIn Appendix 9.7, we demonstrate that, despite its small size, the finetuned DeBERTA model does\ngeneralize to out of domain RAG datasets in the same way that LLM-based approaches do.\nEstimating Context Relevance is Difficult As shown in Table 3, Relevance RMSE scores are\ngenerally higher than those for Utilization, indicating a greater difficulty in the relevance prediction\ntask. Utilization can be assessed through a straightforward semantic comparison between the context\nand the response. In contrast, relevance is a more intricate metric. Due to the nature of RAG, the\nmajority of retrieved documents are semantically related to the query. However, mere semantic\nsimilarity is insufficient. The model must ascertain whether the provided context includes specific\ninformation necessary to accurately answer the question. Thus, the task inherently involves deriving"}, {"title": "7 Conclusion", "content": "In this paper we introduce RAGBench, a large-scale dataset composed of real-world RAG examples\nintended for training and benchmarking RAG evaluation models. Additionally, we formulate TRACE,\na RAG evaluation framework comprising four metrics: uTilization, Relevance, Adherence, and\nCompleteness. TRACe standardizes the evaluation process, offering a consistent and systematic\napproach to measuring RAG system performance across various dimensions.\nWe benchmark existing RAG evaluation framework using RAGBench and demonstrate that LLM-\njudges struggle to compete with a fine-tuned RAG evaluation expert model. Future work may involve\nfine-tuning larger expert models to explore the potential for narrowing the performance gap between\nthese models and the ground truth.\nOur contributions address the need for standardized benchmarks and methodologies, enabling more\nprecise and actionable insights into the strengths and weaknesses of different RAG systems. This,\nin turn, will facilitate the iterative improvement of RAG models, driving forward the capabilities of\nretrieval-augmented generation in real-world applications."}, {"title": "8 Limitations", "content": "LLM Annotations Though LLMs demonstrate high correlations with human judgements on a\nvariety of tasks [6, 11], using them as a singular source of ground truth remains controversial [23]. At\nthe same time, human judgements of LLM outputs are also prone to inconsistencies and bias. In [13],\nthe authors find that human evaluators are often misled by the assertiveness and complexity of the\nLLM model output, which leads them to underestimate the rate of factuality errors in LLM responses.\nIn this work, we acknowledge the potential of noise and bias in RAGBench resulting from automated\nGPT-4-turbo annotations, and the concerns about the potential transmission of such biases into\nsubsequent RAG systems. One way to address this in future may be to replace the GPT-4-annotator\nwith and LLM \"jury\" as suggested in [38]. By aggregating judgements from diverse models, this\napproach can help reduce the noise and bias in the output judgements at low cost."}, {"title": "9 Appendix", "content": "We release RAGBench data on Hugginggface: https://huggingface.co/datasets/\nrungalileo/ragbench. Refer to model card and documentation there.\nWe publish our inferfence and evaluation code on Gihub: https://github.com/rungalileo/\nragbench/tree/main/ragbench.\nRAGBench is sourced from publicly released acadmic and industry datasets. As far as we know, none\nof the component datasets contain personally identifiable information or offensive content.\nPubMedQA is a collection of PubMed research abstracts with corresponding\nyes/no/maybe questions paired with each abstract. The original dataset comprises 3 subsets: PQA-L,\nPQA-U, and PQA-A, with 1k, 60k, and 210k abstracts, respectively. For all subsets, the question\nis derived from the title of the PubMed article using rule-based heuristics. Long answers are\nautomatically derived from the last sentence of the abstract for PQA-L and PQA-U, and QA-L\nanswers are further reviewed by expert annotators and annotated as yes/no/maybe. PQA-A comprises\nexclusively automatically generated questions and short answers.\nFor RAGBench we utilize the PQA-U subset and re-frame it from QA into a RAG task. To simulate\nRAG, we leverage already segmented PQA-U abstracts context chunks and we encode them into a\nvector DB with OpenAI embeddings. The size of the resulting DB is 200k. We retrieve 4 chunks for\neach PQA-U question using FAISS with eucledian distance as the similarity function. We ignore the\nresponses and labels in the original dataset and generate new responses with an LLM.\nCovidQA-RAG CovidQA-RAG is a combination of 2k expert-annotated questions sourced from\nCOVID-QA [27] and a vector database of 250,000 100-word passages built by Siriwardhana et al.\n[35]. Both questions and answers are sourced from CORD-19 [39] collection of research articles\nabout COVID-19.\nWe embed the questions and database passages with OpenAI embeddings and retrieve up to N\npassages for each COVID-QA question from the vector database using FAISS with eucledian distance\nas the similarity function and max_distance=0.25. We generate responses for each resulting RAG\n(context, question) instance with an LLM."}, {"title": "HotpotQA", "content": "HotpotQA comprises 113K crowd-sourced question-answer pairs sourced from\nWikipedia. Each pair is associated with a set of related context passages from one or multiple\nWikipedia pages. The dataset is constructed in a way that requires multi-hop reasoning over multiple\ncontext documents to arrive at the answer, which renders it a valuable candidate for our benchmark.\nWe sample data from the dev-distractor split, which contains up to 8 distractor context documents\nper sample. We downsample the context documents to 4 per example, making sure to include the\ndocument containing the response. We treat the context passages in HotpotQA as RAG context\ndocuments, and generate responses for each (context, question) instance with an LLM.\nMS Marco is an open-domain question answering dataset sourced from Bing\nsearch engine user query logs. Each question is associated with 10 context passages retrieved via\nBing web search. Human annotators compose a response based on the provided context documents,\nand label the documents utilized in the response as relevant. We sample data from the original version\nof the dataset, comprising 80k train, 10k validation, and 10k test samples. As with other datasets, we\nignore the human annotated answers and generate responses with an LLM in RAG setting.\nCUAD is a collection of commercial legal contracts with expert annotated questions\nand responses. The contracts are sourced from a public legal contract library(EDGAR) and range\nfrom 1-100 pages in length. Experts in the legal domain compose multiple questions per contract\nand label the relevant parts of the contract that are useful for answering the questions. There are\n21k questions pertaining to 510 documents in total. The questions are very specific to each contract,\nthus we don't perform additional retrieval over the contract corpus, and form RAG examples with 1\ncontext contract each for our benchmark. Due to high anntoation costs associated with long-context\nRAG, we sample 5 question per doc. As with other datasets, we generate responses with an LLM in\nRAG setting.\nDelucionQA is a domain-specific RAG dataset leveraging Jeep's 2023 Gladiator\nmodel manual as the source of knowledge. The questions and answers are automatically generated by\nlarge language models. RAG context passages are retrieved from the Jeep car manual via both sparse\nand dense retrieval methods to add variance in the sample distribution. Further, MTurk workers\nannotate whether or not responses are supported by the context.\nUpon closer inspection, we found only 1 relevant passage associated with each question in the\nDelucionQA dataset. To make the dataset more challenging for RAGBench, we build a vector\ndatabase from the 1,046 context passages in DelucionQA and and retrieve up to 3 context documents\nper question from it. We use text-embedding-ada-002 embeddings from OpenAI to build the\ndatabase. There are 913 unique questions in DelucionQA. For each resulting (context, question)\nsample, we generate responses with an LLM.\nEManual is a question answer dataset comprising consumer electronic device\nmanuals and realistic questions about them composed by human annotators. The subset made\navailable at the time of writing amounts to 659 unique questions about the Samsung Smart TV/remote\nand the accompanying user manual, segmented into 261 chunks. To form a RAG dataset, we embed\nthe manual segments into a vector database with OpenAI embedding and retrieve up to 3 context\ndocuments per question from it. For each resulting (context, question) sample, we generate responses\nwith an LLM."}, {"title": "FinQA", "content": "FinQA is a QA dataset of financial report passages and associated questions. Questions\nare curated such that numerical reasoning over multiple unstructured and tabular inputs is required to\narrive at the answer. FinQA totals 8,281 financial QA pairs, split between train, validation, and test\nsplits. We retain the original splits and generate 2 LLM responses per each context-query example in\nFinQA.\nTAT-QA is another financial QA dataset that requires numerical reasoning over\ntables and text. The data are sourced from 500 financial reports released on https://www.\nannualreports.com/. Expert annotators with background in finance annotate question-answer\npairs based on the available documents. We leverage the full dataset (13k train, 1.6k validation and\ntest) but generate new responses with LLMs for RAGBench.\nHAGRID is a QA dataset built on top of MIRACL [45], a multi-lingual information-\nretrieval dataset. HAGRID passes questions and relevant context documents from MIRACLE through\nan LLM to produce a response for each example in the dataset. Annotors then rate the response\non informativeness and attribution dimensions. The original context documents are sourced from\nWikipedia and associated questions are generated by expert annotators. Since HAGRID already\ncontains LLM-generated responses, we directly use them and don't generate additional responses for\nRAGBench.\nExpertQA is a collection of curated questions from domain-experts in various\nfields of sicence, arts, and law. The dataset also contains expert curated passsages relevant to each\nquestion, alongside LLM-generated responses. As with HAGRID, we leverage the LLM-generated\nresponses in ExpertQA directly for our RAG dataset."}, {"title": "9.3 Response Generation Prompt", "content": "We use the following prompt template to generate LLM responses for each sample in RAGBench.\nContext documents, separated by line breaks, along with the question are slotted in for each generation\nsample.\nUse the following pieces of context to answer the question.\n{documents}\nQuestion: {question}"}, {"title": "9.4 GPT Labeling Prompt", "content": "We use the following prompt template to generate annotations with GPT-4\nI asked someone to answer a question based on one or more documents.\nYour task is to review their response and assess whether or not each sentence\nin that response is supported by text in the documents. And if so, which\nsentences in the documents provide that support. You will also tell me which\nof the documents contain useful information for answering the question, and\nwhich of the documents the answer was sourced from.\nHere are the documents, each of which is split into sentences. Alongside each\nsentence is associated key, such as 'Oa.' or 'Ob.' that you can use to refer\nto it:\n\u060c \u060c \u060c\n{documents}\n\u060c \u060c \u060c\nThe question was:\n\u060c \u060c \u060c\n{question}\n\u060c \u060c \u060c"}, {"title": "9.5 Annotation Post-Processing Steps", "content": "As shown in Appendix 9.4, we request very detailed annotations with explanations from GPT-4-turbo.\nWe pivot on chain-of-thought [40] and redundancy to encourage high quality labels from the annotator\nmodel.\nFor Adherence, we request both response-level and sentence-level annotations that we compare in\npost-processing to identify inconsistencies where GPT-4 disagrees with its own judgements. For\nexample, if GPT-4 claims a response as supported by the context as a whole, but identifies no\nsupporting information for one or more claims in the response, we send the example for re-annotation.\nWe re-annotate all data up to 3 times, after which a fraction (<2%) of the data are still conflicting.\nAfter manual inspection, we find that the majority of the conflicts arise from partially hallucinated\nsentences that are somewhat, but not fully, grounded in the context. We leverage a sentence-level\n\"fully_supported\" boolean annotation to identify and resolve such cases. According to our annotation\nschema, we treat all partially supported sentences as hallucinations.\nSince all TRACe metrics are related, we qualitatively observe that taking the extra measures for\nAdherence also positively impacts the quality and stability of the relevance and utilization labels."}, {"title": "9.6 DeBERTa model training", "content": "We train the model on a Google Cloud Platform A-100 GPU instance for 3 epochs with initial learning\nrate 5-6 for the base model layers and 2-5 for the heads, with warmup and a linear decay rate."}, {"title": "9.7 OOD DeBERTa", "content": "We evaluate generalizability of a fine-tuned DeBERTa model to Out-of-Domian (OOD) data. For\nthis evaluation, we train DeBERTA on the general knowledge subset of RAGBench. This subset\nincludes academic datasets that are less aligned with real-world industry used cases (e.g. compared\nto customer service subset). With the exception of FinQA and TAT-QA, we find that the model still\nachieves reasonable generalization to the other domains in RAGBench. FinQA and TAT-QA are\nthe two financial numerical reasoning datasets in RAGBench. The tabular nature of the FinQA and\nTAT-QA datasets contribute to the poor performance of the OOD model as such format would not\nhave been seen in training."}]}