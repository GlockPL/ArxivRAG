{"title": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems", "authors": ["Robert Friel", "Masha Belyi", "Atindriyo Sanyal"], "abstract": "Retrieval-Augmented Generation (RAG) has become a standard architectural pattern for incorporating domain-specific knowledge into user-facing chat applications powered by Large Language Models (LLMs). RAG systems are characterized by (1) a document retriever that queries a domain-specific corpus for context information relevant to an input query, and (2) an LLM that generates a response based on the provided query and context. However, comprehensive evaluation of RAG systems remains a challenge due to the lack of unified evaluation criteria and annotated datasets. In response, we introduce RAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k examples. It covers five unique industry-specific domains and various RAG task types. RAGBench examples are sourced from industry corpora such as user manuals, making it particularly relevant for industry applications. Further, we formalize the TRACe evaluation framework: a set of explainable and actionable RAG evaluation metrics applicable across all RAG domains. We release the labeled dataset at https://huggingface.co/datasets/rungalileo/ragbench. RAGBench explainable labels facilitate holistic evaluation of RAG systems, enabling actionable feedback for continuous improvement of production applications. Thorough extensive benchmarking, we find that LLM-based RAG evaluation methods struggle to compete with a finetuned ROBERTa model on the RAG evaluation task. We identify areas where existing approaches fall short and propose the adoption of RAGBench with TRACe towards advancing the state of RAG evaluation systems.", "sections": [{"title": "1 Introduction", "content": "Despite remarkable reasoning and conversational abilities, out-of-the-box pre-trained Large Language Models (LLMs) struggle to reason about out-of-domain, knowledge-intensive queries [21, 14]. In response, Retriever-Augmented Generation (RAG) systems [21, 20] are becoming increasingly popular in user-facing dialogue applications [35]. Generally, RAG systems comprise a retriever component that queries relevant documents from an in-domain corpus and a downstream LLM generator model that incorporates the retrieved documents along with the original user query to output an informed response. The additional context helps ground the LLM in factual information and has been shown to boost performance on knowledge-intensive tasks [21].\nStill, when used in production settings, RAG systems are prone to hallucinations as the generator model struggles to retrieve relevant information from the context [1, 31, 7]. In the absence of a one-fits-all approach, application-specific RAG systems must be fine-tuned for optimal performance on domain-specific tasks. However, the choice of retriever and generator models for each application is complex and has serious implications on overall system quality and costs. With numerous"}, {"title": "2 Related Work", "content": "RAG benchmarks Numerous general LLM evaluation benchmarks, such as ChatbotArena [46] have been proposed in past work. However, human preference datasets, constructed through pairwise comparisons, have limitations. While these data are appropriate for fine-tuning general purpose LLM judges, they are insufficient for building RAG evaluation systems because preference judgements under-represent important RAG dimensions like factuality and completeness of the response [13].\nCHATRAGBENCH [24] is a recent initiative that is similar in intent to our work in that it contributes a large-scale unified RAG benchmark. However, CHATRAGBENCH only contains ground truth responses and lacks the granular component-specific labels that we release with RAGBench. As future work, we can consider annotating CHATRAGBENCH with the schema proposed in this paper, to further scale RAGBench.\nRAGTruth [41] is another recent effort at a RAG Benchmark. RAGTruth combines QA, Data-toText, and Summarization RAG data with human annotated hallucinated spans in the response. While it is an excellent benchmark for hallucination detection, it does not offer the level of granularity we present with RAGBench that is necessary to understand the RAG system as a whole.\nRAG evaluation Recently, several parallel efforts have proposed approaches to automated RAG evaluation. In RAGAS [9], the authors query an LLM-judge (GPT-3.5) with a curated prompt to evaluate context relevance, answer relevance and faithfulness of a RAG response. Next, Saad-Falcon et al. [33] propose ARES, a framework for fine-tuning smaller NLI models to predict the same metrics. This approach benefits from fine-tuning, though domain-specific annotated validation sets are required for each domain adaptation. In parallel, Chen et al. [4] develop a heuristic system to probe LLM's robustness to noisy and irrelevant context documents, and Adlakha et al. [1] explore heuristic algorithms to estimate RAG correctness and faithfulness. The lack of established RAG"}, {"title": "3 RAGBench Construction", "content": "3.1 Component Datasets\nRAGBench is a collection of real-world datasets that span different domains and RAG task types. We source data from open-book Question-Answer (QA) datasets (CovidQA [27], PubmedQA [15], HotpotQA [42], MS Marco [29], CUAD [12], EManual [28], TechQA [3], FinQA [5], TAT-QA [47], ExpertQA [26], HAGRID [16]), as well one that was specifically adapted for RAG (DelucionQA [34]). We transform all 12 component datasets to a standardized RAG format with consistent annotations. To best represent real-world RAG scenarios, we vary a number parameters to construct the benchmark: the source domain, number of context documents, context token length, and the response generator Figure 1 illustrates where these variable parameters fall in the RAG pipeline.\nSource Domains RAGBench comprises five distinct domains: bio-medical research (PubmedQA, CovidQA), general knowledge (HotpotQA, MS Marco, HAGRID, ExperQA), legal contracts (CuAD), customer support (DelucionQA, EManual, TechQA), and finance (FinBench, TAT-QA). We select these specific domains based on availability of data, and applicability to real-world RAG applications across different industry verticals. For detailed descriptions of each component data source, refer to Appendix 9.2.\nContext Token Length Context token length in RAGBench ranges from 100 to 11k tokens, which we report in Table 1. Notably, CUAD documents feature long contexts of up to 11k tokens each, compared to the relatively short context in PubMedQA.\nTask Types We curate RAGBench to inlcude a variety of difficult RAG task types. Customer support datasets simulate a common application of RAG in industry settings. FinQA and TAT-QA require numerical reasoning over hybrid tabular and text data. HotpotQA, CovidQA, and PubMedQA"}, {"title": "3.2 TRACe Evaluation Framework", "content": "We propose a suite of four comprehensive metrics to evaluate the quality of the retriever and the response generator components of RAG. An optimal RAG system must balance accuracy and efficiency. The retriever should precisely return all the necessary information to address the user query, avoiding any superfluous data. The generator must effectively utilize the retrieved information, ensuring the response is strictly based on the provided context without introducing any hallucinations in the output.\nTowards comprehensive evaluation of the abovementioned criteria, we introduce the TRACe evaluation framework to measure uTilization, Relevance, Adherence, and Completeness of a RAG system. Utilization, Adherence, and Completeness measure the quality of the generator. Adherence here is synonymous with previously proposed answer faithfullness, groundednes, and attribution, all terms used in literature to measure how well an LLM output adheres to a source of factual information. Relevance measures the quality of the retriever output with respect to the query. Below we formalize the definition of each metric.\nDefinitions Let \\(D\\) be a set of context documents \\({d_1...d_n}\\) retrieved for a RAG input query. We define a set of relevant tokens in \\(d_i\\) as \\(R_i = {t_1, ...t_r}\\). \\(R_i\\) encodes information in context document \\(d_i\\) that is useful for answering the query. Similarly, we define \\(U_i = {t_1, ...t_u}\\) as the set of utilized tokens in document \\(d_i\\), which reflect information that the generation model is using to produce a response. Refer to Figure 3 for a visual representation of relevant and utilized spans. \\(Len(x)\\) measures the length of strings in \\(x\\), which can be interpreted as character length, token length, or sentence length. For calculating ground-truth metrics, we employ sentence-length, since it aligns best with our annotation schema (Section 3.3). However, token or character length may also be suitable for other use cases.\nContext Relevance Context Relevance is defined in [9, 33] as the fraction of the retrieved context that is relevant to the input query. Low relevance points to an inefficient retriever that supplies excess"}, {"title": "4 Annotation Validation", "content": "We validate out metric formulations and labeling approach on simulated RAG datasets of varying quality. We use mock RAG datasets generated by Saad-Falcon et al. [33] for this analysis. Their RAG validation set is sampled from KILT [30], including Natural Questions (NQ)[18], HotpotQA[42], FEVER[36], and Wizards of Wikipedia (WoW) [8] datasets. The authors synthetically generate systems of varying quality by adjusting the ratio of relevant documents and responses in the data. We sample 500 examples from each simulated RAG dataset and annotated them as described in section 3.3. Next, we calculate average annotated context relevance and adherence scores for each dataset and use those to rank the mock systems. We compare our rankings to ground truth with the Kendall rank correlation (Kendall's \\(\\tau\\)) metric, which evaluates the agreement between two sets of ranks on a scale from 0 (no agreement) to 1 (perfect agreement).\nAs shown in Table 2, the GPT-4 annotations achieve high Kendall's \\(\\tau\\) ranging from 0.78 to 1. For a fair comparison with the ground truth labels, we derive binary context relevance and labels from the GPT-4 annotations by thresholding the example Relevance score (equation 2) at 0. For comparison, we also report ranking results with out more granular example-level Relevance scores that range from 0-1. We find that these metric produce a different ranking (see lower Kendall's \\(\\tau\\) in Table 2), which we attribute to the metrics capturing differences in retrieved context length across the different examples."}, {"title": "5 Experiments", "content": "5.1 LLM Judge\nWe benchmarks a few LLM evaluators on RAGBench: (1) zero-shot GPT-3.5-judge, where we query GPT-3.5 with our annotation prompt, (2) RAGAS [9], and (3) TruLens [37]. RAGAS employs a series of few-shot prompts to GPT-3.5 to measure answer groundedness (Adherence) and Context Relevance metrics. Trulens is another zero-shot prompting approach that measures answer faithfulness (Adherence) and Context Relevance.\n5.2 Fine-tuned Judge\nWe fine-tune a DeBERTa-v3-Large [10] NLI checkpoint from Laurer et al. [19] with one key architecture modification: we add a shallow prediction head for each of the output RAG metrics, which allows us to compute all TRACe metrics in a single forward pass. This is both cost-effective and enables transfer learning from head to head through back-propagation down to the shared base layers. Each prediction head is a single layer feed-forward net that acts on the token-level output of the last DeBERTa layer.\nWe attach two heads on the context tokens to estimate Relevance and Utilization probabilities, and another head on the response tokens to estimate Adherence. For training, we broadcast sentence-level annotations to tokens, and tune to maximize token-level probabilities of Relevant, Utilized, and"}, {"title": "6 Discussion", "content": "Table 3 reports results on test splits of each RAGBench component dataset. We compare baseline LLM methods with a finetunes DeBERTA encoder that trained on the full RAGBench train split.\nLLMs underperform on the RAG evaluation task We observe that the finetuned DeBERTa model outperforms the few/zero-shot LLM-judge baselines on most datasets. While GPT-3.5 demonstrates competitive performance with DeBERTa on a few metrics, DeBERTa consistently achieves superior performance metrics across all evaluations. Despite the versatility of LLM judges across various tasks, their lack of specialization necessitates finetuning for optimal results. Future work may focus on finetuning LLM judges to close the gap between DeBERTa and GPT-4 evaluation performance. In Appendix 9.7, we demonstrate that, despite its small size, the finetuned DeBERTA model does generalize to out of domain RAG datasets in the same way that LLM-based approaches do.\nEstimating Context Relevance is Difficult As shown in Table 3, Relevance RMSE scores are generally higher than those for Utilization, indicating a greater difficulty in the relevance prediction task. Utilization can be assessed through a straightforward semantic comparison between the context and the response. In contrast, relevance is a more intricate metric. Due to the nature of RAG, the majority of retrieved documents are semantically related to the query. However, mere semantic similarity is insufficient. The model must ascertain whether the provided context includes specific information necessary to accurately answer the question. Thus, the task inherently involves deriving"}, {"title": "7 Conclusion", "content": "In this paper we introduce RAGBench, a large-scale dataset composed of real-world RAG examples intended for training and benchmarking RAG evaluation models. Additionally, we formulate TRACe, a RAG evaluation framework comprising four metrics: uTilization, Relevance, Adherence, and Completeness. TRACe standardizes the evaluation process, offering a consistent and systematic approach to measuring RAG system performance across various dimensions.\nWe benchmark existing RAG evaluation framework using RAGBench and demonstrate that LLM-judges struggle to compete with a fine-tuned RAG evaluation expert model. Future work may involve fine-tuning larger expert models to explore the potential for narrowing the performance gap between these models and the ground truth.\nOur contributions address the need for standardized benchmarks and methodologies, enabling more precise and actionable insights into the strengths and weaknesses of different RAG systems. This, in turn, will facilitate the iterative improvement of RAG models, driving forward the capabilities of retrieval-augmented generation in real-world applications."}, {"title": "8 Limitations", "content": "LLM Annotations Though LLMs demonstrate high correlations with human judgements on a variety of tasks [6, 11], using them as a singular source of ground truth remains controversial [23]. At the same time, human judgements of LLM outputs are also prone to inconsistencies and bias. In [13], the authors find that human evaluators are often misled by the assertiveness and complexity of the LLM model output, which leads them to underestimate the rate of factuality errors in LLM responses.\nIn this work, we acknowledge the potential of noise and bias in RAGBench resulting from automated GPT-4-turbo annotations, and the concerns about the potential transmission of such biases into subsequent RAG systems. One way to address this in future may be to replace the GPT-4-annotator with and LLM \"jury\" as suggested in [38]. By aggregating judgements from diverse models, this approach can help reduce the noise and bias in the output judgements at low cost."}, {"title": "9 Appendix", "content": "9.1 RAGBench Code and Data\nWe release RAGBench data on Hugginggface: https://huggingface.co/datasets/rungalileo/ragbench. Refer to model card and documentation there.\nWe publish our inferfence and evaluation code on Gihub: https://github.com/rungalileo/ragbench/tree/main/ragbench.\n9.2 RAGBench Dataset Details\nRAGBench is sourced from publicly released acadmic and industry datasets. As far as we know, none of the component datasets contain personally identifiable information or offensive content.\nPubMedQA [15] PubMedQA is a collection of PubMed research abstracts with corresponding yes/no/maybe questions paired with each abstract. The original dataset comprises 3 subsets: PQA-L, PQA-U, and PQA-A, with 1k, 60k, and 210k abstracts, respectively. For all subsets, the question is derived from the title of the PubMed article using rule-based heuristics. Long answers are automatically derived from the last sentence of the abstract for PQA-L and PQA-U, and QA-L answers are further reviewed by expert annotators and annotated as yes/no/maybe. PQA-A comprises exclusively automatically generated questions and short answers.\nFor RAGBench we utilize the PQA-U subset and re-frame it from QA into a RAG task. To simulate RAG, we leverage already segmented PQA-U abstracts context chunks and we encode them into a vector DB with OpenAI embeddings. The size of the resulting DB is 200k. We retrieve 4 chunks for each PQA-U question using FAISS with eucledian distance as the similarity function. We ignore the responses and labels in the original dataset and generate new responses with an LLM.\nCovidQA-RAG CovidQA-RAG is a combination of 2k expert-annotated questions sourced from COVID-QA [27] and a vector database of 250,000 100-word passages built by Siriwardhana et al. [35]. Both questions and answers are sourced from CORD-19 [39] collection of research articles about COVID-19.\nWe embed the questions and database passages with OpenAI embeddings and retrieve up to N passages for each COVID-QA question from the vector database using FAISS with eucledian distance as the similarity function and max_distance=0.25. We generate responses for each resulting RAG (context, question) instance with an LLM."}]}