{"title": "PSMGD: Periodic Stochastic Multi-Gradient Descent for Fast Multi-Objective\nOptimization", "authors": ["Mingjing Xu", "Peizhong Ju", "Jia Liu", "Haibo Yang"], "abstract": "Multi-objective optimization (MOO) lies at the core of many\nmachine learning (ML) applications that involve multiple,\npotentially conflicting objectives (e.g., multi-task learning,\nmulti-objective reinforcement learning, among many others).\nDespite the long history of MOO, recent years have witnessed\na surge in interest within the ML community in the develop-\nment of gradient manipulation algorithms for MOO, thanks\nto the availability of gradient information in many ML prob-\nlems. However, existing gradient manipulation methods for\nMOO often suffer from long training times, primarily due\nto the need for computing dynamic weights by solving an\nadditional optimization problem to determine a common de-\nscent direction that can decrease all objectives simultaneously.\nTo address this challenge, we propose a new and efficient al-\ngorithm called Periodic Stochastic Multi-Gradient Descent\n(PSMGD) to accelerate MOO. PSMGD is motivated by the\nkey observation that dynamic weights across objectives ex-\nhibit small changes under minor updates over short intervals\nduring the optimization process. Consequently, our PSMGD\nalgorithm is designed to periodically compute these dynamic\nweights and utilizes them repeatedly, thereby effectively re-\nducing the computational overload. Theoretically, we prove\nthat PSMGD can achieve state-of-the-art convergence rates for\nstrongly-convex, general convex, and non-convex functions.\nAdditionally, we introduce a new computational complexity\nmeasure, termed backpropagation complexity, and demon-\nstrate that PSMGD could achieve an objective-independent\nbackpropagation complexity. Through extensive experiments,\nwe verify that PSMGD can provide comparable or superior\nperformance to state-of-the-art MOO algorithms while signifi-\ncantly reducing training time.", "sections": [{"title": "1 Introduction", "content": "Just as the human world is full of diverse and potentially\nconflicting goals, many machine learning paradigms are also\nmulti-objective, such as multi-task learning (Caruana 1997;\nSener and Koltun 2018), multi-objective reinforcement learn-\ning (Hayes et al. 2022), learning-to-rank (Mahapatra et al.\n Formally, MOO can be mathematically cast as:\nmin F(x) := [f1(x),\u2026, fs(x)],  (1)\nXED\nwhere x \u2208 D C Rd is the model parameter, and fs : Rd \u2192\nR, s \u2208 [S] is the objective function.\nIn this paper, we present Periodic Stochastic Multi-\nGradient Descent (PSMGD), a simple yet effective algorithm\nto accelerate the MOO process. Our PSMGD is motivated by\nthe key observation that dynamic weights across objectives"}, {"title": "2 Related works", "content": "The MOO problem, as stated in Eq. 1, has a long and rich\nhistory that dates back to the 1950s. MOO algorithms can be\nbroadly categorized into two main groups. The first category\ncomprises gradient-free methods, such as evolutionary MOO\nalgorithms and Bayesian MOO algorithms (Zhang and Li\n2007; Deb et al. 2002; Belakaria et al. 2020; Laumanns and\nOcenasek 2002). These methods are more suitable for small-\nscale problems but are less practical for high-dimensional\nMOO models, such as deep neural networks. The second cat-\negory is the gradient-based approach by utilizing (stochastic)\ngradients (Fliege and Svaiter 2000; D\u00e9sid\u00e9ri 2012; Fliege,\nVaz, and Vicente 2019; Liu and Vicente 2021), making them\nmore practical for high-dimensional MOO problems. In this\nwork, we primarily focus on gradient-based approaches to\nsolve MOO in high-dimensional deep-learning models. In\nwhat follows, we provide a brief overview of two typical ap-\nproaches for gradient-based methods for MOO: linear scalar-\nization (LS) and gradient manipulation methods.\n1) LS: A straightforward approach for MOO 1 is to trans-\nform it into a single objective problem by using pre-defined\nweights {\\s, 8 \u2208 [s]}: minxed Ese[s] dsfs(x). Thanks to\nthe LS, one can leverage many existing and familiar single-\nobjective methods, such as stochastic gradient descent, for\ntraining at each iteration. However, in LS, it is not uncommon\nto see conflicts among multiple objectives during the opti-\nmization process, i.e., (\u2207fs(x), \u2207 fs, (x)) < 0. This implies\nthat the update using static weights may decrease some ob-\njectives, while inevitably increasing others at the same time\nand leading to slow convergence and poor performance.\n2) Gradient Manipulation Methods: A popular alterna-\ntive is to dynamically weight gradients across objectives to\navoid such conflicts and obtain a direction to decrease all\nobjectives simultaneously. For example, multiple gradient de-\nscent algorithm (MGDA) (Fliege and Svaiter 2000) is seeking\nto find the common descent direction by solving the follow-\ning optimization problem given each objective's gradient:\n(d, \u03b2) \u2208 arg mind\u2208RS,\u00df\u2208RB + \u00bd||d||2, s.t., \u2207 fs(x)Td\n\u03b2 \u2264 0,\u2200s \u2208 [S]. By doing so, if x represents a first-order\nPareto stationary point, then (d, \u03b2) = (0,0). Otherwise, we\ncan find the d such that Vfs(x)Td < \u03b2 < 0,s \u2208 [S] as\nthe solution. Using such a non-conflicting direction d to up-\ndate the model, i.e., x \u2190 x + nd where \u03b7 is the learning\nrate, has been shown to achieve better performance in prac-\ntice (Sener and Koltun 2018; Liu et al. 2021a). Following\nthis token, many gradient manipulation methods attempt to\nobtain a common descent direction through various formu-\nlations and solutions. For example, GRADDROP (Chen et al.\n2020) randomly dropped out highly conflicted gradients, Ro-\ntoGrad (Javaloy and Valera 2021) rotated objective gradients\nto alleviate the conflict, and many other methods exploring\nsimilar principles (Kendall, Gal, and Cipolla 2018; Chen et al.\n2018; Yu et al. 2020; Liu et al. 2021a; Liu and Vicente 2021;\nChen et al. 2024; Xiao, Ban, and Ji 2024). However, due to\nthe additional optimization process required to generate such\nd, gradient manipulation methods often incur an expensive\nper-iteration cost, thereby prolonging training time. In this\nwork, we propose a new algorithm, PSMGD, which com-\nbines the advantages of both approaches and offers a fast\nper-iteration runtime, a fast overall convergence speed, and\nimproved model performance."}, {"title": "3 Periodic stochastic multi-gradient descent", "content": "In this section, we first present the basic concept of MOO, fol-\nlowed by a pedagogical example comparing linear scalariza-\ntion with MGDA, a representative of gradient manipulation\nmethods. Based on one observation of the stable variation\nof dynamic weights for MGDA, we propose the PSMGD\nalgorithm, followed by its convergence analyses.\n3.1 Preliminaries of MOO\nAnalogous to the stationary and optimal solutions in single-\nobjective optimization, MOO seeks to adopt the notion of\nPareto optimality/stationarity:\nDefinition 1 ((Weak) Pareto Optimality). For any two solu-\ntions x and y, we say x dominates y if and only if fs(x) <\nfs(y), s \u2208 [S] and fs(x) < fs(y),\u2203s \u2208 [S]. A solution x\nis Pareto optimal if it is not dominated by any other solution.\nOne solution x is weakly Pareto optimal if there does not\nexist a solution y such that fs(x) > fs(y), \u2200s \u2208 [S].\nDefinition 2 (Pareto Stationarity). A solution x is said to\nbe Pareto stationary if there is no common descent direction\nd \u2208 Rd such that \u2207 fs(x)d < 0, \u2200s \u2208 [S].\n Definition 2, if x is not a Pareto stationary point, we can find a"}, {"title": "3.2 A pedagogical example", "content": "We consider the Fonseca problem(Fonseca and Fleming\n1996; Pardalos et al. 2017), which is to solve a two-\nobjective optimization problem, minx[f1(x), f2(x)].\nThese two objective functions are defined by\nf1(x)\nexp\n1\nd\nXd\n(-= (-)\u00b2),\n\u03a3i=1 x2i\nf2(x)\n= 1-exp (-(+))),\n\u03a3i=1 (xi\u2212d\u221ad)2\nwhere x = (X1, X2, ..., Xd)\nrepresenting the d-dimensional decision variable. The\nproblem exhibits a non-convex Pareto front in the objective\nspace. We run three algorithms on this pedagogical example:\nlinear scalarization, MGDA, and our proposed PSMGD.\nSetting. We run each algorithm 10 times. In Figure 1, we\nshow the trajectory of different methods from random starting\npoints. The blue dashed lines represent the trajectories, with\nthe final solutions highlighted by red dots.\nObservation 1: Linear scalarization can only converge to\nspecific points and is unable to explore different trade-offs"}, {"title": "3.3 PSMGD", "content": "Based on our observations, we propose a new and efficient\nalgorithm called Periodic Stochastic Multi-Gradient Descent\n(PSMGD) for MOO, as shown in Algorithm 1. Our PSMGD\nalgorithm is designed to periodically compute these dynamic\nweights and utilizes them repeatedly, thereby reducing the\ncomputational load effectively. Specifically, in each iteration\nt \u2208 [T], we offer two options: 1) If t%R == 0, we calculate\nthe dynamic weights \u00c2 based on the stochastic gradient of\neach objective \u2207 fs(xt, \u00a7t), s \u2208 [S], by solving optimization\nproblem 2. Following this, we apply momentum to A to\nfurther stabilize the weights: \u5165t = att\u2212R + (1 \u2212 at).\n2) Otherwise, we reuse the dynamic weights: At = At-1.\nWith predefined At, it is worth pointing out that the MOO\ncan be naturally transformed into a single-objective problem\nby using pre-defined weighted sum of the objectives, thereby\nrequiring only one backpropagation step. Subsequently, after\nobtaining the weights across objectives, we can approximate\nthe common descent direction by dt = A\u2207F(xt, t). Then\nthe model can be updated by Xt+1 = Xt Ntdt."}, {"title": "3.4 Convergence analysis", "content": "Before providing the convergence analysis, we have the fol-\nlowing assumptions:\nAssumption 3.1 (L-Lipschitz continuous). There exists a\nconstant L > 0 such that ||\u2207fs(x) \u2013 \u2207fs(y) || \u2264 L||x\ny||, \u2200x, y \u2208 Rd, s \u2208 [S].\nAssumption 3.2 (Unbiased stochastic gradient with bounded\nvariance). We assume the stochastic gradient estimation of\neach objective function is unbiased with bounded variance.\nE [Vfs (x, \u00a7)] = Vfs(x), (3)\nE [||\u2207fs (x, \u00a7) \u2013 \u2207fs(x)||2] \u2264 \u03c3\u00b2, s \u2208 [S]. (4)\nAssumption 3.3 (Bounded Weights). There exists a constant\nB s.t., 0 \u2264 >t,s < \u0392,\u2211s\u2208[s] dt,s \u2265 1,\u2200s \u2208 [S], t \u2208 [T].\nAssumption 3.4 (Bounded Gradient). The gradient of each\nobjective is bounded, i.e., there exists a constant H > 0 such\nthat ||f(x)|| \u2264 H,\u2200s \u2208 [S].\nTheorem 3.5 (Non-Convex Functions). Under Assump-\ntions 3.1- 3.4, when each objective is bounded by F (fs(x) \u2264\nF, s \u2208 [S]), the sequence of iterates generated by the PSMGD\nAlgorithm in non-convex functions satisfies:\n1\n\u03a3\u0395[||G(xt, \u03bb\u03b5)||2] <\nte[T]\n4SBF 2F\n+\n\u03a4\u03b7\u03c4\nAt\n\u03a3 (1 \u2013 \u03b1\u03c4) -\n(5)\n+ 2LS2B202\u03a3 \u0395 [\u03a3\u20ac[5] (1 \u2013 \u03b1\u03b9) \u2013 -R]\nt%R==0\nSetting 1 = min{, At maxs\u2208 [5] |--R\nNt \u221aT ' Nt \u03b71}Nt =\u03a3 (1 \u2013 \u03b1\u03c4),\nt%R==0\nO(7), the convergence rate is O(\u30c6)."}, {"title": "Theorem 3.6 (General Convex Functions)", "content": "Theorem 3.6 (General Convex Functions). Under Assump-\ntions 3.1- 3.4, when the distance from sequence to Pareto set\nis bounded (||x+ \u2212 x*|| < D), the sequence of iterates gener-\nated by the PSMGD in general convex functions satisfies:\n1\n< \u03a3\u0395[G(x, t) \u2013 G(x, t)]\nte[T]\n||X1 - X* ||2\n\u03a4\u03b71\n1\nT\n(6)\n+2D6S3/2\u0392\u03a3 (1-t)\nt%R==0\n+(2S2B20\u00b2 + 2S2B2H2)\u03a3.\nSetting Nt = 1/sqrt(T) and (1\nte [T]\n- at) = nt, the convergence rate\nis O(1/\u221aT)."}, {"title": "Theorem 3.7 (\u03bc-Strongly Convex Functions)", "content": "Theorem 3.7 (\u03bc-Strongly Convex Functions). Under As-\nsumptions 3.1- 3.4 and we further assume each objective\nfunction fs(x), s \u2208 [S] is \u00b5-strongly convex and each ob-\njective is bounded by F (fs(x) < F, s \u2208 [S]), the sequence\nof iterates generated by the PSMGD satisfies:\nE[G(xt+1, At+1) \u2013 G(x+1, +1)]\n\u2264 (1 \u2013 2\u03bc\u03b7t)E[G(xt, At) \u2013 G(x,t)] + \u03b7 \u03a6, (7)\nwhere \u0424 = 2LS2B20\u00b2 + 4FSB1{(t + 1)%R = 0} +\n4S3/2B2H\u03c31{t%R = 0} and 1 is the indicator function.\nSet n = with c>, the convergence rate is\nE[G(x\u0442, \u0442) \u2013 G(x, \u03c4)]\n<max{2c\u00b2\u03a6\u0384 (2\u00b5c \u2212 1) \u22121, G(xo, \u03bb\u03bf) \u2013 G(x, \u03bb\u03bf)}\n= O(1/T),\n(8)\nwhere \u03a6' = 2LS2B\u00b2\u03c3\u00b2 + 4FSB/R+ 4S3/2B2H\u03c3/R."}, {"title": "Remark 3.9.", "content": "Our PSMGD algorithm can achieve BP com-\nplexity of O(+ (R1)) for stongly-convex functions and\nO(R+R) for general convex and non-convex func-\ntions. Compared to existing MOO methods, PSMGD can\nachieve a linear speedup in terms of R."}, {"title": "4 Experiment", "content": "In this section, we conduct a comprehensive empirical evalu-\nation of the proposed PSMGD algorithm, focusing primarily\non multi-task learning with deep neural networks. The pri-\nmary goal is to verify the following key points:\n\u2022 Improved Performance: Can PSMGD provide improved\nmodel performance?\n\u2022 Fast Training: Can PSMGD have a fast training process?\n\u2022 Ablation Study: How does the hyper-parameter impact the\ntraining? (Results In Appendix)\nBaseline Algorithms. To conduct an extensive compari-\nson, we use 14 algorithms as the baselines, including a single\nobjective learning baseline, linear scalarization, and 12 ad-\nvanced MOO algorithms. They are: (1) Single task learning\n(STL), training independent models fs(x) for all objectives;\n(2) Linear scalarization (LS) baseline; (3) Scale-invariant (SI)\nthat minimizes se[s] log fs(x); (4) Dynamic Weight Aver-\nage (DWA) (Liu, Johns, and Davison 2019) adaptively up-\ndates weights based on the comparative rate of loss reduction\nfor each objective; (5) Uncertainty Weighting (UW) (Kendall,\nGal, and Cipolla 2018) leverages estimated task uncertainty\nto guide weight assignments; (6) Random Loss Weighting\n(RLW) (Lin, Feiyang, and Zhang 2021) samples objective\nweighting with log-probabilities by normal distribution; (7)\nMGDA (Sener and Koltun 2018) identifies a joint descent\ndirection that concurrently decreases all objectives; (8) PC-\nGRAD (Yu et al. 2020) projects each objective gradient onto\nthe normal plan of the gradients to avoid conflicts; (9) CA-\nGRAD (Liu et al. 2021a) balances the average loss while\nguaranteeing a controlled minimum improvement for each\nobjective; (10) IMTL-G (Liu et al. 2021b) determines the\nupdate direction by having equal projections on objective\ngradients; (11) GRADDROP (Chen et al. 2020) randomly\ndrops out certain dimensions of the objective gradients for\ntheir level of conflict; (12) NASHMTL (Navon et al. 2022)\ndetermines the game's solution that is advantageous for all\ngoals by a bargaining game; (13) FAMO (Liu et al. 2024) is\na dynamic weighting method that reduces objective losses in\nspace and time in a balanced manner; (14) FAIRGRAD (Ma-\nheshwari and Perrot 2022) aims to achieve group fairness in\nMOO by dynamically re-weighting; (15) SDMGRAD (Xiao,\nBan, and Ji 2024) utilizes direction-oriented regularization."}, {"title": "4.1 Regression: QM-9", "content": "QM-9 dataset (Blum and Reymond 2009) is a crucial bench-\nmark widely used in the field of graph neural network learn-\ning, especially in chemical informatics applications. With\nover 13K molecules, each molecule is intricately depicted as\na graph, with nodes representing atoms and edges symboliz-\ning the chemical bonds between them. These graphs feature\ndetailed node and edge characteristics such as atomic number,\natom type, bond type, and bond order. The goal is to predict\n11 molecule properties. Utilizing 110K molecules from QM9\nin PyTorch Geometric (Fey and Lenssen 2019) for training,\n10K for validation, and the remaining 10K for testing.\nPerformance. We assess two common metrics, MR and\nAm%, for various MOO algorithms to ensure effective eval-\nuations. Table 2 shows that PSMGD achieves comparable\nor even better performance in both MR and Am% when\ncompared with existing baselines. Specifically, it has the best\nperformance in 3 out of the 11 total tasks, namely a, \u20acLUMO,\nand cv. When we take a closer look at the training process, we\nobserve that PSMGD has a faster and more smooth process\nas illustrated in Figure 2. Specifically, PSMGD demonstrates\na smoother descent towards lower test loss at least in the early\nstages. In addition, PSMGD exhibits resistance to overfitting,"}, {"title": "4.2 Dense Prediction: NYU-v2", "content": "NYU-v2 (Silberman et al. 2012) is an indoor scene dataset\ncontaining 1449 RGBD images, each annotated with dense\nper-pixel labeling across 13 distinct classes. The dataset sup-"}]}