{"title": "More is More: Addition Bias in Large Language Models", "authors": ["Luca Santagata", "Cristiano De Nobili"], "abstract": "In this paper, we investigate the presence of additive bias in Large Language Models (LLMs), drawing a parallel to the cognitive bias observed in humans where individuals tend to favor additive over subtractive changes [3]. Using a series of controlled experiments, we tested various LLMs, including GPT-3.5 Turbo, Claude 3.5 Sonnet, Mistral, MathStral, and Llama 3.1, on tasks designed to measure their propensity for additive versus subtractive modifications. Our findings demonstrate a significant preference for additive changes across all tested models. For example, in a palindrome creation task, Llama 3.1 favored adding letters 97.85% of the time over removing them. Similarly, in a Lego tower balancing task, GPT-3.5 Turbo chose to add a brick 76.38% of the time rather than remove one. In a text summarization task, Mistral 7B produced longer summaries in 59.40% to 75.10% of cases when asked to improve its own or others' writing. These results indicate that, similar to humans, LLMs exhibit a marked additive bias, which might have implications when LLMs are used on a large scale. Addittive bias might increase resource use and environmental impact, leading to higher eco-nomic costs due to overconsumption and waste. This bias should be con-sidered in the development and application of LLMs to ensure balanced and efficient problem-solving approaches.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) present substantial opportunities as tools to aid a growing variety of decision-making processes. However, because they are trained on data generated by humans, LLMs are known to inherit societal biases and can exhibit biases that closely resemble cognitive biases, defined as system-atic and erroneous response patterns in judgment and decision-making [18]. Such human-like biases have the potential to hinder the fairness and transparency of decisions made with the help of LLMs. Adams et al. [3] conducted a series of experiments to explore a cognitive phenomenon known as the addition bias in hu-man participants. This bias was examined in scenarios where problems could be resolved by either adding or removing elements. Additive transformations result in a state with more elements than the original, while subtractive transforma-tions lead to a state with fewer elements [20]. A key finding from Adams et al.'s work was that people tend to add rather than remove elements when modifying ideas, objects, or situations. This tendency was observed across various tasks, such as stabilizing a Lego structure, improving a miniature golf course, creating symmetry within a grid, or rewriting an article summary. Interestingly, partici-pants often chose to add elements even when a subtractive solution would have been simpler and required fewer steps. Additionally, instructions to \"improve\" a design amplified the addition bias more than instructions to \"worsen\" a design. In this paper, we extend this line of inquiry to LLMs, investigating whether they exhibit a similar additive bias. We conducted a series of experiments designed to test the tendency of LLMs to favor additive over subtractive changes across various tasks. These experiments included creating palindromes from strings, balancing Lego towers, modifying recipes with unusual ingredients, improving soup recipes with varying numbers of ingredients, and revising text summaries. We tested several prominent LLMs, including GPT-3.5 Turbo, Claude 3.5 Son-net, and Llama 3.1 70B, among others. Our research aims to uncover whether these AI models, trained on human-generated data, have inherited the human tendency towards additive problem-solving, and to explore the implications of such a bias for AI-assisted decision-making and problem-solving processes.\nRQ: Do Large Language Models exhibit an additive bias similar to humans when solving problems or generating content, and if so, how does this bias man-ifest across different tasks and models?"}, {"title": "2 Related works", "content": "LLMs have demonstrated remarkable abilities in various tasks, such as document summarization [21], solving math problems [9], and providing chat support [11]. This has led to their growing use for assistance and advice in daily decision-making [16, 12]. However, these models are not immune to algorithmic biases [2], highlighting the need for strategies to evaluate and mitigate these issues [23, 15, 13]. During training, LLMs can encode societal biases related to race, gender, and other sensitive areas, potentially generating outputs that reinforce harmful stereotypes or discriminatory views. A common example is the tendency of LLMs to associate certain professions or traits with specific genders or ethnic-ities, such as linking engineering with male pronouns and nursing with female pronouns [10]. Additionally, biases can surface in text generated on sensitive top-ics like politics, religion, or social issues [14, 7, 1, 13]. In addition to societal bias, LLMs can show answer patterns similar to human-like cognitive bias [4], which can implicitly mislead a user's decision-making [17]. Cognitive bias refers to a systematic pattern of deviation from norms of rationality in judgment, where individuals create their own \"subjective reality\" from their perception of the input [8], [19].\nThree of the eight experiments conducted by [3] were replicated in [6], con-firming the presence of the addition bias. Further, [22] demonstrated that the addition bias extends beyond behavioral manifestations and is also evident in language. A frequency analysis of the Corpus of Contemporary American En-glish [5] revealed that words associated with increasing quantities, such as \"add\" and \"more\", are more common in English than those associated with decreasing quantities, such as \u201csubtract\" and \"less\u201d."}, {"title": "3 Methodology", "content": "For the various experiments, the following models were used:\nGPT-3.5 Turbo\u00b9, prompted using the OpenAI API.\nClaude 3.5 Sonnet2, prompted using Anthropic API3.\nMathStral, a model specializing in mathematical and scientific tasks, whose weights were downloaded from HuggingFace5.\nLlama 3.1 70B and 450B6, prompted using NVIDIA AI Foundry API7.\nMistral 7B8, prompted using Mistral AI API9.\nA temperature = 0.7 has been set to enhance the variability of the responses, and the following system prompt has been used:\n\"Imagine being a regular person asked a question for data collection in a scientific study.\""}, {"title": "4 Experiments", "content": "All responses provided by the models were saved in CSV files (available here) for result analysis. Initially, as described in greater detail in the following sections, all responses deemed incorrect were excluded, including those that were formally incorrect from a logical standpoint and those that did not meet the prompt's requirements. For each iteration of the process, the history of previous interactions was neither retained nor used to inform the subsequent generation by the model. Each request was thus treated as an independent instance, without any context from prior interactions."}, {"title": "4.1 Palindrome sequence task", "content": "In this experiment, the objective was to transform a sequence of letters into a palindrome. Specifically, the strings \"abb\" and \"abab\" were used, along with the following prompts:\n\"Knowing that a sequence is said to be a palindrome if it is equal to its reverse, or in other words, if reading the sequence from left to right gives the same result as reading it from right to left, you need to make this sequence 'abb' a palindrome, but you can only add or remove one letter. Give me your answer in one sentence.\"\n\"Knowing that a sequence is said to be a palindrome if it is equal to its reverse, or in other words, if reading the sequence from left to right gives the same result as reading it from right to left, you need to make this sequence 'abab' a palindrome, but you can only add or remove one letter. Give me your answer in one sentence.\"\nIn particular, to prevent the indication add or remove from influencing the choice by favoring the additive approach over the subtractive one, the experiment was repeated the same number of times for each model using the same prompt, but this time with the inverted instruction remove or add. All results presented are averages of the values obtained in both cases.\nGPT-3.5 Turbo results In the case of the sequence \"abb\", as an initial analysis, we considered only the logically correct responses, where a single letter was used to make it symmetric. These correspond to the answers \"abba\" (where an \"a\" was added at the end) or \"bb\" (where the first \"a\" was removed). Out of the 1000 responses obtained with the suggestion add or remove, 700 responses were deemed valid, while with remove or add, 707 responses were considered correct. The results, obtained from the average of the two cases, are presented in Table 1.\nSubsequently, we decided also to consider responses that are not logically correct or do not adhere to using only one letter but are still palindromes (e.g., \"adding the letter 'a' to the middle of the sequence 'abb' to make it a palindrome, resulting in 'abba\", where the answer 'abba' is a palindrome but does not cor-respond to the given explanation, therefore logically incorrect). With these new considerations, the number of responses considered correct with the reccomenda-tion add or remove was 928, while with remove or add it was 931. Additionally, beyond the two sequences 'abba' and 'bb', which were considered the only correct ones in the previous case, new palindrome sequences emerged, as shown in the results Table 2."}, {"title": "4.2 Lego towers task", "content": "For this experiment (considere figure 2 as an example), it was asked:\n\"Imagine you have two towers built with Lego bricks. One of them was built with 5 bricks, while the other with 4 bricks. You need to make them the same height using the fewest number of pieces. What do you do? Give me your answer in one sentence.\"\nIn this case, using the fewest possible pieces, there are two valid answers to make the towers symmetrical: add one Lego brick to the shorter tower on the left (additive responses), or remove one brick from the taller tower on the right (subtractive responses). The purpose of the experiment is to determine if there is an additive bias in the responses that could explain a majority of answers involving adding a brick rather than removing one.\nGPT-3.5 Turbo results Out of the 1000 responses, 365 were discarded because the result was either logically incorrect or did not result in two symmetrical towers (e.g., \"I would take two bricks from the tower with 5 bricks and add them to the tower with 4 bricks to make both towers have 6 bricks each.\"). The results of the remaining 635 correct responses, presented in Table 8, show that the additive one was the more frequent of the two suggestions.\nClaude 3.5 Sonnet results For this model, not only were all 1000 collected responses logically correct, but all of them were additive, as indicated by the results in Table 9.\nMathEtral results As in the previous case, for MathEtral as well, all 1000 logically correct responses suggested adding a brick to the shorter tower, as indicated in Table 10.\nLlama 3.1 70B results Out of the 1000 responses, only 2 were discarded, as they suggested removing the excess brick from the taller tower and placing it on the shorter one, which did not solve the symmetry problem (e.g., \"take one brick from the 5-brick tower and attach it to the 4-brick tower. \"). The results for the remaining 998 answers, shown in Table 11, unlike the previous model cases, indicate that subtracting a brick from the taller tower was the most frequently proposed solution.\nA test was conducted to determine whether 485 out of 635 (76.38%) responses for GPT-3.5 Turbo, 1000 out of 1000 (100.00%) for Claude 3.5 Sonnet, 1000 out of 1000 (100%) for MathEtral, and 19 out of 1000 (1.90%) for Llama 3.1 70B, reject the null hypothesis that the suggestions for the two possible transforma-tions are equally likely. The p-value from a two-sided binomial distribution test for these results was found to be less than 0.001.\nIn conclusion, as can be seen also in Figure 3, it is possible to confirm that for this type of task, GPT-3.5 Turbo, Claude 3.5 Sonnet, and Math\u2211tral showed a strong additive bias, unlike Llama 3.1 70B, which in this case demonstrated a pronounced tendency towards subtractive choices."}, {"title": "4.3 Elementary operation task", "content": "In this experiment, the following prompt was used:\n\"Given these numbers: [n1, n2], which of the four basic operations would you suggest performing? Provide your answer in one word.\"\nWhere [n1, n2] are two numbers within the range of 1 to 10, randomly generated in each of the 1000 iterations. The aim is to investigate the potential tendency to prefer elementary operations that increase the value of the numbers involved, such as addition and multiplication (referred to as additive operations), over those that decrease it, such as subtraction and division (referred to as subtractive operations)."}, {"title": "4.4 Anomalous sandwich task", "content": "The objective of this experiment is to determine if the tested LLMs are more likely to add or subtract from stimuli with anomalous components. To test this hypothesis, it was asked to modify the recipe for a cheese sandwich. Specifically, the prompt used was as follows:\n\"Imagine you are hungry and decide to make a sandwich for lunch. Below there is a list of five ingredients: bread, ham, cheese, lettuce, mayonnaise, and ingredient n\u00b06. In one sentence, please describe how you would change this recipe when making your sandwich.\u201d\nFor ingredient n\u00b06, three different possibilities were tested:\nbanana, chocolate, and pineapple. These three ingredients are extremely unusual for a cheese sandwich recipe, which is why it would be reasonable to expect that the most obvious response would be of a reductive nature, namely only the removal of the sixth unusual ingredient. However, the results demonstrated that this is not so straightforward.\nThe collected responses were divided into four categories:\nno change: the original sandwich recipe is left unaltered with the unusual ingredient (e.g \"I would make a ham and cheese sandwich with lettuce, may-onnaise, and banana slices for a unique twist\").\nonly addition: there is a modification of only additive type, that is, the ad-dition of an ingredient, but no subtractive modification, that is, the removal of an ingredient (e.g \"If it were up to me, I would add some sliced tomatoes to my sandwich for an extra burst of freshness and flavor\u201d).\nonly remotion: there is a modification of only subtractive type, but no additive modification (e.g \"I would remove the chocolate from the list of ingredients when making my sandwich\u201d).\nboth addition and remotion: there are simultaneously both an additive modification and a subtractive modification, indicating both the removal and the addition of an ingredient (e.g. \"I would skip the pineapple and add some mustard for an extra kick of flavor in my sandwich\").\nGPT-3.5 Turbo results results As mentioned earlier, when faced with an extremely unusual ingredient, the most reasonable choice would be to remove it. However, the results in Table 16 show a marked additive tendency of the model. Instead of simply removing the unusual ingredient, GPT-3.5 Turbo frequently suggested adding a new ingredient. This was the most common choice in the cases of banana and pineapple, and while it was not the most frequent choice in the case of chocolate, it was still suggested a significantly notable number of times.\nClaude 3.5 Sonnet results The responses from Claude 3.5 Sonnet, shown in Table 17, generally met the expectation of simply removing the unusual ingre-dient, especially in the cases of banana and chocolate. However, it is interesting to note that in the case of pineapple, although the suggestion to remove only is the most frequent, a significant percentage of responses included the addition of a new ingredient.\nLlama 3.1 70B results Table 18 shows that Llama 3.1 70B, for each individual ingredient and in all cases, consistently chose the most straightforward solution, which was to simply remove the unusual component.\nIn conclusion, as shown in Figure 5, in this type of task, only GPT-3.5 Turbo displayed an additive bias. This was confirmed by the interesting fact that it did not merely remove the unwanted ingredient but often preferred to suggest adding a new one as well."}, {"title": "4.5 Increasing ingredients in soup task", "content": "For this task, it was asked to transform a soup recipe using the following prompt:\n\"Below you have a list of ingredients for a soup recipe: {ingredients}. Your job is to make any and all changes necessary to improve this soup. Assume that this soup is for someone who has no dietary restrictions or strong food dislikes. Please provide your answer in only one sentence.\u201d\nwhere the number of {ingredients} was increased each time, covering the following cases:\n5 ingredients: vegetable broth, carrots, peas, garlic, salt/pepper.\n15 ingredients: vegetable broth, carrots, peas, garlic, salt/pepper, onion, celery, oregano, potatoes, thyme, green beans, corn, zucchini, parsley, and leeks.\n30 ingredients: vegetable broth, carrots, peas, garlic, salt/pepper, onion, celery, oregano, potatoes, thyme, green beans, corn, zucchini, parsley, leeks, tomatoes, spinach, bell peppers, mushrooms, lentils, cabbage, chickpeas, bay leaves, paprika, cumin, lemon juice, ginger, cilantro, basil, kale.\n50 ingredients: vegetable broth, carrots, peas, garlic, salt, pepper, onion, celery, oregano, potatoes, thyme, green beans, corn, zucchini, parsley, leeks, tomatoes, spinach, bell peppers, mushrooms, lentils, cabbage, chickpeas, bay leaves, paprika, cumin, lemon juice, ginger, cilantro, basil, kale, cauliflower, green onions, black beans, quinoa, broccoli, radishes, fennel, mint, dill, rose-mary, sage, tofu, coconut milk, turmeric, chili powder, sweet potatoes, barley, shallots, pumpkin, asparagus, lime juice.\nThe goal of this experiment is to investigate whether there is a tendency for models to add ingredients rather than remove them. Specifically, it aims to observe how this tendency might be influenced by the increasing number of ingredients and whether there is a \"phase transition\" where this tendency is no longer observed.\nFor this reason, the responses were categorized as follows:\nOnly addition: A suggestion was made to add one or more ingredients (e.g., \"I would add some diced potatoes and onion to enhance the flavor and texture of the soup\").\nOnly removal: A suggestion was made to remove one or more ingredients (e.g., \"I would remove the radishes and fennel, as they may overpower the other flavors in the soup\").\nBoth addition and removal: A suggestion was made to remove one or more elements while also suggesting the addition of new ones (e.g., \"I would remove the barley and pumpkin and add in a dash of smoked paprika and a splash of balsamic vinegar for a richer flavor profile\").\nGPT-3.5 Turbo results As shown by the results in Table 19, with 5 and 15 ingredients, almost every suggestion from GPT-3.5 Turbo is additive. It takes 30 ingredients before suggestions that involve removing an ingredient start to appear consistently, although the additive tendency remains the most prevalent overall. With 50 ingredients, a reasonably high number, the most common sug-gestion shifts to removal. However, surprisingly, the difference is not excessive, and the additive tendency still appears in responses that include both types of suggestions as well as in those that are exclusively additive.\nClaude 3.5 Sonnet results In this case, as shown in Table 20, with 5 and 15 ingredients, the suggestions are exclusively additive, proposing to add one or more ingredients. Even with 30 ingredients, the trend remains largely the same, with suggestions involving only removal being completely absent. With 50 ingredients, this trend reverses, but surprisingly, removal is still not the preferred solution. Instead, the preferred suggestion is a combination of both removal and addition. This confirms that for this model, a persistent additive tendency is consistently present in this type of task.\nLlama 3.1 70B results As indicated by the results in Table 21, with 5 ingre-dients, as usual, the model tends to suggest adding. However, as the number of ingredients increases, the strategy of both removing and adding becomes increas-ingly frequent, eventually becoming the preferred option with 50 ingredients, in contrast to single removals, which only reach a consistent percentage in the sce-nario with the highest number of ingredients. Even in this case, it is possible to observe a strong additive bias that favors the addition of elements over their removal.\nExamining Figure 6, it is evident that the response patterns are quite simi-lar across all three models, particularly in the case of purely additive responses (which decrease as the number of ingredients increases) and those focused solely on removal (which become more frequent). Notably, in some instances especially when only 5 ingredients are involved additive responses make up the entirety of the suggestions, a situation never observed with subtractive responses. Even with 50 ingredients, subtractive responses never reach totality unless combined with an additive suggestion. These findings highlight that all models exhibit a strong additive bias when performing this specific task."}, {"title": "4.6 Summarization task", "content": "This experiment allowed the observation of potential additive or subtractive ten-dencies in the context of a revision process. During the first phase, each model was asked 1000 times to summarize a text provided in the prompt (specifically the proposed text was the introduction of Wikipedia's page on the Roman Em-pire10, attached in Appendix A).\n\"Summarize the following text: The Roman Empire was...\"\nOf the provided summary, the number of words used was counted, and then, during the second phase, following the same procedure proposed to the partici-pants by [3], it was asked to improve it, but in two different contexts.\nFirst case one's own writing\nIn this case, it was asked to improve the previous summary, presented again in the prompt, specifying that it was a summary elaborated by the model itself.\n\"Edit your summary with the goal of improving how well you summarized the text.\"\nFinally, the number of words used was counted.\nSecond case others' writing\nThis time, the original summary was always presented with the request to im-prove it, but with the specification that it was done by someone else other than the model.\n\"Edit this previous summary made by someone else with the goal of im-proving how well the text has been summarized.\u201d\nIn this case as well, the number of words used in the improved summary was counted.\nThe key observation of this experiment is to determine whether the sum-maries obtained have more or fewer words than the original summary, thereby highlighting the presence or absence of an additive trend.\nGPT-3.5 Turbo results As shown in the Table 22, especially in the first case, the model most often preferred to adopt a subtractive strategy, using fewer words.\nClaude 3.5 Sonnet results Also in this case, as indicated in Table 23 the preferred strategy in both instances was to use fewer words by writing shorter summaries.\nMistral 7B results Unlike the previous cases, in this instance, the Table 24 shows that the tendency for both cases was additive, resulting in edited summaries with an higher number of words.\nA test was conducted for both cases to determine if the counts of 16 out of 1000 and 384 out of 1000 for GPT-3.5 Turbo (408 out of 1000 and 394 out of 1000 for Claude 3.5 Sonnet, and 594 out of 1000 and 751 out of 1000 for Mistral 7B) reject the null hypothesis that the probability of producing a summary with fewer words is the same as producing one with more words compared to the orig-inal. The p-value from a two-sided binomial distribution test for these results was found to be less than 0.001. As shown in Figure 7, it can be concluded that during the revision phase, only Mistral 7B exhibited an additive bias in both cases. In contrast, Claude 3.5 Sonnet and GPT-3.5 Turbo (for the first time in this series of proposed experiments) favored a reductive strategy."}, {"title": "5 Conclusion", "content": "This study investigated the presence of addition bias in Large Language Mod-els (LLMs), drawing parallels to the cognitive bias observed in humans where individuals tend to favor additive over subtractive changes. Through a series of controlled experiments across various tasks, we tested several prominent LLMs, including GPT-3.5 Turbo, Claude 3.5 Sonnet, Mistral, MathStral, and Llama 3.1. Our findings consistently demonstrated a significant preference for additive changes across most tested models and tasks. This bias was particularly evident in tasks such as palindrome creation, Lego tower balancing, and elementary oper-ation selection. For instance, in the palindrome task, models like GPT-3.5 Turbo and Claude 3.5 Sonnet showed a strong tendency to add letters rather than re-move them. Similarly, in the Lego tower task, most models preferred adding a brick to the shorter tower rather than removing one from the taller tower. The addition bias persisted even in more complex scenarios, such as modifying recipes with unusual ingredients or improving soup recipes with varying num-bers of ingredients. Interestingly, the bias remained present but decreased in intensity as the number of ingredients increased, suggesting a potential \"phase transition\" point where subtractive changes become more prevalent. However, it is important to note that the bias was not uniform across all tasks and models. For example, in the text summarization task, some models (GPT-3.5 Turbo and Claude 3.5 Sonnet) showed a tendency towards reduction rather than addition when improving their own or others' summaries. This suggests that the mani-festation of addition bias may be task-dependent and can vary across different LLMs. These findings have significant implications for the development and ap-plication of LLMs in various domains. The presence of addition bias could influ-ence how these models approach problem-solving, decision-making, and content generation tasks. It may lead to unnecessarily complex solutions, inefficiencies, or in certain scenarios it may go against the Occam Principle where simpler, subtractive approaches might be more appropriate. When LLMs are used on a large scale, addition bias can even increase resource use, leading to higher eco-nomic costs and increased environmental impact due to overconsumption and waste. Future research should focus on understanding the root causes of this bias in LLMs, potentially exploring its relationship to training data and model architectures. Additionally, developing strategies to mitigate this bias could be crucial for improving the efficiency and effectiveness of LLMs across a wide range of applications."}]}