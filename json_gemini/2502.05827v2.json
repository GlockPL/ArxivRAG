{"title": "HYGEN: Regularizing Negative Hyperedge Generation for Accurate Hyperedge Prediction", "authors": ["Song Kyung Yu", "Da Eun Lee", "Yunyong Ko", "Sang-Wook Kim"], "abstract": "Hyperedge prediction is a fundamental task to predict future high-order relations based on the observed network structure. Existing hyperedge prediction methods, however, suffer from the data sparsity problem. To alleviate this problem, negative sampling methods can be used, which leverage non-existing hyperedges as contrastive information for model training. However, the following important challenges have been rarely studied: (C1) lack of guidance for generating negatives and (C2) possibility of producing false negatives. To address them, we propose a novel hyperedge prediction method, HYGEN, that employs (1) a negative hyperedge generator that employs positive hyperedges as a guidance to generate more realistic ones and (2) a regularization term that prevents the generated hyperedges from being false negatives. Extensive experiments on six real-world hypergraphs reveal that HYGEN consistently outperforms four state-of-the-art hyperedge prediction methods.", "sections": [{"title": "1 Introduction", "content": "In real-world networks, high-order relations (i.e., group-wise relations) are prevalent [5, 6, 9], such as (i) a research paper co-authored by a group of researchers and (ii) a chemical reaction co-induced by a group of proteins. A hypergraph, a generalized data structure, is capable of modeling such group-wise relations as a hyperedge without any information loss. Due to its powerful expressiveness, hypergraph-based network learning [6, 14] has been widely studied and shown to outperform graph-based methods in various downstream tasks, including node classification [4, 5], node ranking [13, 16], and link prediction [12, 17].\nHyperedge prediction (i.e., link prediction on hypergraphs) is a fundamental task in many real-world applications, such as recommender systems [16] and social network analysis [4, 13]; it predicts future high-order relations (i.e., hyperedges) based on an observed hypergraph structure. A general approach to hyperedge prediction is two-fold [8, 9] (see Figure 1: (1) (hypergraph encoding) the embeddings of nodes are produced by hypergraph neural networks [5, 6] and (2) (hyperedge candidate scoring) the embeddings of nodes in each hyperedge candidate are aggregated and fed into a predictor to determine whether the candidate is real.\nIn real-world networks, however, high-order relations are often extremely sparse [10] (i.e., $2^{|V|} > |E|$, where V and E are the sets of nodes and hyperedges, respectively). Such a data sparsity problem is the fundamental cause of low accuracy in hyperedge prediction. To address this problem, negative sampling (NS) can be used [8, 12, 15], utilizing non-existing hyperedges as contrastive information for model training. Specifically, the model is trained so that positive examples get higher scores while negative examples get lower scores, which enhances the distinguishing ability of the model. Thus, it is crucial to carefully choose negative hyperedges to maximize the effect of negative sampling.\nHowever, sampling 'good' negative hyperedges is challenging in the context of hyperedge prediction since there exist too many possible negative hyperedges (i.e., $2^{|V|} - |E|$). Although existing hyperedge prediction methods [11, 12, 15], enhanced by negative sampling methods, have achieved breakthroughs in many fields, they focus primarily on hypergraph encoding while employing simple heuristic-based negative sampling methods [10]. Thus, negative sampling for hyperedge prediction is still rarely explored.\nAlthough one recent work [8] proposed an adversarial-training-based hyperedge prediction method (AHP) that leverages model-generated negative hyperedges useful for model training, it has overlooked the following two important challenges:"}, {"title": "2 Related Works", "content": "Hyperedge prediction. There have been a number of works to study hyperedge prediction. they solve the hyperedge prediction problem as a classification task [8, 11, 12, 15]. Expansion [11] models a hypergraph as multiple n-projected graphs and applies a logistic regression model to the projected graphs for predicting future hyperedges HyperSAGNN [15] uses a self-attention-based graph neural networks (GNN) model to learn hyperedges of variable sizes. NHP [12] employs hyperedge-aware GNN models to learn node embeddings in hypergraphs, using the max-min pooling to aggregate the embeddings of nodes within each hyperedge candidate for prediction. AHP [8], the state-of-the-art hyperedge prediction method, employs adversarial training to generate negative hyperedges for model training and uses max-min pooling for node aggregation.\nNegative hyperedge sampling. For enhancing the training of hyperedge prediction models, the following three heuristic-based methods for negative hyperedge sampling have been proposed [10]: (1) Sized NS (SNS) samples n nodes uniformly at random; (2) Motif NS (MNS) transforms a hypergraph into an ordinary graph via a clique-expansion and samples a n-connected component in the expanded graph; and (3) Clique NS (CNS) selects a hyperedge e and replaces one of its incident nodes u \u2208 e with a node v \u2209 e, which is linked to all the other incident nodes, i.e., $(e \\ {u}) \\cup \\{v\\}$."}, {"title": "3 Proposed Method: HYGEN", "content": "In this section, we present a novel hyperedge prediction method, named as HYGEN, for accurate hyperedge prediction."}, {"title": "3.1 Problem Definition", "content": "Notations. The notations used in this paper are described in Table 1. A hypergraph is defined as $H = (V, E)$, where $V = \\{v_1, v_2, ..., v_{|V|}\\}$ and $E = \\{e_1, e_2, ..., e_{|E|}\\}$. A hypergraph can generally be represented by an incidence matrix $H \\in \\{0,1\\}^{|V| \\times |E|}$, where each element $h_{ij} = 1$ if $v_i \\in e_j$, and $h_{ij} = 0$ otherwise. The node and hyperedge features are represented by the matrices $P \\in \\mathbb{R}^{|V| \\times d}$, $Q \\in \\mathbb{R}^{|E| \\times d}$, where each row $p_i$ and $q_i$ represents the d-dimensional feature of a node and a hyperedge, respectively.\nPROBLEM 1 (HYPEREDGE PREDICTION). Given a hypergraph $H \\in \\{0,1\\}^{|V| \\times |E|}$ and the initial node features $X \\in \\mathbb{R}^{|V| \\times d}$, and a hyperedge candidate $e' \\notin E$, to predict whether $e'$ is real or not."}, {"title": "3.2 Methodology", "content": "Overview of HYGEN. Figure 2 illustrates the overview of Hy-GEN, which consists of (1) hypergraph encoding (upper) and (2) hyperedge candidate scoring (lower) that we focus on.\n(1) Hypergraph encoding. Given a hypergraph $H = (V, E)$, Hy-GEN produces node embeddings $P \\in \\mathbb{R}^{|V| \\times d}$ and hyperedge embeddings $Q \\in \\mathbb{R}^{|E| \\times d}$. Following [3, 5], HYGEN adopts a 2-stage aggregation approach, which repeats (1) (node-to-hyperedge) producing a hyperedge embedding by aggregating the node embeddings and (2) (hyperedge-to-node) producing a node embedding by aggregating the hyperedge embeddings. Formally, the node and hyperedge embeddings at the l-th layer are defined as:\n$Q^{(l)} = \\sigma \\left(H^{T} \\phi\\left(P^{(l-1)}\\right) W_E^{(l)} + b_E^{(l)}\\right), P^{(l)} = \\sigma \\left(H Q^{(l)} W_V^{(l)} + b_V^{(l)}\\right),$ (1)"}, {"title": "3.3 Model Training", "content": "We train the model parameters of HYGEN in an adversarial way [1]: given a batch B of positive hyperedges, (1) generate |B| negative hyperedges using the generator G (enc() and dec()), (2) classify the positive and negative hyperedges using the discriminator D (agg() and pred()), and (3) update the model parameters of HYGEN based on their losses. Specifically, as D aims to compute the probabilities of positive hyperedges higher than those of negative hyperedges, the loss function for D is defined as:\n$\\mathcal{L}_D = \\frac{1}{|B|} \\sum_{e^{+} \\in B} \\left[\\mathcal{D}\\left(e^{+} | H, X\\right)\\right] + \\frac{1}{|B|} \\sum_{j=1}^{|B|} \\left[\\mathcal{D}\\left(\\mathcal{G}\\left(z_j | e^{+}\\right) | H, X\\right)\\right],$ (4)\nwhere $e^{+}$ is a positive hyperedge and $\\mathcal{G}\\left(z_j | e^{+}\\right)$ is the negative hyperedge generated from a noise z and the positive hyperedge $e^{+}$. This loss $\\mathcal{L}_D$ is also used for training the hypergraph encoder f(.). On the other hand, G aims to deceive D to misclassify negative hyperedges as positive. Thus, the loss function for G is defined as:\n$\\mathcal{L}_G = \\frac{1}{|B|} \\sum_{j=1}^{|B|} \\left[\\mathcal{D}\\left(\\mathcal{G}\\left(z_j | e^{+}\\right) | H, X\\right)\\right].$ (5)\nRegularization for (C2). 'Hard' negative hyperedges, generated by our hyperedge generator, could enhance the distinguishing ability of a hyperedge prediction model [8]. Without any regularization on a generator, however, it may completely copy the distribution of the original positive hyperedges; result in generating hyperedges too similar to positive hyperedges that might potentially become positive hyperedges in the future. Using such hyperedges as negative hyperedges in training can lead to incorrect learning.\nTo address this challenge, we propose a novel regularization term that is integrated into the loss function. This regularization loss gives a smaller penalty as the generated negative hyperedges are more similar to positive hyperedges until a certain degree and gives a larger penalty as they are too similar to positive hyperedges. Specifically, given the embeddings of positive and negative hyperedges $Q^+$ and $Q^-$, the regularization loss is defined as:\n$\\mathcal{L}_{reg} = \\frac{1}{|P|} \\sum_{\\theta \\in \\{Q^+, Q^-\\}} \\theta^{(1-k)} \\left(1-\\theta\\right)^{k}\\,$  $\\theta = sim(Q^+, Q^-).$ (6)"}, {"title": "4 Experimental Validation", "content": "In this section, we comprehensively evaluate HYGEN by answering the following evaluation questions (EQs):\n*   EQ1 (Accuracy). To what extent does HYGEN improve the existing hyperedge prediction methods in terms of the accuracy?\n*   EQ2 (Ablation study). Is each of our strategies beneficial to generating negative hyperedges useful for model training?\n*   EQ3 (Sensitivity). How sensitive is the effect of the regularization loss in model training to its hyperparameters (k and p)?"}, {"title": "4.1 Experimental Setups", "content": "Datasets and competitors We use six widely used real-world hypergraphs: (1) three co-citation datasets (Citeseer, Cora, and Pubmed), (2) two authorship datasets (Cora-A and DBLP-A), and (3) one collaboration dataset (DBLP). In the co-citation datasets, each node represents a paper and each hyperedge represents a group of papers co-cited by a paper; in the authorship dataset, each node represents a paper and each hyperedge represents a group of papers written by an author; in the collaboration dataset, each node represents a researcher and each hyperedge represents a group of researchers who wrote the same paper. For all the datasets, we use the bag-of-word features from the abstract of each paper as in [8, 9].\nWe select four state-of-the-art hyperedge prediction methods as our competitors in the experiments (Expansion [11], NHP [12], HyperSAGNN [15], and AHP [8]).\nEvaluation protocol. We evaluate HYGEN by using the protocol exactly same as that used in [8]. For each dataset, we use five data splits 1, where positive hyperedges are randomly divided into training (60%), validation (20%), and test (20%) sets. We use three validation and test sets constructed with negative hyperedges sampled by SNS, MNS, and CNS, explained in Section 2. As metrics, we use AUROC (area under the ROC curve) and AP (average precision). We (1) measure AUROC and AP on each test set when the averaged AUROC over the validation sets is maximized, and (2) report the averaged AUROC and AP over five runs on each test set. For all competing methods, we use the results reported in [8] since we follow the exactly same protocol with the same data splits."}, {"title": "4.2 EQ1. Hyperedge Prediction Accuracy", "content": "We first compare HYGEN with four competing methods in the hyperedge prediction task. Table 2 shows that HYGEN consistently outperforms all competing methods in terms of both the averaged AP and AUROC over three test sets across all datasets. We note that these improvements of HYGEN over AHP (the best competitor) are remarkable, given that AHP [8], the state-of-the-art method, has already improved other existing methods significantly in those datasets. Via the t-tests with a 95% confidence level, we verify that the improvements of HYGEN are statistically significant (i.e., the p-values \u2264 0.05). As a result, these results demonstrate that HYGEN can generate informative negative hyperedges by effectively addressing the two challenges of negative sampling, thereby enhancing the accuracy of a hyperedge prediction task.\nInterestingly, NHP [12] achieves the highest accuracies in the SNS test setting of the Citeseer, Pubmed, and DBLP-A, However, NHP shows very low accuracies on the CNS test set (i.e., the most difficult test set) of all datasets, which is similar to or even worse than the accuracy of the random prediction (\u2248 0.5). These accuracy gaps between the CNS and SNS test sets imply that NHP may be overfitting to the easy negative examples, thus which limits"}, {"title": "4.3 EQ2. Ablation Study", "content": "We verify the effectiveness of our proposed strategies of HYGEN individually by ablating one of them: (i) positive-guided negative hyperedge generator and (ii) regularization term. Table 3 shows that the original version of HYGEN always achieves the highest accuracy across all datasets, which indicates that each of the proposed strategies is always beneficial to improving the accuracy of HYGEN. These results verify that our proposed strategies are able to address the two challenges successfully: (C1) lack of guidance for generating negatives and (C2) possibility of false negatives.\nFurthermore, HYGEN w/o Lreg shows the worst results in all cases, indicating that ablating the regularization term can lead to significant accuracy degradation. These results verify that HYGEN can generate negative hyperedges that are sufficiently distinct from positive hyperedges, making them useful for model training."}, {"title": "4.4 EQ3. Sensitivity Analysis", "content": "In this experiment, we evaluate the impacts of regularization hyperparameters k and p on the accuracy of HYGEN. We measure the model accuracy of HYGEN with varying k from 0 to 1.0 in step of 0.1 and p from 1 to 5 in step of 1. Figure 4 shows the results, where the x-axis represents the converge point hyperparameter k, the y-axis represents the curvature hyperparameter p, and the z-axis represents the averaged AUROC. HYGEN with k\u2265 0.4 consistently achieves higher accuracy than HYGEN with k < 0.4 regardless of p (i.e., the wide red area on the surface in Figure 4). Based on these results, we believe that the accuracy of HYGEN is insensitive to the regularization hyperparameters k and p provided that k \u2265 0.4."}, {"title": "5 Conclusion", "content": "In this paper, we identify two key challenges of negative hyperedge sampling in the hyperedge prediction task: (C1) lack of guidance for generating negatives and (C2) possibility of producing false negatives. To address both challenges, we propose a novel hyper-edge prediction method, HYGEN that employs (1) a positive-guided negative hyperedge generator leveraging positive hyperedges as a guidance to generate informative negative hyperedges for (C1) and (2) a regularization term to prevent the generated hyperedges from being false negatives (C2). Comprehensive experiments on six real-world datasets verified the superiority of HYGEN over four state-of-the-art hyperedge prediction methods."}]}