{"title": "MMAD: THE FIRST-EVER COMPREHENSIVE BENCHMARK FOR MULTIMODAL LARGE LANGUAGE MODELS IN INDUSTRIAL ANOMALY DETECTION", "authors": ["Xi Jiang", "Jian Li", "Hanqiu Deng", "Yong Liu", "Bin-Bin Gao", "Yifeng Zhou", "Jialin Li", "Chengjie Wang", "Feng Zheng"], "abstract": "In the field of industrial inspection, Multimodal Large Language Models (MLLMs) have a high potential to renew the paradigms in practical applications due to their robust language capabilities and generalization abilities. However, despite their impressive problem-solving skills in many domains, MLLMs' ability in industrial anomaly detection has not been systematically studied. To bridge this gap, we present MMAD, the first-ever full-spectrum MLLMs benchmark in industrial Anomaly Detection. We defined seven key subtasks of MLLMs in industrial inspection and designed a novel pipeline to generate the MMAD dataset with 39,672 questions for 8,366 industrial images. With MMAD, we have conducted a comprehensive, quantitative evaluation of various state-of-the-art MLLMs. The commercial models performed the best, with the average accuracy of GPT-40 models reaching 74.9%. However, this result falls far short of industrial requirements. Our analysis reveals that current MLLMs still have significant room for improvement in answering questions related to industrial anomalies and defects. We further explore two training-free performance enhancement strategies to help models improve in industrial scenarios, highlighting their promising potential for future research. The code and data are available at https://github.com/jam-cc/MMAD.", "sections": [{"title": "1 INTRODUCTION", "content": "Automatic vision inspection is a crucial challenge in realizing an unmanned factory (Benbarrad et al., 2021). Traditional AI research for automatic vision inspection, represented by industrial anomaly detection (IAD) (Jiang et al., 2022b; Ren et al., 2022), typically relies on discriminative models within the conventional deep learning paradigm. These models can only perform trained detection tasks and cannot provide detailed reports like quality inspection workers. Additionally, when production lines or requirements change, traditional methods necessitate retraining or redevelopment. The development of MLLMs (Jin et al., 2024) has the potential to alter this situation. These generative models can flexibly produce the required textual output based on input language and visual prompts, allowing us to guide the model using language similar to instructing humans.\nNowadays, multimodal large language models, represented by GPT-4 (Achiam et al., 2023), can already finish many human jobs, especially high-paying intellectual jobs like programmers, writers, and data analysts (Eloundou et al., 2023). In comparison, the work of quality inspectors is simple, typically not requiring a high level of education but relying heavily on work experience. Therefore, we are greatly interested in the question:\nHow well are current MLLMs performing as industrial quality inspectors?"}, {"title": "2 RELATED WORK", "content": "In recent years, substantial efforts on benchmarks have been dedicated to exploring MLLMs (Li & Lu, 2024) from various perspectives. However, the capabilities assessed by these benchmarks differ significantly from those required for IAD. Firstly, most MLLM benchmarks primarily focus on single-image input scenarios, such as MME (Fu et al., 2023), MMBench (Liu et al., 2023b), and MMVP (Tong et al., 2024b). In contrast, industrial quality inspection necessitates the ability to process multiple images, as industrial knowledge is highly specialized and often requires additional images for product recognition. Secondly, most benchmarks emphasize the general capabilities of MLLMs rather than the specific needs of particular domains. For instance, TextVQA (Singh et al., 2019) focuses on text recognition in images, MATH-Vision (Wang et al., 2024a) evaluates mathematical reasoning in visual contexts, and Video-MME (Fu et al., 2024) emphasizes video understanding. Most importantly, these benchmarks do not address the industrial domain. For example, MMMU (Yue et al., 2024) covers lots of fields, such as Art, Business, Science, Social Science, and Engineering, but not industry. Seed-Bench (Li et al., 2024a) and CompBench (Kil et al., 2024) test multi-image comparison capabilities but do not use industrial images. In contrast, the medical field, which has tasks and data formats similar to the industrial domain, already has numerous benchmarks for MLLMs, including Asclepius (Wang et al., 2024b), GMAI-MMBench (Chen et al., 2024) and PMC-VQA (Zhang et al., 2023b). Therefore, proposing the first dedicated MLLMs benchmark for the industrial domain would fill a significant gap."}, {"title": "2.2 INDUSTRIAL ANOMALY DETECTION", "content": "Traditional IAD research primarily aims to address a significant issue in industrial visual inspection: discriminating and localizing defects without samples of defects. Consequently, traditional IAD methods typically involve training on a large number of normal samples and then using outlier detection techniques to identify anomalies in test samples. Common approaches include memory bank-based methods (Roth et al., 2022; Jiang et al., 2022a), reconstruction-based methods (Deng & Li, 2022; Jiang et al., 2024a), and methods based on training with synthetic anomalies (Zavrtanik et al., 2021). Recent research has begun to focus on the generalization capability of IAD. Leveraging vision-language models such as CLIP, some few-shot, and even zero-shot models have emerged, such as AnomalyCLIP (Zhou et al., 2023) and InCTRL (Zhu & Pang, 2024). However, these discriminative models heavily rely on predefined anomaly concepts in the CLIP model, limiting their ability to generalize to new scenarios. For instance, models trained on structural anomalies struggle to detect logical anomalies (Bergmann et al., 2022). The emergence of MLLMs may help address this challenge, as they can understand complex textual inputs and provide diverse responses in conjunction with visual components.\nSome studies have already demonstrated the advantages of open input-output formats of MLLMs in IAD. For example, LogiCode (Zhang et al., 2024b) uses MLLMs for logical reasoning and automatically generates code to address different types of logical anomalies. Xu et al. (2024) proposes that designing visual and language prompts can directly enhance the performance of MLLMs in IAD tasks. Additionally, some research has begun to train MLLMs directly on IAD datasets, such as AnomalyGPT (Gu et al., 2024), Myriad (Li et al., 2023), and FabGPT (Jiang et al., 2024b). However, these three models heavily depend on the capabilities of expert models and cannot freely"}, {"title": "3 THE MMAD DATASET", "content": "An excellent industrial quality inspector should be able to adapt to the inspection tasks of different products, as the skills involved in visual inspection are similar. Therefore, our designed benchmark needs to cover multiple scenarios of IAD. We achieved data diversity by collecting and sampling from four different IAD datasets with distinct focuses, resulting in over 38 product categories and 244 defect types, as detailed in Table 1. Among these, MVTec AD (Bergmann et al., 2019) is one of the most prominent datasets for IAD, encompassing multiple categories of objects and textures, where we use finer annotations from Defect Spectrum (Yang et al., 2023). MVTec LOCO AD (Bergmann et al., 2022) focuses on logical anomalies, thereby testing the model's understanding of logical-level anomalies. The VisA (Zou et al., 2022) dataset includes multiple instances and complex examples, reflecting more intricate IAD scenarios. The GoodsAD (Zhang et al., 2024a) dataset primarily consists of industrial goods, and due to the varying appearances of finished products from different brands, it significantly expands the number of object categories."}, {"title": "3.2 QUESTION DEFINITION", "content": "To evaluate whether MLLMs can fulfill the role of an industrial quality inspector, it is necessary to test a wide range of capabilities. On the production line, a quality inspector must not only identify defective samples but also classify and grade defects, analyze causes, and diagnose faults. Therefore, in addition to basic anomaly detection, we have designed four anomaly-related subtasks and two object-related subtasks, as detailed below:\n\nAnomaly Discrimination/Detection: Asking whether a sample has defects. These questions are binary classification problems that test the ability to judge anomalies in samples."}, {"title": "3.3 DATA GENERATION", "content": "Due to the lack of semantic annotation in open-source IAD datasets, the currently collected data cannot be directly used to evaluate MLLMs. Therefore, we designed a novel pipeline to generate evaluation questions for each IAD image. As shown in Figure 3, our process leverages the text generation capabilities of GPT-4V (Achiam et al., 2023), combined with rule-based program outputs, language prompts, and human filtering to ensure the reliability of the generated content. First, we generate rich textual descriptions for each image. Since GPT-4V is primarily trained on natural scenes and may struggle with industrial quality inspection, we provide additional visual and textual prompts. In the visual prompts, we highlight the ground truth mask in red on the original image to make the model aware of the defects. Additionally, we retrieve the most similar normal image to serve as a comparison template, using a similarity metric combining the SSIM score (Wang et al., 2004) and the Bhattacharyya distance (Bhattacharyya, 1943) of color histograms. The language prompts include object and defect category labels and textual descriptions of defect positions within the image, utilizing the nine-grid division proposed by Gu et al. (2024). To ensure diversity, we designed instructions for caption generation with variations in the prior knowledge provided, preventing the captions from merely copying the supplied text. Once we have the captions for each image, we generate questions, options, and answers based on predefined subtasks. Unlike natural images, each IAD image corresponds to multiple questions, which are generated simultaneously and"}, {"title": "3.4 BOOST METHODS", "content": "Retrieval-Augmented Generation (RAG). RAG is a method that combines information retrieval and generation to enhance the performance of language models, particularly for tasks requiring external knowledge (Zhao et al., 2024). Knowledge related to IAD is often specialized and rarely encountered during the training of Multimodal Language Models (MLLMs). Therefore, we propose a RAG method tailored for IAD, as illustrated in Figure 4(a). Experts, with the assistance of large language models, first summarize the existing IAD datasets. For each category, they summarize the characteristics of normal samples and the features of each possible anomaly. The domain knowledge summarized from all datasets forms a retrieval database. During testing, the query image is used to retrieve the relevant category knowledge, which is then incorporated into the text prompts.\nExpert Agent. Autonomous agent is an independent module within a large model, responsible for specific functions (Xi et al., 2023). In the evaluation of MMAD, exploring whether expert models can enhance MLLMs is an intriguing topic. We designed a simple method to investigate its feasibility. As shown in Figure 4(b), we treat the IAD model as a visual expert model. Since the output anomaly map is difficult to understand by MLLMs, we visualize it and then input it into the MLLMs. We use SOTA models in IAD, such as AnomalyCLIP and PatchCore, as experts and test the differences among 3 visualization methods. Additionally, we applied a special expert, ground truth, which directly uses the mask as the output of the expert model to verify the upper limit."}, {"title": "4 EXPERIMENTS", "content": "We organize multiple settings for MLLMs, split by the quantity and categories of prompt samples. Such approaches have enabled our constructed benchmark to consider an array of application scenarios and to thoroughly examine the capacities of MLLMs. We default to a 1-shot setting, where, in addition to the query image, we randomly provide a normal image from the dataset and inform the model that it can use this image as a reference. This approach aims to help the model understand the normal state of objects, which is essential for anomaly detection. We also include 0-shot and few-shot settings for further comparison. Randomly providing normal images may lead to misunderstandings in some object categories; for instance, in GoodsAD, the same object category may have multiple brands, each with different normal states. Therefore, we introduce a 1-shot+ setting,"}, {"title": "4.2 EXPERIMENTAL RESULTS", "content": "We compare the performance of over a dozen models, including commercial APIs, interleaved MLLMs, industrial MLLMs, and vision-centric MLLMs, as shown in Table 2. All models outperform the random baseline. The open-source models perform the best, with the average accuracy of the GPT-4o and Gemini-1.5-pro models reaching 74.9% and 73%, respectively. However, their cost-efficient counterparts, GPT-40-mini and Gemini-1.5-flash, only achieved 66.3% and 68.9%, respectively, falling short of the best open-source model, InternVL2-76B, which achieved 70.8%. AnomalyGPT performs poorly overall, primarily due to its training on the IAD task in a fixed question-and-answer format, leading to severe overfitting issues. It demonstrates decent performance in anomaly discrimination because we specifically adapted the question format to suit its training. Similarly, the vision-centric MLLMs, Cambrian-1 and SPHINX, do not exhibit superior performance on the fine-grained visual tasks of MMAD, likely due to their foundational language models not being advanced enough. Among the general open-source MLLMs, earlier models like Qwen-VL-Chat and LLaVA-1.5 underperform compared to newer models like LLaVA-OnVision and MiniCPM-V2.6, indicating that advancements in general capabilities benefit performance on IAD tasks. MMAD uses a default 1-shot format, providing a normal image for comparison with"}, {"title": "4.3 FURTHER ANALYSES", "content": "Can MLLMs effectively utilize template normal images? When humans assess anomalies in industrial images, having a template image to understand what a normal image looks like is often very helpful. To further investigate whether current MLLMs effectively utilize template images, we designed a comparative experiment. As shown in Table 3, we compared the performance of different models under 0-shot and 1-shot+ settings. In the O-shot scenario, no template image is provided. In the 1-shot+ scenario, the closest image to the test image is selected from a set of normal images as the template, making it easier to observe anomalies through comparison. However, the experimental results indicate that many models do not effectively utilize the information from the template image, leading to a performance decline. Due to cost constraints, we only tested the most cost-effective Gemini-1.5-flash among commercial models, which still showed a noticeable performance improvement, suggesting that open-source models perform better in contextual image understanding. Among open-source models, only the larger-scale InternVL2 effectively utilized the template image information.\nHow significant is the impact of model scale on performance? In the main experiment results, we observed that larger MLLMs generally exhibit better performance. We evaluate the scaling law using the InternVL2 series models, which utilize the same training data and are among the best-performing open-source models. As shown in the left panel of Figure 5, performance significantly improves with increasing model size, with the average accuracy difference between the largest and smallest models reaching 23.37%. Specifically, the classification performance of industrial objects improves the most with increasing size, and there are notable enhancements in several subtasks related to anomalies and defects. Although the performance improvement trend slows as the model size increases, further enlarging the model remains a promising option, albeit one that must be balanced against cost considerations.\nCan increasing the number of images further enhance performance? For traditional few-shot IAD models, increasing the number of normal samples significantly enhances the performance of anomaly detection and localization. This is because more normal images allow the model to better understand the distribution of normal samples, thereby preventing misclassification of some noise. Therefore, we investigate whether MLLMs can leverage additional normal images. We conducted experiments on the best-performing open-source model, InternVL2-76B. As shown in the right panel of Figure 5, although the performance improvement from 0-shot to 1-shot demonstrates its ability to utilize contextual images, further increasing the number of normal samples does not significantly"}, {"title": "4.4 EXPLORATION", "content": "Input Domain Knowledge to MLLMs. One significant challenge identified in our evaluation is the lack of industrial knowledge in MLLMs for specific tasks. During the training, MLLMs rarely encounter knowledge related to industrial quality inspection, while different products correspond to different specific knowledge. In practical applications, senior experts typically train workers. We use RAG to provide domain knowledge guidance to MLLMs to simulate this process. As shown in Table 4, the inclusion of RAG significantly improves performance on MMAD across multiple models, with the most notable improvements in anomaly classification and object classification. This is primarily because domain knowledge contains extensive category information. The performance in defect localization also improves, indicating that models can leverage textual knowledge to enhance their perception of images. Notably, the performance in anomaly discrimination shows substantial improvement for InternVL2-40B, surpassing all other models in this metric.\nModel Collaboration. Given that MLLMs have not been specifically trained on IAD data, their anomaly detection capabilities are relatively weak. Detecting anomalies is not particularly challenging for existing IAD models. Therefore, we are interested in whether MLLMs can perform answering with the help of the visual outputs of expert models. As shown in Table 6, we tested three"}, {"title": "5 CONCLUSION", "content": "In this work, we introduce MMAD, the first benchmark for MLLMs in the IAD field, aimed at exploring the feasibility of using MLLMs for industrial quality inspection. Our evaluation of over ten SOTA MLLMs yielded less than optimistic results, revealing their weaknesses in industrial scenarios, particularly the lack of industrial knowledge and the ability to perform fine-grained comparisons across multiple images. However, our further investigations suggest that MLLMs have the potential to address some of these issues through additional enhancements. We hope that MMAD will inspire future research into improving the relevant capabilities of MLLMs and promote the development of practical applications."}]}