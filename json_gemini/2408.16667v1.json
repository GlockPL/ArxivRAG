{"title": "Iterative Graph Alignment", "authors": ["Fangyuan Yu", "Hardeep Arora", "Matt Johnson"], "abstract": "By compressing diverse narratives, LLMs go beyond memorization, achieving\nintelligence by capturing generalizable causal relationships. However, they suffer\nfrom local 'representation gaps' due to insufficient training data diversity, limiting\ntheir real-world utility, especially in tasks requiring strict alignment to rules. Tradi-\ntional alignment methods relying on heavy human annotations are inefficient and\nunscalable. Recent self-alignment techniques also fall short, as they often depend\non self-selection based prompting and memorization-based learning. To address\nthese issues, we introduce Iterative Graph Alignment (IGA), an annotation-free\nrule-based alignment algorithm. A teacher model (VLM) employs Iterative Graph\nPrompting (IGP) to create logical graphs and reference answers. The student model\n(LLM) identifies local knowledge gaps by attempting to align its responses with\nthese references, collaborating with helper models to generate diverse answers.\nThese aligned responses are then used for iterative supervised fine-tuning (SFT).\nOur evaluations across five rule-based scenarios demonstrate IGP's effectiveness,\nwith a 73.12% alignment improvement in Claude Sonnet 3.5, and Llama3-8B-\nInstruct achieving an 86.20% improvement, outperforming Claude Sonnet 3.5 in\nrule-based alignment.", "sections": [{"title": "Introduction", "content": "Language modeling fundamentally aims to maximize lossless compression [Del\u00e9tang et al., 2024].\nEmpirical evidence suggests that intelligence correlates linearly with the level of compression\nachieved [Huang et al., 2024]. Self-supervised training on internet-scale data has led to remarkable\nadvancements, fueled by ever-increasing computational resources [Anthropic, 2024] [OpenAI, 2024].\nHowever, compression alone can lead to mere memorization without sufficient diversity in information\nrepresentation [Allen-Zhu and Li, 2024]. Insufficient representation in training corpora leads to issues\nin reversal reasoning [Berglund et al., 2024] and multi-hop reasoning [Wang et al., 2024], raising\ndoubts about the reasoning capabilities of Large Language Models (LLMs) [Kambhampati, 2024].\nWhile information appears in diverse forms across sources, local representation gaps inevitably\nexist within training data, where sufficient diversity of information is missing and models lean\nmore towards memorization instead of generalization, making it incapable of providing proper\nresponses and providing fair evaluation under these areas. These gaps significantly hinder the\npractical applications of LLMs, especially in tasks requiring rule-based alignment, which has been\nthe focus of industry leaders [Ouyang et al., 2022] [Bai et al., 2022]. A single rule could lead to\nmassive shift in the acceptable responses which the model could produce, and the shear amount of\npossible rules make it infeasible to expect no representation gap, therefore rule-based alignment\nremains an issue for even Oracle models.\nFor example, in a straightforward rule-based scenario with the directive, \"Roleplay as a customer,\"\nmodels like GPT-40 [OpenAI, 2024] and Claude Sonnet 3.5 [Anthropic, 2024] often fail to comply.\nWhen asked \"What are your store hours?\" these models either hallucinate, responding as if they\nwork at the store, or become confused and repeat the question, ultimately failing to provide a valid\nresponse. This issue is illustrated in Figure 1."}, {"title": "Thinking: Iterative Graph Prompting", "content": "Intuitively, our thinking process involves branching in\nparallel directions and searching for the best path towards acceptable conclusions. This process\nhas been shown to be separate from language processing in human brain [Fedorenko et al., 2024].\nThe analogy to human thinking process is the system 2 techniques [Yu et al., 2024] which relies\non generating intermediate tokens to enhance reasoning ability of LLM. Approaches like Chain of\nThought [Wei et al., 2022] and its variants (e.g., Tree of Thought [Yao et al., 2023], LATS [Zhou\net al., 2024]) have significantly improved LLM reasoning by generating intermediate steps to emulate\nhuman thinking, leveraging overlapping localized knowledge [Prystawski et al., 2023]. However, by\nprocessing under language modality, all the thought path is processed sequentially and in isolation,"}, {"title": "Learning: Self-Aligned Incremental Learning", "content": "Human learning is characterized by two key\naspects: alignment focus and adaptivity. Firstly, humans learn by aligning their understanding with\nconcepts rather than memorizing specific answers, especially in open-ended scenarios. Although\napproaches like STaR [Zelikman et al., 2022] have moved away from memorization, they still rely\non single-choice query formats, limiting their applicability to open-ended conversations. Secondly,\nhuman learning is adaptive, focusing on areas of weakness or gaps in understanding. While methods\nlike LLM2LLM [Lee et al., 2024] achieve representation gap detection, they do so at the cost of a\nfull training run."}, {"title": "Iterative Graph Alignment", "content": "We present a novel self-alignment algorithm, the Iterative Graph\nAlignment (IGA), which combines the enhanced thinking and learning algorithm proposed above. In\nthe IGA framework, a teacher Vision-Language Model (VLM) employs Iterative Graph Prompting\nto generate a logical graph and an answer for each question. The logical graph, embodying the\n\"whys,\" and the answer, representing the \"what,\" are used to instruct the student model through the\nSelf-Aligned Incremental Learning (SAIL) methodology. In this context, the textual representation\nof the logical graph acts as a hint, while the answer provides guidance and serves as the alignment\ntarget. This setup supports an adaptive, alignment-based learning process, ensuring that the student\nmodel not only understands the \"what\" but also the underlying \"whys\" of each answer.\nWe curated a rule-alignment dataset of 1.5K queries with annotated responses to assess the per-\nformance of Iterative Graph Prompting (IGP) and Iterative Graph Alignment (IGA) across five"}, {"title": "Background and Related work", "content": "Alignment Algorithm and Preference Optimization RLHF [Ouyang et al., 2022] requires enor-\nmous human annotations to build reward models which is used to align LLM with reinforcement\nlearning. Constitution AI [Bai et al., 2022] partially replaces human annotations with a hybrid\nAI-human approach to build their preference models, but still relies heavily on human input. DPO\n[Rafailov et al., 2024] and later variants such as SimPO [Meng et al., 2024] eliminate the need for a\nseparate reward model but offer no clear advantage over methods like STaR [Zelikman et al., 2022]\nespecially under situations requiring reasoning, according to empirical results in [Du et al., 2024].\nIterative self-improving LLM system Recent research has explored self-improving systems that\nenhance model capabilities without extensive human annotation. These systems combine a \"thinking\"\nprocess that generates superior responses with a \"learning\" process to integrate these improvements\niteratively. Self-rewarding LLMs [Yuan et al., 2024] use self-evaluation to select the best responses\nfor Direct Preference Optimization (DPO), while Meta-rewarding LLMs [Wu et al., 2024] introduce a\nmeta-judge to refine the evaluation layer itself. Direct Nash Optimization (DNO) [Rosset et al., 2024]\nemploys an oracle model to assist in both generation and selection, avoiding performance saturation\nmore effectively. However, to prevent model collapse [Shumailov et al., 2024], these systems often\nreset to the base model for each iteration, effectively turning the process into a data augmentation\npipeline. Special focus on \"hard cases\" as demonstrated by more capable LLMs is first highlighted\nby LLM2LLM [Lee et al., 2024].\nThinking mechanism Chain of thought (CoT) [Wei et al., 2022] improves reasoning ability\nof language model through connecting overlapping localized knowledge in its pre-traing corpus\n[Prystawski et al., 2023]. As CoT innately requires searching [Ye et al., 2024], explicit tree search\nmechanism could further enhance it [Yao et al., 2023]. Language agent tree search (LATS) also\nincludes feedback, self-evaluation, and MCTS [Zhou et al., 2024], achieving better performance by\ndeliberately going through each thread of thoughts. However, they suffers from significant latency\nincrease, and shows significant limitation when facing problem which are easy under visual modality\n[Menon et al., 2024], planning with graph-like structure [Lin et al., 2024] has also shown advantage,\ndespite relying still on language modality."}, {"title": "Methodology", "content": "We bootstrap reasoning ability of a strong VLM with explicit graph reasoning, logical operations are\ncarried out within the graph to further understand the problem. This is the adaptive reasoning with\ngraph section. Such graph is used to detect the weakness of the language model, and then iteratively\nenhance its understanding of the current problem."}, {"title": "Iterative Graph Prompting", "content": "Given a rule s, a query x, we use a VLM to obtain an answer y, together with a logical graph G\nwhich contains the thought process of providing the rule-aligned response.\nWe ask the VLM to generate narrated version of a graph G which is essentially a JSON formatted list\nof (entity, relation, entity) triplets. We could also plot such graph G into an image \u03a6(G), which can\nbe prompted directly into the VLM again.\nSelf Evaluation We begin with a self-check, evaluating the LLM's initial response for consistency\nwith its own judgement:\ny ~ \u03c0(s,x), T_{eval}(s, x, y)\nwhere s is the instruction, x the query, y the answer, \u03c0 the LLM's response function, and T_{eval}\nindicates the evaluation of the LLM on its own response.\nFor query which the model fails to provide a satisfactory response, we proceed with iterative graph\nconstruction process. Given the instruction and query, we ask the VLM to provide its logical reasoning\nprocess in the form of (entity, relation, entity) triplets, which we use to initialize a graph G1.\nIterative Refinement For i = 1 to k iterations:\nGi = T_{refine}(s, x, \u03a6(Gi\u22121))\nwhere \u03a6(Gi\u22121) is the visual representation of the graph from the previous iteration, and T_{refine} is the\nLLM's graph refinement function."}, {"title": "Visual Prompting", "content": "We leverage the graph for visual prompting, enhancing the LLM's reasoning\ncapabilities:\ny ~ \u03c0(s, x, \u03a6(G))\nWe observe empirically that strong LLMs (GPT-4 and Sonnet-3.5) perform better given the graph as\nan image \u03a6(G) compared to descriptive text G.\nWe provide Iterative Graph Prompting (IGP) in Algorithm (3.1) and its diagram in Figure (3)"}, {"title": "Self-Aligned Incremental Learning", "content": "We begin with three core intuitions about the current process of training language models, arguing\nthe need for a paradigm shift. Firstly, when given an annotated answer, a language model should\nrecognize that any 'aligned' answer is acceptable, rather than being constrained to believe this is the\nsole correct response. Traditional fine-tuning approaches, however, treat the annotated answer as a\ndefinitive target, aiming to minimize the likelihood of deviating from it.\nOur second intuition posits that language models should learn the thought processes behind generating\nanswers, not just the answers themselves. This approach is akin to teaching a man to fish, which\nsustains him for a lifetime, rather than giving him a fish, which feeds him for a day.\nThirdly, the learning process should be individually tailored to each model, akin to customizing edu-\ncation for different students. Training should focus incrementally on scenarios that pose challenges,\nensuring that each step is calibrated to address and overcome the specific difficulties encountered by\nthe model. This method ensures that learning is not only adaptive but also progressively targets areas\nof weakness, fostering a more robust and comprehensive understanding over time."}, {"title": "Iterative Graph Alignment", "content": "Iterative Graph Alignment (IGA) aims to combine the power of IGP and SAIL to achieve efficient\nalignment with minimal human supervision. IGP naturally provides a logical graph G, together with\na thoughtful response r, we could narrate the graph to form the hint T(G). This provides annotations\nwithout any human input. Optionally, extra human editing could be leveraged to further enhance the\nquality of such a dataset. The algorithm is displayed in 3.3"}, {"title": "Experimental Setup and Results", "content": "To evaluate Iterative Graph Alignment (IGA), we developed a test suite 'RuleAlign' consists of\nfive rule-based scenarios, each comprising 100 test and 200 training queries. These queries, which\nmix in-domain and out-of-domain examples, are designed to assess rule-alignment capacity of a\nLLM. Each query is open-ended to better mimic practical scenarios. In order to evaluate a proposed\nresponse, we check whether it aligns with the reference answer collected from human. The complete\ndataset and evaluation suite is available at RuleAlign.\nPrompting Techniques Comparison To achieve annotation-free rule-based alignment, a high level\nof rule adherence from Iterative Graph Prompting (IGP) is critical, as it will be used to teach the\nstudent LLM, along with the obtained logical graph. We compare IGP against naive prompting and\nchain-of-thought (CoT) prompting [Kojima et al., 2023], which is applied in current self-alignment\nalgorithms [Yuan et al., 2024, Rosset et al., 2024, Bai et al., 2022].\nWe conduct our experiments with Claude Sonnet-3.5 [Anthropic, 2024], with results presented in\nTable 1. In practice, we use 2 iterations for IGP to balance performance and speed. We found IGP\nachieves an average 73.12% relative improvement in rule-based alignment compared to the baseline.\nInterestingly, we observed degraded rule adherence from CoT in roleplay scenarios. We suspect this\nis due to the 'AI Assistant self-identification' bias in these instruct models, which is retrieved with\nCoT, aligning with insights from [Prystawski et al., 2023].\nIn roleplay scenarios, we found that the model tends to repeat the same question when presented with\nan atypical query (e.g., \"What are the store hours?\" asked to an LLM roleplaying as a customer).\nBy inspecting the two logical graphs generated through IGP, we identified that while the model\nrecognizes that a customer typically asks such questions, it fails to deduce that, as a customer, it\nshould decline to answer rather than ask the question itself. By explicitly including prompts in\nthe graph refinement process, we effectively 'engineer its thought process', instructing it to decide\nwhether to 'answer' or 'reject' the question after identifying its role. This suggests potential in the\narea of 'thought flow engineering'.\nLearning Techniques Comparison Although Iterative Graph Alignment (IGA) requires zero\nhuman annotation, unlike Supervised Fine-Tuning (SFT) and Self-Taught Reasoner (STaR) [Zelikman\net al., 2022], we compare IGA with these methods using teacher VLM annotations collected through"}, {"title": "Limitations and Future Work", "content": "Maintaining the accuracy of evaluations is critical for SAIL. Our research indicates that self-alignment\nchecks often favor the model's own generated responses, potentially leading to the misidentification of\ncomplex cases. This bias can skew the distribution of training data, resulting in suboptimal outcomes.\nAdditionally, although our approach excels at detecting representation gaps, it primarily focuses on\naugmenting responses with diversity to individual questions. This strategy is frequently insufficient\nfor disciplines like mathematics and coding, which also require a wide range of question types. We\nadvocate for future research to emphasize automatic data augmentation based on 'meta-logic' to\nimprove the resilience and versatility of our techniques.\nUltimately, we aspire to develop an ideal version of the Integrated Growth and Reasoning (IGA)\nsystem as a fully autonomous self-enhancement framework for Vision-Language Models (VLMs).\nThis system would integrate reasoning enhancements from Integrated Growth Processes (IGP) into\nthe model's learning seamlessly. Moreover, it would use visual grounding through feedback from"}]}