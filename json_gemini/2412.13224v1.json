{"title": "Physics-model-guided Worst-case Sampling for Safe Reinforcement Learning", "authors": ["Hongpeng Cao", "Yanbing Mao", "Lui Sha", "Marco Caccamo"], "abstract": "Real-world accidents in learning-enabled CPS frequently occur in challenging corner cases. During the training of deep reinforcement learning (DRL) policy, the standard setup for training conditions is either fixed at a single initial condition or uniformly sampled from the admissible state space. This setup often overlooks the challenging but safety-critical corner cases. To bridge this gap, this paper proposes a physics-model-guided worst-case sampling strategy for training safe policies that can handle safety-critical cases toward guaranteed safety. Furthermore, we integrate the proposed worst-case sampling strategy into the physics-regulated deep reinforcement learning (Phy-DRL) framework to build a more data-efficient and safe learning algorithm for safety-critical CPS. We validate the proposed training strategy with Phy-DRL through extensive experiments on a simulated cart-pole system, a 2D quadrotor, a simulated and a real quadruped robot, showing remarkably improved sampling efficiency to learn more robust safe policies.", "sections": [{"title": "1 Introduction", "content": "Deep reinforcement learning (DRL) has been integrated into many cyber-physical systems (CPS; see examples in Figure 1), defining learning-enabled CPS that have succeeded tremendously in many complex control tasks. Notable examples range from autonomous driving [25, 26] to chemical processes [19, 40] to robot locomotion [22, 28]. Learning-enabled CPS promise to revolutionize many processes in different industries with tangible economic impact [33, 42]. However, the public-facing AI incident database [1] reveals that machine learning (ML) techniques, including DRL, can deliver much high performance but no safety assurance [48]. For instance, in 2023, the US NHTSA reported nearly 224 crashes linked to self-driving and driver-assist technologies within a 9-month period [36]. Hence, a high-performance DRL with enhanced safety assurance is even more vital today, aligning well with the market's need for ML safety."}, {"title": "1.1 Related Work on Safe DRL", "content": "To train a safe DRL policy, many literature adopts a constrained Markov decision process (CMDP) formulation, aiming to find a policy that jointly optimizes the objective of increasing the accumulated reward and decreasing the cost of safety violation [2, 29, 43]. Furthermore, incorporating safety knowledge into the reward function design incentivizes the DRL to learn a safe policy. For instance, the control Lyapunov function (CLF) is widely used in constructing the safety-embedded reward [3, 11, 37, 49]. However, the safety of those learned policies can not be formally guaranteed due to the neural network parameterized policy, whose behaviors are hard to predict [20] and verify [24].\nInstead of focusing on learning a safe policy, the system-level safety framework sandboxes the unverified potential unsafe DRL policies regardless of the concrete design of the learning algorithm, and the safety is assured by an external verified safety controller [8, 14, 21, 45, 50]. However, those frameworks are often sensitive to the changes of the assumed dynamics models during the deployment.\nMoreover, another focus of safe DRL has been shifted to integrating data-driven DRL action policy and physics-model-based action policy, leading to a residual action policy diagram [12, 13, 23, 30, 38]. However, the physics models considered in those works are nonlinear and intractable, which thwarts delivering a verifiable safety, if not impossible. Recently, a physics-regulated deep reinforcement"}, {"title": "1.2 Open Problems", "content": "In particular, incidents of learning-enabled CPS (e.g., self-driving cars) often occur in infrequent corner cases [5, 6, 51]. This underscores that \"corner cases\" induce a formidable safety challenge for DRL and other ML techniques. From a control-theoretic perspective, system-state samples close to the safety boundary represent the corner cases where a slight disturbance or fault can take a system out of control. Intuitively, focusing the training on such corner-case samples will enable a more robust and safe action policy. In the safe DRL community, how to define those corner cases and how to use them for learning safe policies remains unclear."}, {"title": "1.3 Core Contributions", "content": "To bridge the gap of training on corner cases in the existing literature, we propose a formal definition of the worst case for DRL based on the system's dynamics model. Furthermore, we propose an algorithm to efficiently generate the worst cases for policy learning. At last, we integrate the worst-case sampling into the Phy-DRL framework to learn safer and more robust policies. As shown in Figure 1, the integrated Phy-DRL framework defines worst-case samples as the state of the system located on the boundary of a safety envelope. These corner-case samples are not often visited during training via random sampling. Worst-case sampling thus lets Phy-DRL's training focus on the safety boundary, enabling a more robust and safe action policy. We demonstrate the worst-case empowered Phy-DRL in three case studies including a cart-pole system, a 2D quadrotor, and a quadruped robot, showing remarkable improvement in sampling efficiency and safety assurance."}, {"title": "2 Preliminaries", "content": null}, {"title": "2.1 Notations", "content": "We summarize notations used through the paper in Table 1"}, {"title": "2.2 Safety Definition", "content": "The dynamics model of a real plant can be described by\n$s(k + 1) = As(k) + Ba(k) + f(s(k), a(k)), k \\in N$ (1)\nwhere $f(s(k), a(k)) \\in \\mathbb{R}^n$ is the unknown model mismatch, $A \\in \\mathbb{R}^{n \\times n}$ and $B \\in \\mathbb{R}^{n \\times m}$ denote known system matrix and control structure matrix, respectively, $s(k) \\in \\mathbb{R}^n$ is system state, $a(k) \\in \\mathbb{R}^m$ is action. The available knowledge of the model related to the real plant (1) is represented by (A, B). We are interested in an action policy that can constrain the system states to the safety set $\\mathcal{X}$:\n$\\mathcal{X} = \\{s\\in\\mathbb{R}^n | \\underline{v} \\leq D^\\mathsf{T}s - \\underline{v} \\leq \\overline{v}, D \\in \\mathbb{R}^{h\\times n}, with \\underline{v}, \\overline{v}, \\tilde{v} \\in \\mathbb{R}^h\\}$, (2)\nwhere D, $\\underline{v}, \\overline{v}$ and $\\tilde{v}$ are given in advance for formulating $h\\in\\mathbb{N}$ safety conditions. To guarantee the system always stays in the safety set, we introduce a subset of the safety set $\\mathcal{X}$ called safety envelope $\\Omega$ based on Lyapunov-stability theorem.\nSafety Envelope $\\Omega \\triangleq \\{ s\\in \\mathbb{R}^n | s^\\mathsf{T} \\cdot P\\cdot s \\leq 1, P > 0\\}$, (3)\nwhere $P \\in \\mathbb{R}^{n\\times n}$ is a positive definite matrix, that defines the shape of $\\Omega$. With the safety envelope $\\Omega$, the safety problem is defined as the follows:\nDefinition 2.1. [10] Consider the safety set $\\mathcal{X}$ (2) and the safety envelop $\\Omega$ (3). The real plant (1) is said to be safe, if given any $s(1) \\in \\Omega \\subseteq \\mathcal{X}$, the $s(k) \\in \\Omega \\subseteq \\mathcal{X}$ holds for any time $k \\in \\mathbb{N}$.\nTo guarantee a system controlled by a DRL agent staying in the safety envelope $\\Omega$ is non-trivial due to its unverifiable action output. The recent literature Phy-DRL [10] suggests that incorporating the knowledge of the physics model into the standard DRL framework can significantly improve safety assurance toward guaranteed safety. We summarize the Phy-DRL framework in the next section."}, {"title": "2.3 Phy-DRL Agent", "content": "Phy-DRL is built on the deterministic policy algorithms [16, 31]. As shown in Figure 1, its control action is in residual form:\n$a(k) = \\underset{\\text{data-driven}}{a_{drl}(k)} + \\underset{\\text{model-based}}{a_{phy} (k) (:= F\\cdot s(k))}$, (4)\nwhere $a_{drl} (k)$ denotes a date-driven action from DRL, while $a_{phy} (k)$ is a model-based action. Meanwhile, Phy-DRL embeds safety envelope (3) into reward design, creating safety-embedded reward:\n$\\begin{aligned}\nR(s(k), a_{drl} (k)) & \\\\\n= s^\\mathsf{T}(k) \\cdot H \\cdot s(k) &- s^\\mathsf{T}(k + 1) \\cdot P \\cdot s(k + 1) + w(s(k), a(k)), \\\\\nc(s(k), s(k+1))\n\\end{aligned}$ (5)\nwhere the term $w(s(k), a(k))$ aims at high-performance operations (e.g., minimizing energy consumption of resource-limited robots [18, 46]). The term $c(s(k), s(k + 1))$ is safety-critical, in which\n$\\tilde{H} \\triangleq \\alpha P - \\tilde{A}^\\mathsf{T} \\cdot P \\cdot \\tilde{A} > 0$, with $\\tilde{A} = A + B \\cdot F$ and $0 < \\tilde{H} < \\alpha \\cdot P, \\alpha \\in (0,1)$, (6)\nwhere $\\alpha$ is a pre-defined parameter to determine the decrease rate of the Lyapunov value. The matrix P is the matrix for building the safety envelope $\\Omega$ (3) and F is the feedback control law. With the available physics-model knowledge (A, B) at hand, the F and P can be computed using LMI toolbox [7, 17]. We refer interested readers to [10] for a more detailed explanation of LMI formulations.\nThe intuition of the sub-reward $c(s(k), s(k + 1))$ is that we encourage the DRL agent to learn a safe policy in conjunction with the model-based policy $a_{phy}$ to stabilize the real plant (1) toward the equilibrium point."}, {"title": "3 Worst-case Sampling for Phy-DRL Training", "content": "The safety envelope is centered at the control equilibrium point. The plant is more likely to violate the safety constraint when its state is near the envelope boundary. For a DRL-controlled system, it is hard to certify and predict the output of the DRL output due to its non-convexity and non-linearity. Therefore, ensuring the safety of the DRL at the boundary of the envelope is critical. In this section, we propose a definition of the worst conditions for DRL in\na safety-critical system and a practical algorithm to generate these conditions for DRL training and testing.\nDefinition 3.1 (Worst-case conditions). Referring to the safety envelop in Equation (3), a state s is said to be a worst-case condition if $s^\\mathsf{T} \\cdot P \\cdot s = 1$ (i.e., locating on the boundary of safety envelope).\nRecall that, given n dimensional safety constraints, the solution for the safety envelope becomes a n dimensional hyperellipsoid, to generate worst-case samples referring to Definition 3.1, we need to solve $s^\\mathsf{T} \\cdot P \\cdot s = 1$, where $s \\in \\mathbb{R}^n$. To achieve this, we present the following lemma for having explicit solutions of worst-case conditions.\nLemma 3.2. Given $P > 0$, the solution of $s \\in \\mathbb{R}^n$, being subject to $s^\\mathsf{T} \\cdot P \\cdot s = q$, is\n$s = Q(P) \\cdot y$, with $[y]_i = $\n$\\begin{cases}\n$\\frac{\\varphi}{\\sqrt{\\lambda_i(P)}}$ sin(\\theta_1). \\prod_{m=2}^{p-1} sin(\\theta_m), & i = 1 \\\\\n$\\frac{\\varphi}{\\sqrt{\\lambda_i(P)}}$ cos(\\theta_{i-1}). \\prod_{m=i}^{p-1} sin(\\theta_m), & i \\geq 2\n\\end{cases}$ (7)\nwhere $Q(P)$ is P's orthogonal matrix, and $\\lambda_i(P)$ is the i-th eigenvalue of matrix $P \\in \\mathbb{R}^{n \\times n}$.\nPROOF. See Appendix A.2.\n$\\sum_{i=1}^{3} \\frac{\\lambda_i (P)}{\\varphi} y_i^2 = 1, y = Q^T (P) \\cdot s$ (8)\nfor which we define\n$\\tilde{\\lambda_i (P)} = \\frac{\\lambda_i (P)}{\\varphi} y_i,$ (9)\nin light of (9), Equation (8) can be rewritten as\n$\\tilde{y_1}^2 + \\tilde{y_2}^2 + \\tilde{y_3}^2 = 1,$ (10)\nwhich describes a sphere in $\\mathbb{R}^3$ space, as shown in Figure 2 (a). From Figure 2 (b), we notice that every point on the sphere can be parameterized using angles $\\theta_1$ and $\\theta_2$ as:\n$\\begin{cases}\n\\tilde{y_i} = sin(\\theta_2) sin(\\theta_1), & i = 1 \\\\\n\\tilde{y_i} = sin(\\theta_2) cos(\\theta_1), & i = 2 \\\\\n\\tilde{y_i} = cos(\\theta_2), & i = 3.\n\\end{cases}$ (11)\nBy selecting different values for $\\theta_1$ and $\\theta_2$, we can sample any point [$\\tilde{y_1}, \\tilde{y_2}, \\tilde{y_3}$] on the sphere and, consequently, obtain the conditions on the boundary of the safety envelop (indicated by Equation (8) and Equation (9))\nWe now are ready to propose an algorithm to automatically generate sampling conditions located at the safety envelope's boundary. Moreover, we design a training curriculum to periodically visit the worst-case conditions for policy learning.\nAs shown in Algorithm 1, the proposed algorithm includes worst-case condition generation Line 3-Line 15, and training curriculum Line 16 - Line 21. In worst-case condition generation, we sample $\\theta_1, \\theta_2,..., \\theta_{n-1}$ sparsely in the interval $[0, 2\\pi)$. This is motivated"}, {"title": "Algorithm 1 Periodic and Worst-case Sampling for Phy-DRL Training", "content": "1: Input: System-state dimension $n \\in \\mathbb{N}$; sample numbers $q_r \\in \\mathbb{N}$, $r = 1, ..., n - 1$; matrix P; parameter $q = 1$, Period number $p \\in \\mathbb{N}$.\n2: Initialize boundary set: $\\mathcal{B}_P \\leftarrow \\emptyset$;\n3: for $\\theta_1 = 0 : \\frac{2\\pi}{q_1} : (2\\pi - \\frac{2\\pi}{q_1})$ do $\\triangleright$ Generating worst-case conditions\n4: Set: $\\theta_2 = 0$, $\\theta_3 = 0,..., \\theta_{n-1} = 0$;\n5: Generate $s \\in \\mathbb{R}^n$ by Equation (7);\n6: Update set: $\\mathcal{B}_P \\leftarrow \\mathcal{B}_P \\cup \\{s\\}$;\n7: for $\\theta_2 = 0 : \\frac{2\\pi}{q_2} : (2\\pi - \\frac{2\\pi}{q_2})$ do\n8: .\n9: for $\\theta_{n-1} = 0 : \\frac{2\\pi}{q_{n-1}} : (2\\pi - \\frac{2\\pi}{q_{n-1}})$ do\n10: Generate $s \\in \\mathbb{R}^n$ by Equation (7);\n11: Update set: $\\mathcal{B}_P \\leftarrow \\mathcal{B}_P \\cup \\{s\\}$;\n12: end for\n13: .\n14: end for\n15: end for\n16: for j = 1 to p do $\\triangleright$ Start training curriculum\n17: for $s \\in \\mathcal{B}_P$ do\n18: Set $s(1) \\leftarrow s$ for system in Equation (1);\n19: Train (test) Phy-DRL agent for one episode;\n20: end for\n21: end for\nconditions commonly used in literature [27, 32, 35]. We evaluate these two condition sampling strategy on simulated cart-pole system, a 2D quadrotor, and a quadruped robot, and we cite the following safety samples from [10] for defining safety metrics.\nInternal-Envelope (IE) sample s:\nif $s(1) = s \\in \\Omega$, then $s(k) \\in \\Omega, \\forall k \\in \\mathbb{N}$. (13)\nExternal-Envelope (EE) sample s:\nif $s(1) = s \\in \\mathcal{X}$, then $s(k) \\in \\mathcal{X} \\setminus \\Omega, \\exists k \\in \\mathbb{N}$. (14)\nIntuitively, IE samples means that the system starts from the safety envelope and it always stay in the safety envelope. EE means that the system starts from the safety set but not in the safety envelope and always stays in the safety set."}, {"title": "4.1 DRL Policy Setup", "content": "We implement our policy using Phy-DRL framework [10], where the $a_{drl}$ is implemented based on DDPG algorithm [31]. The action value function and actor network are both parameterized using a multi-layer-perceptions (MLP) model. The reward function is designed as the format of Equation (5), where the matrix P and H are obtained as in [9] by solving LMI problems for each robot using their own linear dynamics models."}, {"title": "4 Experiments", "content": "In this section, we evaluate the effectiveness of the proposed training algorithm using worst-case conditions and uniform sampled"}, {"title": "4.2 Cart pole", "content": "In the cart pole case study, the objective is to learn a safe policy that stabilizes the pole from as many initial conditions as possible without violating safety constraints. For Algorithm 1, we let p = 2 and $q_1 = q_2 = q_3 = 5$, which leads to in total 170 episodes, calculated using (12). For the uniform sampling scheme, we let the initial position, velocity, angle, and angular velocity be uniformly sampled over the intervals [-0.9, 0.9], [-3.0, 3.0], [-0.8, 0.8], and [-4.5, 4.5], respectively. The bounds of intervals are the same as those of the safety envelope used for worst-case conditions generations. For training, the maximum length of one episode is 500"}, {"title": "5 Conclusion and Discussion", "content": "This paper proposes the sparse worst-case sampling for Phy-DRL training. The particular design aims of worst-case sampling include i) automatically avoiding state samples that are physically infeasible and ii) focusing the training on corner cases represented by worst-case samples. The spare worst-case sampling makes the Phy-DRL features much more efficient and fast training towards safety guarantee.\nUnder worst-case sampling, a potential negative issue could be increased instability of the model-based policy in Phy-DRL, when training starts from boundary. To address this, Phy-DRL shall run on a fault-tolerant software architecture called Simplex [41]. In Simplex, we use the Phy-DRL as the complex and high-performance controller, which may have unknown defects. Meanwhile, Simplex's high-assurance controller (HAC) is function-reduced and simplified but verified, and it only guarantees the system's basic stable and safe operations. HAC is thus complementary to Phy-DRL and coordinated by a monitor. For example, the monitor triggers the switch from Phy-DRL to HAC once the real-time system states (under the control of Phy-DRL) leave the safety envelope. In other words, the HAC takes over at the cost of lower performance. When the system returns to the safety set, the Phy-DRL can be restarted and control can be retaken."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Auxiliary Lemmas", "content": "Lemma A.1 (Positive Definiteness [4]). A matrix A \u2208 Rnxn is called positive definite if it is symmetric and all its eigenvalues are positive. In other words, there exists an orthogonal matrix Q \u2208 Rn\u00d7n, such that\n$\\begin{aligned}\nQ^T A Q = \\begin{pmatrix}\n\\lambda_1 & 0 & \\cdots & 0 \\\\\n0 & \\lambda_2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\lambda_n\n\\end{pmatrix},\n\\end{aligned}$ (17)\nwith $\\lambda_1 > 0, \\lambda_2 > 0, ..., \\lambda_n > 0$."}, {"title": "A.2 Proof of Lemma 3.2", "content": "The P > 0 means the matrix P is positive definite. In light of Lemma A.1 in Appendix A.1, there exists an orthogonal matrix Q(P) \u2208 Rnxn, such that\n$\\begin{aligned}\nQ^T(P) P Q(P) = \\begin{pmatrix}\n\\lambda_1(P) & 0 & \\cdots & 0 \\\\\n0 & \\lambda_2(P) & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\lambda_n(P)\n\\end{pmatrix},\n\\end{aligned}$ (18)\nwith $\\lambda_1(P) > 0, \\lambda_2(P) > 0, ..., \\lambda_n(P) > 0$.\nConsidering Equation (19), the $s^\\mathsf{T} \\cdot P \\cdot s = q$ equates to\n$s^\\mathsf{T} \\cdot P \\cdot s = s^\\mathsf{T} \\cdot Q(P) \\cdot \\Lambda(P) \\cdot Q^\\mathsf{T}(P) \\cdot s = \\varphi,$ (20)\nwhose transformation utilizes a well-known property of orthogonal matrix: $Q^\\mathsf{T}(P) = Q^{-1}(P)$.\nLet us define $y \\triangleq Q^\\mathsf{T}(P) \\cdot s$. Recalling the $\\Lambda(P)$ defined in Equation (19), Equation (20) equivalently transforms to\n$\\sum_{i=1}^{n} \\frac{\\lambda_i (P)}{\\varphi} y_i^2 = 1.$ (21)"}, {"title": "A.3 Ablation study", "content": ""}]}