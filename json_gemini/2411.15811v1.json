{"title": "FastTrackTr:Towards Fast Multi-Object Tracking with Transformers", "authors": ["Liao Pan", "Yang Feng*", "Wu Di", "Yu Jinwen", "Zhao Wenhui", "Liu Bo"], "abstract": "Transformer-based multi-object tracking (MOT) methods have captured the attention of many researchers in recent years. However, these models often suffer from slow inference speeds due to their structure or other issues. To address this problem, we revisited the Joint Detection and Tracking (JDT) method by looking back at past approaches. By integrating the original JDT approach with some advanced theories, this paper employs an efficient method of information transfer between frames on the DETR, constructing a fast and novel JDT-type MOT framework: FastTrackTr. Thanks to the superiority of this information transfer method, our approach not only reduces the number of queries required during tracking but also avoids the excessive introduction of network structures, ensuring model simplicity. Experimental results indicate that our method has the potential to achieve real-time tracking and exhibits competitive tracking accuracy across multiple datasets.", "sections": [{"title": "1. Introduction", "content": "Multi-object tracking (MOT) has long been a computer vision task of great interest to researchers and developers, with extensive application prospects across various industries. Currently, the mainstream tracking models are primarily online models, and the paradigm that has garnered widespread attention in recent years within online models remains tracking-by-detection (TBD). This paradigm, originating from SORT [1], has stood the test of time and demonstrates remarkable vitality. Its improved derivative methods [7, 19, 41] continue to be considered state-of-the-art (SOTA).\nIn recent years, some tracking methods based on transformer [34] have also attracted considerable attention. However, these methods [10, 11, 20, 31, 39, 42, 43] face a significant challenge: suboptimal inference speed. There are various reasons for this, such as the slow performance of the MOTR series, which relies on query-based tracking. The main reason is not the network structure, but issues related to hardware acceleration caused by the variable number of queries (detailed justification can be found in the appendix). Both our experiments and other works [22] have also demonstrated that the inference speed after lightening the network is still not very ideal. Regarding another recent method, MOTIP [11], while it addresses many of MOTR's issues, its main problem lies in its complex network structure."}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Transformer-based MOT Methods", "content": "The earliest Transformer-based MOT methods were TransTrack [31] and TrackFormer [20]. The former is a JDT model, while the latter introduced the concept of track queries, utilizing these special queries to achieve tracking. Subsequently, MOTR [43] proposed tracklet-aware label assignment (TALA), which optimized label assignment during the training phase to achieve end-to-end tracking. Later improvements to MOTR, such as MOTRv2 [47], MeMOTR [10], MOTRv3 [42], and CO-MOT [39], demonstrated the powerful tracking performance of the MOTR paradigm. Some efforts have been made to address MOTR's slow inference speed through model lightweighting, but the results have been less than ideal. Recently, alternative Transformer architectures have been proposed. MOTIP's [11] novel approach of directly predicting tracked object ID numbers is refreshing, while PuTR's [14] use of Transformers to directly construct a tracking-by-detection paradigm tracker has also been inspirational."}, {"title": "2.2. Tracking-by-Detection", "content": "The TBD paradigm can be considered one of the simplest and most practical tracking methods. Since it essentially associates detected targets, this class of methods often yields satisfactory tracking accuracy as long as the detection results are not poor.\nThe initial SORT algorithm simply utilized the motion state of detected objects for association. Subsequent models like DeepSORT [36] and ByteTrack [46] improved tracking accuracy by enhancing detection precision and employing embedding-based Re-identification (Re-id) networks. Some approaches, like OC-SORT [3] and HybridSORT [41], improved upon SORT using purely kinematics-based methods. Later models such as MotionTrack [24], DeepOC-SORT [19], and SparseTrack [16] demonstrated that simultaneous improvements in both Re-id network utilization and kinematic models yield the most significant performance enhancements.\nCurrently, there are also algorithms that directly use networks for association. Besides the aforementioned PuTR [14], SUSHI [5] is a tracking method based on graph neural networks."}, {"title": "2.3. Joint Detection and Tracking", "content": "Joint detection and tracking aims to achieve detection and tracking simultaneously in a single stage. This approach was common a few years ago but has been seen less frequently in recent years. Here, we briefly review its development history. D&T [8] first proposed a multi-task architecture based on frame-based object detection and cross-frame trajectory regression. Subsequently, Integrated-Detection [52] improved detection performance by combining detection bounding boxes in the current frame with trajectories from the previous frame. Tracktor [30] directly used tracking boxes from the previous frame as candidate regions, then applied bounding box regression to provide tracking boxes for the current step, thus eliminating the box association process. JDE [35] and FairMOT [45] learned object detection and appearance embedding tasks from a shared backbone network. CenterTrack [49] localized targets through tracking-conditioned detection and predicted their offsets from the previous frame. TransTrack used two decoders to handle tracking and detection separately, then matched and associated the results from both decoders. Our FastTrackTr, by introducing a novel decoder, returns to the essence of the JDT method, simplifies the network structure, and achieves fast tracking."}, {"title": "3. Method", "content": ""}, {"title": "3.1. FastTrackTr Architecture", "content": "The overall structure of FastTrackTr is illustrated in Figure 2. In frame T1, a standard decoder is used to initialize the initial historical information. Subsequently, in the following frames, a Historical Encoder is employed to initialize this historical-representative information. In the next frame, a Historical Decoder, which includes an additional cross-attention layer compared to normal decoder layers, processes this historical historical information. The tracking principle is very similar to that of JDT and FairMOT, utilizing an additional ID Embedding head to obtain the object\u2019s appearance embedding, which is then used along with detection results for association and matching. It is worth noting that the backbone network is ResNet [13]."}, {"title": "3.2. Learning Detection and ID Embedding", "content": "To enable our FastTrackTr to simultaneously output both the target\u2019s position and appearance embedding in a single forward pass, we formulate the problem as follows: Suppose we have a training dataset \n                                                                                                                                                                        { I , B , y }\n                                                                                                                                                                  N\n                                                                                                                                                                     i = 1\n                                                                                                                                                                  .\nHere,  I \u2208\nR\nc \u00d7 h \u00d7 w represents an image frame, and B \u2208\nR\nk\u00d74 denotes the bounding box annotations for the k targets in this frame. y \u2208 Z\nk represents the partially annotated identity labels, where -1 indicates targets without an identity label. We hope model can output predicted bounding boxes\nB\u02c6 \u2208\nR\nk\u02c6\u00d74 and appearance embeddings F\u02c6 \u2208\nR\nk\u02c6\u00d7D, where D is the dimension of the embedding.\nFor the output box, this requirement is easily met, as DETR[4] itself is a detection model. To achieve rapid performance, this paper employs RT-DETR [48] as the base model, which inherently possesses state-of-the-art detection capabilities. We have not made significant modifications to its detection component. The loss function for this part remains consistent with RT-DETR, as shown below:\nLdet = L(\u02c6y, y) = Lbox(\u02c6b, b) + Lcls(\u02c6c, \u02c6b, y, b)\n= Lbox(\u02c6b, b) + Lcls(\u02c6c, c, IoU)   (1)\nwhere y\u02c6 and y denote prediction and ground truth, y\u02c6 =\n{c,  \u02c6\nb} and y = {c, b}, c and b represent categories and bounding boxes, respectively. Similar to RT-DETR, we incorporate the IoU score into the objective function of the classification branch, following the approach from VFL [44], to enforce consistency between the classification and localization of positive samples. Lbox includes the L1 loss and the generalized IoU loss [25], which are not described here. And, the auxiliary loss from DAB-DETR [15] is also employed in our approach.\nThe second objective addresses a metric learning problem, specifically the learning of an embedding space where instances of the same identity are close to each other while those of different identities are far apart. Previous methods like JDE [35] and FairMOT [45] used one-hot labels. While this approach is simple and effective, it requires prior knowledge of the number of IDs in the training set to configure the output dimensions of the ID embedding head. This requirement obviously limits the model\u2019s generalization capability and becomes impractical in dense scenarios, as it leads to excessively high embedding dimensions. Moreover, subsequent Transformer-based tracking models predominantly rely on queries for tracking, and the drawbacks of this method have been discussed earlier.\nMOTIP [11] employs a predefined ID dictionary and classification methods for object association, whereas PuTR utilizes an association method based on embedding similarity. These two approaches are undoubtedly more generalizable. However, possibly because their decoders primarily process the trajectories of object movements when outputting embeddings, and since the queries used by FastTrackTr to generate embeddings contain more image features and positional information of the current target, these methods fail to enable FastTrackTr to achieve satisfactory convergence.\nTo address the aforementioned issues, we need to find a method more suitable for image features and with greater generalizability for generating ID embeddings. Therefore, we turn our attention to the training methods of ReID (Reidentification) models. By converting the generation of ID embeddings into a ReID problem, we only need to consider the distances and similarities between embeddings, thereby enhancing the model\u2019s generalization capability.\nFor the above reasons and for better performance, in this paper, we adopt the more advanced circle loss [33] in this"}, {"title": "3.3. Historical Decoder and Encoder", "content": "Historical Decoder: Previous DETR-based MOT models[10, 20, 43] typically feed the final output queries of the previous frame, after some processing, directly into the decoder of the current frame to achieve tracking. This is an explicit tracking method, but it increases the number of queries, thereby increasing computational costs. To more efficiently utilize these historical queries, we replaced the self-attention mechanism in DETR's decoder with a cross-attention mechanism to process the query features between consecutive frames. This significantly reduces the computational load. The differences between our decoder and the standard decoder are shown in Figure 3.\nSpecifically, each decoder module in DETR [4] consists of a sequential structure comprising a self-attention layer for the queries, a visual cross-attention layer, and a feedforward neural network (FFN). Initially, the self-attention layer performs global modeling over the queries to enhance the model's object detection performance. To further leverage historical information and improve tracking accuracy, we concatenate the final decoder output from frame t -1, denoted as q-1', which is processed by the trajectory encoder, with the initial queries q\u2081 of frame t. This concatenation replaces the key and value in the self-attention layer while keeping the queries unchanged. The implementation can be formally expressed as follows:\nQT = Linear(q), KT = Vr = Linear(concat(qa, q-1'))   (6)\nwhere QT \u2208 RN\u00d7C, KT and VT \u2208 R2N\u00d7C. Then, we calculate the query-trajectory attention map AT \u2208 R2N\u00d7N, and aggregate informative depth features weighted by \u0410\u0442 to produce the Historical-aware queries q', formulated as:\nAT = Softmax (QTKT/VC)\nq' = Linear (ATVT)   (7)\nThis mechanism allows each object query to adaptively capture features of the same object from images across consecutive frames, thereby achieving better object understanding. The queries with estimated historical information are then input into the inter-query self-attention layer for feature interaction between objects, and the visual cross-attention layer for collecting visual semantics. We use 6 decoder layers.\nHistorical Encoder: In FastTrackTr, we have introduced an additional Historical Encoder to enhance the modeling of temporal relationships, and we utilize a masking mechanism during the self-attention process to provide contextual priors for the tracked objects. For the historical feature information from the previous frame or multiple frames, only a subset of the data is utilized. To effectively model these historical data, we employ a masking mechanism and process the data through standard encoder layers. During training, the generation of masks is determined by the ground truth labels, while in inference, masks are based on confidence scores to decide which objects to retain.\nAfter generating the mask, we fuse the decoder output qf with the historical information q\u00b9 through an attention mechanism, computed as follows:\nQE KE -1 -1 Linear (q + qpos), VE Linear(q)   (8)\nThe attention calculation follows the same formula as previously used in the decoder, which is not repeated here. Finally, the fused features are processed by an additional network layer to obtain qf. It is noted that for t = 1,\nq1 = q.\nThe mask generation module in this method includes two modes: (1) In the training phase, the mask is determined by ground truth information, generated by checking if each"}, {"title": "3.4. Matching and Association", "content": "Although matching and association and not the primary focus of our paper, it is essential to outline some details of this process. Following the work of JDE, for a given video, after processing each frame, the model outputs detection results along with their corresponding ID embeddings. We then use these values to compute a similarity matrix between the embeddings of the observations and those of the existing tracking chains. The Hungarian algorithm is employed to assign observations to tracking chains. Subsequently, a Kalman filter is used to smooth the trajectories and predict the position of the previous frame's tracking chain in the current frame. If the assigned observations are spatially distant from the predicted position, the assignment is rejected. Finally, the embeddings of the tracking chains are updated as follows:\nft = nft\u22121 + (1 \u2212 n) f   (9)\nwhere f indicates the embedding of the assigned observation, and ft indicates the embedding of the tracklet at"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Setting", "content": "Datasets: Our proposed method mainly evaluated on DanceTrack [32] and SportsMOT [6], two recently introduced datasets with large-scale training data, which helps prevent overfitting. Both datasets also provide official validation sets, allowing for exploratory experiments. Additionally, we present results on the MOT17 [21] benchmark.\nMetrics: We assessed our method using widely recognized MOT evaluation metrics. The primary metrics employed for evaluation included HOTA, AssA, DetA,IDF1 and MOTA [18, 26]. These metrics collectively provided a comprehensive and robust evaluation of our method's tracking performance.\nImplementation Details: For inference speed, our FastTrackTr is built on RT-DETR [48], with ResNet50 [13] as the backbone unless specified otherwise. The model is implemented in PyTorch [23], and the training is conducted on 8 NVIDIA RTX 4090 GPUs. The sequence length for training is set to 8 unless otherwise noted.\nAll ablation experiments use an image size of 640 \u00d7 640. For further testing of the best tracking accuracy of our method, we use an image size of 800 \u00d7 1333 when comparing with SOTA methods. The Trajectory Encoder during training applies random mask transformations, with the probability related to the number of ground-truth objects. Given the number of ground-truth objects ngt and the number of queries ng, the probability of a non-masked flip is min(0.5,2.ngt/nq), while for the rest, the probability is min(max(0.1, ngt/nq),0.2). For all three datasets, nq is set to 300. Dancetrack and SportMOT are trained for 27 epochs, while MOT17 is trained for 80 epochs. The optimizer is AdamW [17] with a learning rate of 10-4 and weight decay of 5.0 \u00d7 10-4. Due to GPU memory limitations, the batch size is set to 4 for smaller images and 1 for larger images. Lastly, for all datasets, the final output dimension of ID embedding is 256."}, {"title": "4.2. Main Result", "content": "Comprehensive Comparison of Speed and Accuracy:\nTable 1 highlights FastTrackTr's superior latency performance compared to existing methods on the Dancetrack validation set. FastTrackTr consistently delivers faster processing speeds while maintaining competitive HOTA scores,"}, {"title": "4.3. Ablation Study", "content": "We tested the impact of various components of our model and other factors on performance on the val set of DanceTrack.\nThe ablation study in Table 5 highlights the progressive enhancements of FastTrackTr on the DanceTrack validation set. Starting with a baseline HOTA of 47.3, adding the Decoder increases HOTA to 51.6, AssA to 37.2, and IDF1 to 51.3. Incorporating both Decoder and Encoder further boosts HOTA to 53.8, AssA to 39.9, and IDF1 to 53.5. Finally, including the Mask module achieves the highest scores with HOTA of 54.1, AssA of 40.0, and IDF1 of 54.7. These results demonstrate that each component significantly contributes to the overall tracking performance of FastTrackTr."}, {"title": "5. Conclusion and Limitations", "content": "Conclusion: In this paper, we introduce a rapid tracking method, FastTrackTr, which demonstrates superior performance across multiple datasets. By incorporating novel trajectory encoder and decoder designs, we effectively enhance the accuracy and robustness of tracking. Additionally, FastTrackTr achieves significant improvements in computational efficiency, presenting substantial potential for real-time applications. Future work will focus on further optimizing the model structure to adapt to more complex scenarios and exploring deployment possibilities across different hardware platforms.\nLimitations: The primary limitation of FastTrackTr is its tracking accuracy, which does not surpass that of MOTIP. This issue likely arises because MOTIP employs a specialized network structure for ID encoding and its training method for long sequences, which ensures better temporal information learning. As the saying goes, \"there are trade-offs\"; these aspects also result in poor inference speed and slow training for MOTIP. Conversely, FastTrackTr does not exhibit these drawbacks."}, {"title": "A. Apologies for formatting errors", "content": "Due to some reasons that are difficult to mention, there are some formatting errors in the main text. For example, the historical decoder is mistakenly referred to as the trajectory decoder in some places, and there may be other issues as well. If this has affected your review process, please forgive us. We sincerely apologize here"}, {"title": "B. More discussion on FastTrackTr", "content": ""}, {"title": "B.1. Description of calculations", "content": "In this section, we analyze the computational complexity of FastTrackTr and MOTR. The goal is to determine the threshold at which the computational cost of the MOTR exceeds that of our FastTrackTr when the number of queries increases. We focus on the decoder layer, including the self-attention, cross-attention, feed-forward network (FFN), and normalization layers."}, {"title": "B.1.1. Model Definitions", "content": "MOTR (Variable Query Number): The number of queries increases from N to Nq = N + \u2206N, where \u0394\u039d is the additional number of queries. Both the self-attention and subsequent components are affected by this increase.\nFastTrackTr (Modified Self-Attention): The selfattention mechanism is altered such that the key and value tensors have dimensions 2N \u00d7 C, while the query tensor remains N \u00d7 C. Other components retain their original computational complexity."}, {"title": "B.1.2. Notations", "content": "\u2022\n    N: Original number of queries.\n\u2022\n    AN: Additional queries in MOTR (Nq\n\u2022\n    C: Dimensionality of each query.\n\u2022\n    M: Length of the memory (number of keys/values in cross-attention).\n\u2022\n    dff: Dimensionality of the FFN's hidden layer (typically 4C)."}, {"title": "B.1.3. Computational Complexity Analysis", "content": "MOTR:\n1. Self-Attention:\nOSA\u2081 = (\u039d + \u0394\u039d)\u00b2 \u00b7 C   (10)\n2. Cross-Attention:\nOCA1 = (N + \u2206N) \u00b7 M \u00b7 C   (11)"}, {"title": "B.1.4. Inequality for Computational Cost Comparison", "content": "To find the threshold AN where 01 > O2, we set up the inequality:\n(N + \u2206N)\u00b2 \u00b7 C + (N + \u2206N) \u00b7 M \u00b7 \u0421\n+ 2(\u039d + \u0394\u039d) \u00b7 C \u00b7 dff + 2(N + \u2206\u039d) \u00b7 C\n> 2N2 C + N \u00b7 M \u00b7 C + 2N \u00b7 C \u00b7 dff + 2N \u00b7 C   (20)\nSimplifying the inequality by eliminating common terms and dividing both sides by C:\n(N + \u2206\u039d)2 \u2013 2N\u00b2 + (N + \u2206N \u2013 N) \u00b7 \u041c\n+ 2(N + AN \u2013 N) \u00b7 dff + 2(N + \u2206N \u2013 N) > 0\n\u21d2(N + \u2206\u039d)2 \u2013 2N\u00b2 + \u2206N \u00b7 (M + 2dff + 2) > 0   (21)"}, {"title": "B.1.5. Determining the Threshold AN", "content": "To find the minimum AN satisfying the inequality, we proceed as follows.\n\u2212N\u00b2 + \u2206N (2N + \u2206N + M + 2dff + 2) > 0   (25)"}, {"title": "B.1.6. Final Solution with Specific Numerical Values", "content": "Assuming typical values for a Deformable DETR decoder layer:\n\u2022\n    Number of original queries: N = 300\n\u2022\n    Memory length: M = 8400\n\u2022\n    Query dimensionality: C = 256\n\u2022\n    FFN hidden dimension: dff = 4C = 1024\nSubstituting the numerical values:\n-(300) 2 + \u0394\u039d (2 + 2 x 1024 + 2) > 0   (26)\nComputing constants:\n-90,000 + \u0394\u039d (600 + \u0394\u039d + 8400 + 2048 + 2) > 0   (27)\nSimplifying the expression inside the parentheses:\n-90,000 + \u0394\u039d (\u0394\u039d + 11,050) > 0   (28)\nForming a quadratic inequality:\n(\u0394\u039d)\u00b2 + 11,050\u0394\u039d \u2013 90,000 0   (29)\nSolving the quadratic equation:\n(\u0394\u039d)\u00b2 + 11,050\u0394\u039d \u2013 90,000 = 0   (30)\nUsing the quadratic formula:\n\u0394\u039d =-11,050 \u00b1 \u221a(11,050)2 \u2013 4 \u00d7 1 \u00d7 (-90,000) / 2   (31)\nCalculating the discriminant:\nD = (11,050)\u00b2 + 360,000\n= 122, 102, 500 + 360,000 = 122, 462, 500   (32)\nFinding the square root:\nD\u2248 11,062.83   (33)\nDetermining the positive root:\n\u0394\u039d = -11,050 + 11,062.83 / 2 = 12.83 / 2 \u22486.415   (34)\nThe computational cost of MOTR exceeds that of FastTrackTr when the number of additional queries satisfies AN > 7. This indicates that increasing the number of queries by at least seven will result in higher computational complexity for MOTR compared to FastTrackTr, considering all components of the decoder layer.\nIn summary, the computational load of FastTrackTr is generally lower than that of MOTR, except in scenarios where there are very few objects to track; in those cases, the theoretical computation may be slightly higher for FastTrackTr. Otherwise, FastTrackTr consistently requires less computational effort. Additionally, in practical applications, MOTR tends to be significantly slower due to memory constraints. For a detailed analysis, please refer to Section. C."}, {"title": "B.2. Possibilities of End-to-end", "content": "We originally developed FastTrackTr with the goal of proposing a real-time, end-to-end multi-object tracking model. However, we observed that eliminating one decoder and directly applying MOTIP's ID loss resulted in significant challenges in model convergence. Upon further analysis, we determined that although MOTIP's ID loss appears similar to the appearance loss used in the previous JDT models, they fundamentally differ.\nFor models like JDE [35], the focus is on the similarity of object appearances, which does not incorporate specific object trajectories. In contrast, the ID decoder in MOTIP primarily handles trajectory information, with image data serving only as a supplementary element. The function and training method of this decoder might be similar to the entire PuTR model. Although our FastTrackTr attempts to integrate some temporal information by passing queries from one frame to the next, these queries predominantly contain image features. Under such conditions, supervising motion trajectories directly using MOTIP's method becomes less effective, leading to poor or nearly impossible model convergence.\nFollowing this analysis, we attempted to incorporate more historical information into the decoder. Our preliminary design introduces an identity-specific decoder layer"}, {"title": "C. How to Improve MOTR inference Speed", "content": "Although we have demonstrated in the previous sections that our FastTrackTr model generally requires less computation than the MOTR series of models, there remains an issue that puzzles us. Under the same configuration, models such as Deformable-DETR, MOTIP, and our FastTrackTr typically push GPU utilization to around 90%. However, MOTR only reaches about 40% on the same hardware (e.g., an RTX 4090), which is clearly problematic.\nThis led us to speculate whether the issue might be due to the dynamic nature of the query system, which could result in variable memory usage. As shown in Figure 6, we compare the memory consumption of the decoder when using fixed queries versus dynamic queries. After further investigation, we found that this is indeed a plausible cause. In PyTorch, memory is managed through cudaMalloc and cudaFree, and this dynamic memory management approach could lead to frequent memory allocation calls in models like MOTR, where memory demand fluctuates. This could slow down the model and prevent full GPU utilization.\nWe experimented with PyTorch's memory management code, and observed that when the tensor size remains relatively constant, there are significantly fewer memory allocation and release calls. The specific code we tested can be easily found in GitHub, when you search for 'torch mem'(we would like to note that the owner of that link is not associated with the authors of this paper.\nSo how can we address this issue? Modifying PyTorch's underlying logic directly is quite complex and would likely require intervention from PyTorch's developers (we are preparing to submit an issue on GitHub or send an email to the maintainers). An alternative approach could be to fix the number of tracking queries in MOTR and use masks to differentiate between empty tracking queries and those that are actively used. This may resolve the issue. As shown in Figure 7, we trained an updated version of MOTR for one epoch on the DanceTrack dataset and tested the speed and GPU utilization. Due to limitations in funding, equipment, and time, we did not fully train the model to evaluate its performance. However, preliminary results show promise: after one epoch, the HOTA score on the validation set reached 18.5 and fps has increased from 16.5 to 25 with fp32 precision. Note that we have not reduced any network structure at this time. Interested readers are welcome to experiment with this approach themselves. It is relatively straightforward to implement, though we advise against using GPUs with less than 24GB of memory, as this may lead to out-of-memory errors.\nOf course, this is just one possible explanation. Other factors may also contribute to the underutilization of GPU resources in MOTR-like models. If any readers have alternative insights, we welcome further discussion and collaboration."}]}