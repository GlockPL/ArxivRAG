{"title": "Robot Instance Segmentation with Few Annotations for Grasping", "authors": ["Moshe Kimhi", "David Vainshtein", "Chaim Baskin", "and Dotan Di Castro"], "abstract": "The ability of robots to manipulate objects relies heavily on their aptitude for visual perception. In domains characterized by cluttered scenes and high object variability, most methods call for vast labeled datasets, laboriously hand-annotated, with the aim of training capable models. Once deployed, the challenge of generalizing to unfamiliar objects implies that the model must evolve alongside its domain. To address this, we propose a novel framework that combines Semi-Supervised Learning (SSL) with Learning Through Interaction (LTI), allowing a model to learn by observing scene alterations and leverage visual consistency despite temporal gaps without requiring curated data of interaction sequences. As a result, our approach exploits partially annotated data through self-supervision and incorporates temporal context using pseudo-sequences generated from unlabeled still images. We validate our method on two common benchmarks, ARMBench [34] mix-object-tote and OCID [37], where it achieves state-of-the-art performance. Notably, on ARMBench, we attain an AP50 of 86.37, almost a 20% improvement over existing work, and obtain remarkable results in scenarios with extremely low annotation, achieving an AP50 score of 84.89 with just 1% of annotated data compared to 72 presented in [34] on the fully annotated counterpart 4.", "sections": [{"title": "Introduction", "content": "Acquiring accurate instance segmentation masks requires training a model on vast amounts of data with high-quality pixel-level annotations. While collecting raw sensory data (images) is relatively easy, annotating object instance masks down to individual pixels becomes prohibitively expensive when scaling up perception tasks. As a result, models trained on limited annotated data inevitably face challenges when deployed in the real world due to domain variation and evolving environments. This problem is central in robotics, where robots rely on spatial perception extracted from sensory inputs.\nTo use large amounts of unlabeled data, Semi-Supervised Learning (SSL) assumes that only a portion of the data is labeled: either a subset of observed scenes or some objects within each scene. The model then uses its own predictions as pseudo-labels to extract learning signals from the remaining unlabeled data [36,39,48,50,52]. Therefore, a model attempting to learn from its own noisy labels early in training may stagnate rather than generalize.\nLooking beyond spatial cues of still images, video sequences contain temporal information that a model can exploit to enforce consistency across frames and improve generalization. Recent advancements focusing on Learning Through Interaction (LTI) highlight the significance of providing the model with temporal perception. LTI enables the model to peer into the underlying dynamics of its domain by observing actions and their consequences [8, 21, 38]. The leading approaches entail observing a scene that undergoes various changes, such as objects being placed or extracted. By constructing the data in the form of \"before\" and \"after\" sequences [24, 27, 43, 44, 51], localized changes in illumination, deformation, and articulation of objects allow the model to refine its interpretation of the environment. The leading LTI techniques either prescribe multi-stage training that pre-trains on specialized datasets [27] or restrict input observations to strictly gradual changes at small time intervals [2]. Interestingly, leading methods for Video Image Segmentation regularly resolve long image sequences depicting instances popping in and out of view [16,43,44,51]. These incorporate a reassociation loss to overcome changes of occlusion, instance pose and appearance. However, learning from videos relies either on significant investment in manual annotation of every object in every video frame or on video frames occurring at sufficiently small time intervals. Our main insight is that although each paradigm compensates for the weakness of the other (SSL lessens the annotation effort while LTI leverages temporal information), naively combining the two amplifies their drawbacks-reinforcing noisy labels across entire sequences. In this work, we propose a solution in the form of a novel framework that incorporates the learning paradigms of SSL and LTI to enhance performance in the few-annotations scenario, in which only a tiny fraction of the dataset is annotated (and the rest is unlabeled). Our method simultaneously addresses the challenges of LTI and SSL. We eliminate the need for specialized datasets required for LTI by using pseudo-sequences generated from still images to mimic scene interaction. We also overcome the main obstacles to SSL by preventing noisy self-predictions from obscuring the learning signal through coupling prediction heads, thus stabilizing predictions early in training. Our framework is model-agnostic, complementing existing (and future) segmentation models with temporal perception through end-to-end training. Additionally, we propose an automated pseudo-label criteria that discards low-quality predictions.\nThe resulting framework can be considered the first to employ self-supervised learning through interaction, achieving better performance than each paradigm individually. We set a new state-of-the-art on the ARMBench [34] benchmark and OCID [37] (RGB only). Notably, our method trained on 1% of annotated data surpasses the performance of the well-established Deformable DETR [53] architecture, even when trained on 10\u00d7 additional annotated data (improving +16.86 AP using Swin-L Transformer as feature extractor)."}, {"title": "Background and Related Work", "content": "Of the various approaches to instance segmentation, we are interested in those that excel without full supervision [8, 21]. This section provides an overview of relevant works on partial supervision and learning from sequences.\nPartial supervision methods use the few annotated examples available (if any) and maintain consistent predictions for similar objects in the scene [57]. In recent years, most efforts focused on contrastive learning that extracts embedding from object instances and aims to bring same-class embedding closer while pushing other classes further apart [4,36]. That said, progress in object classification and detection does not readily carry over to image segmentation, where the effectiveness of self-supervision lags behind full-supervision in challenging domains of cluttered objects with many occlusions [54]. Unsurprisingly, these domains are also more complicated for humans to annotate.\nScene modulation is a concept that aims to extract additional learning signal by familiarizing the model with objects that undergo gradual alterations within a scene [7, 24, 41, 42, 47] where objects are viewed in many configurations, as well as different clutter and lighting conditions. This offers a substantial advantage in detecting and identifying objects that may deform or exhibit variations, thereby enhancing the robustness of the segmentation. Note, however, that assembling large dedicated datasets of objects is resource-intensive and challenging to apply effectively to new scenes featuring previously unseen objects. A recent work [47] achieved significant improvement by incorporating simulated data before transferring to real world scenes [15,24]. The main drawback of using synthetic data is the high cost of creating photorealistic rendering that accurately captures the physical properties of every object in the scene. Often times this results in idiosyncrasies that are picked up by the model and become a source of error when encountering real world data.\nFrame sequences offer additional information along the time dimension. As with scene modulation, the model learns to recognize and identify related instances throughout a series of images [2]. Recent advancements in video instance segmentation (VIS) methods, exemplified by SeqFormer [43] and IDOL [44], leverage sequential consistency of instances for online object segmentation and tracking. They employ contrastive loss to ensure that instance representations are distinguishable from other instances in the same frame and over previous frames. In CTVIS [51] the model also taps into future frames.\nLearning through interaction pushes the notion of sequences even further by specializing in image sequences that depict predefined and controlled scene manipulation. Consecutive frames in these meticulously assembled datasets exhibit large temporal gaps, unlike video data, and changes are usually confined to localized actions on few object instances [27]. This locality constraint persists through frame sequences, allowing LTI approaches to infer which instances have actually changed and which are merely affected by variations in lighting, occlusion and deformation, as a result of the action performed. The model quickly learns to segment an object that is added or removed, using a few hundred labeled image pairs. Since assembling such specialized datasets requires significant effort, the next stage in training artificially inserts cropped instances from high-confidence mask predictions into unlabeled still-images to emulate interactions. In STOW [24], the model is additionally trained on synthesized virtual scenes and then evaluated on real-world data.\nThe above advancements present an interesting question: In real-world applications where the model inevitably encounters a changing domain, is it possible to continuously learn (post deployment) without supervision by leveraging the temporal information of video sequences using the causal awareness of LTI? Importantly, can this be achieved without investing in a proprietary dataset or reliance on a specific instance segmentation model?"}, {"title": "RISE", "content": "We introduce a novel framework called Robot Instance Segmentation for Few-Annotation Grasping (RISE) that unifies learning from temporal signals (through interactions) and spatial signals (through self-supervision). RISE is trained end-to-end on still images, and enables self-supervision to learn from temporal consistency when scene objects are moved, added or removed. Because of this, RISE does not require a meticulously compiled dataset of before and after image pairs of scene interaction, nor does it require a large dataset of labeled instances thus it is more readily capable of handling domain variations that commonly occur in the real world."}, {"title": "Instance Segmentation", "content": "Object instance segmentation begins with the input image x which first undergoes feature extraction by an extractor backbone. The features are then fed into an instance level embedding encoder that outputs 300 tokens that serve the decoder, which emits instance embedding $z_i$ into the predictions heads for class, bounding box and mask of object instance i. In this work we evaluate various established backbones: Resnet50, Resnet101 [14], and Swin-L [28] as options for the feature extractor. As embedding decoder we chose Deformable DETR [53] as a strong spatial decoder for its ability to learn object queries as features. The prediction heads for class labels and box coordinates are feed-forward networks (FFNs), whereas the mask prediction head is a Feature-Pyramid network (FPN) [25] that uses multi-scale features from the decoder's last layers, followed by an FFN whose output mask is scaled up to match the original image size.\nWhen the input is also accompanied by labels y, the supervised component of the loss constitutes a class label loss $L_{cls}$; $L_{box}$ that combines $L_1$ loss and generalized Intersection over Union (gIoU) loss [35]; $L_{mask}$ as the sum of the Dice loss [33] and Focal loss [26]:\n$L_s = L_{cls} + \\lambda_1 L_{box} + \\lambda_2 L_{mask}$,\nwhere $\u03bb_1$ and $\u03bb_2$ are the loss coefficients.\nRecent advancements in object detection incorporated optimal transport (OT) to address the optimal assignment between predictions and ground truth,"}, {"title": "Learning Through Interaction", "content": "Observing interactions shares commonality with Object Tracking, which goes beyond traditional object detection. It leverages discriminative representation of instances across frames and of different instances belonging to the same class. The resulting representation is more robust to occlusion and identity switches, as demonstrated in [22,45].\nGiven an unlabeled input frame x containing an unknown number of object instances, we introduce a new augmentation strategy to create a pair of pre- and post-interaction frames. The first, pre-interaction frame $x_1 = \\varphi_1(x)$ is an augmentation $\u03c6_1$ of x where we stochastically insert K objects from a bank of known instances. Each of the K objects is also individually augmented (e.g., scale, position, rotation, flip, color). The second, post-interaction frame $x_2 = \\varphi_2(x_1)$ is an augmentation $\u03c6_2$ of $x_1$ where we also remove several of the objects added to $x_1$ or insert a few more objects from the instance bank (with augmentations). Note that both $x_1$ and $x_2$ are spatially augmented with rotation, crop, and scale to convey a sense of motion to the observer (inspired by SeqFormer [43]).\nAugmentation Strategy Inserting new objects into a dense scene may lead to significant occlusions and even conceal the objects we intend to learn. Therefore, we devise a strategy that randomizes labeled objects from the instance bank and distributes them preferentially around the periphery of the frame:\n$(u,v) = Beta(\\alpha, \\beta) \\cdot [w, h]$\nwhere (u, v) is the top-left corner where the object is inserted, drawn from distribution $Beta(\u03b1, \u03b2) \u2208 R^2$, and w, h are the feasible horizontal and vertical regions that ensure that the object is contained within the frame (see Appendix A). The choice of Beta and its parameters reduces the likelihood of objects inserted near the center, where they might obstruct unlabeled objects. Another safeguard prevents inserting an object if it would overlap with any of the previously inserted objects by more than 85%.\nAssociation Loss The resulting frames $x_1$, $x_2$ contain a total of N and M object instances, respectively. Importantly, the objects' small projective and illumination transformations compel the model to learn robust representations that maintain consistency for occurrences of the same instance in a changing scene (illustrated in Fig. 2). Each embedding i \u2208 N extracted from the first frame $x_1$ is matched against every embedding $j\u2208 M$ in the second frame $x_2$,"}, {"title": "Self-Supervision", "content": "To better leverage spatial information in unlabeled data, we employ the segmentation model (Sec. 3.1) toward Semi-Supervised Learning (SSL). Inspired by [36], we include a consistency regularization loss and extend it to accept unlabeled images alongside labeled objects inserted from the instance bank.\nRecall that $x_1 = \\varphi_1(x)$ is a weak augmentation of the unlabeled input image x. In this context, we'll denote $x_w = x_1$. We apply another round of weak augmentations to $x_w$, followed by a strong augmentation $\u03c6$ to produce $x_s = x_3 = \u03c6(x_1)$. The strong augmentations comprise Color jitter, Planckian jitter [55], Gaussian blur, and gray-scale that are applied via RandAugment [6]. We feed both $x_w$ and $x_s$ into the model. Class labels, bounding boxes, and segmentation masks for weakly augmented inputs $x_w$ are treated as pseudo-label targets (in the absence of ground truth) that are compared against the model's prediction on $x_s$. The unsupervised consistency regularization loss:\n$L_u = L_{cls} + \\lambda_1 L_{box} + \\lambda_2 L_{mask}$\nIt is similar to the supervised loss $L_s$ (Eq. (1)), with the distinction that pseudo-labels are used in place of ground-truth labels. Gradients are not computed during the forward pass of $x_w$ (as illustrated in Fig. 2) as it constitutes the ground truth. We introduce the following refinements to stabilize the model during self-supervised training.\nRefined Consistency Learning It is common practice to filter out pseudo-labels with low prediction scores in order to reduce the model's exposure to errors during self-supervised training. The filters are often thresholds or quantiles that are either fixed, dynamic, or scheduled [18,40]. Thresholds, by nature, are more restrictive, discarding all predictions below their stated value. However, during the early stages of self-supervision, the model may emit most of its predictions slightly below the threshold, resulting in very few labels contributing towards learning. On the other hand, quantiles ignore the scores entirely and allow any prediction, provided that its score meets the rank requirement of the quantile. Because most models output a fixed number of predictions to accommodate crowded scenes (regularly exceeding 300 predictions), a quantile may become too lenient and include low-score predictions of poor quality, potentially degrading the model's performance as training progresses.\nIn Appendix D, we demonstrate that early in training, setting the quantile too low lets in more predictions of low-quality signals, interfering with the model. Alternatively, setting the bar (too) high [36] risks missing out on meaningful supervision signals.\nTo reconcile the limitations of both thresholds and quantiles, we propose a cascade approach. First, a more relaxed class threshold $\\gamma^{cls}_t$ removes instances whose class scores $c_i \u2208 \u0109$ are deemed unusable, followed by a quantile selection $Q$ [40] of the leading predictions. The resulting class pseudo-labels \u0177 is given by:\n$\\hat{y} = Q( \u0109 > \\gamma^{cls}_t ; P_t)$.\nThe threshold $\\gamma^{cls}_t$ discards instances with class scores $c_i$ below it and tightens over time (training steps t). Conversely, the quantile Q(pt) loosens over time with its probability $p_t = 0.995 \\cdot (1 \u2013 t/T)$ decays over subsequent training steps t, with T denoting the total number of training steps. As a result, the quantile allows more predictions into the model as training progresses. This strategy can mitigate incorrect model beliefs and reduce confirmation biases. We evaluate this strategy quantitatively and demonstrate its advantage over thresholds and quantiles in Tab. 4, with additional details in Appendix C. We recognize that exploring different quantile strategies may further improve self-supervision and set it aside for future work.\nCoupled Prediction Heads The standard approach to filtering pseudo-mask predictions employs a pixel-wise confidence threshold $\\gamma^{mask}$ that is applied to each pixel (u, v) of instance mask $m_i$:\n$m_{i}^{u, v} = \\begin{cases}\n1 & \\text{if } h^{mask}(z_i^{w})_{u, v} > \\gamma^{mask},\n\\\\ 0 & \\text{otherwise}\n\\\\\\end{cases}$\nwhere $h^{mask}$ is the mask head output for instance embedding $z_i^w$ obtained from the weakly augmented frame $x^w$.\nUnlike masks, the prediction quality of bounding boxes is less correlated with high label scores. As such, recent SSL methods for object detection employ multiple passes to refine box predictions [1, 49]. Interestingly, we observe that the model learns to predict high quality masks well before it effectively predicts bounding boxes. Thus, we propose a coupling of the mask and box prediction"}, {"title": "Unified Framework", "content": "The complete architecture of RISE is presented in Fig. 2, comprising an LTI branch and an SSL branch that converge into a unified loss:\n$L_{total} = \ud835\udfd9[y \u2260 \u2205]L_s + \u03bb_3L_{embed} + \ud835\udfd9[y = \u2205]\u03bb_4L_u$,\nwhere \ud835\udfd9 indicates that the supervised loss $L_s$ and unsupervised loss $L_u$ are used according to the availability of ground-truth labels y, and $L_{embed}$ denotes the weighted combinations of the association loss. Hyperparameter search for $\u03bb_3$ and details on $\u03bb_1$, $\u03bb_2$ and $\u03bb_4$ are provided in Appendix A."}, {"title": "Experiments", "content": "We conduct a series of experiments to evaluate the performance of the proposed approach in the Robotic Item Grasping domain. This domain is of high relevance to automated distribution warehouses, where robotic arms pick and place items inside totes. The experiments target a range of labeled data ratios, meaning that we intentionally restrict the model's access to only a certain portion (%) of the labeled samples, and treat the remaining samples as unlabeled."}, {"title": "Setup", "content": "Datasets Our main focus is the ARMBench [34] mix-tote benchmark comprising 44,234 images, split into 30,992 training images and 6,637 and 6,605 images for validation and testing, respectively. The images are not organized into sequences nor do they describe a localized action. Every object in the scene belongs to a single \"object\" category and is associated with a manually annotated instance mask.\nThe OCID [37] dataset (containing 2,390 images and 31 classes) for various rates of labeled-to-unlabeled data, and compare it to the current state-of-the-art [32]. We use the same RISE configuration (e.g., Beta function, thresholds, etc.) for both datasets. The results in Tab. 2 illustrate that our method is readily applied to new datasets without requiring domain-specific configuration adjustments.\nEvaluation We evaluate our method using the standard Average Precision (AP). We measure the overall AP across 10 IoU thresholds [0.50,..., 0.95], as well as the IoU thresholded precision AP50 and AP75. For OCID we use only the AP50 to be consistent with prior art.\nIn terms of partitioning, we use 100%, 10%, 2%, 1% and 0.5% of the data as fully annotated, and the remaining as unlabeled for ARMBench, and 100%, 10% and 5% for OCID. We compare RISE with the officially reported performance from the ARMBench [34] and RoboLLM [29], in which a model was trained on the entire training set. This baseline is the existing state-of-the-art on the ARMBench dataset. In addition, we compare RISE with Deformable DETR [56] (denoted DeDETR).\nResults\nTab. 1 shows the results of RISE on various data partitions of labeled/unlabeled ratios of the ARMBench, compared with Deformable DETR and SAM [19] (fine-tuned), as well as the results reported by the authors of ARMBench [34]. Both DeDETR and RISE use Swin-L [28] (197M parameters) as backbone, while SAM uses ViT-H [23] (636M parameters) and RoboLLM [29] uses Beit-3 base (87M parameters). Across all partitions, RISE outperforms the other methods. Fig. 4 illustrates high-quality masks predicted by RISE trained on 1% of the labeled, with 99% of the remaining data treated as unlabeled. Most of the line-of-sight objects are accurately segmented and a few heavily occluded objects are missed. Importantly, RISE trained on 1% annotated data performs better than DeDETR and SAM trained on 10% annotated samples (10\u00d7 the amount of annotations for training/fine-tuning).\nTab. 2 compares performance with partitions of labeled/unlabeled data ratios from OCID, showing the advantage of RISE.\nFoundation Model Comparison\nAs an additional baseline we compare RISE with the \"Segment Anything\" (SAM) foundation model [19], fine-tuned on a subset of the ARMBench dataset. In Tab. 1 we demonstrate that despite SAM's unrivalled ability to segment anything, it is prone to over-segment and produce mask artifacts, even after fine-tuning on a small portion of domain-specific images."}, {"title": "Ablation Study", "content": "We provide an ablation study of the various design choices made in implementing RISE: impact of losses, pseudo-label threshold strategies and parameters. Tab. 3 details the contribution of the different elements within RISE on the fully-supervised training set. The most substantial improvement is attributed to the Pseudo-Sequence (PS) strategy outlined in Sec. 3.2. The coupling of prediction heads in Mask-to-Box (M2B in Eq. (7)) refines the supervision signal for box predictions, further improving the performance. Combining it with Multi-Label Matching (MLM) and Optimal Transport (OT) yields the best performing version of RISE.\nNext we evaluate the pseudo-label elimination strategy of either a standard score threshold or quantile function, compared with the proposed cascade approach (Eq. (6)). Notably, setting the threshold or quantile too low would include more false positive predictions in training. Setting them too high would eliminate correct predictions since very few predictions would meet the required prediction"}, {"title": "Conclusion", "content": "In this work, we present RISE, a novel framework that incorporates semi-supervised learning with learning through scene interaction in the context of a few-annotation data regime. RISE is modular and can complement other segmentation models that emit intermediate instance embedding. We demonstrate that RISE improves AP50 by over +10 compared to previous state-of-the-art, after training end-to-end on just 0.5% of the labeled data (with 99.5% of the data treated as unlabeled). With just 1% of the labeled data, RISE achieves better performance than the baselines (DeDETR, SAM, RoboLLM) trained on 10\u00d7 the amount of labeled data. On OCID (RGB), RISE sets a new state-of-the-art, and is near state-of-the-art when restricted to a fraction of the annotations. For future work, we intend on leveraging \"before\" and \"after\" observations directly using robotic item grasping in real-world environments (rather than synthetically inserting instances into images), with the overarching goal of lifelong learning for robot perception.\nA limitation of the proposed approach is that it underperforms when presented with objects that make few or no appearances in the truncated (labeled) training data. Access to a handful of annotated examples means that not all objects are encountered during training, resulting in some cases where two objects are segmented as one. In the context of robotic grasping, this may lead to a failed object-grasping attempt. However, since grasp failures also alter the scene, we believe that capturing snapshots of the scene before and after the attempted interaction would help improve the grasping precision in the long term."}, {"title": "Technical details", "content": "The RISE framework begins with an image augmentation step that feeds into a feature extractor followed by an instance segmentation model, and ends at prediction heads for class, bounding box, mask and instance association. We use ResNet-50, ResNet-101 [14] and Swin-L transformer [28] as backbones throughout our experiments, followed by Deformable DETR [53] with 6 encoders and decoders, width of 256 and 300 fixed instance queries, converging on an FPN-like dynamic mask head (as in SeqFormer [43]). In our evaluation, we measure the contribution of the proposed approach to Deformable DETR which serves baseline, and all feature extractors are pretrained on COCO instance segmentation, as is common in Instance segmentation pretraining [57]. The proposed method incorporates a contrastive head (inspired by IDOL [44]) and introduces instance bank, self-supervision branch for non-labeled data, coupled prediction heads for stability (M2B) and label matching strategy during training (MLM). These, in aggregate, allow RISE to outperform both Deformable DETR and SAM, even when these are trained on \u00d710 more data (1% vs 10%).\nHyperparameters Recall from Sec. 3.1 and Sec. 3.3 that the supervised loss $L_s$ and unsupervised loss $L_u$ (Eq. (1), Eq. (5), respectively) are a combination of the class loss $L_{cls}$, bounding-box loss $L_{box}$ weighted by $\u03bb_1$, and the mask loss $L_{mask}$ weighted by $\u03bb_2$. We set the loss weights to be $\u03bb_1 = 2.0, \u03bb_2 = 1.0$. The total loss $L_{total}$ (in Eq. (9)) combines the supervised loss $L_s$ or unsupervised loss $L_u$ (depending on availability of label y), with association loss $L_{embed}$ weighted by $\u03bb_3$.\nAugmentation Strategy The input images are downsampled and randomly cropped so that the longest side is at most 600 pixels, and so that the shortest"}, {"title": "Prediction Matching", "content": "The model predicts up to 300 instance labels, boxes and masks which are often far beyond the actual instance count in a given image. In order to compute the loss between valid predictions and ground-truth annotations, we compute the bipartite cost matrix which measures the IoU of each prediction against each ground-truth annotation (either based on box IoU or using the Mask-to-Box method detailed in Sec. 3.3). We then find the fitting assignment for each ground-truth annotation by solving an Optimal Transport (OT) Problem [9]. A similar approach described in Sec. 3.2 serves toward computing $L_{embed}$ which requires positive and negative views of an instance. We introduce a method inspired by IDOL [44], where the top-10 prediction matches of each ground-truth annotation are treated as positive views and the rest are considered negative views. The impact of matching is evident in the ablation study in Tab. 3 where we use either OT or a more standard approach of using the top 0.7 IoU as positive and bottom 0.3 IoU as negative.\nThis flow is similarly applied during the self-supervision phase, with the distinction of using pseudo- labels, boxes and masks instead of manually annotated ground truth. Here we also employ Multi-Label Matching (MLM, Sec. 3.3) to allow the model to learn from multiple pseudo-labels predicted from the weak augmentation $x_w$. The impact of MLM is demonstrated in Tab. 3 and inspired by [1], where it further contributes to the framework's performance."}, {"title": "Thresholds", "content": "We use time-dependent thresholds [17,52], whereby an initial threshold value increases every 1000 training steps. The class and mask thresholds start at $\u03b3^{cls} = \u03b3^{mask} = 0.85$ and peak at 0.98. For the Cascade approach (Sec. 3.3) which combines a lenient threshold followed by a quantile $Q_t$ described in Eq. (6). We set the initial class and mask thresholds to be $\u03b3^{cls} = \u03b3^{mask} = 0.5$ and peak at 0.85. The quantile $Q_t$ follows the schedule $p_t = a_0 \\cdot (1 \u2013 t/T)$ where t is the training step, T denotes the total number of training steps, and $a_0 = 0.995$ is the quantile base value. Upon ranking the model's predicted instances by their class score, only the top $p_t$ are retained, and the rest are discarded."}, {"title": "Study of Cascade Threshold", "content": "We study the behavior of pseudo-label and pseudo-mask Cascade filter strategy (Eq. (6)). We evaluate the per-instance prediction score of the model using different base values for the quantile $Q_t$ of the Cascade threshold. In Fig. 7, each color band represent 1000 iterations. The figure shows that setting the base value of the quantile too low would allow in more false-negatives as pseudo- labels and masks. Alternatively, setting it too high would discard valuable predictions as they don't meet the ranking requirement of the qunatile. Following this evaluation we set the quantile base value to $a_0 = 0.995$, which leads to the most"}, {"title": "Failure Cases", "content": "In both the supervised and self-supervised stages, we randomly draw instance-bank objects and distribute them in the image according to a 2d $Beta(\u03b1, \u03b2)$ distribution (Eq. (2)), and prevent object overlap that exceeds 85% by resampling from the distribution in case of such overlap. In the supervised phase, we also ensure that inserted objects do not overlap existing (ground-truth) objects by more than 85%, whereas in the self-supervised phase, the Beta(a, \u03b2) distribution (Eq. (2)) helps reduce the likelihood of inserted memory-bank objects overlapping actual objects in the image (since no ground-truth is available). Despite these precautions, failure cases still occur, particularly at very low annotation rates. Since our method incorporate noisy pseudo-labels in low annotated data regime, we will follow improvements in noisy spatial labels [12] for combating with noise and improve pseudo-labels. Fig. 8 shows how a model trained on 1% of the labeled data (99% treated as unlabeled) accurately predicts the masks of all objects in the \"before\" image $x_1$ (and ignores the background). However, in the \"after\" image $x_2$ (post-interaction), which contains an additionally inserted object (bottom row), the model fails to produce masks for the occluded cardboard box."}]}