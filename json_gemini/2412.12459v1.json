{"title": "LITA: An Efficient LLM-assisted Iterative Topic Augmentation Framework", "authors": ["Chia-Hsuan Chang", "Jui-Tse Tsai", "Yi-Hang Tsai", "San-Yih Hwang"], "abstract": "Topic modeling is widely used for uncovering thematic structures within text corpora, yet traditional models often struggle with specificity and coherence in domain-focused applications. Guided approaches, such as SeededLDA and CorEx, incorporate user-provided seed words to improve relevance but remain labor-intensive and static. Large language models (LLMs) offer potential for dynamic topic refinement and discovery, yet their application often incurs high API costs. To address these challenges, we propose the LLM-assisted Iterative Topic Augmentation framework (LITA), an LLM-assisted approach that integrates user-provided seeds with embedding-based clustering and iterative refinement. LITA identifies a small number of ambiguous documents and employs an LLM to reassign them to existing or new topics, minimizing API costs while enhancing topic quality. Experiments on two datasets across topic quality and clustering performance metrics demonstrate that LITA outperforms five baseline models, including LDA, SeededLDA, CorEx, BERTopic, and PromptTopic. Our work offers an efficient and adaptable framework for advancing topic modeling and text clustering.", "sections": [{"title": "1 Introduction", "content": "Topic modeling is a powerful method for uncovering thematic structures within large text corpora, enabling users to gain insights into the underlying topics of textual data. By clustering documents into topics based on word patterns, topic models support a wide range of applications, from exploratory data analysis to targeted decision-making in diverse fields like healthcare, social sciences, and business. Traditionally, unsupervised topic models such as Latent Dirichlet Allocation (LDA) [1] have been employed for these tasks by learning topic distributions across words and documents, helping users to broadly explore the structure of a corpus."}, {"title": "2 Related Works", "content": "Topic modeling is a text clustering technique for uncovering thematic structures within a corpus. One notable foundational algorithm, LDA [1], infers document-topic and topic-word distributions, providing a framework for users to explore prevalent topics. Building on this, guided topic modeling allows users to inject domain knowledge into the topic identification process, tailoring results to specific needs and generating more coherent topics. For instance, SeededLDA [6] and CorEx [4] enable users to provide topic seed sets, helping shape the model's topic structure in line with their desired themes. Afterward, with the development of pre-trained language models such as BERT [12], frameworks like BERTopic [5] have leveraged contextualized embeddings for topic clustering by applying traditional clustering algorithms, such as HDBSCAN or K-means, to these embeddings. Our proposed framework is inspired by BERTopic but introduces a significant advancement: it starts with human-provided topic seeds and then integrates the LLM's text understanding capability, iteratively refining and augmenting clustering results based on LLM feedback.\nRecent advances in large language models (LLMs) have driven prompt-based methods in topic modeling and text clustering [13,14,10,15]. Viswanathan et al. [13] propose using LLMs to enrich documents with key phrases and generate pair document constraints for better clustering. While both text enrichment and pairwise constraint generation lead to improved performances, they demand significant amount of LLM queries. PromptTopic [14] assigns topics by prompting LLMs with demonstrations to label each document, later merging similar labels either by word similarity or LLM-assisted comparison. TopicGPT [10] initiates topic modeling by generating a list of potential topics based on a sampled subset of documents and then uses the LLM to assign a topic to each document. GoalEx [15] combines a multi-stage process of topic proposal, assignment, and selection, prompting LLMs to derive a list of explanations (as potential topics), followed by assigning texts based on these explanations and selecting a final topic for each document through integer linear programming. ClusterLLM [16] uniquely incorporates LLM feedback to refine embeddings rather than clustering assignments, using ambiguous text triplets to enhance embedding quality. Despite their unique processes, these approaches request LLMs for every document, which makes them costly and impractical for large datasets. Their approaches also limit dynamic topic discovery as they rely on LLM-assigned labels without iterative refinement.\nIn contrast, our iterative topic augmentation framework balances efficiency and flexibility. Instead of requiring LLM queries for every document, it focuses on ambiguous documents only, thereby drastically reducing API usage. By in-"}, {"title": "3 LLM-assisted Iterative Topic Augmentation Framework", "content": "Our framework requires a corpus $D = {d_i}_{i=1}^N$ and a set of user-provided seed word lists $S = {s_i}_{i=1}^K$, where $d_i$ represents a document, $N$ is the number of documents, and $s_i$ is a seed word list that represents a topic. The framework identifies $K$ topics from the corpus. Please note that users are only required to provide seed words for partial topics (i.e., $|S| < K$). This framework iteratively refines topic assignments and augments new topics, providing flexibility in identifying the optimal number of topics $K$ based on the corpus.\nFollowing the common topic modeling practice [1,6,4,5], each topic $k \\in K$ is described by a set of representative words $\\phi_k$. For a document cluster $k$, we identify these representative words of topic $k$ using c-TF-IDF [5]:\n$\\qquad tf_{w,k} \\times log(1 + \\frac{A}{tf_w}),\n$\nwhere $tf_{w,k}$ is the word frequency of word $w$ in the cluster $k$, $A$ is the average word frequency across all clusters, and $tf_w$ is the word frequency of $w$ in the corpus. We select top-$M$ words with highest values of each topic to form $\\phi_k$. We prompt an LLM to review represented words of each topic and generate an appropriate topic name."}, {"title": "3.2 Procedure", "content": "Step 1, Document and Seed Word Embedding In the first step, we embed all documents in the corpus and seed words using a pre-trained embedding model $f$. This yields document embeddings $D = {d_i = f(d_i)}_{i=1}^N$ and seed word embeddings $S = {s_i = f(s_i)}_{i=1}^K$, placing them in the same vector space for direct comparison.\nStep 2, Document Clustering We apply the K-means clustering to the document embeddings to identify topic (i.e., cluster) assignment of each document. To incorporate the user-provided seed words, we initialize the K-means with the seed word embeddings as the initial centroids, thereby injecting the initial topic structure based on user's guidance.\n$\\qquad K\\text{-means}(data = D, init = S, number of clusters = |S|)$\nStep 3, Ambiguous Document Identification Because users may provide only partial topic seed words, the initial clustering might not be able to fully capture the intended topic structure, leaving some documents ambiguously assigned. To address this, we identify ambiguous documents by evaluating their proximity to the two nearest cluster centroids. After K-means assigns each document to a cluster, we compute the centroid of each topic by averaging its member document embeddings. We then apply a margin-based metric to flag ambiguous documents:\n$\\qquad |\\delta(d_i, \\mu_k^1) - \\delta(d_i, \\mu_k^2)| \\le \\epsilon,$\nwhere $\\delta$ is the cosine distance measuring the distance between two embeddings, and $\\mu_k^1$ and $\\mu_k^2$ are the nearest and second nearest topic centroids for $d_i$. Ambiguous documents are those for which the distance difference is below the ambiguous distance threshold $\\epsilon$, indicating uncertainty in their topic assignments.\nStep 4, LLM as Topic Evaluator Once ambiguous documents are identified, we leverage an LLM to evaluate topic assignments. An LLM is prompted to re-evaluate the topic assignment of each ambiguous document. To achieve this task, we create a prompt, which includes the task description, a ambiguous document $d_i$, a list of topic names and their corresponding representative words, i.e., $\\phi_k$ for each $k$. Therefore, an LLM determines one of the following cases for each ambiguous document:\n(a) The document belongs to its assigned topic,\n(b) The document fits another existing topic, or\n(c) The document does not belong to any existing topics."}, {"title": "Step 5, Agglomerative Clustering for Identifying Unknown Topics", "content": "We adopt the agglomerative clustering on ambiguous documents that belong to case (c) to identify potential new topic clusters. The reasons of the choice of agglomerative clustering are (1) it is free from setting the number of topics, which is important because the optimal number is unknown, and (2) it is a bottom-up clustering approach, which avoids augmenting a new topic cluster with only a single document. We use the euclidean distance and ward linkage criterion for the agglomerative clustering. The only parameter is the agglomerative distance threshold \u03b3, which is dataset-dependent and empirically decided. \nAfter applying agglomerative clustering, clusters containing at least five documents are considered new topics, and documents within these clusters receive new topic assignments. For those isolated documents not clustered in new topics, we keep using their originally assigned topics. As a result, we have the latest topic assignments to all documents. We apply c-TF-IDF (Eq. 1) to generate representative word lists for all topics and treat them as updated seed sets S for next refinement round. The framework repeats Step 2 - Step 5 iteratively until no new topics are identified."}, {"title": "4 Experiment", "content": "Datasets. We evaluate the proposed LITA and baselines on two datasets, namely 20 Newsgroups [8] and CLINC(D) [16]. The 20 Newsgroups is widely used"}, {"title": "4.2 Results & Analysis", "content": "Performances of Topic Modeling. Fig 2 (a) examines topic coherence (measured by NPMI) across multiple baselines. The findings indicate that LITA consistently demonstrates superior performance in terms of topic coherence. It achieves the highest NPMI values across most cluster configurations. In comparison, PromptTopic, BERTopic and CorEx exhibit similar but slightly lower coherence levels. Traditional methods such as LDA and SeededLDA perform substantially worse, whereas SeededLDA produces negative NPMI scores, highlighting its inability to generate semantically coherent topics. In addition to coherence, Fig 2 (b) shows that LITA demonstrates competitive performance in topic diversity. To maintain high diversity while preserving coherence is a critical attribute for practical applications. While SeededLDA records a maximum diversity score in the 20 Newsgroups, its negative coherence metrics indicate a tradeoff that undermines semantic quality. The comparison results for NMI and cluster accuracy are presented in Fig.2 (c) and (d), respectively. The results reveal an upward trend in NMI and accuracy for LITA during iterations (as the number of clusters increases), reflecting improved alignment with the ground truth as granularity becomes finer. Although PromptTopic achieves results comparable to LITA in NMI and accuracy, LITA's enhanced efficiency offers a clear advantage, which will be explored in detail in subsequent sections.\nEfficiency Discussion. Previous prompt-based methods necessitate a greater number of LLM requests due to their workflow. For instance, PromptTopic processes each document individually to assign topics, resulting in 7,532 and 4,500 API calls for the 20 Newsgroups and CLINC(D) datasets, respectively. Furthermore, PromptTopic requires additional API access during the topic collapse phase. In contrast, LITA strategically limits LLM invocations to only the evaluation of ambiguous instances. As illustrated in Figure 3, the number of ambiguous instances decreases substantially across iterations for both datasets, reaching topic convergence within just 5-7 iterations. As a result, LITA requires just 1,325 and 487 API calls on the same datasets. Compared to PromptTopic, LITA reduces over 80% LLM requests on both 20 Newsgroups and CLINC(D) datasets. Considering to LITA's competitive performances reported in Fig. 2, LITA identifies topics of better quality while achieving computational efficiency, making LITA as cost-effective and scalable solution for real-world applications."}, {"title": "5 Conclusion", "content": "This paper presents LITA, an LLM-assisted iterative topic augmentation framework that balances cost-effectiveness and high-quality topic modeling. By leveraging LLMs to refine ambiguous documents and expand topics dynamically, LITA outperforms baseline methods across key metrics, including topic coherence, diversity, NMI, and clustering accuracy. Its efficient integration of LLM feedback without over-relying on costly API queries distinguishes it from other approaches. While LITA shows strong empirical results, limitations such as dependency on user-provided seeds and dataset-specific performance variations remain. Future work could focus on expanding to larger datasets. In conclusion, LITA provides a scalable, efficient solution for advancing topic modeling and demonstrates the potential of LLMs in this domain."}]}