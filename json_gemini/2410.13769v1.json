{"title": "Transformer Guided Coevolution: Improved Team Formation in Multiagent Adversarial Games", "authors": ["Pranav Rajbhandari", "Prithviraj Dasgupta", "Donald Sofge"], "abstract": "We consider the problem of team formation within multiagent adversarial games. We propose BERTeam, a novel algorithm that uses a transformer-based deep neural network with Masked Language Model training to select the best team of players from a trained population. We integrate this with coevolutionary deep reinforcement learning, which trains a diverse set of individual players to choose teams from. We test our algorithm in the multiagent adversarial game Marine Capture-The-Flag, and we find that BERTeam learns non-trivial team compositions that perform well against unseen opponents. For this game, we find that BERTeam outperforms MCAA, an algorithm that similarly optimizes team formation.", "sections": [{"title": "1 INTRODUCTION", "content": "We inspect multiagent adversarial games, characterized by an environment with multiple teams of agents, each working to achieve a team goal. Their performance is evaluated by an outcome, a real number assigned to each team at the end of a game (episode).\nVarious complex team games can be formulated as a multiagent adversarial game, including pursuit-evasion games [10, 18, 40, 49], robotic football [17, 23, 41], and robotic capture-the-flag [32]. The problem of creating a cooperative team of robots is useful for solving these games, as well as applications such as search and rescue.\nA crucial problem in adversarial multiagent team formation is the selection of teams. Given a set of agents and potentially information about the environment, a team must be selected to perform best against opponents. This problem is difficult since a good team formation algorithm must consider both intra-team and inter-team interactions to select an optimal team. Additionally, the set of agents must often learn their individual policies, increasing the complexity of the problem."}, {"title": "2 RELATED WORKS", "content": "Self-Play: Self-play is a central concept for training autonomous agents for adversarial games. The main idea of self-play is to keep track of a set of policies to play against during training [20]. These policies are usually current or past versions of agents being trained. Extending the concept of self-play, Alpha-Star [21] utilized league play, a technique where a diverse set of agents with different performance levels play in a tournament style game structure. League play improved the adaptability of trained agents to play against different opponent difficulties in the Starcraft-II real-time strategy video game. Similar to the league-play concept, in our proposed technique, we use coevolution of agents to improve their adaptability against varying opponent strategies and difficulty levels.\nTeam Formation in Adversarial Games: In [27], researchers proposed Team-PSRO (Policy Space Response Oracle), a technique within a framework called TMECor (Team Max-min Equilibrium with Correlation device), for multi-player adversarial games. In Team-PSRO, agents improve their policies iteratively through repeated game-play. In each iteration, the policy for each agent in a"}, {"title": "3 TEAM SELECTION IN ADVERSARIAL GAMES", "content": "Preliminaries: A Markov Decision Process (MDP) is a framework capturing a broad range of optimization tasks. An MDP is described by a tuple $(S, A, T, R, \\gamma)$, containing a state space $S$, an action space $A$, a transition function $T(S' | S \\times A)$, a reward function $R: S \\times A \\times S \\rightarrow \\mathbb{R}$, and a discount factor $\\gamma \\in [0, 1)$. An agent is a player in an MDP and is described by its policy $\\pi(A | S)$. The sequence $(s_0, a_0, r_0), (s_1, a_1, r_1),...$ is referred to as a trajectory, where $a_i$ is sampled from $\\pi(a | s_{i-1})$, $s_i$ is sampled from the transition function $T(s | s_{i-1}, a_{i-1})$, and $r_t = R(s_t, a_t, s_{t+1})$. An agent's objective is optimizing the sum of discounted rewards in a trajectory: $E[\\sum \\gamma^i r_i]$.\nReinforcement Learning (RL) is one method of optimizing a policy for an MDP. While there are various different techniques for different classes of MDPs, most of them keep track of a policy $\\pi_\\theta(A | S)$ and a reward estimator $r_\\theta : S \\rightarrow \\mathbb{R}$. After sampling an episode, the reward parameters $\\theta$ are updated towards the observed discounted rewards from each state. The policy parameters $\\theta$ are updated using the reward estimator to optimize the expected discounted rewards from each state. Deep RL utilizes Deep Neural Networks to create policy and reward networks $\\pi_\\theta, r_\\theta$.\nWe are concerned with multiagent k-v-k adversarial games. Given the policies of all agents playing, this scenario can be formally defined as an instance of an MDP for each agent, with a distinct reward function for each agent. At each time step, the actions of all other agents are considered in the transition function $T$. We divide the agents into teams and additionally define an outcome evaluation that considers the trajectories of a game and returns a set of teams that 'won'. We assume the MDP rewards of an agent correlate with its team's outcome, so agents that get high rewards in a game are likely to be on winning teams. We use this framework as opposed dec-MDPs, a formalization used in team reach-avoid games [5, 10, 18, 39] since we would like each agent to have their own reward structure, and for these rewards to be separate from the game outcomes. Within our framework, the problem we consider"}, {"title": "3.1 Transformer based Sequence Completion for Team Selection", "content": "Why Transformers? Transformers are generative models that can be used to query 'next token' conditional distributions. A size k team can be generated in k queries, giving us efficient use of the model. The form of the output also allows us make specialized queries for cases where valid teams may have specific constraints. Transformers are also able to condition their output on input embeddings, allowing us to form teams dependent on observations (of any form) of the opponent. We may also pass in an empty input sequence for unconditional team formation. We expect that through their initial embeddings, transformers will encode similarities between agents and allow the model to infer missing data. This is supported by the behavior of word embeddings in the domain of NLP [28].\nThe main issue with transformers is their $\\Theta(k^2)$ complexity for a sequence of size k. However, in practice, sequence lengths of up to 512 are easily calculated [13]. This indicates that our algorithm can scale up to this team size. Even for team sizes beyond this limit, there exist workarounds like sliding-window attention [3].\nWe propose BERTeam, a method for selecting a team of agents in a multi-agent adversarial game utilizing a transformer model."}, {"title": "3.1.1 Model Architecture", "content": "The core of BERTeam is a transformer model whose tokens represent each possible agent in the population, along with a [MASK] token. A separate input embedding model transforms observations of any form into a short sequence"}, {"title": "3.1.2 Training Procedure", "content": "The BERTeam model uses MLM training, which requires a dataset of 'correct sequences'. The model is trained to complete masked sequences to match this distribution.\nTo generate this dataset, we consider the outcomes of games played between various teams. Since our goal is for the model to predict good teams, we fill the dataset with only teams that win games, along with the observations of their players. We also include examples of winning teams without any observations to train the model to generate unconditioned output.\nSince it is usually infeasible to sample all possible team pairs uniformly, we must decide which games to sample to efficiently create a dataset. We do this by using BERTeam's sampling to create a dataset of teams that win against BERTeam's current distribution of opponents. The motivation of this is that is is natural to favor generating teams that win against powerful opponents, as opposed to teams that win against poorly coordinated opponents. Additionally, under certain assumptions and dataset weights, BERTeam imitating a dataset formed in this way will converge to a Nash Equilibrium. We describe this in Appendix D.1, but do not implement this in our experiment, as it may cause poorly behaved learning. This appendix also describes a strategy for games with general outcomes, where cannot necessarily distinguish a 'winning' team.\nThus, as in Figure 2, we will train BERTeam alongside generating its dataset, and utilize the partially trained model to sample games. Our dataset will take the form of a replay buffer so that outdated examples are eventually replaced by newer ones. One concern is that filling the dataset with teams generated from BERTeam"}, {"title": "3.1.3 Training along with Coevolution", "content": "The BERTeam model is able to be trained alongside coevolution, as its past knowledge of the agent policies can be utilized and updated with recent information. The outcomes of games sampled in coevolution can be used in the dataset of BERTeam, and the BERTeam partially trained model can be used to sample better teams, a cycle displayed in Figure 2. While we detail the coevolution algorithm in Section 3.2, the main thing we must define using BERTeam is an individual agent fitness function. This is tricky, as we have only assumed the existence of a comparative team outcome. We solve this by utilizing Elo, a method of assigning each player a value from the results of pairwise 1v1 games [15]. Given a team selector (i.e. BERTeam) that can sample from the set of all possible teams containing some specified agent (the captain), we define the fitness of each agent as the expected Elo of a team chosen with that agent as captain. To update these values, we sample teams to play training games, choosing our captains by adding noise to BERTeam's initial distribution. We update the fitnesses of each team captain with a standard Elo update (Appendix A). Since it is confusing to distinguish these individual fitnesses from team Elos, we will refer to them as fitness values from now on."}, {"title": "3.2 Coevolution For Training Agent Policies", "content": "The main idea in coevolution is that instead of optimizing a single team, a population of agents learns strategies playing in games between sampled teams. We choose to use Coevolutionary Deep RL since it allows agents to be trained against a variety of opponent policies, addressing performance against an unseen opponent.\nThus, we use Algorithm 1, heavily inspired by [12], to produce a population of trained agents. The input is an initialized population of agents, as well as parameters like team size. In each epoch, we sample $n_{games}$ games, which each consist of selecting teams and captains using a team selector (lines 6, 7), playing the game (line 8), and collecting trajectories and outcomes (lines 9, 10). The outcomes are sent to the team selector for training and also used to update individual agent fitnesses in line 11 (see Figure 1). At the end of each epoch, the trajectories collected update each agent policy in place in line 13, and the population is updated in lines 14-16."}, {"title": "4 EXPERIMENTS", "content": "To better analyze the effectiveness of this algorithm, we consider 2v2 team games. This small team size allows us to more easily analyze the total distribution learned by BERTeam."}, {"title": "4.1 Aquaticus", "content": "Aquaticus is a Capture-the-Flag competition played with teams of autonomous boats [32]. We are interested in this task because it is an example of a team-based multiagent adversarial game. Each agent has low-level motor control, and thus any complex behaviors must be learned through a method like RL. Additionally, there are various strategies (e.g. offensive/defensive) that agents may adopt that perform well in competition. Finally, we believe that optimal team composition in this game is non-trivial, and expect that a good team is composed of a balanced set of strategies."}, {"title": "4.1.1 Pyquaticus", "content": "Due to the difficulty of testing on real robotic platforms, we test and evaluate our methods on Pyquaticus, a simulated version of Aquaticus [2]. The platform is implemented as a PettingZoo environment [44], the standard multiagent extension of OpenAI Gymnasium [6]. In experiments, we use the MDP structure implemented in Pyquaticus. We terminate an episode when a team captures a flag, or after seven in-game minutes."}, {"title": "4.2 Team Selection with Fixed Policy Agents", "content": "To test the effectiveness of BERTeam independent of coevolution, we use fixed policy agents predefined in Pyquaticus: a random agent, three defending agents, and three attacking agents. The attacking and defending agents each contain an easy, medium, and hard policy. The easy policies move in a fixed path, and the medium policies utilize potential field controllers to either attack the opponent flag while avoiding opponents, or capture opponents by following them. The hard policies are the medium policies with faster movement speed. We follow the training algorithm in Figure 2 without the coevolution update. We conduct an experiment with the 7 agents, and detail experiment parameters in Appendix E.\nThroughout training, we record the occurrence probability of all possible teams using BERTeam. We do this exhaustively, by considering all possible sequences drawn from the set of agents with replacement. We expect that as BERTeam trains, this will gradually approach a distribution that favors well performing teams."}, {"title": "4.2.1 Elo Calculation", "content": "Since we have a fixed set of policies, we can compute true Elos of all 28 unordered 2 agent teams (see Appendix B), obtained from an exhaustive tournament of all possible team pairings. For these results, we perform 10 experiments for each choice of two teams. We use the scaling of standard chess Elo [15], and shift all Elos so that the mean is 1000."}, {"title": "4.3 Team Selection with Coevolution of Agents", "content": "We train BERTeam alongside Algorithm 1, as illustrated in Figure 2. For each individual agent, we use Proximal Policy Optimization (PPO), an on-policy RL algorithm [38]. We detail experiment parameters in Appendix E. To find the Elos of each possible team, we utilize the Elos calculated for the fixed policy teams as a baseline. For each of the 1275 possible teams, we test against all possible teams of fixed policy agents. We then use the results of these games to calculate the true Elos of our teams of trained agents. We do not update the Elos of our fixed policy agents during this calculation.\nFor policy optimization, we use stable_baselines3 [33], a standard RL library. Since this library is single-agent, we create unstable_baselines3, a wrapper allowing a set of independent learning algorithms to train in a PettingZoo multiagent environment."}, {"title": "4.3.1 Aggression Metric", "content": "The BERTeam model is difficult to interpret in the case of learned policies, as the team composition is unclear. Thus, we create an aggression metric to estimate the behavior of each agent in a game. Specifically, this metric for agent a is $\\frac{1 + (#a tags opponent)}{1 + 2(#a captures flag) + 1.5(#a grabs flag) + (#a is tagged)}$, where (#a event) denotes the number of times that event happened to agent a in the game. We evaluate the aggressiveness of each trained agent by considering a game where one team is composed of only that agent. We evaluate the average aggression metric against all possible teams of fixed policy agents. We expect aggressive agents to have a metric larger than 1, and defensive agents to have a metric less than 1."}, {"title": "4.4 Comparison with MC\u0410\u0410", "content": "We notice that the MCAA mainland team selection algorithm is playing an analogous role to BERTeam team formation, and that the MAP-Elites policy optimization on each island is analogous to our coevolutionary RL algorithm. Since both our algorithm and MCAA distinguish team formation and individual policy optimization as separate algorithms, we may hybridize the methods and compare the results of four algorithms. The algorithms are distinguished by choosing MAP-Elites or Coevolutionary Deep RL for policy optimization, and BERTeam or MCAA for team selection. The hybrid trials will allow us to individually evaluate BERTeam as a team selection method, independent of the policy optimization.\nWe must make some changes to be able to implement MCAA and MAP-Elites in an adversarial scenario. First, a step of the MCAA algorithm ranks a set of generated teams with a team fitness function. To approximate this in an adversarial environment, we play a set of games each epoch, and consider the teams that won as 'top ranked', and include them in the MCAA training data. Similarly, MCAA assumes an evaluation function for fitness of individual agents. We approximate this by considering the teams each individual has been included in. We take a moving average of the comparative team evaluations, and assign this average to the selected agent.\nFor MAP-Elites, we adapt our aggression metric as a behavior projection. While this could be made more complex, we believe this is sufficient, as our work is focused on the team selection aspect."}, {"title": "5 RESULTS", "content": "In MCTF, reordering the members of a team has no effect on the team's performance. However, BERTeam is a sequence generation model, so it does distinguish order. To make results more readable, we assume any reordering of a given team is equivalent to the original team, and calculate distributions and Elos accordingly. A caveat to this is that teams with two distinct agents are counted twice in a total distribution, while teams with two copies of one agent are counted once. To compensate for this during analysis, we double the distribution value of the second type of team, and normalize. This is relevant mainly in Figure 4(a) and Table 1, where we inspect the total distribution of a small number of teams. For the analysis of cases with many agents, this effect is negligible."}, {"title": "5.1 Team Selection with Fixed Policy Agents", "content": "Throughout training, we inspect the total distribution of teams learned by BERTeam. From Figure 4(a), we notice that our proposed training algorithm certainly seems to learn something non-uniform. The top seven out of 21 possible team compositions account for about 75% of the total distribution. It seems like BERTeam immediately favored teams containing the hard attacking agent, as the"}, {"title": "5.2 Team Selection with Coevolution of Agents", "content": ""}, {"title": "5.2.1 Team Elos", "content": "We calculate the true Elos for all 1275 teams, using the fixed policy agent teams as a baseline. We plot these in Figure 6, along with BERTeam's occurrence probability. We partition all teams into 'Defensive', 'Balanced', and 'Aggressive' based on whether they have zero, one, or two aggressive agents. We also conduct a linear regression on all 1275 teams and plot the line.\nWe find that there is a correlation with the true Elo of a team and BERTeam's probability of outputting that team. Our linear regression had a correlation coefficient $R^2 \\approx .25$, implying that about 25% of the variance in BERTeam's output is explained by the performance of the team. This indicates the distribution learned by BERTeam, while noisy, favors teams that perform well.\nWe find that the true Elo of the best performing team is around 1017, indicating a performance slightly better than an average fixed policy team. This specific team (composed of two distinct aggressive agents) is also the most probable output of BERTeam.\nThus, we find that BERTeam, trained alongside coevolution, was able to produce and recognize a team that performed competitively against previously unseen opponents. In fact, from the rankings in Table 1, we see that the top choice from BERTeam outperforms any team that does not contain agent 2 (the hard offensive agent).\nWhile the performance of the teams learned from self-play are lower than the top fixed policy agents, this may be a result from difficulties in the environment, such as a lack of correlation between game outcomes and MDP rewards. This could also potentially be improved by hyperparameter tuning in either the base RL algorithm, the coevolution algorithm, or in BERTeam.\nOverall, the reasonable performance of top coevolved teams, as well as the positive correlation in Figure 6, indicate that BERTeam trained alongside coevolution is successful at optimizing policies for a multiagent adversarial game against unknown opponents."}, {"title": "5.2.2 Agent Embeddings", "content": "Recall that the first step of a transformer is to assign each token a vector embedding. We directly inspect the agent embeddings learned by BERTeam. For a subset of the total population, we consider the average cosine similarity of each pair chosen from the subset. We use this as an estimate of how similar BERTeam believes agents in that subset are.\nFor our subset choices, we divide the total population into aggressive and defensive agents, as in Figure 5(a). We further divide each subset into 'strong' and 'weak' based on whether their Elo is above the population average. We expect that BERTeam's embeddings of each class of agents have more similarity than a subset chosen uniformly at random. We calculate the cosine similarity of a uniform random subset in Appendix C.\nFrom the results in Table 2, we can see that the majority of the subsets we chose have a stronger similarity than a random subset of the same size. The only subsets that do not support this are the 'Defensive' and 'Weak Defensive' subsets, which are slightly lower. This suggests that the initial vector embeddings learned by BERTeam are not simply uniquely distinguishing each agent. The agents that perform similarly are viewed as similar by BERTeam. This indicates known properties of token embeddings in the domain of NLP (such as word vectors learned in [28]) apply in our case as well. The vectors learned by BERTeam encode aspects of each agent's behavior, and similarities in agents can be inferred through similarities in their initial embeddings. This suggests BERTeam can"}, {"title": "5.3 Comparison with MCAA and MAP-Elites", "content": "We directly compare the trained policies using our algorithm, MCAA, and the hybrid methods. We produce teams using the specified team formation method, and use the outcomes of these games to estimate their comparative expected Elos in Table 3.\nWe find that trials that used BERTeam as a team formation method outperformed trials that used the MCAA mainland algorithm. One possible explanation for this is the lack of specificity in the MCAA mainland algorithm. While BERTeam learns the distribution on an individual agent level, MCAA chooses the proportion of each island included in a team. This method seems to be most effective when the islands are distinct (i.e. in the original MCAA paper, islands had different proportions of robot types). However, in our case, it seems varying the RL algorithm did not have a similar effect. Another possible explanation is while BERTeam may learn an arbitrarily specific total distribution, MCAA can only learn a total distribution that is independent for each position. This restricts MCAA from learning distributions that do not factor in this way.\nAdditionally, this supports previous results. When taking the weighted average of Elos in Figure 6, we find BERTeam's expected Elo is about 930. This is close to the result of 917, and the minor difference can be explained by the fact we trained for less epochs.\nAs for runtime complexity, we separately inspect the clock time of team training and of agent policy updates. The difference in using BERTeam or MCAA to generate teams for policy updates was negligable. The runtime was dominated by conducting the sample games and conducting the RL updates. For team training, we find the update of MCAA took almost no time. In contrast, training the BERTeam model took about 46 seconds on a batch size of 512. This significant cost is justified by its stronger performance, and that it can be trained offline without interfering with the flow of the rest of the algorithms. Finally, we find our implementation of MAP-Elites is costly, as we sample separate games to perform behavior projection. We could mitigate this by implementing MAP-Elites more similar to the original implementation."}, {"title": "6 CONCLUSION", "content": "In this paper, we propose BERTeam, an algorithm and training procedure that is able to learn team composition in multiagent adversarial games. The algorithm is effective both in choosing teams of fixed policy agents and when being trained along with the policies of individual agents. BERTeam can also take in input, allowing it to generate teams conditional on observations of opponent behavior.\nWe evaluate this algorithm on Pyquaticus, a simulated capture-the-flag game played by boat robots. We test BERTeam both with fixed policy agents and training alongside a coevolutionary deep RL algorithm. We find that in both cases, BERTeam effectively learns strong non-trivial team composition. For fixed policy agents, we find that BERTeam learns the correct optimal team. In the coevolution case, we find that BERTeam learns to form a balanced team of agents that performs competitively. Upon further inspection, we find that like its inspiration in the field of NLP, BERTeam learns similarities between agent behaviors through initial embeddings. This allows it to account for missing data by inferring the behavior of agents. We also find that BERTeam outperforms MCAA, an algorithm designed for team selection.\nOverall, BERTeam is a strong team selection algorithm with roots inspired by NLP text generation models. BERTeam's ability to learn similarities in agent behavior results in efficient training, and allows BERTeam to train alongside individual agent policies."}, {"title": "6.1 Future Research", "content": "We do not explore the capability to condition BERTeam's output on observations of the opponent. In future research, we plan to show that teams generated through conditioning outperform teams generated with no information.\nWe test only size 2 teams to easily analyze the output of BERTeam. We plan to test larger teams in future research.\nWe can change our weighting and training so that BERTeam will converge on a Nash Equilibrium, assuming certain properties of the game outcomes (Appendix D.1). We do not make these changes because they may be incompatible with MLM training. We plan to explore this in future research.\nBERTeam is applicable to games with more than two teams. In future experiments, it would be interesting to evaluate the performance of BERTeam on multi-team games.\nFor coevolution, we only consider a basic evolutionary algorithm with reproduction through cloning, as in [12]. However, there is a vast literature of evolutionary algorithm variants that could replace this. It would be interesting to explore which are most compatible with our training scheme.\nWe do not focus on optimizing hyperparameters in our algorithms or their interactions. It would be interesting to optimize these across a wide variety of problem instances, and inspect their relation with aspects of each instance."}, {"title": "A ELO UPDATE EQUATION", "content": "Consider a 1v1 game where outcomes for each player are non-negative and sum to 1. Elos are a method of assigning values to each agent in a population based on their ability in the game [15].\nIf agents 1 and 2 with elos $f_1$ and $f_2$ play a game, we expect agent 1 to win with probability $Y_1 := \\frac{1}{1 + exp(f_2 - f_1)}$. When a game between agents 1 and 2 is sampled, we update the Elo of agent i using the game outcome $S_i$ with the following equation:\n$f_i' = f_i + c(S_i - Y_i).$  (3)\nNote that if the outcome $S_i$ is larger (resp. smaller) than our expectation $Y_i$, we increase (resp. decrease) our Elo estimate. We set c as the scale to determine the magnitude of the updates."}, {"title": "B NUMBER OF POSSIBLE TEAMS OF SIZE K", "content": "We will find the number of possible size k teams with indistinguishable members from n total policies.\nWe first partition all possible teams of k members based on their number of distinct policies i. In the case of i policies, we must choose which of the n policies to include (${n \\choose i}$ choices). We then assign an agent to each of the k team members. Since we do not care about order, we must consider the number of ways to assign k indistinguishable objects (members) into i distinct bins (policies). There are (${k - 1 \\choose i - 1}$) ways to do this by \u2018stars and bars\u2019 [16]. Thus, overall there are $({n \\choose i})({k - 1 \\choose i - 1})$ team choices with i distinct members. We sum this over all possible values of i to obtain $\\sum_{i=1}^{k} {n \\choose i}{k - 1 \\choose i - 1}$.\nIf we do care about order (i.e. members are distinguishable), there are trivially $n^k$ ways to choose a sequence of k agents from n with repeats. Any intermediate order considerations must fall between these two extremes. In either case, with fixed k, there are $O(n^k)$ possible choices of a team of size k from n total agents."}, {"title": "C COSINE SIMILARITY OF RANDOM SUBSET", "content": "A random subset of size k chosen uniformly from a population of vectors will have the same average cosine similarity as the whole population.\nFor a proof, we may consider a fully connected graph where each node is one of n vectors, and each edge joining two agents is weighted by their cosine similarity. The above statement is equivalent to saying the expected average weight of edges in a random induced subgraph of size k is the overall average edge weight.\nConsider taking the expectation across all possible k subsets. Each edge weight will be counted ${n - 2 \\choose k - 2}$ times, as this is the number of k subsets containing it (choose the remaining k - 2 from the n-2 other nodes). When taking the edge average in each k subset, we divide by ${k \\choose 2}$, and when taking the expectation across all k subsets, we divide by ${n \\choose k}$. Thus, an edge contributes $\\frac{{n - 2 \\choose k - 2}}{{k \\choose 2}{n \\choose k}} = \\frac{1}{{n \\choose 2}}$ times its weight to the expectation, the same as it would in an average across all ${n \\choose 2}$ edges."}, {"title": "D DATASET WEIGHTING/SAMPLING", "content": "Consider the general case with m teams in an adversarial game. We assume BERTeam has a current distribution for each team, and we would like to sample a dataset for the team in ith position. We also assume we have a notion of a 'winning' team in a certain game. Our goal is that the occurrence of a team in the dataset is proportional to its win probability against opponents selected by BERTeam.\nFor team A, denote this win probability W(A). Let the set of all valid teams for the ith position be $T_i$.\nThe na\u00efve approach is to simply sample uniformly from $T_i$ and include teams that win against opponents sampled from BERTeam. While this results in the correct distribution, this method will rarely find a successful team, as we assume BERTeam's choices are strong.\nTo increase the probability of finding a successful team, we may sample using BERTeam instead of uniformly from $T_i$. This increases our success rate, but fails to generate the correct distribution. In particular, the occurrence of team A is proportional to $P{A \\sim \\text{BERTeam}} W(A)$. To fix this, we use inverse probability weighting [19], weighting each inclusion of team A by the inverse of its selection probability. This ensures that the weighted occurrence of team A in the dataset is W(A).\nThis additionally creates symmetry, since each team is sampled from BERTeam. Thus, we may consider m datasets and each game update the datasets corresponding to the winning teams. This increases the number of samples we get per game by a factor of m. In our experiments, where the players and opponents are symmetric, we keep one dataset and do this implicitly."}, {"title": "D.1 Relation to Nash Equilibria", "content": "We analyze the general case where there are m teams in a game, and the sets of valid teams are $T_1,..., T_m$. The act of choosing teams to play multiagent adversarial matches suggests the structure of a normal form game with m players. The ith player's available actions are teams in $T_i$, and the utilities of a choice of m teams are the expected outcomes of each team in the match.\nThe distribution of BERTeam defines a mixed strategy of a player i in this game (i.e. a distribution over all teams, an element of the simplex $ \\Delta(T_i)$). Ideally, we would like the training to cause BERTeam to approach a Nash Equilibrium of the game. This is possible, under some assumptions.\nConsider a weighted dataset S of teams sampled uniformly from $T_i$. We construct a vector $v_S \\in \\mathbb{R}^{|T_i|}$ such that the dimension corresponding to team $T \\in T_i$ is the weighted occurrence of T in S. Assume BERTeam's distribution is $p \\in \\Delta(T_i)$. We define $u_{S,p}$ as the projection of $u_S$ onto the tangent space of p wrt. $ \\Delta(T_i)$. If p is on a boundary of $ \\Delta(T_i)$ and $v_{S,p}$ exits the simplex, we may need to instead project $u_S$ onto the tangent space of p with respect to a lower dimensional face of $ \\Delta(T)$.\nOur first assumption is that there is a BERTeam architecture and training scheme such that updating with dataset S is equivalent (in expectation) to updating p in the direction of $v_{S,p}$.\nNow consider weighting each team in S according to its expected outcome against a distribution $q \\in [\\Delta(T_j)$. We claim that the resulting vector $v_{S,p}$ is in the direction of an update that improves the expected utility of p against q unless p is optimal."}, {"title": "E EXPERIMENT PARAMETERS", "content": ""}]}