{"title": "Open Domain Question Answering with Conflicting Contexts", "authors": ["Siyi Liu", "Qiang Ning", "Kishaloy Halder", "Wei Xiao", "Zheng Qi", "Phu Mon Htut", "Yi Zhang", "Neha Anna John", "Bonan Min", "Yassine Benajiba", "Dan Roth"], "abstract": "Open domain question answering systems frequently rely on information retrieved from large collections of text (such as the Web) to answer questions. However, such collections of text often contain conflicting information, and indiscriminately depending on this information may result in untruthful and inaccurate answers. To understand the gravity of this problem, we collect a human-annotated dataset, Question Answering with Conflicting Contexts (QACC), and find that as much as 25% of unambiguous, open domain questions can lead to conflicting contexts when retrieved using Google Search. We evaluate and benchmark three powerful Large Language Models (LLMs) with our dataset QACC and demonstrate their limitations in effectively addressing questions with conflicting information. To explore how humans reason through conflicting contexts, we request our annotators to provide explanations for their selections of correct answers. We demonstrate that by finetuning LLMs to explain their answers, we can introduce richer information into their training that guide them through the process of reasoning with conflicting contexts. We will release our dataset and code to promote research along this line.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have shown impressive capabilities on question answering tasks. In an open domain setting, a typical approach involves (1) retrieving relevant documents as contexts from the web or knowledge bases, and (2) using LLMs to generate the answer with the guide of the context. However, retrieved contexts from the web could often present conflicting information: e.g., 22.62% pregnant women reported to find conflicting medical information from different websites in a survey (H\u00e4meen-Anttila et al., 2014), such conflicts can lead to undesirable consequences when a language model relies indiscriminately on them to answer questions.\nPrevious work have explored different aspects of conflicts in the field of Natural Language Processing (NLP), including having different perspectives (Chen et al., 2019; Liu et al., 2021), fake news and misinformation (Chen et al., 2022b; Pan et al., 2023), conflicts due to ambiguous or inadequate questions (Min et al., 2020; Zhang and Choi, 2021), knowledge that change over time (Kasai et al., 2023), and knowledge clashes between the parameters and contexts (Longpre et al., 2021; Chen et al., 2022a; Xie et al., 2024).\nIn this work, we target the conflicts among the contexts when retrieving from the web with an unambiguous query and study their impact on the downstream question answering task. Figure 1 shows the results of querying \"when did kendrick lamars first album come out?\" on Google\u00b9. We observe that in the top-10 returned results, there are different evidence suggesting different answers to the question, and such inconsistencies may confuse the language models when they refer to the contexts to answer the question. Earlier works study this issue of conflicting contexts through perturbations with entity-substitution (Chen et al., 2022a; Hong et al., 2024), machine-generation (Pan et al., 2023; Wan et al., 2024; Hong et al., 2024), rule-based templates (Kazemi et al., 2023), or on controversial, multi-perspective questions (Liu et al., 2021; Wan et al., 2024). However, none of them examine the scenario where realistic, unambiguous open domain questions can also lead to conflicting contexts on the web and its effect in downstream question answering.\nTo quantify how often conflicting contexts occur on the web, we construct our dataset named QACC (Question Answering with Conflicting Contexts). We consider unambiguous open domain questions from AmbigQA (Min et al., 2020) and use Google Search API\u00b2 to retrieve up to 10 results for each question. We then use Amazon Mechanical Turk and ask human annotators to determine whether there exists different answers in the contexts. We find that about 25% of the unambiguous open domain questions will yield conflicting evidence from Google. We evaluate three popular LLMs (GPT-3.5, Claude-3, and Phi-3) on our dataset with different prompting and finetuning strategies and establish that conflicting contexts can lead to substantial performance degradation in them. To understand how humans reason through conflicting contexts, we ask our annotators to select from a pre-defined set of reasons when deciding on the answer. Our findings indicate that humans often adhere to majority vote (i.e. selecting the most popular answer) when seeing conflicting contexts. In addition, we also request our annotators to provide a single sentence, natural language explanation for their answers. We find that by finetuning LLMs to explain their answers, we can introduce richer information into their training that guide them through the process of reasoning with conflicting contexts and improve their performance in both QACC and a perturbed NQ-Open dataset (Lee et al., 2019).\nTo summarize, our contributions in this work are the following:\n\u2022 We construct a human-annotated dataset QACC and find that about 25% of unambiguous, open domain questions can lead to conflicting contexts when queried with Google Search."}, {"title": "Related Work", "content": "Retrieval Augmented Question Answering\nOpen-domain question answering (ODQA) aims to answer factoid questions with a large collection of documents (Voorhees and Tice, 2000). With the new advances in large language models (LLMs), a typical approach to OPQA involves a two-stage framework: (1) first retrieve a small subset of passages where some of them contain the answer to the question, and then (2) use a LLM to answer the question using the retrieved passages as contexts (Chen et al., 2017; Karpukhin et al., 2020; Guu et al., 2020; Khandelwal et al., 2020; Izacard and Grave, 2021; Borgeaud et al., 2022; Zhong et al., 2022). Retrievers augment question answering by retrieving up to 100 passages and set new state-of-the-art for ODQA (Izacard and Grave, 2021); however, we believe that with such large amount of passages retrieved as context, it's frequent for them to contain conflicting information, and such conflicts will confuse the downstream language models in question answering. In this work, we validate this hypothesis and show that for a retriever like Google Search, 25% of the time it will return conflicting contexts in its top ten results when queried with a realistic, unambiguous question. We further demonstrate the limitations of current LLMs under this scenario of conflicting contexts through our experiments."}, {"title": "Knowledge Conflicts", "content": "Parametric v.s. Contextual One line of work studies knowledge conflicts in the setting of parametric v.s. contextual knowledge. Parametric knowledge refers to the knowledge a model learns during pre-training, and contextual knowledge refers to the contextual information a model sees at inference time. Longpre et al. (2021) proposes a entity-substitution framework that identifies QA instances with named entity answers and then substitutes mentions of the entity in the gold document with an alternate entity to create entity-based knowledge conflicts. Chen et al. (2022a) expands the study to consider multiple evidence passages and shows that when some passages are perturbed not to support an answer, language models largely ignore semantic perturbations and outputs potential answer entity in the retrieved passages. Xie et al. (2024) proposes another new framework to elicit the parametric memory of LLMs in order to construct the corresponding counter-memory and shows that with both supportive and contradictory evidence to their parametric memory, LLMs show a strong confirmation bias and tend to cling to their parametric memory. (Liu et al., 2024) proposes a machine-generated dataset of knowledge conflicts and studies different strategies to enable LLMs to resolve conflicting knowledge.\nContextual v.s. Contextual Another line of work focuses on the scenario when a language model is given conflicting contexts as in our setting. Some previous work create conflicting contexts with perturbations, including entity-substitution, (Chen et al., 2022a; Hong et al., 2024) and machine-generation (Pan et al., 2023; Wan et al., 2024; Hong et al., 2024), and some other work define conflicts over rule-based templates (Kazemi et al., 2023). However, most of these previous work are built on synthetic data, whereas our dataset are real-world search results of unambiguous questions. Wan et al. (2024) also uses Google Search to extract conflicting contexts, but they focus specifically on controversial and contentious questions and analyze the linguistic features in the text that affect language models' predictions, whereas we show that realistic, unambiguous questions can also lead to conflicting contexts from Google and finetuning LLMs' on our human written explanations can teach them to reason through the conflicts."}, {"title": "Question Answering with Conflicting Contexts", "content": "In this section, we discuss our exploration of the problem: question answering with conflicting contexts. We first suggest our definition of what constitutes as conflicting context, then introduce how we collect a our dataset QACC for the analysis, and lastly share our findings and analysis of QACC."}, {"title": "Problem Definition", "content": "Given a question q, a list of retrieved contexts C = {C\u2081, C\u2082, ..., C\u1d62}, and a question answering system \u03d5, we can get a list of individual answers A = {a\u2081, a\u2082, ..., a\u1d62}, where a\u1d62 = \u03d5(q, c\u1d62). We state that the question q has conflicting evidence if and only if \u2203(a\u1d62, a\u2c7c \u2208 A)(a\u1d62 \u2260 a\u2c7c). In other words, at each step a question answering system (a human or a language model) is given the question and only one of the context in order to answer the question. Iterate this through all of the contexts, and we state that there are conflicting contexts if and only if there are different answers generated when given different contexts. We believe that this definition over the space of the answers makes it clearer and more well-defined to instruct the human annotators to determine if there are conflicting contexts."}, {"title": "QACC Dataset", "content": "Ambiguous questions can frequently lead to multiple different answers (Min et al., 2020). However, we believe that even when questions are unambiguous, it is still common to see conflicting evidence on the web. To this end, we consider AmbigQA (Min et al., 2020), a dataset with questions labeled as either ambiguous or unambiguous, and take only questions that are labeled as unambiguous as the the questions in our dataset. We then use Google Search API to retrieve top-10 search results as the contexts for each question, and use Amazon Mechanical Turk to collect annotations for each question and its associated contexts. The statistics of our dataset QACC is shown in Table 1."}, {"title": "Human Annotation", "content": "We employ a rigorous human annotation pipeline with a qualification exam before the main annotation task, a strategy commonly used to ensure the collection of high-quality datasets (Han et al., 2021; Dasigi et al., 2021). Only annotators that have passed our qualification exams can participate in the main annotation task. We use Amazon Mechanical Turk to collect the annotations and CROW-DAQ to design the annotation interface(Ning et al., 2020). Each question is annotated by one annotator and paid with $0.35 in the main annotation task. Examples of our qualification and annotation interfaces are shown in Appendix A.\nQualification Exam Since the annotation task requires critical thinking and an attention to detail, we design interactive tutorials and request the annotators to review them before the qualification exams. We first show them instructions and our definition of conflicting contexts for a question, and then ask them to complete a set of tutorial questions where we display the expected answers and reasons once they answer them. After they understand the goals and formats of the annotations, we request them to complete a set of 12 random, multiple-choice qualification questions. Only workers with more than 90% accuracy on the exam can pass and get the qualification to participate in our main annotation task. We allow only the workers that have a master's qualification\u00b3 to take the exam, and 12 among 41 of them (29%) have passed our exam and participate in our main annotation.\nMain Annotation Following our definition of the problem, we ask the annotators to identify the conflict in the contexts by finding different possible answers. In each Human Intelligence Task (HIT), we show the annotator an open domain question, a list of contexts retrieved by Google Search, as well as the website domains these contexts are from. We then ask the annotators the following questions: 1. are there more than one possible answer when looking at the question and each context individually (conflict identification)? 2. which of the contexts support which of the different answers (answer attribution)? 3. which answer do you think is the correct answer (question answering)? 4. why do you think the answer you choose is correct (QA with explanation)?. The fourth question here includes both a multiple-choice question that asks them to select a reason from a pre-defined set and a free-form question that asks them to explains their reasoning in a single sentence. These procedures result in a rich annotation of QACC that can also support other QA-related tasks not covered in the scope of this work, like answer attribution. An example of the dataset and the data collection process is shown in Figure 9."}, {"title": "Expert Verification", "content": "To verify the quality of the annotations, we take a random sample of 20 examples from QACC and annotate whether there is a conflict in the contexts ourselves. We match our annotation with the Mechanical Turk Workers' annotations and observe a Cohen's kappa agreement of 0.9, showing a high quality of the annotations in our dataset."}, {"title": "Analysis", "content": "Table 1 shows the statistics of our dataset. We can see that about 25% of all the unambiguous, open domain questions in our dataset have conflicting contexts when retrieved using Google Search.\nTo better understand humans' reasoning process when presented with conflicting evidence, we ask the annotators to choose from a pre-defined list of reasons that can best categorize why they think one of the answers is correct. We allow them to choose more than one option since different factors can simultaneously affect one's decision in choosing the correct answer. Figure 3 shows their reasons when the question is labeled by them as having conflicting contexts. We find that humans favor answers that are the most popular in the contexts the most, and also refer to the sources of the context (trustworthy or not) and their own intuitions and common sense about the question when deciding on the answer. On the other hand, less annotators select correct answers based on the publish time of the information.\nWe also conduct data analysis to study the types of questions that lead to conflicting contexts more often. In figure 4, we see that the two most prominent types of questions are Why and How questions. 40% of questions in QACC that start with Why and 38.75% of questions that start with How are determined as having conflicting contexts on Google. This aligns with our hypothesis since questions that start with Why and How are typically open-ended questions with a more complex answer and can involve different perspectives."}, {"title": "Experiments", "content": "In this section, we benchmark the problem of question answering with conflicting contexts on our dataset with three popular LLMs. We demonstrate that teaching language models to explain its answer can guide their inference process and improve their performance in QACC, and such improvement can generalize to another perturbed NQ-Open dataset."}, {"title": "Datasets", "content": "We run our experiments on two datasets. The first is the QACC dataset we collect, and the second is a perturbed NQ-Open dataset. For our QACC dataset, we use the validation set to find the best instruction and prompt formats for the LLMs, and report their results on the test set. For the perturbed NQ-Open dataset, we use an entity-substitution method to replace the answer in the contexts to other named entities of the same type in order to create conflicts among the contexts, following (Longpre et al., 2021). We construct this perturbed dataset over the test split of NQ-Open with 3,610 questions. We retrieve top ten results from Google as the contexts for these questions and apply the entity-substitution algorithm with different perturbation ratio. The higher the perturbation ratio means the more contexts in a question are perturbed."}, {"title": "Methods", "content": "Retrieval Augmented QA Language models can leverage contexts to answer open domain questions (Izacard and Grave, 2021; Zhong et al., 2022). We prompt LLMs with question and contexts retrieved from Google and instruct them that \"Given the following contexts, provide a direct, concise answer to the question\".\nMajority Vote As shown in Figure 3, humans are inclined to choose the majority answer when there are conflicting evidence. Therefore, we prompt LLMs question and contexts and instruct them to \"use majority vote to decide which context to trust if there are conflicting contexts\".\nDiscern and Answer Hong et al. (2024) proposes to explicitly instruct the model to first discern the counterfactual, perturbed passages and then ignore them to answer the question. We follow the same strategy and instruct the models to \"Find the perturbed passages if there are any, and ignore them when eliciting the correct answer\" with question, contexts, and an example message indicating which of the contexts are \"perturbed\", using the annotations in QACC that attribute correct/wrong answers to their supporting contexts.\nExplain and Answer Prompting with explanations introduces richer information that can guide the inference process. Recent work have shown that letting the language model \u201cexplain itself\u201d through in-context learning gains more insights into predictions and improves their performances in a variety of reasoning tasks, including question answering (Lampinen et al., 2022b; Ye and Durrett, 2022; Nye et al., 2021; Wei et al., 2023; Lampinen et al., 2022a). We believe answering question with conflicting contexts requires similar reasoning abilities and therefore can benefit from eliciting explanations during inference. We instruct the models to \"Explain the reasons and then provide a direct, concise answer\" with question, contexts, and a natural language explanation as in-context example in few-shot and training input in finetuning. Table 4 shows our experiments of comparing Explain then Answer to Answer then Explain. Similar to previous work, we observe only slight impact of the orders of explanation in their performances."}, {"title": "Experiment Setup", "content": "We conduct experiments on three popular families of instruction-tuned large language models: GPT3.5-turbo-0125, Claude3-Sonnet, and Phi-3-Medium-Instruct (14B), with zero-shot inference, few-shot inference, and finetuning. We find that LLMs greatly benefit from in-context examples (few-shot) compared to zeroshot (See Appendix B) when answering open domain questions, so we only present few-shot inference and finetuning results in Table 2.\nFor few-shot inference experiments, we include one in-context example of expected input-output pair when prompting the three language models. For finetuning experiments, we finetune Phi-3-Medium-Instruct using LoRA (Hu et al., 2021) with language modeling loss (SFT). We first find the best hyperparameters of finetuning using the validation set of QACC and then train on both the training and validation set and report results in the test set. We also use the validation set of QACC to find the best prompt and instruction format for each methods and use them for both fewshot inference and finetuning. We use Exact Match and F1 scores as the metrics for all our evaluations."}, {"title": "Experiment Results", "content": "QACC Table 2 exhibits our experiment results in QACC. We can see that all LLMs that we evaluate inevitably experience worse performance when there are conflicting contexts, comparing their results on EM-C and EM-NC, as well as their results on F1-C and F1-NC. We also find that in different LLMs, their best prompting methods in the few-shot setting are also different. GPT-3.5 has the best performance when prompted to take majority vote when seeing conflicting contexts, Claude-3 gives the best results when instructed to first explain and then answer the question. Phi-3 presents comparable performances when instructed to take majority vote and explain then answer.\nWe also demonstrate that by instructing the model to explain its answer and finetuning with our human-written explanations, Phi-3 can improve its performance on question answering with conflicting contexts. We observe an improvement of 2.9% on EM and 3.37% on F1 comparing the models finetuned with just the contexts (Context) and with contexts and the explanations (+Exp & Ans). Interestingly, we find that by finetuning Phi-3 with contexts and the instruction to take Majority Vote, the model cannot further improve its performance, and finetuning with Discern and Answer instruction and examples hurts the model and diminishes its performance. We hypothesize the reason is that by finetuning with the instruction to take Majority Vote, we are not introducing any new learning signals to the models besides the format, which it already learns from in-context examples, and some QA examples, which Phi-3 may have already seen during its pre-training. On the other hand, finetuning with Discern and Answer data hurts the performance since, although we can attribute the answers to their supporting contexts to create finetuning data for it, our conflicting contexts are naturally existed conflicting information on the web, rather than synthetic perturbed data with only a few entities replaced. This discrepancy re-emphasizes the usefulness of our dataset with naturally conflicting contexts.\nPerturbed NQ-Open The improvements we observe from finetuning Phi-3 on human-written explanations can generalize to perturbed data and general open-domain question answering as well. Table 3 exhibits the performance of zeroshot, fewshot Phi-3 models on perturbed NQ-Open, as well as Phi-3 finetuned on QACC and evaluated on perturbed NQ-Open. As illustrated in Table 3, Phi-3 finetuned with explanation data consistently outperforms other finetuned models under different ratio of perturbation. We believe that the extra fine-tuning signals of natural language explanations improve Phi-3's reasoning abilities in general, therefore demonstrating its consistent improvements across all perturbation ratio, including regular open domain QA (0%) and when there are perturbed contexts (25% and 50%)."}, {"title": "Conclusion", "content": "In this work, we construct a dataset named QACC to study open domain question answering with conflicting contexts. We find that unambiguous, open domain questions are exposed to conflicting evidence on the web: 25% of the questions will lead to conflicting contexts when retrieved using Google, and popular LLMs are very brittle to such conflicts. We show that by finetuning on natural language explanations, we can improve the reasoning abilities of Phi-3 and improve its performances when there are conflicting contexts as well as open domain question answering in general. We will release our dataset and code to promote further research along this line."}, {"title": "Limitations", "content": "Eliciting natural language explanations from LLMs have several limitations. Previous work have shown that explanations generated by LLMs can be unreliable and can lead to wrong interpretations of the models. However, in this work, we focus on the improvement of reasoning abilities of LLMs when finetuning explanations, rather than making interpretations of their explanations."}]}