{"title": "Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning", "authors": ["Akshara Prabhakar", "Thomas L. Griffiths", "R. Thomas McCoy"], "abstract": "Chain-of-Thought (CoT) prompting has been shown to enhance the multi-step reasoning capabilities of Large Language Models (LLMs). However, debates persist about whether LLMs exhibit abstract generalization or rely on shallow heuristics when given CoT prompts. To understand the factors influencing CoT reasoning we provide a detailed case study of the symbolic reasoning task of decoding shift ciphers, where letters are shifted forward some number of steps in the alphabet. GPT-4 achieves zero accuracy on most shift ciphers with standard prompting, but with CoT its accuracy improves to an average of 32%. By focusing on a single relatively simple task, we are able to identify three factors that systematically affect CoT performance: the probability of the task's expected output (probability), what the model has implicitly learned during pre-training (memorization), and the number of intermediate operations involved in reasoning (noisy reasoning). We show that these factors can drastically influence the task accuracy; e.g., varying the output's probability of occurrence can shift accuracy from 26% \u2192 70%. We also demonstrate that it is essential for the model to explicitly produce intermediate steps as output that can be conditioned on to increase the probability of the correct answer. Our experiments indicate that as long as the model does so, the validity of the demonstrations in the prompt does not matter. Overall, we conclude that CoT prompting performance reflects both memorization and a probabilistic version of genuine reasoning.", "sections": [{"title": "1 Introduction", "content": "Reasoning, one of the key aspects of human intelligence, is the process of thinking about something logically and systematically using evidence and past experiences to make a decision (Wason, 1968; Wason and Johnson-Laird, 1972; Fagin et al., 2004). The impressive performance of Large Language Models (LLMs) across a wide range of tasks has spurred extensive research into their reasoning capabilities (Huang and Chang, 2022; Qiao et al., 2022). It remains unclear whether the behavior of these systems is based on true reasoning or other heuristics. Some results provide evidence that LLMs are able to reason (Suzgun et al., 2022; Dasgupta et al., 2023; Saparov and He, 2022), while others show that they still struggle on tasks that humans can easily solve through reasoning (Han et al., 2022; Valmeekam et al., 2023; McCoy et al., 2023; Razeghi et al., 2022; Cao et al., 2023).\nThe Chain-of-Thought (CoT; Wei et al., 2022) prompting strategy has played a significant role in this debate. CoT involves prompting an LLM to generate a sequence of intermediate reasoning steps before producing the final answer, given a few in-context exemplar(s) of how to break the task into steps. CoT and its several variants (Kojima et al., 2023; Zhou et al., 2022; Wang et al., 2023b) have been shown to substantially improve performance over standard prompting. Recent works have tried to identify which aspects of the demonstration contribute to CoT's enhanced performance (Huang and Chang, 2022; Madaan and Yazdanbakhsh, 2022; Jin et al., 2024), typically relying on assessing performance across a wide range of tasks.\nIn this work, we take a different approach: we present an extensive case study on a single task that allows us to disentangle reasoning from memorization. The task we selected is solving shift ciphers, a simple type of code in which each letter is shifted forward a certain number of positions in the alphabet (Figure 1, panel 1). We choose this task because it allows us to independently manipulate several factors that could be relevant for characterizing how LLMs solve reasoning tasks when prompted with CoT: difficulty, frequency, and answer probability. Our results suggest that CoT performance reflects three factors: probability, memorization, and noisy reasoning. First, the accuracy of CoT is affected by the probability of the correct output, with more probable outputs resulting in a stronger effect of CoT. Second, performance is higher when memorization is possible, as indicated by the frequency of encountering different shift cipher variants during pre-training. The effects of probability and memorization show that CoT performance is not fully systematic abstract reasoning. Nonetheless, CoT performance is not solely driven by superficial heuristics: it also shows some hallmarks of true reasoning-albeit a noisy version of true reasoning, in which the error rate increases along with task difficulty (where we quantify task difficulty by the number of implicit reasoning steps involved). In addition, we find evidence that the effect of CoT fundamentally depends on generating sequences of words that increase the probability of the correct answer when conditioned upon; as long as this is the case, CoT can thus succeed even when the demonstrations in the prompt are invalid. In the ongoing debate about whether LLMs reason or memorize, our results thus support a reasonable middle-ground: LLM behavior displays aspects of both memorization and reasoning, and also reflects the probabilistic origins of these models."}, {"title": "2 Related Work", "content": "In-Context Learning in Language Models. It has been argued that LLMs can learn a task purely from demonstrations given in their context without any additional training (Brown et al., 2020), a phenomenon known as in-context learning (ICL). There have been many investigations into how ICL operates. Theoretical frameworks have modeled the pretraining data as a mixture of Hidden Markov Models (Xie et al., 2022) have argued that ICL is the result of implicit Bayesian inference (Zhang et al., 2023), an argument supported with evidence from synthetic data and tasks (Chiang and Yogatama, 2024). The emergence of ICL has been attributed to factors including data distributional properties (Chan et al., 2022), pretraining term frequencies (Razeghi et al., 2022), and the creation of task vectors (Hendel et al., 2023). However, the extent to which ICL is true learning is unclear (Pan et al., 2023; Min et al., 2022). For example, Kossen et al. (2024) shows that ICL relies on in-context label information but cannot fully overcome preferences acquired during pre-training, which is evidence against the view that ICL is true learning.\nUnderstanding CoT. Theoretical arguments have been formulated about how CoT improves the expressivity (Feng et al., 2023; Li et al., 2024) and sample complexity (Li et al., 2023) of ICL. Empirical studies have shown that CoT performance can be dramatically influenced by many features of the CoT prompt (Madaan and Yazdanbakhsh, 2022; Wu et al., 2023; Jin et al., 2024); for example, the relevance and ordering of reasoning fragments is more important than their accuracy (Wang et al., 2023a,b; Ye and Durrett, 2022), and minor input perturbations can substantially bias models' answers (Turpin et al., 2024), indicating a lack of general reasoning ability (Stechly et al., 2024).\nOur high-level goal in this work is to characterize what type of reasoning happens in LLMs when they are prompted with CoT prompting: to what extent is CoT performance driven by abstract reasoning vs. simple heuristics such as memorization? This question also implicitly underlies many of the papers discussed above, but directly focusing on this question leads us to investigate probability, frequency, and difficulty (see Section 4)\u2014a different set of factors than those studied in prior work."}, {"title": "3 Approach", "content": "One challenge of evaluating the role of memorization and reasoning in the performance of LLMs is that these models are typically evaluated on a wide range of complex reasoning tasks, whose variety and complexity can obscure the factors that drive performance. By contrast, we propose to tease apart the factors behind the efficacy of CoT prompting by focusing on a single relatively simple task: deciphering text encoded with a shift cipher.\nEncoding a message with a shift cipher involves replacing every letter with another letter that is some fixed number of positions (called shift_level) forward in the alphabet; decoding is the reverse (shifting backward) as shown in Figure 1. These are also known as rotation ciphers, since they rotate the alphabet forward some number of steps, and are given the name rot-k where k corresponds to shift_level. For example, given the test word \"FDW\u201d and that rot-3 encryption has been used (shift_level = 3), decoding involves shifting every letter 3 steps backward\u2500i.e., F \u2192 C, D \u2192 A, and W \u2192 T to obtain \u201cCAT\u201d as the output. In our experiments, we give an LLM a single word encoded with a shift cipher and ask it to decode this text to recover the original word."}, {"title": "3.1 Motivation for using shift ciphers", "content": "Our main reason for using shift ciphers is because they involve a sharp dissociation between task complexity and task frequency (a key factor in memorization). The complexity of the decipherment task is determined by the shift level-ciphers that require more intermediate steps are more complex. Different shift levels also vary in their frequency in internet text, and hence in the training data of large language models. Specifically, rot-13 is widely used in internet forums to conceal text such as puzzle solutions and spoilers, and rot-3 and rot-1 commonly appear in tutorials on decipherment (rot-3 is also known as the Caesar cipher, having apparently been used by the eponymous Caesar to encrypt his messages). In addition, shift ciphers facilitate investigation of the effect of probability because the correct answer can be any string, allowing us to modulate the probability of that string easily. Further, the systematic nature of the task makes it easy to generate examples and to verify correctness. Finally, decoding each letter in the message is an independent step, allowing us to easily analyze these individual steps.\nMCCoy et al. (2023) previously evaluated LLMs on shift ciphers, focusing on standard prompting along with some initial results using CoT. We conduct a much more extensive investigation into LLM behavior when prompted with CoT."}, {"title": "3.2 The effect of CoT on shift ciphers", "content": "Data. To provide an initial exploration of the effect of CoT and other aspects of prompting on shift ciphers, we constructed a dataset comprising 7-letter words having exactly 2 tokens (measured using the tokenizer used by GPT-4) to control for confounding factors relating to tokenization. The words were scored by their log probability and arranged in descending order. Subsequently, five bins were formed by selecting equidistant log probability values as centers, with bin1 having the highest probability and bin5 having the lowest probability. For each bin, we constructed a set of 150 words by choosing the words with log probability values nearest to each respective center (refer to Appendix A.1 for further details)."}, {"title": "4 Disentangling the factors influencing CoT performance", "content": "We consider four types of reasoning processes that GPT-4 might be adopting.\n(a) Symbolic reasoning is the use of discrete, deterministic inference rules. Shift ciphers can be perfectly decoded with a simple symbolic algorithm, so a system using fully systematic reasoning should attain 100% accuracy.\n(b) Noisy reasoning is like symbolic reasoning but with the addition of noise that introduces some possibility of each intermediate operation in a reasoning step being wrong. Thus, if the system uses noisy reasoning, we should see accuracy decrease as we increase the number of operations that need to be performed. Shift ciphers let us test this possibility: by varying shift_level, we can modulate the number of operations that need to be performed in every reasoning step, and observe if accuracy varies accordingly.\n(c) Memorization is a strategy in which a system memorizes the tasks it has encountered in pre-training but does not generalize to new tasks."}, {"title": "4.1 A simple probabilistic approach to modeling the reasoning process", "content": "To make these intuitively-stated observations more rigorous, we perform a logistic regression to determine the statistical significance of several factors. The outcome variable is a binary variable indicating whether GPT-4 got the correct answer on each example. We include the following predictors:\n\u2022 input_logprob: log probability of the encoded input text as measured by GPT-2 (Radford et al., 2019). The inputs tend to have a very low probability because they are enciphered.\n\u2022 output_logprob: log probability of the ground-truth output text as measured by GPT-2.\n\u2022 shift_freq: we used the frequency of occurrence of all shift levels that McCoy et al. (2023) provided based on analysis of the C4 corpus (Raffel et al., 2020). The assumption is that the distribution of shifts in C4 is similar to the distribution in the training data for GPT-4.\n\u2022 min(shift_level, 26 \u2013 shift_level): this value is the minimum number of steps that must be performed to decode each letter, under the assumption that decoding can be done by moving shift_level steps backward or (26 - shift_level) steps forward; as discussed above, GPT-4 indeed shows evidence of using both of these decoding directions.\nSeveral of these variables correspond to the critical properties that are indicative of our hypothesized reasoning processes: output_logprob should have a significant effect if probabilistic reasoning is used, shift_freq should have a significant effect if memorization is used, and min(shift_level, 26 - shift_level) quantifies the difficulty of the task, which should have a significant effect if noisy reasoning is used. The remaining factors are included as potential confounds to control for. The overall logistic regression thus took the following form:\ncorrect ~ min(shift_level, 26 \u2013 shift_level)\n+ input_logprob + output_logprob\n+ shift_freq\nLogistic regression results. The following features had a statistically significant effect on model performance: output_logprob, shift_frequency, and min(shift_level, 26 \u2013 shift_level) (p < 10-15 in all cases). These results therefore quantitatively support the conclusion that GPT-4 incorporates processes based on probability, memorization, and noisy reasoning."}, {"title": "4.2 Analyzing the effect of probability", "content": "If an LLM is influenced by probability, we would expect to occasionally observe unfaithfulness between the chain of reasoning steps produced by the LLM and the LLM's final answer. Specifically, if the individual reasoning steps would point to a final output that is low probability, a probabilistic reasoner might instead produce a different final answer that is higher probability. For example, in our CoT experiments, each step produces one letter, and these letters must be concatenated to form the final answer. If the individual step outputs are S, T, A, Z, the final answer should be STAZ, but a model might instead \u201cself-correct\u201d by producing the higher probability word STAY.\nSuch unfaithfulness can help or hurt the model. When the correct answer truly is a low-probability word such as STAZ, then correcting to STAY would reduce the model's accuracy. However, if the model had made a mistake during the reasoning chain\u2014such as by producing S, T, A, Z when the chain should have been S, T, A, Y\u2014then correcting to STAY would rescue the model from its error.\nTo investigate unfaithfulness, we compare the faithful accuracy that would be obtained by concatenating the model's step outputs to the actual overall accuracy. We indeed observe that overall accuracy is generally lower than faithful accuracy, illustrating that unfaithfulness occurs. Further, the drop in accuracy is more pronounced in the low-probability setting than the high-probability setting, which is consistent with the intuition that the lower the probability of a concatenated answer is, the more likely it will be that a probability-reliant model will be steered away from that answer. See Figure 8 in Appendix A.1 for the full results.\nTable 1 provides a more detailed view of unfaithfulness. Incorrect intermediate chains (i.e., concatenated step outputs) are followed by correct final answers much more often in the setting where the correct answer has a high probability (34% and 55% of the time for rot-4 and rot-13, respectively) than in the low probability setting (1% and 19% of the time for rot-4 and rot-13, respectively). On the other hand, correct intermediate chains are followed by incorrect final answers less often in the high probability setting (7% and 1% of the time for rot-4 and rot-13, respectively) than in the low-probability setting (14% and 9% of the time for rot-4 and rot-13, respectively).\nThese results support the hypothesis that GPT-4 over-relies on the prior probability of potential outputs (see Jang et al. (2023) for some related observations). If the answer has a high probability of occurrence, the model's priors favor generating it even if its intermediate reasoning steps suggest an alternative output. Conversely, if the answer is of lower likelihood, then even if the chain of reasoning is correct, the priors exert a detrimental influence leading to incorrect final answers."}, {"title": "4.3 Analyzing the effects of noise", "content": "The statistically significant impact of shift_level is evidence that GPT-4's CoT behavior is in part a noisy version of symbolic reasoning. Accuracy falls as the shift level increases from 1 to 12 and then recovers at higher shift levels (Table 2), consistent with a noisy reasoning process in which deciphering each letter with a shift level of n involves min(n, 26 \u2013 n) implicit steps, with noise that gives each step some probability of being performed incorrectly. Note that the implicit steps referred to here are different from the steps that are explicitly produced in the chain of thought: the chain of thought uses one explicit step per letter (Figure 2), but here we are discussing the operations that must be implicitly carried out within each step of this chain in order to decode each letter.\nNoise relating to complementary shift levels. We have argued that the relation between accuracy and shift level is evidence that GPT-4 uses a two-way strategy. That is, accuracy is high for small shifts such as 1 but also for large shifts such as 25, which could plausibly be explained by GPT-4 implicitly selecting whichever direction will minimize the number of steps it needs to compute-shifting letters backward for small shift levels or forward for large shift levels. This two-way strategy is effective in that it supports strong performance on large shift levels such as 25. However, we also observe evidence that it contributes to the noise that causes accuracy to decline as the shift level increases. Figure 9 (in Appendix A.4) shows the actual shift level that GPT-4 produces for each letter in its chain of thought, for each of four intended shift levels. Across all four of these cases, GPT-4 shows peaks at both shift_level and 26 \u2013 shift_level. Thus, while some of the noise affecting the reasoning process may be random, it appears that at least some of the noise can be attributed to confusion between possible shift levels. Decoding a shift level of n can be done by shifting backward n steps or forward 26 \u2013 n steps, but it appears that GPT-4 sometimes mixes up these two strategies by shifting forward n steps (or, equivalently, shifting backward 26 \u2013 n steps), contributing to the overall noise.\nDiscretization. Another factor that interacts with noise is temperature. In principle, if CoT entailed pure symbolic reasoning, it would assign 100% probability to the correct continuation (i.e., each predicted next token) and 0% probability to everything else. If so, the temperature used would not affect performance. However, we observe that CoT scores better with a low temperature. For instance, in rot-13, the accuracy is 30.0% at temperature=0 and 0.33% at temperature=1. This shows that its predicted distribution over the vocabulary is not fully discrete-it has some noise in it that a low temperature can remove. However, even a temperature of 0 does not make the performance perfect because noise does not solely arise in the final distribution over the vocabulary (which temperature modifies) but also influences the implicit intermediate steps used to produce that distribution (which temperature does not change)."}, {"title": "4.4 Analyzing the effect of memorization", "content": "To further investigate memorization, we focus on rot-13, because frequency is generally confounded with simplicity for the other shift levels (e.g., rot-1 is simple as well as frequent). 13 is the most frequent shift level (McCoy et al., 2023), and we observe in Table 2 that GPT-4 shows a spike in accuracy at this shift level in both Text-CoT and Math-CoT, providing strong evidence that memorization plays a role in GPT-4's CoT performance.\nWe also observe that memorization influences unfaithfulness. Consider the cells in Table 1 that involve incorrect chain steps outputs but correct final answers; such cases are much more common for rot-13 than for other levels, including rot-4 (the other shift level shown in that table); e.g., in the high-probability case, 55% of rot-13 examples fall in this category, while only 34% of rot-4 examples do. This pattern also provides some evidence for memorization: for rot-13, the model may have two \u201cpaths\u201d for producing the final output-it could use the chain of thought it has produced, or it could go directly from the input to the output due to memorization. Thus, when it produces the final output, it might implicitly weigh both of those paths, which helps it to correct faulty chains because it has a back-up path to consider. However, for rot-4, it may be that only the path involving the chain of thought is available, such that GPT-4 cannot fix incorrect chains as easily because it does not have this alternative path to fall back on."}, {"title": "4.5 The role of intermediate reasoning steps", "content": "Finally, we study the role of the intermediate reasoning steps that are involved with CoT prompting-both the chain that GPT-4 produces and the chain provided in the demonstration.\nStrong reliance on the surface strings produced in reasoning steps. First we focus on the chain of thought that GPT-4 produces before providing its final answer. We consider two potential roles that this chain could have. First, it could be that the chain is helpful because it provides text that is useful for GPT-4 to condition on in later steps. Alternatively, it could be that the critical aspect of CoT reasoning is internal rather than depending on the text that is produced, CoT could be helpful because it gives the LLM the opportunity to internally perform additional reasoning steps.\nTo disentangle these possibilities, we modify the prompt so that GPT-4 is told to perform the same steps of reasoning as before, but to have the intermediate output that it produces be uninformative. Specifically, we used the Text-CoT prompt but instructed the model to not reveal step answers and instead output a *. The step answers in the demonstration were also replaced by '*'; thus we left the format of reasoning intact but the expected generation token was no longer a component of the final answer. Next, we asked the model to explicitly think about the correct letter that should go in the place of the * but just not write it down. In the demonstration, we first provided an example with all step answers and then repeated the same example but with a * in place of each output letter (see Appendix A.5 Figure 10 for prompts).\nIn both settings, performance was similar to that of standard prompting. This is evidence that CoT depends on \u201cself-conditioning\u201d\u2014explicitly producing text that will be useful as context to condition on when producing the final answer. Merely instructing a model to \u201cthink\u201d silently is not helpful, suggesting that the reasoning does not occur internally. These results corroborate prior work finding that CoT is unhelpful when the model is told to produce contentless \u201cfiller\u201d tokens instead of contentful text (Lanham et al., 2023); models can reason internally when explicitly trained to do so (Pfau et al., 2024), but current LLMs without this explicit training do not seem to have this ability.\nLittle reliance on the validity of the demonstration. In experiments described in Appendix A.5, we also find that the validity of the reasoning shown in the prompt does not have a strong effect of CoT performance for shift ciphers. That is, even when the demonstration is perturbed such that it contains many errors, GPT-4's CoT performance remains approximately the same. This finding corroborates prior work showing that the validity of demonstrations did not matter much (Wang et al., 2023a; Madaan and Yazdanbakhsh, 2022; Ye et al., 2023); the demonstration seems to merely guide the model to solve the task by providing a format to generate accurate reasoning steps (Min et al., 2022)."}, {"title": "5 Conclusion", "content": "We have used the case study of shift ciphers to disentangle the factors that influence CoT reasoning, with a focus on characterizing what type of reasoning is used in models prompted with CoT. We found that CoT performance is statistically significantly influenced by the probability of occurrence of the expected task output, the frequency of the task in corpora, and the number of reasoning steps that must be (implicitly) performed. These results suggest that CoT reasoning can be characterized as probabilistic, memorization-influenced noisy reasoning, meaning that LLM behavior displays traits of both memorization and generalization."}, {"title": "6 Limitations", "content": "We have conducted extensive studies on a single task, decoding shift ciphers; we chose this task because it enables us to separate memorization from reasoning in a controlled manner as explained in Subsection 3.1, and these factors cannot be easily disentangled for most other tasks. Our prompts contain one example demonstration (i.e. one-shot CoT prompting), but a single demonstration contains at least 5 reasoning steps which provide more than just one reference of decoding to the model. We also studied a single model, GPT-4, because focusing on just one task and model made it possible to do a deep dive in which we carefully investigated each factor that we considered. An important direction for future work would be to investigate additional tasks and models. In addition, while our experiments showed that GPT-4 displays some hallmarks of true reasoning, they also showed some ways in which it makes errors due to a reliance on memorization and probability; future work could investigate how these limitations could be overcome."}, {"title": "7 Ethical Considerations", "content": "We do not believe that this work raises major potential risks. As is the case with any analysis work, there is some risk that our results could lead some readers to overestimate or underestimate the abilities of LLMs, either of which could have negative consequences: overestimation can contribute to hype, while underestimation can result in the field paying too little attention to potential harms. However, we believe that this risk is minimal because we have aimed to present our results in a balanced way that highlights both strengths and limitations."}, {"title": "A Additional Details & Results", "content": "A.1 Dataset details\nTo create our dataset of 7-letter words, we found all 3-letter and 4-letter tokens from the lowercase English alphabet and formed words by considering all possible combinations of a 3-letter token followed by a 4-letter token. Note that GPT-4's tokenizer distinguishes between word-initial tokens and non-word-initial tokens; thus, to be precise, the tokens that we used were 3-letter word-initial tokens and 4-letter non-word-initial tokens. Following McCoy et al. (2023), we compute the log probability as the log probability that GPT-2 (Radford et al., 2019) assigns to the sentence 'The word is \u201cWORD\u201d', minus the log probability that it assigns to \u2018The word is \u201c'; thus, this yields the log probability assigned to just the word and the following quotation mark in the context of 'The word is \u201c\". The closing quotation mark is included because it indicates the end of the word. Finally, we manually checked the words in the dataset and filtered them to ensure there were no inappropriate words used. We partitioned each group of 150 examples into two subsets: a subset containing 100 words used to evaluate GPT-4, and a subset containing 50 words used to evaluate logistic regression models that were fitted to GPT-4's performance on the 100-word subset. We run all evaluations a single time; the accuracies that we report are accuracies over these 100-example sets.\nA.2 Standard, Math & Number-CoT prompts\nThe Standard prompt (Figure 5) used is adopted from McCoy et al. (2023) and yields poor performance on most shift levels. The Number-CoT prompt (Figure 6) in contrast gives nearly perfect scores across the shift levels. It is to be noted that some miscellaneous noise is captured in the Number-CoT case. This arises mostly due to incomplete generations/half-completed chains requiring more tokens than needed as the model does some additional sub-reasoning steps, and in very\"\n    }"}]}