{"title": "Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models", "authors": ["Zhiyuan Hu", "Chumin Liu", "Xidong Feng", "Yilun Zhao", "See-Kiong Ng", "Anh Tuan Luu", "Junxian He", "Pang Wei Koh", "Bryan Hooi"], "abstract": "In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 Questions' game, UoT achieves an average performance improvement of 57.8% in the rate of successful task completion across multiple LLMs compared with direct prompting, and also improves efficiency (i.e., the number of questions needed to complete the task). Our benchmark and code are publicly released2.", "sections": [{"title": "1 Introduction", "content": "As the capabilities of large language models (LLMs) grow, they are being increasingly deployed in challenging real-world settings involving uncertainty and ambiguity. In particular, recent work aims to develop LLM agents or assistants Xi et al. (2023); Park et al. (2023) that effectively complete tasks in interactive environments, leading to a growing need for LLMs that can actively seek the information they need to solve a task by asking questions in conversational settings. For example, in medical diagnosis, patients often do not initially report their symptoms in full detail. In such situations, the ability of the doctor to ask effective questions is crucial, as a successful diagnosis often depends on revealing important details that the patient did not initially provide (Figure 1)."}, {"title": "2 Methodology", "content": ""}, {"title": "2.1 Problem Formulation", "content": "The problem setting involves two roles: the Questioner and the Answerer, performed by the LLM and a human, respectively. The goal of the Questioner is to deduce an unknown piece of information. We formulate this using a possibility space \\(\\Omega\\), which is the set of all possible options, of which a single element \\(\\omega \\in \\Omega\\), is the true option in each given scenario. For example, in a medical diagnosis setting, \\(\\Omega\\) is the set of all possible diseases relevant in the context, e.g., \\(\\Omega = \\{Bronchitis, Flu, ..., Hypertension\\}\\), and for each patient, \\(\\omega\\) is the actual disease of the patient."}, {"title": "2.2 Uncertainty of Thoughts: Overview", "content": "As Figure 2 shows, to effectively reduce uncertainty, our UoT method first generates multiple questions as candidates to ask, and simulates possible futures for each one in the form of a tree structure. Next, uncertainty-based rewards, motivated by information gain, are used to assess the questions within the simulation. Finally, a reward propagation scheme is used to compute the expected reward from asking each candidate question, allowing us to select the one with highest expected reward, to ask the Answerer."}, {"title": "2.3 Question Generation and Simulation", "content": "UoT starts by using an LLM to generate several candidate questions, then simulates future scenarios for each one. This simulation process allows us to measure how much information we can expect to gain in the next few steps from each question, and thus to choose the most suitable question.\nQuestion Generation Recall that our setting involves sequential interactions between a Questioner (e.g., a chatbot) and an Answerer (e.g., a human patient). During the \\(i\\)th interaction step, the Questioner generates candidate questions, then selects one of these to ask, denoted as \\(q_i\\).\nTo generate candidate questions to ask, UoT uses two inputs: (1) the history of past interactions \\(h_i = \\{q_1, a_1, q_2, a_2,\\cdots, q_{i-1}, A_{i-1}\\}\\), comprising the sequence of past questions and answers; and (2) the current possibility set \\(\\Omega_i\\). These two inputs are combined to form a prompt that includes instructions explaining the nature of the task (e.g., how the 20 Questions game works), provides the current history \\(h_i\\) and the current possibility set \\(\\Omega_i\\), and asks an LLM to generate \\(m\\) candidate next questions, conditioned on the previous context. This prompt, denoted as \\(Prompt_{gen}(h_i, \\Omega_i)\\), is fed to"}, {"title": "Multistep Simulation", "content": "As shown in Figure 2 (a), the Question Generation stage generates candidate questions such as \\(q_1^1\\) = \"Did you vomit?\" Next, during Simulation stage, for each such generated candidate question, we simulate possible futures for a few steps, forming a tree of possibilities. This process will enable us to compute rewards for each question, helping us to decide which question to ask.\nEach node of the tree can be one of two types: Answerer Nodes where it is the Answerer's turn to answer a question, and Questioner Nodes where it is the Questioner's turn to ask a question. At the root, a question has just been asked (e.g., \\(q_1^1\\)), so the root is an Answerer Node. Next, we explain how to construct tree by recursively expanding (or 'branching') each node to construct its children, i.e., starting from the root, then proceeding to its children, and so on.\n\u2022 At each Answerer Node, a question has just been asked. Next, we need to further 'branch' the tree based on the possible answers to the current question. Rather than allowing completely open-ended answers, we instead focus on affirmative and negative responses5, as this allows us to compute meaningful uncertainty metrics, as we discuss later. Hence, we branch the node into two children, corresponding to affirmative and negative answers.\n\u2022 At each Questioner Node, we prompt an LLM to generate \\(m\\) questions using the current history and possibility set, in the same way as in the Question Generation step. Note that while the generation procedure is similar, the purpose is different: the Question Generation step generates candidate questions to select from, while here we are generating simulated questions to form a tree for the purpose of evaluating the current question. The resulting \\(m\\) generated questions are added to the tree as children of the current node.\nIn this way, we recursively generate nodes of the tree, stopping when we reach a fixed number of levels (i.e., depth).\nWhile generating this tree, we also recursively compute the current possibility set \\(\\Omega_v\\) at each node \\(v\\). Specifically, let \\(h_v\\) be the current conversation history up to node \\(v\\), combining both the actual conversation history \\(h_i\\) and the simulated conversation up to node \\(v\\). Then the current possibility set at this node, denoted \\(\\Omega_v\\), is the subset of the possibility space consistent with \\(h_v\\). At the root, the current possibility set is only limited by the actual conversation history, i.e., \\(\\Omega_i\\). Then, as we proceed over the simulated tree, note that the current possibility set only changes at Answerer nodes, when an answer is added to the current history. Hence, at each Answerer node \\(v\\), we prompt a new LLM (an \u2018Answerer Simulator\u2019 \\(LLM_{ans}\\)), to determine the further subset \\(\\Omega_v^A \\subseteq \\Omega_v\\) for which the answer to the current question is affirmative, and the corresponding \\(\\Omega_v^N = \\Omega_v \\setminus \\Omega_v^A\\) for which the answer is negative. This allows us to recursively compute the possibility sets of the children of \\(v\\) (which themselves correspond to the affirmative and negative answers)."}, {"title": "2.4 Uncertainty-Based Reward Calculation", "content": "To develop suitable information-seeking approaches, a critical question is how to evaluate the effectiveness of a question, i.e., its contribution to reducing uncertainty. To address this, we turn to information theory, specifically the concept of information gain, which measures the amount by which uncertainty decreases after a particular observation. To reward information-seeking behavior, we assign rewards to questions based on how much they reduce the model's uncertainty about the unknown random variable. These reward signals are used by our UoT framework to determine which question to select, to maximize the reduction of uncertainty."}, {"title": "Entropy", "content": "Entropy and information gain are well-known concepts in information theory Shannon (1948). In our work, we use these concepts to measure how much information is gained (or equivalently, how much uncertainty is reduced) by asking a question, to formulate our rewards. Entropy measures the level of uncertainty in a random variable: higher entropy indicates greater uncertainty. The entropy of a discrete random variable X taking values \\(x_1, ..., x_n\\) is:\n\\[H(X) = - \\sum_{i=1}^n p(x_i) \\log p(x_i)\\]"}, {"title": "Information Gain at a Node", "content": "We now quantify the uncertainty reduction when receiving answers at an Answerer node \\(v\\). Recall that the answer given at \\(v\\) partitions \\(\\Omega_v\\) into two disjoint subsets: \\(\\Omega_v = \\Omega_v^A \\cup \\Omega_v^N\\), where \\(\\Omega_v^A\\) and \\(\\Omega_v^N\\) are the subsets of possibilities resulting in affirmative and negative answers respectively to the last asked question. Given an affirmative answer, the remaining entropy becomes:\n\\[H_v^A(X) := \\sum_{i: \\omega_i \\in \\Omega_v^A} p(x_i | \\Omega_v^A) \\log p(x_i | \\Omega_v^A)\\]\nWe define \\(H_v^N(X)\\) analogously for negative answers. Let \\(p^A = p(\\Omega_v^A)/p(\\Omega_v)\\) and \\(p^N = p(\\Omega_v^N)/p(\\Omega_v)\\) be the conditional probabilities of affirmative and negative answers at node \\(v\\). To compute the expected entropy after receiving the answer at node \\(v\\), since we have a \\(p^A\\) probability of receiving an affirmative answer and \\(p^N\\) of a negative answer, the expected entropy is:\n\\[p^A H_v^A(X) + p^N H_v^N(X)\\]\nAs such, the expected information gain at node \\(v\\) is the difference in entropies before and after receiving the answer:\n\\[IG_v(X) := H_v(X) - p^A H_v^A(X) - p^N H_v^N(X)\\]"}, {"title": "Reward Formulation", "content": "A natural approach would be to define the reward function \\(R_u(v)\\) at node \\(v\\) as the information gain \\(IG_v(X)\\): that is, the reward from the question at node \\(v\\) is the expected information gain \\(IG_v(X)\\) from receiving its answer. In practice, we find that a slightly modified function \\(IG_v(X)\\) is preferable. In particular, we find that \\(IG_v(X)\\) does not result in sufficiently sharp differences in reward over the typical ranges we encounter. Hence, we introduce an additional hyperparameter \\(\\lambda \\geq 0\\) which helps to sharpen the rewards using a scaling approach:\n\\[R_u(v) := IG_{\\lambda}(X) := \\frac{-p^A \\log p^A - p^N \\log p^N}{1 + \\lambda | p^A - p^N |}\\]\nThis definition ensures that \\(R_u(v)\\) falls within the range [0, 1], providing a normalized and consistent reward to measure uncertainty reduction. The reward function reaches its maximum when the subsets \\(\\Omega_v^A\\) and \\(\\Omega_v^N\\) have equal probability, reflecting the maximum reduction in uncertainty. It reaches its minimum when one of the subsets has zero probability, indicating no reduction in uncertainty. Appendix D plots the reward function curve across values of \\(p_v^A\\) and \\(p_v^N\\)."}, {"title": "2.5 Question Selection Via Reward Propagation", "content": "Single-step rewards often fall short in dynamic settings as they only consider immediate impact, overlooking long-term effects. To overcome this, our method employs a reward propagation scheme over simulation trees. Specifically, we will define 'accumulated rewards' which accumulate rewards over multiple steps of the simulations, capturing the effectiveness of past decisions. These are used to calculate 'expected rewards,' indicating the expected information from the questions. The rewards then guide the selection of candidate questions.\nAccumulated Reward We first define the accumulated reward at each node \\(v\\), which accumulates the rewards at \\(v\\) and all its ancestors on the tree, defined recursively as:\n\\[R_a(v) := R_u(v) + \\begin{cases} 0 & \\text{v is root} \\\\ R_a(\\text{Parent}(v)) & \\text{otherwise} \\end{cases}\\]\nHere \\(R_u(v)\\) is the uncertainty-based reward at node \\(v\\) defined in Eq. (10), and \\(R_a(\\text{Parent}(v))\\) is the accumulated reward of the parent of \\(v\\). We compute these accumulated rewards by starting at the root and propagating down to the leaves. Intuitively, the accumulated reward at each leaf node represents the total reward we end up with at the end of the conversation at that node.\nExpected Reward Next, we compute the expected reward for each node \\(R_e(v)\\), which represents the expected total value of rewards received on expectation on a node and all its descendants on tree:\n\\[R_e(v) := \\begin{cases} R_a(v) & \\text{if v is a leaf; otherwise:} \\\\ p^A R_e(v_A) + p^N R_e(v_N) & \\text{if v is an Answerer Node} \\\\ \\frac{1}{m} \\sum_{\\omega \\in \\text{Children} (v)} R_e(w) & \\text{if v is a Questioner Node} \\end{cases}\\]\nFor the case where \\(v\\) is an Answerer Node, recall that \\(p^A\\) and \\(p^N\\) are the conditional probabilities of affirmative and negative answers at node \\(v\\), defined in section 2.4. \\(v_A\\) and \\(v_N\\) are its children, corresponding to the affirmative and negative answers. For the case where \\(v\\) is a Questioner Node, we assign equal probability to the \\(m\\) questions asked from this node. In this way, we propagate the expected rewards from the leaves up to the root, allowing us to compute the expected gain at the root (i.e., the expected reward for that candidate question).\nDetermining the Optimal Question Finally, to decide the question to ask, we select the question with highest expected reward (and therefore, the highest expected information gain, considering both immediate and future information gains):\n\\[q_i = \\arg \\max_{n=1}^m R_e(q_i^n)\\]"}, {"title": "2.6 UoT Summary", "content": "UoT first generates candidate questions \\(q_1^1, q_1^2, ..., q_1^m\\) based on the history \\(h_i\\) and current possibility set \\(\\Omega_i\\). Then, we conduct multistep simulation to generate a tree for each candidate question \\(q_1^n\\). Next, we compute the uncertainty-based rewards \\(R_u(v)\\), and propagate over the trees to compute accumulated reward \\(R_a(v)\\) and expected reward \\(R_e(v)\\). Lastly, the optimal question \\(q_1^r\\) with highest expected reward will be selected as \\(q_i\\) to interact with the Answerer."}, {"title": "2.7 Extensions and Discussion", "content": "Pruned UoT. To enhance efficiency during simulation, pruning akin to Beam Search can be employed when constructing the simulation trees, which limits the number of paths to explore over the tree to a predetermined size.\nOpen Set UoT. Recall that in the closed set scenario, the Questioner starts with knowledge of the possibility space \\(\\Omega\\). In practice, the possibility space is often unknown, resulting in the open set setting. To adapt UoT to this case, we prompt the Questioner to initialize the possibility space \\(\\Omega\\) and then reinitialize the possibility set \\(\\Omega_i\\) according to current history \\(h_i\\). Following this, the rest of UoT proceeds unchanged."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Experimental Setup", "content": "Models We test various LLMs to evaluate the generality of our method, including Llama 2-70B-Chat Touvron et al. (2023), Cohere Cohere (2023), PaLM 2 Anil et al. (2023), Claude 2 Anthropic (2023), GPT-3.5-turbo OpenAI (2023a) and GPT-4 OpenAI (2023b).\nBaselines Direct Prompting in Open-Set (DPOS) prompts an LLM directly to generate the next response, without using information about the possibility set. Direct Prompting in Closed-Set (DPCS) At the start of the interaction, the LLM is provided with full information about the possibility set. The subsequent baselines also follow this setting. Planning Prompting (PP) is motivated by Wang et al. (2023). We leverage another LLM to plan the future and, consequently, determine the question to ask. Chain-of-Thought (CoT) Wei et al. (2022) includes a series of intermediate reasoning steps, which significantly improves the ability of LLMs to perform complex reasoning. CoT-SC (Self-Consistency) Wang et al. (2022) is an ensemble approach that samples a variety of reasoning paths rather than just the most obvious one. We set a specific sampling count to fairly compare CoT-SC with other approaches in computational cost. Reflexion Shinn et al. (2023) enables the agent to propose an action, and uses self-evaluation-based rewards to assess whether the agent should reflect to develop a new idea. Tree-of-Thoughts (ToT) Yao et al. (2023) enables LMs to make decisions by exploring and evaluating multiple reasoning paths over a tree structure. We examine ToT under two setups: Original-ToT, which uses the standard approach of generating and evaluating questions, and Adapted-ToT, where we integrate heuristic experience into prompt for question generation and evaluation, focusing on questions that halve the search space. We matched the tree depth to the simulation steps in our UoT method for a fair comparison. Hyperparameters and detailed experimental settings are in Appendix F.1, and prompts in Appendix H.\nScenarios and Datasets 20 Questions is a classic game where the answerer thinks of a word or item, and the questioner asks up to 20 yes-or-no questions to guess it. We use two distinct datasets, namely 20 Questions (20Q) in BIG-bench Srivastava et al. (2022), and Common (collected by us, refer to Appendix F.2 for more details), including 29 and 111 items separately. In this scenario, the maximal turns is set to 20. In Medical Diagnosis, the doctor needs to ask questions to patients about their symptoms, to determine an accurate diagnosis. We use two datasets: DX Xu et al. (2019), with 104 doctor-patient dialogues and 5 diseases in test set, and MedDG Liu et al. (2022), with over 17K conversations and 15 type diseases, from which we manually select 350 high-quality test samples for evaluation. Both datasets have a maximum of 5 turns. Troubleshooting is a scenario where a customer support technician interacts with customers to identify and resolve faults or issues within computer systems, electronic devices, machinery, or other complex systems. Raghu et al. (2021) introduce FloDial with 894 dialogues, containing 153 faults. We evaluate using a maximum of 20 turns. The answerer is simulated by GPT-4, prompted with the ground truth information for a case (e.g., the patient's ground truth disease, self-report, and conversation). For more details, refer to Appendix F.2. We provide examples of these scenarios in Appendix G."}, {"title": "3.2 Performance", "content": "20 Questions As illustrated in Table 1, for all types of LLMs, those equipped with UoT outperform the baselines in both DPOS and DPCS settings. Among the methods used on GPT-4 to enhance planning and reasoning, CoT and PP show inferior performance even compared to GPT-4 alone. UoT achieves the highest success rate, surpassing the second-best Reflexion by an average of 7.5%. Moreover, it demonstrates superior efficiency, with an average of 2 fewer rounds than Reflexion in successful cases.\nMedical Diagnosis UoT outperforms baselines in simplified medical diagnostics, achieving a 97.0% success rate on the DX dataset with GPT-4. On the MedDG dataset, UoT on PaLM 2 and GPT-4 achieve success rates of 80.7% and 88.0%. UoT significantly reduces conversation length, with average MSC of 2.0 on GPT-4 for DX, lower than 3.5 and 3.0 for DPOS and DPCS methods.\nTroubleshooting UoT similarly achieves the highest SR of 67.3%, and the lowest MSC of 7.8. When equipped with UoT, GPT-3.5 shows a remarkable improvement in SR from 22.6% to 67.1% in the closed set setting.\nOverall Performance On average, UoT enhances the success rate by 57.8% compared to DPCS across 5 datasets and 6 different LLMs, including open source and commercial models. Notably, success rate increases 102.8% for Cohere. Furthermore, UoT outperforms CoT-SC by 38.4% and Reflexion by 31.2%. Even compared to tree structure methods like Original-ToT and Adapted-ToT, UoT still shows superior performance with gains of 33.7% and 17.7% respectively. Additionally, Pruned UoT, our pruning method to improve efficiency, outperforms Adapted-ToT by 10.4%."}, {"title": "3.3 Analysis", "content": ""}, {"title": "3.3.1 Comparing Model Performance at Equal Computational Efficiency", "content": "Here, we compare performance across approaches with similar computational cost, in terms of token consumption. To do so, we first prune our UoT as described in section 2.7. Secondly, we expand the exploration depth of Adapted-ToT method to bring its token cost in line with that of UoT."}, {"title": "3.3.2 Effectiveness of Uncertainty Rewards", "content": "To further demonstrate the effectiveness of our uncertainty-based reward, we compare it with the self-evaluation reward used in the original ToT based on GPT-4 model. We implement the uncertainty-based reward in place of the self-evaluation reward in ToT, creating a variant we call ToT (+UR). The results, as shown in left side of Figure 3, indicate that our reward significantly enhances planning efficacy by an average of 9.3%. Additionally, we use the heuristic self-evaluation reward in Adapted-ToT to replace our current uncertainty-based reward in UoT, a variant we refer to as UoT (-UR). This change results in a performance decrease shown in the right part of Figure 3, further validating the effectiveness of our uncertainty-based reward. Moreover, the performance of UoT (-UR) still surpasses that of Adapted-ToT illustrated in Table 1,"}, {"title": "3.3.3 Case Studies", "content": "As demonstrated in Figure 4, compared to direct prompting, UoT asks questions that better reduce uncertainty and narrow down candidates, rather than overly specific questions that yield limited information. Moreover, once our method acquires initial information (e.g., stomach pain), it generates more targeted questions (related to digestive issues), instead of general inquiries."}, {"title": "3.3.4 Performance of Open Set UoT", "content": "To further assess the practicality of UoT, we test it in the open set setting and compare its performance with DPOS. As shown in Table 3, UoT enhances DPOS by an average relative improvement of 17.4%."}, {"title": "3.3.5 Further Analysis", "content": "Our study finds that the UoT's one-step planning performs well, thanks to effective reward design and question selection. Although deeper planning improves performance, we cap simulations at three steps for budget reasons, striking a balance between efficiency and effectiveness. We also assess GPT-4's accuracy as an answerer by analyzing 10% of interactions from each dataset. GPT-4 consistently provides accurate and reliable responses. For more details and quantitative results, see Appendix B and C."}, {"title": "4 Related Work", "content": "Planning and Reasoning of LLMs LLMs show prowess in planning and reasoning. Wei et al. (2022) introduced CoT prompting for intermediate reasoning; Yao et al. (2023) proposed ToT prompting using DFS/BFS. Besta et al. (2023) present GoT to solve elaborate problems. Feng et al. (2023) illustrated TS-LLM's tree-search guided decoding. ReAct Yao et al. (2022) offers acting-based prompting, while Reflexion Shinn et al. (2023) enhances this with feedback reflection. Zhou et al. (2023) unify reasoning and planning.\nDecision-making and Information-seeking by LLMs LLMs have evolved as decision-making tools, with models like LLM+P Liu et al. (2023) and LLM-DP Dagan et al. (2023) combining external planners and LLMs for natural language-based programming. RAP Hao et al. (2023) goes beyond structured language, using LLMs with Monte Carlo Tree Search (MCTS) Chaslot et al. (2008) for dynamic decision-making. This approach is also seen in the work of Zhao et al. (2023), applying MCTS and LLM knowledge for complex tasks like robot control. However, MCTS struggles in uncertain scenarios due to its reliance on terminal states and specific modules for rewards and action selection. Additionally, to enhance LLMs' questioning abilities, Deng et al. (2023) introduce the Rephrase and Respond method. AVIS Hu et al. (2023) represents an autonomous visual question answering system that uses external tools. Pan et al. (2023) introduce KwaiAgents for processing queries, following guidelines, and accessing external documents."}, {"title": "5 Conclusion and Discussion", "content": "This paper presents the Uncertainty of Thoughts (UoT) algorithm, significantly improving LLMs in tasks requiring active information seeking through tree-based simulation, uncertainty-based rewards and a reward propagation scheme. On five datasets UoT increases success rate by 57.8% on average, establishing a new benchmark for evaluating LLMs in active information-seeking tasks. We evaluate UoT on simplified scenarios; more realistic scenarios raise challenges like allowing incomplete elimination of possibilities by answers, and issues with completely open-ended questions/answers, which we leave for future work. We further discuss these limitations and future work in Appendix E."}, {"title": "A Derivation of Information Gain Formula", "content": "Recall that the information gain at node v is defined as the expected change in uncertainty (or entropy) when receiving an answer at this node, which we defined as:\n\\[IG_v(X) := H_v(X) - p^A H_v^A(X) - p^N H_v^N(X)\\]\nWe now show that:\nProposition 1. The information gain at node v is equal to:\n\\[IG_v(X) = -p^A \\log p^A - p^N \\log p^N\\]\nProof. Note that for any outcome \\(x_i\\), we have by the rules of conditional probability:\n\\[p(x_i | \\Omega_v) = \\frac{p(x_i | \\Omega_v^A)}{p(\\Omega_v^A | \\Omega_v)} = \\frac{p(x_i | \\Omega_v^N)}{p(\\Omega_v^N | \\Omega_v)} = \\frac{p(x_i)}{p^A}\\]\nNow the information gain is:\n\\begin{align*}\nIG_v(X) &= H_v(X) - p^A H_v^A(X) - p^N H_v^N(X) \\\\\n&= - \\sum_{i: \\omega_i \\in \\Omega_v} p(x_i | \\Omega_v) \\log p(x_i | \\Omega_v) \\\\\n&+ p^A \\sum_{i: \\omega_i \\in \\Omega_v^A} p(x_i | \\Omega_v^A) \\log p(x_i | \\Omega_v^A) \\\\\n&+ p^N \\sum_{i: \\omega_i \\in \\Omega_v^N} p(x_i | \\Omega_v^N) \\log p(x_i | \\Omega_v^N) \\\\\n&= \\sum_{i: \\omega_i \\in \\Omega_v} p(x_i) (\\log p(x_i | \\Omega_v^A) - \\log p(x_i | \\Omega_v)) \\\\\n&+ \\sum_{i: \\omega_i \\in \\Omega_v} p(x_i) (\\log p(x_i | \\Omega_v^N) - \\log p(x_i | \\Omega_v)),\n\\end{align*}\nwhere the last equality holds by \\(p^A \\cdot p(x_i | \\Omega_v^A) = p(x_i | \\Omega_v)\\), and similarly for \\(p^N\\). We further compute that\n\\begin{align*}\n& \\sum_{i: \\omega_i \\in \\Omega_v} p(x_i) (\\log p(x_i | \\Omega_v^A) - \\log p(x_i | \\Omega_v)) \\\\\n&= \\sum_{i: \\omega_i \\in \\Omega_v} p(x_i) \\log \\frac{p(x_i | \\Omega_v^A)}{p(x_i)} \\\\\n&= \\sum_{i: \\omega_i \\in \\Omega_v} p(x_i) \\log p^A \\\\\n&= - p \\log p^A\n\\end{align*}\nAnalogously the remaining term is \\(-p^N \\log p^N\\). Finally we conclude that\n\\[IG_v(X) = -p^A \\log p^A - p^N \\log p^N\\]\nIn fact, this proposition can also be proven using some properties of information theory, particularly the definitions of conditional entropy and mutual information. As the more computational proof shown here is still relatively short and does not require defining certain additional probability distributions, we provide the computational proof here instead."}, {"title": "B Effect of Simulation Depth", "content": "As Figure 5 illustrates, we analyze the impact of simulation steps. Even with one-step reasoning and planning, our method can still have a strong performance, further indicating the effectiveness of our reward design and question selection mechanism. With the increase of the step, the performance can gradually rise. However, due to the constraints of computation resources and OpenAI API budgets, we only explore the simulation to the third step and argue that it can be the practical tradeoff between performance and efficiency."}, {"title": "C Reliability of GPT-4 as the Environment", "content": "As the impressive understanding ability of LLMs, previous research has validated the effectiveness of evaluators served by ChatGPT or GPT-4 Chiang & Lee (2023); Liu et al.. Consequently, we also adopt GPT-4 as the environment to provide feedback on our work. Prompts can be found in Appendix H.4. To assess the accuracy and reliability of employing GPT-4 as the environment simulator, we randomly sample 10% interaction records (including the final judgment and intermediate feedback from the environment) from each dataset. As Figure 4 shows, GPT-4 can provide completely accurate judgment and also keep a high level of accurate feedback during the interaction. These experimental results can further support the effectiveness of our method."}, {"title": "D Reward Function Details and Its Curve", "content": "Refer to Figure 6 for the curve of uncertainty-based reward function."}, {"title": "E Limitation and Future Work", "content": "In practice,  and  might overlap, as different answers (such as \"yes\" or \"no\") may lead to the exclusion of different sets of possibilities. Another similar limitation is that some questions"}, {"title": "F Experimental Setups", "content": ""}, {"title": "F.1 Baselines Setup", "content": "Chain-of-Thought (CoT) We adapt the typical CoT prompt which instruct LLM to generate the explanation or motivation for the proposed question first, then give the question to ask.\nChain of Thought with Self-Consistency (CoT-SC) To make the method spend comparable compute to our approach for a fair comparison, we sampled 33 times before deciding on each action with the LLM's temperature of 0.7. The final selected question is the one repeated most times among 33 samples.\nPlanning Prompting To measure whether LLMs' planning ability can be enhanced through some crafted prompts like CoT, ToT or Reflexion. We design the prompt to enable LLM to simulate multiple different sets of future interactions between questioner and answerer, then let LLM choose one most promising interaction (question) to ask.\nTree of Thoughts In the case of Original-ToT, a sampling method is employed"}]}