{"title": "Exploring Information Processing in Large Language Models: Insights from Information Bottleneck Theory", "authors": ["Zhou Yang", "Zhengyu Qi", "Zhaochun Ren", "Zhikai Jia", "Haizhou Sun", "Xiaofei Zhu", "Xiangwen Liao"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of tasks by understanding input information and predicting corresponding outputs. However, the internal mechanisms by which LLMs comprehend input and make effective predictions remain poorly understood. In this paper, we explore the working mechanism of LLMs in information processing from the perspective of Information Bottleneck Theory. We propose a non-training construction strategy to define a task space and identify the following key findings: (1) LLMs compress input information into specific task spaces (e.g., sentiment space, topic space) to facilitate task understanding; (2) they then extract and utilize relevant information from the task space at critical moments to generate accurate predictions. Based on these insights, we introduce two novel approaches: an Information Compression-based Context Learning (IC-ICL) and a Task-Space-guided Fine-Tuning (TS-FT). IC-ICL enhances reasoning performance and inference efficiency by compressing retrieved example information into the task space. TS-FT employs a space-guided loss to fine-tune LLMs, encouraging the learning of more effective compression and selection mechanisms. Experiments across multiple datasets validate the effectiveness of task space construction. Additionally, IC-ICL not only improves performance but also accelerates inference speed by over 40%, while TS-FT achieves superior results with a minimal strategy adjustment.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have achieved remarkable success in natural language processing (NLP), demonstrating exceptional performance across a wide range of tasks such as text generation, machine translation, and sentiment analysis. By understanding input information and predicting corresponding outputs, LLMs have shown strong capabilities in handling complex tasks. However, despite their impressive real-world performance, the internal mechanisms by which LLMs comprehend input and make accurate predictions remain largely unexplored.\nIn this paper, we investigate the information processing mechanisms of Large Language Models (LLMs) from the perspective of Information Bottleneck Theory. We propose a non-gradient-based task space detection strategy, which helps trace the internal information flow within LLMs. Using this strategy, we investigate the information flow across layers of LLMs during comprehension and prediction phase. During the understanding phase, LLMs compress input information into specific task spaces. In the prediction phase, LLMs extract and integrate relevant information from the task spaces, decompressing it at critical moments to generate predictions. That is, LLMs perform task comprehension and prediction by compressing and decompressing information within specific task spaces. Further results show that while LLMs effectively compress high-quality information during the compression phase, they struggle to decompress it during the prediction phase, leading to suboptimal performance.\nBased on these insights, we propose two novel methods derived from Information Bottleneck Theory: Information Compression-based Context Learning (IC-ICL) and Task-Space-guided Fine-Tuning (TS-FT). IC-ICL retrieves relevant examples and maps them into the task space, enhancing LLMs' decompression capabilities to improve pre-"}, {"title": "2 Information Detection for LLMS", "content": "In this section, we explore the information detection mechanisms within Large Language Models (LLMs) from the perspective of Information Bottleneck Theory, focusing on how information is processed and optimized during task comprehension and prediction."}, {"title": "2.1 Task-Space-based Information Detection Strategy", "content": "We propose a task-space-based strategy for detecting information flow in LLMs, which leverages the concept of compressing input information into task-specific spaces for efficient processing.\nThe task space is composed of multiple basic vectors, each constructed from the features that best represent the task at hand. For instance, in emotion classification, the most representative feature is the emotion categories. Therefore, we construct the emotional categories involved in the task as basic dimension vectors. Similarly, for topic classification task, we construct the topic types as the basic vectors.\nFor simplicity, we use emotion categories as an example to describe the construction process. For a emotion category $e_i \\in E$, such as \"joyful\", we construct the input pairs shown in Table 1, i.e., positive-related prompt $P^+$ and negative-related prompt $P^-$. The negative-related prompt pairs consist of randomly sampled other emotion types, while the dialogue context is drawn from Empathetic Dialogues, containing dialogue statements with the emotion category $e_i$.\nBased on these prompts, the LLM generates tokens $y_t^+$ and $y_t^-$ at time step t.\n$\\begin{aligned}\nY^+ &= LLM(y^+_t|P^+, y^+_{<t}) \\\\\nY^- &= LLM(y^-_t|P^-, y^-_{<t})\n\\end{aligned}$                                                                                                                                     (1)\n(2)\nWe then extract the hidden vectors at layer $l$ of the LLM, obtaining the positive-related hidden layer $h_{t,l}^+$ and the negative-related hidden vector representation $h_{t,l}^-$ for tokens $y_t^+$ and $y_t^-$ at time step t. Following prior methods (Liu et al., 2024), we obtain the directional hidden vector representation $H_{t,l}$ for time step t at layer l by subtracting the two vectors.\n$h_{t,l} = h_{t,l}^+ - h_{t,l}^-$\n(3)\nUsing the above method, we collect $N_h$ direction vectors for the emotion type $e_i$. These vectors contain both the representation of the emotion type $e_i$ and noise. To purify the emotion type representation, Principal Component Analysis (PCA) is applied, resulting in refined direction vectors $H_{e_i}^l$ for $e_i$. In this paper, we treat these direction vectors as dimensional vectors of the space elements.\n$H_{e_i}^l = PCA(H_{e_i}^l)$                                                                                                                    (4)"}, {"title": "2.2 Constructing Task Spaces", "content": "The construction of task spaces is central to our approach, as it defines how information is organized and compressed to facilitate task comprehension and prediction. To validate the rationality of the task space, we visualize it for observation and analysis.\nFigures 1 and 2 present the 2D and 3D visualizations of the emotion space after dimensionality reduction using t-SNE. The distribution of emotions in the figures shows that the emotions are fairly evenly distributed in the space, with similar emotions clustering closer together. For example, \"terrified,\" \"afraid,\" \"anxious,\" and \"angry\" are located closer to each other.\nTo more clearly illustrate the similarity between emotions, we also construct a heatmap of the cosine distances between emotions. The heatmap, shown in Figure 3, calculates the cosine distance between pairs of emotions, where a brighter color indicates a smaller distance (i.e., higher similarity). The results explicitly show that more similar emotions, such as \"excited\" and \"joyful\u201d, have higher similarity scores, while more distinct emotions, such as \"annoyed\" and \"caring\u201d, exhibit lower similarity."}, {"title": "2.3 Information Compression in Understanding", "content": "During the understanding phase, LLMs compress input information into task-specific spaces, retaining only the most relevant details for effective task processing."}, {"title": "2.3.1 Calculation of Mutual Information", "content": "Objective: The goal of this experiment is to verify the information state of input samples in the emotion space.\nHypothesis: The representation of the samples compresses toward a specific emotion space.\nSteps: (1) Sample Representation. We selected n samples S from the Empathetic Dialogue dataset for mutual information statistical experiments. For sample $s_i$, its hidden layer representation $h_i^l$ at the l-th layer is obtained by inputting it into the LLMs.\n$\\begin{aligned}\nh_i^l &= [h_0^l, ..., h_j^l, ..., h_N^l] \\\\\nh_i^l &\\in R^{dXN}\n\\end{aligned}$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (5)\n(6)\n(2) Emotion Space Representation. For sample $s_i$, we define its ground-truth emotion label as $e_{s_i}$. To observe the sample's state in the emotion space, we define multiple emotion spaces: $e_{s_i}^{top1}, e_{s_i}^{top2}, e_{s_i}^{topd_k}$, and $e_{s_i}^{topd_e}$.\n$e_{s_i}^{topd_k} = Top_j^{cosine}(e_{s_i}, e_j)$                                                                                                 (7)\n$o_{mj} = Cosine(v_{s_i}^j, v_{m_i}), m_i \\in n_d,$                                                                                                (14)\n$\\begin{aligned}\ne_{s_i}^{topdk} &= \\frac{1}{d_k}\\sum_{j=1}^{d_k}e_j \\\\\ne_j &\\in e_{s_i}^{top}\n\\end{aligned}$\n(8)\nHere, $Top_j^{cosine}$ is a function that sorts emotions based on cosine similarity and selects the top $d_k$ most similar emotion types. $e_j \\in E$ represents the emotions from the set of all emotion types E. When $d_k=1$, the emotion space at the l-th layer corresponds to the base vector derived from the ground-truth emotion label. When $d_k \\ne 1$, the emotion space at the l-th layer is the mean of the $d_k$ nearest neighbors (including the sample's own label) based on cosine distance.\n(3) Projection to Emotion Space. For the j-th token's representation at the l-th layer, $h_j^l$, we project it onto the corresponding emotion space $e_{s_i}^{ltop}$.\n$\\begin{aligned}\nh_{j}^{p,l} = \\frac{h_j^l e_{s_i}^{ltop}}{||e_{s_i}^{ltop}||^2}e_{s_i}^{ltop}\n\\end{aligned}$\n(9)\nFor the N tokens in sample $s_i$ during the comprehension process, we sum the projected representations $h_j^{p,l}$ to obtain the overall projection of the sample:\n$\\begin{aligned}\nh_{s_i}^{p,l} = \\sum_{j=1}^N h_j^{p,l}\n\\end{aligned}$\n(10)\nMutual Information Estimation. Mutual Information (MI) measures the dependency between two random variables X and Y. It quantifies the reduction in uncertainty of one variable given the other. The mutual information is defined as:\n$I(X; Y) = \\int\\int p(x,y) \\log \\frac{p(x, y)}{p(x)p(y)} dx dy,$(11)\nwhere p(x, y) is the joint probability density function of X and Y, and p(x), p(y) are their respective marginal probability densities.\nIn practice, exact computation of mutual information is infeasible as the true probability distributions are unknown. Therefore, we employ K-Nearest Neighbors (KNN)-based methods to approximate the densities. Using these approximations, the MI can be expressed as:\n$I(X; Y) \\approx \\frac{1}{N} \\sum_{i=1}^N log \\frac{p(x_i, Y_i)}{\\hat{p}(x_i)\\hat{p}(y_i)},$                                                                  (12)\nwhere $\\hat{p}(x_i, Y_i), \\hat{p}(x_i)$, and $\\hat{p}(y_i)$ are the estimated joint and marginal probabilities for the samples $x_i$ and $Y_i$.\nUsing the method described above, the mutual information $I(h_{s_i}^l; e_{s_i}^{ltop})$ between the sample $s_i$ and the emotion space $e_{s_i}^{ltop}$ at the l-th layer can be obtained. At the same time, we also computed the mutual information $I(h_{s_i}^l; h_{s_i}^0)$ between the l-th layer and the 0-th layer."}, {"title": "2.3.2 Analysis of Results", "content": "According to information bottleneck theory, information during the comprehension process should move away from the initial space and towards the target emotion space. That is, the mutual information between the l-th layer's hidden state and the 0-th layer's hidden state should gradually decrease, while the mutual information with the target emotion space should gradually increase.\nFigure 4 shows the variation in mutual information of the l-th layer's hidden state in the ground-truth emotion space, i.e., with $d_k = 1$. According to the results, the mutual information in the earlier layers of the LLM fluctuates significantly. This is primarily because the shallow layers of LLMs mainly process basic information such as syntax and grammar. However, after layers 12 to 28, the mutual information between the hidden state z and the input x gradually decreases, while the mutual information with the target emotion space y gradually increases. This suggests that LLMs compress the input content towards a specific emotion space.\nFigures 5 and 6 show the variation in mutual information across a broader range of emotion spaces, i.e., with $d_k = 1$. The results also exhibit the same trend.\nSpecifically, we plotted the mutual information variation for the widest emotion space, with $d_k = 32, as shown in Figure 7. We observe that"}, {"title": "2.4 Information Decompression in Prediction", "content": "In the prediction phase, LLMs decompress the relevant information from task spaces to generate accurate predictions, highlighting the challenges and limitations in the decompression process."}, {"title": "2.4.1 Information Measurement During the Prediction", "content": "Objective: Explore the decompression mechanism of LLMs. Hypothesis: LLMs decompress information from the emotion space at the key time step t to make predictions.\nSteps: We tested the mutual information between the LLMs' hidden state at time step t and the hidden state at time step 0, as well as the mutual information with the emotion space $e_{s_i}^{top})$. Here,"}, {"title": "2.4.2 Detection of Key Prediction Steps", "content": "To further investigate whether LLMs rely on certain key time steps t during prediction, we conducted additional experiments.\nObjective: Explore the key time steps during LLMs' prediction process.\nHypothesis: There exist key time steps t that significantly affect the prediction results.\nSteps: At each time step t, we directly add or subtract the ground-truth emotion space to the LLM's hidden state to examine the effect of each time step. The 0-th step represents the information representation during the understanding phase, while steps 1-5 represent the information representations during the generation process.\nThe experimental results are shown in Table 1. The results indicate that the most crucial time steps are the 0-th, 4-th, and 5-th steps. Modifying the LLM's hidden state at these time steps significantly altered the prediction accuracy. The impact of the 0-th step is mainly due to the fact that LLMs are processing and understanding the input sample $s_i$ at this point. The significant influence of the 4-th and 5-th steps is mainly because LLMs decompress key information at these moments.\nIn general, when LLMs are understanding the input information at t=0, removing the hidden state's representation in the emotion space significantly reduces emotional accuracy. This is mainly because LLMs rely on decompressing the state at the understanding stage to make accurate predictions. Removing the hidden state's representation in the emotion space at this point makes it difficult for the model to decompress effective information. On the other hand, adding the emotion space projection to the hidden state does not have a significant impact on the results, suggesting that LLMs have already effectively compressed information in the emotion space.\nAt the prediction stage (t=4,5), both adding and subtracting the hidden state's representation in the emotion space significantly affect the results. This indicates that there are key moments during the prediction process when LLMs rely on specific emotional representations.\nFurthermore, when the hidden state is added to the emotion space, the LLMs' performance greatly improves. This suggests that LLMs struggle to decompress high-quality information on their own, and better decompression requires additional strategies or support to effectively extract the desired information."}, {"title": "3 Method", "content": "To further enhance LLMs' ability to decompress information, we propose two models: Information Compression-based Context Learning and Task-Space-guided Fine-Tuning."}, {"title": "3.1 Information Compression-based Context Learning", "content": "We first used a pre-trained model, ROBERTalarge, to retrieve the top $k_s$ most similar samples.\n$\\begin{aligned}\nP_{s_i}, V_{s_i} &= ROBERTalarge(\\text{S}_i) \\\\\nO_{m_i} &= Cosine(v_{s_i}^j, V_{m_i}), m_i \\in n_d, \\\\\nS_j &= Top^{k_2}(\\Omega_1, \\Omega_2, ..., \\Omega_{m_1}), j \\in [1,k_2],\n\\end{aligned}$\n(13)\n(14)\n(15)\nNext, we extracted the emotion labels of these $k_s$ samples and converted them into emotion vector representations. These vectors were then weighted by $w_e$ and added to the hidden layers of the LLMs.\n$\\begin{aligned}\nh_e^l &= \\sum_{n_i=1}^{k_1} w_{e_i}h_{e_i}^l \\\\\nh' &= h + h_e \\\\\nh^2 &= h^2 + h_e \\\\\n\\end{aligned}$\n(16)\n(17)\nIntuitively, since the weight $w_e$ is fixed, the introduction of $k_2$ direction vectors, each added with equal weight, may not accurately provide the model with a high-quality hidden state. Therefore, we further refined the adjustment of these direction vectors.\n$\\begin{aligned}\nO_E &= \\varphi_l + h_E_i \\\\\ng_E_i &= Softmax(O_E) \\\\\nh' &= h + w_a(g_E h_e^l)\n\\end{aligned}$\n(18)\n(19)\n(20)\nFinally, we prompt the LLM to predict the emotion category of the dialogue context.\n$\\begin{aligned}\ne_i &= LLM(s_i) \\\\\nY &= LLM(y_t|S_i, C_i)\n\\end{aligned}$\n(21)\n(22)"}, {"title": "3.2 Task-Space-guided Fine-Tuning", "content": "For the sample $s_i$, this section transforms its corresponding emotion label $e^*$ into the direction vector at the l-th layer. Then, it is added to the hidden layer of the LLMs at the l-th layer to obtain a higher-quality hidden layer.\n$h_i^{*,l} = h_i^l + h_e^{*,l}$                                                                                                                                   (23)\nTo ensure that the hidden layers converge towards a better-quality direction during training, we design a mean squared error (MSE) loss.\n$\\begin{aligned}\nL_{mse}^l = \\frac{1}{d}\\sum_{i=1}^d(h'^{l,i} - h^{l,i})^2 \\\\\n\\end{aligned}$                                                                                                                                                                                                                                                                                             (24)\n$L_{mse} = \\sum_{l=1}^{L} W_{mse}* L_{mse}^l$(25)\nwhere $W_{mse}$ is a hyperparameter, and n and L are the dimensionality and number of layers of the LLMs, respectively.\nFor $s_i$, we also employ cross-entropy as the generation loss to encourage the LLMs to produce outputs in the corresponding response format.\n$L_{LLM} = - \\sum_{t=1}^Tlog p_\\theta (x_t|x_{<t}, S_i)$                                                                 (26)\nwhere $x_{<t}$ is the previously generated text. Curriculum learning is used for optimization in this case. T represents the training time step.\nOverall, we optimize the model based on the two losses:\n$L = L_{mse} + L_{LLM}$                                                                                                              (27)"}, {"title": "4 Results and Analysis", "content": "We validate the proposed method on the Empathetic Dialogues dataset, with the results shown in Table 3. The results indicate that the performance of the proposed IC-ICL method significantly outperforms the baseline. Along with improving inference performance, the inference speed also shows a substantial improvement. At the same time, the proposed TS-FT method also outperforms the baseline. The advantages of both methods show that enhancing the information decompression capability of LLMs further promotes their performance."}, {"title": "5 Related Work", "content": "In-Context Learning. Wei et al. (Wei et al., 2022) propose chain-of-thought prompting, which decomposes reasoning into sequential logical steps to enhance LLMs' structured reasoning. Wang et al. (Wang et al., 2022) introduce self-consistency prompting, generating multiple reasoning paths and selecting the final answer through majority voting. Yao et al. (Yao et al., 2023) develop a tree-based approach that breaks complex problems into hierarchical sub-problems to improve reasoning accuracy."}, {"title": "6 Conclusion", "content": "This paper investigates the information processing mechanisms of Large Language Models (LLMs) through the lens of Information Bottleneck Theory. We show that LLMs compress input into task-specific spaces but struggle with decompression during prediction. Based on these insights, we propose two methods: Information Compression-based Context Learning (IC-ICL) and Task-Space-guided Fine-Tuning (TS-FT).\nIC-ICL improves reasoning accuracy and accelerates inference by over 40%, while TS-FT enhances decompression capabilities through a simple loss function. Our experiments validate the effectiveness of these approaches, demonstrating significant performance improvements. These findings offer a deeper understanding of LLMs' information processing and provide practical solutions for enhancing model performance.\nFuture work will explore refining these methods and applying them to other NLP tasks, further enhancing the efficiency and accuracy of LLMs."}, {"title": "Ethical Considerations", "content": "Regarding the potential ethical impacts of our work: (1) The dataset we use is EMPATHETIC-DIALOGUE, which is open source and does not involve any potential ethical risks. (2) The baseline models we use are also public and do not have potential moral impacts. Moreover, the components employed in our model are open-sourced or innovative and do not involve potential ethical risks."}]}