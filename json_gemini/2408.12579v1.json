{"title": "RuleAlign: Making Large Language Models Better Physicians with Diagnostic Rule Alignment", "authors": ["Xiaohan Wang", "Xiaoyan Yang", "Yuqi Zhu", "Yue Shen", "Jian Wang", "Peng Wei", "Lei Liang", "Jinjie Gu", "Huajun Chen", "Ningyu Zhang"], "abstract": "Large Language Models (LLMs) like GPT-4, MedPaLM-2, and Med-Gemini achieve performance competitively with human experts across various medical benchmarks. However, they still face challenges in making professional diagnoses akin to physicians, particularly in efficiently gathering patient information and reasoning the final diagnosis. To this end, we introduce the RuleAlign framework, designed to align LLMs with specific diagnostic rules. We develop a medical dialogue dataset comprising rule-based communications between patients and physicians and design an alignment learning approach through preference learning. Experimental results demonstrate the effectiveness of the proposed approach. We hope that our work can serve as an inspiration for exploring the potential of LLMs as Al physicians.", "sections": [{"title": "1 Introduction", "content": "Medical diagnosis involves physicians obtaining patients' subjective symptoms through questioning, as well as objective examination results, to infer the most likely diseases or health issues (Zhang et al., 2022; He et al., 2023; G\u00f6nd\u00f6cs and D\u00f6rfler, 2024; Caruccio et al., 2024). Large Language Models (LLMs) can extend medical resources by leveraging medical corpora in pretraining to simulate physicians, providing initial screening services that alleviate the burden on physicians and accelerate the diagnostic process for patients (Singhal et al., 2023a; Peng et al., 2023; Wu et al., 2024; Zhang et al., 2023; Thirunavukarasu et al., 2023; Bao et al., 2023a). However, LLMs face a significant gap compared to real physicians (Zhou et al., 2023; Jiang et al., 2023; Tu et al., 2024), particularly in specialized inquiries for specialty diseases.\nThe paradigms of existing works are too coarse in disease categories and differ greatly from actual consultations (Chen et al., 2023b; Jin et al., 2023; Gao et al., 2023). One critical issue is logical consistency: LLMs may propose diagnostic hypotheses without adequate informational support or disregard previous replies, leading to unsatisfactory responses (Li\u00e9vin et al., 2024). Moreover, LLMs face significant challenges in adhering to rules (Mesk\u00f3 and Topol, 2023; Lu et al., 2024; Luo et al., 2024). Note that medical practice is governed by stringent professional guidelines and standards as shown in Figure 1, yet LLMs often demonstrate limited familiarity with regulations, resulting in informational shortfalls. Furthermore, the lack of specialized knowledge is evident as LLMs struggle to accurately comprehend and utilize professional terminology, rendering them incapable of providing convincing diagnoses and actionable advice.\nTo this end, we introduce the RuleAlign framework to make LLMs better physicians with diagnostic rule alignment. Specifically, we collect the necessary diagnostic rules within the field of urology and provide a rule-based dialogue dataset UrologyRD. It is designed to guide the behavior of LLMs, ensuring that their responses align with established protocols. We train the model by constructing optimized preference pairs without additional human-annotation resource, which can achieve improvements in both single-round evaluation and multi-round Standard Patient testing (Liu et al., 2024)."}, {"title": "2 Related Works", "content": "Medical LLMs. Medical LLMs hold significant application value, with both academic and industrial sectors actively advancing their development. By introducing medical data into general LLMs, supervised finetuning (SFT) becomes more common, such as MedPaLM-2 (Singhal et al., 2023b), and Med-Gemini (Saab et al., 2024). It also leads to the emergence of many Chinese medical LLMs, including DoctorGLM (Xiong et al., 2023), HuatuoGPT-II (Chen et al., 2023a), and Zhongjing (Yang et al., 2024). BianQue (Chen et al., 2023b) utilize BianQueCorpus dialogue dataset optimized and constructed via ChatGPT, balancing the capability of asking questions and providing health advice. The characteristic of DISC-MedLLM (Bao et al., 2023b) lies in enhancing conversational ability by incorporating knowledge graph question-answer pairs and human preference guided conversations into instruction data. PLPF (Dou et al., 2024) involves rule modeling and preference learning to integrate the doctor's diagnostic logic into LLMs.\nModel Alignment. Human feedback (Ouyang et al., 2022) is widely applied to enhance the performance of LLMs through optimized preference learning. RLHF (Christiano et al., 2017) employs a reward model trained via the Bradley-Terry (BT) model to suggest optimized outputs that maximize rewards through RL algorithms like PPO (Schulman et al., 2017). SLiC (Zhao et al., 2023) ranks model-generated preferences by combining losses, while RRHF (Yuan et al., 2023) assumes multiple ranked responses for zero-margin likelihood contrastive loss; both methods rely on explicit reward models. The propose of DPO (Rafailov et al., 2023) provides a theoretical and technical basis for alignment methods free from the dependence on reward models. DPO directly uses the BT model to fit the preferences. RSO (Liu et al., 2023) combines the benefit of SLiC and DPO with data augmentation through statistical rejection sampling. SPIN (Chen et al., 2024) enhances DPO using self-iterative training with SFT data, treating synthetic data as rejection responses and high-quality SFT data as selection responses, iterating until the model can no longer distinguish between them."}, {"title": "3 Dataset Construction", "content": "To meet the needs of real patient clinical scenarios, we introduce more detailed diagnostic rules during the dataset construction. By summarizing relevant conversations and extracting key rules from standardized diagnostic guidelines, we specify the physician's diagnostic rules at the level of target diseases and strongly associated evidence as shown in Figure 2. To further focus our research and better address specialized issues, we take urology as an example and construct rules for its common target diseases, that is, each disease \\(d_i\\) is equipped with its corresponding diagnostic rules \\(r_i\\).\nDiagnostic rules \\(r_i\\) can be divided into two aspects: constraints within the diagnosis logic and the search for important evidence. The former at the logical layer primarily proceed according to the trajectory reformulated as \\(T_i = (S_i \\rightarrow C_i \\rightarrow h_i \\rightarrow d_i)\\). Here, the trajectory components are defined as follows: the patient's subjective symptom description \\(s_i\\), the collection of patient's objective examination results \\(e_i\\), the inquiry into the patient's personal medical history \\(h_i\\), and the final diagnostic opinion. Patients often first report pain or discomfort; strategically following this trajectory can effectively collect more comprehensive medical information.\nAs for key patient evidence, patients frequently fail to identify which information is relevant for an accurate diagnosis, necessitating detailed inquiries by doctors. Within the symptom set \\(S = \\{S_a, S_b, S_c, ...\\}\\), based on diagnostic knowledge, symptoms \\(S_a\\) and \\(S_b\\) are often associated with the target disease, making them key diagnostic indicators. For example, fever and difficulty urinating are important symptom evidences for bladder cancer. Objective examination results \\(E = \\{E_a, E_b, E_c, ...\\}\\), comprising physical exams, laboratory tests, and imaging studies, are essential for their reliability and minimal bias, thereby helps confirm the validity of a diagnosis. Professional doctors select and categorize examination evidence based on importance and provide a corresponding order \\(ER\\) based on practical experience. Personal medical history \\(H\\) can be categorized into medication, surgical, past medical, and reproductive histories; selective questioning based on necessity."}, {"title": "3.2 Dataset Generation", "content": "Data Collection. Open-sourced medical dialogue datasets (Zeng et al., 2020) frequently lack structured examination results, leading to incomplete diagnostic trajectory. Due to the overlap between disease categories and our specific targets, we choose the RJUA-QA (Lyu et al., 2023) as initial data. After filtering the categories, we convert single-turn question q and corresponding disease d into dialogues M, encompassing the trajectory from initial symptoms to physician diagnoses, represented as \\(M = G_M(q, d)\\), where G denotes the GPT-4 turbo API.\nDisease Name Mapping. In data construction, it is imperative to map disease names meticulously. This involves distilling intricate pathology descriptions found in clinical records into broader medical categories (e.g., \"right renal hydronephrosis\" to \"renal hydronephrosis\"). Details like which side and progression stage while critical for clinical investigations, wield limited influence on query and risk complicating study designs unduly. Consequently, we map disease terms to defined categories, ensuring robust data compilation and precise alignment with established diagnostic rules.\nDiagnostic Rule Adaption. By applying rules r into the prompts for LLMs, we transform the original dialogue M into a rule-based dialogue \\(M_r\\). This adaptation process can be formulated as:\n\\(M_r = G_p(M_i, r_i) = G_p(G_M(q_i, d_i), T(T_i, K_i))\\)\nwhere T represents the template of rules and p encapsulates the constraints necessary to ensure the patient's honesty and clarity of intent. Through this approach, the synthesized dialogues not only conform more closely to the prescribed diagnostic rules but also maintain consistency and reliability."}, {"title": "3.3 Quality Control", "content": "To ensure precision and integrity, UrologyRD undergoes rigorous annotation by professional doctors. In our process, urology specialists review all diagnostic rules \\(r_i\\). Based on these validated rules, the pass rate for a hundred samples in UrologyRD approaches 70%, with experts deeming these dialogues to be comprehensive and logical."}, {"title": "4 The Proposed Approach: RuleAlign", "content": "Preference learning (Jiang et al., 2024), which leverages human-labeled preference pairs, is considered an effective method for aligning LLMs with human objectives. DPO stands as a prominent approach in offline preference optimization (Rafailov et al., 2023), notable for eliminating the need for an explicit reward model. High-quality preference pairs are essential for DPO but typically demand substantial manual annotation resources."}, {"title": "4.2 Preference Learning", "content": "The UrologyRD dataset D encompasses both precise inquiry information and complex diagnostic rules, forming a comprehensive resource for disease diagnosis. The progression from pre-trained models to aligned LLMs using the DPO method generally involves two distinct phases: training the policy model through supervised fine-tuning, and conducting preference optimization.\nIn SFT phase, the language model is finetuned with high-quality instruction data D closely aligned with the objectives of the downstream task. This includes instructions from users' inputs x accompanied by suitable responses y. The LLM \\(\\pi_\\theta\\) generates a probability distribution \\(\\pi_\\theta(x, y)\\), optimized by minimizing the negative log-likelihood.\n\\[L_{SFT}(\\pi_\\theta) = \\mathbb{E}_{(x,y)\\sim D} log \\pi_\\theta(y|x)\\]\nIn the second phase, the SFT model serves as the reference policy \\(\\pi_{ref}\\), while the policy model \\(\\pi_\\theta\\) acts as the trainable parameters for preference learning. Unlike the prior RLHF methods, DPO calculates the optimization strategy's reward using a closed-form expression, implicitly defined by the models \\(\\pi_\\theta\\) and \\(\\pi_{ref}\\), denoted as\n\\[r_\\theta(x, y) = \\beta log \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)}\\]\nHere, \\(\\beta\\) signifies a hyper-parameter controlling the scale of the reward difference. DPO directly simplifies the learning of the optimal strategy using preference pairs \\(D_P\\), aiming to increase the likelihood of preferred completions \\(y_w\\) and decrease the likelihood of dispreferred completions \\(y_l\\), as\n\\[L_{DPO}(\\pi_\\theta; \\pi_{ref}) = \\mathbb{E}_{(x, y_w, y_l)\\sim D_P} [log \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l))]\\]"}, {"title": "4.3 Preference Pair Optimization", "content": "We propose an efficient method, RuleAlign, which is designed to automatically generate and optimize preference data for alignment (in Figure 3). Building on previous research, we utilize dialogues synthesized using diagnostic rules to represent positive preferences, while responses after the SFT stage are designated as dispreferred for DPO. When the LLM output \\(y_l\\) closely matches \\(y_w\\), the loss approaches zero. Conversely, a greater disparity results in increased loss and larger gradient updates, thereby enhancing the learning of preferences.\nHowever, the experiment reveals that original preference pairs do not consistently improve the performance of LLMs. In fact, models refined using DPO can occasionally perform worse than those solely optimized through SFT. This exploration underscores the critical need to match the dispreferred completions with these less satisfactory instances for efficient learning. Consequently, we propose an approach to enhance dispreferred completions \\(y_l\\) through sample filtering using semantic similarity and the deliberate disruption of dialogue order. Specifically, the former entails multiple sampling outputs after SFT and selecting the item with the lowest similarity to \\(y_w\\) by BLEU score and lower than the threshold. Additionally, the dispreferred completions are constructed by repeating existing responses or using the subsequent response to disrupt the diagnostic logic."}, {"title": "5 Experiments", "content": "Evaluation Scenarios. When examining LLMs as physicians, the standardization of patient responses presents a challenge due to the inherently history-dependent nature. Therefore, it is recommended that model evaluations be conducted separately in both single-round and multi-round settings to more effectively assess their competencies. In single-round test, the test set is selected from previously constructed dataset, using target dialogue rounds as input/output pairs with prior rounds as history. In multi-round context, we find that Standardized Patient (SP) Testing is a requisite and commonly employed methods for assessing clinical dialogue. We follow the framework proposed by Liu et al. (2024) covering diseases we involve.\nBaselines. Three Chinese LLMs are chosen as base models. Baichuan2-7B-chat (Baichuan, 2023) and Qwen1.5-7B-chat (Bai et al., 2023) represent widely accessible open-source LLMs, exemplifying distinct architectures. While Huatuo-II-7B (Chen et al., 2023a) as a medical LLM is predicated upon the foundations of Baichuan2-7B-Base, augmented with a corpus of medical SFT data.\nMetrics. In single-round test, several metrics perplexity, Rouge and BLEU series, and Length Rate serve as key evaluative tools. Perplexity assesses the likelihood of LLMs generating a specific sequence, with lower values suggesting better training. ROUGE and BLEU metrics evaluate the semantic and syntactic similarity of generated text with ground-truth, where higher values reflect greater congruence. Lastly, Length Rate measures the proportional length of generated text to ground-truth, near 1.0 indicating closer alignment.\nIn SP testing, we adopt metrics in the framework, encompassing five critical dimensions of diagnosis: Information Completeness, Guidance Rationality, Diagnostic Logicality, Clinical Applicability and Treatment Logicality. Collectively, these metrics assess the effectiveness of LLMs from the initial gathering of patient information to the logical reasoning behind diagnostic opinions and the practical execution of reaching a conclusive diagnosis, ensuring a comprehensive evaluation in simulating reasoning and decision-making processes."}, {"title": "5.2 Main Results", "content": "We report the main results of the experiments in Table 2, highlighting the superior performance of RuleAlign. Applying RuleAlign to three distinct models demonstrates its effectiveness in aligning LLMs with medical diagnosis task, resulting in more accurate and natural responses. RuleAlign reduces the perplexity to a range of 3 to 4, while also improving the ROUGE-1 by 20-30 and increasing the BLEU to nearly 20. By leveraging the instructions, LLMs are capable of fitting rules; however, there are still certain constraints in the model post-SFT. On the one hand, there exists hallucinations after SFT, and in the case of Qwen, it continues to output irrelevant content after necessary outputs, resulting in longer responses and lower scores. Employing DPO with direct SFT outputs and targets can alleviate such hallucinations. Thus, the performance of DPO is superior to that of SFT in Qwen. On the other hand, DPO may underperform SFT, since the disparities have the potential to confuse the LLMs in alignment. When it comes to Baichuan and Huatuo, there are cases of poor performance in terms of specific logic following where the order of inquiry is either repeated or skipped. It is noted that RuleAlign, optimizing dispreferred completions by semantic similarity filtration and order disruption, could improve the quality of generated text in metrics such as Rouge and BLEU, as well as Length Rate, despite a minor shortfall in perplexity. What's more, the comparison between Huatuo and Baichuan demonstrates that normal medical corpus show little affect on diagnoses or further alignment on diagnostic rules."}, {"title": "5.3 Analysis", "content": "As displayed in Table 3 and Figure 4, we conduct further analysis on the primary components of preference pairs that affect on model's ability to predict the next round of inquiry. To verify the validity of strategies in preference pair optimization, we compare RuleAlign with only applying semantic similarity filtration or order disruption to build the dispreferred completions. The ablation study reveals that adding both strategies of RuleAlign could bring about better performance in metrics Rouge and BLEU. Also, it explains that the unsatisfied rise of the perplexity score in RuleAlign is attributed to the introduction of order disruption. In addition, the scale of preference pairs, depicted in Figure 4, shows that a quarter of the preference pairs are sufficient to train the SFT model and learn about the preference. Increasing the number of preference pairs on this basis does not bring significant improvement to the effect. Thus, for the rule alignment of medical diagnosis, the strategy of designing preference pairs has a more important impact on the accuracy of single-round prediction."}, {"title": "5.4 Standardized Patient (SP) Testing", "content": "This sophisticated multi-agent system is explicitly designed to emulate the dynamic dialogues between patients and the physician agent. This incorporates Retrieval-Augmented Evaluation (RAE) mechanism that predicts patient responses based on the repository that archives a vast array of symptoms and test outcomes, thereby providing a realistic simulation of patient behavior.\nSP Results. Figure 5 illustrates RuleAlign's multidimensional abilities to communicate with patients that mimic real-world diagnoses and rank the scores among different methods. The increase of information completeness mirrors the advancements of LLMs in collecting sufficient critical patient information during conversations. Especially, utilizing evidence knowledge in diagnostic rules, RuleAlign instructs LLMs to query about more targeted test and examination reports which brings about the improvement of guidance rationality. Additionally, the rise in diagnostic logicality measures how the model logically deduces accurate diagnostic conclusions in accordance with the accessible information. RuleAlign also reaches over 5 turns dialogues in average in metric clinical applicability. But in the absence of treatment knowledge enhancement, all four approaches are tied in terms of metric treatment logicality. Meanwhile, RuleAlign still exists a great disparity in delivering final accurate diagnoses and treatment suggestions comparable to those provided by real-world physicians. We illustrate a case of SP testing in Figure 7 to demonstrate the effectiveness of RuleAlign."}, {"title": "6 Conclusion", "content": "In this study, we build the medical dialogue dataset UrologyRD based on the diagnostic rules and propose the innovative method RuleAlign to automatically synthesize the preference pairs for alignment. The experiment results show the effectiveness of RuleAlign on various evaluation settings. And we anticipate our work will bring benefits to further research on LLMs as medical applications or AI physicians."}, {"title": "Ethical Considerations", "content": "This work adheres strictly to ethical standards and best practices in research. All medical data utilized are extracted from publicly available datasets that do not contain any sensitive or proprietary patient information. Consequently, this research is conducted without any ethical concerns."}]}