{"title": "CBRAMOD: A CRISS-CROSS BRAIN\nFOUNDATION MODEL FOR EEG DECODING", "authors": ["Jiquan Wang", "Sha Zhao", "Zhiling Luo", "Yangxuan Zhou", "Haiteng Jiang", "Shijian Li", "Tao Li", "Gang Pan"], "abstract": "Electroencephalography (EEG) is a non-invasive technique to measure and record\nbrain electrical activity, widely used in various BCI and healthcare applications.\nEarly EEG decoding methods rely on supervised learning, limited by specific\ntasks and datasets, hindering model performance and generalizability. With the\nsuccess of large language models, there is a growing body of studies focusing on\nEEG foundation models. However, these studies still leave challenges: Firstly,\nmost of existing EEG foundation models employ full EEG modeling strategy. It\nmodels the spatial and temporal dependencies between all EEG patches together,\nbut ignores that the spatial and temporal dependencies are heterogeneous due\nto the unique structural characteristics of EEG signals. Secondly, existing EEG\nfoundation models have limited generalizability on a wide range of downstream\nBCI tasks due to varying formats of EEG data, making it challenging to adapt to.\nTo address these challenges, we propose a novel foundation model called CBraMod.\nSpecifically, we devise a criss-cross transformer as the backbone to thoroughly\nleverage the structural characteristics of EEG signals, which can model spatial\nand temporal dependencies separately through two parallel attention mechanisms.\nAnd we utilize an asymmetric conditional positional encoding scheme which\ncan encode positional information of EEG patches and be easily adapted to the\nEEG with diverse formats. CBraMod is pre-trained on a very large corpus of\nEEG through patch-based masked EEG reconstruction. We evaluate CBraMod\non up to 10 downstream BCI tasks (12 public datasets). CBraMod achieves the\nstate-of-the-art performance across the wide range of tasks, proving its strong\ncapability and generalizability. The source code is publicly available at https:\n//github.com/wjq-learning/CBraMod.", "sections": [{"title": "1 INTRODUCTION", "content": "Brain-Computer Interface (BCI) refers to a system that allows direct communication between the\nbrain and external devices or computers. Electroencephalography (EEG) is a technique used to\nmeasure and record electrical activity in the brain, where electrodes are placed on the scalp to detect\nand amplify the brain's electrical signals. EEG plays a crucial role in BCI as it provides a non-invasive\nand real-time measure of brain activity that can be used to decode and interpret user's intentions or\ncommands. By analyzing the patterns and features in EEG signals, algorithms and models have been\ndeveloped to decode specific brain states, including but not limited to emotion recognition (Dadebayev"}, {"title": "2 METHOD", "content": "The pre-training framework of CBraMod is shown in Figure 2. Firstly, we segment EEG samples\ninto patches using a fixed time window and randomly mask some patches with a mask token. Next,\neach EEG patch is fed into a patch encoding network to obtain corresponding patch embedding.\nSpatial-temporal positional embeddings are obtained through an asymmetric conditional positional\nencoding (ACPE) scheme and added to the patch embeddings. Then, the patch embeddings are fed\ninto criss-cross transformer blocks with criss-cross attention mechanism to learn EEG representations.\nFinally, a reconstruction head is utilized to reconstruct the masked EEG patches from the learned\nrepresentations. We will provide a detailed description of CBraMod."}, {"title": "Patching & Masking", "content": "We denote an EEG sample as $S \\in \\mathbb{R}^{C \\times T}$, where $C$ is the number of electrode\nchannels and $T$ is the number of timestamps. In order to ensure that the neural network can deal"}, {"title": "Time-Frequency Patch Encoding", "content": "In order to extract the local features from each patch $x_{i,j}$ of\n$\\mathcal{X}$, we devise a patch encoder applied to each patch. Our patch encoder consists of two branches,\ntime-domain branch and frequency-domain branch. The time-domain branch comprises multiple\nconvolution blocks designed to extract time-domain features within each patch. A convolution block\nconsists of an one-dimensional convolution layer, a group normalization layer, and a GELU activation\nfunction. We feed each EEG patch $x_{i,j}$ into the time-domain branch to obtain the time-domain\nembedding $e_{i,j}^t \\in \\mathbb{R}^d$, where $d$ is the dimension of the embedding. The frequency-domain branch\ncomprises fast Fourier transform (FFT) and a fully-connected layer to extract frequency-domain\nfeatures from each patch. Specifically, we leverage FFT to extract an energy vector for each patch\n$x_{i,j}$ where every dimension indicates the energy of a specific frequency. Then we feed the energy\nvector into a fully-connected layer to get frequency-domain embedding $e_{i,j}^f \\in \\mathbb{R}^d$. Afterwards, we\nadd each time-domain embedding and the corresponding frequency-domain embedding as follows:"}, {"title": "Asymmetric Conditional Positional Encoding", "content": "Differing from existing EEG foundation models\nthat primarily utilize an absolute positional encoding (APE) scheme, we employ an asymmetric\nconditional positional encoding (ACPE) scheme to encode spatial and temporal positional information\nof EEG patches. This is a modification upon conditional positional encoding (CPE) (Chu et al., 2021)\nthat is applied to encode positional information of image patches. Compared to APE, ACPE can\ndynamically encode the temporal and spatial positional information of each EEG patch, improving\nadaptation to diverse channel configurations and time lengths. Compared to CPE, ACPE is designed\nin an asymmetric fashion to encode short-range temporal positional information and long-range\nspatial positional information. It is because EEG signals are multi-channels sequential neural\nsignals, requiring the model to encode different-range positional information in spatial and temporal\ndimension."}, {"title": "Criss-Cross Transformer", "content": "We propose criss-cross transformer to capture the heterogeneous spatial\nand temporal dependencies among EEG patches, whose architecture is shown in Figure 3."}, {"title": "Criss-Cross Transformer Block.", "content": "As shown in Figure 3(a), a criss-cross transformer block consists\nof layer normalization layer, criss-cross attention, addition component, and feed forward layer. In\norder to ensure the stability and efficiency of the transformer training process, we utilize the pre-norm\nstrategy. We incorporate layer normalization to the queries and keys prior to the attention mechanism\nto prevent the occurrence of excessively large values in attention logits. This approach helps to\nmaintain more consistent gradients across layers, resulting in better convergence during the training\nprocess (Xiong et al., 2020; Dehghani et al., 2023). Here, we feed EEG patch embeddings $E^o$ into\nlayer normalization to get normalized patch embedding $\\tilde{E} \\in \\mathbb{R}^{C \\times n \\times d}$. Then we feed $\\tilde{E}$ into the\ncriss-cross attention to capture spatial-temporal dependencies among EEG patches."}, {"title": "Criss-Cross Attention Mechanism.", "content": "The criss-cross attention is shown in Figure 3(b). It consists of\nparallel spatial attention (S-Attention) and temporal attention (T-Attention). The S-Attention\nis used to capture spatial dependencies among EEG patches within the same time interval and the\nT-Attention is used to capture temporal dependencies among EEG patches within the same channel.\nBased on the multi-head self-attention mechanism, the input embeddings $\\tilde{E} \\in \\mathbb{R}^{C \\times n \\times d}$ will undergo\nan initial linear projection to $K$ heads, and each head will subsequently apply either S-Attention or\nT-Attention. For S-Attention, we can partition $\\tilde{E}$ into $n$ spatial stripes as $\\tilde{E} = [\\tilde{E}_1, \\tilde{E}_2, ...\\tilde{E}_n]$, where\n$\\tilde{E}_1,..., \\tilde{E}_n \\in \\mathbb{R}^{C \\times d}$. We suppose the projected queries, keys and values of the $k$-th head all have\ndimension $d_k$, then the process of S-Attention for the $k$-th head is as follows:"}, {"title": "Masked EEG Reconstruction", "content": "In order to learn powerful generic representations from unlabeled\nEEG data, we need to utilize the task of self-supervised representation learning to pre-train CBraMod.\nMasked autoencoder (MAE) has been proved to be a simple and effective self-supervised pre-\ntraining approach in the fields of natural language understanding, computer vision, and time series\nforecasting (Devlin et al., 2018; He et al., 2022; Nie et al., 2022). We utilize patch-based masked\nEEG reconstruction to learn generic representation from EEG. Specifically, a reconstruction head,\ncomposed of a fully-connected layer, is used to project learned EEG representations $\\mathcal{E}^r$ into predicted\nEEG patches $\\hat{X} = \\{\\hat{x}_{i,j}|i \\in [1,2, ..., C], j \\in [1, 2, ..., n]\\}$, where $\\hat{x}_{i,j} \\in \\mathbb{R}^t$ is the predicted EEG\npatch corresponding to one original EEG patch $x_{i,j}$, and $\\hat{X} \\in \\mathbb{R}^{C \\times n \\times t}$ is the set of predicted EEG\npatches corresponding to the set of original EEG patches $\\mathcal{X}$ .\nConsistent with most of existing MAE studies(He et al., 2022; Xie et al., 2022), we only reconstruct\nthe masked EEG patches. Given the mentioned mask $\\mathcal{M} = \\{m_{i,j}|i \\in [1, 2, ..., C], j \\in [1, 2, ..., n]\\}$,\nwe get the set of the masked predicted EEG patches $\\hat{\\mathcal{X}}^M$ and the set of the masked original EEG\npatches $\\mathcal{X}^M$ as follows:"}, {"title": "3 EXPERIMENTS", "content": "C = ||\u00c2M \u2013 XM ||2"}, {"title": "3.1 PRE-TRAINING", "content": "Pre-training Dataset CBraMod is pre-trained on a very large public dataset, Temple University\nHospital EEG corpus (TUEG) (Obeid & Picone, 2016). The TUEG dataset consists of a diverse\narchive of 69,652 clinical EEG recordings from 14,987 subjects across 26,846 sessions, with a total\nduration of 27,062 hours. The archive has over 40 different channel configurations and varying\nduration of recordings. Most of the recordings are sampled at 256 Hz. Unfortunately, the TUEG\ndataset suffers from significant data contamination, including a substantial amount of unmarked noise,\nartifacts, and faulty channels. Manual removal of these interference factors presents considerable\nchallenges. Thus we utilize a range of automated techniques to preprocess the data."}, {"title": "Pre-training Settings", "content": "We implemented CBraMod based on the Python 3.11.7 and PyTorch 2.1.2 +\nCUDA 12.1. We set the time duration of each EEG patch as 1 second (200 data points), and one\n30-second EEG sample is segmented to 19 \u00d7 30 = 570 EEG patches. For the model configurations,\nthe patch encoder consists of 3-layer 1D convolution with group normalization and GELU activation.\nThe positional encoder is an one-layer 2D depthwise CNN. The backbone of CBraMod is a 12-layer"}, {"title": "3.2 EXPERIMENT SETUP OF DOWNSTREAM BCI TASKS", "content": "Downstream BCI Tasks and Datasets To comprehensively evaluate the performance of our\nmethod, we select up to 10 downstream BCI tasks as follows: I. Emotion Recognition refers to\ndetecting and interpreting emotional states or patterns based on EEG. We choose FACED (Chen\net al., 2023) and SEED-V (Liu et al., 2021b) as downstream datasets for this task. II. Motor\nImagery Classification is a process that involves decoding or classifying motor imagery tasks\nbased on brain activity patterns captured through EEG. We use PhysioNet-MI (Schalk et al., 2004)\nand SHU-MI (Goldberger et al., 2000b) to evaluate the performance of CBraMod on this task.\nIII. Sleep Staging, also known as sleep stage classification, categorizes different stages of sleep\nbased on physiological signals (e.g. EEG) recorded during sleep. ISRUC (Khalighi et al., 2016)\nis a commonly used benchmark for sleep staging. IV. Seizure Detection refers to the process of\nidentifying and recognizing epileptic seizures based on physiological signals, such as EEG. Here,\nCHB-MIT (Shoeb, 2009) is choosed for this task. V. Imagined Speech Classification involves the\nidentification and interpretation of imagined speech or covert speech intentions based on brain activity\npatterns such as EEG. BCIC2020-3 (Jeong et al., 2022) is a dataset for this task on 2020 international\nbrain-computer interface competition. VI. Mental Disorder Diagnosis refers to the process of\nidentifying and categorizing mental health states based on EEG. Mumtaz2016 (Mumtaz, 2016) is\nan EEG dataset collected from patients with major depressive disorder and normal controls. VII.\nVigilance Estimation refers to the assessment and measurement of an individual's level of vigilance\nor sustained attention from EEG. SEED-VIG (Min et al., 2017) is dataset used to assess the level of\nvigilance. VIII. Mental Stress Detection involves using EEG to identify an individual's level of\nstress. MentalArithmetic (Zyma et al., 2019) is a dataset containing EEG recordings of subjects before\nand during the performance of mental arithmetic tasks. IX. EEG Events Classification involves the\ncategorization and identification of specific events or patterns in EEG. We choose TUEV (Obeid &\nPicone, 2016) for evaluation, which was consistent with BIOT (Yang et al., 2023) and LaBraM (Jiang\net al., 2024). X. Abnormal Detection refers to the process of identifying and detecting abnormal\npatterns or events in EEG. Similarly, TUAB (Obeid & Picone, 2016) is used for evaluation, consistent\nwith BIOT (Yang et al., 2023) and LaBraM (Jiang et al., 2024). The corresponding datasets of all\ndownstream BCI tasks are shown in Table 1. For all the downstream datasets, we resampled the\nEEG signals into 200 Hz and set the time duration of each EEG patch as 1 second (200 data\npoints), consistent with the pre-training data. More details of each dataset and its preprocessing are\nintroduced in Section 3.3 and Appendix E."}, {"title": "Baselines", "content": "We compare CBraMod with both non-foundation-model and foundation-model baseline\non all the downstream BCI tasks, for comprehensive evaluation. We adopt the following methods\nas non-foundation-model baselines: EEGNet (Lawhern et al., 2018), EEGConformer (Song et al.,"}, {"title": "3.3 RESULTS", "content": "For proving the capability and generalizability of CBraMod, we evaluate CBraMod and baselines\non up to 10 downstream BCI tasks using 12 publicly available datasets, listed in Table 1. In all the\nexperiments, we ensure strict consistency in the splits of training, validation, and test sets for\nevery method. In this section, we show the experimental results on downstream BCI tasks of emotion\nrecognition and motor imagery classification on 4 datasets. More results are shown in Appendix E."}, {"title": "Attention Mechanism Comparison", "content": "In order to prove the effectiveness of criss-cross attention\nmechanism, we compare it with other attention mechanism such as full attention (Vaswani et al.,\n2017; Dosovitskiy et al., 2020), axial attention (Ho et al., 2019) and the criss-cross attention module\nin CCNet (Huang et al., 2019). Full attention mechanism equally models the spatial-temporal\ndependencies among all EEG patches in one transformer block. Axial attention mechanism models\nthe dependencies along specific axial dimensions of EEG patches in one transformer block. It can\nsequentially model spatial or temporal dependencies among EEG patches in different blocks. In our\nexperiment, the axial attention only models the spatial dependencies in transformer block 1\u20136 and\nmodels the temporal dependencies in transformer block 7\u201312. The criss-cross attention mechanism in\nCCNet is based on the affinity operation, using a single attention map to achieve criss-cross modeling.\nWe pre-trained and fine-tuned the models based on the three attention mechanisms with the same\nsettings as CBraMod. The performance comparison on emotion recognition and motor imagery\nclassification is shown in Figure 4. Full attention achieves the worst performance across all datasets.\nIt indicates that full attention equally models the dependencies among all EEG patches, ignoring the\nunique structural characteristics of EEG signals. Meanwhile, there are more than 570 EEG patches in\none EEG sample (19 channels, 30 seconds) in the pre-training dataset, which exceeds the appropriate\nmodeling range for full attention. Axial attention outperforms full attention because it prioritizes\ndirectly modeling specific axial dimensions of EEG patches, which can effectively capture the spatial-"}, {"title": "Positional Encoding Comparison", "content": "We conduct the positional encoding comparison to evaluate the\neffectiveness of asymmetric conditional positional encoding (ACPE). Specifically, we compare ACPE\nwith following settings: 1) without positional encoding (w/o PE): do not use positional encoding to\nencode postional information; 2) with absolute positional encoding (w/ APE): replace ACPE with\nAPE; 3) with conditional positional encoding (w/ CPE): replace ACPE with CPE. The results are\nshown in Figure 5. CBraMod without PE performs the worst, indicating that positional encoding\nis important in EEG modeling. APE performs better than w/o PE but worse than CPE and ACPE,\nindicating that APE is not so effective as CPE in adapting to different EEG formats. CPE, usually\nused for image patches, performs slightly worse compared to ACPE in CBraMod, indicating that the\nasymmetric designs are slightly better than symmetric designs in EEG modeling. It may be because\nthat EEG patches exhibit different dependencies in spatial (channel) and temporal dimension, which\nare different from the dependencies between image patches."}, {"title": "Ablation Study on Pre-training", "content": "To further evaluate the effectiveness of pre-training, we conduct\nan ablation study on pre-training. The pre-training ablation experiment is designed as follows: 1) w/o\npre-training: directly training CBraMod on downstream datasets; 2) dirty pre-training: pre-training\nCBraMod on TUEG corpus without bad samples dropping. 3) clean pre-training: pre-training\nCBraMod on TUEG corpus with bad samples dropping. The results are presented in Table 4.\nObviously, clean pre-training achieves the best performance, obtaining significant performance\nincreases compared to dirty pre-training and w/o pre-training. Besides, the performance has a\nsmaller variance compared to dirty pre-training and w/o pre-training. These indicate that our pre-\ntraining strategy can help CBraMod learn generic representations from EEG, which can improve\nthe generalizability and stability of CBraMod on downstream datasets. Dirty pre-training performs\nslightly better than w/o pre-training, indicating that a large amount of dirty data in the original dataset\ncan weaken the effectiveness of pre-training."}, {"title": "4 CONCLUSION", "content": "In this paper, we propose an EEG foundation model called CBraMod, which can learn generic\nrepresentations of EEG signals through patch-based masked EEG reconstruction. Specifically, we\ndevise a criss-cross transformer as the backbone of CBraMod to model the spatial and temporal\ndependencies between EEG patches in parallel, and an asymmetric convolutional positional encoding\nscheme to encode spatial-temporal positional information of EEG signals with diverse formats.\nCBraMod is pre-trained on TUEG, a very large corpus of EEG. CBraMod achieves the state-of-\nthe-art performance across up to 10 downstream BCI tasks (12 public datasets), proving its strong\ncapability and generalizability. We hope that the proposed modeling approach and positional encoding\nscheme can provide meaningful insights for building EEG foundation models, thereby advancing the\ndevelopment of real-world BCI systems."}, {"title": "A RELATED WORK", "content": "EEG Decoding. EEG is a non-invasive technique to measure brain activity. Early studies on EEG\ndecoding predominantly employed traditional machine learning methods (Bashashati et al., 2007;\nMcFarland et al., 2006; Lotte et al., 2007), which usually depend on hand-crafted features that\nrequire lots of prior knowledge and could have weak generalizability. With the development of deep\nlearning (LeCun et al., 2015) techniques, an increasing number of researchers are shifting their focus\nto studying EEG decoding methods based on deep learning (Parvaneh et al., 2019; Craik et al., 2019;\nAl-Saegh et al., 2021; Sekkal et al., 2022; Yang et al., 2022). For example, some studies utilize\nconvolutional neural network (CNN) to extract temporal and spatial features from EEG for different\nBCI tasks like motor imagery classification, emotion recognition and seizure detection (Schirrmeister\net al., 2017; Sakhavi et al., 2018; Lawhern et al., 2018; Abdelhameed & Bayoumi, 2021; Ding\net al., 2022). Long Short-Term Memory (LSTM) is also used for EEG feature extraction and\nclassification on BCI tasks such as motor imagery classification and sleep staging (Wang et al.,\n2018; Phan et al., 2019). Some researchers propose CNN-LSTM to learn EEG features for motor\nimagery classification, emotion recognition and sleep staging (Supratak et al., 2017; Zhang et al.,\n2019; Dar et al., 2020; Li et al., 2022; Wang et al., 2023a), where CNN is usually used to extract\nlocal features and LSTM is utilized to capture global dependencies. Transformer architecture is also\nutilized to learn spatial-temporal feature for BCI tasks including emotion recognition, sleep staging\nand person identification (Song et al., 2021; Liu et al., 2021a; Du et al., 2022; Phan et al., 2022). To\ncombine the strengths of CNN and transformer, some works devise a CNN-Transformer network\nfor automated artifact detection and other tasks (Song et al., 2022; Peh et al., 2022; Wang et al.,\n2023b; Zhou et al., 2024). In addition, some studies use graph neural networks learn spatial-temporal\nfeatures from multi-channel EEG for various BCI tasks (Jia et al., 2020; 2021; Ding et al., 2023).\nThese methods perform well on specific tasks or datasets, but collecting labeled EEG data is costly\nand labor-intensive. Meantime, the variations in EEG signal formats across different datasets pose\nchallenges for enhancing model performance and generalizability."}, {"title": "B MORE DETAILS FOR EXPERIMENTAL SETTINGS ON PRE-TRAINING", "content": "Here, we introduce more details for experimental settings on CBraMod pre-training.\nFor preprocessing of pre-training dataset, as CBraMod adopts criss-cross EEG modeling which can\naddress EEG signals with long time duration well, we segment all EEG recordings to 30-second\nEEG samples, which is longer than the time duration of pre-training samples on existing studies\nsuch as BIOT (Yang et al., 2023) (10 seconds) and LaBraM (Jiang et al., 2024) (4 or 8 seconds).\nWe choose this time duration for two main reasons: 1) longer segments may help the model learn\nmore long-term dependencies, potentially improving performance on downstream tasks, as noted in\nBENDER (Kostas et al., 2021); 2) 30 seconds generally covers the length of EEG sample segments\nfor all the downstream tasks in this work. More hyperparameters are listed in Table 5."}, {"title": "C PRE-TRAINING VISUALIZATION", "content": "The pretraining loss curve of CBraMod is presented in Figure 6. From the curve, it is evident that\nduring the 40-epoch pretraining process, the loss function generally follows a downward trend,\nwith minor fluctuations observed between epochs 10 and 14. The overall trend suggests that our\nmodel is effectively learning from the pretraining data, leading to the extraction of reliable EEG\nrepresentations."}, {"title": "D MORE DETAILS FOR EXPERIMENTAL SETTINGS ON DOWNSTREAM BCI\nTASKS", "content": "We provide more details for experiment settings on downstream BCI tasks."}, {"title": "D.1 BASELINES", "content": "Here, we introduce the details of the baselines for performance evaluation.\nEEGNet (Lawhern et al., 2018) is a compact convolutional neural network based on depthwise and\nseparable convolutions.\nEEGConformer (Song et al., 2022) is EEG model using CNN to learn low-level local features and\nself-attention to extract the global correlation within the local temporal features.\nSPaRCNet (Jing et al., 2023) is a deep neural network based on 1D CNN with dense residual\nconnections.\nContraWR (Yang et al., 2021) is a CNN based model, which first transforms the biosignals into\nmulti-channel spectrogram and then uses 2D-CNN based ResNet (He et al., 2016) to extract features\nfrom spectrogram.\nCNN-Transformer (Peh et al., 2022) is model utilizing CNN to extract local features and using\ntransformer to capture global dependencies.\nFFCL (Li et al., 2022) is neural network combining CNN and LSTM in parallel, where the CNN\nextracts spatial features and the LSTM extracts temporal features.\nST-Transformer Song et al. (2021) is a transformer based network which relies on the attention\nmechanism to learn the spatial and temporal features of EEG signals.\nBIOT (Yang et al., 2023) is an EEG foundation model which learns EEG generic representations\nbased on linear transformer and supervised-unsupervised-combined pre-training. It is worth noting\nthat the pre-trained BIOT can only accept EEG signals with a maximum of 18 channels as input.\nTherefore, for EEG signals with more than 18 channels, we will use multiple BIOT models to process\ndifferent sets of channels.\nLaBraM (Jiang et al., 2024) is a large brain model, which learns EEG generic representations\nby predicting the corresponding neural tokens of masked EEG patches based on full-attention\ntransformer."}, {"title": "D.2 METRICS", "content": "In this section, we introduce the details of the metrics used in the paper. Consistent with\nLaBraM (Jiang et al., 2024), we adopt the following metrics:\nBalanced Accuracy is a performance metric that considers the accuracy of each class in imbalanced\ndatasets, which is defined as the average of recall obtained on each class. We use it for both binary\nclassification and multi-class classification.\nAUC-PR is a performance metric calculating the area under the precision recall (PR) curve for binary\nclassification task.\nAUROC is a widely used statistic calculating the area under the receiver operating characteristic\n(ROC) curve. We use it for binary classification.\nCoken's Kappa is a statistical measure used to assess the agreement between two raters or classifiers\nin categorical classification tasks, which is usually used for imbalanced multi-class classification task.\nWeighted F1 is a weighted average of individual F1-scores from each class, with each score weighted\nby the number of samples in the corresponding class, which provides a reliable measure in multi-class\nclassification tasks.\nPearson's correlation is a statistical measure that quantifies the linear relationship between two\ncontinuous variables, which can be used to quantify the performance of a regression model.\nR2 score, also known as the coefficient of determination, is a statistical measure commonly used to\nevaluate the goodness of fit of a regression model.\nRMSE (Root Mean Square Error) is a commonly used performance metric in regression tasks to\nmeasure the average magnitude of the errors made by a regression model, which calculates the square\nroot of the average of the squared differences between the predicted values and the true values."}, {"title": "D.3 FINE-TUNING", "content": "We load the pre-trained weights of CBraMod and replace the reconstruction head with a task-specific\nhead which is composed of multi-layer perceptrons. Here the learned EEG representations are\nflattened and fed into the task-specific head for downstream tasks. Then we fine-tune CBraMod in\ndownstream datasets. We employ binary cross-entropy (BCE) loss for binary classification, cross-\nentropy loss for multi-class classification, and mean-square-error loss function for regression.\nMore hyperparameters for CBraMod fine-tuning on downstream datasets are shown in Table 6."}, {"title": "E MORE RESULTS ON OTHER DOWNSTEAM BCI TASKS", "content": "In this section, we report more results of CBraMod and baselines on other downstream BCI tasks."}, {"title": "E.1 SLEEP STAGING", "content": "ISRUC (Khalighi et al., 2016) is a sleep dataset, which comprises 100 all-night PSG recordings of\n100 adults. In our experiments, we used PSG recordings in Sub-group 1 to evaluate our method. The\nEEG signals of ISRUC are collected with 6 channels (F3-A2, C3-A2, 01-A2, F4-A1, C4-A1, 02-A1)\nand 200 Hz sampling rate. All EEG signals are segmented into 89,240 30-second samples, which are\nclassified into five different sleep stages (Wake, N1, N2, N3, REM) by sleep experts according to the\nAmerican Academy of Sleep Medicine (AASM) sleep standard (Iber et al., 2007). In our experiment,\nwe set subject 1 to 80 as training set, subject 81 to 90 as validation set and subject 91 to 100 as test set.\nNotably, according to AASM standard (Iber et al., 2007), experts usually identify the current stage\nbased on the transition patterns, combining the sleep stages of other samples in a sleep sequence\ntogether. The transition patterns of sleep stages between epochs play a critical role in automatic sleep\nstaging. Existing works on sleep staging (Phan & Mikkelsen, 2022) usually regard sleep staging\nas a sequence-to-sequence classification task and set 20 as the sequence length, where one sleep\nsequence has 20 30-second samples. Therefore, in experiment on sleep staging, we set each compared\nmodel as sample encoder, and use a one-layer transformer as sequence encoder. Specifically, we\nincluded two widely recognized baseline in the field of sleep staging, DeepSleepNet (Supratak et al.,\n2017) and USleep (Perslev et al., 2021), to compare the performance of our method with that of\nclassic algorithms in the field. As shown in Table 7, CBraMod outperforms all baselines. Specifically,\nCBraMod obtain significantly better performance compared to existing best method LaBraM (0.7442\nv.s. 0.7231 in Cohen's Kappa)."}, {"title": "E.2 SEIZURE DETECTION", "content": "CHB-MIT (Goldberger et al., 2000a; Shoeb, 2009) is a database, collected at the Children's Hospital\nBoston, consisting of EEG recordings from 23 pediatric subjects with intractable seizures. Subjects"}, {"title": "E.3 IMAGINED SPEECH CLASSIFICATION", "content": "BCIC2020-3 (Jeong et al., 2022) is a dataset for imagined speech classification on 2020 international\nbrain-computer interface competition. During the data collection experiment, 15 subjects were\nseated in a comfortable chair in front of a 24-inch LCD monitor screen. The subjects were instructed\nto imagine the silent pronunciation of the given word as if they were performing real speech,\nwithout moving any articulators nor making the sound. EEG signals of five-class imagined speech\nwords/phrases (\u201chello\u201d, \u201chelp me\u201d, \u201cstop\u201d, \u201cthank you\u201d and \u201cyes\u201d) were recorded at 64 channels\nand 256 Hz sampling rate. Training, validation and test set are provided by the dataset. Specifically,\n60 trials per class are released for training purpose, 10 trials per class are released for validation\npurpose and 10 trials per class are released for test purpose. The sum of trials is 80 \u00d7 5 \u00d7 15 = 6000.\nOne trial is a 3-second EEG signal, which is regard as one sample. Thus we get 6,000 64-channel\n3-second EEG samples, which are resampled to 200 Hz. The experimental results of imagined speech\nclassification are shown in Table 9. CBraMod achieves a great performance increases compared to\nthe best baseline LaBraM (0.4216 v.s. 0.3800 in Cohen's Kappa)."}, {"title": "\u0395.4 \u039c\u0395\u039dTAL DISORDER DIAGNOSIS"}]}