{"title": "INVERSE REINFORCEMENT LEARNING WITH SWITCHING REWARDS AND HISTORY DEPENDENCY FOR CHARACTERIZING ANIMAL BEHAVIORS", "authors": ["Jingyang Ke", "Feiyang Wu", "Jiyi Wang", "Jeffrey E. Markowitz", "Anqi Wu"], "abstract": "Traditional approaches to studying decision-making in neuroscience focus on simplified behavioral tasks where animals perform repetitive, stereotyped actions to receive explicit rewards. While informative, these methods constrain our understanding of decision-making to short timescale behaviors driven by explicit goals. In natural environments, animals exhibit more complex, long-term behaviors driven by intrinsic motivations that are often unobservable. Recent works in time-varying inverse reinforcement learning (IRL) aim to capture shifting motivations in long-term, freely moving behaviors. However, a crucial challenge remains: animals make decisions based on their history, not just their current state. To address this, we introduce SWIRL (SWitching IRL), a novel framework that extends traditional IRL by incorporating time-varying, history-dependent reward functions. SWIRL models long behavioral sequences as transitions between short-term decision-making processes, each governed by a unique reward function. SWIRL incorporates biologically plausible history dependency to capture how past decisions and environmental contexts shape behavior, offering a more accurate description of animal decision-making. We apply SWIRL to simulated and real-world animal behavior datasets and show that it outperforms models lacking history dependency, both quantitatively and qualitatively. This work presents the first IRL model to incorporate history-dependent policies and rewards to advance our understanding of complex, naturalistic decision-making in animals.", "sections": [{"title": "1 INTRODUCTION", "content": "Historically, decision making in neuroscience has been studied using simplified assays where animals perform repetitive, stereotyped actions (such as licks, nose pokes, or lever presses) in response to sensory stimuli to obtain an explicit reward. While this approach has its advantages, it has limited our understanding of decision making to scenarios where animals are instructed to achieve an explicit goal over brief timescales, usually no more than tens of seconds. In contrast, in natural environments, animals exhibit much more complex behaviors that are not confined to structured, stereotyped trials. For example, a freely moving mouse may immediately rush toward the scent of food when hungry, but after eating, it might seek out a quiet spot to rest for an extended period. Thus, real-world animal behaviors form long sequences composed of multiple decision-making processes. Each decision-making process involves a series of states and actions aimed at achieving a goal, and such decision switching is unlikely to occur on very short timescales in simplified assays. Additionally, many of the goals animals pursue in natural settings are generated by intrinsic motivations and thus unobservable. To truly understand animal's decision-making in a naturalistic context, we need methods to uncover animals' intrinsic motivations during multiple decision-making processes."}, {"title": "2 RELATED WORK", "content": "IRL for animal behavior understanding. IRL has been widely used to infer animals' behavioral strategies and decision-making policies when the reward is unknown. For instance, Pinsler et al. (2018) applies IRL to uncover the unknown reward functions of pigeons, explaining and reproducing their flock behavior, and developed a method to learn a leader-follower hierarchy. Similarly, Hirakawa et al. (2018) uses IRL to learn reward functions from animal trajectories, identifying environmental features preferred by shearwaters, and discovered differences in male and female migration route preferences based on the estimated rewards. In another study, Yamaguchi et al. (2018) apllies IRL to C. elegans thermotactic behavior, revealing distinct behavioral strategies for fed and unfed worms. Additionally, Sezener et al. (2014) maps reward functions for rats freely moving in a square area, showing how these rewards changed before and after training. While these studies demonstrate the utility of IRL in uncovering behavioral strategies of freely moving animals, they share a key limitation: they all assume a single reward function governs all animal behaviors, which does not account for the complexities of long-term decision-making.\nHeterogeneous and time-varying IRL. Recent works have extended traditional IRL, which assumes a constant reward, to models with time-varying or multiple reward functions driving behavioral trajectories. For example, Babes-Vroman et al. (2011) introduced Multi-intention IRL, which infers multiple reward functions across different trajectories but still assumes a single reward function within each trajectory. On the other hand, the Dynamic IRL (DIRL) method (Ashwood et al., 2022a) models reward functions as a linear combination of feature maps with time-varying weights, addressing the issue of varying rewards within a trajectory. However, DIRL requires trajectories to be highly similar or clustered beforehand, significantly limiting its applicability. Moreover, it cannot capture switching decision-making processes over long-term periods where each process may vary in length. Additionally, BNP-IRL (Surana & Srivastava, 2014), locally consistent IRL (Nguyen et al., 2015) and multi-intention inverse Q-learning (IQL) (Zhu et al., 2024) all extended the multi-intention IRL framework to allow for changing reward functions within trajectories, making them the closest models to our proposed SWIRL. However, all models do not account for both decision-level and action-level history dependency, an important biologically plausible factor that SWIRL incorporates to achieve more accurate behavior modeling. In our experiments, we will use multi-intention IQL and locally consistent IRL as baseline models, as they are special cases of SWIRL.\nDynamics-based behavior analysis in animal neuroscience. Traditional approaches to analyzing animal behavior in neuroscience often rely on autoregressive dynamics models. For instance, MoSeq and related works (Wiltschko et al., 2015; Weinreb et al., 2024) assume that animal behavior consists of multiple segments modeled by an HMM, with each segment evolving through an autoregressive process. Stone (2023) introduces a switching linear dynamical system (SLDS), similar to an AR-HMM, but with an additional layer of continuous latent states between the behavioral trajectories and the hidden states representing behavioral segments. We argue that if each segment lasts only a few seconds, it represents meaningful action motifs, such as grooming and sniffing. However, if a segment is significantly longer and reflects a decision-making process, traditional dynamics-based models may not be suitable for identifying these long-term segments. However, these dynamics-based models are not entirely independent of SWIRL. We will demonstrate that SWIRL generalizes purely dynamics-based models by relying on a more principled IRL framework to identify multiple decision-making processes. Our goal is to offer profound insights that bridge these traditional and new models for animal behavioral analysis."}, {"title": "3 METHODS", "content": null}, {"title": "3.1 HIDDEN-MODE MARKOV DECISION PROCESS", "content": "A discounted Hidden-Mode Markov Decision Process (HM-MDP) is defined by the tuple M = (Z,S,A,P,Pz,r,y). Here, Z represents a finite set of hidden modes z, S denotes the finite state space, and A indicates the finite action space. rz represents the reward function r under hidden mode z. The discount factor y is constrained to the interval [0, 1]. Starting from an initial state 80, the agent (animal) selects an action a based on its policy (behavioral strategy) \u03c0 and subsequently receives a reward determined by rz, z \u2208 Z := {Z1, Z2,..., Zm}, where m represents the total number of modes. The agent then transitions to the next state s' according to the transition kernel P(s's, a), while the agent's hidden mode also transitions to z' based on the transition probability Pz(z'z)."}, {"title": "3.2 INVERSE REINFORCEMENT LEARNING", "content": "Inverse Reinforcement Learning (IRL) addresses the scenario where we have gathered multiple trajectories from an expert agent \u03c0*, comprising a set of state-action pairs {(s,a)}. The goal is to estimate the policy \u03c0* and reward r that generated these state-action pairs, often referred to as demonstrations in the literature. We assume that we have collected N expert trajectories, denoted as D = {\u00a71,2,...,\u00a3}. Each trajectory consists of a sequence of state-action pairs, represented as \u00a7n = {(s1, a\u2081), (s\u00bd, a\u00bd), . . .}, with Tn time steps, which may vary across trajectories."}, {"title": "3.3 SWITCHING INVERSE REINFORCEMENT LEARNING", "content": "Our SWIRL model is built on the HM-MDP. Instead of explicitly knowing the reward for each mode, we will use IRL to infer these rewards. Mathematically, at each time step t, we represent the agent's internal reward function rz\u2081 with an additional dependency on the hidden mode zt. This means that the agent receives a reward rz\u2081 based on its current hidden mode zt, which indicates the decision-making state the animal is in (e.g., water seeking or home seeking), with rzt representing the corresponding intrinsic motivation. Consequently, the optimal policy \u03c0t is determined by rzt. However, SWIRL goes beyond merely embedding IRL within HM-MDP. We also introduce two levels of history dependency into the model. The full graphical model is depicted in Fig. 1.\nThe decision-level dependency is characterized by the idea that animals make new decisions based on their previous choices. The transitions between decision-making processes already account for this decision-level dependency since Pz(Zt+1 zt). However, the hidden modes with such a classical transition are generated through an open-loop process: the mode 2t+1 depends solely on the preceding mode zt, with Zt+1 zt being independent of the observation state st. Consequently, if a discrete switch should occur when the animal enters a specific region of the state space, the classical transition will fail to capture this dependency. To address this, we extend the transition model to include the state st as a condition, resulting in Pz(Zt+1|2t, St), which effectively captures the desired relationship between decisions and the animal's location.\nFor action-level history dependency, we treat both the reward and policy under a hidden mode z as functions dependent on the previous L states, specifically rz : SL \u00d7 A \u2192 R and \u03c0\u2248 :\nSLA, where L\u2208 N and SL denotes the cartesian product of L state spaces. To simplify\nthe notation, we denote an element of SL as\ns, so that r\u2082(s,a) := r(s1,s2,...,s,a)\nand \u03c0\u2082(a|s\u00a3) := \u03c0z(a|s1, s2,..., s), and pad\nboth functions with dummy variables if the cur-\nrent time step is less than L. We can also\nadd the dependency of previous actions for the\nreward function. But we use state-only re-\nwards for simplicity and the IRL tradition. It's\nstraightforward to do so, though. This makes it natural to extend from a single state dependency to\na history of state dependencies in our work.\nFurthermore, we can view the decision process as being non-Markovian, meaning that the current decision or action depends not only on the current state but also on the history of previous states and actions. Noticeably, there are various approaches to address non-Markovian decision processes, including state augmentation (Sutton, 1991), recurrent neural networks (Bakker, 2001; Hausknecht & Stone, 2015), Neural Turing Machines (Parisotto & Salakhutdinov, 2017) and so forth. In this paper, we adopt the most common approach-state augmentation; however, the framework can also be implemented using more advanced and scalable methods."}, {"title": "3.4 SWIRL INFERENCE PROCEDURE", "content": "The goal of inference is to learn the hidden modes z and the model parameters \u03b8 =\n(Pz,rz, \u03c0z,p(81), p(21)) given the collected trajectories D. Here, p(s1) and p(21) represent the probabilities of the initial state and hidden mode, respectively. The variables rz and \u03c0\u2248 denote the reward and policy associated with the hidden mode z, while P\u2082 is the transition matrix between hidden modes. We can maximize the likelihood of the demonstration trajectories D to learn the optimal *, such that 0* = arg max, log P(D|0) (MLE). However, achieving this objective requires marginalizing over the hidden modes z, which is intractable. To address the intractability, we employ the Expectation-Maximization (EM) algorithm, alternating between updating the parameter estimates and inferring the posterior distributions of the hidden modes."}, {"title": "3.5 CONNECTION TO DYNAMICS-BASED BEHAVIOR ANALYSIS METHODS", "content": "Traditional methods for analyzing animal behavior in neuroscience often use autoregressive dynamics models, with the autoregressive hidden Markov model (ARHMM) being the most prevalent (Wiltschko et al., 2015; Weinreb et al., 2024). ARHMMs assume that the animal behavior consists of multiple segments represented by a hidden Markov model, where each segment evolves through an autoregressive process. Using the notation established earlier, we denote hidden modes as zt at time t, following the transition p(Zt+1|2t). At each time step t, the observation state st follows conditionally linear (or affine) dynamics, determined by the discrete mode zt. This can be expressed as St+1 = Azt St + Ut, where Az\u2081 is the linear dynamics associated with z\u0142 and vt represents Gaussian noise. If z\u0142 changes, the linear dynamics will also change accordingly. More generally, we can represent the dynamics as p(St+1 St, zt). Consequently, the overall generative model for the ARHMM can be summarized as follows: (1) zt ~ p(ZtZt-1), and (2) st+1 ~ p(St+1|St, zt). Let's outline the generative model of SWIRL without history dependency: (1) zt ~ p(zt|zt\u22121), and (2) St+1 ~ \u03a3\u03b1, P(St+1|St,at)\u03c0(at|St, zt). The term \u03c0(at|st, zt) arises because the policy is derived from rzt. Consequently, the primary distinction between ARHMM and SWIRL lies in the dynamics used to generate St+1.\nWe can show that SWIRL is a more generalized version of ARHMM. In a deterministic MDP, where p(st+1|st, at) is a delta function and each action at uniquely determines st+1, St+1 directly implies at. Thus, \u03a3\u03b1\u2081 P(St+1|St, at)\u03c0(at|St,zt) = \u03c0(\u03b1t|St, zt) = P(St+1|St, 2t). This effectively reduces SWIRL to ARHMM. In the second real-world experiment, the MDP setup satisfies these assumptions. In such a case, ARHMM can be seen as performing policy learning through behavioral cloning without learning a reward function, whereas SWIRL employs IRL to learn the policy.\nHaving established this connection, we can view SWIRL as a more generalized version of ARHMM, as it permits the MDP to be stochastic and allows multiple actions to result in the same preceding state. Additionally, explicitly modeling the policy introduces a reinforcement learning framework that better represents the decision-making processes of animals and reveals the underlying reward function. For SWIRL with history dependency, we can further connect it to the recurrent ARHMM (Linderman et al., 2016), which expands p(2t+1|zt) to p(Zt+1|zt, St).\nAn advanced version of the ARHMM is the switching linear dynamical system (SLDS), which assumes that the state st is unobserved. Instead, the observed variable yt is a linear transformation of st. Thus, the complete generative model for SLDS consists of: (1) zt ~ p(zt zt\u22121), (2) St+1 ~ P(St+1 St, zt), and (3) Yt+1 ~ P(Yt+1|St+1). This suggests that the representation st capturing the primary dynamics is, in fact, a latent representation of the external world yt. Building on this concept, we can extend SWIRL into a latent variable model, where st serves as the latent representation of the true observation state yt. This corresponds to the setup of Partial Observation Markov Decision Processes (POMDPs) in the literature. This extension will link SWIRL to representation learning in reinforcement learning, which we plan to explore further in future work.\nThus, we argue that SWIRL offers a more generalized and principled approach to studying animal behavior compared to commonly used dynamics-based models, as one can draw inspiration from the development of (latent) dynamics models to enhance advanced IRL methods for analyzing animal decision-making processes."}, {"title": "4 RESULTS", "content": "Throughout the experiment section, we use the following terminology to denote our proposed algorithms and the baseline models we compare.\n\u2022 MaxEnt (Ziebart et al., 2008; 2010): Maximum Entropy IRL where the reward function is only a function of the current state and action. It is a single-mode IRL approach with a single reward function.\n\u2022 Multi-intention IQL (Zhu et al., 2024): learns time-varying reward functions based on HM-MDP. It is a SWIRL model with no history dependency.\n\u2022 Locally Consistent IRL (Nguyen et al., 2015): learns time-varying reward functions based on HM-MDP. It is a SWIRL model with no action-level history dependency.\n\u2022 ARHMM (Wiltschko et al., 2015): learns the segmentation of animal behaviors using autoregressive dynamics combined with a hidden Markov model.\n\u2022 rARHMM (Linderman et al., 2016): recurrent ARHMM whose transition probability of the hidden modes also relies on the state.\n\u2022 I-1, I-2: the baseline variant of our proposed SWIRL method which assumes the transition kernel Pz is independent of the state. The reward and policy depend either on the current state (in the case of I-1) or on both the current and previous states (in the case of I-2). Note that I-1 represents the simplest version of SWIRL, which corresponds to Multi-intention IQL. Thus, we use I-1 to denote Multi-intention IQL. The model can incorporate an arbitrary history length L for the policy and reward; in this paper, we use L = 1 and L = 2.\n\u2022 S-1, S-2: our proposed SWIRL method where P\u2082 is state dependent. The suffix follows the same setup as above. S-1 corresponds to Locally Consistent IRL."}, {"title": "4.1 APPLICATION TO A SIMULATED GRIDWORLD ENVIRONMENT", "content": "We begin by testing our method on simulated trajectories within a 5 \u00d7 5 gridworld environment, where each state allows for five possible actions: up, down, left, right, and stay. The agent alternates between two reward maps: a home reward map and a water map (see Fig. 2A). Following the design of real animal experiments (Rosenberg et al., 2021), we assume that the water port provides water to the agent only once per visit. Therefore, under the water reward map, the agent receives a reward for (1) visiting the water state if it was not in the water state previously or (2) leaving the water state. The home reward map returns a reward at the home state. This leads to a non-Markovian reward function that relies on both the current state and the previous state. We employed soft-Q iteration to determine the optimal policy for each reward function and generated 200 trajectories based on the learned policy, using a history-dependent hidden-mode switching dynamic Pz(Zt+1|2t, St). Accordingly, the agent is more likely to switch to the home map after visiting the water port and to switch to the water map after returning home. Each trajectory consists of 500 steps.\nWe then used SWIRL to learn the reward functions and the transition dynamics between them, based on 80% of the generated trajectories. As a baseline, we employed the Maximum Entropy IRL (MaxEnt) method and tested four variations of the SWIRL models (I-1, I-2, S-1, S-2), with I-1 representing multi-intention IQL. Fig. 2A displays a comparison between the true and discovered reward functions, while Fig. 2B presents boxplots showing the Pearson correlation between the true and recovered reward functions, along with the test log-likelihood (LL) and test segmentation accuracy (which measures the ability to predict the correct segments for home and water modes). The test performance was evaluated using the remaining 20% of the trajectories. Notably, accurate reward recovery was only achieved with the S-2 model. All four SWIRL variations outperformed MaxEnt, indicating the presence of more than one hidden model. Both the state dependency of hidden-mode transitions (decision-level dependency) and the history dependency reward function (action-level dependency) contributed to further improvements in test LL and segmentation accuracy. Specifically, only the state-dependent models (S-1, S-2) could accurately and robustly recover test segments, while the independent models (I-1, I-2) exhibited lower accuracy with higher variance. This is attributed to the non-Markovian reward design, where the agent can only receive water once per visit. Notably, S-2, the full SWIRL model incorporating both decision-level and action-level dependencies, demonstrated the best performance across all metrics.\nTo assess the robustness of SWIRL, in Appendix B.5, we evaluated the performance of SWIRL (S-2) under increasing levels of random perturbations in this simulated gridworld dataset. We found that SWIRL can tolerate moderate levels of noise or incomplete data (with 30% of the states and actions in training data permuted), making it suitable for real-world animal behavior datasets where such challenges are common."}, {"title": "4.2 APPLICATION OF SWIRL TO LONG, NON-STEREOTYPED MOUSE TRAJECTORIES", "content": "We then applied SWIRL to the long, non-stereotyped trajectories of mice navigating a 127-node labyrinth environment with water restrictions (Rosenberg et al., 2021). In this experiment, a cohort of 10 water-deprived mice moved freely in the dark for 7 hours. A water reward was provided at an end node (Fig. 3A), but only once every 90 seconds at most. Similar to the simulated experiment, the 90-second condition forces the mice to leave the port after drinking water, leading to a non-Markovian internal reward function. For our analysis, we segmented the raw node visit data into 238 trajectories, each comprising 500 time points. This data format presents a considerably greater challenge compared to the same dataset processed with more handcrafted methods in previous IRL applications (Ashwood et al., 2022a; Zhu et al., 2024), which were limited to clustered, stereotyped trajectories of only 20 time points in length."}, {"title": "4.2.1 SWIRL INFERRED INTERPRETABLE HISTORY-DEPENDENT REWARD MAPS", "content": "We applied SWIRL to 80% of the 238 mouse trajectories from the water-restricted labyrinth experiments. According to Rosenberg et al. (2021), mice quickly learned the labyrinth environment and began executing optimal paths from the entrance to the water port within the first hour of the experiment. Therefore, we assume the mice acted optimally concerning the internal reward function guiding their behavior. We tested number of hidden modes Z from 2 to 5 in Appendix B.4.1 and found Z = 3 to be the best fit. Fig. 3E displays the held-out test LL for MaxEnt and the SWIRL variations based on the remaining 20% of trajectories. The state dependency in hidden-mode switching dynamics and the history dependency in the reward function contributed to improved test performance. The final SWIRL model (S-2) successfully inferred a water reward map, a home reward map, and an explore reward map (Fig. 3B). For better visualization, we averaged the S-2-recovered history-dependent rewards across previous states and normalized the reward values to a range of (0, 1). In the water reward map, mice received a high reward for visiting the water port. In the home reward map, there was a high reward for visiting state 0 at the center of the labyrinth, which also served as the entrance and exit. Mice occasionally went to state 0 to enter or leave the labyrinth and sometimes passed by on their way to other nodes. In the explore reward map, mice received a high reward for exploring areas of the labyrinth other than state 0 and the water port.\nWe are particularly excited to have inferred an interpretable history-dependent reward map for the water port (Fig. 3C). It indicates that mice receive a high reward (1.0) for reaching the water port when their previous location was not the water port. If their prior location was the water port, there is still a reward (0.7) for staying there, but the reward for leaving the water port is even higher (0.9). This observation aligns with the water port design, as mice can only obtain water once every 90 seconds. Consequently, it makes sense that the mice would want to leave the water port after reaching it. Such insights would not be captured by a Markovian reward function that depends solely on the current state."}, {"title": "4.2.2 SWIRL INFERRED INTERPRETABLE HISTORY-DEPENDENT HIDDEN-MODE SEGMENTS", "content": "We then visualized all mouse trajectories based on the hidden-mode segments predicted by SWIRL (S-2) (Fig. 3D). In segments classified as water mode, mice start from various locations in the labyrinth and move toward the water port. In segments identified as home mode, mice begin from distant nodes and head toward the center of the labyrinth (home). In segments categorized as explore mode, mice start from junction nodes or the water port and explore end nodes other than the water port. This result demonstrates that SWIRL can identify sub-trajectories of varying lengths from raw data spanning 500 time points, allowing us to visualize them together and reveal clustered behavioral strategies. This capability has not been achieved by previous studies on freely moving animal behavior over extended recording periods, and we conducted this analysis without prior knowledge of the locations of the water port or home.\nWe also provide a detailed visualization of the hidden-mode segments from an example trajectory in the held-out test data and compare the segmentation performance of the four SWIRL variations (Fig. 3F). In the S-2 segments, visits to the water port (indicated by orange dots) consistently occur at the end of a water mode segment, while visits to state 0 (home) (indicated by red crosses) typically happen at the conclusion of a home mode segment. Notably, home mode segments that do not include a visit to state 0 can still be valid, as these segments may end at state 1 or 2 (see Appendix C.1). In contrast, the I-1, I-2, and S-1 segments exhibit instances of water segments that do not involve a visit to the water port, along with many home segments that lack clear interpretability. Overall, S-2 successfully identifies robust segments of reasonable length, avoiding the numerous rapid switches seen in the other variations. We attribute this to both the state dependency of hidden mode transitions and the history dependency in rewards. This suggests that mice are unlikely to make quick changes in their decisions; instead, they make choices based on their current location and take into account at least two locations while navigating the maze."}, {"title": "4.3 APPLICATION OF SWIRL TO MOUSE SPONTANEOUS BEHAVIOR TRAJECTORIES", "content": "We also employed SWIRL on a dataset in which mice wandered an empty arena without explicit rewards (Markowitz et al., 2023). In this experiment, mouse behaviors were recorded via depth camera video, and dopamine fluctuations in the dorsolateral striatum were monitored. The dataset includes behavior \u201csyllables\u201d inferred by MoSeq (Wiltschko et al., 2015), which indicate the type of behavior exhibited by the mice during specific time periods (e.g., grooming, sniffing, etc.). Consequently, the trajectories consist of behavioral syllables, with each time point representing a syllable. We selected 159 trajectories, each comprising 300 time points, by retaining only the 9 most frequent syllables and merging consecutive identical syllables into a single time point. This method, also used in previous reinforcement learning studies on this dataset (Markowitz et al., 2023), ensures that each syllable has sufficient data for learning and allows the model to concentrate on the transitions between different syllables.\nThe MDP for this experiment comprises 9 states and 9 actions, where the state represents the current syllable and the action signifies the next syllable. As mentioned in Section 3.5, the ARHMM can be viewed as a variant of SWIRL that learns the policy through behavior cloning. In other words, the policy for this MDP aligns with the emission probability of the ARHMM. This setup offers an excellent opportunity to compare the performance of SWIRL with ARHMM and its variant, rARHMM.\nWe applied SWIRL, rARHMM, ARHMM, and MaxEnt to 80% of the trajectories and assessed the held-out test LL on the remaining 20% (Fig. 4B). All four SWIRL models outperformed ARHMM and rARHMM on this dataset, indicating that learning rewards is more beneficial for behavior segmentation and explaining the behavior trajectories. Interestingly, the history dependency in the reward function resulted in lower test LL, as S-1 and I-1 demonstrated higher test LL than S-2 and I-2. We believe this is attributable to the merging of consecutive identical syllables and the selection of the top 9 syllables during the preprocessing phase for this dataset. As a result of these steps, the actual time interval between st\u22121 and st may vary significantly, leading to a poorly defined time concept that complicates the model's ability to capture the history dependency in the reward function. However, we can use SWIRL with different variations as a hypothesis-testing tool. The variation yielding the highest test LL may be regarded as more accurately reflecting the dynamics and structure of the data. Consequently, these results suggest that the behavior trajectories exhibit only Markovian dependency rather than long-term non-Markovian dependency. Since S-1 remains higher than I-1, we conclude that the state dependency in the hidden mode transition contributes to explaining the data.\nThe best SWIRL model (S-1) recovered reward maps and hidden-mode segments provide insights into the variability of dopamine impacts on animal spontaneous behavior: As illustrated in Fig. 4A, the reward maps exhibit some similarities along with distinct differences. For certain reward maps, there is a relatively high correlation (e.g., 0.32 and 0.28) with dopamine fluctuations during the corresponding modes. This suggests that dopamine fluctuations can reflect a certain extent of reward during hidden modes 3 and 5. Furthermore, the plot of hidden mode segments across all trajectories (Fig. 4C) reveals identifiable patterns. For instance, hidden mode 4 tends to occur more frequently at the beginning of trajectories, while hidden mode 2 is more prevalent at the end. Previous work by Markowitz et al. (2023) showed that mice are generally more active and move quickly at the start of a trajectory and become slower and less active as they progress. Keeping this in mind, we examined the reward maps in Fig. 4A and found that hidden mode 4 is more rewarding for transitions into walk and run, whereas hidden mode 2 offers much weaker rewards for such transitions. In comparison, hidden mode 2 is associated with larger movements than hidden mode 4."}, {"title": "5 DISCUSSION", "content": "We introduce SWIRL, an innovative inverse reinforcement learning framework designed to model history-dependent switching reward functions in complex animal behaviors. Our framework can infer interpretable switching reward functions from lengthy, non-stereotyped behavioral tasks, achieving reasonable hidden-mode segmentation\u2014a feat that, to the best of our knowledge, has not been accomplished previously.\nWhile the current implementation of SWIRL performs efficiently for typical animal behavior datasets in neuroscience, we acknowledge the need for a more general and scalable implementation to address broader applications. In its current form, every step of the SWIRL inference procedure, except for the Soft-Q iteration, is compatible with large or continuous state-action spaces. However, the Soft-Q iteration is limited to discrete state-action spaces and can be slow with large state-action space. For moderate discrete state-action cases, we still recommend the Soft-Q iteration, as it provides a robust and accurate approach for the RL inner loop of MaxEnt IRL. Nevertheless, for applications requiring scalability and compatibility with general state-action spaces, alternative methods can be adapted to replace the Soft Q iteration in the RL inner loop. For instance, Soft Actor-Critic (Haarnoja et al., 2018).\nA promising future direction is to reformulate the standard MaxEnt IRL r-\u03c0 bi-level optimization problem in SWIRL as a single-level inverse Q-learning problem, based on the IRL approach known as IQ-Learn (Garg et al., 2021). This method has has been successfully adapted to large language models training, demonstrating great scalability(Wulfmeier et al., 2024). Additionally, the MaxEnt IRL framework can be viewed in an adversarial learning perspective (Fu et al., 2018). Prior work has explored adversarial IRL within the EM framework for continuous state-action spaces, although it relies on a future-option dependency at the decision level, which is not biologically plausible, and does not account for action-level history dependency (Chen et al., 2023).\nThese advancements suggest that the SWIRL framework has the potential to handle MDPs with larger and general state-action spaces. This scalability positions SWIRL as a valuable tool not only for computational neuroscience but also for broader interest of the machine learning community."}, {"title": "A APPENDIX A", "content": null}, {"title": "A.1 DERIVATION OF SWIRL OBJECTIVES BY EM ALGORITHM", "content": "Here we need to learn 0 \u2252 (rz,Pz,p(81),p(21)). Note that the total probability for a sequence\n{(St, at, zt)}t=1:T is\nlog p(z, s, a) = log p(21)p(81)\u03c0z\u2081(a1|81;rz) \u03a0 P(St|St\u22121, at-1)Pz (zt|2t\u22121, St\u22121)\u03c0zt (at|st;rz).\nThe expectation across all possible sequences is given by\n\nNow we use Expectation-Maximization to find 0. E step:\n\n\nM step:"}, {"title": "A.2 DISCUSSION ON THE CONVERGENCE", "content": "The SWIRL inference procedure follows the Expectation-Maximization (EM) algorithm, which has a convergence guarantee (Wu, 1983). For inferring the reward function under each hidden mode, SWIRL adopts the Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL) framework, with Soft-Q iteration serving as the RL inner loop. Both Soft-Q iteration (Haarnoja et al., 2017) and MaxEnt IRL (Zeng et al., 2022) have also been rigorously analyzed for convergence. Therefore, the overall convergence of the SWIRL inference procedure can be established based on above works."}, {"title": "A.3 COMPLEXITY ANALYSIS", "content": "Below, we provide a detailed complexity analysis of SWIRL inference procedure under tabular representation of rz(s\u0141, a) and Pz(Zt+1/zt, St)."}, {"title": "A.3.1 NOTATION", "content": "\u2022 N: Number of expert trajectories.\n\u2022 T: Length of each trajectory.\n\u2022 Z = Z: Number of hidden modes.\n\u2022 S = |S|: Number of states.\n\u2022 A = |A|: Number of actions.\n\u2022 L: Length for action-level history dependency.\n\u2022 I: Number of iterations in Soft-Q iteration.\n\u2022 Pr: Number of parameters in the reward function r. P\u2081 = Z \u00b7 SL \u00b7 A when r is represented in a tabular form.\n\u2022 PP: Number of parameters in the hidden mode transition"}]}