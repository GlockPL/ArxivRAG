{"title": "Study on the Helpfulness of Explainable Artificial Intelligence (XAI)", "authors": ["Tobias Labarta", "Elizaveta Kulicheva", "Ronja Froelian", "Christian Gei\u00dfler", "Xenia Melman", "Julian von Klitzing"], "abstract": "Explainable Artificial Intelligence (XAI) is essential for building advanced machine learning-powered applications, especially in critical domains such as medical diagnostics or autonomous driving. Legal, business, and ethical requirements motivate using effective XAI, but the increasing number of different methods makes it challenging to pick the right ones. Further, as explanations are highly context-dependent, measuring the effectiveness of XAI methods without users can only reveal a limited amount of information, excluding human factors such as the ability to understand it. We propose to evaluate XAI methods via the user's ability to successfully perform a proxy task, designed such that a good performance is an indicator for the explanation to provide helpful information. In other words, we address the helpfulness of XAI for human decision-making. Further, a user study on state-of-the-art methods was conducted, showing differences in their ability to generate trust and skepticism and the ability to judge the rightfulness of an AI decision correctly. Based on the results, we highly recommend using and extending this approach for more objective-based human-centered user studies to measure XAI performance in an end-to-end fashion.", "sections": [{"title": "1 Introduction", "content": "Society is facing the exponential application and exploitation of deep neural network-powered artificial intelligence (AI) in various areas of daily life. AI systems that make use of learned models to solve classification, segmentation, and transformation tasks for different input modalities such as images, video, text, and natural spoken language have shown or have the potential to outperform humans on specific tasks [14,17]. Since they are used in domains such as mobility, energy management, finance, medical diagnosis, and in general health, security,"}, {"title": "2 Measuring Explainability", "content": "Previous theoretical work on explainability and its measurement are often inspired by research from domains such as human-computer interaction, psychol-"}, {"title": "2.1 Approaches for measuring explainability", "content": "Previous theoretical work on explainability and its measurement are often inspired by research from domains such as human-computer interaction, psychol-"}, {"title": "2.2 User studies on the performance of XAI", "content": "Within the health domain, a study on XAI was conducted [13] to evaluate the helpfulness of various approaches to AI assistance in digital pathology. Clinical pathologists were shown various examples of the AI-assistance in a questionnaire, combined with expert interviews. Results of the study show a preference for pathologists for simple visual explanations that correspond to their way of making diagnostical decisions. On the other hand, participants expressed a concern that simplistic explanations allow for a lot of ambiguity in their interpretation.\nA study on XAI in the automotive domain [21] investigated the difference in perception of the end users of seeing a textual description of the decision and seeing a textual explanation of the decision without a decision itself. A user was asked to evaluate whether the AI model made a correct decision based on a textual description or an explanation of said decision. Results of this survey show that users tended to trust the explanation without a decision rather than a decision without an explanation.\nLakkaraju and Bastani [23] performed a study on the impact of misleading explanations on users. They hypothesized that existing XAI trust measures are not sufficient, as explanations could be perturbed, leading to users trusting a problematic AI. For this purpose, they constructed a black-box AI making bail decisions based on prohibited features like race or gender. On top, they added an intrinsic explanation method but distorted it such that it returns other, desired features as explanations that were not part of the decision, e.g. prior jail incarcerations. After conducting the study with 41 participants, they could show that users were 9.8 times more likely to trust the black-box AI with a misleading explanation than just the black-box AI although the AI was making the same, wrong decisions in both groups. They advocate for more interactive and explorable explanations, which they back with research findings [24] and a second, slightly adapted study they conducted.\nRibiero et al. [35] present an explanation method that is evaluated in this study, LIME. In addition to that, they conduct two studies to evaluate their newly developed method. First, they perform simulated user experiments to investigate the explanation usefulness of LIME and compare it to other methods. Second, they evaluate LIME with real participants. Here, they checked if users can decide which model performs better, based on the explanations. Ribiero et al. also asked the users whether they trust the decision of a biased model, in this case, the \"Husky vs. Wolf\" example [3,19,33]. They could show, that after displaying an explanation that highlights the model bias, significantly fewer users trusted the bad model.\nA different approach presented by [30] is to measure and quantify the amount of information the model provides, by measuring the number of rules a user discovers while interacting with an XAI system. While great at providing concrete"}, {"title": "3 An objective Methodology for evaluating XA\u0391\u0399", "content": "A key finding from the literature research was the lack of objective methods for evaluating XAI methods in a human-centered setting. Most of the existing approaches focus on qualitative studies investigating subjective features like satisfaction or just collecting feedback. Quantitative Methods without human involvement are often called to be more objective but are lacking the user focus which is essential for applied XAI methods. The only approach combining human involvement with quantitative methods was by Achtibat et al. [1], where users were asked to assess if the model relied on artifacts in the prediction process."}, {"title": "3.1 Objective Human-Centered X\u0391\u0399 Evaluation", "content": "We propose to use a proxy task, to evaluate our approach and answer the research questions about the ability of XAI methods to enable the user to judge, trust, and question AI decisions. The task is designed such that the performance to solve it directly represents the ability to use the explanations for the investigated purpose, e.g. in our case, to determine if the explanation allows the user to judge whether a model predicted the class of an input image correctly. Participants should judge this by reviewing the input image, knowing the ground truth label and the output of a single explainability method. During the experiment, the participants were not aware of the model's actual prediction or its correctness.  \nEvaluation Metrics As participants judge whether a model's output is correct, their prediction can either be correct or false. Therefore, their replies can be viewed as a confusion matrix of a binary classifier [45] as summarized in table 1. With this abstraction, it is possible to calculate participants' accuracy, sensitivity, and specificity. Accuracy = $\\frac{TP+TN}{TP+TN+FP+TN}$ expresses the participant's ability to correctly judge model predictions. As we used a balanced number of instances, an accuracy of 0.5 is the baseline for random guessing. This baseline means that in a binary classification task in balanced settings, the simplest random guessing will result in correct judgments about half the time, assuming an equal likelihood of either outcome. This sets a baseline accuracy of 0.5 as the point of comparison, indicating that any performance above this threshold suggests a better-than-random ability to discern the correctness of the model's outputs. Sensitivity and specificity are used to measure participants' ability to identify when they can trust a model's prediction and when they should question a model's prediction. The sensitivity is also known as true positive rate TPR = $\\frac{TP}{TP+FP}$. The specificity is also known as the true negative rate TNR = $\\frac{TN}{TN+FN}$."}, {"title": "3.2 Image classification & XAI methods", "content": "To avoid model-specific biases, two different AI models, AlexNet [22] and VGG16 [38], were selected for classification. The models were used without hyperparameter optimization. As image source, the imagenetv2-matched-frequency dataset [34] was chosen. It is a subset of imagenet, which originally contains 1000 classes, and each class has 500 images on average. The objects shown in the images range from everyday items to specific animals.\nBased on the experimental setup with a classification of individual images, only local explanation methods fit for the imaging modality could be used. Additionally, the methods had to be applied post-hoc so they could work on the same set of images. Other selection criteria included the use of well-documented methods and the availability of code. Acceptance and relevance within the scientific community were also key factors in the selection process. Following these restrictions, six XAI methods were selected: Layer-wise relevance propagation (LRP) [2,31], Gradient class activation mapping (GradCAM) [7,10], Local Interpretable Model-Agnostic Explanations (LIME) [15], SHapley Additive exPla-nations (SHAP) [28,29,37,40], Integrated Gradients (Integrated Gradients) [41] and Confidence Scores."}, {"title": "3.3 Survey design", "content": "Besides a set of demographic questions, incl. educational background, machine learning experience, and visual impairment, the survey consisted of 12 independent questionnaires with 24 pictures each. Of these 24 pictures, 12 were the same across all questionnaires to provide a reliable baseline and the other 12 were semi-randomly picked. They were picked to ensure that each combination of the XAI method, AI model, and output was uniformly represented.  \nTo minimize biases, the following precautions were made: All XAI output was used to generate a heatmap which then was overlayed over the original image. Additionally, the heatmaps used the same colormap, one suitable for most color-based visual impairments, and a color bar was added to make the survey as accessible as possible."}, {"title": "4 Survey Results", "content": ""}, {"title": "4.1 Questionnaire responses", "content": "The survey was closed after one month of execution time. Until the date of 13.07.2022, 139 participants completed the questionnaire. The survey was advertised using the TU Berlin social media pages, resulting in a substantial number of participants with a university background. Of all participants, 24 were undergraduates, while 44 were graduate students. Additionally, 26 postgraduate students took part, as well as seven participants with a PhD. Among the participants, no bias due to education, experience with machine learning, or visual impairment could be identified."}, {"title": "4.2 Qualitative Feedback", "content": "In addition to the questionnaire responses, participant feedback about the study was received. Some of the feedback could be valuable for future studies. One of the participants pointed out that it was hard to decide without knowing if a model was trained to recognize a specific class. On one hand, it would help the participant to make a decision, but on the other hand, it would also enable certain bias since it is possible to assume a participant would rather tend to answer \"yes\" to those classes which are in the trained classes list, and rather \"no\" to those classes not on the list. Also due to the broad participant scope of this study, it can be assumed that this information would not be helpful for all participants. Another point was in the case of multiple objects shown in a picture. Feedback was that it would be helpful to highlight the object to be classified if"}, {"title": "5 Discussion", "content": "Continuing with the discussion, table 2 states the results for the one-sample t-test performed on the accuracy means. All means were significantly greater than the random baseline of 0.5 (better than chance). Only Confidence Scores's accuracy was significantly higher than for all other methods. These findings were drawn from the pairwise conducted paired t-tests. Thus, Confidence Scores helped a user most to judge an Al's decision with a strong effect. All other methods were similarly helpful, with small to no practically measurable effect.\nIt can be assumed that Confidence Scores's superior accuracy (i.e. overall performance) originates partially from its relatively simple structure and clear results of just class probabilities. The explanations generated by the other methods have considerably more information than just classes with a probability and therefore need more interpretation. Participants had to consider the location of highlighted areas, as well as the color and brightness.\nFurthermore, table 3 states the results for the one-sample t-test performed on the sensitivity and specificity means. All XAI methods but SHAP had means that were significantly greater than 0.5. However, only GradCAM had a large positive effect.\nOverall, GradCAM, Confidence Scores and LRP helped a user most to trust an Al's decision with a medium to strong effect. Interestingly, SHAP even negatively impacted trust in an Al's decision with a medium negative effect. SHAP and Confidence Scores achieved means that were significantly greater than 0.5. Integrated Gradients's and LIME's means were not significantly different from"}, {"title": "6 Conclusion", "content": "Within this paper, an objective methodology for evaluating XAI methods was proposed. From literature research, a lack of quantitative methods for human-centered XAI evaluation was identified, which this work aimed to contribute to. This approach was evaluated in a user study, where six state-of-the-art \u03a7\u0391\u0399 methods were implemented. The goal of the user study was to examine how far existing methods enable users to judge, trust, and question AI decisions.\nThe results show that of the tested methods only Confidence Scores substantially enabled users to judge an AI decision, responding to the first research question. Aimed at the second research question, the methods GradCAM, Confidence Scores and LRP performed best in making users trust an Al decision. For the final research question, it can be concluded that SHAP enabled users by far the most in questioning an AI decision.\nFrom the research findings but also literature research, it can be concluded that using individual explanation methods is not sufficient for enabling users to judge, trust, and question an AI effectively. Instead, the design of an interactive"}]}