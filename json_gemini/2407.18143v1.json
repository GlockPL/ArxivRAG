{"title": "MAXIMUM ENTROPY ON-POLICY ACTOR-CRITIC VIA ENTROPY ADVANTAGE ESTIMATION", "authors": ["Jean Seong Bjorn Choe", "Jong-Kook Kim"], "abstract": "Entropy Regularisation is a widely adopted technique that enhances policy optimisation performance and stability. A notable form of entropy regularisation is augmenting the objective with an entropy term, thereby simultaneously optimising the expected return and the entropy. This framework, known as maximum entropy reinforcement learning (MaxEnt RL), has shown theoretical and empirical successes. However, its practical application in straightforward on-policy actor-critic settings remains surprisingly underexplored. We hypothesise that this is due to the difficulty of managing the entropy reward in practice. This paper proposes a simple method of separating the entropy objective from the MaxEnt RL objective, which facilitates the implementation of MaxEnt RL in on-policy settings. Our empirical evaluations demonstrate that extending Proximal Policy Optimisation (PPO) and Trust Region Policy Optimisation (TRPO) within the MaxEnt framework improves policy optimisation performance in both MuJoCo and Procgen tasks. Additionally, our results highlight MaxEnt RL's capacity to enhance generalisation.", "sections": [{"title": "Introduction", "content": "Entropy regularisation is pivotal to many practical deep reinforcement learning (RL) algorithms. Practical algorithms such as Trust Region Policy Optimization (TRPO) [Schulman et al., 2015a] penalise the policy improvement or greedy step using Kullback-Leibler (KL) divergence (also called as relative entropy) to regularise the deviations between consecutive policies. This method, often termed KL regularisation, has been the foundational approach for contemporary deep RL algorithms [Vieillard et al., 2020, Geist et al., 2019].\nAnother critical approach is to regularise the policy evaluation step by augmenting the conventional RL task objective with an entropy term, thereby directing policies toward areas of higher expected trajectory entropy. This scheme is often called Maximum Entropy RL (MaxEnt RL) [Ziebart, 2010, Haarnoja et al., 2018, Levine, 2018]. MaxEnt RL formulation is known to improve the exploration and robustness of policies by promoting stochasticity [Eysenbach and Levine, 2019, 2021]. In practice, MaxEnt RL can simply be implemented by adding an entropy reward to the original task reward.\nRecent theoretical advancements inspired by the Mirror Descent theory have developed a unified view of these approaches [Vieillard et al., 2020, Geist et al., 2019, Tomar et al., 2020], suggesting that their combination could lead to faster convergence to the solution of the regularised objective [Shani et al., 2020]. Furthermore, the latest studies on policy gradient (PG) methods have shown the effectiveness of the MaxEnt RL in accelerating the convergence of PG algorithms [Mei et al., 2020, Agarwal et al., 2021, Cen et al., 2022]. However, despite the enticing theoretical support, its practical application remains underexplored, particularly in stochastic policy gradient methods in on-policy settings.\nWe hypothesise that this research gap is potentially attributed to the difficulty of handling the entropy reward in practice. Yu et al. [2022] empirically analysed the problematic nature of the entropy reward using Soft Actor-Critic (SAC) [Haarnoja et al., 2018], an off-policy MaxEnt algorithm. Authors pointed out that in an episodic setting, the entropy return is largely correlated to the episode's length, thereby rendering the policy overly optimistic or pessimistic, and even in infinite-horizon settings, the entropy reward can still obscure the task reward.\nInspired by this observation, we proposed a simple but practical approach to control the impact of the entropy reward. In this paper, we introduce Entropy Advantage Policy Optimisation (EAPO), a method that estimates the task and entropy objectives of the regularised (soft) objective separately. By employing a dedicated discount factor for the entropy reward and utilising Generalised Advantage Estimation (GAE) [Schulman et al., 2015b] on each objective separately, EAPO controls the effective horizon of the entropy return estimation and the entropy regularisation on policy evaluation. EAPO's simplicity requires only minor modifications to existing advantage actor-critic algorithms. This work extends the well-established PPO [Schulman et al., 2017b] and TRPO [Schulman et al., 2015a]."}, {"title": "Background", "content": ""}, {"title": "Preliminaries", "content": "This work considers a finite discounted deterministic Markov Decision Process (MDP) $(\\mathcal{S}, \\mathcal{A}, r, \\rho, \\mathcal{T}, \\gamma_v, \\gamma_{\\eta})$, where $\\mathcal{S}$ is the set of states $s$ and $\\mathcal{A}$ is the set of actions $a$, and $\\rho$ is the initial state distribution. $\\mathcal{T}$ is the deterministic transition function $\\mathcal{T}: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{S}$, and $r$ is the reward function $r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$. $\\gamma_v$ and $\\gamma_{\\eta}$ are the discount factors.\nWe define the value function of state $s$ under the policy $\\pi$ as\n$V^{\\pi}(s) := \\mathbb{E}_{s_0=s, a_t \\sim \\pi(\\cdot | s_t), s_{t+1}=\\mathcal{T}(s_t, a_t)} [\\sum_{t=0}^{\\infty} \\gamma_v^t r(s_t, a_t)].$"}, {"title": "Soft advantage function", "content": "Analogous to the definition of the action-value function $Q^{\\pi}$ as the expected cumulative rewards after selecting an action $a$ [Sutton and Barto, 2018], we define $Q_{\\eta}^{\\pi}$ as the expected future trajectory entropy after selecting an action:\n$Q_{\\eta}^{\\pi}(s_t, a_t) := \\gamma_{\\eta} V_{\\eta}^{\\pi}(\\mathcal{T}(s_t, a_t)) = \\gamma_{\\eta} V_{\\eta}^{\\pi}(s_{t+1}).$\nThe definition arises naturally from the consideration that uncertainty exists due to the stochastic policy at the current state, which has settled by the time an action is performed. Consequently, the $Q_{\\eta}^{\\pi}$ is simply the discounted trajectory entropy of the next state determined by the deterministic transition function.\nFrom the recursive relation of trajectory entropy from (1) and the defintion (4), the following relation is derived:\n$V_{\\eta}^{\\pi}(s_t) = \\mathbb{E}_{a_t \\sim \\pi(\\cdot|s_t)} [- \\log \\pi(a_t|s_t) + Q_{\\eta}^{\\pi}(s_t, a_t)]$.\nWe now define the entropy advantage function $A_{\\eta}^{\\pi}$ analogous to (2.1):\n$A_{\\eta}^{\\pi}(s_t, a_t) := Q_{\\eta}^{\\pi}(s_t, a_t) - \\mathbb{E}_{a \\sim \\pi(\\cdot|s_t)} [Q_{\\eta}^{\\pi}(s_t, a)]$\n$= Q_{\\eta}^{\\pi}(s_t, a_t) - V_{\\eta}^{\\pi}(s_t) + \\mathbb{E}_{a \\sim \\pi(\\cdot|s_t)} [- \\log \\pi(a|s_t)].$\nWe let $V^{\\pi}(s) := V^{\\pi}(s) + \\tau V_{\\eta}^{\\pi}(s)$ as the soft value function, and let $Q^{\\pi}(s, a) := Q^{\\pi}(s) + \\tau Q_{\\eta}^{\\pi}(s, a)$ as the soft Q-function. Finally, we define the soft advantage function:\n$\\tilde{A}^{\\pi}(s_t, a_t) := A^{\\pi}(s_t, a_t) + \\tau A_{\\eta}^{\\pi}(s_t, a_t)$\n$= Q^{\\pi}(s_t, a_t) - V^{\\pi}(s_t) + \\tau(Q_{\\eta}^{\\pi}(s_t, a_t) - V_{\\eta}^{\\pi}(s_t) + \\mathbb{E}_{a \\sim \\pi(\\cdot|s_t)} [- \\log \\pi(a|s_t)])$\n$= Q^{\\pi}(s_t, a_t) - V^{\\pi}(s_t) + \\tau \\mathbb{E}_{a \\sim \\pi(\\cdot|s_t)} [- \\log \\pi(a|s_t)].$"}, {"title": "Soft policy gradient theorem", "content": "Shi et al. [2019] showed that it is possible to optimise the soft objective using direct policy gradient from samples. Thus, we can use the soft advantage function to find the policy that maximises the MaxEnt RL objective.\nTheorem 1 (Soft Policy Gradient). Let $J(\\pi)$ the MaxEnt RL objective defined in 2. And $\\pi_{\\theta}(a|s)$ be a parameterised policy. Then,\n$\\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{s_0 \\sim \\rho,\\atop a_t \\sim \\pi,\\ s_{t+1}=\\mathcal{T}(s_t, a_t)} [(A^{\\pi}(s_t, a_t) + \\gamma_{\\eta} \\tau A_{\\eta}^{\\pi}(s_t, a_t))\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)].$"}, {"title": "Proposed method", "content": ""}, {"title": "Overview", "content": "In this section, we develop our Entropy Advantage Policy Optimisation (EAPO) method. At its core, EAPO independently estimates both the value advantage function and the entropy advantage function and combines them to derive the soft advantage function. EAPO adopts a separate prediction head to the conventional value critic to approximate the trajectory entropy of a state, which is then used for entropy advantage estimation. We extend the PPO [Schulman et al., 2017b] and TRPO [Schulman et al., 2015a] by substituting the advantage estimate with the soft advantage estimate and omitting the entropy bonus term."}, {"title": "Entropy advantage estimation", "content": "The entropy advantage $A_{\\eta}^{\\pi}$ is estimated from the sampled log probabilities of the behaviour policy. We utilise the Generalised Advantage Estimation (GAE) [Schulman et al., 2015b] for a variance-reduced estimation of the entropy advantage:\n$\\hat{A}_{\\eta, GAE}^{(\\lambda_\\eta, \\gamma_{\\eta})}(s_t, a_t) := \\sum_{l=0}^{\\infty} (\\lambda_{\\eta} \\gamma_{\\eta})^l \\delta_{t+l},$\nwhere $\\delta_t:= - \\log \\pi(a_t|s_t) + \\gamma_{\\eta} V_{\\eta}^{\\pi}(s_{t+1}) - V_{\\eta}^{\\pi}(s_t)$, and $\\gamma_{\\eta}$ and $\\lambda_{\\eta}$ are the discount factor and GAE lambda for entropy advantage estimation, respectively. Note that the equation is the same as the GAE for the conventional advantage, except the reward term is replaced by the negative log probability. This simplicity is also consistent with the remark that the only modification required for the MaxEnt policy gradient is to add the negative log probability term to the reward at each time step [Levine, 2018]."}, {"title": "Entropy critic", "content": "An entropy critic network, parameterised by $w$, approximates the trajectory entropy $V_{\\eta}^{\\pi}$, and it is trained by minimising the mean squared error $\\mathcal{L}_H(w) := \\mathbb{E}_t (V_{\\eta}^{\\pi}(s_t; w) - \\hat{V}_{\\eta}^{\\pi}(s_t))^2$, where the trajectory entropy estimate $\\hat{V}_{\\eta}^{\\pi}(s_t)$ is calculated using TD(0): $\\hat{V}_{\\eta}^{\\pi}(s_t) = \\hat{A}_{\\eta, GAE}^{(\\lambda_\\eta, \\gamma_{\\eta})}(s_t, a_t) + V_{\\eta}^{\\pi}(s_t; w)$. Throughout the conducted experiments, we implemented the entropy critic network to share its parameters with the return value critic $V_V$, with only the final linear layers for outputting its prediction distinct. This form of parameter sharing allows minimal computational overhead to implement EAPO.\nFurther, we employ the PopArt normalisation [van Hasselt et al., 2016] to address the scale difference of entropy and return estimates. It is important to note that the negative log probability $\u2013 \\log \\pi(a_t|s_t)$ is collected for every timestep. In contrast, the reward can be sparse, leading to significant magnitude variations based on the dynamics of the environment [Hessel et al., 2019]. This discrepancy can pose challenges, especially when using a shared architecture. Thus, utilising the value normalisation technique like PopArt is pivotal for the practical implementation of EAPO."}, {"title": "Entropy advantage policy optimisation", "content": "Subsequently, we integrate the entropy advantage with the standard advantage estimate $\\hat{A}_V$, also computed using GAE and return value critic parameterised by $\\phi$, analogously to the entropy advantage estimation process we describe above. Then the soft advantage function $\\tilde{A}^{\\pi}$ is\n$\\tilde{A}^{\\pi}(s_t, a_t) = \\hat{A}_{V, GAE}^{(\\lambda_v,\\gamma_v)}(s_t, a_t) + \\tau \\hat{A}_{\\eta, GAE}^{(\\lambda_\\eta, \\gamma_{\\eta})}(s_t, a_t),$\nwhere $\\hat{A}_{V, GAE}^{(\\lambda_v,\\gamma_v)}$ is the value advantage estimation using GAE. Finally, we substitute the estimated conventional advantage function in the policy objective of PPO and TRPO with $\\tilde{A}^{\\pi}$. The PPO objective function becomes:\n$\\mathcal{L}(\\theta, \\phi, w) = \\mathbb{E}_t [min(r_t(\\theta) \\tilde{A}^{\\pi}(s_t, a_t), clip(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\tilde{A}^{\\pi}(s_t, a_t))]$$\n$+ c_1 \\mathcal{L}_V(\\phi) + c_2 \\mathcal{L}_H(w),$\nwhere $r_t(\\theta)$ is the probability ratio between the behaviour policy $\\pi_{\\theta_{old}}(a_t|s_t)$ and the current policy $\\pi_{\\theta}(a_t|s_t)$, and $c_1, c_2$ and $\\epsilon$ are hyperparameters to be adjusted. The value critic loss $\\mathcal{L}_V$ is also defined by the mean square error,\n$\\mathcal{L}_V(\\phi) = \\frac{1}{2} \\mathbb{E}_t [(V(s_t; \\phi) - V^{\\pi}(s_t))^2]$\nwhere $V^{\\pi}$ is the return value estimate.\nSimilarly, the optimisation problem of TPRO becomes:\n$max_{\\theta} \\mathbb{E}_t [r_t(\\theta) A^{\\pi}(s,a)], s.t. \\mathbb{E}_t[KL(\\pi_{\\theta_{old}}|\\pi_{\\theta})] \\le \\delta,$\nwhere $\\delta$ is a hyperparameter."}, {"title": "Experiments", "content": "In this section, we evaluate the policy optimisation performance of EAPO against the corresponding baseline on-policy algorithms, PPO and TRPO. Specifically, we assess the optimisation efficiency for episodic tasks and the generalisation capability of EAPO on 16 Procgen [Cobbe et al., 2020] benchmark environments. Moreover, we investigate EAPO's efficacy on continuing control tasks using 4 discretised popular MuJoCo [Todorov et al., 2012] environments, and we analyse the impact of hyperparameters $\\tau$, $\\gamma_{\\eta}$ and $\\lambda_{\\eta}$. Finally, we include MiniGrid-DoorKey-8x8 environment [Chevalier-Boisvert et al., 2023] to examine if EAPO can help solve the hard exploration task.\nWe implemented EAPO using Stable-baseline3 [Raffin et al., 2019] and conducted experiments on environments provided by the Envpool [Weng et al., 2022] library. All empirical results are averaged over from 10 random seeds, with a 95% confidence interval indicated.\nFor the hyperparameter selection, we conducted a brief search for baseline algorithm hyperparameters that perform reasonably well, tuning only the EAPO-specific hyperparameters such as $\\gamma_{\\eta}$ to ensure fair comparisons. Implementation details and hyperparameters are reported in Appendix B."}, {"title": "Procgen benchmark environments", "content": "We evaluate the performance and the generalisation capability of PPO-based EAPO on the 16 Procgen benchmark environments [Cobbe et al., 2020]. These environments have a discrete action space and use raw images as observations. Procgen suite includes episodic tasks with both positive (e.g., BigFish) and negative (e.g., Climber) correlations between the episode length and the return, making them suitable for testing MaxEnt RL algorithms. Following the evaluation procedure in [Cobbe et al., 2020], we trained agents on 200 procedurally generated levels and test the performance as the mean episodic return on 100 unseen levels for each environment. We use the easy difficulty setting. We employed a single set of hyperparameters for all environments. For hyperparameter tuning, we used three representative environments: BigFish, Climber and Dodgeball. Each represents different levels of correlation between episode length and the return. BigFish requires the agent to survive as long as possible, Climber allows indefinite exploration, and in Dodgeball, the agent must survive during the first phase and reach the goal quickly in the second stage.\nFigure 2 and Table 1 summarise the generalisation test results of EAPO and baseline PPO agents with varying $\\tau$ and the entropy bonus coefficient. EAPO, with a lower discount factor of $\\gamma_{\\eta} = 0.8$ and $\\lambda_{\\eta} = 0.95$ surpasses the baseline PPO agent in most of the environments. Additionally, EAPO also outperforms the baseline during the training phase, demonstrating its efficiency in policy optimisation.\nWe also investigate the impact of the GAE $\\lambda_{\\eta}$ hyperparameter for the entropy advantage estimation. The result shows that $\\lambda_{\\eta}$ does not affect the performance significantly, suggesting that adjusting $\\gamma_{\\eta}$ and $\\tau$ would be sufficient for most cases.\nFigure 2 (Right) shows the improved generalisation capability of high entropy policies. The policy trained with higher temperature $\\tau$ favours high entropy trajectories (see Figure 3) and performs similar or worse than the one with lower $\\tau$ during the training but achieves better during the test. This result is in coherence with the previous study of [Eysenbach and Levine, 2021] that a MaxEnt policy is robust to the distributional shift in environments."}, {"title": "Discretised continuous control tasks", "content": "We measure the performance of EAPO extending PPO (EAPO-PPO) and TRPO (EAPO-TRPO) on continuing control tasks in 4 MuJoCo environments, comparing them against their corresponding baselines. For the PPO baselines, we searched for the best entropy coeffcient within the set $c \\in (0.0001, 0.001, 0.01)$. Additionally, we tested the PPO and TRPO agent with the reward augmented by the entropy reward $-\\tau \\log \\pi(a_t|s_t)$ to evaluate the impact of separating the MaxEnt objective. Note that the entropy reward-augmented baseline is effectively regarded as EAPO with $\\gamma_{\\eta} = \\gamma$ and $\\lambda_{\\eta} = \\lambda$, but without the entropy critic. We discretise the continuous action space using the method proposed by Tang and Agrawal [2020]. Experiments using continuous action space are provided in Appendix C. The training curves are presented in Figure 4.\nThe result shows that by adjusting $\\gamma_{\\eta}$ and $\\lambda_{\\eta}$, we can configure EAPO to outperform or match the conventional entropy regularisation method throughout all environments. We found that the best-performing values of $\\gamma_{\\eta}$ and $\\lambda_{\\eta}$ vary depending on the characteristics of the environment, similar to their value counterparts $\\gamma$ and $\\lambda$, respectively. Although EAPO demonstrates more stable performance compared to the entropy bonus, this relatively modest performance gain suggests that EAPO may be less efficient for continuing tasks."}, {"title": "MiniGrid-DoorKey-8x8 environment", "content": "Finally, we evaluate the exploration performance of PPO-based EAPO on the MiniGrid-DoorKey-8x8 environment [Chevalier-Boisvert et al., 2023]. Figure 5 shows that EAPO, with the given hyperparameters, can solve this hard exploration task within 5,000,000 frames for all 10 seeds, whereas the baseline PPO only achieves the goal for 3 seeds. However, unlike other tasks presented in this work, this task was highly sensitive to hyperparameters. As noted by [Mei et al., 2020], entropy regularization may not effectively mitigate epistemic uncertainty. Our results show inconclusive evidence for better exploration using MaxEnt RL in this context."}, {"title": "Conclusion", "content": "We have introduced EAPO, a model-free on-policy actor-critic algorithm based on the maximum entropy reinforcement learning framework. Our approach shows that a straightforward extension of existing mechanisms for standard value learning in on-policy actor-critic algorithms to the trajectory entropy objective can facilitate the practical implementation of MaxEnt RL. Through empirical evaluations, our method has been shown to replace the conventional entropy regularisation method and that a more principled entropy maximisation method enhances generalisation. While this paper focuses on PPO and TRPO, one can seamlessly adapt EAPO to other advantage actor-critic algorithms such as A3C [Mnih et al., 2016]. This adaptability lays the groundwork for deeper investigations into the interactions between the MaxEnt RL framework and various components of reinforcement learning algorithms. We anticipate that the inherent simplicity of on-policy algorithms and EAPO will encourage broader applications of the MaxEnt RL algorithm to promising areas like competitive reinforcement learning and robust reinforcement learning."}, {"title": "Proof of the soft policy gradient theorem", "content": "We begin with the proof of Shi et al. [2019]. Let us denote the discounted state distributions induced by the policy $\\pi$ as $p^{\\pi}(s)$ for the discount factor $\\gamma_v$, and as $p_{\\eta}^{\\pi}(s)$ for the discount factor $\\gamma_{\\eta}$, respectively. Then,\n$\\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{s_0 \\sim \\rho,\\atop a_t \\sim \\pi,\\ s_{t+1}=\\mathcal{T}(s_t, a_t)} [(Q^{\\pi}(s_t, a_t) - \\tau \\log \\pi(a_t|s_t) - 1)\\nabla_{\\theta} \\log \\pi(a_t|s_t)]$\n$=\\sum_{s \\in S} p^{\\pi}(s) [\\nabla_{\\theta} \\sum_a \\pi(s|a) (Q^{\\pi}(s, a) - V^{\\pi}(s))]$\n$+ \\tau \\sum_{s \\in S} p_{\\eta}^{\\pi}(s) [\\nabla_{\\theta} \\sum_a \\pi(s|a) (Q_{\\eta}^{\\pi}(s, a) - \\log \\pi(a|s))]$\n$= \\mathbb{E}_{s \\sim p^{\\pi},a_t \\sim \\pi} [\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)A^{\\pi}(s_t, a_t)] + \\tau \\mathbb{E}_{s \\sim p_{\\eta}^{\\pi},a_t \\sim \\pi} [\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)A_{\\eta}^{\\pi}(s_t, a_t)]$\n$= \\mathbb{E}_{s_0 \\sim \\rho,\\atop a_t \\sim \\pi,\\ s_{t+1}=\\mathcal{T}(s_t, a_t)} [\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)A^{\\pi}(s_t, a_t)]$\n$+ \\tau \\mathbb{E}_{s_0 \\sim \\rho,\\atop a_t \\sim \\pi,\\ s_{t+1}=\\mathcal{T}(s_t, a_t)} [\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) \\gamma_{\\eta} A_{\\eta}^{\\pi}(s_t, a_t)]$\n$= \\mathbb{E}_{s_0 \\sim \\rho,\\atop a_t \\sim \\pi,\\ s_{t+1}=\\mathcal{T}(s_t, a_t)} [(A(s_t, a_t) + \\gamma_{\\eta} \\tau A_{\\eta}^{\\pi}(s_t, a_t))\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)].$"}, {"title": "Hyperparameters and implementation details", "content": ""}, {"title": "Details of the MiniGrid-Empty-8x8-v0 example", "content": "The action space of MiniGrid-Empty-8x8-v0 [Chevalier-Boisvert et al., 2023] consists of 7 discrete actions: 2 actions for turning left or right, 1 action for moving forward, and 4 no-op actions. The reward, calculated as $1 - 0.9^{t/T}$ is given only when the agent reaches the goal state, where $t$ is the number of steps taken and $T$ is the maximum number of steps allowed. For $T$, the default value (256) is used. We modified the turning actions so that they also move the agent forward to the corresponding directions (i.e., no extra step is required to maneuver), enabling multiple optimal trajectories. This modification reduces the optimal number of steps from 11 to 10. The observation used is the full $8 \\times 8 \\times 3$ image without partial observability. We employ a simple CNN architecture from [Willems, 2023] as a shared feature extractor. The state visitation plots are generated from 100 rollouts using policies trained over 4M frames. The mean trajectory entropy and the mean steps are averaged from 10 different random seeds. We select the representative heatmap for each setup from the run with the trajectory entropy closest to the average trajectory entropy across all seeds. The normalised state visitation frequency is calculated by dividing the number of visits to each state divided by the total number of state visits in the trajectories."}, {"title": "Hyperparameters", "content": "We use the default values in stable-baseline3 [Raffin et al., 2019] and envpool [Weng et al., 2022] libraries for the settings not specified in the table 2. EAPO-specific hyperparameters are reported in Table 3. The parameters for the MuJoCo tasks are found in a coarse hyperparameter search."}, {"title": "Network architecture", "content": "For discretised MuJoCo tasks, we used simple tanh networks for the policy and the critics with hidden layers of depth [64, 64] and [128, 128], respectively. We implemented the entropy critic as the independent output layer that shares the hidden layers with the value network. For the continuous MuJoCo tasks we use [256, 256] and [512, 512] for policy and critic networks and the state- and action-dependent o output for the Gaussian policy with the Softplus output layer.\nIn Procgen benchmark experiments, we adopt the same IMPALA CNN architecture used in Cobbe et al. [2020]. The entropy critic is again a single final output layer that shares the CNN feature extractor."}, {"title": "Additional experiments", "content": ""}]}