{"title": "Change Detection-Based Procedures for Piecewise Stationary MABs: A Modular Approach", "authors": ["Yu-Han Huang", "Argyrios Gerogiannis", "Subhonmesh Bose", "Venugopal V. Veeravalli"], "abstract": "Conventional Multi-Armed Bandit (MAB) algorithms are designed for stationary envi-\nronments, where the reward distributions associated with the arms do not change with\ntime. In many applications, however, the environment is more accurately modeled as be-\ning nonstationary. In this work, piecewise stationary MAB (PS-MAB) environments are\ninvestigated, in which the reward distributions associated with a subset of the arms change\nat some change-points and remain stationary between change-points. Our focus is on the\nasymptotic analysis of PS-MABs, for which practical algorithms based on change detection\n(CD) have been previously proposed. Our goal is to modularize the design and analysis of\nsuch CD-based Bandit (CDB) procedures. To this end, we identify the requirements for\nstationary bandit algorithms and change detectors in a CDB procedure that are needed for\nthe modularization. We assume that the rewards are sub-Gaussian. Under this assumption\nand a condition on the separation of the change-points, we show that the analysis of CDB\nprocedures can indeed be modularized, so that regret bounds can be obtained in a unified\nmanner for various combinations of change detectors and bandit algorithms. Through this\nanalysis, we develop new modular CDB procedures that are order-optimal. We compare the\nperformance of our modular CDB procedures with various other methods in simulations.", "sections": [{"title": "1 Introduction", "content": "In the Multi-Armed Bandit (MAB) problem, an agent chooses between finitely many arms,\neach of which, when selected, generates a stochastic reward. The goal of the agent is to\nmaximize the reward obtained over a horizon of interest without knowing the actual reward\ndistributions. See Lattimore and Szepesv\u00e1ri (2020); Slivkins et al. (2019) for recent books\non the topic.\nIn the most common setting of the MAB problem, the reward distributions associated\nwith the arms are stationary, meaning that they remain unchanged across the horizon of\ninterest. However, this stationarity assumption may not hold in many practical settings.\nFor instance, in the context of recommendation systems, preferences of users may change\nover time due to changing fashions and trends.\nAs the initial step towards addressing nonstationary bandits, multiple prior works have\nfocused on PS-MABs (Kocsis and Szepesv\u00e1ri, 2006) in which the reward distributions as-\nsociated with a subset of the arms change at specific change-points and remain stationary\nbetween change-points. The piecewise stationary model is good approximation for many\nreal-world scenarios (Auer, 2002; Seznec et al., 2020).\nFor environments that change at every time step gradually, if the nonstationarity (amount\nof change) at each time step stays strictly bounded away from zero no matter how large the\nhorizon, then the optimal regret (loss in cumulative rewards) is linear (Auer et al., 2019b).\nIn the piecewise stationary setting, it is possible to achieve sublinear regret as long as the\nnumber of change-points over a horizon grows sublinearly with the horizon.\nThere are two main approaches to achieving good performance in PS-MABs. The first\napproach is based on continuously adapting the bandit algorithm to the changing environ-\nment without restarting (Kocsis and Szepesv\u00e1ri, 2006; Garivier and Moulines, 2011). The\nsecond approach involves restarting an algorithm (designed for stationary environments) at\ncertain time steps based on some prior knowledge of the non-stationarity or through de-\ntecting changes in the environment. Peng and Papadimitriou (2024) recently demonstrated\nthat restarting the learning process in the general non-stationary reinforcement learning\nproblem can be more effective than continuous adaptation. This is because the complexity\nof the former is often comparable to or even better than the latter in the worst-case sce-\nnario. This, therefore, motivates our study and implementation of effective restarting-based\nsolutions in the PS-MAB problem.\nRestarting-based approaches fall into two categories: those that follow a predetermined\nrestarting schedule based on knowledge of the non-stationarity (see, e.g., Besbes et al.\n(2014)), and those that trigger restarts by actively detecting non-stationarity (see, e.g.,\nAuer et al. (2019b); Besson et al. (2022)). Algorithms that we refer to as Change Detection-\nbased Bandit (CDB) procedures, employ the second approach by utilizing quickest change\ndetection (QCD) tests (Auer et al., 2002; Liu et al., 2018; Cao et al., 2019; Besson et al.,\n2022). To the best of our knowledge, the General Likelihood Ratio-Kullback Leiber Upper\nConfidence Bound (GLR-klUCB) algorithm proposed by Besson et al. (2022) is the state-\nof-the-art CDB procedure, which, unlike some of the other CDB procedures, does not\nrequire prior knowledge about the changes, other than that a certain condition on the\nseparation between change-points is met (Besson et al., 2022, Assumption 4). In the recent\nwork of Gerogiannis et al. (2024), an empirical comparison was conducted between CDB"}, {"title": "2 Piecewise Stationary Sub-Gaussian Bandits and CDB Procedures", "content": "2.1 Problem Formulation\nA PS-MAB consists of $A \\in \\mathbb{N}$ arms. At each time step (round) $t \\in \\mathbb{N}$, the agent pulls\narm $A_t$ and obtains reward $X_{A_t,t}$. The agent employs a policy $\\{\\tau_t\\}_{t=1}^\\infty$ adapted to the\nfiltration generated by the history of actions and observations up to that point, i.e., $A_t =$\n$\\tau_t(A_1, X_{A_1,1},..., A_{t-1}, X_{A_{t-1}})$. The goal of the agent is to acquire the maximum total accu-\nmulated reward over horizon $T$. We assume that the rewards $\\{X_{a,t} : a \\in \\{1, ..., A\\}, t \\in \\mathbb{N}\\}$\nare mutually independent and $\\sigma^2$-sub-Gaussian$^1$ for some $\\sigma > 0$ known to the agent. The\npiecewise stationarity of the MAB implies that the reward distributions of the arms remain\nthe same between consecutive change-points, and at each change-point, the reward distri-\nbutions of one or more arms are abruptly altered (while still remaining $\\sigma^2$-sub-Gaussian).\nThe details are as follows:\n\\begin{itemize}\n    \\item Let $N_T$ be the number of change-points over a finite horizon $T \\in \\mathbb{N}$. For $k \\in$\n    $\\{1,..., N_T\\}$, let $\\nu_k$ be the $k^{th}$ change-point, and define $\\nu_0 := 1$ and $\\nu_{N_T+1} = T + 1$.\n    In addition, we refer to the time steps $\\{\\nu_{k-1},..., \\nu_k - 1\\}$ as the $k^{th}$ interval.\n    \\item For each $a \\in \\{1, ..., A\\}$ and $k \\in \\{1, ..., N_T\\}$, the rewards $(X_{a,t})^{\\nu_k-1}_{t=\\nu_{k-1}}$ are i.i.d., with\n    the common mean denoted by $\\mu_{a,k}$\n    \\item Once the change-point $\\nu_k$ occurs, arm $a$ experiences a mean change of the magnitude\n    $|\\mu_{a,k+1} - \\mu_{a,k}|$, with $\\underset{a}{\\text{max }} |\\mu_{a,k+1} - \\mu_{a,k}| > 0$.\n    \\item Let $a_k := \\underset{a \\in \\{1,..,A\\}}{\\text{argmax}} \\mu_{a,k}$ be the optimal arm during the $k^{th}$ interval for each\n    $k \\in \\{1, ..., N_T\\}$.\n    \\item Define $\\Delta_{a,k} := \\mu_{a_k,k} - \\mu_{a,k}$ to be the suboptimality gap of arm $a \\in \\{1, ..., A\\}$ during\n    the $k^{th}$ interval.\n\\end{itemize}\nThe (dynamic) regret $R_T$ of policy $\\pi$ over a finite horizon $T$ can be defined as follows:\n$$R_T := \\mathbb{E} \\Big[\\sum_{k=1}^{N_T+1} \\sum_{t=\\nu_{k-1}}^{\\nu_k - 1} \\Delta_{A_t,k} \\Big] \\quad (1)$$\nThe goal of the agent is to choose a policy to minimize the regret in (1).\nWe use the following additional notations in our regret analysis:\n\\begin{itemize}\n    \\item Without loss of generality, we assume that the suboptimality gap is upper bounded\n    by some constant $C$, i.e., $\\Delta_{a,k} < C$ for all $a \\in \\{1, ..., A\\}$ and $k \\in \\mathbb{N}$.\n    \\item We also define $\\Delta_{c,k} = \\underset{a \\in \\{1,...,A\\}}{\\text{max}}|\\mu_{a,k+1} - \\mu_{a,k}|$ to be the change gap at the $k^{th}$\n    change-point. Furthermore, we use $\\Delta_{\\text{change}}$ to denote the smallest change gap, i.e.,\n    $\\Delta_{c,k} > \\Delta_{\\text{change}}$ for all $k \\in \\mathbb{N}$.\n\\end{itemize}"}, {"title": "3 Modular Regret Analysis of CDB Learning Procedures", "content": "As discussed in Section 2.2, CDB procedures proposed in prior works all combine a sta-\ntionary bandit algorithm $B$ with a change detector $D$. It is desirable to develop a modular\nanalysis that unifies the evaluation of these CDB procedures by integrating the analyses of\nthe two components. However, to the best of our knowledge, no such modular framework\nhas been proposed so far. The main objective in this work is to modularize the regret anal-\nysis of CDB procedures The modular analysis can then be applied seamlessly to various"}, {"title": "3.1 Requirements for Stationary Bandit Algorithms and CD procedures", "content": "To minimize the regret, a CDB procedure incorporates a stationary bandit algorithm $B$ with\nlow regret on stationary bandits and a change detector $D$ that detects a change quickly while\nnot raising false alarms too often. Therefore, it is reasonable to assume that $B$ satisfies the\nfollowing property:\nProperty 1 (stationary bandit algorithm regret). For the stationary bandit algorithm $B$\non a fully stationary bandit with $A$ arms and suboptimality gaps $\\{\\Delta_{a}\\}_{a=1}^A$ where $\\Delta_{a} \\le C$\nfor any $a \\in \\{1, ..., A\\}$, its regret upper bound over $T$ rounds is expressed as $R_B(T)$, which\nis concave and increases sublinearly with $T$.\nThis property holds for many well-known bandit algorithms. We emphasize that the\nregret bound in Property 1 can be instance-independent, meaning that $R_B(T)$ does not\nhave to depend on the suboptimality gaps. For example, the regret upper bound for UCB,\nwhich is independent of the suboptimality gaps, is $8 \\sqrt{AT \\log{(T)}} + O(1)$ (Lattimore and\nSzepesv\u00e1ri, 2020, Theorem 7.2) and satisfies Property 1.\nAs for the change detector $D$, the goal is to detect changes as soon as possible while\nnot raising false alarms too often over the horizon. Taking cues from the regret analysis in\nBesson et al. (2022), if the change detector gets falsely triggered or if it detects a change too\nlate, the samples for detecting the next change-point will be insufficient, making the change\ndetector unable to detect the next change. When any of the changes remain undetected\nover the entire horizon, which is defined as a missed detection event, the worst-case regret\nbound is linear (Besson et al., 2022). It is therefore essential to control the probability\nof missed detection. Because false alarm events and late detection events could possibly\nlead to missed detection events, we would like to ensure these events happen with a small\nprobability. In addition, since the suboptimal arms pulled during the detection delay will\nalso lead to linear regret, the threshold for detection delay, referred to as the latency, should\nbe small.\nBefore formally laying out the properties a good change detector should possess, we\nformulate the general QCD problem associated with our analysis: Let $\\{X_n : n \\in \\mathbb{N}\\}$ be a\nsequence of independent random variables observed sequentially by the detector. When the\nchange-point occurs at $\\nu \\in \\mathbb{N}$,\n$$X_n \\sim\n\\begin{cases}\nf_0, n < \\nu \\\\\nf_1, n \\ge \\nu.\n\\end{cases} \\quad (2)$$\nIn other words, before the change-point $\\nu$, the stochastic samples follow the pre-change dis-\ntribution with density $f_0$. The remaining samples follow the post-change distribution with\ndensity $f_1$. Additionally, $\\mathbb{P}_\\nu$ denotes the probability measure under which the change-point\noccurs at $\\nu \\in \\mathbb{N}$, while $\\mathbb{P}_\\infty$ denotes the probability measure under which there is no change-\npoint (i.e., $\\nu = \\infty$). We assume that the densities $f_0$ and $f_1$ are $\\sigma^2$-sub-Gaussian with mean\n$\\mu_0$ and $\\mu_1$, and let $\\Delta_c := |\\mu_0 - \\mu_1|$. We also assume that the change detector only knows\nthat the pre- and post- change distributions are $\\sigma^2$-sub-Gaussian and is agnostic to the"}, {"title": "3.2 Modularized Regret Analysis", "content": "As described in Property 2 in Section 2, a change detector needs at least $m$ pre-change\nsamples and $d$ post-change samples to detect changes quickly with high probability. For\nthe CDB procedures, however, each arm is not pulled every time step. With the forced\nexploration frequency $\\alpha_k$, the change detector is guaranteed to obtain one sample from\neach arm every $\\frac{A}{\\alpha_k}$ rounds. Then, we define the latency and the pre-change window for a\nCDB procedure at the $k^{th}$ change-point as:\n$$m_k := \\Big[\\frac{A}{\\alpha_k}\\Big] m(\\Delta_{\\text{change}}), \\quad (6)$$\n$$d_k := \\Big[\\frac{A}{\\alpha_k}\\Big] d(\\Delta_{\\text{change}}). \\quad (7)$$"}, {"title": "3.3 Application to Various Combinations of Change Detectors and Stationary Bandit Algorithms", "content": "Theorem 1 allows us to study regret upper bounds for CDB procedures that combine differ-\nent stationary bandit algorithms with different change detection algorithms. Consider any\nstationary bandit algorithm for which the regret satisfies Property 1 and scales with $T$ as\nat most $\\sqrt{T \\log{(T)}}$. Examples include UCB (Lattimore and Szepesv\u00e1ri, 2020) and klUCB\n(Capp\u00e9 et al., 2013), for which we have the following (instance-independent) regret upper\nbounds:\n$$R_{\\text{UCB}} (T) := 8\\sqrt{\\sigma^2 A T \\log{(T)}} + O (1), \\qquad R_{\\text{klUCB}} (T) := 2\\sqrt{2 \\sigma^2 A T \\log{(T)}} + O (1). \\quad (10)$$\nNext, we consider change detection procedures that satisfy Property 2. The first can-\ndidate we study is a generalized likelihood ratio (GLR) based QCD test designed for sub-\nGaussian observation statistics, which is similar to the GLR QCD test for sub-Bernoulli\nstatistics used in Besson et al. (2022). For any desirable false alarm probability $\\delta_F \\in (0,1)$,\ndefine\n$$\\beta (n, \\delta_F) := 6 \\log{(1 + \\log{(n)})} + \\frac{5}{2} \\log{\\Big( \\frac{4 n^{3/2}}{\\delta_F} \\Big)} + 11. \\quad (11)$$\nThe stopping time of the GLR test is given by\n$$\\tau := \\inf \\{n \\in \\mathbb{N} : G_n \\geq \\beta (n, \\delta_F) \\} \\quad (12)$$\nwhere the GLR statistics $G_n$ is\n$$G_n := \\underset{1 < s < n}{\\text{sup}} \\log{ \\Bigg( \\underset{\\text{sup}_{\\theta_0 \\in \\mathbb{R}}}{\\text{sup}_{\\theta_1 \\in \\mathbb{R}}} \\frac{\\prod_{i=1}^{s} f_{\\theta_0} (X_i) \\prod_{i=s+1}^{n} f_{\\theta_1} (X_i)}{\\prod_{i=1}^{n} f_{\\theta_0} (X_i)} \\Bigg)} \\quad (13)$$\nin which $f_{\\theta}$ is the density of a Gaussian random variable with mean $\\theta \\sigma^2$ and variance $\\sigma^2$.\nThe GLR test can be considered as a generalization of the CuSum test (Veeravalli and\nBanerjee, 2013). A well-known alternative to the CuSum test for QCD problems is Shiryaev-\nRoberts (SR) test (Veeravalli and Banerjee, 2013), and we can construct a generalization of\nthis test, which we call the generalized Shiryaev-Roberts (GSR) test, which is characterized\nby the stopping rule,\n$$\\tau := \\inf \\{n \\in \\mathbb{N} : \\log W_n \\geq \\beta (n, \\delta_F) + \\log n \\} \\quad (14)$$"}, {"title": "4 Experimental Study", "content": "In our experiments, we perform numerical simulations on synthetic data. These simulations\naim to compare the efficacy of our approach by combining different change detectors and\nstationary bandit algorithms. We also benchmark our proposed CDB procedures against\nrepresentative methods from prior works."}, {"title": "4.1 Experimental Benchmark", "content": "The model for the change-points that we assumed in our analysis is a deterministic model.\nHowever, it is not clear a priori how the change-points should be placed for the simula-\ntion of a realistic PS-MAB environment. One could try and find a worst-case placement\nof change-points, but such a worst-case placement may be different for different choices of\nthe magnitudes of the changes at the change-points, and for different procedures. There-\nfore, we design a PS-MAB environment with stochastic change-points, where the intervals\nbetween change-points are i.i.d. geometric random variables. Such a stochastic model natu-\nrally introduces variability in the placement of change-points, enabling a broader evaluation\nacross scenarios. This helps assess performance in environments where change-points oc-\ncur unpredictably as would be the case in practice. The geometric change-point model is\nstraightforward to implement, making it a practical tool for generating diverse experimental\nscenarios and testing robustness under varying conditions.\nSpecifically, we simulate environments where the number of arms $A = 5$, the horizon\n$T = 100000$, and the intervals between change points are i.i.d. geometric with parameter\n$\\rho$, where $p = T^{\\xi-1}$, for $\\xi \\in \\{0.7, 0.6, 0.5, 0.4, 0.3, 0.2\\}$. Note that the expected number of"}, {"title": "5 Conclusions", "content": "In this paper, we studied CDB procedures, a collection of procedures for piecewise-stationary\nMABs that meaningfully marry efficient stationary MAB algorithms with efficient change\ndetectors. While a good learning algorithm for the stationary variant should quickly learn\nan optimal arm and favor pulling said arm over time, detection of changes in the reward\nstructures in other arms requires pulling sub-optimal arms often enough (forced exploration)\nfor change detectors to function effectively. We formulated a QCD problem tailored to such"}]}