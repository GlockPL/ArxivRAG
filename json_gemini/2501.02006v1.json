{"title": "Multi-Task Semantic Communication With Graph Attention-Based Feature Correlation Extraction", "authors": ["Xi Yu", "Tiejun Lv", "Weicai Li", "Wei Ni", "Dusit Niyato", "Ekram Hossain"], "abstract": "Multi-task semantic communication can serve multiple learning tasks using a shared encoder model. Existing models have overlooked the intricate relationships between features extracted during an encoding process of tasks. This paper presents a new graph attention inter-block (GAI) module to the encoder/transmitter of a multi-task semantic communication system, which enriches the features for multiple tasks by embedding the intermediate outputs of encoding in the features, compared to the existing techniques. The key idea is that we interpret the outputs of the intermediate feature extraction blocks of the encoder as the nodes of a graph to capture the correlations of the intermediate features. Another important aspect is that we refine the node representation using a graph attention mechanism to extract the correlations and a multi-layer perceptron network to associate the node representations with different tasks. Consequently, the intermediate features are weighted and embedded into the features transmitted for executing multiple tasks at the receiver. Experiments demonstrate that the proposed model surpasses the most competitive and publicly available models by 11.4% on the CityScapes 2Task dataset and outperforms the established state-of-the-art by 3.97% on the NYU V2 3Task dataset, respectively, when the bandwidth ratio of the communication channel (i.e., compression level for transmission over the channel) is as constrained as $\\frac{1}{12}$", "sections": [{"title": "I. INTRODUCTION", "content": "In data-intensive and real-time applications like the Internet of Things (IoT), challenges arise from not only the sheer volume of data but also many complex tasks, e.g., localization, identification, image segmentation, and tracking. Transmitting the raw data can strain bandwidth, particularly in bandwidth-limited (e.g., wireless) systems. On the other hand, many tasks involve overlapping data and exhibit correlations [1]. In navigation and object detection applications, tasks, e.g., semantic segmentation and depth estimation, often operate on the same image data [2]. Semantic communication has emerged as a task-oriented approach that transmits only relevant semantics [3]\u2013[7]. By intelligently encoding and transmitting task-specific semantics, it reduces data volume and simplifies processing at the receivers. Typically focusing on a single task [8]\u2013[13], semantic communication is increasingly considered to accomplish multiple tasks at one go.\nAs a new paradigm, multi-task semantic communications serve multiple tasks using a unified model and joint training [14], [15]. A multi-task semantic communication system involves a single encoder and multiple decoders. Each decoder is associated with a specific task and can be deployed at different receivers. The encoder is responsible for generating feature representations for all tasks, while the decoders receive their respective feature representations and execute their tasks. Multi-task semantic communication is suitable for scenarios where the transmitter (e.g., unmanned aerial vehicles or satellites) can only afford limited hardware and computational complexity, but needs to transmit semantic features for multiple tasks, such as object detection and image segmentation.\nA key challenge faced by multi-task semantic communication is an effective design of the encoder architecture, especially under constrained channel conditions with limited bandwidths, so that the feature representations required for different tasks at different decoders can be generated effectively and simultaneously at the encoder. Existing studies have focused primarily on extracting features for tasks in isolation, and they have overlooked the potential benefits of utilizing the feature correlations between tasks. There is an opportunity to exploit the correlations and associated features to enrich the feature representations to be encoded and transmitted.\nProgress has been made on multi-task semantic communication [14]\u2013[17]. For instance, Tong et al. [16] proposed a task-oriented semantic communication model with an emphasis on reconstruction rate-distortion for image reconstruction and detection. Shen et al. [18] developed a text multi-task semantic communication system based on the bidirectional encoder representations from transformers (BERT) [19], where text semantics were extracted at the transmitter for text classification and regression tasks. The Semantic-based Multi-level Feature Extraction Model (SMFEM) [17] was designed to capture multi-layer features with several encoders to accomplish different tasks. Recently, the authors of [15] put forth a unified semantic communication system (U-DeepSC) using domain adaptation to tailor features for different tasks to reduce storage and training redundancy. The study in [14] further advanced U-DeepSC by designing a lightweight feature selection module, adapting to varying channel conditions."}, {"title": "A. Related Works", "content": "Task-oriented semantic communication systems prioritize the precise transfer of task information over the exact replication of the entire data or signal content [20]. Initial research focused on single-task semantic communication, e.g., images, voice, and text, with a dedicated pair of encoder and decoder designed for each task. For instance, Bourtsoulatze et al. [8] introduced Deep Joint Source-Channel Coding (DJSCC) for image transmission in wireless channels, utilizing CNNs to extract features. Xie et al. [9] proposed DeepSC, a semantic communication architecture using Transformers to extract text semantics. Considering speech, Shi et al. [10] proposed a comprehension-before-transfer framework with high semantic fidelity and verified its effectiveness in audio transmission. Moreover, Wu et al. [13] presented a Grad-CAM-based method for semantic transfer between tasks.\nMulti-task semantic communication has been increasingly considered to improve the utilization of computational and storage resources. For instance, the authors of [17] presented a coarse-to-fine architecture for image feature extraction and compression, enhancing reconstruction performance and adaptable to various tasks by modifying the decoding layers. In [16], the TOSC-SR scheme was developed by extending rate-distortion optimization theory to enhance the performance of multiple tasks while ensuring image reconstruction quality. The authors of [18] built a multi-task semantic communication system using BERT [19], which can handle text-related tasks by extracting and transmitting semantics from text at the transmitter. The authors of [21] introduced a scalable multi-task semantic communication system that dynamically adjusts the encoding rate based on the importance ranking of features for different tasks. The authors of [15] proposed a unified deep learning-based semantic communication system, which employs domain adaptation techniques at the transmitter to tailor the shared and private features for different tasks and adjusts decoding efficiency at the receiver. U-DeepSC [14] was further developed utilizing a vector-based dynamic scheme and a lightweight feature selection module to adapt to different tasks and channel conditions. It was trained for a single task within a shared encoder, rather than dealing with multiple tasks simultaneously.\nIn the context of computer vision (CV) and natural language processing (NLP), multi-task learning has been developed, which can be classified into two categories. The first category focuses on efficient feature sharing across tasks, e.g., Cross-Stitch [22], which linearly combines features from different task layers, and Sluice Network [23], which uses a gating strategy for flexible feature sharing. Other examples include MTAN [24], which links task-specific attention networks to a shared backbone, and NDDR-CNN [25], which introduces an asymmetric domain adaptation layer to manage object detection and semantic segmentation tasks concurrently. The second category comprises models emphasizing dynamic network selection. For instance, DEN [26] integrates a selector to designate subnetworks based on input data, and AdaShare [27] selects network blocks for tasks via a task-specific policy. In light of this, Dynashare [28] captures input characteristics for encoding path selection.\nThese existing models [14], [16]\u2013[18], [22]\u2013[28] still face constraints in deploying flexible and effective encoding strategies tailored to different tasks. Accurately extracting semantic features needed for the execution of multiple tasks is of paramount importance, particularly when operating under restricted communication bandwidth. Moreover, none of the existing models has attempted to capture the correlations among features extracted in the encoders."}, {"title": "B. Contribution", "content": "This paper presents a new graph attention inter-block (GAI) module to the encoder/transmitter of multi-task semantic communication to enrich features captured and delivered over constrained communication channels, thereby enhancing the completion of multiple tasks simultaneously. The key novelty is that we interpret the output of each feature extraction block of the encoder as a node, and then build a graph to effectively reveal the correlations among the extracted intermediate features. Another important advancement is that we update the node representation using a graph attention mechanism to capture the correlations among the intermediate features. The connections among the intermediate features and the tasks are adaptively adjusted to meet specific task requirements by generating task-node weights through a multi-layer perceptual network. A task-node weight is a tensor with the same size as the output channel, accounting for the contribution of the corresponding feature to a task.\nThe contributions of this paper are summarized as follows:\n\u2022 We develop a new GAI module to capture the correlation among the intermediate features extracted from different feature extraction blocks of the encoder of a multi-task semantic communication system to improve the utilization of each encoding block and enrich the representations of the features delivered.\n\u2022 We interpret the intermediate features extracted at different feature extraction blocks of the encoder as nodes on a graph. The intermediate features are unified by a Feature Transformation Layer into a standardized node representation. Their correlations are captured iteratively and enhanced by employing a Graph Attention Layer in the GAI module.\n\u2022 We design a Relation Mapping Layer in the GAI module to generate task-node weights by feeding the node representations generated by the Feature Transformation Layer into task-specific multi-layer perceptual networks to satisfy task-specific requirements.\nExtensive experiments demonstrate that when the communication channel is efficient (e.g., with a bandwidth ratio of 1.3), the proposed GAI notably improves average accuracy across all tasks. It surpasses the leading baseline by 2.71% on the CityScapes 2Task dataset; outperforms the respective leading baselines by at least 0.87% and 0.80% on the NYU v2 dataset for two-task and three-task semantic communications, respectively; by 9.47% on the TaskonomyTiny 5Task dataset;"}, {"title": "II. SYSTEM STRUCTURE, TASK MODEL, AND USE CASE", "content": "The considered multi-task semantic communication system trains an end-to-end multi-task learning model over a bandwidth-limited wireless channel. The system consists of a transmitter with an encoder and a receiver with multiple decoders at both ends of the channel; see Fig. 1. Unlike traditional communication systems with the primary objective of preserving data fidelity by performing the source and channel coding separately, the encoder here extracts and encodes task-relevant features from the input data. Upon receiving the encoded data, the decoders process the encoded features directly to accomplish multiple tasks."}, {"title": "A. Task Model", "content": "Given the input (image) data $x \\in \\mathbb{R}^{C_{in}\\times W_{in}\\times H_{in}}$ to the encoder, multiple complex tasks need to be completed at the decoder in the receiver. $C_{in}$, $W_{in}$, and $H_{in}$ represent the channel number, width, and height of the input data, respectively. The decoding output of the t-th task is $\\hat{y}_{t}$, t = 1,\u2026,T, with T being the number of tasks. In the case of semantic segmentation, $\\hat{y}_{t}$ indicates the decoded pixel category. In the case of depth prediction, $\\hat{y}_{t}$ denotes the model\u2019s decoded depth value. In the case of surface normal estimation, $\\hat{y}_{t}$ provides the estimated vectors of the surface normal for each pixel."}, {"title": "B. Encoder and Decoder", "content": "Our primary focus is on the encoder\u2019s design on the transmitter side. We propose to enhance the feature maps to be transmitted by capturing their correlations so that the decoders at the receiver side can achieve improved task performances without modifications to the decoders.\n1) Encoder: The encoder performs joint source-channel coding on the input data $x \\in \\mathbb{R}^{C_{in}\\times W_{in}\\times H_{in}}$, which extracts and encodes the features required for multiple tasks at the decoder. The output of the encoder is denoted by $z \\in \\mathbb{R}^{C_{out}\\times W_{out}\\times H_{out}}$, where $C_{out}$, $W_{out}$, and $H_{out}$ are the channel number, width, and height of the output, respectively.\nThe encoding process can be characterized as a conditional distribution, as given by\n$P_{\\varphi}(z|x)$,\nwhere $\\varphi$ denotes the model parameters of the encoder. After encoding, z is transmitted through the channel to the receiver. Considering the hardware complexity of the transmitter, we employ a shared encoder that employs a ResNet architecture with multiple, sequentially connected residual blocks.\n2) Decoder: At the receiver, there are T task-specific joint source-channel decoders, each specialized for a specific task. Each decoder, i.e., the t-th decoder, is responsible for decoding the received signal, denoted by $\\hat{z}$, based on the requirements of the corresponding task t, generating the result $\\hat{y}_{t}$. Every decoder consists of several convolutional layers, each meticulously designed to handle the specificity of the corresponding task. The following conditional probability distribution can represent the operation of the t-th decoder:\n$P_{\\theta_{t}}(\\hat{y}_{t}|\\hat{z}), t = 1,\u2026\u2026,T,$\n(1)\nwhere the model parameters $\\theta_{t}$ are tailored for task t. The decoders operate independently. Their parameters $\\theta_{t}$ are tuned according to the specific requirements of each task t."}, {"title": "C. Channel Model", "content": "The encoder/transmitter and the decoder/receiver are connected by a bandwidth-limited and noisy channel. The input and output of the channel are z and $\\hat{z}$, respectively. The transmission in the channel process can be characterized by a conditional probability, as given by\n$P_{ch}(\\hat{z}|z)$.\nWithout loss of generality, we use additive white Gaussian processes to model the channel\u00b9, i.e.,\n$\\hat{z} = z + \\eta,$\n(2)\nwhere $\\eta \\in \\mathbb{R}^{C_{out} \\times H_{out} \\times W_{out}}$ collects the receiver noise with each element sampled from $\\mathcal{N}(0, \\sigma^{2})$. Here, $\\mathcal{N}(0, \\sigma^{2})$ denotes the zero-mean Gaussian distribution with variance $\\sigma^{2}$. At the receiver, the SNR of the received signal is given by\nSNR (dB) = 10 log ($P/\\sigma^{2}$),\n(3)\nwhere P is the transmit power of the encoded signal z. Let n denote the input data size (in pixels). Then, $n = C_{in} W_{in} \\times H_{in}$. Also, let k denote the size (in pixels) of the input to the channel. Then,\n$k = C_{out}\\times W_{out} \\times H_{out}/2,$\n(4)\nwhere the denominator \"2\" accounts for the transmission of the complex latent vectors in the channel. As a result of encoding, the dimension of the input feature is transformed as\n$\\begin{aligned}\nW_{out} &= \\frac{W_{in}}{\\alpha} \\\\\nH_{out} &= \\frac{H_{in}}{\\alpha},\n\\end{aligned}$\n(5)\nwhere \u03b1 is known as the reduction coefficient for the height and width of the input feature during encoding. \u03b1 depends on the stride settings in the convolutional operations in the ResNet architecture. As a result, the widths and heights of the features extracted at the residual blocks decrease progressively from the input to the output of the encoder. With reference to [8], [29], [30], the bandwidth ratio of the channel, denoted by R, is $R = \\frac{n}{k}$, which indicates the level to which the extracted features need to be compressed for transmissions over the channel with limited bandwidth.\n\u00b9The technique developed in this paper can be readily extended to other channel models, such as Rayleigh fading channels."}, {"title": "D. End-to-End Multi-Task Semantic Communication Model", "content": "The overall probability of accurately encoding and decoding the label $\\hat{y}_{t}$, t = 1,\u2026\u2026\u2026, T, is expressed as\n$p(\\hat{y}_{t} | x)=P_{\\theta_{t}}(\\hat{y}_{t} | \\hat{z}) \\cdot P_{ch}(\\hat{z} | z) \\cdot P_{\\varphi}(z | x), t=1, \\ldots, T.$\n(6)"}, {"title": "III. GRAPH ATTENTION INTER-BLOCK MULTI-TASK SEMANTIC COMMUNICATION", "content": "In this section, we delineate the new GAI module to implement the encoder for effective multi-task transmission and execution at the decoders. The GAI module interprets the output of each feature extraction residual block of the encoder as a node in a fully-connected graph. Then, the graph attention mechanism is employed to quantify the relationships among the nodes (or the feature extracted by the blocks). Correlations among features extracted at different residual blocks can be captured to enrich the features with an additional dimension reflecting the correlations across the outputs of the residual blocks. This differs from traditional methods, which linearly encode features through blocks and often inadvertently overlook the encoded details from the preceding blocks.\nThe GAI module comprises (a) a Feature Transformation Layer, which applies convolution and interpolation to normalize the sizes of features extracted at different blocks. (b) a Graph Attention Layer, which employs graph attention mechanisms to transform the outputs of the feature extraction blocks into nodes on a graph, and (c) a Relation Mapping Layer, which generates the task-node weights on the graph. These weights are utilized to fine-tune the importance of each feature in line with the specific requirements of each task. It is noted that the Feature Transformation and the Graph Attention Layers are task-agnostic, enabling generalization across various tasks. The Relation Mapping Layer is customized for each task, enhancing task-specific performance."}, {"title": "A. Feature Transformation Layer", "content": "The ResNet encoder comprises multiple residual blocks. Each block is an independent feature extraction encoder unit. Let N denote the number of such blocks. Their outputs are $F_{1},\u2026, F_{N}$ with $F_{i} \\in \\mathbb{R}^{C_{i} \\times W_{i} \\times H_{i}, i = 1,\u2026, N$ being the output of the i-th block. The Feature Transformation Layer unifies the latent features generated by these blocks into the initial standardized node representation in an N-vertex graph. The vertices of the graph retain the latent features from their corresponding residual blocks and are enhanced through the exchange of information with other nodes.\nThe objective of this layer is to convert latent features from various residual blocks into node representations with consistent dimensions. To accomplish this, we design a series of processes involving convolution layer transformation, pooling, and interpolation layer adjustments.\nThe convolution layer transforms the latent features with different channel sizes into the target output channel size while retaining critical spatial information, as given by\nConvLayer: $[C_{i}, W_{i}, H_{i}] \\rightarrow [C_{out}, W_{i}, H_{i}]$.\n(7)\nThis convolution layer involves multiple convolution kernels (filters), i.e., $Conv_{i}$ in Fig. 2, to progressively capture the local features of the input $F_{i}$. These filters scan over the input data, apply the convolution operation at each position, and generate a series of feature maps, denoted by $F^{'}$\u2208 $\\mathbb{R}^{C_{out} \\times W_{i} \\times H_{i}}$, \\forall i, to retain semantic features from the input:\n$F^{'} = Q_{i} * F_{i} + b_{i}, \\forall i,$\n(8)\nwhere $Q_{i}$ is the weights of the convolutional filters of the i-th block, $b_{i}$ is a bias of the i-th block, and * denotes the convolution operation. The feature maps, i.e., $F^{'}, \\forall i$, capture complex patterns and information, contributing to the subsequent generation of node representations.\nThe pooling operation further refines the feature dimension of each node, $F_{i}, \\forall i$, as given by\nPooling: $[C_{out}, W_{i}, H_{i}] \\rightarrow [C_{out}]$.\n(9)\nWith the convolution and pooling operations, we ensure the consistency in the output size of each feature extraction block when transforming $F_{i}$ through $F^{'}$ to an initial node representation $V \\in \\mathbb{R}^{C_{out} \\times 1}$, $\\forall i = 1,\u2026, N$.\nWe also incorporate an interpolation layer to ensure consistent dimension sizes during the final fusion of node representation. The interpolation layer adjusts the heights and widths of the features $F^{'}$ to unify their sizes. This is achieved by utilizing the weighted average of the nearest four-pixel values for resizing. It can preserve the fidelity of the original features while ensuring smooth and accurate size transitions. The interpolation process can be written as\nInterpolateLayer : $[C_{out}, W_{i}, H_{i}] \\rightarrow [C_{out}, W_{out}, H_{out}]$.\n(10)\nWe use $K_{i} \u2208 \\mathbb{R}^{C_{out} \\times W_{out} \\times H_{out}}$ to denote the interpolated version of $F^{'}$, $\\forall i = 1,\u2026\u2026, N."}, {"title": "B. Graph Attention Layer", "content": "The initial node representations $V_{i}, i = 1, . . ., N$, are input into the Graph Attention Layer, where they are iteratively updated by adjusting the relationships between the nodes. By leveraging the attention mechanism, the Graph Attention Layer evaluates the importance of neighboring nodes\u2019 features correlating each node, allowing the model to refine each node\u2019s representation by integrating relevant information from its connections. This process results in more accurate and task-specific node representations that can better capture the underlying graph structure.\nAs shown in Fig. 3, we employ Graph Attention Network (GAT) to tune the representations of each node. GAT stands out as a sophisticated structure tailored for analyzing interconnected graph data [31]. It assesses the interconnections within a graph by employing an attention mechanism, where the states of neighboring nodes are considered when updating a node\u2019s state. Moreover, GAT can capture multiple features to optimize node representations by utilizing multiple attention heads.\nIn the encoder, correlations exist among the features encoded by different feature-extracting residual blocks. The attention mechanism can dynamically assign importance to each neighbor, enabling the model to prioritize relevant information and thus refine the representations of a node. Leveraging the attention mechanism in GAT, we update the representation of each node and establish relationships with every other node. By further constructing the relationship between each node and the task, the updated node representation comprehensively integrates information from other nodes, resulting in more accurate task-node weights.\nThe node representations are updated through an iterative process involving information interactions between the nodes. Assume the node representations are updated for a total of M times. During the m-th iteration, with m = 1,\u2026, M, for nodes i and j, a learnable weight matrix $U^{m} \u2208 \\mathbb{R}^{C_{out} \\times C_{out}}$ and the LeakyReLU activation function are used to compute the attention coefficients, denoted by $a_{i,j}^{m}$. The Softmax function, softmax(.), is applied at each node to normalize the attention coefficients across all adjacent nodes, as given by\n$a_{i,j}^{m}= softmax_{j}(LeakyReLU (a [U^{m}V_{i}^{m-1} || U^{m}V_{j}^{m-1}])),$\n(11)\nwhere $a \u2208 \\mathbb{R}^{2C_{out}}$ is a learnable weight vector, and $V_{i}^{m-1} \u2208 \\mathbb{R}^{C_{out} \\times 1}$ is the representation of node i after the (m \u2013 1)-th iteration. (.) stands for transpose. (\u00b7||\u00b7) denotes concatenation. Then, $V_{i}^{m}$ is derived by calculating the weighted sum of the representations of node i and its neighboring nodes collected in the set H(i), as given by\n$V_{i}^{m} = ReLU(\\sum_{j\u2208H(i)}(P^{m} a_{i,j}^{m} V_{j}^{m-1})),$\n(12)\nwhere $P^{m} \u2208 \\mathbb{R}^{C_{out} \\times C_{out}}$ is a learnable weight matrix used for refining node representations. After M updates, the final node representation is obtained, i.e., $V_{1}^{M},\u2026,V_{N}^{M} \u2208 \\mathbb{R}^{C_{out} \\times 1}$.\nCompared to a simpler attention mechanism [32], the GAT benefits from the additional information explicitly modeled in the graph structure. In a simple attention network, each node i has an initial feature vector. The features are used to compute query, key, and value vectors for attention score calculation to capture the relationships based on node features without modeling the connections between the nodes. In contrast, the Graph Attention Layer exploits the graph structure to incorporate additional information during attention score calculation. Specifically, GAT uses the adjacency information to dynamically adjust the importance of neighboring nodes. By concatenating the features of connected nodes and applying a learnable attention vector, GAT captures richer non-linear relationships within the graph. The non-linearity introduced by the LeakyReLU function extends the capability of the model to discern complex patterns."}, {"title": "C. Relation Mapping Layer", "content": "The Relation Mapping Layer is specifically designed to establish connections among the encoding node representations, i.e., $V_{i}^{M}$, and various tasks, thereby maximizing the utilization of encoding feature from every block to each task. The Relation Mapping Layer consists of a multi-layer perceptual network incorporating linear and activation layers.\nTo tailor the encoding feature for diverse tasks, the final representation of each node i, i.e., $V_{i}^{M}$, is first passed through two linear networks, denoted by $Linear_{i1,t}$ and $Linear_{i2,t}$, with a ReLU activation function connecting them, generating task-node weights $e_{i,t}$ for each task t, as given by\n$e_{i,t} = Linear_{i2,t}(ReLU (Linear_{i1,t}(V_{i}^{M})),$\n(13)\nwhere $e_{i,t} \u2208 \\mathbb{R}^{C_{out}}$ stands for the weight of node i in regards of task t in the interpolated node representation $K_{i}$ to be transmitted, and hence, it has $C_{out}$ channels. Each task generates corresponding weights at each node, with a total of $T \\times N$ task-node weights, quantifying the importance and contribution of each encoding node in handling specific tasks.\nAfter calculating the task-node weights $e_{i,t}$, a fusion method is employed so that these weights are combined with the corresponding features $K_{i}$ to generate the feature for the execution of each task t, as given by\n$z_{t} = \\sum_{i=1}^{N}(e_{i,t} K_{i}), \\forall t = 1,\u2026 ,T,$\n(14)\nwhere $z_{t} \u2208 \\mathbb{R}^{C_{out} \\times W_{out} H_{out}}$ represents the final transmitted encoding feature specifically tailored for task t."}, {"title": "D. Objective Function", "content": "The primary objective of the considered multi-task semantic communication system is to minimize transmission loss across all tasks by optimizing the parameters of the designed encoder, denoted by $\\varphi$, and the decoder parameters, denoted by \u03b8. The parameter set $\\varphi$ includes the parameters of the encoder and the GAI module. The parameter set \u03b8 comprises of parameters $\\theta_{1},\u2026\u2026,\\theta_{T}$ for the T task-specific decoders. Each decoder consists of three convolutional layers.\nThe total loss of the multi-task semantic communication framework can be written as the following weighted sum of the losses of the tasks:\n$\\mathcal{L} (\\theta, \\varphi)=\\sum_{i=1}^{T} w_{t} L_{t}$\n(15)\nwhere $L_{t}$ denotes the loss for task t, and $w_{t}$ is the corresponding weight. {$w_{1},...,w_{T}$} are configured to ensure that the loss for each task is of the same magnitude. The optimization problem is thus formulated as\n$\\min_{\\theta, \\varphi} \\mathcal{L} (\\theta, \\varphi)$.\n(16)\nOptimizing these parameters is crucial for reducing the total loss across all tasks, which, in turn, enhances the overall efficacy of the multi-task learning system. For example, the loss functions for different tasks, $L_{t}, t = 1,...,T$, can be defined as follows:\n1) Semantic Segmentation Loss: Semantic segmentation involves classifying each pixel in an image into a predefined category. The loss function can be defined as\n$L_{seg} = -y_{gt}.log(p(\\hat{y})),$\n(17)\nwhere $y_{gt}$ is the true label and $p(\\hat{y})$ is the predicted category probability for a pixel.\n2) Loss for Depth Prediction, Keypoint Detection, and Edge Detection: Depth prediction and keypoint detection, and edge detection tasks employ a similar loss function to quantify the difference between predictions and ground-truth values. The unified loss function is given by\n$L_{dep/key/edg} = |\\hat{y} - y_{gt}|,$\n(18)\nwhere $\\hat{y}$ is the predicted value and $y_{gt}$ is the corresponding ground-truth. In the context of depth prediction, $\\hat{y}$ and $y_{gt}$ are the predicted and ground-truth depth values, respectively, aiming to estimate the distance of each object in an image from the viewpoint. For keypoint detection, $\\hat{y}$ and $y_{gt}$ correspond to the predicted and ground-truth keypoint values, with the goal of identifying points of interest in an image. For edge detection, $\\hat{y}$ and $y_{gt}$ are the predicted and ground-truth edge values, respectively.\n3) Surface Normal Estimation Loss: Surface normal estimation involves calculating the angle of surfaces relative to the viewer. The loss function is given by\n$L_{sn} = 1- \\frac{\\hat{y} y_{gt}}{\\|\\hat{y}\\|\\|y_{gt}\\|},$\n(19)\nwhere $\\hat{y}$ is the predicted normal and $y_{gt}$ is the true normal, both of which are normalized to unit length."}, {"title": "E. Workflow and Complexity Analysis of GAI", "content": "Algorithm 1 summarizes the proposed GAI algorithm, which commences with feeding data into the encoder; see Section II-B1. Each encoding block in the encoder produces an encoding feature. The features are fed to the Feature Transformation Layer, which adjusts the channel from each block to a unified size and refines these adjusted features into initial node representations; see Section III-A. The spatial dimensions of features are unified by interpolation. Next, the Graph Attention Layer updates the node representations using a GAT, where the attention coefficients are computed between node pairs to obtain the correlations; see Section III-B. Then, the updated node representations are processed to generate task-node weights in the Relation Mapping Layer, where the weights are combined with block features to produce features to be transmitted; see Section III-C. The receiver decodes the received encoding features to execute specific tasks, as described in Section II-B2.\nTo analyze the computational complexity of Algorithm 1, we employ ResNet-18 as the basic encoder (which is also empirically tested in Section IV). In ResNet-18, there are eight blocks. The first six blocks require convolution operations to align their channel sizes with the final output dimension $C_{out}$, as the last two blocks inherently match this size.\nIn the Feature Transformation Layer, the convolution computation for unifying the channel size of the first six nodes into the final size $C_{out}$ is $O(\\sum_{i=1}^{6} C_{out} H_{i}W_{i}(C_{i}+1))$ [33]. Similarly, interpolating nodes with varying widths and heights into a unified size incurs a complexity of $O (9NC_{out} H_{out} W_{out})$, accounting for two rounds of interpolations on the values of each pixel\u2019s four nearest neighboring points, including a total of nine operations (six multiplications and three additions).\nIn the GAT process, the computational complexity for the two learnable representation transformations $P^{m} \u2208 \\mathbb{R}^{C_{out} \\times C_{out}}$ and $U^{m} \u2208 \\mathbb{R}^{C_{out} \\times C_{out}}$ is $O(NC_{out}^{2})$ each, since in this process each output element is the sum of the products of every element of the input tensor and the corresponding weight. Therefore, for each output element, $C_{out}$ multiplications are required. Furthermore, calculating attention between the nodes incurs a computational complexity of $O(N(N - 1)C_{out})$, as each node requires the computation of attention values with all other nodes. Multiplying the obtained attention weights with each node results in $O (N^{2}C_{out})$, as each node needs to update its features using the attention weights obtained from other nodes.\nIn the Relation Mapping Layer, the computational complexity of generating linear weights is $O(2NTC_{out}C_{rm})$, where $C_{rm}$ is the channel size for relation mapping. Moreover, for each of the N nodes and T tasks, two linear layers with a weight size of $C_{out} \u00d7 C_{rm}$ are employed, each requiring $C_{rm}$ multiplications per output element. The complexity of linearly weighting a node is $O(NTC_{out} H_{out} W_{out})$, and the complexity of summing the weights of all tasks and nodes is $O((N \u2212 1)TC_{out} H_{out}W_{out})$, since these operations involve aggregating N weighted block features across $H_{out}$ and $W_{out}$ spatial dimensions. As a result, the overall computational complexity of the Relation Mapping Layer is $O(NTC_{out}(C_{rm} + H_{out}W_{out}))$, which scales linearly with the number of tasks T.\nGiven that $C_{out}$ significantly outweighs N, T, $H_{out}$, and $W_{out}$, the overall computational complexity of Algorithm 1 is $O(NC_{out}^{3})$. The complexity is relatively low, scales linearly with the number of encoding blocks N at the encoder, and is little affected by the number of tasks T."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "In this section, we conduct experiments on various datasets to assess the efficacy of our proposed GAI module. For an input data sample (e.g., an image and text), the encoder generates features for multiple simultaneous tasks on the data sample (e.g., on the image), such as semantic segmentation, surface normal estimation, depth prediction, keypoint detection, and edge detection; and then transmits the features to the decoders for accomplishing the tasks simultaneously."}, {"title": "A. Experimental Setup", "content": "1) Datasets: We validate our experiments using five widely recognized public datasets: CityScapes 2Task [34", "35": "TaskonomyTiny 5Tasks [36", "37": "and the MVSA dataset [38", "34": "The CityScapes 2Task dataset offers a diverse collection of urban scenes specifically curated for semantic segmentation and depth prediction tasks. Regarding semantic segmentation", "metrics": "mean Intersection over Union (mIoU) and pixel accuracy (Pixel Acc), both desirable when higher. mIoU measures the mean ratio of the intersection to the union of the predicted and actual areas of a category. Pixel accuracy is the ratio of correctly classified pixels to the total number of pixels in the image. For the depth prediction tasks, the evaluation involves absolute error (Abs), relative error (Rel), and threshold accuracy (\u03b4). Abs represents the direct difference between the predicted and actual values, while Rel is the proportion of this difference to the actual value. Another metric in depth prediction is quantified using \u03b4, which measures the relative difference between the predicted depth value d and the actual depth value dgt"}]}