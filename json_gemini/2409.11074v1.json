{"title": "RoMath: A Mathematical Reasoning Benchmark in Romanian", "authors": ["Adrian Cosma", "Ana-Maria Bucur", "Emilian Radoi"], "abstract": "Mathematics has long been conveyed through natural language, primarily for human understanding. With the rise of mechanized mathematics and proof assistants, there is a growing need to understand informal mathematical text, yet most existing benchmarks focus solely on English, overlooking other languages. This paper introduces RoMath, a Romanian mathematical reasoning benchmark suite comprising three datasets: RoMath-Baccalaureate, RoMath-Competitions and RoMath-Synthetic, which cover a range of mathematical domains and difficulty levels, aiming to improve non-English language models and promote multilingual AI development. By focusing on Romanian, a low-resource language with unique linguistic features, RoMath addresses the limitations of Anglo-centric models and emphasizes the need for dedicated resources beyond simple automatic translation. We benchmark several open-weight language models, highlighting the importance of creating resources for underrepresented languages. We make the code and dataset available.", "sections": [{"title": "1 Introduction", "content": "Mathematics has been a central intellectual preoccupation to humans since the beginning of civilization, the first mathematical writings dating back approximately 4000 years (Friberg, 1981). Historically and in the present, mathematics has been mostly written, spoken and taught in natural language, albeit with its own specialized vocabulary, having strict formalism only sparsely introduced between free-text explanations and reasoning. The primary audience of mathematical reasoning is other humans, not computers. The natural language of mathematics contains a mix of formulas, symbols, neologisms, jargon and words with different meanings than their common meaning (e.g., 'real' / 'imaginary' numbers). Recently, there has been a surging interest in the mechanization of mathematics (Beeson, 2004) in which all mathematical statements are precisely formalized into a set of rules enabling the use of proof-assistants and automatic proof verification (Li et al., 2024; de Moura et al., 2015). Mathematics implies rigor and precise reasoning, qualitatively different from general NLP. However, there is a pressing need to process and understand the existing large amount of mathematical text written in natural language for automatic proof-generation, verification and translation from informal, sketch-level proofs to formal languages.\nRecently, Large Language Models (LLMs) have shown great promise in handling a multitude of natural language tasks, including tackling simple mathematical reasoning problems (Ahn et al., 2024; Yue et al., 2023; Azerbayev et al., 2024; Shao et al., 2024). Out of the common benchmark suite for evaluating LLMs, datasets such as GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) remained challenging, even for the larger, proprietary models (Arora et al., 2023). More substantial progress has been made by the development of neuro-symbolic algorithms (Yang et al., 2024b; Polu and Sutskever, 2020) such as AlphaGeometry (Trinh et al., 2024) and AlphaProof (Gowers et al., 2024). However, there is no equivalent in other languages than English.\nCurrent mathematics benchmarks and datasets have focused solely on English, mostly disregarding other low-resourced languages. However, mathematical reasoning ability is independent of the underlying language (Rescorla, 2024). This phenomenon is a symptom of the current Anglo-centric development of AI, in which present models and"}, {"title": "2 Related Work", "content": "In this section, we briefly survey the existing datasets for mathematical reasoning. While interest in representation learning of mathematical expressions and text has existed in the past (Peng et al., 2021; Collard et al., 2022), with the recent success of Large Language Models in a wide range of tasks, increased attention has been given to training and evaluating mathematical reasoning of LLMs. For pretraining, the general approach is to filter Common Crawl web pages and PDFs to obtain high quality math tokens. For instance, datasets such as MathWebPages (Lewkowycz et al., 2022), Proof-Pile (Azerbayev et al., 2023a) and OpenWebMath (Paster et al., 2023) are used to pretrain high performing LLMs specialized in math such as Minerva (Lewkowycz et al., 2022) and LLema (Azerbayev et al., 2023b).\nFor benchmarks, the most popular dataset is GSM8K (Cobbe et al., 2021), containing middle-school Math Word Problems (MWPs). An improved variant that contains process supervision (supervision at each intermediary reasoning step) is PRM800K (Lightman et al., 2023). However, these datasets are regarded as too simple to prove advanced mathematical reasoning. Consequently, MATH (Hendrycks et al., 2021) is a comparatively more difficult dataset, containing high-school problems from domains such as calculus, linear algebra, geometry and number theory. MathVISTA (Lu et al., 2024) is another similar benchmark, that contains mathematical reasoning in visual contexts (e.g., plots, natural images, functions).\nSome approaches leverage pretrained LLMs to further augment existing datasets and synthesize new problems and solutions. In MetaMathQA (Yu et al., 2023), the authors use LLMs for rewriting mathematical questions from multiple perspectives to improve accuracy by increasing dataset diversity. OpenMATHInstruct (Toshniwal et al., 2024) is comprised of different synthesized code-interpreter solutions for GSM8K and MATH, using an LLM. Similarly, LILA (Mishra et al., 2022) is a unified benchmark of mathematical reasoning, with tasks collected from 20 datasets (including GSM8k and MATH) and solutions comprised of synthesized and expertly annotated Python programs.\nAside from simple word problems (Cobbe et al., 2021) and datasets focused on QA-type problems, more difficult competition-level benchmarks have been proposed. For instance, ARB (Sawada et al., 2023) is a dataset comprised of problems from math competitions and problems from specialized books, with special care taken to avoid data contamination. While it contains some proof-type problems, ARB only contains 105 problems. More recently, MathOdyssey (Fang et al., 2024) contains difficult high-school and university-level problems constructed for the Global Artificial Intelligence Championship Math 2024. Similar to ARB, it contains only 387 problems.\nRegarding datasets in languages other than English, there have been some efforts in Arabic with datasets such as ArMATH (Alghamdi et al., 2022) and Chinese with Ape210k (Zhao et al., 2020), Math23k (Ling et al., 2017), CMath (Wei et al., 2023). Otherwise, outside of (automatically) trans-"}, {"title": "3 Method", "content": "We describe below the process for collecting RoMath-Baccalaureate and RoMath-Competitions, the two subsets that are collected by crawling publicly available PDFs. RoMath-Synthetic is comprised of programmatically generated problems directly in Romanian."}, {"title": "3.1 Dataset Construction", "content": "In order to construct a high quality set of mathematical problems paired with solutions, we crawl publicly available PDFs from competitions and baccalaureate exam questions. By mining content from PDFs, we minimize the amount of data contamination (Jiang et al., 2024b) that would arise from dumps such as Common Crawl. Figure 1 showcases our approach. After collecting raw PDFs (separate documents for problem sets and their respective solutions), we utilize an academic document-focused OCR (i.e., MathPix (Mathpix, 2024)) to extract the underlying text and mathematical formulas / statements in LaTeX format. The final output is represented in Markdown format.\nTo parse the content, instead of relying on brittle handcrafted rules and regex expressions, we utilize a powerful commercial LLM (i.e., Claude 3 Sonnet (Anthropic, 2024)) to parse the raw text and to output structured JSON from unstructured Markdown. The LLM is provided with several examples of how to structure the final JSON (see Appendix A for the system prompt). The JSON output contains the LaTeX-formatted problem statement and its appropriate solution. Finally, we again utilize a commercial LLM to annotate the domain of the problem and to extract final answers for non-proof problems for easier evaluation (similar to Hendrycks et al. (2021), we enclose the final answer, if it exists, into a $\\boxed { }$ tag). If a problem contains multiple sub-problems, we ensure that each sub-problem is self-contained and that the solution does not rely heavily on previous sub-problems' solutions. Since problems are sourced from PDFs, data contamination from existing pretraining datasets is kept at a minimum."}, {"title": "3.2 RoMath Suite", "content": "RoMath is comprised of three subsets: RoMath-Baccalaureate, RoMath-Competitions and RoMath-Synthetic.\nRoMath-Baccalaureate is composed of problems and solutions from the Romanian Baccalaureate exam. The Romanian Baccalaureate is a"}, {"title": "3.3 Evaluation", "content": "Evaluating the correctness of a solution to a mathematics problem is still an open problem. Generally, there are two ways to rigorously evaluate solutions: (i) string comparison for solutions containing a single final answer (Hendrycks et al., 2021; Cobbe et al., 2021) and (ii) using a proof-checker for problems requiring proofs (Li et al., 2024). However, using a proof-checker is not always feasible as it requires the problems and solutions to already be formalized into the language of the proof-checker, an unrealistic requirement for mathematics written in natural language. For proof-type problems, where it is necessary to check for correctness at every reasoning step in natural language, there is no consensus on the evaluation procedure. However, more recent methods (Fang et al., 2024) have adopted"}, {"title": "4 Baselines and Results", "content": "In Table 3, we showcase the performance of multiple LLMs-as-judges on our programmatically generated dataset to estimate judge performance. We tested Qwen2 (Yang et al., 2024a) family of models, as well as the math-specialized variant Qwen2-Math-7B, deepseek-math (Shao et al., 2024), Phi-3 (Abdin et al., 2024), Llama3-70B (Dubey et al., 2024), Mathstral (Mistral AI, 2024), and Mixtral-8x7b (Jiang et al., 2024a). We tested system prompts both in Romanian and English (see Appendix A). For our dataset, we obtained that Qwen2-7B-Instruct prompted in English obtained the best overall results of 91% accuracy. Surprisingly, the math-specialized models severely underperformed. As such, unless otherwise specified, we used Qwen2-7B-Instruct prompted in English as a judge for the rest of the results."}, {"title": "4.2 Model Benchmark", "content": "We chose to benchmark several open-weight LLMs, as opposed to proprietary models, to make the benchmark reproducible and to avoid unnecessary inference costs. We evaluated instruct under 0-shot, 5-shot and fine-tuned models for Qwen2-7B, Phi-3, Meta-Llama-8B and math-specialized variants such as Qwen2-Math-7B, deepseek-math-7b, Mathstral-7b. We evaluated larger models under 0-shot and 5-shot settings: Meta-Llama-70B and Mixtral-8x7B. Furthermore, we also evaluated Romanian-specialized models trained with continual pretraining on Romanian tokens, but with no focus on math tokens: RoLlama3-8B and RoMistral-7b (Masala et al., 2024)."}, {"title": "4.3 Ablation Study: Translating Romanian text.", "content": "Translating domain-specific technical language from low-resourced languages into English is nontrivial (Cosma et al., 2024). For example, in the context of code generation, the word 'subsequence', defined as a monotonically increasing (but not necessarily contiguous) sequence of indices, would be naively translated into Romanian as 'subsecven\u021b\u0103', which implies contiguous sequence \u2013 a more appropriate translation would be 'sub\u0219ir' (i.e., 'subseries'). The use of LLMs for translation is not sufficiently reliable to be performed due to the high hallucination rate, absence of rigorous translation evaluations of models, and absence of a curated parallel training corpus of technical jargon. In this work, we used the NLLB (NLLB Team et al., 2022) family of models (600M, 1.3B, and 3.3B) to translate from Romanian to English the test sets for RoMath-Baccalaureate and RoMath-Competitions, as they have established numerical benchmarks on Romanian to English translation.\nDirectly translating the full problem statement and solution resulted in 'gibberish' translations due to the mathematical symbols present in the text. As such, we opted to keep the LaTeXdelimited section intact and only translate the surrounding natural language. While this approach might lose some of the larger context, we found it to be the only satisfactory approach. Still, the resulting translations contain unnatural English for-"}, {"title": "4.4 Ablation Study: Effect of the Judge Model", "content": "Upon manual inspection, it seems that some generated solutions for proof-type problems obtain the correct final result, but the intermediate steps are incorrect. In some cases, the judge model deemed these types of solutions as correct, whereas they are not. This highlights the need for a more formal evaluation system for proof verification. In Figure 4 we compared multiple judge models to gauge their effect on downstream performance. Based on Table 3, we used Qwen2-7B, Llama-70B and Mixtral-8x7b as judges and used them to evaluate the performance of the same Qwen2-7B, Llama-70B and Mixtral-8x7b. We chose the same judges"}, {"title": "5 Conclusions", "content": "In this paper, we proposed RoMath, a benchmarking suite consisting of three datasets with mathematical problems written in Romanian: RoMath-Baccalaureate, RoMath-Competitions and RoMath-Synthetic. We detailed the construction process and composition for each subset and benchmarked several open-weight LLMs (Romanian models, general-purpose English models and math-specialized models) under 0-shot, 5-shot and fine-tuned scenarios. Our work is the first to provide quantitative results for mathematical reasoning in Romanian. Furthermore, we evaluated the effect of translating the problem statements in English and showed that translation artifacts severely reduce performance compared to 0-shot performance directly in Romanian. Our analysis of judge models for evaluation reveals that properly evaluating mathematical solutions in natural language is still an open problem and requires further research. Surprisingly, we found that mathematics problems written in Romanian can be properly handled by English-centric models (such as deepseek-math-7b (Shao et al., 2024)), outputting proper solutions in Romanian. It is unclear why this occurs, especially since such models are not explicitly trained on Romanian math tokens and most models have"}, {"title": "6 Limitations", "content": "The main limitation of this work is the use of an external LLM as a judge to estimate solution correctness, which might skew the results, artificially inflate performance. While this is an inherent limitation in literature for mathematics datasets that contain proofs, this is currently an open problem and there are on-going efforts to formalize proof verification (Gowers et al., 2024). Furthermore, we argued that the proper way to evaluate solutions of generated proofs is by using an external proof verification tool such as Lean (de Moura et al., 2015)."}, {"title": "A Appendix", "content": "Given the following mathematics problems in Romanian formatted in MathPix markdown, make a JSON with subject and solution pairs, removing unnecessary boilerplate and extra problem identifiers. The JSON must contain the full problem definition and subject number (e.g. subject 1b). Each sub-question must contain the whole problem definition for completeness. Each subject must be self-contained. Do not output anything else besides the required JSON. Do not modify the latex describing the mathematical formulas.\nExample (truncated): \"\nPROBLEMS:\nSe consider\u0103 matricea A = $\\begin{pmatrix} 1 & 1\\\\ 1 & 0 \\end{pmatrix}$ \u015fi \u015firul $(F_n)_{n\\geq 0}$ definit prin rela\u021bia de recuren\u021b\u0103 $F_{n+1} = F_n + F_{n-1}$, $n \\in N^*$, cu $F_0 = 0$, $F_1 = 1$.\na) S\u0103 se calculeze determinantul \u015fi rangul matricei A.\nb) S\u0103 se calculeze $F_2$ \u015fi $F_3$.\nSOLUTIONS:\na) det A = -1\u2260 0 \u21d2 rang A = 2; b) $F_2 = 1$, $F_3 = 2$.\n\"\nExample Output JSON:\n[{\n\"subject\": \"1a\",\n\"definition\": \"Se consider\u0103 matricea A = $\\begin{pmatrix} 1 & 1\\\\ 1 & 0 \\end{pmatrix}$ \u015fi \u015firul $(F_n)_{n\\geq 0}$ definit prin rela\u021bia de recuren\u021b\u0103 $F_{n+1} = F_n + F_{n-1}$, $n \\in N^*$, cu $F_0 = 0$, $F_1 = 1$. S\u0103 se calculeze determinantul \u015fi rangul matricei A.\",\n\"solution\": \"det A = -1 \u2260 0 \u21d2 rang A = 2\"\n},\n{\n\"subject\": \"1b\",\n\"definition\": \"Se consider\u0103 matricea A = $\\begin{pmatrix} 1 & 1\\\\ 1 & 0 \\end{pmatrix}$ \u015fi \u015firul $(F_n)_{n\\geq 0}$ definit prin rela\u021bia de recuren\u021b\u0103 $F_{n+1} = F_n + F_{n-1}$, $n \\in N^*$, cu $F_0 = 0$, $F_1 = 1$. S\u0103 se calculeze $F_2$ \u015fi $F_3$.\",\n\"solution\": \"$F_2 = 1$, $F_3 = 2$\"\n}\nIn this example, each sub-problem is self-contained and is paired with the appropriate solution. The sub-problem identifiers (e.g., \"a)\" and \"b)\") are stripped. The latex markdown is left intact.\nReal Input:"}]}