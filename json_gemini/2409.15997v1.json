{"title": "Improvements to SDXL in NovelAI Diffusion V3", "authors": ["Juan Ossa", "Eren Do\u011fan", "Alex Birch", "F. Johnson"], "abstract": "In this technical report, we document the changes we made to SDXL in the process\nof training NovelAI Diffusion V3, our state of the art anime image generation\nmodel.", "sections": [{"title": "1 Introduction", "content": "Diffusion based image generation models have been soaring in popularity recently, with a variety of\ndifferent model architectures being explored. One such model, Stable Diffusion, has achieved high\npopularity after being released as Open Source. Following up on it, Stability AI released SDXL, a\nlarger and extended version following its general architecture [23]. We chose SDXL as the basis\nfor our latest image model, NovelAI Diffusion V3, and made several enhancements to its training\npractices.\nThis technical report is structured as follows. In Section 2, we describe our enhancements in detail.\nFollowing that, we evaluate our contributions in Section 5. Finally, we draw conclusions in Section 6."}, {"title": "2 Enhancements", "content": "In this section, we present the enhancements we applied to SDXL to improve generation results."}, {"title": "2.1 v-Prediction Parameterization", "content": "We uptrained SDXL from e-prediction to v-prediction[26] parameterization. This was instrumental\nto our goal of supporting Zero Terminal SNR (see section 2.2). The e-prediction objective (\"where's\nthe noise?\") is trivial at SNR=0 (\"everything is noise\"), hence e loss fails to teach the model how\nto predict an image from pure noise. By comparison, v-prediction transitions from e-prediction to\nxo-prediction as appropriate, ensuring that neither high nor low SNR timesteps are trivially predicted.\nAs a secondary benefit, we sought to access the merits described in [9]: improved numerical stability,\nelimination of colour-shifting[30] at high resolutions, and faster convergence of sample quality."}, {"title": "2.2 Zero Terminal SNR", "content": "Stable-diffusion[25] and SDXL were trained with a flawed noise schedule[20], limiting image-\ngeneration to always produce samples with medium brightness. We aimed to remedy this.\nDiffusion models[10] such as these learn to reverse an information-destroying process (typically[2]\nthe application of Gaussian noise). A principled implementation of diffusion should employ a noise\nschedule that spans from pure-signal to pure-noise. Unfortunately SDXL's noise schedule stops short\nof pure-noise (fig. 1). This teaches the model a bad lesson: \"there is always some signal in the noise\".\nThis assumption is harmful at inference-time, where our starting point is pure-noise, and results in\nthe creation of non-prompt-relevant features (fig. 2)."}, {"title": "2.3 Sampling from High-Noise Timesteps Improves High-Resolution Generation", "content": "Curving the schedule to converge on $\\sigma = \\infty$ confers a ramp of higher sigmas along the way (fig. 5).\nThis helps to resolve another problem with SDXL: its $\\sigma_{max}$ is not high enough to destroy the low\nfrequencies of signal in high-resolution images. In fact $\\sigma_{max}$ was not increased since SD1[25],\ndespite the increase in target resolution. As resolution increases (or rather as the amount of redundant\nsignal increases increasing latent channels could be another way to increase redundancy): more\nnoise is required to achieve a comparable SNR (i.e. destroy an equivalent proportion of signal)[12].\nLarge features, such as close-to-camera limbs can lose coherence in SDXL's default regime of\n$\\sigma_{max} = 14.6$. Doubling $\\sigma_{max}$ (for example) recovers coherence (fig. 6). In practice we hit higher\nsigmas than this in our standard 28-step native schedule, which is a uniform linear spacing of\ntimesteps over the 1000-step ZTSNR schedule."}, {"title": "2.4 MinSNR", "content": "We used MinSNR[7] loss-weighting to treat diffusion as a multi-task learning problem, balancing\nthe learning of each timestep according to difficulty, and avoiding focusing too much training on\nlow-noise timesteps."}, {"title": "3 Dataset", "content": "Our dataset consisted of approximately 6 million images gathered from crowd-sourced platforms.\nIt was enriched with highly detailed, tag-based labels. The images are mostly illustrations in styles\ncommonly found in Japanese animation, games and pop-culture."}, {"title": "4 Training", "content": "We trained the model on our 256x H100 cluster for many epochs and roughly 75k H100 hours. We\nalso used a staged approach, with later stages consisting of more curated, higher quality data. We\ntrained in float32, with tf32[19] optimization. Our total compute budget was above that of the original\nSDXL training run, allowing us to thoroughly adapt the model to our data's distribution."}, {"title": "4.1 Aspect-Ratio Bucketing", "content": "As in previous NovelAI Diffusion models[1], we prepared like-aspect minibatches via aspect-ratio\nbucketing. This enabled us to frame images better than in a center-crop regime, and achieve better\ntoken-efficiency than in a padding regime. In sections 4.1.1 to 4.1.3, we will reiterate our approach\nhere for the sake of completeness."}, {"title": "4.1.1 Reasoning", "content": "Existing image generation models[25] are very prone to producing images with unnatural crops. This\nis due to training practicality: uniform batches are simple to implement. Often practitioners opt to\ntrain on square data, taking crops from the center of the image. This is not conducive to modeling\ntypical image data distributions, as most photos and artworks are not square.\nAs a consequence, humans are often generated without feet or heads, and swords consist of only\na blade with a hilt and point outside the frame. As we are creating an image generation model to\naccompany our storytelling experience, it is important for our model to be able to produce proper,\nuncropped characters, and generated knights should not be holding a metallic-looking straight line\nextending to infinity.\nAnother issue with training on cropped images is that it can lead to a mismatch between the text and\nthe image.\nFor example, an image with a \"crown\" tag will often no longer contain a crown after a center crop is\napplied and the monarch has been, thereby, decapitated.\nWe found that using random crops instead of center crops only slightly improves these issues.\nUsing Stable Diffusion with variable image sizes is possible, although it can be noticed that going too\nfar beyond the native resolution of 512 \u00d7 512 tends to introduce repeated image elements, and very\nlow resolutions produce indiscernible images. This is likely to be due to its lack of an explicit position\nembedding, resulting in a model which determines position from side-channels like convolution\npadding[14, 17]. Some training-free approaches seek to generalize inference to resolutions outside of\nthe training distribution via techniques such as convolution dilation [13, 8, 29] and attention entropy\nscaling [15, 4]. We chose instead to train on varied image sizes.\nTraining on single, variable sized samples would be trivial, but also extremely slow and more liable\nto training instability due to the lack of regularization provided by the use of mini batches."}, {"title": "4.1.2 Custom Batch Generation", "content": "As no existing solution for this problem seems to exist, we have implemented custom batch generation\ncode for our dataset that allows the creation of batches where every item in the batch has the same\nsize, but the image size of batches may differ.\nWe do this through a method we call aspect ratio bucketing. An alternative approach would be to use\na fixed image size, scale each image to fit within this fixed size and apply padding that is masked out\nduring training. Since this leads to unnecessary computation during training, we have not chosen to\nfollow this alternative approach.\nIn the following, we describe the original idea behind our custom batch generation scheme for aspect\nratio bucketing.\nFirst, we have to define which buckets we want to sort the images of our dataset into. For this purpose,\nwe define a maximum image size of 512 \u00d7 768 with a maximum dimension size of 1024. Since\nthe maximum image size is 512 \u00d7 768, which is larger than 512 \u00d7 512 and requires more VRAM,\nper-GPU batch size has to be lowered, which can be compensated through gradient accumulation.\nWe generate buckets by applying the following algorithm:\n\u2022 Set the width to 256.\n\u2022 While the width is less than or equal to 1024:\nFind the largest height such that height is less than or equal to 1024 and that width\nmultiplied by height is less than or equal to 512.768.\nAdd the resolution given by height and width as a bucket.\nIncrease the width by 64.\nThe same is repeated with width and height exchanged. Duplicated buckets are pruned from the, and\nan additional bucket sized 512 \u00d7 512 is added.\nNext, we assign images to their corresponding buckets. For this purpose, we first store the bucket\nresolutions in a NumPy array and calculate the aspect ratio of each resolution. For each image in\nthe dataset, we then retrieve its resolution and calculate the aspect ratio. The image aspect ratio is\nsubtracted from the array of bucket aspect ratios, allowing us to efficiently select the closest bucket\naccording to the absolute value of the difference between aspect ratios:\nimage_bucket = argmin(abs(bucket_aspects \u2013 image_aspect)))\nThe image's bucket number is stored associated with its item ID in the dataset. If the image's aspect\nratio is very extreme and too different from even the best-fitting bucket, the image is pruned from the\ndataset.\nSince we train on multiple GPUs, before each epoch, we shard the dataset to ensure that each GPU\nworks on a distinct subset of equal size. To do this, we first copy the list of item IDs in the dataset\nand shuffle them. If this copied list is not divisible by the number of GPUs multiplied by the batch\nsize, the list is trimmed, and the last items are dropped to make it divisible.\nWe then select a distinct subset of $\\frac{world\\_size*bsz}{1}$ item IDs according to the global rank of the current\nprocess. The rest of the custom batch generation will be described as seen from a single shard of\nthese processes and operate on the subset of dataset item IDs.\nFor the current shard, lists for each bucket are created by iterating over the list of shuffled dataset item\nIDs and assigning each ID to the list corresponding to the bucket that best fits that image's aspect\nratio.\nOnce all images are processed, we iterate over the lists for each bucket. If its length is not divisible\nby the batch size, we remove the last elements on the list as necessary to make it divisible and add\nthem to a separate catch-all bucket. As the overall shard size is guaranteed to contain a number of\nelements divisible by the batch size, this process is guaranteed to produce a catch-all bucket with a\nlength divisible by the batch size as well.\nWhen a batch is requested, we randomly draw a bucket from a weighted distribution. The bucket\nweights are set as the size of the bucket divided by the size of all remaining buckets. This ensures\nthat even with buckets of widely varying sizes, the custom batch generation does not introduce bias\nduring training. If buckets were chosen without weighting, small buckets would empty out early\nduring the training process, and only the biggest buckets would remain towards the end of training.\nA batch of items is finally taken from the chosen bucket. The items taken are removed from the\nbucket. If the bucket is now empty, it is deleted for the rest of the epoch. The chosen item IDs and\nthe chosen bucket's resolution are now passed to an image-loading function."}, {"title": "4.1.3 Image Loading", "content": "Each item ID's image is loaded and processed to fit within the bucket resolution. For fitting the image,\ntwo approaches are possible.\nFirst, the image could be simply rescaled. This would lead to a slight distortion of the image. For this\nreason, we have opted for the second approach:\nThe image is scaled, while preserving its aspect ratio, in such a way that it:\n\u2022 Either fits the bucket resolution exactly if the aspect ratio happens to match,\n\u2022 or it extends past the bucket resolution on one dimension while fitting it exactly on the other.\nIn the latter case, a random crop is applied."}, {"title": "4.2 Conditioning", "content": "Like in our previous models, we used CLIP[24] context concatenation and conditioned on CLIP's\npenultimate hidden states (on this part, no change was required; SDXL base already does this). To\nproduce SDXL's pooled CLIP condition given multiple concatenated CLIP contexts: we take a mean\naverage over all CLIP segments' pooled states."}, {"title": "4.3 Tag-based Loss Weighting", "content": "During training, we employed a tag-based loss weighting scheme, which keeps track of how often\ntags in certain tag type classes occur. Images with tags that are overly common within their class\nhave their loss downweighted, while images with tags that are rare within their class may have their\nloss up-weighted. This allows our model to better learn concepts from rare tags while reducing the\ninfluence of over-represented concepts."}, {"title": "4.4 VAE Decoder Finetuning", "content": "As in NovelAI Diffusion V1, we finetune the Stable-Diffusion (this time SDXL) VAE decoder,\nwhich decodes the low-resolution latent output of the diffusion model, into high-resolution RGB\nimages. The original rationale (in V1 era) was to specialize the decoder for producing anime textures,\nespecially eyes. For V3, an additional rationale emerged: to dissuade the decoder from outputting\nspurious JPEG artifacts, which were being exhibited despite not being present in our input images."}, {"title": "5 Results", "content": "We find empirically that our model produces relevant, coherent images at CFG[11] scales between\n3.5-5. This is lower than the default of 7.5 recommended typically for SDXL inference, and suggests\nthat our dataset is better-labelled."}, {"title": "6 Conclusions", "content": "NovelAI Diffusion V3 is our most successful image generation model yet, generating 4.8M images\nper day. From this strong base model we have been able to uptrain a suite of further products, such as\nFurry Diffusion V3, Director Tools, and Inpainting models."}, {"title": "A Practicalities for Implementing ZTSNR in k-diffusion/EDM", "content": "ZTSNR poses practicality issues when implemented in the EDM[16] framework (i.e. via the k-\ndiffusion[6] library). Models using the EDM formulation are expected to accept VE (Variance-\nExploding, in this case $\\sigma = \\infty$) noise, but this poses numeric representability issues. Likewise,\nEDM-space samplers are forced to take an infinitely large Euler step with an infinitesimally small\nvelocity. We discuss two solutions for how overcome such practical issues."}, {"title": "A.1 Principled Implementation of ZTSNR", "content": "The usual prescription of k-diffusion/EDM is to start with VE noise and scale it to VP (Variance-\nPreserving, i.e. unit variance). Practically, we can neither numerically represent a $\\sigma = \\infty$ random\ntensor nor can we divide it by \u221e to convert it to a variance-preserving formulation. We must find a\nway to skip this step. The only problems are practical/numeric; algebraically we possess alternative\nways to compute this.\nFirst we will need a bypass within the k-diffusion model wrapper, to \"pass unit-variance pure-noise\ndirectly to the model\", instead of converting $\\sigma = \\infty$ VE noise to VP noise. We also need to add a\nspecial case to discretization, to map o = \u221e to the index of the maximum timestep.\nNext we will need to special-case how we sample from $\\sigma = \\infty$; this is the first step of a ZTSNR\ninference schedule (though we may skip it if we are doing img2img)."}, {"title": "A.1.1 Sampling from the Infinite-Noise Timestep", "content": "During inference, we sample with the goal of predicting the clean image. This is not the objective\nour neural network was trained on. SDXL was trained to predict the noise present in the sample.\nAnd in NAIv3 we adapted SDXL (section 2.1) to instead predict velocity v[26], an objective which\ntransitions from noise-prediction to image-prediction as SNR changes.\nk-diffusion provides model wrappers to adapt our neural network $F_\\theta(\\cdot)$ (which may predict noise,\nvelocity, or some other objective) into an abstract denoiser $D_\\theta$ which predicts the clean image. Con-\ncretely, the wrapper invokes the model through the parameterization of the Karras preconditioner[16].\nOn top of this $D_\\theta$ denoiser abstraction, k-diffusion provides samplers which iterate on the clean-image\npredictions, converging on a fully-denoised image.\nThe Karras preconditioner describes how a clean-image denoiser $D_\\theta$ can be related to our trained\nnetwork $F_\\theta(\\cdot)$ through noise level o-dependent scaling functions $C_{skip}(\\sigma)$, $C_{out}(\\sigma)$, and $C_{in}(\\sigma)$.\n$D_\\theta(x; \\sigma) = C_{skip}(\\sigma)x + C_{out} (\\sigma)F_\\theta(C_{in}(\\sigma)x; \\sigma),$\nThese scaling functions allow our model $F_\\theta(\\cdot)$ to transition between predicting noise content and\nclean sample as the noise level changes. By assigning values to these scaling functions, we can\nexpose which terms can be eliminated at o = 8.\nTo sample an image from $D_\\theta$, we can take Euler steps to bring the noise level down from $\\sigma_i$ to $\\sigma_{i+1}$,\n$\\sigma_{i+1} < \\sigma_i$,\n$x_{i+1} + x_i + (\\sigma_{i+1} - \\sigma_i)\\cdot\\frac{x_i \u2013 D_\\theta (x_i;\\sigma_i)}{\\sigma_i}$,\nand in so doing iterate towards a clean image. But in its current form, we cannot handle a Euler step\ndown from 6 = \u221e. The step size $\\sigma_{i+1} \u2013 \\sigma_i$ would be infinite, and the derivative $x_i-D_\\theta(\\cdot)$ has infinite\nterms on its numerator and denominator. The infinite numerator is due to the noised image $x_i$ being\nformulated as VE, which at $\\sigma = \\infty$ entails infinite variance. Moreover the step is accumulated by\nsummation with an infinite term $x_i$.\nWe can make this tractable by formulating a special-case Euler step for the first step i = 0 of sampling.\nWe decompose the initial VE noise into a standard Gaussian and its standard deviation, via the\nrelationship $x_0 = \\sigma_0n$, where $n \\sim N(0, I)$. By assigning infinite standard deviation $\\sigma_0 \\rightarrow \\infty$, we can"}, {"title": "A.2 Trivial Implementation of (Almost) Zero Terminal SNR", "content": "We opted for a simpler solution: use k-diffusion as usual, but clamp the sigmas by which our schedule\nis defined, to a maximum of 20000. Noise is drawn in float32, as is the scaling of VE noised latents to"}, {"title": "B VAE Scale-and-Shift", "content": "We disclose the scale-and-shift of our anime dataset, in hopes of encouraging a scale-and-shift of\ntraining practices. Whilst this technique was not used on NAIv3, it is a practice we would like to\nraise awareness of for future model training."}, {"title": "B.1 Background: Current Latent Scaling Practices", "content": "Conventionally, latent diffusion setups adapt from the VAE's latent distribution to the diffusion\nmodel's distribution like so:\n\u2022 A scale factor of 0.18215 (SD1) or 0.13025 (SDXL) is used\n\u2022 Latents encoded by the VAE would be multiplied by this scale factor before being (noised\nand) given to the diffusion model.\n\u2022 Denoised latents output by the diffusion model would be divided by this scale factor before\nbeing given to the VAE decoder.\nThis practice was established in [25], where the value comes from the (reciprocal of the) standard\ndeviation of a single batch of images. The intention is to allow the diffusion model to train on data\nwhich has unit variance, by scaling down the comparatively high-variance VAE output. This is all the\nmore important when training using the EDM formulation[16], which is parameterized on odata (the\nstandard deviation of the dataset), which in latent diffusion is typically configured to 1. Scaling our\ndata to have unit variance helps to satisfy this property we claim of our dataset."}, {"title": "B.2 Proposal: Something Entirely More Gaussian", "content": "We suggest a few changes to this practice:\n\u2022 Apply scale and shift; seek to center our data on its mean.\nThis gives our data a mean of 0 and unit variance, making it a standard Gaussian. Mean-\ncentering the data may help the model to work without biases in its conv_in and\nconv_out.\n\u2022 Apply a per-channel scale-and-shift\nThis makes each channel a standard Gaussian. Benefits of standardizing neural network\ninputs in this way are discussed in [27].\nComes with a potential downside of decorrelating the channels, which may make it harder\nto identify signals such as \"all channels high = white\". So there is a question of which\nbenefit is preferable to have.\nThis is not a new idea, but it is underutilized in the SDXL community. Per-channel scale-and-shift\nhas been applied to the latent distribution of the original stable-diffusion[25] VAE finetunes, for the\ntraining of diffusion decoders[3] and image-generative diffusion models[18]. Per-channel scale-and-\nshift is a prevalent idea in pixel-space training, with common dataloader examples normalizing using\nImageNet statistics[5].\nTo advance SDXL training practices, we share the scale-and-shift of our anime dataset (fig. 8),\ncomputed over 78224 images using a Welford average[28]. This online average enables us to\naccumulate an average over multiple batches without precision loss or memory growth. Though it is\nnot an entire-dataset average, it is satisfactorily converged."}]}