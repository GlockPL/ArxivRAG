{"title": "From Sub-Ability Diagnosis to Human-Aligned Generation: Bridging the Gap for Text Length Control via MARKERGEN", "authors": ["Peiwen Yuan", "Chuyi Tan", "Shaoxiong Feng", "Yiwei Li", "Xinglin Wang", "Yueqi Zhang", "Jiayi Shi", "Boyuan Pan", "Yao Hu", "Kan Li"], "abstract": "Despite the rapid progress of large language models (LLMs), their length-controllable text generation (LCTG) ability remains below expectations, posing a major limitation for practical applications. Existing methods mainly focus on end-to-end training to reinforce adherence to length constraints. However, the lack of decomposition and targeted enhancement of LCTG sub-abilities restricts further progress. To bridge this gap, we conduct a bottom-up decomposition of LCTG sub-abilities with human patterns as reference and perform a detailed error analysis. On this basis, we propose MARKERGEN, a simple-yet-effective plug-and-play approach that: (1) mitigates LLM fundamental deficiencies via external tool integration; (2) conducts explicit length modeling with dynamically inserted markers; (3) employs a three-stage generation scheme to better align length constraints while maintaining content quality. Comprehensive experiments demonstrate that MARKERGEN significantly improves LCTG across various settings, exhibiting outstanding effectiveness and generalizability.", "sections": [{"title": "1 Introduction", "content": "As a fundamental attribute of text generation, ensuring controllability over text length is of great importance (Liang et al., 2024). Different text types (e.g., summary, story), user needs (e.g., preference for detailed or concise writing), and external requirements (e.g., social media character limits) shape varied length constraints, which are widely present in real-world scenarios (Zhang et al., 2023a). With the rapid development of LLMs, their expanding range of applications has made length-controllable text generation (LCTG) even more crucial in current era (Foster et al., 2024; Gu et al., 2024b).\nHowever, the ongoing enhancements in LLM capabilities have yet to deliver the expected performance in LCTG while ensuring semantic integrity (Foster et al., 2024; Wang et al., 2024; Song et al., 2024). Yuan et al. (2024) reports that even advanced LLMs (e.g., GPT-4 Turbo (OpenAI, 2023)) violate the given length constraints almost 50% of the time. To address this, training-based methods (Park et al., 2024; Yuan et al., 2024; Jie et al., 2023; Li et al., 2024b) have been studied to reinforce LLMs' adherence to length constraints, yet they face two key challenges: (1) Limited generalization: Since text types are diverse and length constraints vary widely (e.g., ranging from an exact 500 words to coarse intervals like 500-600 words or below 500 words), training-based methods often fail to generalize effectively across different settings, as demonstrated in Appendix E.1. (2) Inferior controllability: These methods strengthen LCTG by enforcing implicit length modeling during generation in a top-down manner via training, lacking the decomposition and targeted enhancement of LCTG sub-capabilities, thereby limiting their progress (Retkowski and Waibel, 2024).\nTo fill this gap, we take humans as a reference and conduct a bottom-up decomposition of sub-capabilities for LCTG. When writing a 500-word story, humans typically begin by planning the content and word allocation for each section. During writing, they continuously track the word count and compose the text in alignment with the plan. This process progressively tests four key abilities: (1) Identifying and splitting the words correctly. (2) Counting the words precisely. (3) Planning the word counts of each part to meet the length constraints. (4) Aligning the generated text with length constraints while ensuring semantic integrity.\nOn this basis, we conduct a decoupled error analysis of LCTG. The experimental results indicate that counting error > perception error > aligning error \u226b planning error. This suggests"}, {"title": "2 Preliminaries", "content": "We model the LCTG process of LLMs by drawing an analogy to human patterns in this task. Specifically, the model first performs content and length planning based on task requirements and length constraints. Under this plan, the semantic space expands progressively at the word level during generation, accompanied by an implicit counting process. Meanwhile, length estimation acts as a real-time constraint, dynamically regulating further extension. Ultimately, the model strives to align the length constraints while preserving semantic integrity. From this perspective, the overall LCTG ability of LLMs can be systematically decomposed in a bottom-up manner into Identifying, Counting, Planning, and Aligning sub-capabilities (Figure 1). Below we explore LLMs' mastery of these abilities through detailed error analysis on TruthfulQA dataset (Lin et al., 2021)."}, {"title": "2.1 Identifying Error", "content": "Identifying error refers to the misidentification of fundamental length units (e.g., words), leading to discrepancies between the model's estimated and actual text length. To systematically analyze this error, we instruct the model to recognize the length"}, {"title": "2.2 Counting Error", "content": "Counting error refers to the inaccurate enumeration of length units in a given sequence, leading to deviations from the intended length. We analyze this error by prompting LLMs to count sequences with varied interval $n$. The case of $n = 1$ corresponds to identifying error (see \u00a72.1). A larger $n$ poses a greater challenge for counting accuracy. To decompose counting error from identifying error, we calculate $e_c^n$ as follows:\n$e_{rc}^n = \\frac{|N_{pred} - N_{true}|}{N_{true}}$\n$e_c^n = e_{rc}^n - e_l$\n(2)\n(3)"}, {"title": "2.3 Planning Error", "content": "Planning error refers to the misallocation of word counts across different sections, leading to a discrepancy from target length. For given query and precise length constraint $N_{target}$, we prompt LLMs to explicitly plan both content and length for each part of the response. We assess the quality 2 of the plan $s_p$, and calculate the planning error rate $e_p$ as:\n$e_p = \\frac{|N_{plan} - N_{target}|}{N_{target}}$\n(4)"}, {"title": "2.4 Aligning Error", "content": "Aligning error refers to the discrepancy between the model's perceived length and the target length, arising from the challenge of maintaining semantic integrity while adhering to length constraints. We calculate aligning error as follows:\n$e_A^n = \\frac{|N_{pred}^n - N_{target}|}{N_{target}}$\n(5)\nwhere $N_{pred}^n$ represents the model's perceived length with counting interval $n$, i.e., the length the model assumes it has generated. We calculate and show the $e_A^n$ in Figure 4."}, {"title": "2.5 LCTG Error", "content": "LCTG error refers to the discrepancy between the actual length of generated text and the target length:\n$E = \\frac{|N_{true} - N_{target}|}{N_{target}}$\n(6)\nAs established above, this error is systematically composed of four components: Identifying Error (\u00a72.1), Counting Error (\u00a72.2), Planning Error (\u00a72.3), and Aligning Error (\u00a72.4). To investigate the key factors influencing LLMs' LCTG error, we calculate their absolute contributions $\\dot{e}$ under different length interval $n$ as follows:\n$\\dot{e}_i^n = \\frac{e_i}{\\sum_{i\\in{I,C,P,A}} e_i} \\times E^n, i \\in [I,C,P,A]$\n(7)"}, {"title": "3 Methodology", "content": "Based on the analyses and findings above, we propose MARKERGEN, a simple-yet-effective plug-"}, {"title": "3.1 Auxiliary Marker Insertion Decoding", "content": "External Tool Invocation. Our analysis in \u00a72 reveals that LLMs exhibit significant identifying and counting errors, which directly contribute to inaccuracies in length modeling. To mitigate these fundamental deficiencies, we introduce external tokenizer and counter for unit recognition and counting, respectively. As Finding 1 indicates that LLMs perceive words better than tokens, we select words as the length unit.\nLength Information Injection. With precise length information, we consider feeding it into the model for length modeling. Since Finding 3 indicates that LLMs' inherent implicit length modeling leads to significant errors and is inconvenient for incorporating external length information, we actively insert precise length markers during gener-"}, {"title": "3.2 Three-Stage Decoupled Generation", "content": "Finding 7 validates that aligning error primarily arises from the inferior semantic modeling, which causes premature termination of the generation process. While the planning before generation scheme alleviates interference in semantic modeling by decoupling the planning process (Finding 6), it still entangles length modeling with semantic modeling. To mitigate this, we introduce a three-stage decoupled generation scheme to further reduce the alignment error and improve the text quality, as illustrated in Figure 5.\nStage One: Planning. The model generates a reasonable plan based on the input query and length constraints, including the content of each section and the word allocation.\nStage Two: Ensuring Semantic Integrity. The model focuses on semantic modeling to generate a high-quality response per the plan without being strictly required to adhere to length constraints.\nStage Three: Aligning Length Constraints. Responses generated in stage two are usually of high quality but may not meet length restrictions. To refine them, we use these non-compliant responses as input and apply the Auxiliary Marker Insertion Decoding mechanism for rewriting. The rewriting requirements include: (1) Retaining the high-quality semantic modeling of the input content. (2) Strictly adhering to the specified length constraints. In terms of workflow, the model is required to: (1) Firstly analyze the previous stage's response for potential improvements; (2) If its output does not meet the length constraints, it will be regenerated up to T times or until the constraints are met."}, {"title": "4 Experiments", "content": "We conduct comprehensive experiments to examine MARKERGEN. Specifically, we validate its effectiveness in \u00a74.2, analyze its generalizability in \u00a74.3, explore the impact of its key components in \u00a74.4, and provide further insights into its mech-"}, {"title": "4.1 Experimental Settings", "content": "Benchmarks We choose five benchmarks for experiments, where HelloBench includes two subsets, as shown in Table 2. See details in Appendix D.\nBaselines\n\u2022 Ruler (Li et al., 2024b): A training-based\u00b3 method that defines length control templates to regulate generation at the range level.\n\u2022 Implicit (Bai et al., 2024): Conduct a plan-and-generate process without explicit counting. To ensure a fair comparison, the model generates multiple responses until token count outperforms MARKERGEN and the candidate with the smallest LCTG error is selected.\nDetails We conduct extensive experiments using Qwen2.5 series (Qwen2.5-7B/14B/32B-Instruct) (Yang et al., 2024) and the Llama3.1 series (Llama-3.1-8B/70B-Instruct) (Dubey et al., 2024), with sampling temperature as 0.5. We experiment under coarse-grained length constraints on the Open-ended QA subset of HelloBench and assess the LCTG error rate under precise length constraints on other benchmarks, following Eq. (6). To evaluate the text quality, we use GPT-40 mini (Hurst et al., 2024) as the judge, with a calibration algorithm to mitigate the length bias (Zheng et al., 2023) (See details in Appendix E). For precise constraints, we set the length of ground truth response as desired target length. We run each setting for three times and report the average results."}, {"title": "4.2 Main Results", "content": "As shown in Table 3, the commonly used two-stage implicit counting baseline results in a substantial"}, {"title": "4.3 Generalizability", "content": "Across LLMs and Tasks. Table 3 demonstrates the strong generalizability of MARKERGEN to LLMs and generation tasks.\nAcross Length Scale. Table 3 also shows MARKERGEN's strong performance across benchmarks with varying length scale (18\u20131450). To further investigate, we analyze progressively increasing the target length from 100 to 400. The results in Table 4 show a declining trend in MARKERGEN's error rate, which can be attributed to the auxiliary marker insertion decoding mechanism that prevents error accumulation from implicit modeling.\nAcross Constraint Types. In addition to exact length constraints, users may impose range-based limits. We evaluate Er, the proportion of responses violating these constraints. Table 4 shows that MARKERGEN maintains an Er below 3% in all cases, significantly lower than the baseline.\nAcross Lingual. We further validate the effectiveness of MARKERGEN in Chinese setting on GAOKAO benchmark, as shown in Table 8."}, {"title": "4.4 Ablation Studies", "content": "In this section, we validate the effectiveness of each module in MARKERGEN with Qwen2.5-32B-Instruct on TruthfulQA, as shown in Table 5.\nTool Invocation. When the model is required to insert markers independently without relying on an external tokenizer and counter, its fundamental limitations lead to a significant increase in the error rate, exceeding 15%.\nDecaying Interval Marker Insertion. When using a fixed marker insertion interval $n$, since length control is inversely proportional to $n$, while semantic modeling is directly proportional to $n$ (which induces alignment errors), we observe unstable LCTG error rate. In contrast, by adopting a sparse-to-dense insertion approach, the Decaying Interval Marker Insertion strategy ensures explicit length modeling while maximizing semantic integrity, leading to lower E and superior S.\nThree-Stage Decoupled Generation. The introduction of explicit length markers in the two-stage scheme leads to a substantial reduction in LCTG error relative to the implicit baseline (8.7 \u2192 2.66). However, this scheme places greater emphasis on length modeling, which consequently diminishes text quality (4.45 \u2192 4.28). In comparison, the three-stage scheme achieves a better balance by"}, {"title": "4.5 Working Mechanism of MARKERGEN", "content": "To better understand how LLMs leverage the inserted length markers in MARKERGEN, we visualize the attention matrices of the first and last layers of Llama-3.1-8b-Instruct (Figure 6). In the shallow layers, the attention distribution reveals a clear focus on the length information represented by the length markers (in the red box). As the model progresses to the deeper layers, attention shifts from the length information to the adjacent semantic content (in the orange box). This pattern demonstrates that at shallow layers, the model uses markers to establish length modeling and encode precise length information. At deeper layers, it relies on this length information for semantic modeling, producing tokens that align with the length constraints while maintaining semantic integrity."}, {"title": "Conclusions", "content": "To improve the performance of LLMs in length-controllable text generation, we conduct a bottom-up error analysis of relevant sub-abilities. The results reveal that deficiencies in identifying, counting, and aligning are key limitations. To fill this gap, we propose MARKERGEN, which leverages"}, {"title": "Limitations", "content": "In this work, we conduct a bottom-up sub-capability analysis in the LCTG ability and propose the MARKERGEN method, achieving strong LCTG performance. One major limitation of MARKERGEN is that it is currently only applicable to open-source models and cannot yet be used with closed-source models. To address this, we will release our code, allowing closed-source model providers interested in adapting MARKERGEN to benefit from our method in enhancing LCTG performance."}, {"title": "Ethics Statement", "content": "All of the datasets used in this study were publicly available, and no annotators were employed for our data collection. We confirm that the datasets we used did not contain any harmful content and was consistent with their intended use (research). We have cited the datasets and relevant works used in this study."}, {"title": "A Related Work", "content": "LCTG Methods Text length is a fundamental aspect of natural language that carries semantic information, making LCTG a task of balancing length and semantic constraints. Achieving precise length control remains a challenge for LLMs due to limitations in their architecture, such as position encoding (Butcher et al., 2024; Kazemnejad et al., 2024; Chang and Bisk, 2024) and decoding mechanisms(Huang et al., 2025). Consequently, existing methods focus on injecting length information to help LLMs model length accurately, which can be categorized into training-based and inference-based approaches.\nTraining-based methods inject varying levels of length signals during fine-tuning or reinforcement learning. For instance, Jie et al. (2023); Li et al. (2024b) use prompt templates to teach LLMs the mapping between length and textual content, while Song et al. (2024); Wang et al. (2024) design fine-grained datasets to guide correct length modeling. Other methods, like Yuan et al. (2024); Jie et al. (2023), utilize reward functions to align length preferences during training. While effective in certain scenarios, these methods suffer from limited generalization across diverse LCTG tasks, including varying length constraints and instructions.Inference-based methods adjust inputs multiple times during generation to inject, such as through prompt-based Automated Revisions and Sample Filtering (Retkowski and Waibel, 2024; Juseon-Do et al., 2024), or length-controlled importance sampling during decoding (Gu et al., 2024b). Although these approaches can better generalize length alignment, they still struggle with achieving precise control.\nWhile both approaches enhance LCTG, they often apply a top-down strategy that lacks deep understanding and targeted enhancement of LCTG sub-capabilities. This limits progress in meeting length constraints accurately. Furthermore, many methods neglect semantic constraints, and injecting length information may degrade text quality. Therefore, we propose MARKERGEN to bridge this gap for precise length control and preserving semantic integrity."}, {"title": "B Detailed Sub-ability Error analyses in LCTG", "content": "B.1 Identifying Error\nIdentifying error refers to the misidentification of fundamental length units. To systematically analyze this error, we design a counting experiment in which the model is prompted to sequentially recognize and accumulate length units, then compare its predicted count with the ground truth. Experimental results confirm that in the one-by-one accumulation setting, counting errors do not occur, meaning that the final length error entirely arises from identifying error (as shown in Figure 7).\nB.2\nCounting Error\nCounting error refers to the inaccurate enumeration of units in a given sequence, leading to deviations from the intended length. Therefore, in the setting where n > 1 in the counting experiment, the final counting result error is caused by both identifying error and counting error. In this case, counting error can be decoupled by resolving identifying errors in the accumulation process, where errors result from the accumulation step. We also conducted the same counting experiment as in Section 2.2 on the CNN/DailyMail summarization dataset, as shown in Figures 8.\nFrom the figure, we can further validate the same conclusions as in Findings 1, 2, 3, and 4 in Section 2, revealing that the length errors in the generated results of the LCTG task stem from significant errors in the LLM's perception and modeling of length.\nB.3 Planning Error\nPlanning error refers to the misallocation of word counts across different sections, leading to a discrepancy from target length.The planning ability of LLMs encompasses not only length planning but also semantic planning. To effectively assess the quality of LLM's semantic planning, we use Qwen-Plus (Yang et al., 2024) as the judge, with a scoring range of [1, 5]. The specific evaluation prompt is as follows:\nYou are tasked with evaluating the qual-ity of a generated answer plan for a Truth-fulQA question. The evaluation should focus on the truthfulness, logical coher-ence, and adherence to the given prompt and instructions. Rate the answer plan on a 5-point scale as follows:"}, {"title": "C Exploration of Interval Marker Insertion Strategy Variants", "content": "C.1 Length Marker Forms\nWe explored the impact of different forms of length marker insertion on performance, such as the number of words generated \"[k]\", the semantic marker \"[k words]\", and the remaining words to be generated \"[ Ntarget - k]\" (remaining words). As shown in Table 6, we used Llama-3.1-8B-Instruct on the CNN/DailyMail dataset to investigate the effects of various marker forms under multiple n conditions on generation error and text quality. The results show that using a semantic length marker representing the number of words generated achieved the best performance in both length error and text quality."}, {"title": "D Detailed Benchmarks Introduction", "content": "The benchmarks used in our experiments are as follows:\n\u2022 CNN/DailyMail(Nallapati et al., 2016): A summarization dataset of news articles, with 500 randomly sampled items. (18\u2013165 words)\n\u2022 HANNA(Chhun et al., 2022): A long-form story generation dataset with 200 selected items. (139\u2013995 words)\n\u2022 TruthfulQA(Lin et al., 2021): A benchmark for factual accuracy in open-domain QA. (101-294 words)\n\u2022 HelloBench(Que et al., 2024): A long-text generation benchmark. We selected subsets from heuristic text generation (e.g., argumentative and roleplaying writing, covering five types) and open-ended QA (spanning ten domains). (489\u20131450 words)\n\u2022 GAOKAO-Bench(Zhang et al., 2023b): A benchmark collected from the Chinese college entrance examination (GAOKAO). We selected the 2010-2022 History Open-ended Questions subset. (71\u2013901 words)"}, {"title": "E Detailed Experimental Results", "content": "E.1 Performance and Generalization Study of Training-based Methods\nTo investigate the performance and generalization of training-based methods in diverse, real-world LCTG task scenarios, we selected Ruler, a training-based method that defines length control templates"}, {"title": "E.2 Length Bias Correction in LLMs-as-a-Judge", "content": "It has been demonstrated that LLMs-as-a-judge exhibit a noticeable length bias (Li et al., 2024a; Gu et al., 2024a). To evaluate the quality of generated text objectively and accurately for LCTG tasks, it is essential to correct for this length bias. We adopt the length-controlled AlpacaEval (Dubois et al.,"}, {"title": "E.3 Residual Length Error Analysis in MARKERGEN", "content": "This subsection focuses on analyzing the residual length errors in the MARKERGEN framework. Building upon the sub-decomposition of LCTG errors presented in Section 2, we eliminate identifying and counting errors through Auxiliary Length Marker Insertion Decoding 3.1. Moreover, by employing the Three-Stage Decoupled Generation strategy 3.2, we effectively reduce aligning errors, thus improving the robustness of all models in semantic expansion under precise length modeling with explicit length markers. This approach ensures"}, {"title": "E.4 Cross-layer Attention Analysis from the MARKERGEN Perspective", "content": "In this section, we perform a cross-layer attention analysis from the MARKERGEN perspective. By examining attention patterns across different layers of the model, we aim to gain a better understanding of how length and semantic information are processed at various stages of generation, providing insights into improving the accuracy of LCTG tasks.\nCombining the analyses from Figures 6 and 10, we infer that in the shallow layers, attention is primarily focused on the length information represented by the length markers. This suggests that the model's early stages prioritize processing and understanding the input length. The higher entropy in these layers indicates that the model needs to integrate various details and information to effectively comprehend the input. As the model progresses to deeper layers, attention shifts from the length information to the adjacent semantic content. The lower entropy in these layers indicates that the model refines its focus, extracting key features and generating more relevant output.\nThis pattern of attention distribution aligns with the findings from (Moon et al.), which emphasize that length modeling in the early layers serves as a foundation for semantic processing in the later layers. Our analysis further supports the notion that LCTG tasks depend on a dynamic interaction between length control and semantic generation, where early layers focus on length constraints and deeper layers prioritize semantic coherence."}]}