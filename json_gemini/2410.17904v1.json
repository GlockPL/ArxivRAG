{"title": "Reinforcement Learning under Latent Dynamics: Toward Statistical and Algorithmic Modularity", "authors": ["Philip Amortila", "Dylan J. Foster", "Nan Jiang", "Zakaria Mhammedi", "Akshay Krishnamurthy"], "abstract": "Real-world applications of reinforcement learning often involve environments where agents operate on\ncomplex, high-dimensional observations, but the underlying (\"latent\") dynamics are comparatively simple.\nHowever, outside of restrictive settings such as small latent spaces, the fundamental statistical requirements\nand algorithmic principles for reinforcement learning under latent dynamics are poorly understood.\nThis paper addresses the question of reinforcement learning under general latent dynamics from a\nstatistical and algorithmic perspective. On the statistical side, our main negative result shows that\nmost well-studied settings for reinforcement learning with function approximation become intractable\nwhen composed with rich observations; we complement this with a positive result, identifying latent\npushforward coverability as a general condition that enables statistical tractability. Algorithmically, we\ndevelop provably efficient observable-to-latent reductions that is, reductions that transform an arbitrary\nalgorithm for the latent MDP into an algorithm that can operate on rich observations in two settings:\none where the agent has access to hindsight observations of the latent dynamics [LADZ23], and one where\nthe agent can estimate self-predictive latent models [SAGHCB20]. Together, our results serve as a first\nstep toward a unified statistical and algorithmic theory for reinforcement learning under latent dynamics.", "sections": [{"title": "Introduction", "content": "Many application domains for reinforcement learning (RL) require the agent to operate on rich, high- dimensional observations of the environment, such as images or text [WSD15; LFDA16; KFPM21; NRKFG22; Bak+22; Bro+22]. However, the environment itself can often be summarized by latent dynamics for a low-dimensional or otherwise simple latent state space. The decoupling of latent dynamics from the complex observation process naturally suggests a modular framework for algorithm design: first learn a representation that decodes the latent state from observations, then apply a reinforcement learning algorithm for the latent dynamics on top of the learned representation. This paper investigates the algorithmic and statistical foundations of this framework. We ask: Can we take existing algorithms and sample complexity guarantees for reinforcement learning in the latent state space and lift them to the observation space in a modular fashion?\nThere is a growing body of theoretical and empirical work developing algorithms that combine representation learning and reinforcement learning to develop scalable algorithms. On the empirical side, a plethora of representation learning objectives have been deployed to varying degrees of success [PAED17; Tan+17; ZMCGL21; LSA20; YFK21; Lam+24; Guo+22; HPBL23], but we lack a mathematical framework to systematically compare these objectives and understand when one might be preferred to another. On the theoretical side, all existing approaches suffer from three primary drawbacks: (a) they are tailored to restricted classes of latent dynamics models (tabular MDPs [KAL16; DKJADL19; MHKL20; ZSUWAS22; MFR23], LQR [DR21; Mha+20], or factored MDPS [MLJL21]), limiting generality; (b) the analyses, despite focusing on restrictive settings, are unwieldy, limiting progress in algorithm development; and (c) they are not modular, in the sense that the representation learning procedures are specialized to specific choices of latent reinforcement learning algorithm, limiting ease of use."}, {"title": "1.1 Contributions", "content": "We address the aforementioned limitations by introducing a new framework, reinforcement learning under general latent dynamics.\nReinforcement learning under general latent dynamics (Section 2). In our framework, the agent performs control based on high-dimensional observations, but the dynamics of the environment are governed by an unobserved latent state space. Following prior work (particularly the so-called Block MDP formulation [DKJADL19]), we assume that the latent states can be uniquely decoded from observations, but that the true decoder is unknown and must be learned. To aid in the decoding process, we supply the learner with a class of representations that is realizable in the sense that it is powerful enough to represent the true decoder. Our point of departure from prior theoretical works is that we do not assume specific structure (e.g., tabular or linear dynamics) on the Markov decision process (MDP) that governs the latent dynamics. Instead, we make the minimal assumption that the latent dynamics belong to a base MDP class which is statistically tractable, in the sense that when the latent states are directly observed there exists some reinforcement learning algorithm with low sample complexity that is capable of learning a near-optimal policy for every MDP in the class. We take the first steps toward building a unified and modular theory for reinforcement learning in this setting.\nContributions: Statistical modularity (Section 3). A central consideration for reinforcement learning under latent dynamics is that representation learning and exploration must be intertwined: an accurate decoder is required to explore the latent state space, but exploration is required to learn an accurate decoder. To develop provable sample complexity guarantees, one must prevent errors from compounding during this interleaving process, a challenging statistical problem which prior work addresses through strong structural assumptions on the base MDP [KAL16; DKJADL19; MHKL20; ZSUWAS22; MFR23; DR21; Mha+20; MLJL21]. For the general latent-dynamics setting we consider, it is unclear whether similar techniques can be applied, or whether the setting is even statistically tractable, ignoring computational considerations. Thus, our first contribution considers the question of statistical modularity:\nIf a base MDP class is tractable when observed directly, is the corresponding latent-dynamics problem tractable?\nStatistical modularity adopts a minimax perspective by assuming that the base MDP lies in a given class, and demands that the sample complexity of the latent-dynamics setting is controlled by a natural bound on the sample complexity of the base MDP class. We show, perhaps surprisingly, that most well-studied reinforcement learning settings involving function approximation [Li09; RVR13; JKALS17; SJKAL19; MJTS20; AJSWY20; WSY20; ZGS21; WAS21; WAJAYJS21; Du+21; JLM21; FKQR21] do not admit statistical modularity (Theorem 3.1). In other words, statistical tractability of an MDP class does not extend to the latent- dynamics setting. We complement these negative findings with a positive result, identifying pushforward cover- ability as a general structural condition on the latent dynamics that enables sample efficiency (Theorem 3.2).\nContributions: Algorithmic modularity (Section 4). Beyond developing a modular understanding of the statistical landscape, we investigate modular algorithm design principles for RL under general latent dynamics. Specifically, we consider the question of observable-to-latent reductions, whereby RL under latent dynamics can be reduced to the simpler problem of RL with latent states directly observed:\nCan we generically lift algorithms for a base MDP class to solve the corresponding latent-dynamics problem?\nThis property, which we refer to as algorithmic modularity, enables modular, greatly simplified algorithm design, allowing one to use an arbitrary base algorithm for the base MDP class to solve the corresponding latent-dynamics problem. Algorithmic modularity is a stronger property than mere statistical modularity, and thus is subject to our statistical lower bound. Accordingly, we consider two settings that sidestep the lower bound through additional feedback and modeling assumptions. Our first algorithmic result considers hindsight observability [LADZ23], where latent states are revealed during training, but not at deployment"}, {"title": "2 Reinforcement Learning under General Latent Dynamics", "content": "In this section we formally introduce our framework, reinforcement learning under general latent dynamics."}, {"title": "2.1 MDP preliminaries", "content": "We consider an episodic finite-horizon online reinforcement learning setting. With H denoting the horizon, a\nMarkov decision process (MDP) $M^* = \\{X, A, \\{P_h\\}_{h=0}^H, \\{R_h\\}_{h=1}^H, H\\}$ consists of a state space X, an action\nspace A, a reward distribution $R : X \\times A \\rightarrow \\Delta([0,1])$ (with expectation r(x, a)), and a transition kernel\n$P : X \\times A \\rightarrow \\Delta(X)$ (with the convention that $P_0(\\cdot)$ is the initial state distribution).\nAt the beginning of the episode, the learner selects a randomized, non-stationary policy $\\pi = (\\pi_1,...,\\pi_H)$,\nwhere $\\pi_h: X \\rightarrow \\Delta(A)$; we let $\\Pi_{RNS}$ denote the set of all such policies. The episode evolves through the\nfollowing process; beginning from $x_1 \\sim P_0(\\cdot)$, the MDP generates a trajectory $(x_1, a_1,r_1),...,(x_H,a_H,r_H)$\nvia $a_h \\sim \\pi_h(x_h)$, $r_h \\sim R(x_h, a_h)$, and $x_{h+1} \\sim P_h(\\cdot | x_h, a_h)$. We let $P_{M^*,\\pi}$ denote the law under this process,\nand let $E_{M^*,\\pi}$ denote the corresponding expectation, and likewise let $P_{M,\\pi}$ and $E_{M,\\pi}$ denote the analogous\nlaws and expectations in another MDP M.\nFor a policy $\\pi$ and MDP M, the expected reward for $\\pi$ is given by $J^M(\\pi) := E_{M,\\pi} [\\sum_{h=1}^H r_h]$, and the value\nfunctions are given by\n$V_h^{M,\\pi}(x) := E_{M,\\pi} \\left[ \\sum_{h'=h}^H r_{h'} \\middle| x_h = x \\right]$ and $Q_h^{M,\\pi}(x,a) := E_{M,\\pi} \\left[ \\sum_{h'=h}^H r_{h'} \\middle| x_h = x, a_h = a \\right]$\nWe let $\\{\\pi_{M,h}^*\\}_{h=1}^H$ denote an optimal deterministic policy of M, which maximizes $V^{M,\\pi}$ (over $\\pi$) at all\nstates (and in particular, satisfies $\\pi_M^* \\in \\arg \\max_{\\pi \\in \\Pi_{RNS}} J^M(\\pi)$), and write $Q^{M,*} := Q^{M,\\pi_M^*}$. For $f: X \\times A \\rightarrow \\mathbb{R}$,\nwe write $\\pi_f(x) := \\arg \\max_a f(x, a)$ as well as $V_f(x) = \\max_a f(x,a)$. For MDP M, horizon h$\\in [H]$, and\n$g: X \\rightarrow \\mathbb{R}$, we let $T_M$ denote the Bellman (optimality) operator defined via\n$[T_Mg](x, a) = E_M [r_h + g(x_{h+1}) | x_h = x, a_h = a]$,\nand we overload notation by letting $[T_M f](x, a) = E_M [r_h + V_f(x_{h+1}) | x_h = x, a_h = a]$. We also let $T_h^{M,\\pi}$\ndenote the Bellman evaluation operator defined via\n$[T_h^{M,\\pi} f](x, a) = E_M [r_h + E_{a'\\sim\\pi_{h+1}(\\cdot|x_{h+1})} [f(x_{h+1},a')] | x_h = x, a_h = a]$,\nfor any $\\pi\\in \\Pi_{RNS}$. We define the induced occupancy measures for layer h via\n$d_h^{M,\\pi}(x) = P_M^\\pi [x_h = x] \\& d_h^{M,\\pi}(x, a) = []P_M^\\pi [x_h = x, a_h = a]$.\nOnline reinforcement learning. In online reinforcement learning, the learning algorithm ALG repeatedly\ninteracts with an unknown MDP M* by executing a policy and observing the resulting trajectory. After T\nrounds of interaction, the algorithm outputs a final policy $\\bar{\\pi}$, with the goal of minimizing their risk, defined\nvia\n$\\text{Risk}(T, \\text{ALG}, M^*) := J^{M^*}(\\pi_{M^*}) - J^{M^*}(\\bar{\\pi})$.(1)"}, {"title": "2.2 Framework: Reinforcement learning under general latent dynamics", "content": "In reinforcement learning under general latent dynamics, we consider MDPs M* where the dynamics are\ngoverned by the evolution of an unobserved latent state sh, while the agent observes and acts on observations\nxh generated from these latent states. Formally, a latent-dynamics MDP consists of two ingredients: a\nbase MDP $M_{\\text{lat}} = \\{S, A, \\{P_{\\text{lat},h}\\}_{h=0}^H, \\{R_{\\text{lat},h}\\}_{h=1}^H, H\\}$ defined over a latent state space S, and a decodable\nemission process $\\psi := \\{\\psi_h : S \\rightarrow \\Delta(X)\\}_{h=1}^H$, which maps each latent state to a distribution over observations.\nThe former is an arbitrary MDP defined over S, while the latter is defined as follows.\nDefinition 2.1 (Emission process). An emission process is any function $\\psi := \\{\\psi_h : S \\rightarrow \\Delta(X)\\}_{h=1}^H$, and is\nsaid to be decodable if\n$\\forall h,\\forall s' \\neq s\\in S : \\text{supp}\\psi_h (s) \\cap \\text{supp} \\psi_h (s') = \\emptyset$.(2)\nWhen $\\psi = \\{\\psi_h\\}_{h=1}^H$ is decodable, we let $\\psi^{-1} := \\{\\psi_h^{-1} : X \\rightarrow S\\}_{h=1}^H$ denote the associated decoder.\nWith this, we can formally introduce the notion of a latent-dynamics MDP.\nDefinition 2.2 (Latent-dynamics MDP). For a base MDP $M_{\\text{lat}} = \\{S, A, \\{P_{\\text{lat},h}\\}_{h=0}^H, \\{R_{\\text{lat},h}\\}_{h=1}^H, H\\}$, and\na decodable emission process $\\psi$, the latent-dynamics MDP $(M_{\\text{lat}}, \\psi) := \\{X, A, \\{P_{\\text{obs},h}\\}_{h=0}^H, \\{R_{\\text{obs},h}\\}_{h=1}^H, H\\}$\nis defined as the MDP where the latent dynamics evolve based on the agent's action $a_h \\in A$ via the process\n$s_{h+1} \\sim P_{\\text{lat},h}(s_h, a_h)$ and $r_h \\sim R_{\\text{lat},h}(s_h, a_h)$. The latent state is not observed directly, and instead the agent\nobserves $x_h \\in X$ generated by the emission process $x_h \\sim \\psi_{h+1}(s_h)$.\nNote that under these dynamics, the decoder $\\psi^{-1}$ associated with $\\psi$ ensures that $\\psi_h^{-1}(x_h) = s_h$ almost surely\nfor all $h\\in [H]$. That is, the latent states can be uniquely decoded from the observations. To emphasize the\ndistinction between the latent-dynamics MDP $(M_{\\text{lat}}, \\psi)$ (which operates on the observable state space X)\nand the MDP $M_{\\text{lat}}$ (which operates on the latent state space S), we refer to the latter as a base MDP rather\nthan, for example, a \"latent MDP\", and apply a similar convention to other latent objects whenever possible.\nDeparting from prior work, we do not place any inherent restrictions on the base MDP, and in particular\ndo not assume that the latent space is small (i.e., tabular). Rather, we aim to understand in a unified\nfashion-what structural assumptions on the base MDP $M_{\\text{lat}}$ are required to enable learnability under latent\ndynamics. To this end, it will be useful to considers specific classes (i.e., subsets) of base MDPS $\\mathbb{M}_{\\text{lat}}$ and\nthe classes of latent-dynamics MDPs they induce.\nDefinition 2.3 (Latent-dynamics MDP class). Given a set of base MDPs $\\mathbb{M}_{\\text{lat}}$ and a set of decoders\n$\\Phi \\subset \\{X \\rightarrow S\\}$, we let\n$\\langle \\mathbb{M}_{\\text{lat}}, \\Phi \\rangle := \\{\\langle M_{\\text{lat}}, \\psi \\rangle : M_{\\text{lat}} \\in \\mathbb{M}_{\\text{lat}}, \\psi \\text{ is decodable}, \\psi^{-1} \\in \\Phi \\}$(3)\ndenote the class of induced latent-dynamics MDPs.\nStated another way, $\\langle \\mathbb{M}_{\\text{lat}}, \\Phi \\rangle$ is the set of all latent-dynamics MDPS $\\langle M_{\\text{lat}}, \\psi \\rangle$ where (i) the base MDP\n$M_{\\text{lat}}$ lies in $\\mathbb{M}_{\\text{lat}}$, and (ii), the emission process $\\psi$ is decodable, with the corresponding decoder belonging\nto $\\Phi$. The class $\\mathbb{M}_{\\text{lat}}$ represents our prior knowledge about the underlying MDP $M_{\\text{lat}}$; concrete classes\nconsidered in prior work include tabular MDPs [KAL16; DKJADL19; MHKL20; ZSUWAS22; MFR23], linear\ndynamical systems [DMRY20; DR21; Mha+20], and factored MDPs [MLJL21]. In particular, the class\n$\\mathbb{M}_{\\text{lat}}$ may itself warrant using function approximation. At the same time, the class $\\Phi$ represents our prior\nknowledge or inductive bias about the emission process, enabling representation learning. In what follows, we\ninvestigate what conditions on $\\mathbb{M}_{\\text{lat}}$ make the induced class $\\langle \\mathbb{M}_{\\text{lat}}, \\Phi \\rangle$ tractable, both statistically (statistical\nmodularity; Section 3) and via reduction (algorithmic modularity; Section 4)."}, {"title": "3 Statistical Modularity: Positive and Negative Results", "content": "This section presents our main statistical results. We begin by formally defining the notion of statistical\nmodularity introduced in Section 1, present our main impossibility result (lower bound) and its implications\n(Section 3.2), then give positive results for the general class of pushforward-coverable MDPs (Section 3.3)."}, {"title": "3.1 Statistical modularity: A formal definition", "content": "We first define the statistical complexity for a MDP class (or, model class) M.\nDefinition 3.1 (Statistical complexity). We say that an MDP class M can be learned up to $\\epsilon$-optimality\nusing comp(M, $\\epsilon$, $\\delta$) samples if there exists an algorithm ALG which, for every $M\\in M$, attains\n$\\text{Risk}(T, \\text{ALG}, M) < \\epsilon$\nwith probability at least 1 \u2013 $\\delta$ after $T = \\text{comp}(M,\\epsilon, \\delta)$ rounds of online interaction in M.\nWe say that a base MDP class $\\mathbb{M}_{\\text{lat}}$ admits statistically modularity if, for any decoder class $\\Phi$, the induced\nlatent-dynamics MDP class $\\langle \\mathbb{M}_{\\text{lat}}, \\Phi \\rangle$ can be learned with statistical complexity that is polynomial in: (i)\nthe statistical complexity for the base class, and (ii) the capacity of the decoder class.\nDefinition 3.2 (Statistical modularity). For a decoder class $\\Phi$, we say the MDP class $\\mathbb{M}_{\\text{lat}}$ is statistically\nmodular under complexity comp($\\mathbb{M}_{\\text{lat}}$, $\\epsilon$, $\\delta$) if\n$\\text{comp}(\\langle \\mathbb{M}_{\\text{lat}}, \\Phi \\rangle, \\epsilon, \\delta) = \\text{poly}(\\text{comp}(\\mathbb{M}_{\\text{lat}}, \\epsilon, \\delta), \\log|\\Phi|)$.(4)\nWe say that $\\mathbb{M}_{\\text{lat}}$ admits strong statistical modularity if Eq. (4) holds when comp($\\mathbb{M}_{\\text{lat}}$, $\\epsilon$, $\\delta$) is the minimax\nsample complexity for $\\mathbb{M}_{\\text{lat}}$.\nIn the sequel, we examine well-studied MDP classes $\\mathbb{M}_{\\text{lat}}$ (e.g., those which admit low Bellman rank\n[JKALS17]) and choose comp($\\mathbb{M}_{\\text{lat}}$, $\\epsilon$, $\\delta$) based on natural upper bounds on their optimal sample complexity;\nin this case we will simply say they are (or are not) statistical modular, leaving the complexity upper bound\ncomp implicit. Following prior work [KAL16; DKJADL19; MHKL20; ZSUWAS22; MFR23; DR21; Mha+20;\nMLJL21], we use $\\log|\\Phi|$ as a proxy for the statistical complexity of supervised learning with the decoder class\n$\\Phi$.\nThe two most notable examples of statistical modularity covered by prior work are: (i) taking $\\mathbb{M}_{\\text{lat}}$ as the set\nof tabular MDPs admits strong statistical modularity [DKJADL19; MHKL20; MFR23], and (ii) taking $\\mathbb{M}_{\\text{lat}}$ as\nthe set of linear MDPs admits statistical modularity with complexity poly(d, H, |A|,$\\epsilon^{-1}$, $\\log(\\delta^{-1})$) [AKKS20;\nUZS22; MCKJA24; MBFR23]. Interestingly, the latter does not admit strong statistical modularity, because\nthe optimal rate for $\\mathbb{M}_{\\text{lat}}$ does not scale with |A|, but the rate for $\\langle \\mathbb{M}_{\\text{lat}}, \\Phi \\rangle$ necessarily does [LS20; HLSW21].\nThe results of Mhammedi et al.; Misra et al.; Song et al. [Mha+20; MLJL21; SWFK24] can be viewed as\ninstances of statistical modularity for other base MDP classes."}, {"title": "3.2 Lower bounds: Impossibility of statistical modularity", "content": "Our main result in this section is to show that for most MDP classes $\\mathbb{M}_{\\text{lat}}$ considered in the literature on\nsample-efficient reinforcement learning with function approximation [RVR13; JKALS17; SJKAL19; MJTS20;\nAJSWY20; Li09; DVRZ19; WSY20; ZGS21; Du+21; JLM21; FKQR21], statistical modularity (under the\nnatural complexity upper bound for the class of interest) is impossible. Our central technical result is the\nfollowing lower bound, which shows that statistical modularity can be impossible even when $|\\mathbb{M}_{\\text{lat}}| = 1$. That\nis, even when the base MDP is known to the learner a-priori. The lower bound is a significant generalization\nof the result from Song et al. [SWFK24]; we first state the lower bound, then discuss implications."}, {"title": "3.3 Upper bounds: Pushforward-coverable MDPs are statistically modular", "content": "Our main postive result concerning statistical modularity is to highlight pushforward coverability [XJ21;\nAFK24; MFR24]\u2014a strengthened version of the coverability parameter introduced in Xie et al. [XFBJK23] as\na general structural parameter that enables sample-efficient reinforcement learning under latent dynamics.\nDefinition 3.3 (Pushforward coverability). The pushforward coverability coefficient $C_{\\text{push}}$ for an MDP $M_{\\text{lat}}$\nwith transition kernel $P_{\\text{lat}}$ is defined by\n$C_{\\text{push}} (M_{\\text{lat}}) = \\max_{h \\in [H]} \\inf_{\\mu \\in \\Delta(S)} \\sup_{(s,a,s')\\in S\\times A \\times S} \\frac{P_{\\text{lat},h-1}(s' | s, a)}{\\mu(s')}$.(5)\nConcrete examples [AFK24; MFR24] include: (i) tabular MDPS $M_{\\text{lat}}$ admit $C_{\\text{push}} (M_{\\text{lat}}) \\leq |S|$; and (ii)\nLow-Rank MDPS $M_{\\text{lat}}$ (with or without known features) in dimension d admit $C_{\\text{push}} (M_{\\text{lat}}) \\leq d$. Further\nexamples include analytically sparse Low-Rank MDPs [GMR24] and Exogenous Block MDPs with weakly\ncorrelated noise [MFR24]. Our main result is as follows.\nTheorem 3.2 (Pushforward-coverable MDPs are statistically modular). Let $\\mathbb{M}_{\\text{lat}}$ be a base MDP class such\nthat each $M_{\\text{lat}} \\in \\mathbb{M}_{\\text{lat}}$ has pushforward coverability bounded by $C_{\\text{push}}(M_{\\text{lat}}) \\leq C_{\\text{push}}$. Then, for any decoder\nclass $\\Phi$, we have:\n1. $\\text{comp}(\\mathbb{M}_{\\text{lat}}, \\epsilon, \\delta) \\leq \\text{poly}(C_{\\text{push}}, |A|, H, \\log|\\mathbb{M}_{\\text{lat}}|,\\delta^{-1},\\log(\\delta^{-1}))$, and\n2. $\\text{comp}(\\langle \\mathbb{M}_{\\text{lat}}, \\Phi \\rangle, \\epsilon, \\delta) \\leq \\text{poly}(C_{\\text{push}}, |A|, H, \\log|\\mathbb{M}_{\\text{lat}}|, \\log|\\Phi|, \\epsilon^{-1},\\log(\\delta^{-1}), \\log \\log|S|)$.\nTheorem 3.2 shows that, modulo a term that is doubly-logarithmic in |S|, latent pushforward coverability\nenables statistical modularity. That is, when the base (latent) dynamics satisfy pushforward coverability,\nthere exists an algorithm for the latent-dynamics setting which scales with the statistical complexity of the\nbase MDP class and $\\log|\\Phi|$. We suspect that the additional $\\log \\log|S|$ factor is not essential and can be\nremoved with a more sophisticated analysis. We note that the complexity comp chosen above is not the\nminimax complexity for $\\mathbb{M}_{\\text{lat}}$, since every set of pushforward coverable MDPs is also a set of coverable MDPS\nwith a potentially smaller coverability parameter [AFK24].\nLet us provide some intuition for this result. We firstly note that when $M_{\\text{lat}}$ has pushforward coverability\nparameter $C_{\\text{push}}$, it holds that for any emission process $\\psi^*$, the observation-level MDP $M_{\\text{obs}} := \\langle M_{\\text{lat}},\\psi^* \\rangle$\nalso satisfies pushforward coverability with the same parameter $C_{\\text{push}}$ (Lemma C.5). Yet, despite access to\nrealizable base MDP class $\\mathbb{M}_{\\text{lat}}$ and decoder class $\\Phi$, it is unclear whether the latent-dynamics MDP $M_{\\text{obs}}$\nsatisfies any of the observation-level function approximation conditions required by existing approaches that\nprovide sample complexity guarantees under pushforward coverability. In particular, known algorithms for\nthis setting either require a Bellman-complete value function class [XFBJK23], a class realizing certain density\nratios [AFJSX24; AFK24], or a realizable model class [AFK24], and it is highly nontrivial to construct these\nfor the latent-dynamics MDP $M_{\\text{obs}} = \\langle M_{\\text{lat}},\\psi^* \\rangle)$ given only the base MDP class $\\mathbb{M}_{\\text{lat}}$ and the decoder class\n$\\Phi$. Intuitively, this is because the former observation-level function approximation classes capture properties\nof the observation-level dynamics which cannot be obtained without some knowledge of the emission process."}, {"title": "4 Algorithmic Modularity", "content": "We now turn our attention to algorithmic modularity. Specifically, we aim for observable-to-latent reductions,\nwhereby via representation learning-RL under latent dynamics can be efficiently reduced to the simpler\nproblem of RL with latent states directly observed. Since algorithmic modularity is a stronger property than\nstatistical modularity, we sidestep the previous lower bounds in Section 3 through additional feedback and\nmodeling assumptions. Our main result for this section is a new meta-algorithm, O2L, which, under these\nassumptions (and when equipped with an appropriately designed representation learning oracle), acts as a\nuniversal reduction in the sense that, whenever the representation learning oracle has low risk, the reduction\ntransforms any sample-efficient algorithm for any base MDP class into a sample-efficient algorithm for the\ninduced latent-dynamics MDP class."}, {"title": "4.1 Setup and O2L meta-algorithm", "content": "For the results in this section, we denote the (unknown) latent-dynamics MDP of interest by $M_{\\text{obs}} :=\n\\langle M_{\\text{lat}}, \\psi^*\\rangle$, and use $\\psi^* := (\\psi^*)^{-1}$ to denote the true decoder. The O2L meta-algorithm (Algorithm 1) learns\na near-optimal policy for $M_{\\text{obs}}$ by alternating between performing representation learning and executing a\nblack-box \"base\" RL algorithm (designed for the base MDP) on the learned representation; this approach is\ninspired by empirical methods that blend representation learning and RL in the latent space (e.g., [GKBNB19;\nSAGHCB20; Ni+24]).\nConcretely, the algorithm takes as input a representation learning oracle REPLEARN and a base RL algorithm\nALGlat that operates in the latent space. In each epoch t$\\in [T]$, REPLEARN produces a new representation\n$\\phi^{(t)}: X \\rightarrow S$ based on data observed so far (potentially using additional side information, which we will\nelaborate on in the sequel). Then, the reduction invokes ALGlat, using $\\phi^{(t)}$ to simulate access to the true latent\nstates. In particular, ALGlat runs for K episodes, where at each episode k: (i) ALGlat produces a latent policy\n$\\pi_{\\text{lat}}^{(t,k)}: S \\times [H] \\rightarrow \\Delta(A)$, (ii) the latent policy is transformed into an observation-level policy via composition\nwith $\\phi^{(t)}$, i.e. $\\pi_{\\text{lat}}^{(t,k)} o\\phi^{(t)}$, which is then deployed to produce a trajectory $\\{x_h^{(t,k)}, a_h^{(t,k)}, r_h^{(t,k)}\\}_{h=1}^H$, and (iii) the\ntrajectory is compressed through $\\phi^{(t)}$ and used to update ALGlat via $\\{(\\phi^{(t)} (x_h^{(t,k)}), a_h^{(t,k)}, r_h^{(tk)} \\}_{h=1}^H$ (cf. Line 8\nof Algorithm 1). After the K rounds conclude, ALGlat produces a final latent policy $\\bar{\\pi}^{(t)}: S \\times [H] \\rightarrow \\Delta(A)$.\nThe final policy $\\bar{\\pi}$ chosen by the O2L algorithm is a uniform mixture of $\\bar{\\pi}^{(t)}o\\phi^{(t)}$ over all the epochs.\nThe central assumption behind O2L is that the base algorithm ALGlat can achieve low-risk in the underlying\nbase MDP $M_{\\text{lat}}$ if given access to the true latent states $s_h = \\psi^*(x_h)$. Beyond this assumption, we require\nthat the representation learning oracle REPLEARN can learn a sufficiently high-quality representation. In our\napplications, this will be made possible by assuming access to a realizable decoder class and two distinct as-\nsumptions: hindsight observability (Section 4.2) and conditions enabling self-predictive representation learning\n(Section 4.3). We will show that under these conditions, we can instantiate a representation learning oracle\nsuch that O2L inherits the sample complexity guarantee for ALGlat, thereby achieving algorithmic modularity."}, {"title": "4.2 Algorithmic modularity via hindsight observability", "content": "Our first algorithmic result bypasses the hardness in Section 3 by considering the setting of hindsight\nobservability, which has garnered recent interest in the context of POMDPs [LADZ23; GCWXWB24; SLS23;\nLXJZV24]. Here, we assume that at training time (but not during deployment), the algorithm has access to\nadditional feedback in the form of the true latent states, which are revealed at the end of each episode.\nAssumption 4.1 (Hindsight Observability [LADZ23]). The latent states $(\\phi_1(x_1),...,\\phi_H(x_H))$ are revealed\nto the learner after each episode $(x_1,a_1,r_1,...,x_H,a_H,r_H)$ concludes.\nWe emphasize that in the hindsight observability framework, the learner must still execute observation-space\npolicies $\\pi_{\\text{obs}}: X \\rightarrow \\Delta(A)$, as the latent states are only revealed at the end of each episode. Under hindsight\nobservability, we can instantiate the representation learning oracle in O2L so that the reduction achieves"}, {"title": "4.3 Algorithmic modularity via self-predictive estimation", "content": "In this section, we remove the assumption of hindsight observability used in Section 4.2 and instantiate O2L\nin the general online RL setting. Rather than assume access to additional side-information, we adopt a\nmodel-based representation learning approach, and augment our ability to perform representation learning by\nequipping the representation learning algorithm with a set of base MDPS $\\mathbb{M}_{\\text{lat}}$ in addition to the decoder\nclass $\\Phi$. We will learn a representation by jointly fitting a decoder and the base (latent) dynamics, which is a\ncommon approach in practice [GKBNB19; HLBN19; Haf+19; HLNB21; Sch"}]}