{"title": "MTMamba++: Enhancing Multi-Task Dense Scene Understanding via Mamba-Based Decoders", "authors": ["Baijiong Lin", "Weisen Jiang", "Pengguang Chen", "Shu Liu", "Ying-Cong Chen"], "abstract": "Multi-task dense scene understanding, which trains a model for multiple dense prediction tasks, has a wide range of application scenarios. Capturing long-range dependency and enhancing cross-task interactions are crucial to multi-task dense prediction. In this paper, we propose MTMamba++, a novel architecture for multi-task scene understanding featuring with a Mamba-based decoder. It contains two types of core blocks: self-task Mamba (STM) block and cross-task Mamba (CTM) block. STM handles long-range dependency by leveraging state-space models, while CTM explicitly models task interactions to facilitate information exchange across tasks. We design two types of CTM block, namely F-CTM and S-CTM, to enhance cross-task interaction from feature and semantic perspectives, respectively. Experiments on NYUDv2, PASCAL-Context, and Cityscapes datasets demonstrate the superior performance of MTMamba++ over CNN-based and Transformer-based methods. The code is available at https://github.com/EnVision-Research/MTMamba.", "sections": [{"title": "1 INTRODUCTION", "content": "MULTI-task dense scene understanding has a variety\nof practical applications in computer vision [1], such\nas autonomous driving [2], healthcare [3], and robotics [4].\nThis problem is challenging because it requires training a\nmodel to simultaneously handle multiple dense prediction\ntasks, such as semantic segmentation, monocular depth\nestimation, surface normal estimation, and object boundary\ndetection.\nThe widely adopted architecture for multi-task dense\nscene understanding is based on the encoder-decoder\nframework, which features a common encoder for extracting\ntask-shared features and multiple decoders for generating\ntask-specific predictions [1]. This framework is very general\nand many variants [5]\u2013[10] have been proposed to improve\nits performance. One promising approach is the decoder-\nfocused method [1], which aims to enhance cross-task in-\nteraction in task-specific decoders by well-designed fusion\nmodules. For example, derived from convolutional neural\nnetworks (CNNs), PAD-Net [5] and MTI-Net [6] use a multi-\nmodal distillation module to enhance information exchange\nacross tasks in the decoder and achieve better performance\nthan the conventional encoder-decoder framework.\nSince the convolution operation mainly captures lo-\ncal features and may struggle with modeling long-range\ndependencies [11], [12], recent methods [7]\u2013[10] adopt\nTransformer-based decoders with attention-based fusion\nmodules. These methods leverage the attention mechanism\n[13] to capture global context information, resulting in bet-\nter performance than CNN-based methods. These works\ndemonstrate that enhancing cross-task correlation and mod-\neling long-range spatial relationships are crucial to multi-\ntask dense prediction.\nRecently, Mamba [14], a new network architecture de-\nrived from state space models (SSMs) [15], [16], has demon-\nstrated better capacity in long-range dependencies model-\ning and superior performance than Transformers in various\ndomains, including language modeling [14], [17], [18], graph\nreasoning [19], [20], medical images analysis [21], [22], and\nimage generation [23], [24]. However, existing works on\nMamba are limited to single-task learning scenarios. Using\nMamba to solve multi-task problems is still unexplored\nsince it needs to model the cross-task interaction, which is\nchallenging.\nTo fill these gaps, in this paper, we propose MT-\nMamba++, a novel multi-task architecture featuring a\nMamba-based decoder. As the overall framework shown in\nFigure 1, MTMamba++ is a decoder-focused method and\nthe decoder consists of three stages. These three stages share\na similar structure and contain three types of blocks: ECR,\nSTM, and CTM blocks. Specifically, the ECR (expand, con-\ncatenate, and reduce) block is responsible for upscaling the\ntask feature and fusing it with the high-level feature from\nthe encoder. It allows each decoder stage to incrementally\nincrease the spatial resolution of the feature maps, which\nplays an important role in precise pixel-level predictions.\nThe core components of the decoder are two Mamba-based"}, {"title": "2 RELATED WORKS", "content": "2.1 Multi-Task Learning\nMulti-task learning (MTL) is a learning paradigm that\naims to jointly learn multiple related tasks using a single\nmodel [31]. Current MTL research mainly focuses on multi-\nobjective optimization [32]\u2013[37] and network architecture\ndesign [5]\u2013[10]. In multi-task visual scene understanding,\nmost existing works focus on designing architecture [1],\nespecially developing specific modules in the decoder to\nfacilitate knowledge exchange among different tasks. For\ninstance, based on CNN, Xu et al. [5] introduce PAD-\nNet, which integrates an effective multi-modal distillation\nmodule aimed at enhancing information exchange among\nvarious tasks within the decoder. MTI-Net [6] is a complex\nmulti-scale and multi-task CNN architecture that facili-\ntates information distillation across various feature scales.\nAs the convolution operation only captures local features\n[11], recent approaches [7]\u2013[10] develop Transformer-based\ndecoders to grasp global context by attention mechanism\n[13]. For example, InvPT [7] is a Transformer-based multi-\ntask architecture that employs an effective UP-Transformer\nblock for multi-task feature interaction at different feature\nscales. MQTransformer [8] uses a cross-task query attention\nmodule in the decoder to enable effective task association\nand information communication.\nThese works demonstrate the significance of long-range\ndependency modeling and the enhancement of cross-task\ncorrelation for multi-task dense scene understanding. Dif-\nferent from existing methods, we propose a novel multi-\ntask architecture derived from the SSM mechanism [14] to\ncapture global information better and promote cross-task\ninteraction.\n2.2 State Space Models\nState space models (SSMs) are a mathematical framework\nfor characterizing dynamic systems, capturing the dynamics\nof input-output relationships via a hidden state. SSMs have\nfound broad applications in various fields such as reinforce-\nment learning [38], computational neuroscience [39], and\nlinear dynamical systems [40]. Recently, SSMs have emerged\nas an alternative mechanism to model long-range depen-\ndencies in a manner that maintains linear complexity with\nrespect to sequence length. Compared with the convolu-\ntion operation, which excels at capturing local dependence,\nSSMs exhibit enhanced capabilities for modeling long se-\nquences. Moreover, in contrast to attention mechanism [13],\nwhich incurs quadratic computational costs with respect to\nsequence length [41], [42], SSMs are more computation- and\nmemory-efficient.\nTo improve the expressivity and efficiency of SSMs,\nmany different structures have been proposed. Gu et al.\n[16] propose structured state space models (S4) to enhance\ncomputational efficiency by decomposing the state matrix\ninto low-rank and normal matrices. Many follow-up works\nattempt to improve the effectiveness of S4. For instance, Fu\net al. [43] propose a new SSM layer called H3 to reduce the\nperformance gap between SSM-based networks and Trans-\nformers in language modeling. Mehta et al. [44] introduce\na gated state space layer leveraging gated units to enhance\nthe models' expressive capacity.\nRecently, Gu and Dao [14] propose a new SSM-based\narchitecture termed Mamba, which incorporates a new SSMC\ncalled S6. This SSM is an input-dependent selection mecha-\nnism derived from S4. Mamba has demonstrated superior\nperformance over Transformers on various benchmarks,\nsuch as image classification [25], [45], image segmentation\n[22], and graph prediction [19]. Different from existing\nresearch efforts on Mamba, which mainly focus on single-\ntask settings, in this paper, we consider a more challenging\nmulti-task setting and propose a novel cross-task Mamba\nmodule to capture inter-task dependence."}, {"title": "3 METHODOLOGY", "content": "In this section, we begin with the foundational knowledge\nof state space models (Section 3.1) and provide an overview\nof the proposed MTMamba++ in Section 3.2. Next, we delve\ninto a detailed exploration of each component in the decoder\nof MTMamba++, including the encoder in Section 3.3, three\ntypes of block in the decoder (i.e., the ECR block in Section\n3.4, the STM block in Section 3.5, and the CTM block in\nSection 3.6), and the prediction head in Section 3.7.\n3.1 Preliminaries\nSSMs [14]\u2013[16], derived from the linear systems theory [40],\nmap an input sequence $x(t) \\in \\mathbb{R}$ to an output sequence\n$y(t) \\in \\mathbb{R}$ though a hidden state $h\\in \\mathbb{R}^{N}$ using a linear\nordinary differential equation:\n$h'(t) = Ah(t) + Bx(t), \\\\ y(t) = Ch(t) + Dx(t),$ \nwhere $A \\in \\mathbb{R}^{N\\times N}$ is the state transition matrix, $B \\in \\mathbb{R}^{N}$\nand $C\\in \\mathbb{R}^{N}$ are projection matrices, and $D \\in \\mathbb{R}$ is the skip\nconnection. Equation (1) defines the evolution of the hidden\nstate $h(t)$, while Equation (2) specifies that the output is\nderived from a linear transformation of the hidden state $h(t)$\ncombined with a skip connection from the input $x(t)$.\nGiven that continuous-time systems are not compatible\nwith digital computers and the discrete nature of real-\nworld data, a discretization process is essential. This process\napproximates the continuous-time system with a discrete-\ntime one. Let $\u2206 \\in \\mathbb{R}$ be a discrete-time step size. Equations\n(1) and (2) are discretized as\n$h_t = \\bar{A} h_{t-1} + \\bar{B}x_t, \\\\ Y_t = Ch_t + Dx_t,$\nwhere $x_t = x(t)$, and\n$\\bar{A} = exp(\\Delta A), \\\\ \\bar{B} = (\\Delta A)^{-1}(exp(\\Delta A) \u2013 I) \\cdot \\Delta B \\approx \\Delta B, \\\\ C = C.$\nS4 [16] treats A, B, C, and \u2206 as trainable parameters and\noptimizes them by gradient descent. However, these param-\neters do not explicitly depend on the input sequence, which\ncan lead to suboptimal extraction of contextual information.\nTo address this limitation, Mamba [14] introduces a new\nSSM, namely S6. As illustrated in Figure 4(a), it incorporates\nan input-dependent selection mechanism that enhances the\nsystem's ability to discern and select relevant information\ncontingent upon the input sequence. Specifically, B, C, and\nA are defined as functions of the input $x \\in \\mathbb{R}^{B\\times L\\times C}$.\nFollowing the computation of these parameters, A, B, and\nC are calculated via Equation (5). Subsequently, the output\nsequence $y \\in \\mathbb{R}^{B\\times L\\times \\hat{C}}$ is computed by Equations (3) and\n(4), thereby improving the contextual information extrac-\ntion. Without specific instructions, in this paper, S6 [14] is\nused in the SSM mechanism.\n3.2 Overall Architecture\nAn overview of MTMamba++ is illustrated in Figure 1.\nIt contains three components: an off-the-shelf encoder, a\nMamba-based decoder, and task-specific prediction heads.\nSpecifically, the encoder is shared across all tasks and plays\na pivotal role in extracting multi-scale generic visual rep-\nresentations from the input image. The decoder consists\nof three stages, each of which progressively expands the\nspatial dimensions of the feature maps. This expansion is\ncrucial for dense prediction tasks, as the resolution of the\nfeature maps directly impacts the accuracy of the pixel-\nlevel predictions [7]. Each decoder stage is equipped with\nthe ECR block designed to upsample the feature and inte-\ngrate it with high-level features derived from the encoder.\nFollowing this, the STM block is employed to capture the\nlong-range spatial relationship for each task. Additionally,\nthe CTM block facilitates feature enhancement for each task\nby promoting knowledge exchange across different tasks.\nWe design two types of CTM block, namely F-CTM and S-\nCTM, as introduced in Section 3.6. In the end, a prediction\nhead is used to generate the final prediction for each task.\nWe introduce two types of head, called DenseHead and\nLiteHead, as described in Section 3.7.\nMTMamba++ and our preliminary version MTMamba\n[29] have a similar architecture. The default configuration\nfor MTMamba++ utilizes the S-CTM block and LiteHead,\nwhile the default configuration for MTMamba employs the\nF-CTM block and DenseHead.\n3.3 Encoder\nThe encoder in MTMamba++ is shared across different tasks\nand is designed to learn generic multi-scale visual features\nfrom the input RGB image. As an example, we consider\nthe Swin Transformer [30], which segments the input image\ninto non-overlapping patches. Each patch is treated as a\ntoken, and its feature representation is a concatenation of\nthe raw RGB pixel values. After patch segmentation, a\nlinear layer is applied to project the raw token into a C-\ndimensional feature embedding. The projected tokens then\nsequentially pass through four stages of the encoder. Each\nstage comprises multiple Swin Transformer blocks and a\npatch merging layer. The patch merging layer is specifically\nutilized to downsample the spatial dimensions by a factor\nof 2x and expand the channel numbers by a factor of\n2x, while the Swin Transformer blocks are dedicated to\nlearning and refining the feature representations. Finally,\nfor an input image with dimensions $H \\times W \\times 3$, where H\nand W denote the height and width, the encoder generates\nhierarchical feature representations at four different scales,\ni.e., $\\frac{H}{4} \\times \\frac{W}{4} \\times C, \\frac{H}{8} \\times \\frac{W}{8} \\times 2C, \\frac{H}{16} \\times \\frac{W}{16} \\times 4C,$ and $\\frac{H}{32} \\times \\frac{W}{32} \\times 8C$.\n3.4 ECR Block\nThe ECR (expand, concatenate, and reduce) block is respon-\nsible for upsampling the feature and aggregating it with the\nencoder's feature. As illustrated in Figure 2(a), it contains\nthree steps. For an input feature, ECR block first 2\u00d7 up-\nsamples the feature resolution and 2\u00d7 reduces the channel\nnumber by a linear layer and the rearrange operation. Then,\nthe feature is fused with the high-level feature from the\nencoder through skip connections. Fusing these features is\ncrucial for compensating the loss of spatial information that\noccurs due to downsampling in the encoder. Finally, a 1 \u00d7 1\nconvolutional layer is used to reduce the channel number.\nConsequently, the ECR block facilitates the efficient recov-\nery of high-resolution details, which is essential for dense\nprediction tasks that require precise spatial information.\n3.5 STM Block\nThe self-task Mamba (STM) block is responsible for learning\ntask-specific features. As illustrated in Figure 2(b), its core\nmodule is the 2D-selective-scan (SS2D) module, which is\nderived from [25]. The SS2D module is designed to address\nthe limitations of applying 1D SSMs (as discussed in Section\n3.1) to process 2D image data. As depicted in Figure 4(b),\nit unfolds the feature map along four distinct directions,\ncreating four unique feature sequences, each of which is\nthen processed by an SSM. The outputs from four SSMs are\nsubsequently added and reshaped to form a comprehensive\n2D feature map.\nFor an input feature, the STM block operates through\nseveral stages: it first employs a linear layer to expand the\nchannel number by a controllable expansion factor \u03b1. A\nconvolutional layer with a SiLU activation function is used\nto extract local features. The SS2D operation models the\nlong-range dependencies within the feature map. An input-\ndependent gating mechanism is integrated to adaptively\nselect the most salient representations derived from the\nSS2D process. Finally, another linear layer is applied to\nreduce the expanded channel number, yielding the output\nfeature. Therefore, the STM block effectively captures both\nlocal and global spatial information, which is essential for\nthe accurate learning of task-specific features in dense scene\nunderstanding tasks.\n3.6 CTM Block\nWhile the STM block excels at learning distinctive repre-\nsentations for individual tasks, it fails to establish inter-task\nconnections, which are essential for enhancing the perfor-\nmance of MTL. To address this limitation, we propose the\nnovel cross-task Mamba (CTM) block, depicted in Figure 3,\nwhich facilitates information exchange across various tasks.\nWe develop two types of CTM blocks, called F-CTM and\nS-CTM, from different perspectives to achieve cross-task\ninteraction.\n3.6.1 F-CTM: Feature-Level Interaction\nAs shown in Figure 3(a), F-CTM comprises a task-shared\nfusion block and T task-specific feature blocks, where T is\nthe number of tasks. It inputs T features and outputs T\nfeatures. For each task, the input features have a channel\ndimension of C.\nThe task-shared fusion block first concatenates all task\nfeatures, resulting in a concatenated feature with a channel\ndimension of TC. This concatenated feature is then fed\ninto a linear layer to transform the channel dimension from\nTC to \u03b1C, aligning it with the dimensions of the task-\nspecific features from the task-specific feature blocks, where\n\u03b1 is the expansion factor introduced in Section 3.5. The\ntransformed feature is subsequently processed through a\nsequence of operations \u201cConv - SiLU - SS2D\" to learn a\nglobal representation $z^{sh}$, which contains information from\nall tasks.\nIn the task-specific feature block, each task indepen-\ndently processes its own feature representation $z^t$ through\nits own sequence of operations \u201cLinear - Conv - SiLU -\nSS2D\u201d. Then, we use a task-specific and input-dependent\ngate $g^t$ to aggregate the task-specific representation $z^t$ and\nthe global representation $z^{sh}$ as $g^t \\times z^t + (1 - g^t) \\times z^{sh}$.\nHence, F-CTM allows each task to adaptively integrate\nthe cross-task representation with its own feature, promot-\ning information sharing and interaction among tasks. The\nuse of input-dependent gates ensures that each task can\nselectively emphasize either its own feature or the shared\nglobal representation based on the input data, thereby en-\nhancing the model's ability to learn discriminative features\nin a multi-task learning context.\n3.6.2 S-CTM: Semantic-Aware Interaction\nWhile feature fusion in F-CTM is an effective way to interact\nwith information, it may not be sufficient to capture all the\ncomplex relationships across different tasks, especially in\nmulti-task scene understanding where the interactions be-\ntween multiple pixel-level dense prediction tasks are highly\ndynamic and context-dependent. Thus, we propose S-CTM\nto achieve semantic-aware interaction.\nAs shown in Figure 3(b), S-CTM contains a task-shared\nfusion block and T task-specific feature blocks. The fusion\nblock first concatenates all task features and then passes the\nconcatenated feature through two convolution layers to gen-\nerate the global representation, which contains knowledge\nacross all tasks. The task-specific feature block in S-CTM is\nadapted from the STM block by replacing the SS2D with a\nnovel cross SS2D (CSS2D). The additional input of CSS2D is\nfrom the task-shared fusion block.\nAs discussed in Section 3.1, SSM only models the internal\nrelationship within a single input sequence, but it does not\ncapture the interactions between two different sequences. To\naddress this limitation, we propose the cross SSM (CSSM)\nto model the relationship between the task-specific feature\nsequence (blue) and the task-shared feature sequence (red),\nas illustrated in Figure 4(c). CSSM receives two sequences\nas input and outputs one sequence. The task-shared feature\nsequence is used to generate the SSMs parameters (i.e.,\nB, C, and \u2206), and the task-specific feature sequence is\nconsidered as the query input x. The output is computed via\nEquations (3) and (4). Consequently, by leveraging the SSM\nmechanism, CSSM can capture the interactions between\ntwo input sequences at the semantic level. Furthermore,\nwe extend SS2D as CSS2D, as shown in Figure 4(d). This\nmodule takes two 2D input features, expands them along\nfour directions to generate four pairs of feature sequences,\nand feeds each pair into a CSSM. The outputs from these\nsequences are subsequently aggregated and reshaped to\nform a 2D output feature.\nTherefore, compared with F-CTM, S-CTM can better\nlearn context-aware relationships because of the CSSM\nmechanism. CSSM can explicitly and effectively model long-\nrange spatial relationships within two sequences, allowing\nS-CTM to understand the interactions between task-specific\nfeatures and the global representation, which is critical\nfor multi-task learning scenarios. In contrast, the feature\nfusion in F-CTM makes it difficult to capture the complex\ndependencies inherent across tasks.\n3.7 Prediction Head\nAs shown in Figure 1, after the decoder, the size of task-\nspecific feature is $\\frac{H}{4} \\times \\frac{W}{4} \\times C$. Each task has its own predic-\ntion head to generate its final prediction. We introduce two\ntypes of prediction heads as follows.\n3.7.1 DenseHead\nDenseHead is inspired by [46] and is used in our prelim-\ninary version MTMamba [29]. Specifically, each head con-\ntains a patch expand operation and a final linear layer. The\npatch expanding operation, similar to the one in the ECR\nblock (as shown in Figure 2(a)), performs 4\u00d7 upsampling\nto restore the resolution of the feature maps to the original\ninput resolution $H \\times W$. The final linear layer is used to\nproject the feature channels to the task's output dimensions\nand output the final pixel-wise prediction.\n3.7.2 LiteHead\nIn DenseHead, upsampling is performed first, which can\nlead to a significant computational cost. Hence, we in-\ntroduce a more simple, lightweight, and effective head\narchitecture, called LiteHead. Specifically, it consists of a\n3\u00d73 convolutional layer, followed by a batch normalization\nlayer, a ReLU activation function, and a final linear layer that\nprojects the feature channels onto the task's output dimen-\nsions. Subsequently, the feature is simply interpolated to\nalign with the input resolution and then used as the output.\nThus, LiteHead is much more computationally efficient than\nDenseHead. Note that since each task has its own head, the\noverall computational cost reduction is linearly related to\nthe number of tasks."}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct extensive experiments to eval-\nuate the proposed MTMamba++ in multi-task dense scene\nunderstanding.\n4.1 Experimental Setups\n4.1.1 Datasets\nFollowing [7], [9], [10], we conduct experiments on\nthree multi-task dense prediction benchmark datasets:\n(i) NYUDv2 [26] contains a number of indoor scenes, in-\ncluding 795 training images and 654 testing images. It\nconsists of four tasks: 40-class semantic segmentation (Sem-\nseg), monocular depth estimation (Depth), surface nor-\nmal estimation (Normal), and object boundary detection\n(Boundary). (ii) PASCAL-Context [27], originated from the\nPASCAL dataset [47], includes both indoor and outdoor\nscenes and provides pixel-wise labels for tasks like semantic\nsegmentation, human parsing (Parsing), and object bound-\nary detection, with additional labels for surface normal\nestimation and saliency detection tasks generated by [48].\nIt contains 4,998 training images and 5,105 testing images.\n(iii) Cityscapes [28] is an urban scene understanding dataset.\nIt has two tasks (19-class semantic segmentation and dispar-\nity estimation) with 2,975 training and 500 testing images.\n4.1.2 Implementation Details\nWe use the Swin-Large Transformer [30] pretrained on the\nImageNet-22K dataset [49] as the encoder. The expansion\nfactor \u03b1 is set to 2 in both STM and CTM blocks. Follow-\ning [7], [9], [10], we resize the input images of NYUDv2,\nPASCAL-Context, and Cityscapes datasets as 448 \u00d7 576,\n512x 512, and 512 \u00d7 1024, respectively, and use the same\ndata augmentations including random color jittering, ran-\ndom cropping, random scaling, and random horizontal flip-\nping. The l\u2081 loss is used for depth estimation and surface\nnormal estimation tasks, while the cross-entropy loss is for\nother tasks. The proposed model is trained with a batch size\nof 4 for 40,000 iterations. The AdamW optimizer [50] with a\nweight decay of 1 \u00d7 10-6 and the polynomial learning rate\nscheduler are used for all three datasets. The learning rate\nis set to 2 \u00d7 10\u22125, 8 \u00d7 10\u22125, and 1 \u00d7 10\u22124 for NYUDv2,\nPASCAL-Context, and Cityscapes datasets, respectively.\n4.1.3 Evaluation Metrics\nFollowing [7], [9], [10], we adopt mean intersection over\nunion (mIoU) as the evaluation metric for semantic seg-\nmentation and human parsing tasks, root mean square error\n(RMSE) for monocular depth estimation and disparity esti-\nmation tasks, mean error (mErr) for surface normal estima-\ntion task, maximal F-measure (maxF) for saliency detection\ntask, and optimal-dataset-scale F-measure (odsF) for object\nboundary detection task. Moreover, we report the average\nrelative performance improvement of an MTL model A\nover single-task (STL) models as the overall metric, which is\ndefined as follows,\n$\\mathcal{A}_{m}(A)=100\\%\\times\\frac{1}{T}\\sum_{t=1}^{T}(\u22121)^{S_{t}}\\frac{M^{A}_{t}\u2212M^{STL}_{t}}{M^{STL}_{t}},$\nwhere T is the number of tasks, $M^{A}_{t}$ is the metric value of\nmethod A on task t, and $s_{t}$ is 0 if a larger value indicates\nbetter performance for task t, and 1 otherwise.\n4.2 Comparison with State-of-the-art Methods\nWe compare the proposed MTMamba++ method with two\ntypes of MTL methods: (i) CNN-based methods, includ-\ning Cross-Stitch [51], PAP [52], PSD [53], PAD-Net [5],\nMTI-Net [6], ATRC [54], and ASTMT [48]; and (ii) Trans-\nformer-based methods, including InvPT [7], TaskPrompter\n[10], InvPT++ [9], and MQTransformer [8].\nTable 1 provides the results on NYUDv2 and PASCAL--\nContext datasets. As can be seen, Mamba-based meth-\nods largely outperform CNN-based and Transformer-based\nmethods, especially MTMamba++, which achieves the best\nperformance in all four tasks of NYUDv2. Specifically,\nMTMamba++ significantly outperforms the previous state-\nof-the-art Transformer-based method (i.e., TaskPrompter)\nby +1.71 (mIoU) and +1.20 (odsF) in semantic segmen-\ntation and object boundary detection tasks, respectively,\nwhich demonstrates the superiority of MTMamba++. More-\nover, MTMamba++ performs better than MTMamba, show-\ning the effectiveness of S-CTM and LiteHead. The re-\nsults on PASCAL-Context also show the clear superior-\nity of Mamba-based methods. Notably, MTMamba++ sur-\npasses the Transformer-based approaches on all tasks ex-\ncept the normal prediction task, which is also compara-\nble. Specifically, compared with the best Transformer-based\napproaches, MTMamba++ achieves improvements of +1.05\n(mIoU), +3.75 (mIoU), and +4.40 (odsF) in semantic segmen-\ntation, human parsing, and object boundary detection tasks,\nrespectively, which demonstrates the superior efficacy of\nMTMamba++. Moreover, compared with MTMamba, MT-\nMamba++ outperforms on three of five tasks and achieves\ncomparable performance on the other two tasks, demon-\nstrating the effectiveness of MTMamba++ again.\nTable 2 shows the results on the Cityscapes dataset.\nWe can see that Mamba-based methods perform largely\nbetter than the previous CNN-based and Transformer-based\napproaches on both two tasks. Moreover, MTMamba++\narchives the best performance. Notably, MTMamba++ out-\nperforms the current state-of-the-art TaksPrompter by +6.72\n(mIoU) in the semantic segmentation task, demonstrating\nthat MTMamba++ is more effective. Besides, MTMamba++\nperforms better than MTMamba, which shows the effective-\nness of S-CTM and LiteHead.\nThe qualitative comparisons with state-of-the-art meth-\nods (i.e., InvPT [7], TaskPrompter [10], and MTMamba [29])\non NYUDv2, PASCAL-Context, and Cityscapes datasets are\nshown in Figures 5, 6, and 7, demonstrating that MT-\nMmaba++ provides more precise predictions and details.\n4.3 Model Analysis\nTo analyze the proposed MTMamba++, we first introduce\ntwo baselines: (i) \u201cMulti-task", "Single-task": "s the single-task counterpart of", "Multi-\ntask": "i.e., each task has a task-specific encoder-decoder).\nWithout specific instructions, the encoder in this section is\nthe Swin-Large Transformer.\n4.3.1 Effectiveness of Each Component\nThe decoders of MTMamba++ contain two types of core\nblocks: STM and CTM blocks. Compared to the preliminary\nversion MTMamba [29], MTMamba++ replaces the F-CTM\nblock and DenseHead of MTMamba with the S-CTM block\nand LiteHead, respectively. In this experiment, we study the\neffectiveness of each component on the NYUDv2 dataset.\nThe results are shown in Table 3, where #5 and #7 are\nthe default configurations of MTMamba and MTMamba++,\nrespectively.\nFirstly, the STM block outperforms the Swin Transformer\nblock [30] in terms of efficiency and effectiveness for multi-\ntask dense prediction, as indicated by the superior results in\nTable 3 (#2 vs. #3). Secondly, merely increasing the number\nof STM blocks from two to three does not enhance perfor-\nmance significantly. When the F-CTM block is incorporated,\nthe performance largely improves in terms of $\\mathcal{A}_{m}$ (#5 vs.\n#3/#4), demonstrating the effectiveness of F-CTM. Thirdly,\nthe comparison between #5 and #6 shows that LiteHead\nis more effective and efficient than DenseHead. Fourthly,\ncompared #6 with #7, we can find that replacing F-CTM with\nS-CTM leads to a significant performance improvement in\nall tasks with a tiny additional cost, demonstrating that the\nsemantic-aware interaction in S-CTM is more effective than\nF-CTM. Finally, the default configuration of MTMamba++\n(i.e., #7) significantly surpasses the \u201cSingle-task"}, {"title": "4.4 Visualization of Predictions", "content": "In this section, we compare the output predictions from MT-"}]}