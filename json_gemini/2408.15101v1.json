{"title": "MTMamba++: Enhancing Multi-Task Dense Scene Understanding via Mamba-Based Decoders", "authors": ["Baijiong Lin", "Weisen Jiang", "Pengguang Chen", "Shu Liu", "Ying-Cong Chen"], "abstract": "Multi-task dense scene understanding, which trains a model for multiple dense prediction tasks, has a wide range of application scenarios. Capturing long-range dependency and enhancing cross-task interactions are crucial to multi-task dense prediction. In this paper, we propose MTMamba++, a novel architecture for multi-task scene understanding featuring with a Mamba-based decoder. It contains two types of core blocks: self-task Mamba (STM) block and cross-task Mamba (CTM) block. STM handles long-range dependency by leveraging state-space models, while CTM explicitly models task interactions to facilitate information exchange across tasks. We design two types of CTM block, namely F-CTM and S-CTM, to enhance cross-task interaction from feature and semantic perspectives, respectively. Experiments on NYUDv2, PASCAL-Context, and Cityscapes datasets demonstrate the superior performance of MTMamba++ over CNN-based and Transformer-based methods. The code is available at https://github.com/EnVision-Research/MTMamba.", "sections": [{"title": "1 INTRODUCTION", "content": "MULTI-task dense scene understanding has a variety of practical applications in computer vision [1], such as autonomous driving [2], healthcare [3], and robotics [4]. This problem is challenging because it requires training a model to simultaneously handle multiple dense prediction tasks, such as semantic segmentation, monocular depth estimation, surface normal estimation, and object boundary detection.\nThe widely adopted architecture for multi-task dense scene understanding is based on the encoder-decoder framework, which features a common encoder for extracting task-shared features and multiple decoders for generating task-specific predictions [1]. This framework is very general and many variants [5]\u2013[10] have been proposed to improve its performance. One promising approach is the decoder-focused method [1], which aims to enhance cross-task interaction in task-specific decoders by well-designed fusion modules. For example, derived from convolutional neural networks (CNNs), PAD-Net [5] and MTI-Net [6] use a multi-modal distillation module to enhance information exchange across tasks in the decoder and achieve better performance than the conventional encoder-decoder framework.\nSince the convolution operation mainly captures local features and may struggle with modeling long-range dependencies [11], [12], recent methods [7]\u2013[10] adopt Transformer-based decoders with attention-based fusion modules. These methods leverage the attention mechanism [13] to capture global context information, resulting in better performance than CNN-based methods. These works demonstrate that enhancing cross-task correlation and modeling long-range spatial relationships are crucial to multi-task dense prediction.\nRecently, Mamba [14], a new network architecture derived from state space models (SSMs) [15], [16], has demonstrated better capacity in long-range dependencies modeling and superior performance than Transformers in various domains, including language modeling [14], [17], [18], graph reasoning [19], [20], medical images analysis [21], [22], and image generation [23], [24]. However, existing works on Mamba are limited to single-task learning scenarios. Using Mamba to solve multi-task problems is still unexplored since it needs to model the cross-task interaction, which is challenging.\nTo fill these gaps, in this paper, we propose MTMamba++, a novel multi-task architecture featuring a Mamba-based decoder. As the overall framework shown in Figure 1, MTMamba++ is a decoder-focused method and the decoder consists of three stages. These three stages share a similar structure and contain three types of blocks: ECR, STM, and CTM blocks. Specifically, the ECR (expand, concatenate, and reduce) block is responsible for upscaling the task feature and fusing it with the high-level feature from the encoder. It allows each decoder stage to incrementally increase the spatial resolution of the feature maps, which plays an important role in precise pixel-level predictions. The core components of the decoder are two Mamba-based"}, {"title": "2 RELATED WORKS", "content": "Multi-task learning (MTL) is a learning paradigm that aims to jointly learn multiple related tasks using a single model [31]. Current MTL research mainly focuses on multi-objective optimization [32]\u2013[37] and network architecture"}, {"title": "3 METHODOLOGY", "content": "In this section, we begin with the foundational knowledge of state space models (Section 3.1) and provide an overview of the proposed MTMamba++ in Section 3.2. Next, we delve into a detailed exploration of each component in the decoder of MTMamba++, including the encoder in Section 3.3, three types of block in the decoder (i.e., the ECR block in Section 3.4, the STM block in Section 3.5, and the CTM block in Section 3.6), and the prediction head in Section 3.7."}, {"title": "3.1 Preliminaries", "content": "SSMs [14]\u2013[16], derived from the linear systems theory [40], map an input sequence \\(x(t) \\in \\mathbb{R}\\) to an output sequence \\(y(t) \\in \\mathbb{R}\\) through a hidden state \\(h\\in \\mathbb{R}^N\\) using a linear ordinary differential equation:\n\\begin{equation}\nh'(t) = Ah(t) + Bx(t),\n\\end{equation}\n\\begin{equation}\ny(t) = Ch(t) + Dx(t),\n\\end{equation}\nwhere \\(A \\in \\mathbb{R}^{N\\times N}\\) is the state transition matrix, \\(B \\in \\mathbb{R}^N\\) and \\(C\\in \\mathbb{R}^N\\) are projection matrices, and \\(D \\in \\mathbb{R}\\) is the skip connection. Equation (1) defines the evolution of the hidden state h(t), while Equation (2) specifies that the output is derived from a linear transformation of the hidden state h(t) combined with a skip connection from the input x(t).\nGiven that continuous-time systems are not compatible with digital computers and the discrete nature of real-world data, a discretization process is essential. This process approximates the continuous-time system with a discrete-time one. Let \\(\\Delta \\in \\mathbb{R}\\) be a discrete-time step size. Equations (1) and (2) are discretized as\n\\begin{equation}\nh_t = \\bar{A}h_{t-1} + \\bar{B}x_t,\n\\end{equation}\n\\begin{equation}\nY_t = Ch_t + Dx_t,\n\\end{equation}\nwhere \\(x_t = x(t)\\), and\n\\begin{equation}\n\\bar{A} = \\exp(\\Delta A),\n\\bar{B} = (\\Delta A)^{-1}(\\exp(\\Delta A) \u2013 I)\\cdot \\Delta B \\approx \\Delta B,\nC = C.\n\\end{equation}\nS4 [16] treats A, B, C, and \\(\\Delta\\) as trainable parameters and optimizes them by gradient descent. However, these parameters do not explicitly depend on the input sequence, which can lead to suboptimal extraction of contextual information. To address this limitation, Mamba [14] introduces a new SSM, namely S6. As illustrated in Figure 4(a), it incorporates an input-dependent selection mechanism that enhances the system\u2019s ability to discern and select relevant information contingent upon the input sequence. Specifically, B, C, and A are defined as functions of the input \\(x \\in \\mathbb{R}^{B\\times L\\times C}\\). Following the computation of these parameters, A, B, and C are calculated via Equation (5). Subsequently, the output sequence \\(y \\in \\mathbb{R}^{B\\times L\\times \\hat{C}}\\) is computed by Equations (3) and (4), thereby improving the contextual information extraction. Without specific instructions, in this paper, S6 [14] is used in the SSM mechanism."}, {"title": "3.2 Overall Architecture", "content": "An overview of MTMamba++ is illustrated in Figure 1. It contains three components: an off-the-shelf encoder, a Mamba-based decoder, and task-specific prediction heads. Specifically, the encoder is shared across all tasks and plays a pivotal role in extracting multi-scale generic visual representations from the input image. The decoder consists of three stages, each of which progressively expands the spatial dimensions of the feature maps. This expansion is crucial for dense prediction tasks, as the resolution of the feature maps directly impacts the accuracy of the pixel-level predictions [7]. Each decoder stage is equipped with the ECR block designed to upsample the feature and integrate it with high-level features derived from the encoder. Following this, the STM block is employed to capture the long-range spatial relationship for each task. Additionally, the CTM block facilitates feature enhancement for each task by promoting knowledge exchange across different tasks. We design two types of CTM block, namely F-CTM and S-CTM, as introduced in Section 3.6. In the end, a prediction head is used to generate the final prediction for each task. We introduce two types of head, called DenseHead and LiteHead, as described in Section 3.7.\nMTMamba++ and our preliminary version MTMamba [29] have a similar architecture. The default configuration for MTMamba++ utilizes the S-CTM block and LiteHead, while the default configuration for MTMamba employs the F-CTM block and DenseHead."}, {"title": "3.3 Encoder", "content": "The encoder in MTMamba++ is shared across different tasks and is designed to learn generic multi-scale visual features from the input RGB image. As an example, we consider the Swin Transformer [30], which segments the input image into non-overlapping patches. Each patch is treated as a token, and its feature representation is a concatenation of the raw RGB pixel values. After patch segmentation, a linear layer is applied to project the raw token into a C-dimensional feature embedding. The projected tokens then sequentially pass through four stages of the encoder. Each stage comprises multiple Swin Transformer blocks and a patch merging layer. The patch merging layer is specifically utilized to downsample the spatial dimensions by a factor of 2x and expand the channel numbers by a factor of 2x, while the Swin Transformer blocks are dedicated to learning and refining the feature representations. Finally, for an input image with dimensions \\(H \\times W \\times 3\\), where H and W denote the height and width, the encoder generates hierarchical feature representations at four different scales, i.e., \\(\\frac{H}{4} \\times \\frac{W}{4} \\times C, \\frac{H}{8} \\times \\frac{W}{8} \\times 2C, \\frac{H}{16} \\times \\frac{W}{16} \\times 4C\\), and \\(\\frac{H}{32} \\times \\frac{W}{32} \\times 8C\\)."}, {"title": "3.4 ECR Block", "content": "The ECR (expand, concatenate, and reduce) block is responsible for upsampling the feature and aggregating it with the encoder's feature. As illustrated in Figure 2(a), it contains three steps. For an input feature, ECR block first 2x upsamples the feature resolution and 2x reduces the channel number by a linear layer and the rearrange operation. Then, the feature is fused with the high-level feature from the encoder through skip connections. Fusing these features is crucial for compensating the loss of spatial information that occurs due to downsampling in the encoder. Finally, a 1 \u00d7 1 convolutional layer is used to reduce the channel number. Consequently, the ECR block facilitates the efficient recovery of high-resolution details, which is essential for dense prediction tasks that require precise spatial information."}, {"title": "3.5 STM Block", "content": "The self-task Mamba (STM) block is responsible for learning task-specific features. As illustrated in Figure 2(b), its core module is the 2D-selective-scan (SS2D) module, which is derived from [25]. The SS2D module is designed to address the limitations of applying 1D SSMs (as discussed in Section 3.1) to process 2D image data. As depicted in Figure 4(b), it unfolds the feature map along four distinct directions, creating four unique feature sequences, each of which is then processed by an SSM. The outputs from four SSMs are subsequently added and reshaped to form a comprehensive 2D feature map.\nFor an input feature, the STM block operates through several stages: it first employs a linear layer to expand the channel number by a controllable expansion factor \\(\\alpha\\). A convolutional layer with a SiLU activation function is used to extract local features. The SS2D operation models the long-range dependencies within the feature map. An input-dependent gating mechanism is integrated to adaptively select the most salient representations derived from the SS2D process. Finally, another linear layer is applied to reduce the expanded channel number, yielding the output feature. Therefore, the STM block effectively captures both local and global spatial information, which is essential for the accurate learning of task-specific features in dense scene understanding tasks."}, {"title": "3.6 CTM Block", "content": "While the STM block excels at learning distinctive representations for individual tasks, it fails to establish inter-task connections, which are essential for enhancing the performance of MTL. To address this limitation, we propose the novel cross-task Mamba (CTM) block, depicted in Figure 3, which facilitates information exchange across various tasks. We develop two types of CTM blocks, called F-CTM and S-CTM, from different perspectives to achieve cross-task interaction."}, {"title": "3.6.1 F-CTM: Feature-Level Interaction", "content": "As shown in Figure 3(a), F-CTM comprises a task-shared fusion block and T task-specific feature blocks, where T is the number of tasks. It inputs T features and outputs T features. For each task, the input features have a channel dimension of C.\nThe task-shared fusion block first concatenates all task features, resulting in a concatenated feature with a channel dimension of TC. This concatenated feature is then fed into a linear layer to transform the channel dimension from TC to \\(\\alpha C\\), aligning it with the dimensions of the task-specific features from the task-specific feature blocks, where \\(\\alpha\\) is the expansion factor introduced in Section 3.5. The transformed feature is subsequently processed through a sequence of operations \u201cConv - SiLU - SS2D\u201d to learn a global representation \\(z^{sh}\\), which contains information from all tasks.\nIn the task-specific feature block, each task independently processes its own feature representation \\(z^t\\) through its own sequence of operations \u201cLinear - Conv - SiLU - SS2D\u201d. Then, we use a task-specific and input-dependent gate \\(g^t\\) to aggregate the task-specific representation \\(z^t\\) and the global representation \\(z^{sh}\\) as \\(g^t \\times z^t + (1 - g^t) \\times z^{sh}\\).\nHence, F-CTM allows each task to adaptively integrate the cross-task representation with its own feature, promoting information sharing and interaction among tasks. The use of input-dependent gates ensures that each task can selectively emphasize either its own feature or the shared global representation based on the input data, thereby enhancing the model\u2019s ability to learn discriminative features in a multi-task learning context."}, {"title": "3.6.2 S-CTM: Semantic-Aware Interaction", "content": "While feature fusion in F-CTM is an effective way to interact with information, it may not be sufficient to capture all the complex relationships across different tasks, especially in multi-task scene understanding where the interactions between multiple pixel-level dense prediction tasks are highly dynamic and context-dependent. Thus, we propose S-CTM to achieve semantic-aware interaction.\nAs shown in Figure 3(b), S-CTM contains a task-shared fusion block and T task-specific feature blocks. The fusion block first concatenates all task features and then passes the concatenated feature through two convolution layers to generate the global representation, which contains knowledge across all tasks. The task-specific feature block in S-CTM is adapted from the STM block by replacing the SS2D with a novel cross SS2D (CSS2D). The additional input of CSS2D is from the task-shared fusion block.\nAs discussed in Section 3.1, SSM only models the internal relationship within a single input sequence, but it does not capture the interactions between two different sequences. To address this limitation, we propose the cross SSM (CSSM) to model the relationship between the task-specific feature sequence (blue) and the task-shared feature sequence (red), as illustrated in Figure 4(c). CSSM receives two sequences as input and outputs one sequence. The task-shared feature sequence is used to generate the SSMs parameters (i.e., B, C, and \\(\\Delta\\)), and the task-specific feature sequence is considered as the query input x. The output is computed via Equations (3) and (4). Consequently, by leveraging the SSM mechanism, CSSM can capture the interactions between two input sequences at the semantic level. Furthermore, we extend SS2D as CSS2D, as shown in Figure 4(d). This module takes two 2D input features, expands them along four directions to generate four pairs of feature sequences, and feeds each pair into a CSSM. The outputs from these sequences are subsequently aggregated and reshaped to form a 2D output feature.\nTherefore, compared with F-CTM, S-CTM can better learn context-aware relationships because of the CSSM mechanism. CSSM can explicitly and effectively model long-range spatial relationships within two sequences, allowing S-CTM to understand the interactions between task-specific features and the global representation, which is critical for multi-task learning scenarios. In contrast, the feature fusion in F-CTM makes it difficult to capture the complex dependencies inherent across tasks."}, {"title": "3.7 Prediction Head", "content": "As shown in Figure 1, after the decoder, the size of task-specific feature is \\(\\frac{H}{4} \\times \\frac{W}{4} \\times C\\). Each task has its own prediction head to generate its final prediction. We introduce two types of prediction heads as follows."}, {"title": "3.7.1 DenseHead", "content": "DenseHead is inspired by [46] and is used in our preliminary version MTMamba [29]. Specifically, each head contains a patch expand operation and a final linear layer. The patch expanding operation, similar to the one in the ECR block (as shown in Figure 2(a)), performs 4x upsampling to restore the resolution of the feature maps to the original input resolution H \u00d7 W. The final linear layer is used to project the feature channels to the task\u2019s output dimensions and output the final pixel-wise prediction."}, {"title": "3.7.2 LiteHead", "content": "In DenseHead, upsampling is performed first, which can lead to a significant computational cost. Hence, we introduce a more simple, lightweight, and effective head architecture, called LiteHead. Specifically, it consists of a 3 \u00d7 3 convolutional layer, followed by a batch normalization layer, a ReLU activation function, and a final linear layer that projects the feature channels onto the task\u2019s output dimensions. Subsequently, the feature is simply interpolated to align with the input resolution and then used as the output. Thus, LiteHead is much more computationally efficient than DenseHead. Note that since each task has its own head, the overall computational cost reduction is linearly related to the number of tasks."}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct extensive experiments to evaluate the proposed MTMamba++ in multi-task dense scene understanding."}, {"title": "4.1 Experimental Setups", "content": "Following [7], [9], [10], we conduct experiments on three multi-task dense prediction benchmark datasets: (i) NYUDv2 [26] contains a number of indoor scenes, including 795 training images and 654 testing images. It consists of four tasks: 40-class semantic segmentation (Semseg), monocular depth estimation (Depth), surface normal estimation (Normal), and object boundary detection (Boundary). (ii) PASCAL-Context [27], originated from the PASCAL dataset [47], includes both indoor and outdoor scenes and provides pixel-wise labels for tasks like semantic segmentation, human parsing (Parsing), and object boundary detection, with additional labels for surface normal estimation and saliency detection tasks generated by [48]. It contains 4,998 training images and 5,105 testing images. (iii) Cityscapes [28] is an urban scene understanding dataset. It has two tasks (19-class semantic segmentation and disparity estimation) with 2,975 training and 500 testing images."}, {"title": "4.1.2 Implementation Details", "content": "We use the Swin-Large Transformer [30] pretrained on the ImageNet-22K dataset [49] as the encoder. The expansion factor \\(\\alpha\\) is set to 2 in both STM and CTM blocks. Following [7], [9], [10], we resize the input images of NYUDv2, PASCAL-Context, and Cityscapes datasets as 448 \u00d7 576, 512 \u00d7 512, and 512 \u00d7 1024, respectively, and use the same data augmentations including random color jittering, random cropping, random scaling, and random horizontal flipping. The l\u2081 loss is used for depth estimation and surface normal estimation tasks, while the cross-entropy loss is for other tasks. The proposed model is trained with a batch size of 4 for 40,000 iterations. The AdamW optimizer [50] with a weight decay of 1 \u00d7 10-6 and the polynomial learning rate scheduler are used for all three datasets. The learning rate is set to 2 \u00d7 10\u22125, 8 \u00d7 10\u22125, and 1 \u00d7 10\u22124 for NYUDv2, PASCAL-Context, and Cityscapes datasets, respectively."}, {"title": "4.1.3 Evaluation Metrics", "content": "Following [7], [9], [10], we adopt mean intersection over union (mIoU) as the evaluation metric for semantic segmentation and human parsing tasks, root mean square error (RMSE) for monocular depth estimation and disparity estimation tasks, mean error (mErr) for surface normal estimation task, maximal F-measure (maxF) for saliency detection task, and optimal-dataset-scale F-measure (odsF) for object boundary detection task. Moreover, we report the average relative performance improvement of an MTL model A over single-task (STL) models as the overall metric, which is defined as follows,\n\\begin{equation}\nA_m(A) = 100\\% \\times \\frac{1}{T} \\sum_{t=1}^{T} (-1)^{s_t} \\frac{M_t^A - M_t^{STL}}{M_t^{STL}},\n\\end{equation}\nwhere T is the number of tasks, \\(M_t^A\\) is the metric value of method A on task t, and \\(s_t\\) is 0 if a larger value indicates better performance for task t, and 1 otherwise."}, {"title": "4.2 Comparison with State-of-the-art Methods", "content": "We compare the proposed MTMamba++ method with two types of MTL methods: (i) CNN-based methods, including Cross-Stitch [51], PAP [52], PSD [53], PAD-Net [5],"}, {"title": "4.3 Model Analysis", "content": "To analyze the proposed MTMamba++, we first introduce two baselines: (i) \u201cMulti-task\u201d represents an MTL model using two standard Swin Transformer blocks [30] after the ECR block in each decoder stage for each task; and (ii) \u201cSingle-task\u201d is the single-task counterpart of \u201cMulti-task\u201d (i.e., each task has a task-specific encoder-decoder).\nWithout specific instructions, the encoder in this section is the Swin-Large Transformer."}, {"title": "4.3.1 Effectiveness of Each Component", "content": "The decoders of MTMamba++ contain two types of core blocks: STM and CTM blocks. Compared to the preliminary version MTMamba [29], MTMamba++ replaces the F-CTM block and DenseHead of MTMamba with the S-CTM block and LiteHead, respectively. In this experiment, we study the effectiveness of each component on the NYUDv2 dataset.\nThe results are shown in Table 3, where #5 and #7 are the default configurations of MTMamba and MTMamba++, respectively.\nFirstly, the STM block outperforms the Swin Transformer block [30] in terms of efficiency and effectiveness for multi-task dense prediction, as indicated by the superior results in Table 3 (#2 vs. #3). Secondly, merely increasing the number of STM blocks from two to three does not enhance performance significantly. When the F-CTM block is incorporated, the performance largely improves in terms of \\(A_m\\) (#5 vs. #3/#4), demonstrating the effectiveness of F-CTM. Thirdly, the comparison between #5 and #6 shows that LiteHead is more effective and efficient than DenseHead. Fourthly, compared #6 with #7, we can find that replacing F-CTM with S-CTM leads to a significant performance improvement in all tasks with a tiny additional cost, demonstrating that the semantic-aware interaction in S-CTM is more effective than F-CTM. Finally, the default configuration of MTMamba++ (i.e., #7) significantly surpasses the \u201cSingle-task\u201d approach across all tasks, thereby demonstrating the effectiveness of MTMamba++."}, {"title": "4.3.2 Comparison between SSM and Attention", "content": "To demonstrate the superiority of the SSM-based architecture in multi-task dense prediction, we conduct an experiment on NYUDv2 by replacing the SSM-related operations in MTMamba++ with attention-based mechanisms. Specifically, the SS2D module in the STM block is replaced with the window-based multi-head self-attention [30], while the CSS2D module in the S-CTM block is substituted with the window-based multi-head cross-attention. The results are shown in Table 4. As can be seen, the SSM-based method (i.e., MTMamba++) is more effective and efficient than the attention-based method, which demonstrates that SSM is more powerful in dealing with multi-task dense prediction tasks."}, {"title": "4.3.3 Effectiveness of Each Decoder Stage", "content": "As shown in Figure 1, the decoder of MTMamba++ consists of three stages. In this experiment, we study the effectiveness of these three stages on the NYUDv2 dataset. Table 5 shows the results, which clearly demonstrate that each stage contributes positively to the performance of MTMamba++."}, {"title": "4.3.4 Effect of Each Scan in CSS2D Module", "content": "As mentioned in Section 3.6.2, the CSS2D module scans the 2D feature map from four different directions. We conduct an experiment on NYUDv2 to study the effect of each scan. The results are presented in Table 6. As can be seen, dropping any direction leads to a performance drop compared with the default configuration that uses all directions, showing that all directions are beneficial to MTMamba++."}, {"title": "4.3.5 Effect of \\(\\alpha\\)", "content": "As mentioned in Sections 3.5 and 3.6.2, in MTMamba++, both STM and S-CTM blocks expand the feature channel to improve the model capacity by a hyperparameter \\(\\alpha\\). We conduct an experiment on NYUDv2 to study the effect of \\(\\alpha\\).\nThe results are shown in Table 7. As can be seen, increasing \\(\\alpha\\) leads to a significant increase in model size and FLOPS cost. As for performance, \\(\\alpha\\) = 2 performs the best and is thus used as the default configuration in MTMamba++."}, {"title": "4.3.6 Performance on Different Encoders", "content": "We perform an experiment on NYUDv2 to investigate the performance of the proposed MTMamba++ with different scales of Swin Transformer encoder. The results are shown in Table 8. As can be seen, as the model capacity increases, MTMamba++ performs better on all tasks accordingly. Moreover, MTMamba++ consistently outperforms MTMamba on different encoders, confirming the effectiveness of the proposed S-CTM and LiteHead."}, {"title": "4.3.7 Analysis of Model Size and Cost", "content": "In Table 9, we compare the model size and FLOPs between the proposed MTMamba++ and baselines on the PASCAL-Context dataset. As can be seen, with comparable parameter size and computational cost, MTMamba++ significantly outperforms previous methods on four of five tasks."}, {"title": "4.4 Visualization of Predictions", "content": "In this section, we compare the output predictions from MTMamba++ against the state-of-the-art methods, including InvPT [7], TaskPrompter [10], and MTMamba [29]. Figures 5, 6, and 7 show the qualitative results on NYUDv2, PASCAL-Context, and Cityscapes datasets, respectively. As can be seen, MTMamba++ has better visual results than baselines in all datasets. For example, as highlighted with yellow circles in Figure 5, MTMamba++ generates more accurate results with better alignments for the semantic segmentation task. Figure 6 demonstrates that MTMamba++ produces better predictions with more accurate details (as highlighted in yellow circles) for both semantic segmentation and human parsing tasks and more distinct boundaries for the object boundary detection task. Similarly, Figure 7 illustrates that MTMamba++ delivers more precise segmentation with superior alignment. Hence, both qualitative study (Figures 5, 6, and 7) and quantitative study (Tables 1 and 2) show the superior performance of MTMamba++."}, {"title": "5 CONCLUSION", "content": "In this paper, we propose MTMamba++, a novel multi-task architecture with a Mamba-based decoder for multi-task dense scene understanding. With two types of core blocks (i.e., STM and CTM blocks), MTMamba++ can effectively model long-range dependency and achieve cross-task interaction. We design two variants of the CTM block to promote knowledge exchange across tasks from the feature and semantic perspectives, respectively. Experiments on three benchmark datasets demonstrate that the proposed MTMamba++ achieves better performance than previous CNN-based and Transformer-based methods."}]}