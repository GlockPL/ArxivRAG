{"title": "CRPO: Confidence-Reward Driven Preference Optimization for Machine Translation", "authors": ["Guofeng Cui", "Pichao Wang", "Yang Liu", "Zemian Ke", "Zhe Liu", "Vimal Bhat"], "abstract": "Large language models (LLMs) have shown great potential in natural language processing tasks, but their application to machine translation (MT) remains challenging due to pretraining on English-centric data and the complexity of reinforcement learning from human feedback (RLHF). Direct Preference Optimization (DPO) has emerged as a simpler and more efficient alternative, but its performance depends heavily on the quality of preference data. To address this, we propose Confidence-Reward driven Preference Optimization (CRPO), a novel method that combines reward scores with model confidence to improve data selection for fine-tuning. CRPO selects challenging sentence pairs where the model is uncertain or underperforms, leading to more effective learning. While primarily designed for LLMS, CRPO also generalizes to encoder-decoder models like NLLB, demonstrating its versatility. Empirical results show that CRPO outperforms existing methods such as RS-DPO, RSO and MBR score in both translation accuracy and data efficiency.", "sections": [{"title": "1 Introduction", "content": "Recent advances in decoder-only large language models (LLMs), such as GPT series (Achiam et al., 2023), LLaMA (Touvron et al., 2023; Dubey et al., 2024), and Falcon (Almazrouei et al., 2023), have showcased their outstanding ability to understand context and perform various natural language processing (NLP) tasks. However, applying LLMs to machine translation (MT) remains a challenging endeavor, especially due to their pretraining on predominantly English-centric datasets. This limitation has generated significant interest in aligning LLMs for translation tasks using further training methods, with particular attention to enhancing their multilingual performance.\nTo mitigate the linguistic bias inherent in LLMs, instruction tuning has become a widely adopted approach. Instruction tuning fine-tunes LLMs using multilingual datasets and translation-specific instructions, with the goal of expanding linguistic diversity and improving translation quality (Yang et al., 2023b; Chen et al., 2023; Zhu et al., 2023b; Zhang et al., 2023). Despite these efforts, gaps remain between the performance of LLMs and specialized machine translation models (Zhu et al., 2023a). To address these challenges, approaches such as reinforcement learning from human feedback (RLHF) (Christiano et al., 2017) have been explored. RLHF allows LLMs to align with human preferences by training a reward model on human-annotated preference data and fine-tuning the LLM to maximize the predicted reward for translation quality. For example, Xu et al. (2024b) construct a preference translation dataset using multilingual books and fine-tune LLaMA-2 with RLHF to optimize translation performance.\nHowever, RLHF introduces several complexities that hinder its efficiency. These include the need for multiple components\u2014a reward model, a policy model, a reference policy, and a value model-which significantly increase memory and computational overhead. Additionally, the robustness of RLHF is a concern due to the disjoint training of the reward and policy models. To address these limitations, Direct Preference Optimization (DPO) (Rafailov et al., 2024) and SLiC (Zhao et al., 2022, 2023) have emerged as more efficient alternatives. These methods directly fine-tune LLMs using human preference data, bypassing the complexity of RLHF. By optimizing the model through closed-form solutions of preference objectives, DPO and SLiC have shown promise in machine translation tasks, particularly in reducing computational complexity while maintaining strong performance (Zeng et al., 2024; Wu et al., 2024)."}, {"title": "2 Preliminaries", "content": "To fine-tune LLMs with human preference annotation on machine translation, translation sentence pair given each source sentence is collected from reference policy \\( \\tau_{ref} \\), larger LLMs such as GPT-4 or human annotator. We define the human preference dataset as \\( D = \\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\\}_{i=1}^{N} \\), where \\( x^{(i)} \\) refers to the ith source sentence, \\( y_w^{(i)} \\) and"}, {"title": "3 CRPO: Confidence-Reward Driven Preference Optimization", "content": "Instead of reward, we consider the acceptance of sentence pair with the value of optimization loss in two ways, loss value and loss change. A higher loss value indicates that the policy achieves worse performance and the information of related data has not been learnt sufficiently. Similarly, a large loss change during training indicates that the policy extracts useful information from the related data to reduce the prediction confusion or even correct the error prediction. Thus the sentence pairs with either high loss value or loss change are potential to benefit model fine-tuning. In this section, we analyze these two terms on DPO loss and derive two formulations of Confidence-Reward Score (CR-Score) for data selection respectively, Confidence-Reward Plus (CR+) to measure loss change and Confidence-Reward Multiplication (CR\u00d7) to measure loss value. Although the derivation is different, we will show that both these two scores share the idea of combining model confidence with sentence reward."}, {"title": "3.1 CR+: Derivation from Loss Change", "content": "We start with the derivation from loss change. For the reason that both log and \\(\\sigma\\) are monotonic increasing functions, we simplify the loss change as the difference of minus term inside \\(\\sigma\\) function of Equation 2 during training. Formally, taking two parameters \\(\\theta_1\\) and \\(\\theta_2\\), the loss change is defined as:\n\\[\\Delta L :=[log \\pi_{\\theta_2} (y_w|x) \u2013 log \\pi_{\\theta_2} (y_l|x)]+[log \\pi_{\\theta_1} (y_l|x) \u2013 log \\pi_{\\theta_1} (y_w|x)]\\]\nwhere \\(\\theta_1\\) is the parameter before the fine-tuning and we set \\(\\pi_{\\theta_1}\\) to be \\( \\tau_{ref} \\). \\(\\theta_2\\) is the parameter"}, {"title": "3.2 CRx: Derivation from Loss Value", "content": "We then consider the derivation from loss value. For the reason that in the first iteration \\(\\pi_{\\theta}\\) is always set to be \\( \\tau_{ref} \\), \\(L_{DPO}\\) is a constant and cannot be used for data selection. So we base on \\(L_{CPO}\\) instead to evaluate sentence pairs. Similar to CR+, we only consider the minus term inside \\(\\sigma\\) function of Equation 3 and construct a more general format as follow:\n\\[L(\\pi_{\\theta}) = \\gamma(R(x, y_w) \u2013 R(x, y_l))\u00b7[log \\pi_{\\theta}(y_w|x) \u2013 log \\pi_{\\theta}(y_l|x)]\\]\nwhere \\(\\gamma(\u00b7)\\) is a mapping function measuring the correlation between reward score R(x, y) and the translation quality of sentence. For CPO loss, \\(\\gamma(\u00b7)\\) is set to be the following format:\n\\[\\gamma(\u00b7) = \\begin{cases}\n1 & \\text{for } R(x, y_w) > R(x, y_l) \\\\\n-1 & \\text{otherwise}\n\\end{cases}\\]\nwhere the reward model is totally trusted. In this case, a sentence with higher reward is considered to have sufficient quality advantage compared to that with lower reward, regardless of the reward difference. This format of \\(\\gamma(\u00b7)\\) does not fit our goal for two reasons. Firstly, error exists in reward model and a margin needs to be maintained for reward difference. Secondly, it is not reasonable to force the policy to separate sentence pairs with small reward difference. As a result, we need the \\(\\gamma(\u00b7)\\) to represent the reward gap in order to measure the quality and trustness of sentence pair. In practise, with point-wise reward model outputting reward within the range of [0, 1], we set:\n\\[\\gamma(R(x,y_w) \u2013 R(x,y_l)) = R(x,y_w) \u2013 R(x,y_l)\\]\nthe value of which also falls within the range of [0,1] when \\(R(x,y_w) > R(x,y_l)\\). Specially, for a ground truth and an irrelevant sentence outputs, the ideal value of \\(R(x, y_w) \u2013 R(x, y_l)\\) is closed to 1. For the reason that a larger \\(L_{CPO}\\) desire a smaller minus term inside \\(\\sigma\\) function, we define CR\u00d7 as the minus of \\(L\\) as:\n\\[CR_{\\times} := -L(\\tau_{ref}) = [R(x, y_w) \u2013 R(x, y_l)]\u00b7[log \\tau_{ref} (y_l|x) \u2013 log \\tau_{ref} (y_w|x)]\\]\nwhich is the multiplication of the reward term and the confidence term. Similar to CR+, CR\u00d7 also"}, {"title": "3.3 Further Discussion", "content": "Comparison between CR+ and CR\u00d7. Although CR+ is derived from loss change and CR\u00d7 from loss value, both scores incorporate reward and confidence terms, aiming to maximize the discrepancy between the reward and the policy \\( \\tau_{ref} \\). While it is possible to multiply CR+ with an additional reward term, as in CR\u00d7, this would introduce redundancy, as CR+ already contains a reward component. The key distinction between the two lies in the way they handle the magnitude difference between the reward and confidence terms. In CR+, this difference necessitates careful tuning of the hyperparameter K, which not only adjusts for the reliability of the reward model but also bridges the gap between the reward and confidence scales. In contrast, CR\u00d7 naturally balances the two terms through multiplication, eliminating the need for such manual adjustments. In practice, for a specific task and LLM, we estimate K by selecting reward and confidence values that best approximate a balanced contribution from both terms, ensuring robustness across various settings.\nWhy CR-Score? In machine translation, methods like DPO and CPO have proven effective for fine-tuning LLMs, but the challenge of selecting high-quality preference data remains unresolved. CR-Score offers a systematic approach to evaluate the potential contribution of sentence pairs before the actual model training, thereby guiding more informed data selection. Unlike RS-DPO and RSO, which focus primarily on reward scores, CR-Score incorporates the likelihood of sentence generation by the LLM. This enables the exclusion of \"easy\" sentence pairs\u2014those where the model already performs well\u2014focusing instead on pairs where the model is uncertain, maximizing the impact of each data point on fine-tuning.\nCRPO Algorithm. The algorithm for data selection with CR-Score is outlined in Appendix A. Instead of evaluating all sentence pairs, we begin by selecting the sentence with the highest reward score, \\(y_w\\), to ensure a baseline of sentence quality. This approach, similar to that used in CPO and recent DPO applications (e.g., LLaMA-3 (Dubey et al., 2024)), enhances fine-tuning by prioritizing"}, {"title": "4 Related Works", "content": "Preference Optimization for Machine Translation. To align LLMs with human preference and enhance their translation ability, RLHF is introduced to fine-tune the language model (Christiano et al., 2017). In order to improve the robustness and efficiency of RLHF, DPO (Rafailov et al., 2024) calculates closed-form solution on RLHF object to optimize BT model and directly train LLMs on preference dataset. CPO (Xu et al., 2024a) develops upon DPO to release the complexity caused by the requirement of reference model and add SFT term for behavior cloning. Although these preference optimization methods achieve dramatic success, the offline training strategy causes their sensitivity toward the quality of preference data. To address this problem, we analyze the loss value and loss change and propose CR-Score to effectively select essential sentence pair to reach the DPO objective.\nRejection Sampling. To select preference data for alignment, rejection sampling is a widely adopted method. RSO (Liu et al., 2023) introduces statistical rejection sampling (Neal, 2003) and sample preference sentences from target policy distrbution. ReST (Gulcehre et al., 2023) iteratively increase reward threshold and apply rejection sampling to select higher quality sentence for further RLHF step. RS-DPO (Khaki et al., 2024) instead sets the threshold of reward difference between sentence pairs and only maintains those with large enough preference difference. Specifically for the machine translation task, the MBR score (Yang et al., 2023a; Finkelstein et al., 2023) leverages reference-based metric to estimate the expected utility of each candidate translation in relation to the set of pseudo-references. To reduce the computational complexity, Finkelstein et al. (2023) further consider to score translations with QE metric and fine-tune LLM with the best translation result. However, these sampling methods only focus on reward value neglecting the performance of the pre-"}, {"title": "5 Experiments", "content": "We evaluate CRPO on machine translation task and compare it with five baselines, evaluated with COMET and BLEURT (Sellam et al., 2020) metrics. Moreover, we adopt ablation studies to consider more data selection strategies and the effect of reward and confidence term on CR-Score."}, {"title": "5.1 Dataset", "content": "Following CPO (Xu et al., 2024a), we consider 10 translation directions in this paper: en \u2194 zh, en de, en \u2194 cs, en \u2194 is, en \u2194 ru. Our preference training dataset of machine translation task is derived from FLORES-200 dataset (Costa-juss\u00e0 et al., 2022) with the same source sentences applied to fine-tune ALMA (Xu et al., 2023) in CPO. In the training dataset, 3,065 source sentences are contained in each of en zh and en de, and 2,009 source sentences are contained in each of other translation directions. In total, 24,314 source sentences are included. Note that ALMA is also pretrained on a subset of FLORES-200 dataset, we collect 64 candidate sentences for each source sentence with the pretrained ALMA to release distribution shift problem which results in 784,640 candidate translation sentences. The sampling temperature is set to be 0.9 and top-p is set to be 0.9. To evaluate the quality of translation sentences during preference dataset construction, we use two 3.5B COMET models, Unbabel/XCOMET-XL (Guerreiro et al., 2023) and Unbabel/wmt23-cometkiwi-da-xl (Rei et al., 2023), as reward models and average the two output scores from them as the final reward of translation sentences. Moreover, we follow CPO to extract data of en \u2194 is from WMT21 (Freitag et al., 2021) and data of the other 8 translation directions from WMT22 (Freitag et al., 2022) as test set, resulting in 17,471 translation pairs in total."}, {"title": "5.2 Experiment Setup", "content": "We train the ALMA-7B in a many-to-many multilingual translation manner, starting with ALMA-7B-Pretrain-LoRA as initial checkpoint. Then we sample preference dataset with CR-Score and apply DPO to fine-tune the pretrained ALMA-7B model on preference dataset. Then we evaluate the translation results with COMET models. Besides the reward models XCOMET and KIWI-XL, we also"}, {"title": "5.3 Baselines", "content": "We compare CRPO with five baselines, QE Fine-tuning (Finkelstein et al., 2023), RSO (Liu et al., 2023), RS-DPO (Khaki et al., 2024), MBR Score (Yang et al., 2023a) and Triplet dataset (Xu et al., 2024a). As an additional comparison, we also calculate the evaluation score of gold reference sentences from the WMT dataset.\nQE Fine-tuning. We choose the sentence with the highest QE reward score from the candidate set as the target sentence to fine-tune policy.\nRSO. We statistically sub-sample 8 sentences from the candidate dataset. The acceptance rate for each sentence is \\( exp(\\frac{\\beta}{T} (r^{(ij)} \u2013 r_{max})) \\).\nRS-DPO. For convenience in hyperparameter tuning, we replace \\( \\sigma(\\frac{r_1 \u2013 r_2}{\\eta}) \\) with \\(\\frac{r_1}{r_1+r_2}\\) for RS-DPO. For the reason that the translation direction task * \u2192 en is harder than en \u2192 *, we set a larger \\(\\eta\\) for the former cases. In general, we consider two groups of values for \\(\\eta\\) for a fair comparison, specifically 0.6 for en \u2192 * and 0.5 for * \u2192 en, 0.65 for en \u2192 * and 0.55 for en.\nMBR Score. We calculate MBR score for candidate translation sentences with BLEURT-20 (Sellam et al., 2020) Metric. Specifically, we consider MBR-BW as selecting the best and worst translation sentences and MBR-BMW as selecting the best, middle, and worst translation sentences.\nTriplet Dataset. We reuse the preference dataset from Triplet Dataset that is used for CPO training."}, {"title": "5.4 Experiment Results", "content": "The average results for ten translation directions are shown in Table 1, where CRPO with CR+ achieves the best performance and CR\u00d7 gets the second best results. As RSO, RS-DPO and MBR Score mainly select the preference dataset based on sentence reward, the evaluation results emphasize the benefit of adding the confidence term to consider policy behavior. Triplet dataset performs worse than RSO and CRPO, which is mainly caused by distribution shift between \\( \\tau_{ref} \\) and response sentences from other resources. Although RS-DPO also constructs"}, {"title": "5.5 Experiment for NLLB", "content": "To evaluate the generalization of CRPO, we extend CRPO to encoder-decoder model, NLLB-1.3B (Costa-juss\u00e0 et al., 2022), and compare CRPO with RSO, RS-DPO (\\(\\eta\\) = 0.82), Triplet dataset and MBR score. Similar to the setting of ALMA experiment, for the 10 translation directions, we"}, {"title": "5.6 Ablation Study", "content": "In the ablation study, we compare CRPO with more data selection methods to evaluate the effect of reward term and confidence term. We consider some questions that can be potentially raised from CRPO in the Appendix C.4. Specifically, we consider the following methods:\nMinMaxR. To further evaluate the contribution of confidence term in CR-Score, we drop it from the CR+ and only maintain the reward difference as the score. In another word, the sentence with maximum reward score is selected as preferred sentence and the sentence with minimum reward"}, {"title": "6 Conclusion", "content": "In this paper, we argue that sentence pairs with large loss value or loss change during training contains information the model has not yet learn and could benefit the model fine-tuning. We analyze loss value and loss change based on DPO and find them to be controlled by confidence terms measuring the prediction likelihood difference and reward terms measuring reward difference of sentence pair, based on which CR+ and CR\u00d7 are designed for data selection. With experiment results, we empirically prove that CRPO outperforms previous data sampling method in machine translation tasks. And"}, {"title": "Limitation", "content": "In CRPO, we only select the sentence pair with maximum CR-Score, which will discard high quality data with slightly smaller CR-Score. Potential solution to this limitation includes leveraging preset threshold, or CR-Score distribution analysis."}, {"title": "A Algorithm", "content": "The algorithm for data selection with CR-Score is outlined in Alg 1 where the inputs are source sentence, \\( \\tau_{ref} \\) and reward model. In line 3-6, we sample candidate translation sentences from \\( \\tau_{ref} \\) and then calculate their reward score and generation likelihood. In line 7, we select the sentence with highest reward score as preferred sentence \\(y_w\\), to ensure a baseline of sentence quality. In lines 9-13, we filter out sentence pairs with negative CR-Score and select the sentence pair with maximum CR-Score, ensuring that only the most informative data is retained in the preference dataset."}, {"title": "B Experimental Details", "content": "In this section, we provide more details for experiment setting of ALMA and NLLB fine-tuning."}, {"title": "B.1 Experiment Setting for ALMA", "content": "During the training phase, we train the ALMA-7B in a many-to-many multilingual translation manner, starting with ALMA-7B-Pretrain-LoRA as initial checkpoint. For model fine-tuning, we focus on updating the weights of added LoRA parameters which have a rank of 16 and only add an additional 7.7M parameters to the original 7B size of"}, {"title": "B.2 Experiment Setting for NLLB", "content": "We also train NLLB-1.3B in many-to-many multilingual translation manner, starting with facebook/nllb-200-1.3B as initial checkpoint. During fine-tuning, we add LoRA parameters to liner layers of NLLB model with additional 27.7M parameters which are the only trainable parameters. Similar to ALMA, we set the \\(\\beta\\) value for DPO as 0.1, the batch size in fine-tuning process to be 16, a warm-up ratio to be 0.01 and learning rate to be 0.0001. For CR+, we again set K to be 50 to bridge the magnitude difference between reward and confidence terms. Following (Costa-juss\u00e0 et al., 2022), we manually set the BOS tokens for both encoder input sentence and decoder output sentence as related language index. We also add SFT term for DPO with coefficient weight set to be 1 for fair comparison. But different from ALMA, we retain the EOS tokens for preference sentence pairs which would not cause the tail content redundancy problem as in ALMA."}, {"title": "C More Experimental Results", "content": null}, {"title": "C.1 Result on Each Translation Direction", "content": "To further evaluate the performance of CRPO, we show the evaluation results on en \u2192 * in Table 4 and * \u2192 en in Table 5. CRPO achieves the best"}, {"title": "C.2 Non-COMET Metric", "content": "Due to the similar training procedure of COMET metrics, concerns may arise that the results in Section 5 and Appendix C.1 could be highly correlated with the reward model - COMET metrics. To address this concern, we also consider BLEURT-20 (Sellam et al., 2020) for evaluation, which is a non-COMET and neural-based metric. We compare CRPO with RSO, RS-DPO, MBR Score and Triplet Dataset for both ALMA-7b and NLLB-1.3B models in Table 6, where CRPO+ and CRPO\u00d7 achieve the best score indicating the robustness and high performance of combining confidence and reward terms."}, {"title": "C.3 Visualization of Reward and Confidence", "content": "To further represent the CRPO strategy, we visualize the correlation between reward scores and log \\( \\tau_{ref} \\) for candidate dataset in Figure 3 and for selected dataset in Figure 4.\nAs the ALMA-7B checkpoint already achieves outstanding performance, Figure 3 shows that before fine-tuning the policy has high probability to generate high reward translation sentences and sentence pairs with low reward difference also tend to have low log \\( \\tau_{ref} \\) difference. This also explains that in Figure 4, for all data selection methods, sentences with high reward tend to have high generation likelihood. But comparing with RS-DPO, RSO and MBR-BW, dis-preferred sentences selected by CRPO generally get higher value of log \\( \\tau_{ref} \\) which refers to difficult or error predicted translation sentences for the policy. Moreover, as CRPO only selects sentence pairs with positive CR-Score, only sentence pairs with negative log \\( \\tau_{ref} \\) difference are"}, {"title": "C.4 Potential Questions", "content": "We further evaluate CRPO by considering potential questions could be raised from CR-Score and attempt to answer them with existed or novel experiment results.\nIs it possible to apply CRPO on sentences from extra resources with higher reward? To"}]}