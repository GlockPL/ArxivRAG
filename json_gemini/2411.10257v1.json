{"title": "The Unreasonable Effectiveness of Guidance for Diffusion Models", "authors": ["Tim Kaiser", "Nikolas Adaloglou", "Markus Kollmann"], "abstract": "Guidance is an error-correcting technique used to improve the perceptual quality of images generated by diffusion models. Typically, the correction is achieved by linear extrapolation, using an auxiliary diffusion model that has lower performance than the primary model. Using a 2D toy example, we show that it is highly beneficial when the auxiliary model exhibits similar errors as the primary one but stronger. We verify this finding in higher dimensions, where we show that competitive generative performance to state-of-the-art guidance methods can be achieved when the auxiliary model differs from the primary one only by having stronger weight regularization. As an independent contribution, we investigate whether upweighting long-range spatial dependencies improves visual fidelity. The result is a novel guidance method, which we call sliding window guidance (SWG), that guides the primary model with itself by constraining its receptive field. Intriguingly, SWG aligns better with human preferences than state-of-the-art guidance methods while requiring neither training, architectural modifications, nor class conditioning. The code will be released.", "sections": [{"title": "1. Introduction", "content": "Diffusion models (DMs) have emerged as a powerful approach for generative tasks, achieving remarkable success in areas such as image synthesis and text-to-image generation [1, 17, 24, 27, 40, 43, 44]. DMs are a class of generative models that iteratively transform noise samples into samples that are close to a desired data distribution. Despite their success, DMs often fail to generate high-quality samples in the visual domain [3] and require guidance techniques to improve visual fidelity (Fig. 1). The current most popular method, classifier-free guidance (CFG), improves image quality by increasing the probability that an image belongs to a certain class label [25]. Unlike its predecessor, classifier guidance [12], which relies on training an external classifier on labeled noisy images, CFG combines conditional and unconditional denoisers, which can be trained jointly [16].\nIn the following, we denote by \u00e6 a noisy image and by e(x, t; c) and e(x, t) the class conditional and unconditional noise predictors at timestep t of the denoising process [12]. CFG linearly combines noise predictions during sampling using the extrapolation scheme\n\u1ebd(x, t; c) = \u20ac(x, t; c) + w[e(x, t; c) \u2013 \u0454(x,t)],\nwith guidance weight w > 0. CFG can be viewed as an error-correcting method [7, 44]. Equivalent extrapolation schemes can be found for all diffusion model formulations, such as target prediction [24] or flow matching [41].\nDespite the widespread use of CFG in conditional synthesis [36], it comes with notable limitations. First, it increases the training budget: when trained jointly, the unconditional task can consume up to 20% of the computational"}, {"title": "2. Related work", "content": "Classifier-free guidance (CFG) requires an unconditional denoiser, which can be trained jointly using dropout or separately. CFG-based sampling can be improved by adding a noise schedule to the condition [38], introducing monotonically increasing guidance weight schedules [47], stepwise intensity thresholding [40], or applying CFG only at an interval in the intermediate denoising steps [30]. Despite recent advancements, CFG cannot be applied to unlabelled datasets or conditional-only trained denoisers by design. While several sampling methods focus on modifying CFG [30, 38, 40, 47], alternative guidance techniques that do not require additional training or fine-tuning and can be applied to both conditional and unconditional sampling remain an open quest [39].\nAlternative guidance methods can be roughly grouped into three categories: i) architecture-based impairments, ii) image-level manipulations [19], and iii) inferior capacity models, which we refer to as weak models. Architectural impairments typically leverage self-attention maps [2, 18] that are known to capture structure-related information [2, 6, 14, 33]. For instance, a handful of manually picked attention maps can be replaced with an identity matrix [2] (PAG) or filtered using Gaussian smoothing [2] (SAG). Nonetheless, the choice of attention maps depends on the model architecture and thus limits the applicability of the method. Finally, architecture-specific impairments can create significant side effects such as deteriorating the visual structure [18]. Image-level manipulations such as frequency filtering have been attempted with limited success [19]. Hong et al. [19] restrict the high-frequency filtering on image patches corresponding to high activation areas, achieved by upsampling self-attention maps. However, similar to architecture-based impairments, it requires cherry-picking while being prone to artifacts [18].\nWeak models of inferior capacity compared to the positive model can be constructed by limiting the size of the network (i.e. number of parameters) or its training time [25] or a combination of both. However, such approaches require additional training from scratch or various weight instances, which are not always available. Concurrent work [4] deteriorates the positive model by fine-tuning with its own generated samples to derive the negative model, enhancing its biases. Similar to [4], we deteriorate the learning capacity of the positive model by fine-tuning with stronger weight regularization.\nOur work is related to [4, 25], yet we aim to systematically study WMG methods and highlight that the negative model needs to exhibit a similar modeling error as the positive model, but stronger. While the latter statement is implicitly ingrained in the design choices of prior works [2, 19, 25], it has not been explicitly formulated."}, {"title": "3. CFG and WMG behave differently", "content": "In the following, we introduce a two-dimensional toy example (Figure 2) to visualize the conceptual differences between CFG and WMG. The data distribution of our toy example consists of three data points D = {Y1, Y2, \u0423\u0437}, where each data point is assigned a different class label c\u2208 {1,2,3}. We can now generate trajectories x(t) by numerically solving the ordinary differential equation (ODE) dx(t) = \u20ac*(x(t),t)do(t), with \u20ac* the optimal noise predictor in the Bayesian sense (posterior mean), and o(t) \u2208 [0, max] the current noise level. Note that do(t) < 0 along a denoising trajectory. It can be shown that for a finite set of data points, the optimal noise predictor takes the explicit form \u20ac*(x, t) = \u03c3(t) \u22121=1(x \u2013 Yn)P(Yn|x, t)dy, with p(y|x,t) = N(x|y, \u03c3(t)\u00b2)/\u2211n=1N(x|yn, o(t)2) (see appendix of [24]). We write \u20ac(x, t) = \u20ac(x, \u03c3(t)) to indicate that the time dependence enters exclusively through \u03c3(t). The optimal noise predictor generates trajectories with endpoints arbitrarily close to one of the data points if the ODE is initialized with xinit ~ N(x0,8max) and max is sufficiently large.\nAssume we are given an error-prone noise predictor, Eerr, whose endpoints are distributed as a superposition of three normal distributions, all with variance 82, and each normal distribution centered around a different data point.\nFor this special case in can be shown that Eerr(x,t) = \u03c3(t)/\u00f5(t)\u20ac*(x, \u00f5(t)), with \u1ee1(t) = \u03c3(t)\u00b2 + \u03b4\u00b2. The question is now, to what extent can CFG and WMG help to shift trajectory endpoints of error-prone predictors closer to the data distribution? To answer this question, we define the error-prone predictors as Epos, Eneg and adjust the guidance weight w of Eq. 2 such that the error between \u010d and e* is minimized. We recover the CFG setting by introducing the class-conditional noise predictor Epos(x,t) = Eerr(x,t; c) and the unconditional noise predictor Eneg(x,t) = Eerr(x,t). Likewise we recover the WMG setting by Epos(x,t) = Eerr(x,t) and Eneg(x,t) =\n\u20ac'err(x, t), where \u20ac'err differs from eerr by a stronger endpoint variance \u03b4' > \u03b4. The denoising trajectories shown in Figure 2 follow from numerically solving the ODE for x(t), using Eq. 2 in place of the optimal noise predictor.\nCFG. As we use just one datapoint per class, the class conditional noise predictor is given by Epos(x,t) =\n\u03c3(t)-1(x \u2013 Yic), with ic the data point index belonging to class c, resulting in the special case of a straight line as denoising trajectory (Figure 2a, CFG). For small guidance weights, the trajectory endpoints match slightly better to the data distribution (Figure 2b, CFG), but for larger guidance weights, they move away from data distribution (Figure 2c, CFG). This behavior follows from the fact that Epos drives the trajectory towards the data point with the corresponding"}, {"title": "4. Diffusion guidance methods", "content": "Baselines. We compare against various baseline methods in our experimental analysis, such as vanilla CFG, CFG using a smaller capacity model (CFG\u2020), classifier guidance (CG), and the recent training-free self-attention guidance (SAG [19]). Sampling without guidance (w = 0) is equivalent to sampling with epos.\n4.1. Weak model guidance (WMG)\nMotivated by the toy example, we focus on diffusion guidance methods that implicitly aim to satisfy the \"similar error but stronger\u201d condition to construct Eneg from Epos. Reducing the compute time and the number of parameters are\nwell-studied ways of bottlenecking model performance [22] and have been utilized in concurrent work [25], unlike increased weight regularization. We generally refer to this category as weak model guidance (WMG).\n1. Reduced training constructs Eneg based on parameter instances of Epos from earlier training iterations. This requires access to previous model instances or checkpoints, which are not always publicly available.\n2. Reduced capacity refers to training a new model on the same task with a smaller model capacity. This is typically achieved by reducing the number of parameters.\n3. Reduced capacity and training (RCT) combines the above two strategies. Concurrent work [25] recently coined this approach as autoguidance. The authors demonstrate a nuanced interconnection between sampling-related hyperparameters that are specific to the EDM2 model [26]. Specifically, the impact of the posttraining exponential moving averages (EMAS) used for the positive and negative models, which requires tuning five hyperparameters simultaneously. To make our analysis more generally applicable, we instead set the EMAlike hyperparameter of EDM2 models to 0.1.\n4. Weight decay refers to creating Eneg using an increased L2 weight regularization compared to epos, also known as higher model bias in the context of bias-variance tradeoff [46]. In practice, Eneg is the result of either retraining Epos from scratch (Weight decay re-training) or fine-tuning Epos (Weight decay fine-tuning) under sufficiently strong weight decay regularization.\n5. Weight decay and reduced training combines (1) and (4) and requires Eneg to be obtained through re-training.\n4.2. Sliding window guidance (SWG)\nTo improve the perceptual quality of images, we introduce a new guidance method that upweights long-range dependencies on image level. The idea is to generate Eneg by restricting the spatial input size (H \u00d7 W) of Epos to a smaller size k xl, with k < H and l < W, which induces a defined cut-off for long-range dependencies. In practice, this is possible if Epos can process multiple image resolutions. To cover the whole image, we use sliding windows to generate N crops of size k \u00d7 l, using fixed strides for each spatial dimension. These crops are independently processed by Epos as illustrated in Fig. 3. The N predictions of Epos are superimposed in the same order and positioned as the crops were taken, which results in a H \u00d7 W output for Eneg. Overlapping pixels are averaged. A pseudo-code for SWG is provided in the supplementary material. We emphasize that inference using sliding windows is a common technique in high-resolution 2D/3D image processing and is ingrained into existing tools and frameworks [9, 21, 42]. Like all guidance methods based on Eq. (2), SWG can be linearly combined with other guidance techniques, using\n(x,t) = Epos (x, t) + i Wi[Epos(x, t) \u2013 Enes (x, t)], where the superscripts indicate different guidance methods. In Table 2, we combine SWG with RCT (RCT+SWG) and CFG (CFG+SWG).\nHyperparameters and limitations. One limitation of SWG is that the noise predictor, Epos, must process inputs of varying image resolution. Nonetheless, current architectures, such as Unets [17] or (diffusion) image transformers [36], generally satisfy this requirement. SWG can be implemented in a few lines of code and requires no training modifications or class conditioning. Unless otherwise specified, we assume H = W and use crop size k = l \u2248 5/8 \u00d7 H and N = 4, which translates into k = 40, s = 24 at 64 \u00d7 64 resolution. We denote the overlap ratio per dimension as r = 1 - = 0.4. We keep the same design choice for latent DMs [37] that operate in \u00d7 feature dimensions. The computational overhead of SWG compared to CFG is determined by the overlap ratio r and the architecture. For EDM-S and r = 0.4, we measured an overhead of less than 30% compared to CFG based on a naive implementation of SWG. Compared to SAG [19], which also requires no additional training, we find that SWG is less time and memory intensive. More details are provided in the supplementary material."}, {"title": "5. Experimental evaluation", "content": "5.1. Implementation details\nAll the experiments were conducted on 4 Nvidia A100 GPUs with 40GB of VRAM. We adopt commonly used datasets such as CIFAR10, CIFAR100 [28], FFHQ [23],"}, {"title": "6. Conclusion", "content": "In this work, we identified \"same error but stronger\" as a general concept for designing guidance methods and validated this concept experimentally for a toy example and in higher dimensions by investigating several WMG methods. However, the question remains how to practically construct negative models if the nature of the prediction error"}, {"title": "A. Appendix: The Unreasonable Effectiveness of Guidance for Diffusion Models", "content": "B. Additional implementation details\nSWG implementation details. To make SWG compatible with DiT, we add positional encoding interpolation on DiT-XL to process various resolutions. The choice of k \u2248 5/8H is determined by practical considerations of Unets. Specifically, k must satisfy k\u2208 N, where n is the number of downsampling steps of the Unet. We provide a practical illustration of how to apply SWG in Pseudocode 6. We leave text-to-image diffusion models to future work.\nState-of-the-art results. While our results using SWG and RCT+SWG do not outperform concurrent results from \"autoguidance\" [25] using EDM2, we believe it is a matter of model-specific multi-hyperparameter search and compute time. This study emphasizes the broader applicability and usefulness of SWG, with or without CFG or RCT, rather than tuning for a specific benchmark (model, dataset, metric).\nADM. Experiments at 256\u00d7256 image resolution using ADM require over 4 days (384 GPU hours) per guidance scale and are left to future work, as recent state-of-the-art latent diffusion models exhibit far superior image quality."}]}