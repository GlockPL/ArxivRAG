{"title": "HM3D-OVON: A Dataset and Benchmark for Open-Vocabulary Object Goal Navigation", "authors": ["Naoki Yokoyama", "Ram Ramrakhya", "Abhishek Das", "Dhruv Batra", "Sehoon Ha"], "abstract": "We present the Habitat-Matterport 3D Open Vocabulary Object Goal Navigation dataset (HM3D-OVON), a large-scale benchmark that broadens the scope and semantic range of prior Object Goal Navigation (ObjectNav) benchmarks. Leveraging the HM3DSem dataset, HM3D-OVON incorporates over 15k annotated instances of household objects across 379 distinct categories, derived from photo-realistic 3D scans of real-world environments. In contrast to earlier ObjectNav datasets, which limit goal objects to a predefined set of 6-21 categories, HM3D-OVON facilitates the training and evaluation of models with an open-set of goals defined through free-form language at test-time. Through this open-vocabulary formulation, HM3D-OVON encourages progress towards learning visuo-semantic navigation behaviors that are capable of searching for any object specified by text in an open-vocabulary manner. Additionally, we systematically evaluate and compare several different types of approaches on HM3D-OVON. We find that HM3D-OVON can be used to train an open-vocabulary ObjectNav agent that achieves both higher performance and is more robust to localization and actuation noise than the state-of-the-art ObjectNav approach. We hope that our benchmark and baseline results will drive interest in developing embodied agents that can navigate real-world spaces to find household objects specified through free-form language, taking a step towards more flexible and human-like semantic visual navigation. Code and videos available at: naoki.io/ovon.", "sections": [{"title": "I. INTRODUCTION", "content": "Visual navigation to a language-specified object is an essential skill for robot assistants that can aid humans in a variety of tasks in indoor environments, such as \u201cfind my keys on the L-shaped couch\". The interest in developing visual navigation systems has increased in recent years, highlighted by the embodied AI community's establishment of standardized evaluation metrics and benchmarks for numerous navigation tasks [1]-[3]. Various navigation tasks have been proposed, each defining goals differently point-goal navigation for 2D coordinates [4], object-goal navigation [2], [5], image-goal navigation [6]-[8], and language-goal navigation (via referring expressions or step-by-step instructions) [9], [10]. In this work, we focus on the ObjectNav task where an agent is initialized in an indoor environment and tasked with navigating to an instance of a specified goal object category (e.g., 'couch'). While existing ObjectNav benchmarks have typically concentrated on a limited, fixed set of object categories (6-21 object categories) and have only tested the generalization of navigation agents to novel environments, robotic agents in the real-world must also learn to generalize and navigate to an open set of object goal categories. To address this, we investigate the problem of open-vocabulary ObjectNav, where an agent will be asked to navigate to an object specified by language (seen or unseen during training). Fig. 1 illustrates an example of such an episode.\nWe introduce a dataset and benchmark named Habitat-Matterport 3D Open-Vocabulary ObjectNav (HM3D-OVON), designed to test the generalization of ObjectNav agents in an open-vocabulary setting using the HM3DSem [2] scene dataset. To examine how well agents can generalize to new goal object categories and environments, we propose three evaluation splits: 1) VAL SEEN- consists of goal categories seen during training, 2) VAL SEEN SYNONYMS- consists of goal categories synonymous to those seen during training (i.e., \"couch\u201d category seen during training, evaluated on \"sofa\" during evaluation), 3) VAL UNSEEN- consists of goal object categories that are not seen during training nor semantically similar to any category from the training set. We then use these evaluation splits to meticulously examine the performance of a variety of agents across goal object categories with varying degrees of semantic similarity to the training data.\nWe benchmark policies using several types of popular"}, {"title": "II. RELATED WORKS", "content": "ObjectNav in virtual environments. In recent years, several benchmarks have been established for training and evaluating a robot's ability to locate an instance of a given object category within a novel environment (ObjectNav). However, these benchmarks often exhibit two main short- learning paradigms on HM3D-OVON, including imitation learning (IL), reinforcement learning (RL), and modular methods [8], [11], [12], to understand their impact on the agent's ability to navigate to and recognize objects in an open-vocabulary setting. Through this benchmarking, we find that training end-to-end policies using imitation learning, specifically DAgger [13], with frontier exploration trajectories and fine-tuning with RL (referred to as DAgRL) outperforms all other end-to-end trained methods. However, DAgRL shows a drop in success rate of 11.9-23.0% compared to VAL SEEN when evaluating on VAL SEEN SYNONYMS and VAL UNSEEN, suggesting that trained methods struggle to generalize to unseen categories using end-to-end learning on ObjectNav alone. In contrast, the modular method VLFM [12], which leverages explicit maps and vision-language foundation models to explore the environment in a semantically meaningful way, achieves consistent performance between 32.4\u201335.2% on success rates across the three evaluation splits. We attribute this to the strong generalization capabilities of the open-vocabulary object detector (OWLv2 [14]) used to detect the goal object. Motivated by this observation, we find that augmenting DAgRL with an object detector and a navigation module for bee-lining to the detected object (referred to as DAgRL+OD) significantly improves the generalization of end-to-end methods on the VAL SEEN SYNONYMS and VAL UNSEEN splits, with a 9.6-18.8% increase in success rate. We also find that DAgRL is much more robust to noise that simulates real-world conditions than VLFM.\nAdditionally, we conduct a comprehensive analysis of different architectures used for encoding temporal information (transformer vs. RNN), imitation learning algorithms (behavioral cloning vs. DAgger), and types of trajectories used for imitation learning (frontier exploration vs. shortest path following). We find that policies perform significantly better when trained using a transformer instead of an RNN, with DAgger instead of behavioral cloning, and with frontier exploration instead of shortest path trajectories. Our findings on the impact of the types of trajectories used for imitation learning directly contradict the findings presented in SPOC [15], which asserted that shortest path trajectories lead to better performance than those that involve exploration for imitation learning. Furthermore, we present a detailed analysis of the failure modes of these agents, which illuminates the challenges and opportunities in developing robotic agents capable of robustly navigating to objects specified in free-form language in real-world environments, paving the way for more capable and generalizable visual semantic navigation robots. Code for HM3D-OVON can be found at naoki.io/ovon."}, {"title": "III. THE HM3D-OVON BENCHMARK", "content": "The ObjectNav task challenges an agent to locate any instance of a specified goal object category (e.g., 'bed') within an unfamiliar environment [1]. At each time step, the agent receives a set of sensory inputs: an RGB image $I_t$, a depth image $D_t$, its relative displacement and heading from the start position (odometry) $P_t$ = $(\u2206x, \u0394y, \u0394\u03b8)$, and the target object category $G$. The agent can select one of several actions: MOVE_FORWARD (by 0.25m), TURN_LEFT and TURN_RIGHT (by 30\u00b0), LOOK_UP and LOOK_DOWN (by 30\u00b0), and STOP actions. Success is defined as the agent invoking STOP within 1m of a goal object within 500 time steps. In our experiments, we configure the simulated agent to match the specifications of the Stretch robot [29], which has a height of 1.41m, a base radius of 17cm, and a 360\u00d7640 resolution RGB-D camera positioned at a height of 1.31m."}, {"title": "A. ObjectNav task definition"}, {"title": "B. The HM3D-OVON dataset", "content": "We utilize the HM3DSem dataset's dense object annotations [30] to compile a vast collection of ObjectNav episodes, termed the HM3D-OVON dataset. HM3D-OVON includes 379 goal object categories across 181 unique, photorealistic virtual scans of real-world environments. We ensure goal objects are of significant size and visibility, occupying at least 5% of the Stretch's camera view from at least one vantage point within 1m of the object to affirm feasibility. The dataset is segmented into training and evaluation splits, with 145 scenes and 36 scenes, respectively, ensuring no scene or goal object instance overlap between splits. The training split features goal object instances across 280 categories, whereas the evaluation split comprises 178 categories.\nTo evaluate generalization to novel objects on varying levels, we divide the evaluation split into three smaller splits, each sharing the same scenes but utilizing mutually exclusive sets of goal object categories:\n\u2022 VAL SEEN: uses goal object categories seen during training.\n\u2022 VAL SEEN SYNONYMS: uses goal object categories semantically similar to those seen during training (i.e., \"couch\" category seen during training, evaluated on \"sofa\" during evaluation).\n\u2022 VAL UNSEEN: uses goal object categories semantically divergent from those encountered during training.\nFor VAL SEEN SYNONYMS and VAL UNSEEN generation, we first uniformly sample ~25% of the object categories from HM3D-OVON. Then, we separate these categories using a semantic similarity metric calculated via Sentence-BERT [31]. SentenceBERT, a fine-tuned variant of the pre-trained BERT network, is designed to gauge the semantic similarity between texts by comparing their embeddings' cosine similarity. We compute SentenceBERT embeddings for each sampled object category and evaluate its cosine similarity with all training split object categories. An object category is allocated to VAL SEEN SYNONYMS if it has a maximum similarity surpassing a threshold; otherwise, it is allocated to VAL UNSEEN."}, {"title": "C. Episode generation", "content": "An episode in HM3D-OVON comprises of a scene, the agent's starting position, and a goal object category. For episode creation, we first randomly select a goal object category and then randomly determine a starting position adhering to the following criteria: 1) at least one instance of the goal is on the same floor as the starting position, as stair climbing is not anticipated in indoor settings; and 2) the length of the shortest path to the nearest goal location must lie between 1m-30m. This approach aligns with the episode generation methodology of the ObjectNav task [20]. Fig. 1 illustrates a goal example for a single episode. Following this protocol, we generate 50k episodes per scene for the 145 training scenes, and 3k episodes per scene for the 36 validation scenes."}, {"title": "IV. HM3D-OVON BASELINES", "content": "In this section, we compare various learning methodologies (imitation learning, reinforcement learning, and modular approaches) and architectural designs (transformer vs. RNN) as proposed in prior studies on object navigation. We evaluate each method on the HM3D-OVON benchmark.\nPolicy architecture. We employ frozen SigLIP [32] RGB and text encoders to encode the visual observations and the goal object category. These encoders have been identified as highly effective for ObjectNav by [15]. The encoders generate two 768-dimensional embeddings for the visual observation, $i_t$ = SigLIP$_{RGB}$($I_t$), and the goal object category, $g_t$ = SigLIP$_{text}$(G). Additionally, the agent's previous action, $a_{t-1}$, is encoded into a 32-dimensional vector using an embedding layer, $p_t = \u03b1(a_{t-1})$. These embeddings are concatenated to form the observation embedding, $O_t = [i_t, g_t, p_t]$, which is fed into a 4-layer, decoder-only transformer [33] $\u03c0_\u03b8$ (8 heads, hidden size of 512), with a maximum context length of 100. $\u03c0_\u03b8$ takes in the past 100 consecutive observations $[O_{t-99}, ..., O_t]$ and outputs a feature vector for the current time step. This vector is passed through a linear layer (action head) that predicts a categorical distribution from which an action $a_t$ is sampled, $a_t \u223c \u03c0_\u03b8(\u00b7| O_{t\u221299}, ..., O_t)$. During RL, an additional linear layer (critic head) is used to project the feature vector into a value estimate for the current state.\nWhen comparing against RNN-based policies, the only architectural change we make is replacing the transformer with a 4-layer LSTM [34] with a similar parameter count, for fair comparison.\nBehavioral cloning (BC). Learning from demonstrations has been shown to be a powerful approach for developing efficient semantic navigation behaviors [15], [26], [28], [35]. Behavioral cloning employs supervised learning on a dataset of observation-action pairs from expert demonstrations to train policies. Consider a policy $\u03c0_\u03b8$ parameterized by \u03b8 that maps observations $o_t$ to an action distribution, $\u03c0_\u03b8(\u00b7|o_t)$. Let $\u03c4$ denote a demonstration consisting of observation-action pairs, $\u03c4 = [(o_0, a_0), (o_1, a_1), ..., (o_n, a_n)]$, and $T = {\u03c4_i}$ denote a dataset of demonstrations. The objective function optimization can be described as:\n$\\theta^* = \\argmax_\\theta \\sum_{i=1}^{N} \\sum_{(o_t, a_t) \\in T} \\log (\\pi_\\theta(a_t \\vert o_t))$\nWhile prior work showed that behavioral cloning using demonstrations collected from humans performing the ObjectNav task can train effective policies [26], [28], these demonstrations were limited in diversity and are expensive to collect, especially for the amount of categories in HM3D-OVON. SPOC [15] used a path planner to generate the shortest possible obstacle-free trajectory from the start pose to the goal object, and demonstrated that these trajectories lead to better results than those generated by an expert that exhibits more exploration. However, this directly contradicts the results of [28], which show learning from demonstrations that involve frontier exploration yield better performing policies than shortest path trajectories. These trajectories are generated by executing frontier-based exploration, which involves the agent systematically moving towards unexplored areas ('frontiers') of the environment, until a goal object is within range (3.5m in our experiments), at which point a shortest path planner is used to plan a path to the goal object ('bee-line'). In this work, we experiment with learning from either frontier exploration or shortest path trajectories. For behavioral cloning, we generate a trajectory for each episode, for each of the two types (7.25 million trajectories for each type).\nDAgger. DAgger [13] is a supervised learning algorithm that adopts the same loss function as behavioral cloning. However, unlike behavioral cloning, DAgger involves an expert who provides new action labels for the agent's trajectories 'online' during training. Additionally, the action generated by the policy $\u03c0_\u03b8$ is utilized to advance the environment, rather than the expert's actions. The formulation of DAgger's learning algorithm is similar to behavioral cloning, except each labeled trajectory $\u03c4$ now consists of observation-action pairs $\u03c4$ = $[(o_0, \\hat{a}_0), (o_1, \\hat{a}_1), ...., (o_n, \\hat{a}_n)]$, where action labels $\\hat{a}_t$ are provided by the expert, given $o_t$.\nUnlike behavioral cloning, which relies on a pre-recorded dataset, DAgger generates $o_t$ (and consequently, $\\hat{a}_t$) using $a_{t-1}$ while $\u03c0_\u03b8$ is updated, necessitating the capability to interact with the environment and consult the expert online during learning. Thus, it is crucial to use an expert that can swiftly provide a label $\\hat{a}_t$ for $o_t$, as a slow expert can substantially reduce the speed of training. Existing implementations of frontier-based trajectory generation for ObjectNav [11], [28] are overly slow for in-the-loop execution, taking 250ms per time step, leading to a separation of trajectory generation and supervised learning into discrete stages to maintain training speed. This constraint has forced prior studies like [28] to adopt behavioral cloning for learning from offline-collected frontier exploration trajectories. To counteract this, we introduce, alongside our benchmark, an implementation for frontier-based exploration and bee-lining engineered specifically for rapid execution at each time step, ensuring minimal impact on training speed. Our method only requires"}, {"title": "V. RESULTS", "content": "In this section, we aim to address the following questions:\n1) What differences in performance can be observed between policies trained using different learning methods (RL, IL, or modular learning)?\n2) To what extent do various trajectory generation strategies influence the success of imitation learning policies?\n3) How do different state encoder architectures (transformers vs. RNNs) impact performance?\n4) How robust are our baselines to noise that simulate real-world conditions?\nWe compare different learning-based approaches: end-to-end policies trained using RL, BC, DAg, BCRL, and DAgRL, as well as a modular approach, VLFM [12]."}, {"title": "E. Failure analysis", "content": "To characterize the types of behavior that can be learned from HM3D-OVON, we present a detailed analysis of the different failure modes of DAgRL, the policy that attains the best performance without relying on an external open-vocabulary object detector. A breakdown of the different failures modes on each of the three evaluation splits is visualized in Fig. 4. As the goal object categories in the evaluation split decrease in similarity to those seen during training (VAL SEEN \u2192VAL SEEN SYNONYMS \u2192VAL UNSEEN), the following trends occur: We observe that failures that occur from timeout (i.e., never calling STOP) increase (orange in Fig. 4, 19.5%\u219225.5%\u219237.3%), primarily due to ignoring the goal object (pink, 2.6%\u21928.6%\u219216.8%), rather than inefficient exploration that causes the agent to miss the goal object (brown, stays between 16.8%-20.5%). We also observe that the agent is more likely to move away from the goal object (enter and leave its success region) and call stop elsewhere (purple, 2.2%\u219211.4%\u219217.8%). These trends indicate that the agent struggles to generalize and correctly navigate to and call STOP when seeing a goal object that is too semantically different from the goal categories in HM3D-OVON's training split. However, as shown by the performance of DAgRL+OD, these shortcomings can be addressed using a pretrained open-vocabulary object detector and a way to navigate the robot to a detected object and call STOP (i.e., a point-goal policy).\nThe causes of failure that consistently contribute a large portion of the failure across all three of the evaluation splits are: (1) stopping at the wrong object (yellow, between"}, {"title": "VI. CONCLUSION", "content": "We present HM3D-OVON, a large-scale dataset and benchmark that provides 379 goal object categories and over 15k annotated instances of household objects across 181 unique, photo-realistic virtual scans of real-world environments. HM3D-OVON facilitates the training and evaluation of models with an open-set of goals defined through free-form language, compared to previous datasets that are limited to a predefined set of object categories at test-time. Through extensive experiments, we demonstrate that HM3D-OVON can be used to train an open vocabulary ObjectNav agent that achieves both higher performance and better robustness to localization and actuation noise than the state-of-the-art.\nWe hope that HM3D-OVON leads to further advancements in embodied Al and opens up new avenues for research in visual semantic navigation and object recognition."}]}