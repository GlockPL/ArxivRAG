{"title": "Object Detection Approaches to Identifying Hand Images with High Forensic Values", "authors": ["Thanh Thi Nguyen", "Campbell Wilson", "Imad Khan", "Janis Dalins"], "abstract": "Forensic science plays a crucial role in legal investigations, and the use of advanced technologies, such as object detection based on machine learning methods, can enhance the efficiency and accuracy of forensic analysis. Human hands are unique and can leave distinct patterns, marks, or prints that can be utilized for forensic examinations. This paper compares various machine learning approaches to hand detection and presents the application results of employing the best-performing model to identify images of significant importance in forensic contexts. We fine-tune YOLOv8 and vision transformer-based object detection models on four hand image datasets, including the 11k hands dataset with our own bounding boxes annotated by a semi-automatic approach. Two YOLOv8 variants, i.e., YOLOv8 nano (YOLOv8n) and YOLOv8 extra-large (YOLOv8x), and two vision transformer variants, i.e., DEtection Transformer (DETR) and Detection Transformers with Assignment (DETA), are employed for the experiments. Experimental results demonstrate that the YOLOv8 models outperform DETR and DETA on all datasets. The experiments also show that YOLOv8 approaches result in superior performance compared with existing hand detection methods, which were based on YOLOv3 and YOLOv4 models. Applications of our fine-tuned YOLOv8 models for identifying hand images (or frames in a video) with high forensic values produce excellent results, significantly reducing the time re- quired by forensic experts. This implies that our approaches can be implemented effectively for real-world applications in forensics or related fields.", "sections": [{"title": "I. INTRODUCTION", "content": "The integration of machine learning-based object detection methods in identifying hand images with high forensic values holds significant promise in enhancing the efficiency and accuracy of forensic investigations. Forensic studies suggest that these images may have relevance in legal or investigative matters [1]. This could include identifying individuals based on hand features or using hand images as evidence in forensic analysis. Measuring the forensic value of hands involves assessing various features and characteristics that can aid in identification or analysis within forensic contexts. Aspects that contribute to the forensic value of hands may include fingerprints, palmprints, hand geometry, vein patterns, scars and tattoos, or occupational markers.\nFingerprints are distinctive ridge patterns on fingers that can be used for authentication and verification needs, making them one of the most valuable forensic features of hands. Fingerprints are unique to each individual and have been a traditional method for personal identification [2]. Automated systems for fingerprint recognition can capture and analyze the ridge patterns and minutiae points present on an individ- ual's fingertips. Similar to fingerprints, palmprints also have unique ridges or distinctive patterns. Palmprint recognition systems can analyze the palm's surface for identification purposes [3]. Likewise, hand geometry involves measuring and analyzing physical characteristics of the hand, such as length and width of fingers, distance between joints, palm width, and overall shape of the hand [4]. Measurements of hand dimensions can provide valuable forensic information, particularly in cases where other biometric features are unavailable.\nVein patterns in the hand, typically those visible on the back of the hand, can also be used for individual recognition, especially in conjunction with other biometric identifiers. The veins in the hand form a unique pattern that can be captured using infrared technology. Vein pattern recognition is a non-intrusive method and can be implemented as an effective identification tool [5]. Similarly, birthmarks and tattoos on the hands can serve as identifying markers and may offer significant forensic insights across various cases [6]. Certain occupations or activities may leave distinctive marks on the hands (i.e., occupational markers), such as corns, calluses or scars, which are useful for identification or event reconstruction [7].\nArtificial intelligence technologies such as object detection methods [8] can be used to locate and isolate fingerprints, palm prints, or other unique features within a hand image. The technologies can assist forensic experts in quickly pro- cessing large volumes of image and video data, reducing the time required for manual analysis. Every day, law en- forcement officers encounter digital video evidence, often necessitating the examination of substantial data volumes. This task can expose them to distressing content, including materials related to child sexual abuse and exploitation. Our overarching objective is to extract forensic value of hand images using artificial intelligence, mirroring the processes conducted manually by humans. The initial stride toward achieving our objective involves building a real-world, noise- tolerant automated hand recognition system.\nThis paper presents a comparison between object de- tection approaches to identifying hand images with high forensic values, aiming to support forensic experts in their daily practice. Many modern object detection methods use learning-based approaches, such as deep learning models. Convolutional Neural Networks (CNNs) [9] and more ad- vanced models like faster R-CNN [10], You Only Look Once"}, {"title": "(YOLO) [11], single shot multibox detector [12], and Vision Transformers (ViT) [13] have demonstrated effectiveness in detecting various objects in an image. These models are trained on large datasets to learn the complex patterns and features associated with the objects of interest.\nIn this study, we implement and compare performance of popular YOLO and modern ViT models for hand de- tection. More specifically, we employ two YOLOv8 vari- ants, i.e., YOLOv8 nano (YOLOv8n) and YOLOv8 extra large (YOLOv8x), and two ViT variants, i.e., DEtection TRansformer (DETR) [14] and Detection Transformers with Assignment (DETA) [15] for experiments using four hand image datasets. We then apply the best-performing approach to identifying images (or frames in a video) that possess high forensic values. We maintain a high tolerance for false positive rates as these images and frames are subject to manual scrutiny by forensic experts afterward. Images or frames containing a substantial hand portion are deemed to have a high forensic value. The hand portion's size or area within an image can be calculated straightforwardly using the dimensions of the bounding boxes detected by the object detection models.\nIn summary, this work's contribution is threefold: 1) create a combined dataset of a wide range of hand images, which is helpful for effectively training hand detection models. In particular, we create bounding boxes on our own and make them available for every image in the 11k hands dataset [16]; 2) evaluate the performance of various object detection models on hand image datasets and identify the best-performing method; 3) implement the best-performing approach for identifying hand images with high forensic values, thereby significantly reducing the time required by forensic experts.\nIt is crucial to highlight that this research does not propose using any elements to identify or store individuals' sensitive or private information, including biometric data. It does not involve collecting new data or data beyond what is already legally accessible to law enforcement. The purpose of this work is solely to rank images based on their potential value to forensic examiners, adhering to established legal processes and controls.", "content": null}, {"title": "II. RELATED WORK", "content": "In forensic contexts, offenders often exhibit forensic awareness by deliberately excluding their faces from images. However, there is less concern about whether other parts of their anatomy are visible, possibly due to the belief that identification is less likely from these areas. These body parts may encompass feet, legs, thighs, genitals, and abdomen. Notably, it is most often the hand (specifically the back of the hand) and forearm that are captured [1]. Hand images thus serve as a valuable resource for forensic analyses.\nHand analysis has been a subject of enduring interest in the field, with considerable research dedicated to aspects such as grasp analysis [17], pose estimation [18] and recon- struction [19]. Nevertheless, these approaches have predom- inantly concentrated on controlled in-lab settings, frequently assuming a pre-localized hand or operating in environments with restricted variability. Despite substantial advancements, deploying these methods in the expansive realm of Internet videos presents a challenge, mainly attributed to the over- whelming diversity in viewpoints and contexts. The objective of the research presented in [20] is to facilitate hand analysis on a large and diverse scale across the Internet. In pursuit of that goal, they propose a model capable of identifying various attributes for each individual hand in a given RGB image, demonstrated across a wide range of scales and contexts. These attributes include a bounding box for the hand, its orientation (left/right), contact state, and more. These identified attributes play a pivotal role in addressing downstream challenges such as pose reconstruction and grasp analysis.\nJoshi and Kanphade present a forensic approach in [21] using a multiple convolutional layer network along with a fully connected and a k-NN layer to identify a person. However, their approach requires a biometric radiograph, which is not readily available in most real-world law en- forcement scenarios. In another work, Narasimhaswamy et al. [22] introduced Hand-CNN, a model based on the CNN architecture for detecting hand masks and projecting hand orientations in unconstrained images. Hand-CNN enhances MaskRCNN [23] by using a novel attention mechanism to integrate contextual cues in the detection procedure. They also created a large-scale hand dataset consisting of hands in unconstrained images with annotations, which are helpful for training and evaluating various machine learning models in this domain.\nOn the other hand, a resilient hand tracking approach that combines a correlation filter with a correction strategy utiliz- ing a fast object detection model, specifically the single-shot detection algorithm, is introduced in [24]. This amalgamation enables the tracker to reinitialize when hand movement is inaccurately traced, ensuring consistent and precise tracking. The approach minimizes computational costs of the detector by detecting the object of interest only during the initial frame and when it is mislocated by the tracker. This reduction in computational load leads to an enhancement in real-time performance."}, {"title": "III. HAND DETECTION METHODS", "content": null}, {"title": "A. YOLOv8 Approaches", "content": "YOLOv8 [25] represents the newest generation within the YOLO-based object detection models by Ultralytics, showcasing cutting-edge performance. Building upon the advancements of previous YOLO versions, the YOLOv8 model offers enhanced speed and accuracy, presenting a uni- fied framework for training models across: object detection, instance segmentation, and image classification.\nYOLOv8 is the 2023 iteration in the YOLO series of models that features an architecture similar to YOLOV5, comprising a sequence of convolutional layers. Notably, YOLOv8 distinguishes itself by incorporating a cross-stage partial bottleneck in the convolutional layer. The output from these convolutional layers is subsequently directed to a decoupled head. This decoupling enables the head to independently focus on distinct tasks, encompassing object detection, classification, and regression. It is important to clarify that our work specifically pertains to object detection in this context.\nWe use YOLOv8 as one of the competing methods. There are several variants of YOLOv8, including nano, small, medium, large and extra large variants. We fine-tune the smallest variant, i.e., YOLOv8n, and the largest variant, i.e., YOLOv8x, for hand detection in this study. Among the YOLOv8 variants, YOLOv8n stands out as the quickest and most compact, whereas YOLOv8x distinguishes itself as the most accurate albeit the slowest."}, {"title": "B. Vision Transformer Approaches", "content": "We fine-tune Detection Transformer (DETR) [14] and Detection Transformers with Assignment (DETA) [15] models for the hand detection applications. The vari- ant of DETR used in this study is the facebook/detr- resnet-50, whereas that of DETA is the jozhang97/deta- swin-large. The pre-trained weights of these models are available on the Hugging Face repositories, respec- tively at https://huggingface.co/facebook/detr-resnet-50 and https://huggingface.co/jozhang97/deta-swin-large.\n1) DETR-ResNet-50: This is a DETR model with a ResNet-50 backbone, which was pre-trained on the COCO 2017 object detection dataset including 118k images with annotations [26]. The DETR model employs an encoder- decoder transformer architecture with a convolutional ResNet model. To enable object detection, two heads are incorpo- rated atop the decoder outputs: a linear layer responsible for predicting class labels and a multi-layer perceptron for bounding boxes. Employing object queries, the model searches for specific objects within an image, with each object query dedicated to locating a particular object.\nThe model undergoes a training process employing a \"bipartite matching loss\", wherein the inferred classes and bounding boxes of each object query are compared with annotated labels. These annotations are padded to match the length of the object queries, accommodating scenarios where an image comprises fewer objects. The Hungarian matching technique is then deployed to establish a best possible one- to-one mapping between each query and each annotated label [27]. Subsequently, in order to optimize the model parameters, the standard cross-entropy loss is employed for the classes, while a linear combination between L1 and generalized Intersection over Union (IoU) loss is applied for the bounding boxes. In our implementation, this model consists of 41.5M parameters in which 41.3M parameters are trainable and 222k parameters are non-trainable.\n2) DETA-Swin-Large: The DETA model was proposed in [15], which aims to rectify a commonly held misconception that one-to-one mapping is indispensable for achieving high- performance detection. Contrary to this belief, the study in [15] demonstrates that the conventional one-to-many training objective can produce equally adept detection transformers. The DETA approach involves crafting a transformer-based object detector that assigns positive-negative labels directly to each query, akin to traditional detectors. Additionally, they employ non-maximum suppression (NMS) method to elimi- nate redundant predictions, deviating from the conventional end-to-end one-to-one matching paradigm. The enhancement in this model involves substituting the one-to-one bipartite Hungarian matching loss, as employed in Deformable DETR [28], with one-to-many label assignments akin to those used in conventional detectors, accompanied by the NMS mechanism. This model includes 218M parameters in total with all of them being trainable."}, {"title": "IV. DATASETS AND PERFORMANCE METRICS", "content": null}, {"title": "A. Used Datasets", "content": "We employed four datasets to evaluate performance of the competing hand object detection methods. The first dataset is extracted from the EgoHands dataset [29], consisting of 4,800 images with 15,053 ground-truth labeled hands. These images are obtained from 48 videos, with 100 frames for each video. Preprocessing the data allows us to acquire 4,787 images that have available labels. Within these images, 3,590 images are selected randomly as the training data, while the validation and test sets include 335 and 862 images, respectively.\nThe second dataset is the 11k hands dataset [16], compris- ing 11,076 hand images (1600\u00d71200 pixels) of 190 subjects, of varying ages between 18 and 75 years old. The third dataset is extracted from the Open Images dataset [30], [31], including 20,500 hand images for training, 1,892 images for validation, and 4,932 images for testing.\nTo obtain a comprehensive comparison between the object detection approaches, we create a combined dataset that includes all aforementioned datasets. More specifically, the training and validation sets of the combined dataset include respectively 32,397 and 3,002 images, while the test set consists of 7,788 images. A statistical summary of these four datasets is presented in Table I.\nWhile the EgoHands and Open Images datasets have ground-truth bounding boxes available and can be obtained from their respective sources, the 11k hands dataset does not have bounding box labels and thus requires more involved processing steps to annotate its 11,076 hand images. As a contribution of this work, we have created and made bounding boxes publicly available and accessible for every image in the 11k hands dataset. Our approach is semi- automatic as it combines both manual human efforts and automated procedures. We start by randomly selecting 500"}, {"title": "B. Performance Metrics", "content": "Average Precision (AP) and Average Recall (AR) are pop- ular metrics for evaluating the accuracy of object detection methods by estimating the Precision-Recall relationship [32]. There are a number of variations of these metrics, depending on the Intersection over Union (IoU) threshold, the area of the object, and the number of objects per image. The most popular metrics are listed in Table II.\nIn this study, we employ all of these metrics to compare performance between the competing methods."}, {"title": "V. RESULTS AND DISCUSSIONS", "content": null}, {"title": "A. Experimental Results on Test Sets of the Used Datasets", "content": "In this section, we present results obtained using recent object detection approaches compared with existing methods in the human hand detection domain, e.g., the approaches implemented in [34]."}, {"title": "1) Results on the EgoHands dataset:", "content": "The best results on this dataset are obtained from the YOLOv8n and YOLOv8x models, with AP measures of 0.768 and 0.766, respectively. The AR1, AR10, and AR100 metrics of these two approaches are also approximate. For YOLOv8n, the AR1 is 0.278 and for YOLOv8x, it is 0.277. The AR10 and AR100 scores of YOLOv8n are both 0.800, while those of YOLOv8x are 0.802.\nThe existing approaches, i.e., YOLOv3 variants and YOLOv4-tiny employed in [34], yield mediocre results. For instance, the AP metric for the normal variant of YOLOv3 is 0.659, with AR1 at 0.254, and both AR10 and AR100 at 0.698. The tiny variants of YOLOv3 and YOLOv4 (i.e., YOLOv3-tiny, YOLOv3-tiny-PRN, and YOLOv4-tiny) produce inferior results compared to the normal variant (YOLOv3). Notably, the YOLOv4-tiny variant implemented in [34] exhibits the poorest performance on this dataset, with an AP metric of 0.069, an AR1 of 0.073, and both AR10 and AR100 at 0.109.\nIt is important to note that the EgoHands dataset is part of the training data for both the YOLOv3 and YOLOv4 approaches employed in [34]. This inclusion is why the performance gap between the YOLOv3 approaches in [34] and the YOLOv8 models is relatively small. Nevertheless, the performance of the YOLOv4-tiny method is surpris- ingly poor, exhibiting a significant gap compared with the YOLOv8 models.\nThe DETA-swin-large model achieves the second-worst result on this dataset among the competing methods, with an AP metric of 0.248, an AR1 of 0.125, and AR10 and AR100 scores of 0.440 and 0.441, respectively.\nThe other ViT-based method, i.e., the DETR-ResNet-50 model, produces reasonable results on this dataset, with its performance ranked third among competing methods, just behind the YOLOv8n and YOLOv8x models. Its AP measure is 0.702, while its AR1 is 0.258, and its AR10 and AR100 are both 0.760."}, {"title": "2) Results on the 11k hands dataset:", "content": "This dataset presents the simplest scenario for hand detection, as each image fea- tures a clear single hand with high resolution. Consequently, the YOLOv8 models achieve nearly perfect results on this dataset, with the AP metric for YOLOv8n and YOLOv8x being 0.998 and 0.999, respectively. All the AR1, AR10, and AR100 metrics of these two models are perfect, each registering at 1.000. The gap between the approaches in [34] and the YOLOv8 models is huge on this dataset. This is understandable because the approaches in [34] were not trained on the 11k hands data. The normal YOLOv3 model achieves the maximum performance among the approaches in [34], with an AP metric of 0.370, AR1 of 0.414, and both AR10 and AR100 at 0.415. Remarkably, the YOLOv4-tiny model fails to detect any hand objects.\nThe DETA-swin-large method achieves comparable results to the YOLOv8 models, with an AP metric of 0.996, and all of its AR1, AR10, and AR100 scores are 0.998. This per- formance is also comparable with that of the DETR-ResNet- 50 method, which exhibits slightly lower AP measures than those of the DETA-swin-large method but slightly higher AR measures."}, {"title": "3) Results on the Open Images dataset:", "content": "This dataset comprises hand images captured in various resolutions, con- ditions, and contexts, reflecting real-world scenarios. Among all methods evaluated, the YOLOv8x model emerges as the top performer, surpassing even the DETR-ResNet-50 and DETA-swin-large models. The YOLOv8x model achieves an AP metric of 0.417, with corresponding AR1, AR10, and AR100 scores of 0.252, 0.499, and 0.506, respectively. Following closely behind, both the DETR-ResNet-50 and YOLOv8n models rank as the second-best methods, each achieving an AP of 0.355. The AR1, AR10, and AR100 values of the DETR-ResNet-50 model are slightly higher than those of the YOLOv8n model. The former model achieves the AR1, AR10, and AR100 scores of 0.234, 0.466, and 0.478, respectively, while the latter model's scores are 0.233, 0.421, and 0.424, respectively.\nThe performance of the DETA-swin-large model ranks third in this dataset according to most evaluation metrics. When comparing the DETA-swin-large and YOLOv8n mod- els, DETA-swin-large outperforms YOLOv8n in only two metrics: AP@.50 (0.678 vs 0.593) and AR-S (0.144 vs 0.142). However, the DETA-swin-large model is inferior to YOLOv8n in all other metrics.\nThe existing approaches based on YOLOv3 and YOLOv4 in [34] rank last among the competing methods, with the highest performance achieved by the normal YOLOv3. Its AP is 0.126, AR1 is 0.108, and both AR10 and AR100 are 0.159."}, {"title": "4) Results on the combined dataset:", "content": "The results on this combined dataset are comprehensive, as we evaluate the performance of models trained on individual datasets on the test set of this combined dataset. Among the YOLOv8x models, YOLOv8x trained on the EgoHands dataset, i.e., YOLOv8x (ego), exhibits the least optimal performance, with an AP of 0.070, AR1 of 0.121, AR10 and AR100 both of 0.206. In contrast, the YOLOv8x model trained on the combined dataset demonstrates the best performance, with an AP of 0.542, AR1 of 0.539, AR10 and AR100 both of 0.588. Additionally, the model trained on the Open Images dataset, i.e., YOLOv8x (oi), outperforms the model trained on the 11k dataset, i.e., YOLOv8x (11k), with an AP of 0.475 compared to 0.269.\nThese comparison results demonstrate the impact of the training data and the similarity between the testing data and training data on the test performance of the object detection models. The YOLOv8x (ego) exhibits the worst performance, primarily due to the EgoHands dataset having the smallest number of training samples among the four datasets and contributing the least to the test set of the combined dataset.\nThe YOLOv8x (oi) model outperforms the YOLOv8x (11k) model due to the larger number of training samples in the Open Images dataset, which contributes more testing samples to the combined dataset compared to the 11k dataset. It is understandable that the YOLOv8x model trained on the combined dataset achieves the best performance. This is"}, {"title": "B. Identifying Images/Frames with High Forensic Values", "content": "In this section, we employ high-performing fine-tuned models to detect hand images within an extensive collec- tion or video frames that may possess significant forensic significance. These images and frames could then be scru- tinized further by forensic professionals in their analysis. We maintain a considerable margin for false positives, given that these images and frames will undergo thorough manual inspection by forensic experts subsequently. Based on the evaluation results obtained from the four datasets, we select the YOLOv8 model fine-tuned on the combined dataset for this purpose. Images or frames containing a significant portion of the hand are identified as having high forensic value. The size or area of the hand portion within an image can be calculated using the height and width of the bounding boxes detected by the object detection models. For a large folder of images, we predict on all images and filter out those with a hand area larger than a predetermined threshold value. The same process is applied to video frames. If an image contains more than one detected hand, the area of the largest hand is compared with the threshold. The threshold value can be adjusted; a higher threshold results in fewer images being selected, and vice versa.\nTable VII provides links to example videos processed by our fine-tuned YOLOv8 model. These videos were originally sourced from https://www.pexels.com/ and analyzed using the YOLOv8 hand detection pipeline to identify frames of high forensic value. Images with significant hand sizes or areas, and thus deemed of high forensic value, are saved separately, with several examples shown in Figs. 2-3."}, {"title": "VI. CONCLUSION AND FURTHER WORK", "content": "This paper presents object detection approaches based on YOLOv8 and detection transformers models for identifying hand images with high forensic value. We provided bounding box labels for the 11k hands dataset and created a com- bined dataset that includes hand images captured in various conditions and contexts. This enables us to effectively train hand object detection models and improve their performance compared to existing hand detection methods in the literature.\nWe assessed different variants of YOLOv8 and detection transformers across four datasets, ultimately selecting the most effective approach for recognizing hand images with notable forensic relevance. The YOLOv8 models have shown superior performance compared to detection transformer methods, such as DETR-ResNet-50 and DETA-swin-large. The utilization of YOLOv8 models in forensic practice yields exceptional results, significantly reducing the time required by forensic experts.\nThe hand region serves as a significant indicator of an image's forensic value. Nonetheless, images containing extensive hand areas can still encounter challenges such as occlusions, low-quality hand segments resulting from intense lighting, illumination effects, motion blur, or complex backgrounds. Further investigation is needed to address these factors and ultimately enhance efficiency, thereby reducing the workload for forensic experts.\nBeyond the technical aspect, it is important to note the ethical and legal considerations of using hand images for identification [35]. Privacy concerns, data protection, and informed consent are critical facets that need to be taken into account [36]. These aspects should be carefully con- sidered in the future when implementing hand image-based identification systems in real-world applications."}]}