{"title": "MAQInstruct: Instruction-based Unified Event Relation Extraction", "authors": ["Jun Xu", "Mengshu Sun", "Zhiqiang Zhang", "Jun Zhou"], "abstract": "Extracting event relations that deviate from known schemas has proven challenging for previous methods based on multi-class classification, MASK prediction, or prototype matching. Recent advancements in large language models have shown impressive performance through instruction tuning. Nevertheless, in the task of event relation extraction, instruction-based methods face several challenges: there are a vast number of inference samples, and the relations between events are non-sequential. To tackle these challenges, we present an improved instruction-based event relation extraction framework named MAQInstruct. Firstly, we transform the task from extracting event relations using given event-event instructions to selecting events using given event-relation instructions, which reduces the number of samples required for inference. Then, by incorporating a bipartite matching loss, we reduce the dependency of the instruction-based method on the generation sequence. Our experimental results demonstrate that MAQInstruct significantly improves the performance of event relation extraction across multiple LLMs.", "sections": [{"title": "1 Introduction", "content": "Event Relation Extraction (ERE) tasks are highly diversified due to their varying sub-tasks (coreference, temporal, causal, sub-event, etc.) and complex relations (symmetrical, asymmetrical, cross, etc.) [11, 28, 32, 34]. However, most previous studies [5, 19, 30, 37, 41] have primarily focused on optimizing specific sub-tasks, making it difficult to transfer model structures, optimization strategies, specialized knowledge sources, and domain data between different sub-tasks. Although a few of works [11, 31] use multi-head classification or prototype matching to tackle multiple sub-tasks simultaneously, these methods depend on pre-defined and mutually exclusive event relations. Recent large language models, such as ChatGPT and Llama, demonstrate exceptional text understanding and instruction-learning capabilities. The instruction-based approach eliminates the need for predefined relation schemas and mutual exclusivity in event relations, effectively solving previous issues. For a more intuitive comparison, we present the different methods in Figure 1. The classification-based method utilizes one-hot embedding to represent the event relation labels, which overlook the semantic information of the labels and struggle to effectively extract unseen event relations. The instruction-based method uses pairs of candidate event mentions and a comprehensive list of all event relations for instructions, leveraging the capabilities of large language models to generate these relations. Although the instruction-based method can address the issues present in classification-based methods, it also has two significant drawbacks. First, it requires a large amount of training and inference samples, reaching n \u00d7 n, where n represents the number of event mentions (dozens or hundreds within a single sample). Second, the model is greatly influenced by the sequence in which relations are generated. Using the instruction-based method shown in Figure 1 as an example, the model generates p(parent child) and p(child parent) with varying probabilities. However, in the ERE task, the sequence of generation should not affect the event relation between event mentions.\nTo address the two issues present in the instruction-based method, we design two strategies: multiple-answer question answering and bipartite matching. First, as shown in Figure 1, we transform the task from extracting event relations using given event-event instructions to selecting events using given event-relation instructions. Since the event relation types k < n (where k has a few kinds, and n is in the dozens or hundreds), we reduce the training and inference samples from n\u00d7n to k\u00d7 n. Second, based on the instruction-based framework, we introduce a bipartite matching loss. As demonstrated in Figure 2, using cross-entropy loss results in two mistakes, while the bipartite matching loss yields one correct answer and one mistake. In summary, the main contributions of this paper are:\n1) We propose an improved unified event relation extraction framework (MAQInstruct) based on multiple answer questions. Compared with InstructERE, our method reduces the training and inference samples from n x n to k \u00d7 n.\n2) In the MAQInstruct framework, we incorporate a bipartite matching loss to reduce the dependency of InstructERE on the generation sequence, making it more suitable for event relation extraction tasks."}, {"title": "2 Methodology", "content": "Instruction: To unify the various inputs for different ERE sub-tasks, we have developed a set of instructions. Each instruction specifies an event relation and a candidate event mention, indicated by the special character <0x64>-<0xFF> in LLMs. For instance, the instruction \"List the coreference event of <0x85> ruled?\" extracts coreferential events, with \"coreference\" as the relation and \"<0x85> ruled\" as the mention.\nContext: In the ERE task, we insert sequential markers (<0x64>-<0xFF>) into the text for each candidate event mention. The first mention receives <0x64>, the second <0x65>, and so on. These markers direct the language model to focus solely on the specified content. These markers enable LLMs to select the correct answer from these candidate events.\nLabel: The output consists of two parts: the dependency parsing chain and multiple answers, separated by a colon. Multiple answers are presented in the order they appear in the text, separated by commas; if there are no associated event mentions, this part will be set to none."}, {"title": "2.2 Multiple Answer Questions Loss", "content": "The generated sequence greatly influences the text generation effectiveness [3, 40], but in ERE, the order of answer generation should not affect the outcome. To minimize the impact of the generation sequence, we calculate distinct losses for the dependency parsing chain and multiple answers, defined as follows:\n$LCE = \\frac{1}{N} \\sum_{i=0}^{N} CE(y_i, p(y_{ix}))$\nwhere N = N1+N2, N1 represents the length of dependency parsing chain and N2 represents the length of multiple answers. CE is the cross-entropy loss. As illustrated in Figure 2, the sequence of generation does not impact the multiple answers. The loss for multiple answers is calculated as follows:\n(a) First, use the Hungarian Algorithm to find the optimal match.\n$O = arg \\min_{O \\in Y_{N2}} \\sum_{1=0}^{N2} 1 - log fp_{O(i)} (c_i)$\n(b) After optimal allocation, the loss function for Multiple Answers is:\n$LBPM = \\sum_{i=0}^{N2} 1 - log (i) (c_i)$\n(c) Finally, the total loss is as follows:\n$L = LCE + ALBPM$\nwhere YN2 denotes a permutation of N2. 0 is one of the permutations. (i) represents the i-th element in permutation \u03b8. ci represents the target vocabulary id of the i-th element. The probability of the i-th element in the permutation @ belonging to the target vocabulary id is denoted by pe(i) (ci). \u00d4 stands for the optimal permutation. The weight parameter is represented by \u03bb."}, {"title": "3 Experimental Results", "content": "Dataset. Our experiments are conducted on four datasets, including MAVEN-ERE [31] for unified event relation extraction, HiEve [8] for sub-event relation extraction, MATRES [21] for temporal relation extraction, and MECI [14] for causal relation extraction.\nComparison Methods. BertERE encodes the entire document with RoBERTa, adding a classification head for contextualized representations of various event pairs, and fine-tunes the model for relation classification. InstructERE uses candidate event pairs and relations as instructions, utilizing a large language model to generate the correct event relations."}, {"title": "3.2 Overall Results", "content": "The overall experimental results are summarized in Table 1. The performance of LLMs, specifically Llama2 and GPT-4, exhibits relative inadequacy in the ERE task when these models are not trained with instructions. This performance deficiency is attributed to the intricate definitions of event relations, which complicate comprehension for LLMs. Within a unified dataset MAVEN-ERE, BertERE demonstrates commendable performance, as there is no overlap between distinct event relations. While InstructERE leverages the excellent understanding capabilities of LLMs, it still does not surpass BertERE across various LLMs, indicating that InstructERE possesses inherent limitations in supervised event relation extraction tasks. Analyses indicate that InstructERE requires the construction of n\u00b2 samples, which often contain considerable overlapping content, thereby hindering the learning process for LLMs. The model MAQInstruct, built on multiple-answer questions and bipartite matching, surpasses InstructERE when evaluated on three LLMs: ChatGLM3 (ChatGLM3-6b), Qwen (Qwen-7B-Chat), and Llama2 (Llama2-7B-Chat). Notably, the MAQInstruct model trained on Llama2 exhibits improvements of 4.9%, 4.1%, 4.2%, and 3.9% in event coreference, temporal, causal, and sub-event relations compared to InstructERE, respectively. Additionally, compared to Bert-ERE, MAQInstruct shows enhancements of 0.4%, 1.7%, 1.6%, and 1.5%, respectively."}, {"title": "3.3 Inference Performance Analysis", "content": "The construction forms of samples for the three methods: BertERE, InstructERE, and MAQInstruct are shown in Figure 1. As can be seen from Table 2, the number of coreference event samples constructed by BertERE and InstructERE is significantly larger than that of MAQInstruct. Using an A100-80G GPU for inference with the same base model, Llama2, while keeping the batch size and sequence length, the inference time of InstructERE is 32.5 times that of MAQInstruct. Compared to BertERE, the inference time of MAQInstruct is still greater, since the parameter count of Llama2 is 70 times that of Bert."}, {"title": "3.4 Bipartite Matching Loss Analysis", "content": "The performance of a generative model is greatly affected by the generation sequence, as shown in Figure 3. \"Random\" indicates that the answers are in a random sequence, \"Sequence\" represents the sequence in which they appear in the text, \"Reverse\" indicates the reverse sequence of their appearance, \"Distance\" means the answers are sorted by distance from the query mention, and \"Dict\" sorts them from A to Z. When the bipartite matching loss is not considered, random answer sequences perform the worst, with a reduction of 4.00% and 3.92% compared to ordered sequences in MATRES and MECI, respectively. However, after incorporating the bipartite matching loss, MAQInstruct is capable of effectively generating the correct results with any answer sequence used."}, {"title": "3.5 Zero-Shot Learning Analysis", "content": "To validate the zero-shot learning capability of the model, experiments are conducted on three event relation extraction datasets: HiEve, MATRES, and MECI. The experimental results are presented in Figure 4. For instance, when comparing InstructERE and MAQInstruct, both trained with Llama2, it is observed that they significantly outperform the untrained Llama2 in event relation extraction tasks. The primary reason for this enhancement lies in the fact that LLMs trained with event relation extraction instructions possess a more comprehensive understanding of the definitions of event relations. Furthermore, the performance of MAQInstruct surpasses that of InstructERE, indicating that instruction based on multiple-answer questions is more effective for event relation extraction.\nAdditionally, to evaluate the effectiveness of data constructed from multiple-answer questions, the performance of MAQInstruct in general natural language understanding tasks is also assessed. As illustrated in Figure 5, the average performance of MAQInstruct slightly exceeds that of InstructERE and Llama2, demonstrating that neither the method of data construction from multiple answer questions nor the bipartite matching training approach adversely impacts the language understanding performance of LLMs."}, {"title": "3.6 Model Ablation Studies", "content": "Due to the substantial volume of MAVEN-ERE data, conducting ablation experiments proves to be excessively costly; consequently, we opt to perform ablation on MATRES and MECI. The experimental results are presented in Table 3. Initially, in the absence of the marker (<0x64>-<0xFF>), we observe performance declines of 2.48% on MATRES and 5.14% on MECI, which substantiates the efficacy of the prefix marker. In scenarios where multiple answers consist solely of markers, such as \"\", this results in a marginal decrease in effectiveness, suggesting that these markers may lack complete semantic information. Moreover, the elimination of the dependency parsing chain results in the most significant performance decline. This phenomenon can be attributed to the fact that the dependency parsing chain enhances the model's capability to extract scattered event relations by utilizing structured information. The removal of the bipartite matching loss function causes a significant drop in model effectiveness, indicating that the bipartite matching loss is particularly suitable for scenarios in which the sequence of generated results is not predetermined."}, {"title": "4 Conclusion", "content": "In this study, we present a unified framework called MAQInstruct, which aims to improve instruction-based methods through multiple-answer questions, effectively extracting various event relations via different types of instructions. By utilizing the instruction-based method, MAQInstruct significantly enhances this model's performance by introducing strategies such as multiple-answer questions and bipartite matching loss."}, {"title": "A Dependency Parsing Chain", "content": "We use the Stanford NLP toolkit's CoreNLP Dependency Parser to create a dependency parse tree from the context, generating various dependency edges. Their meanings are detailed in the toolkit's official documentation. In ERE tasks, we focus solely on the edges between event mentions and retain only the essential nodes and edges needed to connect them. If there is a tie in the number of nodes and edges, we keep them in the order they appear. As shown in Figure 6, both <r\u20811, r2, r4> and <r3, r2, r4>. It is crucial to mention that since the dependency parser functions at the sentence level, we substitute \".\" with \";\" to ensure the generation of the required dependency parsing chain."}, {"title": "B Experimental Settings", "content": "Dataset. Our experiments are conducted on four widely-used datasets (cf. Table 4), including MAVEN-ERE [31] for unified event relation extraction, HiEve [8] for sub-event relation extraction, MATRES [21] for temporal relation extraction, and MECI [14] for causal relation extraction. For a fair comparison, we divided the data into the same training, validation, and test sets as in previous studies [14, 18, 31, 43]. In particular, since the training and test sets are not separated, consistent with previous works, HiEve selects 80 documents for training (with a 0.4 probability for down-sampling negative examples) and 20 documents for testing. Since MAVEN-ERE does not have an open test set, we chose to use the validation set for testing.\nEvaluation Metric. Based on previous research on event relation extraction [4, 6, 19, 20, 30, 38, 41], we adopt the MUC [27], B\u00b3 [1], CEAFe [17] and BLANC [22] metrics for evaluating event coreference relations. For the other three subtasks, we adopt the standard micro-averaged precision, recall, and F1 metrics. In particular, in the sub-event relation extraction task, PC and CP represent the F1 scores for parent-child and child-parent relations, respectively.\nImplementation Details. MAQInstruct is conducted on a 4\u00d7A100-80G setup. The input sequence length is 1536, and the output sequence length is 512. The weight for the bipartite matching loss, denoted as A, is set to 0.2. We use a learning rate of 5e-4, a batch size of 16, and a gradient accumulation of 2. The learning rate scheduler follows a cosine function, and the model is trained for 20 epochs. The results reported in the experiment are the averages of 5 different random seeds (0, 1, 2, 3, 4). We train the model using an Adam optimizer with weight decay, with the weight decay rate set to 1e-4. The warm-up proportion for the learning rate is 0.1, and the dropout rate is also 0.1. The temperature used to adjust the probabilities of the next token is set to 0.01, while the smallest set of the most probable tokens, with top_p probabilities, adds up to 0.9. In the output, we use \":\" as a delimiter to distinguish the"}, {"title": "C Different Instructions Analysis", "content": "The event relation extraction model based on LLMs is significantly influenced by the provided instructions. Experiments are conducted to assess various sets of instructions, revealing that for fixed tasks, shorter and more concise instructions are typically more effective. Additionally, multiple tests are performed, as outlined in Table 5. Firstly, including all potential event mentions in the instructions results in a slight decline in the F1 score. Secondly, when the model is enabled to directly generate event relations based on event mentions, its performance significantly deteriorates due to the high volume of event mention pairs that produce relations labeled as NoRel. Furthermore, the model's performance reaches its lowest when multiple distinct relations are generated simultaneously."}, {"title": "D Different Markers Analysis", "content": "In this study, we employ various markers to elicit event mentions, building upon prior research [16, 39]. We categorize the experiments into three distinct groups, as illustrated in Table 6. The first group utilizes special tokens from Llama2, such as , which yield optimal results. Notably, the addition of a special end character following event mentions does not enhance performance due to insufficient semantic information and the introduction of multiple tokens that compromise coherence. The second group substitutes with , resulting in a significant reduction in effectiveness, as the presence of excessive tokens again leads to incoherence.\nIn the third group, the application of a uniform marker for all event mentions produces markedly poorer outcomes."}, {"title": "E Case Study", "content": "This study presents a qualitative analysis of the extraction of multiple answers, utilizing two examples of event temporal relation extraction, as illustrated in Figure 7. The first example effectively demonstrates the extraction process, supported by a valuable dependency parsing chain. Conversely, the erroneous example reveals two issues: the failure to recall event and the incorrect recall of event . These inaccuracies arise from the complexity of the dependency parsing chain, which obscures pertinent structural information while introducing extraneous details, resulting in errors."}, {"title": "F Related Work", "content": "Previous methods for event relation extraction [2, 10, 12, 13, 18, 25, 31] primarily utilize multi-class classification, MASK prediction, or prototype matching, focusing on addressing specific sub-tasks such as coreference, temporal, causal, or sub-event relations. In the classification-based approach [2, 12, 15, 26, 29, 42], event mentions are paired together, and additional features are incorporated, such as prototypes, logical rules, graph convolutional networks, or prompts. MASK prediction-based methods [7, 24, 35] train a masked language model to predict the relation. The prototype matching method [11] manually selects instances to serve as prototypes for each relation, and new instances are then matched against these prototypes. Segal et al. [23] and Hu et al. [9] each proposed reading comprehension models based on multi-choice and multi-span, respectively, allowing the model to select the correct answer from candidate options or to generate multiple answers simultaneously. Simultaneously, many entity relation extraction methods based on LLMs [33, 36] directly prompt large language models to generate relations between pairs of entities. However, these methods have several drawbacks. Therefore, we have designed a series of improvement measures to address these identified deficiencies."}]}