{"title": "PROGRESSOR: A Perceptually Guided Reward Estimator with Self-Supervised Online Refinement", "authors": ["Tewodros W. Ayalew", "Xiao Zhang", "Kevin Yuanbo Wu", "Tianchong Jiang", "Michael Maire", "Matthew R. Walter"], "abstract": "We present PROGRESSOR, a novel framework that learns a task-agnostic reward function from videos, enabling policy training through goal-conditioned reinforcement learning (RL) without manual supervision. Underlying this reward is an estimate of the distribution over task progress as a function of the current, initial, and goal observations that is learned in a self-supervised fashion. Crucially, PROGRESSOR refines rewards adversarially during online RL training by pushing back predictions for out-of-distribution observations, to mitigate distribution shift inherent in non-expert observations. Utilizing this progress prediction as a dense reward together with an adversarial push-back, we show that PROGRESSOR enables robots to learn complex behaviors without any external supervision. Pretrained on large-scale egocentric human video from EPIC-KITCHENS, PROGRESSOR requires no fine-tuning on in-domain task-specific data for generalization to real-robot offline RL under noisy demonstrations, outperforming contemporary methods that provide dense visual reward for robotic learning. Our findings highlight the potential of PROGRESSOR for scalable robotic applications where direct action labels and task-specific rewards are not readily available.", "sections": [{"title": "1. Introduction", "content": "Practical applications of reinforcement learning (RL) require that a domain expert design complex reward models that encourage desired behavior while simultaneously penalizing unwanted behavior [30, 33, 34]. However, manually constructing dense rewards that enable effective learning is difficult and can yield undesired behaviors [5, 15]. Sparse rewards require far less effort, however the lack of supervision typically results in a significant loss of sample efficiency. These issues are exacerbated for long-horizon tasks, where credit assignment is particularly difficult.\nA promising alternative is to learn effective rewards from unlabeled (i.e., action-free) videos of task demonstrations [2, 8, 19, 23, 26, 38]. Large-scale video data is readily available on the internet, providing a effective alternative to costly, intensive task-specific data collection. An effective way to incorporate diverse video data into a single model without learning a new policy for each task is to condition the model on a goal image that visually specifies the desired environment changes upon task completion [9, 23].\nTo that end, we propose PROGRESSOR, a self-supervised framework that learns a task-agnostic reward function from videos for goal-conditioned reinforcement learning that only requires image observations, without the need for corresponding action labels. Following Yang et al. [38], we assume that expert demonstrations make monotonic progress towards a goal and employ a self-supervised temporal learning approach that utilizes a proxy objective aimed at estimating task completion progress. PROGRESSOR predicts the distribution of progress from the current observation (i.e., the current image) relative to the initial and goal observations. This estimation provides dense supervision of an agent's progression towards reaching the goal, and thus performing the task, guiding exploration in alignment with expert execution trajectories. However, during RL exploration, agents will often encounter states that were not visited as part of the expert trajectories. PROGRESSOR accounts for this distribution shift via an online adversarial refinement that pushes back pushing back predictions for out-of-distribution observations.\nWe evaluate the effectiveness of PROGRESSOR for robot manipulation tasks in both simulation and the real world. These results demonstrate the the benefits that PROGRESSOR's self-supervised reward has on policy learning, particularly in settings where existing methods otherwise require intricate reward design. The key contributions of this paper are as follows:\n\u2022 We present PROGRESSOR, a self-supervised reward model trained on unlabeled videos that guides a reinforcement learning agent by providing dense rewards based on predicted progress toward a goal.\n\u2022 PROGRESSOR achieves state-of-the-art performance on six diverse tasks from the Meta-World benchmark [41] without the need for environment reward.\n\u2022 Pretrained on large-scale egocentric human video from EPIC-KITCHENS [6], PROGRESSOR enables few-shot real-robot offline, even when half of the demonstrations are unsuccessful."}, {"title": "2. Related Work", "content": "Learning from expert videos has gained traction due to the potential to scale the acquisition of complex skills using the vast amount of video content readily available.\nInverse RL Inverse reinforcement learning (IRL) is a framework for learning reward functions from expert demonstrations as an alternative to specifying them manually. In contrast to traditional RL, where the reward function is predefined and the goal is to learn an optimal policy, IRL focuses on deducing the underlying reward structure that results in the expert's behavior [1, 37, 44]. Recent IRL methods leverage adversarial techniques to learn reward functions [10, 11, 21], such as in Generative Adversarial Imitation Learning (GAIL) [18]. A key limitation of typical approaches to IRL is that they require action-labeled demonstrations, which are only available in small quantities. Another is the ambiguity that results from having multiple rewards that can explain the observed behavior [24], potentially leading to ill-shaped reward functions. In contrast, our notion of progress enables the learning of well-shaped dense rewards.\nImitation learning from video Several methods [4, 42] learn from videos by inferring the inverse dynamics model, leveraging a small amount of annotated data. However, these methods depend on the presence and quality of an albeit limited action labels. Other methods have used existing hand pose estimation as intermediate annotation followed by standard imitation learning [3, 29]. However, these methods rely on the accuracy of the pose estimators and camera calibration.\nLearning reward from video Recent methods learn visual representations useful for RL from video data using self-supervised temporal contrastive techniques [9, 23, 26, 32, 39]. Time-contrastive objectives treat observations close in time as positive pairs and those that are temporally distant as negative pairs. They then learn embeddings that encourage small distances between positive pairs and large distances between negative pairs. The learned embeddings can then be used to define reward functions, i.e., as the distance from the goal image in embedding space. Time-contrastive objectives can be sensitive to frame rate, and extracted rewards are assumed to be symmetric [23]. In comparison, our approach is agnostic to the sampling rate and can learn asymmetric dynamics depending on the specified initial and goal images.\nAdditionally, generative approaches have been used for reward learning. VIPER [8] estimates the log-likelihood of an observation as a reward function by training on an expert dataset using a VideoGPT-based autoregressive model. Similarly, Huang et al. [19] employ a diffusion-based approach to learn rewards by leveraging conditional entropy. However, these methods necessitate costly generative processes to estimate rewards.\nParticularly relevant, Rank2Reward [38] demonstrates that learning the ranking of visual observations from a demonstration helps to infer well-shaped reward functions. By integrating ranking techniques with the classification of expert demonstratin data vs. non-expert data obtained through on-policy data collection, Rank2Reward guides robot learning from demonstration. A key limitation of the method is that it necessitates training distinct models for each task due to the task-dependent nature of ranking frames. In contrast, our approach employs a single reward model for all tasks. Additionally, we utilize an adversarial training approach that tackles the domain shift observed during online RL, rather than incorporating a separate classifier as used in Rank2Reward."}, {"title": "3. Preliminaries", "content": "We formulate our method in the context of a finite-horizon Markov decision process (MDP) M defined by the tuple (O, A, P, R, \u03b3, \u03c1\u03bf), which represents the observation space, action space, reward function, transition probability function, discount factor, initial state distribution, respec-"}, {"title": "4. Method", "content": "We propose to learn a unified reward model via an encoder \\(E_{\\theta}(o_i, o_j, o_g)\\) that estimates the relative progress of an observation \\(o_j\\) with respect to an initial observation \\(o_i\\) and a goal observation \\(o_g\\), all of which are purely pixel-based. Figure 2 illustrates PROGRESSOR's framework for task progress estimation. Here, the objective is to learn the temporal ordering of observations that lead to the correct execution of a task. By learning this progress estimation, we can create a reward model that incentivizes progress toward the goal. Similar to Yang et al. [38], our method relies on the assumption that the values of states in optimal policies increase monotonically towards task completion."}, {"title": "4.1. Learning the Self-Supervised Reward Model", "content": "We measure an observation \\(o_j\\)'s progress towards a goal in terms of its relative position with respect to the initial observation \\(o_i\\) and the goal observation \\(o_g\\). In order to learn this progress estimate, we first represent the relative position of an observation \\(o_j\\) within an expert trajectory \\(T_k\\) as\n\\[\\delta(o_i^{T_k}, o_j^{T_k}, o_g^{T_k}) = \\frac{|j - i|}{|g - i|}\\]\nHere, \\(\\delta(o_i^{T_k}, o_j^{T_k}, o_g^{T_k})\\) calculates the ratio of the relative frame position differences between \\(o_j\\) and \\(o_i\\) and between \\(o_g\\) and \\(o_i\\) within a given expert trajectory \\(T_k\\). This progress label is always such that \\(\\delta(o_i^{T_k}, o_j^{T_k}, o_g^{T_k}) \\in [0, 1]\\).\nPredicting progress based on real-world video is challenging due to factors like camera motion and repetitive sub-trajectories. We address this by formulating the problem as one of estimating a Gaussian distribution over progress, allowing the model to estimate uncertainty as variance. With this approach, we approximate the ground-truth distribution for a triplet of frames \\((o_i^{T_k}, o_j^{T_k}, o_g^{T_k})\\) within a video \\(T_k\\) as a normal distribution with mean \\(\\mu_{T_k} = \\delta(o_i^{T_k}, o_j^{T_k}, o_g^{T_k})\\) and standard deviation \\(\\sigma_{T_k} = \\max(\\frac{1}{|T_k|^{1/2}}, c)\\):\n\\[P_{target}(o_i^{T_k}, o_j^{T_k}, o_g^{T_k}) = N(\\mu_{T_k}, \\sigma_{T_k})\\]\nOur upper-bound of \\(\\sigma_{T_k}\\) downweights the penalty for triplets in which there are many frames between the initial and goal images, which we empirically find to improve robustness during training. Therefore, the predicted distribution of this form provides a distribution of the progress estimate. Sampling from this predicted distribution yields values that reflect the observed progress.\nWe optimize our reward model \\(r_{\\theta}\\) to predict the distribution of the progress on expert trajectory. We use a shared visual encoder to compute the per-frame representation, followed by several MLPs to produce the final estimation:\n\\[E_{\\theta}(o_i, o_j, o_g) = N(\\mu, \\sigma^2)\\]\nFollowing Kingma and Welling [20], we estimate \\(\\log \\sigma^2\\) and then compute \\(\\sigma\\) as \\(\\sigma = \\exp(0.5 \\log \\sigma^2)\\). We learn the parameters of the network by optimizing the Kullback-Leibler divergence loss between the ground-truth and the predicted distributions:\n\\[L_{expert} = D_{KL}(P_{target}(o_i^{T_k}, o_j^{T_k}, o_g^{T_k}) || E_{\\theta}(o_i^{T_k}, o_j^{T_k}, o_g^{T_k}))\\]\nDuring the training of our reward model \\(r_{\\theta}\\), we randomly select and rank three frames from a video sequence of expert trajectories as a training triplet. Additionally, since these triplets can be randomly drawn from any trajectory within the dataset of different robotics tasks, a single reward model suffices to handle a variety of tasks.\nUsing the Reward Model in Online RL: Due to the monotonic nature of task progress in our problem formulation, the estimates derived from the trained model can serve as dense rewards for training, replacing the unknown true reward. We create the reward model by defining a function derived from the model's predicted outputs given a sample of frame triplet \\((o_i, o_j, o_g)\\) of trajectory as:\n\\[r_{\\theta}(o_i, o_j, o_g) = \\mu - \\alpha H(N(\\mu, \\sigma^2))\\]\nwhere \\(r_{\\theta}\\) denotes PROGRESSOR'S reward estimate and \\(H\\) is the entropy. The first term \\(\\mu\\) in Eqn. 5 uses the mean prediction from the progress estimation model to determine the position of the current observation relative to the goal. The"}, {"title": "4.2. Adversarial Online Refinement via Push-Back", "content": "Our reward model \\(r_{\\theta}\\) can be used to train a policy that maximizes the expected sum of discounted rewards as:\n\\[\\pi^* = \\arg \\max_{\\pi} E_{\\pi} \\left(\\sum_{t=1}^T \\gamma^t r_{\\theta}(o_i^{\\tau'}, o_j^{\\tau'}, o_g^{\\tau'}) \\right)\\]\nThe reward encourages the agent to take actions that align its observed trajectories \\(\\tau'\\) with those of the expert demonstrations \\(T\\), thereby promoting task completion. However, the all-but-random actions typical of learned policies early in training result in out-of-distribution observations relative to the expert trajectories. As a result, optimizing the policy using a reward \\(r_{\\theta}\\) that is only trained on an expert trajectory \\(T\\) can result in unintended behaviors, since \\(r_{\\theta}\\) does not reflect the actual progress of \\(\\tau'\\).\nTo tackle this distribution shift, we implement an adversarial online refinement strategy [12, 13], which we refer to as \"push-back\", that enables the reward model \\(r_{\\theta}\\) to differentiate between in- and out-of-distribution observations \\(T\\) and \\(\\tau'\\), providing better estimation. Specifically, for a"}, {"title": "5. Experimental Evaluation", "content": "We evaluate the effectiveness with which PROGRESSOR learns reward functions from visual demonstrations that enable robots to perform various manipulation tasks in simulation as well as the real world. The results demonstrate noticeable improvements over the previous state-of-the-art."}, {"title": "5.1. Simulated Experiments", "content": "In our simulated experiments, we used benchmark tasks from the Meta-World environment [41], selecting six table-top manipulation tasks (Figure 3 e-j): door-open, drawer-open, hammer, peg-insert-side, pick-place, and reach. For all tasks, we trained the model-free reinforcement learning algorithm DrQ-v2 [40] using the reward predicted by PROGRESSOR in place of the environment reward.\nWe compare our method to three relevant baselines: (1) TCN [31], a temporal contrastive method that optimizes embeddings for measuring task progress; (2) GAIL [17], an adversarial approach that aligns the state-action distribution of a policy with that of an expert. GAIL's discriminator is used as a reward function for our baseline following Yang et al. [38]; and (3) Rank2Reward [38], a state-of-the-art reward learning method that combines temporal ranking and classification of expert demonstrations to estimate rewards.\nFor a fair comparison, we use their default hyperparameters and trained all methods under the same setup. All methods were trained for 1.5M steps, relying solely on the reward estimates provided by the model.\nWe evaluate the methods in terms of the episodic return, which reflects the cumulative reward received by the agent during each episode of the task. To ensure the robustness of our results, we average the episodic return over five different seeds for both the baseline methods and our approach, providing a comprehensive comparison of performance across all tested tasks. This setup allows us to analyze the impact of our proposed method in a controlled, simulated environment.\nWe present the results in Figure 4. Our method significantly outperforms the baseline methods across most tasks, with a notable advantage in efficiency. In the drawer-open and hammer tasks, PROGRESSOR requires only 10% of the total training budget to achieve higher rewards than the baselines. In the door-open and peg-insert-side tasks, where almost all other methods fail entirely, our approach demonstrates strong performance and a higher success rate.\nAblating Push-back We examine the impact of adversarial online refinement via push-back in our framework. Across various tasks (see Figure 4), we find that online push-back significantly enhances PROGRESSOR's performance, particularly on challenging tasks such as door-open and hammer. These tasks demand more precise discrimination between expert behavior and online roll-outs to provide an informative reward signal. Notably, PROGRESSOR without push-back still outperforms every baseline on the drawer-open and reach tasks.\nIn the case of the reach task, we observe that PROGRESSOR with push-back shows a decrease in return as training progresses, a trend that is less pronounced in the version without push-back. We hypothesize that this occurs because the model initially achieves a high success rate-reaching a peak with only 10% of the total steps-leading the refinement process to overly penalize the agent, even when its behavior closely resembles the expert.\nOnline training with non-expert data is crucial for robust performance. TCN, which is only trained on expert demon-"}, {"title": "5.2. Real-World Robotic Experiments", "content": "In this section, we demonstrate how PROGRESSOR, pretrained on the egocentric EPIC-KITCHENS dataset [6, 7], can efficiently learn robotic tasks from a limited number of demonstrations, even when some are unsuccessful. Our approach enhances sample efficiency and robustness to noisy data in offline RL, making it more effective than traditional behavior cloning (BC) methods."}, {"title": "5.2.1 Pretraining on Kitchen Dataset", "content": "Using ResNet34 [16] as a backbone, we first pretrain our encoder \\(E_{\\theta}\\) with Equation 4 taking P01-P07 sequences from the EPIC-KITCHENS dataset composed of approximately 1.29M frames. We randomly sample frame triplets \\((o_i, o_j, o_g)\\) from the videos ensuring a maximal frame gap \\(||i - g|| \\le 2000\\). To improve the robustness of \\(E_{\\theta}\\), we additionally train with distractor frames where we replace the current observation \\(o_j\\) with a frame randomly sampled from different video sequence \\(o_{j'}\\), as negative examples. For neg-"}, {"title": "5.2.2 Baselines", "content": "We compare PROGRESSOR with R3M [25] and VIP [22]. VIP and R3M are self-supervised visual representations shown to provide dense reward functions for robotic tasks. Both R3M and VIP are pretrained on Ego4D [14] with 4.3M frames from videos from 72000 clips. R3M is trained via time contrastive learning, where the distance between frames closer in time is smaller than for images farther in time. Additionally, they leverage L1 weight regularization and language embedding consistency losses. VIP uses a contrastive approach treating frames close in time as negative pairs and those further away as negative pairs towards learning visual goal-conditioned value functions."}, {"title": "5.2.3 Real-World Few-Shot Offline Reinforcement Learning with Noisy Demonstrations", "content": "Following the offline reinforcement learning experiments of Ma et al. [22], we leveraged the reward-weighted regression (RWR) method [27, 28]. Our aim in applying RWR is to show that a reward model trained on human videos can help robots learn to perform tasks even when the training data contains noisy, suboptimal trajectories. In scenarios with highly suboptimal trajectories, vanilla behavior cloning (BC) approaches often struggle to learn the correct behaviors. However, goal-conditioned reward weighting provides a signal that focuses learning from the correct sub-trajectories, enabling the visual imitation learning model to effectively learn accurate action executions.\nTo this end, we start with a vanilla BC model and adapt the loss function to be weighted by a pretrained reward model. Specifically, we employed Action-Chunking Transformer (ACT) [43], a BC model designed for learning fine-grained manipulation tasks. We modified ACT's recon-struction loss originally defined as the mean absolute error between the predicted action and the ground-truth action-to incorporate RWR as follows:\n\\[L_{reconst} = \\exp(w \\cdot r) \\cdot ||\\pi_{ACT}(o_i) - a_t||_1\\]\nwhere \\(r\\) is the reward prediction for the current observation \\(o_j\\), \\(w\\) is the temperature parameter, \\(a_t\\) is the ground-truth action, and \\(\\pi_{ACT}(o_j)\\) represents the action predicted by ACT by taking the current observation image (\\(o_j\\)) as an input. In this paper, we refer to ACT trained with the reconstruction loss replaced by Equation 8 as RWR-ACT, to distinguish it from the standard ACT.\nWe compare PROGRESSOR with R3M and VIP by freezing the pre-trained models and using them as reward prediction models to train RWR-ACT on downstream robotic learning tasks. The reward predictions from these models are used in place of \\(\\mu\\) in Equation 8. In both VIP and R3M, the reward prediction is parameterized by the current observation, previous observation, and goal observation. In contrast, our method parameterizes reward using the initial observation, current observation, and goal observation. Additionally, we include vanilla ACT as a baseline, which applies uniform weighting (i.e., \\(w = 0\\)) across all observations in the training trajectories .\nWe design four tabletop robotic manipulation tasks (see Figure 3 a-d): Drawer-Open, Drawer-Close, Pick-Place-Cup, and Push-Block. For each task, we collect 40 demonstrations, half of which are suboptimal and fail to complete the task. Including these failed demonstrations is crucial for evaluating whether the learned reward model can accurately signal progress toward a goal by assigning high reward to transitions that lead toward completion and low reward to those that do not. Table 1 summarizes the behaviors in failed demonstrations. Detailed task and data descriptions along with frame sequences from both successful and failed trajectories, are provided in S9. We categorize Drawer-Close as an easy task and the other three as hard tasks. This distinction in difficulty is based on the complexity of the tasks and the level of suboptimality in failed demonstrations.\nWe train all policies using the same hyperparameters employed by Zhao et al. [43] for training ACT in their real-world behavior cloning experiments (full details in S9.2). For all RWR experiments, we set \\(w = 0.1\\). Each method is then evaluated over 20 rollouts, and the success rate is reported. The success criteria for each task are presented in S9.\nThe evaluation results presented in Figure 5 highlight the significant advantage of PROGRESSOR-RWR-ACT, which consistently outperforms all baseline methods across all tasks. This advantage is particularly pronounced in the more challenging tasks, such as Push-Block and Pick-Place-Cup, on which other methods struggle to"}, {"title": "6. Conclusion and Limitations", "content": "In this work, we presented PROGRESSOR, a self-supervised framework that learns task-agnostic reward functions from video via progress estimation. By learning progress estimation between observations in expert trajectories, PROGRESSOR generates dense rewards that effectively guide policy learning. During online RL training, this progress estimation is further refined through an adversarial push-back strategy, helping the model handle non-expert observations and minimize distribution shift. Our method shows enhanced performance compared to previous state-of-the-art approaches across a range of simulated experiments. In real-robot experiments, we applied PROGRESSOR, pretrained on in-the-wild human videos, to learn policies with limited and noisy task demonstrations, outperforming other visual reward models.\nLimitations and Future Work We acknowledge several limitations in our work. (1) Our method assumes a linear progression of tasks and models progress as a unimodal prediction, making it unsuitable for tasks with cyclic observations, such as those in the DeepMind Control Suite [35]. Future enhancements that model progress as a multimodal prediction could address this limitation more effectively. (2) While our online RL experiments demonstrate that our hyperparameters are robust across various tasks, incorporating a dynamic weighting factor \\(\\beta\\) for refinement may further enhance performance."}, {"title": "7. PROGRESSOR Training Details", "content": "PROGRESSOR Training Details"}, {"title": "7.1. Architecture and Training", "content": "PROGRESSOR can, in principle, be trained with any visual encoding architecture, requiring only minor modifications to the final layer to predict Gaussian parameters. In our experiments, we utilize the standard ResNet34 model [16], replacing its final fully-connected layer with an MLP of size [512, 512, 128]. Given that our method processes triplets of inputs (0\u017c, Oj, 0g), the resulting representation has a size of [128 x 3]. This representation is then fed into an MLP with layers of size [128 \u00d7 3, 2048, 256]. Finally, two prediction heads are derived from the 256-dimensional output, predicting \u03bc and log 02.\nWe pretrain PROGRESSOR for 30000 steps using the EPIC-KITCHENS dataset [6] for the real-world experiments, and for 10000 steps for experiments performed in simulation. In the pretrianing steps for both our simulated and real-world experiments, we first sample a trajectory (i.e., a video clip) from the pretraining dataset. We then randomly select an initial frame or as well as a goal frame og from the selected trajectory such that ||g-i|| \u2264 2000. Finally, we uniformly randomly select an intermediate frame oj, where i < j < g."}, {"title": "7.2. Hyperparameters", "content": ""}, {"title": "8. Simulation Experiment Details", "content": "In this section, we describe the tasks and the data generation process employed using the MetaWorld environment [41] for our simulation experiments."}, {"title": "8.1. Meta-World Tasks", "content": "We took six diverse tasks from the Meta-World environment [41], described in Table 3. In all tasks, the position of the target object, such as the drawer or hammer, is randomized between episodes."}, {"title": "8.2. Expert Data Generation", "content": "To collect expert trajectories for our simulated experiments, we execute Meta-World's oracle policies. For each task, we generated 100 successful rollouts for training and 10 for testing. This dataset is subsequently used for pretraining PROGRESSOR in our simulated experiments, following the steps outlined in Section 7.1."}, {"title": "9. Real-World Robot Experiment Details", "content": "Real-World Robot Experiment Details"}, {"title": "9.1. Robotic Experiment Setup", "content": "The real-robot experiments are performed using a Universal Robots UR5 robot arm equipped with a Robotiq 3-Finger Gripper (Figure 8). The setup includes two RealSense cameras: one mounted on the robot's wrist that images the gripper, and the second on a fixed tripod facing the robot."}, {"title": "9.2. Robotic Demonstration Data Collection", "content": "We collect demonstrations by teleoperating the UR5 using a Meta Quest 3 controller [36]. We record each demonstration at 30 Hz for 400 steps. Figure 9 shows video frame sequences from correct and incorrect demonstrations for the four real-world tasks."}, {"title": "9.3. Task Descriptions", "content": "The tasks involve a variety of object manipulation challenges designed to test reward weighting in offline Reinforcement Learning. In the Drawer-Close task, the objective is to close a drawer starting from an open position. The Drawer-Open task requires the agent to pull a drawer open from a closed state. In the Push-Block task, the goal is to push a block toward a specified target, a cup, ensuring the block moves into proximity with the cup. Finally, Pick-Place-Cup involves picking up a cup and carefully placing it into a designated box. Figure 10 displays the goal frames representing the completion of each task."}, {"title": "9.4. Training and Evaluation Details", "content": "Our few-shot offline RL implementation builds upon the Action-Chunking Transformer (ACT) [43]. The inputs to the model include (i) two 640\u00d7480 RGB images from the RealSense cameras, and (ii) proprioceptive data consisting of the 6-DoF joint angles and the binary open or close state of the gripper. The action space consists of the 6-DoF translational and rotational velocity of the end-effector and a binary open or close command of the gripper.\nIn the reward-weighted regression (RWR) setup, rewards are computed by providing all reward models with the final frame of a correct demonstration from each task as the goal image. For all reward predictions, frames from the fixed RealSense camera were used. Figure 10 illustrates the goal images used for the four robotic tasks, which were consistently used across all reward predictions. The temperature scale in RWR was set to w = 0.1 for all tasks."}, {"title": "10. Ablation", "content": "In this section, we present an ablation study evaluating different values of the push-back decay factor (3) while training a DrQ-v2 agent on Meta-World's hammer task, using a fixed seed of 121. The case of f = 0 (PROGRESSOR without Push-back) is discussed in the main paper. Figure 11 depicts the environment rewards accumulated during training. As shown in the figure, the agent achieves higher rewards with \u03b2 = 0.9."}, {"title": "11. Qualitative Analysis", "content": "Figure 12 presents zero-shot reward predictions from PROGRESSOR pretrained on the EPIC-KITCHENS dataset. This figure serves as an extension to Figure 7 for completeness. It includes zero-shot reward predictions for sample correct trajectories from our collected real-robot demonstrations for the tasks Drawer-Open, Drawer-Close, and Push-Block.\nFor completeness, we have included plots similar to the mean reward prediction figure from the main paper (see Figure 7). The plots in Figure 13 provide a qualitative comparison of reward predictions from our robotic demonstration dataset for three additional real-world tasks: Drawer-Close, Push-Block, and Pick-Place-Cup. As illustrated in the figure, for all tasks, our reward model consistently predicts lower average rewards for the sub-trajectories in the incorrect demonstrations, where the failures occur."}]}