{"title": "On the attribution of confidence to large language models", "authors": ["Geoff Keeling", "Winnie Street"], "abstract": "Credences are mental states corresponding to degrees of confidence in propositions. Attribution of credences to Large Language Models (LLMs) is commonplace in the empirical literature on LLM evaluation. Yet the theoretical basis for LLM credence attribution is unclear. We defend three claims. First, our semantic claim is that LLM credence attributions are (at least in general) correctly interpreted literally, as expressing truth-apt beliefs on the part of scientists that purport to describe facts about LLM credences. Second, our metaphysical claim is that the existence of LLM credences is at least plausible, although current evidence is inconclusive. Third, our epistemic claim is that LLM credence attributions made in the empirical literature on LLM evaluation are subject to non-trivial sceptical concerns. It is a distinct possibility that even if LLMs have credences, LLM credence attributions are generally false because the experimental techniques used to assess LLM credences are not truth-tracking.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) such as GPT-4 (Open AI, 2023) and Gemini (Gemini Team, 2023) are neural networks trained to predict the next word in a sequence of words on massive text datasets.\u00b9 Training for next word prediction on broad data allows LLMs to acquire capabilities including theory of mind, spatial reasoning, and coding (Bubeck et al., 2023). LLMs have given rise to a vast empirical literature that seeks to identify and evaluate their capabilities.2\nScientists engaged in LLM evaluation often attribute credences to LLMs; that is, degrees of belief or confidence in propositions. Credences are attributed"}, {"title": "Large Language Models", "content": "Language models are statistical models that, roughly, predict the next word given a sequence of words. Specifically, language models are functions. The inputs are sequences of tokens, which are syntactic units that include words,"}, {"title": "Evaluation and Credence Attribution", "content": "In this section, we characterise the emerging science of LLM evaluation, explain the role of LLM credence attribution within LLM evaluations, and distinguish three experimental techniques for assessing what credences LLMs have.\nModel evaluation is a widespread practice in machine learning that aims to assess the performance of machine learning models at particular tasks. Central to model evaluation are benchmarks.5 For example, the General Language Un-derstanding Evaluation (GLUE) benchmark provides training and test datasets alongside performance metrics for nine natural language understanding tasks including sentiment analysis and sentence classification, allowing for compar-isons between models (Wang et al., 2019). Similarly, the ImageNet Large Scale Visual Recognition Challenge provides a training and test dataset, and a per-formance metric, which serve as a common standard for evaluating image-based object recognition models (Russakovsky et al., 2015; see also Deng et al., 2009).\nEvaluating LLMs is more complex than evaluating narrow models that are trained to perform a particular task such as image classification. LLMs are trained with a generic objective next token prediction that given broad training data enables the model to acquire an open-ended set of capabilities (Bommasani et al., 2021; Chang et al., 2023). The added challenge of LLM evaluation is identifying what capabilities the model has acquired in training,"}, {"title": "Semantics", "content": "How should we interpret LLM credence attributions? Are scientists quite liter-ally ascribing degrees of confidence to LLMs; or are LLM credence attributions a stylised shorthand that readily translates into non-mentalistic language? In this section, we argue that LLM credence attributions should (at least in gen-eral) be interpreted literally; that is, as expressing truth-apt beliefs on the part of scientists that purport to describe facts about LLM credences.\nFirst, there is a presumptive case for interpreting LLM credence attribu-tions literally. LLM credence attributions appear to describe facts about LLM credences. Accordingly, there is a defeasible reason to suppose that LLM cre-dences are literal ascriptions of credences to LLMs. The presumptive case is reinforced by the fact that scientists evidence LLM credence attributions with experimental measurement techniques. That scientists give empirical justifica-tions for LLM credence attributions indicates a background commitment to the claim that there is a fact of the matter about what the LLM's credences are.\nThe presumptive case can be strengthened with an argument from elimina-tion. The two most obvious non-literal interpretations of LLM credence attribu-tions are implausible. On one hand, it may be that LLM credence attributions are shorthand for statements about the token succession probabilities given by LLMs conditional on token sequences. This interpretation would be plausible if attributions of credences to LLMs were restricted to propositions of the form \u300ct succeeds s\u00b9 for tokens t \u2208T and token sequences s \u2208 TN. Then utterances such that the LLM has such-and-such confidence in 't succeeds s\u00b9 could be interpreted as a stylised shorthand for the non-mentalistic claim that the LLM"}, {"title": "Metaphysics", "content": "If LLM credence attributions are correctly interpreted literally, the next obvious question is whether LLM credences exist. For if LLMs lack credences, then LLM credence attributions are all subject to a presupposition failure, and LLM credence attributions as a class of statements are subject to wholesale error. We first clarify the claim that LLM credences exist and then argue that, while this claim is somewhat plausible, the evidence for its acceptance is inconclusive."}, {"title": "The Existence Claim", "content": "What does it mean to say that LLM credences exist or that LLMs have cre-dences? Traditionally, credences admit two rival interpretations that corre-spond to the distinction between mentalist and behaviourist interpretations of decision-theory (Dietrich and List, 2016; Okasha, 2016; see also Hansson, 1988; Berm\u00fadez, 2009). First, mentalists think that credences are psychologically real mental states that, together with utilities, explain an agent's behaviour. On this view, credences and utilities are quantitative extensions of belief-desire psychology, where actions are causally explainable with reference to gradational as opposed to categorical belief-like states (Lewis, 1974, 337; see also Hampton, 1994; Okasha, 2016). Second, behaviourists think that credences are formal con-structs that alongside utilities are derivable from an agent's revealed preferences. This latter view is widely endorsed in microeconomics where, in the tradition of Savage (1972), the idea is that an agent can be represented as an expected utility maximiser relative to a credence function and a family of utility functions so long as their revealed preferences satisfy certain rationality constraints.9\nThe claim that LLM credences exist can be clarified in light of the dispute between mentalism and behaviourism. First, the claim that LLM credences exist should be read along mentalist lines. On the one hand, were LLM scien-tists attributing LLMs credences in the behaviourist sense, then the evidence"}, {"title": "Inference to the Best Explanation", "content": "We now assess the case for the existence of LLM credences. We argue that the existence of LLM credences is at least plausible, but that current evidence is inconclusive. What, then, is the case for accepting that LLM credences exist?\nOne option is to say that LLMs report having credences in propositions, and these reports provide a defeasible reason to suppose that LLMs have cre-dences. This argument is suspect because LLMs are trained to reliably mimic"}, {"title": "Epistemology", "content": "Even if LLMs have credences, it is possible that techniques such as reported confidence, consistency-based estimation, and measuring output probabilities, fail to track facts about LLM credences. If so, then attributions of credences to LLMs made on the basis of these techniques are not even generally true.\nWe discuss several challenges to reported confidence, consistency-based es-timation, and output probabilities, as indicators for LLM credences. We argue, first, that reported confidence is not a reliable indicator of LLM credences. Sec-ond, we argue that under certain conditions consistency-based estimation may be a reliable indicator of LLM credences, but at present we lack a clear mecha-nism which guarantees that signals about the LLM's credences reliably show up in the distribution of answers over multiple independent trials. Third, we argue that it is at best unclear how to infer LLM credences from output probabilities given that output probabilities apply to syntactic units \u2013 tokens as opposed to propositions. We conclude that even if LLMs have credences, we may well be in a sceptical scenario in which techniques for evidencing credences are unreliable."}, {"title": "Stochasticity and Reported Confidence", "content": "We start with the most straightforward method for assessing LLM credences; namely, prompting the LLM to report its confidence in a proposition (Xiong et al., 2023; Lin et al., 2022; Kadavath et al., 2022). Reported confidence judgements from LLMs cannot directly indicate LLM credences in the sense that an LLM has a credence c in X just in case the LLM reports a credence c in X.\nFor reported confidence judgements to evidence LLM credences directly, we need a mechanism to ensure that identifying signals for the LLM's credences show up in the LLM's reported confidence judgements. However, LLM reported confidence judgements are generated by a stochastic sampling process. So, any mechanism linking the LLM's credences to its reported confidence judgements is mediated by a stochastic process such that whatever credence the LLM reports"}, {"title": "Distorting Factors and Consistency-Based Estimation", "content": "Perhaps we can infer LLM credences indirectly from the distribution of reported credences over many independent trials. For example, given the prompt How confident are you that Fincher directed Fight Club?, we could plausi-bly infer that the LLM is around 95% sure that Fincher directed Fight Club if over 100 trials the LLM returns 95% as the most frequent response, and the vast majority of the LLM's responses are 93%, 94%, 95%, 96%, and 97%. Plausi-bly, a similar picture holds for humans, in which having a credence c in X may dispose a human to report confidence levels in the ballpark of c, but where any particular reported confidence is an unreliable indicator of the credence, c.\nThis approach to inferring LLM credences from reported confidence judge-ments is a form of consistency-based estimation (Manakul et al., 2023). We could similarly prompt the LLM 100 times with Did Fincher direct Fight Club?. Then if the LLM responds Yes in a large majority of cases, we can sup-posedly infer that the LLM has high confidence that \u300cFincher directed Fight Club\u00b9. The principle in both cases is the same: Informative signals about LLM credences supposedly show up in the distribution of the LLM's responses con-ditional on an appropriately selected prompt over multiple independent trials.\nWe argue that the distribution of answers reliably signals the LLM's cre-dences only if certain conditions obtain. Distorting factors that have the po-tential to skew the distribution including non-standard temperature values and non-standard sampling methods need to be avoided. Even then, we lack a pos-itive account of the mechanism by which signals about the LLM's credences reliably show up in the distribution of answers over multiple independent trials."}, {"title": "Temperature", "content": "First, the relative frequencies of answers given by the LLM in response to a prompt over multiple independent trials depend in part on the LLM's out-put probabilities for tokens conditional on the initial input sequence.13 Given random sampling, the relative frequencies of (single-token) responses over inde-pendent trials will in the limit converge on the output probabilities for tokens.\nWhat probabilities the LLM outputs conditional on a given input also de-pends on a parameter called temperature. This parameter is set by the user and controls the smoothness of the probability distribution. To explain: Output probabilities are generated in the final layer of the neural network. The penul-timate layer computes logits for each token. Logits are real-numbers that can be positive or negative and which indicate how confident the LLM is that each token succeeds the input sequence. Logits are converted into probabilities (that is, real-numbers between 0 and 1 that sum to 1) in the final layer via softmax normalization. Logit $x_k$ for token $t_k$ is converted to probability $p_k$ as follows:\n$p_k = \\frac{e^{x_k}}{\\sum_{i=1}^{N} e^{x_i}}$\nThe top half of the fraction ensures that the output is positive because $f(x)=e^x$ is positive in all its arguments, and also preserves the order of the logits because $f(x)=e^x$ is strictly increasing. The bottom half of the fraction ensures that the outputs sum to one. Hence softmax normalisation converts logits into probabilities in a way that ensures that tokens with greater logit values receive a greater share of the probability mass. The temperature $T$ determines the smoothness of the distribution. First, take the case where $T \\in (0,1)$. As $T \\rightarrow 0$, the effect is to increase the distances between logits, resulting in more extreme differences in probabilities. So, comparatively higher probability tokens receive a greater fraction of the total probability mass. Second, take the case where $T \\in [1,\\infty)$. Here as $T \\rightarrow \\infty$, the effect is to decrease the distance between logits, such that in the limit the distribution converges on a uniform distribution that assigns equal probability to each token. Hence for higher temperatures comparatively higher probability tokens receive a smaller fraction of the total probability mass.\nThe practical significance of temperature is that temperature controls the creativity or predictability of text generated when sampling from the LLM. Con-sider the token sequence The cat sat on the. When T is low, the bulk of the probability mass will be assigned to comparatively higher probability tokens such as mat and chair, such that sampling from the LLM will typically yield predictable non-creative completions. When T is high, the probability mass will be more uniformly distributed across tokens rendering it more probable that comparatively less likely tokens such as train or pear will be sampled. Hence higher temperature yields less predictable and thus more creative completions.\nIt is hard to see how the distribution of answers given by the LLM could reli-ably indicate the LLM's credences given that the observed distribution depends on the user's choice of temperature - an exogenous factor. Holding fixed the in-put sequence, each temperature value yields a unique set of output probabilities, and these output probabilities determine the relative frequencies of responses over multiple independent trials. Presumably, if LLMs have credences, then those credences are endogenous to the LLM in the sense of being encoded in the LLM's model weights. Because the distribution of responses depends in part on"}, {"title": "Sampling Methodology", "content": "Nevertheless, the problem does not obviously resolve even if we grant the ex-istence of a non-arbitrary temperature. Consistency-based estimation requires sampling responses from the LLM. But the user's choice of sampling methodol-ogy impacts the distribution of responses in much the same way as temperature.\nTo explain: Suppose we input the sequence: How confident are you that Fincher directed Fight Club?. First, the LLM computes a probability dis-tribution over tokens conditional on the input. A token is sampled from the dis-tribution. Suppose we get 99. The sampled token is appended to the input. The resultant sequence (How confident are you that Fincher directed Fight Club? 99) is inputted into the LLM. The LLM computes a distribution, and we sample another token. Suppose we get %. Hence we get a 99% confidence judgement in the proposition \u300cFincher directed Fight Club\u00b9. Two common sam-pling methods are top-p and top-k sampling. Top-p samples a token from the smallest set of tokens whose cumulative probability exceeds some threshold, p. The successor token is selected from this set via a weighted lottery taking into account the probabilities assigned to each token. In contrast, top-k sampling samples from the k tokens with the highest probability, and again the sampling is weighted by the probabilities that the LLM assigns to each token. The dis-tribution of answers that we observe across a given number of trials depends in part on the LLM's output probabilities for each token, but it also depends on the sampling methodology used. For example, whether top-p or top-k sampling is chosen, and the chosen value for p and k respectively. This latter dependency is problematic. For we could in effect force the LLM to exhibit greater or lesser consistency in its answers through our chosen sampling methodology. For exam-ple, using top-p sampling with a value of p close to 0 would all but guarantee a"}, {"title": "Summary", "content": "The user's choice of temperature and sampling methodology have the potential to distort information about the LLM's credences contained in the distribution of answers over multiple independent trials. We have suggested that the distor-tion issue can be overcome by using a temperature of 1 alongside top-p sampling where p = 1. Even so, while the distortion issue is resolvable, we lack a positive rationale for accepting an evidential relationship between the distribution of an-swers and the LLM's credences. We leave open the possibility that a rationale can be provided here, but in the meantime suggest caution in attributing LLMs credences based on the distribution of answers over multiple independent trials."}, {"title": "Syntax, Semantics, and Output Probabilities", "content": "We now argue that LLM credences cannot obviously be inferred from output probabilities. The problem is that output probabilities are indexed to tokens but credences are indexed to propositions (Kuhn et al., 2023). Hence it is not clear how we are supposed to infer the LLM's credences from output probabilities. We need a principle which specifies the evidential relationship between output probabilities and credences. What would such a principle look like? Consider,\nThe Simple Bridge Principle: The LLM has a credence cr(X) in X just in case the LLM assigns a probability cr(X) to the Yes token given an input prompt sx for the LLM to affirm or deny X.\nTo illustrate: Suppose the LLM is prompted with: Did Fincher direct Fight Club?. Then, on the simple bridge principle, the LLM's credence in the proposition that \u300cFincher directed Fight Club\u00b9 is equal to the probability that the LLM assigns to the Yes token conditional on the input Did Fincher direct Fight Club? (Kadavath et al., 2022; Tian et al., 2023). The rationale is that the LLM's credence in the affirmative Yes token is indicative of the LLM's degree of confidence in the proposition that Fincher directed Fight Club\u00b9.\nThe simple bridge principle fails. Given the input Did Fincher direct Fight Club?, the LLM can affirm the proposition using one of several se-mantically equivalent tokens. For example, the tokens Yes, yes, sure, and"}, {"title": "The Normalised Bridge Principle", "content": "The Normalised Bridge Principle: The LLM has a credence cr(X) in X just in case cr(X) is equal to the sum of the LLM's output probability for the Yes token and its semantic equivalents divided by the sum of the Yes and the No tokens and their semantic equivalents given an input prompt sx for the LLM to affirm or deny X.\n$cr(X) = \\frac{\\sum_{t \\in C_{yes}} p(t|s_x)}{\\sum_{t \\in C_{yes}} p(t|s_x) + \\sum_{t \\in C_{no}} p(t|s_x)}$\nThe normalised bridge principle fails. The rationale for this principle is that output probabilities for affirmative and denial tokens are evidentially relevant to the LLM's credences given their semantic content, but output probabilities as-signed to spurious tokens are evidentially irrelevant. But the affirmative, denial, and spurious tokens do not exhaust the LLM's vocabulary. We can distinguish between spurious tokens such as triangle and tokens which are neither affir-mations nor denials but which indicate a legitimate epistemic attitude towards the proposition. These include, inter alia, unlikely, probably, perhaps, and unsure. We cannot admit affirmative and denial tokens as evidentially rele-vant to the LLM's credences because of their semantic content, but exclude tokens whose semantic content reflects other epistemic attitudes as evidentially irrelevant despite their semantic content. Hence the normalised principle fails.\nThere may exist a more complex bridge principle that factors in output probabilities assigned to all tokens which correspond to legitimate epistemic attitudes. Yet such a principle would face the issue that output probabilities assigned to tokens and the epistemic attitudes denoted by the tokens give two separate scales for assessing the LLM's epistemic attitude and there is no obvious way to combine the two. It is at best unclear how to combine (for example) a 70% output probability for Yes and a 12% output probability for probably, among other pairs of output probabilities and legitimate epistemic attitudes, into a single scalar value representing the LLM's credence in a proposition.\nEven if some bridge principle allows us to factor in the output probabilities assigned to all tokens which correspond to legitimate epistemic attitudes along-side the attitudes denoted by the relevant tokens, a second problem remains. Output probabilities are not invariant under semantically equivalent formula-tions of the same question (c.f. Scherrer et al., 2024). LLMs can and do give different output probabilities conditional on semantically equivalent but syn-tactically distinct prompts such as Did Fincher direct Fight Club? and Is it true that Fincher directed Fight Club?. Any principle that links the LLM's credence in X to the LLM's output probabilities conditional on some input sequence sx for the LLM to affirm or deny X arbitrarily privileges one of several semantically equivalent prompts for the LLM to affirm or deny X. This issue motivates the need for an even more complex bridge principle that ac-counts for semantic equivalence of the input sequences in addition to the issues"}, {"title": "Interdependence", "content": "We considered three sources of evidence about LLM credences: reported confi-dence, consistency-based estimation, and output probabilities. We argued that reported confidence is an unreliable indicator of LLM credences, and that it is at best unclear how to infer LLM credences from output probabilities. We further argued that consistency-based estimation could in principle signal the LLM's credences given an appropriate choice of temperature and sampling method-ology, but that a positive mechanism is lacking which guarantees that signals about the LLM's credences reliably show up in the distribution of answers.\nWhile we have presented reported confidence, consistency-based estimation, and output probabilities separately, their limitations are interdependent. Hold-ing fixed LLM model weights, the input sequence and the temperature determine output probabilities. Output probabilities and sampling methods (such as top-p and top-k) determine the distribution from which token responses are sampled, and in turn the relative frequencies of different answers across multiple indepen-dent trials. Hence in all three cases endogenous signals of the LLM's credences are liable to distortion from exogenous factors such as the user's choice of tem-perature and sampling method. LLM scientists should avoid distorting factors when attributing credences to LLMs on the basis of any of these techniques."}, {"title": "Conclusion", "content": "Scientists often attribute credences to LLMs when evaluating LLM capabili-ties. In this paper, we asked three questions about LLM credence attribution. First, are LLM credence attributions literal ascriptions of degrees of belief or confidence to LLMs? Second, do LLMs have credences? Third, even if LLMs have credences, are the experimental techniques used to assess LLM credences reliable? Our analysis suggests that LLM credence attributions are (at least in general) intended by scientists as literal ascriptions of credences to LLMs. Evidence for the existence of LLM credences is at best inconclusive, and even if LLMs have credences, there are non-trivial questions about the reliability of the techniques used to evidence LLM credences - namely, reported confidence,"}]}