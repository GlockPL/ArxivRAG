{"title": "INSTANTSWAP: Fast Customized Concept Swapping across Sharp Shape Differences", "authors": ["Chenyang Zhu", "Chengyu Fang", "Kai Li", "Yue Ma", "Chubin Chen", "Qifeng Chen", "Longxiang Tang", "Xiu Li"], "abstract": "Recent advances in Customized Concept Swapping (CCS) enable a text-to-image model to swap a concept in the source image with a customized target concept. However, the existing methods still face the challenges of inconsistency and inefficiency. They struggle to maintain consistency in both the foreground and background during concept swapping, especially when the shape difference is large between objects. Additionally, they either require time-consuming training processes or involve redundant calculations during inference. To tackle these issues, we introduce INSTANTSWAP, a new CCS method that aims to handle sharp shape disparity at speed. Specifically, we first extract the bbox of the object in the source image automatically based on attention map analysis and leverage the bbox to achieve both foreground and background consistency. For background consistency, we remove the gradient outside the bbox during the swapping process so that the background is free from being modified. For foreground consistency, we employ a cross-attention mechanism to inject semantic information into both source and target concepts inside the box. This helps learn semantic-enhanced representations that encourage the swapping process to focus on the foreground objects. To improve swapping speed, we avoid computing gradients at each timestep but instead calculate them periodically to reduce the number of forward passes, which improves efficiency a lot with a little sacrifice on performance. Finally, we establish a benchmark dataset to facilitate comprehensive evaluation. Extensive evaluations demonstrate the superiority and versatility of INSTANTSWAP.", "sections": [{"title": "1. Introduction", "content": "We explore the task of Customized Concept Swapping (CCS), a subtask of text-to-image (T2I) generation, which aims to replace a concept in a source image with a highly customized new concept. Combined with diffusion models [7, 38, 43], recent CCS methods demonstrate widespread applicability in areas such as selfie enhance-"}, {"title": "2. Related work", "content": null}, {"title": "2.1. Diffusion-based Image Editing", "content": "Image Editing is a fundamental and popular topic in computer vision. Previous works based on Generative Adversarial Networks (GAN) [12] only focus on specific object domains, which limits the application. With the emergence of diffusion model [43], image editing is now able to modify various objects through prompts. These methods are mainly divided into five categories: instruction-based methods, blending-based, attention-based, inversion-based,"}, {"title": "2.2. Concept Swapping", "content": "Concept swapping, a subtask of general image editing, focuses on replacing the source concept in an image with a user-specified target concept. This task is first proposed by PbE [49], which employs a CLIP encoder to extract features of the target concept and inject them into the UNet through a cross-attention layer. After that, concurrent works [4, 13, 28, 47] extend concept swapping into the customization field [10, 54]. They combine attention-based editing methods [18, 46], with tuning-based customization methods [26, 44] to achieve customized concept swapping. Building on Photoswap, SwapAnything [14] further obtains masks with external modules to specify the locations of objects in the source image. We improved the existing method in three key aspects. Firstly, we employ bounding boxes instead of masks as spatial indicators of the source concept, allowing greater flexibility for shape variation during concept swapping. Secondly, we use bounding boxes to prevent background changes via gradient masking, thus ensuring background consistency. Third, we augment concept representation with semantic information to maintain foreground consistency. Finally, rather than executing forward passes at every timestep, we execute them only at specific intervals to enhance efficiency."}, {"title": "3. Method", "content": "Given a set of images, $X_t = \\{x\\}_{i=1}^{M_1}$ representing a specific concept $O_t$, along with an image $x_s$, and a prompt $p_s$ describing a source concept $O_s$, the objective of CCS is to \"seamlessly\" replace $O_s$ in $x_s$ with $O_t$ according to a target prompt $P_t$, resulting in a final target image $x_t$. An ideal customized concept swapping should handle the shape differences between source and target concepts to preserve swapping consistency while maintaining satisfactory efficiency. We introduce INSTANTSWAP to achieve this. INSTANTSWAP is based on Stable Diffusion and extends from the score distillation based image editing methods [19, 39]."}, {"title": "3.1. Preliminaries", "content": null}, {"title": "3.1.1 Stable Diffusion", "content": "In this paper, the foundational model utilized for text-to-image generation is Stable Diffusion [43]. It takes a text prompt $P$ as input and generates the corresponding image $x$. Stable Diffusion consists of three main components: an autoencoder($E(\u00b7), D(\u00b7)$), a CLIP text encoder $7(\u00b7)$ and a U-Net $E(\u00b7)$. Typically, it is trained with the guidance of the following reconstruction loss:\n$L_{rec} = E_{z, \\epsilon \\sim N(0,1), t, P} [|| \\epsilon - \\epsilon_\\theta (z_t, t, \\tau(P)) ||^2],$\nwhere $ \\epsilon \\sim N (0, 1)$ is a randomly sampled noise, t denotes the time step. The calculation of $z_t$ is given by $z_t = a_t z + \\sigma_t \\epsilon$, where the coefficients $a_t$ and $ \\sigma_t$ are provided by the noise scheduler."}, {"title": "3.1.2 Score Distillation Based Image Editing", "content": "Different from traditional attention-based image editing, score distillation based methods achieve image editing through iterative optimization with a score distillation loss. Given the latent feature z of source image and a denoising U-Net $E(\u00b7)$, SDS [39] can optimize the latent feature z of the image to align with the target prompt $P_t$ by employing the following loss:\n$L_{SDS} = || \\epsilon_\\theta (z_t, t, \\tau(P_t)) - \\epsilon ||^2,$\nwhere $ \\epsilon$ and t are randomly sampled noise and timestep. The resulting image SDS is very blurry and only contains foreground objects in the target prompt $P_t$. To address this issue, DDS [19] expresses the gradient of Eq. (2) as\n$\\nabla_z L_{SDS}(z_t, t, \\tau(P_t)) = \\delta_{tgt} + \\delta_{bias},$\nwhere $ \\delta_{tgt}$ indicates the direction aligned with the target prompt and $ \\delta_{bias}$ refers to undesired part that makes the image blurry. Based on this, DDS further utilizes the fixed latent $z_t$ of the source image and the source prompt $P_s$ to approximate the bias component in Eq. (3):\n$\\nabla_z L_{SDS}(z_t, t, \\tau(P_s)) \\approx \\hat{\\delta}_{bias} \\approx \\delta_{bias}.$\nFinally, DDS is represented by the difference of Eq. (3) and Eq. (4):\n$\\nabla_z L_{DDS} = \\nabla_z L_{SDS}(z_t, t, \\tau(P_t)) - \\nabla_z L_{SDS}(z_t, t, \\tau(P_s)) \\sim \\delta_{tgt}.$\nBased on Eq. (5), the loss of DDS is given by\n$L_{DDS} = || \\epsilon_\\theta (z_t, t, \\tau(P_t)) - \\hat{\\epsilon}_\\theta (\\hat{z}_t, t, \\tau(P_s)) ||^2.$"}, {"title": "3.2. INSTANTSWAP", "content": "Directly extending score distillation based editing methods to the task of CCS encounters the challenge of inconsistency. These methods optimize the background and foreground simultaneously, causing cross-interference and leading to undesirable inconsistency. To address these limitations, we first propose a strategy to automatically locate objects to be edited, resulting in the object bounding box (bbox). With this bbox, we propose a background gradient masking technique to remove gradients in the background region and confine swapping to the foreground region. To further enhance foreground swapping consistency, we propose to learn Semantic-enhanced concept representations for both source and target concepts based on an attention map feature injection mechanism. An overview of our method is presented in Fig. 3."}, {"title": "3.2.1 Automatic Bounding Box Generation", "content": "We first automatically obtain the bbox to indicate the position of the concept $O_s$ in the source image. Given the source image $x_s$ and the source prompt $P_s$, we perform a forward pass with the U-Net $E_\\theta(\u00b7)$ and obtain the cross-attention map A and self-attention map $A^s$ through:\n$A = Softmax(\\frac{QK^T}{\\sqrt{d'}}),$\nwhere $Q$ is the query vector projected from the image features, $d'$ represents the output dimension of key and query features. K is the key vector and V is the value vector. For cross-attention maps $A$, K and V are projected from the text embeddings $(P)$. For self-attention maps $A^s$, K, and V are projected from the image features. Directly applying a threshold on the $A^c$ can yield a coarse-grained mask, which cannot accurately reflect the location of $O_s$. Inspired by [37, 53], we modify the $A^c$ as follows:\n$\\hat{A}^c = A^s \\cdot (A^c)^\\alpha.$\nBased on Eq. (7), all values in $A^c$ range between 0 and 1. Therefore, element-wise exponentiation of $A^c$ by $ \\alpha$ can weaken the activation of non-target regions. Additionally, as mentioned in [29], $A^s$ contains rich structural information. This information can effectively assist $ \\hat{A}^c$ in better activating the target regions. Finally, we apply the threshold $ \\beta$ to $\\hat{A}^c$ to obtain the mask. Subsequently, we converted the mask into the bbox $B_s$ based on the minimum and maximum coordinates of all foreground points within the mask. This strategy allows us to obtain the bbox $B_s$ without any additional modules. We intentionally set a relatively loose constraint on the mask to obtain a bbox that completely covers the source concept. We discuss the effectiveness of our automatically obtained bboxes in Sec. 4.5."}, {"title": "3.2.2 Background Gradient Masking", "content": "With the object bbox, we propose a background gradient masking (BGM) approach to ensure that the concept swap is confined to the foreground region. Given the latent feature $z$ of the source image and the latent feature $ \\hat{z}$ of the target image, where $ \\hat{z}$ is initialized to z and is continuously optimized to obtain the final target image $x_t$. Based on Eq. (6), we first obtain the gradient of z:\n$\\nabla_z L = (\\epsilon_\\theta (z_t, t, \\tau(P_t)) - \\hat{\\epsilon}_\\theta (\\hat{z}_t, t, \\tau(P_s))) \\frac{\\partial \\epsilon_\\theta (z_t, t, \\tau(P_t))}{\\partial z_t} \\frac{\\partial z_t}{\\partial z}.$\nAs stated in [39], the mid term is a U-Net Jacobian term and can be omitted, and $a_t = \\frac{\\partial z_t}{\\partial z}$ is a constant which can be represented as w(t):\n$\\nabla_z L = w(t)(\\epsilon_\\theta (z_t, t, \\tau(P_t)) - \\hat{\\epsilon}_\\theta (\\hat{z}_t, t, \\tau(P_s)).$\nThis gradient shares the same dimension as z, which means it can update z in a pixel-wise manner. However, this will update the foreground and background simultaneously, producing inconsistent background. To remedy this, we apply the bbox $B_s$ on Eq. (10) to mask the gradients related to the background before back propagation and obtain our BGM:\n$\\nabla_z L_{BGM} = w(t)(\\epsilon_\\theta (z_t, t, \\tau(P_t)) - \\hat{\\epsilon}_\\theta (\\hat{z}_t, t, \\tau(P_s)) \\cdot B_s.$\nThis simple masking strategy prevents the background from being updated and thus ensures background consistency."}, {"title": "3.2.3 Semantic-enhanced Concept Representation", "content": "The BGM module maintains the background consistency during swapping. However, whether the source concept can be replaced with the target concept cannot be guaranteed. This limitation arises because the optimization of Eq. (11) is still carried out at the entire feature map level of both the source latent $z_t$ and target latent $ \\hat{z}_t$ without distinguishing between the foreground and the background. To address this, we propose to obtain Semantic-enhanced concept representations for both source and target concepts and emphasize their locations within the foreground region during concept swapping.\nLet $F_s$ be the source image feature and $p_s$ represent the prompt of source concept (e.g. \"rose\"), the semantic embedding $c_s$ can be acquired through $c_s = \\tau(p_s)$. We first resize the previously obtained object bbox to fit the dimensions of the source image feature $F_s$, resulting in the feature bbox $B_f$. We then crop $F_s$ with $B_f$ to get a regional image feature $f_s$. With $f_s$, we calculate the query vector through $Q_s = W_q \\cdot f_s$. After that, we can obtain the key and value vectors through:\n$K_s = W_k \\cdot c_s, V_s = W_v \\cdot c_s.$\nThen the final partial attention output is calculated as follows:\n$f_s^{'} = Softmax(\\frac{Q_s(K_s)^T}{\\sqrt{d'}}) \\cdot V_s,$\nwhere d' represents the output dimension of key and query features. In this way, we inject the semantic information of the source concept into the cross-attention map, resulting in regional concept representation $f_s^{'}$. We then map $f_s^{'}$ back to the original feature map $F_s$, to get a Semantic-enhanced representation $F_s^{'}$ for the entire source image. Fig. 4 illustrates the process. In the target branch, we first convert the target concept into semantic space with DreamBooth, using a specific rare token (e.g., \"sks\") to represent the concept. With the target prompt $p_t$ (e.g., \"sks teapot\") and the feature bbox $B_f$, we similarly apply this process for the target image feature $F_t$ and obtain the semantic-enhanced representation $F_t^{'}$ for the target image.\nThrough proactive injection of semantic guidance, we provide the source and target branches with Semantic-enhanced concept representation within the foreground region. Consequently, SECR transforms the target branch into a target concept adder and the source branch into a source concept remover. Their collaboration results in precise and seamless concept swapping, thus enhancing the foreground consistency. Moreover, SECR can also facilitate concept insertion and removal, which is further discussed in Sec. 4.6."}, {"title": "3.2.4 Step-skipping Gradient Updating", "content": "After addressing the problem of inconsistency, we turn our attention to the challenge of inefficiency. As illustrated in Fig. 5, previous methods calculate the gradient at each timestep. However, the success of DDIM [45] in accelerating DDPM [21] motivates us to consider: Can we skip the calculation of gradients at certain timesteps? During concept swapping, we observe that the effect of gradient updates on the target image is similar across adjacent timesteps (see detailed results in Supp.). Based on this observation, we propose the step-skipping Gradient Updating (SSGU) strategy. The key insight of SSGU is that skipping some gradient calculations does not significantly sacrifice the swapping consistency while considerably improving efficiency. As a result, our SSGU calculates gradients at interval timesteps and reuses the previously calculated gradients during the intervening timesteps.\nWe define our entire pipeline as F, given the source image $x_s$, the timestep t, and the intermediate target image $x_t$ at timestep t. We can obtain the gradient $g_t$ and the output intermediate target image $x_{t+1}$ at timestep t as follows:\n$g_t = F(x_s, x_t, t),$\n$x_{t+1} = x_t - \\eta g_t,$\nwhere $ \\eta$ is the learning rate. Our SSGU periodically retains some anchor gradients and skips the forward passes between two anchor gradients. The step-skipping period is controlled by the SSGU factor $ \\lambda$. The set of anchor gradients can be defined as:\n$G = \\{g_{k\\lambda}\\}, k = 0,1\u00b7\u00b7\u00b7, [T/\\lambda],$\nwhere T is the ended timestep. For any intervening timestep t, we use its nearest former anchor gradient to update the intermediate target image $x_t$. Taking $ \\lambda = 2$ as an example, we assume that t is an even number and $g_{t-2}, g_t \\in G$. SSGU updates $x_{t-2}$ and $x_{t-1}$ with the anchor gradient $g_{t-2}$:\n$g_{t-2} = F(x_s, x_{t-2}, t - 2),$\n$x_{t-1} = x_{t-2} - \\eta g_{t-2},$\n$x_t = x_{t-1} - \\eta g_{t-2}.$\nFor the next timestep t, another anchor gradient $g_t$ is used to update $x_t$. As a result, our SSGU reduces the number of forward passes during the entire concept swapping process to 1/$ \\lambda$ of the original count. Since the forward pass accounts for approximately 95% of the total inference time (see detailed analysis in Supp.), our SSGU can improve the overall inference speed of our method by approximately $ \\lambda$ times, with minimal effect on swapping consistency (see Sec. 4.5). Furthermore, our SSGU can be transferred to other score distillation based methods to improve their efficiency in the same way, which is further discussed in Sec. 4.6."}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Implementation Details", "content": "We conduct the experiments with Stable Diffusion [43] v2.1-base on a single RTX3090. We use the customized checkpoint from DreamBooth [44] to introduce concepts. We set the SSGU factor $ \\lambda$ to 5, $ \\alpha$ to 2, $ \\beta$ to 0.5 and the guidance scale to 7.5. The bbox is obtained through the first three steps. Subsequently, we use SGD [42] with a learning rate of 0.1 to optimize for 550 steps of iterations."}, {"title": "4.2. ConSwapBench", "content": "Despite the significant application potential of customized concept swapping, there is currently no dedicated evaluation benchmark. To meet the needs of comprehensive evaluation, we introduce ConSwapBench, the first benchmark dataset specifically designed for customized concept swapping. ConSwapBench consists of two sub-benchmarks: ConceptBench and SwapBench. ConceptBench comprises 62 images covering 10 different target concepts used for customization, while SwapBench includes 160 real images containing one or more objects to be swapped, serving as source images. For each image in SwapBench, we use Grounding SAM [41] to acquire the bbox of the foreground concepts as the ground truth for evaluation purposes. We apply each customized concept from ConceptBench to perform concept swaps on each image in SwapBench, ultimately generating a total of 1,600 images for evaluation. More details can be found in Supp."}, {"title": "4.3. Qualitative Comparison", "content": "Since customized concept swapping is a relatively novel task, there are limited methods available for direct comparison. Consequently, we include SOTA image editing methods and adapt them for customized concept swapping. We include the following methods: (1) Score distillation based methods: SDS [39], DDS [19], and CDS [36]; (2) Attention-based methods: PhotoSwap [13], PnPInv [24], and P2P [18]. We excluded SwapAnything [14] as it is not publicly available. The qualitative results are illustrated in Fig. 6. We find that score distillation based methods can accommodate shape variations during concept swapping. However, they exhibit poor foreground fidelity (3rd and 6th rows) and lead to unnecessary modifications on the background (1st and 2nd rows). Attention-based methods are unable to manage shape variations (4th row) and also struggle with maintaining background consistency (5th row). In contrast, our method demonstrates superior performance in addressing shape variations and maintaining swapping consistency."}, {"title": "4.4. Quantitative Comparison", "content": "We also conduct a thorough quantitative comparison on ConSwapBench. For each generated image, we first use the ground truth bbox in SwapBench to obtain their foreground and background respectively. We use seven different metrics to evaluate the methods from three aspects: (1) Foreground consistency: We calculate the CLIP Image Score [40] between the foreground of generated images and the images of customized concepts. (2) Background consistency: We use the four metrics, PSNR, LPIPS [51], MSE, SSIM [48] to evaluate the background consistency. (3) Overall consistency and efficiency: We calculate the CLIP Text Score [20] between generated images and target prompts to evaluate the overall prompt consistency. We also report the inference time of each method to evaluate their efficiency. As shown in Tab. 1, our method outperforms other methods on all seven metrics."}, {"title": "4.5. Ablation Study", "content": "BGM. To verify the effectiveness of BGM in background preservation, we conduct an ablation study by removing BGM. As illustrated in the second column of Fig. 7, while our method can still achieve concept swapping without BGM, it causes serious modifications on the background. In contrast, our full method not only maintains high foreground fidelity but also effectively preserves the background consistency. We further conduct a quantitative analysis of the background consistency and prompt consistency, as shown in Tab. 2. Our full method outperforms in all metrics.\nAutomatic bounding box detection mechanism. We further verify the effectiveness of our boxes by using ground truth (GT) bboxes from SwapBench to replace the automatically obtained bboxes As shown in Fig. 7, although GT bboxes accurately indicate the location of the source concept, they prevent our method from fully swapping the source concept. Compared to GT bboxes, our bboxes are relatively larger and can fully cover the source concept, thereby facilitating complete concept swapping. We also provide quantitative comparisons of different bboxes. As shown in Tab. 3, Gen stands for the generation bboxes, while Eva stands for the evaluation bboxes. The two types of bboxes do not significantly affect background preservation in our method, whereas our bboxes perform better than GT bboxes on the foreground metric. More detailed comparisons can be found in Supp..\nSECR. To verify the effectiveness of SECR, we conduct ablation studies including removing SECR from (1) source branch (w/o source), (2) target branch (w/o target), (3) both (w/o source & target). The visualization results are illustrated in columns 3 to 5 of Fig. 7. Although all methods preserve the background well, they show reduced foreground fidelity. Additionally, we perform a quantitative analysis of their foreground consistency and prompt consistency. The results presented in Tab. 4 indicate that our full method exhibits superior performance."}, {"title": "SSGU.", "content": "To verify the effectiveness of our proposed SSGU, we first visualize the images generated under different SSGU factors. As shown in Fig. 8, $ \\lambda = 1$ indicates that SSGU is not used. When $ \\lambda \\le 9$, the SSGU can preserve foreground and background consistency well while improving the efficiency of our method. As $ \\lambda$ increases, the images exhibit more artifacts due to excessive neglect of gradients. Therefore, identifying an optimal SSGU factor $ \\lambda$ is crucial. We further conduct a detailed quantitative analysis of different $ \\lambda$ values on foreground consistency and efficiency (see complete results in Supp.), as illustrated in Fig. 11, where the x-axis represents different $ \\lambda$ values and the y-axis represents the respective metric outcomes. When SSGU is not used, our method achieves the best swapping consistency but the lowest efficiency. As $ \\lambda$ increases, our SSGU sacrifices certain swapping consistency but significantly improves efficiency. To balance consistency and efficiency, we ultimately select $ \\lambda = 5$ for our final model."}, {"title": "4.6. Applications of INSTANTSWAP", "content": "Multi-concept swapping. Our INSTANTSWAP can be easily extended to facilitate multi-concept swaps by sequentially performing multiple single-concept swaps. As shown in the left of Fig. 10, our method can swap each concept within the image with both foreground and background consistency.\nHuman face swapping. INSTANTSWAP demonstrates exceptional capabilities in human face swapping. As shown in the middle of Fig. 10, with customized face models from CivitAI [5], users can seamlessly replace the face in a source image with a customized target face.\nConcept insertion and removal. In addition to concept swapping, our method also supports concept insertion and removal. For concept insertion, we employ the same procedure as concept swapping. For concept removal, we adjust the target prompt and the target semantic input $p_t$ of SECR to a null prompt. The results in the right of Fig. 10 further demonstrate the versatility of our method.\nAccelerating other methods. SSGU can be transferred to other score distillation based methods to enhance their efficiency. We select three representative methods: SDS [39], DDS [19] for image editing, and CoSD [25] for video editing. As shown in Fig. 9, combining these methods with SSGU can significantly improve their efficiency while almost not altering the generation quality. We further conduct a quantitative analysis to assess the transferability of the proposed SSGU, as presented in Tab. 5."}, {"title": "5. Conclusion", "content": "This paper introduces INSTANTSWAP, a novel framework for precise and efficient customized concept swap. Our BGM and SECR collaborate to maintain both background and foreground consistency. Furthermore, we propose the SSGU to eliminate redundant computation and improve efficiency. Finally, we introduce ConSwapBench, a comprehensive benchmark dataset for customized concept swapping. The impressive performance of INSTANTSWAP demonstrates its effectiveness. We hope our INSTANTSWAP can inspire future research, particularly in efficiently managing concept swapping with obvious shape variance. Future work could focus on (1) extending image-based customized concept swapping to the video domain; (2) enhancing the images of target concepts with low-level methods [9, 16, 17]; and (3) achieving more lightweight and precise concept swapping."}]}