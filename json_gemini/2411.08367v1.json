{"title": "Surprisingly Popular Voting for Concentric Rank-Order Models", "authors": ["Hadi Hosseini", "Debmalya Mandal", "Amrit Puhan"], "abstract": "An important problem on social information sites is the recovery of ground truth from individual reports when the experts are in the minority. The wisdom of the crowd, i.e. the collective opinion of a group of individuals fails in such a scenario. However, the surprisingly popular (SP) algorithm [PSM17] can recover the ground truth even when the experts are in the minority, by asking the individuals to report additional prediction reports-their beliefs about the reports of others. Several recent works have extended the surprisingly popular algorithm to an equivalent voting rule (SP-voting) to recover the ground truth ranking over a set of m alternatives. However, we are yet to fully understand when SP-voting can recover the ground truth ranking, and if so, how many samples (votes and predictions) it needs. We answer this question by proposing two rank-order models and analyzing the sample complexity of SP-voting under these models. In particular, we propose concentric mixtures of Mallows and Plackett-Luce models with G(\u2265 2) groups. Our models generalize previously proposed concentric mixtures of Mallows models with 2 groups, and we highlight the importance of G > 2 groups by identifying three distinct groups (expert, intermediate, and non-expert) from existing datasets. Next, we provide conditions on the parame-\nters of the underlying models so that SP-voting can recover ground-truth rankings with high probability, and also derive sample complexities under the same. We complement the theoretical results by evaluating SP-voting on simulated and real datasets.", "sections": [{"title": "Introduction", "content": "The recovery of ground truth from individual reports is one of the most vital aspects of social information sharing and online discourse. The wisdom of the crowds phenomenon refers to the observation that the collective value of a group of noisy individual opinions can be used to recover the ground truth [GAL07]. Such a collective value cancels out the biases of individual opinions when the number of participants is large and is often deployed to recover the ground truth on online polling and Q&A platforms (e.g. Reddit).\nHowever, when the experts are in the minority, approaches that rely on the collective opinion of a group of indi-viduals fail to recover the ground truth. The Surprisingly Popular (SP) algorithm [PSM17] is a promising technique capable of recovering the ground truth even when experts are in the minority. In addition to asking individuals' opin-ion (aka vote), it asks them to predict how they believe the majority's answer is (aka prediction). The SP algorithm then picks the outcome which is surprisingly popular i.e. whose actual frequency in the votes is greater than its average predicted frequency. It provably recovers the ground truth as the number of individuals grows, even with a minority of experts.\nThis approach has been extended to voting rules, called SP-voting, in order to recover the ground truth rankings over a set of m alternatives. The naive application of SP-algorithm to voting requires that individuals submit their"}, {"title": "1.1 Our Contributions", "content": "We propose various rank-order models with a ground truth ranking, and analyse the SP-voting rule under these models. In particular, our contributions are the following.\n\u2022 We propose two rank-order models, the Concentric Mixture of Mallows and the Concentric Mixture of Plackett-Luce, and generalize them to accommodate populations of G > 2 groups.\n\u2022 We derive the conditions required for the identification of ground truth ranking under the SP-voting and the proposed concentric rank-order models. The derived conditions highlight a tension between the fraction of different groups and the \"expertise\" (i.e. noise levels) of different groups.\n\u2022 To evaluate practical viability, we fit these models to real-world datasets for populations with G = 2 and G = 3 groups. When G = 3, besides the expert and non-expert groups, we identify an intermediate group of voters of large fraction that explains the observed datasets better than prior approaches with two groups.\n\u2022 Furthermore, we generate synthetic data based on these models and provide empirical results on the sample complexity of SP-Voting, comparing it against the Copeland rule. Finally, experiments on real-world datasets show that SP-voting performs significantly better than the Copeland voting rule even when the dataset size is small."}, {"title": "1.2 Related Work", "content": "The challenge of ground truth recovery using the wisdom of the crowd has been extensively explored in social choice theory [GAL07; De 14; Sur05]. Several vote aggregation rules [De 14; Bor81; Cop51; You77] have been proposed based on this concept to aggregate voters' preferences and recover the underlying ground truth. However, this ap-proach falters when the majority of participants are misinformed [Sim+11], biased [CFH04], or when expert opinions are underrepresented within the population [PSM17]. To address this limitation, Prelec et al. [PSM17] introduced the Surprisingly Popular (SP) algorithm, which requires voters to provide two types of information: their individual vote and their prediction of the consensus vote. This framework has since been used to incentivize truthful behaviour in agents [ST21; SY23], mitigate biases in academic peer review [LK24], elicit expert knowledge [KS18], forecast geopolitical events [DGD20], and aggregate information [CMP23]. However, Prelec et al. [PSM17]'s SP algorithm"}, {"title": "2 Model", "content": "Here we formally introduce the setting and the necessary notations. We will first introduce surprisingly popular vot-ing considering reports over full rankings, and then cover the setting with partial rankings. Let A = {a1, a2, ..., am } be the set of m possible alternatives. The set L(A) represents all possible complete rankings over the alternatives. Let \\(\\sigma\\)\u2208 L(A) represent a complete ranking of the m possible alternatives. We assume that there is a true ranking by \\(\\sigma^*\\) \u2208 L(A); which is drawn from a prior P(\u00b7) over L(A). Voter i observes a ranking \\(o_i\\) that is assumed to be a noisy version of the ground truth ranking \\(\\sigma^*\\). We will write \\(Pr_s(o_i | \\sigma^*)\\) to denote the probability that the voter i observes her ranking \\(o_i\\) given the ground truth ranking \\(\\sigma^*\\).\nGiven voter i's ranking \\(o_i\\) and the prior P(\u00b7), voter i can compute the posterior distribution over the ground truth using the Bayes rule.\n\n\n\n\n\n\\(Pr_g(\\sigma^* | o_i) = \\frac{Pr_s(o_i | \\sigma^*)\\cdot P(\\sigma^*)}{{\\Sigma}_{\\sigma' \\in L(A)} Pr_s(o_i | \\sigma')\\cdot P(\\sigma')}\\)\n\n\n\n\n\n(1)\nUsing the posterior over the ground truth, voter i can also compute a distribution over the rankings observed by another voter.\n\n\n\n\n\n\\(Pr_o(o_j | o_i) = {\\Sigma}_{{\\sigma'} \\in L(A)} Pr_s(o_j | \\sigma'). Pr_g(\\sigma' | o_i)\\)\n\n\n\n\n\n(2)\nThe surprisingly popular algorithm asks voters to report their votes, and posterior over others' votes. For each ranking \\(\\sigma'\\), it then computes the frequency \\(f(\\sigma') = \\frac{1}{|{\\iota : o_i = \\sigma'}|} {\\Sigma}_i 1[o_i = \\sigma']\\), and posterior\n\n\n\n\n\n\\(h(\\sigma | \\sigma') = \\frac{1}{|{\\iota : o_i = \\sigma'}|} {\\Sigma}_{i:o_i=\\sigma'} Pr_o(\\sigma_i o_i),\\)"}, {"title": "3 Concentric Mixtures Models", "content": "Concentric Mixture Models are a class of probabilistic models used to represent how different groups within a pop-ulation rank a set of alternatives, all relative to a single underlying ground truth ranking. These models capture variations in group behavior by incorporating parameters that reflect the degree and nature of each group's deviation from this central ranking. Our main goal in this section is to analyze the performance of SP-voting under different concentric mixture models, by first identifying the conditions required to identify the ground truth, and then provid-ing upper bounds on the sample complexity of SP-voting. We begin with the Concentric Mixture of Mallows Model in Section 3.1, followed by the Concentric Mixture of Plackett-Luce Model in Section 3.2, which is a new model proposed in this work."}, {"title": "3.1 The Concentric Mixture of Mallows Model", "content": "The Concentric Mixture of Mallows Model (CMM) [CI21] uses a distance-based approach to quantify deviations from the central ranking. Specifically, group g's ranking is modeled as a Mallows model with a group-specific dispersion parameter \\(\\phi_g\\), which controls the degree of expertise of the group. The following equation describes the ranking observed by a voter where the voting population has G distinct groups:\n\n\n\n\n\n\\(Pr_s(\\sigma | \\sigma^*) = {\\Sigma}_{g=1}^G p_g \\cdot Pr_s(\\sigma | \\sigma^*, \\phi_g)\\)\n\n\n\n\n\n(6)\nHere \\(\\sigma^*\\) is the underlying ground-truth ranking, and \\(Pr_s(\\sigma | \\sigma^*,\\phi_g)\\) is the probability of a voter observing the ranking \\(\\sigma\\) given the ground-truth ranking \\(\\sigma^*\\) and the dispersion parameter \\(\\phi_g\\) for group g. The parameter \\(p_g\\) represents the probability of voter i belonging to group g, where \\({\\Sigma}_{g=1}^G p_g = 1\\). In the Concentric Mixture of Mallows model, the probability \\(Pr_s(\\sigma | \\sigma^*, \\phi_g)\\) is defined as:\n\n\n\n\n\n\\(Pr_s(\\sigma | \\sigma^*, \\phi_g) = \\frac{\\phi_g^{d(\\sigma,\\sigma^*)}}{Z(\\phi_g,m)}\\)\n\n\n\n\n\n(7)\nwhere \\(d(\\sigma, \\sigma^*)\\) is the Kendall-Tau distance between the observed ranking \\(\\sigma\\) and the central ranking \\(\\sigma^*\\), and \\(Z(\\phi_g, m)\\) is the normalization constant that ensures that the probabilities sum to 1 across all possible rankings. We will assume that \\(\\phi_1 \\le \\phi_2 < ... < \\phi_G\\). Note that, a smaller value of the dispersion parameter implies that the group is more expert i.e. likely to observe a ranking closer to the ground truth ranking.\nFor the case of two groups (i.e. G = 2), Collas et al. [CI21] analyzed the identifiability and sample complexity of the concentric mixture model under the Borda voting rule. Our first goal is to analyze the same model under the SP-Voting rule and an arbitrary number of groups. There are two main steps in the analysis of SP-Voting\n1. Identification: determine the condition needed to ensure\n\n\n\n\n\n\\(V(\\sigma^*) \\ge 2. {max}_{\\tau:d(\\tau,\\sigma^*)\\ge 1} V(\\tau),\\)\n\n\n\n\n\nso that maximizing prediction-normalized-vote returns the ground truth.\n2. Sample Complexity: when the identification condition holds, determine the number of samples necessary to ensure\n\n\n\n\n\n\\(V(\\sigma^*) > {max}_{\\tau:d(\\tau,\\sigma^*)\\ge 1} V(\\tau),\\)\n\n\n\n\n\nso that maximizing the prediction-normalized votes from samples returns the ground truth.\nFor the setting of G = 2, the following result regarding identifying the CMM model has already been proved [HMP24].\nLemma 1 (Hosseini et al. [HMP24]). Suppose \\(p_1 < 1/2\\) and the following condition holds.\n\n\n\n\n\n\\((\\frac{p_1}{1-p_1})^2 \\ge 2 . (\\frac{Z(\\phi_2)}{Z(\\phi_1)})^{\\frac{3m(m-1)}{2\\phi_1}}\\)\n\n\n\n\n\nThen for any \u03c4 with d(\u03c4, \u03c3*) \u2265 1 we have V(\u03c3*) \u2265 2V(\u03c4).\nThe above result says that if the non-experts are too noisy (i.e. \\(\\phi_2 \\gg \\phi_1\\)) then the fraction of experts \\(p_1\\) cannot be too small. Next we generalize the lemma for the case of arbitrary number of groups.\nLemma 2. Suppose the set G can be partitioned into sets \\(G_1 = {1,2,...,s}\\) and \\(G_2 = {s +1,...,G}\\). Let \\(\\alpha = {\\Sigma}_{j \\in G_1} p_j\\) and the following condition holds.\n\n\n\n\n\n\\(\\frac{\\alpha}{1-\\alpha} (\\frac{\\phi_s}{\\phi_1}) + \\frac{1-\\alpha}{Z(\\phi_{s+1})} \\ge 2 (\\frac{\\phi_s}{Z(\\phi_1)} \\alpha + (\\frac{\\phi_G}{Z(\\phi_{s+1})})(1-\\alpha))\\)\n\n\n\n\n\nThen we are guaranteed that V(\u03c3*) \u2265 2V(r) for any \u03c4 such that d(\u03c4, \u03c3*) \u2265 1.\nThe proof is provided in the appendix where we generalize lemma 1 and also simplify the conditions required for identification. One way to interpret the result is that when the experts are in the minority i.e. \\(\\alpha \\ll 1/2\\) then we need \\(Z(\\phi_{s+1}) > 2\\phi_GZ(\\phi_G)\\) i.e. the dispersion parameter of the best non-expert should be sufficiently large. In the next subsection, we derive identifiability results under a different concentric mixture model, and then later provide sample complexity of SP-Voting under different rank-order models."}, {"title": "3.2 The Concentric Mixture of Plackett-Luce Model", "content": "In this subsection, we introduce the Concentric Mixture of Plackett-Luce Model (CMPL), which uses an element-specific probabilistic framework to rank alternatives based on their relative probabilities within each group. Specifi-cally, group g's ranking is modelled as a Plackett-Luce model with a group-specific parameter vector \\(\\theta_g \\in \\mathbb{R}^m\\). As before, the following equation describes the ranking observed by a voter, where the voting population is divided into G distinct groups:\n\n\n\n\n\n\\(Pr_s(\\sigma | \\sigma^*, \\theta) = {\\Sigma}_{g=1}^G p_g \\cdot Pr_s(\\sigma | \\sigma^*, \\theta_g)\\)\n\n\n\n\n\n(8)\nHere \\(\\sigma^*\\) is the ground-truth ranking, and \\(\\theta_g\\) is the vector of strength parameters for group g. The parameter \\(p_g\\) represents the probability that voter i belongs to group g, where the mixture weights satisfy the constraint \\({\\Sigma}_{g=1}^G p_g = 1\\). In the Concentric mixture of Plackett-Luce model, the probability \\(Pr_s(\\sigma | \\sigma^*, \\theta_g)\\) is defined as:\n\n\n\n\n\n\\(Pr(\\sigma | \\sigma^*, \\theta_g) = {\\Pi}_{j=1}^m \\frac{\\theta_{g,\\sigma^{*-1}(\\sigma(j))}}{{\\Sigma}_{l=j}^m {\\theta}_{g,\\sigma^{*-1}(\\sigma(l))}}\\)\n\n\n\n\n\n(9)\nHere, \\(\\sigma(j)\\) denotes the alternative assigned to the j-th position in the ranking \\(\\sigma\\), while \\(\\sigma^{*-1} (\\sigma(j))\\) denotes the position of the alternative \\(\\sigma(j)\\) in the ranking \\(\\sigma^*\\). Equation (9) describes a Plackett-Luce model with ground truth \\(\\sigma^*\\) and strength parameter vector \\(\\theta_g\\), as \\(\\theta_{g,\\sigma^{*-1}(\\sigma(j))}\\) represents the strength parameter for that alternative within group g, and, the denominator, \\({\\Sigma}_{l=j}^m {\\theta}_{g,\\sigma^{*-1}(\\sigma(l))}\\), ensures that the probability of selecting each alternative is normalized, considering only the alternatives that remain to be ranked."}, {"title": "3.2.1 Constraints on Strength Parameters", "content": "Recall that in the concentric mixture of Mallows model the groups were ranked according to their dispersion pa-rameters, i.e. \\(\\phi_{g_1} \\le \\phi_{g_2}\\) implies that group \\(g_1\\) is more expert compared to the group \\(g_2\\). We now impose a similar condition on the parameters of the concentric mixture of Plackett-Luce model.\nThe strength parameters \\(\\theta_g\\) for each group are subject to two key constraints:\n\u2022 Within-group constraint: For each group g, the sum of the strength parameters equals 1, ensuring that the sum of the parameters is identical across the G groups.\n\n\n\n\n\n\\({\\Sigma}_{j=1}^m {\\theta}_{g,j} = 1 \\forall g \\in \\{1,...,G\\}.\\)\n\n\n\n\n\nAdditionally, the entries in \\(\\theta_g\\) are non-increasing i.e. \\(\\theta_{g,i} \\ge \\theta_{g,j}\\) for \\(i \\ge j\\).\n\u2022 Between-group constraint: The strength parameters for the higher-expertise group should stochastically dominate those of the lower-expertise groups. In particular, for any location l the following condition must hold.\n\n\n\n\n\n\\({\\Sigma}_{j=1}^l {\\theta}_{1,j} \\ge {\\Sigma}_{j=1}^l {\\theta}_{2,j} \\ge ... \\ge {\\Sigma}_{j=1}^l {\\theta}_{G,j} \\forall l \\in \\{1,...,m\\}.\\)\n\n\n\n\n\nThis hierarchical constraint ensures that the behavior of the groups is ordered in a way that reflects their relative strengths, with group 1 being closest to the ground-truth ranking, and subsequent groups deviating further from it."}, {"title": "Lemma 3.", "content": "Suppose \\(p_1 \\le 1/2\\) and the following condition holds.\n\n\n\n\n\n\\((\\frac{p_1}{1-p_1})^2 \\ge 2. {(\\Pi}_{j=1}^m {\\frac{{\\Sigma}_{i=j}^m {\\theta}_{2,i}}{{\\Sigma}_{i=j}^m {\\theta}_{1,i}}})^{-1} ( {\\Pi}_{j=1}^m {\\frac{{\\theta}_{1,m-j+1}}{{\\theta}_{2,m-j+1}}})\\)\n\n\n\n\n\nThen for any ranking \u03c4 with d(\u03c4, \u03c3*) \u2265 1 we are guaranteed that V(\u03c3*) \u2265 2V(\u03c4).\nIn order to interpret the condition, let us choose a simple setting of strength parameters. Let \\(\\theta_1 = (\\frac{1}{\\gamma_1}, \\frac{1}{\\gamma_1},..., \\frac{1}{\\gamma_1})/(\\frac{1}{\\gamma_1} + m - 1)\\) and similarly \\(\\theta_2 = (\\frac{1}{\\gamma_2}, \\frac{1}{\\gamma_2},..., \\frac{1}{\\gamma_2})/(\\frac{1}{\\gamma_2} + m \u2212 1)\\). Then it can be verified that condition of Lemma 3 simplifies to the following,\n\n\n\n\n\n\\((\\frac{p_1}{1-p_1})^2 \\ge 2 \\frac{\\frac{\\gamma_2}{\\gamma_2+m-1}}{\\frac{\\gamma_1}{\\gamma_1+m-1}} {\\Pi}_{j=1}^{m-1} {\\frac{1}{\\frac{\\gamma_1}{\\gamma_1 + m - j}}}\\)\n\n\n\n\n\nand for large enough m we need \\(\\frac{p_1}{1-p_1} \\ge 2 \\sqrt{\\frac{\\gamma_2}{\\gamma_1}} m^{-(m-1)/2}\\). This means that as \\(\\frac{\\gamma_2}{\\gamma_1}\\) approaches \\(\\gamma\\) (i.e. non-experts become close to experts), we need a larger value of \\(p_1\\) (i.e. fraction of experts) to succeed. The next lemma generalizes the identifiability condition to an arbitrary number of groups.\nLemma 4. Suppose the set G can be partitioned into sets \\(G_1 = {1,2,...,s}\\) and \\(G_2 = {s +1,...,G}\\). Let \\(\\alpha = {\\Sigma}_{j \\in G_1} p_j\\) and the following condition holds.\n\n\n\n\n\n\\(\\alpha {\\Pi}_{j=1}^m {\\frac{{\\theta}_{s,j}}{{\\Sigma}_{i=j}^m {\\theta}_{s,i}}} + (1 - \\alpha) {\\Pi}_{j=1}^m {\\frac{{\\theta}_{G,j}}{{\\Sigma}_{i=j}^m {\\theta}_{G,i}}}\\\\2 \\frac{\\alpha {\\Pi}_{j=1}^m {\\frac{{\\theta}_{1,m-j+1}}{{\\Sigma}_{i=j}^m {\\theta}_{1,m-i+1}}} + (1 - \\alpha) {\\Pi}_{j=1}^m {\\frac{{\\theta}_{s+1,m-j+1}}{{\\Sigma}_{i=j}^m {\\theta}_{s+1,m-i+1}}}}{\\alpha {\\Pi}_{j=1}^m {\\frac{{\\theta}_{s,j}}{{\\Sigma}_{i=j}^m {\\theta}_{s,i}}} + (1 - \\alpha) {\\Pi}_{j=1}^m {\\frac{{\\theta}_{G,j}}{{\\Sigma}_{i=j}^m {\\theta}_{G,i}}}}\\)\n\n\n\n\n\nThen for any ranking \u03c4 with d(\u03c4, \u03c3*) \u2265 1 we are guaranteed that V(\u03c3*) \u2265 2V(\u03c4)."}, {"title": "3.3 Sample Complexity Bounds", "content": "Once we have derived the identifiability conditions, the derivation of sample complexity is relatively straightforward. When the number of samples is large, the empirical prediction-normalized vote \\(V(\\tilde{\\sigma})\\) concentrates around V(\u03c3) with high probability, and the condition V(\u03c3*) \u2265 2V(\u03c4) guarantees that we can always ensure \\(V(\\tilde{\\sigma^*}) \\ge V(\\tilde{\\tau})\\) for any \u03c4 with d(\u03c4, \u03c3*) \u2265 1. Therefore, picking a ranking that maximizes the empirical prediction-normalized votes returns the ground truth ranking. The next lemma states the sample complexity for the CMM model.\nLemma 5. Under the same setting as Lemma 2, suppose the number of samples is \\(n \\ge 0 (m!/m log(m/\\delta))\\). Then SP-voting recovers the ground truth ranking with probability at least 1 \u2013 \u03b4."}, {"title": "4 Experiments", "content": "In this section, we describe how we infer the parameters for both the CMM and the CMPL using a real-world dataset."}, {"title": "4.1 Concentric Mixture of Mallows", "content": "We fit the CMM with G = 2 and 3 groups to the dataset described earlier in this section. Below we describe the parameter inference procedure for G = 3 groups, the more general case. The three groups are categorized as experts, intermediates, and non-experts. We infer several key parameters, including the proportion of each group (pk), the dispersion parameters for experts' votes (\\(\\phi_{E-votes}\\)) and predictions (\\(\\phi_{E-predictions}\\)), the dispersion parameters for intermediates' votes (\\(\\phi_{I-votes}\\)) and predictions (\\(\\phi_{I-predictions}\\)), and the dispersion parameters for non-experts' votes (\\(\\phi_{NE-votes}\\)) and predictions (\\(\\phi_{NE-predictions}\\))."}, {"title": "4.2 Concentric Mixture of Plackett-Luce", "content": "We fit the CMPL with G = 2 and 3 groups. For G = 3, the groups are labeled as experts, intermediates, and non-experts. Similar to the CMM model, we infer the proportion of each group (pk). Additionally, we infer the strength parameters for experts' votes (\\(\\theta_{E-votes}\\)) and predictions (\\(\\theta_{E-predictions}\\)), intermediates' votes (\\(\\theta_{I-votes}\\)) and predictions (\\(\\theta_{I-predictions}\\)), and non-experts' votes (\\(\\theta_{NE-votes}\\)) and predictions (\\(\\theta_{NE-predictions}\\)). We use the Inference Method described earlier in this section, utilizing the No-U-Turn Sampler (NUTS) to explore the parameter space and infer posterior distributions for the model parameters.\nBefore sampling, the rankings provided by participants (both votes and predictions) are converted into indices, which correspond to the options being ranked. The strength parameters, which reflect the relative probability of ranking an alternative higher than the others within a group, are inferred separately for experts, intermediates, and non-experts. The model's priors for the group proportions and the strength parameters are defined as follows:"}, {"title": "4.3 Predicting Complete Rankings from Partial Rankings using CMM and CMPL", "content": "We predict the complete ranking of 36 alternatives from partial rankings, for each population group (experts, inter-mediates, and non-experts) using the CMM and CMPL models. The dataset containing 36 alternatives is divided into 12 subsets, each containing 5 alternatives and we collect vote information over these subsets.\nIn both models, we use a hierarchical approach. We first fit each model to the subsets independently, learning the parameters for the alternatives within each subset. Since some alternatives appear in multiple subsets, this creates"}, {"title": "5 Sample Complexity Results", "content": "In this section, we analyze the impact of sample size on ground truth recovery by generating synthetic data using the CMM and CMPL models with G = 3. We generate 500 samples with the proportion of experts in the population being 1%. Figure 4 present a comparison of how sample size affects the performance of two aggregation methods: Copeland Rule [Cop51] and SP-Voting. Figure 5 shows the same comparison on real data. Refer to Figure 6 in"}, {"title": "6 Discussion and Future Work", "content": "In this work, we have analyzed SP-voting under two concentric rank-order models (Mallows and Plackett-Luce) with an arbitrary number of groups. We observed that real-world datasets often have multiple groups of experts (G \u2265 3) and SP-voting performs better in terms of sample complexity when compared to standard voting rules. There are many interesting directions for future work. First, Prelec et al. [PSM17] have proposed the self-predicting property for the general SP algorithms. Although this condition is not sufficient to derive finite sample complexity bounds, it would be interesting to see how it compares with the conditions we derived for various concentric rank-order models.\nSecond, we have seen that moving from G = 2 to G = 3 groups gives a significantly better fit (and explanation) with respect to the real data but the improvement is marginal for larger values of G. Then a natural question is can we choose the number of groups G in a a data-dependent way? Finally, in terms of sample complexity, we have analyzed SP-voting for recovering ground truth ranking over m alternatives, and the bound grows with m!. This can be reduced to O(m\u00b2) for the pairwise version of SP-voting considered in prior work [Hos+21] with additional assumptions. However, when the number of alternatives m is large, we want the sample complexity to be independent of m. SP-voting with partial preferences [HMP24] help in such contexts, and we leave a fine-grained analysis of the partial variants of SP (under various concentric rank-order models) as future work."}, {"title": "A.1 Proof of Lemma 2", "content": "Proof. As mentioned in Lemma 2 in the main text, we partition the set G into sets \\(G_1 = \\{1, 2, ..., s\\}\\) and \\(G_2 = \\{s+1,...,G\\}\\). Now that we have simplified the formulation into two partitions, we proceed with an approach inspired by the proof of Lemma 2 in Hosseini et al. [HMP24] and establish the following upper and lower bounds on prediction normalized vote for G groups in CMM model.\n\n\n\n\n\n\\(\\frac{f(\\sigma)}{\\Sigma_{\\sigma} Pr_s(\\sigma\\tilde{\\sigma})} \\le V(\\sigma) \\le \\frac{f(\\sigma)}{min Pr_s(\\sigma\\tilde{\\sigma})}\\)\n\n\n\n\n\nWe can express the probability \\(Pr_s(\\sigma^*|\\tilde{\\sigma})\\) as follows\n\n\n\n\n\n\\(Pr_s(\\sigma^*|\\tilde{\\sigma}) = {\\Sigma}_{j \\in G_1} {\\frac{p_j{\\phi_j^{d(\\sigma,\\sigma^*)}}}{Z({\\phi_j})}} + {\\Sigma}_{j \\in G_2} {\\frac{p_j{\\phi_j^{d(\\sigma,\\sigma^*)}}}{Z({\\phi_j})}} \\)\n\n\n\n\n\nThis gives us the following lower bound on V(\\sigma^*).\n\n\n\n\n\n\\(V(\\sigma^*) = {\\Sigma}_{j \\in G_1} {\\frac{p_j{\\phi_j^{d(\\sigma,\\sigma^*)}}}{Z({\\phi_j})}} + {\\Sigma}_{j \\in G_2} {\\frac{p_j{\\phi_j^{d(\\sigma,\\sigma^*)}}}{Z({\\phi_j})}}\\\\\n\\ge {\\Sigma}_{j \\in G_1} p_j\\frac{{\\phi_j^{d(\\sigma,\\sigma^*)}}}{Z({\\phi_j})} + {\\Sigma}_{j \\in G_2} p_j\\frac{{\\phi_j^{d(\\sigma,\\sigma^*)}}}{Z({\\phi_j})}\\\\\n{\\frac{{\\Sigma}_{j \\in G_1} p_j}{{Z({\\phi_j})}} + {\\frac{{\\Sigma}_{j \\in G_2} p_j}{{Z({\\phi_j})}}}\\\\\n{\\frac{\\alpha}{{Z({\\phi_s})}} + {\\frac{1-\\alpha}{{Z({\\phi_G})}}}\\)\n\n\n\n\n\nWe can also obtain the following upper bound on V(\u03c4).\n\n\n\n\n\n\\(V(\\tau) \\le {\\Sigma}_{j \\in G_1} {p_j}{\\frac{{\\phi_j^{d(\\tau,\\sigma^*)}}}{Z({\\phi_j})}} + {\\Sigma}_{j \\in G_2} {p_j}{\\frac{{\\phi_j^{d(\\tau,\\sigma^*)}}}{Z({\\phi_j})}}\\\\\n{p_j{}\\frac{{\\phi_1}}{Z({\\phi_1})} + {\\Sigma}_{j \\in G_2} {p_j}{\\frac{{\\phi_G}}{Z({\\phi_G})}}}\\\\\n {\\frac{{\\Sigma}_{j \\in G_1} p_j}{{Z({\\phi_1})}} + {\\frac{{\\Sigma}_{j \\in G_2} p_j}{{Z({\\phi_{s+1})}}}}\\\\\n{\\frac{{\\phi_1} \\alpha}{{Z({\\phi_1})}} + {\\frac{{\\phi_G}(1-\\alpha)}{{Z({\\phi_{s+1})}}}}\\)\n\n\n\n\n\nTherefore, in order to ensure V(\u03c3*) > 2V(r) we need the following condition.\n\n\n\n\n\n\\(\\frac{\\alpha}{{Z({\\phi_s})}} + {\\frac{1-\\alpha}{{Z({\\phi_G})}}} \\ge 2({\\frac{{\\phi_1} \\alpha}{{Z({\\phi_1})}} + {\\frac{{\\phi_G}(1-\\alpha)}{{Z({\\phi_{s+1})}}}}) \\)"}, {"title": "A.2 Proof of Lemma 3", "content": "Proof. The proof is a direct application of the proof of Lemma 2 in Hosseini et al. [HMP24"}]}