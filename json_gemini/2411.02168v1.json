{"title": "DO GRAPH NEURAL NETWORK STATES CONTAIN GRAPH PROPERTIES?", "authors": ["Tom Pelletreau-Duris", "Ruud van Bakel", "Michael Cochez"], "abstract": "Graph learning models achieve state-of-the-art performance on many tasks, but this often requires increasingly large model sizes. Accordingly, the complexity of their representations increase. Explainability techniques (XAI) have made remarkable progress in the interpretability of ML models. However, the non-relational nature of Graph Neural Networks (GNNs) make it difficult to reuse already existing XAI methods. While other works have focused on instance-based explanation methods for GNNs, very few have investigated model-based methods and, to our knowledge, none have tried to probe the embedding of the GNNs for well-known structural graph properties. In this paper we present a model agnostic explainability pipeline for Graph Neural Networks (GNNs) employing diagnostic classifiers. This pipeline aims to probe and interpret the learned representations in GNNs across various architectures and datasets, refining our understanding and trust in these models.", "sections": [{"title": "INTRODUCTION", "content": "In the last decade, significant progress has been made towards modelling non-Euclidean, graph-structured data (Kipf & Welling, 2017) on the one hand, and on interpreting the predictions of deep neural networks (DNN) on the other hand. We often qualify DNNs as black box as their predictions are not inherently interpretable. Occlusion, gradient, perturbation, layer-wise relevance propagation, and attention mechanisms have been proposed to solve this problem (Zeiler & Fergus, 2013; Denil et al., 2015; Li et al., 2016; Sundararajan et al., 2017). These methods focus on highlighting the importance of different input features. They can, however, not be directly applied on GNNs due to the lack of a regular structure (e.g. vertices can have different degrees). In this case, explaining a prediction means identifying important parts of the relational structure, or input features of nodes. An issue is that finding the explanation is itself a combinatorial problem, making XAI (explainable AI) methods for GNN intractable (Longa et al., 2023a; Ying et al., 2019; Lucic et al., 2022).\nPrevious surveys (Agarwal et al., 2023; Dai et al., 2022) highlighted the lack of comprehensive, robust and model-agnostic explainability methods. We also identified that there are very few model-level explainability methods. As an alternative to these more traditional XAI methods, we propose to apply probing techniques for graph properties (as developed for Natural Language Processing Giulianelli et al. (2018), Belinkov (2021)) to GNN embeddings. In our pipeline (see fig. 1), we investigate both local properties like betweennes centrality, as well as global properties like average path length. To our knowledge, this is the first work to explore this direction."}, {"title": "BACKGROUND", "content": "Nowadays we have some theoretical understanding of the representational restrictions and capabilities of Graph Neural Networks (GNNs) with regard to the Weisfeiler-Lehman test (Akhondzadeh et al., 2023). We know that this cannot capture certain graph properties, such as connectivity or triangle-freeness (Franks et al., 2024; Kiefer, 2020; Kriege et al., 2018), due to its reliance on local structure. This constraint is also present in (message passing) GNNs.\nGraph Convolutional Network (GCN) (Kipf & Welling, 2017) are GNNs where for a single layer, the node representation is computed as: $X' = \\sigma (\\hat{D}^{-1/2} . \\hat{A} . \\hat{D}^{-1/2} . X . W)$. We know that GNNs which rely solely on local information, like the GCN and its relational variant (R-GCN) (Schlichtkrull et al., 2018), cannot compute important graph properties, such as girth and diameter or eigenvector centrality Garg et al. (2020). We are therefore also investigating more globally aware networks like GAT (Graph Attention Network) (Veli\u010dkovi\u0107 et al., 2018) and GIN (Graph Isomorphism Network) (Xu et al., 2019).\nGAT makes use of self-attention and is thereby more expressive than the GCN. However, its reliance on feature-dependent weights and structure-free normalisation limits its ability to capture specific structural properties that do not directly depend on edges. This is particularly true for tasks where node features alone are not enough, and global graph structures are crucial (e.g., tasks requiring knowledge of subgraphs or non-local patterns). GIN aggregates node features in a way that mimics the Weisfeiler-Lehman test for graph isomorphism, and with its strong inductive learning capabilities, it is likely to excel at encoding complex graph properties and solving classification tasks."}, {"title": "GRAPH PROPERTIES", "content": "Graph theory is a branch of mathematics that studies the properties and relationships of graphs. Graphs can be undirected or directed and analysed through both local and global properties. Local properties like node degree which count the number of connections a node has, identifying highly connected hub nodes, or the clustering coefficient which measures how well a node's neighbours are interconnected, capturing the local density of connections and giving the node a score, are based on a node with regard to its neighbour. In contrast, global properties such as diameter and characteristic path length assess the overall structure. They indicate how far nodes are from one another and how efficiently information can spread through the network. Global graph properties can be associated with higher level complex systems' characteristics like the presence of some repeated motifs in the sub structures of the graphs or information-flow properties.\nWe can distinguish different global properties, basic ones like the number of nodes a graph has, clustering and centrality ones, graph motifs and substructures, spectral and small-world properties."}, {"title": null, "content": "As an higher-order analysis, the recurrence of specific motifs within network substructures\u2014such as triangles, cliques, or feed-forward loops can be seen as the fundamental building blocks that dictate the system's functionality and resilience. Small-worldness 2, as characterised by Barab\u00e1si Albert & Barab\u00e1si (2002), reveal how networks can maintain short path lengths despite their expansive size and sparse connectivity. This kind of higher order properties are very interesting in order to understand how the macroscopic behaviour of complex systems emerges from the intricate interplay of their microscopic components Barab\u00e1si et al. (2002). For example how diseases spread in social networks, how neurons interact in the brain, or how information propagates through the Internet. GNNs synthesise local topological features into global structures, abstract these representations into higher-order graph attributes. Probing their learnt representations should act as a scalable proxy to investigate how global arrangement and connectivity patterns influence a system's function. In other terms, by dissecting these learned embeddings, we can possibly delve into the intricate relationships between a network's macroscopic arrangement and its emergent behaviours.\nA Graph $G = (V, E)$, V the set of vertices, E the set of edges, can be analysed through both local and global properties. Local properties (like node degree or clustering coefficient) are based on the neighbors of a node.\nIn contrast, global properties (such as diameter and characteristic path length) assess the overall graph structure. Global graph properties can be associated with higher level complex systems' characteristics like the presence of repeated motifs in the graphs or information-flow properties. See the appendix B for a list of local and global properties used in our experiments.\nGNNs synthesise local topological features into global structures and then abstract these representations into higher-order graph attributes. Probing their learnt representations should act as a scalable proxy to investigate how global arrangement and connectivity patterns influence a system's function. Based on the message passing paradigm in GNNs, as layers progress, one would expect an increased abstraction in the selection of graph properties. Initially, local features like node degree dominate, but deeper layers progressively capture more global properties, such as connectivity patterns and centrality.\nThrough hierarchical pooling or readout mechanisms, GNNs can aggregate node embeddings into a single, global graph-level embedding. Graphs that share structural similarities or patterns of interaction among nodes are organised closely in the embedding space, allowing the model to differentiate between classes of graphs, such as those with and without long paths."}, {"title": "PROBING CLASSIFIERS", "content": "In prior work (Hupkes et al., 2018) probing classifiers have been used for linguistic properties. Here, we adapt them for graph features. Unlike unsupervised techniques such as Principal Component Analysis (PCA) or T-SNE, which are useful to visualise input data with regard to the embedding latent space, we adopt a supervised framework to quantitatively assess how specific properties are encoded within the embedding space of DNNs. Let $g : f(x) \\rightarrow z$ represent a probing classifier, used to map the learned intermediate representations from the original model f to a specific property 2. The choice of a linear classifier for g is motivated primarily by its simplicity. If a linear probe performs well, it suggests the existence of a hyperplane in the representation space that separates the inputs based on their properties, indicating linear separability.\nAnother advantage of a simple linear probe is avoiding the risk that a more complex classifier might infer features that are not actually used by the network itself Hupkes et al. (2018). While other non-linear probes have been explored in the literature Belinkov (2021), even studies showing improved performance with complex probes maintain the same logic: $Perf(g, f1, Do, Dp) > Perf(g, f2, Do, Dp)$ holds across representations $f1(x)$ and $f2(x)$ when evaluated by a consistent probe g. This consistency ensures valid comparison, underscoring that if a property can be predicted well by a simple probe, it is likely relevant to the primary classification task.\nFrom an information-theoretic perspective, training the probing classifier g can be viewed as estimating the mutual information between the learned representations f(x) and the property z. This mutual information is denoted as I(z; h), where z refers to the property and h represents the intermediate representations Belinkov (2021)."}, {"title": "DATASETS", "content": "All three datasets have the same setup: given a set of graphs ${G1, G2, ...,GN}$, predict the corresponding binary labels ${y1, y2, ...,y}$.\nThe Grid-House dataset inspired by (Agarwal et al., 2023) is designed to evaluate the compositionality of Graph Neural Network (GNN). It features two concepts: a 3x3 grid and a house-shaped graph made of five nodes. The dataset consists of Barab\u00e1si-Albert (BA) graphs (Barab\u00e1si, 2009) with a normal distribution of the number of nodes. The negative class includes a BA graph connected to either a grid or a house, while the positive class contains a BA graph connected to both a grid and a house (see fig. 2). In order to ensure that the average number of nodes is the same between classes, the number of nodes is a uniformly distributed between 6 and 21 for the grid graphs, between 7 and 22 for the house graphs, and between 1 and 16 when both are present. During generation, we ensure no test set leakage by removing isomorphisms. On 2,000 graphs, we perform an 80/20 train/test split."}, {"title": null, "content": "For accurate classification, models need to identify and combine simple patterns. Recognizing isolated patterns or single node features is not sufficient. The dataset helps investigate how GNNs combine multiple concepts and addresses the \"laziness\" phenomenon, where networks learn patterns characterising only one class and predict the other by default (Longa et al., 2023b).\nThe dataset has been structured such that an optimal, linearly separable solution requires the combination of local properties, such as eigenvector centrality and betweenness centrality, or the identification of global structural motifs, like counting the number of squares (i.e., four-node cycles). A random Barab\u00e1si-Albert graph can't contain any four-node cycles, while a grid subgraph will consistently exhibit four such cycles. A house subgraph contains exactly one four-nodes cycle and one three-nodes cycle. Therefore, a graph that contains both a grid and a house will have a total of five four-node cycles. The presence of a three-node cycle could help the diagnostic of one type of graph in the negative class but is not necessary nor sufficient for solving the classification problem. On the contrary, counting the number of four-node cycle is necessary and sufficient. Thus, distinguishing between the classes does not really necessitates leveraging centrality-based measures but only recognizing the presence of a specific number of four-node cycles, enabling the model to effectively differentiate between the positive and negative classes. Thus the interesting results of fig. 3.\nClinTox Molecular contains molecular graphs representing compounds with binary labels indicating whether they are toxic or non-toxic. The dataset consists of 1,491 drug compounds with known chemical structures. Each molecule is represented as a graph where nodes correspond to atoms and edges to bonds, with node features representing atom types and edge features representing bond types. The task is to predict toxicity.\nfMRI FC connectomes consists of two parts. The Autism Brain Imaging Data Exchange I dataset contains 528 ASD patients and 571 typically developed (TD) individuals, the REST-meta-MDD dataset contains 848 MDD patients and 794 healthy controls. For both, the task is to classify these. We use the datasets with functional connectivity (FC) graphs, as prepared by Zheng et al. (2023). We perform a 95/5% train-test random split.\nIn our paradigm, we hope probing functional connectivity matrices (FC) matrices (Farahani et al., 2019) of neurological disorders (ND) could help explore the link between stuctural properties of the brain's functional connectivity and neurological disorders such as Autism Spectrum Disorders (ASD) and Major Depressive Disorders (MDD). Similarly that probed graph properties in toxic molecules align with chemical knowledge."}, {"title": "METHODOLOGY", "content": "For each of the three datasets, we use a similar network architecture consisting of a number of GNN (GCN, GIN, or GAT) layers, followed by a pooling operation (mean- (Kipf & Welling, 2017), sum- (Xu et al., 2019), or max-pooling (Hamilton et al., 2017)), and then a number of dense layers. We optimize the hyperparameters to obtain good models for the binary classification task.\nFor the Grid-House dataset the hyperparameter information can be found in table 4. On this dataset we also compared different regularisation methods. The explicit L2 regularisation encourages the network to keep the weights small, and we expect that this would make the embeddings less sensitive to fluctuations in the input data and smoother. the latter would make them more linearly separable for our probing methods. Dropout randomly disables a fraction of the neurons during each training iteration which forces the network to learn redundant representations, as any neuron could"}, {"title": null, "content": "be dropped out. These redundant representations might make it more difficult to linearly separate the graph embeddings. We ran each model 20 times and took the one with the best accuracy.\nFor the ClinTox Molecular dataset, we ranged the number of layers from 4 to 6 and hidden dimensions from 64 to 256. The final model architectures were selected based on optimal performance on the ClinTox dataset.\nFor fMRI FC connectomes the hyperparamter search space is described in table 15"}, {"title": "PROBING STRATEGY", "content": "Probing is performed on the train and test sets, where train features {f} and graph properties {z} are paired for each graph (equally for the test set). Let's define at least one example for the GCN model. Let $G\u00b2 = (A\u00b2, X\u00b2)$ denote the i-th graph, where $A\u00b2$ is the adjacency matrix and $X\u00b2$ is the node feature matrix as previously defined. The GCN layers iteratively update the node features $H^{(1)}$ through graph convolutions defined previously as $H^{(l+1)} = \\sigma(\\hat{A}H^{(1)}W^{(1)})$, where $\\hat{A}$ is the normalized adjacency matrix, $W^{(1)}$ are the trainable weights, and $\\sigma$ is a non-linear activation function (ReLU). The node embeddings $H^{(1)}$ at each layer I capture both local and global structural information by aggregating features from neighboring nodes. The final node embeddings $H^{(4)}$ are pooled using global max pooling to generate a graph-level embedding $Hglobal$, which is passed through three fully connected layers to produce the final prediction $\\hat{y}$. We deinfe these post pooling operations as $H^{(5)}_{global} = \\sigma(W_1H_{global}), H^{(6)}_{global} = \\sigma(W_2H^{(5)}_{global}), \\hat{y} = Softmax(W_3H^{(6)}_{global})$. For probing purposes, we use $H^{(1)}$ at different layers to evaluate node-level properties, while $H_{global}, H^{(6)}_{global}, H^{(5)}_{global}$ and $\\hat{y}$ are used to assess graph-level properties.\nWe aggregate node embeddings across all graphs to train a single probing classifier for each graph property. For each property, we construct a feature matrix by combining embeddings across all graphs, layer per layer. The classifier g is then trained on this aggregated dataset to predict graph properties $z^{(i)}_k$, where i denotes the i-th graph and k represents the k-th graph property, as defined in table 3. This approach assumes that the relationships between node or graph embeddings and properties are consistent across graphs.\nProbing pre-pooling layers to predict global graph properties presents challenges due to the varying numbers of nodes across graphs and the individual states for each node. To handle this, one approach would involve concatenating and flattening the embeddings into a matrix with dimensions (number of nodes, number of features), padding with zeros if a graph has fewer nodes than the maximum in the dataset. However, flattening introduces issues because nodes do not have a canonical ordering; instead, they follow an arbitrary order based on their appearance in the dataset. This inconsistency can undermine permutation invariance, especially since a simple linear classifier applied to the flattened embeddings is not inherently permutation invariant.\nTo address this, we first sort the embeddings in descending order based on their norms before concatenating, which introduces permutation invariance. Sorting in this way ensures that any padding zeros align at the end of the sequence, enabling learnable representations for graphs with varying node counts. While sorting for permutation invariance is not widely discussed in the literature, it provides a practical solution by using the embeddings' properties to enforce consistent ordering across graphs."}, {"title": "RESULTS", "content": "The models performed as anticipated thanks to their high expressiveness and the linearly separable nature of the classification problem as we can see in table 5. The probing results on the Grid-House dataset demonstrate that the number of squares consistently yields the highest $R^2$ scores across all models in the global graph embeddings (after pooling aggregation has been applied). This aligns with our initial hypothesis.\nIn general, higher-layer embeddings filter out many other graph properties as they are less relevant for making the classification problem linearly separable. The GNN's final layer focuses on the number of squares, effectively partitioning the graphs into two classes: those with #squares < 5 (indicating either the grid or house alone) and those with #squares = 5 (indicating the presence of both substructures). This reduction in feature space through the layers aligns with the model's goal of optimizing the decision boundary for binary classification, where the number of squares becomes"}, {"title": null, "content": "a clear and dominant factor for separability. Further confirming expectations, density and average path length are also prominent as the presence of both a house and a grid does slightly increase the average density and path length of graphs. These findings confirm the correspondence between graph embeddings clustering and property hyperplane separation as shown in fig. 5."}, {"title": null, "content": "We further observe that, for both the GCN and GIN models, the application of L2 regularization yields the expected behavior. The last layer of the GCN in fig. 7 shows a stronger dominance of the number of squares feature when L2 regularization is applied compared to when it is not. Similarly, in the GIN, both number of triangles and density become less detectable relative to the number of squares, by the probing classifier in the final layers under L2 regularization, consistent with the anticipated effects on the feature representation.\nWe observe results consistent with our expectations for the models with dropout. The key property is less dominant, and multiple properties are represented in the final layers. Notably, in fig. 8 the last layer, the separability gap between the #square and the other properties is reduced, indicating a more distributed representation of features when dropout is applied.\nWhen different architectures are compared, the results also align with what we expect from the expressivity of models. For GCN (control), the square detection is strong ($R^2 = 0.77$) in early layers, performance drops slightly in deeper layers, suggesting that GCN captures structural properties early without further refinement. There is less of a presence of #triangle in the control GCN than in the regularised one. The GIN (control) also consistently performs the best on squares ($R^2 = 0.93$) and shows the strong presence of the #triangle before filtering it out in the last layer. The GIN in general is sharper in the aggregation of global graph properties has it shows results only for the three properties of interest (#square, #triangle, density) before filtering them out in the last layer. It highlights that GIN excels at global feature detection and effectively isolates and leverages the most relevant structural property for the task, making it sharp in its ability to simplify complex graph data into essential information for decision-making. In other terms, its reliance on minimal yet critical features reflects its capacity for highly targeted feature extraction.\nThe GAT model stands out by capturing not only squares ($R^2 = 0.88$), but also performing well on other properties like triangles, cliques, and density. GAT assigns a weight to each neighbouring node based on a learned function of the node features, aggregating the neighbours' information"}, {"title": null, "content": "in a weighted manner. This feature-dependent mechanism introduces flexibility but also makes GAT's performance contingent on the quality and richness of the node features. It seems that GAT's broader capability compared to the GCN comes at the cost of focus, as GAT tends to incorporate multiple features, which may dilute its ability to pinpoint the most crucial property (in this case, the number of squares) for the classification task. This over-reliance on feature aggregation can lead to inefficiencies when simpler, more targeted properties suffice, as seen with GIN. These results also make sense with regard to the best score obtained by the GIN architecture as seen in table 5."}, {"title": "CLINTOX MOLECULAR", "content": "As expected GIN outperformed the other models. Based on scores and good properties (inferential mechanism with better expressivity) we focus on the GIN results for the results on other datasets. Detailed results can be found in the appendix, table 8. When looking into the linear probing performance in table 1, we find that the highest scores are consistently yielded by the average degree, the spectral radius, the algebraic connectivity and the density, in that order."}, {"title": null, "content": "The average degree of atoms in a molecule provides a straightforward interpretation, as atoms with higher valencies are generally less stable and less biologically compatible. For instance, hydrogen with a valency of 1 and oxygen with a valency of 2 are more compatible with carbon-based molecules, whereas sulfur, with a valency of 6, is less favorable for biological systems (Komarnisky et al., 2003). Therefore, the average degree serves as a useful indicator of molecular toxicity. Additionally, the spectral radius, often associated with molecular stability and reactivity, is another valuable graph property. Molecules with a lower spectral radius tend to be more stable, while those with a higher spectral radius may exhibit localized electron densities, increasing their reactivity. Using this property to predict molecular toxicity is a logical approach. To the best of our knowledge, there is no comprehensive analysis exploring the role of spectral radius in the emergence of molecular toxicity, highlighting an opportunity for future research."}, {"title": "FMRI FC CONNECTOMES", "content": "In the detailed results, in the appendix table 14, GIN outperforms the other architectures in both parts of the dataset (reproducing the observation by Zheng et al. (2023). Again, the strength of GIN lies in its injective aggregation mechanism. The probing results on the ASD dataset reveal that the number of triangles consistently achieves high $R^2$ scores across all models, with particularly strong performance in GIN models. This property is followed by the spectral radius and the density.\nAs further detailed in the appendix tables 16 to 19 the number of edges is particularly well encoded in the representation of the GAT. This is a consequence of its reliance on feature-dependent weights and structure-free normalisation, which limit its ability to capture specific structural properties that do not directly depend on edges. The GCN results are broadly comparable to those of the GIN, though they tend to be less precise and selective.\nFor the MDD results we also focus the GIN model. Detailed results are in the appendix tables 20 and 22 to 24. The probing results MDD reveal that the number of triangles still consistently achieves high $R2$ scores across all models while being less of a distinctive feature than in ASD. This time, the spectral radius is dominated by the density of the graph. In general, the embeddings from the 7th layer of our GIN architecture exhibit higher $R^2$ scores for relevant graph properties, suggesting improved separability in the embedding space for MDD classification compared to ASD. This indicates that the learned representations at this depth capture more discriminative structural features, facilitating more effective class separation between MDD and healthy controls."}, {"title": "DISCUSSION", "content": "For Grid-House we hypothesized that the GNN would benefit from leveraging both the local clustering coefficient and eigenvector centrality as node-level features. The first one would help characterise a house, the second a grid. However, neither feature alone is sufficient to render the problem linearly separable. We therefore expect either a combination of features or a single global property (e.g. number of squares) to be leveraged. If the tensor embeddings produced by the GNN can be used to predict these properties, this would indicate that the GNN is utilising them in solving the classification task.\nFor the ClinTox Molecular dataset, based on the literature Kengkanna & Ohue (2024); Chen et al. (2021); Jiang et al. (2021) some few properties have been found to be link with toxicity such as the node degree (i.e. the valency), subgraph patterns (functional groups, chemical fragments), and the overall graph connectivity.\nBased on existing literature on functional connectivity (FC) network properties in ASD and MDD, we hypothesized that specific properties will be critical in classifying brain networks for the fMRI FC connectomes dataset. For ASD, we expect betweenness centrality to play a significant role at the node level, reflecting local overconnectivity. At the graph level, we anticipate that clustering coefficient, characteristic path length, and small-worldness will be essential in capturing the local and global network disruptions seen in ASD, particularly the imbalance between local overconnectivity and long-range underconnectivity. For MDD, we hypothesise that increased clustering coefficients, modularity, number of triangles and number of squares will be key features for classification, as they could indicate of heightened local interconnectedness and disrupted global integration."}, {"title": "FINDINGS", "content": "We first demonstrate the feasibility of our probing method through the Grid-House dataset. This acts as a proof of concept on probing classifiers plumbing the representations learned by the GNN. We made sure to choose a classification task which requires learning global structural properties of the graph, such as motifs like squares and triangles, and a long range dependencies between those. In line with our expectations, the results show significant dominance of number of squares in the post pooling layers of every model, while still highlighting the superiority of the GIN model in leveraging superior representations when trained to classify graphs with complex motifs like Grid and House. This shows the expressivity of the models and their ability to reduce the complexity of a graph related problem to known graph properties, making the problem linearly separable in the space of their embeddings. These results are consistently higher than those obtained from probing the models with randomized labeling, highlighting the relevance of the initial findings.\nUsing the ClinTox Molecular dataset to assess molecular toxicity, we explored how key graph properties, such as the average degree and spectral radius, are utilized by our GIN architecture. The average degree, closely linked to atomic valency, reflects a molecule's potential for interactions. The spectral radius offers a complementary hypothesis, suggesting that the overall structural stability of a molecule, independent of specific atomic features, may also be a key factor in toxicity prediction. These results suggest that despite the limitations of our approach, it still holds potential for assisting research in complex systems fields, such as neuroscience or social sciences, where emergent phenomena play a crucial role in understanding system dynamics.\nGiven the previous positive results, we explored a real life applications with the fMRI FC connectomes dataset. Here, the results provide new insights that extended beyond our initial hypothesis. While we expected betweenness centrality, clustering coefficient, and characteristic path length and Small-worldness to be the most relevant for distinguishing ASD from healthy individuals, the prominence of the number of triangles highlighted the importance of local structural motifs. This makes sense in the context of functional connectivity, where local overconnectivity in specific brain regions, such as sensory and association cortices, has been observed in individuals with ASD. The strong role of triangle motifs may reflect the tight, redundant local connections that characterize these regions, supporting the hypothesis that local overconnectivity is a key factor in ASD. The spectral radius and density and graph energy being particularly significant is also logical, as these properties are closely related to the overall connectivity strength and the compactness of connections within subnetworks. The presence of SW is still quite significant in the post-pooling layers which is interesting. In ASD, where global integration is often reduced and local connectivity heightened, these metrics may provide an important reflection of the imbalance between short-range and long-range communication pathways in the brain."}, {"title": "COMPARISON BETWEEN DATASETS", "content": "Comparing the ClinTox and the fMRI datasets an interesting observation emerges: basic graph properties (such as the number of nodes, of edges or the average path length) are almost omnipresent in the early layers of the GIN trained on the ClinTox dataset. However, their presence is less pronounced in the GIN trained on the ASD or MDD datasets. This difference offers a clue in distinguishing the complexity of brain-related neurological disorders from the complexity of chemical qualities such as toxicity. This suggests that the emergent properties of the brain may not be as easily tied to simple, differentiable structural features as those seen in molecular systems.\nAs a confirmation, the types of global graph properties present in the post pooling layers of the GIN-clintox model are of less high level of abstraction than the ones in GIN-MDD or GIN-ASD. The presence of the average degree, the spectral radius, the algebraic connectivity and the density as accurate explanations for the prediction of toxicity in molecules. The presence of the spectral radius in the last layer of the GIN makes it an even more interesting property to study for toxicity. On the other hand, the presence of motifs should be more investigated in the ASD and MDD datasets with eventually more complex motifs being probed (hexagons constituted of neighbored triangles, house, grid, etc)."}, {"title": "FUTURE WORK", "content": "Our methodology has several limitations. While we addressed dataset issues such as leakage and isomorphic graphs, a key challenge remains the lack of guarantees that GNNs find globally optimal solutions, despite their theoretical capacity as universal function approximators. This is particularly evident in fMRI data, where multiple layers of complexity-from MRI limitations and BOLD signal characteristics to Pearson correlation for functional connectivity-introduce noise and inaccuracies. Investigating additional graph properties like girth or complex motifs could be beneficial. Preliminary work on alternative architectures (e.g., GATv2, GraphSAGE, ChebNet, Set2Set, HO-Conv, DiffPool) has begun but is not yet complete."}, {"title": "CONCLUSION", "content": "We demonstrate the relevance of our model-agnostic explainability method for graph neural networks which probe for GNN graph theoretic representations on the Grid-House dataset. We anticipate any lazy learning bias. We manifest both the expressivity of different GNN architectures and their ability to solve a graph classification problem through optimal feature extraction. They render it linearly separable in the space of their embeddings through the computation of the number of squares in the graph.\nThat experiment prompted us to investigate both the Clintox Molecular dataset and fMRI FC connectomes dataset and anchored the possibility of formulating hypotheses on the emergent dependence of complex systems qualities to basic and more higher level structural properties. This kind of higher order properties are very interesting in order to understand how the macroscopic behavior of complex systems emerges from the intricate interplay of their microscopic components. For example how diseases spread in social networks, how neurons interact in the brain, or how information propagates through the Internet. There is a manifest emergence of molecular qualities like toxicity with regard to their structural properties like node degree (atom valency) and spectral radius (the molecule's stability). But the complexity of complex systems like the brain makes blurrier the possibility of understanding what affects what as, for example, one could argue that behavioural therapies might influence the brain connectivity as the brain connectivity might influence behavioural qualities of one patient. Echoing this egg-chicken conundrum, the investigation of motifs and spectral properties' role in neurological disorders like ASD and MDD could allow for promising avenues."}]}