{"title": "Exploring Action-Centric Representations Through the Lens of Rate-Distortion Theory", "authors": ["Miguel De Llanza Varona", "Christopher Buckley", "Beren Millidge"], "abstract": "Organisms have to keep track of the information in the environment that is relevant for adaptive behaviour. Transmitting information in an economical and efficient way becomes crucial for limited-resourced agents living in high-dimensional environments. The efficient coding hypothesis claims that organisms seek to maximize the information about the sensory input in an efficient manner. Under Bayesian inference, this means that the role of the brain is to efficiently allocate resources in order to make predictions about the hidden states that cause sensory data. However, neither of those frameworks accounts for how that information is exploited downstream, leaving aside the action-oriented role of the perceptual system. Rate-distortion theory, which defines optimal lossy compression under constraints, has gained attention as a formal framework to explore goal-oriented efficient coding. In this work, we explore action-centric representations in the context of rate-distortion theory. We also provide a mathematical definition of abstractions and we argue that, as a summary of the relevant details, they can be used to fix the content of action-centric representations. We model action-centric representations using VAEs and we find that such representations i) are efficient lossy compressions of the data; ii) capture the task-dependent invariances necessary to achieve successful behaviour; and iii) are not in service of reconstructing the data. Thus, we conclude that full reconstruction of the data is rarely needed to achieve optimal behaviour, consistent with a teleological approach to perception.", "sections": [{"title": "1 Introduction", "content": "Embodied agents have to focus on the relevant information from their environment to achieve adaptive behaviour. Their resource-limited cognition and the high-complexity structure inherent to the environment force them to economize the transmission of information. Thus, the goal of the perceptual system is to generate representations that are useful for successful behaviour while at the same being encoded in the most efficient manner."}, {"title": "2 Efficient coding and rate-distortion theory", "content": "A well-known hypothesis in theoretical neuroscience called the efficient coding hypothesis proposes that the neural coding in the brain is optimized to maximize sensory information under metabolic and capacity constraints [3, 13, 23]. In particular, this hypothesis suggests that neurons are tuned to the statistical properties of the environment, which allows them to efficiently allocate signaling resources to generate compressed low-dimensional representations of the environment. In this theoretical framework, it is commonly assumed that the function of neurons is to maximize their capacity to account for all the variability in the sensory input. In information theory terms, this means that the brain seeks to maximize the mutual information between stimuli and neurons' response to reduce as much as possible the uncertainty about the environment, which is defined by its entropy. While this hypothesis answers the question about information processing under biological constraints, it leaves aside the utilitarian aspect of perception [11, 14, 15, 18, 20].\nCognition can't be fully understood without its ecological context, as agents are coupled with their environment forming a perception-action feedback loop [22]. In this sense, the functional role of perceptual processing has to be in service of achieving behavioural objectives, and to do that, perceptual representations must efficiently encode the relevant information needed by the motor system to guide future actions. Thus, a key component of the perceptual system is to summarize relevant sensory information to generate action-centric representations.\nThe teleological essence of the perceptual system imposes a normativity on representations: a perceptual representation is accurate if it captures the relevant information needed downstream and discards the irrelevant details. Thus, we need an extra ingredient to account for the goodness of representations under constraints. This is where the rate-distortion theory comes into play [19]. This subfield of information theory defines the optimal trade-off between channel capacity and expected communication error. When error-free communication is not necessary to guide behaviour, the optimal encoding is a lossy compression of the input.\nInterestingly, rate-distortion theory can be seen as a way to perform Bayesian inference under constraints. Under a Bayesian approach to cognition, the brain performs inference to compute an optimal posterior distribution over hidden environmental states given sensory data [8]. As computing the true posterior is usually intractable, the brain approximates the true posterior by optimizing the variational free energy [4,9,10]. The main conceptual contribution of rate-distortion theory is to define the \"goodness\" of that approximation, as computing the true posterior is not always necessary to act optimally. In the context of active inference, it has been shown that action-oriented models learn parsimonious representations of the environment by capturing relevant information for behaviour [21]. In the same spirit, we investigate the information-theoretic properties of action-centric representations and their relation to the formal definition of abstractions we propose.\nIn this work, we explore action-centric representations under the lens of rate-distortion theory to account for the teleological aspect of perception. To"}, {"title": "2.1 Efficient coding", "content": "The efficient coding hypothesis states that neurons are optimized to maximize the information they carry about sensory states. In doing so, neurons have to generate minimal redundancy codes to economically use limited resources. In particular, neurons seek to maximize the ratio between information about sensory inputs, defined by the mutual information $I(X; Z)$ between sensory data X and neural responses Z, and the channel capacity C: $I(X;Z)$. The maximum mutual information is upper bounded by the channel capacity\n$C > I(X; Z)$\nso the best efficient coding satisfies\n$I(X; Z) = C$\nwhere neuronal encoding exploits the whole bandwidth of the channel."}, {"title": "2.2 Rate-distortion theory as goal-oriented efficient coding", "content": "Under the classical conception of efficient coding, the exploitation of information downstream is ignored. When not all sensory information is needed to guide behaviour, error-free communication is not expected. This is precisely what is addressed by the rate-distortion theory, which provides the theoretical foundations for optimal lossy data compression. Formally, the rate-distortion function defines an optimal lossy compression Z of some data X as the minimization of their mutual information $I(X; Z)$ given some expected distortion D associated with reconstructing X from its lossy compression Z. It is defined as [6]\n$R(D) = \\underset{q(z/x):D_q(x,z) \\leq D}{min} I(X; Z)$\nwhere q is the optimal distribution of z given x that satisfies the expected distortion constraint and the rate R is an upper bound on the mutual information:\n$R > I(X; Z)$"}, {"title": "3 Abstractions and action-centric representations", "content": ""}, {"title": "3.1 Mathematical formalization of abstractions", "content": "An abstraction is the reduction of complexity by discarding certain features while preserving others. As a relational concept, an abstraction involves two components: its object (what is being abstracted) and its content (what the abstraction"}, {"title": "3.2 Abstractions as sufficient and non-superfluous representations", "content": "Following [7], an abstract representation Z that captures the relevant details of the data X to answer a query Q should be sufficient ($I(X; Q|Z) = 0$) and non-superfluous ($I(X; Z|Q) = 0$):\n$I(X; Z|Q) = I(X; Z) \u2013 I(X; Q; Z)$\n$ = I(X; Z) \u2013 I(X;Q) + I(X;Q|Z)$\n$= I(X; Z) \u2013 I(X; Q)$"}, {"title": "4 Variational Free Energy and Rate-distortion theory", "content": "As computing the rate-distortion function is intractable in high-dimensional systems [6], variational inference can be used as a proxy of the amount of information transmitted through a communication channel. In variational inference, a"}, {"title": "5 Methods", "content": ""}, {"title": "5.1 Model", "content": "Inspired by the utilitarian perspective on the efficient coding hypothesis and the mathematical foundations of abstractions, we present a modified VAEs to model action-centric representations (Figure 2). The main novelty of the VAEs presented here lies in the accuracy term of the free energy (Eq. (20)). Contrary to vanilla VAEs, where the goal is to learn latent representations of the data to reconstruct it as faithfully as possible, here we are interested in learning action-centric representations that convey sufficient and non-superfluous information about a query. In this model, full reconstruction of the data is not expected. The final form of the objective function for our action-centric VAEs is:\n$F = -D[Q(x)||Q(r(z))] + \\beta D_{KL} [q_{\\phi}(z|x)||p(z)]$\nwhere $\\beta$ is the gradient of the rate with respect to the distortion and here it's used to target specific regimes of the rate-distortion plane [5]. The accuracy is modified to account for the goodness of the abstraction. The training"}, {"title": "5.2 Results", "content": "The results regarding the transmission of information in the two different VAEs are shown in Figure 3. In (Figure 3a) it can be seen how action-oriented VAEs converges faster to an encoding-decoding scheme that is useful for the downstream task (measured by the accuracy), compared to the VAEs. This indicates that action-centric representations might require less exposure to data, which makes them more efficient in terms of exploiting the available information.\nIt is clear how action-oriented representations require significantly less information from the data to achieve better results in the downstream task. In"}, {"title": "6 Discussion", "content": "Agents need to navigate complex environments with limited biological information processing. Under this circumstance, an optimal perceptual system has to efficiently allocate cognitive resources to transmit the relevant sensory information to achieve successful behaviour. Thus, the goal of perception is not to generate faithful reconstructions of the sensory input, but abstract representations that are useful downstream.\nA common approach to representations in Artificial Intelligence and Neuroscience is that they should be in service of fully reconstructing the data. However, such representations will carry irrelevant information for downstream tasks that only depend on the exploitation of specific invariances and symmetries of the data."}, {"title": "7 Conclusion", "content": "In this work, we explore useful efficient coding within the framework of rate-distortion to explore optimal information processing for task-dependent contexts. We have provided a formal definition of abstractions that can be used to learn action-centric representations whose main function is to capture the task-dependant invariances in the data. Such lossy compressions of the data lie near optimal points of the rate-distortion curve. Crucially, we show that action-centric representations i) are efficient lossy compressions of the data; ii) capture the task-dependent invariances necessary to achieve adaptive behaviour; and iii) are not in service of reconstructing the data. This could shed some light on how organisms are not optimized to reconstruct their environment; instead, their representational system is tuned to convey action-relevant information.\nInterestingly, our work resonates with recent research on multimodal learning such as the joint embedding predictive architecture and multiview systems [2, 7]. The main objective of these models is to obtain representations that are useful downstream but from which it's not possible to reconstruct the data. These representations learn the relevant invariances by maximizing only the information shared across different views or modalities of the data. We argue that action-centric representations operate in a similar way, as shared information across views is an implicit way to define a query (see Appendix D).\nAn interesting line of research is to explore faithful reconstruction in the context of fine-grained queries such as pixel predictability. We hypothesize that, as the number of pixel-specific queries approaches the pixel space of the image, the abstract representation might allow for faithful reconstruction of the data. Although that could be to the detriment of worse performance on downstream tasks.\nIn conclusion, this work sets a promising line of research in the field of representational theory by understanding representations not as faithful reconstructions of the data but as action-driven entities."}, {"title": "A Model details", "content": "The classifier used to implement the query is a deep convolutional network (CNN) with three convolutional layers. The number of filters for the first layer is 16, and it is doubled in each layer. The kernel size is 3 in all layers, and padding is set to 1, also in all layers. Stride is 1 in the first two layers, and 2 in the third one. In addition, batch normalization is applied in each layer; 16 for the first one, and doubled in each layer. The activation function in each layer is ReLU, and max pooling is applied in the first two layers, both with a kernel size of 2, and stride of 2 in the first and 1 in the second. Between the first two fully connected layers it is used a dropout of 0.2. The number of neurons for the fully connected layers is 512, 128, and 10. We use the Adam optimizer with a learning rate of 0.001. We trained the classifier for 15 epochs with a batch size of 64.\nRegarding the VAEs, the encoder is a CNN of 4 layers with the same parameters as the CNN. Every VAEs trained has 8 latent dimensions and are trained for 20 epochs using a batch size of 64. In the case of the vanilla VAEs, the $\\beta$ used to draw the rate-distortion curve are 100, 40, 20, 10, 5, 1, 0.5, 0.1, and 0.01. For the custom VAEs, the $\\beta$ values are 6e-2, 3e-2, 1e-2, 6e-3, 3e-3, 1e-3, 5e-4, 1e-4, 1e-5, 1e-6."}, {"title": "B Latent space of VAEs", "content": "PCA to explore and show the latent space of the vanilla and action-centric VAEs that achieve a good performance downstream:"}, {"title": "CELBO and RDT", "content": "One way to derive the upper bound on mutual information from the complexity term of the ELBO is:\n$= E [DKL[q(z|x)||p(z)]] = E ln dxdz]\n= E q(z|x) ln dxdz]\n= E dxdz + dxdz]\n= q(x,z) ln dxdz +\n= ln dz\n= I(X; Z) + $DKL[q(z)||p(z)]\n> I(X; Z)\nAnother way to derive this upper bound is by splitting the expected complexity into conditional entropy and entropy terms:\n= ln dxdz]\n= dxdz dxdz\n= dxdz \nIn the last equation, we can see that the first term is the negative conditional entropy -H(Z|X) which is one of the two terms in which the mutual information is decomposed: I(Z; X) = H(Z) \u2013 H(Z|X). To get the entropy H(Z) we need to replace p(z) by an approximate distribution q(z). By Jensen's inequality, we know that DKL[q(z)||p(z)] \u2265 0, therefore, we know that:\nReplacing that term in the previous expression (34) we get:"}, {"title": "D Multiview architecures and queries", "content": "Given a query Q(X) over X in a multiview scenario it can be understood as the subset of information contained in the intersection of X and t(X) such that:\nQ(X) \u2208 Xnt(X)\nas the transformation t only preserves those symmetries relevant for the query (i.e., relevant to solve a set of tasks that only depend on those invariances). Therefore, the relevant query in a multiview scenario can be defined as:\nQ(X) = p(X, t(X))\nMutual information between X and Z and between Q(X) and Z is (assuming that X, X' and Z form a dag where Z only depends on X):\nI(X; Z) = ln dxdz\nI(Q(X); Z) = ln dqdz\n= ln dxdx'dz\n= ln dxdx'dz\n= ln dxdx'dz\n= ln dxdx'dz\n= ln dxdz\n= ln dxdz\n= I(X; Z) = I(X; X')\nThe mutual information between the latent Z and one of the views X is equal to the mutual information between the query distribution Q(X) and the latent Z. As the mutual information between an optimal lossy representation and its corresponding view is equal to the mutual information between views, then, the information conveyed by the query is the one shared by the views. This shows that the multiview architecture is essentially a query-oriented system where the transformations applied to the data keep specific invariances with respect to a set of implicit queries of interest."}]}