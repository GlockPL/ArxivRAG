{"title": "Being Accountable is Smart: Navigating the Technical and Regulatory Landscape of Al-based Services for Power Grid", "authors": ["Anna Volkova", "Alina Anapyanova", "Mahdieh Hatamian", "Hermann de Meer"], "abstract": "The emergence of artificial intelligence and digitization of the power grid introduced numerous effective application scenarios for AI-based services for the smart grid. Nevertheless, adopting Al in critical infrastructures presents challenges due to unclear regulations and lacking risk quantification techniques. Regulated and accountable approaches for integrating AI-based services into the smart grid could accelerate the adoption of innovative methods in daily practices and address society's general safety concerns. This paper contributes to this objective by defining accountability and highlighting its importance for AI-based services in the energy sector. It underlines the current shortcomings of the AI Act and proposes an approach to address these issues in a potential delegated act. The proposed technical approach for developing and operating accountable AI-based smart grid services allows for as- sessing different service life cycle phases and identifying related accountability risks.", "sections": [{"title": "1 INTRODUCTION", "content": "One of the main objectives of modern power grids is the uninter- ruptible supply of demands. At the same time, the power generation and distribution processes must be conducted safely to prevent any harm to society. The digitalization of the power grid has introduced significant risks, such as easier access to the power grid assets for cyber-attackers, but also substantial benefits, such as improved control systems and enhanced operational efficiency. System internal faults and external influences constantly challenge the power grid's resilience. The widespread adoption of Artificial Intelligence (AI) initiates the next phase of grid digitalization. Advanced smart grid services utilize AI and underlying machine learning techniques to deliver insights beyond the reach of deterministic software and uncover correlations that may not be apparent to human operators. Some AI-based services offer supplementary support, while oth- ers enhance or replace functions critical for maintaining the power grid's operation. Supplementary services include grid operation optimization, such as demand response, where AI predicts fluctuations and reduces costs. As a part of the essential grid functions, Al has also demonstrated being effective in forecasting renewable energy generation and load [1], stability analysis and control at different grid voltage levels [46]. Integrating such AI-based services into the smart grid is associated with risks to the stable system operation and, as a result, to society. Data and algorithms used as the basis of AI-based services might produce incorrect decisions and lead to operational failures, which can lead to blackouts. Ex- cessive dependence on AI can result in a lack of human oversight and hinder intervention in case of system failures. On the other hand, some AI-based services, like failure detection, offer benefits by being capable of reacting much faster than human operators.\nSafely implementing AI-based services for the social good re- quires extensive regulation and methodologies to guarantee risk- free operation. The European Commission has attempted to address AI integration through various regulatory initiatives and proposed AI Act in 2021, which was adopted in March 2024 [17] and com- plemented by a Corrigendum in April 2024 [16]. Unfortunately, AI Act does not provide domain-specific regulations, leaving space for interpretation regarding the AI integration in the power grid. The central argument of this work is that regulating Al integration in the smart grid requires a proper regulatory framework that can limit the risks related to AI-based services operation and does not hinder the innovation in the smart grid domain. A technical frame- work should complement it to ensure precise risk quantification. Such a technical framework can be based on the concept of AI ac- countability, defining it in a quantifiable manner, linking technical risks, their impact on the service operation, and responsible parties.\nThis work aims to support the development of the methodolo- gies for regulated and accountable AI-based smart grid services and makes recommendations for a potential delegated act for the smart grid. To achieve this, the following contributions are made:\n1) A classification of existing ways to address accountability and related terminology in the smart grid context, which enables the derivation of a quantifiable definition of AI-based smart grid service accountability; 2) An analysis of the current shortcomings of the AI Act regarding the future risks of a narrow safety component"}, {"title": "2 RELATED WORK", "content": "Some recent studies specifically address the definition of account- ability and accountability requirements. For instance, in [25], the authors categorize and structure definitions of accountability, pre- senting a model to capture accountability in system design. In [56], authors discuss the need to narrow down high-level definitions of accountability and related terms and make these enforceable. In con- trast, the present work focuses on sector-specific accountability and provides definitions that support quantification of accountability for AI-based smart grid services. In [7], accountability is discussed as the intersection of explainability and responsibility, and the exist- ing regulations covering these concepts are analyzed. However, no approach to quantify accountability is proposed. While in [7] the authors only introduce the concept of implementable accountabil- ity, the present work proposes an approach to how implementable accountability can be achieved in the smart grid.\nAccountability in the smart grid domain has also been addressed from a technical point of view. Thus, in [50], the authors review methods to provide authentication, authorization, and accountabil- ity in smart grids from the communication network perspective. In [55], the authors demonstrate a conceptual monitoring system for Al-based smart grid services and indicate that monitoring of AI in energy grid operations is essential for establishing accountabil- ity. In contrast, while showing the importance of monitoring, the present work proposes an approach to guarantee accountability for all the AI-based service life cycle phases.\nThe challenge of translating accountability into legal regulation has been discussed in [7]. In the absence of AI-specific regulations, IEC 61508 [23] has been reviewed as a source of recommendation for Al integration in critical infrastructures. Thus, authors in [51] state that all AI applications in the critical infrastructure are gener- ally not recommended in the standard. However, the Standard only discourages using AI for particular applications in safety-related functions. An attempt to adjust Satefy Integrity Levels for AI has been discussed in [12]. The proposed approach is valid but too ab- stract: Al safety risk classification should be done more granularly.\nOne of the core insights of the present work is that accountability is only achievable through accountable development. The impact of the development flaws on the AI-based service performance has been discussed in [40] with a focus on data bias. The present work considers bias mitigation, among other steps, as an important ac- countability guarantee. An extensive survey on potential design"}, {"title": "3 AI-BASED SERVICES IN THE SMART GRID", "content": "A smart grid has been defined by many different institutions and authors, for example, as \"an advanced digital two-way power flow power system capable of self-healing, adaptive, resilient, and sustain- able with foresight for prediction under different uncertainties\" [13]. Many of these advanced features elevate the system from its pure physical purpose of transferring the power flow to an intelligent system capable of reacting and overcoming the existing challenges, improving its cost-effectiveness and customer satisfaction. Al has been demonstrated effective in essential technical, including safety- related functions, and user- and grid economy-oriented services. Some functions, such as voltage and frequency control, directly impact the system operation. Demand response, while being a criti- cal function for the system's operation effectiveness, is not strictly required for system operation. Client-oriented functions, such as consumption analysis and billing optimization, do not affect the system's operation directly. The services also function on different time scales, allowing or hindering human oversight. Control and fault mitigation services make decisions quickly during the oper- ation, while forecasting services provide calculations in advance and ensure sufficient time for operators to react to the proposed predictions. In this work, the term AI-based smart grid service con- siders both technical and process optimization functions operating within the smart grid and having AI-based components. This work does not discuss information security-related services.\nTo guarantee the safe operation of the AI, the responsive regula- tions should consider the level of AI involvement in the operation of the particular smart grid function. This section discusses the existing regulations in the Al domain and their applicability to the power grid domain. After the in-depth analysis of the most important formulations, the problem of deriving regulations and recommendations and the need for AI accountability is discussed."}, {"title": "3.1 Current Status of AI Regulation in Energy", "content": "AI and Machine Learning technologies have increasingly come to the attention of European lawmakers in recent years. The result was a consensus at the European level that a common regulatory"}, {"title": "3.2 AI as Safety Components", "content": "The energy sector has been categorized as a high-risk field of ap- plication for Al technologies if Al technology is being used for safety components in the management and operation of critical infrastructures, including the digital critical infrastructures. These are defined in Art. 2 point 4 and in Annex I of the Directive (EU) 2022/2557 [15] and in Annex III point 2 of the AI Act. High-risk AI systems in the energy sector are referred to in Annex III point 2 of the AI Act as: (a) Al systems intended to be used as safety components in the management and operation of road traffic and the supply of water, gas, heating and electricity. Such safety components are fur- ther defined in point (55) of the AI Act, Safety components of critical infrastructure, including critical digital infrastructure, are systems used to directly protect the physical integrity of critical in- frastructure or health and safety of persons and property but which are not necessary in order for the system to function. The failure or malfunctioning of such components might directly lead to risks to the physical integrity of critical infrastructure and thus to risks to the health and safety of persons and property. Components in- tended to be used solely for cybersecurity purposes should not qualify as safety components. Examples of safety components of such critical infrastructure may include systems for monitoring water pressure or fire alarm controlling systems in cloud computing centers. This definition resembles the safety component definition in the Art. 2(b) of the Machinery Directive, where a safety component was described as not necessary for the system to function [14]. Such a definition is too product-oriented and ignores the fact that there are elements in the digital and critical energy infrastructure where safety and functioning are interdependent in the system's algo- rithm. Certain safety functions in generation plants, power grids, or digital infrastructures can be indispensable for the system's func- tioning and safety, e.g., using AI for voltage and frequency-control mechanisms, stability assessment systems, and load balancing.\nThe AI Act states that only if Al technology is used in safety components needed for the safety and physical integrity but which are not necessary for the system to function, high risk can be identified. Such a limitation is very narrow and does not reflect the constantly evolving nature of smart grid AI applications. How to regulate cases where AI technologies might be interconnected and used in safety components for the protection and functioning of the whole system are not yet dealt with in the Act and will have to be clarified in an additional sector-specific delegated act. Until then, this will remain a grey zone, leaving room for interpretation for AI providers. A potential consequence could be that AI applications developed for the safe operation of critical infrastructure, such as voltage or frequency control, but which do not meet the safety component definition under the Act, will simply not be covered by the AI Act and will not be obliged to meet the High-Risk AI"}, {"title": "3.3 Problem Formulation", "content": "As a result, the AI Act does not provide sufficient regulation action for all the variety of use-cases of AI-based services in the power grid. The definition of the safety component is capturing only a subset of possible functions that can be realized using AI, while other potential AI-based services are not covered. Since unreliable usage of Al in critical infrastructure is a major public safety concern, additional actions are required to guarantee that the development and implementation of the AI has been carried out responsively. In IEC 61508 [23], qualitative and quantitative approaches have been proposed in the safety-relevant systems to assign risk levels to the safety functions. In [12], a similar approach was proposed for gen- eral purpose Al functions. Along with qualitative parameters, such as non-determinism, a quantitative framework for risk assessment and definition of rigor activities should be provided. Developing and legislating such safety standards for AI is a complex but nec- essary task to be completed in the next decades. However, already nowadays, actions are required to provide granular control of the developing AI-based services. A delegated act could be introduced and propose a methodology to supervise AI-based service from the planning till the commission phase. Such supervision can be quantified through a mechanism of service accountability. Account- ability is usually defined through its core dimensions: explainability, audibility, and trustworthiness. However, the interpretation of ac- countability varies across different domains [7, 25]. A sector-specific accountability for AI-based service should be defined, described, and quantified to overcome it. In this work, an accountable AI-based smart grid service is defined, and an approach to preserve overall accountability through accountability of the separate development and deployment processes is presented."}, {"title": "4 DEFINING ACCOUNTABILITY", "content": "This section discusses the existing definitions of accountability in different sources: in AI Act, in general and smart grid-specific litera- ture. An approach to provide a quantifiable definition of accountable AI-based smart grid service is presented."}, {"title": "4.1 Limitations of Definitions in the AI Act", "content": "The current AI Act demands accountability from AI applications in different ways. Accountability was already identified by the Com- missions Expert Group on AI in 2019, as a requirement for trustwor- thy AI [22]. In the AI Act itself, accountability is not defined but can be found partially hidden in terms of transparency, explainability and interpretability. These terms can be found in different Articles of the Act and concern primarily AI providers' obligations regarding high-risk applications. The following non-exhaustive list of exam- ples shows the legislators' intention to ensure transparency of AI High-Risk systems through an ex-ante conformity assessment of AI applications. One clear example is the record-keeping requirement. As stipulated in Art. 12 point (1) and Art. 19 of the Act, automatic recording of events should be possible, illustrating the need for traceability of AI. Interpretability and transparency requirements are to be found, among others, in Art. 13 and in Section 3 of the Act. It states that High Risk AI applications shall be designed so that their operation is sufficiently transparent to enable users to interpret and use the system's output. Further, the requirement for human oversight of AI decisions and in AI design in general are to be found in Art. 13 (3)(d) and Art. 14 of the Act. Lastly, data quality of Al is to be ensured through a quality management system and documentation requirements in Art. 17 and 18.\nThus, the Act does not define Al accountability as a separate term but requires High-Risk Al providers and deployers to follow different obligations, leading to more trustworthiness, irrespective of their application sector. The described approach covers the most crucial aspect of AI development and deployment. However, it re- quires more granularity for recommended and undesired techniques at each step for specific domains, e.g., for the smart grid."}, {"title": "4.2 Accountability in Literature", "content": "The general literature on AI lacks a precise definition of accountabil- ity for AI-based systems due to its relation to several other aspects. Nissenbaum defines it as a requirement to provide information regarding a performed action, explain why it was taken, and un- dertake a follow-up action, which could include various responses such as punishment or penalty [38, 56]. In [31], the authors remark that \"In general, accountability for AI indicates how much we can trust these Al technologies and who or what we should blame if any parts of the Al technologies perform below expectation. It is about a declaration of responsibility. It is not trivial to explicitly determine the"}, {"title": "4.3 Deriving a Definition", "content": "A comprehensive definition would support the development of an accountability framework and should encompass various aspects of accountability relevant to different phases of the service life. The other challenge is quantifying accountability and measuring the risks if the AI does not fulfill accountability recommendations. Providing a cross-sector unified definition of accountability is only possible at a high level of abstraction.\nThus, the definition of accountability should be directly applica- ble to the target service or process. In this regard, an accountable AI-based smart grid service can be defined as service that is: 1) conceptualized and developed according to the sector-specific regula- tions and with clear identification of roles and responsibilities of all involved parties 2) regulated over the whole life cycle from planning to commissioning to minimize risks of an incorrect behavior in each development phase by tracing the impact of each step on the service outputs, recording all the decisions and actions performed during the development and deployment as well as responsible parties, guaran- teeing accessibility of collected and processed data for involved parties 3) monitored during the operation time with granular insight into the impact of design choices on the output. It is important to note that explainability [34] has been explicitly not used in this definition since its human-centered nature and the general tendency for re- duced performance due to model simplification are not applicable for some of the power grid services [7, 55]. In the case of account- ability, complexity is seen as the source of potential design and development imperfections and related risks but is acceptable."}, {"title": "5 ACCOUNTABILITY PRESERVATION IN AI-BASED SERVICE DEVELOPMENT", "content": "Essentially, AI-based service accountability, defined in Section 4.3, should be preserved by a technical framework, which should cover"}, {"title": "5.1 Data Collection and Correction Phases", "content": "The established and accountable data collection and correction process is the initial step of accountability assurance. In this regard, the smart grid is a special case since the data can have different origins: assembled from the measurements of the real system, from a digital twin, or from software simulation, each with its specific accountability issues. General accountability issues of this step are listed in Table 3.\nIn the data collection step, data source discovery should be asso- ciated with a detailed understanding of the behavior of the system components. Data collection for smart grid applications should con- sider the physics of the processes, e.g., for voltage and frequency control applications. Furthermore, sensor data collection can be associated with many induced errors due to sensor aging and losses during data transmission. The associated risks should be well docu- mented at this stage and forwarded to the data correction methods. Preliminary data sampling can help to identify and fix faulty com- ponents. Sampling data from the digital twin [47] is a concern for accountability due to the complexity of the twin system modeling and introduces the digital twin provider as a responsible party.\nFor many grid services, the data is heterogeneous with varying resolutions, mostly asynchronous, and is stored in different for- mats (raw or processed) at different locations [6]. Collected and processed data should be stored throughout service deployment and additional time after commissioning to preserve accountability. Access to the original and training data and clear documentation of the data origin, collection process, and format can enable post- factum analysis of incidents and study the impact of data on the service output. Careful and planned data storage is also required to guarantee an accountable re-training process, especially when data is shared between different stakeholders in the energy sector. A well-defined and extensive Service Level Agreement (SLA) between the parties can be used to introduce responsibility in data utilization. A regulatory framework may support SLAs formulation for explicit usage in smart grids and performance metrics for data quality."}, {"title": "5.2 Data Quality Assurance and Preprocessing", "content": "Poor data quality in the training process can directly affect service performance, and the original reason for such behavior will be hard to identify. Data preprocessing methods are mainly employed to address issues inherited from the data collection process (uncer- tainties [20], errors, lack of data). These methods may be one of the core sources of accountability risks, as they involve techniques that transform the original data. The risks related to removing, adding certain data, or aligning data points impact the developed model and should be carefully identified. By data dispersion, several data sources with different schema or conventions are merged into a single dataset. The differences in data structures and types and the various tools required to enable data integration need special atten- tion. The development methodology should be capable of finding a trade-off between the necessary preprocessing steps and the risk to accountability these introduce due to their complexity and required assumptions. Data dispersion from multiple sources is undesired for accountability preservation in smart grid services, and a unified data collection plan should be introduced. An overview of some of the potential technical processes can be found in Table 4.\nSmart grid data may have hard-interpretable labels depending on the type of service. Each method of data labeling has an impact on accountability. Manual labeling is prone to human error, while automated labeling tools usually include different AI methods. Such labels can inherit all issues from the accountability of the tools [3]. Accountability for this step can be preserved by clearly planning the labeling process and continuous validation and refining through feedback loops. Data leakage and dispersion can threaten the sys- tem's accountability during the filtering and labeling processes. By data leakage, target variables (irrelevant or personal data) are leaked in the training. When leakage happens, the model retains characteristics connected with irrelevant data without explicitly including those features in the model. The accountability of this"}, {"title": "5.3 Model Training and Validation", "content": "After the data collection and preparation phase, the datasets are ready for training. From this point, the algorithm will inherit all the remaining issues in the data. Accountability of the data preparation phase allows access to the used assumptions and methods, espe- cially if the dataset is being used by a different party or reused to develop multiple services. Table 5 presents primary training steps and related risks.\nThe model selection and training are based on the objective and available features. The feature engineering process usually includes extraction, selection, and transformation phases. Accountability risks arise from the feature selection process due to incorrect for- mulation of the objectives and assumptions, resulting in feature extraction and transformation flaws. Improper feature selection techniques can lead to losing important features while eliminating unsuitable ones. It can fail to achieve dimensionality reduction and overlook significant feature correlations. The feature transforma- tion methods may require dividing a feature or merging two or more features to build a new one using various methods. Apply- ing the transformation methods changes the essence of data and may introduce biases. Algorithmic bias appears when mathematical rules highlight specific attributes over others in relation to a target variable [4]. The feature engineering process should be carefully planned and documented to track the steps of the process for the ex-post analysis. Feature selection and labeling also allow methods to approximate the model locally with other interpretable models and understand the influence of features on service output.\nAccountability in the model learning phase should be preserved for three main steps: model selection, training, and hyperparameter selection. Justification of the model selection should be provided concerning the complexity of the problem and the application sce- nario. The increased complexity of the selected algorithm impacts accountability as it deals with sensitive hyperparameters. Tuning re- quires a knowledge of the search space, which is not always achiev- able due to complexity. Further constraints from the deployment environment may also introduce bounds on the hyperparameter selection [42]. Accountability issues arise when the hyperparame- ter's choices and bounds are insufficiently justified. Inaccurately tuned hyperparameters result in overfitting, underfitting, or bias.\nThe training process is initiated after carefully implementing and selecting the hyperparameters, usually in parallel with the per- formance evaluation. Accountability of this step is defined through extensive validation with a feasible selection of the validation met- rics that should cover accuracy, bias, and fairness. Accountability may be compromised if the chosen metrics fail to consider bias issues or detect parameter errors for the training and unseen test sets. However, the test set can further compromise accountability by not being representative or infected with training data. Such risks will be explicitly evaluated in a future use-case study."}, {"title": "6 CONCLUSION", "content": "Additional measures are needed to ensure responsible development and deployment for reliable Al usage in critical infrastructures. This paper analyzes the current AI Act and demonstrates its limited granularity in considering technical aspects of the smart grid, e.g., in safety component definition. A delegated act could include a methodology for supervising critical AI-based services in the energy sector, closing the current gap in the AI Act. An accountability- based framework is suggested as a fundamental part of a delegated act. A literature survey of existing definitions of accountability identifies the most significant accountability dimensions, and sector- specific quantifiable accountability definition for AI-based smart grid service is derived. The quantification of the accountability risks can be narrowed down to each phase of the AI-based service development from planning to commissioning. This work discusses accountability risks for data collection, preprocessing, and model training phases. The future work will consider implementing the discussed accountability preservation in an exemplary AI-based service, underlining the impact of design decisions on operation and the assigning responsibility of different parties."}]}