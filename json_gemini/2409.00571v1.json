{"title": "Enhancing Source Code Security with LLMs: Demystifying The Challenges and Generating Reliable Repairs", "authors": ["Nafis Tanveer Islam", "Joseph Khoury", "Andrew Seong", "Elias Bou-Hard", "Peyman Najafirad"], "abstract": "Abstract\u2014With the recent unprecedented advancements in Ar- tificial Intelligence (AI) computing, progress in Large Language Models (LLMs) is accelerating rapidly, presenting challenges in establishing clear guidelines, particularly in the field of security. Numerous academic and industry efforts have focused on exploring Large Language Models (LLMs) to reliably identify, reason about, and address security vulnerabilities. However, current benchmarks indicate that state-of-the-art LLMs still face challenges in reasoning and require further research to overcome their limitations. That being said, we thoroughly identify and describe three main technical challenges in the security and soft- ware engineering literature that spans the entire LLM workflow, namely; (i) Data Collection and Labeling; (ii) System Design and Learning; and (iii) Performance Evaluation. Building upon these challenges, this paper introduces SecRepair, an instruction- based LLM system designed to reliably identify, describe, and automatically repair vulnerable source code. Our system is accompanied by a list of actionable guides on (i) Data Preparation and Augmentation Techniques; (ii) Selecting and Adapting state- of-the-art LLM Models; (iii) Evaluation Procedures. SecRepair uses a reinforcement learning-based fine-tuning with a semantic reward that caters to the functionality and security aspects of the generated code. Our empirical analysis shows that SecRepair achieves a 12% improvement in security code repair compared to other LLMs when trained using reinforcement learning. Furthermore, we demonstrate the capabilities of SecRepair in generating reliable, functional, and compilable security code repairs against real-world test cases using automated evaluation metrics. Our data and source code are made publicly accessible here\u00b9.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, there has been a remarkable leap in Artificial Intelligence (AI) computing, propelled by industry giants such as NVIDIA, accompanied by notable progress in the research and development of Large Language Models (LLMs). These models are garnering attention across diverse domains, spear- heading advancements in tasks such as generation (e.g., [1], [2], [3]), automation (e.g., [4], [5]), and commercial code auto-"}, {"title": "Limitations of Existing Techniques and Critical Research Problems and Directions.", "content": "Initially, pioneering efforts includ- ing Re-DeBug [17], VUDDY [18], MVP [19], and Movery [20] have focused on identifying Vulnerable Code Clones (VCC) using parsing, fingerprinting, and signature-based tech- niques. However, these efforts did not explore the potential for describing and repairing security vulnerabilities in code, primarily due to the limited advancements in Al computing and LLM technologies at that time. Recent endeavors (e.g., [21], [22], [23]) have underscored the advantages of leveraging pre-trained LLM models for automated detection and repair of security vulnerabilities. Furthermore, other research initiatives (e.g., VulRepair [24] and AIBUGHUNTER [25]) have made notable progress in vulnerability detection by utilizing pre- trained LLM models. However, these initiatives still encounter technical limitations in terms of their data and model pipelines. As a result, their capacity to offer comprehensive vulnerability analysis and customized security code repairs for software developers is hindered. Given its nascent stages, the application of state-of-the-art LLM models is currently under scrutiny for their reliability in identifying and reasoning about security vul- nerabilities [26]. Nevertheless, the significant impact of code vulnerabilities and their potential ramifications underscores the critical need to assist software developers in mitigating these risks. This involves leveraging advancements in Al computing and LLM models, which exhibit considerable potential but currently lack comprehensive guidelines and understanding.\nDrawing upon the systematic analysis conducted by Arp et al. on common pitfalls in the machine learning workflow within computer security [27], as well as our extensive inves- tigation of 19 recent top-tier security and software engineering papers, we have identified key challenges that arise when integrating and applying LLMs for the purpose of detecting and repairing security vulnerabilities in source code. These challenges encompass the entire LLM workflow, beginning with the data collection phase and extending through the subsequent phases, including model training, evaluation, and ultimately, deployment.\nFirstly, defining an accurate threat model, along with incorporating relevant real-world Common Weakness Enumer- ation (CWEs), is crucial for establishing the objectives of the LLM pipeline development. Additionally, much of the recent research fails to clearly define how the developed LLM capabilities will be utilized, identify their intended audiences, or specify the types of attacks they aim to address.\nSecondly, a prominent challenge in this domain is the absence or improper utilization of real-world vulnerability datasets, which serve as a fundamental pillar for the devel-"}, {"title": "Our Contribution.", "content": "Given the high stakes and the absence of clear guidelines, we aim to: (i) demystify the technical LLM workflow challenges encountered in the literature. We brake it down into three main categories; data collection and labeling; system design and learning; and performance evaluation. We provide a thorough discussion of each category and highlight specific security limitations associated with each challenge; (ii) propose SecRepair, an instruction-based LLM that utilizes reinforcement learning-based fine-tuning. It incorporates a se- mantic reward system to enable the identification, description, and repair of code security vulnerabilities. We accompanied SecRepair with a set of guidelines that serve as actionable recommendations to enhance the security posture of source code with LLMs; (iii) evaluate SecRepair with real-world datasets.\nIn summary, the contributions of this paper are as follows:\n\u2022 Threat Model Devising. We devise a threat model that precisely outlines the goals of LLMs in enhancing source code security. Particularly, we focus on real- world vulnerability outbreaks, which offer valuable insights into the usage, target audience, and types of attacks that need to be addressed (check \u00a7II).\n\u2022 Challenge Identification. By following a systematic analysis proposed in [27] and thoroughly reviewing 19 top-tier security and software engineering papers, we identify key challenges and limitations in the literature. These challenges span over all stages of the LLM workflow and have serious security implications, impacting the reliability of LLMs in reasoning about security vulnerabilities (check \u00a7III).\n\u2022 Guideline Formulation. We formulate an exhaustive list of actionable guidelines to correctly employ and augment existing vulnerability datasets and carefully select and adapt state-of-the-art LLMs to enhance source code security (check \u00a7IV).\n\u2022 SecRepair. Based on our established guidelines, we introduce SecRepair an LLM for the purpose of identifying, describing, and providing security repairs. Our results demonstrate that SecRepair achieves a significant improvement of 12% in Cosine similarity and CodeBERTScore values for vulnerability repair when using reinforcement learning compared to su- pervised fine-tuning."}, {"title": "II. THREAT MODEL", "content": "Recently, a security vulnerability known as CVE-2024-3094 (i.e., CVSS score 10.0 critical) was discovered in the open-source library XZ Utils [31]. This vulnerability, through a backdoor, enables remote code execution (RCE), which resulted from a rogue maintainer introducing malicious code disguised as test files into the library [31], [32], [33]. By going unnoticed, this vulnerability could have had serious consequences on a significant number of servers on the internet, potentially leading to unauthorized access, data breaches, and other malicious activities.\nOn another hand, the uncontrollable rise of neural language modeling and state-of-the-art Large Language Model (LLM)- based auto-completion tools (e.g., GitHub Copilot [6], TabNine [7], IntelliCode [8], and ChatGPT [9]) has introduced a new set of deficiencies, primarily stemming from generating weak programming implementations, further compounding the challenge of ensuring secure and reliable software [34], [35], [36], [37].\nAccordingly, our threat model focuses on the code in- jection attack, which consists of injecting code that is then interpreted/executed by the application [38]. These attacks originates from (i) a potential cyber threat actor, as seen in the case of the XZ vulnerability outbreak, and (ii) an inexperienced software developer who may inadvertently introduce vulnerabilities while using auto-completion tools or making mistakes. \nTo this end, our work is centered around empowering soft- ware developers and security experts with reliable and trust- worthy LLM capabilities to identify, describe, and automate security vulnerability repair.\nOur threat model primarily focuses on code-based vulner- abilities that occur at the function level of a program, namely, intra-procedural analysis. The vulnerability analysis examined in this paper covers various types of CWEs. Firstly, we address vulnerabilities that can lead to program crashes by inducing"}, {"title": "III. CHALLENGES IN SOURCE CODE SECURITY ANALYSIS USING LLM MODELS", "content": "Despite the considerable success and unprecedented capa- bilities demonstrated across diverse domains, the application of LLM models is still in its nascent stages. Further research and development are imperative to establish their reliability, especially in the context of source code vulnerability detection and repair. Notably, this aspect has been extensively studied and evaluated by Ullah et al. [26], which encompasses multiple vulnerability datasets, state-of-the-art LLM models, and real-world CWE test cases. In this section, we will discuss three primary technical challenges with their respective security implications associated with the research and development of LLM pipelines (i.e., datasets, models, and evaluations) for detecting and repairing security vulnerabilities in source code. Our identification of these challenges is based on the analysis of learning-based security systems presented in [27], and involves extracting deficiencies and weak points rooted in top-tier security and software engineering papers on the topic."}, {"title": "Challenge 1 (C1): Data Collection and Labeling", "content": "Description. Technically speaking, the trainable weights of LLM models are shaped by the data and fitted to represent specific relationships and contextual understanding accurately. Thus, the data serves as the foundation for the model's reason- ing and generation of content. For optimal performance in a learning system, including application-specific LLM models, the utmost importance lies in the elements of data source, quality, quantity, relevance, label, and distribution. In the realm of security, ensuring these data properties become even more complex due to the scarcity of data caused by privacy and security concerns, as well as the manual efforts required by ex- perts for generation, collection, and labeling. As such, this will ultimately lead to data that suffers from (i) label inaccuracy; (ii) data leakage; and (iii) sampling bias. In studies such as [39] and [40], the determination of vulnerable and non-vulnerable code samples were not made by security experts. Instead, it relied on vulnerability keywords and regular expressions proposed by [41], which can not guarantee accurate labeling. Additionally, we observed data leakage in multiple vulnera- bility datasets (e.g., MVD [42], Devign [39] and ReVeal [40]) where the function name in the code samples leaked the CWE number, which reflects the label of the data instance. Other works, such as [40], suffer from a significant imbalance between vulnerable and non-vulnerable data points and real vs synthetically generated vulnerabilities [28].\nSecurity Implications. Using LLM models trained on in- accurate data will undoubtedly result in inaccurately identi- fying vulnerabilities in input source code. In other words, encountering false positives and negatives when identifying vulnerabilities and classifying CWEs as demonstrated in [26]. Additionally, building upon data with leaked label will result into detrimental effect on the LLM model's ability to genuinely understand the code's objectives as demonstrated in [43]. Moreover, the challenge of imbalanced data can inherently lead to bias in determining whether a code sample is vulnerable or not. Not to mention the problem of synthetic vulnerable code, which imposes serious reasoning limitations on LLM models in practice when they are exposed to emerging top CWEs and real-world zero-day vulnerabilities that have never been seen before. To ensure a higher level of security in software systems, it is crucial to consider the security implications of all data- related challenges. This is particularly important when utilizing poorly designed and implemented LLM-based capabilities for security code identification and repair during the development of critical software systems."}, {"title": "Challenge 2 (C2): System Design and Learning", "content": "Description. Numerous LLM models are proliferating, each with its own unique design choices, technicalities, and intended usage. As such, selecting the correct LLM and ensuring a rigorous training process is paramount in building a reliable LLM for security vulnerability identification and repair. In the pursuit of enhancing source code security for software developers, it is vital to enable a chat-based assistant, namely, instruct-based LLM models that can reliably identify, describe, and autonomously repair vulnerabilities, as expounded upon in \u00a7II. Initially, pre-instruct LLMs, such as T5 [44], CodeT5 [45], CodeT5+ [46], and CodeRL [47], solely relied on the input to generate responses, without the ability to respond to specific user instructions. In contrast, instruction-based gener- ative models such as GPT-4 [48], Gemini [49], and trainable models including CodeLLaMa [50], CodeGen2 [51], LLaMa 3 [52], and Mistral [53] are trained with instructions, making them more versatile for general-purpose use. However, the training methodologies employed by these models, such as supervised fine-tuning processes (e.g., LLaMa [52]) and su- pervised fine-tuning with reinforcement learning from human feedback (e.g., InstructGPT [54]), do not directly fit the down task of vulnerability detection, description, and repair. Without careful consideration, several recent works including VulRepair [24] and others (e.g., [55]) have predominately relied on supervised fine-tuning approaches that utilize cross- entropy loss as the sole criterion for training the model to generate the correct output. Moreover, the dearth of real- world vulnerabilities in comparison to synthetic vulnerability datasets as seen in [56], [26] and generated by SARD [57] and NIST [58] using various LLM models (e.g., GPT-4 [48], Falcon [59], among others) exacerbate the problem of data snooping manifested by training and testing data not available in practice.\nSecurity Implications. The sole reliance on supervised fine- tuning in LLM models is fundamentally inadequate when dealing with programming languages that have specific syntax and semantics. This approach fails to address the potential gen- eration of non-compilable code, functionally incorrect code, or code that is susceptible to security vulnerabilities as demon- strated in [60]. This focus on cross-entropy loss overlooks the necessity for syntactical, semantic, and functional accuracy, leading to unreliable and potentially dangerous LLM models in real-world applications. Furthermore, these models often lack robust mechanisms to ensure that the generated code adheres to best practices in security and functionality. The absence of comprehensive evaluation methods compounds this issue, as there is no rigorous assessment of the model's ability to generate secure, functional code. This glaring oversight highlights the urgent need for an integrated training process that seamlessly embeds syntactical, semantic, and functional information, ensuring that LLMs can produce high-quality, secure code repair suitable for real-world applications. More- over, there is the issue of snooping on synthetic vulnerable data, which can lead to optimistic results. While synthetic vulnerabilities are generated based on known data distribution, they often fail in practice when confronted with previously unseen vulnerabilities."}, {"title": "Challenge 3 (C3): Performance Evaluation", "content": "Description. Measuring the performance of generative tasks, particularly when it comes to generating functional or secu- rity repairs for source code, presents significant challenges. Numerous efforts (e.g., [16], [23], [60], and others) have been undertaken to utilize state-of-the-art LLMs for tasks such as code security repair. However, the evaluation methods employed in these studies are still questionable and lack robustness. To illustrate, studies conducted by [16], [26], and [23] showcase the utilization of small sample sizes, typically comprising fewer than 500 instances, for the purpose of code security repair. This limited sample size has proven advanta- geous for facilitating manual evaluation. However, the case is different when dealing with large-scale repair tasks involving over 100,000 samples. As presented in [60], [55], [24], [21], [61], and [62], automatic evaluation becomes increasingly challenging. Two primary methods have been identified for evaluating such works. Firstly, when input test cases for a given function are available, work such as [61] utilizes the Pass@K metric proposed in [63]. Secondly, in the absence of test cases, studies such as [62], [24], and [55] resort to Exact Match, BLEU [64], and Rouge-L [65] scores for automated evaluation. Additionally, as part of the automated evaluation process, [26] employed Cosine Similarity and GPT-4 as an LLM to evaluate generated outcomes.\nSecurity Implications. Despite these efforts, these automated metrics are fundamentally inadequate. They offer a superficial measure of similarity but fail to guarantee the functionality or security correctness of the generated security code repair. Moreover, they do not ensure that the code is even compilable, a critical prerequisite for execution. This lack of reliable evaluation metrics leads to the risk of deploying non-functional or insecure code, undermining the entire purpose of generative models for code repair."}, {"title": "IV. SE CREPAIR: PROPOSED APPROACH VIA ACTIONABLE GUIDELINES", "content": "Problem Formulation. Each function is defined as f; if the function is vulnerable, it is defined as $f_{vul}$ and the correspond- ing ground truth repaired function as $f_{rep}$. The vulnerability description is denoted as D. The vulnerable function $f_{vul}$ is converted to tokens denoted as T. Our methodology utilizes a multitask approach encompassing vulnerability identification, repair, and description. Initially, it identifies vulnerable code V as a binary classification task, followed by repairing the vulnerability $f_{rep}$ and, subsequently, generating a description D of the vulnerability.\nLet us consider the task of vulnerability identification and repair as our first step. For this set of tasks, A function f could be vulnerable or non-vulnerable. It is used as the input to the model with an instruction to generate V and $f_{rep}$ where V is a binary variable indicating the existence of security vulnerability using the words \u201cYES\u201d and \u201cNO\u201d in the input function, and $f_{rep}$ is the repaired function. For the tasks of describing the vulnerability, we define D as the vulnerability description. Following the instructions, our system generates D, where D provides a detailed description of the identified vulnerability. When the task is to repair the vulnerability $f_{rep}$, the input is the vulnerable function $f_{vul}$ with instruction. Here, we denote y as the generated by the model. The instruction changes based on whether we want to identify, describe, or repair the function. \nIn this study, we aim to address three research questions (RQs) regarding the effectiveness and capabilities of our pro- posed system, SecRepair.\nRQ1: Can we automatically identify code vulnerability of static source code?\nRQ2: Can we comprehensively describe the code vulner- ability of the vulnerable code\nRQ3: Can we optimize using reinforcement learning and repair the security issue in a vulnerable code?\nBy addressing these research questions, we aim to ad- vance cybersecurity techniques by adhering to our proposed guidelines in the next Subsections. These guidelines ensure balanced and obfuscated data collection, aligning training data and metrics with real-world applications, and implementing rigorous evaluation to guarantee syntactic, functional, and security correctness.\nSecRepair. In this section, we introduce SecRepair, a novel code repair system designed to address the critical challenges in static code vulnerability analysis. Our approach is under- pinned by three fundamental Guidelines: G1:Data Preparation and Augmentation Techniques, G2: Selecting and Adapting Models, and G3: Evaluation Procedures."}, {"title": "Guideline 1 (G1): Data Preparation and Augmentation Techniques", "content": "Our data collection strategy prioritizes balanced data from diverse sources to mitigate issues such as Data Snooping, Label Inaccuracy, and Sampling Bias following G1. Additionally, we ensure the obfuscation of data to prevent unintended data leakage. This rigorous approach to data collection lays a solid foundation for the subsequent training and evaluation stages, guaranteeing a more robust and unbiased dataset.\nThis study introduces InstructVul, the first instruction- based dataset designed for a vulnerability identifica- tion and repair system, including vulnerability description."}, {"title": "Formal Definition.", "content": "Each data entity contains three compo- nents: instruction, context input, and output. The instruction is denoted by I, which is a statement stating one of the three tasks demonstrated in Figure 1. The second component context input Ci is a supplement for each instruction, which is either the vulnerable and non-vulnerable function denoted as $f_{vul}$ $f_{rep}$. The third component is the output y, the expected LLM- generated response."}, {"title": "Code Obfuscation.", "content": "In order to extract all function definitions and variable names from the functions, we used the S-expression of Tree-Sitter [68]. An S-expression or symbolic expression is a method to represent the Abstract Syntax Tree (AST) of source code. Each node in the tree is represented as a list, starting with the node type followed by its children, which can be terminal tokens or further nested lists. This format provides a clear and concise textual representation of the code's syntactic structure. \nTo extract the functions and the variables, we used the following S-expression, (function_definition)@func \u2013 def in Listing 1 to replace function and variable names. We replace the user-defined function name and variable with random strings. For the same function name or the variable names, the random strings remain the same."}, {"title": "Description Generation.", "content": "The functions extracted from MVD predominantly include comments provided by software secu- rity experts. However, these comments are often incomplete, focusing on specific statements, and some crucial vulnerable lines have multiple layers of comments. To address this, we employ tree-sitter [68] to create a method in Listing 2 for extracting comments in C/C++. Using this method, we extract the comments from within these functions. We then present the source code, stripped of comments, alongside the extracted comments to GPT-4, prompting it to generate a clear and comprehensive description of the code."}, {"title": "Guideline 2 (G2): Selecting and Adapting Models", "content": "The training phase of SecRepair involves choosing the cor- rect model, exposing it to data similar to what it will encounter during fine-tuning, and ensuring alignment between training and real-world application. We employ appropriate reward metrics tailored to the nature of the data, enhancing the model's learning process. This principle ensures that the trained model can effectively handle the intricacies of code repair tasks, maintaining high standards of accuracy and performance.\nCausal Decoder Model. In order to adhere to G2 in our proposed vulnerability analysis system, SecRepair, we use a unidirectional causal-decoder model called CodeLLaMa. CodeLLaMa was pre-trained to generate code based on a given instruction. Unlike regular encoder-decoder-based LLM architectures, which exhibit bidirectional properties by reading tokens forward and backward, our model focuses solely on the unidirectional aspect. It emphasizes the significance of previous tokens that are responsible for the vulnerability. By utilizing the responsible tokens for vulnerability, our causal LLM generates code repair and offers enhanced generalization capability [69]. Furthermore, causal decoder models eliminate the need for an encoder layer, resulting in faster inference times and reduced memory and energy footprint.\nVulnerability Identification. A prior fundamental step that needs to be completed for code vulnerability repair is vul- nerability identification. Initially, we concatenate and convert the instruction I and the input function f into a sequence of tokens, $t_1, t_2, ...t_p \\in T$ and $y_1, y_2, ...Y_q \\in y$, which is the set of output tokens respectively. Then we combine both into a unique sequence $W_1, W_2, ...W_{p+q} = (t_1, tp, $, y_1, ..., Ya) correspondingly. In this definition, \u201c$\u201d is a special token used as a separator between the input tokens into the model and the output tokens generated by the model, p is the total number of input tokens, and q is the total number of output tokens.\nThe instruction-based supervised fine-tuning method is employed to classify vulnerabilities as binary for identification purposes. The model outputs a \u201cYES\u201d or \u201cNO\u201d response indicating the presence of a vulnerability. As we utilize a generative model for vulnerability analysis, we associate each function with a specific instruction for identifying vulnera- bilities. During training, the model is trained using a Cross- entropy loss function for identification purposes.\nReinforcement Learning for Security Repair. Given the causal decoder architecture, our model is forced to predict the next code token, and the model cannot overlook future tokens by looking at the next token during output generation. The model is provided with the input sequence $t_i \\in T$ during inference, which auto-regressively generates the output, $Y_i$.\nHowever, generating repaired code autoregressively in a supervised fine-tuning method has some challenges. In an autoregressive generative model, the model generates a token from a pool of previous tokens, which hinders the true semantic understanding of code."}, {"title": "a) Policy Optimization:", "content": "The reinforcement learning objective is to find the optimal policy by maximizing the reward metric by adding security measures while ensuring functionality is reserved. If the original vulnerable code is $f_{vul}$, the repaired code generated by the model is $f_{r ep}$, and the ground truth repaired code is $f_{rep}$. As such, we calculate the policy optimization $r_e$, using the following equation:\n$L(r_e) = log(o(r_e(f_{vul}, f_{rep}) \u2014 r_o(f_{vul}, f_{rep}))$ (1)\nwhere $r_e(f_{vul}, f_{rep})$ and $r_e(f_{vul}, f_{rep})$ is the scalar output of the reward model for the vulnerable code $f_{vul}$. Here o is an activation function, and O is a learnable parameter."}, {"title": "Guideline 3 (G3): Evaluation Procedures", "content": "For SecRepair, the evaluation process is meticulously designed based on G3 to ensure that the generated code is syntactically correct, with functionally security. We incorporate comprehensive evaluation metrics that assess the generated code's structural and security attributes. This holistic evalua- tion framework ensures that SecRepair delivers reliable and secure code repairs, addressing the limitations of existing evaluation methodologies.\nWe use F1, Precision, Recall, and Accuracy for vulner- ability identification tasks. Since a generative model is used for vulnerability description and repair generation, metrics like BLEU, Rouge-L, Cosine Similarity, and CodeBERTScore are used. Moreover, for code vulnerability and code repair"}, {"title": "BLEU Score.", "content": "The BLEU [64] score is a syntax-based way of evaluating machine-generated text between 0 and 1. It is a reference-based metric and captures token-based n-gram Similarity."}, {"title": "Cosine Similarity.", "content": "We aim to evaluate the semantic simi- larity between the embeddings of LLM-generated text and the ground truth. Formally, given a set of token sequences gener- ated by the LLM description $D = {w_1, w_2, ..., w_n}$ and the ground truth reference description $D = {w^1, w^2, ..., W^n}$, we use the sentence encoder E to produce embeddings E(D) = {$e_{u1}$, $e_{u2}$, ..., $e_{un}$} and E(D) = {$e_{w^1}$, $e_{w^2}$, ..., $e_{w^n}$}.\n$e_D$ = $\\frac{1}{|D|} \\sum_{i=1}^m e_{wi}, D = \\frac{1}{|D|} \\sum_{j=1}^n e_{wj}$\nTherefore, we calculate the similarity score as,\n$sim(D, D) = \\frac{E_D E_D}{\\|E_D\\| \\cdot \\|E_D\\|}$ (3)\n(4)"}, {"title": "V. EXPERIMENTS AND DISCUSSIONS", "content": "This section uses the evaluation metrics from the previous section to answer all the research questions (RQs) we defined before."}, {"title": "A. Results and Discussions", "content": "Our datasets were randomly shuffled across all of our experiments and divided into 80/10/10 for training, validation, and testing. The pretrained CodeLLaMa model with 40 layers of decoders was used to build our proposed SercRepair.\nThe model was trained for three epochs with a maximum token length of 512 while the learning rate was set to 2e-5 with a batch size of 2 for our 7B parameter model. A beam size of 4 was used for the generation task, and a temperature value of 0.5 was used for optimal performance. The model"}, {"title": "RQ1: Can we automatically identify code vulnerability of static source code?", "content": "To ensure accurate vulnerability repair, a robust code vulnerability analysis system's primary task is to identify vulnerabilities with higher accuracy and fewer false positives and false negatives. In order to identify vulnerable code using a generative model, we compare the exact matching of the generated output. If the input code is vulnerable, then the model generates the token \"YES\", and otherwise, it gener- ates the token \u201cNO\u201d. Therefore, when the generated token \"YES/NO\" matches with the ground truth, it is considered a correct prediction of the vulnerability.\nDiscussion. Our instruction-based vulnerability identification system aims to determine the capability to identify vulnerable code and compare the result with the existing models. More- over, we compare our RL technique with SFT to show the difference in the training process. We compare our proposed SecRepair [50] trained using SFT. We compare with 4 other state-of-the-arts LLM CodeGen2 [51], Mistral [53], StarCoder [72] and LLaMa 3 [52]. We used accuracy, precision, recall and F1 score to measure the performance of the model. Furthermore, we also tested the model on Acc (Vul) and Acc (Ben), where we only used either the vulnerable or the non- vulnerable function as the input to the model."}, {"title": "RQ2: Can we comprehensively describe the code vulnerabil- ity of the vulnerable code?", "content": "While vulnerability identification can confirm the presence of vulnerabilities in the code, it often doesn't specify the vulnerabilities or how the vulnerability affects the code. To assist developers in properly analyzing code vulnerabilities, we aim to provide detailed descriptions of the identified vulnerabilities."}, {"title": "RQ3: Can we optimize using reinforcement learning and repair the security issue in a vulnerable code?", "content": "In order to repair the vulnerability, we trained our model with instructions to repair the vulnerability. We trained with two different processes: supervised fine-tuning and reinforce- ment learning."}, {"title": "A. Ablation Studies on Hyperparameters", "content": "This section explores the impact of two components, temperature and beam size, on generative models. Higher temperature leads to generating tokens with lower probabilistic values. Meanwhile, increasing the beam size in beam search enables tracking the top-k probable sequences and extending them with next-token probabilities."}, {"title": "Temperature.", "content": "shows how the beam BLEU and Rouge-L score change with the change in temperature. In order to test the effect of temperature, we kept the beam search at its default value and tested the outcome with temperature values of 0.0, 0.25, 0.50, 0.75, and 1.0. We observe in that the scores are the lowest when the temperature is 0.0 and begin to increase when the value is at its sweet spot at 0.50. However, we again see a decrease in the score when the temperature is close to 1."}, {"title": "Beam Search.", "content": "Similarly, in when we keep the temperature value constant and increase the beam value to 1, 2, 4, 6, and 8. We see slight improvements in the results as the beam size increases. However, this improvement comes with some drawbacks. The yellow line shows the relative time converted to a range between 0 and 1 for convenience. We see that the inference time increases almost exponentially with the increase in beam values. Here, we converted the time values within the range of 0 to 1 for the convenience of displaying them in a single graph. While the increase of beam value improves the performance, higher inference time makes this an inconvenience for the developers to use in real time."}, {"title": "VII. RELATED WORKS", "content": "a) Advancements in Vulnerability Detection: Vulnera- bility detection methods have evolved from traditional machine learning (ML) techniques to more adaptable deep learning- based solutions with universal applicability. Lin et al. [73] emphasize the potential of ML methods for automated vulner- ability discovery. However, deep learning solutions, including VulDeepecker [74], and uVulDeepecker [42] rely heavily on feature engineering for vulnerability detection. In contrast with these methods, Devign [39], VulBERTa [75] and REVEAL [40] employ Code Property Graph-based techniques (CPG) [76] for vulnerability detection. Abstract Syntax Tree (AST)- based methods, proposed by Bilgin et al. [77] and others [78], [79], [80] retain syntactic information during detection."}, {"title": "b) Code Vulnerability with LLMs:", "content": "With the rapid progress of LLMs, their application in code development- related tasks has significantly increased. GitHub Copilot [6] has initiated a trend towards AI-assisted software development [34], [35], [36]. Notably, Codex [63] introduced a docstring- based approach where developers input instructions as doc- strings, and LLMs generate corresponding code. However, most LLMs are less concerned about the security issues that come with programming languages. Pearce [23] examines LLMs' zero-shot vulnerability repairing capabilities and the bugs' challenges. Their experimental results showed that, al- though LLMs can generate bug fixes, they need a specifically constructed prompt to fix a particular bug. SVEN [84] proposes an adversarial technique to evaluate LLMs. They propose to generate safer code by leveraging property-specific continuous vectors to guide program generation. Moreover, [85], [86] and [87] provided an extensive study on the effectiveness of autocompletion by integrating them with different IDEs and analyzing their outcome. However, Sandoval et al. [87] suggested that LLMs help generate more functional codes with increased security issues."}, {"title": "c) Vulnerability Repair:", "content": "Repairing programs poses a significant challenge as it requires identifying the vulnerable line and generating a suitable compilable line to fix the vulnerability. Previous approaches to generating vulnerability patches have involved specifying safety properties manually [88], such as preventing a program from accessing memory beyond its boundaries. Other approaches, like those proposed by Zhang et al. [89], aim to identify patch invariants or vulnerable code locations and use a set of templates to generate a repair. Recent advancements, such as Vrepair [90] and [21], employ transformer-based transfer learning methods to fix vulnerabilities in real-world programs. Similarly, VulRepair [24] utilizes pre-trained models like CodeT5 [45] and BPE tokenizer [91] for vulnerability repair. Zhang et al. [92] have further investigated the benefits and limitations of pre-trained"}, {"title": "d) LLM Training and Evaluation:", "content": "Recent advance- ments in large language model training [52", "54": "demonstrate that reinforcement learning (RL) is highly effective in guiding large language"}]}