{"title": "A Split-Window Transformer for Multi-Model Sequence Spammer Detection using Multi-Model Variational Autoencoder", "authors": ["Zhou Yang", "Yucai Pang", "Hongbo Yin", "Yunpeng Xiao"], "abstract": "This paper introduces a new Transformer, called MS2Dformer, that can be used as a generalized backbone for multi-modal sequence spammer detection. Spammer detection is a complex multi-modal task, thus the challenges of applying Transformer are two-fold. Firstly, complex multi-modal noisy information about users can interfere with feature mining. Secondly, the long sequence of users' historical behaviors also puts a huge GPU memory pressure on the attention computation. To solve these problems, we first design a user behavior Tokenization algorithm based on the multi-modal variational autoencoder (MVAE). Subsequently, a hierarchical split-window multi-head attention (SW/W-MHA) mechanism is proposed. The split-window strategy transforms the ultra-long sequences hierarchically into a combination of intra-window short-term and inter-window overall attention. Pre-trained on the public datasets, MS2Dformer's performance far exceeds the previous state of the art. The experiments demonstrate MS2Dformer's ability to act as a backbone.", "sections": [{"title": "I. INTRODUCTION", "content": "Spammers are important promoters of directing social opinion. Users who create spam or fake news for a long time are defined as spammers. Information dissemination is usually carried out by graph structures. Therefore, the graph neural network (GNN) becomes a generalized backbone for identifying spammers. Subsequently, GNN has been further developed. By combining the ideas of Convolution (GCN [1]), Attention (GAT [2], [3] and Graph Transformer [4]), and Sampling (Graph-SAGE [5]), the performance of GNN is further improved.\nOn the other hand, scholars believe that spammers usually use sudden and short-term coherent behaviors to guide public opinion. Subsequently, they make quick disguises and gather followers with the help of behaviors such as spreading positive energy to prepare for the subsequent guidance. Meanwhile, they also use outdated information as a vehicle and add new claims to guide public opinion. Moreover, the interval between outdated information and current activities is very long (see Fig. 9). Therefore, sequence modeling strategies have also recently been applied to the current task. Unlike the GNN backbone, this strategy focuses on the hidden correlations between users' historical long-term and short-term behaviors. If a single historical behavior is considered a Token, the entire problem of modeling historical behavioral sequences can be transformed into the problem of natural language processing (NLP). In the field of NLP, Transformer is designed for sequence modeling and translation tasks, and its distinguishing feature is the use of attention to model long-term or short-term dependencies in the data. This is highly consistent with the sequence modeling needs of the task at hand."}, {"title": "II. RELATED WORK", "content": "Spammers guide public opinion by sending spam (also called fake news or rumors by some scholars). Therefore, the academic definition of spammer detection contains two sub-tasks: short-term burst and long-term hidden user detection. In the former, several short-term behaviors of a user are combined to identify whether the user is a spammer or not. This task is usually called spam (fake news or rumor) detection. In this case, the model is usually constructed by combining the spread space and the GNN backbone because the user has fewer short-term behaviors. The latter is more suitable for identifying long-term potential users. Meanwhile, it can also consider the task of sudden user detection. Moreover, with the arithmetic power improvement, multi-modal feature mining is combined in these two sub-tasks.\nGNN Backbone-Based Models: Social diffusion processes are usually carried out in graphs. Therefore, GNN-based models are popularly used for the task at hand. For instance, Bian et al. [16] used top-down and bottom-up bi-directional GCN models to solve the task of fake news recognition. Subsequently, Wei et al. [17] introduced randomization theory based on Bian et al. Consequently, this strategy dramatically enhances the generalizability of the fake news recognition model. Meanwhile, the GNN backbone incorporating Markov fields [18] was also applied to the task of fake maker detection. It has been proven that the GNN-based model has reached an almost unsurpassable performance in terms of recognition accuracy. However, as the historical user behavior increases, i.e., when identifying long-hidden spammers, the GNN-based model consumes a large amount of GPU memory to derive suspicious behaviors. With hundreds of millions of social behaviors every day, models must be built with GPU memory in mind and ideally run on consumer GPUs. Therefore, the GNN backbone is not the most effective in latent spammer detection against the background of ultra-long historical be-havioral sequences.\nSequence-Based Models: Social network spread space contains graph structure and temporal sequence information. Therefore, some researchers try constructing a detection model using a sequence modeling strategy. By combining temporal features, Yang et al. [19] constructed a spreading audio based on emotional entropy. Subsequently, fake news is recognized with the help of audio classification techniques. Similarly, Ma et al. [20] proposed a temporal feature-based modeling strategy for propagated response information and identified the fake news with the help of RNN (Recurrent Neural Network). Subsequently, Ma et al. [21] constructed a GAN (Generative Adversarial Network) style model based on TD-RvNN [20]. In this case, the Transformer model is used to model time-propagated sequences, and it is used as a generator. Subsequently, the RNN model is introduced as a discriminator to identify fake news. At this point, the authors mainly address modeling the short-term behavior-derived spread space. Therefore, they used the well-established Transformer architecture of the NLP domain that supports 512 tokens. Their model removes behaviors exceeding 512 tokens when confronted with ultra-long user sequences. Meanwhile, temporal spreading graphs were also applied. For instance, Sun et al. [22] mined the structural information of the propagation subspace based on GCN models. Subsequently, a bidirectional fusion strategy for structural features of the spread subspace was constructed based on temporal features. They did not address the training and deployment of temporal GNN backbone networks on consumer GPU platforms.\nMulti-Modal Models: With the rapid development of com-puter and communication technologies, online social behav-iors are no longer limited to text models. Therefore, multi-modal data mining and cross-modal alignment have become a hot topic in current research. In short-term burst detection, researchers have focused on the multi-modal domain. For instance, Wang et al. [23] introduced adversarial learning to align cross-modal features. Zhang et al. [24] introduced a reinforcement learning strategy to learn cross-modal features. However, multi-modal features have not been widely used in the long-term hidden user detection sub-task. For instance, Qu et al. [25] considered using a multi-modal modeling strategy. However, they only transformed the text into three channels of similar visual information, not accurate multi-modal data. To this end, we construct a generalized backbone for multi-modal spammer detection, called MS2Dformer, from a sequence modeling perspective. Firstly, two-channel feature mining and alignment are constructed using multi-modal variational auto-encoding. Subsequently, sequence features are deeply quanti-fied based on a hierarchical split-window attention mechanism."}, {"title": "III. PROBLEM DEFINITION", "content": "Spammer detection is a typically multi-modal and complex task. Therefore, the input sequence data for the MS2Dformer model is defined in this section starting from multi-modal feature extraction of text and images.\nDefinition 1. Text modal feature T\u03c5\nAs shown in Fig. 1, the data from the text model in the user's historical behavior often provides very important data. Therefore, in this section, a pre-trained BERT2 model with only an encoder structure is used to embed the text. The equation is shown as follows:\n$T^{\\circledR} = BERT(CLST + T) \\in R^{768}$ (1)\nwhere T represents the text data. In the NLP domain, the textual data is concatenated with the \"CLS\" token at the beginning of the sentence. Thus, CLST represents this token. \u03a4\u03c5 \u2208 R768 represents the embedded representation of the \u201cCLS\" token encoded by the pre-trained BERT model.\nDefinition 2. Image modal feature I\nGuiding public opinion through textual modalities alone can cause the target group to trigger a crisis of confidence. Ultimately, it leads to a failure of provocation. Therefore, a spammer may provide multi-modal information, such as edited images, to testify his viewpoint. Thus, while considering text features, image features are equally indispensable. For this purpose, a pre-trained ViT\u00b3 model is used to extract image features. The equation is shown as follows:\n$I^{\\circledR} = ViT(I) \\in R^{768}$ (2)\nwhere I represents the input image data. I\u00ba \u2208 R768 represents the embedded representation of the \u201cCLS\u201d token encoded by the pre-trained ViT model. Because ViT adds \"CLS\" tokens inside the model, they are not represented in Eq. (2).\nDefinition 3. User Historical Behavior Sequence S = {(T, I), ..., (T, I)}\nThis paper proposes a serialized model for spammer detec-tion. Therefore, it is first necessary to transform a sequence of historical user behaviors U = {bo, ..., b} into a standard representation that the model can process. As shown in Fig. 1, any behavior in the sequence contains both text and image modal data, i.e., b\u2081 = (Ti, Ii), and i \u2208 [0,1]. In particular, a history behavior bi exists that fills an empty image when it does not contain image data. Subsequently, the sequence of history behaviors {bo, ..., b} into standard form. The equation is shown as follows:\n$S = BERT({To, ..., T\u2081}) and ViT({Io, ..., I1})\n= {(T\u00b0\u00b0, I\u00ba), ..., (T, I\u00ee)} \u2208 \u20a8l\u00d72\u00d7768$ (3)\nwhere S\u2208 Rl\u00d72\u00d7768 represents the standard sequence of inputs for user U. I represents the length of the sequence supported by the model. The user's real behavior sequence will be truncated when its length exceeds l. On the other hand, insufficient sequences supplement the empty behaviors. The effect of these empty behaviors is eliminated in the model by the MASK mechanism. Meanwhile, to accelerate the extraction of multi-modal features, BERT({To, ..., T\u2081}) and ViT({Io, ..., I\u2081}) strategies are employed to parallelize the extraction of features.\nB. Problem Formulation\nTo solve the problem of serialized multi-modal spammer detection mathematically, we propose a new Transformer, called MS2Dformer. The model models this complex problem in four separate stages. Thus, the overall representation of the model is as follows:\n$U = {bo, ..., bi}\nbi = (Ti, Ii)\n} \u21d2 MS2Dformer \u21d2 P{s,n | nuser}$ (4)\n1) Model Input:\nThe input data for the MS2Dformer model is shown as follows:\n\u2022 The sequence of historical user behaviors U = {bo, ..., bi}. l represents the length of the sequence sup-ported by the model.\n\u2022 Individual user behavior b\u2081 = (Ti, Ii). The i-th behavior of user U contains text T\u2081 and image I\u2081, and i\u2208 [0,1].\n2) Model Output:\nBased on the model input data from the previous section, the MS2Dformer model (see Fig. 3) needs to solve the following problems in stages.\n\u2022 Stage 1: Multi-modal hidden features S1 \u2208 RH\u00d72D. Firstly, the model input sequence U = {bo, ..., bi} after Eq. (3) to the embedding representation S = {(T\u00ba, I\u00ba), ..., (T\u00ba,I)} \u2208 Rl\u00d72\u00d7768. Secondly, a two-channel multi-modal variational autoencoder (MVAE) is constructed from the variational autoencoder (VAE). After MVAE, S\u2208 Rl\u00d72\u00d7768 is transformed into R1\u00d72D. D represents the dimension of the MVAE embedding part. Finally, the classification token CLSS is added. Thus, S\u2208 Rl\u00d72\u00d7768 is transformed into S\u2081 \u2208 RH\u00d72D, and H = 1 + 1.\n\u2022 Stage 2: Solve the problem of modeling ultra-long se-quences. Firstly, the hierarchical split-window attention mechanism (SW/W-MHA) is constructed. Subsequently, the SW-MHA Transformer block is constructed based on the intra-window attention mechanism. In this case, S\u2081 \u2208 RH\u00d72D is transformed by SW-MHA into R\u00d72W1DA1. represents the stride size of the window. W\u2081 represents the length of the window. Afterwards, the original multi-layer linear perceptron (MLP) is modified to SW-MLP. In addition, S\u2081 \u2208 R\u00d72W1D is transformed by SW-MLP to \u2208 R \u00d74D. Secondly, multiple W-MHA Transformer blocks are constructed based on the inter-window atten-tion mechanism. In this case, the W-MHA Transformer block is used to deeply mine the inter-window sequence features. Therefore, no changes are made to the input data dimensions, i.e., S2\u2208R\u00d74D.\n\u2022 Stage 3: Solve the problem of deep sequence feature mining. Compared with Stage 2, there are two differences in Stage 3. Firstly, the window sliding step 2 set by the SW-MHA component in Stage 3 should be much smaller than \u5165\u2081. Secondly, the W-MHA component uses a deeper Block structure to mine sequence features. Overall, S2ER\u00d7 is transformed into RX8D i.e., S3 \u2208 R2\u00d78D.\n\u2022 Stage 4: Solve the problem of spammer detection. Firstly, the classification token CLSS \u2208 R&D in S3 \u2208 RX12X8D is selected. Lastly, the classification token is mapped to"}, {"title": "IV. MS2DFORMER MODEL", "content": "A. Overview\nA novel Transformer, called MS2Dformer, is constructed that can be used as a generalized backbone for multi-modal sequence spammer detection. The model is divided into four stages to identify spammers (see Fig. 3). Firstly, stage 1 com-pletes tokenizing the user's historical behavior based on two-channel MVAE. Secondly, ultra-long sequences are modeled with stages 2 and 3, and sequence features are deeply mined. Finally, stage 4 employs a linear layer to mine classification token CLSS features and subsequently identifies spammers.\nB. MVAE-based Historical Behavior Tokenization\nThe primary problem to be solved for the serialized spam-mer identification model is how to tokenize the user's multi-modal historical behavior. To this end, we construct a two-channel MVAE-based tokenization strategy for user history behaviors. Subsequently, the serialization of the entire user his-tory behavior is completed by the joint CLSS token. Therefore, this section contains three steps: Input Embedding, MVAE, and Behavior Tokenization.\nInput Embedding: Input embeddings generated by large pre-trained models are capable of encoding rich contextual information. To this end, we use BERT (see Eq. 1) to obtain a single behavioral text feature embedding Tu \u2208 R768 , and use ViT (see Eq. 2) to obtain the image embedding I\u00ba \u2208 R768. Subsequently, the input embedding sequence S\u2208 Rl\u00d72\u00d7768 of the MVAE component is constructed based on the original sequence U (see Eq. 3).\nMVAE: Because the embedding already contains rich se-mantic information, there is no need to go through complex and deep layers to further extract contextual information in the MVAE's encoder. Therefore, we use two linear layers followed by Batch Normalization (BN) and Dropout (rate = 0.2) as the encoder structure for MVAE. BN is used to reduce the internal covariance bias by projecting the inputs to have mean zero and variance one. Subsequently, the formal representation of the encoder is shown below:\n$z = \u03bc + \u03c3e$ (5)\n$L = Eq4(z|x) [log po(x|z)] \u2013 DKL(q4(z|x) || p(z))$ (6)\nwhere e represents the noise sampled from the standard normal distribution N(0, I). represents the elemental product. z de-notes the latent variable. It is satisfying a Gaussian distribution. \u03bcand are the mean and variance of z, respectively. These parameters describe the conditional distribution po(x|z) of the latent variable z. & represents the parameters of the encoder. The objective of the decoder is to approximate the posterior distribution q(z|x) to the posterior distribution po(x|z).\nIn MVAE, the input data x contains text embedding Tu and image embedding I\u00ba, i.e., x = (Tv, I\u00ba). Subsequently, the potential variables 2T and z\u00b9 are obtained after the encoding component of the two channels. Then, the composite feature z = concat(zT, z\u00b9) is obtained by combining the two-channel multi-modal potential features.\nAfterwards, MVAE constructs two channel decoders for multi-modal feature reconstruction. The two encoders share a latent variable z. Therefore, the reconstruction loss function for MVAE is shown as follows:\n$LT = Eq4(z|Tv) [log pe(T\u00ae|z)] \u2013 DKL(q$(z|T\u00ae) || p(z))$ (7)\n$C\u00b9 = Eq\u2084(2/12) [log pe(I\u00b0|z)] \u2013 DKL(q\u00a2(z|I\u00ae) || p(z))$ (8)\nwhere LT and L\u00b9 represent the loss for the text and image channels. Eqq(z|Tv) [log pe(Tu|z)] and Eq$(z|Iv) [log po(Iu|z)] represent the reconstruction loss for the two channel. DKL(q4(z|Tv) || p(z)) and DKL(q$(z|I\u00ae) || p(z)) represent the KL divergence Loss. p(z) represents the standard normal distribution, i.e., p(z) = N(0, I).\nBehavior Tokenization: In MVAE, the latent variable z is included across multi-modal features. Meanwhile, the com-plexities appearing in Fig. 1 are simulated by introducing the noise \u20ac. Subsequently, the interference features are elim-inated by cyclically reconstructing the two-channel features. Therefore, the individual behavior corresponding to the latent variable z is used as a token for the input sequence of stage 2.\nSubsequently, a parallelization strategy is used to simul-taneously extract the latent variables z \u2208 Rl\u00d72D from the entire input sequence S\u2208 Rl\u00d72\u00d7768. Where D is a hyper-parameter indicating the final representation dimension of the MVAE encoder component. Later, the importance of classi-fication tokens in the traditional Transformer architecture is taken into account. Therefore, we construct an all-zero matrix CLSS \u2208 R1\u00d72D to act as the classification token. Lastly, we combine CLSS and z to serve as the input sequence for stage 2, S1 \u2208 RH\u00d72D. In this case, to facilitate the computation, we define H = 1 + 1.\nMHA: The multi-head attention mechanism (MHA) has shown to be extremely powerful in sequence modeling tasks. Therefore, the MHA-based Transformer architecture is used as the basic backbone for sequence feature mining. Among them, the computational process of classical MHA is shown as follows:\n$Q = XW\u00b0, K = XWK, V = XWV$ (9)\n$Attention(Q, K, V) = softmax(\\frac{QKT}{\\sqrt{d}})V$ (10)\n$head\u2081 = Attention(QW, KW, VW)$ (11)\n$X = MHA(Q, K, V)\n= Concat(head1, ..., headn)WM$ (12)\nwhere X represents the input sequence. X represents the sequence after MHA computation. D represents the final representation dimension of the MVAE encoder component. H represents the sequence length. Q, K, and V represent the query, key, and value of the sequence X, respectively. Attention(Q, K, V) represents the attention computation for a individual head. KT represents the transpose of the K matrix. d represents the dimension of the key. headi represents the attention of the i-th head. n represents the number of heads in the MHA. W, W, W, and WM represent the trainable parameter matrices.\nIn the spammer detection task, the sequence of historical user behaviors is an ultra-long sequence. Therefore, QKT in traditional MHA computation (see Eq. 8) will construct an oversized matrix thus causing GPU memory crash (see Table V). For this reason, we propose hierarchical split window attention. The core idea is to construct sliding windows W in QKT. Subsequently, the window W is sliding sampled in steps A. Subsequently, the length H of the ultra-long sequence is transformed into . Thus, the problem of modeling ultra-long sequences is transformed into the problem of modeling standard sequences. In this case, the intra-window and inter-window attention is computed in chunks for QKT at two levels, thus approximating the attention of the entire sequence. To this end, a two-level windowed attention computation is proposed: SW-MHA and W-M\u041d\u0410.\nSW-MHA: To avoid QKT oversized matrices, sliding win-dow splitting is performed in the Q and K vectors. Subse-quently, the intra-window attention of the window sequence is computed in parallel. The equation is shown as follows:\n$Q = SW(XW\u00b0), K = SW(XW), V = SW(XWV)$ (13)\n$SW-Att = softmax(\\frac{QiKi^T}{\\sqrt{d}}) Vi, i \u2208 [1, k]$ (14)\n$SW-Attention(Q, K, V) = concat(SW-Att1, ..., SW-Attk)$ (15)\nwhere SW represents the sliding window splitting function, and the implementation process is shown in Fig. 4 (a). k represents the length of the sequence after the split window. It is assumed that Q, K and V are matrices of sequence length H and embedding dimension \u03b7. Then, Q, K and V are all transformed into R\ubd97\u00d7W\u00d7n by the SW algorithm. W and \u5165 represent the window length and sliding step, respectively.\nSubsequently, the multi-head attention is computed in com-bination with Eq. (12). Finally, to satisfy the Transformer architecture calculation, X is expanded in the last two di-mensions. The equation is shown as follows:\n$X^{SW} = Flatten(X)$ (16)\nwhere XSW \u2208 R\ubd97\u00d7Wn represents the feature matrix after intra-window attention (SW-MHA).\nW-MHA: The computation process for inter-window atten-tion uses traditional MHA (see Eqs. (9-12)). Unlike MHA, the input sequence for inter-window attention is the features obtained after the calculation of intra-window attention. There-fore, inter-window attention does not change the shape of the input features.\nTransformer Block: Subsequently, a comprehensive ap-proximate representation of the overall attention is made based on the intra-window attention (SW-MHA) and inter-window attention (W-MHA). Given the effectiveness of the Transformer architecture, SW-Transformer Block and W-Transformer Block are constructed based on SW-MHA and W-MHA. The specific equations are shown as follows:\n$X^{SW} = SW-MHA(LN(X)))$ (17)\n$\u03be = SW-MLP(LN(X^{SW}))$\n$\u00a7 = \u03be \u03be + W-MHA(LN(\u00a7))$\n$X\u00b0 = \u00a7 + MLP(LN())$ (18)\nwhere Eq. (17) describes the architecture of SW-Transformer Block and Eq. (18) describes the architecture of W-Transformer Block. LN represents Layer Normalization. MLP represents standard multilayer linear perceptron. SW-MLP represents the multilayer linear perceptron specifically for SW-Transformer Block. Compared to MLP, SW-MLP acts not to increase the hidden state space but to decrease the feature dimension. For instance, the output feature XSW \u2208 R \u00d7Wn of SW-MHA has an embedding dimension of W\u03b7. MLP does it by raising the dimension first to 2W\u03b7 and subsequently lowering it to W\u03b7. The very large dimension space of 2W\u03b7 may also cause GPU memory crash. Therefore, SW-MLP reduces the dimensionality first to 4\u03b7 and subsequently to 27. In this case, SW-MLP prevents GPU memory explosion and also constructs the hidden state space. \u00a7 and \u00a7 represent intermediate variables. X\u00ba represents the Block output. Let's assume that the dimension of the input feature X is RH\u00d7n, then the dimensions of \u03be, \u03be, and X\u00ba\u00afare R\ubd97\u00d72n.\nStage 2 and 3: The input sequence X for Stage 2 is S1 \u2208 RH\u00d72D. Therefore, the main task of Stage 2 is to address the threat of GPU memory crashes caused by ultra-long sequences. To this end, a larger sliding step \u5165\u2081 of the window W\u2081 for intra-window feature mining (SW-Transformer Block) is required in Stage 2. Meanwhile, to mine inter-window relationship features, 3 layers of W-Transformer Block are set. Comparing with Stage 2, Stage 3 focuses on mining sequence features. Therefore, the sliding step 2 of the SW-Transformer Block window W2 in Stage 2 does not need to be too large. Meanwhile, more W-Transformer Block needs to be set to mine sequence features.\nD. Spammer Detection\nStage 1-3 are described in the previous two sections, respectively. In this section, the objective of Stage 4 is to identify spammers with an input sequence S3 of dimension R2X8D Firstly, the classification token CLS\u2208 8D is selected from the sequence S3. Subsequently, CLSS is input into two linear layers for feature learning and dimension transformation. Finally, spammers are identified with the help of softmax function. Thus, the final objective function of the model is shown as follows:\nY = Liner(Dropout(Liner(CLSS))) (19)\nwhere \u0176 represents the model output. Liner represents the linear layer. Dropout represents the Dropout layer, which is in-tended to adequately train the model parameters. Subsequently, the model classification loss uses the cross-entropy function. The equation is shown as follows:\n$L = - \\frac{1}{N} \\sum\\limits_{k} Yklog(Yk)$ (20)\nwhere N represents the number of input samples in a batch. Yk represents the true labels of the samples. Yk represents the model prediction. Subsequently, the model total loss Total is obtained by combining the MVAE two-channel loss L\u00b9 and LT. The equation is shown as follows:\n$LTotal = 1 \u00d7 L + V2 \u00d7 L\u00b3 + 43 \u00d7 LT$ (21)\nwhere 41, 42, and 43 represent the decay factors of the three losses, respectively.\nE. Learning Algorithm\nA new Transformer, called MS2Dformer, is proposed. Firstly, a two-channel MVAE for multi-modal behavior quan-tification is built based on the classical VAE (see Algorithm 1). Subsequently, SW-Transformer Block based on the split-window attention mechanism is proposed (see Algorithm 2).\nF. Time Complexity Analysis\nAs shown in Fig. 3 (a), MVAE adopts a two-channel VAE structure. Therefore, the time complexity of MVAE is TMVAE = Tencoder + Tdecoder + Ten encoder + T ec decoder + T= \u039f(4((H-1)\u00b7256(768+D))+D) ~ O(H \u2212 1). Subsequently, the time complexity of SW-MHA is O(k(W\u00b2 \u00b7 \u03b7)). Where k denotes the length of the window sequence. W denotes the window length. \u03b7 denotes the dimension of the input sequence features. Therefore, the time complexity of Stage 2 is Tsatge 1 = TSW-Block + b. TW-Block. where Tsw-Block = O(2D-W\u00b2+4D(2D+2DW\u2081)) ~ O(D\u00b7(W\u2081)\u00b2+D2W\u2081), and Tsw-Block = 2DW1 \u00b7 (W1)\u00b2 + (4DW1)\u00b2 ~ DW\u2081\u00b7 (H/>1)\u00b2 + (DW1)2. Thus, TSatge 1 = O(D\u00b7(W1)\u00b2+D2W1)+O(b(DW1\u00b7(H/11)\u00b2 + (DW1)\u00b2)) ~ O((H/A\u2081)2). Similarly, the time complexity of Stage 3 is O((H/(11\u00b712))2). Therefore, the overall time complexity of MS2former is O(H \u2212 1) + O((H/11)\u00b2) + O((H/(11\u00b712))2) ~ O((H/11)\u00b2). Com-pared with the traditional MHA-based Transformer architec-ture (O(H2)), MS2former is superior in terms of operational efficiency (see Table V)."}, {"title": "V. EXPERIMENT AND ANALYSIS", "content": "A. Experimental Data\nWe conducted model training on two publicly available fake news detection datasets [26", "26": "to filter out the real spammers. Subsequently", "Variants": "Three variants of the MS2Dformer model are constructed to cope with different usage environments:\nMS2Dformer_B : {D = 16", "22)\nMS2Dformer_M": {"23)\nMS2Dformer_L": {"Settings": "We have written the model source code on the software platform of Tensorflow 2.9.0 and Python 3.8. Moreover", "7": 2}, "Settings": "We selected advanced baseline algo-rithms in three dimensions. Firstly", "1": "GAT [2", "5": "RNN [27", "28": "GRU [29", "30": ".", "31": "R-GCN [32", "33": "on the basis of GCN. Finally", "18": "Graph Transformer [34", "35": ".", "Fusion": "As shown in Table III", "36": "", "37": "", "38": "w/o (Bert+ViT)) using pre-trained models. For instance", "Variants": "This work proposes a new MHA mechanism based on hierarchical split windows. Sub-sequently, the modified MHA mechanism can run on GPU platforms with lower memory (see Table V up). Currently, sparse attention mechanisms are proposed in academia to solve the problem of attention computation for ultra-long sequences. Sparse attention has made excellent contributions in relieving GPU memory pressure (see Table V bottom). However, sparse attention inherently fails to effectively mine the long-term interactions of ultra-long sequences. Thus, it performs poorly in the current task (see Table VI). In particular, to address the problem of modeling ultra-long sequences, CNN is introduced before the Transformer block, thus reducing the input sequence to a length supported by the GPU platform. The SW-MHA mechanism proposed in this work works similarly to"}}]}