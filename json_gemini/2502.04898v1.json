{"title": "ARTInp: CBCT-to-CT Image Inpainting and Image Translation in Radiotherapy", "authors": ["Ricardo Coimbra Brioso", "Leonardo Crespi", "Andrea Seghetto", "Damiano Dei", "Nicola Lambri", "Pietro Mancosu", "Marta Scorsetti", "Daniele Loiacono"], "abstract": "A key step in Adaptive Radiation Therapy (ART) workflows is the evaluation of the patient's anatomy at treatment time to ensure the accuracy of the delivery. To this end, Cone Beam Computerized Tomography (CBCT) is widely used being cost-effective and easy to integrate into the treatment process. Nonetheless, CBCT images have lower resolution and more artifacts than CT scans, making them less reliable for precise treatment validation. Moreover, in complex treatments such as Total Marrow and Lymph Node Irradiation (TMLI), where full-body visualization of the patient is critical for accurate dose delivery, the CBCT images are often discontinuous, leaving gaps that could contain relevant anatomical information. To address these limitations, we propose ARTInp (Adaptive Radiation Therapy Inpainting), a novel deep learning framework combining image inpainting and CBCT-to-CT translation. ARTInp employs a dual-network approach: a completion network that fills anatomical gaps in CBCT volumes and a custom Generative Adversarial Network (GAN) to generate high-quality synthetic CT (SCT) images. We trained ARTInp on a dataset of paired CBCT and CT images from the SynthRad 2023 challenge, and the performance achieved on a test set of 18 patients demonstrates its potential for enhancing CBCT-based workflows in radiotherapy.", "sections": [{"title": "I. INTRODUCTION", "content": "In adaptive radiation therapy (ART), cone beam computed tomography (CBCT) is used to assess any change in the patient's anatomy or tumor volume during the treatment course. This is particularly important because modern radiotherapy (RT) techniques, such as intensity-modulated radiation therapy (IMRT) and volumetric modulated arc therapy (VMAT), are highly conformal, requiring precise alignment of the patient's anatomy with the treatment plan to deliver the dose accurately. To this end, CBCT is a cost-effective technology that can be integrated into the Linear Accelerator (LINAC) used to deliver the treatment, allowing the acquisition of real-time images of the patient before each treatment session. However, when compared to computer tomography (CT), the gold standard imaging modality for radiation therapy (RT) planning, CBCT has a lower image quality and a smaller field of view, which can limit the ability to visualize the patient's anatomy and the regions of interest (ROI) accurately. This is particularly problematic in complex clinical scenarios, like the conditioning regimes for marrow transplantation [1], where the ROI spans from head to toe and requires a full view of the patient's anatomy to ensure the adherence of the treatment plan. A notable example is the total marrow and lymph node irradiation (TMLI) [2], a modern treatment option that involves the precise irradiation of the bone marrow and/or the lymph nodes. As a result, TMLI bears considerably lower late-toxicity issues and is associated with better overall response and clinical outcome [3].\nUnfortunately, TMLI is still rather underused because of the challenges in its implementation, which include an accurate delineation of the targets and organs at risk (OARs), a demanding planning process to guarantee an accurate dose delivery, and the need for a precise validation of the treatment plan before each session. In particular, due to the inherent complexity of this treatment, it is extremely important to evaluate the patient's anatomy before each session to ensure the adherence of the treatment plan and to avoid any potential toxicity [4]. This is typically done through the acquisition of CBCT scans that must be matched with planning CT, i.e., the CT scan used to develop the treatment plan, to make sure that the patient's anatomy has not changed significantly and that the plan is still valid. However, this process is significantly hindered by the limitations of CBCT, which cannot cover the whole body and has a lower image quality than CT. We aim at addressing the limitations of CBCT in ART workflows for complex treatments like TMLI by proposing a novel deep learning-based framework, called ARTInp (Adaptive Radiation Therapy Inpainting). Our framework combines image inpainting to complete the gaps in CBCT images and image translation to enhance the quality of CBCT images, providing a synthetic CT (sCT) image that can be more easily matched with the planning CT to validate the treatment plan. To the best of our knowledge, ARTInp is one of the first approaches that deal with the problem of generating synthetic images to fill the gaps in CBCT scans acquired during RT procedures. In this paper, we present a proof-of-concept experiment where we apply ARTInp to a dataset of paired CBCT/CT images of the brain from the SynthRad2023 challenge [5], where gaps are artificially introduced in the"}, {"title": "II. RELATED WORKS", "content": "Synthetic image generation has been a popular research topic since the surge of DL for computer vision tasks in recent years; in the medical field, solutions to generate synthetic CTs are typically based on Generative Adversarial Networks (GANs) or diffusion models [6], [7]. Domain transfer and modality generation are two of the most studied tasks, as the necessity for different imaging modalities for the same patient is often considered the optimal approach to correctly define several clinical situations, particularly in RT. Among existing works targeting image-to-image translation, MedGAN is one of the first end-to-end approaches [8] fully employing DL models successfully; it combines the adversarial loss of GANS with non-adversarial losses to build a framework that allows for PET to CT translation and noisy Magnetic Resonance (MR) refinement in a supervised scenario; from the same authors [9], an approach based on cycle consistent GANS (CycleGAN) [10] is proposed to tackle a similar problem in an unsupervised setting, which is the most common in the medical field, as it is rather challenging to find paired data for different imaging modalities. In [11], an adapted version of CycleGAN was fed slices from the sagittal plane of different brain MRI modalities, stacked, to generate sCT; in [12], a double consistency cycle leads the networks to learn the generation of synthetic MR from CT comparing the synthetic ones with both paired and unpaired images in the discriminator's input; this forces the network to generate a synthetic image which is consistent with both modalities; more recently, another iteration featuring a multi-modal approach to CycleGAN can be found in [13], where multiple branches are added to the generator to develop a multi-modal approach to sCT from MRI acquired with the Dixon method. Among works targeting specifically sCT from CBCT, in [14], a CycleGAN approach is used to generate synthetic images that allow for dose calculation from only CBCT through the generation of sCT, focusing on the treatment planning; in [15] the authors aimed at generating sCT from low-dose CBCT using pix2pix [16] on paired sets of images and attention-guided GAN [17] in a CycleGAN setting for the unpaired sets, validating the system with a similar attention to dose calculation; [18] tried to improve the CycleGAN approach modifying the architecture with an auxiliary chain containing a diversity branch block; [19] focus on CBCT-based adaptive planning, tackling the sCT generation with CycleGAN modified with self-attention blocks included in the generator; [20] aims at eliminating streaking artifacts from CBCT to generate once again a sCT on which dose calculation is possible, proposing SARN, a novel architecture, in cascade with CycleGAN and attention-gated CycleGAN; in [21] sCT generation with diffusion models is explored with an energy diffusion model whose de-noising process is performed with a ResUNet with attention blocks and an energy-guided function that retains modality-independent features from the images.\nSeveral efforts can be found in the literature with the objective of inpainting medical images for different purposes. In [22], an Anatomically-guided Contextual Attention inpainting Network (AnaCAttNet) is guided by anatomical structures to transform pathological tissue of the lungs into healthy tissue. [23] proposed a GAN, GatedConv, to inpaint a metal artefact in MRI of the dental implant patients. In [24], the focus is on inpainting parts of tissues excluded in the field of view of CT acquisition with Globally and Locally Consistent Image Completion (GLIC) [25], obtaining promising results on structural similarity index measure (SSIM). An approach based on diffusion models is shown in [26], with denoising diffusion"}, {"title": "III. METHODS", "content": "In this section, details about the dataset and the general implementation of the ARTInp framework are presented, describing the dual architecture, which includes the completion and the translation networks, the dataset and the metrics used for the evaluation."}, {"title": "A. Dataset", "content": "The dataset used in this work is from the SynthRad 2023 challenge [5]. It includes paired CBCT and CT images of the brain, acquired from three Dutch medical centers between 2018 and 2022, with a total of 180 patients (60 patients per center). The patients were included regardless of tumor etiology, with age ranging from 3 to 93 years (mean is 65); the images were acquired using different scanner models and manufacturer; these aspects helped in promoting a substantial degree of diversity in the data. All images were resampled with a voxel spacing of 1\u00d71 \u00d71mm\u00b3; rigid image registration between the CBCT and the CT was performed using Elastix. Finally, the images were anonymized by defacing the patients and converted in the Nifti format, with each axial slice having a resolution of 256 \u00d7 256 pixels. For this work we generated 80%-10%-10% random splits on the patients (144-10-10 patients) for, respectively, training, validation, and test sets; then, we extracted each patient's axial slices and stored them as individual images, encoded in a single channel TIFF format to preserve the entire range of Hounsfield units (HUs)."}, {"title": "B. ARTInp Framework", "content": "We propose a novel framework, Adaptive Radiation Therapy Inpainting, dubbed ARTInp, that combines deep learning-based inpainting and CBCT-to-CT translation to enhance CBCT-based workflows in radiotherapy. Fig. 2 shows the overview of the framework, consisting of two main components: a completion network, that fills the anatomical gaps in the CBCT volumes, and an image translation network, that generates synthetic CT (sCT) images from the inpainted CBCT images. Both these networks are based on a GAN architecture, which involves a generator for image synthesis a discriminator for evaluating the quality of the generated images. The completion network has been trained to receive in input a sagittal CBCT slice of the patient with a gap, the corresponding CT slice, and a binary mask indicating the location of the gap in the CBCT slice; the network outputs a synthetic CBCT slice with the gap filled (sCBCT in Fig. 2); then, the inpainted CBCT is generated by combining the original CBCT with the gap region of the generated sCBCT, employing a Poisson image blending [27] to ensure a smooth transition between the inpainted part and the original ones. The resulting inpainted CBCT slices generated by the completion"}, {"title": "C. Completion Network", "content": "Fig. 3 shows the architecture of the completion network, which is based on GLCIC [25] and consists of an encoder-decoder structure with convolutional, dilated convolutional, and transposed convolutional layers. The encoder-decoder architecture combines convolutional, dilated convolutional, and transposed convolutional layers. The generator network takes three paired sagittal images of equal size in input: a CT slice, the corresponding CBCT with a gap, and the binary mask of the gap region in the CBCT; the images are 16-bit encoded and have a resolution of 160 x 160 pixels. Convolutional layers in the encoding portion of the network have the purpose of extracting features at increasing scale (relatively to the original image) while while dilated convolutions expand the receptive field without reducing resolution; the decoding layers upsample the features, refining details through transposed and standard convolutions in sequence; the final layer produces the grayscale image in output using a 3x3 convolution and sigmoid activation function. To finalise the output, the 16-bit grayscale sCBCT yielded by the network is combined to the input gaped one applying the Poisson image blending to smooth the transition between the inpainted area and the area where the original image was available; the result is a 16-bt grayscale inpainted CBCT; repeating this process for all the slices in the volume and stacking the results, yields the final inpainted CBCT volume.\nThe discriminator is a dual-network architecture (a Global Context discriminator and a Local Context discriminator) that evaluates the inpainted CBCT slices at both global and local levels. The Global Context Discriminator processes the entire inpainted CBCT through 6 convolutional layers (stride 2, padding 2) with 5 \u00d7 5 filters, each halving spatial dimensions and increasing the number of feature maps; the terminal fully connected layer outputs a 1024-dimensional feature vector capturing the global context. The Local Context Discriminator focuses on 96 \u00d7 96 patches centered on the inpainted regions; its architecture mimics the Global Discriminator but uses 5 convolutional layers; it returns a 1024-dimensional vector representing the local context. The two output vectors are concatenated into a 2048-dimensional vector, processed by a fully connected layer, and passed through a sigmoid activation that yields an estimate of the probability of the image being real.\nThe training of the completion network consists of three phases:\n1) the generator is pre-trained by itself in a supervised fashion;"}, {"title": "D. Translation Network", "content": "Fig. 4 shows the architecture of the translation network, based on pix2pix [16]; it employs a UNet256 generator and a PatchGAN discriminator. The generator takes as input a 16-bit 256 \u00d7 256 axial CBCT slice in input and outputs a 16-bit sCT slice of the same size. The architecture of the generator consists of an encoder-decoder structure with symmetric layer and skip connections; it features 8 downsampling blocks comprising a convolutional layer with 4\u00d74 filters (stride 2, padding 1), batch normalization, and leaky-ReLU activation; after the bottleneck, the symmetric decoder starts; each decoder block is composed by 4 \u00d7 4 transposed convolutions (stride 2, padding 1) to increase the spatial dimensionality, batch normalization, ReLU activation, and skip connections with corresponding encoder layers, that combine high-resolution details from the encoder with the upsampled feature maps, preserving fine-grained details and spatial structure; finally, the outermost block generates the output image through a 4 \u00d7 4 transpose convolution (stride 2, padding 1) and a tanh activation function. The discriminator is designed to classify 70 \u00d7 70 pixel patches of input images as real or fake; by focusing on local patches rather than entire images, it captures fine-grained details efficiently, making it adaptable to various image sizes through its fully convolutional structure. The architecture of the discriminator consists of a 4\u00d74 convolutional layer (stride 2, padding 1) featuring 64 filters, followed by a Leaky ReLU activation; subsequent layers progressively double the number of filters (128, 256, and 512), using the same convolutional structure with batch normalization and leaky-ReLU activation; the final layer generates a single-channel output using a 4\u00d74 convolution (stride 1, padding 1), predicting the authenticity for the patches.\nThe translation network is trained using a loss function composed of two main components: adversarial loss and L1 loss. The adversarial loss ensures that the generator G produces images that are indistinguishable from real target images Y. To achieve this, a the discriminator D learns to differentiate between real images Y and generated images G(X). The discriminator assigns a probability D(y) close to 1 for real images and D(G(x)) close to 0 for fake images. The adversarial loss function is formulated as follows:\n$L_{GAN} (G, D) = E_{y~P_{data} (y)} [log D(y)] + E_{x~P_{data} (x)} [log(1 \u2013 D(G(x)))]$"}, {"title": "E. Evaluation Metrics", "content": "To evaluate the quality of sCT images generated by our models, we used three metrics:\nMean Percentage Absolute Error (MAE%): measures the average absolute difference between the pixel values of the generated images and the ground truth images as a percentage of the overall pixel value range;\n$MAE\\%= \\frac{100}{N}\\sum_{i=1}^{N}\\frac{|I_{i} - \\widehat{I}_{i}|}{R}$\nPeak Signal-to-Noise Ratio (PSNR): quantifies the ratio between the maximum possible power of the image and the power of the noise that affects the image;\n$PSNR = 10 \\cdot log_{10}\\left ( \\frac{I_{MAX}}{\\frac{1}{N}\\sum_{i=1}^{N}(I_{i} - \\widehat{I}_{i})^{2}} \\right )$\nStructural Similarity Index (SSIM).\n$SSIM(I, \\widehat{I}) = \\frac{(2\\mu_{I}\\mu_{\\widehat{I}} + C_{1})(2\\sigma_{I\\widehat{I}} + C_{2})}{(\\mu_{I}^{2} + \\mu_{\\widehat{I}}^{2} + C_{1})(\\sigma_{I}^{2} + \\sigma_{\\widehat{I}}^{2} + C_{2})}$\nIn the formulas above, N is the number of pixels in the image, $I_{i} \\widehat{I}_{i}$ are the pixel values of the original and generated image respectively, R is the range of pixel values in the dataset, $I_{MAX}$ represents the maximum possible pixel value,"}, {"title": "IV. RESULTS", "content": "To evaluate the performance of the proposed ARTInp framework, we conducted two experiments: an evaluation of the performance of the translation network on the task of converting CBCT to sCT in the absence of the completion network; a measure the performance of the whole ARTInp framework.\nIn the first experiment, the translation network was trained using the CBCT and CT images pairs from the 144 patients of the training set (see Section III-A) for 60 epochs (around 1800000 iterations) using an Adam [28] optimizer with a learning rate of 2\u00b710-4 and a batch size of 1. A checkpoint was saved every 5 epochs and the model with the best performance on the validation set was selected.\nIn the second experiment, we trained the completion network by generating artificial gaps in the same images used for the previous experiment. In particular, we created random gaps in the CBCT slices included in the training set by removing vertical strips of the images with a random location and a pixel width w randomly sampled from a uniform distribution \u30b3~ U(48,96). The resulting CBCT images with artificial gaps were used, along with a binary mask of the generated gaps and the corresponding original CT images, to train the completion network. These artificial gaps mimic the ones that would be found in a real clinical scenario, having similar width and positioning. The three training phases described in Section III-C where implemented as follows:\n1) in the first phase the generator network is trained using only the L1 loss for 180000 iterations;\n2) in the second phase, the generator network's weights are frozen and only the discriminator network is trained for 20000 iterations;\n3) in the third phase, both the generator and the discriminator networks are trained together for 620000 iterations.\nThe batch size was 1 and the ADADELTA optimization algorithm [29] was used; a checkpoint was saved every 2000 iterations and the model with the best performance on the validation set was selected.\nThe evaluation of the models trained in the two experiments described above was performed on the test set of 18 patients. To evaluate the performance of the translation network, we tested it on the 3456 CBCT axial slices included in the test set and computed the MAE%, PSNR, and SSIM metrics. Then, we combined the translation network with completion network trained in the second experiments to evaluate the performance of the whole ARTInp framework. We created artificial gaps in the CBCT slices included in the test set, similarly to what we did in the training set; however, in this case, the location and the width of the gaps were sampled patient-wise, i.e., we removed the same portion in all the slices of the C\u0412\u0421\u0422 volume of a patient, to adhere as much as possible to the actual clinical situation. The inpainted CBCT images of the test set generated with the completion network were then used to generate the sCT images through the translation network. Finally, we computed the MAE%, PSNR, and SSIM metrics to evaluate the quality of the images generated by the ARTInp framework with respect to the original CT images."}, {"title": "V. CONCLUSION", "content": "We have presented ARTInp, a novel framework for inpainting and translating CBCT images into synthetic CT images, consisting of two networks: a completion network that fills the gaps in CBCT images and a translation network that generates synthetic CT images from the inpainted CBCT images. We designed ARTInp to enhance the application of CBCT images in the ART workflow, particularly in complex treatments such as TMLI, where the target area spans on the whole body of the patient and an analysis over a large volume is crucial.\nTo evaluate the potential of ARTInp, we trained it on a dataset of paired CBCT and CT images made available for the SynthRad 2023 challenge [5]. In particular, the completion network was trained to inpaint CBCT images with artificial gaps, while the translation network was trained to generate synthetic CT images from paired CBCT images.\nAt inference time, the completion network was used to in-paint the CBCT images with artificial gaps, which were then fed to the translation network to generate the synthetic \u0421\u0422 images, which were compared to the original CT images. The performance of ARTInp is promising, with an MAE% below 2.5% and a PSNR around 27dB, suggesting that the quality of the synthetic CT images generated is similar to the one achieved by models with similar target tasks in the literature [30], but no other works presented the full pipeline, including the inpainting. In particular, when CBCT images are inpainted before generating the synthetic CT images, the final performance is only slightly degraded, showing that the inpainting process is effective. Nevertheless, the SSIM values are below 0.8, and a qualitative analysis of the generated images shows the presence of artifacts and distortions such as blurriness and noise, limiting so far the possibility to apply the framework in clinical processes. Overall, the results suggest that ARTInp has the potential to enhance CBCT-based workflows in radiotherapy and is worth further investigation.\nFuture work will focus on investigating additional training strategies, such as training both networks together, as well as using more complex architectures, such as 3D networks and diffusion models [7], to improve the quality of the generated images. Moreover, we plan to evaluate the performance of ARTInp on a specific clinical setting, such as TMLI, to assess its potential for enhancing the ART workflow in radiotherapy."}]}