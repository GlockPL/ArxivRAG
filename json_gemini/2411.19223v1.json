{"title": "On the Unknowable Limits to Prediction", "authors": ["Jiani Yan", "Charles Rahal"], "abstract": "The classic dichotomisation of prediction error into statically defined 'reducible' and 'irreducible' terms is unable to capture the intricacies of progressive research design where specific types of truly reducible error are being rapidly eliminated at differential speeds. This is especially true of prediction in social systems where questions regarding the 'predictability' of outcomes are gradually reaching the fore (Rahal et al., 2024). Canon-ical work which convenes common predictive tasks involving conventional social surveys (Salganik et al., 2020) shows low power, but rapidly emerging computational approaches which utilize non-standard administrative data and information contained within things such as children's autobiographical essays - often combined with generative Artificial Intelligence and distributed computing (Wolfram et al., 2022; Savcisens et al., 2024) \u2013 begin to allow high accuracy. We propose an enhanced framework which builds upon existing work (H\u00fcllermeier and Waegeman, 2021) in a way which we believe helpfully decomposes various types of truly reducible 'epistemic' error which are in theory eliminable (resulting from a lack of knowledge), and isolates residual, \u2018aleatoric' error (inherent randomness which can never be modeled) that research endeavor will not be able to capture in the limit of human progress. We further emphasize the impossibility of claiming whether things are \u2018predictable', not least in the context of open, dynamically evolving social systems.\nDecomposing prediction error into reducible and irreducible terms is not new. However, statements regarding 'predictability' and 'irreducibility' require the reinforcement of an important quantifier; they are entirely conditional on information sets. For example, the insightful mixed-methods work of Lundberg et al. (2024) begins its abstract with \u2018Why are some life outcomes difficult to predict?'. In that case, they are difficult to predict based on information which is currently measurable, and the 'task' at hand. An outcome which is difficult to predict with one feature set may not be 'unpredictable' in an aleatoric sense: life trajectories may be trivial to predict in the future. More reflexive terminology is essential, lest conclusions be drawn that there is no space for future positive policy interventions based on ever increasingly accurate predictions which prevent negative outcomes. It is impossible to know whether we have eliminated all reducible epistemic error to approach the practical 'ceiling' of accuracy, make statements about how predictable outcomes will become, or understand where we are in the process. Predictability of an outcome can only be asserted when all epistemic errors are eliminated: when 'unmeasured' information is measured and constructed perfectly, features are constructed perfectly, and when functional forms are able to be exactly recovered.\nConsider Equation 1 which better conceptually decomposes error when predicting within contexts under the assumption of stationarity, where $y_{true}$ represents the true outcome, measured after constructing on the phenomenon of interest:", "sections": [{"title": null, "content": "$\\begin{array}{l}Y_{\\text {true }} \\\\= \\frac{f^{*}\\left(X_{\\text {true }}\\right)+\\varepsilon}{\\begin{array}{c}\\text { Predictive } \\\\ \\text { Ceiling }\\end{array}}+\\\\+\\frac{\\left[f^{*}\\left(x_{\\text {true }}\\right)-f\\left(X_{\\text {true }} \\mid Y_{\\text {true }}\\right)\\right]}{\\text { Model approximation gain }}+(\\text { Epistemic })\\\\+\\frac{\\left[f\\left(X_{\\text {true }} \\mid Y_{\\text {true }}\\right)-f\\left(X_{\\text {true }} \\mid Y_{\\text {observed }}\\right)\\right]}{\\begin{array}{c}\\text { Measurement gain from } y \\\\ (\\text { Epistemic })\\end{array}}\\\\+\\frac{\\left[f\\left(X_{\\text {true }} \\mid Y_{\\text {observed }}\\right)-f\\left(X_{\\text {observed }} \\mid Y_{\\text {observed }}\\right)\\right]}{\\begin{array}{l}\\text { Measurement gain from } x \\\\ (\\text { Epistemic })\\end{array}}+\\\\=\\frac{Y_{\\text {predicted }}}{\\begin{array}{c}\\text { Current Prediction }\\end{array}}+\\frac{\\text { Irreducible }}{\\text { (Aleatoric) }}\\end{array}$"}, {"title": "Supplementary Information", "content": ""}, {"title": "Delineating Epistemic and Aleatoric Errors", "content": "In predictive modeling, researchers often aim to predict a phenomenon or concept of interest; they aim to con-struct a concrete and measurable representation of this as $Y_{true}$. However, due to measurement errors that arisefrom factors such as subjective assessment, accounting inaccuracies, or sampling bias, researchers are oftenforced to conceptualize this phenomenon which they want to measure as something similar to $Y_{observed}$. Suchmeasurement errors exist in both feature and target variables; it is essential to understand how these errorspropagate through the model and affect predictive accuracy. This supplementary section presents a math-ematical framework to link $Y_{true}$ with $Y_{observed}$, highlighting epistemic (reducible) and aleatoric (irreducible)errors in prediction, and then equates it to the familiar bias-variance trade-off."}, {"title": "Problem Setup", "content": "Consider the following definition of the 'true' target and features, the observable target and features, and the model which binds them as follows."}, {"title": "True Target and Features", "content": "Allow us to equate the relationship between the 'true' target variable and feature set as:\n$Y_{\\text {true }}: f^{*}\\left(X_{\\text {true }}\\right)+\\varepsilon,$\nwhere:\n$\\bullet$ $Y_{true}$ is the true outcome, measured after constructing on the phenomenon of interest.\n$\\bullet$ $X_{true}$ represents the best possible feature set both in terms of quantity and in quality; a perfectly chosen,constructed and measured feature set.\n$\\bullet$ $f^*()$ is the conceptually true underlying function which maps $x_{true}$ onto $Y_{true}$.\n$\\bullet$ $\\varepsilon$ is aleatoric error (inherent randomness) for $y_{true}$, with $E[\\varepsilon] 0$ and variance $Var(\\varepsilon) = \\sigma^{2}$. It is uncor-related with $f^* (x_{true})$. A proof of this is available upon request."}, {"title": "Observed Target and Features", "content": "Next, allow us to introduce measurement error in $Y_{true}$ and $X_{true}$ by relating them to what we can observe:\n$\\begin{aligned}&Y_{\\text {observed }}:=Y_{\\text {true }}+\\delta_{y}=f^{*}\\left(X_{\\text {true }}\\right)+\\varepsilon+\\delta_{y}, \\\\&X_{\\text {observed }}:=X_{\\text {true }}+\\delta_{x},\\end{aligned}$\nwhere:\n$\\bullet$ $\\delta_{y}$ is measurement error in y, with $E[\\delta_{y}] = \\mu_{\\delta_{y}}$, and $Var(\\varepsilon) = \\sigma_{y}^{2}$.\n$\\bullet$ $\\delta_{x}$ is measurement error in x, with $E[\\delta_{x}] = \\mu_{\\delta_{x}}$, and variance $Var(\\varepsilon) = \\sigma_{x}^{2}$. $X_{\\text {observed }}$ is a vector with$1 \\times K$ dimensions, with each $x_{k} \\in X$ for $k=\\{1,2, \\ldots, K\\}$ as each one of the predictors. Note: $X_{\\text {observed }}$can have different dimensions to $X_{\\text {true }}$.\nAs $Y_{\\text {observed }}$ is observed based on a constructed concept, $\\delta_{y}$ is a function of how well $Y_{\\text {true }}$ is measured. $\\delta_{x}$constitutes measurement and construct error both at the single predictor and aggregate levels. At the level ofan individual feature ($x_{k}$), it regards how well the feature is constructed and measured, and at the aggregatemodel, how well the feature set itself is measured (as a function of individual features $x_{k}$ and the set $X$ as awhole). Construct validity of $x_{k} \\in X$ can be achieved with the same amount of underlying information (seeFigure 1c). For instance, if $x_{k}$ is calculated by averaging the response of multiple items, construct validitycan be improved by finding optimal weights by which to combine the individual predictors. $\\delta_{x}$ can be reducedthrough improved construction of each $x_{k} \\in X$; increased sample representativeness, and conceptualizing thefeature set so that it contains the most relevant set of predictors. In general, we have assumed that $Y_{\\text {observed }}$captures all aleatoric errors in $Y_{\\text {true }}$; changing $\\delta_{y}$ does not impact the level of aleatoric error $\\varepsilon$. A discussion ofthe violation of this condition can be found in SI.1.5"}, {"title": "Predictive Model", "content": "The predictive model is estimated using observable training data $X_{\\text {observed }}$ and $Y_{\\text {observed }}$:\n$Y_{\\text {pred }}:=f\\left(X_{\\text {observed }} \\mid Y_{\\text {observed }}\\right),$\nwhere $f$ is the estimated model trained on $\\{x_{\\text {observed }}, Y_{\\text {observed }}\\}$. The notation $f\\left(x_{\\text {observed }} \\mid Y_{\\text {observed }}\\right)$ emphasizesthat $f$ depends on $Y_{\\text {observed }}$ in training processes. The difference between the conceptually true underlyingfunction $f^*()$ and the currently selected predictive function $f()$ on $x_{\\text {true }}$ and $y_{\\text {true }}$ is defined as follows:\n$f\\left(x_{\\text {true }} \\mid Y_{\\text {true }}\\right):=f^{*}\\left(x_{\\text {true }}\\right)+\\delta f,$\nwhere $\\delta f$ is defined based on the optimal $x_{\\text {true }}$ and $y_{\\text {true }}$, and $E[\\delta f] = \\mu_{f}, \\operatorname{Var}[\\delta f] 0 \\sigma_{f}^{4}$. The difference between$f\\left(x_{\\text {true }} \\mid Y_{\\text {true }}\\right)$ and $y_{\\text {pred }}$ attributes to improvement in $\\delta x$ and $\\delta y$; further details can be found in Equation S7."}, {"title": "Decomposing  $Y_{true}$", "content": "Following these aforementioned definitions, we can decompose the output of a predictive machine which predicts $Y_{true}$ as perfectly as possible, and in the process link it to existent predictions generated by $f()$ to create $y_{pred}$ as follows:\n$\\begin{aligned}&Y_{\\text {true }}=f^{*}\\left(X_{\\text {true }}\\right)+\\varepsilon \\\\&=\\left[f^{*}\\left(X_{\\text {true }}\\right)-f\\left(X_{\\text {true }} \\mid Y_{\\text {true }}\\right)\\right] \\\\&+\\left[f\\left(X_{\\text {true }} \\mid Y_{\\text {true }}\\right)-f\\left(X_{\\text {true }} \\mid Y_{\\text {observed }}\\right)\\right] \\\\&+\\left[f\\left(X_{\\text {true }} \\mid Y_{\\text {observed }}\\right)-f\\left(X_{\\text {observed }} \\mid Y_{\\text {observed }}\\right)\\right] \\\\&+f\\left(X_{\\text {observed }} \\mid Y_{\\text {observed }}\\right)+\\varepsilon \\\\&=\\frac{-\\delta_{f}}{\\begin{array}{c}\\text { Model approximation gain } \\\\ (\\text { Epistemic })\\end{array}} \\\\&+\\left[f\\left(X_{\\text {true }} \\mid Y_{\\text {true }}\\right)-f\\left(X_{\\text {true }} \\mid Y_{\\text {true }}+\\delta_{y}\\right)\\right] \\\\&+\\left[f\\left(X_{\\text {true }} \\mid Y_{\\text {observed }}\\right)-f\\left(X_{\\text {true }}+\\delta_{x} \\mid Y_{\\text {observed }}\\right)\\right] \\\\&+\\frac{Y_{\\text {predicted }}}{\\begin{array}{c}\\text { Current Prediction }\\end{array}}+\\varepsilon .\\end{aligned}$\nTherefore, we can achieve improved predictive performance in comparison to existing predictions based on $X_{\\text {observed }}, Y_{\\text {observed }}$, and $f()$. This is made possible through opportunities to eliminate measurement and construction error in $X_{\\text {observed }}$ (i.e., reducing $\\delta x$), measurement in $Y_{\\text {observed }}$ (i.e., reducing $\\delta y$), and better model approximation $f()$ (i.e., reducing $\\delta f$). When $\\delta x=\\delta y=\\delta f=0$, we reach a scenario where the difference between $y_{\\text {true }}$ and $y_{\\text {predicted }}$ is $\\varepsilon$: pure aleatoric error. This essentially represents the 'upper bound' of predictiveaccuracy, occasionally referred to as a \u2018predictive ceiling\u2019 (Garip, 2020)."}, {"title": "Error Decomposition", "content": "For any individual observation, the total prediction error (denoted below as $Error$) is defined as the difference between the predicted and true outcome:"}, {"title": null, "content": "$\\begin{aligned}\\text { Error } &: Y_{\\text {pred }}-Y_{\\text {true }} \\\\&=f\\left(X_{\\text {observed }} \\mid Y_{\\text {observed }}\\right)-Y_{\\text {true }} \\\\&=\\left[f\\left(X_{\\text {observed }} \\mid Y_{\\text {observed }}\\right)-f\\left(X_{\\text {true }} \\mid Y_{\\text {observed }}\\right)\\right] \\\\&+\\left[f\\left(X_{\\text {true }} \\mid Y_{\\text {observed }}\\right)-f\\left(X_{\\text {true }} \\mid Y_{\\text {true }}\\right)\\right] \\\\&+\\left[f\\left(X_{\\text {true }} \\mid Y_{\\text {true }}\\right)-f^{*}\\left(X_{\\text {true }}\\right)\\right]+\\left[f^{*}\\left(X_{\\text {true }}\\right)-Y_{\\text {true }}\\right] .\\end{aligned}$"}, {"title": null, "content": "Substituting for Equations S2-S4, we obtain:\n$\\begin{aligned}\\text { Error } &=\\left[f\\left(X_{\\text {observed }} \\mid Y_{\\text {observed }}\\right)-f\\left(X_{\\text {observed }}-\\delta_{x} \\mid Y_{\\text {observed }}\\right)\\right] \\\\&\\frac{\\text { Error due to measurement error in } X}{\\text { (Epistemic) }} \\\\&+\\left[f\\left(X_{\\text {true }} \\mid Y_{\\text {observed }}\\right)-f\\left(X_{\\text {true }} \\mid\\left(Y_{\\text {observed }}-\\delta_{y}\\right)\\right)\\right] \\\\&+\\frac{\\begin{array}{c}\\text { Error due to measurement error in } y \\\\ \\text { affecting model estimation }\\end{array}}{\\text { (Epistemic) }} \\\\&+\\frac{\\delta_{f}}{\\begin{array}{c}\\text { Model approximation error } \\\\ (\\text { Epistemic })\\end{array}}+\\frac{\\varepsilon}{\\text { Irreducible }}\\text { (Aleatoric) }\\end{aligned}$"}, {"title": null, "content": "It then logically follows that predicting the target variable as accurately as can be done in practice (seeSection SI.1.1) results in the case where all epistemic errors are eliminated, and only aleatoric error remains:\nError $=y_{\\text {pred }}-Y_{\\text {true }}=\\varepsilon$."}, {"title": "Relationship to Existing Frameworks", "content": "Our framework mainly decomposes errors in a conceptually simple way (i.e., up until now, we have been dealing with scalar target outcome observations). We next develop the aggregate level decomposition of the expected sum of squared errors and relate our framework to the bias, variance, and error frameworks ofcanonical textbooks (e.g., James et al., 2013; Hastie et al., 2009) as well as papers (Pedro, 2000; James, 2003) in the field of statistical learning. Suppose we have a sample with N observations, for each $Y_{\\text {true }, i} \\in y_{\\text {true }}$ and $Y_{\\text {pred }, i} y_{\\text {predicted }}$ for $i=\\{1,2, \\ldots, N\\}$. Equation S9 holds as follows:"}, {"title": null, "content": "$\\begin{aligned}\\text { Error } &=Y_{\\text {pred }}-Y_{\\text {true }} \\\\&=\\left[f\\left(X_{\\text {observed }} \\mid Y_{\\text {observed }}\\right)-f\\left(X_{\\text {observed }}-\\delta_{x} \\mid Y_{\\text {observed }}\\right)\\right] \\\\&=\\frac{\\text { Error due to measurement error in } X}{\\text { (Epistemic) }} \\\\&+\\left[f\\left(X_{\\text {true }} \\mid Y_{\\text {observed }}\\right)-f\\left(X_{\\text {true }} \\mid\\left(Y_{\\text {observed }}-\\delta_{y}\\right)\\right)\\right] \\\\&+\\frac{\\begin{array}{c}\\text { Error due to measurement error in } y \\\\ \\text { affecting model estimation }\\end{array}}{\\text { (Epistemic) }} \\\\&+\\frac{\\delta_{f}}{\\begin{array}{c}\\text { Model approximation error } \\\\ (\\text { Epistemic })\\end{array}}+\\varepsilon ,\nwhere:\n$\\bullet$ $\\delta_{\\text {error }} :=f\\left(X_{\\text {observed }} \\mid Y_{\\text {observed }}\\right)-f\\left(X_{\\text {observed }}-\\delta_{x} \\mid Y_{\\text {observed }}\\right)$; the difference in predicted values owing to$\\delta_{x} . E\\left[\\delta_{\\text {error }}\\right]=\\delta_{\\text {error }}$ and variance $\\operatorname{Var}\\left(\\delta_{\\text {error }}\\right)=\\delta_{\\text {error }}^{2} . A s \\delta_{\\text {error }}$ is expressed as the difference betweeny$ the fitted values, the calculation of $\\operatorname{Var}\\left(\\delta_{\\text {error }}\\right)$ depends on the specific form of $f$, the distribution of\\Delta X_{\\text {observed }}$ and $X_{\\text {observed }}-\\delta_{x}$, and their dependencies.\n$\\bullet$ $\\delta_{\\text {error }} :=f\\left(X_{\\text {true }} \\mid Y_{\\text {observed }}\\right)-f\\left(X_{\\text {true }} \\mid\\left(Y_{\\text {observed }}-\\delta_{y}\\right)\\right)$; the difference in predicted values owing to\\delta_{y}$. E\\left[\\delta_{\\text {error }}\\right]=\\delta_{\\text {error }}$ and variance $\\operatorname{Var}\\left(\\delta_{\\text {error }}\\right)=\\delta_{\\text {error }}^{2}$. Similarly, the calculation of Var error\\right) depends on the specific form of $f$, the distribution of $Y_{\\text {observed }}$ and $Y_{\\text {observed }}-\\delta_{y}$, and their dependencies.\n+\\frac{\\varepsilon}{\\text { Irreducible }}\n\\end{aligned}"}, {"title": null, "content": "$\\begin{aligned}&\\delta f \\text { is defined as before in Equation S5. } \\\\&\\text { Therefore, } \\\\&E[\\text { Error }]=E\\left[\\delta_{\\text {error }}+\\delta_{\\text {error }}+\\delta_{f}+\\varepsilon\\right] \\\\&=\\delta_{\\text {error }}+\\delta_{\\text {error }}+\\mu_{\\delta f} .\\\\&\\text { Since } \\varepsilon \\text { is uncorrelated with other sources of error: } \\\\&\\operatorname{Var}[\\text { Error }]=\\operatorname{Var}\\left[\\left(\\delta_{\\text {error }}+\\delta_{\\text {error }}+\\delta_{f}+\\varepsilon\\right)\\right] \\\\&=\\operatorname{Var}\\left[\\left(\\delta_{\\text {error }}+\\delta_{\\text {error }}+\\delta_{f}\\right)\\right]+\\operatorname{Var}[\\varepsilon] .\\\\&\\text { With Equations S11-S12 we can write our squared error loss as: } \\\\&E\\left[\\left(Y_{\\text {true }}-Y_{\\text {pred }}\\right)^{2}\\right]=E\\left[\\left(Y_{\\text {true }}-Y_{\\text {pred }}\\right)\\right]^{2}+\\operatorname{Var}\\left[\\left(Y_{\\text {true }}-Y_{\\text {pred }}\\right)\\right] \\\\&=E[\\text { Error }]^{2}+\\operatorname{Var}[\\text { Error }] \\\\&=\\left(\\delta_{\\text {error }}+\\delta_{\\text {error }}+\\mu_{\\delta f}\\right)^{2}+\\operatorname{Var}\\left[\\left(\\delta_{\\text {error }}+\\delta_{\\text {error }}+\\delta_{f}\\right)\\right]+\\frac{\\operatorname{Var}(\\varepsilon)}{\\text { Aleatoric Error }},\\\\& \\text { Bias } \\text { Variance }\\end{aligned}$"}, {"title": "Partially captured aleatoric error in observed data", "content": "In Equations S3-S4, we assume that $Y_{\\text {observed }}$ captures all aleatoric error in $Y_{\\text {true }}$. However, since $Y_{\\text {observed }}$ is measured after constructing $y_{\\text {true }}$, this assumption may not hold in all situations. For example, observed data may be a biased sub-sample of the whole population. With such observed data, we can only optimize epistemic errors ($\\delta_{f}$, $\\delta_{x}$ and $\\delta_{y}$) that are inherent in training data. In this case even when all epistemic error is reduced $E_{Y_{\\text {observed }}}$ cannot represent the true $\\varepsilon_{y}$. Consider a true target variable which is measurable across a population ($y_{\\text {true }}$), but $Y_{\\text {observed }}$ only covers a non-representative 70\\% of the population. In this case, $E_{Y_{\\text {observed }}}$ only captures the $\\varepsilon_{y}$ of the observed sample, and lacks the ability to capture the component which would be inherent in the remaining 30\\% of the population. The representativeness of the error term $E_{Y_{\\text {observed }}}$ depends on how well $y_{\\text {observed }}$ represents true reality ($Y_{\\text {true }}$). It also affects assumptions we need to make in terms of the bias-variance decomposition as per Equations S12-S13. If $Y_{\\text {observed }} \\neq \\varepsilon_{y}$, the covariance between $E_{Y_{\\text {observed }}}$ and $\\delta_{f}$, $\\delta_{\\text {error }}$ and $\\delta_{\\text {error }}$ are no longer zero. The Var(Error) term must also incorporate  Cov\\left(E_{Y_{\\text {observed }}}, f\\right), \\operatorname{Cov}\\left(E_{Y_{\\text {observed }}}, \\delta_{\\text {error }}\\right), \\operatorname{Cov}\\left(E_{Y_{\\text {observed }}}, \\delta_{\\text {error }}\\right)$. Therefore, guaranteeing the representativeness of$Y_{\\text {observed }}$ to $Y_{\\text {true }}$ (which also affects the representativeness of $X_{\\text {observed }}$ to $X_{\\text {true }}$ from a measurement perspective)is a necessary step before considering epistemic errors in the form of $\\delta_{f}$, $\\delta_{y}$, and $\\delta_{x}$, as it interrogates whetherthe questions we are answering are really what we meant to ask."}, {"title": "Supplementary Figures", "content": ""}]}