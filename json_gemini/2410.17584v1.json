{"title": "EXPLORING TOKENIZATION METHODS FOR MULTITRACK SHEET MUSIC GENERATION", "authors": ["Yashan Wang", "Shangda Wu", "Xingjian Du", "Maosong Sun"], "abstract": "This study explores the tokenization of multitrack sheet music in ABC notation, introducing two methods-bar-stream and line-stream patching. We compare these methods against existing techniques, including bar patching, byte patching, and Byte Pair Encoding (BPE). In terms of both computational efficiency and the musicality of the generated compositions, experimental results show that bar-stream patching performs best overall compared to the others, which makes it a promising tokenization strategy for sheet music generation.", "sections": [{"title": "1. INTRODUCTION", "content": "Sheet music generation, particularly using ABC notation-a compact, text-based format, has gained prominence in symbolic music generation. [1\u20135]. Tokenizing multitrack ABC notation in language models presents unique challenges due to inter-track dependencies. FolkRNN [1] represented musical elements like pitch and duration as multi-character tokens. In contrast, CLaMP [6] and bGPT [7] introduced bar patching and byte patching, respectively, which tokenize score text into patches and then decode them with a character-level decoder. MuPT [4] used the Byte Pair Encoding (BPE) method [8] from NLP. Nevertheless, challenges regarding musicality and computational efficiency still exist.\nIn this work, we investigate tokenization as a critical initial step in training a sheet music generation model, aiming to minimize computational costs while maintaining the quality of the generated music. Building on bar patching and byte patching methods, we introduce two new techniques-bar-stream patching and line-stream patching. We evaluate all patching methods, including BPE, within a pre-training and fine-tuning framework."}, {"title": "2. METHODOLOGY", "content": "We adopted Tunesformer [3], a hierarchical GPT-2 [9] decoder architecture, for our patching methods. In this framework, patch-level decoders embed and process patches to generate features for a character-level decoder, which performs auto-regressive character prediction. The context lengths are determined by the patch length for the patch-level decoder and the patch size for the character-level decoder. For BPE, we use a standard GPT-2 decoder."}, {"title": "2.1 Model Architecture", "content": "We adopted Tunesformer [3], a hierarchical GPT-2 [9] decoder architecture, for our patching methods. In this framework, patch-level decoders embed and process patches to generate features for a character-level decoder, which performs auto-regressive character prediction. The context lengths are determined by the patch length for the patch-level decoder and the patch size for the character-level decoder. For BPE, we use a standard GPT-2 decoder."}, {"title": "2.2 Data Tokenization", "content": "To ensure multitrack score voice alignment, we use interleaved ABC notation [4] for multiple musical parts. Then, we tokenize score text with four patching methods and BPE, as shown in Fig. 1. Existing methods include:\nBar patching: Divide score text into bar patches, where each bar corresponds to a single voice, and truncate/pad bars based on patch size.\nByte patching: Divide score text into fixed-length patches regardless of musical score semantics.\nBPE: A 50,000-token vocabulary was created through an iterative tokenization approach that merges frequent character or sub-word pairs in the score text.\nTo avoid the truncation in bar patching and ensure division according to semantic units of musical scores, two patching methods are proposed:\nBar-stream patching: An improvement on bar patching. First, the score text is divided into bars. Then, each bar is split into fixed-length patches as per the patch size; if a bar's final patch is shorter than the patch size, it is padded.\nLine-stream patching: Like bar-stream patching, but this method divides the score by line breaks. In interleaved ABC, each line represents a bar with all voices."}, {"title": "2.3 Dataset", "content": "Pre-training used an in-house 160K ABC-notation score dataset. To evaluate models' generalization with different tokenization, we fine-tuned on three classical music datasets of different instrumentation: 398 Bach chorales [10], 103 Haydn string quartets [11], 54 Mozart piano sonatas [12]. Additionally, data augmentation on 15 key signatures was done in both pre-training and fine-tuning."}, {"title": "3. EXPERIMENTS", "content": "For patching methods, we used a 6-layer patch-level decoder and a 3-layer character-level decoder. For BPE, a 6-layer decoder was directly applied. To balance bar truncation and efficiency, the patch size was set to 64 for bar patching (covering 97.7% of all bars) and 16 for other patching methods where truncation is not an issue. The patch length was 512 for all patching methods and 4096 for BPE, ensuring comparable score lengths across attention spans. All pre-training was carried out using 2 H800 GPUs with the batch size maximized."}, {"title": "3.1 Settings", "content": "For patching methods, we used a 6-layer patch-level decoder and a 3-layer character-level decoder. For BPE, a 6-layer decoder was directly applied. To balance bar truncation and efficiency, the patch size was set to 64 for bar patching (covering 97.7% of all bars) and 16 for other patching methods where truncation is not an issue. The patch length was 512 for all patching methods and 4096 for BPE, ensuring comparable score lengths across attention spans. All pre-training was carried out using 2 H800 GPUs with the batch size maximized."}, {"title": "3.2 Evaluation Metrics", "content": "We evaluated models' efficiency and musicality across different tokenization strategies using these metrics:\nSec/Epoch: This represents the average duration of each pre-training epoch, measured in seconds.\nBits-per-byte (BPB): Calculates the average bits to predict the next token on the validation set.\nInference Speed: Average characters generated per second during inference.\nCLAMP 2 Score: Calculated by extracting semantic features with CLAMP 2 [13] and computing the cosine similarity between the validation set and the generated data. A higher score means the generated data is more similar to the real data."}, {"title": "3.3 Results", "content": "Regarding efficiency, byte patching, line-stream patching, and bar-stream patching require shorter training times and have faster inference speeds, with byte patching performing the best. Bar patching and BPE are less computationally efficient because bar patching has a larger patch size and BPE has a longer context length.\nFor BPB, BPE generally performs best. This is likely because BPE tokenizes score text into high-frequency combinations, thus providing the model with more prior knowledge compared to the character-level decoding in patching methods.\nHowever, BPE underperforms in CLaMP 2 Scores, suggesting a semantic gap between the generated results and real music. In contrast, bar-stream patching achieves high CLaMP 2 Scores. It not only avoids bar truncation issues but also incorporates prior knowledge of bar units during patching, leading to better musicality.\nOverall, our experiments show that bar-stream patching is the top-performing method, presenting a balanced performance across all metrics. It combines high training and inference efficiency with generated results that closely resemble real classical compositions."}, {"title": "4. CONCLUSION", "content": "In this study, we explored tokenization methods for sheet music generation based on ABC notation. We introduced bar-stream and line-stream patching and compared them with bar patching, byte patching, and BPE. Focusing on the balance between computational efficiency and musicality, the results demonstrated that bar-stream patching outperformed the others in general.\nFor future work, we will scale up the model size and dataset with employing bar-stream patching and a hierarchical decoder. Additionally, we will establish a classical-music-centered dataset for fine-tuning to enhance the musicality of the generated results."}]}