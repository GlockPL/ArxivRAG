{"title": "S2ALM: Sequence-Structure Pre-trained Large Language Model for Comprehensive Antibody Representation Learning", "authors": ["Mingze Yin", "Hanjing Zhou", "Jialu Wu", "Yiheng Zhu", "Yuxuan Zhan", "Zitai Kong", "Hongxia Xu", "Chang-Yu Hsieh", "Jintai Chen", "Tingjun Hou", "Jian Wu"], "abstract": "Antibodies safeguard our health through their precise and potent binding to specific antigens, demonstrating promising therapeutic efficacy in the treatment of numerous diseases, including COVID-19. Recent advancements in biomedical language models have shown the great potential to interpret complex biological structures and functions. However, existing antibody specific models have a notable limitation that they lack explicit consideration for antibody structural information, despite the fact that both 1D sequence and 3D structure carry unique and complementary insights into antibody behavior and functionality. This paper proposes Sequence-Structure multi-level pre-trained Antibody Language Model (S2ALM), combining holistic sequential and structural information in one unified, generic antibody foundation model. We construct a hierarchical pre-training paradigm incorporated with two customized multi-level training objectives to facilitate the modeling of comprehensive antibody representations.", "sections": [{"title": "1 Introduction", "content": "Antibodies, also known as immunoglobulins, are specialized proteins produced by the immune system to protect against a range of diseases (e.g., SARS-CoV-2), which fulfill the responsibility as guardians in the human body via evolving to complement their structures with the corresponding antigen structures [1-3]. Due to the high specificity and low adverse effect of antibodies, antibody drugs have important clinical significance and account for nearly one-fifth of new drug approvals by the FDA each year [4]. By mimicking the actions of the immune system, these drugs specifically target harmful agents like viruses and cancer cells, to detect viral infections or stimulate T-cell immunity in cancer treatment [5-8]. Therefore, deciphering the information stored in antibody sequences and structures is crucial for understanding immune responses, disease development, and may accelerate the development of therapeutic antibodies for broad-spectrum disease treatment.\nTo alleviate the burden of time-consuming wet-lab experiments, in recent years, the computer-aided antibody engineering has emerged to improve the efficiency of antibody evolution and identify promising therapeutic antibodies with desirable developability profiles. While interpretable, traditional representation learning approaches depend on inefficient hand-crafted features that may miss hidden or latent patterns in diverse data. Drawing inspiration from Natural Language Processing (NLP) with the introduction of pre-trained foundational models, similar AI technologies have emerged as the pivotal technology to interpret the language of biology [9-12]. Pre-trained on large-scale antibody corpora, Antibody specific large Language Models (ALMs) [5, 6, 13-16] have exhibited powerful capabilities in advancing the understanding of antibody structures and functions. Additionally, ALMs consistently outperform general-purpose Protein Language Models (PLMs) because the mechanism of antibody evolution is strongly biased to the target antigen. This finding inspires us to dive deep into the development of comprehensive antibody specific pre-training.\nThe central tenet of molecular biology is that an antibody's amino acid sequence determines its three-dimensional structure, and the three-dimensional structure determines its biological function. The spatial structure plays an integral role for antibody functional characterization, as the principle of antigen-antibody structural binding indicates that antibody structures directly determine biological functions. Consequently, injecting structural information into antibody pre-training emerges a highly compelling endeavor. In this paper, we propose Sequence-Structure multi-level pre-trained"}, {"title": "2 Materials and Methods", "content": null}, {"title": "2.1 Pre-training Data", "content": "In the field of antibody pre-training, there is an abundance of sequential data, whereas the quantity of structure data remains limited, especially those determined by experiments. To compensate for the inadequacy of experimentally-determined antibody structures, we additionally introduce computationally-predicted antibody structures and general protein structures for comprehensive large-scale pre-training. Eventually, our holistic pre-training data encompasses various levels and spans multiple domains, including 75 million 1D sequences and 11.7 million 3D structures from protein and antibody domains. Fig. 2a-b presents compositional ratios of various parts in the pre-training dataset, and details of each part are as follows."}, {"title": "2.1.1 Protein Sequence and Structure Data", "content": "Extensive protein sequence and structure data is incorporated for general sequence-structure learning. Specifically, for the primary structural information (i.e., sequential information), 65 million protein sequences are derived from UniRef50 [17], which is a clustering of UniRef90 seed sequences at 50% sequence identity. Moreover, to alleviate the insufficiency of antibody structure data, the extra protein structure data is incorporated. For the secondary and tertiary structural information, we utilize 0.2 million experimentally-determined general protein 3D structures sourced from Protein Data Bank (PDB) [18] and randomly sample 10 million computationally-predicted protein 3D structures from AlphaFold Protein Structure Database (AFDB) [19]."}, {"title": "2.1.2 Antibody Sequence Data", "content": "The pre-training antibody sequence data comes from the Observed Antibody Space (OAS) database [23], which contains over two billion unpaired sequences and two million paired sequences of antibody heavy and light chains. The OAS database boasts a rich collection of antibody sequences from 825 unique subjects spanning across six different species, where humans account for 88% and mice account for 11%. We downloaded the OAS database of unpaired and paired antibody sequences on January 11th, 2024, and pre-processed the antibody sequence data by filtering, cleaning and clustering. First, all duplicate antibody sequences were filtered out and removed. Additionally, we excluded antibody sequences containing unusual residues (Selenocysteine, Pyrrolysine), along with those having any framework region shorter than IMGT defined. Furthermore, to mitigate data leakage, we clustered antibody sequences at 70% sequence identity over the whole sequence using the MMseq2 algorithm [24]. Alignment coverage in MMseq2 was calculated with respect to the target sequence (\"cov-mode 1\"), with all other parameters setting to their default values. Ultimately, the antibody sequence data comprises 216,437,989 unique antibody sequences, from which we randomly select 10 million sequences for training."}, {"title": "2.1.3 Antibody Structure Data", "content": "Given the central tenet that antibody structure significantly governs its function, we innovatively explore to inject antibody specific structural knowledge into ALMs. To achieve this, we collect experimentally-determined antibody structure data based on Structural Antibody Database (SabDab) [25], which is composed of approximately 10 thousand antibody structures retrieved from"}, {"title": "2.2 Structural Encoding Technique", "content": "Foldseek [22] is a computational tool for fast and accurate protein structure search, merging features such as amino acid spatial angles, distances, and sequence positions. It utilizes the VQ-VAE model [26] to encode the 3D protein structure as distinct and information-rich 3D interaction (3Di) tokens. Each amino acid is assigned a token based on its distance and relative position to the nearest amino acids in the folded protein structure. Foldseek achieves this transformation through the identification of the nearest neighbors and the extraction of distinctive features for individual residues [27]. In this paper, we introduce Foldseek to accomplish the efficient encoding of 3D structures, as depicted in Fig. 2c. Concretely, we adopt Foldseek with the default setting of 20 3Di tokens. Aligning all residue sites, protein and antibody 3D structures are transformed into 3Di sequences (i.e., 1D sequences storing 3D structural information). Such technique fills the void in effective encoding of structure data and enables S\u00b2ALM to handle sequence-structure multi-level information in a hybrid and unified manner, promoting comprehensive antibody representation learning."}, {"title": "2.3 Hierarchical Pre-training Paradigm", "content": null}, {"title": "2.3.1 Stage I: General Sequence-Structure Learning", "content": "In pre-training stage I, we first tokenize the sequences and structures of proteins by an innovative multi-level vocabulary F = V1 + V2. The original sequence alphabet V\u2081 consists of 20 standard amino acids. A protein or antibody sequence can be denoted as s = {$1, $2,...,Sn}, where si \u2208 V\u2081 represents the 1D residue token at the ith site. Building upon the concept of Foldseek [22], the newly-built structure alphabet V2 utilizes 20 distinct 3Di tokens to accomplish the generation of pseudo structural sequences (i.e., 3Di sequences). A protein or antibody structure can be denoted as a = {a1,a2,...,am}, with aj \u2208 V2 indicating the 3Di token at the jth site.\nBuilding on the multi-level vocabulary, we obtain 1D and 3Di sequences and feed them into the model alternately. During pre-training stage I, we train S\u00b2ALM using the BERT-style Masked Language Modeling (MLM) objective [28] to integratively learn from the 1D and 3Di sequences, enabling support for both sequence-level and structure-level tasks:\n$L_{1D-MLM} = E \\sum_{i \\in M} -log p(s_i|s_{\\setminus M})$\n$L_{3Di-MLM} = E \\sum_{i \\in M} -log p(a_i|a_{\\setminus M})$"}, {"title": "2.3.2 Stage II: Antibody Specific Multi-level Learning", "content": "After pre-training stage I, S2ALM has thoroughly comprehended 1D and 3Di sequences across the general protein domain. Subsequently in pre-training stage II, we can primarily focus on multi-level representation learning in the target antibody sub-domain. To better absorb comprehensive knowledge of antibody sequences and structures, exploring new pre-training mechanisms is worthwhile. Two multi-level learning objectives are introduced to inject different granularities of antibody specific sequential and structural information into an ALM: Sequence-Structure Matching (SSM) and Cross-Level Reconstruction (CLR). The customized learning objectives facilitate the extraction of complex patterns and interdependency inherent in antibody sequences and structures.\nSequence-structure matching captures the coarse-grained alignment between antibody sequential and structural information. It is a binary classification task to predict whether a sequence-structure pair is matching or unmatching, as depicted in Fig. 1c. The model extracts representations of the corrupted antibody 1D and 3Di sequences, and then exploits a linear layer to make classification on their matching relationships. SSM optimizes the following loss function:\n$L_{SSM} = \\sum_{(\\bar{s},\\bar{a}) \\in (S, A)} H [y(\\bar{s}, \\bar{a}), p(E_\\Theta(\\bar{s}, \\bar{a}))]$"}, {"title": "3 Results", "content": null}, {"title": "3.1 S2ALM: pre-trained ALM integrating sequences and structures", "content": "In this paper, we pre-trained a large-scale antibody specific language model S2ALM, which integrates multi-level information of 1D sequences and 3D structures for comprehensive antibody representation learning. To accomplish this, the model architecture is built upon ESM-2 [29], comprising 650 million trainable parameters to facilitate the large-scale pre-training. Additionally, we collect a large-scale comprehensive training dataset, composed of sequences and structures from both protein and antibody domains. Furthermore, Foldseek [22] is introduced to execute the efficient encoding of protein and antibody 3D structures, transforming 3D structures to 3Di sequences. Given sufficient 1D and 3Di sequences of proteins and antibodies, we propose a hierarchical pre-training paradigm, where two pre-training stages are constructed to foster holistic understandings of general proteins and specific antibodies correspondingly. To further combine sequential and structural information, we develop two customized pre-training objectives, namely sequence-structure matching and cross-level reconstruction. Equipped with a comprehensive dataset, an efficient encoding technique, a hierarchical pre-training paradigm and tailored pre-training objectives, the resulting S\u00b2ALM acquires general-purpose antibody representations containing biological information derived from both"}, {"title": "3.2 Observation of Multi-scale Organization in Antibody Representations", "content": "The variation observed in large antibody sequence datasets is influenced by processes at many scales, including functional properties that affect binding to specific antigens, species selection biases, and isotype biases that reflect different B cell maturation stages. Uncovering inherent patterns and properties is crucial for comprehending antibody properties. Unsupervised learning captures latent factors that, while unobserved, prove instrumental in elucidating the biological sequence variation [8]. We investigate the representation space of the pre-trained large language models at aforementioned multiple scales to look for signatures of biological organization.\nFor a more intuitive comparison, t-SNE algorithm [30] is employed to visualize the antibody representations generated by the last layer of LLMs. To comprehensively evaluate the effectiveness of large-scale pre-training, it is necessary to compare representations before and after the pre-training phase [6]. We also include the well-established and powerful ESM-2 [29] (pre-trained on protein sequences) for comparison to further verify the superiority of the constructed S\u00b2ALM."}, {"title": "3.2.1 Pre-training Encodes Functional Specificity", "content": "Antibodies have emerged as essential therapeutic agents in the treatment of various autoimmune, infectious and metabolic diseases, mainly owing to their ability to specifically bind to corresponding antigens [5, 31]. Precise identification of antibody functional specificity will greatly enhance the progress of antibody development and optimization. From the OAS database [23], we systematically identify and filter out 6,000 antibodies that exhibit specific binding affinities towards three distinct pathogens: HIV, Ebola virus and SARS-CoV-2.\nWe hereby present the t-SNE visualization analyses in Fig. 3a. Surprisingly, ESM-2 [29] somewhat succeeds in distinguishing among these three types of antibodies, but there remains some confusion regarding antibodies targeting SARS-CoV-2 and the Ebola virus. The pre-trained S2ALM produces more clearly aggregated antibody representations for all three pathogens, contrasting with scattered representations from the untrained model and ESM-2. This further verifies the necessity of our pre-training phase and indicates that the representations extracted by S\u00b2ALM contain antibody functional specificity information."}, {"title": "3.2.2 Pre-training Encodes Biological Species", "content": "Due to significant differences in their genetic information and immune function mechanisms, antibodies from different species are expected to exhibit notable distribution disparities. We randomly"}, {"title": "3.2.3 Pre-training Encodes Evolutionary Isotypes", "content": "Antibodies of different isotypes activate distinct effector mechanisms, manifesting at different stages of the immune response, and differing in structures and locations. Isotype expression reflects the maturation stage of a B cell. Naive B cells express IgM and IgD isotypes with unmutated variable genes, while expression of other antibody isotypes (IgG, IgA, and IgE) occurs after antigen exposure [32]. Distinguishing antibody isotypes evaluates the injection of evolutionary information during model pre-training phase. We select 2,000 antibody sequences from the OAS database [23] per distinct isotype, resulting in a total of 10,000 sequences for t-SNE visualization analyses.\nAs illustrated in Fig. 3c, when exploiting representations extracted by untrained S2 ALM and the pre-trained ESM-2 [29] for t-SNE projection, the organizations of antibodies with various isotypes are predominantly diffuse. This may be attributed to the fact that isotype classification heavily relies on antibody specific evolutionary information, which untrained S\u00b2ALM and the pre-trained ESM-2 are unable to capture. In contrast, we can observe a distinct clustering phenomenon utilizing antibody representations with different isotypes derived from pre-trained S\u00b2ALM. The visualization provides a sanity check for the ability of S\u00b2ALM to extract the valuable information of antibody evolutionary isotypes."}, {"title": "3.3 SALM Captures Clues on Antibody Structure", "content": "It is essential for antibody specific language models to identify and focus on crucial interaction sites during the representation learning process. And the multi-head self-attention mechanism, which specifically focuses on the different aspects of antibodies, has the potential to capture sophisticated interdependencies within the antibody structure. As a practical demonstration of S\u00b2ALM's proficiency in modeling the antibody structural interaction patterns, we execute a structural interpretability analysis. And we find that residue pairs with high self-attention scores can accurately reveal long-range structural contacts. Specifically, the antibody STE90-C11 (PDB: 7B3O), a SARS-CoV-2 neutralizing antibody that binds to the ACE2-RBD interface, is selected as an example input. The self-attention scores from the last hidden layer in S2ALM are extracted to build the heatmap. And we identify the potential hydrogen bond locations within the antibody structure. This allows assessing whether S\u00b2ALM identifies antibody residue structural interactions.\nAs shown in Fig. 4a, the heatmap displays a high self-attention score between residues TRP157 and SER183 (highlighted in orange), serving as an indicator of potential structural associations among these two residues. Correspondingly, the intricate crystal structure precisely substantiates the existence of a hydrogen bond between residues TRP157 and SER183. While the interpretation of attention mechanisms remains an active area of research, this analysis provides an intuitive way to gain profound comprehension of the model's functional mechanism. S2ALM showcases its ability to meaningfully highlight the key interaction sites within antibody structures, reaffirming the superiority of incorporating structural information during ALM pre-training."}, {"title": "3.4 SALM Accurately Predicts Antigen Binding Capacity", "content": "Establishing the precise binding specificity between antibodies and their target antigens is central to accelerating advancements in therapeutic antibody optimization and deepening our comprehension of the intricacies inherent to the immune response. Consequently, rapid and accurate evaluation of the antigen binding capacity stands as a pressing and fundamental requirement in the realm of antibody research and development. Antigen binding capacity prediction is a binary sequence classification task to determine whether the antibody can bind to the specific antigen, where the CDR-H3 region plays a pivotal role. With a keen focus on the interaction between the target antigen human epidermal growth factor receptor 2 (HER2) and the clinically approved wild-type antibody trastuzumab, we compile a dataset of antibody-expressing sequences. These sequences are designed by replacing the original trastuzumab sequence with a myriad of variant CDR-H3 fragments from the heavy chain, as detailed in [16, 33]. The resulting dataset boasts an impressive scale of 21,612 unique antibody sequences and applies the training/validation/test split of 15,128/3,242/3,242 (i.\u0435., 75%/15%/15%). Our comparative analysis encompasses baselines of different types, including those pre-trained on protein sequences (i.e., ESM-1b [34], MSA-1b [35]) and those pre-trained on antibody sequences (i.e., Ablang-H [15], Ablang-L [15], AntiBERTa [13], EATLM [16]). Three classification metrics are utilized for evaluation. AUC is the area under the receiver operating characteristic curve, which shows the performance at all classification thresholds. F1 is the average weighted score of precision and recall. Matthews correlation coefficient (MCC) is the coefficient between true and predicted values.\nWe report the evaluation results of the proposed S2ALM and various baselines in Table 1 (left). On the one hand, ESM-1b and MSA-1b yield relatively inferior performance due to the domain knowledge gap between proteins and antibodies. On the other hand, among baselines pre-trained on antibody sequences, Ablang-H outperforms Ablang-L and AntiBERTa, indicating the advantage of separate training for heavy and light chain sequences. However, the true standout in these evaluations is S\u00b2ALM, which surpasses all baselines and sets a new state-of-the-art performance in this task. Such accomplishment underscores the capacity of S\u00b2ALM to transcend traditional sequence-based models in antigen binding prediction."}, {"title": "3.5 SALM Precisely Distinguishes B Cell Maturation States", "content": "B cells occupy a central position in the immune system's protective arsenal, due to their distinctive ability to generate antibodies. In the human body, these antibodies serve as a vital line of defense against invading pathogens [6]. Therefore, exploring and analyzing the process of B cell antibody maturation remains crucial, enhancing our understanding of the intricate mechanisms that unfold during immune system evolution (i.e., a critical biological process affecting the function and antigen binding specificity of antibodies) [36, 37]. \u0412 cell maturation analysis is a 6-category classification task. Its core objective lies in accurately distinguishing the maturation states of B cell antibody sequences. Each antibody sequence belongs to one of {immature, transitional, mature, plasmacytes, memory IgD+, memory IgD-} states. Accomplishing this necessitates a model capable of learning an evolution-aware representation sensitive to different B cell maturation states. We utilize a total"}, {"title": "3.6 SALM Thoroughly Identifies Antibody Paratopes", "content": "In immunology research, identifying the amino acids in immunoglobulins that specifically bind to antigens (i.e., the antibody paratope) is one of the most crucial and challenging tasks [39]. The antibody paratope, usually composed of multiple amino acids in the complementarity-determining regions, forms the antibody binding site which is essential for antigen interactions in the immune response [40]. Antibody paratope prediction could significantly help to enhance our understanding of the binding mechanisms of therapeutic antibodies. It is a binary sequence labeling task that identifies specific binding positions within antibody sequences, assigning a 0/1 label to each amino acid residue. For paratope prediction, we utilize two antibody datasets sourced from [13, 16], which comprise 277 and 900 antibody sequences annotated with token-wise paratope regions. Numerous baseline models are utilized, including Sapiens-H [41], ProtBERT [42], AntiBERTa [13], AntiBERTy [14], ESM-1b [34], ESM-2 [29], BALM [8], MSA-1b [35] Ablang-H [15], Ablang-L [15], and EATLM [16]. Experimental metrics used to evaluate the prediction capability include area under the receiver operating characteristic curve (AUC), F1 score and Matthews correlation coefficient (MCC).\nTable 1 (right) reports results on antibody paratope prediction benchmark [16], and we observe"}, {"title": "3.7 SALM Accurately Predicts Antigen-Antibody Binding Affinity", "content": "Therapeutic antibody development has become an increasingly popular strategy for drug discovery, but still involves intensive laboratory experiments [46]. Recently, deep learning methods have emerged as a promising approach to accelerate such process by identifying high-affinity antibody candidates [46, 47]. The property of binding affinity, which quantifies the binding strength between antigen-antibody pairs, plays a critical role in antibody development and optimization. To assess the model's performance in predicting the binding affinity, we first utilize two datasets, 14H and 14L [48], which contain abundant antibodies labeled with binding affinity values. Within these two datasets, the affinity values quantify the binding strength of antibodies to a stable peptide in the HR2 region of SARS-CoV-2. To further expand the diversity of binding antigens and thereby enhance both the comprehensiveness and reliability of this experiment, we also include the BioMap [49] dataset for the evaluation of binding affinity prediction. The BioMap dataset contains 1,706 antigen-antibody pairing data with labeled binding affinity values, in which most antibodies stem from human and mouse sources. As for the experiment execution, the 14H, 14L and BioMap datasets are each divided into training set, validation set and test set according to the split of 80%/10%/10% following [6]. Based on the test sets, we record experimental results for fair comparison with extensive baselines (i.e., Vanilla BERT [28], Ens-Grad [43], AbMAP [44], AntiBERTa2 [45], ESM-F [29], and A2binder [6]).\nIn this binding affinity prediction task, we use the Pearson correlation and the Spearman correlation as evaluation metrics. Building upon pre-trained S2ALM, we extend and enhance the model architecture to enable the accurate learning of antigen-antibody binding rules. Concretely, S2ALM is utilized to extract features of antibodies, while ESM-2 [29] is introduced for the feature encoding"}, {"title": "3.8 SALM Enables Computational Antibody CDR Design", "content": "Given the extraordinary performance on antibody understanding tasks related to the evolutionary process and functional mechanism, we additionally incorporate a novel generation task to further evaluate model's capability of antibody generation and optimization. The Complementarity-Determining Region (CDR), also known as the hypervariable region, primarily determines the antibody's affinity and specificity for its target. Given the crucial role of CDRs, computational antibody design primarily focuses on generating these regions. Specifically, antibody CDR design is a sequence infilling task to generate masked CDRs (more exactly, CDR-H3) based on the contextualized representation. For antibody CDR design, the dataset curated by [50] follows the training/validation/test split of 2,282/291/291 samples derived from the Coronavirus Antibody Database (CoV-AbDab) [51]. CoV-AbDab is a public database documenting molecular information (sequences and structures) of all published and patented antibodies and nanobodies that can bind to coronaviruses, including SARS-COV-2, SARS-CoV-1 and MERS-CoV [51]. During this experiment, we mask the CDRs in both 1D and 3Di antibody sequences for S2ALM to reconstruct. To benchmark the quality of antibody sequences generated by S\u00b2ALM, we employ ProtBERT [42], AbLang-H [15] and ReprogBERT [31] for comparison. And three metrics are utilized for holistic evaluation. Perplexity (PPL) measures how well the language model predicts the generative tokens. Here we exploit off-the-shelf ProGen [52] for calculation following [31]. Lower values signify better performance, indicating stronger \"naturalness\" of the generated CDRs. Amino acid recovery (AAR) is computed for the specific sequence region of interest, measuring the percent of the exact matches between ground truth and sampled sequences. The higher the AAR, the more accurate the recovery. Diversity (DIV) computes the complement of the average recovery of all pairwise comparisons using sampled CDRs. A higher value corresponds to an increased dissimilarity among the samples themselves.\nThe evaluation results are shown in Fig. 4b. In spite of the high AAR, ProtBERT struggles in low generation diversity. AbLang-H and ReprogBERT are particularly talented in generation diversity, but demonstrate relatively high perplexity. In contrast, S\u00b2ALM achieves substantially high"}, {"title": "3.9 3D Structural Visualization", "content": "To further manifest the advantages of incorporating structural information during ALM pre-training, we conduct 3D visualization of antigen-antibody complexes generated by S\u00b2ALM. Based on a holistic antigen-antibody complex dataset of 60 antigen-antibody pairs constructed by [53], S\u00b2ALM designs antibody CDR-H3 via sequence infilling. For each antigen-antibody complex, 100 CDR-H3 samples are generated and combined with standard framework regions to obtain 100 full-length antibody heavy chains. We select the top-3 antibody heavy chains with the maximum naturalness using Pro-Gen2 [54], following the standard protocol [6, 31]. Subsequently, AlphaFold3 [55], highly capable of predicting the antigen-antibody joint structure, is applied to generate the 3D structures of complexes comprising antigen and generated antibodies (including antibody heavy chains and corresponding light chains). Evaluation metrics of the folded 3D structure are the predicted template-modeling score (pTM), the interface pTM (ipTM), and the predicted local distance difference test (pLDDT).\nMoreover, to facilitate a thorough comparison, we involve a variety of language models: ESM-2 [29], pre-trained on general protein sequences; SaProt [27], which harnesses both protein sequences and structures for pre-training; and two dedicated ALMS, AntiBERTa [13] and AbLang-H [15].\nThe structural evaluation results through AlphaFold3 are illustrated in Fig. 4e. SaProt and AbLang-H stand out among all baselines, verifying the advantages of injecting structural information and promoting antibody specific learning. S\u00b2ALM consistently achieves the optimal pTM, ipTM, and PLDDT scores compared to other methods, demonstrating the generated antibodies are more likely to target the corresponding antigens and form stable binding complexes. Furthermore, we visualize the generated 3D complex structures targeting three clinically significant pathogens (i.e., Vaccinia virus, Neisseria meningitidis and Influenza B virus). Fig. 4f depicts some visualization examples. The stable and regular 3D structures of designed antigen-antibody complexes fully exhibit the superior generative capability of S\u00b2ALM, which pioneeringly incorporates 3D structural information into ALM pre-training."}, {"title": "4 Discussion", "content": "Antibodies are vital proteins produced by the immune system to safeguard against and combat with a variety of diseases [1]. There are abundant clinical applications of antibody analysis in therapeutic drug discovery and finding novel diagnostics [5, 6, 13, 56]. Most analyses have focused on comparing high-level differences between cohorts to facilitate antibody understanding, such as number of somatic hypermutations, isotype subclass usage, and V(D)J gene segment usage. With the advancements of machine learning techniques, it has been verified in various studies that transformer-"}, {"title": "Note S1. Research Background", "content": "Utilizing massive amounts of text sequences, the transformer-based large-scale pre-training has become a widely adopted paradigm that demonstrates its remarkable capabilities in the field of Natural Language Processing (NLP) [28, 59]. Similar in the realm of life sciences, protein language models (PLMs) and antibody language models (ALMs), pre-trained on extensive biomolecular sequences, have showcased outstanding performance across a diverse array of tasks related to biological structures and functions [5, 13, 16, 29, 34]. Leveraging 1D sequential information, these PLMs and"}, {"title": "Note S2. Motivations of S\u00b2ALM", "content": "Existing ALMs learn antibody representations primarily based on their 1D residue sequences. However, the 3D structures encompass intricate spatial geometric knowledge and are directly relevant to antibody functions, which demonstrates the full potential to enhance antibody representation learning. In this paper, we organize pre-training data including antibody sequences and structures and propose to integrate sequential and structural information during pre-training. Since the scale of available antibody structure data is relatively small compared to the protein data, the additional protein data comprising sequences and structures is introduced as a compensation, assisting the ALM to comprehensively incorporate structural information. By taking advantage of information from multiple levels and additional domains, we execute hierarchical pre-training to obtain an ALM effective on various antibody specific downstream tasks."}, {"title": "Note S3. Pre-training Details", "content": "Building on the architecture of ESM-2 [29], S2ALM has 650 million trainable parameters. We incorporate 33 transformer encoder blocks, each containing 20 self-attention heads. And the hidden dimension is set to 1280. We exploit layer normalization to stabilize and speed up the training process by reducing internal covariate shift. The Rotary Position Embedding (RoPE) is employed to supply token positional information. Additionally, we truncate all training sequences to a maximum length of 1024. Sequences of length less than 1024 are padded, and padded tokens are excluded from the loss computation. By utilizing token-type encoding, the model with mixed training distinguishes 1D and 3Di sequences, assigning 0 to 1D sequences and 1 to 3Di sequences.\nFollowing BERT [28] and ESM-2 [29], during training of Masked Language Modeling (MLM), 15% of tokens in each batch are randomly selected and masked. For these selected tokens, 80% are substituted with the [MASK] special token, while 10% of them are replaced by random amino acids and the remaining 10% are left unchanged. For the training of Sequence-Structure Matching (SSM), we use an in-batch sampling strategy similar to CLIP [60] to corrupt the pairs of 1D and 3Di sequences. Given a mini-batch with N paired data, we construct an ensemble of N\u00b2 1D and 3Di sequence pairs (including N correct pairings and N\u00b2 \u2013 N incorrect pairings) for model to predict whether they match or not. Regarding Cross-Level Reconstruction (CLR), we aim to reconstruct"}, {"title": "Note S4. Advantages over other LLMs", "content": "Table S1 presents distinct pre-training data types, objectives, downstream tasks. Earlier PLMs [29, 34, 42, 52] exclusively focus on protein sequences, which is similar to current ALMs [5, 6, 13-16] pre-trained solely on antibody sequences. Recently, an increasing number of PLMs [27, 63-67] have explored the incorporation of protein structures for pre-training, further enriching the pre-training data and obtaining powerful and generalized protein representations. Motivated by their promising"}, {"title": "Note S5. Ablation Study", "content": "To evaluate effectiveness of the proposed pre-training objectives, which are tailored for integrating the information from both antibody 1D sequences and 3D structures, we conduct a holistic ablation analysis on multi-type antibody related tasks. The ablation results are reported in Table S2. Overall, the performance will decay if any one of the constructed objectives is absent, indicating that both SSM and CLR are essential and advantageous to learn comprehensive antibody representations. In particular, for antibody paratope prediction task, the absence of SSM leads to a more significant performance drop compared to the lack of CLR. This is likely because the matching prediction of 1D and 3Di sequences efficiently injects information from antibody 3D structures that directly determines the spatial binding specificity with antigens. Furthermore, we notice that removing both SSM and CLR for pre-training yields the worst performance among all the ablation experiments. Such phenomenon confirms the superiority of incorporating structural information during the pre-training process and provides more insights into our method."}, {"title": "Note S6. Related Work", "content": "Encouraged by the success of PLMs in protein representation learning, series of works seeks to conduct large-scale pre-training for antibody specific representation learning. AntiBERTy [14] pioneeringly executes the antibody specific pre-training using 558 million antibody sequences. AntiBERTa [13] proposes an ALM proficient at the paratope prediction task. AbLang [15] separately trains Ablang-H and Ablang-L using the heavy-chains and light-chains of antibody sequences to restore missing residues of antibody sequence data. IgLM [5], pre-trained on 558 million antibody sequences while conditioning on the chain type and species-of-origin, innovatively creates synthetic"}]}