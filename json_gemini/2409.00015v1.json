{"title": "Navigating the sociotechnical labyrinth:\nDynamic certification for responsible embodied AI", "authors": ["Georgios Bakirtzis", "Andrea Aler Tubella", "Andreas Theodorou", "David Danks", "Ufuk Topcu"], "abstract": "Sociotechnical requirements shape the governance of artificially intelligent (AI) systems. In an era where embodied AI technologies are rapidly reshaping various facets of contemporary society, their inherent dynamic adaptability presents a unique blend of opportunities and challenges. Traditional regulatory mechanisms, often designed for static-or slower-paced-technologies, find themselves at a crossroads when faced with the fluid and evolving nature of AI systems. Moreover, typical problems in AI, for example, the frequent opacity and unpredictability of the behaviour of the systems, add additional sociotechnical challenges.\nTo address these interconnected issues, we introduce the concept of dynamic certification, an adaptive regulatory framework specifically crafted to keep pace with the continuous evolution of AI systems. The complexity of these challenges requires common progress in multiple domains: technical, socio-governmental, and regulatory. Our proposed transdisciplinary approach is designed to ensure the safe, ethical, and practical deployment of AI systems, aligning them bidirectionally with the real-world contexts in which they operate. By doing so, we aim to bridge the gap between rapid technological advancement and effective regulatory oversight, ensuring that AI systems not only achieve their intended goals but also adhere to ethical standards and societal values.", "sections": [{"title": "Introduction", "content": "Autonomous technologies are revolutionising contemporary society, calling into question traditional policies and modes of regulation [1]\u2013[3]. The primary driver for this paradigm shift is artificial intelligence (AI), which underlies systems that can demonstrate highly context-sensitive and emergent behaviors. In particular, AI systems are able to do inferences, decisions, and actions that have historically been thought to require human cognition.\u00b9 While these systems' flexibility and creativity provide compelling reasons to adopt AI over conventionally engineered systems for self-sufficiency, they also present unprecedented challenges to our ability to design and implement trustworthy [4], safe [5], and ultimately well-calibrated autonomous systems [6]. On the one hand, the potential for AI to adapt, learn, and respond in dynamic ways is what sets it apart from traditional technologies. On the other hand, this adaptability makes it challenging to predict or understand the performance of these systems. As a result, comprehensive and effective governance measures have proven to be a complex and often daunting goal.\nRegulatory and certification bodies across the globe have been grappling with these issues. The European Union (EU) has, for example, engaged in extensive efforts to align growth and innovation in AI with societal values and legal norms [7]. The recently voted \u201cAI Act\u201d is meant to establish a cross-domain minimum set of requirements of AI systems by following a risk-based approach [8]. In the United States, different agencies have begun to expand their expertise to account for autonomous systems and the ways that they are covered by existing rules and regulations. Broader U.S. efforts\u2014for example, the Blueprint for an AI Bill of Rights [9] or the NIST AI Risk Management Framework (RMF)\u2014have aimed to provide overarching frameworks to support and address various aspects of AI governance.\nThese regulations complement the over 700 guidelines and other soft policy documents released over the past decade around the world which provide recommendations (sometimes quite abstract) around the responsible use of AI technologies [10]. There is significant variability within these approaches for governing this fast-changing technology. Some approaches opt for focusing on the application domain for the AI system, with different rules for medical applications, than for entertainment, for example. Other approaches focus on the type of technology, for example distinguishing facial analysis systems from recommender systems, with very different rules applying to each. Still other"}, {"title": "Al governance challenges: Towards dynamic frameworks", "content": "The first key aspect when it comes to AI governance is the context-specificity of requirements, especially those relating to law and ethics. Different requirements may apply to different contexts. Even cross-border legislation initiatives\u2014for example, a directive from the European Union\u2014may have different local interpretations and, hence, requirements [18] depending on the complex interplay between national and international laws, as well as differing national priorities and policies.\nIn addition, when it comes to AI, the vastness of application domains and technical techniques make the development of a single \u201chorizontal\u201d regulation or law (that is, applicable across all domains) extremely difficult to execute. The AI Act, the first legally-binding horizontal regulation on AI, takes a risk-based approach [19] that applies different regulatory requirements\u2014perhaps an outright ban on use within the EU\u2014depending on initial and ongoing risk assessments. However, in the AI Act, the necessary conformity assessment requires grounding and connection with a plethora of other sector-specific rules, regulations, and laws; that is, in practice, the EU AI Act has a significant vertical (that is, sector-specific) component, even though it is written as a horizontal regulation. In fact, this latter feature creates conflict between the Act's requirements, set through newly implemented standards, and government standards set by public agencies [19]. Such conflicts need to be resolved if the Act is to be successfully implemented.\nOne solution would be to turn towards ad-hoc rule-making, in which a regulatory or oversight body takes a largely case-by-case approach to governing AI systems, rather than defining rules and standards across the board. For example, within the US, the Consumer Financial Protection Bureau (CFPB) has pursued this strategy through rulemaking by enforcement actions. Internationally, negotiations within groups such as the Technology and Trade Council (TTC) may help to reconcile governance approaches in sector-specific, though ad hoc, ways. These approaches can flexibly adapt to novel technologies but risk inconsistent interpretations and applications between sectors, leaving stakeholders with risk uncertainties. The net result is likely increased costs of developing and deploying AI systems for real-world environments, with all of"}, {"title": "Dynamic certification", "content": "As we continue to develop and deploy autonomous systems, we are increasingly recognizing the context-dependent nature of their performance. These systems-whether autonomous vehicles or drones, or robotic systems\u2014exhibit varying levels of competence and reliability depending on the specific environments and scenarios in which they operate. This realization has exposed the limitations of traditional static certification frameworks, which typically assess a system's capabilities at a single point in time and under a narrow set of predefined conditions.\nStatic certification approaches, while valuable for many conventional technologies, prove inadequate for the dynamic and evolving nature of AI-driven autonomous systems. These systems learn and adapt over time, potentially expanding their capabilities or developing unforeseen behaviors as they encounter new situations. In fact, any system as it interacts with the environment will output emerging behavior that is hard to predict and understand [26]. Consequently, a one-time approval process fails to capture the ongoing changes in system performance and the associated risks. The challenge, therefore, lies in implementing a process that can adapt to the evolving capabilities of these systems while continuously ensuring safety and reliability.\nThese considerations naturally suggest a turn to dynamic certification, an iterative approach that allows for the ongoing assessment and adjustment of what autonomous systems should and should not be allowed to do in various operational contexts. Dynamic certification offers a structured approach for regulating autonomous AI systems that adapts to their evolving capabilities. This framework allows for the gradual expansion of a system's approved operational envelope based on demonstrated performance and safety in real-world"}, {"title": "Dynamic certification in action", "content": "Consider the design and deployment of an advanced delivery robot for urban areas. This is a complex task: it can be challenging to have an effective delivery robot even in an austere environment, but cities are bustling, chaotic, and constantly changing. How can we ensure our robot safely and effectively carries out its duties, including delivery and behaviors from dodging pedestrians to navigating traffic? While a testbed or lab environment might provide valuable insights, it is implausible that we could have a set of regulatory standards that apply to all urban environments for useful lengths of time. We show how our"}, {"title": "Open problems in dynamic certification for embodied Al", "content": "An essential set of challenges for adapting dynamic certification to embodied AI is technical. AI system complexity and a lack of understanding about appropriate uses and failure modes already too often lead to systems engaging in hazardous behavior [27], [28]. Dynamic certification can reduce such occurrences only if engineers, computer scientists, and AI ethicists can provide regulators and stakeholders with the necessary predictive and analytic capabilities within a dynamic certification regime. \u201cDeploy and monitor\u201d is a recipe for repeated failures and harmful incidents with AI systems. However, doing better through frameworks of dynamic certification will require substantial advances in methods and uses of predictive and environmental modeling.\nWe propose that much of the critical research for practical dynamic certification falls under the scope of model-based design. Technically, the relationships between models and reality must become more traceable and computationally tractable. Simulation capabilities must also be augmented with domain- and application-relevant dynamics of the world, such as pedestrian behavior, to enable more precise design and transitional testing. Another challenge is integrating different programming types, marrying AI's adaptability with precise rule-based commands; neurosymbolic programming is one example of such an integration. These potential advances could enable us to synthesize architectures of controllers that provide guarantees of trustworthy, value-supporting behavior. For early-phase testing, we need advanced simulations that mirror real-world unpredictability. We need formal languages and frameworks to represent the relevant aspects of environments for the other testing phases."}, {"title": "Conclusion", "content": "Early in the system's lifecycle, formal models represents information about the AI system's likely use and deployment contexts, informing stakeholders of potentially clashing requirements and design specifications. Once requirements and design decisions are reconciled in this early-stage testing, a series of high-fidelity simulators (for example, based on the underlying physics) can be constructed and used to identify likely behavior violations in core deployment contexts. Those violations can then be mitigated or addressed through design changes prior to substantial (and costly) development. In the transitional testing stage prior to deployment, controlled testing procedures can be designed to ensure that artificial test and evaluation settings are realistic given current knowledge and expectations, including appropriate variation in potential use and deployment contexts. Once appropriate contexts and uses are identified and validated, confirmatory testing processes enable deployment of the AI system in those contexts with necessary supervision, ensuring safety while allowing for additional learning that can iteratively improve the formal models, therefore increasing our understanding of what we do and do not know, and guide further testing, therefore increasing our trust in the deployed AI system.\nWe contend that dynamic certification provides a regulatory approach with the features and capabilities needed to govern AI and autonomous systems appropriately. However, we require significant advances in theories, methods, and tools to make dynamic certification a practical possibility for regulators and policy-makers. Dynamic certification requires the ability to measure and understand the uncertainty entailed by AI systems; the ability to understand precisely what the models, simulations, and testing vectors mean at each step; the ability to represent appropriate contextual factors; the ability to determine whether values are appropriately implemented in a system; and much more. Absent these capabilities, dynamic certification risks becoming a mere regulatory checklist replicating the current \u201cdeploy and monitor\u201d types of regulation that are known to be insufficient for the complexity and corner cases introduced by AI systems. There are also bidirectional effects. Initially, regulation may be inadequate to prevent catastrophic losses, requiring investigations, lessons learned, and continual improvements on the compliance requirements of systems and expectations of regulatory bodies.\nMore generally, the unique challenges posed by the rapidly evolving AI technologies, such as their context-dependent behaviors and far-reaching im-"}]}