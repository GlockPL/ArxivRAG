{"title": "Efficient DNN-Powered Software with Fair Sparse Models", "authors": ["Xuanqi Gao", "Weipeng Jiang", "Juan Zhai", "Shiqing Ma", "Xiaoyu Zhang", "Chao Shen"], "abstract": "With the emergence of the Software 3.0 era, there is a growing trend of compressing and integrating large models into software systems, with significant societal implications. Regrettably, in numerous instances, model compression techniques impact the fairness performance of these models and thus the ethical behavior of DNN-powered software. One of the most notable example is the Lottery Ticket Hypothesis (LTH), a prevailing model pruning approach. This paper demonstrates that fairness issue of LTH-based pruning arises from both its subnetwork selection and training procedures, highlighting the inadequacy of existing remedies. To address this, we propose a novel pruning framework, BALLOT, which employs a novel conflict-detection-based subnetwork selection to find accurate and fair subnetworks, coupled with a refined training process to attain a high-performance model, thereby improving the fairness of DNN-powered software. By means of this procedure, BALLOT improves the fairness of pruning by 38.00%, 33.91%, 17.96%, and 35.82% compared to state-of-the-art baselines, namely Magnitude Pruning, Standard LTH, SafeCompress, and FairScratch respectively, based on our evaluation of five popular datasets and three widely used models. Our code is available at https://anonymous.4open.science/r/Ballot-506E.", "sections": [{"title": "1 INTRODUCTION", "content": "We envisage that the Software 3.0 era, characterized by software powered by large-scale models, will pave the way for numerous potential applications, such as artificial intelligence generated content (AIGC) and autonomous driving, which are poised to exert substantial influence on societal transformations [75, 97]. The magnitude of AI software has notably surged, primarily driven by the escalating size of deep neural network models [62]. For instance, state-of-the-art computer vision models now encompass over 15 billion parameters, while large language models like GPT-3 exceed 175 billion parameters, demanding nearly 1TB of storage exclusively for the model itself [18].\nHowever, the deployment of large-scale AI software introduces several challenges. These large models require substantial memory and storage resources during deployment, presenting difficulties in running them on resource-constrained devices like edge devices, smartphones, or wearables [36]. Furthermore, the size of these models often leads to slow inference times due to the sheer number of parameters and computations involved, which can be particularly problematic in time-critical applications such as real-time object detection or autonomous driving [54]. Additionally, when Al models are deployed on cloud servers and accessed remotely by client devices, the transfer of large model files over limited bandwidth can result in high latency and increased data consumption [87]. In addition to these deployment difficulties, the resource-intensive nature of large Al applications poses further challenges. The computational demands of these models significantly consume energy, which becomes a critical concern for battery-operated devices or data centers striving for energy efficiency [57]. Moreover, the carbon footprint associated with the training and deployment of large models can be substantial, contributing to environmental concerns [73].\nTo address the aforementioned challenges, researchers employ model compression techniques, which aim to reduce the size and complexity of Al models while retaining their performance. Various model compression techniques have been introduced, encompassing model pruning [25, 41, 51, 58, 70], model quantization [44, 65], and knowledge distillation [11, 12, 42]. Notably, the model pruning algorithm stands out as one of the most widely adopted methods in this domain [55, 62]. Pruning involves the selective removal of unnecessary parameters, connections, or entire neurons, leading to a more streamlined and efficient model. As a state-of-the-art method, the Lottery Ticket Hypothesis (LTH) has garnered significant attention and extensive research efforts [25]. The hypothesis posits the existence of a winning ticket, namely a properly pruned subnetwork combined with the original weight initialization, which can achieve competitive performance comparable to that of the original dense network. This discovery underscores the immense potential for efficient training and network design in the realm of deep learning.\nUnfortunately, among the various research on the LTH and also our experiment results (see \u00a72.3), LTH-based pruning methods suffer from model bias problem [60]. Through our analysis, we found that the bias problem is caused by both ticket selection and ticket training. The process of ticket selection entails the elimination of redundant neurons, resembling high-dimensional feature selection for images, and potentially introducing bias. Fairness and accuracy can exhibit conflicts during ticket selection, with prevailing LTH-based pruning methods prioritizing accuracy, resulting in tickets that lack fairness. Even after obtaining a ticket that balances accuracy and fairness, there is no assurance that the ticket can be trained to reach its theoretical performance upper limit. The training process faces potential issues of overfitting and underfitting, impacting the model's ability to generalize and make accurate and fair predictions. Current efforts to enhance the LTH-based pruning primarily concentrate on improving its efficiency and accuracy performance, neglecting its fairness concerns.\nInspired by ethics-aware software engineering [10, 19, 28], we present BALLOT, a novel fairness-aware pruning framework by revised winning ticket finding and training for deep neural networks. The key idea of BALLOT is that it follows the best practice of software engineering by first observing and analyzing the root cause of the bias problem. Concretely, it performs a conflict detection between the fairness optimization direction and the accuracy optimization direction of the model during the model training process to identify neurons with the highest degree of conflict. These neurons are more likely to induce the model to optimize towards greater variance among distinct subgroups during training, leading to biased model behavior and subsequently giving rise to instances of discriminatory behavior in the software. Each neuron is assigned a score representing its importance in training optimization conflicts, and a corresponding mask is generated to remove these neurons, resulting in the acquisition of an accurate and fair ticket. To ensure that the ticket can eventually be trained into high-performance models, we also refine the training procedure. This involves adapting the learning rate based on the training condition to facilitate the model's learning of richer features. Furthermore, we verify the fairness performance of the model upon the completion of training. If the fairness performance is inferior to that of the original model, we reload the weights from earlier training rounds (the specific number of rounds predetermined by empirical considerations) and iteratively retrain the model until further performance improvement is unattainable. This refined training approach is designed to maximize the ticket's performance potential.\nBALLOT has been implemented as a self-contained toolkit. Our experiments on CIFAR-100, TinyImageNet, and CelebA datasets show that BALLOT can effectively mitigate the pruning fairness problem while maintaining accuracy, compared with existing pruning methods [26, 38, 80, 98].\nOur contribution can be summarized as follows:\n\u2022 We propose a novel fairness-aware DNN pruning framework. It leverages conflict-detection-based mask generation to find fair and accurate tickets, and training refinement to achieve optimal performance.\n\u2022 We develop a prototype BALLOT based on the proposed idea, and evaluate it with CIFAR-100, TinyImageNet and CelebA. On average, BALLOT improves fairness by 38.00%, 33.91%, 17.96%, and 35.82% compared to state-of-the-art baselines, namely Magnitude Pruning, Standard LTH, SafeCompress, and FairScratch, outperforming all baselines in terms of fairness.\n\u2022 Our implementation, configurations and collected datasets are available at [1]."}, {"title": "2 BACKGROUND AND MOTIVATION", "content": ""}, {"title": "2.1 DNN Software and DNN Model Deployment", "content": "The continuous advancements in deep neural network (DNN) research and the availability of powerful computing resources have contributed to the widespread adoption of DNNs in various applications, making DNN-powered software a key driver of innovation in the field of software engineering [61]. DNN-powered software refers to applications and systems that leverage the capabilities of DNNs for various tasks, which are often developed with deep learning frameworks and libraries such as PyTorch [3], TensorFlow [5], and Keras [34]. However, as the capabilities of deep learning models gradually increase, their scale also becomes larger and larger, naturally taking up a lot of storage space [64]. The deployment DNN software encounters complexities due to the diverse software landscape and deployment demands on various mobile platforms. Confronted with resource limitations, such as computational power, storage capacity, and battery life, especially in embedded devices like mobile devices, the imperative arises for mobile models to adhere to stringent conditions encompassing reduced model size, diminished computational complexity, and minimal battery power consumption [55]. Consequently, a challenge is raised for DNN-powered software developers: how can a DNN-powered software be effectively deployed to meet performance requisites on platforms characterized by constrained resources? The compression of DNN models emerges as an indispensable operation in the deployment of DNN-powered software.\nNumerous studies emphasize the requisite step of model compression for deploying DNN models across diverse platforms [55, 64]. Predominantly employed techniques for DNN model compression encompass model quantization [65, 79], model pruning [58, 70], and knowledge distillation [12, 42]. Among these, the model pruning is widely favored and commonly utilized, due to its capability to directly reduce the number of computational operations involved, fundamentally alleviating computation and memory pressures [53, 54]. Popular AI software development frameworks such as PyTorch [84], Tensorflow [81] and PaddlePaddle [67], all integrate pruning-based model compression toolkits.\nModel pruning is a kind of a prominent and effective compression technique in the field of large-scale AI software deployment. The motivation behind model pruning stems from the recognition that many deep learning models, especially those with an over-parameterized nature, contain numerous redundant or less critical parameters. By selectively removing unimportant components or parameters from a model, this approach achieves a remarkable reduction in model size, rendering it more lightweight and computationally efficient as well as not significantly sacrificing model performance. In a DNN, the architecture consists of layers of interconnected nodes (neurons), and each connection between nodes is associated with a weight. These weights, along with biases, are the parameters of the model, and they are collectively represented by the symbol \u03b8. A DNN model f(\u03b8) is a function that takes input data and produces output based on these parameters. More formally, model pruning aims to produce a lightweight model f(\u03b8*) satisfying the following conditions:\n$\\frac{|\\theta^*|}{|\\theta|} < \\Omega$ (1)\n$Acc(f(\\theta)) - Acc(f(\\theta^*)) \\leq \\epsilon$ (2)\nThe Acc() measures accuracy, which is the most frequently utilized performance metric. |\u03b8*| and |\u03b8| are the numbers of parameters, \u03f5 and \u03a9 are the tolerable decline of performance and the sparsity metric respectively. Sparsity metric \u03a9 declares the level of pruning,"}, {"title": "2.2 Fairness Problem in Pruning", "content": "While model pruning strikes a trade-off between accuracy and model size, recent research take attention to potential fairness concerns. For example, Paganini points out that pruning should not only focus on overall performance metrics but should also consider performance differences across subgroups and individuals, i.e., the fairness of model [68]. In machine learning, model fairness refers to the ethical and unbiased treatment of individuals or groups throughout the development, deployment, and utilization of machine learning models. Consequently, the definition of fairness is also divided into two categories: individual fairness and group fairness. Blakeney et al. find that the bias of the model grew progressively as the pruning rate increased [15]. For a pruning technique to be considered fair, the bias degree of the small model after pruning should not significantly increase compared to the metrics of the original model before pruning. Formally, for an original model f(\u03b8) and a pruned model f(\u03b8*), the fairness criterion can be defined as:\n$d(f(\\theta^*)) \u2013 d(f(\\theta)) \\leq \\delta$ (3)\nwhere d() computes the bias degree (i.e. higher values represent lower levels of fairness), and \u03b4 is the tolerance. As previously mentioned, the definition of fairness is categorized into individual fairness and group fairness. This paper specifically concentrates on group fairness. For datasets with sensitive features, fairness metrics such as demographic parity (DP) [22] and equal opportunity (EO) [39] are commonly employed to assess whether the model exhibits discrimination against distinct groups. Since vision and natural language processing tasks usually have neither explicit sensitivity characteristics nor explicit favorable/unfavorable treatment, we utilize class-wise variance (CWV) [83] and maximum class-wise discrepancy (MCD) [83] to measure variations in the model's performance across different groups, i.e. to measure the bias degree.\nThese metrics are formally defined below:\nClass-wise Variance (CWV). Given a dataset with C classes and a given model, the accuracy on the subdataset belonging to cth class is denoted as accc.\n$CWV = \\sum_{c=1}^{C} (acc_c - acc)^2, acc = \\frac{1}{C} \\sum_{c=1}^{C} acc_c$ (4)\nMaximum Class-wise Discrepancy (MCD). For a model, given the maximum and minimum class accuracy acc\u207a and acc\u00af, MCD can be defined as:\n$MCD = acc^+ - acc^-$ (5)\nIntuitively, CWV represents the average discrepancy, while MCD assesses the extreme discrepancy among classes to estimate the fairness of a model. Larger values of these metrics indicate a higher degree of bias in the model. For example, for a software that includes a face recognition module, each class corresponds to one user, and if there are some users whose recognition accuracy is significantly lower than the average or other users (resulting in a large CWV or MCD), it implies unfair treatment, as users with equivalent qualifications receive disparate service quality. While our primary assessment of model fairness revolves around these two metrics, our approach readily extends to other fairness metrics such as DP and EO, contingent on the user defining advantageous outcomes based on the scenario. We selected these metrics because they are well-established and commonly used metrics [13, 31, 80] that are generally applicable to various tasks and datasets including both vision and natural language processing tasks discussed in this paper."}, {"title": "2.3 Lottery Ticket Hypothesis", "content": "A recently proposed technique known as the Lottery Tickets Hypothesis (LTH) has emerged as a rapidly growing method in the field of model pruning, which focuses on sparse trainable subnetworks within fully dense networks [25]. The LTH posits the presence of a sparse subnetwork within a randomly initialized dense network, which can achieve test accuracy comparable to that of the original dense network within at most the same number of iterations during independent training. This sparse subnetwork is known as the \"winning ticket\". More specifically, for a randomly initialized dense neural network f(\u03b8\u2080) parameterized by \u03b8\u2080 \u2208 R|\u03b8\u2080|, LTH suggests searching a mask m\u2208 ({0,1}|\u03b8\u2080|), i.e., the winning ticket is the sparse subnetwork f(\u03b8\u2080 \u2299 m), where \u2299 denotes the element-wise product and |\u03b8\u2080| denotes the numbers of parameters. To find a winning ticket, it is common practice to first train the initialized network for some epochs and compute a mask based on the behaviors of the trained model. LTH-based pruning usually follows the train-prune-retrain pipeline as shown in Figure 4. Given a dense neural network f(\u03b8\u2080) initialized by the parameter \u03b8\u2080 \u2208 R|\u03b8\u2080| and the sparsity metric \u03a9, the process of finding a winning ticket and perform pruning can be expressed as:\ni) Train the network f(\u03b8\u2080) for E epochs to get a well-trained model f(\u03b8).\nii) Remove parameters in \u03b8 which satisfied the sparsity metric \u03a9,by creating a mask m\u2208 ({0, 1}|\u03b8\u2080|). Reset the remaining parameters to their initial values in \u03b8\u2080, creating the winning ticket f(\u03b8\u2080 \u2299 m).\niii) Retrained the f (\u03b8\u2080 \u2299 m) to get final pruned model f(\u03b8*).\nIdeally, the performance of f(\u03b8*) is expected to surpass that of f (\u03b8) within a limited number of training epochs, not exceeding E. However, in a standard LTH setup [25], a winning ticket can only be found when the learning rate is very small, and the performance advantage will become smaller when the learning rate is large. How to find lotteries that actually win is still a problem worth exploring, but this does not detract from the practical value of LTH-based pruning to select high-value subnetworks. A series of efforts were devoted to finding the most promising ticket, which can achieve higher performance[59].\nDespite the remarkable success of LTH-based pruning in achieving high performance, it is crucial to recognize that it still grapples with the challenge of fairness. To demonstrate this matter, we pruned the widely utilized open-source model ResNet50 Gender Classifier [2] using PyTorch's official LTH pruning algorithm package [3]. Subsequently, we assessed the fairness and accuracy of the original model on the CelebA dataset [56] concerning its sparser versions obtained through LTH-based pruning, with pruning ratios set at 0.9, 0.95, and 0.99, respectively. The accuracy, CWV, and MCD are recorded for each model, and the results are presented in Figure 1. The bars in the figure are pattern-coded, with diagonal-hatching representing the original model, and crosshatching, dotted pattern, and plus sign pattern corresponding to pruning rates of 0.9, 0.95, and 0.99 (i.e. sparsity of 0.10, 0.05 and 0.01), respectively. As depicted in the figure, while not leading to a significant decrease in model accuracy, an increase in model sparsity results in a significant reduction in fairness performance.\nThis observation highlights that the LTH approach, being primarily focused on identifying a winning ticket that can retain its capacity toward test accuracy throughout training, tends to overlook potential fairness issues. Consequently, a pertinent question arises: Is it feasible to identify a winning ticket that excels in both accuracy and fairness concurrently?"}, {"title": "3 SYSTEM DESIGN", "content": ""}, {"title": "3.1 Problem Statement", "content": "In this paper, we aim to develop an automated model pruning framework that concurrently addresses both pruning rate and fairness considerations. For a given initialized complex network f(\u03b8\u2080) and training epochs E, the f(\u03b8\u2080) can be normally trained into a model f(\u03b8) with E epochs. We focus on how to select a ticket to satisfy the specified sparsity metric \u03a9, and retrain this ticket into a final sparse model f(\u03b8*) by a training method T. The f(\u03b8*) should attain a high level of fairness, while fulfilling the availability requirement stated by the sparsity metric \u03a9 and accuracy specifications \u03f5 as Equation 1 and Equation 2. Formally, our objective is:\n$\\underset{m,T}{min} [d(f(\\theta^*)) \u2013 d(f(\\theta))]$ (6)\ns.t. $\\frac{|\\theta^*|}{|\\theta|} \\leq \\Omega, Acc(f(\\theta)) \u2013 Acc(f(\\theta^*)) \\leq \\epsilon$\nwhere the d() \u2208 {CWV, MCD} is a measure of bias degree referred to \u00a72.2, which represents a higher fairness with lower indicator value. The |\u03b8| and |\u03b8*| calculate the number of parameters, while Acc() calculates the accuracy of the model on the test dataset. Note the pruning rate is (1 \u2013 \u03a9). Our objective is to enhance the fairness of the pruning model as much as possible while preserving its performance (i.e., accuracy). This involves finding a balance between accuracy and fairness, making trade-offs when necessary."}, {"title": "3.2 Root Cause Analysis", "content": "In pursuit of an accurate and fair ticket, it is essential to consider the trade-off involved in optimizing both aspects. We believe that trade-off arises at two primary levels: first, in determining what components of the model should be removed during pruning (referred to as ticket selection); and second, in devising an appropriate training strategy for the model after the removal (referred to as ticket training). Below, we conduct an exhaustive analysis of the factors contributing to fairness issues in both ticket selection and ticket training."}, {"title": "3.2.1 Ticket Selection.", "content": "Fairness and accuracy may conflict in the search for tickets. For DNN models, the ticket selection and pruning process involves removing redundant neurons. Typically, high-dimensional features extracted from samples are stored in neurons as combinations. Thus, ticket selection is equivalent to feature selection on the dataset, which represents a form of high-dimensional feature selection for a sample. However, this selection process may introduce bias. As previously discussed, LTH methods predominantly prioritize accuracy, potentially leading to the identification of tickets that lack fairness. Such tickets may resort to taking shortcuts instead of genuinely learning the intended solution. For instance, in a face recognition software deployed in a predominantly white neighborhood, a ticket may only use skin color as the determinant of whether a visitor is a local resident, resulting in reduced generalization and fairness [32]. Conversely, exclusively focusing on fairness during ticket identification may result in a decline in accuracy. Intuitively, if a face recognition software blocks out all features related to skin color to be fair, its face recognition performance is likely to suffer considerably. Hence, a trade-off must be carefully considered throughout the process of ticket selection to strike a balance between fairness and accuracy."}, {"title": "3.2.2 Ticket Training.", "content": "Even upon obtaining a ticket that strikes a balance between accuracy and fairness, there is no guarantee of successfully \"claiming a prize\", that is, training a model that attains the upper limit of its theoretical performance. There are two potential issues related to the training process. First, if the model excessively focuses on learning the features of existing training samples, it may treat some of these specific features as if they are universally representative of all potential samples, leading to overfitting. In such cases, the model becomes too tailored to the training data and may not generalize well to new, unseen samples, thus introducing bias. Conversely, if the model's learning capacity is too limited, it may not adequately capture the general features of the samples. This situation, known as underfitting, results in a failure to learn critical patterns from the data, leading to poor performance on both training and unseen samples. Achieving an optimal balance between overfitting and underfitting is essential for obtaining a model that generalizes well and demonstrates accurate predictions on new and diverse samples.\nWe conduct a series of training experiments on the CIFAR-100 dataset, utilizing the same ticket (model prototype) acquired through standard LTH pruning with a pruning rate of 0.95. The experiments involve training the model for 50 and 150 epochs (underfitting model), 250 epochs (normally fitting model), and 500, 750, and 1000 epochs (overfitting model). Throughout all experiments, a uniform learning rate of 0.03 is employed. Then we evaluate the accuracy and fairness of these trained models. Figure 3 compares the accuracy, CWV, and MCD of these models. As we can see, the normal model exhibits the most favorable performance in both fairness and accuracy, whereas the underfitting model demonstrates the poorest performance. This highlights that, despite employing the same ticket, the training process significantly influences the final performance of the retrained model."}, {"title": "3.3 System Overview", "content": "Through our analysis, it becomes evident that existing methods encounter challenges in finding tickets that simultaneously achieve both accuracy and fairness while successfully training a model. In response, our proposed framework, BALLOT, introduces novel conflict-detection-based mask generation and training refinement techniques to obtain accurate and fair sparse models. We first locate the root cause, namely neurons exhibiting conflicts between fairness and accuracy during optimization, and then generating masks that block out these neurons to get a sparse model and train it by a comprehensive strategy to improve its fairness performance. The primary objective of mask generation is to ensure that the model focuses on the most relevant features during the learning process. The training refinement is pivotal in ensuring that the model not only captures essential features but also achieves equitable outcomes across different groups or classes."}, {"title": "3.4 Mask Generation", "content": "To address this challenge mentioned in \u00a73.2.1, we propose a novel mask generation approach termed conflict-detection-based mask generation. Specifically, we collect the gradients corresponding to the fairness and accuracy of each neuron during training the model in the normal way, and then locate neurons with optimization conflicts and generating masks to remove them.\nPrior to delving into our approach, let us first introduce the precise definitions of the accuracy loss function (denoted as la) and the fairness loss function (denoted as lf) employed in our methodology. The la and lf can be formulated as:\n$l_a = -\\sum_{c=1}^{C} w_c log \\frac{exp(x_{n,c})}{\\sum_{i=1}^{C} exp(x_{n,i})}$ (7)\n$l_f = -\\sum_{c=1}^{C} w_c log \\frac{exp(x_{n,c})}{\\sum_{i=1}^{C} exp(x_{n,i})}y_{n,c}$ (8)\nIn the context of the provided equations, C denotes the number of classes, x represents the input data, y denotes the target and w corresponds to the weight. Here, we set the weights as $\\frac{1}{acc_c}$, where accc denotes the accuracy of the previous iteration of the model. For each respective class, we take this value as an estimate of the current iteration's accuracy. The choice of weights follows an inverse proportionality to the accuracy achieved in the prior iteration for individual classes.\nInitially, during the standard training process, we collect the gradient values of each neuron pertaining to both the fairness and accuracy loss functions for each training round (line 4-7). This information serves as the foundation for evaluating potential optimization conflicts. Then we compute the sum of these two values for every round, weighted by a certain percentage \u03b3 to ensure that both values are on the same order of magnitude (line 12-18). Here, the value of \u03b3 is predetermined based on the outcomes of hyperparameter experiments, as discussed in \u00a74.5. The resulting sums are sorted to derive a reference value, denoted as conflict degree l, delineate the degree to which the optimization objectives of the two loss functions are in conflict during each respective round (line 19). Next, we find out the neurons with the highest degree of conflict and increment their respective count (line 20-21). By summing the count values of each neuron across all rounds, we identify the neurons with the highest number of conflict rounds (line 22-24). These neurons are then removed based on the sparsity metric to obtain the desired mask for our approach.\nThrough an examination of the metadata from the training process, we identify neurons whose optimization exhibited conflicts between the fairness and accuracy objectives. This analysis aid in the discovery of potential models that are more inclined to achieve fairness-accuracy conjugate optimization by eliminating these neurons. However, merely identifying such a sparse model prototype (ticket) is insufficient; it remains crucial to consider how to effectively train this prototype into a fully functional and high-performing model (claim a prize). The optimization and validation of ticket training are imperative."}, {"title": "3.5 Training Refinement", "content": "Upon identifying a ticket, our task is only halfway completed. While this ticket holds the potential to serve as a fair and accurate model, careful training is indispensable to enable it. Traditional LTH methods involve loading the ticket with the initial base model weights \u03b8\u2080 and subsequently performing retraining, akin to the initial training. However, such an approach lacks optimization steps and neglects performance validation of the trained model. Thus, there is a need for a more comprehensive training strategy that includes optimization techniques and robust performance validation to ensure the model attains its full potential.\nTo address this, we undertake a comparison of various training policies and optimize the training procedure employing a technique termed training refinement. In detail, we commence the training refinement process by reloading the initial weights into the pruned model. Subsequently, we conduct model training using a decreasing learning rate strategy to facilitate learning as many essential features as possible while avoiding overfitting. The objective is to strike a balance between adequately capturing necessary patterns in the data and ensuring the model's ability to generalize effectively to new, unseen samples. Ultimately, we validate the retrained model. If the model's fairness performance falls below that of the original model, we initiate a feedback loop by returning to the first step. During this process, instead of reloading the base model initialization weights, we reload the weights after completing a specified number (typically a small single or double digit number predetermined by empirical considerations, see \u00a74.4) of training epochs and recommence the training procedure. This is because sparse subnetworks that remain stable to the optimization noise in the early stages of training are more likely to be winning tickets [27]. We iterate through the aforementioned steps until fairness or accuracy decreases above the threshold value \u03f5 or ceases to improve further. It increases the likelihood of identifying and training winning tickets to the desired levels of accuracy and fairness. By following the aforementioned process, we mitigate potential issues such as convergence failure or convergence to a local optimum, which may arise during the training process. Our approach endeavors to ensure that the previously obtained accurate and fair tickets can be effectively employed to train a high-performing model."}, {"title": "4 EVALUATION", "content": "We aim to answer the following research questions through our experiments:\nRQ1: Can BALLOT effectively claim the winning ticket?\nRQ2: How efficient does BALLOT find and claim the winning ticket?\nRQ3: How do mask generation and training policies affect the performance of BALLOT?\nRQ4: What is the impact of different configurable parameters in BALLOT?"}, {"title": "4.1 Setup", "content": ""}, {"title": "4.1.1 Software and Hardware.", "content": "The prototype of BALLOT is implemented on top of PyTorch 2.0. We conduct our experiments on a server with 64 cores Intel Xeon 2.90GHz CPU, 256 GB RAM, and 4 NVIDIA 3090 GPUs running the Ubuntu 16.04 operating system."}, {"title": "4.1.2 Datasets.", "content": "We evaluate BALLOT on five popular datasets: CIFAR-100 [4], TinyImageNet [49], CelebA [56], LFW [43], and Moji [16].\n\u2022 CIFAR-100 is a widely-used colored image dataset used for object recognition. It contains 60,000 32x32 color images in 100 classes, of which 50,000 are training images and the rest are test images.\n\u2022 TinyImageNet is a subset of the ILSVRC2012 classification dataset. It consists of 100k colored images of 200 classes, and all images have been down-sampled to 64 * 64 * 3 pixels.\n\u2022 Celeba is a large-scale face attributes dataset with more than 200K celebrity images. The images in this dataset cover large pose variations and background clutter. We perform face gender classification task on this dataset.\n\u2022 LFW is a collection of over 13,000 face photographs sourced from the web. Many groups are underrepresented in this dataset, such as children, babies, individuals over the age of 80, and women. Therefore, LFW is suitable for evaluating model performance in the data imbalance setting. We perform face gender classification task on this dataset.\n\u2022 Moji contains more than 100,000 tweets written in either \"Standard American English\" or \"African American English\", annotated with positive or negative sentiment. We perform text sentiment classification task on this dataset."}, {"title": "4.1.3 Models.", "content": "For a fair comparison with baseline methods, we reproduce the baselines and use the same models and setup on each method in the experiments. We train the 50-layer ResNet model (i.e., ResNet50) [40] and the VGG16 [77] with SGD and set the batch size to 32. In the training process, the learning rate starts from 0.1 and reduces to 1/10 of the previous learning rate after 100, 150 and 200 epochs (250 in total). When pruning models, we set the sparsity to 0.05, i.e., remove 95% of the parameters in the model. Random cropping, horizontal flip, and normalization are adopted for data augmentation.\nTo assess the performance of the BALLOT in natural language processing tasks, we conducted experiments on the BERT model [21]. We finetune the BERT model with AdamW, set the batch size to 32 and the learning rate to 5e-5."}, {"title": "4.1.4 Baselines.", "content": "We compare BALLOT with other state-of-the-art model pruning methods, including Magnitude Pruning [38], Standard LTH [26], SafeCompress [98], and FairScratch [80]. In experiments, We run the baseline methods with the default settings which are recommended in their papers and open-source code and record their performance.\nMagnitude Pruning. Han et al. [38] trained a model to learn which neuronal connections are important, and then they pruned those unimportant ones and fine-tuned the target model. Their method can effectively reduce the number of parameters of the target model by an order of magnitude without affecting their accuracy.\nStandard LTH. Frankle et al. [26] proposed the Lottery Ticket Hypothesis (i.e., Standard LTH in this paper) that a DNN contains subnetwork (e.g., the winning ticket) that is able to match the test accuracy of the original network after training. They found the winning ticket by masking the original model, thereby reducing the number of parameters of the model while ensuring accuracy.\nSafeCompress. Zhu et al. [98] proposed SafeCompress, which is a test-driven sparse training framework for safe model compression. SafeCompress first prunes the big model to an initial sparse model,"}, {"title": "4.1.5 Evaluation metrics.", "content": "We compare BALLOT with the baselines on five metrics: accuracy, precision, recall, CWV and MCD (see \u00a72.2)."}, {"title": "4.2 RQ1: Effectiveness in Finding and Claiming Winning Ticket", "content": "Experiment Design: To evaluate the effectiveness of BALLOT in finding and claiming in the winning ticket to fix the bias problems", "fairness.\nResults": "The comparison results are presented in Table 1. The first column shows two datasets and the second column lists four methods in experiments including BALLOT. We record the model performance of the original models that have not been pruned", "Origin": "n the second column. The remaining columns list the performance of the pruned model"}, {"title": "4.3 RQ2: Efficiency in Finding Winning Ticket", "content": "Experiment Design: To evaluate the efficiency of BALLOT in finding and claiming the winning ticket, we measure the time cost of BALLOT and the baselines (i.e. Standard LTH, SafeCompress, and FairScratch) in performing a complete pruning and retraining process on these datasets. Since the Magnitude Pruning method does not contain the retraining process after pruning, which makes it difficult to make a fair comparison, we do not involve this method in the comparison. To avoid the effect of randomness, we perform 10 trials that use random training/test data splitting and compute and record the average time overhead of each method. The corresponding results and analysis are presented below.\nResults: Figure 5 shows the time cost of each method and demonstrates the efficiency of BALLOT in finding fair sparse model. The blue, orange, gray, and yellow bars represent the time cost of the four methods respectively. On average, on the CIFAR-100, TinyImageNet, and CelebA datasets, BALLOT spends 255.8 minutes, 530.4 minutes, and 200.5 minutes, which are 16.80%, 18.76%, and 4.89% faster than the SafeCompress, and 18.85%, 12.05%, and 5.07% faster than the FairScratch. Compared with the Standard LTH method, BALLOT takes slightly more time. The average execution time of BALLOT iS 3.02%, 6.00%, and 2.35% longer than the Standard LTH.Analysis: As shown in Figure 5, compared to baselines, BALLOT costs close or less time on pruning and retraining. BALLOT spent significantly less time than the SafeCompress method on finding and claiming lottery tickets, and its time cost is close to that of the Standard LTH. Combined with the results in Table 1, we can see that BALLOT achieves far better results than the baseline in terms of fairness and utility while costs similar or even less time, which demonstrates the efficiency of BALLOT in finding fair sparse models."}, {"title": "4.4 RQ3: Impacts of Mask Generation and Training Policies", "content": "Experiment Design: BALLOT leverages two core components, namely the conflict-detection-based mask generation and training refinement, to find and claim the winning tickets. In this section, we aim to demonstrate the effectiveness of these two components by comparing the performance of the pruned model under different mask generation and training policies. To this end, 1) for mask generation policies, we compare the results of finding winning tickets among the conflict-detection-based mask generation implemented in BALLOT (see \u00a73.4), random mask generation, and magnitude mask generation. The random policy randomly selects neurons to generate masks. The magnitude one removes those neurons with the smallest weights to generate a mask, which is applied in Standard LTH [26]. 2) for training policies, we compare the performance among BALLOT rewinding to 5th epoch, BALLOT rewinding to 10th epoch (default), BALLOT rewinding to 15th epoch, BALLOT without learning rate decreasing, and BALLOT without rewind. The latter two are training policies that do not use learning rate decreasing or the rewind method, which are introduced in \u00a73.5. We follow the experiment settings in \u00a74.1 and conduct experiments on the CIFAR-100 dataset. We record and compare the accuracy, precision, recall, CWV, and MCD of these models to observe how different mask and training policies affect their utility and fairness.\nResults: Table 2 presents the results, where the best results are marked in bold. The first columns list different mask generation and training policies, and the following five columns show the results of the pruned models on accuracy, precision, recall, CWV, and MCD. Overall, BALLOT achieve the comprehensively best fix results among the compared methods, which indicates that the conflict-detection-based mask generation and training refinement in BALLOT can help to find fair sparse models.\nImpact of mask generation policy: The first three rows of Table 2 show the comparison results of mask generation methods, where the first line shows the results of the default method in BALLOT. When BALLOT applies the random mask generation instead of conflict-detection-based mask generation, the accuracy of pruned models decreases by 1.02%, and CWV and MCD increase by 15.87% and 4.73%. In addition, using the magnitude mask generation method instead of the one in BALLOT also causes performance degradation. It decreases the accuracy by 0.74% and increases CWV and MCD by 14.67% and 7.51% respectively. It follows that the model obtained by BALLOT pruning is more effective in maintaining fairness, in line with our observations in \u00a73.2.1.\nImpact of training policy: The last five rows show the effect of different training policies. Compared to the default training refinement in BALLOT, BALLOT without learning rate decreasing and BALLOT without rewind reduce the accuracy of pruned models by 8.17% and 2.54% and increase the CWV by 25.47% and 4.44%. We can observe that removing any method in the learning rate decreasing and the rewind will cause a significant degradation in the performance of BALLOT. In the context of employing the rewind method, loading the weights from the 10th epoch yields superior performance compared to loading from the 5th and 15th epoch, resulting in an increase of 1.75% and 3.43% in CWV and 1.51% and 0.92% in MCD, respectively.\nAnalysis: The above results show that compared with the other optional methods, the conflict-detection-based mask generation and training refinement in BALLOT achieves the highest utility and fairness performance, which can improve model fairness when ensuring effectiveness. In addition, both mask generation and training policy implemented in BALLOT is helpful to improve the performance of the pruned model. It is noteworthy that BALLOT rewidning to 10th epoch exhibits marginal superiority over both 5th epoch and 15th epoch. This observation aligns with the theoretical framework proposed in [27], suggesting that the original network's weights are prone to stability against stochastic gradient descent (SGD) noise after an initial training period. However, an extended training duration may result in weight solidification."}, {"title": "4.5 RQ4: Impacts of Configurable Parameters", "content": "Experiment Design: BALLOT leverages two hyperparameters \u03b3 and \u03b7 in mask generation. The former is used to control the contribution of fairness loss Lf and accuracy loss La. The latter controls the neuron sorting ratio, which determines which neurons in each iteration will be considered to have the highest degree of conflict and increment the count. We conduct experiments to investigate and understand how different values of these configurable hyperparameters affect the performance of BALLOT in finding and claiming the winning ticket. In the experiment, we run BALLOT on the CIFAR-100 dataset with different log \u03b3 values from -1.0 to 2.0 and different \u03b7 values from 0.7 to 0.99. To evaluate the effect of different values of configurable parameters, we record the accuracy, CWV, and MCD of the model pruned by BALLOT with different parameter settings.\nImpact of \u03b3: BALLOT uses the parameter \u03b3 to control the impact of the fairness loss in mask generation. A higher \u03b3 leads the pruned model to pay more attention to fairness. As shown in Figure 6(a) and Figure 6(b), when the value of log \u03b3 increases, the accuracy of the model pruned by BALLOT first increases and then decreases, while the values of CWV and MCD generally show a downward trend. When log \u03b3 is 1.0, the accuracy of the pruned models reaches the highest value of 64.49%. It is noteworthy that at the two points of 0.5 and 1.0, there are abnormal increases in the values of CWV and MCD, and then their values continue to decline as the value of log \u03b3 increases. However, the increment brought by these outliers is relatively small, and the impact on the model fairness is not significant. Considering the trade-off between accuracy and fairness, BALLOT selects 1.0 as the default value of log \u03b3 (i.e., \u03b3 is set to 10) to maximize the effectiveness on improve model fairness and utility.\nImpact of \u03b7: \u03b7 is used to determine which neurons will be regarded as conflicting neurons and counted. A higher \u03b7 indicates that in mask generation of BALLOT, the criteria for evaluating conflicting neurons are more strict. The experiment results in Figure 6(c) and Figure 6(d) show that the model performance of both utility and fairness first increase and then decreases as \u03b7 rises, and BALLOT performs best when \u03b7 is set to 0.95. Therefore, we choose 0.95 as the default value of \u03b7.\nAnalysis: From Figure 6, we can observe that different values of \u03b3 and \u03b7 have a significant impact on the performance of the pruned model. In terms of accuracy, as the values of \u03b3 and \u03b7 increase, the model accuracy first increases and then decreases. Similarly, in terms of fairness, the increase of \u03b7 first improves the performance of the model fairness and then worsens it. In addition, the increase of \u03b3 improves the model fairness in general. As a result, to effectively select the winning ticket and prune a sparse model toward both accuracy and fairness, we set the default value of \u03b3 to 10, and the value of \u03b7 to 0.95 in BALLOT.\nSummarization. We have proved the advancement of BALLOT through the above experimental evaluations. Compared to the baseline, BALLOT effectively balances fairness and accuracy throughout the pruning process and efficiently identifies equitable subnetworks, thereby expediting training. BALLOT requires a more intricate procedural approach than conventional pruning methods, resulting in higher training costs than direct pruning. However, this additional expense remains relatively modest, averaging no more than a 6% increase."}, {"title": "5 THREAT TO VALIDITY", "content": "BALLOT is evaluated on five mainstream datasets with ResNet50, VGG16, and BERT under the sparsity of 0.05, which may be limited. Additionally, the existence of several configurable parameters introduces some threats. While our experiments demonstrated promising pruning results, the effectiveness remains uncertain and challenging when applied to more complex and advanced models like transformer. To address those concerns, we have taken the step of open-sourcing the implementation of BALLOT and providing comprehensive experimental details that encompass models before and after pruning, as well as training configurations. For reproduction purposes, all codes and data are available at [1]."}, {"title": "6 RELATED WORK AND DISCUSSION", "content": "Model Compression. Model compression aims to reduce the size and computational complexity of large neural network models. There are various model compression methods, such as network pruning, parameter quantization, and knowledge distillation.\nNetwork pruning. Pruning methods focus on removing redundant components without sacrificing performance significantly. Channel-based pruning removes entire channels (feature maps) from convolutional neural networks [70]. Filter-based pruning selectively removes individual filters (kernels) [41, 51]. The unimportant connections selection is based on criteria such as weight magnitude [37, 38], gradients [63], and hessian [50] statistics.\nParameter quantization. Parameter quantization of neural networks is the process of converting the weights and activation values of a network model from high precision to low. Quantization-aware training (QAT) [65] introduces a pseudo-quantization method to simulate the error brought by the quantization process. PostTraining quantization (PTQ) methods [44] directly quantize the pre-trained model without retraining, which is more efficient but may cause more accuracy loss.\nKnowledge distillation. Knowledge distillation is a training paradigm with teacher-student architecture. The teacher network (a complex pre-trained network) provides the student network (a simple small network) with prior knowledge so that the student network achieves similar performance to that of the teacher. Researchers have tried to train student networks using different prior knowledge [12, 42].\nThe software engineering community has exhibited significant interest in model compression. Shi et al. investigate the application of knowledge distillation techniques to condense pre-trained code models, reducing their size to a mere 3MB, thereby rendering them easily deployable [76]. Zhu et al. delve into the advantages of employing test-driven development for pruning, incorporating bi-objective optimization to balance performance and safety properties [98]. Quality assurance for the deployment of compression-based Al models is a subject of great concern. Xie et al. conduct differential testing to compare the behaviors of models before and after compression [89]. Additionally, Zhang et al. propose an ILP-based formal verification approach for quantized neural networks [96].\nFairness of ML. The issue of fairness in ML is gaining increasing prominence, in the areas of CV [82], NLP [9, 91] and especially critical automated decision-making systems like higher education [20], employment [33, 71, 86], and re-offense judgement [17, 66].\nTo address the concern above, fairness testing for ML systems is gaining attention in the software engineering community. AEQUITAS proposes a directed search for individual discriminatory instances [85]. Symbolic Generation (SG) integrates symbolic execution and local model explanation to craft individual discriminatory instances [7]. ExpGA uses the genetic algorithm to generate discriminatory instances [23]. Besides, ADF and EIDIG combine global search and local search to systematically explore the input space with the guidance of gradient [94, 95]. There are also concerns about group fairness. THEMIS considers group fairness using causal analysis and uses random test generation to evaluate fairness [8]. A recent framework namely FairRec is dedicated to uncovering disadvantaged groups in recommender systems [35].\nWhile uncovering fairness issues through testing, researchers attempt to mitigate and repair fairness deficiencies. Pre-processing approaches focus on mitigate dataset bias by correcting labels [46, 93], revising attributes [24, 47], generating non-discrimination data [72, 90], and constructing fair data representations [14, 52]. In-processing and post-processing approaches aim to mitigate algorithm bias by improving the learning process or the learned model. More specifically, these approaches propose an objective function considering the fairness metric of prediction [92], adapt fairness-driven generative adversarial framework [6, 30, 90], or directly change the predictive labels of bias models' output [39, 69], etc.\nIn the SE community, the most common way to improve model fairness is to retrain based on testing-generated discriminatory instances [23, 94, 95]. Further, Sun et al. propose to apply causal analysis to identify neurons that are guilty of introducing bias [78]. Gao et al. adapt path analysis to selecting neurons that lead to conflicting accuracy and fairness joint-optimizations [29]. Recently, the fairness issue in the maintenance and deployment of ML models has arrived at concern, e.g. CILIATE is proposed to mitigate bias amplification in the incremental learning process [31].\nDiscussion. To the best of our knowledge, none of the current repairing methods have been designed to consider addressing the fairness issues of model compression typified by pruning. We aim to raise awareness of fairness issues in model compression, which is a critical step towards the development of responsible AI systems."}, {"title": "7 CONCLUSION", "content": "Inspired by ethics-aware software engineering, we propose and develop BALLOT, an innovative fairness-aware deep neural network pruning framework, powered by conflict-detection-based mask generation and training refinement. It can identify accurate and fair tickets (pruned model prototype) and refine the ticket training process to obtain the high-performance pruned models. Our evaluation results show that BALLOT effectively mitigates pruning fairness problems and outperforms all baselines in terms of fairness and accuracy."}, {"title": "8 DATA AVAILABILITY", "content": "All source code and data used in our work can be found at [1]."}]}