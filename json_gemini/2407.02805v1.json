{"title": "Efficient DNN-Powered Software with Fair Sparse Models", "authors": ["Xuanqi Gao", "Weipeng Jiang", "Juan Zhai", "Shiqing Ma", "Xiaoyu Zhang", "Chao Shen"], "abstract": "With the emergence of the Software 3.0 era, there is a growing\ntrend of compressing and integrating large models into software\nsystems, with significant societal implications. Regrettably, in nu-\nmerous instances, model compression techniques impact the fair-\nness performance of these models and thus the ethical behavior\nof DNN-powered software. One of the most notable example is\nthe Lottery Ticket Hypothesis (LTH), a prevailing model prun-\ning approach. This paper demonstrates that fairness issue of LTH-\nbased pruning arises from both its subnetwork selection and train-\ning procedures, highlighting the inadequacy of existing remedies.\nTo address this, we propose a novel pruning framework, BALLOT,\nwhich employs a novel conflict-detection-based subnetwork selec-\ntion to find accurate and fair subnetworks, coupled with a refined\ntraining process to attain a high-performance model, thereby im-\nproving the fairness of DNN-powered software. By means of this\nprocedure, BALLOT improves the fairness of pruning by 38.00%,\n33.91%, 17.96%, and 35.82% compared to state-of-the-art baselines,\nnamely Magnitude Pruning, Standard LTH, SafeCompress, and\nFairScratch respectively, based on our evaluation of five popular\ndatasets and three widely used models. Our code is available at\nhttps://anonymous.4open.science/r/Ballot-506E.", "sections": [{"title": "1 INTRODUCTION", "content": "We envisage that the Software 3.0 era, characterized by software\npowered by large-scale models, will pave the way for numerous\npotential applications, such as artificial intelligence generated con-\ntent (AIGC) and autonomous driving, which are poised to exert\nsubstantial influence on societal transformations [75, 97]. The mag-\nnitude of AI software has notably surged, primarily driven by the\nescalating size of deep neural network models [62]. For instance,\nstate-of-the-art computer vision models now encompass over 15 bil-\nlion parameters, while large language models like GPT-3 exceed 175\nbillion parameters, demanding nearly 1TB of storage exclusively\nfor the model itself [18].\nHowever, the deployment of large-scale AI software introduces\nseveral challenges. These large models require substantial memory\nand storage resources during deployment, presenting difficulties in\nrunning them on resource-constrained devices like edge devices,\nsmartphones, or wearables [36]. Furthermore, the size of these mod-\nels often leads to slow inference times due to the sheer number of\nparameters and computations involved, which can be particularly\nproblematic in time-critical applications such as real-time object\ndetection or autonomous driving [54]. Additionally, when Al mod-\nels are deployed on cloud servers and accessed remotely by client\ndevices, the transfer of large model files over limited bandwidth can\nresult in high latency and increased data consumption [87]. In addi-\ntion to these deployment difficulties, the resource-intensive nature\nof large Al applications poses further challenges. The computational\ndemands of these models significantly consume energy, which be-\ncomes a critical concern for battery-operated devices or data centers\nstriving for energy efficiency [57]. Moreover, the carbon footprint\nassociated with the training and deployment of large models can\nbe substantial, contributing to environmental concerns [73].\nTo address the aforementioned challenges, researchers employ\nmodel compression techniques, which aim to reduce the size and\ncomplexity of Al models while retaining their performance. Various\nmodel compression techniques have been introduced, encompass-\ning model pruning [25, 41, 51, 58, 70], model quantization [44, 65],\nand knowledge distillation [11, 12, 42]. Notably, the model pruning\nalgorithm stands out as one of the most widely adopted methods in\nthis domain [55, 62]. Pruning involves the selective removal of un-\nnecessary parameters, connections, or entire neurons, leading to a\nmore streamlined and efficient model. As a state-of-the-art method,\nthe Lottery Ticket Hypothesis (LTH) has garnered significant at-\ntention and extensive research efforts [25]. The hypothesis posits\nthe existence of a winning ticket, namely a properly pruned sub-\nnetwork combined with the original weight initialization, which\ncan achieve competitive performance comparable to that of the\noriginal dense network. This discovery underscores the immense\npotential for efficient training and network design in the realm of\ndeep learning.\nUnfortunately, among the various research on the LTH and also\nour experiment results (see \u00a72.3), LTH-based pruning methods suf-\nfer from model bias problem [60]. Through our analysis, we found\nthat the bias problem is caused by both ticket selection and ticket\ntraining. The process of ticket selection entails the elimination of\nredundant neurons, resembling high-dimensional feature selection\nfor images, and potentially introducing bias. Fairness and accuracy\ncan exhibit conflicts during ticket selection, with prevailing LTH-\nbased pruning methods prioritizing accuracy, resulting in tickets\nthat lack fairness. Even after obtaining a ticket that balances ac-\ncuracy and fairness, there is no assurance that the ticket can be\ntrained to reach its theoretical performance upper limit. The train-\ning process faces potential issues of overfitting and underfitting,\nimpacting the model's ability to generalize and make accurate and\nfair predictions. Current efforts to enhance the LTH-based prun-\ning primarily concentrate on improving its efficiency and accuracy\nperformance, neglecting its fairness concerns.\nInspired by ethics-aware software engineering [10, 19, 28], we\npresent BALLOT, a novel fairness-aware pruning framework by re-\nvised winning ticket finding and training for deep neural networks.\nThe key idea of BALLOT is that it follows the best practice of soft-\nware engineering by first observing and analyzing the root cause\nof the bias problem. Concretely, it performs a conflict detection\nbetween the fairness optimization direction and the accuracy opti-\nmization direction of the model during the model training process to\nidentify neurons with the highest degree of conflict. These neurons\nare more likely to induce the model to optimize towards greater\nvariance among distinct subgroups during training, leading to bi-\nased model behavior and subsequently giving rise to instances of\ndiscriminatory behavior in the software. Each neuron is assigned a\nscore representing its importance in training optimization conflicts,\nand a corresponding mask is generated to remove these neurons,\nresulting in the acquisition of an accurate and fair ticket. To ensure\nthat the ticket can eventually be trained into high-performance\nmodels, we also refine the training procedure. This involves adapt-\ning the learning rate based on the training condition to facilitate\nthe model's learning of richer features. Furthermore, we verify the\nfairness performance of the model upon the completion of training.\nIf the fairness performance is inferior to that of the original model,\nwe reload the weights from earlier training rounds (the specific\nnumber of rounds predetermined by empirical considerations) and\niteratively retrain the model until further performance improve-\nment is unattainable. This refined training approach is designed to\nmaximize the ticket's performance potential.\nBALLOT has been implemented as a self-contained toolkit. Our\nexperiments on CIFAR-100, TinyImageNet, and CelebA datasets\nshow that BALLOT can effectively mitigate the pruning fairness\nproblem while maintaining accuracy, compared with existing prun-\ning methods [26, 38, 80, 98].\nOur contribution can be summarized as follows:\n\u2022 We propose a novel fairness-aware DNN pruning framework.\nIt leverages conflict-detection-based mask generation to find\nfair and accurate tickets, and training refinement to achieve\noptimal performance.\n\u2022 We develop a prototype BALLOT based on the proposed idea,\nand evaluate it with CIFAR-100, TinyImageNet and CelebA.\nOn average, BALLOT improves fairness by 38.00%, 33.91%,\n17.96%, and 35.82% compared to state-of-the-art baselines,\nnamely Magnitude Pruning, Standard LTH, SafeCompress,\nand FairScratch, outperforming all baselines in terms of fair-\nness.\n\u2022 Our implementation, configurations and collected datasets\nare available at [1]."}, {"title": "2 BACKGROUND AND MOTIVATION", "content": "2.1 DNN Software and DNN Model Deployment\nThe continuous advancements in deep neural network (DNN) re-\nsearch and the availability of powerful computing resources have\ncontributed to the widespread adoption of DNNs in various appli-\ncations, making DNN-powered software a key driver of innovation\nin the field of software engineering [61]. DNN-powered software\nrefers to applications and systems that leverage the capabilities of\nDNNs for various tasks, which are often developed with deep learn-\ning frameworks and libraries such as PyTorch [3], TensorFlow [5],\nand Keras [34]. However, as the capabilities of deep learning mod-\nels gradually increase, their scale also becomes larger and larger,\nnaturally taking up a lot of storage space [64]. The deployment\nDNN software encounters complexities due to the diverse software\nlandscape and deployment demands on various mobile platforms.\nConfronted with resource limitations, such as computational power,\nstorage capacity, and battery life, especially in embedded devices\nlike mobile devices, the imperative arises for mobile models to\nadhere to stringent conditions encompassing reduced model size,\ndiminished computational complexity, and minimal battery power\nconsumption [55]. Consequently, a challenge is raised for DNN-\npowered software developers: how can a DNN-powered software\nbe effectively deployed to meet performance requisites on platforms\ncharacterized by constrained resources? The compression of DNN\nmodels emerges as an indispensable operation in the deployment\nof DNN-powered software.\nNumerous studies emphasize the requisite step of model com-\npression for deploying DNN models across diverse platforms [55,\n64]. Predominantly employed techniques for DNN model compres-\nsion encompass model quantization [65, 79], model pruning [58, 70],\nand knowledge distillation [12, 42]. Among these, the model prun-\ning is widely favored and commonly utilized, due to its capabil-\nity to directly reduce the number of computational operations in-\nvolved, fundamentally alleviating computation and memory pres-\nsures [53, 54]. Popular AI software development frameworks such\nas PyTorch [84], Tensorflow [81] and PaddlePaddle [67], all inte-\ngrate pruning-based model compression toolkits.\nModel pruning is a kind of a prominent and effective compres-\nsion technique in the field of large-scale AI software deployment.\nThe motivation behind model pruning stems from the recognition\nthat many deep learning models, especially those with an over-\nparameterized nature, contain numerous redundant or less critical\nparameters. By selectively removing unimportant components or\nparameters from a model, this approach achieves a remarkable\nreduction in model size, rendering it more lightweight and com-\nputationally efficient as well as not significantly sacrificing model\nperformance. In a DNN, the architecture consists of layers of inter-\nconnected nodes (neurons), and each connection between nodes\nis associated with a weight. These weights, along with biases, are\nthe parameters of the model, and they are collectively represented\nby the symbol \u03b8. A DNN model f(\u03b8) is a function that takes input\ndata and produces output based on these parameters. More for-\nmally, model pruning aims to produce a lightweight model f(\u03b8*)\nsatisfying the following conditions:\n$\\frac{|\theta^*|}{|\theta|} < \\Omega$\nAcc(f(\u03b8)) - Acc(f(\u03b8*)) \u2264 \u0454"}, {"title": "2.2 Fairness Problem in Pruning", "content": "While model pruning strikes a trade-off between accuracy and\nmodel size, recent research take attention to potential fairness con-\ncerns. For example, Paganini points out that pruning should not\nonly focus on overall performance metrics but should also con-\nsider performance differences across subgroups and individuals,\ni.e., the fairness of model [68]. In machine learning, model fair-\nness refers to the ethical and unbiased treatment of individuals or\ngroups throughout the development, deployment, and utilization of\nmachine learning models. Consequently, the definition of fairness\nis also divided into two categories: individual fairness and group\nfairness. Blakeney et al. find that the bias of the model grew progres-\nsively as the pruning rate increased [15]. For a pruning technique to\nbe considered fair, the bias degree of the small model after pruning\nshould not significantly increase compared to the metrics of the\noriginal model before pruning. Formally, for an original model f (\u03b8)\nand a pruned model f(\u03b8*), the fairness criterion can be defined as:\nd(f(\u03b8*)) \u2013 d(f(\u03b8)) \u2264 \u03b4\nwhere d() computes the bias degree (i.e. higher values represent\nlower levels of fairness), and \u03b4 is the tolerance. As previously\nmentioned, the definition of fairness is categorized into individual\nfairness and group fairness. This paper specifically concentrates\non group fairness. For datasets with sensitive features, fairness\nmetrics such as demographic parity (DP) [22] and equal oppor-\ntunity (EO) [39] are commonly employed to assess whether the\nmodel exhibits discrimination against distinct groups. Since vision\nand natural language processing tasks usually have neither explicit\nsensitivity characteristics nor explicit favorable/unfavorable treat-\nment, we utilize class-wise variance (CWV) [83] and maximum\nclass-wise discrepancy (MCD) [83] to measure variations in the\nmodel's performance across different groups, i.e. to measure the\nbias degree.\nThese metrics are formally defined below:\nClass-wise Variance (CWV). Given a dataset with C classes and a\ngiven model, the accuracy on the subdataset belonging to cth class\nis denoted as $acc_c$.\nCWV = $\\sum_{c=1}^{C} (acc_c - acc)^2$, acc = $\\frac{\\sum_{c=1}^{C} acc_c}{C}$\nMaximum Class-wise Discrepancy (MCD). For a model, given\nthe maximum and minimum class accuracy $acc^+$ and $acc^-$, MCD\ncan be defined as:\nMCD = $acc^+ - acc^-$\nIntuitively, CWV represents the average discrepancy, while MCD\nassesses the extreme discrepancy among classes to estimate the\nfairness of a model. Larger values of these metrics indicate a higher\ndegree of bias in the model. For example, for a software that in-\ncludes a face recognition module, each class corresponds to one"}, {"title": "2.3 Lottery Ticket Hypothesis", "content": "A recently proposed technique known as the Lottery Tickets Hy-\npothesis (LTH) has emerged as a rapidly growing method in the field\nof model pruning, which focuses on sparse trainable subnetworks\nwithin fully dense networks [25]. The LTH posits the presence of a\nsparse subnetwork within a randomly initialized dense network,\nwhich can achieve test accuracy comparable to that of the original\ndense network within at most the same number of iterations dur-\ning independent training. This sparse subnetwork is known as the\n\"winning ticket\". More specifically, for a randomly initialized dense\nneural network f(\u03b8\u2080) parameterized by \u03b8\u2080 \u2208 R|\u03b8\u2080|, LTH suggests\nsearching a mask m\u2208 ({0,1}|\u03b8\u2080|), i.e., the winning ticket is the\nsparse subnetwork f(\u03b8\u2080 \u2299 m), where \u2299 denotes the element-wise\nproduct and |\u03b8\u2080| denotes the numbers of parameters. To find a\nwinning ticket, it is common practice to first train the initialized\nnetwork for some epochs and compute a mask based on the behav-\niors of the trained model. LTH-based pruning usually follows the\ntrain-prune-retrain pipeline as shown in Figure 4. Given a dense\nneural network f(\u03b8\u2080) initialized by the parameter \u03b8\u2080 \u2208 R|\u03b8\u2080| and\nthe sparsity metric \u03a9, the process of finding a winning ticket and\nperform pruning can be expressed as:\ni) Train the network f(\u03b8\u2080) for E epochs to get a well-trained\nmodel f(\u03b8).\nii) Remove parameters in \u03b8 which satisfied the sparsity metric\n\u03a9,by creating a mask m\u2208 ({0, 1}|\u03b8\u2080|). Reset the remaining\nparameters to their initial values in \u03b8\u2080, creating the winning\nticket f(\u03b8\u2080 \u2299 m).\niii) Retrained the f (\u03b8\u2080 \u2299 m) to get final pruned model f(\u03b8*).\nIdeally, the performance of f(\u03b8*) is expected to surpass that of\nf (\u03b8) within a limited number of training epochs, not exceeding E.\nHowever, in a standard LTH setup [25], a winning ticket can only\nbe found when the learning rate is very small, and the performance\nadvantage will become smaller when the learning rate is large. How\nto find lotteries that actually win is still a problem worth exploring,\nbut this does not detract from the practical value of LTH-based\npruning to select high-value subnetworks. A series of efforts were\ndevoted to finding the most promising ticket, which can achieve\nhigher performance[59].\nDespite the remarkable success of LTH-based pruning in achiev-\ning high performance, it is crucial to recognize that it still grap-\nples with the challenge of fairness. To demonstrate this matter, we\npruned the widely utilized open-source model ResNet50 Gender"}, {"title": "3 SYSTEM DESIGN", "content": "3.1 Problem Statement\nIn this paper, we aim to develop an automated model pruning frame-\nwork that concurrently addresses both pruning rate and fairness\nconsiderations. For a given initialized complex network f(\u03b8\u2080) and\ntraining epochs E, the f(\u03b8\u2080) can be normally trained into a model\nf(\u03b8) with E epochs. We focus on how to select a ticket to satisfy the\nspecified sparsity metric \u03a9, and retrain this ticket into a final sparse\nmodel f(\u03b8*) by a training method T. The f(\u03b8*) should attain a\nhigh level of fairness, while fulfilling the availability requirement\nstated by the sparsity metric \u03a9 and accuracy specifications \u0454 as\nEquation 1 and Equation 2. Formally, our objective is:\nmin [d(f(\u03b8*)) \u2013 d(f(\u03b8))]\nm,T\ns.t. $\\frac{|\\theta^*|}{|\\theta|} \\le \\Omega$\nAcc(f(\u03b8)) \u2013 Acc(f(\u03b8*)) \u2264 \u0454\nwhere the d() \u2208 {CWV, MCD} is a measure of bias degree referred\nto \u00a72.2, which represents a higher fairness with lower indicator\nvalue. The |\u03b8| and |\u03b8* | calculate the number of parameters, while\nAcc() calculates the accuracy of the model on the test dataset.\nNote the pruning rate is (1 \u2013 \u03a9). Our objective is to enhance the\nfairness of the pruning model as much as possible while preserving\nits performance (i.e., accuracy). This involves finding a balance\nbetween accuracy and fairness, making trade-offs when necessary."}, {"title": "3.2 Root Cause Analysis", "content": "In pursuit of an accurate and fair ticket, it is essential to consider\nthe trade-off involved in optimizing both aspects. We believe that\ntrade-off arises at two primary levels: first, in determining what\ncomponents of the model should be removed during pruning (re-\nferred to as ticket selection); and second, in devising an appropriate\ntraining strategy for the model after the removal (referred to as\nticket training). Below, we conduct an exhaustive analysis of the\nfactors contributing to fairness issues in both ticket selection and\nticket training.\n3.2.1 Ticket Selection. Fairness and accuracy may conflict in the\nsearch for tickets. For DNN models, the ticket selection and prun-\ning process involves removing redundant neurons. Typically, high-\ndimensional features extracted from samples are stored in neurons\nas combinations. Thus, ticket selection is equivalent to feature se-\nlection on the dataset, which represents a form of high-dimensional\nfeature selection for a sample. However, this selection process may\nintroduce bias. As previously discussed, LTH methods predomi-\nnantly prioritize accuracy, potentially leading to the identification of\ntickets that lack fairness. Such tickets may resort to taking shortcuts\ninstead of genuinely learning the intended solution. For instance,\nin a face recognition software deployed in a predominantly white\nneighborhood, a ticket may only use skin color as the determi-\nnant of whether a visitor is a local resident, resulting in reduced\ngeneralization and fairness [32]. Conversely, exclusively focusing\non fairness during ticket identification may result in a decline in\naccuracy. Intuitively, if a face recognition software blocks out all\nfeatures related to skin color to be fair, its face recognition perfor-\nmance is likely to suffer considerably. Hence, a trade-off must be\ncarefully considered throughout the process of ticket selection to\nstrike a balance between fairness and accuracy."}, {"title": "3.2.2 Ticket Training", "content": "Even upon obtaining a ticket that strikes\na balance between accuracy and fairness, there is no guarantee\nof successfully \"claiming a prize\", that is, training a model that\nattains the upper limit of its theoretical performance. There are two\npotential issues related to the training process. First, if the model\nexcessively focuses on learning the features of existing training\nsamples, it may treat some of these specific features as if they\nare universally representative of all potential samples, leading to\noverfitting. In such cases, the model becomes too tailored to the\ntraining data and may not generalize well to new, unseen samples,\nthus introducing bias. Conversely, if the model's learning capacity\nis too limited, it may not adequately capture the general features\nof the samples. This situation, known as underfitting, results in\na failure to learn critical patterns from the data, leading to poor\nperformance on both training and unseen samples. Achieving an\noptimal balance between overfitting and underfitting is essential for\nobtaining a model that generalizes well and demonstrates accurate\npredictions on new and diverse samples.\nWe conduct a series of training experiments on the CIFAR-\n100 dataset, utilizing the same ticket (model prototype) acquired\nthrough standard LTH pruning with a pruning rate of 0.95. The\nexperiments involve training the model for 50 and 150 epochs (un-\nderfitting model), 250 epochs (normally fitting model), and 500, 750,\nand 1000 epochs (overfitting model). Throughout all experiments,\na uniform learning rate of 0.03 is employed. Then we evaluate the\naccuracy and fairness of these trained models. Figure 3 compares\nthe accuracy, CWV, and MCD of these models. As we can see, the\nnormal model exhibits the most favorable performance in both fair-\nness and accuracy, whereas the underfitting model demonstrates\nthe poorest performance. This highlights that, despite employing\nthe same ticket, the training process significantly influences the\nfinal performance of the retrained model."}, {"title": "3.3 System Overview", "content": "Through our analysis, it becomes evident that existing methods\nencounter challenges in finding tickets that simultaneously achieve\nboth accuracy and fairness while successfully training a model.\nIn response, our proposed framework, BALLOT, introduces novel\nconflict-detection-based mask generation and training refinement\ntechniques to obtain accurate and fair sparse models. We first lo-\ncate the root cause, namely neurons exhibiting conflicts between\nfairness and accuracy during optimization, and then generating\nmasks that block out these neurons to get a sparse model and train\nit by a comprehensive strategy to improve its fairness performance.\nThe primary objective of mask generation is to ensure that the\nmodel focuses on the most relevant features during the learning\nprocess. The training refinement is pivotal in ensuring that the\nmodel not only captures essential features but also achieves equi-\ntable outcomes across different groups or classes."}, {"title": "3.4 Mask Generation", "content": "To address this challenge mentioned in \u00a73.2.1, we propose a novel\nmask generation approach termed conflict-detection-based mask\ngeneration. Specifically, we collect the gradients corresponding to\nthe fairness and accuracy of each neuron during training the model\nin the normal way, and then locate neurons with optimization\nconflicts and generating masks to remove them.\nPrior to delving into our approach, let us first introduce the\nprecise definitions of the accuracy loss function (denoted as la)\nand the fairness loss function (denoted as lf) employed in our\nmethodology. The la and lf can be formulated as:\nla = -$\\sum_{c=1}^{C} w_c log \\frac{exp(x_{n,c})}{\\sum_{i=1}^{C} exp(x_{n,i})}$\nlf = -$\\sum_{c=1}^{C} w_c log (y_{n,c} \\frac{exp(x_{n,c})}{\\sum_{i=1}^{C} exp(x_{n,i})})$\nIn the context of the provided equations, C denotes the number\nof classes, x represents the input data, y denotes the target and\nw corresponds to the weight. Here, we set the weights as,\nwhere $acce$ denotes the accuracy of the previous iteration of the\nmodel. For each respective class, we take this value as an estimate\nof the current iteration's accuracy. The choice of weights follows\nan inverse proportionality to the accuracy achieved in the prior\niteration for individual classes."}, {"title": "3.5 Training Refinement", "content": "Upon identifying a ticket, our task is only halfway completed. While\nthis ticket holds the potential to serve as a fair and accurate model,\ncareful training is indispensable to enable it. Traditional LTH meth-\nods involve loading the ticket with the initial base model weights \u03b8\u03bf\nand subsequently performing retraining, akin to the initial training.\nHowever, such an approach lacks optimization steps and neglects\nperformance validation of the trained model. Thus, there is a need\nfor a more comprehensive training strategy that includes optimiza-\ntion techniques and robust performance validation to ensure the\nmodel attains its full potential.\nTo address this, we undertake a comparison of various training\npolicies and optimize the training procedure employing a technique\ntermed training refinement. In detail, we commence the training\nrefinement process by reloading the initial weights into the pruned\nmodel. Subsequently, we conduct model training using a decreas-\ning learning rate strategy to facilitate learning as many essential\nfeatures as possible while avoiding overfitting. The objective is to\nstrike a balance between adequately capturing necessary patterns\nin the data and ensuring the model's ability to generalize effec-\ntively to new, unseen samples. Ultimately, we validate the retrained\nmodel. If the model's fairness performance falls below that of the\noriginal model, we initiate a feedback loop by returning to the\nfirst step. During this process, instead of reloading the base model\ninitialization weights, we reload the weights after completing a\nspecified number (typically a small single or double digit number"}]}