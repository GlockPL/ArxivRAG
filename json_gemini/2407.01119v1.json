{"title": "Pron vs Prompt: CAN LARGE LANGUAGE MODELS ALREADY CHALLENGE A WORLD-CLASS FICTION AUTHOR AT CREATIVE TEXT WRITING?", "authors": ["Guillermo Marco", "Julio Gonzalo", "Ram\u00f3n del Castillo", "Mar\u00eda Teresa Mateo Girona"], "abstract": "It has become routine to report research results where Large Language Models (LLMs) outperform average humans in a wide range of language-related tasks, and creative text writing is no exception. It seems natural, then, to raise the bid: Are LLMs ready to compete in creative writing skills with a top (rather than average) novelist? To provide an initial answer for this question, we have carried out a contest between Patricio Pron (an awarded novelist, considered one of the best of his generation) and GPT-4 (one of the top performing LLMs), in the spirit of AI-human duels such as DeepBlue vs Kasparov and AlphaGo vs Lee Sidol. We asked Pron and GPT-4 to provide thirty titles each, and then to write short stories for both their titles and their opponent's. Then, we prepared an evaluation rubric inspired by Boden's definition of creativity, and we collected 5,400 manual assessments provided by literature critics and scholars. The results of our experimentation indicate that LLMs are still far from challenging a top human creative writer, and that reaching such level of autonomous creative writing skills probably cannot be reached simply with larger language models.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have recently showed strong competences generating human-like text, and in particular in creative writing tasks [Achiam et al., 2023], which is the focus of this paper. LLMs are increasingly influencing creative industries, impacting both the economy and the labor market, as highlighted by significant events such as the Hollywood screenwriters' strike [Lee, 2022, Eloundou et al., 2023, Koblin and Barnes, 2023]. Experimentation shows"}, {"title": "2 Related Work", "content": "Since the rise of LLM technology, creative text writing has gained renewed interest within the NLP research community. Franceschelli and Musolesi [2024] survey machine learning and creativity, discussing computational creativity theories, generative techniques, and evaluation methods. Evaluating creativity remains challenging [H\u00e4m\u00e4l\u00e4inen and Alnajjar, 2021, Chakrabarty et al., 2023], but progress is being made; for an extensive explanation of the challenges of evaluating computational creativity see Lamb et al. [2018].\nRegarding machine-assisted human writing, Swanson et al. [2021] introduced Story Centaur, a tool for creative writers to prototype few-shot learning models. And Chakrabarty et al. [2022] presented CoPoet, a system for poetry writing that enhances user-generated content. In both cases, evaluators prefer texts generated in co-authorship with IA systems. However, Kreminski and Martens [2022] highlighted limitations in current LLM tools, such as issues with narrative consistency and plot development.\nOur focus is rather on autonomous LLMs creative writing. Gunser et al. [2022] examined the stylistic quality of AI-generated texts, finding them generally rated lower than human-written texts despite being indistinguishable. Marco et al. [2023] compared a fine-tuned BART model and ChatGPT 3.5 with human writers on a creative task, noting that while the BART model excelled in grammaticality and coherence, it matched humans in creativity but used more clich\u00e9s. Limitations of their study included the non-creative nature of human synopses and a casual reader assessment, unlike our expert-based approach.\nThe study by G\u00f3mez-Rodr\u00edguez and Williams [2023] examines the capability of several large language models (LLMs) in autonomous English creative writing, focusing on a single imaginative task where models and humans compose a story about a combat between Ignatius J. Reilly and a pterodactyl. They reveal that LLMs performed well in fluency and coherence but lagged in creativity and humor. Their study's single-task focus contrasts with our broad evaluation of 60 titles.\nLastly, Chakrabarty et al. [2024] proposed the Torrance Test of Creative Writing (TTCW) to evaluate AI-generated stories. Their findings reveal that while LLMs perform well in terms of fluency and structure, they lag significantly behind human writers in originality and emotional depth. A limitation of the study is that the tasks given to humans and machines are asymmetrical: human stories are selected from already published material. Then, GPT-4 summarizes the stories, and LLMs are asked to generate a full story starting from each of the summaries, which is only a part of the creative writing process. Another difference in methodology is that they adapt the TTCW test for their rubric, while we design our rubric following Boden's notion of creativity.\nOverall, our study complements previous work being the only one that simultaneously (i) uses the best possible writer and LLM for the experimentation; (ii) gives the same tasks to both contenders in equal conditions (iii) explores 60 different writing assignments (proposed by the contenders) and collects 5,400 expert assessments for a rubric that adapts Boden's notion of creativity to the task, and (iv) includes a study on the effect of the prompt and also measures the gap between texts written in English and Spanish."}, {"title": "3 Experimental Design", "content": "Contenders. The LLM chosen for the experiment is GPT-4 Turbo (in gpt-4-0125-preview version), which was the strongest LLM when we initiated the experimentation. After some initial experimentation with the system, we fixed temperature at 1. Going beyond this value occasionally impacted on grammaticality (particularly with Spanish texts), so we chose the highest value that produced always formally correct texts. Once the experiment was initiated, other LLMs that seemed to rival the performance of GPT-4 were launched: most notably Claude 3 Opus, Gemini Ultra and Llama 3. Experimenting with these models, we did not notice any clear advantages with respect to GPT-4, so we proceed with our initial setup.\nFinding a top novelist that would engage in this experiment was easier than we initially thought. We contacted Patricio Pron because, besides being awarded with some of the most prestigious distinctions in Spanish literature (the Alfaguara"}, {"title": "Task design", "content": "In the first stage, each contender proposed 30 movie titles. In the second stage, both contenders wrote synopses (approximately 600 words) for each of the 60 titles. The prompt for GPT-4 was as follows: \u201cWe are conducting an experiment to compare your creative writing skills with those of the renowned novelist Patricio Pron. Your task is to generate synopses for imaginary movie titles. These synopses should be creative, appealing to critics and audiences, and possess inherent literary value. Here is some information about Patricio Pron: he is a celebrated writer, recognized as one of the top young writers in Spanish by Granta in 2010, and the winner of the Alfaguara Prize in 2019 for his work Ma\u00f1ana tendremos otros nombres. The proposed title is: {title}. Please write a 600-word synopsis that meets these criteria.\"\nThe initial process was in Spanish, and titles were later translated into English for GPT-4 to generate English synopses. Our dataset includes 60 titles, 60 texts by Pron, and 60 texts by GPT-4 for each of the two languages."}, {"title": "Rubric Design", "content": "The rubric, designed by three experts in pedagogy, psychometric, literature, and NLP, focuses on creativity-related dimensions, as previous work has shown that LLMs already excel at grammaticality, coherence and fluency [Marco et al., 2023].\nThe point of departure is Margaret Boden's definition [Boden, 2003]: Creativity is the ability to come up with ideas that are new, surprising, and valuable.; it is a simple, operative definition compatible with most studies on the subject, both from philosophers and psychologists, with a long tradition [Gaut, 2010]. It is a conceptualization of creativity in three specific dimensions: novelty, surprise and value.\nThe experts agreed that, in fiction writing, novelty and surprise can be conflated into one single feature, originality. They rely on Bartel's definition: a work is original if it is the first to display some unique or different attribute that is then adopted by other works [Lamb et al., 2018].\nValue, on the other hand, is a catch-all, which involves both economic and historical dimensions of art. The approach the experts take is intrinsically product-based: they evaluate the creativity of the text in itself; regardless of historical or social considerations that would make the evaluation noisy. In the context of fiction writing, they mapped value to attractiveness: a synopsis is valuable if it engages the reader and provides a satisfying reading experience.\nThe experts' rubric encompasses the following quality dimensions (see Appendix A for details) rated from 0 to 3:\n\u2022 Attractiveness: literary appeal of the title, the style of the text, and its content (theme/plot). Criteria include the title's captivation, style's enjoyment, and the engagement of story and characters.\n\u2022 Originality: novelty and uniqueness of the title, the text style and the text theme/plot. Criteria include the title's uniqueness, the style's distinctiveness, and the plot's innovation.\n\u2022 Creativity: Overall creativity of the title and synopsis. The rubric focuses on how effectively these elements introduce novel ideas and capture the narrative essence. The criteria for this assessment include determining the creativity of the title and the synopsis, but in this dimensions we do not differentiate between style and theme. The particularity of this feature is that it included the word itself in the definition of each level. With this, we intended to see if the mental model of what the evaluator considered creativity could correlate with the aspects of attractiveness and originality, whose levels were univocally defined.\n\u2022 Critical Assessment: Evaluates the text's fit within its genre and its potential to be included in an anthology.\n\u2022 Own voice: evaluates if the author has a recognizable style.\nIn addition, we also ask our expert annotators (i) whether the text has been written by a machine or a human writer, (ii) if their opinion would match other experts' opinion; and (iii) if their opinion would match the opinion of general readers."}, {"title": "3.1 Evaluators", "content": "We recruited six literary experts, all critics or university scholars. These experts were different from those who developed the rubric. Three of them evaluated the 60 synopses written by Pron, and the 60 synopses written by GPT-4 in Spanish. The other three were bilingual and experts in English Literature, and evaluated the 60 synopsis by Pron and the 60 synopses written by GPT-4 in Spanish."}, {"title": "4 Results and Discussion", "content": "RQ1: Can the current state of generative AI compare to a prestigious author in creative writing tasks?\nFigure 1 shows the results of our study across its main quality dimensions for the texts, as rated on a Likert scale from 0 to 3. Overall, Pron receives significantly better assessments in all dimensions.\nFor attractiveness, both GPT-4's style and theme were rated as unattractive (0-1 scores) in 95% and 83% of the assessments, respectively. Pron's texts received much more favorable ratings, with 72% finding his style attractive (2-3 scores) and 62% his themes/plots engaging. Questions about originality, about suitability to be included in an anthology, and about the author having its own voice, all follow a very similar pattern.\nRatings for creativity are slightly higher than for the other dimensions, but following the same comparative pattern: only 24% of the assessments are positive (2-3 scores) for GPT-4, compared to 88% positive assessments for Pron.\nAs for relevance to the title, GPT-4's synopses often failed to align well with the titles, while Patricio's synopses showed strong and surprising connections with the proposed title.\nOverall, these results strongly suggest that the current generation of LLMs are not yet ready to compete with the best fiction writers. Although we have only evaluated one LLM, the observed difference is so large that it is unlikely to be improved upon by its peers Claude 3 Opus [Anthropic, 2024], Gemini Ultra [Google, 2023], Llama 3 [Meta, 2024], etc."}, {"title": "RQ2: What is the role of the prompt in the creativity of the result?", "content": "Figure 2a shows the experimental results for this research question. The leftmost figure shows that Pron's titles receive significantly higher scores in originality, attractiveness and creativity than its GPT-4 peers.\nIn Figure 2b (right) we can see the effect of both sets of titles in the texts written for them. The figure shows a radar chart with average likert scores for five quality dimensions. Remarkably, GPT-4 receives better scores in all quality dimensions when the titles have been provided by Pron. Differences are particularly high in style originality (+57%), style attractiveness (+30%), suitability for an anthology (+45%), and author having its own voice (+30%). A mere title provided by a creative writer can induce the LLM to produce texts with a better creative style. In contrast, the quality of Pron texts seem to be mostly independent of the procedence of the title and, for some quality dimensions the (less creative) GPT-4 titles seem to be a challenge that Pron resolves with even higher average scores: theme originality is 10% better with GPT-4 titles, style originality is 6% better, and creativity is 9% better. We asked Pron about this and he replied that \"I did not like GPT-4 titles at all, so I tried to take them in completely different directions\".\nIn order to find out if the differences are statistically significant, we used the Mann-Whitney U test [McKnight and Najab, 2010], a non-parametric test that is ideal for comparing differences between two not-paired independent groups when the data does not necessarily meet the assumptions required for parametric tests. According to this test, GPT-4"}, {"title": "4.1 RQ3: Are models more creative in English than in Spanish?", "content": "To compare GPT-4's creativity in English and Spanish, we employed the Wilcoxon signed-rank test [Wilcoxon, 1992], suitable for paired samples without assuming a normal distribution. We found significant differences in three quality dimensions: style attractiveness (p=0.01, effect size = 9.09), style originality (p=0.0004, effect size = 8.03), and creativity (p=0.02, effect size = 12.47). These substantial effect sizes indicate strong preferences for English over Spanish in these aspects, with the largest disparity in the creativity of synopses.\nUnlike style and creativity, no significant differences are found in theme attractiveness (p=0.56) and theme originality (p=0.11). Only style (and creativity by extension) seem to be significantly affected by the language switch.\nIn summary, the Wilcoxon test shows significant differences in style and synopsis creativity between English and Spanish, with large effect sizes, while theme-related traits are perceived similarly in both languages."}, {"title": "RQ4: Does GPT-4 have a recognizable style for a literature expert when generating creative text?", "content": "To answer this question, we want to measure if the ability to detect LLMs authored text improves along the evaluation process, i.e., if the experts learn about the traits of GPT-4 writing vs Pron's writing by reading its texts (even if the expert is not informed about authorship)."}, {"title": "RQ5: Is Boden's definition of creativity operational when assessing creative text writing?", "content": "We assessed creativity by mapping Boden's dimensions into attractiveness and originality of both theme and style, and we also asked assessors to evaluate creativity as a whole. Do Boden's dimensions correlate with creativity assessments? Table 1 shows Spearman correlations between creativity, attractiveness and originality. All variables are correlated with values above 0.7, which is a strong positive signal. Note, however, that the relation between originality and attractiveness is higher (0.78) than the relation of each of the components with creativity (0.73 and 0.72), which suggests that the relationship is nuanced and may depend on each assessor's take of what is creativity.\nFigure 4 confirms these correlations visually. Each dot represents an expert assessment for a given text. It shows a much higher density of points along the diagonal, suggesting a positive correlation between these dimensions, both in terms of style and theme. In addition, it seems that attractiveness and originality are upper bounds for creativity, because the zones below the diagonal are more populated than the zones above the diagonal in both graphs."}, {"title": "5 Conclusions", "content": "Are LLMs ready to challenge world-class novelists, rather than average writers, in literary writing tasks? Our study, inspired by historic duels such as DeepBlue vs Kasparov and AlphaGo vs Lee Sidol, was designed to answer this question. We designed an experimental setting where both GPT-4 and Patricio Pron, our contenders, received the same information about the contest, were proposed the same tasks, and were blindly evaluated with the same rubric by a set of six literature experts (critics and scholars). We collected 60 titles (proposed by the contenders), 120 literary texts (60 provided by each of the contenders for each of the 60 titles), and 5,400 expert assessments on different quality aspects of the titles and texts produced.\nOur results indicate that GPT-4 Turbo, despite its impressive writing capabilities, still falls short of matching the skills of a world-class novelist. Texts generated by GPT-4 are consistently rated lower in all quality dimensions in our study: attractiveness and originality of both style and theme, and overall creativity, among others. Comparing with previous results, this indicates that it is much easier to match the average performance of human writers than to actually match the best ones: LLMs still lack the nuanced depth, originality and intent characteristic of a top novelist such as Patricio Pron."}, {"title": "Limitations", "content": "These are the main limitations of our work:\n\u2022 Prompt design and influence in the results: Careful prompt engineering would imply a de-facto collaboration between man and machine; therefore, to avoid contamination we decided not to fine-tune our prompts in any way, and simply provide similar instructions to our human writer and to GPT-4, without further fine-tuning. This means that there might be alternative prompts that result in better GPT-4 texts that we have not explored.\n\u2022 Limited scope of our creative writing task: The study focused on a specific creative writing task: writing short synopsis for imaginary films with a given title. Creativity writing encompasses a broader range of tasks which were not evaluated. Consequently, our findings may not be generalizable to other forms of creative expression where different skills and qualities are required. Also, for larger texts (such as a novel), internal coherence may be a challenge for LLMs, which is not an issue in our experimental setup.\n\u2022 Scope of language and cultural contexts: The study only considered texts in English and Spanish, limiting the scope of our findings. Creativity is deeply influenced by cultural context, and our study does not account for the vast diversity of linguistic and cultural nuances across other languages. In any case, we would expect to find an even larger gap between GPT-4 and top human writers in other languages with less online resources.\n\u2022 Focus on a Single AI model: While GPT-4 is a state-of-the-art language model, it represents only one approach to AI text generation. Other models, possibly with different architectures or training paradigms, might exhibit different strengths and weaknesses in creative tasks. Our study does not account for these variations, potentially limiting the applicability of our findings to a broader range of AI systems.\n\u2022 Multilingual design: In order to avoid undesired translation effects, Pron texts were kept in its original language (Spanish) for all evaluators. Our bilingual experts (all scholars in English literature with bilingual language skills) evaluated GPT-4 texts in English together with Pron texts in Spanish. Although results are consistent with the Spanish evaluation, there might be undetected effects of language in the comparative evaluation of GPT-4 english texts. In particular, the decision of authorship might be influenced by the fact that all English texts had been written by GPT-4, which was an easy to spot signal. In average they were not, however, better authorship predictors than their monolingual counterparts.\n\u2022 Only expert assessments: There are always two types of veredict for a creative text: the opinion of the experts (critics and scholars), and the reception of the audience (the readers). Both are relevant and not always correlate with each other. We have only collected experts' assessments, so the question of whether the audience would perceive a similar gap between Pron and GPT-4 texts remains open.\nIn view of these limitations, future research should consider:\n\u2022 Expanding the scope of creative tasks and considering man-machine co-authoring processes, including prompt engineering techniques.\n\u2022 Incorporating readers (the audience, rather than the critics) as evaluators to capture a broader notion of value in the experimentation.\n\u2022 Exploring other models and architectures to identify different approaches to enhance the creativity of AI systems."}, {"title": "B Titles given in the Prompt", "content": "Titles proposed by Patricio Pron\n1. After all I almost did for you\n2. All love songs are sad songs\n3. Another episode in the Class Struggle\n4. Don't tell mom\n5. Eclipse in the botanical garden\n6. Edith loves him (we'll come back to this)\n7. Every picture from when we were young\n8. Future ghosts\n9. I have no fear because I have nothing\n10. I keep trying to forget your promise\n11. Lindsay Hilton visits Paris\n12. Mental illness three days a week\n13. Monsters live here\n14. Paradise can't be seen from here\n15. Pick a card, any card. No, not that one! Another!\n16. Rise and fall of R. S. Turtleneck, children's author\n17. Silks from Bursa, tiles from K\u00fctahya\n18. Spanish Youth, keep trying\n19. The day after Groundhog day\n20. The delights of the garden of delights\n21. The last journey of Santiago Calatrava\n22. The last laugh of that year\n23. The Lego woman\n24. The national red button\n25. The nightmares of the invisible man\n26. The nocturnal emissions\n27. The tied cow\n28. Two cops stand between us\n29. When you are at the top you can't fall any lower\n30. Who killed Patricio Pron?\nTitles proposed by GPT-4 Turbo\n1. Among clouds and mirages\n2. Between the lines of fate\n3. Beyond the broken horizon\n4. Bits of reality\n5. Echoes of a lost dream\n6. Echoes of the future\n7. Fragments of an invisible yesterday\n8. Parallel paths\n9. Reflections of another world\n10. Shadows in the mist\n11. Song of the captive moon\n12. Sparks in the dark\n13. The awakening of the aurora\n14. The crystal labyrinth\n15. The echo of silenced voices\n16. The forgotten melody\n17. The garden of withered dreams\n18. The inverted city\n19. The journey of the dawn\n20. The last flight of the butterfly\n21. The last night on Earth\n22. The mosaic of time\n23. The painter of memories\n24. The shadows of time\n25. The whisper of the cosmos\n26. The wind in the moorlands\n27. Traces in the sea of sand\n28. Twilight of the titans\n29. Under the copper sky\n30. Whispers from the eternal city"}]}