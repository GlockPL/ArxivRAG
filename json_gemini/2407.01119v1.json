{"title": "Pron vs Prompt: CAN LARGE LANGUAGE MODELS ALREADY\nCHALLENGE A WORLD-CLASS FICTION AUTHOR AT CREATIVE\nTEXT WRITING?", "authors": ["Guillermo Marco", "Julio Gonzalo", "Ram\u00f3n del Castillo", "Mar\u00eda Teresa Mateo Girona"], "abstract": "It has become routine to report research results where Large Language Models (LLMs) outperform\naverage humans in a wide range of language-related tasks, and creative text writing is no exception.\nIt seems natural, then, to raise the bid: Are LLMs ready to compete in creative writing skills with a\ntop (rather than average) novelist? To provide an initial answer for this question, we have carried out\na contest between Patricio Pron (an awarded novelist, considered one of the best of his generation)\nand GPT-4 (one of the top performing LLMs), in the spirit of AI-human duels such as DeepBlue vs\nKasparov and AlphaGo vs Lee Sidol. We asked Pron and GPT-4 to provide thirty titles each, and then\nto write short stories for both their titles and their opponent's. Then, we prepared an evaluation rubric\ninspired by Boden's definition of creativity, and we collected 5,400 manual assessments provided by\nliterature critics and scholars. The results of our experimentation indicate that LLMs are still far from\nchallenging a top human creative writer, and that reaching such level of autonomous creative writing\nskills probably cannot be reached simply with larger language models.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have recently showed strong competences generating human-like text, and in particular\nin creative writing tasks [Achiam et al., 2023], which is the focus of this paper. LLMs are increasingly influencing\ncreative industries, impacting both the economy and the labor market, as highlighted by significant events such as the\nHollywood screenwriters' strike [Lee, 2022, Eloundou et al., 2023, Koblin and Barnes, 2023]. Experimentation shows"}, {"title": "2 Related Work", "content": "Since the rise of LLM technology, creative text writing has gained renewed interest within the NLP research community.\nFranceschelli and Musolesi [2024] survey machine learning and creativity, discussing computational creativity theories,\ngenerative techniques, and evaluation methods. Evaluating creativity remains challenging [H\u00e4m\u00e4l\u00e4inen and Alnajjar,\n2021, Chakrabarty et al., 2023], but progress is being made; for an extensive explanation of the challenges of evaluating\ncomputational creativity see Lamb et al. [2018].\nRegarding machine-assisted human writing, Swanson et al. [2021] introduced Story Centaur, a tool for creative writers\nto prototype few-shot learning models. And Chakrabarty et al. [2022] presented CoPoet, a system for poetry writing\nthat enhances user-generated content. In both cases, evaluators prefer texts generated in co-authorship with IA systems.\nHowever, Kreminski and Martens [2022] highlighted limitations in current LLM tools, such as issues with narrative\nconsistency and plot development.\nOur focus is rather on autonomous LLMs creative writing. Gunser et al. [2022] examined the stylistic quality of\nAI-generated texts, finding them generally rated lower than human-written texts despite being indistinguishable. Marco\net al. [2023] compared a fine-tuned BART model and ChatGPT 3.5 with human writers on a creative task, noting that\nwhile the BART model excelled in grammaticality and coherence, it matched humans in creativity but used more clich\u00e9s.\nLimitations of their study included the non-creative nature of human synopses and a casual reader assessment, unlike\nour expert-based approach.\nThe study by G\u00f3mez-Rodr\u00edguez and Williams [2023] examines the capability of several large language models (LLMs)\nin autonomous English creative writing, focusing on a single imaginative task where models and humans compose a\nstory about a combat between Ignatius J. Reilly and a pterodactyl. They reveal that LLMs performed well in fluency\nand coherence but lagged in creativity and humor. Their study's single-task focus contrasts with our broad evaluation of\n60 titles.\nLastly, Chakrabarty et al. [2024] proposed the Torrance Test of Creative Writing (TTCW) to evaluate AI-generated\nstories. Their findings reveal that while LLMs perform well in terms of fluency and structure, they lag significantly\nbehind human writers in originality and emotional depth. A limitation of the study is that the tasks given to humans and\nmachines are asymmetrical: human stories are selected from already published material. Then, GPT-4 summarizes the\nstories, and LLMs are asked to generate a full story starting from each of the summaries, which is only a part of the\ncreative writing process. Another difference in methodology is that they adapt the TTCW test for their rubric, while we\ndesign our rubric following Boden's notion of creativity.\nOverall, our study complements previous work being the only one that simultaneously (i) uses the best possible writer\nand LLM for the experimentation; (ii) gives the same tasks to both contenders in equal conditions (iii) explores 60\ndifferent writing assignments (proposed by the contenders) and collects 5,400 expert assessments for a rubric that adapts\nBoden's notion of creativity to the task, and (iv) includes a study on the effect of the prompt and also measures the gap\nbetween texts written in English and Spanish."}, {"title": "3 Experimental Design", "content": "Contenders. The LLM chosen for the experiment is GPT-4 Turbo (in gpt-4-0125-preview version), which was\nthe strongest LLM when we initiated the experimentation. After some initial experimentation with the system, we fixed\ntemperature at 1. Going beyond this value occasionally impacted on grammaticality (particularly with Spanish texts), so\nwe chose the highest value that produced always formally correct texts. Once the experiment was initiated, other LLMs\nthat seemed to rival the performance of GPT-4 were launched: most notably Claude 3 Opus, Gemini Ultra and Llama 3.\nExperimenting with these models, we did not notice any clear advantages with respect to GPT-4, so we proceed with\nour initial setup.\nFinding a top novelist that would engage in this experiment was easier than we initially thought. We contacted Patricio\nPron because, besides being awarded with some of the most prestigious distinctions in Spanish literature (the Alfaguara"}, {"title": "Task design", "content": "In the first stage, each contender proposed 30 movie titles. In the second stage, both contenders wrote\nsynopses (approximately 600 words) for each of the 60 titles. The prompt for GPT-4 was as follows: \u201cWe are conducting\nan experiment to compare your creative writing skills with those of the renowned novelist Patricio Pron. Your task is to\ngenerate synopses for imaginary movie titles. These synopses should be creative, appealing to critics and audiences,\nand possess inherent literary value. Here is some information about Patricio Pron: he is a celebrated writer, recognized\nas one of the top young writers in Spanish by Granta in 2010, and the winner of the Alfaguara Prize in 2019 for his\nwork Ma\u00f1ana tendremos otros nombres. The proposed title is: {title}. Please write a 600-word synopsis that meets\nthese criteria.\"\nThe initial process was in Spanish, and titles were later translated into English for GPT-4 to generate English synopses.\nOur dataset includes 60 titles, 60 texts by Pron, and 60 texts by GPT-4 for each of the two languages."}, {"title": "Rubric Design", "content": "The rubric, designed by three experts in pedagogy, psychometric, literature, and NLP, focuses on\ncreativity-related dimensions, as previous work has shown that LLMs already excel at grammaticality, coherence and\nfluency [Marco et al., 2023].\nThe point of departure is Margaret Boden's definition [Boden, 2003]: Creativity is the ability to come up with ideas that\nare new, surprising, and valuable.; it is a simple, operative definition compatible with most studies on the subject, both\nfrom philosophers and psychologists, with a long tradition [Gaut, 2010]. It is a conceptualization of creativity in three\nspecific dimensions: novelty, surprise and value.\nThe experts agreed that, in fiction writing, novelty and surprise can be conflated into one single feature, originality.\nThey rely on Bartel's definition: a work is original if it is the first to display some unique or different attribute that is\nthen adopted by other works [Lamb et al., 2018].\nValue, on the other hand, is a catch-all, which involves both economic and historical dimensions of art. The approach\nthe experts take is intrinsically product-based: they evaluate the creativity of the text in itself; regardless of historical\nor social considerations that would make the evaluation noisy. In the context of fiction writing, they mapped value to\nattractiveness: a synopsis is valuable if it engages the reader and provides a satisfying reading experience.\nThe experts' rubric encompasses the following quality dimensions (see Appendix A for details) rated from 0 to 3:\n\u2022 Attractiveness: literary appeal of the title, the style of the text, and its content (theme/plot). Criteria include\nthe title's captivation, style's enjoyment, and the engagement of story and characters.\n\u2022 Originality: novelty and uniqueness of the title, the text style and the text theme/plot. Criteria include the\ntitle's uniqueness, the style's distinctiveness, and the plot's innovation.\n\u2022 Creativity: Overall creativity of the title and synopsis. The rubric focuses on how effectively these elements\nintroduce novel ideas and capture the narrative essence. The criteria for this assessment include determining\nthe creativity of the title and the synopsis, but in this dimensions we do not differentiate between style and\ntheme. The particularity of this feature is that it included the word itself in the definition of each level. With\nthis, we intended to see if the mental model of what the evaluator considered creativity could correlate with\nthe aspects of attractiveness and originality, whose levels were univocally defined.\n\u2022 Critical Assessment: Evaluates the text's fit within its genre and its potential to be included in an anthology.\n\u2022 Own voice: evaluates if the author has a recognizable style."}, {"title": "3.1 Evaluators", "content": "We recruited six literary experts, all critics or university scholars. These experts were different from those who developed\nthe rubric. Three of them evaluated the 60 synopses written by Pron, and the 60 synopses written by GPT-4 in Spanish.\nThe other three were bilingual and experts in English Literature, and evaluated the 60 synopsis by Pron and the 60\nsynopses written by GPT-4 in Spanish."}, {"title": "4 Results and Discussion", "content": "RQ1: Can the current state of generative AI compare to a prestigious author in creative writing tasks?\nFigure 1 shows the results of our study across its main quality dimensions for the texts, as rated on a Likert scale from 0\nto 3. Overall, Pron receives significantly better assessments in all dimensions.\nFor attractiveness, both GPT-4's style and theme were rated as unattractive (0-1 scores) in 95% and 83% of the\nassessments, respectively. Pron's texts received much more favorable ratings, with 72% finding his style attractive (2-3\nscores) and 62% his themes/plots engaging. Questions about originality, about suitability to be included in an anthology,\nand about the author having its own voice, all follow a very similar pattern.\nRatings for creativity are slightly higher than for the other dimensions, but following the same comparative pattern:\nonly 24% of the assessments are positive (2-3 scores) for GPT-4, compared to 88% positive assessments for Pron.\nAs for relevance to the title, GPT-4's synopses often failed to align well with the titles, while Patricio's synopses showed\nstrong and surprising connections with the proposed title.\nOverall, these results strongly suggest that the current generation of LLMs are not yet ready to compete with the best\nfiction writers. Although we have only evaluated one LLM, the observed difference is so large that it is unlikely to be\nimproved upon by its peers Claude 3 Opus [Anthropic, 2024], Gemini Ultra [Google, 2023], Llama 3 [Meta, 2024], etc."}, {"title": "RQ2: What is the role of the prompt in the creativity of the result?", "content": "Figure 2a shows the experimental results for this research question. The leftmost figure shows that Pron's titles receive\nsignificantly higher scores in originality, attractiveness and creativity than its GPT-4 peers.\nIn Figure 2b (right) we can see the effect of both sets of titles in the texts written for them. The figure shows a radar\nchart with average likert scores for five quality dimensions. Remarkably, GPT-4 receives better scores in all quality\ndimensions when the titles have been provided by Pron. Differences are particularly high in style originality (+57%),\nstyle attractiveness (+30%), suitability for an anthology (+45%), and author having its own voice (+30%). A mere title\nprovided by a creative writer can induce the LLM to produce texts with a better creative style. In contrast, the quality\nof Pron texts seem to be mostly independent of the procedence of the title and, for some quality dimensions the (less\ncreative) GPT-4 titles seem to be a challenge that Pron resolves with even higher average scores: theme originality is\n10% better with GPT-4 titles, style originality is 6% better, and creativity is 9% better. We asked Pron about this and he\nreplied that \"I did not like GPT-4 titles at all, so I tried to take them in completely different directions\".\nIn order to find out if the differences are statistically significant, we used the Mann-Whitney U test [McKnight and\nNajab, 2010], a non-parametric test that is ideal for comparing differences between two not-paired independent groups\nwhen the data does not necessarily meet the assumptions required for parametric tests. According to this test, GPT-4"}, {"title": "4.1 RQ3: Are models more creative in English than in Spanish?", "content": "To compare GPT-4's creativity in English and Spanish, we employed the Wilcoxon signed-rank test [Wilcoxon, 1992],\nsuitable for paired samples without assuming a normal distribution. We found significant differences in three quality\ndimensions: style attractiveness (p=0.01, effect size = 9.09), style originality (p=0.0004, effect size = 8.03), and\ncreativity (p=0.02, effect size = 12.47). These substantial effect sizes indicate strong preferences for English over\nSpanish in these aspects, with the largest disparity in the creativity of synopses.\nUnlike style and creativity, no significant differences are found in theme attractiveness (p=0.56) and theme originality\n(p=0.11). Only style (and creativity by extension) seem to be significantly affected by the language switch.\nIn summary, the Wilcoxon test shows significant differences in style and synopsis creativity between English and\nSpanish, with large effect sizes, while theme-related traits are perceived similarly in both languages."}, {"title": "RQ4: Does GPT-4 have a recognizable style for a literature expert when generating creative text?", "content": "To answer this question, we want to measure if the ability to detect LLMs authored text improves along the evaluation\nprocess, i.e., if the experts learn about the traits of GPT-4 writing vs Pron's writing by reading its texts (even if the\nexpert is not informed about authorship)."}, {"title": "RQ5: Is Boden's definition of creativity operational when assessing creative text writing?", "content": "We assessed creativity by mapping Boden's dimensions into attractiveness and originality of both theme and style, and\nwe also asked assessors to evaluate creativity as a whole. Do Boden's dimensions correlate with creativity assessments?\nTable 1 shows Spearman correlations between creativity, attractiveness and originality. All variables are correlated\nwith values above 0.7, which is a strong positive signal. Note, however, that the relation between originality and\nattractiveness is higher (0.78) than the relation of each of the components with creativity (0.73 and 0.72), which suggests\nthat the relationship is nuanced and may depend on each assessor's take of what is creativity.\nFigure 4 confirms these correlations visually. Each dot represents an expert assessment for a given text. It shows a\nmuch higher density of points along the diagonal, suggesting a positive correlation between these dimensions, both\nin terms of style and theme. In addition, it seems that attractiveness and originality are upper bounds for creativity,\nbecause the zones below the diagonal are more populated than the zones above the diagonal in both graphs."}, {"title": "To robustly test this, we applied mixed-effects models [Bates et al., 2014], accounting for variability in titles and evaluators", "content": "The model we fit is: Creativity = \u03b20 + \u03b2\u2081Style_Attractiveness+ \u03b22Theme_Attractiveness + \u03b23Style_Originality+ B4Theme_Originality + Utitle + Vusername + \u20ac. We obtain significant contributions from all predictors (p < 0.001).\nResults show that the REML criterion at convergence is 991.7, with scaled residuals between -4.47 and 3.59. Variance\ncomponents are 0.006 for titles, 0.079 for evaluators, and 0.21 for residuals. The fixed effects are: Style Attractiveness\n(estimate = 0.18, p < 0.001), Theme Attractiveness (est. = 0.15, p < 0.001), Style Originality (est. = 0.33, p < 0.001),\nand Theme Originality (est. = 0.33020, p < 0.001).\nThe intercept is estimated at 0.25 with marginal significance (p = 0.08). Correlations between fixed effects are low,\nthe highest being -0.712 between style attractiveness and style originality. This analysis shows that both attractiveness\nand originality contribute significantly to creativity, with originality having a slightly stronger impact. The residual\nvariability suggests other factors may influence creativity, pointing to the robustness of our rubric, which effectively\ncaptures the complexity of creative evaluation.\nIn summary, all predictors have an impact on creativity, with originality playing a more prominent role; and high\nresidual variability suggests other factors influencing creativity which are not capture by the model. Overall, the model\nindicates that attractiveness and originality are essential to characterize creativity in our creative writing task, validating\nthe approach taken in our rubric."}, {"title": "5 Conclusions", "content": "Are LLMs ready to challenge world-class novelists, rather than average writers, in literary writing tasks? Our study,\ninspired by historic duels such as DeepBlue vs Kasparov and AlphaGo vs Lee Sidol, was designed to answer this\nquestion. We designed an experimental setting where both GPT-4 and Patricio Pron, our contenders, received the same\ninformation about the contest, were proposed the same tasks, and were blindly evaluated with the same rubric by a set\nof six literature experts (critics and scholars). We collected 60 titles (proposed by the contenders), 120 literary texts (60\nprovided by each of the contenders for each of the 60 titles), and 5,400 expert assessments on different quality aspects\nof the titles and texts produced.\nOur results indicate that GPT-4 Turbo, despite its impressive writing capabilities, still falls short of matching the skills\nof a world-class novelist. Texts generated by GPT-4 are consistently rated lower in all quality dimensions in our study:\nattractiveness and originality of both style and theme, and overall creativity, among others. Comparing with previous\nresults, this indicates that it is much easier to match the average performance of human writers than to actually match\nthe best ones: LLMs still lack the nuanced depth, originality and intent characteristic of a top novelist such as Patricio\nPron.\nAlso, our study highlights the significant role of prompts in creative text writing: titles provided by Pron resulted in\nGPT-4 texts which are significantly more creative and original than the ones written for its own titles. Even the simplest\nprompting (short titles in our case) should be considered co-authorship, as it has a profound influence on the results.\nWe also found that GPT-4's performance in generating creative texts was significantly better in English than in Spanish,\nin spite of being also a resource-rich language. This discrepancy is likely due to the model being trained on a larger\ncorpus of English text, reflecting a bias towards English in the available training data. The results underscore the need\nfor more balanced and comprehensive training datasets to enhance the multilingual creative writing capabilities of AI\nsystems.\nOur expert evaluators were able to identify AI-generated texts with increasing accuracy over time, suggesting that GPT-4\nhas a recognizable style that becomes more apparent as evaluators gain experience with its outputs. This indicates that\ndespite its ability to mimic human writing, GPT-4's generated text retains a certain uniformity that can be detected by\nexpert readers.\nFinally, our study successfully applied Boden's approach to creativity (as a combination of novelty, surprise, and value)\nto create a rubric that serves to evaluate creative writing texts, either human or machine-generated. A statistical analysis\nof the 5,400 manual assessments collected shols that both attractiveness (value) and originality (novelty and surprise)\nsignificantly contribute to the perceived creativity of texts. This validates the use of Boden's dimensions in evaluating\nthe creative outputs of AI systems.\nIn conclusion, we think that there are inherent limitations in LLMs approach to creative writing. LLMs rely on pattern\nextraction from large corpora of text, which allows them to generate text that is contextually appropriate and often\nmimics the stylistic nuances of human writing. However, this approach can also lead to the generation of content\nthat tends to conform to common patterns and clich\u00e9s, which may be enough when compared to average professional\nwriters, but lacks the originality, depth and intent found in the best human writers.\nProbably, a key limitation of LLMs is their tendency to approximate meaning through probability. While human writers\ncan produce low-probability text that carries deep meaning and creativity, LLMs tend to generate content that aligns\nwith the most likely patterns observed in their training data. This probabilistic approach can result in outputs that are\nhigh in coherence and fluency but low in innovative thinking and originality. As LLMs are refined and improved, they\nare likely to become more adept at solving objective tasks. However, their creative output may remain constrained by a\ntendency to replicate familiar patterns, leading to a literature filled with clich\u00e9s.\nOverall, our study suggests that while LLMs can be valuable tools for generating text and assisting with various writing\ntasks, they are not yet capable of fully replicating the creative process of top human writers, who often produce work\nthat is not only meaningful but also surpass conventional expectations. For now, we will have to wait until a duel\nbetween top human and machine writers is actually disputed."}, {"title": "Limitations", "content": "These are the main limitations of our work:\n\u2022 Prompt design and influence in the results: Careful prompt engineering would imply a de-facto collaboration\nbetween man and machine; therefore, to avoid contamination we decided not to fine-tune our prompts in any\nway, and simply provide similar instructions to our human writer and to GPT-4, without further fine-tuning.\nThis means that there might be alternative prompts that result in better GPT-4 texts that we have not explored.\n\u2022 Limited scope of our creative writing task: The study focused on a specific creative writing task: writing\nshort synopsis for imaginary films with a given title. Creativity writing encompasses a broader range of tasks\nwhich were not evaluated. Consequently, our findings may not be generalizable to other forms of creative\nexpression where different skills and qualities are required. Also, for larger texts (such as a novel), internal\ncoherence may be a challenge for LLMs, which is not an issue in our experimental setup.\n\u2022 Scope of language and cultural contexts: The study only considered texts in English and Spanish, limiting\nthe scope of our findings. Creativity is deeply influenced by cultural context, and our study does not account\nfor the vast diversity of linguistic and cultural nuances across other languages. In any case, we would expect to\nfind an even larger gap between GPT-4 and top human writers in other languages with less online resources.\n\u2022 Focus on a Single AI model: While GPT-4 is a state-of-the-art language model, it represents only one\napproach to AI text generation. Other models, possibly with different architectures or training paradigms,\nmight exhibit different strengths and weaknesses in creative tasks. Our study does not account for these\nvariations, potentially limiting the applicability of our findings to a broader range of AI systems.\n\u2022 Multilingual design: In order to avoid undesired translation effects, Pron texts were kept in its original\nlanguage (Spanish) for all evaluators. Our bilingual experts (all scholars in English literature with bilingual\nlanguage skills) evaluated GPT-4 texts in English together with Pron texts in Spanish. Although results are\nconsistent with the Spanish evaluation, there might be undetected effects of language in the comparative\nevaluation of GPT-4 english texts. In particular, the decision of authorship might be influenced by the fact\nthat all English texts had been written by GPT-4, which was an easy to spot signal. In average they were not,\nhowever, better authorship predictors than their monolingual counterparts.\n\u2022 Only expert assessments: There are always two types of veredict for a creative text: the opinion of the experts\n(critics and scholars), and the reception of the audience (the readers). Both are relevant and not always correlate\nwith each other. We have only collected experts' assessments, so the question of whether the audience would\nperceive a similar gap between Pron and GPT-4 texts remains open."}]}