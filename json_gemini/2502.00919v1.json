{"title": "Attention Sinks and Outlier Features: A 'Catch, Tag, and Release' Mechanism for Embeddings", "authors": ["Stephen Zhang", "Mustafa Khan", "Vardan Papyan"], "abstract": "Two prominent features of large language models (LLMs) is the presence of large-norm (outlier) features and the tendency for tokens to attend very strongly to a select few tokens. Despite often having no semantic relevance, these select tokens, called attention sinks, along with the large outlier features, have proven important for model performance, compression, and streaming. Consequently, investigating the roles of these phenomena within models and exploring how they might manifest in the model parameters has become an area of active interest. Through an empirical investigation, we demonstrate that attention sinks utilize outlier features to: catch a sequence of tokens, tag the captured tokens by applying a common perturbation, and then release the tokens back into the residual stream, where the tagged tokens are eventually retrieved. We prove that simple tasks, like averaging, necessitate the 'catch, tag, release' mechanism hence explaining why it would arise organically in modern LLMs. Our experiments also show that the creation of attention sinks can be completely captured in the model parameters using low-rank matrices, which has important implications for model compression and substantiates the success of recent approaches that incorporate a low-rank term to offset performance degradation.", "sections": [{"title": "1. Introduction", "content": "In marine biology, the 'catch, tag, and release' mechanism is a vital tool for tracking fish populations. A fish is caught, fitted with a tracking tag encoding critical information, and then released back into a water stream, where it is monitored over time. This enables researchers to understand migration patterns, survival rates, and ecosystem interactions."}, {"title": "1.2. Pervasive Phenomena in LLMS", "content": "Remarkably, modern large language models (LLMs) exhibit a strikingly similar mechanism when processing tokens. To understand why, we first examine two recent discoveries in LLM research:\nAttention Sinks: Coined by Xiao et al. (2024), the authors found that all tokens attend strongly to the first token. Follow-up works demonstrated that sinks may also appear in later tokens (Yu et al., 2024; Sun et al., 2024a; Cancedda, 2024).\nOutlier Features: Feature dimensions in the activations that are significantly larger in magnitude (Kovaleva et al., 2021; Dettmers et al., 2022). Unlike attention sinks, outlier feature dimensions are consistent across tokens.\nBoth phenomena have been linked to model performance, particularly in streaming (Xiao et al., 2024; Guo et al., 2024c), quantization techniques (Dettmers et al., 2022; Son et al., 2024; Ashkboos et al., 2024), and pruning techniques (Sun et al., 2024b; Zhang and Papyan, 2024), as behaviors that need to be preserved in the model's attention weights and features. This, in turn, has driven prior research to investigate the following question."}, {"title": "1.3. Why Do Sinks and Outliers Emerge?", "content": "Existing answers generally fall into two main categories:\nTo Create Implicit Attention Biases Models lack explicit bias parameters in the attention layers, leading attention sinks and large outliers to emerge as compensatory mechanisms that artificially introduce such biases. As a result, these effects can be mitigated by incorporating explicit key or value bias parameters (Sun et al., 2024a; Gu et al., 2024).\nTo Turn Off Attention Heads Attention sinks emerge as a learned, data-dependent mechanism that disables an attention head when it is not needed for the current sequence (Bondarenko et al., 2023; Guo et al., 2024b)."}, {"title": "1.4. Open Questions", "content": "Both perspectives still leave the following unanswered questions:\nQ1: How do attention sinks and outlier features manifest in model parameters?\nWhile sinks and outlier features play a crucial role in model compression, prior research has primarily focused on their presence in model embeddings. Their manifestation within model parameters remains largely unexplored, raising questions about the best strategies for preserving these critical properties during compression.\nQ2: Why do attention sinks emerge in later tokens?\nNeither perspective fully explains why attention sinks appear in later tokens alongside the first token sink (Yu et al., 2024; Cancedda, 2024). These sinks are sequence-dependent, contradicting the bias perspective, and the presence of multiple sinks seems unnecessary from the active-dormant perspective.\nQ3: Are attention sinks beneficial or better dispersed?\nIt remains unclear whether attention sinks are critical for model performance and need to be preserved (Xiao et al., 2024; Son et al., 2024), or if they should be dispersed or removed altogether (Sun et al., 2024a; Yu et al., 2024)."}, {"title": "1.5. Contributions", "content": "Our first contribution is a theoretical study of a two-layer attention model trained to average a sequence of numbers. This analysis answers the questions posed and establishes the following claims, summarized in the schematic below:\nA1: A low-rank structure in the attention weight matrices, $W_Q$, $W_K$, $W_V$, is responsible for attention sinks and outlier features.\nA2: Attention sinks and outlier features are leveraged by the model to simulate a 'catch, tag, and release' mechanism, depicted and explained in Figure 1.\nA3: The mechanism is useful for averaging a sequence, few-shot learning, and many other tasks.\nOur second contribution involves showing that pruning algorithms that do not preserve low-rank structure in weight matrices, destroy attention sinks and feature outliers, harm the 'catch, tag, release' mechanism, and perform worse on few-shot learning."}, {"title": "1.6. Relationship with Prior Perspectives", "content": "The 'catch, tag, and release' mechanism does not contradict the perspectives on attention sinks presented in prior works by Sun et al. (2024a); Gu et al. (2024); Guo et al. (2024b). We hypothesize that the prior perspectives are accurately describing the behavior of the attention sink associated specifically with the first token in the sequence. The 'catch, tag, and release' mechanism expands on this by describing the behavior associated with the data-dependent sinks that emerge in later tokens (Yu et al., 2024)."}, {"title": "2. Evidence for \u2018Catch, Tag, Release\u2019", "content": "This subsection provides empirical evidence for the existence of the 'catch, tag, release' mechanism. To this end, we input a prompt into the Phi-3 Medium model (Abdin et al., 2024), and compute various statistics, each offering evidence for the existence of a specific component of the mechanism."}, {"title": "3. Intervention Through Model Pruning", "content": "This subsection aims to intervene at the root of the logical sequence, displayed in Section 1.5, namely the low-rank structure of the attention weight matrices and to test its effect on the attention sinks and outlier features, the 'catch, tag, release' mechanism, and ultimately performance on a down-stream task which relies on it: few-shot learning. The intervention is done through the use of pruning algorithms."}, {"title": "3.1. How is Pruning Related?", "content": "Pruning algorithms compress models by retaining the fewest possible nonzero parameters while preserving performance (Mozer and Smolensky, 1988; LeCun et al., 1989; Hassibi and Stork, 1992). However, they typically do not prioritize accurately capturing low-rank structures in the weights.\nAn exception to this is a recently proposed compression algorithm for LLMs, called OATS (Zhang and Papyan, 2024), which approximates the model's weight matrices through a sparse-plus-low-rank decomposition:\n$W \\approx S + UV^T$."}, {"title": "3.2. Low-Rank Terms  \u21d2  Sinks and Outliers", "content": "Figure 4 presents the attention weights for layer 2, attention head 6, of a Phi-3 Medium model in three configurations: a dense model, a model compressed by 50% using OATS, and a model compressed by 50% using OATS where the low-rank matrices for $W_Q$, $W_K$, $W_V$ for that layer are set to 0. The plots reveal clear sinks and outliers, except in the model lacking low-rank terms, suggesting that the low-rank term in OATS is indeed responsible for both phenomena."}, {"title": "3.3. No Low-Rank Terms  \u21d2  No Sinks and Outliers", "content": "Figure 5 compares the attention weights in layer 36, head 16, of a dense Phi-3 Medium model with models that are pruned by 50% using OATS, and pruned by 50% using Wanda (Sun et al., 2024b), and SparseGPT (Frantar and Alistarh, 2023) \u2013 two methods that do not prioritize a low-rank structure in their formulation. The visualizations reveal that pruning methods like Wanda and SparseGPT lack certain attention sinks found in both the dense and OATS models."}, {"title": "3.4. Sinks and Outliers \u21d2 \u2018Catch, Tag, and Release\u2019", "content": "The previous two subsections show that classical pruning algorithms fail to retain the low-rank components of attention weights, which results in the loss of attention sinks and outlier features. In contrast, OATS preserves them. Building on Section 2, which establishes the necessity of sinks and outliers for the 'catch, tag, release' mechanism, we conclude that pruning algorithms like Wanda and SparseGPT break the 'catch, tag, release' mechanism, whereas OATS maintains it."}, {"title": "3.5. \u2018Catch, Tag, and Release\u2019  \u21d2  Few-Shot Learning", "content": "Since the 'catch, tag, release' mechanism segments sequences, it is likely crucial for distinguishing between examples in few-shot learning. To test this, we measure the gap in k-shot performance on the MMLU dataset (Hendrycks et al., 2021) using LM Harness (Gao et al., 2024), where k ranges from 0 to 5. We compare the performance of OATS with Wanda and SparseGPT in Figure 3."}, {"title": "4. Theoretical Foundation", "content": ""}, {"title": "4.1. Setup and Main Result", "content": "Task Given a sequence of T tokens, consisting of numbers, $x_i \\in \\mathbb{R}$, separated by a special $[SEP]$ token:\n$x = (x_1, ..., x_{t-1},[SEP],x_{t+1}, ..., x_T)$,\nthe objective is to compute the average of the numbers appearing after the $[SEP]$ token. To increase the complexity of the task, the position of the $[SEP]$ token, denoted as $t$, varies across different sequences.\nEmbeddings The number tokens are embedded into:\n$e_i = Embed(x_i) = \\begin{bmatrix} x_i \\\\ 1 \\end{bmatrix} \\in \\mathbb{R}^2, i \\neq t$,\nwhile the $[SEP]$ is embedded into:\n$e_t = Embed([SEP]) = \\begin{bmatrix} S_{num} \\\\ S_{tag} \\end{bmatrix} \\in \\mathbb{R}^2$,\nwhere $S_{num}, S_{tag} \\in \\mathbb{R}$ are learnable parameters. The first coordinate of the embeddings represents the numbers, while the second coordinate represents the tag.\nThe embeddings are concatenated into a matrix to form:\n$E = [e_1 \\ e_2 \\ ... \\ e_t \\ ... \\ e_T] \\in \\mathbb{R}^{T \\times 2}$.\nModel The embeddings are passed as input to a two-layer causal transformer:\n$H = Attention(E, E, EW^V) + E$ (1)\n$f_0(x) = Attention(hW_Q, HW_K, HW_V),$(2)"}, {"title": "4. Catch Mechanism", "content": "The attention weight when token $x_i$, for $i > t$ is attending to $[SEP]$ is:\n$A_{i, t} = \\frac{exp(e_i e_t)}{\\sum_{k=1}^T exp(e_i e_k)} = \\frac{exp(e_i e_t)}{exp(e_i e_t) + \\sum_{k=1, k\\neq t} exp(e_i e_k)} = \\frac{exp(x_i S_{num} + S_{tag})}{exp(x_i S_{num} + S_{tag}) + \\sum_{k=1, k\\neq t} exp(x_i x_k + 1)} = \\frac{exp(S_{tag})}{exp(S_{tag}) + \\sum_{k=1, k\\neq t} exp(x_i x_k + 1)} =\\frac{1}{1 + \\sum_{k=1, k\\neq t} exp(x_i x_k + 1 - S_{tag})}  \\xrightarrow{S_{tag} \\rightarrow \\infty} 1$.\nFurthermore, for $i > t$ and $j \\neq t$:\n$A_{i, j} = \\frac{exp(1 + x_i x_j)}{exp(S_{tag}) + \\sum_{k \\neq j} exp(1 + x_i x_k)} \\xrightarrow{S_{tag} \\rightarrow \\infty} O(e^{-S_{tag}})$(4)\nand therefore:\n$A_{i, j} \\xrightarrow{S_{tag} \\rightarrow \\infty} 0 \\ \\text{for} \\ j \\neq t$.\nThus, $[SEP]$ acts as an attention sink, where all tokens $x_i$ for $i > t$, as well as the $[SEP]$ token itself, attend exclusively to the $[SEP]$ token. As a result, the attention of all tokens $x_i$ for $i > t$ and the $[SEP]$ token have been caught. These observations are depicted in Figure 6 above."}, {"title": "Tag Mechanism", "content": "The attention output of the i-th token for $i > t$ is:\n$Attention(e_i, E, EW^V) = \\sum_{j=1}^T A_{i,j} e_j W^V = A_{i,t} e_t W^V + \\sum_{j=1,j\\neq t}^T A_{i,j} e_j W^V $(5)\n$=A_{i,t} \\begin{bmatrix} 0 \\\\ S_{tag} \\end{bmatrix} + \\sum_{j=1,j\\neq t}^T A_{i,j} \\begin{bmatrix} 0 \\\\ \\infty \\end{bmatrix} \\xrightarrow{S_{tag} \\rightarrow \\infty} [0 \\ \\infty]$.\nRecall that the second coordinate of the embeddings represents the tag. The limit above implies that a tag has been created for all tokens $x_i$, for $i > t$.\nAfter adding the tag to the residual stream, we obtain for $i > t$:\n$h_i = e_i + Attention(e_i, E, EW^V) \\xrightarrow{S_{tag} \\rightarrow \\infty} \\begin{bmatrix} x_i \\\\ \\infty \\end{bmatrix} $ (6)\nThe above implies that all tokens, $x_i$, for $i > t$ have now been tagged."}, {"title": "4. Release Mechanism", "content": "The tagged tokens have now been released into the residual stream. As we will show in the next subsection, the tags will be leveraged by the second attention layer to generate the desired averaging mechanism."}, {"title": "4.3. Proof of Theorem, Part 2: Leveraging the Tags", "content": "The second part of the proof focuses on the second attention layer.\nThe attention weight when token $x_r$ is attending to token $x_j$ is given by:\n$A_{r, j}^2 = \\frac{exp(h_r W_Q W_K h_j)}{ \\sum_{k=1}^T exp(h_r W_Q W_K h_k)}$\nDividing the numerator and denominator by $exp(h_r W_Q W_K h_T)$, the expression becomes:\n$A_{r, j}^2 = \\frac{ exp(h_r W_Q W_K (h_j - h_T))}{ \\sum_{k=1}^T exp(h_r W_Q W_K (h_k - h_T))} = \\frac{ exp(h_r W_Q W_K (h_j - h_T))}{ \\sum_{k=1}^T exp(h_r W_Q W_K (h_k - h_T))}$ (7)\nConsider the term inside the exponent:\n$h_r W_Q W_K (h_j - h_T)$.\nThere are three cases for $j$, depending on whether it is less than, equal to, or greater than $T$. In the next subsection, we prove that"}, {"title": "Case 1: For j<t (non-tagged tokens)", "content": "$exp(h+WW(hj - hr)) \\xrightarrow{Stag \\rightarrow \\infty}0$.\nCase 2: For j = t (sink token),\n$exp(h+WW(ht - hr)) \\xrightarrow{Stag \\rightarrow \\infty}0$.\nCase 3: For j > t (tagged tokens),\n$exp(h+WW(hj-hr)) \\xrightarrow{Stag \\rightarrow \\infty}1$.\nCombining all three cases together with Equation (7) leads to:\n$ lim A_{r,j}^2 \\xrightarrow{Stag \\rightarrow \\infty} \\begin{cases} 0, & j0 \\end{cases}.\nCase 2: For j = t (sink token),\n$h+WW(ht - h\u0442)= h+w2w([0-\u0445\u0442\\1-Stag]\\=(A\\T,t) \\begin{bmatrix} -Art \\\\ d.Stage \\end{bmatrix} +\n(A\\t,t -Art) \\begin{bmatrix}b. (1-Stage)\\\\Stage\\end{bmatrix} \\=-Art.dstage \\+\\d. Art(At -Art)stag \\xrightarrow{Stag \\rightarrow \\infty}\\infty$.\nCase 3: For j > t (tagged tokens),\n$h+WW(hj - h\u0442)= h+w2w([XTj]\\+\n(A\\T,t -Art)\\(\\begin{bmatrix}b.Stag\\\\\nStag\\end{bmatrix}\\))\\ =(A\\j,t -Art) (A\\b.Stag\\\\\\end{bmatrix}$\\\\++0(Stage))\nlim(A)- Art)stage = 0 is proved in Appendiz\n129109010120204\\\nwhere"}, {"title": "5. Related Work", "content": "Register Buffers in Transformers Darcet et al. (2024) observed that vision transformers exhibit high-norm outlier tokens, similar to LLMs, and that registers (data-independent tokens) are needed to prevent them from arising. This aligns closely with the perspective proposed by Sun et al. (2024a); Gu et al. (2024) that the first token attention sink is serving as a bias term for the attention mechanism.\nAdam Causes Sinks and Outliers Another recent avenue of investigation is whether attention sinks and outlier feature dimensions are a by-product of the optimizer. Both Kaul et al. (2024) and Guo et al. (2024b), show that the Adam optimizer (Kingma and Ba, 2015) leads to attention sinks and outlier features. In the former, they propose OrthoAdam which applies a rotation to the gradients to prevent any specific outlier dimensions.\nRank Collapse Recent studies have shown that stacking self-attention layers in transformers can lead to rank collapse, where token representations lose dimensionality due to inherent architectural properties (Noci et al., 2022; Geshkovski et al., 2024). This phenomenon may stem from the 'catch, tag, release' mechanism, in which attention concentrates around attention sinks, causing tokens to cluster around them and become confined to a low-rank subspace.\nLow-Rank Terms for Model Compression Beyond OATS, other works have also proposed leveraging low-rank terms to mitigate compression loss for deep networks (Yu et al., 2017; Mozaffari and Dehnavi, 2024; Li et al., 2025). Another line of research is to incorporate a low-rank adapter during compression that is fine-tuned to mitigate drops in performance (Li et al., 2023; Zhang et al., 2023; Guo et al., 2024a; Zhao et al., 2024; Mozaffari et al., 2025)."}, {"title": "6. Conclusion", "content": "This paper establishes that low-rank structures in attention weight matrices cause attention sinks and outlier features, which, in turn, enable the 'catch, tag, and release' mechanism. This mechanism serves as a fundamental organizational principle in transformers, dynamically segmenting and tagging token sequences to facilitate tasks such as few-shot learning and chain-of-thought reasoning. Empirically, we demonstrate that pruning techniques that fail to preserve low-rank structures disrupt this mechanism, significantly impairing few-shot generalization. Through rigorous theoretical analysis, we prove that 'catch, tag, and release' is not incidental but necessary for performing fundamental tasks like averaging a sub-sequence of numbers. This provides a compelling explanation for why this phenomenon emerges organically in transformer architectures."}, {"title": "C. Experiment Details", "content": "We use the HuggingFace Transformers library to load the models utilized in our experiments (Wolf et al., 2019)."}, {"title": "C.1. Pruning Hyperparameters", "content": "For all pruning experiments, we prune uniformly across all linear layers in the model, excluding the embedding and head layers, remaining consistent with (Frantar and Alistarh, 2023; Sun et al., 2024b; Zhang and Papyan, 2024). As calibration data, we use 128 sequences of length 2048 from the first shard of the C4 dataset (Raffel et al., 2019). The algorithm-specific hyperparameters are:\n\u2022 SparseGPT\nHessian Dampening: 0.1\nBlock Size: 128\n\u2022 OATS\nIterations: 80\nRank Ratio: 0.25\nThe prompt used to generate Figure 5 is:\n\nRead the following paragraph and determine if the hypothesis is true. \n Premise: A: Oh, oh yeah, and every time you see one hit on the side of the road you say is that my cat. B: Uh-huh. A: And you go crazy thinking it might be yours. B: Right, well I didn't realize my husband was such a sucker for animals until I brought one home one night. Hypothesis: her husband was such a sucker for animals. Answer:\nwhich we sourced from Yu et al. (2024)."}]}