{"title": "Angel or Devil: Discriminating Hard Samples and Anomaly Contaminations for Unsupervised Time Series Anomaly Detection", "authors": ["Ruyi Zhang", "Hongzuo Xu", "Songlei Jian", "Yusong Tan", "Haifang Zhou", "Rulin Xu"], "abstract": "Training in unsupervised time series anomaly detection is constantly plagued by the discrimination between harmful anomaly contaminations and beneficial hard normal samples. These two samples exhibit analogous loss behavior that conventional loss-based methodologies struggle to differentiate. To tackle this problem, we propose a novel approach that supplements traditional loss behavior with parameter behavior, enabling a more granular characterization of anomalous patterns. Parameter behavior is formalized by measuring the parametric response to minute perturbations in input samples. Leveraging the complementary nature of parameter and loss behaviors, we further propose a dual Parameter-Loss Data Augmentation method (termed PLDA), implemented within the reinforcement learning paradigm. During the training phase of anomaly detection, PLDA dynamically augments the training data through an iterative process that simultaneously mitigates anomaly contaminations while amplifying informative hard normal samples. PLDA demonstrates remarkable versatility, which can serve as an additional component that seamlessly integrated with existing anomaly detectors to enhance their detection performance. Extensive experiments on ten datasets show that PLDA significantly improves the performance of four distinct detectors by up to 8%, outperforming three state-of-the-art data augmentation methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Time series anomaly detection (TSAD) aims to detect data that deviate from expected normal patterns in continuous systems, with wide-ranging applications in fields like healthcare, finance, and transportation [1], [2]. Due to the challenging and costly data labeling process, unsupervised TSAD is the mainstream of current research [3]\u2013[5]. The key concept of un-supervised TSAD lies in accurately discerning normal patterns in the training set to detect anomalies in the testing set that differ from these patterns [6]. But this relies on the assumption of a pristine training set devoid of anomalies, which is often violated [7]-[10]. Real-world training sets commonly contain unknown anomalies, termed anomaly contaminations (AC) [11]- [15]. Unthinkingly utilizing contaminated training sets can distort latent representations, leading to anomaly overfitting [8]. The overfitting can complicate anomaly detection by potentially assigning low scores to certain anomalies in the testing set.\nHandling unsupervised TSAD in a contaminated training set meets a primary gap: The discrimination of anomaly contami-nations and hard samples. Hard-to-learn normal samples, also known as hard samples (HS), are normal samples located close to the decision boundary, which act as angels in clarifying the normal patterns [16]\u2013[18]. On the contrary, AC act as devils, which can severely damage the learned normal patterns. It is thus vital to keep more HS while reducing AC. Unfortunately, empirical observations show that AC and HS exhibit similarities in certain characteristics like large loss [19], [20]. Current studies widely utilize the small-loss trick to discern AC (large loss) from normal samples (small loss) [10], [21]-[23]. As depicted in Figure 1 (a), the single loss value often mistakenly discriminates between AC and HS. Therefore, we intend to introduce a new dimension to express abnormal behavior, thus further discriminating between AC and HS.\nIn TSAD training, loss $l = L(s, \\theta)$ is the mapping result of the data samples s under a set of parameters $\\theta$, which leads to a coarse-grained reflection on the data. Instead, the fluctuations in parameters $\\theta$ can reflect the subtle changes in the data at a finer granularity. Therefore, we introduce the parameter behavior alongside the loss behavior to describe the anomaly behavior.\nTo formalize the parameter behavior for a specific sample, it is natural to compare the parameters between the presence and absence of this sample. However, retraining the model for every data sample to get the results is prohibitively costly. Inspired by the works [24], [25], we evaluate how the TSAD model's parameters respond when a sample is slightly disturbed, i.e., the parameter sensitivity. As shown in Figure 1 (b), we use the parameter sensitivity to quantify the parameter behavior and construct a dual-dimensional metric with loss behavior to"}, {"title": "II. RELATED WORK", "content": "Unsupervised Time Series Anomaly Detection. Due to the rarity and concealed nature of anomalies within a large volume of normal data points, accurately labeling data is both challenging and costly [26]. As a result, the primary focus of recent research has shifted towards unsupervised TSAD [3], [4]. Methods based on one-class classification [27], [28] involve modeling the distribution of normal data, with any observations deviating from this distribution being identified as anomalies. Clustering-based methods [29], [30] typically consider observa-tions that are significantly distant from any clustering centers as anomalies. Reconstruction-based methods [31], [32] aim to reconstruct data from a low-dimensional space to capture essential normal patterns, operating under the assumption that anomalies cannot be accurately reconstructed. Overall, the fundamental concept of unsupervised TSAD is to learn the normal pattern or data distribution, which requires a clean training set devoid of any anomalies.\nSolutions for Deep TSAD in Contamination. Re-cently, TSAD in contamination prompts numerous explorations. Kieu et al. employ robust PCA for noise-signal data decomposi-tion [8]. Some VAEs-based methods apply techniques that deal better with anomalies, such as heavy-tailed distributions [11] or $\\alpha, \\beta, \\gamma$-divergences [33]. Some work adjusts the loss function based on the clustering result between the data [34]. It relies on surface feature disparities, overlooking the underlying insights of their differences during the anomaly detection process. Other solutions focus on filtering anomalies from the training set. Du et al. create pseudo-labels using the reconstruction error during training [9], potentially deepening the model's \"bias\" against AC. Leveraging the memorization effect, other researchers employ the small-loss trick to filter anomalies [21]. Li et al. [10] propose discarding samples with either high loss values or rapid changes in the loss. This method solely focuses on loss variations but overlooks the broader parameter space.\nSolutions for Other Fields in Contamination. Training set contamination, or noise labeling, also gains notable attention in other fields. Liu et al. compute the likelihood of data being clean using intra-class and inter-class similarity [35]. This class feature-based method may not suit the imbalanced binary classification problem i.e., anomaly detection. Some studies also apply the small-loss trick to refine their loss functions. Arozo et al. employ a two-component beta mixture model on training loss to identify clean and noisy data [22], while Li et al. use a loss density metric to describe differing data loss distributions [23]. These approaches separate noisy data by using a loss-related metric, yet they struggle to differentiate HS. Xia et al. leverage the parameter gradients and restrict the model to update only the key parameters with significant gradients each epoch [36]. This work provids a valuable lesson in utilizing gradients to mitigate AC but potentially diminishing model sensitivity to HS. Some studies claim to retain HS. Bai and Liu utilize the small-loss trick to find the confident normal data, and then they alternately update the confident samples and refine the classifier [37]. This method overemphasizes simple normal data and fails to reuse HS. Zhang et al. calculate the prediction history over multiple training and set a threshold for HS [38]. This off-line method cannot be iterated according to the training process of the model.\nCurrent research neglects the parameters, focusing solely on loss values, whose granularity is too coarse to identify HS. Moreover, these studies typically rely on a singular metric to pinpoint AC, missing out on the benefits of integrating multiple metrics that could help differentiate HS through their complementary relationships."}, {"title": "III. PARAMETER BEHAVIOR MODELING", "content": "In this section, we formalize the impact of a sample on the model as the parameter behavior by evaluating how the TSAD model's parameters respond when a sample is slightly disturbed.\nGiven a TSAD model with a parameter space $\\Theta$, let $L(s, \\theta)$ be the loss function of a sample s under parameters $\\theta \\in \\Theta$. Set $\\hat{\\theta}$ as the optimized parameters:\n$\\hat{\\theta} = \\underset{\\theta}{arg \\min} \\frac{1}{n}\\sum_{i=1}^{n}L(s_i, \\theta).$ (1)\nDisturbing a sample s with a small weight $\\epsilon$, we get the optimized parameters $\\theta_{\\epsilon,s}$:\n$\\theta_{\\epsilon,s} = \\underset{\\theta}{arg \\min} [\\sum_{i=1}^{n}L(s_i, \\theta) + \\epsilon L(s, \\theta)].$ (2)\nWe present the parameter sensitivity in Theorem III.1 (proof in Appendix A).\nTheorem III.1. When a training sample s is disturbed with a small weight $\\epsilon$, the gradient of the optimal parameter $\\theta_{\\epsilon,s}$ with respect to $\\epsilon$ (i.e., parameter sensitivity) is:\n$\\frac{\\alpha \\theta_{\\epsilon,s}}{d\\epsilon} |_{\\epsilon=0} = -H_{\\theta}^{-1} \\nabla_{\\theta} L(s, \\hat{\\theta}),$ (3)\nin which $H_{\\theta} = \\frac{1}{n}\\sum_{i=1}^{n} \\nabla_{\\theta}^2 L(s_i, \\hat{\\theta})$ is the Hessian matrix.\nAs the distribution could be positive or negative, we define our parameter behavior function as:\n$\\mathcal{P}(s,\\theta) = |H_{\\theta}^{-1} \\nabla_{\\theta} L(s, \\hat{\\theta})|.$ (4)\nIn practical applications, we utilize only the key param-eters to compute the behavior value, thereby reducing the consumption of computational resources. These key parameters are defined as the top-k parameters, which exhibit the most significant average behavior values during the first epoch.\nIn this section, we employ Fourier transform [39] to prove our parameter behavior's effectiveness in discriminating AC. Fourier transform decomposes signals into periodic functions, allowing us to view a sample's parameter behavior as the sum of values across different frequency components:\n$\\mathcal{P}(s,\\theta) = \\sum_f \\mathcal{P}(s(f), \\theta).$ (5)\nThen we only need to focus on the behavior of parameter $\\theta$; on each frequency f component of the data s(f). We give the Theorem III.2 (proof in Appendix A).\nTheorem III.2. The absolute contribution from frequency f to this total amount at parameter $\\theta$; is:\n$\\mathcal{P}(s(f), \\theta_{\\hat{;}}) = |\\frac{\\partial L(s(f), \\theta_{\\hat{;}})}{\\partial \\theta_i}| \\approx \\frac{A(f)}{\\omega}, e^{-\\frac{\\pi f}{\\omega}},$ (6)\nin which A(f) indicates the amplitude.\nBased on Theorem III.2, neural networks tend to learn the lower frequency components of data preferentially. In other words, there is an inverse relationship between parameter behavior and data frequency. As shown in Figure 2, normal data and AC have different spectral characteristics, with AC"}, {"title": "IV. METHOD", "content": "In this section, we detail the implementation of PLDA's key modules and demonstrate how PLDA interacts with the TSAD model to augment the training set during the training phase iteratively. In the real-world implementation, the calculations are performed in batches to improve computation speed. To simplify the expression, we take a single sample as a state in this paper, for example.\nLet $X = \\{x_0, x_1, ..., x_N\\}$ be a time series with N observations $x \\in R^D$, where D denotes the feature dimension. The classic sliding window approach usually transforms X into a series of samples $S = \\{s^0, s^h, s^{2h}, ...\\}$, where each $s^i = (x^i, x^{i+1}, ..., x^{i+w-1})$ forms a time series sample starting at $x^i$ with length w, using a window size w and stride h. The goal of TSAD is to train a model $M_{\\theta}$, with $\\theta$ representing the parameters, to capture normal patterns in the training set and detect anomalies in the testing set. Meanwhile, the goal of PLDA is to reduce AC and enrich HS in the training set, enhancing $M_{\\theta}$'s ability to learn the normal patterns. Table I summarises main notations.\nWe restructure the data augmentation into a deep reinforce-ment learning problem under the following four key modules:\n(1) Agent. The agent is designed for data augmentation to determine an optimal action $a^*$ for the given state (i.e., a sample) $s_t$. The action space is represented by the set {$a^0, a^1, a^2$}, which represents expansion, preservation, and deletion operations, respectively. (2) Data augmentation. The data augmentation adjusts the sampling rate of different data types by controlling the sliding window stride according to $a^*$. In this way, the training set $S_{i,t}$ is enhanced to $S_{i,t+1}$. (3) State transition. The state transition selects the next state $s_{t+1}$ to be analyzed according to either randomness or predefined rules. The predefined rules ensure that critical data, such as AC and HS, are more likely to be chosen by imposing appropriate constraints. (4) Data investigation. The data investigation computes the dual-dimensional reward $r_{t+1}$ for the state $s_{t+1}$ based on the parameter behavior and loss behavior. The reward provides varying feedback depending on the type of action performed and the data type involved.\nOur agent aims to learn an action-value function, defined as the expected total future rewards that take action a in the current state s. The action value function can be approximated as follows:\n$Q(s, a) = E(r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... | s_t = s, a_t = a),$ (7)\nin which $\\gamma \\in [0, 1]$ is a discount factor to balance present and future rewards. To avoid overestimating the action value, we use a well-known double DQN [42] to learn Q(s, a). This involves two networks: a policy net $Q(s, a)$ for optimal action finding and a target net $Q_{\\phi'}(s, a)$ to assess current state value. These two networks share the same MLP architecture but different parameters (i.e., $\\phi$ and $\\phi'$). $\\phi'$ is updated each iteration to minimize the following loss:\n$L' = ||r_t + \\gamma \\underset{a_{t+1}}{max}Q_{\\phi'}(s_{t+1}, a_{t+1}) - Q_{\\phi}(s_t, a_t)||^2.$ (8)"}, {"title": "V. EXPERIMENTS", "content": "Datasets: We employ eight publicly available benchmarks to evaluate PLDA: (1) ASD is collected from a large Interna-tional company. It contains the application server data every 5 minutes in 45 days, in which the first 30 days are normal actions [43]. (2) MSL is collected by NASA, which shows"}, {"title": "VII. CONCLUSION", "content": "This paper introduces PLDA, a dual parameter-loss data augmentation method, to overcome the challenges of unsu-pervised time series anomaly detection in contamination. Our work highlights the limitations of current methods in handling anomaly contaminations and hard samples. Innovatively, we introduce a parameter behavior function and propose a dual-dimensional metric to identify sample types. PLDA utilizes reinforcement learning to iteratively identify and address hard samples and anomaly contaminations during the TSAD model's training phase. It serves as an additional step to augment the detection performance in a plug-and-play manner. The efficacy of PLDA is validated through extensive experiments. Looking ahead, PLDA paves new avenues for understanding neural network behaviors in anomaly detection, suggesting promising future explorations."}, {"title": "APPENDIX", "content": "Inspired by the work [25], we provide a theoretical derivation of the parameter behavior function.\nProof. Given a training set $S = \\{s_1, s_2, ..., s_n\\}$ where the weight of all samples are 1, its empirical risk $E(\\theta)$ is:\n$E(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} L(s_i, \\theta).$ (15)\nDisturbing a sample s with a small weight $\\epsilon$, we get the new training set:\n$S' = S \\cup \\epsilon s = \\{s_1, s_2, ..., s_n, \\epsilon s\\},$ (16)\nin which the weight of sample s becomes 1+$\\,\\epsilon$. The optimized parameter $\\theta_{\\epsilon,s}$ becomes:\n$\\theta_{\\epsilon,s} = \\underset{\\theta}{arg \\min} [\\sum_{i=1}^{n} L(s_i, \\theta) + \\epsilon L(s, \\theta)]$. (17)\nThen we define $\\Delta_{\\epsilon} = \\theta_{\\epsilon,s} - \\theta$ to evaluate the change of $\\theta$. Note that $\\theta$ is the minimization result of empirical risk, and it is independent of $\\epsilon$. Then we get:\n$\\frac{d \\Delta_{\\epsilon}}{d \\epsilon} = \\frac{d(\\theta_{\\epsilon,s} - \\theta)}{d \\epsilon} = \\frac{d \\theta_{\\epsilon,s}}{d \\epsilon} - \\frac{d \\theta}{d \\epsilon} = \\frac{d \\theta_{\\epsilon,s}}{d \\epsilon}.$ (18)\nBecause $\\theta_{\\epsilon,s}$ is the minimization result of Equation (2), it satisfies the first derivative condition, that is, the first derivative with respect to $\\theta$ is 0:\n$0 = \\nabla_{\\theta} E(\\theta_{\\epsilon,s}) + \\epsilon \\nabla_{\\theta} L(s, \\theta_{\\epsilon,s}).$ (19)\nWhen $\\epsilon$ approaches 0, $\\theta_{\\epsilon,s}$ also approaches $\\theta$. We perform a Taylor expansion of first order on the right side of Equation (19), that is, expand $\\theta_{\\epsilon,s}$ near $\\theta$. Then we obtain:\n$0 \\approx [\\nabla_{\\theta} E(\\theta) + \\epsilon \\nabla_{\\theta} L(s, \\theta)]$ (20)\n$+ [\\nabla_{\\theta}^2 E(\\theta) + \\epsilon \\nabla_{\\theta}^2 L(s, \\theta)] \\Delta_{\\epsilon}.$\nwhere we omit the $R(\\theta_{\\epsilon,s})$ terms.\nSolving for $\\Delta_{\\epsilon}$, then we get:\n$\\Delta_{\\epsilon} \\approx -[\\nabla_{\\theta}^2 E(\\theta)]^{-1} [\\nabla_{\\theta} E(\\theta) + \\epsilon \\nabla_{\\theta} L(s, \\theta)].$ (21)\n$\\theta$ minimizes E, which means $\\nabla_{\\theta} E(\\theta) = 0$. Keeping only the O($\\epsilon$) terms, we have:\n$\\Delta_{\\epsilon} \\approx -[\\nabla_{\\theta}^2 E(\\theta)]^{-1} \\nabla_{\\theta} L(s, \\theta) \\epsilon.$ (22)\nCombining Equation (18), relationship between parameter change is finally obtained:\n$\\frac{\\alpha \\theta_{\\epsilon,s}}{d \\epsilon} |_{\\epsilon=0} = -H_{\\theta}^{-1} \\nabla_{\\theta} L(s, \\theta),$ (23)\nin which $H_{\\theta} = \\nabla_{\\theta}^2 E(\\theta)$ is the Hessian matrix.\nInspired by the work [58], we provide the proof of parameter behavior function effectiveness.\nProof. Given a neural network with one hidden layer with N nodes and a tanh function $\\sigma(s)$ as the activation function. It can be expressed as:\n$D(s) = \\sum_{j=1}^{N} a_j \\sigma(w_j s + \\beta_j),$ (24)\nin which $a_j, w_j, \\beta_j \\in \\mathbb{R}$ are parameters of the DNN. The Fourier transform of D(s) is:\n$F(D(s) (f) = \\sum_{j=1}^{N} a_j F(\\sigma(w_j s + \\beta_j)),$ (25)\n$\\ \\ \\ \\ = \\sum_{j=1}^{N} a_j \\sqrt{\\frac{\\pi}{2}} \\frac{1}{w_j} e^{-\\frac{\\pi f}{2 w_j}} e^{-\\frac{i \\beta_j f}{2 w_j}}.$ (26)\nAssume $\\frac{\\pi f}{2 w_j} > 0$ and $w_j > 0$, then Equation (25) is reduced to:\n$F(D(s) (f) = \\sum_{j=1}^{N} a_j \\sqrt{\\frac{\\pi}{2}} \\frac{1}{w_j} e^{-\\frac{\\pi f}{2 w_j}}.$ (27)\nWrite the difference between DNN output and the original sample at each frequency as:\n$J(s(f), \\theta) = F(D(s))(f) - F(s)(f) = A(f) e^{i \\varphi(f)},$ (28)\nin which $\\theta_i \\in \\{a_j, \\beta_j, w_j\\}$. A(f),$\\varphi(f) \\in [-\\pi, \\pi]$ are the amplitude and phase of $J(s(f), \\theta)$, respectively. The loss at\nInspired by the work [25], we provide a theoretical derivation of the parameter behavior function."}]}