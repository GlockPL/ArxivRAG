{"title": "FREQTSF: TIME SERIES FORECASTING VIA SIMULATING\nFREQUENCY KRAMER-KRONIG RELATIONS", "authors": ["Rujia Shen", "Liangliang Liu", "Boran Wang", "Yi Guan", "Yang Yang", "Jingchi Jiang"], "abstract": "Time series forecasting (TSF) is immensely important in extensive applications, such as electricity\ntransformation, financial trade, medical monitoring, and smart agriculture. Although Transformer-\nbased methods can handle time series data, their ability to predict long-term time series is limited\ndue to the \"anti-order\" nature of the self-attention mechanism. To address this problem, we focus\non frequency domain to weaken the impact of order in TSF and propose the FreqBlock, where we\nfirst obtain frequency representations through the Frequency Transform Module. Subsequently, a\nnewly designed Frequency Cross Attention is used to obtian enhanced frequency representations\nbetween the real and imaginary parts, thus establishing a link between the attention mechanism and\nthe inherent Kramer-Kronig relations (KKRs). Our backbone network, FreqTSF, adopts a residual\nstructure by concatenating multiple FreqBlocks to simulate KKRs in the frequency domain and\navoid degradation problems. On a theoretical level, we demonstrate that the proposed two modules\ncan significantly reduce the time and memory complexity from O(L2) to O(L) for each FreqBlock\ncomputation. Empirical studies on four benchmark datasets show that FreqTSF achieves an overall\nrelative MSE reduction of 15% and an overall relative MAE reduction of 11% compared to the\nstate-of-the-art methods. The code will be available soon.", "sections": [{"title": "1 Introduction", "content": "Time series forecasting (TSF) has become increasingly widespread in real-world applications, such as energy, finance, medical treatment"}, {"title": "2 Related Work", "content": "In this section, related work includes the problem statement (Section 2.1), the summarization of TSF methods (Section\n2.2), the standard method for transforming time domain information into frequency domain information - the Short\nTime Fourier Transform (Section 2.3), and Kramers-Kronig relations between the real and imaginary parts of causal\nsystems in the frequency domain (Section 2.4), which form the theoretical foundation of our work."}, {"title": "2.1 Problem Statement", "content": "Given data points $X = \\{x_1, ..., x_L\\}_{t=1}^L \\in \\mathbb{R}^{D\\times L}$ in a lookback window of time series as input, where L is the size of\nthe window, D > 1 is the number of variables, and $x_i$ is the value of the i-th variable at the t-th time step. The task\nof TSF is to predict the forecasting horizon $X = \\{\\hat{x}_1, ..., \\hat{x}_T\\}_{t=L+1}^{L+T} \\in \\mathbb{R}^{D\\times T}$. In this paper, our research is based on\nmulti-step TSF, i.e., T > 1."}, {"title": "2.2 Time Series Forecasting", "content": "Data-driven time series forecasting allows researchers to understand the evolution of systems without the need to\nconstruct precise physical laws. With the development of deep neural networks, RNNs, specialized\nfor sequence data analysis, have been proposed and achieved remarkable results. However, due to the gradient problems,\nRNN, LSTM, GRU and LSSL still need help adapting to long-term time series forecasting tasks.\nTransformer has been developed and applied to time series forecasting. As a core component of Transformer, the\nattention mechanism plays a crucial role in identifying dependencies between different variables at various time points.\nMany improvements have been proposed to make the attention mechanism suitable for time series forecasting at lower\ncomplexity . However, complex temporal patterns may\nmask such correlations and are challenging to manifest as significant trends or periodicities in the time domain. Therefore, some researches aim to use the simple multilayer perceptron (MLP) to deal with the above\nproblems. Other methods shift the perspective from time domain to frequency\ndomain, and then discover the inherent trends or periodicities of time series . However, most of the SOTA models  still process time series data in the time domain without modeling the potential frequency domain causal mechanisms of\ntime series data, facing challenges in dealing with complex temporal variations. More discussions about TSF methods\nare in the Appendix."}, {"title": "2.3 Short-Time Fourier Transform", "content": "A commonly used method for transforming time series data from the time domain to the frequency domain is the\nShort-Time Fourier Transform (STFT). The fundamental component of\nSTFT, consisting of multiple windows of different sizes, is the Fourier transform, which can be accelerated using FFT.\nSTFT initially divides more extended time series into shorter segments of equal length and then computes the Fourier\ntransform separately for each of these shorter segments. As the window slides along the time axis until it reaches the\nend, the STFT of the time series is computed, resulting in a two-dimensional representation of the series. Moreover,\ndifferent window sizes form multi-scale STFT results. Mathematically, this process can be expressed as:\n$\\displaystyle STFT\\{X\\}(\\omega, \\tau) = \\sum_{t=1}^L x_t w(t - \\tau)e^{-i\\omega t},$\\nwhere $w(\\tau)$ represents the window function, usually a Hann window, $X$ denotes the time series to be transformed and $\\omega$\nis the angular frequency with $i^2 = -1$. The $STFT\\{X\\}$ is essentially the Fourier transform of $x_tw(t - \\tau)$, representing\na function that captures the variations of the signal concerning both time domain and frequency domain."}, {"title": "2.4 Kramers-Kronig Relations", "content": "In control theory, a causal system is defined as one in which the output depends on past and present inputs but not on\nfuture ones. Such a relationship is also consistent with the inherent generation mechanism\nof time series data, where the data unfolds gradually over time, and future data points do not precede those with a time"}, {"title": "3 FreqTSF", "content": "In this section, we propose the FreqTSF, depicted in Figure 2, with a modular architecture to model the KKRs of time\nseries in the frequency domain derived from different variables. More details about the overview of the framework\nare in the Appendix. To capture intra-variable and inter-variable variations, shown in Figure 1, we design a set of"}, {"title": "3.1 Frequency Transform Module", "content": "The Frequency Transform Module transforms the input into frequency components using the STFT:\n$C_{i,p,k} = STFT\\{X_i\\}(w_k, T_p) = \\sum_{t=1}^L x_t w_p(t - T_p)e^{-iw_kt},$\nwhere $X_i$ is the i-th time series, $w_p(T_p)$ represents the p-th window function with center $T_p$ (totally P window\nfunctions with the set of window functions denoted as $W = \\{W_p\\}_{p=1...P}$), and $w_k$ is the k-th angular frequency. $C_{i,p,k}$\ndenotes the k-th result of the STFT using the p-th predefined window function for the i-th time series (including\nthe real and imaginary parts). The Equation 3 is further decomposed into the real and imaginary parts based on\n$e^{-iw_kt} = cos(-w_kt) + i sin(-w_kt) = cos(w_kt) - i sin(w_kt)$ as follows:\n$C_{i,p,k}^{Re} = \\sum_{t=1}^L x_t w_p(t - T_p) cos w_kt,$\n$C_{i,p,k}^{Im} = -\\sum_{t=1}^L x_t w_p(t - T_p) sin w_kt,$\nwhere $C_{i,p,k}^{Re}$ and $C_{i,p,k}^{Im}$ represent the k-th real and imaginary components, respectively, of the i-th time series obtained\nby decomposition using the p-th window. Considering the sparsity of the frequency domain, we select only the top-M\namplitude values to reduce the computational burden of the attention mechanism. Then we obtain the most significant\nreal components $C_{i,p,1}^{Re},..., C_{i,p,M}^{Re}$ and imaginary components $C_{i,p,1}^{Im},..., C_{i,p,M}^{Im}$, where M is a hyperparameter. Addi-\ntionally, we concatenate the two parts obtained through all P windows to get $Re \\in \\mathbb{R}^{D\\times P\\times M}$ and $Im \\in \\mathbb{R}^{D\\times P\\times M}$\nwith Amp() denoting the amplitude values:\n$\\{C_{i,p,1}^{Re},..., C_{i,p,M}^{Re}\\}, \\{C_{i,p,1}^{Im},..., C_{i,p,M}^{Im}\\} = arg Top-M(Amp(C_{i,p,k}^{Re}, C_{i,p,k}^{Im})),$\n$Re = \\underset{\\underset{i\\in\\{1,...,D\\},p\\in\\{1,...,P\\},m\\in\\{1,...,M\\}}{k\\in\\{1,...,M\\}}}{Concatenate}(C_{i,p,m}^{Re}),$\n$Im = \\underset{\\underset{i\\in\\{1,...,D\\},p\\in\\{1,...,P\\},m\\in\\{1,...,M\\}}{k\\in\\{1,...,M\\}}}{Concatenate}(C_{i,p,m}^{Im}).$"}, {"title": "3.2 Frequency Cross Attention", "content": "Based on the Theorem 1, we can obtain the corresponding imaginary part from the real part based on the KKRs, and\nvice versa. The key idea is to compute an importance score between the real and imaginary parts of each variable,\nwhich can characterize the KKRs. Therefore, we design the Frequency Cross Attention, which includes two parts, $B^{Re}$\nand $B^{Im}$, where $B^{Re}$ characterizes the ability of the real part to preserve the imaginary part, while $B^{Im}$ establishes the\nability of the imaginary part to transform into the real part, as illustrated in Figure 2 right.\nWe consider the above establishments as extensions of the relationships in the frequency domain. Therefore, to learn\nthe correct representation of the real part, we need to enrich the real part Re with the information from the imaginary\npart Im, resulting in the KKR-compliant real part $Re'$ and vice versa. To allow the model to jointly attend to information\nfrom different representation subspaces, we use H heads of cross-attention mechanisms. For the h-th head, we use\nlinear mappings to transform the real part Re into $Q_h^{Re}$, $K_h^{Re}$, and $V_h^{Re}$, and the imaginary part Im into $Q_h^{Im}$, $K_h^{Im}$, and\n$V_h^{Im}$.\n$Q_h^{Re} = Re^T W_h^{Re, Q} + b_h^{Re, Q}, Q_h^{Im} = Im^T W_h^{Im, Q} + b_h^{Im, Q},$\n$K_h^{Re} = Re^T W_h^{Re, K} + b_h^{Re, K}, K_h^{Im} = Im^T W_h^{Im, K} + b_h^{Im, K},$\n$V_h^{Re} = Re^T W_h^{Re, V} + b_h^{Re, V}, V_h^{Im} = Im^T W_h^{Im, V} + b_h^{Im, V}.$\nHere, $W_{*Q/K} \\in \\mathbb{R}^{D \\times d_k}$, $W_{*V} \\in \\mathbb{R}^{D \\times d_v}$, $b_{*Q/K} \\in \\mathbb{R}^{d_k}$, and $b_{*V} \\in \\mathbb{R}^{d_v}$ are learnable parameters. Therefore,\n$Q_{*}, K_{*} \\in \\mathbb{R}^{P \\times M \\times d_k}, V_{*} \\in \\mathbb{R}^{P \\times M \\times d_v}$. Then, when calculating $Attn_h^{Im}$, the representation of the real part Re is\nenriched with the imaginary part Im. In contrast, when calculating $Attn_h^{Re}$, the representation of the real part Re needs\nto be filtered by learnable weights to select information that can establish the KKRs with the imaginary part Im:\n$Attn_h^{Im}(Q_h^{Im}, K_h^{Im}, V_h^{Re}) = softmax(\\frac{Q_h^{Im} (K_h^{Im})^T}{\\sqrt{d_k}})V_h^{Re},$\n$Attn_h^{Re}(Q_h^{Re}, K_h^{Re}, V_h^{Im}) = softmax(\\frac{Q_h^{Re} (K_h^{Re})^T}{\\sqrt{d_k}})V_h^{Im}.$\nAfter computing the attention weight matrices for all H real attention heads and imaginary attention heads, we\nconcatenate all $Attn_h^{Im}$ and $Attn_h^{Re}$ and then apply linear transformations to map them to the real part $Re \\in \\mathbb{R}^{P\\times M\\times d_{hidden}}$\nand imaginary part $Im \\in \\mathbb{R}^{P\\times M\\times d_{hidden}}$ with implicit KKRs.\n$Re = MultiHead(Q^{Im}, K^{Im}, V^{Re}) = Concat(Attn_1^{Im},..., Attn_H^{Im})W^{Re},$\n$Im = MultiHead(Q^{Re}, K^{Re}, V^{Im}) = Concat(Attn_1^{Re}, ..., Attn_H^{Re})W^{Im}.$\nHere, H denotes the number of parallel attention heads, and the projections are parameter matrices $W^{Re}, W^{Im} \\in\n\\mathbb{R}^{H d_v \\times d_{hidden}}$. Thus, we achieve adaptive attention weight allocation for the real and imaginary parts within the variables\nby Frequency Cross Attention while determining the KKRs."}, {"title": "3.3 FreqBlock", "content": "As shown in Figure 2, we organize the FreqBlock in a residual way . Specifically, for the length-L\ninput time series $X \\in \\mathbb{R}^{D\\times L}$, we first apply a normalization block at the very beginning to deal with the nonstationary\ncharacteristics arising from varying mean and standard deviation. Additionally, a denormalization block is applied\nat the very end to transform the model outputs back to the original statistics. Subsequently, for the\nnormalized input time series $X' \\in \\mathbb{R}^{D\\times L}$, we utilize the linear embedding layer Embed to project the column inputs\ninto deep features $X' \\in \\mathbb{R}^{E \\times L}$. For the nth layer of the FreqTSF (containing a total of N layers) with the input\n$X^{n-1} \\in \\mathbb{R}^{E\\times L}$.\n$X^n = FreqBlock^n(X^{n-1}) + X^{n-1}.$\nAs depicted in Figure 2, in the nth FreqBlock, the process consists of two successive phases in the frequency domain:\ncapturing intra-variable causal variations and integrating inter-variable information."}, {"title": "3.3.1 Capturing Intra-variable Causal Variations", "content": "In each FreqBlock, we apply the Frequency Transform Module and the Frequency Cross Attention in sequence. The\ndeep features $X^{n-1} \\in \\mathbb{R}^{E \\times L}$ undergo the Frequency Transform Module to obtain enhanced real and imaginary parts in\nthe frequency domain, which are selectively Top-M enhanced in the frequency domain. Using the Frequency Cross\nAttention, we connect the real part Re and the imaginary part Im with the KKRs. This process allows us to efficiently\nderive informative causal representations using a parameter-efficient inception block. The\nformalized procedure is:\n$Re^n, Im^n = Frequency Transform Module^n(X^{n-1}),$\n$Re'^n, Im'^n = Frequency Cross Attention^n(Re^n, Im^n),$\n$\\hat{X}^n = Inception^n(Linear^n(Concat(Re'^n, Im'^n))),$\nwhere $Re^n, Im^n \\in \\mathbb{R}^{E \\times P \\times M}$ are the enhanced real and imaginary parts obtained by encoding $X^{n-1}$ through the n-th\nFrequency Transform Module, where P is the number of windows in the STFT, and M is a predefined parameter as\nmentioned in Section 3.1. $Re'^n, Im'^n \\in \\mathbb{R}^{P \\times M \\times d_{hidden}}$ represent the real and imaginary parts with potential KKRs."}, {"title": "3.3.2 Integrating Inter-variable Information", "content": "After the Frequency Cross Attention, we concatenate the real and imaginary parts with KKRs and then process them\nthrough a linear mapping $Linear^n$ with learnable parameters $W^n \\in \\mathbb{R}^{d_{hidden} \\times E}$ and $b \\in \\mathbb{R}^E$. Subsequently, we pass the\nresult through a parameter-efficient inception block denoted as $Inception^n(\\cdot)$, which incorporates multi-scale kernels\nand is widely recognized as a prominent architectural component in computer vision. The learned representations serve\nas the output of the n-th FreqBlock, combining inter-variable information that captures correlations between variables\nthrough their mutual influence."}, {"title": "3.4 Efficient computation", "content": "For each FreqBlock, the time complexity is the sum of the time complexity of the STFT in the Frequency Transform\nModule and the attention mechanism in the Frequency Cross Attention. The time complexity of the STFT with a\nwindow size of $||w_p||$ is typically expressed as $O(||w_p|| log ||w_p||)$. This is because applying the FFT to each window\nrequires computational effort of $O(||w_p|| log ||w_p||)$, and for a time series data with length L and variable number\nD, the total time complexity for P windows is $O(\\sum_{p=1}^P LD||w_p|| log ||w_p||)$. Considering that W, D, P are usually\npredefined constants and are typically much smaller than L, the time complexity is finally O(L).\nOn the other hand, the memory complexity of FreqTSF depends on the requirements of storing intermediate results in\nthe STFT and attention weight matrices in the Frequency Cross Attention. For the STFT, the FFT results must be stored\nfor each window, resulting in a cost of $O(\\sum_{p=1}^P DL)$. Therefore, the memory complexity of the Frequency Cross\nAttention is mainly due to the storage of attention weights and intermediate results. However, since the Frequency\nTransform Module freezes the input size for Frequency Cross Attention, i.e., $Re'^n, Im'^n \\in \\mathbb{R}^{E \\times P \\times M}$, where E, P, M\nare predefined hyperparameters, the complexity of Frequency Cross Attention in this work is O(EPM) = O(1).\nFurthermore, for D, P, which are generally predefined constants and typically much smaller than L, the memory\ncomplexity of FreqTSF is O(L)."}, {"title": "4 Experiments", "content": "In this section, we compare the well-acknowledged and advanced models in time series forecasting, including the\nRNN-based models: LSTM and LSSL ; CNN-based Model:\nTimesNet ; MLP-based models: LightTS and DLinear ;\nTransformer-based models: Reformer , Informer , Pyraformer ,\nAutoformer , FEDformer , Non-stationary Transformer and\nETSformer . Overall, 12 baselines are discussed in the Appendix and included for a comprehensive\ncomparison.\nTo thoroughly evaluate the performance of FreqTSF, we evaluate the proposed FreqTSF on two real-world benchmarks\nand two widely acknowledged time series simulators, covering mainstream applications in TSF: energy (the ETT\ndataset ), economics (the Exchange dataset ), medicine (the Type 1 Diabetes Mellitus\nSimulator (T1DMs) ), and agriculture (the Lintul3 ).\nSimilar to TimesNet , We follow standard protocol and split all datasets into training, validation and\ntest set in chronological order by the ratio of 6:2:2 for the ETT dataset and 7:1:2 for the other datasets. We use the mean\nsquare error (MSE) and mean absolute error (MAE) to evaluate the accuracy of TSF. Details about datasets and metrics\nare shown in the Appendix."}, {"title": "4.1 Main Results", "content": "For better comparison, we follow the experimental settings of TimesNet and Autoformer . Expressly, for the ETT, Exchange and T1DMs datasets, the lookback window length is consistently set to 96, and\nthe forecasting lengths for both training and evaluation are fixed to be 96, 192, 336, and 720, respectively. Furthermore,\nin the case of the Lintul3 dataset, the fixed lookback window length is 30, with corresponding forecasting lengths for\nboth training and evaluation set to 7, 14, 21, and 30.\nThe results of TSF are presented in Table 1. We count the number of first predictions achieved by each model across\nvarious datasets, which serves as a comprehensive metric for evaluating model performance. Experimental results show\nthat FreqTSF achieves the best overall performance across all benchmark datasets. Specifically, FreqTSF achieves\nstate-of-the-art performance in over 30% of the cases. Compared to TimesNet and Dlinear, FreqTSF exhibits an\noverall relative reduction of 15% in MSEs and 11% in MAEs. Notably, on specific datasets, such as ETTh1, the"}, {"title": "4.2 Model Efficiency Analysis", "content": "To summarize the model performance and efficiency, we conduct a comparison of parameter scale and GPU memory\nsize on the ETTh1 dataset using five baseline models that achieved the first MSE and MAE in TSF (i.e. TimesNet,\nETSformer, LightTS, DLinear, and FEDformer). Among these models, our proposed FreqTSF and TimesNet exhibit\nnearly identical parameter sizes and GPU memory, with parameter sizes of 4.6MB and 4.5MB and GPU memory\nsizes of 1703MiB for both. In contrast, the MLP-based models, such as LightTS and Dlinear, have relatively smaller\nparameter sizes and GPU memory, averaging around 0.04MB and 1699MiB, respectively. However, these models may\nstruggle to simulate time series data effectively due to their small parameter size. On the other hand, ETSformer and\nFEDformer have parameter sizes and GPU memory similar to MLP-based models, but they fail to capture the extreme\nvalues in time series data. We attribute the parameter size of our model to the adoption of the inception block at the end\nof the FreqBlock, a design choice shared with TimesNet. Despite the increased parameters, FreqTSF achieves the best\noverall performance within an acceptable single-GPU model size."}, {"title": "5 Discussion", "content": "This study presents, for the first time, an attention-based algorithm for simulating the KKRs in the frequency domain.\nMajor findings include: (1) our proposed FreqTSF demonstrates significantly superior performance compared to\nbaseline TSF algorithms in terms of both accuracy and computational complexity; (2) through ablation experiments on\nmodules, shown in the Appendix, the combination of the two proposed modules (the Frequency Transform Module and\nthe Frequency Cross Attention) leads to a significant improvement in model performance; (3) ablation experiments on\nthe frequency domain filter in the Appendix confirm the importance of M, which affects the final model performance;\n(4) ablation experiments on hyperparameter sensitivity in the Appendix show the robustness and reproducibility of\nFreqTSF. However, it is worth noting that Table 1 presents the performances of FreqTSF compared to state-of-the-art\nbaselines. Nevertheless, for Exchange and T1DMs, the performances of FreqTSF do not exhibit statistical significance"}, {"title": "6 Conclusion, Limitations and Future Works", "content": "In this paper, we propose FreqTSF that leverages FreqBlocks connected in a residual way, resulting in state-of-the-art\nperformance. Each FreqBlock employs the STFT for the multi-period signal analysis of variables, facilitating the\nacquisition of intra-variable time series KKRs from a frequency perspective. The proposed Frequency Cross Attention\nfully exploits the KKRs of the real and imaginary parts in the frequency domain of causal systems, demonstrating its\neffectiveness in time series forecasting. Extensive experiments demonstrate that FreqTSF achieves superior forecasting\nperformance with minimal resource consumption on four benchmark datasets, outperforming 12 state-of-the-art\nalgorithms. Furthermore, FreqTSF reduces the time and memory complexity of each FreqBlock computation from\nO(L2) to O(L).\nWhile FreqTSF demonstrates exceptional performance, there are still some limitations. For example, FreqTSF may\nincorrectly categorize mutations in time-series data as noise and overlook the accurate modeling of simple periodic\ncurves. In addition, FreqTSF currently relies solely on attention to learn the KKR representation, but the use of neural\nordinary differential equations (ODEs)  could potentially provide a superior solution. Therefore,\nto further improve and overcome the limitations of FreqTSF, in our future work, we intend to utilize both time- and\nfrequency-domain information to improve model performance. In addition, we aim to explore large-scale pre-training\nmethods specifically tailored to time series applications, using FreqTSF as the underlying architecture. This approach\nhas the potential to bring significant benefits to a wide range of downstream tasks."}, {"title": "A.1 Time Series Forecasting", "content": "Many classical methods, such as ARIMA , follow a Markovian process\nby constructing autoregressive TSF models based on predefined time patterns. However, this autoregressive process\nbecomes ineffective in real-world nonlinear and non-stationary conditions.\nWith the flourishing development of deep neural networks, RNNs, specialized for sequence data analysis, have been\nproposed and achieved remarkable results. Vanilla RNN , as the simplest form in the RNN\nfamily with a single hidden layer and recurrent connections, often suffers from the problems of vanishing or exploding\ngradient, which makes it challenging for long sequence prediction . To address the gradient\nproblems, Long Short-Term Memory (LSTM)  and Gated Recurrent Unit (GRU)\n employ gated structures to control the flow of information, allowing better handling of dependencies\nin long sequences. LSSL  conceptually unifies the strengths of RNNs and maps a sequence by simply\nsimulating a linear continuous-time state-space representation. However, due to inherent design flaws, recurrent models\nstill need help adapting to long-term TSF tasks.\nTransformer has been developed and applied to TSF. As a core component of the Transformer, the attention mechanism\nplays a crucial role in identifying dependencies between different variables at various time points. Reformer \naddresses the problem of quadratic computational complexity in the attention mechanism. It replaces\ndot-product attention with locality-sensitive hashing to reduce the time complexity of attention computation and\nemploys reversible residual layers to reduce memory complexity. Informer  proposes the probspare\nself-attention to reduce the time and memory complexity to O(Llog L). Pyraformer  adopts pyramidal\nattention to capture hierarchical multi-scale temporal dependencies, thereby reducing the computational cost. Non-\nstationary Transformer  focuses on improving the forecasting ability to model the non-stationary\ndata. It introduces series stationarization and de-stationary attention and demonstrates its effectiveness in forecasting\nnon-stationary data prediction through both theoretical analysis and experiments. However, relying on the attention\nmechanism in the above Transformer-based methods to directly discover correlations between variables at different\ntime points is unreliable. Complex temporal patterns may mask such correlations and are challenging to manifest as\nsignificant trends or periodicities in the time domain. Furthermore, the primary capability of the\nTransformer comes from its attention mechanism, which inevitably leads to the loss of temporal information .\nSome researches aim to use the simple multilayer perceptron (MLP) to deal with the above problems. LightTS \napplies an MLP-based structure on top of two delicate down-sampling strategies, including interval and\ncontinuous sampling. Dlinear  introduces an embarrassingly simple one-layer linear model and\nemploys direct multistep forecasting, achieving high accuracy. Other approaches shift the focus from the time domain\nto the frequency domain. Autoformer  proposes autocorrelation mechanisms which first employ period-\nbased dependencies and then aggregate time delay using FFT, for accelerated computation. Fedformer \nadopts an expert mixture design to improve trend and periodicity decomposition and proposes a sparse attention\nmechanism in the frequency domain, ensuring efficiency while improving prediction accuracy. ETSformer \nemploys exponential smoothing attention and frequency attention mechanisms to replace the self-attention\nmechanism in the vanilla Transformer, improving both accuracy and efficiency. TimesNet  provides\na new approach to TSF by decomposing time series into multiple periods using predefined windows. Through the\ninteraction of intra-period and inter-period information, it connects generic inception blocks to complete TSF. However,\nmodels mentioned above process time series data without modeling intra- and inter-variable variations. As a result, they\nstill face challenges in dealing with complex temporal variations."}, {"title": "A.2 Theoretical Proof", "content": "Theorem 1 If X(t) is a causal time function with respect to time t, i.e., X(t) =\n$X(t) = \\begin{cases} X(t), & t \\ge 0\\\\\n0, & t0\ncontain the continuous-time unit impulse function $d(t) = $\n$\nSo,\n(X(t), t\u22650\nand derivatives at t = 0. Suppose the Fourier\ntransform $F$ of $X (t)$ is represented in the form of the real part $Re$ and the imaginary part $Im$, i.e., $X(t) \u2194 F(w) =$\n$Re(w) + iIm(w)$. Then, we have:"}, {"title": "A.3 General Overview of FreqTSF", "content": "The input lookback window, $X \\in \\mathbb{R}^{D\\times L}$, undergoes normalization and embedding modules to obtain $X' \\in \\mathbb{R}^{E\\times L}$. It\nis then fed into a stack of residual FreqBlocks, and the output of the Nth-layer FreqBlock, after denormalization, serves\nas the forecasting horizon $X \\in \\mathbb{R}^{D\\times T}$. The nth FreqBlock is defined as $X^n = FreqBlock^n(X^{n-1}) + X^{n-1}$. First, the\nFreqBlock utilizes Equation 3 to apply the STFT with windows of various sizes to the time series data, obtaining the\ncorresponding real and imaginary parts in the frequency domain, which follows the Top-M frequency enhancement\n(Equation 5). Next, the FreqBlock establishes intra-variable KKRs by applying the Frequency Cross Attention module\nbetween the enhanced real and imaginary parts. Finally, the FreqBlock integrates inter-variable information through a\nparameter-efficient inception block. The Frequency Cross Attention module leverages KKR-based attention to construct\nmathematical mechanisms, and concatenates the real part Re and the imaginary part Im of the attention output (Equation\n8), representing the hidden information with inherent KKRs in the frequency domain for output."}, {"title": "A.4 Datasets", "content": "To thoroughly evaluate the performance of FreqTSF, we evaluate the proposed FreqTSF on two real-world benchmarks\nand two widely acknowledged time series simulators, covering mainstream applications in TSF: energy (the ETT\ndataset ), economics (the Exchange dataset ), medicine (the Type 1 Diabetes Mellitus\nSimulator (T1DMs) ), and agriculture (the Lintul3 ).\n\u2022 The ETT dataset  comprises data collected from electricity transformers, recorded every 15\nminutes from July 2016 to July 2018. The dataset contains seven time-series variables, including load and oil\ntemperature. Specifically, ETTh1 and ETTh2 are variant datasets with hourly granularity.\n\u2022 The Exchange dataset  contains daily exchange rates for eight countries recorded from 1990\nto 2016, encompassing eight time-series variables."}, {"title": "A.5 Evaluation Metrics", "content": "We use the mean square error (MSE) and mean absolute error (MAE) to evaluate the accuracy of TSF, which is\ncalculated as follows:\n$MSE(\\hat{X}, X) = \\frac{1}{DXT} \\sum_{d=1}^{D} \\sum_{t=L+1}^{L+T} (\\hat{x}_t^d - x_t^d)^2$\n$MAE(\\hat{X}, X) = \\frac{1}{DXT} \\sum_{d=1}^{D} \\sum_{t=L+1}^{L+T} |\\hat{x}_t^d - x_t^d|$\nHere, $X \\in \\mathbb{R}^{D\\times T}$ represents the estimated time series, while X denotes the corresponding ground-truth."}, {"title": "A.6 Implementation details", "content": "Similar to TimesNet , our proposed FreqTSF is trained using the L2 loss and optimized using the\nAdam optimizer with an initial learning rate of 10-3. The batch size is set to 32. All experiments are repeated five\ntimes, implemented in PyTorch, and conducted on a single NVIDIA RTX TITAN 24GB GPU.\nNote that we set the embedding dimension E to 32 and map the input $X \\in \\mathbb{R}^{D\\times L}$ to $X' \\in \\mathbb{R}^{32\\times L}$. Subsequently,\nwe employ N = 4 FreqBlocks with residual connections to avoid the degradation issues. In each FreqBlock, we\nutilize P = 3 windows with sizes W = {100, 50, 20}, all of which are Hann windows. We make the top-M = 10\nselection for the STFT result in each window to reduce computational complexity. The dimensions $d_k$, $d_v$, and $d_{hidden}$\nin the Frequency Cross Attention are all set to 64. Within"}]}