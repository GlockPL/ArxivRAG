{"title": "Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners", "authors": ["Daniele Paliotta", "Junxiong Wang", "Matteo Pagliardini", "Kevin Y. Li", "Aviv Bick", "J. Zico Kolter", "Albert Gu", "Fran\u00e7ois Fleuret", "Tri Dao"], "abstract": "Recent advancements have demonstrated that the performance of large language models (LLMs) can be significantly enhanced by scaling computational resources at test time. A common strategy involves generating multiple Chain-of-Thought (CoT) trajectories and aggregating their outputs through various selection mechanisms. This raises a fundamental question: can models with lower complexity leverage their superior generation throughput to outperform similarly sized Transformers for a fixed computational budget? To address this question and overcome the lack of strong subquadratic reasoners, we distill pure and hybrid Mamba models from pretrained Transformers. Trained on only 8 billion tokens, our distilled models show strong performance and scaling on mathematical reasoning datasets while being much faster at inference for large batches and long sequences. Despite the zero-shot performance hit due to distillation, both pure and hybrid Mamba models can scale their coverage and accuracy performance past their Transformer teacher models under fixed time budgets, opening a new direction for scaling inference compute.", "sections": [{"title": "1. Introduction", "content": "Reasoning in large language models (LLMs) has seen a significant boost in performance recently, largely driven by scaling inference compute. A key technique to enhance \"reasoning\" performance is the use of intermediate reasoning steps before producing a final answer, known as Chain-of-Thought (CoT) (Wei et al., 2023). Building on this, many test-time compute techniques often involve generating multiple CoTs (Wu et al., 2024; Snell et al., 2024) and selecting the best one. Even simple strategies, such as majority voting, can be surprisingly effective (Brown et al., 2024; Beeching et al., 2024). Furthermore, trained reward models can provide scores for the final model answers and even for the individual steps of the CoTs (Luo et al., 2024).\nHowever, these test-time compute techniques introduce significant challenges for LLM systems. Generating long CoT sequences or large batches of completions places substantial demands on memory and compute resources. Transformers, in particular, struggle with such workloads due to their linear memory scaling and memory-bound nature during generation. This raises an important question: how should we optimize model architectures to best scale test-time compute? In particular, can alternative architectures with faster and more efficient generation outperform current LLMs under fixed compute budgets? Addressing this problem could unlock new avenues for deploying reasoning models with different architectures, enabling them to run and scale more efficiently on hardware and environments with limited memory and compute.\nRecent subquadratic architectures have training time or pre-fill time linear in sequence length, and constant memory requirement (instead of linear) during inference. This enables up to 5\u00d7 higher inference throughput (Gu & Dao, 2024; Peng et al., 2023) as inference time for large batch size or long sequences is dominated by the time to load the model states (KV cache or RNN states). Despite their efficiency, subquadratic models have not been extensively explored in reasoning tasks, primarily due to the lack of large-scale pre-trained models compared to Transformer-based counterparts. As a result, it remains unclear whether: (1) scaling inference compute for subquadratic models improves reasoning performance, and (2) subquadratic models can match or exceed Transformers models under fixed compute budgets.\nIn this work, we explore the reasoning capabilities of sub-quadratic architectures by distilling knowledge from pre-trained Transformers into hybrid and pure Mamba models. To address the scarcity of pretrained subquadratic models with robust reasoning abilities, we develop recipes to distill specific reasoning skills into these architectures. We then benchmark the models for multiple Chain-of-Thought (CoT) completions, providing a comprehensive analysis of performance under fixed compute and memory constraints. Our approach advances the Pareto front established by existing models, achieving a better trade-off between efficiency and reasoning capability.\nOur distilled pure and hybrid subquadratic reasoners are able to outperform their Transformer teachers on both coverage and accuracy on MATH (Lightman et al., 2023) and GSM8K (Cobbe et al., 2021) mathematical reasoning tasks on most time budgets, reaching the same quality with 2.5\u00d7 less inference time. Our results highlight the potential for distilling reasoning and math capabilities across architectures in a cost-effective manner while maintaining the inference compute scaling properties of Transformers."}, {"title": "2. Related Work", "content": "Scaling inference time compute has emerged as a promising strategy to improve the performance of LLMs. Techniques such as Chain of Thought (CoT) and its variants have demonstrated significant performance improvements across various reasoning benchmarks by decomposing complex tasks into intermediate steps (Wei et al., 2023; Yao et al., 2023)\nWhile these approaches improve reasoning through task decomposition, they also increase computational demands due to longer generation sequences. Recent work suggests that this additional compute may itself contribute to improved model abilities (Pfau et al., 2024). Dynamic compute allocation during inference has further advanced this paradigm. For instance, Goyal et al. (2024) introduced pause tokens into the LLM vocabulary, enabling models to allocate compute more effectively and achieve better reasoning and task performance.\nAnother prominent approach involves generating and searching through multiple model outputs to select the best answer. Various sampling algorithms have been proposed to increase the diversity and quality of generated outputs to increase the likelihood of the correct or best answer being selected (Wang et al., 2023; Renze & Guven, 2024; Zhang et al., 2023). In parallel, outcome and process reward models (ORMs and PRMs) have been introduced to help evaluate the best response and guide intermediate generation steps within the LLM model (Lightman et al., 2023; Zhang et al., 2024a; Luo et al., 2024; Uesato et al., 2022).\nRecent work has shown that smaller LLMs, when scaled through inference-time compute (e.g., via majority voting or PRM-guided search), can outperform larger models under fixed compute budgets (Snell et al., 2024; Wu et al., 2024; Beeching et al., 2024). However, these findings are primarily limited to Transformer-based architectures. The extent to which these scaling laws apply to subquadratic architectures, which offer faster inference but may trade off expressiveness, remains underexplored."}, {"title": "2.2. Subquadratic Architecture Alternatives", "content": "While Transformers dominate the landscape of reasoning models (Grattafiori et al., 2024; Qwen et al., 2025), alternative architectures have been proposed to mitigate their high computational cost. These models, based on RNNs (Beck et al., 2024; Peng et al., 2023), SSMs (Gu et al., 2022; Gu & Dao, 2024), and linear attention mechanisms (Katharopoulos et al., 2020; Yang et al., 2024), offer improved inference and memory efficiency especially for long-context tasks and large-batch generation, making them attractive for large-scale language modeling. Notably, the Mamba family of models (Mamba-1 and Mamba-2) has introduced selective state spaces, enabling linear-time sequence modeling without sacrificing performance (Gu & Dao, 2024; Dao & Gu, 2024). Hybrid architectures that combine sub-quadratic layers (e.g., Mamba) with a limited number of self-attention layers have also emerged, achieving superior performance compared to pure Transformer or subquadratic models (Lieber et al., 2024; Ren et al., 2024; Dong et al., 2024). These architectures are particularly well-suited for the increased compute demands of inference-time scaling. Our work evaluates the inference-time scaling properties of both pure and hybrid subquadratic models."}, {"title": "2.3. Knowledge Distillation", "content": "Knowledge distillation has proven effective in transferring capabilities from large teacher models to smaller, more efficient student models (Hinton et al., 2015). In the context of LLMs, distillation is commonly used to compress a larger pre-trained LLM into a smaller version while maintaining core knowledge and functionality (Gu et al., 2024; Xu et al., 2024). Although larger models exhibit better reasoning and overall abilities due to the properties of scale (Xu et al., 2025; Wei et al., 2022), distillation has enabled smaller models to achieve strong reasoning performance (DeepSeek-AI et al., 2025; Labs, 2025). While most distillation efforts focus on within-architecture transfer (e.g., Transformer to Transformer), recent work has explored cross-architecture distillation. Pretrained Transformers have been successfully distilled into recurrent architectures such as RNNs (Kasai et al., 2021; Mercat et al., 2024), linear attention (Zhang et al., 2024b), convolutions (Ralambomihanta et al., 2024), and SSMs (Bick et al., 2024; Wang et al., 2025). Whether strong reasoning can be distilled across architectures remains an open question."}, {"title": "3. Distilling Student Reasoners", "content": "In this section, we describe how we distilled Llama models into pure Mamba and hybrid architectures. We refer to our pure Mamba models as Llamba, and our hybrid models as MambaInLlama. We distill both our hybrid and pure Mamba models using Llama 3.2-1B-Instruct and Llama 3.2-3B-Instruct from the Llama family of models (Grattafiori et al., 2024)."}, {"title": "3.1. Distilling into Llamba", "content": "Distillation method. In order to distill pure Mamba models, we modify the MOHAWK distillation procedure introduced by Bick et al. (2024). MOHAWK is composed of three stages: 1) matrix orientation, 2) hidden state alignment, and 3) weight transfer and knowledge distillation. Stage 1 (matrix orientation) aligns the Mamba-2 model's SSM matrix mixer (Dao & Gu, 2024) with the teacher's self-attention matrix by minimizing the distance between the two matrices. Stage 2 (hidden state alignment) matches the student and teacher's layers' hidden state outputs. Both of these stages are run independently across layers to prevent previous optimization gaps from propagation through the model. This is done by setting the input of the student layer to be that of the previous teacher layer's output. Stage 3 (weight transfer and knowledge distillation) transfers the remaining, unoptimized parameters, e.g., MLPs, embeddings, and norms, and finetunes the complete end-to-end student model using a distillation loss on the student and teacher logits (Hinton et al., 2015). We deviate from the original MOHAWK paper by transferring the MLP weights and norms of each teacher decoder layer to the student and training those parameters as well during Stage 2. This stems from the architectural differences between Phi (Li et al., 2023) (MLP and self-attention in parallel) and Llama (Grattafiori et al., 2024) (sequential self-attention and MLP). Stage 3 remains the same with fewer weights transferred.\nExperimental details. Our pure Mamba-distilled models, Llamba-1B and Llamba-4B, dubbed after Bick et al. (2025), which uses a similar methodology and the same teacher models, are distilled from their respective teacher models using our adjusted MOHAWK distillation approach with only 8 billion tokens total each. Following Bick et al. (2024), we use a variant of Mamba-2 that converts the SSM head structure to multi-head (compared to the original multi-value and Llama's grouped-query structure) and converts the sequence mixer to entirely discrete-time. The post-convolution activation and pre-output projection normalization are also removed. The 8B token distillation dataset is composed of 4B tokens from FineMath-4+ (Lozhkov et al., 2024), allocated as 1B and 3B to Stages 1 and 2 respectively, and 4B tokens from OpenMathInstruct-2 (Toshniwal et al., 2024) used in Stage 3, which is the only stage in which we apply the chat template to the inputs. Unlike for our hybrid models, we find that computing the loss on both the assistant output and user prompt improves model performance over just the assistant output. All three distillation stages use the AdamW optimizer with \u03b2 = (0.9, 0.95) and weight decay of 0.1 and a Warmup-Stable-Decay (WSD) scheduler with 10% warmup and 10% decay (Hu et al., 2024) at a 2048 context length. In Stages 1 and 2, we set the learning rate to 1 \u00d7 10-4, while in Stage 3, it is set to 1 \u00d7 10\u22125. The hyperparameters are the same for the 1B and 4B distillation runs. Our final Llamba-1B model has 16 layers of our Mamba-2 variant with a state size of 64 and multi-head pattern of 32 heads and state size of 64. Likewise, our Llamba-4B utilizes the same pattern with 24 heads and state size of 128 for 28 layers. We note that our pure Mamba-2 models are slightly larger than their Transformer counterparts due to the pattern conversion from grouped-query to multi-head and additional parameters found within the Mamba-2 layer, e.g., gating."}, {"title": "3.2. Distilling into MambaInLlama", "content": "Distillation method. We follow two separate directions for distillation. For the hybrid models, we modify the protocol proposed by Wang et al. (2025) in order to distill some specific capabilities. These techniques have been shown to be effective for hybrid architectures. The Mamba-in-Llama framework (Wang et al., 2025) introduces a method for distilling hybrid Transformer-Mamba models by reusing weights from the attention layers. In the distillation process shown in Alg 1, the linear projections for Q, K, V and O are initialized using the corresponding linear projections for C, B, X and O respectively. The only additional learned parameters in the new layers are the sampling rate A and the dynamic A. These new parameters will control the constructed Mamba through the discretization function. Specifically, we take A \u2208 \\mathbb{R}^{N'} to discretize Bt, Ct \u2208 \\mathbb{R}^{N\u00d71} and obtain B_{t}, C_{t} \u2208 \\mathbb{R}^{N'\u00d7N\u00d71} as shown in Alg 1. We directly reuse the MLP layers. Differently from Wang et al. (2025), we replace the attention layers with Mamba layers in a single round and finetune the whole model. For distillation, we employ token-level KL divergence. The full probability distribution of the student model, p(\u00b7; \u03b8), is trained to align with the full distribution of the teacher model, p(\u00b7; \u03b8T), by minimizing the KL divergence across all possible next tokens at position t. Different from (Wang et al., 2025), we use the reverse KL divergence, D_{KL}(p(\u00b7; \u03b8) || p(\u00b7; \u03b8_{T})) instead of the forward KL divergence as the loss function, since reverse KL behaves more like mode-seeking and better mimics the peak values. And we find that it yields better results empirically. We adopt the Mamba-1 architecture for our hybrid models, as Lieber et al. (2024); Wang et al. (2024); Dong et al. (2024) demonstrates that using Mamba-1 in a hybrid architecture yields better results, especially for challenging reasoning tasks.\nExperimental details. Our hybrid Mamba models, named MambaInLlama-1B (with 4 attention layers in 16 total layers) and MambaInLlama-3B (with 6 attention layers in 26 total layers), are distilled with 8B tokens from OpenMathInstruct-2 (Toshniwal et al., 2024). We apply the Llama chat template, mask the user prompt, and compute the loss only over the tokens generated in the assistant's output. Thus, the total number of supervised tokens is reduced to roughly 7B. To speed up training, we use data packing to merge different sequences into a single one until we reach the maximum sequence length which is set to 8192. We use the AdamW optimizer with learning rate 2 \u00d7 10\u22125, \u03b2 = (0.9, 0.95) and a weight decay of 0.1. The hyperparameters are the same for the 1B and 3B models. In Mamba layers, we set the SSM state size to 16. Consequently, the number of SSM groups after expansion is 2048/16 = 128 for the 1B model and 3072/16 = 192 for the 3B model.\nRemarks on the distillation dataset. We finetune the Llama teacher models on the same data used during distillation to avoid our models from potentially gaining an unfair advantage. The results, reported in Figure 3, show that the continuous training of the base model on the distillation data mix has a negligible effect on performance. Moreover, we find that the selection of data used during distillation has a significant impact on the final capabilities of the distilled models. Switching the Stage 3 dataset in Llamba-1B from OpenMathInstruct-2 to OpenHermes-2.5 (Teknium, 2023) decreased greedy decoding accuracy on MATH (acc@1) by more than 10 percentage points.\nLack of correlation between reasoning and general benchmarks. We also highlight the lack of correlation between common multiple choice-based benchmarks and mathematical reasoning, as the OpenHermes variant of Llamba-1B outperforms the final Llamba-1B by more than 5 points on MMLU (Hendrycks et al., 2021a) and 0.5 point on ARC-Challenge (Clark et al., 2018). Moreover, analyzing the acc@1 performance of Llamba-1B and Llamba-4B on MATH after each of the three stages, we see that the sharp increase in reasoning ability between Stage 2 and Stage 3 is not reflected in general knowledge benchmarks (Fig. 12)."}, {"title": "3.3. Improving performance after distillation", "content": "We show that it is possible to improve the accuracy and coverage of our distilled models by performing some supervised fine-tuning (SFT) after distillation. This is inspired by Wang et al. (2025), where SFT is an integral part of the distillation protocol. Starting from the distilled MambainLlama-1B and 3B, we fine-tune the models for two epochs using 8 billion tokens from OpenMathInstruct-2. The distilled models achieve impressive performance both in coverage and accuracy, even surpassing the original Llama models. This is illustrated in Figure 8."}, {"title": "4. Scaling Inference Time Compute", "content": "We scale test-time compute using our distilled models by generating multiple CoTs to solve a set of math problems. The system prompt (Figure 7) contains instructions on how to properly format the response. The model outputs are parsed to extract the final solution, which is then compared to the ground truth. This approach enables us to evaluate the model's performance in generating correct solutions across multiple attempts. Moreover, the results demonstrate that the models are able to retain their instruction following ability after distillation.\nEvaluation metrics. We evaluate our model using two primary metrics: coverage and accuracy. In domains like coding and formal proofs, where answers can be automatically verified, coverage directly translates to improved performance and has been widely adopted (Chen et al., 2021; Brown et al., 2024). Coverage is commonly referred to as the pass@k metric, where k denotes the number of samples per problem (Chen et al., 2021; Brown et al., 2024). This metric estimates the probability that at least one correct solution exists among the k samples. To reduce the variance when calculating coverage, we adopt the unbiased estimation formula from Chen et al. (2021). Specifically, we generate N > k total samples per task. The probability that a correct solution exists among a pool of k generated samples can then be determined given the total number of correct solutions Ci for each task.\npass@k = \\frac{1}{\\text{# of problems}} \\sum_{i=1}^{\\text{# of problems}} \\bigg(1 - {\\binom{n - C_i}{k} \\over \\binom{n}{k}} \\bigg)\nWe implement this formula using a numerically stable approach as suggested by Chen et al. (2021)(see Appendix C).\nFor accuracy, we use multiple aggregation strategies. Majority voting, or self-consistency decoding (Wang et al., 2023)"}, {"title": "5. Results", "content": "We (i) measure the inference speedup of our distilled models (\u00a7 5.1), and (ii) show that this speedup can result in better scaling for a given inference time budget (\u00a7 5.2)."}, {"title": "5.1. Inference time results", "content": "Experimental protocol. In order to measure the inference time and throughput of our distilled models, we focus on creating a realistic setup that matches the prompt and CoTs lengths that we find for MATH and GSM8K. We compare the runtime of a Llama 3.2 architecture against our distilled models. For Llama, we use FlashAttention2 (Dao, 2023) and torch compile. The implementations of our models rely on the standard Mamba implementation. Given a varying batch size, we consider the tasks of generating 512 tokens from a 512 token prompt, which reflect the length found in the evaluation datasets. The prefilling time to process the prompt is not included in the benchmark, as it may depend on the redundancy of a given prompt within the batch. In fact, in our setting, we are only interested in the time to generate the multiple completions given one prompt. Our benchmark is done on a single NVIDIA H100 GPU, and averaged results over multiple runs are shown in Figure 2.\nDistilled models are significantly faster. Results in Figure 2 show our distilled models are up to \u00d73.7 and \u00d74.2 faster than their respective Llama 1B and 3B baselines. Moreover, MambaInLlama and Llamba are more memory efficient and thus can run larger batches. In Figure 2b, our models can accommodate batches of 512 while Llama-3B returns an out-of-memory error. We also notice that MambaInLlama models are slightly faster than Llamba. We speculate that this is because MambaInLlama has a smaller SSM state size of 16, while Llamba uses a larger SSM state size of 64. Additionally, Llamba has larger projections because of its multi-head structure.\nRemark. In our speed experiments, both the prompt and the generation lengths are relatively short compared to many other domains. When evaluating multi-turn conversations, for example, where distilled Mamba architectures are already shown to excel (Wang et al., 2025), it is clear that the length of the context and of the CoTs can be significantly larger than what has been considered in our experiments. Such longer sequences would significantly increase the throughput advantage of our models. Unfortunately, it is not yet clear how to evaluate and exploit test-time compute for more subjective, conversational tasks.\nLimitations. We try to ensure fair speed comparisons between the model types by utilizing roughly equivalent implementations: standard implementation of Mamba-1/2 and FlashAttention-based self-attention with Pytorch compilation. However, it is worth noting that there exist optimizations for current Transformer architectures, such as memory management improvements (Kwon et al., 2023), that enable faster generation; similar boosts are currently not readily available for alternative architectures. Deeper optimizations for each architecture are beyond the scope of this work."}, {"title": "5.2. Results on reasoning tasks", "content": "Experimental protocol. We benchmark the teacher models (Llama-3.2 1B-Instruct and 3B-Instruct) and the distilled Mamba students (MambaInLlama-1B, MambaInLlama-3B, Llamba-1B, Llamba-4B) on the MATH-500 subset of the MATH dataset (Lightman et al., 2023; Hendrycks et al., 2021b) and a randomly selected 500-sample subset of GSM8K (Cobbe et al., 2021). We evaluate performance based on coverage and accuracy with majority voting and weighted Best-of-N (\u00a7 4). We sample responses from the distilled models with temperatures T = 0.6, 0.8 and top_k = -1 to consider all tokens. For each response, we sample up to 2048 tokens. For the distilled models, we find that T = 0.6 and T = 0.8 are ideal for the 1B and 3B scale, respectively, and subsequent results are obtained using these temperatures. The official Llama evaluation system prompt is used for the MATH dataset, while the original prompt is kept for GSM8K as displayed in Figure 7. For our process reward model, we utilize a base Llama3.1-8B-Instruct model trained on Mistral-generated data (Xiong et al., 2024).\nDistilled Models can Cover like Teachers. When observing the scaling of coverage as the number of generation k increases in Figure 1a (resp. Fig. 5a for GSM8K), we see distilled models closely matching the coverage of their teachers. Only a small degradation is observed. When we plot the coverage as a function of the time budget, by associating each coverage measurement to the time required to generate the associated number of completions (see Fig. 1b), we find that our distilled models are exceptional in their ability to generate correct answers fast. By generating many more completions for the same time budget, the overall Pareto front for coverage in both MATH and GSM8K (Fig. 1b, 5b) is heavily dominated by our distilled models where both pure and hybrid Mamba reasoners are able to achieve the same degree of coverage in nearly half the time of their respective teachers. Given a sufficiently strong reward model or verifiable solutions, those coverage scores would in large parts translate to accuracy.\nDistilled Models Achieve Competitive Accuracy Under Fixed Time. Training with distillation and utilizing sub-quadratic architectures\u2014which are less expressive than Transformers can degrade model quality. However, we find that this is a worthy trade-off when comparing performance under fixed time budgets in Figure 6 and Figure 4.\nSimilarly to coverage, the lighter and faster batch inference of the distilled models, allowing for more generations, results in a better accuracy/time Pareto front at several completion scales. Interestingly, while comparing models of similar sizes indicates that larger time budgets are dominated by the teacher models, we observe that the larger distilled model can provide better accuracy than the smaller baseline while still being faster. For example, while Llama-1B provides better accuracy than MambaInLlama-1B for larger time budgets, MambaInLlama-3B takes over where MambaInLlama-1B left off, providing a better accuracy than Llama-1B for inference time. Similarly, we conjecture that it would be possible to distill a larger baseline model that would outperform Llama-3B, providing better accuracy for a given time budget.\nLarger Students Faster and Better than Smaller Teachers. The core driving force behind the growing interest in subquadratic models is their computational efficiency. Such properties enable our distilled models of larger sizes (3B scale) to generate samples faster than even a smaller Transformer (1B). Our MambaInLlama-3B and Llamba-4B models outperform the slower Llama-1B baseline on coverage and accuracy while being faster. These inference speedups, rooted in the underlying architecture, allow larger and more capable models to be used in time-constrained environments, despite their increased number of parameters.\nSmaller models have great coverage. When focusing on coverage, we observe in Figure 1b and Figure 5b that most of the Pareto front is occupied by 1B models. Contrasting those results with the majority voting accuracy results in Figure 4a and Figure 6a where the gap between 1B and 3B models is much more significant. An interpretation is that, while smaller models have the ability to generate the correct answer, the probability of generating it within a constrained number of samples increases with the model size. This finding has implications in choosing model size for tasks involving formal language where the answer is easily verifiable, such as coding and mathematical proofs. In those applications, coverage matters most, and smaller models may be preferred given their better time/coverage efficiency compared to larger models.\nSFT improves the models significantly Following distillation, which establishes a strong foundation for our models, we observe that additional supervised fine-tuning (SFT) significantly enhances their performance, as illustrated in Figure 8. This suggests that while distillation effectively transfers knowledge from the teacher model, SFT further refines and aligns the model's capabilities, enabling it to become highly competitive. Our results indicate that by leveraging both distillation and SFT, subquadratic architectures can match or even surpass their Transformer teachers in absolute performance on reasoning tasks."}, {"title": "6. Conclusion", "content": "In our work, we investigate whether lower-complexity models can leverage their superior generation throughput to outperform similarly sized Transformers under a fixed computational budget. We focus on reasoning tasks where we can scale test-time compute to improve performance. Through extensive experimentation, we distill both pure and hybrid Mamba models at the 1B and 3B scales and evaluate their reasoning capabilities on mathematical reasoning benchmarks, where the ability of subquadratic models to quickly generate many completions enables them to take advantage of their scaling properties when increasing inference compute. When fixing memory and/or compute, our models achieve better coverage and accuracy for most time budgets compared to their Transformer, teacher counterparts. These findings highlight the potential of Mamba and other attention alternatives as strong substitutes to Transformers for tasks that benefit from scalable inference compute.\nWe hope this work inspires future work in pretraining sub-quadratic reasoners and further exploring their inference scaling properties. More research is required to determine the best way to distill reasoning capabilities across architectures, as performance remains highly sensitive to both data and distillation techniques. Moreover, since our distilled models demonstrate exceptional coverage, developing better reward models to better identify correct answers can close the accuracy gap. Finally, further investigation into scaling inference compute for conversational and subjective tasks would open to a scenario where lighter subquadratic models can achieve larger gains in performance and speed"}]}