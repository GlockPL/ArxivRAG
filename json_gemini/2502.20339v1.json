{"title": "Thinking Slow, Fast:\nScaling Inference Compute with Distilled Reasoners", "authors": ["Daniele Paliotta", "Junxiong Wang", "Matteo Pagliardini", "Kevin Y. Li", "Aviv Bick", "J. Zico Kolter", "Albert Gu", "Fran\u00e7ois Fleuret", "Tri Dao"], "abstract": "Recent advancements have demonstrated that the\nperformance of large language models (LLMs)\ncan be significantly enhanced by scaling com-\nputational resources at test time. A common\nstrategy involves generating multiple Chain-of-\nThought (CoT) trajectories and aggregating their\noutputs through various selection mechanisms.\nThis raises a fundamental question: can models\nwith lower complexity leverage their superior gen-\neration throughput to outperform similarly sized\nTransformers for a fixed computational budget?\nTo address this question and overcome the lack\nof strong subquadratic reasoners, we distill pure\nand hybrid Mamba models from pretrained Trans-\nformers. Trained on only 8 billion tokens, our\ndistilled models show strong performance and\nscaling on mathematical reasoning datasets while\nbeing much faster at inference for large batches\nand long sequences. Despite the zero-shot perfor-\nmance hit due to distillation, both pure and hybrid\nMamba models can scale their coverage and accu-\nracy performance past their Transformer teacher\nmodels under fixed time budgets, opening a new\ndirection for scaling inference compute.", "sections": [{"title": "1. Introduction", "content": "Reasoning in large language models (LLMs) has seen a\nsignificant boost in performance recently, largely driven by\nscaling inference compute. A key technique to enhance \"rea-\nsoning\" performance is the use of intermediate reasoning\nsteps before producing a final answer, known as Chain-of-\nThought (CoT) (Wei et al., 2023). Building on this, many\ntest-time compute techniques often involve generating mul-\ntiple CoTs (Wu et al., 2024; Snell et al., 2024) and selecting\nthe best one. Even simple strategies, such as majority voting,\ncan be surprisingly effective (Brown et al., 2024; Beech-\ning et al., 2024). Furthermore, trained reward models can\nprovide scores for the final model answers and even for the\nindividual steps of the CoTs (Luo et al., 2024).\nHowever, these test-time compute techniques introduce sig-\nnificant challenges for LLM systems. Generating long CoT\nsequences or large batches of completions places substantial\ndemands on memory and compute resources. Transform-\ners, in particular, struggle with such workloads due to their\nlinear memory scaling and memory-bound nature during\ngeneration. This raises an important question: how should\nwe optimize model architectures to best scale test-time com-\npute? In particular, can alternative architectures with faster\nand more efficient generation outperform current LLMs\nunder fixed compute budgets? Addressing this problem\ncould unlock new avenues for deploying reasoning models\nwith different architectures, enabling them to run and scale\nmore efficiently on hardware and environments with limited\nmemory and compute.\nRecent subquadratic architectures have training time or pre-\nfill time linear in sequence length, and constant memory re-\nquirement (instead of linear) during inference. This enables\nup to 5\u00d7 higher inference throughput (Gu & Dao, 2024;\nPeng et al., 2023) as inference time for large batch size or\nlong sequences is dominated by the time to load the model\nstates (KV cache or RNN states). Despite their efficiency,\nsubquadratic models have not been extensively explored in\nreasoning tasks, primarily due to the lack of large-scale pre-\ntrained models compared to Transformer-based counterparts.\nAs a result, it remains unclear whether: (1) scaling inference\ncompute for subquadratic models improves reasoning per-\nformance, and (2) subquadratic models can match or exceed\nTransformers models under fixed compute budgets.\nIn this work, we explore the reasoning capabilities of sub-\nquadratic architectures by distilling knowledge from pre-\ntrained Transformers into hybrid and pure Mamba models.\nTo address the scarcity of pretrained subquadratic models\nwith robust reasoning abilities, we develop recipes to distill"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Scaling Inference Time Compute for Reasoning", "content": "Scaling inference time compute has emerged as a promis-\ning strategy to improve the performance of LLMs. Tech-\nniques such as Chain of Thought (CoT) and its variants have\ndemonstrated significant performance improvements across\nvarious reasoning benchmarks by decomposing complex\ntasks into intermediate steps (Wei et al., 2023; Yao et al.,\n2023)\nWhile these approaches improve reasoning through task de-\ncomposition, they also increase computational demands due\nto longer generation sequences. Recent work suggests that\nthis additional compute may itself contribute to improved\nmodel abilities (Pfau et al., 2024). Dynamic compute allo-\ncation during inference has further advanced this paradigm.\nFor instance, Goyal et al. (2024) introduced pause tokens\ninto the LLM vocabulary, enabling models to allocate com-\npute more effectively and achieve better reasoning and task\nperformance.\nAnother prominent approach involves generating and search-\ning through multiple model outputs to select the best an-\nswer. Various sampling algorithms have been proposed to\nincrease the diversity and quality of generated outputs to\nincrease the likelihood of the correct or best answer being\nselected (Wang et al., 2023; Renze & Guven, 2024; Zhang\net al., 2023). In parallel, outcome and process reward mod-\nels (ORMs and PRMs) have been introduced to help evaluate\nthe best response and guide intermediate generation steps\nwithin the LLM model (Lightman et al., 2023; Zhang et al.,\n2024a; Luo et al., 2024; Uesato et al., 2022).\nRecent work has shown that smaller LLMs, when scaled\nthrough inference-time compute (e.g., via majority voting or\nPRM-guided search), can outperform larger models under\nfixed compute budgets (Snell et al., 2024; Wu et al., 2024;\nBeeching et al., 2024). However, these findings are primarily\nlimited to Transformer-based architectures. The extent to\nwhich these scaling laws apply to subquadratic architectures,\nwhich offer faster inference but may trade off expressiveness,\nremains underexplored."}, {"title": "2.2. Subquadratic Architecture Alternatives", "content": "While Transformers dominate the landscape of reasoning\nmodels (Grattafiori et al., 2024; Qwen et al., 2025), alterna-\ntive architectures have been proposed to mitigate their high\ncomputational cost. These models, based on RNNs (Beck\net al., 2024; Peng et al., 2023), SSMs (Gu et al., 2022; Gu &"}, {"title": null, "content": "Dao, 2024), and linear attention mechanisms (Katharopou-\nlos et al., 2020; Yang et al., 2024), offer improved infer-\nence and memory efficiency especially for long-context\ntasks and large-batch generation, making them attractive\nfor large-scale language modeling. Notably, the Mamba\nfamily of models (Mamba-1 and Mamba-2) has introduced\nselective state spaces, enabling linear-time sequence mod-\neling without sacrificing performance (Gu & Dao, 2024;\nDao & Gu, 2024). Hybrid architectures that combine sub-\nquadratic layers (e.g., Mamba) with a limited number of\nself-attention layers have also emerged, achieving superior\nperformance compared to pure Transformer or subquadratic\nmodels (Lieber et al., 2024; Ren et al., 2024; Dong et al.,\n2024). These architectures are particularly well-suited for\nthe increased compute demands of inference-time scaling.\nOur work evaluates the inference-time scaling properties of\nboth pure and hybrid subquadratic models."}, {"title": "2.3. Knowledge Distillation", "content": "Knowledge distillation has proven effective in transferring\ncapabilities from large teacher models to smaller, more effi-\ncient student models (Hinton et al., 2015). In the context of\nLLMs, distillation is commonly used to compress a larger\npre-trained LLM into a smaller version while maintaining\ncore knowledge and functionality (Gu et al., 2024; Xu et al.,\n2024). Although larger models exhibit better reasoning\nand overall abilities due to the properties of scale (Xu et al.,\n2025; Wei et al., 2022), distillation has enabled smaller mod-\nels to achieve strong reasoning performance (DeepSeek-AI\net al., 2025; Labs, 2025). While most distillation efforts\nfocus on within-architecture transfer (e.g., Transformer to\nTransformer), recent work has explored cross-architecture\ndistillation. Pretrained Transformers have been successfully\ndistilled into recurrent architectures such as RNNs (Kasai\net al., 2021; Mercat et al., 2024), linear attention (Zhang\net al., 2024b), convolutions (Ralambomihanta et al., 2024),\nand SSMs (Bick et al., 2024; Wang et al., 2025). Whether\nstrong reasoning can be distilled across architectures re-\nmains an open question."}, {"title": "3. Distilling Student Reasoners", "content": "In this section, we describe how we distilled Llama models\ninto pure Mamba and hybrid architectures. We refer to our\npure Mamba models as Llamba, and our hybrid models\nas MambaInLlama. We distill both our hybrid and pure\nMamba models using Llama 3.2-1B-Instruct and Llama 3.2-\n3B-Instruct from the Llama family of models (Grattafiori\net al., 2024)."}, {"title": "3.1. Distilling into Llamba", "content": "Distillation method. In order to distill pure Mamba models,\nwe modify the MOHAWK distillation procedure introduced"}, {"title": null, "content": "by Bick et al. (2024). MOHAWK is composed of three\nstages: 1) matrix orientation, 2) hidden state alignment, and\n3) weight transfer and knowledge distillation. Stage 1 (ma-\ntrix orientation) aligns the Mamba-2 model's SSM matrix\nmixer (Dao & Gu, 2024) with the teacher's self-attention\nmatrix by minimizing the distance between the two matrices.\nStage 2 (hidden state alignment) matches the student and\nteacher's layers' hidden state outputs. Both of these stages\nare run independently across layers to prevent previous opti-\nmization gaps from propagation through the model. This is\ndone by setting the input of the student layer to be that of\nthe previous teacher layer's output. Stage 3 (weight transfer\nand knowledge distillation) transfers the remaining, unop-\ntimized parameters, e.g., MLPs, embeddings, and norms,\nand finetunes the complete end-to-end student model using\na distillation loss on the student and teacher logits (Hinton\net al., 2015). We deviate from the original MOHAWK paper\nby transferring the MLP weights and norms of each teacher\ndecoder layer to the student and training those parameters as\nwell during Stage 2. This stems from the architectural differ-\nences between Phi (Li et al., 2023) (MLP and self-attention\nin parallel) and Llama (Grattafiori et al., 2024) (sequential\nself-attention and MLP). Stage 3 remains the same with\nfewer weights transferred.\nExperimental details. Our pure Mamba-distilled models,\nLlamba-1B and Llamba-4B, dubbed after Bick et al. (2025),\nwhich uses a similar methodology and the same teacher mod-\nels, are distilled from their respective teacher models using\nour adjusted MOHAWK distillation approach with only 8\nbillion tokens total each. Following Bick et al. (2024), we\nuse a variant of Mamba-2 that converts the SSM head struc-\nture to multi-head (compared to the original multi-value and\nLlama's grouped-query structure) and converts the sequence\nmixer to entirely discrete-time. The post-convolution ac-\ntivation and pre-output projection normalization are also"}, {"title": "3.2. Distilling into MambaInLlama", "content": "Distillation method. We follow two separate directions for\ndistillation. For the hybrid models, we modify the protocol\nproposed by Wang et al. (2025) in order to distill some\nspecific capabilities. These techniques have been shown to\nbe effective for hybrid architectures. The Mamba-in-Llama\nframework (Wang et al., 2025) introduces a method for\ndistilling hybrid Transformer-Mamba models by reusing\nweights from the attention layers. In the distillation process\nshown in Alg 1, the linear projections for Q, K, V and O\nare initialized using the corresponding linear projections for\nC, B, X and O respectively. The only additional learned\nparameters in the new layers are the sampling rate A and\nthe dynamic A. These new parameters will control the con-\nstructed Mamba through the discretization function. Specif-\nically, we take $\\mathbf{A} \\in \\mathbb{R}^{N'}$ to discretize $\\mathbf{B}_t, \\mathbf{C}_t \\in \\mathbb{R}^{N \\times 1}$ and\nobtain $\\widetilde{\\mathbf{B}}_t, \\widetilde{\\mathbf{C}}_t \\in \\mathbb{R}^{N' \\times N \\times 1}$ as shown in Alg 1. We directly\nreuse the MLP layers. Differently from Wang et al. (2025),\nwe replace the attention layers with Mamba layers in a sin-\ngle round and finetune the whole model. For distillation,\nwe employ token-level KL divergence. The full probabil-"}, {"title": null, "content": "ity distribution of the student model, $p(\\cdot; \\theta)$, is trained to\nalign with the full distribution of the teacher model, $p(\\cdot; \\theta_T)$,\nby minimizing the KL divergence across all possible next\ntokens at position $t$. Different from (Wang et al., 2025),\nwe use the reverse KL divergence, $D_{KL}(p(\\cdot; \\theta) || p(\\cdot; \\theta_T))$\ninstead of the forward KL divergence as the loss function,\nsince reverse KL behaves more like mode-seeking and better\nmimics the peak values. And we find that it yields better\nresults empirically. We adopt the Mamba-1 architecture\nfor our hybrid models, as Lieber et al. (2024); Wang et al.\n(2024); Dong et al. (2024) demonstrates that using Mamba-1\nin a hybrid architecture yields better results, especially for\nchallenging reasoning tasks.\nExperimental details. Our hybrid Mamba models, named\nMambaInLlama-1B (with 4 attention layers in 16 total\nlayers) and MambaInLlama-3B (with 6 attention layers\nin 26 total layers), are distilled with 8B tokens from\nOpenMathInstruct-2 (Toshniwal et al., 2024). We apply\nthe Llama chat template, mask the user prompt, and com-\npute the loss only over the tokens generated in the assistant's\noutput. Thus, the total number of supervised tokens is re-\nduced to roughly 7B. To speed up training, we use data\npacking to merge different sequences into a single one until\nwe reach the maximum sequence length which is set to 8192.\nWe use the AdamW optimizer with learning rate 2 \u00d7 10\u22125,\n\u03b2 = (0.9, 0.95) and a weight decay of 0.1. The hyperpa-\nrameters are the same for the 1B and 3B models. In Mamba\nlayers, we set the SSM state size to 16. Consequently, the\nnumber of SSM groups after expansion is 2048/16 = 128\nfor the 1B model and 3072/16 = 192 for the 3B model.\nRemarks on the distillation dataset. We finetune the\nLlama teacher models on the same data used during distilla-\ntion to avoid our models from potentially gaining an unfair\nadvantage. The results, reported in Figure 3, show that the\ncontinuous training of the base model on the distillation data\nmix has a negligible effect on performance. Moreover, we\nfind that the selection of data used during distillation has a\nsignificant impact on the final capabilities of the distilled\nmodels. Switching the Stage 3 dataset in Llamba-1B from\nOpenMathInstruct-2 to OpenHermes-2.5 (Teknium, 2023)\ndecreased greedy decoding accuracy on MATH (acc@1) by\nmore than 10 percentage points."}, {"title": null, "content": "Lack of correlation between reasoning and general\nbenchmarks. We also highlight the lack of correlation\nbetween common multiple choice-based benchmarks and\nmathematical reasoning, as the OpenHermes variant of\nLlamba-1B outperforms the final Llamba-1B by more than 5\npoints on MMLU (Hendrycks et al., 2021a) and 0.5 point on\nARC-Challenge (Clark et al., 2018). Moreover, analyzing\nthe acc@1 performance of Llamba-1B and Llamba-4B on\nMATH after each of the three stages, we see that the sharp\nincrease in reasoning ability between Stage 2 and Stage 3 is"}, {"title": "3.3. Improving performance after distillation", "content": "We show that it is possible to improve the accuracy and cov-\nerage of our distilled models by performing some supervised\nfine-tuning (SFT) after distillation. This is inspired by Wang\net al. (2025), where SFT is an integral part of the distillation\nprotocol. Starting from the distilled MambainLlama-1B\nand 3B, we fine-tune the models for two epochs using 8\nbillion tokens from OpenMathInstruct-2. The distilled mod-\nels achieve impressive performance both in coverage and\naccuracy, even surpassing the original Llama models. This\nis illustrated in Figure 8."}, {"title": "4. Scaling Inference Time Compute", "content": "We scale test-time compute using our distilled models by\ngenerating multiple CoTs to solve a set of math problems.\nThe system prompt (Figure 7) contains instructions on how\nto properly format the response. The model outputs are\nparsed to extract the final solution, which is then compared\nto the ground truth. This approach enables us to evaluate\nthe model's performance in generating correct solutions\nacross multiple attempts. Moreover, the results demonstrate\nthat the models are able to retain their instruction following\nability after distillation.\nEvaluation metrics. We evaluate our model using two\nprimary metrics: coverage and accuracy. In domains like\ncoding and formal proofs, where answers can be automati-\ncally verified, coverage directly translates to improved per-\nformance and has been widely adopted (Chen et al., 2021;\nBrown et al., 2024). Coverage is commonly referred to as\nthe pass@k metric, where $k$ denotes the number of sam-\nples per problem (Chen et al., 2021; Brown et al., 2024).\nThis metric estimates the probability that at least one cor-\nrect solution exists among the $k$ samples. To reduce the\nvariance when calculating coverage, we adopt the unbiased\nestimation formula from Chen et al. (2021). Specifically,\nwe generate $N > k$ total samples per task. The probability\nthat a correct solution exists among a pool of $k$ generated\nsamples can then be determined given the total number of\ncorrect solutions $C_i$ for each task.\n$\\text{pass}@k = \\frac{1}{\\text{# of problems}} \\sum_{i=1}^{\\text{# of problems}} (1 - {N-C_i \\choose k} / {N \\choose k})$\nWe implement this formula using a numerically stable ap-\nproach as suggested by Chen et al. (2021)(see Appendix C).\nFor accuracy, we use multiple aggregation strategies. Major-\nity voting, or self-consistency decoding (Wang et al., 2023)"}, {"title": null, "content": "is the most straightforward method to aggregate responses\nand compute an accuracy score. A more refined strategy\ninvolves using a trained verifier to select the best response\n(we call this approach Best-of-N).\nAs our verifier, we utilize a reward model trained using\nprocess supervision, where the model receives feedback on\neach step of the reasoning process. Inspired by Snell et al.\n(2024), we utilize a Llama-3.1 8B-based reward model to\nscore the solutions for Best-of-N. As PRMs produce a cumu-\nlative sequence of step-level scores per solution, we perform\na reduction over the steps to obtain a single solution-level\nscore which we use for answer selection. Following Snell\net al. (2024), we use the final score in all the steps as the\nscore for Best-of-N.\nWhile Best-of-N simply selects the generation with the high-\nest reward, weighted Best-of-N aggregates the rewards of"}, {"title": "5. Results", "content": "We (i) measure the inference speedup of our distilled models\n(\u00a7 5.1), and (ii) show that this speedup can result in better\nscaling for a given inference time budget (\u00a7 5.2)."}, {"title": "5.1. Inference time results", "content": "Experimental protocol. In order to measure the inference\ntime and throughput of our distilled models, we focus on\ncreating a realistic setup that matches the prompt and CoTs\nlengths that we find for MATH and GSM8K. We compare\nthe runtime of a Llama 3.2 architecture against our distilled\nmodels. For Llama, we use FlashAttention2 (Dao, 2023)\nand torch compile. The implementations of our models rely\non the standard Mamba implementation. Given a varying\nbatch size, we consider the tasks of generating 512 tokens\nfrom a 512 token prompt, which reflect the length found in\nthe evaluation datasets. The prefilling time to process the\nprompt is not included in the benchmark, as it may depend\non the redundancy of a given prompt within the batch. In\nfact, in our setting, we are only interested in the time to\ngenerate the multiple completions given one prompt. Our\nbenchmark is done on a single NVIDIA H100 GPU, and\naveraged results over multiple runs are shown in Figure 2.\nDistilled models are significantly faster. Results in Fig-\nure 2 show our distilled models are up to \u00d73.7 and \u00d74.2\nfaster than their respective Llama 1B and 3B baselines.\nMoreover, MambaInLlama and Llamba are more memory\nefficient and thus can run larger batches. In Figure 2b, our\nmodels can accommodate batches of 512 while Llama-3B\nreturns an out-of-memory error. We also notice that Mam-\nbaInLlama models are slightly faster than Llamba. We spec-\nulate that this is because MambaInLlama has a smaller SSM\nstate size of 16, while Llamba uses a larger SSM state size\nof 64. Additionally, Llamba has larger projections because\nof its multi-head structure.\nRemark. In our speed experiments, both the prompt and\nthe generation lengths are relatively short compared to many\nother domains. When evaluating multi-turn conversations,\nfor example, where distilled Mamba architectures are al-\nready shown to excel (Wang et al., 2025), it is clear that\nthe length of the context and of the CoTs can be signifi-"}, {"title": null, "content": "cantly larger than what has been considered in our experi-\nments. Such longer sequences would significantly increase\nthe throughput advantage of our models. Unfortunately, it is\nnot yet clear how to evaluate and exploit test-time compute\nfor more subjective, conversational tasks.\nLimitations. We try to ensure fair speed comparisons be-\ntween the model types by utilizing roughly equivalent im-\nplementations: standard implementation of Mamba-1/2 and\nFlashAttention-based self-attention with Pytorch compila-\ntion. However, it is worth noting that there exist optimiza-\ntions for current Transformer architectures, such as memory\nmanagement improvements (Kwon et al., 2023), that enable\nfaster generation; similar boosts are currently not readily\navailable for alternative architectures. Deeper optimizations\nfor each architecture are beyond the scope of this work."}, {"title": "5.2. Results on reasoning tasks", "content": "Experimental protocol. We benchmark the teacher models\n(Llama-3.2 1B-Instruct and 3B-Instruct) and the distilled\nMamba students (MambaInLlama-1B, MambaInLlama-3B,\nLlamba-1B, Llamba-4B) on the MATH-500 subset of the\nMATH dataset (Lightman et al., 2023; Hendrycks et al.,\n2021b) and a randomly selected 500-sample subset of\nGSM8K (Cobbe et al., 2021). We evaluate performance\nbased on coverage and accuracy with majority voting and\nweighted Best-of-N (\u00a7 4). We sample responses from\nthe distilled models with temperatures $T = 0.6, 0.8$ and\ntop_k = -1 to consider all tokens. For each response, we\nsample up to 2048 tokens. For the distilled models, we find\nthat T = 0.6 and T = 0.8 are ideal for the 1B and 3B scale,\nrespectively, and subsequent results are obtained using these\ntemperatures. The official Llama evaluation system prompt\nis used for the MATH dataset, while the original prompt is\nkept for GSM8K as displayed in Figure 7. For our process\nreward model, we utilize a base Llama3.1-8B-Instruct model\ntrained on Mistral-generated data (Xiong et al., 2024)."}, {"title": null, "content": "Distilled Models can Cover like Teachers. When observ-ing the scaling of coverage as the number of generation $k$\nincreases in Figure 1a (resp. Fig. 5a for GSM8K), we see\ndistilled models closely matching the coverage of their teach-ers. Only a small degradation is observed. When we plot\nthe coverage as a function of the time budget, by associating\neach coverage measurement to the time required to generate\nthe associated number of completions (see Fig. 1b), we find\nthat our distilled models are exceptional in their ability to\ngenerate correct answers fast. By generating many more\ncompletions for the same time budget, the overall Pareto\nfront for coverage in both MATH and GSM8K (Fig. 1b, 5b)\nis heavily dominated by our distilled models where both\npure and hybrid Mamba reasoners are able to achieve the\nsame degree of coverage in nearly half the time of their re-spective teachers. Given a sufficiently strong reward model\nor verifiable solutions, those coverage scores would in large\nparts translate to accuracy.\nDistilled Models Achieve Competitive Accuracy UnderFixed Time. Training with distillation and utilizing sub-quadratic architectures\u2014which are less expressive thanTransformers\u2014can degrade model quality. However, wefind that this is a worthy trade-off when comparing perfor-mance under fixed time budgets in Figure 6 and Figure 4.Similarly to coverage, the lighter and faster batch infer-ence of the distilled models, allowing for more generations,results in a better accuracy/time Pareto front at several com-pletion scales. Interestingly, while comparing models ofsimilar sizes indicates that larger time budgets are dom-inated by the teacher models, we observe that the largerdistilled model can provide better accuracy than the smallerbaseline while still being faster. For example, while Llama-1B provides better accuracy than MambaInLlama-1B forlarger time budgets, MambaInLlama-3B takes over whereMambaInLlama-1B left off, providing a better accuracy thanLlama-1B for inference time. Similarly, we conjecture thatit would be possible to distill a larger baseline model thatwould outperform Llama-3B, providing better accuracy fora given time budget.\nLarger Students Faster and Better than Smaller Teach-ers. The core driving force behind the growing interestin subquadratic models is their computational efficiency.Such properties enable our distilled models of larger sizes(3B scale) to generate samples faster than even a smallerTransformer (1B). Our MambaInLlama-3B and Llamba-4B"}, {"title": null, "content": "models outperform the slower Llama-1B baseline on cov-erage and accuracy while being faster. These inferencespeedups, rooted in the underlying architecture, allow largerand more capable models to be used in time-constrainedenvironments, despite their increased number of parameters.Smaller models have great coverage. When focusing oncoverage, we observe in Figure 1b and Figure 5b that mostof the Pareto front is occupied by 1B models. Contrastingthose results with the majority voting accuracy results inFigure 4a and Figure 6a where the gap between 1B and3B models is much more significant. An interpretation isthat, while smaller models have the ability to generate thecorrect answer, the probability of generating it within a con-strained number of samples increases with the model size.This finding has implications in choosing model size fortasks involving formal language where the answer is easilyverifiable, such as coding and mathematical proofs. In thoseapplications, coverage matters most, and smaller modelsmay be preferred given their better time/coverage efficiencycompared to larger models.\nSFT improves the models significantly Following distilla-tion, which establishes a strong foundation for our models,we observe that additional supervised fine-tuning (SFT)significantly enhances their performance, as illustrated inFigure 8. This suggests that while distillation effectivelytransfers knowledge from the teacher model, SFT furtherrefines and aligns the model's capabilities, enabling it tobecome highly competitive. Our results indicate that byleveraging both distillation and SFT, subquadratic architec-tures can match or even surpass their Transformer teachersin absolute performance on reasoning tasks."}, {"title": "6. Conclusion", "content": "In our work, we investigate whether lower-complexity mod-els can leverage their superior generation throughput to out-perform similarly sized Transformers under a fixed compu-tational budget. We focus on reasoning tasks where we canscale test-time compute to improve performance. Throughextensive experimentation, we distill both pure and hybridMamba models at the 1B and 3B scales and evaluate theirreasoning capabilities on mathematical reasoning bench-marks, where the ability of subquadratic models to quicklygenerate many completions enables them to take advantageof their scaling properties when increasing inference com-pute. When fixing memory and/or compute, our modelsachieve better coverage and accuracy for most time budgetscompared to their Transformer, teacher counterparts. Thesefindings highlight the potential of Mamba and other atten-tion alternatives as strong substitutes to Transformers fortasks that benefit from scalable inference compute.\nWe hope this work inspires future work in pretraining sub-quadratic reasoners and further exploring their inference"}, {"title": "7. Acknowledgements", "content": "We thank Cartesia and TogetherAI for their support in pro-viding compute for this project. KL is supported by fundingfrom the Bosch Center for Artificial Intelligence."}]}