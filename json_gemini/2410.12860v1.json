{"title": "LLMD: A Large Language Model for Interpreting Longitudinal Medical Records", "authors": ["Robert Porter", "Adam Diehl", "Benjamin Pastel", "J. Henry Hinnefeld", "Lawson Nerenberg", "Pye Maung", "Sebastien Kerbrat", "Gillian Hanson", "Troy Astorino", "Stephen J. Tarsa"], "abstract": "We introduce LLMD\u00b9, a large language model (LLM) designed to analyze a patient's medical history based on their medical records. Along with domain knowledge, LLMD is trained on a large corpus of records collected over time and across facilities, as well as tasks and labels that make nuanced connections among them. This approach is critical to an accurate picture of patient health, and has distinctive advantages over models trained on knowledge alone, unlabeled records, structured data from electronic health record (EHR) aggregators, or records from a single health system. Today, LLMD is deployed to support virtual care, care coordination, and the curation of datasets behind 60+ research studies, including data submitted to the FDA.\n\nThe recipe for LLMD first continues pretraining a foundational model on both domain knowledge and the contents of millions of records. These span an average of 10 years of care and as many as 140 care sites per patient. LLMD is then instruction fine-tuned on structuring and abstraction tasks - the former jointly identify and normalize document metadata, provenance information, clinical named-entities, and ontology mappings, while the latter roll these into higher-level representations, such a continuous era of time a patient was on a medication. LLMD is deployed within a layered validation system that includes continual random audits and configurable review by experts, e.g. based on uncertainty, disease-specific rules, or end use-case. This provides feedback to improve LLMD and fine-grained control over data quality for a spectrum of needs.\n\nLLMD exhibits large gains over both more-powerful generalized models and domain-specific models. On medical knowledge benchmarks, LLMD-8B achieves state of the art accuracy on PubMedQA text responses, besting orders-of-magnitude larger models. On production tasks, we show that LLMD significantly outperforms all other models evaluated, and among alternatives, large general purpose LLMs like GPT-40 are more accurate than models emphasizing medical knowledge. We find strong evidence that accuracy on today's medical benchmarks is not the most significant factor when analyzing real-world patient data, an insight that validates our approach and has implications for future medical LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "LLMD is a large language model (LLM) for understanding and ana-lyzing a patient's medical history. Today, it is deployed to power patient- and research-facing products offered by PicnicHealth. LLMs represent an astonishing breakthrough in Artificial Intelligence (AI) [1-3], and exhibit nuanced pattern matching and informa-tion recall capabilities. In the medical domain, LLMs fine-tuned on"}, {"title": "2 MEDICAL RECORDS ON THE PICNICHEALTH PLATFORM", "content": "PicnicHealth works with patients to retrieve and manage their medical records, regardless of the format they are in or the facility holding them. This includes electronic records as well as paper-based records. Paper records are of particular importance for visits to providers practicing outside of large health systems, for histori-cal records produced before Electronic Health Records (EHRs) were ubiquitous, and for facilities whose systems impede sharing. PicnicHealth's platform is format-agnostic under its base case support for processing physical copies of records. It is also facility agnostic based on the legal right patients have to access records on request.\n\nBy building a complete, longitudinal picture of one's health, PicnicHealth is able to offer compelling services to both patients and researchers. For patients, PicnicHealth offers products for virtual care, care coordination, and records-management. For researchers, PicnicHealth offers products to improve the speed, flexibility, and cost of observational studies supporting new therapies by directly engaging with willing patients."}, {"title": "3 TASKS & TRAINING DATA", "content": "This section describes how we build LLMD, including the details of our tasks and training data."}, {"title": "3.1 Continued Pre-Training", "content": "Table 1 shows the breakdown of the dataset we use to continue pretraining a foundational model using a large unlabeled corpus. This step follows the same training configuration as the founda-tional model, e.g. next-token prediction for Llama models. The primary purpose is to adapt the foundational model to the patterns of medical records, while imbuing it with key information needed by downstream tasks [11, 12]. Our dataset consists of 28B tokens, approximately 90% sourced from records and 10% representing med-ical knowledge. Along with widely-available sources for general medical knowledge, we include two data sources supporting Pub-MedQA - the PubMedQA training dataset enriched with CoT, and papers from PubMed Central \u2013 but none directly supporting other benchmarks. This deliberate choice shapes LLMD to PubMedQA for the results in this paper, allowing us to analyze how well knowl-edge transfers to other benchmarks and to map out the relationship between accuracy on benchmarks and records-processing."}, {"title": "3.2 Instruction Fine-Tuning", "content": "Any task that we use to process records can be performed by either a human abstractor using software developed internally or by our LLM. Data generated by abstractors is used for LLM training, as well as model auditing, label correction, and inter-rater reliability studies against clinicians.\n\nWe categorize tasks into two types: structuring and abstraction:"}, {"title": "3.2.1 Structuring Tasks", "content": "Once retrieved, the challenge of turning medical records into usable, reliable data is daunting [14, 15]. Issues found in their pages include contradictions, errors, omissions, and even notoriously difficult-to-read handwriting for paper records. Pervasive problems like the misdiagnoses in Section 1 can happen for reasons as mundane as a provider choosing the wrong option from a drop down list in EHR software. Or, a Medication List section that is intended to be a universal source of truth may combine patient recollections with provider sourced data. Even aligning data representing the same real-world concept is a challenge, and though ontologies such as SNOMED, RxNorm, and the ICD standard are intended as solutions, coding standards change over time, and differ across facilities and providers [16-18].\n\nFor all of these reasons, getting data out of records and into a structured form suitable for analysis \u2013 in our case, modeled after the OMOP Common Data Model - requires more than just digitization for paper records or parsing for electronic records. Our structuring tasks implement the following steps (Figure 2):\n\n\u2022 OCR/CDA Parsing - Accurate text is critical to all down-stream processing. For paper records, we apply an OCR model trained on 6.2B words and bounding boxes, captur-ing the layouts, styles, artifacts, and language common in our records. For electronic records, we parse data in the Clinical Document Architecture (CDA) format to access text directly [19].\n\n\u2022 Document Labeling - We tag all records with document-level metadata. This includes document boundaries, e.g. since facilities often consolidate many visits into a single paper record, as well as attributes like visit length and type, provider identity and specialty, and facility names.\n\n\u2022 Sectioning - We subdivide documents into sections, which capture data provenance. This step includes finding sec-tion boundaries based on arbitrary contents and, for paper records, physical layouts. We code types such as History of Present Illness, Chief Complaint, Medication List, etc.\n\n\u2022 Mentions-Processing - Similar to named entity recog-nition (NER) [20], we identify clinical concepts, their at-tributes, and the relationships that link them. We do this for medications, lab tests, vital signs, procedures, and con-ditions. Example attributes include reference ranges for labs, doses for medications, etc. We then align the resulting entity mentions to ontologies appropriate for their domain."}, {"title": "3.2.2 Abstraction Tasks", "content": "While structured data reflects what is written, our abstracted data represents the clinical view of a patient's medical history. For example, a structuring task will recognize and normalize every mention of a drug, but those named-entities alone are often not enough to confidently say when a patient started the drug, when they stopped, and why [21]. This is apparent in Figure 3, which shows snippets used to abstract the treatment course for a medication. We see that the most recent note captures a discussion of bladder control difficulties alongside two medications. It definitively notes the date that the drug was stopped, which is confirmed by the prior note. However, only the oldest note calls out worsening side effects to give us the stop reason, while leaving unclear the date of final administration. In this example, all three notes must be examined together to model the treatment course - no one note nor its structured data tells the full story - and this need motivates abstraction.\n\nNew abstraction tasks are defined by configuring the desired output, the input source material, and a protocol to follow. The output is a target concept, e.g. a drug code, and one of three data types:\n\n\u2022 Distinct variables, such as a primary diagnosis.\n\n\u2022 Multi-occurrence variables, such as episodic pain crises or clinical relapses.\n\n\u2022 Era variables that capture spans of time associated with a clinical status, such as when a patient was on a medication.\n\nThe inputs define what context from the patient's full set of records to consider when completing the task. Inputs are configured based on document and visit metadata, such as document type, date, and provider specialty, as well as search hits for concepts related to the output. The protocol consists of definitions, guidelines, and exam-ples to mold clinical expertise into a rigorous, repeatable process. They are designed collaboratively by clinicians and researchers, and can include multiple rounds of training with abstractors, as-sessment, feedback, and revision.\n\nIn our prior example, drug era abstraction would be instanti-ated with the drug's associated RxNorm code and an era datatype; source material would be configured to look to Progress Note sec-tions from neurology documents, as well as search hits for any drug associated with Multiple Sclerosis (MS); and the protocol would pro-vide guidelines for navigating ambiguities, such as contradictions between primary care and specialist providers.\n\nOur approach to designing abstraction tasks started from the ob-servation that clinicians intuitively abstract medical information as they read records. Through user study, we discovered that the key to abstracting a nuanced treatment course lay in supplementing structured data with a provider's clinical knowledge and prove-nance information to contextualize and filter what was written. In this way, abstraction tasks enable us to train an LLM to mimic clinicians: when a new task is launched to abstractors at scale their outputs become the labels for training, the configuration of input source material is the basis for LLM context generation, and the abstraction protocols provided to abstractors become the starting point for task prompts."}, {"title": "3.2.3 Fine-Tuning Dataset", "content": "Table 2 further categorizes the 86 task types used to fine tune LLMD today. Figure 4 provides an example prompt for drug structuring, while Figure 5 provides an example for drug era abstraction. Each task is paired with labels collected from our corpus.\n\nMany tasks look longitudinally across several records. On aver-age, patient data spans 10 years of care, and 20 years at the 90th"}, {"title": "4 SAFETY AND QUALITY CHECKING", "content": "Both LLMD's outputs and abstractor labels pass through several layers of validation before they are shown to users or included in a research dataset. This section summarizes them."}, {"title": "4.1 Uncertainty-Driven Manual Review", "content": "A primary use of LLMD is to automate record processing, while maintaining the same accuracy as clinicians performing the same task. Even for clinicians, we observe that some records are far more difficult to understand than others, often for complex reasons. For example, a poor quality scan of a decades-old handwritten note likely induces more mistakes due to confusion about the text than the output of a modern, widely used EHR system. At the same time, we see that modern EHRs produce large amounts of redundant information, spreading the most important data sparsely over many pages.\n\nFor these reasons, we train secondary uncertainty models to clas-sify when the outputs of LLMD should be routed for additional manual review. Today, these models are analytic classifiers, not LLMs, and take into account features such as LLMD's logits, its outputs, information about input text such as OCR confidence, and document metadata. They are trained to detect when outputs fall short of gold standard labels. We audit the decisions of these un-certainty models continually in production by randomly selecting tasks for review by abstractors - the resulting dataset can then compared to the uncertainty-model's prediction. Should the quality of routing decisions slip, we are able to reprocess affected data, while retraining or recalibrating the uncertainty model. We remark that these models are highly accurate, though their implementation is not discussed in detail in this paper.\n\nAbstraction tasks for research study datasets are not processed in a fully automated fashion today and are always routed for manual review. This is because they are often intended for use cases that require human verification to meet regulatory standards. For this subset of tasks, LLMD's abstraction outputs are instead treated as hypotheses that can speed abstractors' work. For these tasks, we track LLMD's impact on abstractor task-time for a fixed accuracy bar, though we do not report on abstractor efficiency in this paper."}, {"title": "4.2 Rules-Based QC", "content": "All LLMD and abstractor outputs are subjected to rules-based qual-ity control (QC) for data conformance and plausibility. Conformance checks look to ensure basic correctness, e.g. that dates are valid, codes are present in an ontology, and attributes that cannot be null are indeed populated. Plausibility rules incorporate more clinical and disease-specific knowledge, for example that a drug does not start before a confirmed diagnosis date when appropriate, or that conditions only possible in females are not associated with male patients. When a rule violation is detected, it is logged with a 'warn' or 'error' priority level. Errors are prevented at point of entry, while warnings are routed to an escalation workflow for manual correc-tion or suppression. Both general and disease-specific QC rules are created and continually expanded by a team of epidimiologists, clinicians, and biostatisticians. An example set of plausibility rules in production today is shown in Table 3."}, {"title": "4.3 Agreement and Accuracy", "content": "Labels assigned or verified by human abstractors are subject to additional quality checking (QC) tasks that ensure consistent per-formance over time and among abstractors [24]. For abstraction task types, a second blinded task is performed based on a configurable sampling rate. In cases of disagreement, the result is adjudicated by a third abstractor. We also perform a smaller number of random audits by clinicians with a higher level of expertise than abstractors to ensure consistent results are indeed correct. For structuring tasks, which involve smaller units of work and less clinical judgment, QC tasks are not blinded and are performed by team members identi-fied to be high performers. All QC sampling rates are configurable by percent of data volume, by concept, by abstractor performance level, and by study in the case of research datasets."}, {"title": "5 LLMD TRAINING & EVALUATION", "content": "This section presents results from a small version of our LLM built from Meta's Llama3.1-8B foundational model. We continue pre-training it with one pass over the 28B token dataset in Table 1, and then perform instruction fine-tuning using a single pass over ap-proximately 8B tokens representing the task mix in Table 2. During fine-tuning, we regularize using loss smoothing [25], and linearly ramp loss from 0 to 2.0e-5 over 500 steps before linearly decaying back to 0. We evaluate the performance of LLMD-8B on medical benchmarks and production tasks, comparing it to the best general and domain-adapted models available."}, {"title": "5.1 Common Benchmarks", "content": "To analyze accuracy on medical benchmarks, we consider both log-probability scoring and text-response scoring. Given a prompt question as input, log-probability scoring ranks several possible answers in terms of the probability of emitting them as the next set of tokens; if the correct answer has highest probability rela-tive to other options, the model is deemed correct. This approach is most commonly reported due to the ease of comparing LLMs with different, ostensibly superficial, response styles, as well as its compatibility with legacy classification models [26]. In contrast, text-response scoring simply feeds a question into an LLM and scores its output for correctness. This method is more difficult to re-port because it requires bespoke system-prompt tuning and output parsing when evaluating multiple models.\n\nThese two methods reveal different things about LLMs. Log-probability scoring probes how well a model learns the relationships between domain concepts, while factoring out sensitivity to input and output perturbations. Text response scoring provides a more direct assessment of how an LLM might perform in production when inputs and outputs are unconstrained. A consistent relation-ship between the two is not guaranteed and Figure 6 confirms this: similar to recent work [26], we find that LLMs with the highest benchmark scores are far less accurate when giving text responses than their log-probabilities would suggest. In fact, no 8B parameter model met the 60% bar in its text responses to MedQA colloquially associated with passing the US medical licensing exams, despite log-probability scoring showing that several encode the knowledge to do so. We note that LLMD is trained based on the quality of its text responses, minimizing the gap between scoring methods.\n\nFocusing in on the quality of text responses, Figure 7 shows that LLMD-8B achieves state of the art responses on PubMedQA over all models, regardless of domain specialization or parameter count. This result confirms the power of continued pretraining and suggests that records themselves have content useful for improving benchmark performance. These may include examples of medical facts made manifest in patient assessments and test results, or practical explanations of knowledge in the notes of providers.\n\nWe also notice two important behaviors in the text response scores across models. First, we find that good performance transfers less-effectively among benchmarks when scoring text responses than probability scoring has previously suggested [27]. In fact, several models slide below the accuracy of the Llama3.1-8B-Instruct base model despite strong performance on one or two benchmarks. Second, we see that general models with large parameter counts routinely outperform domain models: on the MedQA benchmark, llama3.1 Instruct has the best performance among 8B parameter models, while GPT-40 bests MedLM, the most advanced extension of the med-palm2 family.\n\nThese results support our experience that performance on medi-cal knowledge benchmarks rarely determines the effectiveness of an LLM when working with records in production. Even on questions probing the same domain that have been curated of phrased differ-ently, the tolerance to variations inherent in large production-grade general models is more important than medical knowledge. In the next section, when we evaluate performance on medical records with a much higher degree of noise and variation, this effect is even more pronounced."}, {"title": "5.2 Production Workload Accuracy", "content": "Our production tasks allow us to analyze the strengths and weak-nesses of LLMs on real-world records drawn from a broad, repre-sentative patient population. Results in this section report accuracy against gold labels assigned and checked by abstractors.\n\nThe comparison in Figures 7 and 8 breaks model performance out by structuring and abstraction, and provides an overall score reflecting the task mix in Table 2. It shows that LLMD-8B handily beats comparison models, reflecting the importance of fine-tuning on tasks with labels when analyzing records. Consistent with the results in Section 5.1, the next best performers are large general-purpose LLMs. Our data further shows that the gap between these and models focused on domain knowledge is substantial on real-world patient data.\n\nExamining individual responses, we observe that LLMD-8B lever-ages both the pretraining and fine-tuning datasets to improve ac-curacy. For example, we see structuring tasks appropriately biased towards more plausible answers. In one representative structur-ing task, the domain-specific JSL-MedLlama-3 model identifies a patient's height value correctly as \"6\" but improperly assigns the \"inches\" unit, a choice in the LLM's context window that is close \n\n_inches are a valid unit for height - but implausible given the patient age, also included as input. LLMD-8B does not make this mistake and correctly outputs \"feet\" as the unit. Overall we observe a substantially lower incidence of implausible results in the outputs of LLMD-8B, which we attribute to training datasets that capture many examples of measurements for people in various states of health, at various ages, etc.\n\nAnother class of data issue that we find LLMD-8B handles better than alternative LLMs involves the manipulation of lab test codes and medication identifiers. We observe a significantly higher rate of incorrect codes with other LLMs, both when they are transcribed from inputs or recalled from LLM knowledge. Moreover, we observe that the most powerful models like GPT-40 often produce hallu-cinated codes in plausible formats, whereas much smaller models produce non-conforming outputs that are easier to detect. This complicates quality checking, and we earmark this effect - that a little knowledge can be a dangerous thing for LLMs - for further study, while noting that multi-layered validation and consistency checking is necessary for safe deployment of today's LLMs.\n\nFigure 8 also annotates some tasks based on whether they require nuanced reasoning germane to medical records. We include tasks in this category that are interpretive in nature, such as those requiring disease-specific adjudication of conflicting information. In spot checks, we see LLMD-8B shine on these tasks, for example properly resolving the status of a medication found in a Medication List that was also listed as stopped in a Progress Note from the same day. Large LLMs like GPT-40 and Llama-3-70B also perform well given their ability to consistently latch onto plausible answers, though LLMD wins by more often finding correct answers.\n\nThe last category we analyze - tasks that require date reasoning - demonstrates how mistakes on mundane-seeming metadata can lead to poor application-level behaviors. In many failure cases, we saw comparison models confused by the meaning of dates in medical records. When looking into the records themselves, we found dates and times documenting facility workflows, such as when notes were written, amended, signed, or when test samples were sent off to a lab, returned, etc. Answering straightforward questions about medical histories at the application level requires disentan-gling this timing information. Again we see in Figure 8 that direct training on example data produces the best model, and also call out this as a case where the type of medical knowledge reflected in com-mon benchmarks is little help getting basic, fundamental questions about a patient right."}, {"title": "5.3 Long Tail Performance", "content": "Finally, we report LLMD's performance on structuring two specific sets of labs: the top-100 most-common and 100 tests deemed by our clinical team to be both rare and clinically important, which we refer to as long-tail labs. An example of this latter set are measure-ments associated with the marker panel administered to patients diagnosed with PNH (Table 4). Given the disease's incidence of less than ten per million people, the frequency of these tests is very low in most data samples, but their importance high.\n\nIn data audited over the course of April, 2024, we find precision and recall on our top 100 labs strictly above those computed from agreement studies between two abstractors performing manual abstraction. This indicates that LLMD's outputs after validation are as-good or better than a trained human abstractor. Among the set of long tail labs, we find that 60 of the 100 appeared more than 10 times in our audit sample - of these, 85% had an F1 score above 0.80, suggesting that performance in the long tail is good, but not guaranteed. In practice, when we detect this, we are able to flag sections for patients with the associated disease for manual review by abstractors, implement QC rules to ensure we find expected measurements, and ultimately retrain LLMD.\n\nWe have experimented both with upsampling and data augmen-tation to shore up long tail concepts, and for both methods find that LLMD responds smoothly. We find these dynamics supportive of our claim that a large labeled dataset is absolutely critical to good performance: precision and recall on these obscure concepts are not a given, but we do see that LLMs are well-behaved enough that model blindspots are discoverable and addressable. Our results also highlight how important the input of clinicians is, and suggests that disease-by-disease rollout is likely to produce incremental generalization for medical LLMs."}, {"title": "6 CONCLUSION", "content": "This paper presented LLMD, an LLM capable of analyzing patient health from data available today. Central to LLMD's success is the finding is that for medical LLMs, training on real-world data is necessary: even the most knowledgeable models struggle when working with medical records, and dealing effectively with messy, idiosyncratic data is the limiting factor when building medical LLMs for the real-world.\n\nBeyond top-line accuracy, real-world medical LLMs must per-form well on data that is important and potentially under repre-sented in training datasets. We find ample evidence that guard rail and validation system design is critical for even the most power-ful LLMs known. We also find that approaches for assessing and improving performance on long-tail data are a critical issue. For LLMD, we address this through disease specific analysis and sys-tems that help automate feedback from clinicians. Above all, for future medical LLMs to consistently progress, there is a need for more representative training and benchmarking datasets.\n\nBut, while these problems are difficult, they are tractable and the results compelling. We showed LLMD can operate at human-level accuracy and be used to improve patient care today. User feedback has demonstrated patients discovering new things about their health history, advocating for the highest standards of care for themselves, and making better use of precious time with their doctors. Researchers are working the same underlying data, con-tributed by willing patients who are highly motivated to improve treatment options for themselves and others. To date, this has pro-duced 60+ datasets covering 50+ rare diseases, and has been the basis for compelling evidence submitted to the FDA."}]}