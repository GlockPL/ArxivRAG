{"title": "Enhancing Reverse Engineering: Investigating and Benchmarking Large Language Models for Vulnerability Analysis in Decompiled Binaries", "authors": ["Dylan Manuel", "Nafis Tanveer Islam", "Joseph Khoury", "Ana Nunez", "Elias Bou-Harb", "Peyman Najafirad"], "abstract": "Security experts reverse engineer (decompile) binary code to identify critical security vulnerabilities. The limited access to source code in vital systems \u2013 such as firmware, drivers, and proprietary software used in Critical Infrastructures (CI) makes this analysis even more crucial on the binary level. Even with available source code, a semantic gap persists after compilation between the source and the binary code executed by the processor. This gap may hinder the detection of vulnerabilities in source code. That being said, current research on Large Language Models (LLMs) overlooks the significance of decompiled binaries in this area by focusing solely on source code. In this work, we are the first to empirically uncover the substantial semantic limitations of state-of-the-art LLMs when it comes to analyzing vulnerabilities in decompiled binaries, largely due to the absence of relevant datasets. To bridge the gap, we introduce DeBin-Vul, a novel decompiled binary code vulnerability dataset. Our dataset is multi-architecture and multi-optimization, focusing on C/C++ due to their wide usage in CI and association with numerous vulnerabilities. Specifically, we curate 150,872 samples of vulnerable and non-vulnerable decompiled binary code for the task of (i) identifying; (ii) classifying; (iii) describing vulnerabilities; and (iv) recovering function names in the domain of decompiled binaries. Subsequently, we fine-tune state-of-the-art LLMs using DeBin-Vul and report on a performance increase of 19%, 24%, and 21% in the capabilities of CodeLlama, Llama3, and CodeGen2 respectively, in detecting binary code vulnerabilities. Additionally, using DeBinVul, we report a high performance of 80-90% on the vulnerability classification task. Furthermore, we report improved performance in function name recovery and vulnerability description tasks. All our artifacts are available at \u00b9.", "sections": [{"title": "Introduction", "content": "It is crucial to perform vulnerability analysis in software that plays a vital role in shaping Critical Infrastructure (CI) sectors such as water, energy, communications, and defense, to name a few. Despite the many advancement in software security, the reported number of Common Vulnerabilities and Exposures (CVEs) has been increasing annually, from 14,249 in 2022, to 17,114 in 2023, and surging to 22,254 in 2024 (Qualys 2024). These CVEs are correlated with the Common Weakness Enumeration (CWE) categories maintained by MITRE, which provide a baseline for identifying, mitigating, and preventing security weaknesses during source code development. Notably, during the compilation optimization, the source code transitions into binary code, resulting in mismatches and changes in code properties (Eschweiler et al. 2016). This inherently creates a vulnerability semantic discrepancy not addressed by CWEs or other vulnerability categorization systems. As such, vulnerability analysis of source code and binary code remains two distinct and separate areas of research (Mantovani et al. 2022). This phenomenon is succinctly captured by the statement, \u201cWhat you see is not what you execute\u201d (Balakrishnan and Reps 2010)."}, {"title": "Why Decompiled Binary Code Vulnerability Analysis?", "content": "Significance & Technical Challenges. Binary code (i.e., binaries/executables) is a fundamental component of computing and digital systems taking the form of firmware, drivers/agents, and closed-source software. To safeguard these systems, reverse engineers attempt to uncover source code from binary code using decompilation tools such as Ghidra, angr, and IDA Pro, subsequently performing essential vulnerability analysis on decompiled binary code (Burk et al. 2022). This is particularly important for two main reasons; first, access to source code is most of the time limited/restricted for proprietary or security reasons; second, vulnerabilities may not be apparent in the source code, such as those related to the execution environment, operating system, specific compiler optimizations, and hardware specifications. For instance, use-after-free (memory corruption) vulnerabilities, which affect many closed-source system components and network protocols written in C/C++, are known to be one of the most difficult types to identify using source code static analysis. (Lee et al. 2015; Nguyen et al. 2020). On a different note, due to the NP-complete nature of the compiler optimization problem (Eschweiler et al. 2016), decompiled binary code loses important constructs, such as structured control flow, complex data structures, variable names, and function signatures (Burk et al. 2022). As a consequence, these setbacks impede the ability of reverse engineers to analyze vulnerability in binary code, necessitating significant manual effort and time investment."}, {"title": "Avant-garde Advancements and Perceived Opportunities", "content": "More recently, state-of-the-art Large Language Models (LLMs) have been employed as an optimizer to improve the readability and simplicity of decompilers' output, ultimately reducing the cognitive burden of understanding decompiled binary code for reverse engineers (Hu, Liang, and Chen 2024). Similarly, a cross-modal knowledge prober coupled with LLMs have been utilized to effectively lift the semantic gap between source and binary code (Su et al. 2024). Furthermore, comprehensive benchmarking was conducted on ChatGPT/GPT-4 and other LLMs to evaluate their effectiveness in summarizing the semantics of binary code (Jin et al. 2023). This assessment revealed the transformative capabilities of LLMs in the field while also highlighting key findings on their limitations, which demands further research. While these efforts aim to improve the readability and comprehension of decompiled binary code semantics, they overlook the vulnerability semantic gap between source code and deccompiled binary. To date, no comprehensive research has been conducted to thoroughly investigate and explore the potential of LLMs in decompiled binary code vulnerability analysis. This task remains far from straightforward due to the following two main limitations; (i) lack of real-world decompiled binary code vulnerability datasets; and (ii) vulnerability semantic gap between source and decompiled binary code in LLMs. Currently state-of-the-art LLMs are trained on textual-like input, including source code, but they lack semantic knowledge of vulnerabilities in the decompiled binary code domain due to the absence of representative datasets. Through an empirical and pragmatic investigation of the analytical abilities of LLMs, we find a consistent low performance of 67%, 54%, and 33% in decompiled binary code, compared to a slightly higher performance of 75%, 68%, and 45% in source code with GPT4, Gemini, and LLaMa 3, respectively. Table 1 highlights some of the insights we derived from our investigation. More information on the investigation is provided in the Appendix and Table 8 in Section Source & Decompiled Binary Code Vulnerability Semantic Gap: Investigating LLMs' Analytical Abilities. To this end, significant manual effort is required to curate decompiled binary code samples that include relevant vulnerabilities, realistic compilation and decompilation settings, and representative input formats for LLMs. Moreover, this entails of state-of-the-art LLMS through extensive fine-tuning and instructive/prompting techniques."}, {"title": "Our Contribution", "content": "To tackle these challenges and capitalize on the perceived opportunities, this work aims to ask:\nCan we enhance reverse engineering by bridging the semantic gap between source and decompiled binary code vulnerability analysis in state-of-the-art LLMs?\nTo answer this question, we undertake the following quests.\nFirstly, we empirically investigate the analytical abilities of state-of-the-art LLMs and uncover a vulnerability semantic gap between source and decompiled binary code. Our investigation encompasses real-world code injection in public repositories, simulating an emergent cybersecurity attack that targets the widely recognized Linux-based XZ Utils (Akamai 2023). Secondly, we introduce DeBinVul a novel decompiled binary vulnerability dataset with zero-shot prompt engineering. Our dataset comprises relevant non-vulnerable and vulnerable source code samples, tagged with CWE classes, and compiled using Clang and GCC across four different CPU architectures: x86, x64, ARM, and MIPS. During compilation, we applied two levels of optimizations; 00 and 03. Then, using GHIDRA we decompile the compiled code to obtain the decompiled binary code samples. Furthermore, we augment our dataset with code descriptions and instruction/prompting techniques. Thirdly, we fine tune and optimize state-of-the-art LLMs, aiming to enhance their capabilities in assisting reverse engineers in uncovering vulnerabilities in decompiled binary code. In summary, the contributions of this paper are as follows:\n\u2022 To the best of our knowledge, we are the first to empirically investigate the vulnerability semantic gap between source and decompiled binary code in state-of-the-art LLMs. Our findings highlight the suboptimal performance of these models in performing vulnerability analysis on decompiled binaries.\n\u2022 We compile and release, DeBinVul, a novel decompiled binary code vulnerability dataset comprising 150,872 samples of openly sourced, synthetically generated, and manually crafted corner case C/C++ code samples. It is designed to tackle four important binary code vulnerability analysis tasks, including vulnerability detection, classification, description, and function name recovery.\n\u2022 We employ our proposed dataset to fine-tune and enhance the reverse engineering capabilities across a range of state-of-the-art LLMs. Our results shows a performance increase of 19%, 24%, and 21% in the capabilities of CodeLlama, Llama 3, and CodeGen2 respectively, in detecting vulnerabilities in binary code."}, {"title": "Proposed Approach", "content": "In order to mitigate the challenges faced by LLMs in understanding decompiled binaries and improve their performance in understanding their security impact, we propose an extensive dataset comprised of source code and their decompiled binaries. Further details are provided in the sequel.\nStep 1: Data Collection\nWe compiled our dataset from three distinct sources: the National Vulnerability Database (NVD), the Software Assurance Reference Dataset (SARD), and a collection of real-world code enhanced with synthetically added vulnerabilities to cover corner cases where real-world code is not available for certain vulnerabilities for proprietary and security reasons.\nNVD. NVD provides real-world source code vulnerabilities which were reported by developers and security experts from various repositories. But these are often individual functions, making it difficult to compile them into executables and decompile them due to unclear documentation and library dependencies. Therefore, during collection, we had to skip the ones which were not compilable.\nAdditionally, the NVD often lacks coverage of preparatory vulnerabilities, such as those involving specific configurations or security mechanism bypasses, which don't lead directly to exploits but can set the stage for more severe issues. While the information on the vulnerabilities is exposed, the source code is not exposed to NVD since it may contain sensitive information like code or data structure of the system, file, or operating system. These preparatory or indirect vulnerabilities are often not published in the NVD, as they require specific, often complex conditions to manifest in a real-world exploit.\nSARD. SARD is a valuable resource for the software security community. It's a curated collection of programs containing deliberate vulnerabilities. Researchers and tool developers use SARD to benchmark their security analysis tools, identifying strengths and weaknesses. By exposing programs to a wide range of known vulnerabilities. SARD, while providing code examples with known vulnerabilities and all the code samples are executable, it often lacks real-world complexity and diversity.\nVulnerability Injection. Furthermore, we proposed an innovative automatic injection process to inject vulnerabilities in the source of real-world repositories to emulate this scenario. After getting the repositories, we injected vulnerabilities into randomly selected functions from the repositories by prompt engineering using LLMs. We selected 8 of the top 25 CWE vulnerabilities from MITRE that are common in C/C++ programs. Out of the initial 500 randomly selected code samples, we injected vulnerabilities into 462, of which 38 were not compilable and were subsequently ignored. We provide the details of the repository selection and vulnerability injection process in the Appendix (Vulnerability Injection Process).\nCombining the SARD and the NVD for training LLMs can significantly enhance their capabilities in vulnerability analysis. While these two datasets offer either fully synthetic or fully real vulnerabilities, our method of injecting vulnerabilities tries to overcome the issues we see in NVD. Together, these datasets allow the LLM to generalize from structured, annotated examples to broader, complex, real-world scenarios, resulting in a more versatile model that can analyze decompiled binaries across various software contexts.\nHence, we opt to construct a dataset DeBin Vul by extending the capabilities of MVD to analyze decompiled binaries with instructions to align with LLMs.\nStep 2: Data Processing and Increase\nFunction Extraction. We developed a methodology using the Tree-Sitter parser to extract the functions from a file. Since we are extracting C/C++ functions, we use C or C++ grammar for function extraction. If the file name suffix is \".c,\" then a C grammar is used; otherwise, if the suffix is \".cpp,\" a C++ grammar is used. The available functions from SARD have some special signatures in the function declaration which helps us to determine whether the function is vulnerable or not. For example, if the function name contains the term \"good\" or \"bad\", we consider them non-vulnerable or vulnerable functions. Furthermore, if the function if vulnerable, the function name also contains the CWE number as well. We utilize this information to annotate the vulnerable and the non-vulnerable functions and find the CWE numbers of the vulnerable function using regular expression. The code from NVD and our injection technique are functions. Therefore, they do not need to be extracted.\nCompilation and Optimization. After we have extracted the vulnerable and non-vulnerable functions, we locate the necessary header files and other source code files needed to compile the CWE file. These files are conveniently located in the same directory as the CWE file. Each source code function was compiled six times to ensure comprehensive analysis, resulting in six binaries of a single function. This process involved using two compilers, two optimization levels, and four architectures.\nDecompilation. We used Ghidra (NSA 2019) to decompile the functions. Decompilation can usually be done two ways, stripping and non-stripping the function or variable names. In real-world applications, the functions are variable names are stripped due to security reasons. Therefore, we emulate the same process by stripping function and variable names during compilation using a special flag -s during compilation.\nDescription. The functions we extracted mainly contain comments written by software security experts. However, these comments are partial and explain only a particular statement. However, there are multiple levels of comments for some important vulnerable-prone lines. Therefore, we use tree-sitter to create a method to define comments in C/C++. Then, we use the definition of the method to extract the comments inside these functions. Finally, we use the source code without the comments and the extracted comments and prompt GPT-4 to write a clean, comprehensive description of the code using a prompt. Furthermore, we also want to ensure that we use the same description when we use decompiled functions. Therefore, ensure that function and variable names are not present when describing the function objectives and vulnerabilities."}, {"title": "Instructions", "content": "We provide an instruction-based dataset, enabling the user or developer to use our system by providing instructions with code. Therefore, we created four types of robust instructions.\nWe created four carefully curated prompts to instruct GPT-4 to create 20 instructions for each task; therefore, we have 80 instructions. Moreover, we provided 2 sample examples with each prompt that would guide GPT-4 to generate the most appropriate comment. These instructions are manually appended during training and testing with the input code based on the desired task. shows the prompts we used to generate 20 instructions for each task. The prompts generated by the instructions are available in our repository. We provide more details on our data preparation in Section DeBinVul Dataset Preparation in Appendix."}, {"title": "Fine Tuning Process", "content": "Tokenization of Decompiled Code. We use a byte-pair encoding (BPE) tokenizer, common in natural language and programming language processing, to efficiently manage large vocabularies by merging frequent byte or character pairs into tokens. This approach reduces vocabulary size while preserving common patterns, balancing granularity and efficiency for handling diverse language data. From each function f, we extract a set of tokens T, trimming the input size to 512 tokens. We also add special tokens < BOS > and < EOS > at the start and end of the program, respectively, and pad sequences shorter than 32000 tokens with < PAD >. The tokenized decompiled code is then used as input for the model.\nModel Training and Optimization. In this work, we explore the application of generative language models to four tasks: i) Vulnerability identification, ii) Vulnerability classification, iii) Function name prediction, and iv) Description generation. Although the first two tasks are typically classification tasks (binary and multiclass, respectively), we convert all four tasks into generative ones by leveraging our model's instruction-following capability. Specifically, the model outputs \"Yes/No\" for vulnerability identification, generates a \u201cCWE-XXX\u201d code for classification, predicts a single token for the function name, and produces multiple tokens for description generation, enabling a unified multitask approach."}, {"title": "Evaluation", "content": "In this section, we evaluate the effectiveness of our proposed dataset DeBin Vul by benchmarking them on state-of-the-art LLMs and comparing their performance on the test set before and after fine-tuning. We evaluate our proposed system to answer the following Research Questions (RQs):\nRQ1: Using our instruction-based dataset DeBin Vul, how effectively can it be used to identify and classify binaries using different LLMs?\nRQ2: How do the trained models with our dataset perform in function name prediction and description?\nRQ3: Are the current LLMs generalized enough to analyze vulnerabilities in different architectures and optimiza-"}, {"title": "Answering Research Question 1", "content": "In answering RQ1, we investigate the effectiveness of the impact of the proposed dataset in analyzing binary code for four tasks, namely i) Vulnerability Identification, ii) Vulnerability Classification, iii) Function Name Prediction, and iv) Description of Code Objective. Throughout answering all our RQs for vulnerability identification and classification, we use Accuracy, Precision, Recall, and F1 scores. We use BLEU, Rouge-L, BERTScore, and Cosine Similarity for function name prediction and description generation. To answer RQ1, we only used O0 optimization on x86 architecture. shows the baseline comparison of the identification task on binary code. The Training column with value Base implies the results were before training the model, and Our DS denotes after the LLM was fine-tuned with our dataset. Overall all the LLMs, when trained with our proposed dataset, show an improvement of F1 score of 18% or higher. While we see that without training, CodeGen2 and StarCoder outperform by 59% in identifying vulnerability. However, since this is a binary task, it is very close to a randomized guess, which is approximately 50%. Moreover, if we see the individual accuracy only on vulnerable and only on non-vulnerable or benign code, we can see that some models like CodeGen2 , StarCoder , and CodeLLaMa have significantly lower accuracy (Starcoder: 70% lower) in identifying the non-vulnerable or benign functions while maintaining a higher accuracy in identifying the vulnerable functions. This phenomenon concludes that these models prefer to determine that most functions are vulnerable, hence the identification imbalance. However, after all the models were individually trained on our proposed dataset, we see an overall increase in the accuracy and F1 score, and CodeGen2 and LLaMa 3 top on this task with an accuracy of 91%, which is almost a 30% improvement from the baseline models. Furthermore, when we see the accuracy on only vulnerable and only benign functions, we see that, for both of the cases, the performance has remained high where CodeGen is 94% successful at finding the vulnerable functions and LLaMa 3 is 87% successful in finding the non-vulnerable or the benign functions. For classification, we show the F1 score comparison. We can see that all the base models have a classification F1 score of less that 5%, and interestingly, while CodeGen2 is a code-based LLM, it shows a score of 0 (zero) for vulnerability classification. compares all the CWEs in different models more in-depth. We provide more details on the classification task in Subsection Further Discussion on RQ1 in Appendix."}, {"title": "Answering Research Question 2", "content": "Our aim in answering RQ2 is to analyze one of the top-performing models to understand the performance of different architectures. Hence, we selected CodeLLaMa for this task to analyze the vulnerability of decompiled code. Here, we again train the based models on the same four tasks we performed in RQ1. However, RQ2 differs from RQ1, using a multi-architecture compilation of source code into decompiled code. For identification in Figure 2, we see that the performance is close to approximately 90% when we test by combining all the architectures. However, we see an improvement in Precision, F1, and accuracy in non-vulnerable or benign functions for MIPS. Moreover, we see a significant performance drop of 2-3% overall metrics for x64 architecture, wherein the performance of x86, ARM, and MIPS remains relatively similar. Similarly, we see mixed results on F1 score for multiclass classification of CWEs in Figure 3. For example, on CWe-121, CWE-122, CWE-427, CWE-665, CWE-758, CWE-758 and, CWE-789 MIPS performs the higher. However, for CWE-401, CWE-690, and CWE-761, we see a relatively stable performance across all architectures. An interesting observation from Figure 3 is that, for CWE-666, the F1 score goes down to zero, which implies a limitation of our dataset on CWE-666. If we follow the trend line of the moving average for \"All Architecture\" we see that, overall, the model performs lower for CWE-126, CWE-617, CWE-666, and CWE-78 while maintaining good performance on the other CWEs."}, {"title": "Answering Research Question 3", "content": "Our goal in answering RQ3 is to demonstrate how well our CodeLLaMa performs when trained on a subset of architectures and tested on a different subset of architectures for function name prediction and description generation tasks. we define the set of architectures that were present during the training and the \u201cTes\u201d column defines the set of architectures that were present during the testing. however, we kept some overlap in the architectures between the training and testing for comparison tasks. All - x64 defines that the model trained will all three architectures except x64, and ARM + x86 defines that the model was only trained on ARM and x86 architecture. For function name prediction, we can see that when the model was trained without x64, we see a very slight performance drop of only 1% on the Cosine Similarity score when tested on x64. However, when the model was trained on ARM and x86, we see that for x86, there was a 4% drop in the performance compared to ARM, while x86 was still in the training data. Furthermore, for description, when the model was trained with All-x64, the performance of x64 only dropped by 2% for the Cosine Similarity score, and when the model was trained on ARM + x86, and tested with \"Al\" we see almost no performance change. Furthermore, we also tested generalizability on O0 and O3 optimization levels on function name prediction and description tasks. For both tasks, the model was trained on O0 optimization and tested on the 03 optimization level. We see a mere 1% improvement when the model was trained and tested on the same optimization level. From this analysis, we can safely conclude that using different architectures has almost little to no effect on function name prediction and description generation tasks.\nTo evaluate the generalizability of Large Language Models (LLMs) in real-world scenarios, we conducted a small-scale experiment using a generalized instruction-based dataset. Specifically, we tested Mistral and LLAMA 3 on the Stanford Alpaca dataset , performing inference on the base models prior to training with our dataset. Initial cosine similarity scores were 0.67 for Mistral and 0.73 for LLaMA 3. After training the models on our proposed dataset, we reassessed performance on the Stanford Alpaca dataset, observing that cosine similarity scores for Mistral and LLaMA 3 dropped to 0.56 and 0.70, respectively. The notable decrease in Mistral's performance is likely due to its smaller model size (2B parameters), which led to catastrophic forgetting when trained on new data, whereas the 7B-parameter LLaMA 3 retained much of its prior learning. Additionally, we conducted an N-day vulnerability analysis, where LLaMA 3 and Mistral identified 15 and 6 N-day vulnerabilities, respectively."}, {"title": "Related Work", "content": "Recent advances in binary vulnerability detection have focused on leveraging intermediate representations and deep learning techniques to address the challenges posed by code reuse. VulHawk employed an intermediate representation-based approach using RoBERTa to embed binary code and applied a progressive search strategy for identifying vulnerabilities in similar binaries. Asteria-Pro utilized LSTM for large-scale binary similarity detection, while VulANalyzeR proposed an attention-based method with Graph Convolution and Control Flow Graphs to classify vulnerabilities and identify root causes. QueryX took a different approach by converting binaries into static source code through symbolic analysis and decompilation to detect bugs in commercial Windows kernels. In the realm of code summarization for decompiled binaries, Kawsan et al. fine-tuned the CodeT5 model on decompiled function-summary pairs, while HexT5 extended CodeT5 for tasks like code summarization and variable recovery. BinSum introduced a binary code summarization dataset and evaluated LLMs such as GPT-4, Llama-2 , and Code-LlaMa across various optimization levels and architectures. Additionally, Asm2Seq focused on generating textual summaries of assembly functions for vulnerability analysis, specifically targeting x86 assembly instructions."}, {"title": "Conclusion", "content": "In this study, we present a comprehensive investigation of large language models (LLMs) for the classification and identification of vulnerabilities in decompiled code and source code to determine the semantic gap. The primary contribution of our work is the development of the De-BinVul dataset, an extensive instruction-based resource tailored for vulnerability identification, classification, function name prediction, and description across four architectures and two optimization levels. Our experiments demonstrate that DeBinVul significantly improves vulnerability identification and classification by up to 30% compared to baseline models on the x86 architecture. Additionally, we provide an in-depth analysis of how different LLMs perform across various computer architectures for all four tasks. We also evaluated how our proposed dataset aids LLMs in generalizing across different architectures and optimization levels. Our results indicate that the LLMs maintained consistent performance even when exposed to new architectures or optimization methods not included in the training data."}, {"title": "Appendix", "content": "In this appendix, we explain in detail the investigation injection process on different repositories and dataset collection methodologies.\nSource & Decompiled Binary Code Vulnerability Semantic Gap: Investigating LLMs' Analytical Abilities\nIn this section, we are the first to empirically and pragmatically investigate the analytical abilities of state-of-the-art LLMs in analyzing vulnerabilities in the decompiled binary code domain. To explore this, we randomly select 200 pairs of vulnerable and non-vulnerable source code and decompiled binary code samples from our proposed dataset, De-BinVul for the task of classifying vulnerabilities. Specifically, we evaluate the ability of several LLMs, including ChatGPT-4 , Gemini , CodeLLaMA-7B , Mistral , LLaMa 3 , and CodeGen2 , to identify and classify vulnerabilities in both source code and decompiled binary code. Table 8 presents the comparison results and underscores the semantic vulnerability limitations of state-of-the-art black-box LLMs in classifying CWEs in decompiled binary code, in contrast to source code, which presented moderately better results. The reported results focuses on CWES: 787,416,20, 125, 476, 190, 119, 798 and reports their average F1-scores.\nA carefully designed prompt was used to leverage the generative capabilities of these LLMs, asking them to respond with a \"Yes\" or \"No\u201d regarding the presence of vulnerabilities. Additionally, the prompt required the LLMs to generate the corresponding CWE number if a vulnerability was detected. To classify the vulnerabilities, the prompt was adjusted to ensure that the LLMs only output the CWE category. The results from these LLMs are summarized and analyzed in Table 8. The specific prompts used in this analysis are provided in Appendix Prompts and Investigation.\nResult Analysis The results from  show that API-based models like GPT4 could accurately identify a vulnerability in decompiled binaries with an accuracy of 70%, where Gemini is at 56%. Moreover, open models like CodeLLaMa is at 61%, Mistral is at 54%, LLaMa 3 at 50%, and CodeGen2 at 54%. Moreover, from , we see that GPT-4 performs comparatively higher overall than other API-based or open-access models. To investigate the details of the identification task, we investigate how accurately these LLMs can classify each vulnerability category. The results show that all models were experts at identifying some vulnerabilities and failing at others. For example, GPT4, Gemini, Mistral, and LLaMa 3 produce higher performance on CWE-416, CodeLLaMa on CWE-20, LLaMa 3 on CWE-190, and CodeGen2 on CWe-476. However, one interesting observation from our analysis is that while CodeLLaMa, Mistral, and CodeGen 2 are moderately successful in identifying vulnerability, these LLMs fail to predict most CWEs. Therefore, we can conclude that CodeLLaMa has a very limited understanding of the vulnerability category. Furthermore, provides more detailed numerical results, including Accuracy, Precision, Recall, and F1 score. Section Reasoning of Weak Performance in Investigation in Appendix explains the reason behind the poor performance of LLMs when analyzing decompiled binaries.\nVulnerability Injection Process\nThis section shows the different prompts we used to investigate state-of-the-art LLMs and for instruction generation tasks.  shows the three prompts we needed to generate the outcomes for vulnerability identification and classification tasks. However, we noticed some disparity when the models followed the same command. Initially, we created the prompt for GPT-4. However, when we used the same prompt for Gemini, we saw that it produced some extra outputs, which are the URLs of the CWEs. Therefore, we had to ground the behavior by updating and adding instructions with the prompt.\nThe other LLM Instruction columns refer to CodeLLaMa, Mistral, CodeGen2, and LLaMa 3. When generating the output, the model was generating the CWE description. However, this time, we were unsuccessful in grounding that behavior using extra instruction. Therefore, when processing the output generated by the model, we wrote extra code to remove the description CodeLLaMa was generating."}, {"title": "Reasoning of Weak Performance in Investigation", "content": "Reasoning on Poor Performance on LLMS\nReverse engineers face many challenges when analyzing decompiled code. Understanding the objective and semantics of decompiled code is generally more complex than understanding the semantics of source code. During the decompilation process, the variables or the flow of statements sometimes get changed for optimization. As a result, working with decompiled code for tasks such as vulnerability analysis or decompiled code summarization  is more challenging. Some of the primary reasons for poor performance could be directly related to the removal of comments during decompilation, function name changes to the memory address of the function, complex control flow, and obfuscation of variable names.\nC1: Comments do not Exist. When source code is compiled, the compiler typically ignores comments and no longer exists in the compiled binary. Therefore, comments are irrecoverable in decompiled code. Without comments, comprehending decompiled code is incredibly challenging and time-consuming as it provides limited information about its purpose and intended behavior. Therefore, the reverse engineer has to derive meaning from syntax and structure.\nC2: Ambiguous Function Calls. When source code is compiled, the compiler may optimize the code by replacing standard function calls, such as strcpy, with a custom"}, {"title": "DeBinVul Dataset Preparation", "content": "In this section, we highlight the technical details of how we extract each dataset component in detail.\nFunction Extraction. To extract all function definitions from the file, we used the S-expression of Tree-Sitter. In Tree-sitter, an S-expression (symbolic expression) is a way to represent the Abstract Syntax Tree (AST) of source code in a nested, parenthetical format. Each node in the tree is represented as a list, starting with the node type followed by its children, which can be terminal tokens or further nested lists. This format provides a clear and concise textual representation of the code's syntactic structure. To extract the functions, we used the following S-expression, (function_definition)@func \u2013 de f.\nAfter extracting the functions using S-expression, our next task is to separate vulnerable from non-vulnerable functions. We found a specific pattern that makes this task more straightforward for us. We observe that all function definitions that are extracted from the file either contain \"good\" (a benign entry) or \"bad\" (a vulnerable entry) in the function's name. For each of the extracted function definitions, we used another S-expression to extract function names from each function definition: (function_declarator(identifier)@func_name). Complete definitions of these S-expressions are available in the repository we provided earlier.\nAfter extracting the functions and function names, our next task is to classify the functions. This part is relatively straightforward. If the function name contains the substring \"good,\" we consider it a benign or non-vulnerable function. However, if the function contains the sub-string \"bad,\" we consider the function vulnerable. If the function is non-vulnerable, the function name has the format that appears as CWEXXX_rest_of_function_name. Therefore, we take the first part of the function name (CWEXXX) to capture the CWE number of the vulnerable code. shows the total number of decompiled functions we generated for each"}, {"title": "Compilation and Optimization", "content": "Our compilation process strategically employs the \u221200 and -03 optimization levels to assess the impact of compiler optimizations on the security and functionality of executables. By selecting these two extreme levels", "commands": "gcc \u2013 00(x86), gcc \u2013 03(x86), clang \u2013 00(x86), clang \u2013 03(x86), aarch64 \u2013 linux \u2013 gnu \u2013 gcc \u2013 00 (ARM), and aarch64 \u2013 linux \u2013 gnu \u2013 gcc \u2013 03 (ARM). The -DINCLUDEMAIN option was included to define the main function necessary for compiling the CWE code. We compiled the source code twice for each compiler command using the -DOMITGOOD and -DOMITBAD options to generate the vulnerable and benign executables respectively. This systematic approach ensured that we could thoroughly examine the impact of different compilers, optimization"}]}