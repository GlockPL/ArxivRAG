{"title": "What Is Wrong with My Model?\nIdentifying Systematic Problems with Semantic Data Slicing", "authors": ["Chenyang Yang", "Yining Hong", "Grace A. Lewis", "Tongshuang Wu", "Christian K\u00e4stner"], "abstract": "Machine learning models make mistakes, yet sometimes it is difficult to identify the systematic problems behind the mistakes. Practitioners engage in various activities, including error analysis, testing, auditing, and red-teaming, to form hypotheses of what can go (or has gone) wrong with their models. To validate these hypotheses, practitioners employ data slicing to identify relevant examples. However, traditional data slicing is limited by available features and programmatic slicing functions. In this work, we propose SemSlicer, a framework that supports semantic data slicing, which identifies a semantically coherent slice, without the need for existing features. SemSlicer uses Large Language Models to annotate datasets and generate slices from any user-defined slicing criteria. We show that SemSlicer generates accurate slices with low cost, allows flexible trade-offs between different design dimensions, reliably identifies under-performing data slices, and helps practitioners identify useful data slices that reflect systematic problems.", "sections": [{"title": "1 INTRODUCTION", "content": "Machine learning models exhibit undesired behaviors, yet it is often hard to identify systematic problems behind individual errors. Models have been found to perform worse on under-represented subgroups [28], exhibit unintended biases [46], and produce harmful content [55], which can cause project failures, media controversies, and even lawsuits once they are integrated into software products [17]. Academia and industry have spent significant effort to help identify these problems, through activities such as error analysis, testing, auditing, and red-teaming [15, 35, 36, 43]. All these activities (Figure 1, top) curate individual errors, but more importantly, also create concrete hypotheses about the underlying systematic problem.\nAs our running example, suppose an ML practitioner is conducting error analysis on their toxicity classifier [22]. The practitioner observed that in a few examples, the model classifies non-toxic text mentioning Muslims as toxic. From these individual errors, they hypothesize that the systematic problem behind the errors might be that \"the classifier disproportionately misclassified text related to Muslims.\" To understand whether their hypotheses hold, the practitioner needs to validate the hypotheses on more data points (Figure 1, bottom).\nTo validate their hypotheses, developers need to (a) conduct synthetic data generation [21] to create more data points or (b) identify relevant data points from existing data. Data slicing [39, 56], an example of the latter type of technique, identifies a subset of examples sharing common characteristics from existing data. Data slicing often assumes access to existing relevant features, which might not always be available for users' slicing criteria of interest. For example, it is unlikely that the input data is readily labeled with whether they are related to Muslims.\nIn practice, developers try to create additional features to augment datasets for data slicing but are limited by how they can create such features: Existing practices mostly apply programmatic slicing (see statistics in Table 1), often implemented as simple Python programs, to identify slices. For our example hypothesis (\"the classifier\nAlternatively, the hypotheses can also be formed top-down through explicit requirements analysis [e.g., 5, 59], which is less common in current ML practice."}, {"title": "2 DATA SLICING", "content": "Data slicing in ML engineering. ML engineering is data-centric. ML practitioners usually start with data curation to obtain appropriate training and evaluation data; with LLMs, practitioners may need less data but still need data to develop and test their prompts [63].\nWith curated data, ML practitioners perform data analysis, model training or prompt engineering, and model evaluation (with potential debugging) in multiple iterations. Afterwards, the model can be deployed, with further model monitoring and updates based on production data. All these activities can benefit from data slicing.\nData slicing can help model debugging, as illustrated by our running example. After developers observe model mistakes, conduct error analysis, and formulate hypotheses, data slicing helps them validate the hypotheses on additional relevant examples [10, 56]. Once the hypotheses are validated, data slices can be used for further model fixing, via data pre-processing to construct useful features for model training, targeted data augmentation, or even as guidance for prompting [63].\nData slicing is also useful for fine-grained model evaluation, where multiple behavioral aspects are systematically examined [9, 44], as also recognized in the software engineering community [4]. Rather than error-chasing as in model debugging, developers have specific upfront behavioral aspects for model evaluation. The behavioral aspects can come from the developer's intuition, but can also be elicited from deliberate requirements analysis [e.g., 5, 59]. In contrast to traditional benchmarking, where practitioners only examine model accuracy on a single static benchmark, fine-grained model evaluation can expose nuanced model strengths and weaknesses and help pinpoint areas for model improvement. In our running example of toxicity detection, the developer might want to see if the model treats certain demographic groups systematically unfairly (e.g., regarding race, gender, religion) and use data slicing to identify many concrete examples for each subgroup.\nThis naturally extends to continuous model monitoring, where developers track model performance on multiple aspects through data slicing [41]. Developers can build regression test suites from data slices [32], as well as continuously analyze new production data. From model monitoring, developers can fix degrading aspects when needed, and potentially discover new data patterns (e.g., new hateful slang) for slicing when there is a distribution shift.\nIn earlier ML engineering phases, data slicing can contribute to data curation, as the insights and data produced from debugging, evaluation, and monitoring can be used to inform which slices are under-performing or under-represented and hence guide what data to curate [5].\nFor all these activities, the key motivation for data slicing is to generalize from individual data points to the underlying systematic problems. This generalization is necessary because model evaluation looks at accuracy in distributions rather than at mistakes for individual inputs a single model mistake is not considered a bug [24]. This is in stark contrast to traditional software testing, where one single error is considered a bug that can be worth fixing. Parallels to data slicing, however, do exist in software testing: Equivalence class testing [49] divides the input space into several partitions and creates test cases for each partition, akin to how data slicing partitions datasets into slices; strong equivalence class testing explores interaction across these dimensions, which is also explored for data slicing [4, 39]. Despite the parallels, data slices are expected to come from distributions that practitioners care about, rather than from any failing inputs as in software testing. This key distinction from software testing is why ML engineering benefits from data-centric approaches like data slicing and dedicated innovations like SEMSLICER.\nStatus quo and limitations. Despite being useful for many activities, (semantic) data slicing is not well-supported. Data slicing often assumes access to a set of existing features, but relevant features are not always available for users' slicing criteria, limiting developers in practice to create only \"easy\" slices. We found strong evidence for this by analyzing the 20 most popular projects on ZenoHub [1], a platform to share model evaluation results with built-in support for slicing, where we observed that developers almost exclusively create programmatic slicing functions (e.g., string length, question type) that are easy to implement, as shown in Table 1.\nIn contrast, various activities by practitioners and researchers suggest that developers often want to conduct semantic data slicing that cannot be matched with simple patterns: For example, Naik et al. [36] identified eight different error hypotheses for natural language inference models, with half of them being hard to cover with programmatic slicing (e.g., \u201ccontradicting sentence pairs containing antonyms are hard to classify,\u201d \u201csentence pairs relying on real-world knowledge are hard to classify\u201d). Similarly, for machine"}, {"title": "3 SEMANTIC DATA SLICING", "content": "SEMSLICER is a framework for semantic data slicing, allowing users to create slicing prompts from specified slicing criteria, and use them to generate data slices. In this section, we will describe (1) the design dimensions of SEMSLICER and (2) our system design and implementation details.\n3.1 Design Dimensions\nWe design SEMSLICER considering four dimensions: slicing accuracy needed (accuracy), latency expected (latency), human effort available (human-effort), and computational resources available (compute).\n3.1.1 Slicing accuracy needed. Intuitively, we would want higher slicing accuracy, as it makes the observed slices more reliable. However, as we will demonstrate in Section 4.4, moderately accurate slices can also be useful for downstream use cases like model evaluation, as long as they can reliably detect where the model under-performs, making it possible to consider accuracy one of the four trade-off dimensions.\n3.1.2 Slicing latency expected. Our second dimension, latency, considers how fast slicing should be. Depending on the downstream use cases, users can trade off latency for other dimensions, or vice versa. For example, interactive model debugging would expect a lower latency, while systematic model evaluation or monitoring can accept a higher latency.\n3.1.3 Human effort available. Our third dimension considers how much human-effort SEMSLICER requires, which also depends on the use cases. For example, for interactive model debugging, we might want to prioritize lower latency over lower human-effort, as users can put more effort into shaping the slicing function (through prompting) but would expect faster interaction. In contrast, for model evaluation, we might prioritize human-effort over compute such that evaluation can scale to a larger number of slices.\n3.1.4 Computational resources available. Our last dimension considers the practical concerns of how much compute is available for data slicing. Low computational cost is important for scaling up SEMSLICER in practice. When resources are limited, users can accept lower accuracy to accommodate available resources, by using a smaller model or having a simpler prompt construction pipeline.\nDesign trade-off. There is no optimal design for all four dimensions: We often need to make trade-offs depending on the downstream use cases. Our system design aims to allow users to easily trade off along these dimensions, through using LLMs of different sizes and capabilities (accuracy vs. compute/latency), having a human in the prompt construction loop (accuracy/compute vs. human-effort), or having different setups of few-shot examples (accuracy vs. compute/latency). We will discuss these trade-offs in more detail in Section 3.2.\n3.2 System Design\nSystem overview. At a high level, SEMSLICER runs in two stages (as depicted in Figure 3). In the first prompt construction stage,"}, {"title": "3.2.1 Model", "content": "We design SEMSLICER such that different steps and stages can flexibly leverage different LLMs, allowing trade-offs between accuracy and compute/latency. The general rationale behind our design is that a more powerful model can produce higher-quality outputs, but would increase the compute needed, incurring higher cost (and latency), while a less powerful model would be cheaper yet less accurate. Users should have the agency to decide which model fits best for their use case, according to their specific constraints.\nIn SEMSLICER, LLMs are used in almost every step: In\u2460 instruction generation, \u2461 instruction refinement, and example synthesis, we need a model with human-like creativity and capabilities to generate and refine instructions, as well as synthesize new examples. In example labeling, we need a model with high-quality classification capabilities for small-scale labeling. In data slicing, we need a model with good-quality classification capabilities for larger-scale labeling.\nImplementation. In our evaluation, we use gpt-4-turbo-preview for steps 125 (temperature=1) and step \u2463 (temperature=0), as GPT-4 is known to be among the strongest LLMs available at the"}, {"title": "3.2.2 Instructions", "content": "We design SEMSLICER to allow more LLM-based automated prompt engineering (compute), with a flexible human-AI collaboration mechanism (human-effort) to improve instruction quality (accuracy). This is achieved through two steps: \u2460 instruction generation, to produce a good initial instruction, and \u2461 instruction refinement, to further refine the produced instruction.\nThe first step aims to produce an initial instruction from a user-provided slicing criterion. Depending on what users provide, SEMSLICER provides options for (1) template-based construction and (2) LLM-based generation:\n(1) Template: When users provide a criterion in simple phrases, SEMSLICER can use the generic template \"Is the text related to {concept}?\" to translate a criterion into an instruction. We observe this simple template works very well for many slicing tasks (cf. Section 4).\n(2) LLM-generated: When users provide more detailed descriptions, SEMSLICER can leverage an LLM, which offers strong information processing capabilities, to generate an initial instruction.\nThe produced instruction provides a good starting point, which can be further refined in the second step. This can be achieved through (1) human post-editing, (2) LLM-based refinement, or (3) a mixture of both. For LLM-based refinement, our design is inspired by Self-Refine [33], where we ask an LLM to provide suggestions and revisions on a to-be-revised instruction. This refinement step can increase accuracy at the cost of more human-effort or compute.\nImplementation. We prompt an LLM for both instruction generation and instruction refinement (Figure 4).\nWhen using the instruction in slice labeling, SEMSLICER deliberately instructs the model to only produce a label token (Figure 4), without any intermediate reasoning steps (e.g., as in chain-of-thought prompting [54]). This design is based on our observation that the intermediate steps sometimes help accuracy, yet incur high latency, which scales linearly with the number of output tokens (accuracy vs. latency)."}, {"title": "3.2.3 Few-shot examples", "content": "The design of SEMSLICER automates the construction of few-shot examples, which demonstrate how a model should label a slice. Users can make trade-offs between accuracy and compute/latency by controlling different options, including (1) whether they need few-shot examples at all, (2) how many examples they need, (3) where the examples are from, and (4) how to label the examples. The more examples they use in the slicing prompt, the higher compute cost and latency.\nAs a first step to construct few-shot examples, SEMSLICER needs to curate a set of example inputs. Assuming access to user-provided data, SEMSLICER provides different sampling strategies: (1) random sampling and (2) diversity sampling. We support diversity sampling as we observe that random sampling can miss examples from smaller slices, which biases the second stage of data slicing.\nWith the sampled example inputs, SEMSLICER next labels the examples with the generated slicing instruction. In this step, we need to bootstrap from zero-shot labeling (i.e., without any examples), as there are usually no existing labels on a user's slicing criteria, which can be arbitrary. SEMSLICER Supports different \u2463 labeling strategies, (1) student-label and (2) teacher-label, to accommodate different trade-off decisions (accuracy vs. compute). A teacher model is a stronger LLM (e.g., GPT-4), whose labels are usually more accurate but also more expensive, while a student model is a smaller LLM that is less accurate but also cheaper. Using teacher-labeled examples can effectively distill a teacher model's knowledge about some particular slicing criterion to a student model.\nIn the last step, SEMSLICER creates optional synthetic examples to balance the in-slice and out-of-slice examples, as sometimes sampled examples can be extremely imbalanced for small slices, leaving no in-slice demonstrations for data slicing. To create synthetic examples that look similar to the real dataset, SEMSLICER queries an LLM to write extra examples of the underrepresented label, given sampled examples. The given examples can condition the LLM on what inputs (style, content, etc.) it should generate.\nImplementation. We implement diversity sampling by (1) vectorizing user-provided data using SentenceTransformer embeddings [42] and (2) clustering the data into N clusters with KMeans and selecting one example from each cluster. This strategy produces semantically different clusters and hence diverse examples.\nIn our evaluation, we set the number of few-shot examples to 8, following existing work [12]. For \u2463 example labeling, we experiment with both a teacher model (gpt-4-turbo-preview) and a student model (flan-t5-xx1). For example synthesis, we use gpt-4-turbo-preview (Figure 4)."}, {"title": "3.2.4 Data slicing", "content": "With the slicing prompt constructed, the final data slicing step applies the prompt using a student model-we use flan-t5-xxl in our evaluation. This step produces slicing annotations for the entire dataset."}, {"title": "3.3 System Interface", "content": "SEMSLICER is packaged as a Python library. As shown in Figure 2, a user first provides a dataset to slice (line 13) and a slicing criterion (line 14). Next, the user specifies what slicing function they want to generate with the desired configurations (line 16), with fine-grained control on each component of SEMSLICER. SEMSLICER will generate a slicing prompt, following the workflow summarized in Figure 3, and produce a slicing function (line 24), which can be further applied on any datasets (line 26). The entire process can be easily batched and extended to multiple slices.\nThe above example demonstrates how SEMSLICER can support workflows with minimal human-effort. However, as we have discussed, humans can easily provide more guidance by (1) providing more detailed slicing instructions instead of simple keywords"}, {"title": "4 EVALUATION", "content": "First, to demonstrate feasibility and practicality, we evaluate our method's ability to produce accurate slices and its associated cost across different system configurations:\n\u2022 RQ1: How accurate are SEMSLICER'S predicted slices across different configurations?\n\u2022 RQ2: How much cost/latency does it take to produce the slices across different configurations?\nNext, to demonstrate usefulness for downstream usage, we evaluate our method's ability to assist model evaluation:\n\u2022 RQ3: How useful is SEMSLICER for model evaluation?\n4.1 Experiment Setup\nDatasets. To evaluate our method, we collect existing datasets with ground truth slices, as there are no existing benchmarks on semantic data slicing. We look at three different strategies to collect suitable datasets:\n\u2022 Human annotations. Sometimes dataset curators ask humans (usually crowdworkers) to annotate existing datasets with additional attributes of input texts (e.g., ethnicity groups referenced in texts). These attributes make plausible slices that are hard to obtain without human annotations. These slices are the most realistic (though potentially noisy due to crowdworker mistakes and subjectivity [2]), but they are challenging to find due to the high cost of human annotation. We use the existing CivilComments [7] dataset-the task is toxicity detection, but in addition to the toxicity label, the dataset has been annotated by crowdworkers with additional attributes regarding referenced demographic groups on five categories (gender, sexual orientation, religion, race, and disability) with 23 concrete attributes (e.g., Black, Christian). We derive slices from these additional attributes. To reduce experiment cost, we randomly sampled 6000 examples and used the 8 largest slices.\n\u2022 Synthetic data. For model testing and evaluation [44], developers often create synthetic test data for different subgroups of the target population. We can treat data from each subgroup as a slice. These slices are accurate by construction, as they are created specifically for each subgroup, but can be less realistic due to their synthetic nature. We use two datasets created with this strategy, HateCheck [45] and AdaTest [43]. The first is a hate speech test suite (n=3728) with comments for 7 different subgroups (e.g., women, trans). The second is a sentiment analysis test suite (n=196) with data for 6 aspects (e.g., price, location) of a hotel review.\n\u2022 Metadata. Many datasets come with metadata produced as part of data collection, such as tags on QA websites from which data was scraped. We can create slices from such metadata, but these slices may be less accurate as different categories (e.g., clothes vs. sports) might overlap semantically, leaving some examples missing from the slices. We use the existing Amazon [6] sentiment analysis dataset, which in addition to the sentiment label, contains metadata about what product category was reviewed (e.g., books, electronics). To reduce experiment cost, we randomly sampled 6000 examples and used the 5 largest slices.\nTo summarize, we selected four datasets with 26 ground-truth slices-see Table 3 for their names and proportions. These datasets cover different uses of slicing: slicing on sensitive attributes for fairness (HateCheck and CivilComments) and slicing on topics/domains for fine-grained model evaluation (AdaTest and Amazon). We collected all slicing criteria (the inputs to SEMSLICER) directly from the datasets, or from the descriptions in the associated papers.\nConfigurations. To understand how different configuration options impact the accuracy and cost of SEMSLICER, we selected and analyzed 9 representative configurations of SEMSLICER, following a fractional factorial design [3] to cover all values in each dimension (as shown in Table 2). These configurations cover whether to use few-shot examples (zero-shot vs. few-shot), how few-shot inputs are collected (provided vs. synthetic), sampled (random vs. diversity), and labeled (student vs. teacher), as well as how instructions"}, {"title": "4.2 RQ1: Slicing Accuracy", "content": "Setup. We measure slicing accuracy for each slice, that is, how well the slices generated by SEMSLICER correspond to the ground-truth slices in the dataset. We measure slicing accuracy using F1-score, an established metric for imbalanced datasets, as slices often represent a small fraction of the entire dataset (ranging from 0.5%\nResult: SEMSLICER produces accurate annotations with 75.9% average F1-score across all slices in the best configuration. SEMSLICER produced the most accurate slices with 75.6% average F1-score in configuration Mfs-hai with few-shot examples and human+model refinement, as shown in Table 3. For the configurations without human intervention, we found Mfs-teacher has the highest average F1-score of 71.6%. As discussed earlier, SEMSLICER does not need perfect accuracy to generate useful slices-our results for RQ3 will show that the current level of accuracy can already reliably detect under-performing slices.\nResult: Few-shot prompting improves accuracy by 7% on average. Comparing few-shot configurations with zero-shot, we found few-shot prompting improves performance by 7% on average (Table 3). Among different few-shot setups, we found teacher labeling improves performance by 1.9% (Mfs-teacher vs. Mfs-div), but observed a limited impact from sampling methods (Mfew-shot vs. Mfs-div) and a negative impact from synthetic examples (Mfs-teacher vs. Mfs-syn). We hypothesize that this is because the generated synthetic examples are still too unrealistic to help later slice labeling.\nResult: Human intervention significantly improves accuracy. We observe that LLM-based instruction generation and refinement have an unstable impact compared to simple templates: They sometimes generate good instructions that improve accuracy a lot (e.g., Mzs-model for HateCheck) but can also generate bad instructions that hurt accuracy (e.g., Mzs-tmodel for HateCheck). However, with human interventions, generated instructions significantly improve accuracy, with an 8.9% increase for zero-shot (Mzero-shot vs. Mzs-hai), and a 4.3% increase for few-shot (Mfs-teacher vs. Mfs-hai). This shows that human feedback is particularly useful to guide instruction optimization in the right direction.\nDiscussion. Because we noticed that SEMSLICER performs poorly on some slices of the CivilComments dataset, especially due to low precision, we investigated the reasons for this behavior. We sampled 40 false positive examples from four slices with low precision scores (male, female, homosexual, christian) under the Mfs-teacher setting and manually inspected the examples: Surprisingly, we found that 24 out of 40 examples actually cover nuanced patterns that (we argue) should be in the slices, yet were missed by human annotators. These patterns are often related to the slicing criterion in a less"}, {"title": "4.3 RQ2: Cost and Latency", "content": "Setup. In our experiments, we use a local machine with 2 A6000 GPUs for running flan-t5-xxl and use OpenAI's APIs to query gpt-4-turbo-preview. We collected the end-to-end slicing latency and calculated the number of annotations per second as the normalized latency. To measure cost, we collected the number of input/output tokens for each LLM query and estimated the cost (in USD) based on charges from LLM providers. We estimated the human cost based on the average pay for crowdworkers and data scientists. The calculation details can be found in the appendix.\nResult: SEMSLICER can slice a dataset with 6,000 rows in 13.5 minutes with a cost of $1.70 per slice in the most accurate automated configuration. We found SEMSLICER can annotate 6 to 14 examples per second (Table 4). This translates into using 7.1 minutes (zero-shot) to 13.5 minutes (few-shot) for annotating one slice in a dataset of size of 6k (CivilComments). We estimate that the associated cost is about $0.26 (zero-shot) to $1.70 (few-shot) per slice for automated configurations, and up to $9.56 (Mfs-hai) for human-in-the-loop configurations (Table 2). In contrast, using crowdworkers for the same task would cost $432.00, which is 44x to 1661x more than SEMSLICER but not necessarily more accurate.\nResult: SEMSLICER allows flexible trade-offs between design dimensions through different configurations. Comparing different configurations, we observed there are clear trade-offs between design dimensions among different configurations (Figure 5). Using few-shot examples can increase the cost by around $1.40 per slice and takes 91% more time (Table 4), with significant accuracy improvement by 7% on average (accuracy vs. compute/latency). Human interventions can increase the cost a lot ($7.87 per slice), but also come with significant accuracy improvement by 4.3% to 8.9% (accuracy vs. human-effort)."}, {"title": "4.4 RQ3: Usefulness", "content": "We approach usefulness from two perspectives, by understanding (1) whether SEMSLICER can help identify known under-performing slices and (2) whether SEMSLICER can help practitioners conduct model evaluation and generate additional insights.\nFor the first part, we demonstrate usefulness if SEMSLICER can reliably detect under-performing slices, a common use of data slicing (cf. Section 2). For the second part, we conduct a human-subject case study to triangulate the automated experiments, by showing that practitioners found SEMSLICER useful in their real-world cases.\n4.4.1 Can SemSlicer help identify under-performing slices?\nSetup. A common use of slicing is to identify parts of the input space where the model under-performs. That is, we want to identify the slices in which their downstream task performance (i.e., the accuracy of the original task, such as toxicity detection) is statistically significantly worse than the average, even if the slicing accuracy is not perfect. Specifically, we want to evaluate whether our slicing accuracy is good enough to detect those same slices as under-performing that we would have detected with perfect slicing accuracy from the ground-truth attributes in our datasets.\nTo understand whether SEMSLICER can identify under-performing slices, we measure task performance for each dataset or slice. For AdaTest, we reuse the reported predictions from their commercial sentiment analysis model and compare them against the labels in the dataset. For HateCheck, Amazon, CivilComments, we use popular models (listed in the appendix) from Hugging Face and compare their predictions against the labels in the dataset. For all datasets, we report task performance with the standard accuracy metric.\nFor our analysis, we determine whether a slice (ground truth or computed with SEMSLICER) under-performs by comparing the slice's task performance against the overall task performance on the entire dataset, using Fisher's exact test to test whether the difference is statistically significant (p-value \u2264 0.05). We then compare under-performing slices detected through SEMSLICER with those detected on ground-truth slices. For this experiment, we use the most accurate automated configuration, Mfs-teacher.\nResult: SEMSLICER identifies all 7 under-performing slices. We found that 7 out of 26 slices are statistically significantly under-performing and SEMSLICER is able to identify all of them (Table 3). In addition, SEMSLICER identifies only one slice as under-performing that is not under-performing according to the ground-truth slice (slice \"gay\" in HateCheck; false positive). This demonstrates that SEMSLICER'S slicing accuracy is sufficient for the practical application of identifying under-performing slices.\n4.4.2 Can SEMSLICER help practitioners conduct model evaluation?\nSetup. We approached researchers in our contact network to identify ML-related projects and selected one participant. The participant worked on a project to understand how LLM annotations align differently with different demographic groups. More specifically, the participant had annotators annotate the Social Acceptability [16] dataset and computed the correlations between model and human annotations on various slices of annotator attributes. The participant observed that LLMs (e.g., GPT-4) aligned better with Western, White, college-educated, and younger populations and was interested in using SEMSLICER to tie correlations back to specific slices on input text, which they could not have done without support from semantic data slicing.\nBefore the study, we first discussed with the participant about what kinds of input text slices they would be interested to see for their dataset. The participant mentioned that slices concerning different demographics in the input text would be interesting- following this, we collected 6 slicing criteria for our case study (\"gender\", \"age\", \u201cnationality\u201d, \u201ceducation\u201d, \u201creligion"}, {"title": "5 RELATED WORK", "content": "In Section 2, we already discussed the most closely related work on data slicing. Here, we additionally discuss related work on automated prompt engineering relevant to how we implemented SEMSLICER, as well as how slice discovery can augment semantic slicing.\n5.1 Automated Prompt Engineering\nInstructions. Automated improvement of prompt instructions is an emerging direction that has attracted lots of attention. Zhou et al. [66] generate and paraphrase instruction candidates using LLMs and select instructions based on scores on labeled evaluation data. Pryzant et al. [40] improve instructions iteratively with erroneous examples. Both studies assume access to a labeled dataset. In contrast, SEMSLICER'S instruction generation and refinement are designed for a zero-label setting.\nFew-shot examples. Few-shot examples are known to have a large impact on prompt performance [65], and few-shot example selection has been an active field of research. Liu et al. [30] propose to retrieve examples that are semantically similar to a test sample. Other work found the retrieved examples are often redundant and proposed to find diverse examples with high coverage [19, 61].\nMost of the existing work assumes access to a labeled training dataset, while SEMSLICER requires few-shot example selection without labels. Even though we cannot use these methods directly, they still inspired our design of the input sampler, where we provide the option to retrieve diverse inputs.\nClosest to our work is universal self-adaptive prompting [53], where they select high-confidence examples (based on the self-predictions) as few-shot examples. Our early experimentation did not observe consistent improvement from this method and hence we did not report it in our evaluation. However, it is still available as an option for input sampling.\nDSPy [26] is a framework that can automatically generate and optimize prompts for generic LLM pipelines. SEMSLICER is designed specifically for data slicing as a standalone framework, but can potentially be implemented using existing frameworks like DSPy.\nPrompt selection. Another line of work on zero-label prompt selection [60] aims to select a good prompt from multiple candidates. Existing work has explored selection using pseudo labels from prompt ensembles [29], perplexity scores [18], and mutual information [50]. Our early experimentation suggested limited improvement from these methods, and hence we did not use them in the final design.\n5.2 Automated Slice Discovery\nAutomated slice discovery [13, 14] is an error analysis approach based on the idea of automatically identifying under-performing areas in the input space, often using clustering or dimensionality reduction techniques. These areas can be interpreted as slices and this line of work explores ways to identify meaningful names for those areas, which could then be considered as slicing criteria. However, these methods are designed to discover under-performing areas, without considering whether these are complete according to some human-interpretable abstraction. As pointed out by Johnson et al. [23], their produced slices can be misleading, in that models"}, {"title": "6 CONCLUSION", "content": "In this work", "opportunities": "nSemantic slicing at a reduced cost. In its current design, SEMSLICER costs $0.26 to $1.70 to automatically produce slices from a dataset of 553k tokens. This is much cheaper than recruiting crowdworkers but can still incur substantial cost for bigger datasets, longer inputs, or more slices. One promising direction is to explore retrieval-based methods [27", "58": "for semantic slicing, where the slicing agent actively refines instructions and few-show examples, based on automated or LLM-generated feedback.\nInteractive semantic slicing. The current design of SEMSLICER allows human intervention through"}]}