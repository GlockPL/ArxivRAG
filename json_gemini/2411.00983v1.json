{"title": "IMPROVING HOW AGENTS COOPERATE: ATTENTION SCHEMAS IN ARTIFICIAL NEURAL NETWORKS", "authors": ["Kathryn T. Farrell", "Kirsten Ziman", "Michael S. A. Graziano"], "abstract": "Growing evidence suggests that the brain uses an \u201cattention schema\" to monitor, predict, and help control attention. It has also been suggested that an attention schema improves social intelligence by allowing one person to better predict another. Given their potential advantages, attention schemas have been increasingly tested in machine learning. Here we test small deep learning networks to determine how the addition of an attention schema may affect performance on a range of tasks. First, we found that an agent with an attention schema is better at judging or categorizing the attention states of other agents. Second, we found that an agent with an attention schema develops a pattern of attention that is easier for other agents to judge and categorize. Third, we found that in a joint task where two agents paint a scene together and must predict each other's behavior for best performance, adding an attention schema improves that performance. Finally, we find that the performance improvements caused by an attention schema are not a non-specific result of an increase in network complexity. Not all performance, on all tasks, is improved. Instead, improvement is specific to \"social\" tasks involving judging, categorizing, or predicting the attention of other agents. These results suggest that an attention schema may be useful in machine learning for improving cooperativity and social behavior.", "sections": [{"title": "Introduction", "content": "Growing evidence suggests that the brain uses an \"attention schema\" to monitor, predict, and help control attention [1-5]. The attention schema may be used to model more than one's own attention. It may also be directed to model the attention of others, affording better theory of mind, better predictions of the behavior of others, and therefore more effective social interaction [1,6-12]. In the present study, we ask whether adding an attention schema to an artificial agent improves its ability to make judgements about the attention of other agents, improves its ability to have its own attention judged by others, and improves its ability to coordinate with another agent in a simple joint task.\nRecently, versions of attention schemas have been in-corporated into artificial neural networks to test whether the same principles apparent in the human brain might provide benefits in machine learning, and to determine whether lessons from the more controlled, artificial case might provide conceptual insights into human cognition [13-17]. Two studies in particular suggest that self-models may improve performance in cooperative tasks. In one study, Liu et al. [15] trained interacting groups of networks on visual tasks that required the agents to coordinate with each other. A transformer-style visual attention was used. In some conditions, networks were provided with an attention schema, in the form of a learned ability to predict the state of their own attention mechanism. With an attention schema, agents performed the social coordination tasks better. One interpretation is that when an agent is trained to predict its own attention states, it also becomes better at predicting the attention states of other agents in the same environment. In that hypothesis, learning to self-model leads to an improvement in the ability to model others and thus to an improvement in group task performance.\nIn a second recent study on the effects of self-modeling, Premakurma et al. [16] trained artificial networks on a"}, {"title": "Experiment 1: Do attention schemas improve the ability of agents to model each other's attention states?", "content": "Introduction to Experiment 1\nThe purpose of Experiment 1 was to train artificial agents with and without attention schemas and to test two hypotheses. Hypothesis 1 was that an agent with an attention schema will be better at learning to make categorical judgements about the attention of other agents. Hypothesis 2 was that when an agent has an attention schema, other agents will perform better at learning to make categorical judgements about its attention.\nAgents were first trained on a visual categorization task (e.g. categorize a picture as showing a golf ball or a garbage truck). We then froze the weights in all layers except a final block, and trained the agents on a new task, the \"attention judgement\u201d task. The task allowed us to test the ability of one agent to make categorical determinations about the attention states of another agent. The attention judgment task was modified from Ziman et al. [18], who demonstrated the ability of people to assess the attention states of others. In the human version of the task, participants looked at a spotlight of attention moving around a picture and judged whether the spotlight represented real attention (based on eye movements recorded previously from a real person), or whether it represented a randomly scrambled version of attention (the recorded attention loca-tions presented in a random order). Since people were able to perform above chance, the human brain must contain information about the typical or predicted patterns of at-tention and must be able to recognize when those patterns have been violated. Thus, humans demonstrate an implicit predictive model of attention.\nTo modify the task for the present study, we assigned agents to one of two roles: either the receiver or the sender. If the agent was assigned the sender role, it performed its originally trained visual categorization task. The attention activation values, for each trial of the task, were saved in the form of a multidimensional tensor. In one version, the tensor was left in its correct, original form. In another ver-sion, the attention values were randomly scrambled along one dimension of the tensor.\nIf the agent was assigned the receiver role, it was first trained on the image categorization task; then its weights were frozen except in the final block of layers; and then it was trained on the attention judgement task. On each trial of the attention judgement task, instead of receiving a picture as an input, the agent was provided with an atten-tion tensor; and instead of being rewarded for categorizing the picture, it was rewarded for categorizing the tensor as either a valid example of attention (recorded from the performance of a sender agent) or as an invalid example (constructed by randomly modifying a valid tensor). The task therefore tested the ability of the receiver agent to learn to recognize violations of predictable patterns in the attention behavior of the sender.\nThe experiment used a 2\u00d72 design: the sender agent could be trained with or without an attention schema, and"}, {"title": "Methods for Experiment 1", "content": "We trained networks on a visual categorization task. Given an image as input (for example, a picture of a golf ball on a lawn), the network was trained to arrive at a dichoto-mous categorization (for example, golf ball versus garbage truck). All networks and task environments were cre-ated using PyTorch [19] and all experiment code and data are publicly available at github.com/kathrynfarrell/attention_schemas_in_anns. We used an image-adapted implementation of the Attention Schema Neural Network architecture proposed by Liu et al. [15], which utilized a transformer-based attention mechanism. Figure 1 shows the architecture, with 1A showing the control architecture (control network: attention but no attention schema) and 1B showing the architecture that includes an added attention schema (attention schema network).\nFor details on how the attention schema mechanism works, see Liu et al. [15]. Briefly, the objectives of the schema network are twofold: learning to model and re-fine its own attention and learning to complete a task (e.g. classifying images). The schema network uses its applied attention in service of both goals: modeling its own atten-tion (by predicting its final attention), and further editing the attention to its final refined state. To accomplish this, the schema network uses a transformer block with six atten-tion heads, and passes the applied attention into a recurrent neural network module with learnable parameters. The output of this module (Rout) is further processed and used to model attention; specifically, it is passed into the Pre-dictive Layers, which output the predicted attention. Rout is also further processed and used to refine attention; it is passed into the Decision Control Layers which produce the final refined attention (with the Decision Control Layers employing activator units, suppressor units, and categori-cal reparameterization with Gumbel-Softmax). In this way, manipulations of the initial applied attention are used to generate both the prediction of the final attention as well as the final attention itself. Schema networks are trained on their combined ability to accurately complete the main task (e.g. classifying an image) and predict their own attention.\nIn the present study, the network received as input an image of size 256 pixels \u00d7 256 pixels \u00d7 3 color channels, and was trained to produce an output that classified the image into one of two categories. To ensure that the result was not specific to one type of categorization, separate networks were trained on three different sets of images and categorizations (image classification task A, parachute versus bench; image classification task B, French horn ver-sus cassette player; image classification task C, golf ball versus garbage truck). All three image sets were subsam-pled from Imagenette [20]. Two kinds of networks were trained: control networks without an attention schema, and networks with an attention schema. For those with an at-tention schema, the relative weight on the main task versus the attention prediction task was 0.95 to 0.05.\nEach network was trained on 1,912 images for 20 train-ing epochs, and tested on 730 images. For each network that did not contain an attention schema, for each of its 730 testing discriminations, we saved the attention scores from three of its six attention heads (randomly selected) in the transformer encoder block. For each network that con-tained an attention schema, we used a similar procedure but saved the modified attention scores that resulted from element-wise multiplication between the attention scores and the output of the decision control layers.\nTo produce an attention tensor, we stacked the scores from the three attention heads into a 257 token \u00d7 257 token \u00d7 3 attention head tensor (257 tokens in the first two dimensions resulting from the 16\u00d716 image patches from the original input image plus one class token). From these tensors, we created the veridical and scrambled at-tention data that would be sent to the receiving networks for discrimination. The veridical attention tensors were created by reducing the size of the tensors from 257 \u00d7 257 \u00d7 3 to 256 x 256 \u00d7 3, matching the size of the original input image. For the scrambled attention, we randomly permuted each tensor along its last dimension, which ran-domly shuffled the attention scores within the data from each attention head. Then, we reduced the resulting tensor to 256 x 256 \u00d7 3, again matching the dimensions of the original input image. This procedure resulted in a set of 1,460 attention tensors from each network, half veridical and half scrambled, with the same dimensions as the input images the networks were originally trained on.\nReceiving networks were constructed by first pre-training them on the image discrimination task following the same training procedures described above. We then froze all weights in the network except those in the final multilayer perceptron (MLP) head (a module consisting of two linear layers, which computes the final classification decision). We further trained the network for 800 training epochs by providing the network with new training data in the form of 1,314 labeled attention tensors saved from the sending networks, half veridical and half scrambled. The receiving networks were then tested on an additional 146 attention inputs (half veridical, half scrambled). To avoid any possible bias from the original image classification"}, {"title": "Results for Experiment 1", "content": "The experiment used a 2\u00d72 design in which the sender network could be a control network or an attention schema network and the receiver network could be a control net-work or an attention schema network. We hypothesized that networks with attention schemas should be better at modeling other agents' attention than networks without schemas. Thus, a receiving network with an attention schema should be better at discriminating the attention data from sending networks. We also predicted that atten-tion from networks with attention schemas should be easier to model than attention from networks without schemas. Thus, a receiving network, whether it does or does not have an attention schema, should be better at discriminating at-tention data from sending networks that have attention schemas.\nFigure 2 shows the result. Performance on the attention discrimination task was well above chance (mean = 80.87% correct, SEM = 2.38) when both the sending network and the receiving network contained an attention schema. Per-formance dropped when the sending network contained an attention schema but the receiving network did not (mean = 70.61, SEM = 2.37). Performance dropped further when the sending network did not have an attention schema, but the receiving network did (mean = 54.58, SEM = 1.25). Finally, performance was lowest when neither sending nor receiving network contained an attention schema (mean = 52.78, SEM = 1.21). These results support both hypothe-ses. With respect to Hypothesis 1: When the receiving network contained an attention schema, it had an advan-tage in learning to categorize the attention data from the sending network. With respect to Hypothesis 2: When the sending network contained an attention schema, the network was in some manner altered such that its attention data was easier for the receiving network to categorize.\nThese effects were statistically significant. We per-formed a 2\u00d72 ANOVA and found that the main effect of the receiving network (receiving network does or does not have an attention schema) was statistically significant (F=10.17, p=0.0018). The main effect of the sending net-work (sending network does or does not have an atten-tion schema) was also statistically significant (F=136.18, p=1.28\u00d710-20). The interaction between the two factors was also significant (F=4.99, p=0.027). The largest effect was whether the sending network contained an attention schema. Having an attention schema made the network more interpretable to other networks.\nWe also examined performance of networks on the vi-sual categorization task. We did not have any specific hypothesis about whether a network with an attention"}, {"title": "Discussion for Experiment 1", "content": "Experiment 1 showed that the presence of an attention schema in a network confers two specific advantages, at least in one kind of network and with a specific implemen-tation of an attention schema. The first advantage is that when a network with an attention schema is pretrained on a task, that network is then better able to learn to make judgements about the attention states of other networks. Through initial training in which it predicts its own at-tention states, it gains an improved ability to model the attention states of others. The second advantage is that when a network contains an attention schema, its attention states become easier for other networks to interpret and categorize. In principle, both of these changes should be advantageous in joint tasks in which interpreting, categoriz-ing, and predicting the behavior of others is useful. It may be of interest that the largest impact we observed here was that when a network contained an attention schema, its at-tention states were more easily recognized and categorized by other agents. It has been suggested that self-models such as attention schemas help to restructure networks, causing them to make their internal computations simpler and more regularized, and as a result could make those networks an easier target for others to predict and interpret [16]. The present results are consistent with that hypothe-sis."}, {"title": "Experiment 2: Are the performance benefits of an attention schema caused by added network complexity?", "content": "In Experiment 1, attention schemas improved the abil-ity of networks to categorize each other's attention states (showing improved performance on the attention catego-rization task), but did not improve the ability of networks to categorize visual images in general (showing no significant change on the image classification task). This specificity of the effect suggests that the benefit of an attention schema is not just the general result of added network complexity, but instead a specific improvement in the ability to learn about attention patterns.\nHowever, the results of Experiment 1 are subject to an alternative hypothesis. The image classification task was learned by networks first. After being trained on that initial task, networks had their weights frozen except in the final MLP block, and then underwent transfer learning on the attention categorization task. Could it be that having an at-tention schema does not benefit performance on a primary task, but adds enough extra computational complexity to benefit performance on transfer learning to a secondary task? Would transfer learning to any secondary task be improved by an attention schema, or is the improvement specific to the attention categorization task?\nTo check this possibility, we conducted transfer learn-ing on the networks. First we pretrained them on the image classification task, replicating the procedure in Experi-ment 1. However, instead of then training the networks to classify attention (real versus shuffled), we trained the net-works on another image classification task. Networks that were pretrained on one of the three image classification tasks (for example, task A) were retrained on a different one (for example, task B). We then asked whether transfer learning from one classification task to another was better for networks that contained an attention schema.\nThe results are shown in Figure 3B. Transfer learning was comparable between the two network architectures. No significant difference was found (t = -0.74, p = 0.46). These results show that schema networks and control net-works perform similarly on basic discrimination tasks both before and after transfer learning.\nThe results of Experiment 2 help to confirm that the effect of an attention schema, observed in Experiment 1, was not a nonspecific result of greater network complexity. The improvements in performance seen in Experiment 1 were specific to categorizing attention performance, and did not extend to image categorization in general."}, {"title": "Experiment 3: Do agents with attention schemas perform better on a cooperative task?", "content": "Introduction to Experiment 3\nA central hypothesis of the attention schema theory is that, in people, the presence of an attention schema allows for better social cognition and better performance in co-operative tasks [1,6]. In the present study, in Experiment 1, networks that contained attention schemas showed bet-ter ability to categorize each other's attention states. We hypothesized that networks with attention schemas would also perform better on a joint task that requires interaction with another agent. We chose a simple task involving two networks mutually coloring an image, much like two chil-dren sharing a page in a coloring book. The networks took turns placing colored pixels on the image. Reward was earned jointly by both networks for painting as many pixels as possible over multiple turns, but reward was lost if the painted area of one network overlapped the painted area of the other network. One each turn, each agent chose which pixels to color, but did not have any direct information about which pixels its partner would choose on the same turn. Ideal performance on the task should require each network to make turn-by-turn predictions of the behavior of its partner. If one network could predict the part of the image that its partner was likely to color, it could then choose to color a different part of the image, avoiding over-lap. Thus, mutual behavioral prediction should improve performance. The behavior of each network was in turn influenced by its visual attention to the underlying image. We therefore reasoned that it would be beneficial for each network to learn an implicit, predictive model of the other network's attention.\nMethods for Experiment 3\nUsing an image-based Multi-Agent Reinforcement Learn-ing (MARL) environment, we designed a coloring task wherein, over the course of multiple turns, two networks colored in an image. The two networks were presented with the same image selected from a pool of 2,000 images, subsampled from 10 categories in Imagenette [20]. On each turn, each network chose a selection of pixels from the image and marked them as painted. The network could see which pixels had previously been painted and which member of the team had painted it, but had no information about which pixels its partner network would choose to color on the current turn. Reward, given jointly to both networks, increased with each novel pixel that was colored on each turn, but decreased for each pixel that was colored by both networks on that turn (\u201coverlapping", "formula": "n$R = \\frac{A_{discovery} * N_{discovery}}{1+C_{overlap} * N_{overlap}}$\nwhere $n_{discovery}$ denotes the total number of novel pix-els selected by both agents on a given turn, $N_{overlap}$ de-notes the number of overlapping pixels selected by both agents on that turn, and $A_{discovery}$ and $C_{overlap}$ signify scal-"}, {"title": "Results for Experiment 3", "content": "Figure 4A shows performance on the task across fifty train-ing epochs, for all three types of network pairings. In line with our predictions, we found that schema-schema teams achieved the highest rewards on the cooperative coloring task (blue line in Figure 4A, averaged across epochs, mean = 2.04). Mixed teams received the second highest rewards (green line in Figure 4A, averaged across epochs, mean = 1.91). Control-control teams obtained the smallest rewards (orange line in Figure 4A, averaged across epochs, mean = 1.76).\nWe also analyzed the average number of overlapping pixels as a metric of within-team coordination, since net-works that are well coordinated with their teammates should be better at reducing the overlap. The results are shown in Figure 4B. We found that schema-schema teams and mixed teams exhibited better coordination, selecting fewer overlapping pixels on average (blue line for schema-schema teams, mean across epochs = 54.51; green line for mixed teams, mean across epochs = 54.89), relative to the control-control teams (orange line for control control teams, mean across epochs = 57.79).\nBy the rules of the game, both networks in a team earned the same number of points, and both networks accrued the same number of overlapped pixels. However, each net-work chose a different number of pixels to color on each"}, {"title": "Discussion for Experiment 3", "content": "The results of Experiment 3 show that having an atten-tion schema confers a benefit on the joint coloring task. An agent needs to color as many pixels as it can on each trial, while avoiding overlap with the predicted coloring locations of its partner. Performance therefore depends on being able to predict where its partner will color on the current trial. Having an attention schema makes an agent better at this coordination. There are two possible reasons why performance might have improved. First, it could be that when a network has an attention schema, it is better able to predict the behavior of its partner. Second, it could be that when a network has an attention schema, it becomes more easily predictable to its partner. Both possi-bilities could be true simultaneously. The data suggest that the stronger effect is that when a network has an attention schema, it becomes more predictable to its partner. This re-sult is consistent with the results of Experiment 1, in which the largest impact of having an attention schema was to make a network more interpretable to other networks.\nOne of the properties of the joint coloring task that makes it useful in the present context is that on each turn, an agent has a large range of options. It chooses how many and which pixels to color. That choice is heavily shaped by the parts of the image that are enhanced by its attention mechanism. In this sense, the task resembles a typical hu-man situation with open-ended choice. Should you reach for the coffee cup, the donut, the phone, or start a conver-sation? Moment by moment, the focus of your attention shifts to the feature of your world that will be the target"}, {"title": "General Discussion", "content": "It has been proposed that the human attention schema, a predictive model of attention, is important to our social cognition and our cooperative ability [1,6]. Using similar principles in artificial systems might be a way to improve the prosocial and cooperative behavior of machines. The goal of the present study was to test whether adding an attention schema to artificial agents would make them bet-ter cooperators in a joint task. We used a version of an attention schema in which a network learns to predict its own attention state and uses those predictions to modify its attention state [15].\nIn Experiment 1, we found that adding an attention schema to an agent improved its ability to learn how to categorize the attentional state of another agent. Even more so, it improved the ability to have its own attentional state categorized by others. This finding is consistent with recent work suggesting that when an agent learns a pre-dictive self-model, it also partly self-regularizes [16]. It learns to make itself more predictable. One interpretation of the present results, therefore, is that an agent with an attention schema, by virtue of learning to predict its own attentional states, regularizes its own attentional dynamics and makes itself more easily categorized, interpreted, and predicted by others.\nIn Experiment 2, we found that the improvements caused by the presence of an attention schema were spe-cific to a task involving the categorization of attention states. Adding an attention schema had no significant ef-fect on other categorization tasks. This finding suggests that the effects are not the result of a general increase in network complexity. Instead, there is something about an attention schema that allows for improvement on attention-related tasks.\nIn Experiment 3, we tested agents in a simple joint task, a coloring task. Two agents took turns coloring in an image while minimizing overlap. On each turn, each agent chose pixels to color without knowing the pixels chosen by its partner on that turn. Ideal performance should require each agent to predict the choices of its partner, such that it can maximize the number of pixels it colors while minimizing overlap with its partner's choices. The earned rewards were shared by both agents. We found, as hypothesized, that performance on the task was best when both agents contained an attention schema, weaker when only one of the two agents contained an attention schema, and weak-est when neither agent in the pair contained an attention schema. Further analysis suggested that the largest effect of adding an attention schema to an agent may have been to make that agent more predictable to its partner, allowing the partner to perform better at the task.\nThe study of social cognition has focused almost exclu-sively on how mechanisms in the brain of person A can allow that person to better understand person B. However, it may be equally important, if not more so, to consider the mechanisms in the brain of person A that render that person more easily understood by person B. It is a matter of not just rendering oneself a better receiver of socially useful information, but also a better sender of socially use-ful information. We speculate that possessing an attention schema may provide both properties \u2013 a greater ability to understand others, and a greater ability to be understood by others - thereby improving social cooperation. In a competitive interaction, it is probably disadvantageous to be more easily understood by others. But in a cooperative interaction, rendering oneself more transparent, more in-terpretable, and ultimately more predictable, may be an advantage.\nThe reason why an attention schema, in particular, may be of central importance, is that attention is almost entirely determinative of behavior. People react to items that they are paying attention to, and do not react to items they are not attending. Attention enhances the processing of an item and brings it to higher cognition and action planning. To ask what is in a person's mind at any particular moment is essentially a colloquial way of asking what items are in the focus of that person's attention. In a much simplified way, the present study might be seen as reflecting the same kind of processes."}, {"title": "Data availability", "content": "All data, and code for running the experiments and analyzing the data are publicly available online at\nhttps://github.com/kathrynfarrell/attention_schemas_in_anns."}]}