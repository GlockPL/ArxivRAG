{"title": "Can Diffusion Models Learn Hidden Inter-Feature Rules Behind Images?", "authors": ["Yujin Han", "Andi Han", "Wei Huang", "Chaochao Lu", "Difan Zou"], "abstract": "Despite the remarkable success of diffusion models (DMs) in data generation, they exhibit specific failure cases with unsatisfactory outputs. We focus on one such limitation: the ability of DMs to learn hidden rules between image features. Specifically, for image data with dependent features (x) and (y) (e.g., the height of the sun (x) and the length of the shadow (y)), we investigate whether DMs can accurately capture the inter-feature rule (p(y|x)). Empirical evaluations on mainstream DMs (e.g., Stable Diffusion 3.5) reveal consistent failures, such as inconsistent lighting-shadow relationships and mismatched object-mirror reflections. Inspired by these findings, we design four synthetic tasks with strongly correlated features to assess DMs' rule-learning abilities. Extensive experiments show that while DMs can identify coarse-grained rules, they struggle with fine-grained ones. Our theoretical analysis demonstrates that DMs trained via denoising score matching (DSM) exhibit constant errors in learning hidden rules, as the DSM objective is not compatible with rule conformity. To mitigate this, we introduce a common technique - incorporating additional classifier guidance during sampling, which achieves (limited) improvements. Our analysis reveals that the subtle signals of fine-grained rules are challenging for the classifier to capture, providing insights for future exploration.", "sections": [{"title": "1. Introduction", "content": "Despite the remarkable capabilities demonstrated by diffusion models (DMs) in generating realistic images (Ho et al., 2020; Song et al., 2020; Vahdat et al., 2021; Dhariwal & Nichol, 2021; Karras et al., 2022; Tian et al., 2024b), videos (Ho et al., 2022; Yu et al., 2024; Yuan et al., 2024), and audio (Liu et al., 2023a; Yang et al., 2024; Lemercier et al., 2024), they still encounter specific failures in synthesis quality, such as anatomically incorrect human poses (Borji, 2023; Zhang et al., 2024; Huang et al., 2024) and misalignment between generated content and prompts (Feng et al., 2022; Borji, 2023; Chefer et al., 2023; Liu et al., 2023b; Lim & Shim, 2024), which could harm the reliability and applicability of DMs in real-world scenarios.\nWe focus on a specific type of failure with limited attention: the failure of DMs in learning hidden inter-feature rules behind images. Specifically, consider image data containing dependent feature pairs (x, y), such as the height of the sun (x) affecting the length of a pole's shadow (y). Our investigation centers on whether DMs targeting the joint distribution p(x, y) can accurately capture the underlying relationships between x and y, effectively recovering the conditional distribution p(y|x). Theoretically, a diffusion model that perfectly estimates the joint distribution should naturally capture the conditional distribution, thereby learning the latent rules between features. However, in practice, numerous factors, such as non-negligible score function estimation errors, can cause the sampled joint distribution to deviate significantly from the true distribution (Chen et al., 2022; 2023; Benton et al., 2024). How do these deviations propagate to inter-feature rule learning? This gap between theory and practice remains largely unexplored.\nAlthough existing studies have explored whether DMs can learn specific rules, they primarily focus on independent features, such as DMs' compositional capabilities (Okawa et al., 2024; Deschenaux et al., 2024; Wiedemer et al., 2024). Some works have investigated inter-feature dependencies in DMs, but the varying complexity of rules has led to contradictory findings. For example, DDPM has been reported to fail in generating images satisfying numerical equality constraints (Anonymous, 2025), while succeeding in reasoning about shape patterns in RAVEN task (Wang et al., 2024a). These inconsistencies highlight the need for a unified experimental setting that allows for adjustable rule difficulty, enabling an accurate evaluation of DMs' rule-learning capabilities. Moreover, existing studies rely heavily on empirical observations, lacking theoretical analysis to elucidate the limitations of DMs in rule conformity.\nOur investigation into inter-feature rules begins with observing the limited ability of mainstream DMs (e.g., SD-3.5"}, {"title": "2. Related Work", "content": "We summarize prior studies on the ability of DMs to learn specific rules, and discuss the relations to inter-feature rules.\nFactual Knowledge Rules. The violation of factual rules in DMs refers to generated images failing to accurately reflect factual information and common sense, often characterized as hallucinations in existing work (Aithal et al., 2024; Lim & Shim, 2024; Anonymous, 2025). Typical examples include violating common sense, such as extra, missing, or distorted fingers (Aithal et al., 2024; Pelykh et al., 2024; Ye et al., 2023), unreadable text (Gong et al., 2022; Tang et al., 2023; Xu et al., 2024) and snowy deserts (Lim & Shim, 2024). Additionally, inconsistencies between generated images and given textual prompts (Liu et al., 2023b; Fu & Cheng, 2024; Mahajan et al., 2024; Li et al., 2024b) can be regarded as violations of prompt-based knowledge. Unlike inter-feature rules, factual knowledge rules do not involve relationships between multiple features and are typically attributed to imbalanced training data distribution (Samuel et al., 2024) or mode interpolation caused by inappropriate smoothing of training data (Aithal et al., 2024).\nIndependent Features Rules. Prior work has investigated DMs' ability to combine independent features, i.e., compositionality. Through controlled studies with independent concepts (e.g., color, shape, size), Okawa et al. (2024) observe that DDPM can successfully compose different independent features. Similar findings are reported in (Deschenaux et al., 2024), where interpolation between portraits without and with clear smiles resulted in generations with mild smiles. However, numerous studies highlight DMs' limitations in complex compositional tasks (Liu et al., 2022; Gokhale et al., 2022; Feng et al., 2022; Marioriyad et al., 2024), potentially due to insufficient training data for reconstructing each individual feature (Wiedemer et al., 2024). These studies primarily examine compositional tasks with independent features, in contrast to our focus on feature dependencies.\nAbstract (Dependent Feature) Rules. This type closely aligns with our work, which studies feature relationships like shape consistency in generations. Prior studies give mixed conclusions on DDPM's rule-learning ability. For example, DDPM struggles with numerical addition rule (Anonymous, 2025) but maintains shape consistency rule in RAVEN task (Wang et al., 2024a). Inconsistent rule complexity leads to ambiguous evaluation conclusions, and the lack of theoretical analysis leaves the underlying factors behind DMs' performance in rule learning poorly understood. Through controlled experiments with adjustable rule complexity, we provide a unified assessment of DMs' rule-learning abilities and offer a theoretical explanation of their fundamental limitations, as a result of their training paradigm."}, {"title": "3. Exploring Hidden Inter-feature Rule Learning via Synthetic Tasks", "content": "In real-world image generation tasks, rules between features are often complex and difficult to define or quantify precisely. To systematically investigate DMs' ability in rule learning, as previous work (Okawa et al., 2024; Deschenaux et al., 2024; Anonymous, 2025; Wang et al., 2024a), we design simplified and controllable synthetic tasks in Figure 1. These synthetic tasks not only provide explicitly defined inter-feature rules but also abstract essential feature rules present in real-world data, thereby making our conclusions practically relevant. For example, Synthetic Task A in Figure 1 simulates the Light-Shadow relationship, while Task B simplifies the physical rules of Reflection/Refraction."}, {"title": "3.1. Synthetic Tasks Inspired by Real-World Insights", "content": "Inspired by Borji (2023), we investigate several common scenarios where inter-feature rules exist, as illustrated in Figure 1. Specially, we categorize these hidden rules into two types, spatial rules and non-spatial rules, based on whether the inter-feature relationships exist in the form of spatial arrangements or feature attributes themselves.\nSpatial Rules are defined as constraints on the relative positions and layouts between features, such as the correlation between the sun's height and the shadow's length. In Figure 1, scenario Light-shadow demonstrates how the position of a light source should precisely determine the placement of building shadows. However, both 8-billion Multimodal SD-3.5 Large and 12-billion model Flux.1 Dev, fail to generate proper shadows, either producing incorrect directions or merely creating symmetrical duplicates of the actual buildings. Similarly, in scenario Reflection/Refraction, while objects in front of mirrors should dictate the layout of their reflections, we observe completely unreasonable generations from both models. Furthermore, semantic consistency in Semantics scenario is violated, as shown by sunflowers not facing the"}, {"title": "3.1.1. Real-World Hidden Inter-Feature Rules", "content": "Inspired by Borji (2023), we investigate several common scenarios where inter-feature rules exist, as illustrated in Figure 1. Specially, we categorize these hidden rules into two types, spatial rules and non-spatial rules, based on whether the inter-feature relationships exist in the form of spatial arrangements or feature attributes themselves.\nSpatial Rules are defined as constraints on the relative positions and layouts between features, such as the correlation between the sun's height and the shadow's length. In Figure 1, scenario Light-shadow demonstrates how the position of a light source should precisely determine the placement of building shadows. However, both 8-billion Multimodal SD-3.5 Large\u00b2(Rombach et al., 2022) and 12-billion model Flux.1 Dev\u00b3(Labs, 2023), fail to generate proper shadows, either producing incorrect directions or merely creating symmetrical duplicates of the actual buildings. Similarly, in scenario Reflection/Refraction, while objects in front of mirrors should dictate the layout of their reflections, we observe completely unreasonable generations from both models. Furthermore, semantic consistency in Semantics scenario is violated, as shown by sunflowers not facing the"}, {"title": "3.1.2. Synthetic Tasks", "content": "Inspired by real-world rules in Section 3.1.1, we design four synthetic tasks (A-D), each with two levels of rule granularity (coarse and fine), as shown in Figure 1. We provide a brief overview of synthetic tasks here, with more details presented in Appendix C. Specially,\nTask A is inspired by the spatial rules behind the Light-shadow case, simulating the physical law between the sun and pole shadows. In Task A of Figure 1, the coarse-grained rule requires the sun and shadow to be on opposite sides of the pole, while the fine-grained rule requires sun's center, pole top, and shadow endpoint align linearly, i.e., satisfying $l_1h_2 = l_2h_1$ (see notations in Task A, Figure 1).\nTask B abstracts the spatial rule from the Reflection/Refraction case, where an object's reflection size depends on its size and distance from the mirror. Task B uses two rectangles with lengths $h_1$ and $h_2$ (notations shown in Task B, Figure 1) to simulate this perspective rule, where size diminishes with distance. Assuming the viewpoint is at the leftmost edge, the coarse-grained rule requires the left rectangle (closer to the viewpoint) to be longer than the right one (farther from the viewpoint), i.e., $h_1 > h_2$, while the fine-grained rule dictates rectangle lengths be proportional to their distances from the viewpoint, i.e., $l_1h_2 = l_2h_1$.\nTask C consists of two tangent circles of different radii, aiming to capture the relationship between shape/outlook and size as illustrated in non-spatial rule. The coarse-grained rule simply requires distinct radii for the two circles, i.e., $r_1 \\neq r_2$, while the fine-grained rule specifies a precise ratio between the radii, requiring $r_2 = \\sqrt{2}r_1$.\nTask D simplifies the non-spatial rule from scenario Size/Region- Color in Figure 1, where, in candle flame generations, colors transition from blue near the wick to yellow at the outer regions. We construct two such squares, with smaller squares positioned in the upper half and larger ones in the lower half of the image. The coarse-grained rule requires that the upper square's side length $l_1$ be smaller than the lower square's side length $l_2$, i.e., $l_1 < l_2$, while"}, {"title": "3.2. Experimental and Evaluation Setup", "content": "After designing synthetic tasks with well-defined inter-feature rules, we can systematically investigate the capability of DMs to learn these underlying relationships.\nExperimental Setup. In subsequent experiments, we train an unconditional DDPM (Ho et al., 2020) on four synthetic tasks. Unlike latent-space DMs (e.g., SD-3.5 Large), pixel-space DDPM makes the conformity of inter-feature relationships potentially simpler, as no additional compression-induced information loss occurs (Rombach et al., 2022; Yao & Wang, 2025). Following the training setting (Aithal et al., 2024), we fix the total timesteps at T = 1000 and employ the widely-used U-Net architecture (Ronneberger et al., 2015) as the denoiser. 4000, 2000, 2000, and 2000 samples are generated for synthetic task A, B, C and D, respectively, with an image size of 32 \u00d7 32. Additionally, in Appendix D.3, we explore more advanced architectures such as DiT (Peebles & Xie, 2023) and SiT (Ma et al., 2024), alongside larger synthetic datasets of 20000 and 40000 samples and higher image resolutions of 64 \u00d7 64. These factors enhance the training of DMs, thus leading to better alignment between generated and real data distributions (Chen et al., 2022; Benton et al., 2024; Chen et al., 2023) and enabling more effective learning of hidden rules. More experimental details are in Appendix D.1.\nEvaluation Method. To evaluate whether generated images follow the inter-feature rules, Figure 2 designs a three-step feature extraction pipeline: (1) Color-based Mask: Segment element masks (e.g., sun, pole, shadow in Task A) based on predefined color (HSV) ranges when synthesizing training data; (2) Elements Count: Apply contour detection based on masks to verify the presence of essential elements, marking images as Invalid if any are missing; (3) Feature Extraction: Extract key feature points (e.g., sun center, pole top/center and shadow endpoint in Figure 2) and compute geometric features of interest, such as horizontal sun-to-pole distance $l_1$, vertical sun-to-pole-top distance $h_1$, pole height $h_2$, shadow length $l_2$. All features are scaled to [0, 1] by dividing them by the image size to eliminate scale effects.\nWith these features, we can verify whether generated im-"}, {"title": "3.3. Experimental Results", "content": "For each synthetic task, we generate 2000 samples and report the evaluated results as follows:\nDMs' Success on Coarse-Grained Rules. Table 1 demonstrates that DMs rarely generate samples that violate the coarse-grained rules across all tasks. This observation aligns with expectations: generating samples that violate coarse-grained rules requires DMs to generate out of the (training) distribution (OOD) - an extrapolation challenge for DMs observed in prior work (Okawa et al., 2024; Kang et al., 2024). In Task A, for example, all training samples place the sun and shadow on opposite sides of the pole; violating this rule would require generating a never-seen mode with both elements on the same side.\nDMs' Failure on Fine-Grained Rules. While following coarse-grained rules only requires DMs to avoid unreasonable OOD generations, fine-grained rules are much harder, demanding accurate learning of the in-distribution training data. Figure 4 demonstrates the models' performance across four synthetic tasks, where deviations from the ground truth in linear fitting and the coefficient of determination $R^2$ below 1 indicate that DMs fail to fully capture the predefined fine-grained rules. Additionally, we observe that DMs struggle more with learning non-spatial rules, such as Task C, compared to spatial rules, such as Task A, as evidenced by worse linear fitting and smaller $R^2$. This discrepancy likely arises from the fact that non-spatial rules are more implicit and lack explicit cues, such as object positions and lengths,"}, {"title": "4. DMs' Failure from a Theoretical Perspective", "content": "This section provides theoretical explanations for our observed phenomenon - DMs' inability to effectively learn precise rules. Our analysis reveals that without prior knowledge on the hidden rules, DMs trained by minimizing the DDPM loss (Ho et al., 2020) exhibit a constant error in rule conformity, indicating that they cannot accurately learn the ground-truth rule.\nWe consider the following multi-patch data setup, which has been widely employed for theoretical analysis of classification (Allen-Zhu & Li, 2020; Cao et al., 2022; Zou et al., 2023; Lu et al., 2024), and recently for diffusion models (Han et al., 2024a).\nDefinition 4.1 (Data distribution with Inter-Feature Rules). Let u, v \u2208 Rd be two orthogonal feature vectors with unit norm, i.e., $||u|| = ||v|| = 1$ and $(u, v) = 0$. Let be a random variable with its distribution $D_{\\zeta}$ supporting on a bounded domain $[c_{\\zeta}, \\overline{c_{\\zeta}}]$ for some constants $0 < c_{\\zeta} < \\overline{c_{\\zeta}} < \\infty$. Each image data consists of multiple patches\nwhere\n$x = [x^{(1)T}, x^{(2)T}, ..., x^{(P)T}]^T$,\n$x^{(1)} = \\zeta u, x^{(2)} = (1 - \\zeta)v$,\nand $x^{(1)}, x^{(2)}$ are independent with the remaining patches.\nDefinition 4.1 specifies a inter-feature rule on the first two patches of the data, requiring that the norm of the first two feature patches sum up to one, i.e., $||x^{(1)} || + ||x^{(2)} || = 1$. Furthermore, we show such a rule will further lead to a structural constraint on the score function. Specifically, let $x_0 = [\\zeta u, (1 - \\zeta)v^T, x^{(3)T}, ..., x^{(P)T}]^T$ represent an input image. For arbitrary noise scedules {at, \u1e9et}, $x_t = a_tx_0 + B_t\\epsilon_t$ represents the noised image at timestep t. We derive the score function along the diffusion path as follows.\nTheorem 4.2. The score function is $\\nabla \\log p_t(x_t) = [\\nabla \\log p_t(x^{(1)}, x^{(2)}), \\nabla \\log p_t(x^{(3)}, ..., x^{(P)})^T]^T$, where\n$\\nabla \\log p_t(x^{(1)}, x^{(2)}) = \\frac{a_t}{\\beta_t^2} E_{D_{\\zeta}} [\\pi_t(\\zeta, x_t)] u$\nwhere $\\pi_t(\\zeta,x_t) = \\frac{N(x_t; \\mu_t(\\zeta), B_t^2 I_{2d})}{\\int_{\\zeta \\in D_\\zeta} N(x_t; \\mu_t(\\zeta), B_t^2 I_{2d})} $, $\\mu_t(\\zeta) = [a_t \\zeta u, a_t (1 - \\zeta) v^T]^T$.\nIt is clearly noted that the ground truth score (restricted to the first two patches) exhibits the following identity:\n$E_{D_{\\zeta}} [\\pi_t(\\zeta, x_t)] + E_{D_{\\zeta}} [\\pi_t(\\zeta, x_t)(1 - \\zeta)] = E_{D_{\\zeta}} [\\pi_t(\\zeta, x_t)] = 1$. (*)\nThen, we aim to investigate whether a score network, trained via DSM objective, can accurately conform to such a hidden rule (*). Specifically, we follow (Han et al., 2024a) and consider the following two-layer neural network model: $s_w(x_t) = [s_w^{(1)}(x_t),..., s_w^{(P)}(x_t)^T]^T$, with\n$s_w^{(p)}(x_t) = \\frac{1}{B_t^2} \\sum_{r=1}^m \\sigma (w_{r,t}^{(p)T} x_t) (w_{r,t}^{(p)}, u)$. (1)"}, {"title": "5. Mitigation Strategy with Guided Diffusion", "content": "Motivated by our finding that DMs can produce rule-conforming samples but instability, we mitigate this by a common technique, Guided DDPM, which introduces additional classifier guidance (Dhariwal & Nichol, 2021) during sampling. Specifically, we train the classifier $f_\\theta(x,t)$ through contrastive learning with constructed contrasting data pairs, where positive samples follow fine-grained rules while negative samples violate fine rules while maintaining coarse-grained compliance. The training objective is\n$L_{total} = L_{classification} + \\lambda \\cdot L_{contrastive}$, (3)\nwhere \\lambda is weight parameter, $L_{classification}$ is Cross-Entropy loss and $L_{contrastive}$ is NT-Xent loss (Sohn, 2016). More details on NT-Xent loss are in Appendix G.1. Then, following Dhariwal & Nichol (2021), gradients from $f_\\theta(x, t)$ are used to guide sampling toward fine-grained rule compliance.\nAdditionally, based on constructed contrastive data, we directly train a classifier in raw images to determine whether a generation satisfies fine-grained rules. We filter samples predicted as non-rule-conforming to ensure generation quality. This approach, called Filtered DDPM, which directly provides guidance based on the noise-free pixel space, can be seen as the upper bound for guided diffusion strategies."}, {"title": "5.1. Experiment Results", "content": "Setup. We use a U-Net classifier $f_\\theta(x,t)$ with guidance weight \\lambda = 1. Details of the data construction and training process are provided in Appendix G.1.\nResults. In addition to $R^2$, inspired by the theorical analysis in Section 4, we introduce Error, a metric capturing how well DMs learn hidden rules from variance and bias. Given the Ground Truth line $y = B_1x$ and the Estimation line $y = \\beta_1x + \\beta_0$ in Figure 3 and 4, Error is defined as:\n$Error := |B_1 - \\beta_1| + |\\beta_0| + \\sqrt{Var(y - y)}$. Bias Error  Variance Error (4)"}, {"title": "5.2. Discussions on the Limitation of Guided Diffusion", "content": "While guided and filtered diffusion provides some mitigation for rule learning, we acknowledge that this improvement is limited. The limited improvement stems from the inherent nature of our problem: unlike conventional classification tasks, the fine-grained rules that differentiate our contrastive samples exhibit subtle signals, making effective classifier training particularly challenging. In Appendix G.1, we provide additional experimental evidence that, even on such simple synthetic tasks, the classification accuracy on the test set remains between 60% and 80%, supporting the difficulty of precise classification in contrastive data.\nAdditionally, the effectiveness of this strategy relies on prior knowledge of fine-grained rules. In real-world scenarios, fine-grained rules are often difficult to accurately define and detect, making the construction of contrastive data impossible. We leave the solution to DMs' inability to learn fine-grained rules in real-world scenarios for future work."}, {"title": "6. Conclusion", "content": "This study evaluates DMs from the perspective of inter-feature rule learning, revealing through carefully designed synthetic experiments that DMs can capture coarse rules but struggle with fine-grained ones. Theoretical analysis attributes this limitation to a fundamental inconsistency in DMs' training objective with the goal of rule alignment. We further explore some common techniques, such as guided diffusion, to enhance fine-grained rule learning, but observe limited success. Our in-depth findings underscore the inherent difficulty of capturing subtle fine-grained rules, providing valuable insights for future advancements."}, {"title": "A. Low FID and Worse Inter-Feature Learning: A Gaussian Mixture Case", "content": "In this section, we provide a toy example based on the Gaussian Mixture Distribution to explain how low FID and incorrect inter-feature relationships can coexist. This supports the point that even though DMs may perform excellently on classical metrics such as FID, this does not necessarily mean they can perfectly learn the hidden inter-feature rules.\nConsider a 2-dimensional population, i.e., the true distribution p(x, y), which is a Gaussian Mixture Model (GMM) with two components as:\n$p(x,y) = F(\\mu_p, \\Sigma_p) = \\frac{1}{2} \\cdot N \\bigg( \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\bigg) + \\frac{1}{2} \\cdot N \\bigg( \\begin{bmatrix} -1 \\\\ -1 \\end{bmatrix}, \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\bigg)$. (5)\nwhere we can have\n$\\mu_p = \\frac{1}{2} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + \\frac{1}{2} \\begin{bmatrix} -1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\nand the covaraince matirx as\n$\\Sigma_p = \\sum_{i=1}^2 w_i (\\Sigma_i + (\\mu_i - \\mu_q)(\\mu_i - \\mu_q)^T)$\n$= 0.5 \\bigg( \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} + \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\bigg) + 0.5 \\bigg( \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} + \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\bigg) = \\begin{bmatrix} 1 & 1 \\\\ 1 & 2 \\end{bmatrix}$\nWe assume the estimated data distribution learned by DMs is a joint Gaussian distribution:\n$q(x, y) = N(\\mu_q, \\Sigma_q) = N \\bigg( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\end{bmatrix} \\bigg)$. (6)\nWith means and covariance matrices of true distributon p and estimated distribution q are identical, that is $\\mu_p = \\mu_q = [0, 0]^T$ and $\\Sigma_p = \\Sigma = \\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\end{bmatrix}$, we easily have the FID between p(x, y) and p(x, y) is computed as:\n$FID = ||\\mu_p - \\mu_q||^2 + Tr (\\Sigma_p + \\Sigma_q - 2(\\Sigma_p \\Sigma_q)^{1/2})$\n$= || \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} ||^2 + Tr \\bigg( \\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\end{bmatrix} + \\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\end{bmatrix} - 2 \\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\end{bmatrix} \\bigg)$\n$= 0$. (7)\nAlthough the FID is small (i.e., 0), the inter-feature relationships between x and y in true and estimated distribution are fundamentally different. In the true distribution, x and y are independent within each Gaussian component but exhibit dependence in the overall distribution due to the mixture of components. In the estimated distribution q(x, y), \u00ee and \u0177 are dependent with Cov(x, y) = 1. Therefore, low FID does not imply a correct recovery of the inter-feature rules."}, {"title": "B. Details and More Example on Real-Wold Hidden Inter-Feature Rules", "content": "Table 3 provides a detailed description of the prompts for each case in Figure 1 and Figure 7, including scenarios with inter-feature rules and the corresponding rules themselves. We also consider more DMs such as SDXL (Podell et al., 2023), Flux.1.1 Ultra (Labs, 2023), DALL\u00b7E 3\u2076 (Betker et al., 2023), and VAR-based (Tian et al., 2024a) text-to-image model Infinity\u2077 (Han et al., 2024b) in the evaluation. By comparing these rules, we observe that most mainstream DMs fail in some or all scenarios. For instance, in the Reflection/Refraction scenario, none of the DMs successfully generate plausible images: the reflected toy in the mirror faces the camera just like the real one, and the submerged bottle shows no refraction. Our evaluation covers both classic latent diffusion models (e.g., SD-3.5 Large) and the latest next-scale prediction-based diffusion models (e.g., Infinity). Surprisingly, none of them can perfectly handle these inter-feature relationships, highlighting the widespread limitation of DMs in this regard."}, {"title": "C. Details and More Example on Synthetic Tasks", "content": "This section presents supplementary details and examples regarding our synthetic datasets.\nTask A generates synthetic images featuring a simple outdoor scene composed of a vertical pole, a sun, and their corresponding shadow. The height of the pole is randomly selected within the range of [6.4, 12.8] pixels, which corresponds to [20%, 40%] of the total image size (32 \u00d7 32 pixels). The sun's horizontal position is sampled from two predefined distance intervals: far distances (0 \u2013 6 pixels or 26 \u2013 32 pixels) and near distances (10 \u2013 16 pixels or 16 \u2013 22 pixels), ensuring a varied distribution of sun locations. The shadow length is computed using the formula:\n$shadow\\_length = \\frac{pole\\_height \\times sun\\_distance}{sun\\_height - pole\\_height}$. (8)\nwhere the sun height is determined as twice the pole height, clipped within [9.6, 25.6] pixels (30%-80% of the image size). Colors for the sun, pole, and shadow are randomly selected from predefined HSV (Hue-Saturation-Value) ranges: Sun color (yellowish tones) has a hue in [0, 30], saturation in [100, 255], and value in [200, 255]. Pole color (blue-green tones) has a hue in [90, 150], saturation in [100, 255], and value in [100, 255]. Shadow color (dark tones like black, brown, gray) has a hue in [0, 180], saturation in [0, 50], and value in [50, 150].\nTask B generates synthetic images containing two rectangular objects placed within a 32 \u00d7 32 pixel space. The first rectangle's position and size are determined as follows: its leftmost position l\u2081 is chosen randomly from the range [0, 9.6] pixels (30% of the image width), and its height l\u2082 is chosen randomly from [6.4, 19.2] pixels (20% to 60% of the image height). The color of the first rectangle is randomly selected from a yellowish hue range with hue [0,30], saturation [100, 255], and value [200, 255] in HSV space. The second rectangle's position is determined by h\u2081, which is chosen randomly within a range dependent on l\u2081. Specifically, h\u2081 is sampled from the range [l\u2081 + 6.4, 25.6] pixels (ensuring h\u2081 > l\u2081). The height of the second rectangle h\u2082 is calculated based on the first rectangle's height l\u2082, ensuring the relation l\u2081h\u2081 = h\u2082l\u2082. The color of the second rectangle is chosen randomly from a blue-green hue range with hue [90, 150], saturation [100, 255], and value [100, 255] in HSV space.\nTask C generates images containing two circles: one large and one small. The large circle's diameter is randomly chosen between 10% and 30% of the image size, and the small circle's diameter is determined to be \u221a2 times the diameter of the large circle. The colors of the circles are randomly selected from predefined color ranges in the HSV color space. Specifically, the large circle is assigned a color from the blue-green hue range, with hue values between 90 and 150, saturation between 100 and 255, and brightness between 100 and 255. The small circle is assigned a color from the yellowish hue range, with hue values between 0 and 30, saturation between 100 and 255, and brightness between 200 and 255. The circles are randomly positioned such that they are adjacent to each other-either on the left, right, top, or bottom of the large circle."}, {"title": "D. More Synthetic Tasks Setup and Results", "content": "This section provides additional details to complement the experimental results in Section 3.3. Notably, to ensure more accurate quality assessment of generated images, we upscale the 32 \u00d7 32 images to 128 \u00d7 128 during evaluation. This"}, {"title": "D."}]}