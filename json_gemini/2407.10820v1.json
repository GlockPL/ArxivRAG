{"title": "Enabling MCTS Explainability for Sequential Planning Through Computation Tree Logic", "authors": ["Ziyan An", "Hendrik Baier", "Abhishek Dubey", "Ayan Mukhopadhyay", "Meiyi Ma"], "abstract": "Monte Carlo tree search (MCTS) is one of the most capable online search algorithms for sequential planning tasks, with significant applications in areas such as resource allocation and transit planning. Despite its strong performance in real-world deployment, the inherent complexity of MCTS makes it challenging to understand for users without technical background. This paper considers the use of MCTS in transportation routing services, where the algorithm is integrated to develop optimized route plans. These plans are required to meet a range of constraints and requirements simultaneously, further complicating the task of explaining the algorithm's operation in real-world contexts. To address this critical research gap, we introduce a novel computation tree logic-based explainer for MCTS. Our framework begins by taking user-defined requirements and translating them into rigorous logic specifications through the use of language templates. Then, our explainer incorporates a logic verification and quantitative evaluation module that validates the states and actions traversed by the MCTS algorithm. The outcomes of this analysis are then rendered into human-readable descriptive text using a second set of language templates. The user satisfaction of our approach was assessed through a survey with 82 participants. The results indicated that our explanatory approach significantly outperforms other baselines in user preference.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) is now intricately woven into the fabric of our daily lives. Al's influence extends into virtually every sector, redefining the way we work, learn, and interact with the world around us. For example, AI algorithms play a pivotal role in personalized recommendations, virtual social networks, medical diagnosis, and transportation management. As AI becomes ubiquitous, explainability of the underlying algorithmic processes is imperative for fostering trust and ensuring transparency in automated decision-making processes, thereby enabling stakeholders to comprehend and effectively scrutinize autonomous and data-driven systems. The importance of developing transparent and explainable AI systems has been highlighted and, to some extent, mandated in public policy-both the European Union and the US have highlighted its importance through legislation [6, 24].\nThe field of Explainable AI has acknowledged this challenge and developed several novel and effective approaches. Indeed, there are several well-known approaches for interpreting learning-based methods, i.e., methods that use historical data to make estimates about a dependent variable as a function of independent variables, such as"}, {"title": "2 Sequential Planning in Public Transit", "content": "In this section, we introduce the underlying Markov Decision Process (MDP) that models our specific use case in public transit, and defines the search space for our MCTS implementation. Following related work [16, 33] in using MDPs for transit planning, we define a transit planning task \u03a0 as the tuple $(S, A, T, R, \\gamma)$, where S is the set of states of the environment, A the set of available actions for the planning agent, and $T(s_t, a_t, s_{t+1})$ defines the probabilities of moving from state $s_t$ to state $s_{t+1}$ under action $a_t$, where every transition to a next state implies a new predicted request. The reward function $R(s, a)$ and the discount factor \u03b3 assign a numerical reward for taking action a in state s.\nA state s in the transit planning task is defined by the tuple $(\\Theta, r, V, \\{R^{|V|}\\}) $, where \u0398 denotes the current route plans for all vehicles, r is an outstanding request, V denotes the locations of all vehicles, and $\\{R^{|V|}\\}$ is a list of assigned requests to each vehicle [33]. We denote the state at time t with $s_t = (\\Theta_t, r_t, V_t, \\{R^{|V|}_t\\})$. An action $a_t$ at time t involves assigning an outstanding trip request $r^i$ to a vehicle $v^j$. A transit service request $r^i$ is defined by: the time the request was made $t_r^i$, requested pickup time $t_p^i$, requested drop-off time $t_d^i$, pickup location $l_p^i$, and drop-off location $l_d^i$, as well as its current status $u^i \\in \\{waiting, assigned, in-transit, dropped-off\\}$. Each transit vehicle $v^j \\in V$ has a fixed capacity $c^j$ and an occupancy $p^j$ that varies based on its current route plan. The designated route plan of each vehicle is denoted as $\\Theta^j$, specifying a list of future locations that the vehicle is scheduled to visit.\nWe use Monte Carlo Tree Search (MCTS) [19] as the decision making algorithm, mapping the current state $s_t$ to the next action $a_t$ with the help of sampling-based planning over the MDP model described above. Our transit scenario operates within a dynamic environment where requests can be made at any moment. When a new request r is initiated by a passenger, the planning algorithm is engaged on-the-fly to generate a new action to accommodate this request. This time point is referred to as a \"decision epoch\" [18, 33], where each decision epoch addresses a new planning problem independently. Specifically, MCTS seeks to find or approximate the optimal action $a_t$ that allocates the current request to one of"}, {"title": "3 CTL-Based Explainable MCTS", "content": "The goal of CTL [11]-based explainable MCTS is to help users (e.g., transit operators) understand the relationship between the system state, the desired outcome, non-negotiable constraints, and the route plan produced by MCTS. In each decision epoch of the planning task \u03a0, the user is presented with the current system state $s_t$ and the action $a_t$ proposed by MCTS. The user also has knowledge of a pre-defined set of hard constraints that the recommended actions must consistently adhere to. Given this information, the user poses a set of queries, denoted as $Q = \\{q_1, q_2, ...\\}$. Following the running example in Figure 1, one such query can be: \"Why wasn't the passenger assigned to the green vehicle's route?\" (Q2, Table 1).\nGiven the query, the explainer's goal is to provide explanations, denoted as E, concerning recommended actions $a_t$ in response to specific queries. The explainer must deduce answers to these questions based on criteria that include the goal of the search, the stringent state requirements such as vehicle capacity violations, and user-proposed criteria like additional efficiency requirements. In Figure 3, we provide a overview of this process. In the following sections, we describe the detailed methods employed to derive this response from the MCTS search tree."}, {"title": "3.1 Formulating User Queries", "content": "Dispatchers lacking technical knowledge in MCTS can inquire about various aspects of the planning result, such as constraint compliance, route plan efficiency, and its soundness. To address these varying interests, we organize user queries into three distinct categories. Table 1 demonstrates one example query of each category. In principle, our explanation framework can handle any query that can be structured in this way. In our examples, free variables are indicated by brackets. Furthermore, these queries are initially left empty and are associated with predefined criteria such as efficiency and soundness. When a user submits a query in natural language, the first step involves identifying the completed variables within the query. For instance, if a query asks about an alternative action, we map the free variables in the query to the corresponding state variables of the alternative action in question. Additionally, we incorporate these state variables and the query criteria into automatically generated CTL formulas, denoted as $\\Phi = {\\phi_1, \\phi_2, ...\\}$. In Section 3.3, we describe how variables in queries are processed, incorporated into CTL formulas, and how their handling differs within the search tree."}, {"title": "3.2 CTL Verification and Quantitative Evaluation", "content": "Before discussing how specific queries and criteria are translated into CTL, we detail our approach to employing the branching-time logic extensively applied in model checking [8]. The CTL state quantifiers evaluate the breadth of the computation tree, where the A (for all) operator examines every child of a given state, and the E (exists) operator focuses on at least one child, requiring the formula's validity for any one of them. In contrast, the path quantifiers consider the tree's depth. The F (future) operator is concerned with a condition's eventual occurrence and the G (globally) operator requires that a condition holds true in all future states. For a CTL formula to be well-formed, a state quantifier must be followed by a path quantifier, as defined in Definition 1.\n\\textbf{Definition 1} (Syntax of CTL formulas).\n$\\Phi ::= \\top | \\bot | p | \\neg \\Phi | \\Phi \\lor \\Phi | AX \\Phi | EX \\Phi | AF \\Phi | EF \\Phi | AG \\Phi | EG \\Phi$"}, {"title": "3.3 Mapping Queries to CTL Formulas", "content": "Factual Queries - \"Why?\" Factual queries relate to reasons that indicate why a particular action is recommended [12, 29]. We specifically address the following types of questions in which individuals seek to determine whether the specific user-specified criteria will be met. For instance, a common question is, \"Based on the current vehicle assignment, is it expected that passenger 1 will be dropped off too late?\" For such queries, we employ the following structured language template: \"Based on the current vehicle assignment, is it expected that [passenger number] will be [action] [time]?\", where the action can be either dropped off or picked up, and time corresponds to either late or early. For the example query, we decompose the template-based query into the following tuple: [passenger 1, drop off, late]. As shown in Table 1, the specification type for this factual query is \"Efficiency\", and the queried state variable is \"drop off\" or $t_d$. Thus, the corresponding CTL formula regarding efficiency would be $\\phi_1: AG (test \\leq (t_p + t_d + t_{allowed}))$, where the variable $t_{est}$ represents the estimated time required for dropping off passenger 1. The CTL operator AG checks for all states and all future paths starting from the root. The relation $\\leq$ in the formula is used to determine whether passenger 1 will arrive late, based on the comparison of the estimated travel time with the allowed time window.\nContrastive Queries - \"Why Not?\" The second type of query in our explainer involves alternative plans specified by technical users. In this case, we aim to explain why the planning algorithm did not recommend the alternative action $a_t$ as proposed by the user. We formulate the type of user queries about alternative plans with the following language template: \"Why wasn't [passenger] assigned to [another vehicle] located at [location]?\", where the missing variables can be represented as [passenger, another vehicle, location]. For example, a common query is \"Why wasn't passenger 1 assigned to the red vehicle (which is closer)?\" In line with the nature of contrastive queries, the evaluation criteria for this query involve asking about potential violations of hard constraints and efficiency. More precisely, when querying about an alternative action $a_t$, we can provide two types of reasons why MCTS did not produce $a_t$. First, $a_t$ could be infeasible due to violations of hard constraints. Second, $a_t$ could be less efficient than the near-optimal solution $a_t$ generated by MCTS. This evaluation includes several CTL formulas: $\\phi_1: AG, (t_{est} \\leq (t_p + t_d))$, which examines efficiency in pickup times; $\\phi_2: AG, (t_{est} \\leq (t_d + t_{allowed}))$, which addresses efficiency in drop-off times; and $\\phi_3: AG, (v_o \\leq v_c)$, which checks for violations of capacity constraints. Initially, the search tree nodes are checked against these hard constraints. If no violations are found, the efficiency formulas are then evaluated.\nQueries with Tree Expansion - \"What If?\" MCTS focuses on exploring the most promising paths, intentionally leading to less exploration of other, lower-scoring branches. Therefore, while extracting information from the search tree suffices for explaining recommended plans, it can provide limited insights for alternative plans, which might not have been explored enough [4]. For example, consider a user querying about an alternative action, such as assigning request 1 to vehicle 2 in Figure 1. In this case, while there are no direct violations, the user might question whether the algorithm considers the action less optimal due to insufficient exploration depth. To accommodate such queries, the MCTS explainer enables users to query about a broader set of potential actions that were neither recommended nor sufficiently explored. We formulate them as \u201cCan you tell me more about assigning the [passenger] to [another vehicle]?\u201d. For instance, a user may ask, \u201cCan you tell me more about assigning passenger 1 to the green vehicle?\u201d In this format, the free variables are encapsulated in the tuple: [passenger 1, green vehicle]. This query type requires additional search and exploration to provide comprehensive explanations [5], as illustrated in Figure 4. A pseudocode of this process is provided in Appendix A1. At a high level, to address such queries, we locate the under-explored node $N(s,a)$ in the original tree, where a is the alternative action being queried and s is the state in the query. We then create a sub-tree by designating the previous path as the parent node and execute MCTS, continuing to explore possible scenarios and outcomes in response to the query."}, {"title": "3.4 Generating Natural Language Explanations", "content": "As outlined in Algorithm 1, the explainer is invoked once at the end of each decision epoch if there are user queries. Following the steps specified in Algorithm 2, the CTL-based explainer checks the search tree against each formula upon formulating the set of CTL formulas \u03a6. Each explanation in $E_i$ is obtained through CTL specification checking and quantitative evaluation. Specifically, at line 13 of Algorithm 2, we derive two key quantitative insights from the aggregated list of violations across all expanded nodes within the MCTS tree. First, we compute the violation percentage and the average degree of these violations. Second, for timing-related violations, we compute the temporal range of these violations, identifying both the earliest (lower bound) and latest (upper bound) occurrences. These results are then translated into a set of natural language explanations, denoted as \u039e = {$\u03f5_1,\u03f5_2,...$}. The details of language templates are provided in Appendix A2.6. An example of these templates to address the query in the running example is: \"Based on the extensive set of scenarios examined by MCTS, there's a chance that the passenger might encounter a delay in their drop-off time. This anticipated delay averages around [23 minutes]. The primary reason for this delay is that the proposed vehicle is expected to make stops at about [4 other locations] prior to reaching the passenger's drop-off point. However, the delay can be as short as [19 minutes] or extend up to [27 minutes]. The percentage of times the suggested vehicle doesn't meet the desired drop-off time is about [10%].\""}, {"title": "4 Evaluation", "content": "We evaluate the effectiveness of the explanations generated by our explainable framework in the context of five distinct vehicle routing scenarios through an IRB-approved user study. We aim to assess the overall quality and user-friendliness of the natural language explanations from the perspective of end users, comparing them with two other baseline methods. Additionally, the study aims to understand the participants' preferences among three types of queries."}, {"title": "4.1 Study Design", "content": "Testing Environment. Participants in this questionnaire-based user study first receive a comprehensive overview of the paratransit problem, which is designed to familiarize them with the key concepts. Then, participants are presented with five different scenarios of the route-planning problem. Each scenario involves different conditions and variables relevant to the problem, simulating real-world traffic planning situations. In the next step, the decision computed by the MCTS planning algorithm is shown to the participants. Finally, participants review three types of MCTS explanations regarding this decision. For each type of explanation, participants are asked to rate their quality using a pre-defined questionnaire. For all questions that require a rating, participants will be prompted to give their assessment using a 5-point Likert scale, where a rating of 1 is strongly disagree and a rating of 5 is strongly agree. Illustrative examples of the questionnaire is shown in Appendix A2.5.\nParticipants. The complete details and statistics of the participants were provided in Appendix A2.2. We recruited a total number of 82 eligible participants. Among the participating clients, 40.3% of the participants reported having no knowledge of the MCTS algorithm, while 32.8% indicated they have a basic understanding. The remaining 26.9% of participants are either very familiar with the algorithm or have hands-on experience with MCTS."}, {"title": "4.2 Explanations Quality Assessment", "content": "Evaluation Metrics. We designed the questionnaire following the explainable artificial intelligence metrics proposed by Hoffman et al. [17]. Specifically, we evaluate the following four criteria: understandability, satisfaction level, completeness, and reliability employing the following survey questions: (1) Understandability: I understand this explanation of the planning algorithm result. (2) Satisfaction: This explanation of the planning result is satisfying. (3) Completeness: This explanation of the planning result seems complete. (4) Reliability: This explanation helps me to assess the reliability of the planning algorithm.\nBaselines. Our method is benchmarked against two baselines. The first baseline is designed to represent the information typically displayed in current route dispatching interfaces, while the second focuses on the states, actions, and scores stored within the MCTS search tree. Baseline (1): A map visualization that integrates passenger and vehicle locations, with indications of rule violations. Baseline (2): A detailed visualization depicting the states, actions, and scores within the search tree.\nAssessment on explanation quality by query type. We assess the effectiveness of each query type using the following questions [17]: (1) Detail: This explanation of the planning result has sufficient detail. (2) Irrelevance: This explanation of the planning result contains irrelevant details. (3) Accuracy: This explanation says how accurate the planning algorithm is. The second assessment question focuses on the irrelevance of the information presented in the explanations. Therefore, in our evaluation metrics, for the categories of detail and"}, {"title": "4.3 Result Analysis", "content": "Comparison of CTL-based explainable MCTS with baselines. In Figure 5, we present a comparative evaluation and analysis across all scenarios. Particularly, subplot (a) aggregates and averages the Likert ratings for each evaluation criteria across all scenarios. The result shows that, on average, our explanations outperformed both baselines in all four evaluation criteria. While participants considered baseline 1 with map visualization as more understandable and satisfying than baseline 2 with search tree visualization, the results show that the natural language-based explanations of our approach were superior in terms of understandability, satisfaction, and completeness. Furthermore, these explanations were most effective in helping participants to assess the reliability of the route planning algorithm. Although approximately 60% of the participants have a basic or higher level of understanding of the MCTS algorithm, our proposed approach significantly outperformed the search tree visualization baseline in all four evaluation criteria. This performance improvement can be observed even with a technically informed participant group. Subplots (b)-(f) in Figure 5 showcase the results for each individual scenario. Our proposed method outperforms the other two baseline approaches across all evaluation criteria in all scenarios. Among these criteria, the most significant improvement offered by our method is in aiding participants to evaluate the reliability of the MCTS route planning algorithm. This improvement is likely attributable to the comprehensive level of detail provided by our method, as further shown by the subsequent evaluations focusing on each type of query.\nEvaluation of each query type. In Table 2, we present the average Likert ratings for three key evaluation criteria across all five scenarios. Note that in assessing irrelevance, a lower rating is more favorable. Across all five scenarios, the result reveals that contrastive queries are considered as having the most sufficient detail, particularly in explaining why certain requests were not assigned to a vehicle. Moreover, participants rated the factual queries as the most relevant, highlighting their ability to convey essential information without including unnecessary details. About the evaluation of the accuracy of the MCTS algorithm, participants showed a preference for queries that require tree expansion. This preference suggests that they find the information through additional search, beyond what is available in the original search tree, is helpful in further understanding the accuracy of MCTS.\nUser preferences by technical background. Data in these figures reveal that our explainer received higher ratings across all four evaluation criteria from both user groups. Notably, the advantage of our explainer is more pronounced in the feedback from non-technical users, indicating its effectiveness and adaptability in meeting the needs of users with little to no technical expertise."}, {"title": "4.4 Discussion", "content": "Learning curve. In Figure 5, we observe a trend where the performance gap between our proposed method and the two baseline methods widens progressively from scenario 1 to scenario 5. This trend could be attributed to participants experiencing a learning curve in using our system and gaining increased familiarity with the route planning task. Initially, the map visualization might seem adequate for understanding why a particular request is assigned to a vehicle, especially for non-technical users. However, as participants look deeper into the more detailed natural language explanations provided by our explainer, they begin to understand more about the complexity of the scenarios, which is often more complicated than initially perceived. The increasing complexity of the scenarios, especially in the last two tasks where the number of paratransit vehicles grows, likely further emphasizes the superiority of our proposed method over the map visualization. Interestingly, we observe that participants' comprehension of the search tree visualization does not show marked improvement with increased exposure. This insight underscores the need for explanations that can adapt to the complexity of the scenario and the user's growing understanding.\nAdditional feedback. To deepen our understanding of user preferences regarding the explanations provided, we included additional open-ended questions at the end of the survey. For instance, one participant positively highlighted our proposed system, stating that it \u201cprovides me more useful information on how to interpret what the algorithm is doing.\" This feedback emphasizes the effectiveness of our system in enhancing user comprehension of the algorithm's processes. Several participants highlighted the effectiveness of our explainer in clarifying the routing decisions in scenario 5, particularly why the closer vehicle with ID 1 was not chosen, while a more distant vehicle with ID 3 was assigned instead. For instance, one participant remarked, \"I like the solution that was given this time; it clearly explains that vehicle 3 should be taken over vehicle 1.\" These comments underscore the explainer's ability to provide reasons behind complex decision-making processes in a comprehensible way."}, {"title": "5 Related Work", "content": "Explainable Al research has its roots in the need to comprehend, trust, and enhance AI algorithms [3]. Recent studies have highlighted the importance of assisting human users in comprehending and trusting the decisions made by a wide range of models, including regression trees, classification models, and neural networks (e.g. [13, 26, 15, 2]). In general, our approach is situated within the broader context of prior research centered on explaining plans and decisions generated by computer algorithms. While there is currently no comprehensive taxonomy for this research domain, we can broadly categorize these efforts as follows.\nThe first category is related to result-oriented explanations, where the objective of explainability algorithms is to reveal the rationale behind a specific decision post hoc by considering the attributes of that decision. Previous work, such as Langley [21], discusses the desired behaviors of post-hoc explainability modules at a high level, encompassing features like explaining the objectives of the planning task and presenting alternative plans. A comprehensive survey, Sreedharan et al. [31], formally defines distinct primary considerations of explainability systems. These considerations are roughly categorized based on the intended audience (the \"explainee\") and the nature of the planning results, which can encompass plans or policies. Furthermore, Fox et al. [14] identify four key types of questions that should be addressed by an explanation system. They illustrate the practical application of the system with two examples, demonstrating how it should function in practice. Nonetheless, the majority of these papers discuss post-hoc explainability systems in a general manner. They often lack specific examples and use cases that showcase explanations in action. Thus, the development of meaningful solutions motivated by real-world problems remains a significant challenge. In contrast, our work is the first to develop an explanation framework for a public transportation-related planning system in the real world, and evaluates the system with human users.\nThe second category aims to generate explanations that are included as a sub-goal of the planning algorithm. For instance, the explainability feature may influence the planning process, where the objective is to also account for system interactions with other system components [32]. Chakraborti [9] views the planning problem as a system with two components: the first component is the AI algorithm, and the second component is the human agent. The goal of the AI algorithm is not to make decisions but to provide suggestions to the human. Similar approaches can also be found in the field of multi-agent reinforcement learning (MARL). Boggess et al. [7] focus on explaining MARL policies by providing policy summaries in natural language. To ensure comprehensive explanations for user queries, their system conducts additional guided rollouts of the policy to generate informative explanations. Our work shares similar practices in providing explanations for under-explored tree branches with the help of additional computation. However, our approach not only generates more fine-grained explanations leveraging CTL and quantitative evaluations, but also derives explanations from the search tree of an online planner as opposed to a fixed RL policy."}, {"title": "6 Conclusions", "content": "In this study, we present a CTL-based explainer specifically designed for the MCTS algorithm, focusing on the paratransit route planning application. Our explainer classifies user queries into three categories: factual queries, contrastive queries, and queries requiring path explanations. It then dynamically generates CTL formulas based on the free variables, the type of query and its specifications. After checking different branches of the existing MCTS search tree or even the results of newly initiated searches, the explainer converts the outcomes of its specification verification into natural language explanations with the help of language templates. This approach is designed to enhance the usability of the system for both technical and non-technical audiences. The effectiveness of our method is evidenced by a user study involving 82 participants, which shows notable improvements in user comprehension and satisfaction compared to two baseline methods. We believe that our explainer is versatile enough to be seamlessly adapted to other MCTS implementations in various application fields, broadening its potential for impact."}, {"title": "Technical Appendix", "content": "In this technical appendix, we include supplementary information about our approach and details of the survey experiment. Section A1 expands upon Section 3 of the main text, presenting further details such as the conversion of queries to CTL and a pseudocode. In Section A2, we present our survey design, interface, query and answer templates utilized in the questionnaire, and additional quantitative results."}, {"title": "A1. Additional Details on Section 3", "content": "In Table 3 and Table 4, we present additional examples demonstrating how queries related to transit planning tasks can be formulated and translated into CTL formulas, which supplement the information provided in Table 1 of the main text. In Algorithm 3, we provide the pseudocode for generating additional search processes for queries that requires tree expansion."}, {"title": "A2. Additional Details on Survey Design", "content": "A2.1. Study Procedures\nOur study included the following procedures:\n\u2022 Participants would review a description of the paratransit problem, which is a specialization of the vehicle routing problem and includes serving trip requests from passengers.\n\u2022 Participants were given: several scenarios, one after the other; the decision computed by the planning algorithm; multiple types of explanations as to \"why\" the algorithm chose the decision. One of the types of explanations is interactive, i.e., they could query the algorithm through a set of pre-defined questions.\n\u2022 Participants' goal was to evaluate which form of explanation helps them the most. We asked them to rate the explanations through a pre-defined questionnaire.\nA2.2. Participants\nWe recruited a total number of 82 eligible participants. We identify and contact potential participants via a mass email network and institute Slack messages. Among the participating clients, 36.9% identified as women, 58.5% as men, and 4.6% preferred not to disclose their gender. Additionally, 73.1% of the participants have a basic understanding of public transportation but lack familiarity with the para-transit domain, while 26.9% are knowledgeable about or have experience with para-transit and vehicle routing. Regarding their technical expertise with the MCTS algorithm, 40.3% of the participants reported having no knowledge of the algorithm, while 32.8% indicated they have a basic understanding. The remaining 26.9% of participants are either very familiar with the algorithm or have hands-on experience with MCTS."}]}