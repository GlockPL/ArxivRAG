{"title": "Learning to Synthesize Compatible Fashion Items Using Semantic Alignment and Collocation Classification: An Outfit Generation Framework", "authors": ["Dongliang Zhou", "Haijun Zhang", "Kai Yang", "Linlin Liu", "Han Yan", "Xiaofei Xu", "Zhao Zhang", "Shuicheng Yan"], "abstract": "The field of fashion compatibility learning has attracted great attention from both the academic and industrial communities in recent years. Many studies have been carried out for fashion compatibility prediction, collocated outfit recommendation, artificial intelligence (AI)-enabled compatible fashion design, and related topics. In particular, AI-enabled compatible fashion design can be used to synthesize compatible fashion items or outfits in order to improve the design experience for designers or the efficacy of recommendations for customers. However, previous generative models for collocated fashion synthesis have generally focused on the image-to-image translation between fashion items of upper and lower clothing. In this paper, we propose a novel outfit generation framework, i.e., OutfitGAN, with the aim of synthesizing a set of complementary items to compose an entire outfit, given one extant fashion item and reference masks of target synthesized items. OutfitGAN includes a semantic alignment module, which is responsible for characterizing the mapping correspondence between the existing fashion items and the synthesized ones, to improve the quality of the synthesized images, and a collocation classification module, which is used to improve the compatibility of a synthesized outfit. In order to evaluate the performance of our proposed models, we built a large-scale dataset consisting of 20,000 fashion outfits. Extensive experimental results on this dataset show that our OutfitGAN can synthesize photo-realistic outfits and outperform state-of-the-art methods in terms of similarity, authenticity and compatibility measurements.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the fashion sector has undergone a proliferation in economic terms. According to a business report\u00b9 from Statista.com, the economy is expected to maintain an approximate annual growth rate of 7.2% in the future. A Mck-"}, {"title": "II. RELATED WORK", "content": "This research falls into the field of fashion learning, which has a large existing body of literature. In this section, we review related works on image-to-image translation, fashion compatibility learning, and fashion synthesis. We also highlight the features of this research in comparison to those of prior works.\nImage-to-Image Translation. This is an important task in computer vision. A model takes an image as input and learns a conditional distribution of the corresponding image with a mapping function. There are many applications for this task, such as image colorization [2], image style transfer [16], super-resolution [12], and virtual try-on [14], [17]. Numerous previous studies have suggested that GANs [1] are capable of producing realistic synthesized images via image-to-image translation. Existing GAN-based translation methods can be roughly divided into two categories: supervised and unsupervised approaches. Using a supervised method, Isola et al. [2] proposed a Pix2Pix translation framework to alleviate blurring in this task. Later, Wang et al. [3] introduced an improved Pix2Pix model with the aim of achieving more stable and realistic image generation in a coarse-to-fine manner. Using an unsupervised method, Zhu et al. [4] proposed a cycle consistency loss to handle a lack of paired images. Subsequently, Huang et al. [5] addressed the latent space of image samples using a composition of style and content code, and used two separate encoders to disentangle these components. Lee et al. [6] also disentangled the latent space into a shared content space and an attribute space for each domain. In a later study, Choi et al. [7] extended the concepts of style code and content code, employing a multi-layer perceptron (MLP) to synthesize a diverse range of style codes and injecting them into a decoder to synthesize various images.\nFashion Compatibility Learning. With the increasing popularity of online stores, fashion recommendation is now playing an essential role in online retail. Fashion compatibility learning is an important aspect of fashion recommendation, and researchers have adopted metric learning to predict compatibility. Each fashion item in the same outfit is firstly embedded into a shared space, and the compatibility between items is then evaluated based on the distance between them. A shorter distance or a higher similarity indicates better compatibility, and vice versa. To measure the compatibility between items, McAuley et al. [18] proposed a method for comparing the distance between the features extracted by a pre-trained CNN. Veit et al. [19] then used a SiameseNet to extract visual features to compare the distance between items. These methods regarded the different types of fashion items as the same, and handled them in an embedding space. In order to keep different categories of fashion items with different mappings into embeddings, Vasileva et al. [20] tackled this problem by learning the similarity and compatibility simultaneously, in different spaces, for each pair of item categories. Another inspired idea was to regard the fashion items in the outfit as a sequence from the perspective of human vision. Han et al. [21] adopted Bi-LSTM to learn the compatibility of an outfit in the form of a sequence. The other mainstream idea that has emerged is the use of graph-based networks to address the issue of compatibility, and these methods have attracted the attention of several researchers. In particular, Cui et al. [22] and Li et al. [23] employed graph convolutional networks to model the compatibility problem. In this task, fashion compatibility is a crucially important perspective for generating an outfit. In our OutfitGAN, we use Bi-LSTM in our implementation of collocation classification in order to guide the compatibility of the generated items.\nFashion Synthesis. Due to the ever-increasing demand for fashion applications, fashion synthesis has started to become an important aspect of the field of computer vision [24]. Fashion synthesis includes virtual try-on, pose transformation and the synthesis of compatible fashion items. In the field of virtual try-on, Han et al. [14] employed a thin plate spline (TPS) and a GAN to synthesize new images, given images of the user's body and the target clothing. Subsequently, a new model called characteristic-preserving image-based virtual try-on network (CP-VTON) [25] was proposed, which included a geometric matching module that could improve the spatial deformation in comparison to TPS. Zhu et al. [13] proposed FashionGAN to synthesize clothes on a wearer while maintaining consistency with a text description. In addition to virtual try-on, pose transformation is also an important task in fashion synthesis. A model takes a reference image as input and a target pose based on the key points of the human body, and aims to synthesize a pose-guided image of the person while retaining the personal information of the reference image. A network called PG2"}, {"title": "III. OUTFITGAN FOR THE GENERATION OF MULTIPLE FASHION ITEMS", "content": "In this section, we first formulate our research problem and give the descriptions and definitions needed for outfit generation. We then present the entire OutfitGAN framework. Finally, the implementation details of our proposed models are discussed."}, {"title": "A. Problem Formulation", "content": "In general, previous fashion compatibility learning methods [18], [19], [20], [21], [22], [23] have focused on discriminating the collocation given a set of fashion items. In contrast, generative models allow us to synthesize a entirely new outfit as well as maintaining the collocation for compatibility learning. In this work, we focus on synthesizing a set of compatible items based on a given fashion item in order to compose a complete outfit. Formally, let $O = [O_1,\\ldots, O_i,\\ldots,O_N]$ denote an outfit, where $O_i$ is the $i$-th fashion item in the outfit arranged in a fixed order based on its categories, i.e., from top to bottom according to perspective of human vision, e.g., [upper clothing, bag, lower clothing, shoes]. $N$ represents the number of fashion items in an outfit. In addition, each fashion item $O_i$ is associated with a mask that indicates the outline of $O_i$. For each $O_i$, let $Mask_i$ be the corresponding mask of $O_i$. Our task is to synthesize a complementary outfit set $O$ for a user based on a given fashion item $O_k$ and reference masks $[Mask_1,\\ldots, Mask_{k-1}, Mask_{k+1},\\cdots, Mask_N]$, which represent the user's rough idea of the outlines of the newly synthesized outfit items. Here, the reference masks may be given by the user, selected by the user from a candidate dataset containing various outlines of fashion items, or produced automatically by a pre-trained generative model."}, {"title": "B. OutfitGAN", "content": "Based on the problem formulation presented above, we design a new generative framework called OutfitGAN to accomplish the task of outfit generation. The detailed structure of OutfitGAN is illustrated in Fig. 2. In particular, Fig. 2(a) shows three key modules: an outfit generator $G$, an outfit discriminator $D$, and a collocation classification module CCM. For clarity, the outfit generator and collocation classifier that make up the key components of our OutfitGAN are described firstly. The training losses of our model are shown in Fig. 2(b), and are elaborated later in this section.\n1) Outfit Generator: To synthesize a set of complementary items to make up an outfit, our framework needs to learn a mapping function from extant fashion items to new synthesized ones, by considering the compatibility between fashion items. To accomplish this, we train an outfit generator $G$ to translate a given fashion item into multiple collocated ones. In particular, to synthesize a whole outfit that includes $N$ fashion items, we introduce an outfit generator $G$, which includes $(N-1)$ item generators to synthesize a set of fashion items, conditioned on a given item. The $i$-th item generator $G_i$ includes three components, as shown in Fig. 2(a): an encoder $Enc_i$, a decoder $Dec_i$, and a semantic alignment module SAM. Here, we employ $Enc_i$ and $Dec_i$ in a similar way to a general image-to-image translation generator [3]. The detailed structures of $Enc_i$ and $Dec_i$ can be found in [3]. The SAM was developed to capture the correspondence between the input and output images. In a compatible outfit, harmonizing"}, {"title": "2) Collocation Classification Module:", "content": "In this subsection, we describe the CCM, which is used to model the compatibility prediction in order to supervise the compatibility during the outfit generation process.\nMore specifically, to ensure that the synthesized outfits fall into the collocation domain, we pre-train a CCM (see Section III-C3), which is leveraged to identify whether or not a synthesized outfit is compatible. During the training of OutfitGAN, we fix the parameters of the pre-trained CCM to supervise the compatibility of synthesized items. If the synthesized items are compatible, the CCM applies a smaller penalty to the outfit generator $G$, and if not, the penalty is larger. In particular, the CCM is designed as a sequence model that regards the outfit as a sequence from the perspective of human vision [21]. To synthesize compatible outfits, we employ a pre-trained sequence model to maintain the compatibility for outfit generation. This includes a pre-trained CNN and two directional LSTMs [15], in order to supervise the compatibility from two directions. Formally, given a fashion outfit $O = [O_1,\\ldots,\\ldots, O_i,\\ldots, ON]$, we regard it as a sequence, where $O_i$ is the $i$-th fashion item in $O$. As shown in Fig. 4, we first extract the latent feature $f_i$ for $O_i$ using a pre-trained CNN, and this is then fed into a Bi-LSTM module. For example, the forward LSTM recurrently takes the feature $f_{i-1}$ and the last hidden state $h_{i-1}$ as input and outputs a hidden state $h_i$ from $i = 2$ to $N$, as follows:\n$h_i = LSTM(f_1,\\ldots, f_{i-1}).$\nSimilarly, the backward LSTM takes the features in the reverse order and outputs the hidden state $h_i$ from $i = N - 1$ to 1. We then attempt to maximize the probability of the next item in the outfit given the previous sequence. More formally, we minimize the following compatibility objective function using a cross-entropy loss [29] in the form:\n$L_{ccm} = \\frac{1}{N-1}\\sum_{i=2}^{N-1}log(\\frac{exp(\\vec{h}_i\\vec{f}_i)}{\\sum_{f,\\in F} exp(\\vec{h}_i\\vec{f}_{i,\\ast})}) - \\frac{1}{N-1}\\sum_{i=N-1}^{1}log(\\frac{exp(\\vec{h}_i\\vec{f}_i)}{\\sum_{f,\\in F} exp(\\vec{h}_i\\vec{f}_{i,\\ast})}).$"}, {"title": "3) Training Losses:", "content": "In addition to the components of OutfitGAN mentioned above, the training losses are of the utmost importance in terms of supervising the training process. As shown in Fig. 2(b), the loss function for OutfitGAN includes two types of losses: the outfit discriminator loss and the outfit generator loss. We first discuss the adversarial training loss for the outfit discriminator. As shown in Fig. 2(a), our outfit discriminator uses $(N \u2013 1)$ independent item discriminators to guide the outfit generation. In a similar way to MUNIT [5], for each item discriminator $D_i$ we adopt a multi-scale discriminator architecture [3] and an LSGAN objective [30] to guide the training of our generator. We take the discriminator for the $i$-th item $O_i$ as an example. We first downsample the real and synthesized images by factors of two and four. The item discriminator $D_i = {D_{i,1}, D_{i,2}, D_{i,3}}$ is then applied to distinguish between the real and synthesized images at three different scales. Formally, the objective function of our adversarial loss for the training of each item discriminator is expressed as follows:\n$L^D = \\sum_{s=1}^{3}(L^D)_{i,s}.$"}, {"title": "C. Implementation Details", "content": "In this subsection, we introduce two reference mask generation strategies used to synthesize the alternative masks. Pix2Pix mask generation is used to synthesize reference masks via a pre-trained generative model, and random mask generation is used to return the sampled reference masks from the training set. In this following, we discuss the details of the detailed network architecture of OutfitGAN. Finally, we illustrate the overall adversarial algorithm used to train OutfitGAN.\n1) Strategies for Reference Mask Generation: The reference mask is an essential component of OutfitGAN, and provides important guidance information in terms of supervising the generation of compatible fashion items. In addition to the reference masks given by users, we may in practice need to synthesize reference masks based on models, such that they can then be fed into OutfitGAN. To overcome this issue, we design two strategies for the synthesis of a diverse range of masks: Pix2Pix and random mask generation. These two methods mimic the phase in which fashion designers or common users generate reference masks, and extend the basic functions of OutfitGAN.\nMask generation using Pix2Pix: In order to take advantage of reference masks to improve the effectiveness of OutfitGAN, we propose a method of synthesizing reference masks using a pre-trained generative model to extend the function of OutfitGAN. In fact, reference mask generation can be regarded as another image-to-image translation task, in which the input is an RGB image of a given fashion item and the output consists of the corresponding masks of compatible fashion items. There are many methods that are capable of synthesizing reference masks for a given fashion item [2][3]. In this research, Pix2Pix [2], as a representative framework among these methods, was chosen for this task. We constructed a large-scale dataset called OutfitSet (more details are given in Section IV-A), which consisted of 20,000 outfits with their associated masks, and used this to train the mask generator. Using (N-1) fashion items for each complete outfit (i.e., excluding the given fashion item), we pre-trained (N-1) independent Pix2Pix mask generators on the training set of OutfitSet to synthesize the reference masks for the target"}, {"title": "2) Network Architecture:", "content": "In this subsection, we describe the detailed network architecture of OutfitGAN. For the $i$-th item generator $G_i$, as shown as an example in Fig. 5, we employ the architecture of encoder $Enc_i$, and the decoder $Dec_i$ from [5], in which their effectiveness in image-to-image translation is proven [4]. The encoder $Enc_i$ includes four convolutional blocks (conv-blocks) and three residual blocks (res-blocks), whereas the decoder contains three res-blocks, three upsampling and conv-block modules, and one conv-block followed by a Tanh function. We apply a ReLU activation function to all the conv-blocks. As illustrated in Fig. 5, the SAM applies four conv-blocks to each branch of the feature extractor, followed by a correspondence layer and an alignment layer. Our discriminator is designed using a multi-scale architecture, in the same way as in [5]. In the CCM, we extract image features with a pre-trained ResNet-50 [33] provided by PyTorch [34] and a fully-connected network for 512-dimensional embeddings. A Bi-LSTM (which includes forward and backward LSTM) is then used to model the collocation relationship. In the same way as in [35], the number of layers for each LSTM is set to one, and the number of hidden features is set to 512."}, {"title": "3) Adversarial Training Process:", "content": "In this subsection, we present the design of an adversarial training scheme which is used to optimize the generator G of OutfitGAN. For clarity, the entire training process of OutfitGAN is summarized in Algorithm 1. We first pre-train the collocation classification module on our training set by minimizing the loss in Eq. (4) with a learning rate $\u03b7_{ccm}$. We then select the best CCM model with our validation set (see Section IV-A) by calculating the smallest $L_{ccm}$ according to Eq. (4) (shown in lines 1-2). Following this, we fine-tune the VGG-16, which was pre-trained on ImageNet, by applying the attribute classification method used in DeepFashion (shown in line 3). We initialize the parameters of G and D, and fix all the parameters of the"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we first describe the construction of our dataset in detail. Parameter settings of models and evaluation metrics are then described sequentially. The performance of our proposed OutfitGAN is compared against several competitive image-to-image translation baselines, and we perform an ablation study to verify the effectiveness of the main modules in OutfitGAN. Furthermore, we conduct a parametric study on our model and an extra study on different sequences of fashion items used in the collocation classification module. Finally, the limitation in our framework is discussed."}, {"title": "A. Dataset", "content": "When carrying out fashion outfit generation, accurate fashion datasets are of the utmost importance in terms of providing the ground truths for model training and evaluation. Although many public fashion outfit compatibility datasets are available for fashion modeling, such as UT Zappos50K [36], the Maryland Polyvore dataset [21], FashionVC [37], and IQON3000 [38], all of these lack explicit common category annotations for fashion items and clear compositions for outfits. To overcome these issues in current datasets and to verify the effectiveness of our proposed outfit generation models, we collected fashion outfits from a fashion matching website, Polyvore.com, which contained numerous compatible outfits constructed by fashion experts. These outfits were put together based on the preferences of fashion experts, with the aim of clearly and attractively presenting specific fashion styles. The original dataset consisted of over 344,560 outfits, which were composed of 2,131,607 fashion items. We selected four types of fashion items (upper clothing, bag, lower clothing and shoes) that are common components of outfits worn in daily life. We used the upper clothing as the given fashion item in order to exploit the richer information on styles that can be obtained from the upper clothing compared with the other fashion items in the same outfit. This means that for each outfit set [upper clothing, bag, lower clothing, shoes] (i.e., N = 4), the extant given fashion item represents upper clothing. We therefore kept only those outfits that included all four of these categories. Since images of shoes have diverse orientations due to the different shooting angles used, we filtered out images that only contained one shoe, and flipped all"}, {"title": "B. Experimental Setup and Parameter Settings", "content": "In the experiments, all images were resized to 256 x 256, and we used random cropping for data augmentation during training. In the training phase, the batch size was set to four, and the number of training iterations for the model was set to 200,000. All experiments were performed on a single NVIDIA GeForce RTX 3090, and the implementation was carried out in PyTorch [34]. We set the coefficients to balance the losses as follows: $\u5165\u2081 = 100$ and $\u5165\u2081 = 10$ for OutfitGAN with real reference masks, and $\u5165\u2081 = 10$ and $\u5165\u2081 = 10$ for OutfitGAN with reference masks produced by mask generation strategies. The CCM was trained with an SGD [40] optimizer with a learning rate $\u03b7_{cmp}$ of 0.2 and a momentum of 0.9. OutfitGAN was trained with an Adam [41] optimizer with $\u03b2\u2081 = 0$ and $\u03b22 = 0.99$, and the learning rate $\u03b7$ for G and D was set to $10^{-4}$."}, {"title": "C. Evaluation Metrics", "content": "To evaluate the performance of our proposed model, we used a variety of evaluation metrics from three perspectives, as follows:\n1) A similarity measurement was used to measure the similarity between the synthesized images and the target ones. We adopted two metrics: a structural similarity (SSIM) [42] and a learned perceptual image patch similarity (LPIPS) [43]. SSIM [42] is a traditional, widely used image quality index for image comparison. Given two local patches extracted from input images, i.e., a real image patch $x$ and a synthesized image patch $y$, SSIM measures the luminance, contrast and similarity of $x$ and $y$, where a higher score indicates a higher similarity. LPIPS [43] is another common metric used to evaluate the image similarity between two images, particularly for a synthesized image and a target one, with a pre-trained deep model. We used the default pre-trained AlexNet [44] provided by the authors [43] to calculate the LPIPS metric. Here, a higher score indicates a lower similarity, and vice versa.\n2) An authenticity measurement was applied to reflect the quality of the synthesized images in terms of their authenticity. Previous studies [7] have suggested that the Fr\u00e9chet inception distance (FID) can be used to estimate the authenticity of synthesized images in feature space. More specifically, the FID measures the similarity between two domains of images, and is particularly suitable for real images and images synthesized by GANs. To calculate the FID between two image domains Y and y', we first embed both images into the same feature space F given by an Inception model [45]. The FID can be defined as follows:\n$FID(Y, Y') =||\u03bc\u03bd \u2013 \u03bc\u03b3\u03bd||2+Tr(\u03a3y + \u03a3y \u2013 2(\u03a3\u03b3\u03a3\u03b3\u03bd)),$"}, {"title": "D. Performance Comparison", "content": "1) Compared Methods: To examine the effectiveness of our proposed OutfitGAN, we compared it with six state-of-the-art methods: Pix2Pix [2], Pix2PixHD [3], CycleGAN [3], MUNIT [5], DRIT++ [6], and StarGAN-v2 [7]. These include both supervised and unsupervised models. For completeness, we give a brief introduction to these methods as follows:\nPix2Pix [2] is the first framework developed for supervised image-to-image translation, and uses a U-Net architecture for the generator and a single discriminator to classify real and fake image pairs.\nPix2PixHD [3] is an improved version of Pix2Pix framework based on a coarse-to-fine approach, which uses a coarse-to-fine generator, a multi-scale discriminator and a feature matching loss.\nCycleGAN [4] is an unsupervised image-to-image translation method with a cycle reconstruction loss, and was the first framework to address the issue of unpaired image-to-image translation.\nMUNIT [5] is based on the idea that an image representation can be decomposed into a style code and a content code. It can learn disentangled representations for image-to-image translation.\nDRIT++ [6] is an improved version of DRIT [46], which disentangles the latent spaces into a shared content space and an attribute space for each domain and was developed to synthesize diverse images for image-to-image translation.\nStarGAN-v2 [7] is an improved version of StarGAN [47] that employs an MLP to synthesize different styles and then injects them into decoders to synthesize a diverse range of images."}, {"title": "E. Ablation Study", "content": "In this subsection, two sets of experiments are carried out to validate the effectiveness of the SAM and the CCM, which are the main components of OutfitGAN.\nEffectiveness of the SAM: In order to investigate the effectiveness of the SAM, we validated it from two perspectives. Firstly, we trained our OutfitGAN without the SAM. In Table II, 'OutfitGAN w/o SAM' means that we concatenated a reference mask with only the feature from the i-th encoder $Enc_i$ and fed the concatenated feature into the i-th decoder $Dec_i$. The results show that the OutfitGAN model with the SAM consistently outperformed the model without the SAM in terms of the SSIM, LPIPS, and FID. This indicates that the SAM in our original framework was able to learn a correspondence relationship between a given fashion item and the targeted collocation items, allowing the visual similarity and authenticity to be significantly improved. To further examine the impacts of the SAM, we elaborate the explanation of the correspondence Mcorr for the i-th synthesized fashion item during the generation process in Fig. 8. As shown in Fig. 8(a), there is a selected mapping relationship between the extant upper clothing and the synthesized lower clothing. For clarity, the corresponding four highest semantic regions for the areas of each white block in the synthesized lower images are annotated in Fig. 8(a). We can see that the red regions of synthesized images are always from the red patches or other salient patches of the extant upper clothing. This suggests that the SAM captures the translation correspondence relationship. In addition to the precise mapping between the given fashion items and the synthesized ones, we also average the correspondence matrix for visualization. It can be seen that the"}, {"title": "Effectiveness of the CCM:", "content": "We also explore the impact of the CCM in OutfitGAN. Specifically, we examine its effect on the FCTS in terms of visual compatibility. A comparison of the results is given in Table III, where 'OutfitGAN w/ CCM' and 'OutfitGAN w/o CCM' denote models with and without collocation classification, respectively. We can see that the model without the CCM gives a significant decrease in the FCTS, from 87.1% to 67.5%, thus demonstrating that the CCM markedly improves the compatibility of synthesized outfits. In addition, Fig. 10 shows that OutfitGAN with the CCM synthesizes more compatible outfits with more harmonious styles than the model without the CCM. The collocation module enhances the frequency of co-occurrence of compatible elements or style for compatible outfits. These quantitative and qualitative results suggest that the CCM can effectively improve the compatibility of the synthesized outfits."}, {"title": "F. Parametric Study", "content": "The main hyperparameters used in OutfitGAN are the number of feature channels for the SAM and the coefficients in the training losses.\nNumber of feature channels for the SAM. We first investigate the influence of the number of feature channels on the results of outfit generation. We set the number of feature channels c to [0, 8, 16, 32, 64, 128] in OutfitGAN. The results for the SSIM, LPIPS and FID over all categories are illustrated in Fig. 9. In particular, the use of zero channel means that we concatenate only the reference mask with the feature extracted by the i-th encoder Enc, in OutfitGAN. From Fig. 9, we can see that an increase in the number of feature channels within the range [0,64] generally increases the performance of OutfitGAN in terms of the SSIM, LPIPS and FID. When the number of feature channels is increased beyond 64, the performance of OutfitGAN may become slightly worse. We ascribe this to the fact that the outfit generation process requires a much larger exploration space when the number of feature channels becomes large. In our experiments, setting the parameter c to 64 was sufficient to deliver satisfactory results.\nCoefficients of the training loss. To further investigate the impacts of the coefficients used in weighting the training losses, we present the results from OutfitGAN with different"}, {"title": "G. Study on Different Sequences of Fashion Items", "content": "As previously stated, the fashion compatibility task can be addressed with a sequence model which is motivated by the human observation perspective [21]. However, the sequence of the fashion items in an outfit has many possible arrangements. For an outfit with N fashion items, it has N! possible orders to model the fashion compatibility, where N! denotes factorial N. In this section, we further investigate all possible sequences under our problem settings on the performance of OutfitGAN in terms of the fashion compatibility metric, FCTS. Considering that the used collocation classification module is based on Bi-LSTM, here we only have $^NC_k$ possible unique orders in our task. We implemented different variants of OutfitGAN with all possible orders which were trained to validate the effectiveness of our pre-defined order, i.e., [upper, bag, lower, shoes]. Additional eleven versions of OutfitGAN were carried out in total. As shown in Fig. 12, we observe that our pre-defined order used in Section IV-A obtains the best fashion compatibility in comparison to other possible orders, despite that 'USLB' and 'UBLS' have the same FCTS values (see Fig. 12). Moreover, most models with other orders show relatively decent performance on fashion compatibility. This may be ascribed to the fact that there only exist four fashion items in an outfit in our current research and Bi-LSTM may have sufficient ability to build the compatibility relation among fashion items in the same outfit even if we provide an arbitrary order."}, {"title": "H. Limitation", "content": "Although the proposed method achieves state-of-the-art performance in outfit generation, OutfitGAN still has certain limitations at the current stage. Firstly, an outfit includes N fashion items, where N = 4 in our implementation. During the process of our dataset construction, we crawled outfits which are composed by fashion experts from Polyvore.com. To cover as many fashion items as possible, we define our outfit generation on four commonly used items by women \u2013 upper, bag, lower, and shoes. It is possible to build a large-scale dataset with more kinds of fashion items when more relevant fashion compatibility-related resources are available in the future. Secondly, for an outfit with N fashion items, OutfitGAN needs (N - 1) item generators to synthesize the"}, {"title": "V. CONCLUSION", "content": "This paper has presented an outfit generation framework with the aim of synthesizing photo-realistic fashion items that are compatible with a given item. In particular, in order to exploit the harmonious elements and styles shared in a compatible outfit, OutfitGAN uses a mask-guided strategy for image synthesis which can overcome the issue of spatial misalignment that arises in general image-to-image translation tasks. OutfitGAN consists of an outfit generator, an outfit discriminator and a CCM. An SAM is adopted to capture the mapping relationships between the extant fashion items and the synthesized ones, in order to improve the quality of fashion synthesis. A CCM is developed to improve the compatibility of the synthesized outfits. To evaluate the effectiveness of the proposed model, we constructed a large-scale dataset that consists of 20,000 outfits. Extensive experimental results show that our method can achieve state-of-the-art performance on the task of outfit generation and outperforms other methods.\nIn the future, we plan to concentrate on synthesizing outfits with finer detail, and to use other reference information such as textual descriptions in a multi-modal manner to guide the process of outfit generation."}]}