{"title": "SCP-116K: A High-Quality Problem-Solution Dataset and a Generalized\nPipeline for Automated Extraction in the Higher Education Science Domain", "authors": ["Dakuan Lu", "Xiaoyu Tan", "Rui Xu", "Tianchu Yao", "Chao Qu", "Wei Chu", "Yinghui Xu", "Yuan Qi"], "abstract": "Recent breakthroughs in large language models\n(LLMs) exemplified by the impressive math-\nematical and scientific reasoning capabilities\nof the ol model-have spotlighted the criti-\ncal importance of high-quality training data\nin advancing LLM performance across STEM\ndisciplines. While the mathematics commu-\nnity has benefited from a growing body of\ncurated datasets, the scientific domain at the\nhigher education level has long suffered from\na scarcity of comparable resources. To address\nthis gap, we present SCP-116K, a new large-\nscale dataset of 116,756 high-quality problem-\nsolution pairs, automatically extracted from het-\nerogeneous sources using a streamlined and\nhighly generalizable pipeline. Our approach\ninvolves stringent filtering to ensure the scien-\ntific rigor and educational level of the extracted\nmaterials, while maintaining adaptability for fu-\nture expansions or domain transfers. By openly\nreleasing both the dataset and the extraction\npipeline, we seek to foster research on scientific\nreasoning, enable comprehensive performance\nevaluations of new LLMs, and lower the barrier\nto replicating the successes of advanced mod-\nels like ol in the broader science community.\nWe believe SCP-116K will serve as a critical\nresource, catalyzing progress in high-level sci-\nentific reasoning tasks and promoting further\ninnovations in LLM development. The dataset\nand code are publicly available at https://\ngithub.com/AQA6666/SCP-116K-open.", "sections": [{"title": "1 Introduction", "content": "Recent years have witnessed remarkable advances\nin large language models (LLMs), particularly in\ntheir capacity to handle complex reasoning tasks\nin mathematics and science (OpenAI, 2024; Team,\n2024; Min et al., 2024; DeepSeek-AI et al., 2025).\nModels like o1 (OpenAI, 2024) have demonstrated\nunprecedented capabilities in solving sophisticated\nmathematical problems and engaging in scien-\ntific discourse, highlighting the critical role of\nhigh-quality training data in achieving such break-\nthroughs (DeepSeek-AI et al., 2025; Min et al.,\n2024). The ability to reason effectively across\nSTEM disciplines represents not only a significant\ntechnical achievement but also a crucial step toward\nmore capable and versatile artificial intelligence\nsystems.\nWhile the field has made substantial progress in\nmathematical reasoning, supported by well-curated\ndatasets and benchmarks (Liu et al., 2024; Cherny-\nshev et al., 2025; Glazer et al., 2024; Hendrycks\net al., 2021), there remains a notable gap in com-\nparable resources for scientific disciplines, particu-\nlarly at the higher education level. This disparity\nhas resulted in an uneven development of LLM\ncapabilities, where progress in scientific reasoning\nhas not kept pace with advances in mathematical\nproblem-solving (Rein et al., 2023). The scarcity of\nhigh-quality scientific problem-solution pairs, es-\npecially those targeting undergraduate to doctoral-\nlevel content, has emerged as a significant bottle-\nneck in advancing LLMs' scientific reasoning ca-\npabilities.\nTo address this critical gap, we present SCP-\n116K, a Science Problem and solution dataset of\n116,756 rigorously curated problem-solution pairs\nin various scientific disciplines. Our work is moti-\nvated by the dual objectives of providing a compre-\nhensive resource for training and evaluating LLMs\nin scientific reasoning, while establishing a scalable\nmethodology for dataset creation in specialized do-\nmains. The dataset spans multiple scientific fields\nand educational levels, offering a rich testbed for\ndeveloping and assessing scientific reasoning capa-\nbilities.\nCentral to our contribution is an innovative, auto-\nmated pipeline for extracting high-quality problem-\nsolution pairs from heterogeneous source materi-\nals. Our approach begins with diverse document"}, {"title": "2 Related Work", "content": "Recent advances in LLMs have sparked growing\ninterest in scientific reasoning capabilities, leading\nto the development of various datasets and models\nin this domain. We organize our discussion of\nrelated work around three key aspects: training and\nevaluation datasets, benchmarking standards, and\nstate-of-the-art models.\nScientific Question-Answering Datasets Sev-\neral datasets have been created to facilitate the\ndevelopment of scientific reasoning capabilities\nin LLMs. The CAMEL dataset (Li et al., 2023)\ncontains 20,000 physics problem-solution pairs\ngenerated using GPT-4, covering 25 topics with\ntheir respective sub-topics. Similarly, ScienceQA\n(Lu et al., 2022) provides 21,208 multimodal sci-\nence questions spanning natural, language, and so-\ncial sciences, with comprehensive explanations and\nthought chains, collected from elementary and high\nschool science curricula. The Kaggle LLM Sci-\nence Exam dataset (Kaggle and Google, 2023) of-\nfers middle school-level multiple-choice questions\nwith accompanying context information. While\nthese resources have contributed significantly to the\nfield, they either rely on synthetic data generation\nor focus on lower educational levels. In contrast,\nSCP-116K distinguishes itself by offering 116,756\nproblem-solution pairs extracted from authentic ed-\nucational materials, ensuring higher quality, greater\ndiversity, and more challenging content suitable for\nadvanced scientific reasoning.\nBenchmarking Standards The recently intro-\nduced Graduate-level Google-Proof Q&A (GPQA)\nbenchmark (Rein et al., 2023) represents a sig-\nnificant step forward in evaluating advanced sci-\nentific reasoning capabilities. Comprising 448\nexpert-crafted questions across biology, physics,\nand chemistry, GPQA sets a high bar for both hu-\nman experts and AI systems. While GPQA excels\nat assessment, our work complements it by pro-\nviding a comprehensive training resource that can\nhelp models achieve better performance on such\nchallenging evaluations. The substantial scale of\nSCP-116K, combined with its focus on higher ed-\nucation content, makes it particularly suitable for\ndeveloping models capable of tackling graduate-\nlevel problems.\nAdvanced Scientific Reasoning Models The in-\ntroduction of OpenAI's o1 model (OpenAI, 2024)"}, {"title": "3 Methodology", "content": "Our methodology comprises a comprehensive\npipeline for extracting, filtering, and matching high-\nquality problem-solution pairs from diverse aca-\ndemic sources. The pipeline consists of six main\nstages: document retrieval and filtering, unified\npreprocessing, segmentation, extraction, quality fil-\ntering, and problem-solution matching. Figure 1\nillustrates the complete pipeline architecture."}, {"title": "3.1\nDocument Retrieval and Filtering", "content": "The initial stage involves large-scale retrieval from\na corpus of 6.69 million academic documents. We\nemploy a keyword-based approach, targeting doc-\numents with \"problem\" or \"question\" in their ti-\ntles-an empirically effective heuristic for iden-\ntifying content-rich sources. This initial retrieval\nyields 4,270 candidate documents. We then employ\nGPT-40 (OpenAI, 2024) to filter these candidates\n(see Appendix A.1 for prompt details), retaining\nonly those that contain university to doctoral-level\ncontent in physics, chemistry, or biology, result-\ning in a refined corpus of 467 high-quality source\ndocuments."}, {"title": "3.2 Unified Preprocessing", "content": "A significant challenge in processing academic con-\ntent is handling heterogeneous document formats\ncontaining complex scientific formulas and chem-\nical equations. We address this through a novel\nunified preprocessing framework. Our approach\nfirst converts all documents to a standardized image\nformat using open-source rendering tools, then ap-\nplies GPT-40 to convert the content into markdown\ntext with LaTeX scientific notation (see Appendix\nA.2). This transformation ensures consistent han-\ndling of scientific formulas while preserving the\nsemantic structure of the content."}, {"title": "3.3 Content Segmentation", "content": "To manage documents of varying lengths, we im-\nplement an intelligent segmentation strategy. Our\napproach leverages GPT-4o to identify structural\nboundaries such as chapters, sections, problems,\nand solutions (prompt details in Appendix A.3).\nThe segmentation algorithm operates on these natu-\nral boundaries while maintaining a maximum token\ncount constraint. This approach preserves the in-\ntegrity of logical units while optimizing for down-\nstream processing efficiency."}, {"title": "3.4 Structured Extraction", "content": "The segmented content undergoes generative ex-\ntraction using GPT-40 to identify and isolate prob-\nlems, solutions, and their respective numerical\nidentifiers (see Appendix A.4). This stage pro-\ncesses each segment to extract structured infor-\nmation while maintaining contextual relationships.\nThe initial extraction yields approximately 190,000\nproblems and 80,000 solutions, forming a compre-\nhensive candidate pool for subsequent refinement."}, {"title": "3.5 Quality Filtering", "content": "We implement a rigorous two-stage filtering pro-\ncess to ensure data quality. First, we eliminate\nincomplete extractions where either the problem\nor solution lacks critical components. Second, we\nfilter out entries that reference external elements\n(equations, figures, or other problems) not con-\ntained within the extracted content. This qual-\nity control process results in a refined dataset of\n116,756 problems and 70,000 solutions."}, {"title": "3.6 Problem-Solution Matching", "content": "A significant challenge in constructing this dataset\nlies in the diverse organizational structures of sci-\nence materials. Problems and their corresponding\nsolutions often appear in different locations within\na document: some solutions immediately follow\ntheir problems, others are grouped at the end of\nchapters, and some are collected in appendices or\nseparate solution manuals. This structural variety\nnecessitates a sophisticated matching approach.\nWe develop a novel dual-pathway retrieval and\nmatching framework to address this challenge. The\nfirst component, numerical matching, leverages\nproblem numbers as primary identifiers, match-\ning problems with solutions sharing the same nu-\nmerical designation. This approach is particularly\neffective for materials where solutions are physi-\ncally separated from problems but maintain consis-\ntent numbering. The second component, semantic\nmatching, employs the Stella similarity model (No-\nvaSearch, 2024) to identify potential matches based\non content alignment, addressing cases where nu-\nmerical matching is insufficient or where number-\ning schemes are inconsistent across different sec-\ntions.\nThe dual-pathway approach generates up to four\ncandidate solutions per problem, ranked by similar-\nity scores. These candidates undergo verification\nusing GPT-40 (prompt provided in Appendix A.5),\nwhich assesses the semantic correspondence be-\ntween each problem-solution pair. This comprehen-\nsive matching process yields 43,000 high-quality,\nverified problem-solution pairs."}, {"title": "3.7 Model Response Collection", "content": "To facilitate future model distillation and estab-\nlish performance benchmarks, we collect solutions\nfrom two representative advanced reasoning mod-\nels: 01-mini and QwQ-32B-preview. These models\ngenerate solutions for all problems in our dataset,\ncreating a valuable resource for knowledge distilla-\ntion experiments. GPT-4o-mini validates the corre-\nspondence between model-generated solutions and\nour extracted ground truth solutions.\nThis collection process serves two key purposes.\nFirst and foremost, it creates a rich set of model-\ngenerated solutions that can be used for future\nknowledge distillation tasks, enabling the devel-\nopment of more efficient models that maintain high\nperformance on scientific reasoning tasks. Addi-\ntionally, it establishes baseline performance met-\nrics for current state-of-the-art models on complex\nscientific problem-solving tasks."}, {"title": "4 Experiments", "content": "We conduct extensive experiments to evaluate both\nthe utility of our dataset and its effectiveness in\nimproving scientific reasoning capabilities through\nknowledge distillation. Our experiments focus on\ntwo main aspects: (1) assessing the performance\nof current state-of-the-art models on SCP-116K,\nand (2) investigating the potential of our dataset for\nenhancing model performance through knowledge\ndistillation."}, {"title": "4.1 Baseline Performance on SCP-116K", "content": "To establish baseline performance metrics, we eval-\nuate two representative advanced reasoning models\non our dataset: 01-mini and QwQ-32B-preview.\nThese models achieve accuracy rates of 58.40%\nand 55.79%, respectively, on SCP-116K. While\nthese results demonstrate meaningful reasoning ca-\npabilities, they also indicate substantial room for\nimprovement in scientific problem-solving tasks,\nunderscoring the challenging nature of our dataset\nand its potential utility for advancing the field."}, {"title": "4.2 Knowledge Distillation Experiments", "content": "To validate the effectiveness of SCP-116K in im-\nproving model performance, we conduct a series of\nknowledge distillation experiments using Qwen2.5-\n32B-Instruct as the base model. We explore three\ndistinct distillation approaches:\n\u2022 Direct Distillation: We distill knowledge\nfrom 01-mini's correct responses to Qwen2.5-\n32B-Instruct, resulting in the Qwen2.5-32B-\nInstruct-distill-01-mini variant.\n\u2022 STILL-2 Variants: We explore two variants\nusing STILL-2's framework:\nQwen2.5-32B-Instruct-still-2: Trained\nusing STILL-2's original distillation data\nderived from the CAMEL dataset\nQwen2.5-32B-Instruct-still-2-scp:\nTrained using STILL-2's data with\nCAMEL's scientific QA replaced by\n1,000 examples from SCP-116K\n\u2022 Hybrid Approach: We develop Qwen2.5-\n32B-Instruct-distill-mix by combining two\ndata sources: (1) 1,000 selected problem-\nsolution pairs from SCP-116K, chosen based\non semantic similarity to GPQA-diamond\nquestions, and (2) STILL-2's distillation data."}, {"title": "4.3 Results and Analysis", "content": "We evaluate all models on the challenging GPQA-\ndiamond benchmark to assess the effectiveness of\nour distillation approaches. The results reveal several key findings. First, all\ndistillation approaches significantly improve upon\nthe base Qwen2.5-32B-Instruct model's perfor-\nmance, demonstrating the effectiveness of knowl-\nedge transfer. Second, both our hybrid approach\nand the SCP-116K-enhanced STILL-2 variant\nachieve notably strong performance, with accura-\ncies of 58.59% and 58.08% respectively. These\nresults represent substantial improvements of 11.12\nand 10.61 percentage points over the base model,\nvalidating the effectiveness of incorporating care-\nfully selected examples from SCP-116K into the\ntraining process.\nNotably, the hybrid approach's performance ap-\nproaches that of o1-mini (60.61%), despite using\na significantly smaller model. This suggests that\nour dataset and distillation methodology can help\nbridge the performance gap between smaller, more\nefficient models and their larger counterparts in\nscientific reasoning tasks."}, {"title": "5 Conclusion", "content": "In this work, we present SCP-116K, a compre-\nhensive dataset of 116,756 high-quality scientific\nproblem-solution pairs, accompanied by a gener-\nalizable pipeline for automated content extraction.\nOur contributions address a critical gap in the scien-\ntific reasoning landscape, providing both the data\nresources and methodological framework neces-\nsary to advance LLM capabilities in STEM dis-\nciplines. The experimental results demonstrate\nthe dataset's utility, with our hybrid distillation ap-\nproach achieving notable improvements in model\nperformance on challenging scientific reasoning\ntasks.\nBeyond the immediate empirical results, this\nwork establishes a foundation for future research in"}, {"title": "A Appendix", "content": "Here we provide the prompts used with GPT-40\nthroughout our pipeline for transparency and repro-\nducibility."}, {"title": "A.1.1\nDocument Filtering", "content": "We use the following prompt template with GPT-40\nto filter documents and identify relevant textbooks\nand problem books:\nPlease determine whether the following\nbook belongs to the category of\n**textbooks or problem books in the\nfields of physics, chemistry, or\nbiology (including their subfields)\ntargeted at undergraduate to\ndoctoral-level students**.\nIf the book is either a textbook or\na problem book in these fields,\noutput \"Yes\". If it does not belong\nto either category, output \"No\".\nConsider the information provided\ncarefully and reason through your\njudgment step by step. Provide your\ndetailed reasoning before delivering\nthe final determination.\nHere is the book's metadata:\n**Title**: {meta_data['title']}\n- **Author**: {meta_data['author']}\nAfter reasoning, output the answer in\nthe following format:\n[Determine Begin] Yes/No[Determine End]"}, {"title": "A.1.2 Unified Preprocessing", "content": "For converting document images to markdown text,\nwe use the following prompt with GPT-40:\nPlease convert the content of the image\ninto Markdown text, following a logical\nreading order and ignore headers and\nfooters.\nUse LaTeX for any formulas, equations,\nor chemical structures.\nFor important illustrations, provide\na detailed written description of their\ncontent. Ignore non-essential visuals.\nFor blank pages, return the output as:\n`empty`"}, {"title": "A.1.3 Content Segmentation", "content": "For segmenting the content into logical units, we\nuse the following prompt with GPT-40:\nFor the given book page:\n{page_text_with_line_index}\nPlease identify if there are any:\n1. Chapter beginnings\n2. Section beginnings\n3. Subsection beginnings\n4. Problem (exercise or example) beginnings\nPlease ignore the following:\n1. Headers and footers\n(especially on line 0, 1, 2)\n2. Sub-question markers\nlike \"(1)\", \"(a)\", \"(i)\", etc.\n3. Solution indicators\nsuch as \"**SOLUTION:**\",\n\"## Solution\", \"### General Solution\", etc.\nLet's solve this step by step:\n1. identify any chapter indicators\n(e.g., \"Chapter 1\", etc.)\n2. look for section markers\n(e.g., \"1.1\", \"Section 1\", etc.)\n3. identify subsection markers\n(e.g., \"1.1.1\", etc.)\n4. look for problem indicators\n(e.g., \"1.1\", \"1-1\", \"**1008**\","}, {"title": "A.1.4 Structured Extraction", "content": "For extracting structured information from the seg-\nmented content, we use the following prompt with\nGPT-40:\nInput:\n{chunk['chunk']}\nI am a university professor preparing\nan exercise problem bank.\nPlease help me extract the problems\n(include examples) or solutions from\nprovided textbook pages.\n1. First, find all the problems or\nsolutions in the provided content.\n*Carefully analyze each piece of\ncontent to determine whether it is a\nproblem or a solution.*\n2. Ensure each identified problem is\ncomplete and not part of a solution\nor other content.\n3. *For problems with multiple\nsub-problems, DO NOT omit the problem\nstatement, DO NOT split the problem\nwith multiple sub-problems.*\n4. *DO NOT omit or change any part of\nthe problems and solutions. Ensure the\ncontent is complete.*\nOutput the extracted data as a list of\nJSON objects.\nLet's think step by step, output your"}, {"title": "A.1.5 Problem-Solution Verification", "content": "For verifying the problem-solution pairs, we use\nthe following prompt with GPT-40:\n1. Task Overview\nI have extracted problem-solution pairs\nfrom textbooks using extraction and\nmatching algorithms. Please help me\ndetermine if the following problem and\nsolution constitute a 'valid'\nproblem-solution pair.\n2. Input\nProblem:\n{problem}\nSolution:"}]}