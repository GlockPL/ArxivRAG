{"title": "FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs", "authors": ["Tongyi SpeechTeam"], "abstract": "This report introduces FunAudioLLM, a model family designed to enhance natu- ral voice interactions between humans and large language models (LLMs). At its core are two innovative models: SenseVoice, which handles multilingual speech recognition, emotion recognition, and audio event detection; and CosyVoice, which facilitates natural speech generation with control over multiple languages, timbre, speaking style, and speaker identity. SenseVoice-Small delivers exc\u0435\u0440- tionally low-latency ASR for 5 languages, and SenseVoice-Large supports high- precision ASR for over 50 languages, while CosyVoice excels in multi-lingual voice generation, zero-shot in-context learning, cross-lingual voice cloning, and instruction-following capabilities. The models related to SenseVoice and Cosy Voice have been open-sourced on Modelscope and Huggingface, along with the corresponding training, inference, and fine-tuning codes released on GitHub. By integrating these models with LLMs, FunAudioLLM enables applications such as speech-to-speech translation, emotional voice chat, interactive podcasts, and expressive audiobook narration, thereby pushing the boundaries of voice interac- tion technology.", "sections": [{"title": "1 Introduction", "content": "In recent years, the advancement in artificial intelligence (AI) has dramatically transformed how humans interact with machines, such as GPT-40 (OpenAI, 2023) and Gemini-1.5 (Reid et al., 2024) and so on (Bai et al., 2023b; Chu et al., 2023). This transformation is particularly evident in the realm of voice processing, where capabilities such as high-precision speech recognition (Radford et al., 2023), emotion recognition (Ma et al., 2024b), and voice generation (Wang et al., 2023a; Du et al., 2024a) are paving the way for more intuitive and human-like interactions. In this report, we introduce FunAudioLLM, an innovative framework designed to facilitate natural voice interactions between humans and large language models (LLMs) (Team, 2023; Bai et al., 2023a; Touvron et al., 2023). At the core of FunAudioLLM are our two groundbreaking models: SenseVoice, for voice understanding, and Cosy Voice, for voice generation.\nSenseVoice is our state-of-the-art voice understanding model, which excels in multiple domains of voice processing. We offer both SenseVoice-Small and SenseVoice-Large variants. We have open-sourced SenseVoice-Small, which supports multilingual recognition in Chinese, English, Can- tonese, Japanese, and Korean, delivering extremely low inference latency by employing a non- autoregressive end-to-end architecture. This design choice results in a performance that is more than 5 times faster than Whisper-small and more than 15 times faster than Whisper-large (Radford et al., 2023). On the other hand, SenseVoice-Large supports speech recognition in over 50 lan-"}, {"title": "2 FunAudioLLM Models", "content": "FunAudioLLM consists of two foundation models for voice understanding and generation, named SenseVoice and CosyVoice, respectively. SenseVoice supports multi-lingual speech recognition, which is trained on over 300k hours. Specifically, SenseVoice-Small is efficient in inference, in which the recognition latency is less than 80ms and is more than 5 and 15 times faster than Whisper- Small and Whisper-large, respectively, and SenseVoice-Large supports high-precision ASR for over 50 languages. Furthermore, SenseVoice supports rich transcription, including state-of-the-art emo- tion recognition, audio event detection, inverse text normalization (Pusateri et al., 2017) and punc- tuation (Chen et al., 2020).\nOur voice generation model, Cosy Voice, can generate multi-lingual speeches, which is trained on over 170k hours and five languages, including Chinese (ZH), English (EN), Japanese (JP), Cantonese (Yue) and Korean (KO). CosyVoice generated samples can achieve a WER of less 2% and speaker"}, {"title": "2.1 Overview of FunAudioLLM", "content": "FunAudioLLM consists of two foundation models for voice understanding and generation, named SenseVoice and CosyVoice, respectively. SenseVoice supports multi-lingual speech recognition, which is trained on over 300k hours. Specifically, SenseVoice-Small is efficient in inference, in which the recognition latency is less than 80ms and is more than 5 and 15 times faster than Whisper-Small and Whisper-large, respectively, and SenseVoice-Large supports high-precision ASR for over 50 languages. Furthermore, SenseVoice supports rich transcription, including state-of-the-art emo- tion recognition, audio event detection, inverse text normalization (Pusateri et al., 2017) and punctuation (Chen et al., 2020).\nOur voice generation model, CosyVoice, can generate multi-lingual speeches, which is trained on over 170k hours and five languages, including Chinese (ZH), English (EN), Japanese (JP), Cantonese (Yue) and Korean (KO). CosyVoice generated samples can achieve a WER of less 2% and speaker"}, {"title": "2.2 Voice Understanding Model: SenseVoice", "content": "Sense Voice is a speech foundation model with multiple voice understanding capabilities, including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recog- nition (SER), and audio event classification (AEC) or audio event detection (AED). Two models with different sizes and architectures are proposed to suit different requirements: SenseVoice-Small, an encoder-only speech foundation model for rapid speech understanding, and SenseVoice-Large, an encoder-decoder (Vaswani et al., 2017) speech foundation model for more accurate speech un- derstanding with more languages supported, as illustrated in Figure 2.\nSense Voice-Small is a non-autoregressive encoder-only model for multi-lingual multi-style ASR and multiple speech understanding tasks. Given the input waveform, we first compute the 80- dimensional log-mel filter-bank, and then stack consecutive frames and down-sample them by a factor of 6. The extracted feature is then mapped to the dimension D of the encoder, denoted as \\(X_{speech} \\in \\mathbb{R}^{T \\times D}\\), where T is the length of the down-sampled feature. The encoder is implemented as a memory-equipped self-attention network (SAN-M) (Gao et al., 2020). To specify the task, we prepend four embeddings to the speech feature as the input to the encoder:\n\\(X = \\text{concat}(e_{LID}, e_{SER}, e_{AEC}, e_{ITN/NOITN}, X_{speech})\\)  (1)\n\\(P = \\text{Softmax}(\\text{Linear}_{D\\rightarrow|V'|}(\\text{Encoder}(X)))\\) (2)\n\\(X \\in \\mathbb{R}^{(T+4) \\times D}\\) and \\(P\\in \\mathbb{R}^{(T+4) \\times |V'|}\\). \\(V'\\) is the vocabulary including tokens for ASR and other tasks. \\(e_{LID}, e_{SER}, e_{AEC}, e_{ITN/NOITN}\\) are embeddings of four special tokens:\n(LID) indicates the LID task. If (LID) is prepended, the model is trained to predict the language token, at the corresponding position of the output. In the training stage, we randomly replace (LID)"}, {"title": "2.3 Semantic Speech Tokenizer", "content": "A speech tokenizer transforms vocal signals into discrete tokens, enabling their modeling and pre- diction by autoregressive transformers for speech generation. Our preliminary experiments indicated that the choice of speech tokenizer is pivotal for overall system performance as well as the require- ments of both data quality and volume. We evaluated three classes of speech tokenizers: 1) those based on residual quantization like SoundStream (Zeghidour et al., 2022), Encodec (D\u00e9fossez et al., 2022) and FunCodec (Du et al., 2024b); 2) those utilizing multi-grouped quantization, such as Hifi- Codec (Yang et al., 2023); and 3) \u201csemantic\u201d speech tokens, specifically HuBERT(Hsu et al., 2021). All the above tokenizers are trained in the unsupervised or self-supervised manners. Thus, their association to semantic content is often tenuous, contributing to an unstable synthesis process and a substantial demand for clean training data. Moreover, unsupervised tokenizers are susceptible to data noise, necessitating meticulously curated clean data sets.\nBuilding on the success of SenseVoice models, we introduce a supervised semantic speech tokenizer, denoted as S\u00b3 (Du et al., 2024a). Using the pre-trained SenseVoice-Large model as a foundation, we incorporate a vector quantizer subsequent to the encoder's initial six layers, delineated in Figure 3. Importantly, the integration of an additional positional embedding post-quantization enhances temporal information. The combination of Encoder1 and vector quantizer is considered as the"}, {"title": "2.4 Voice Generation Model: Cosy Voice", "content": "CosyVoice, a family of fundamental speech generation models (Du et al., 2024a), utilizes S\u00b3 to- kens to synthesize natural-sounding voices suitable for various applications. As a versatile model, CosyVoice excels in tasks such as generating multi-lingual voices tailored to specific speakers, adapting to new speakers without training (zero-shot in-context learning), replicating voices across different languages (cross-lingual voice cloning), creating emotionally resonant voices, and offer- ing nuanced influence over speech output through instructional text. CosyVoice supports five lan- guages, including Chinese (ZH), English (EN), Japanese (JP), Cantonese (Yue) and Korean (KO). We released three open-source models. The first, CosyVoice-base-300M, excels in accurately rep- resenting speaker identity, adapting to contexts without any finetuning, and cloning voices across languages. The second, CosyVoice-instruct-300M, is adept in generating emotionally expressive voices and allows for meticulous adjustments via instructional text. Lastly, CosyVoice-sft-300M has been fine-tuned on seven multi-lingual speakers and is ready for immediate deployment. All of them share the common model architecture and learning framework. Compared with other open-sourced projects, CosyVoice released a widest spectrum of supporting features as shown in Table 2."}, {"title": "2.4.1 System Overview", "content": "Cosy Voice incorporates an autoregressive Transformer-based language model (LM) to generate speech tokens for the input text. An ordinary differential equation based (ODE-based) diffusion model, flow matching (Lipman et al., 2023), reconstructs Mel spectrum from the generated tokens. Subsequently, a HiFTNet-based vocoder (Li et al., 2023) is followed to synthesize waveforms from the reconstructed Mel spectrum. Dashed models are optional for certain applications, such as cross- lingual cloning and speaker fine-tuned inference."}, {"title": "2.4.2 Model Training", "content": "At the training stage, the autoregressive language model (LM) is trained using a teacher-forcing paradigm. In this process, tokenized text and a left-shifted version of the speech tokens are provided as input to predict the subsequent speech tokens.\nThe flow matching model is developed to estimate the conditional probabilities \\(P(S|X, v, S_{ref})\\), where X and v denote the speech tokens and speaker embeddings (Wang et al., 2023b), respectively. S and \\(S_{ref}\\) represent the Mel spectrum of target and reference speech, respectively. A convolutional Transformer U-Net (Mehta et al., 2023) is employed to ascertain the vector field between the prior distribution and the desired one, which is derived from the optimal transport ODE. The straightfor- ward nature of resolving the OT-ODE allows for a significantly reduced number of iterations during the inference stage, typically only five to ten iterations are required to produce a satisfactory Mel spectrogram. We also employ the classifier-free guidance (CFG) (Ho & Salimans, 2022) technique and mask out the 70%~100% proceeding feature conditions to boost the in-context learning ability.\nFor the synthesis of waveforms from the predicted Mel spectrograms, we utilize a vocoder based on HiFTNet (Li et al., 2023). Modifications have been made on HiFTNet to support streaming gen- eration, including the replacement and redesign of certain components. Complete details regarding these adjustments are available in our released code."}, {"title": "2.4.3 Zero-shot In-context Learning", "content": "Cosy Voice models exhibit zero-shot in-context learning capabilities, allowing for the replication of an arbitrary voice with only a brief reference speech sample. This process entails the careful con- struction of input sequences for the token language model (LM), depicted in Figure 5. For prompt speech and input text in the same language, we merge them to form a unified input, treating the prompt speech tokens as pre-generated. With this input sequence, the autoregressive LM iteratively predicts subsequent tokens until it encounters the \u201cend of sequence\" token \\(E\\). However, when the prompt speech and input text differ linguistically, we omit the text and tokens associated with the prompt to prevent prosodic characteristics of the original language from influencing the target lan- guage. It is important to note that the prompt text, which corresponds to the prompt speech's content, can be transcribed either through human annotation or ASR models, such as SenseVoice. Similar to the prompt text, the prompt tokens are extracted from the prompt speech with S\u00b3 tokenizer."}, {"title": "2.4.4 Instruction Fine-tuning", "content": "To enable further controllability on Cosy Voice, we experiment with integrating additional instruction fine-tuning (Ji et al., 2023). CosyVoice-instruct extends CosyVoice-base with enhanced instruction- following capabilities. Specifically, it supports controllability over various aspects such as speaker identity (i.e., speaker's characteristics), speaking style (including emotion, gender, speaking rate, and pitch), and fine-grained paralinguistic features. These features include the ability to insert laugh- ter, breaths, speaking while laughing, and emphasizing certain words."}, {"title": "3 Dataset", "content": ""}, {"title": "3.1 Training Set for SenseVoice", "content": "Figure 6 provides an overview of the dataset utilized for training the SenseVoice models. The SenseVoice-Small model was trained on an extensive audio data corpus of approximately 300,000 hours, covering 5 languages including Chinese, Cantonese, English, Japanese, and Korean. To fur- ther enhance the multilingual ability of SenseVoice-Large, an additional 100,000 hours of diverse multilingual data were integrated into the training corpus. To obtain rich transcription labels from speech data, we leveraged open-source models for audio event detection (AED) \u00b9 and speech emo-"}, {"title": "3.2 Training Set for CosyVoice", "content": "To train the CosyVoice models, we have amassed a considerable dataset comprising multiple lan- guages. Throughout the collection process, we utilize specialized in-house tools for speech de- tection, signal-to-noise ratio (SNR) estimation, speaker diarization, and separation. Subsequently, pseudo text labels are generated using SenseVoice-Large and Paraformer. These labels undergo a refinement process with the aid of force-alignment (FA) models, which helps eliminate low-quality data and enhances the accuracy of punctuation. For the CosyVoice-instruct model, we fine-tuned CosyVoice-base using instruction training data without incorporating speaker embedding in the autoregressive language model."}, {"title": "4 Experimental Results", "content": ""}, {"title": "4.1 Multilingual Speech Recognition", "content": "Metrics. We use Character Error Rate (CER) to evaluate the models in five languages: Chinese, Cantonese, Japanese, Korean, and Thai, and use the Word Error Rate (WER) for all other languages. Both the ground truth transcriptions and the recognition outputs are standardized using text normal- ization before the error rate calculation, in alignment with the methodology used by Whisper. All Chinese characters were converted into the simplified Chinese version, together with an additional text normalization pipeline3.\nResults in Table 6 show the comparison of Whisper, SenseVoice and Paraformer (Gao et al., 2022, 2023; Shi et al., 2024) on popular open speech recognition benchmark datasets, including AISHELL-1 (Bu et al., 2017), AISHELL-2 (Du et al., 2018), WenetSpeech (Zhang et al., 2022), Librispeech (Panayotov et al., 2015), and Common Voice (Ardila et al., 2019). It can be seen that SenseVoice-S and SenseVoice-L outperform their Whisper counterparts by a significant margin in most test sets except Librispeech.\nFigure 7 illustrates the comparative performance of SenseVoice-Large and Whisper-Large-V3 on a broader range of languages, with or without ground truth LID as input. While SenseVoice-Large performs comparably with Whisper-Large-V3 in general, SenseVoice-Large obtains significantly better performance in languages like Cantonese (Yue), Catalan (CA), and Marathi (MR).\nThe evaluation of inference efficiency is shown in Table 7. The Real-time factor (RTF, the ratio of the transcribing time to the audio length) and 10s Audio Latency (the average time cost when transcrib- ing a 10s audio.) are benchmarked on an A800 machine, with a decoding batch size of 1. For the encoder-decoder-based model (Whipser-S, Whipser-L-V3, and SenseVoice-L), we perform beam search in decoding with a beam size of 5. Owing to its non-autoregressive architecture, SenseVoice- S obtains extremely low inference latency-more than 5 times faster compared to Whisper-small and more than 15 times faster compared to Whisper-L-V3. SenseVoice-L shows close performance with Whipser-L-V3."}, {"title": "4.2 Speech Emotion Recognition", "content": "We evaluate the SER ability of the SenseVoice on 7 popular emotion recognition datasets, including CREMA-D(Cao et al., 2014), MELD(Poria et al., 2019), IEMOCAP(Busso et al., 2008), MSP-"}, {"title": "4.3 Audio Event Detection", "content": "Both SenseVoice-Small and SenseVoice-Large models can classify the audio event in the speech, including music, applause, and laughter. The SenseVoice-L can further predict the start and end position of the audio event, while the SenseVoice-Small can only predict what happened in the audio, with at most one event per utterance. SenseVoice-Small can detect more kinds of events, such as coughing, sneezing, breathing, and crying which could occur during human-machine interaction.\nWe compare SenseVoice with the SOTA audio event detection models BEATs(Chen et al., 2023a) and PANNs(Kong et al., 2020) on different tasks, including environment sound classification (ESC- 50)(Piczak, 2015), baby cry/laugh detection, coughing detection (Coswara)(Sharma et al., 2020) 6 and in-home talkshow event detection. As SenseVoice only predicts the event of our interest, which may not include event categories in other models, we use the F1 score on each event for evaluation. Qwen-audio is also evaluated for comparison.\nWe find that SenseVoice serves as a good audio event classification or detection model, though BEATS and PANNs may have better F1 scores, which may be attributed to two reasons. Firstly, BE- TAS and PANNs can modify the detection threshold to trade-off the accuracy and recall rate to obtain a higher F1 score, but threshold modification is much more difficult for SenseVoice and Qwen-Audio (An interesting discovery is that SenseVoice and Qwen-Audio always have a much higher accuracy than the recall rate, which could be more friendly for the human-machine interaction). Secondly, SenseVoice is trained with ASR data with AED pseudo labeling rather than AED-specific data."}, {"title": "4.4 Preserving Semantic Information by S\u00b3 Tokenizer", "content": "To assess the S\u00b3 tokenizer's ability to preserve semantic information, we compared the recognition performance of the quantizer-augmented SenseVoice-L against its original version and the Whisper- Large V3 model. The models underwent evaluation using the Common Voice zh-CN and en bench- marks, with the findings detailed in Table 9.\nFrom the table, we can see that our S\u00b3 tokens demonstrate robust recognition performance in both the Chinese and English test sets. Notably, on the common_voice_zh-CN set, S\u00b3 tokens surpass the performance of the Whisper-Large V3 model, achieving a 4.14% relative reduction in error rate. This suggests a substantial correlation between S\u00b3 tokens and semantic content. It is worth noting that there is only a single codebook in the S\u00b3 tokenizer with a dictionary size of 4,096 entries."}, {"title": "4.5 Evaluation on Generation Quality of Cosy Voice", "content": "We evaluate the quality of CosyVoice's speech synthesis by examining content consistency and speaker similarity. The \"test-clean\" subset of LibriTTS (Zen et al., 2019) and the test set of AISHELL-3 (Shi et al., 2021) are employed to construct evaluation set for English and Chinese, respectively. For each text in these sets, we randomly select a prompt speech. Content consistency was evaluated using Whisper-Large V3 (Radford et al., 2023) for English and Paraformer (Gao et al., 2022) for Chinese recognition. Speaker similarity was quantified by calculating the cosine similar- ity between speaker embeddings of the generated and prompt speeches, extracted using ERes2Net (Chen et al., 2023b).\nSimilar to other autoregressive language models, we employ a random sampling decoding strategy for our token LM and assessed the synthesis process using five different random seed values: 0, 7, 42, 123, and 1,337. The resultant evaluation metrics were averaged to determine the mean and standard deviation. Additionally, we conducted an ASR re-ranking to demonstrate potential performance improvements in offline mode."}, {"title": "4.6 Evaluation on Emotion Controllability of Cosy Voice", "content": "To verify the emotion controllability, we use the public speech emotion recognition model emo2vec7 (Ma et al., 2024b). We generate and evaluate 100 English utterances for each of the six emotions: happy, angry, sad, surprised, fearful, and disgusted. The content of the synthesized text is designed to match the target emotion. We then measure the accuracy of the predicted emotions from the synthesized speech for each emotion.\nTable 12 shows the comparison of emotion control accuracy between CosyVoice-base and CosyVoice-instruct. For CosyVoice-instruct, the input consists of content text accompanied by a speaking style instruction (e.g., \u201cHappy.<endofprompt>Content Text\u201d). In contrast, CosyVoice- base only receives the content text as input. The results indicate that CosyVoice-instruct with emotional instructions demonstrates a significant improvement over both CosyVoice-base and CosyVoice-instruct without emotional instructions."}, {"title": "4.7 Cosy Voice as a Data Generator", "content": "A straightforward application of CosyVoice is as a data generator to augment the training data of other tasks, such as ASR, speech-to-speech translation (S2ST). Taking the ASR task an example, we conduct an experiment on the Librispeech corpus to evaluate CosyVoice's capability in generating high-quality data. The experimental results are shown in Table 13, where \u201cLibrispeech\u201d denotes the original 960-hour data. \"Syn on LS text\" and \"Syn on LS text\" denote the generated data with the text from Librispeech and MLS training sets, respectively. From the table, we can see that only training on the synthesized data, the ASR model can achieve a comparable result than the original Librispeech training set. Upon integration of them, a notable enhancement in recognition accuracy is observed. An interesting finding is that involving the synthesized data on the MLS text significantly improve the recognition performance. This may indicates that the text diversity is more critical for ASR task than the duration of speech itself. This improvement can be attributed to the varied"}, {"title": "5 Applications", "content": "The FunAudioLLM is an innovative framework designed to facilitate natural voice interactions be- tween humans and large language models (LLMs). By integrating SenseVoice, CosyVoice, and LLMs, FunAudioLLM offers a variety of rich application demos, including speech-to-speech trans- lation, emotional voice chat, interactive podcasts, and expressive audiobook narration. The demos are available at https://fun-audio-llm.github.io.\nBy combining SenseVoice, LLMs, and CosyVoice, we can effortlessly perform speech-to-speech translation (S2ST), as illustrated in Figure 10. SenseVoice is used to recognize the input speech in its original language, the LLM translates the source language to the target language, and Cosy Voice synthesizes the target speech with cross-lingual voice cloning. This allows users to speak in foreign languages using their own voice.\nBy integrating SenseVoice, LLMs, and CosyVoice, we can develop an Emotional Voice Chat appli- cation, as depicted in Figure 11. SenseVoice recognizes the input speech and its emotion and audio event, the LLM generates the response content with a speaking style description, and Cosy Voice produces emotional speech following the given speaking style description.\nBy leveraging SenseVoice, an LLM-based multi-agent system with real-time world knowledge, and CosyVoice, we can create an interactive podcast, as shown in Figure 12. We can use an LLM plugin to fetch real-time daily knowledge, which a content-generation agent then transforms into a podcast script. The Multi-Agent system matches podcast roles, and Cosy Voice synthesizes the voices. Users can also insert themselves into the podcast for interactive dialogues with the Multi-Agent system."}, {"title": "6 Limitations", "content": "Sense Voice has certain limitations that need to be addressed. Firstly, the ASR performance gener- ally remains much lower for under-resourced languages. Secondly, SenseVoice is not designed for streaming transcription. Therefore, future work may focus on developing streamable voice under- standing models based on SenseVoice.\nCosy Voice also has several limitations. Firstly, it supports a limited number of languages. While it can express emotions and speaking styles based on explicit instructions, it cannot infer the appro- priate emotion or style based on the semantic content of the text. Additionally, Cosy Voice does not perform well when tasked with singing. There's still room for improvement in achieving expressive emotional changes while maintaining the original timbre of the voice.\nAnother limitation is that the two innovative models within FunAudioLLM are not trained end-to- end with LLMs. This pipeline approach may introduce error propagation, which could affect overall performance."}, {"title": "7 Authors (alphabetical order of family name)", "content": "\u2022 Keyu An\n\u2022 Qian Chen\n\u2022 Chong Deng\n\u2022 Zhihao Du\n\u2022 Changfeng Gao\n\u2022 Zhifu Gao\n\u2022 Yue Gu\n\u2022 Ting He\n\u2022 Hangrui Hu\n\u2022 Kai Hu\n\u2022 Shengpeng Ji\n\u2022 Yabin Li\n\u2022 Zerui Li\n\u2022 Heng Lu\n\u2022 Xiang Lv\n\u2022 Bin Ma\n\u2022 Ziyang Ma\n\u2022 Chongjia Ni\n\u2022 Changhe Song\n\u2022 Jiaqi Shi\n\u2022 Xian Shi\n\u2022 Hao Wang\n\u2022 Wen Wang\n\u2022 Yuxuan Wang\n\u2022 Zhangyu Xiao\n\u2022 Zhijie Yan\n\u2022 Yexin Yang\n\u2022 Bin Zhang\n\u2022 Qinglin Zhang\n\u2022 Shiliang Zhang\n\u2022 Nan Zhao\n\u2022 Siqi Zheng"}, {"title": "8 Acknowledgment", "content": "We extend our heartfelt appreciation to the developers and contributors of the following open-source projects: FunASR, FunCodec, Whisper, ESPNet, WeNet, SLAM-LLM, Matcha-TTS, and Tortoise. Their innovative efforts and valuable code contributions have significantly inspired our work and facilitated our research. We are also grateful to numerous other projects not explicitly mentioned here, which have equally provided considerable assistance and played an instrumental role in the success of our endeavors."}]}