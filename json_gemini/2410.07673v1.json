{"title": "Multimodal Clickbait Detection by De-confounding Biases Using Causal Representation Inference", "authors": ["Jianxing Yu", "Shiqi Wang", "Han Yin", "Zhenlong Sun", "Ruobing Xie", "Bo Zhang", "Yanghui Rao"], "abstract": "This paper focuses on detecting clickbait posts on the Web. These posts often use eye-catching disinformation in mixed modalities to mislead users to click for profit. That affects the user experience and thus would be blocked by content provider. To escape detection, malicious creators use tricks to add some irrelevant non-bait content into bait posts, dressing them up as legal to fool the detector. This content often has biased relations with non-bait labels, yet traditional detectors tend to make predictions based on simple co-occurrence rather than grasping inherent factors that lead to malicious behavior. This spurious bias would easily cause misjudgments. To address this problem, we propose a new debiased method based on causal inference. We first employ a set of features in multiple modalities to characterize the posts. Considering these features are often mixed up with unknown biases, we then disentangle three kinds of latent factors from them, including the invariant factor that indicates intrinsic bait intention; the causal factor which reflects deceptive patterns in a certain scenario, and non-causal noise. By eliminating the noise that causes bias, we can use invariant and causal factors to build a robust model with good generalization ability. Experiments on three popular datasets show the effectiveness of our approach.", "sections": [{"title": "Introduction", "content": "With the rapid development of social media, people share views and advertise products by posting content on platforms (Liao et al., 2021) like WeChat, Twitter, Instagram, etc. To increase viewership and obtain more advertising revenue, unscrupulous creators misuse these platforms to publish deceptive, poor-quality posts. Such posts are often described with a catchy thumbnail or a sensational headline. For example, the posts have a sexy thumbnail, with curiosity-inciting phrases like \u201cYou Won't Believe\", and \"X Reasons Why\" in their headlines. That would bait readers to click on the linked articles. However, these articles would be unrelated to the thumbnails and headlines of the posts. Moreover, they are often full of hoaxes, rumors, and fake news, which not only degrade users' experience but also affect the credibility of the platforms (Zhu et al., 2023). Thus, the detection of such clickbait posts has great commercial value for social media.\nDue to the large volume of emerging posts, a manual review of the clickbait is infeasible. Machine detection has become a hot topic (Comito et al., 2023). The detective sources can be summarized into two categories (Yadav and Bansal, 2023). The first is based on social behavior. Since bait posts are required to spread in social networks to expand their influence, typical propagation characteristics can be observed. It can be detected based on social metadata like comments, the number of views, shares, likes, etc (Agarwal et al., 2023). That requires a rich collection of user feedback for judgment, but this feedback is often delayed or some users do not even share it. As a result, malicious posts can only be found after they have been widely spread, which is too late for online applications. Another direction is to analyze the post contents. The bait posts often have certain linguistic characteristics in terms of deceptive words, syntax, subjectivity, writing style (Zhou et al., 2020), and even punctuation (Coste and Bufnea, 2021). Besides, there may be abnormal relations between their sub-parts in various modalities, such as a rumor article text with visual thumbnails of an irrelevant actor to attract clicks. Early works design a set of rules to detect these features and relations, but the rules are hand-crafted and non-scalable. Current mainstream methods turn to the neural model for improving scalability. They seek correlations between the post content and labels to make predictions.\nHowever, to escape detection, malicious creators"}, {"title": "Approach", "content": "Fig.(2) shows our framework with four steps. We first extract multimodal features from the posts. By causal inference, we then disentangle the invariant factors from them, and separate the remaining parts into the causal factor and non-causal factor. Finally, we make the prediction based on the invariant and causal factors. Next, we define some notations and elaborate on each component of our approach."}, {"title": "Notations and Problem Formulation", "content": "Clickbait detection. Given a post $x_i$, our task aims to learn a model $F(y_i|x_i, \\Theta_F)$ to predict whether it is clickbait ($y_i=1$), where $\\Theta_F$ is the model's parameter set, $x_i$ denotes the post contents, including the textual headline, visual thumbnail and their linked mixed-modal article. These sub-parts have some deceptive characteristics and relations, such as containing malicious rhetoric like exaggeration, eroticism, bluffing, weirdness, and distortion; the headline or thumbnail is seductive but unrelated to the article. In addition, the creators constantly yield new bait subspecies to avoid being detected and seized. Thus, our target is to find an optimized $\\Theta_F$ by $arg \\min_{\\hat{\\Theta}_F}, L(F(y_i|x_i, \\hat{\\Theta}_F)|D_{tr})$ which can be generalized well to the test set $D_{te}$ with a lowest cost $L(D_{te})$, where $D_{tr}$ is the training set, $L(\u00b7)$ denotes a cross-entropy classification loss.\nEliminating spurious correlations. As displayed in Fig.3.(a), traditional methods often make predictions based on the co-occurrence between post features $X$ and labels $Y$. Ideally, $X$ is expected to reflect the hidden malicious intention of the post. However, it comes from generic encoders without considering the fraud behavior. That inevitably introduces some false correlations, such as wrongly linking some trivial but irrelevant terms or images with non-bait label. Besides, some bait modes are stable while some writing styles would change in various scenarios, such as periods, types, and creators. It is easy to make erroneous predictions without grasping this distinction. A simple solution is to dissociate $X$ into several factors, i.e., an invariant factor ($IC$) that reflects malicious intention, a causal factor for a certain scenario ($SC$), and non-causal noise ($NF$); and then eliminate the effect of noise $NF$ on $Y$, as Fig.3.(b). However, without necessary constraints, the spurious correlations in $NF$ may permeate via $IC$ and $SC$ to harm the prediction of $Y$. To prevent this effect, we introduce a causal structure to regularize these latent variables, as Fig.3.(c). We first use a confounder $C$ to capture mixed relations of three factors under the condition of a certain scenario $S$. We then isolate the invariant $IC$ by blocking the influence of $C$ on $IC$. The bias in $C$ cannot affect $Y$ through $IC$. To fully put away the noise $NF$, we cut the backdoor paths of $C$ and $S$ on $NF$. The $S$ and $C$ can only impact $Y$ via $SC$. In this way, we can independently elicit salient unbiased factors without mutual influence, which can generalize well to new bait subspecies."}, {"title": "Multimodal Feature Extraction", "content": "To fully capture the characteristics of the posts, we extract five kinds of features in multiple modalities.\n(1) Visual Features: We encode each post image (e.g. thumbnail and article figure) by transformer-"}, {"title": "Disentanglement of Invariant Factors", "content": "The generic $x_i$ may contain spurious correlations, which are mostly unreliable across various scenarios. For example, the false correlation between an unrelated actress and the bait label will change in different contexts of posts. In contrast, it is usually stable for the bait behavior which is caused by some common factors, such as underlying language patterns and deceptive habits. By seeking commonalities in various scenarios, we can capture these inherent factors to achieve better robustness.\nCasual Inference: We use an invariance mask $m \\in R^d$ to dissociate the invariant characteristics of the original representation $x_i$ as $ic_i = m \\odot x_i$, where $\\odot$ is the element-wise product operator. The opposite of $m$ is the variance mask, which can extract the variant representation $vc_i = (1 - m) \\odot x_i$. Ideally, $ic_i$ should have identical joint distributions with the unbiased variable across various scenarios. We pursue it by optimizing $m$ with an invariant risk minimization (IRM) (Arjovsky et al., 2019) objective. IRM introduces a penalty for the variation of empirical risks in all scenarios. It tries to mask some features and calculate their impact on the results, so as to find out factors that have a stable and significant impact on the results. That encourages the mask to suppress spurious features and emphasize the causally invariant ones. We formulate IRM loss as a gradient norm penalty over the empirical risk $L_s$ in each training scenario as Eq.(1):\n$L_m = \\frac{1}{|S|} \\sum_{s \\in S} [L_s(m, F_m) + \\alpha ||\\nabla_{F_m}L_s(m, F_m)||^2] + \\beta ||m||^2.$\nwhere $\\alpha$ and $\\beta$ are trade-off factors, the second term is the constraint over the variance of scenarios, and the third one is a regularization term. $L_s = L(F_m(y_i|x_i, \\Theta_{F_m})|D_r)$ is the classified loss on the training subset $D_r$ under the specific scenario $s$. This objective can enforce the mask $m$ to seek stable inherent patterns in the context, instead of learning an average effect of spurious correlations.\nScenario Estimation: The key to optimizing $m$ lies in the division of scenarios. We thus utilize scenarios to infer the spurious correlations and determine each with a set of operations. In each scenario, the posts have certain characteristics, such as"}, {"title": "Dissociation of Causal Factor from Noise", "content": "After separating the invariant factor, the remaining part $vc_i$ is a mixture of causality and noise in a specific scenario. We thus further elicit the valuable scenario-specific causal factor $sc_i$ from $vc_i$. We first project it into a latent embedding space, as $\\xi(vc_i)$. $\\xi(\u00b7)$ is a network with a multi-layer perceptron, which is parameterized by $\\theta_{\\xi}$. When designing $\\xi$, we do not inject any scenario category information. That allows us to adapt to some new scenarios without refactoring $\\xi$ or relearning the parameters. We then output a causal perception mask $\\gamma = Gumbel - SoftMax(\\xi(vc_i), k_d)$ using the Gumbel-SoftMax technique. The mask $\\gamma$ sets the values of some dimension to 0 and retains $k_d$ effective dimensions. By using the mask $\\gamma$, we can extract the causal representation as $sc_i = \\gamma \\odot vc_i$.\nTo facilitate the separation of causal factors from noises, we introduce contrastive constraints based on the causal interventions. After the causal vector $sc_i$ is extracted from $vc_i$, we collect the remaining non-causal one $nf_i = (1 - \\gamma) \\odot vc_i$. $nf_i$ can be used as a contrastive learning signal with the loss as Eq.(4). For a clickbait sample ($y = 1$), its non-causal feature $nf_i$ exactly blocks out the identifiable clickbait characteristics. When replacing the scenario-specific causal representation $sc_i$ with $nf_i$, the prediction is supposed to be the opposite, i.e., as non-clickbait. Accordingly, for the non-clickbait sample ($y = 0$), since $sc_i$ does not indicate any clickbait characteristics, using $nf_i$ for detection would not change the prediction.\n$L_{contrastive} = L(F_{\\varepsilon}(0|nf_i, \\Theta_{F_{\\varepsilon}})|D^{tr}).$"}, {"title": "Prediction and Data Augmentation", "content": "By repeating the running flow (i.e., vector masking \u2192 scenario division \u2192 mask learning) for $T$ times till converged, we can obtain the key causal representations, namely, the invariant causal factor and scenario-specific causal factor. We concatenate these two vectors to train a Multilayer Perceptron classifier, with an objective as Eq.(5).\n$arg \\min_{\\Theta_{F}} L(F(y_i|[ic_i; sc_i], \\Theta_F)|D^{tr}).$\nTo better train the model, we further employ data augmentation to collect pseudo-labeled data. Since clickbait posts need to be widely spread to gain benefits, social behavior is often an effective clue. We thus design heuristic rules based on social metadata, such as share frequency, and viewing time. This metadata may be lacking in new cases, but it is sufficient in historical data. That can provide abundant clickbait cases as training data to reduce labeled costs."}, {"title": "Evaluations", "content": "We fully conducted experiments with qualitative and quantitative analyses to evaluate our approach."}, {"title": "Data and Experimental Settings", "content": "We performed evaluations on three popular real-world datasets, including CLDInst (Ha et al., 2018), Clickbait17 (Potthast et al., 2018) and FakeNewsNet (Shu et al., 2020a). By crowd-sourcing, these datasets were split as bait/non-bait sets with the size of 4k/3k, 9k/29k, and 5k/17k posts, respectively. For each sample, we crawled the original post from its URL to collect multimodal data, such as the thumbnail and creator's profile.\n\\bullet CLDInst covers 7,769 fashion-related posts crawled from Instagram. They are judged by annotators employed from crowdsourcing websites. A total of 4,260 posts are tagged as clickbait.\n\\bullet Clickbait17 contains 38,517 Twitter posts as well as their linked articles from 27 major US news publishers. To avoid bias, a maximum of 10 posts per day was sampled for each publisher, with 9,276 posts tagging as clickbait.\n\\bullet FakeNewsNet is a large-scale multimodal news dataset that contains over 23k articles with tagged fake/real labels from the websites of PolitiFact and GossipCop. It has a rich social context, with 432 fake and 624 real samples from PolitiFact, as well as 5,323 fake and 16,817 real cases from GossipCop. Similar to bait posts, fake samples often suffer from issues like irrelevance, inconsistency, etc. This dataset can be used to evaluate the model's ability to recognize bait-like low-quality content.\nEach dataset was split into train/validation/test sets. We tuned the model on a validation set and reported results on the test set. In addition, we further evaluated the value of data augmentation technique in Appendix D.4. We employed four typical metrics in the field of classification for evaluations, including accuracy (ACC), precision (PRE), recall (REC), and F1-score (F1). To tackle the class imbalance problem, we trained all evaluated methods by using the oversampling technique. To reduce bias, we repeated running 20 times and reported the average performance."}, {"title": "Comparisons against State-of-the-arts", "content": "To verify the effectiveness of our method, we compared it against six typical baselines in the field of clickbait detection.\nAs displayed in Table 1, our method achieved the best performance. The outperformance was over the best baselines (e.g., VLP) on CLDInst, Clickbait17, and FakeNewsNet by 3.78%, 4.66%, and 4.02% in terms of the accuracy metric, respectively. Methods with multimodal features (i.e., MCAN, CPDM, CCD and VLP) showed better performance, since these features provided useful discriminant clues. In addition, our method performed better than the causal baseline CCD. CCD only tackled the misalignment among the textual and visual features but neglected the spurious bias and disguised content that are widespread in bait posts. Besides, we observed on the larger datasets e.g.,Clickbait17 and FakeNewsNet, our outperformance was bigger. The reason may be that spurious bias in these datasets was more extensive, and our debiased gain was greater. To evaluate it, we further selected 500 test samples randomly from each dataset and annotated each post manually. We found that approximately 23%, 26%, 27% of the posts were disguised type, respectively. The datasets Clickbait17 and FakeNewsNet were more complicated, having a larger percentage of disguised samples. Moreover, we provided the precision-recall curves on three datasets in Fig.(4). Our model achieved the best precision across all recall levels. That reflected the effectiveness of our approach in eliminating spurious bias. Ours can obtain a high recall rate and well identify the bait posts dressed up as valid ones."}, {"title": "Ablation Studies", "content": "To gain insight into the relative contributions of each component in our approach, we performed ablation studies on four aspects, including (1) w/o MF that discarded the multimodal representation module and relied solely on text encoders; (2) w/o EICF that dropped the invariant causal factor by removing the IRM regularization in Eq.(1); (3) w/o ENF which threw away the scenario learning module, randomly assigned scenarios to samples; (4) w/o ESCF that removed scenario-specific causal factor by deleting the module in Eq.(4).\nAs shown in Table 2, the ablation on all evaluated components led to a significant performance drop. This reflected that all four modules were indispensable. Among them, discarding the EICF module caused the most substantial decrease, i.e., more than 6.33% and 6.77% drops in terms of the accuracy and F1 metrics, respectively. That indicated the usefulness of eliminating spurious bias, which can improve discriminant and generalization ability. The multimodal module was also impactful, with its removal leading to a reduction of around 4.61% ~ 5.63% in terms of accuracy. This demonstrated the benefits of inter-modal signals to overcome unimodal one-sidedness. In addition, the ENF and ESCF modules focus on scenario-specific and non-causal factors, which also led to noticeable performance drops when they were ablated. The results validated the rationality of our design."}, {"title": "Study of the Mask Mechanism", "content": "Rrelying on the mask vector m, our model can extract a causal invariant factor applicable to most scenarios. We observed that the setting of m can be binary (B) or float (F), and it can be integrated into the objective Eq.(1) by the $L_2$ norm or a $L_0$ regularizer. To better understand how the mask m impacted the performance, we tested four m configurations: (B + $L_0$), (B + $L_2$), (F + $L_0$) and (F + $L_2$). As shown in Fig.(5), we found that the float masks (F) consistently outperformed binary masks (B). This verified that keeping the continuous mask values provided more representational power than binary ones. In addition, $L_2$ regularization worked better than $L_0$. Among all datasets, $L_2$ norm outperformed $L_0$ norm by around 1.23% ~ 2.01% on the F1 score. The sparse $L_0$ regularization might suppress informative dimensions, while $L_2$ allowed more flexibility."}, {"title": "Study of the Scenario", "content": "Scenario is valuable to learn invariant representations. These stable features can effectively improve the robustness of the model and avoid being deceived by disguised content. To study the characteristics of scenarios, we first checked their separability. It was estimated from the data based on alternating optimization. That is, the samples were grouped to form new scenarios; in turn, after the scenarios were updated, the samples were moved to partition again. If most samples no longer changed, the scenarios can be viewed to be divisible. Thus, we calculated the ratio of moved samples to infer the stability of scenarios. If a sample cannot fit the current scenario and needed to be reassigned to a new one, we counted it as a moved case. As shown in Fig.(6), the curves of the moved rate on three datasets converged after being repeated around 10 rounds. That indicated the scenarios can be separated and help to capture the spurious bias well. Moreover, the curve implied the appropriate loop count for the model optimization. Moreover, we evaluated the scenario size setting in Appendix D.2."}, {"title": "Case Studies", "content": "Furthermore, we conducted case study to see the actual effect of our model. Given a dataset like Clickbait17, we predicted a score for each test sample. The highest score indicated the scenario it belonged to. After classifying all samples, we selected 5 scenario sets with the largest size of samples. We randomly chose 20 bait and 20 non-bait samples from each scenario, and visualized their features by the dimension reduction tool t-SNE. As shown in Fig.(7), subgraph (a) showed the original features cannot differentiate scenarios; (b) presented the causal invariant features learned by our model. These features would be helpful for prediction, but they could not distinguish scenarios. The samples formed loosely separated clusters but remained disorganized within each cluster; (c) visualized the scenario-specific causal features. Here, scenarios were separated into distinct clusters well. The features can be roughly divided into two groups, but the inter-class discrimination was insufficient. This demonstrated the need to combine invariant and scenario-specific causal factors for robust prediction; (d) depicted a noise factor. Since the noises in each scenario are unique, they may separate the scenarios but it is weak to identify true/false samples. Overall, these results demonstrated how our model elicited key causal factors and discarded spurious correlations, which enabled reliable prediction."}, {"title": "Related Work", "content": "Clickbait detection is a hot research topic that aims to predict misleading and sensational social posts. The predictive sources can be divided into two categories. The first one is based on social activity. Malicious creators keep posting clickbait and ceaselessly evolve new subspecies to avoid detection. Some researchers propose to examine this bait behavior based on the creators' account profiles and activity metadata. Nevertheless, some valid creators would produce both high-quality and poor posts, which easily led to false positive predictions. On the other hand, clickbait posts often need to be widely spread on social networks to enhance their influence. These posts should have remarkable propagation characteristics, such as a unique spread path, a large number of shares but a short reading time due to dissatisfaction, etc. The propagation-based methods had been proposed , but they cannot tackle new posts due to lack of user feedback. Thus, such methods often perform high accuracy, but low coverage. Also, they have an alert delay, which would bring a lot of losses and could not support online applications.\nAnother direction is based on the post's content. The bait posts often have certain language characteristics, such as delusive words, fallacious phases, hot topics, raged emotions , linguistic stylometry , etc. To detect them, early works design rules based on various features, such as the semantic , linguistic , or multimodal ones . However, these rules are hand-crafted and non-extensible . To tackle this issue, current research turns to data-driven neural networks. Various network architectures have been proposed, such as RNN , CNN . More advanced techniques like feature attention  and graph attention mechanisms  have been developed. To learn representations from rich unlabeled data , some researchers explore the pre-trained language models (PLMs), like BERT and RoBERTa . They are fine-tuned to fit the classified tasks , so as to use their embedded semantic knowledge to facilitate detection . Besides, some works point out that the clickbait was not only in the unimodal like the text  but also in multimodal , such as giving an attractive image with an unrelated article. They propose to incorporate more features in multiple modalities . For model training, there are other studies on domain adaptation  and data augmentation  to solve the data shortage problem by knowledge transfer.\nDifferently, we found that traditional representations have spurious correlations. Malicious creators would exploit this bug to yield a large number of new subspecies by rewriting the posts' content, such as adding some valid but unrelated terms or images, and concealing the bait content in a normal post, etc. In this mixed content environment, the existing model easily misjudges the bait posts due to spurious bias. We thus propose a debiased method to tackle this problem by causal inference.\nCausality-inspired methods are a hot research topic in many tasks . To grasp the causality in the tasks, some works proposed the causal loss function , while others designed a task-related causal structure to guide decision-making . They pointed out the value of a good representation for the model's performance . Some studies propose to capture the salient representations by feature engineering or noise filtering , but they ignore spurious correlations. In contrast, we propose to elicit key factors to eliminate this bias."}, {"title": "Conclusion", "content": "This paper studied the task of detecting clickbait in multimodal social media posts. Existing methods learned shallow features that introduced spurious correlations, rather than capturing inherent factors that caused clickbait. Malicious creators used this spurious bias to form new bait subspecies by rewriting the posts with tricks, such as disguise with valid content, leading to misjudgment and poor robustness. To tackle this problem, we proposed a new debiased framework by causal representation inference. In detail, we first extracted multimodal features, including textual, visual, linguistic, cross-modal, and creator profile features. By causal inference with structural constraints, we then disentangle them into three latent factors, including the invariant factor that indicated inherent bait intentions, a causal factor for a certain scenario, and noise factor. Based on invariant and causal factors, we can build a robust model. Moreover, we propose a data augmentation technique to reduce training costs. Experimental results on three popular datasets shown the effectiveness of our model."}, {"title": "Limitations", "content": "Detecting clickbait posts on social media is a challenging task. In real application, this task comes up against issues such as biases, multimodal content, evolving bait subspecies, few labeled resources, etc. Moreover, malicious creators would constantly disguise the bait posts with tricks such as replacing confusing terms or images pictures to escape detection. This replaced content is full of spurious correlations which would make the model misjudgment. That would seriously affect the model's robustness. We thus propose a debiased model, which can find the posts with inconsistency on the thumbnail-article and headline-article pairs, such as this thumbnail and headline are catchy and sensational, but irrelevant to the article. Although the current model is effective, there is still room for improvement. For example, it does not verify whether the events described in the post are true or fake. Also, it does not cover the detection of video bait posts. Fake content detection is another task and remains an open challenge. A possible solution is to build a trusted knowledge base. We will investigate more data modalities in future work."}, {"title": "Ethics Statement", "content": "The technology proposed in this paper can be used to filter the bait posts. That can improve the user experience and purify the online environment. Unlike traditional methods based on shallow features, our model is more robust by eliminating spurious bias. Excluding the misusage scenarios, there are few or even no ethical issues with this technology. However, it is essentially a classification method, and the classified results may be misused. For example, it may be abused by malicious persons to filter out the posts created from certain commercial competitors unfairly. This problem can be addressed by analyzing the source and publisher of the posts."}]}