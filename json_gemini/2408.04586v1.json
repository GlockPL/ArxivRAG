{"title": "Sampling for View Synthesis: From Local Light Field Fusion to Neural Radiance Fields and Beyond", "authors": ["Ravi Ramamoorthi"], "abstract": "Capturing and rendering novel views of complex real-world scenes is a long-standing problem in computer graphics and vision, with applications in augmented and virtual reality, immersive experiences and 3D photography. The advent of deep learning has enabled revolutionary advances in this area, classically known as image-based rendering. However, previous approaches require intractably dense view sampling or provide little or no guidance for how users should sample views of a scene to reliably render high-quality novel views. Local light field fusion proposes an algorithm for practical view synthesis from an irregular grid of sampled views that first expands each sampled view into a local light field via a multiplane image scene representation, then renders novel views by blending adjacent local light fields. Crucially, we extend traditional plenoptic sampling theory to derive a bound that specifies precisely how densely users should sample views of a given scene when using our algorithm. We achieve the perceptual quality of Nyquist rate view sampling while using up to 4000x fewer views. Subsequent developments have led to new scene representations for deep learning with view synthesis, notably neural radiance fields, but the problem of sparse view synthesis from a small number of images has only grown in importance. We reprise some of the recent results on sparse and even single image view synthesis, while posing the question of whether prescriptive sampling guidelines are feasible for the new generation of image-based rendering algorithms.", "sections": [{"title": "1 Introduction and Basics of Light Field Sampling Theory", "content": "This article is written in response to the Frontiers of Science Award generously granted in 2024 to the paper [27] on Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines from SIGGRAPH 2019 (published in the ACM Transactions on Graphics). The joint first-authors of this work were then UC Berkeley Ph.D. students Ben Mildenhall and Pratul Srinivasan\u00b9, collaborators at FYusion, Rodrigo Ortiz-Cayon and Abhishek Kar, Ren Ng at UC Berkeley, former UCSD Postdoc (current TAMU Faculty) Nima Kalantari and myself. All authors made key contributions to enable the groundbreaking algorithm and results from local light field fusion, and many have continued to push the field forward in a remarkable sequence of subsequent papers.\nThe local light field fusion paper (LLFF)\u00b2 tackles the core view synthesis problem within image-based rendering (IBR). To create compelling virtual experiences, we need to immerse the viewer within the scene.\nLLFF seeks to develop a simple sample-and-reconstruct approach to view synthesis. As with any sampling problem, one is ultimately limited by the Nyquist rate, depending on the frequencies in the original signal. Ideally, one simply captures images on a (semi)-regular grid, and interpolates them. However, the Nyquist rate view sampling is intractable for scenes at interactive distances as the required sampling rate increases linearly with the reciprocal (disparity) of the nearest scene depth. For a scene with a subject at a depth of 0.5 meters captured by a mobile phone camera with a 64\u00b0 field of view and rendered at 1 megapixel resolution, the required sampling rate is an intractable 2.5 million images per square meter. LLFF seeks to employ sophisticated light field sampling analysis and a data structure based on a multiplane image [43] to reduce the Nyquist rate by a factor of about 4000\u00d7, thus enabling a tractable number of images (typically 20-50) to be used for view synthesis on fairly large baselines, with casual mobile phone capture and only simple light field reconstruction and interpolation.\nThe LLFF work leverages and builds strongly on a seminal paper from SIGGRAPH 2000 on plenoptic sampling [4]. That paper addresses the question of the minimum sampling rate curve for Nyquist rate sampling in image-based rendering. Crucially it argues for a joint geometry-image sampling rate (see Fig. 1), where the number of views depends on the accuracy with which the scene geometry is known. Note that the early light field rendering paper [21] argued that no intermediate representation was required, and one could simply interpolate the originally captured rays. However, the concurrent lumigraph paper [16] did introduce an approximate geometric model, and plenoptic sampling [4] argues that the accuracy of the geometric model influences the sampling rate needed for image capture. Intuitively, if the surface is Lambertian, and we know the geometry exactly, then only one image needs to be captured as all other views will see the same color. On the other hand, if we know nothing about the scene geometry, then many more images would potentially be needed to ensure that interpolation can be done without any aliasing artifacts.\nPlenoptic sampling [4] makes many groundbreaking contributions. First, it performs a theoretical analy-"}, {"title": "2 Local Light Field Fusion", "content": "The first contribution of local light field fusion is in giving precise conditions for practical Nyquist rate view sampling. The theory of plenoptic sampling can be extended, based on the work of Zhang and Chen [42] to account properly for occlusions. As shown in Fig. 2, one can consider the frequency spectrum as being a convolution with the occluder spectrum (multiplication by the occlusion mask in the primal domain). This\nextends the double wedge to a parallelogram, which can only be packed half as densely as the original double wedge. It is possible to derive precisely that the required maximum camera sampling interval Au for a light field with occlusions is:\n$\\Delta_u \\leq \\frac{1}{2K_xf (1/z_{min} - 1/z_{max})},$ \t\t\t\t(1)\nwhere f is the focal length, and Zmin and Zmax are minimum and maximum depths. Kx is the highest spatial frequency in the sampled light field, determined by the highest spatial frequency in the continuous light field\nB2 and the camera spatial resolution Axas,\n$K_x = min (B, \\frac{1}{2 \\Delta_{x,as}} ).$\t\t\t\t\t\t(2)\nOne can now break the scene into layers using a multi-plane image. Following [43], this is a set of fronto-parallel RGBa planes, evenly sampled in disparity within a reference camera's view frustum. Each image is \u201cpromoted\u201d to an MPI through a simple deep-learning algorithm that looks at the image and its neighbors. Rendering from an MPI is straightforward, just involving image compositing. Key for our purposes is that plenoptic sampling theory shows that decomposing a scene into D depth ranges and separately sampling the light field within each range allows the camera sampling interval to be increased by a factor of D. This is because the spectrum of the light field emitted by scene content within each depth range lies within a tighter double-wedge that can be packed D times more tightly than the full scene's double-wedge spectrum. A key aspect of the local light field fusion paper is to extend this simple analysis, conducted without considering occlusions, to also handle occlusions, taking advantage of the predicted opacities in a multiplane image. We show that with this extended analysis, we can still increase the require camera sampling interval by a factor of D when there are D depth layers so that,\n$\\Delta_u \\leq \\frac{D}{2K_xf (1/z_{min} - 1/z_{max})}.$\t\t\t\t\t\t(3)\nA further condition is obtained from the finite field of view, requiring that each point in the scene's bounding volume should fall within the frusta of at least two neighboring sampled views. It can be shown that this can"}, {"title": "3 Analogy with Sampling for Monte Carlo Rendering and Denoising", "content": "Having described the basic local light field fusion algorithm, we will now provide some insights and a brief perspective on related work and subsequent efforts. We note again that this is not intended to be a comprehensive survey, but rather some specific thoughts and comments from the author of this article.\nFirst, we draw analogies to a somewhat different area of physically-based Monte Carlo rendering, where sampling, reconstruction and denoising are typically used nowadays to create synthetic computer graphics imagery for both real-time applications like games and offline applications like movies. Since few researchers have worked on both Monte Carlo rendering and view synthesis, these connections are not usually well appreciated, and so we seek to briefly highlight them in this article. Moreover, this will provide back-ground for some of our later thoughts in terms of sampling analysis of newer methods. For a somewhat dated survey on Monte Carlo sampling and reconstruction, readers are referred to [45].\nMy (Ravi Ramamoorthi's) group has been working on a sample-and-reconstruct framework for low sample count Monte Carlo rendering (now known as Monte Carlo denoising), starting with our work on motion blur in 2009 [12]. In that work, we showed how the same plenoptic sampling theory explored sub-"}, {"title": "4 Perspective on Further Advances in View Synthesis and Challenges", "content": "We now very briefly discuss further work on view synthesis. The subsequent year after the local light field fusion paper, we developed the neural radiance field method for view synthesis (NeRF) [28, 29], which has become the method of choice for view synthesis and a variety of related topics well beyond computer graphics. A number of new repreentations have also been introduced, of which perhaps the best known are instant neural graphics primitives [31] and gaussian splatted radiance fields [20]. Note that the NeRF paper is now approaching 10,000 citations and this is by no means an exhaustive list of the work in the area. NeRF was recognized by a Frontiers of Science Award in 2023, and we encourage readers to read my perspective on that [33], which also has links to surveys.\nAs noted in joint first-author Pratul Srinivasan's ACM award-winning dissertation, NeRFs can be seen\nSince this article is devoted to the local light field fusion method, we will however focus the remainder of this paper on the question of sampling rates. Indeed, the goal of local light field fusion was to reduce the Nyquist sampling rate by orders of magnitude in a principled way to enable sparse capture. This is akin to Monte Carlo rendering discussed earlier, where light field signal processing theory and Monte Carlo denoising have enabled substantially lower sample counts. Like in Monte Carlo rendering, the first wave of methods based on principled light field theory achieved sampling rates of a few tens of images (samples per pixel for Monte Carlo). This was in itself a remarkable advance. However, the next generation of algorithms based on deep learning showed an even more dramatic reduction in sample counts, down to one sample per pixel in Monte Carlo rendering, and we are taking a similar trajectory with corresponding impact in view synthesis.\nTo provide just some examples in recent work, our recent paper enables extremely sparse sampling rates, often just 3-6 images, and does not require the initial step of estimating camera pose (Jiang et al. [19]). Even for the single-image case, remarkable advances has been made in the past year. My group had a few early papers [36, 17, 22], and current methods [25, 24, 14] have demonstrated remarkable fidelity from only a single image. This is in many ways the holy grail, enabling one to take legacy 2D photo collections and turn them into immersive 3D experiences, or to use 2D image generators in generative AI, and create 3D versions for free. A number of startups and established companies are exploring all of these directions, in addition to academic research. The analogy with Monte Carlo rendering is also clear, in that just as those methods have pushed to one sample per pixel rendering as the default, at least for real-time applications,"}, {"title": "Conclusion", "content": "The ability to take a few photographs and capture the appearance of a real scene, to then be able to re-render it seamlessly from other viewpoints is a key challenge in computer graphics, computer vision and virtual reality, referred to as image-based rendering or view synthesis. We have been fortunate to receive back-to-back frontiers of science awards for our papers on local light field fusion and neural radiance fields to address this problem. The current article pertains to the SIGGRAPH 2019 paper on sampling for view synthesis with local light field fusion, where the key contribution is actually a frequency domain analysis of the light field or plenoptic function, which enables prescriptive guidelines regarding how many images to take and where to sample views. In particular, we show that by predicting a 64-layer multiplane image, one can reduce the number of views needed by 642 or 4096\u00d7, which enables view synthesis from a sparse set of images, and makes the method practical with rigorous sampling gurantees. This paper also represents one of the only works with a deep learning component that provides formal theoretical analysis.\nSubsequent work on neural radiance fields and extensions has generalized the discrete volumetric rep-resentation of the multiplane image to continuous volumetric representations that have provided the highest visual quality for view synthesis, even enabling a number of novel applications in domains well outside computer graphics, and connecting with advances in modern artificial intelligence. Besides practical appli-cations, a slew of current methods enable very sparse view synthesis, with excellent results often available from only a single input image, with the potential to take legacy 2D photographs and promote them to full 3D immersive experiences.\nIn many respects, this progression parallels the development of methods for Monte Carlo image de-noising that started with similar theoretical foundations based on a frequency analysis of light transport, followed by deep learning approaches that generalized the earlier methods and drove sample counts down dramatically to one sample per pixel. In both cases, the tremendous practical progress has overtaken the-ory, and there are unfortunately very limited or no theoretical foundations on sampling rates and rigorous sampling guarantees for current methods in either view synthesis or Monte Carlo rendering. At one level, this is unavoidable; once one is working in the limit of a single input image or one sample per pixel, it is unclear if there is a meaningful sampling theory. However, in both cases, image quality does improve as more samples are taken, and a theoretical or even empirical analysis of the image quality versus number of samples tradeoff, comparable to the original plenoptic sampling paper, would be very welcome."}]}