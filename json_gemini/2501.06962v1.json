{"title": "Compact Bayesian Neural Networks via pruned MCMC sampling", "authors": ["Ratneel Deo", "Scott Sisson", "Jody M. Webster", "Rohitash Chandra"], "abstract": "Bayesian Neural Networks (BNNs) offer robust uncertainty quantification in model predictions, but training them presents a significant computational challenge. This is mainly due to the problem of sampling multimodal posterior distributions using Markov Chain Monte Carlo (MCMC) sampling and variational inference algorithms. Moreover, the number of model parameters scales exponentially with additional hidden layers, neurons, and features in the dataset. Typically, a significant portion of these densely connected parameters are redundant and pruning a neural network not only improves portability but also has the potential for better generalisation capabilities. In this study, we address some of the challenges by leveraging MCMC sampling with network pruning to obtain compact probabilistic models having removed redundant parameters. We sample the posterior distribution of model parameters (weights and biases) and prune weights with low importance, resulting in a compact model. We ensure that the compact BNN retains its ability to estimate uncertainty via the posterior distribution while retaining the model training and generalisation performance accuracy by adapting post-pruning resampling. We evaluate the effectiveness of our MCMC pruning strategy on selected benchmark datasets for regression and classification problems through empirical result analysis. We also consider two coral reef drill-core lithology classification datasets to test the robustness of the pruning model in complex real-world datasets. We further investigate if refining compact BNN can retain any loss of performance. Our results demonstrate the feasibility of training and pruning BNNs using MCMC whilst retaining generalisation performance with over 75% reduction in network size. This paves the way for developing compact BNN models that provide uncertainty estimates for real-world applications.", "sections": [{"title": "1. Introduction", "content": "Bayesian neural networks (BNNs) and Bayesian deep learning (BDL) allow for the estimation of uncertainties in predictions, providing a more comprehensive and nuanced understanding of the model's output [1, 2, 3]. This capability is crucial for real-world applications where uncertainty quantification is paramount, and the probabilistic nature of BNNs enhances interpretability by providing insights into the network's decision-making process. BNNs utilise Bayesian inference, a probabilistic framework that updates the probability of a hypothesis as new evidence becomes available. It has been widely used in statistical models for parameter estimation, hypothesis testing, and decision-making [4, 5]. Markov Chain Monte Carlo (MCMC) sampling [6, 7] and variational inference [8] methods have been used to implement Bayesian inference. These methods have facilitated sampling (training) of robust BNN models [9, 2, 10], and have been capable of adapting to data that are either limited or noisy [11, 12].\nAlthough BNNs have shown promise in addressing issues such as uncertainty quantification and data scarcity, their adoption in mainstream off-the-shelf models remains limited. This is mainly because BNNs have challenges with convergence and scalability, especially with larger datasets [13, 14, 3, 15].\nPruning neural networks and deep learning models [16, 17] has been a crucial strategy for removing inessential model parameters without any significant effect on the performance [18, 19, 17, 20]. Han et al. [21] demonstrated that a significant portion of weights can be set to zero without a significant loss in performance through additional L1/L2 loss function and gradual network pruning. Pruning methods exhibit significant variability in their approaches, such as structural matrices [22], model quantization [23], model binarization [24], and parameter sharing [21, 25]. Unstructured pruning, also referred to as, parameter-based pruning, targets individual network parameters, resulting in sparse neural networks with a reduced parameter count[18, 20]. Conversely, structured pruning considers groups of parameters, such as entire neurons, filters, or channels, thus leveraging hardware and software enhancement for dense computation and achieving faster training performance[26, 27].\nScoring-based pruning methods use the absolute values of parameters, trained importance coefficients, or contributions to network activations or gradients. Han et al. [21] applied local scoring and pruned a fraction of the lowest-scoring parameters within each structural subcomponent of NNs. There has been work on employed early pruning using lottery ticket hy-"}, {"title": "2. Background and Related Work", "content": "pothesis on deep neural network architectures, achieving faster training and equivalent predictive performance when compared to unpruned dense networks [28, 29]. Liu et al. [30] used scheduling-based pruning strategies to prune all desired weights in a single step, while [21] iteratively pruned a fixed fraction over several steps. Recent advancements in network pruning literature have introduced dynamic pruning methods that remove parameters during training. [31] used magnitude pruning to adjust the pruning rate throughout the training process. Evolutionary algorithms have also gained traction for neural network pruning, allowing for iterative modifications to the network structure based on an initial sparse topology [32, 33]. However, these structured pruning methods typically focus on optimizing network performance without considering uncertainty in the parameters, which is the focus of our study.\nAlthough not directly related, there has been some work in the area of knowledge distillation using Bayesian neural networks and deep learning. Knowledge Distillation [34] provides the means of transferring the knowledge encapsulated in a large and complex deep learning model (the teacher) to a smaller, more efficient model (the student) [35, 34, 36]. Knowledge Distillation can be seen as an approach for reducing network (model) size and complexity. Schmidhuber [35] was the first to demonstrate an instance of the knowledge distillation process through the compression of Recurrent Neural Networks (RNNs). This was later generalised by Hinton et al. [37], showcasing its effectiveness in image classification tasks. Knowledge distillation has since been applied to various domains, including object detection [38, 39], acoustic modelling [40, 41], and natural language processing [42, 43], demonstrating its versatility and utility in enhancing the performance and efficiency of smaller models across a range of applications. Therefore, knowledge distillation and neural network pruning reduce model complexity while maintaining performance, which facilitates efficient deployment in resource-constrained environments.\nRegularisation techniques have been designed to improve generalisation performance, but they can also reduce the complexity of neural networks by creating simpler models that perform well on both training and test data, thereby improving robustness. Regularisation techniques such as dropout [44] have become integral to training deep neural networks, preventing overfitting and improving model generalisation, while reducing model complexity. Dropout regularisation stochastically deactivates a fraction of neurons during training, effectively introducing noise and encouraging the network (model) to learn more robust features. This technique has been widely adopted in various neural network architectures, including deep learning models such as Convolutional Neural Networks (CNNs) [45, 46], RNNs [47, 48], and Large Language Models (LLMs) [49, 50]. However, we note that the conventional dropout strategy is implemented only once, either before or after training to get a sparse network. This may lead to discrepancies between training and inference phases, as no post-pruning model verification is done [51]. Gal et al. [52] framed dropout as a Bayesian approximation and used a variational inference with a Gaussian process to address this problem. The authors argued"}, {"title": "2.1. Neural network pruning methods", "content": "that deep learning models can converge to a finite Gaussian process as dropout is applied. Hron et al. [53] reviewed some of the pitfalls of using variational dropouts, having improper priors and singular distribution, and proposed alterations that use quasi-KL divergence to work with dropouts in optimisation-based approximate inference algorithms. Graves [54] proposed a stochastic variational method for pruning RNNs using a novel signal-to-noise pruning heuristic. The heuristic removed network weights with high probability density at zero. The method provided less than 3% post-pruning performance loss on automatic speech recognition tasks.\nBNNs have received less attention in pruning research, and some key studies are discussed as follows. Sum et al. [55] used extended Kalman Filters to create an adaptive Bayesian pruning to reduce the complexity of the network. Sharma et al. [56] created BPrune, an open-source software package for pruning CNNs with re-parameterization of variational posterior using Bayes-by-backprop [57]. There have been attempts in pruning in BNNs trained by variational inference. Tripe and Turner [15] explored the counterintuitive phenomenon of variational over-pruning by reviewing the impact of selecting variational families such as weight noise and mean-field, and reported that expressive variational families perform worse than simpler ones. Beckers et al. [58] pruned BNNs based on Bayesian model reduction in a greedy and iterative fashion using variance back-propagation and Bayes-by-backprop.\nAlthough variational inference-based approaches in the Bayesian framework have explored pruning [15, 58], MCMC-based pruning remains underexplored. The limited work in this area motivates the need for further investigation into pruning methods tailored to the Bayesian setting, especially in the MCMC sampling context. Developing effective pruning strategies for BNNs in the MCMC framework could unlock their full potential, enhancing model efficiency and scalability while preserving uncertainty estimation capabilities crucial for real-world applications. This is mainly due to the problem of sampling multimodal posterior distributions using MCMC sampling and variational inference algorithms. Moreover, the number of model parameters scales exponentially with additional hidden layers and features in the dataset. Typically, a significant portion of these densely connected parameters are redundant and pruning a neural network not only improves portability but also has the potential for better generalisation capabilities.\nLangevin MCMC sampling combines Langevin dynamics with Bayesian inference to incorporate gradient information in the proposal distribution for effectively sampling BNN posteriors [3]. Gu et al. [59] addressed the issue of high autocorrelation using neural networks with Langevin MCMC sampling using a novel sampler neural networks Langevin Monte Carlo (NNLMC) using a customised loss function not to break the detailed balance condition. Chandra et al. [3] used parallel tempering MCMC to improve the efficiency of sampling BNN parameters for classification and regression problems. Parayil et al. [60] used a decentralised Langevin MCMC for image classification with improved accuracy with enhanced speed of convergence over conventional MCMC sampling. Look et al. [61] used differential BNNs using an adaptation of Langevin"}, {"title": "2.2. Bayesian Inference for Neural Networks", "content": "MCMC for time series and regression problems. Gurbuzbalaban et al. [62] applied a decentralised Langevin MCMC for Bayesian linear and logistic regression tasks in a decentralised setting. Garriga-Alonso et al. [63] used a gradient-guided Monte Carlo sampler incorporating gradient-based proposal distribution that can be used with stochastic gradients, yielding nonzero acceptance probabilities computed across multiple steps for Bayesian deep learning.\nIn this study, we address some of the challenges faced by BNNs by leveraging MCMC sampling with network pruning for obtaining compact probabilistic models. Hence, we present a novel approach for BNN pruning using MCMC sampling for compact model and uncertainty quantification. We sample the posterior distribution of model parameters (weights and biases) and prune weights with low importance during the sampling process, resulting in a compact model. We ensure that the compact BNN retains its ability to estimate uncertainty via the posterior distribution while maintaining the model training and generalisation performance accuracy. We evaluate the effectiveness of our MCMC pruning strategy on selected benchmark datasets for regression and classification problems. We track samples' signal-to-noise ratio [54] in the posterior distribution of the weight/bias post-sampling to potentially eliminate them based on a pruning threshold. This is done post-training to ensure adequate inference. We look at selected regression and classification problems from the literature [64] to measure pre- and post-pruning performance. The problems are chosen to ensure a fair representation of synthetic and real-world datasets. We further investigate if refining compact BNN can retain any loss of performance. Finally, we apply the methodology to a real-world problem of detecting lithologies in reef drill core data from the Great Barrier Reef.\nThe next section 2 provides a brief overview of the work done in pruning literature. We give details of the proposed methodology in Section 3 with an overview of the datsets, model and results in Section 4. Finally, we provide a discussion and summarise our findings in Sections 5 and 6, respectively."}, {"title": "2.3. Langevin Bayesian Neural Networks", "content": "the network based on the parameter values and contributions. However, as opposed to the earlier methods where parameters are removed after training, they may be removed dynamically during training, using simple criteria such as the weight magnitude [68, 69], momentum magnitude [70] and the signal-to-noise ratio of mini-batch gradients [71].\nPruning and model compression techniques have been critical in the efficiency of deep learning models, including resource-constrained environments. Zhu et al. [68] reported that while reducing the number of model parameters can effectively decrease the model size and computational load, overly aggressive pruning can degrade performance, thus necessitating a careful balance. Tung and Mori [72] introduced a compression technique that combined pruning with quantisation, the process of reducing the numerical precision of weights. The study reported high model compression rates while maintaining model accuracy, making it viable for real-time applications where efficiency is paramount. Hoefler et al. [73] reviewed various sparsity techniques and reported that dynamic sparsity, wherein neurons were pruned or grown adaptively based on relevance, provided faster inference and reduced memory use during training and deployment. Yeom et al. [74] developed a pruning criterion based on model interpretability, which leveraged weight importance interpreted through the model's decision-making ability. The method enabled the model to become smaller and more explainable, which is valuable in applications where model transparency is crucial. Zemouri et al. [75] proposed a pruning algorithm that dynamically adjusts model size by pruning or growing neurons during training based on task complexity. This flexibility enables better generalisation and adaptability to diverse problem domains. Vadera and Ameen [76] provided a broad overview of pruning strategies, discussing the balance between model simplification and accuracy preservation. The study examined the efficiency and model performance of unstructured pruning that removed individual weights, and structured pruning that eliminated larger components such as filters.\nPruning techniques have been extensively studied for improving the efficiency of CNNs [17, 25, 77]. He and Xiao [17] surveyed structured pruning techniques for CNNs, emphasising the importance of balancing model accuracy and efficiency. Molchanov et al. [78] proposed a Taylor expansion-based criterion for pruning convolutional kernels in CNNs. The method demonstrated superior performance in transfer learning tasks and large-scale image classification datasets, achieving significant computational reductions. Anwar et al. [79] developed a structured pruning approach for CNNs, where entire convolutional layer filters, rather than individual weights were pruned to allow for substantial reductions in both model size and computational complexity, making it suitable for embedded systems with limited resources. Tang et al. [80] developed a reborn filter technique for pruning in scenarios with limited data by leveraging a filter re-initialisation strategy to recover model performance after pruning. Wang et al. [81] proposed structural redundancy reduction to strategically identify and remove redundant components in CNNs to improve efficiency. In general, these studies underline the growing sophistication of pruning"}, {"title": "2.4. Model and Likelihood in BNNS", "content": "Langevin MCMC sampling [7] is a sophisticated approach that combines gradient information to develop an effective proposal distribution for BNNs [3]. Langevin MCMC sampling utilises gradient information computed via backpropagation to enhance the proposal distribution and demonstrated to improve both convergence speed and accuracy [3, 96]. The Langevin proposal distribution involves blending deterministic and stochastic components through Langevin dynamics. The deterministic component is the gradient computed for a given set of parameters ($\\theta$) obtained from the gradient of the log-likelihood (log-posterior) through backpropagation. The gradient guides the sampling process that provides an adaptive proposal distribution rather than a fixed one as in the case of the standard random-walk proposal distribution. The second component involves a stochastic noise term, typically a Gaussian, added to the gradient update which prevents the sampling from converging prematurely to local optima. This can also ensure that the parameter space is explored more thoroughly with better mixing. By combining these two components, we get the Langevin proposal distribution:\n$\\theta' = \\theta + \\frac{\\epsilon}{2} \\nabla_{\\theta} log P(\\theta|d) + \\eta,$\nwhere $\\epsilon$ is the step size, $\\nabla_{\\theta} log P(\\theta|d)$ is the gradient of the log posterior, and $\\eta$ represents Gaussian noise. The proposal is then"}, {"title": "3. Methodology", "content": "In Langevin MCMC sampling of BNNs, the model and likelihood functions are essential in defining the probabilistic framework. We need to define the prior and likelihood functions differently for regression and classification tasks as the models are different, as given in Equation 2 and 4. We derive our likelihoods and prior from work previously done by [64, 3, 2] as the model and data we use are similar to these works. In the case of regression, we deal with continuous values in the prediction and will use a Gaussian likelihood function. Therefore, use a Gaussian prior distribution of the weights and biases $\\theta$ and an inverse-Gamma distribution over the noise variance $\\tau^2$. In the case of classification problems, we will deal with discrete values in the prediction and use multinomial likelihood. In this case, we do not have the $\\tau^2$ as a parameter that will be sampled and do not need a prior for it. The prior assumes a Gaussian distribution over the weights and biases $\\theta$.\nWe define the log-prior for regression by:\n$log P(\\theta, \\tau^2) = -\\frac{T}{2} log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^T \\theta_i^2 - (1 + \\nu_1) log(\\tau^2) - \\frac{\\nu_2}{\\tau^2},$"}, {"title": "3.1. Pruning Algorithm", "content": "subjected to an acceptance criterion defined by the Metropolis-Hastings ratio, and upon acceptance, it becomes part of the chain for the posterior distribution of weights and biases.\nLangevin MCMC sampling methods can reduce the number of iterations (samples), making Bayesian inference feasible even for high-dimensional deep-learning models. Although BNNs have been around for more than two decades, their implementation has been slow due to the challenge of sampling including a large number of posterior distributions in complex models. More recently, Bayesian deep learning models such as Bayesian autoencoders [96] and Bayesian CNNs [97] have been trained with MCMC sampling that employed gradient information, including Langevin and Hamiltonian MCMC sampling methods. Chandra et al. [97] utilized tempered MCMC sampling with adaptive Langevin-gradient proposals for Bayesian CNNs on multi-class classification problems. Nguyen et al. [12] proposed a sequential reversible-jump MCMC for dynamic BNNs. The model sampled the network topology and the parameters in parallel to look for uncertainty in model structures with comparable performance on benchmark classification and regression problems. Variational inference has been successfully used for Bayesian deep learning models, including CNNs [10, 97] and RNNs [98]. Hamiltonian MCMC uses Hamiltonian dynamics with momentum variables to efficiently explore posterior distributions, while Langevin MCMC uses gradient-based updates with stochastic noise, making HMC more structured but computationally intensive [96]. Furthermore, Langevin MCMC sampling has demonstrated that the quality of posterior sampling improves since the gradient-informed updates ensure that samples concentrate around regions of high posterior probability, resulting in better uncertainty estimates through improved accuracy and reduced credible interval [97, 3, 2]."}, {"title": "3.1.1. Signal-to-Noise ratio", "content": "where $\\sigma^2$ is the Gaussian prior variance on weights,and $\\nu_1$ and $\\nu_2$ are hyperparameters for the inverse-Gamma prior distribution used for $\\tau^2$. We define the log-prior for classification by using a Gaussian prior given by Equation 6:\n$log P(\\theta) = -\\frac{T}{2} log \\sigma^2 - \\frac{1}{2\\sigma^2} \\sum_{i=1}^T \\theta_i^2 .$\nThe likelihood functions also differ between regression and classification tasks, with a Gaussian likelihood for regression and a multinomial likelihood for classification. For an individual data point ($x_i$, $y_i$), the Gaussian likelihood is:\n$P(y_i|x_i, \\theta) = \\frac{1}{\\sqrt{2\\pi\\tau^2}} exp\\left(-\\frac{(y_i - f(x_i;\\theta))^2}{2 \\tau^2}\\right),$\nwhere $f(x_i;\\theta)$ is the model prediction for input $x_i$, $\\theta$ are the model parameters (weights and biases), and $\\tau^2$ is the variance of the observation noise. Therefore, for the entire dataset D with N data points, the Gaussian log-likelihood is:\n$log P(D|\\theta) = -\\frac{N}{2} log(2\\pi \\tau^2) - \\frac{1}{2\\tau^2} \\sum_{i=1}^N (y_i - f(x_i; \\theta))^2.$\nFor classification tasks, where there are K possible outcomes for each observation, the likelihood assumes a multinomial distribution, where each class has a probability $P_k$ associated with it given by a Softmax function. For an individual data point with label $y_i$ in one-hot encoded form (i.e., $y_{i,j}$ = 1 if the class is j, and 0 otherwise), the likelihood is:\n$y_i \\sim Multinomial(P_1, ..., P_k)$,\n$y_i \\in \\zeta, \\zeta = (1, 2, ..., K)$,\n$\\zeta_i = (y_{i,1},..., y_{i,K}),$\n$P(\\zeta_i|x_i, \\theta) = \\prod_{j=1}^K (p_j(x_i;\\theta))^{z_{ij}},$\nwhere $p_j(x_i;\\theta)$ is the probability that the neural network assigns to class j for input $x_i$. For the entire dataset D with N data points, the multinomial log-likelihood is:\n$log P(D|\\theta) = \\sum_{i=1}^N \\sum_{j=1}^K z_{ij} log p_j(x_i; \\theta).$"}, {"title": "3.1.2. Signal-plus-noise ratio", "content": "Graves et al. [54] proposed the signal-to-noise (STN) ratio for pruning BNNs via variational inference with a Gaussian prior. Inspired by this, we apply it to pruning trained Langevin MCMC BNNs as given by;\n$\\frac{|\\mu_i|}{\\sigma_i} < \\lambda,$\nwhere $\\mu_i$ and $\\sigma_i$ are the mean and standard deviation of the i-th weight of the model, respectively. We select $\\lambda$ as a constant that determines the threshold for pruning. We implement pruning by setting the weights to zero, for which this inequality holds. The weights with a much larger variance than the mean will be interpreted as 'noisy' and hence removed."}, {"title": "3.1.3. Datasets", "content": "We also use a slightly different criterion for pruning, called the signal-plus-noise (SPN) ratio [99], defined by\n$|\\mu_i| + \\sigma_i < \\lambda,$\n.Algorithm 1 outlines the method for pruning the BNN using Langevin MCMC sampling, followed by a pruning phase to reduce model complexity, which is also depicted in our framework (Figure 1). We obtain the compact BNN model by remov-"}, {"title": "4. Experiments and Results", "content": "We utilise benchmark datasets that encompass a diverse range of regression and classification tasks, spanning different domains and levels of complexity. Each dataset presents unique challenges, such as varying numbers of classes, features, and in-"}, {"title": "4.1. Data Preprocessing and model selection", "content": "We preprocessed each dataset by normalising all input features to ensure consistency across models. We utilised all features present in each dataset as shown in Table 1 with a standard"}, {"title": "4.2. Evaluation Metrics", "content": "stances, which test the robustness and effectiveness of the pruning methods employed.\nWe employ three datasets for the regression/prediction tasks including Lazer [101], Sunspots [102], and Abalone (regression) [103]. In classification tasks, we employ the Ionosphere dataset, which is a binary classification task comprising 2 classes [104]. We also use the Iris dataset [105] which is prominent in machine learning and the Abalone dataset presented as a 4-class classification task [103]. Our selected datasets span a broad spectrum of classification and regression problems, allowing for a thorough evaluation of the pruning methods across different domains and model complexities. The diversity in class structures, feature spaces, and data types makes these datasets ideal for testing the resilience of the pruning strategies under various conditions.\nIn addition to these benchmark datasets, we used two marine reef drill-core lithology classification datasets from the Integrated Ocean Drilling Program (IODP) Expedition 325 (Great Barrier Reef Environmental Changes) [106] and Expedition 310 (Tahiti Sea Level) 310 [107]. Expeditions 310 and 325 were international scientific efforts to understand past sea-level and climate changes and their impact on coral reef ecosystems. Expedition 310 [107] )conducted in 2005), focused on Tahiti's coral reefs to reconstruct sea levels during the last deglaciation (around 20,000 to 10,000 years ago) by analysing fossil coral reef structure and composition. Similarly, Expedition 325 [106] (conducted in 2010), explored the Great Barrier Reef (GBR). The drilling of selected areas of the GBR has enabled the study of environmental changes over the past 30,000 years [108, 109], particularly focusing on sea-level fluctuations, temperature changes, and their impact on reef growth [110]. The reef core data has been used for analysis using machine learning and computer vision methods, and also for developing geoscientific models such as pyReef-Core which has been combined with Bayesian inference to estimate unknown parameters [14]. Deo et al. [111] used this data to aid in the segmentation of drill core image data.\nThe reef-core (drilled) datasets represent fossil coral reef lithology classification tasks with six distinct target classes on drill cores extracted offshore on these IODP expeditions. These datasets were curated by mapping physical properties measurements taken on reef drill cores measured using a multi-sensor core logger (MSCL) to physical lithologies seen through expert visual analysis of the reef cores. Figure 2 presents a sam-"}, {"title": "4.3. Results and Analysis", "content": "In this study, we use three evaluation metrics to assess model performance, depending on the task associated with each dataset.\n1. The Root Mean Squared Error (RMSE) measures the average squared difference between the actual and predicted values for regression tasks. A lower RMSE indicates better performance, while an increasing RMSE suggests overfitting or poor model generalisation.\n2. The classification accuracy measures the proportion of correct predictions made by the model out of the total predictions. It is a common metric in classification tasks to evaluate how well the model is distinguishing between different classes. Higher accuracy indicates better performance in terms of correctly classifying instances.\n3. The Receiver Operating Characteristic (ROC) curve [112] evaluates the performance of the models, for classification problems typically used for binary classification but can be extended to multi-class classification using a one-vs-all approach [113]. The Area-Under-Curve (AUC) [114] represents the area under the ROC curve, which is a measure prominently used for the accuracy of predictions."}, {"title": "4.4. Benchamrk datasets", "content": "ple drill core section taken from Expedition 325 that has been classified into 3 distinct lithologies. The data comprises 3 features: bulk density, porosity, and resistivity. The physical properties were classified into 6 distinct lithologies (massive coral, encrusted coral, coralline algae, microbialite, sand, and silt). Figure 3 presents the relative abundance of each lithology in the two datasets. Each expedition collected samples across the same categories, but Expedition 325 generally collected more samples, especially in the Sand and Massive Coral categories. In Expedition 325, the 'sand' stands out with a significantly higher sample count (2004 samples) compared to Expedition 310 (262 samples), while 'massive coral' also shows a higher count in Expedition 325 (680 samples) than in Expedition 310"}, {"title": "4.5. Convergence diagnostic", "content": "We begin by evaluating the effect of the different pruning methods, including signal-plus-noise (SPN), signal-to-noise (STN), and random (RND) pruning for the different datasets. Figure 4 evaluates the selected pruning methods for BNNs using three regression datasets: Lazer, Sunspot, and Abalone. We observe that the accuracy (RMSE) generally deteriorates as the pruning level rises, particularly for random pruning. This trend is evident across all datasets, reflecting the challenges of maintaining network performance when a significant proportion of parameters are removed randomly. We observe that the performance accuracy varies across datasets (Figure 4). For the Abalone dataset, the RMSE remains relatively stable for all methods and pruning methods and levels. All the models can capture the intricacies of the Abalone dataset when resampled. In the Sunspot dataset, we see the impact of the structured pruning methods over random pruning at higher pruning levels. We can clearly see that when a large number (75%) of parameters are randomly removed, we cannot recapture the model performance with resampling. In the Lazer dataset, we continue to see the significance of structured pruning as seen in"}, {"title": "4.6. Reef-Core Application", "content": "higher pruning levels (0.75), indicating its inability to preserve critical information. In contrast, SPN and STN consistently maintain higher accuracy, even under severe pruning, highlighting their robustness in retaining essential network parameters. This is consistent with the regression performance.\nThere are two significant differences in the classification and regression model performance. Firstly, we see that the performance of the models on Exp 310 and Exp 325 datasets are lower than those of the benchmark datasets. We attribute this to the increased complexity of the datasets. These datasets also have the highest number of classes being predicted. These datasets also have half the number of input features for the number of predicted classes (3 features to 6 classes). The second significant difference we see is that unlike in regression, STN consistently outperforms both RND and SPN across all classification datasets and pruning levels. The signal-to-noise ratio crite-"}, {"title": "5. Discussion", "content": "Figure 6 presents posterior distributions and trace plots for BNN model parameters for selected datasets, highlighting the effects of pruning and resampling. Each row corresponds to a randomly selected parameter from the posterior of the BNN model trained on datasets Expedition 310, Expedition 325, and Lazer. The left panels represent the initial posterior distribu-"}, {"title": "6. Conclusion", "content": "rion effectively identifies and preserves critical weights, ensuring minimal degradation in model performance. Table 3 also shows that STN has higher classification accuracy and lower standard deviations in the model performance at 75% pruning as compared to SPN. RND consistently underperforms and exhibits higher variability, as evidenced by larger error bars, making it less suitable for high-stakes applications. The inclusion"}, {"title": "7. Data and Code Availability", "content": "tion and trace plots, and the right panels depict the same metrics after pruning and resampling. The initial posterior distributions, observed in the left panels, are relatively wide, reflecting significant variability in parameter estimates. This variability is further corroborated by the trace plots, which show notable fluctuations in parameter values over 25,000 samples. The wide distributions and noisy trace plot regions indicate that the unpruned models are less confident in their parameter estimates, particularly for datasets such as Expedition 310 and Lazer. This suggests that these datasets may contain significant noise or redundant features, challenging the convergence and stability of the MCMC sampling process. The right panels illustrate the posterior distributions and trace plots following pruning and resampling for an additional 900 post-burnin samples. A notable improvement is observed across all datasets, with posterior distributions becoming narrower and more concentrated around specific parameter values. This indicates that resampling postpruning eliminates noisy or redundant parameters, enabling the model to focus on the most relevant features. The trace plots for resampled models demonstrate smoother convergence, with significantly reduced fluctuations in parameter values, further underscoring the stabilising effect of pruning.\nThe results highlight the significant impact of pruning methods, pruning levels, and resampling on the classification and"}, {"title": "7.0.1. Declaration of generative AI and AI-assisted technologies in the writing process", "content": "performance, with AUC values ranging from 0.21 for Class 3 to 0.83 for Class 4. This variability highlights the inconsistency of the pruned BNN in effectively distinguishing between certain classes. Despite this, it achieves reasonable accuracy for some classes (e.g., Class 4). The resampled BNN demonstrates significant improvement over the pruned model, particularly for Classes 2, 3, and 5, where AUC values reach 0.81, 0.81, and 0.94, respectively. In this dataset, we also see that the overall performance remains inferior yet comparable to that of the Random Forest model. We compare our pruned model performance to the Random forest [115] model, as it was the best-performing model for classification in previous work [116, 111]. The Random Forest model demonstrates superior performance across all six classes, with higher AUC; however, it lacks any uncertainty information about model parameters. We can see that the BNN methods can keep up with the performance of Random Forests, whilst also providing probabilistic prediction models."}, {"title": "8. Appendix", "content": "In this study, we systematically investigated the impact of various pruning methods, pruning levels, and the inclusion of resampling on the performance of BNNs across multiple classification and regression tasks. The results demonstrated that structured pruning methods, namely signal-to-noise and signal-plus-noise, significantly outperformed random pruning, particularly at higher pruning levels. The superior performance of structured methods is attributed to their ability to prioritise the removal of less relevant parameters while preserving those critical for accurate predictions. In contrast, random pruning resulted in substantial performance degradation, particularly in more complex datasets, highlighting its inefficiency as a pruning strategy.\nThe incorporation of resampling after pruning substantially enhanced the performance accuracy of pruned networks by enabling recalibration of the remaining parameters. Resampling proved particularly effective in mitigating the adverse effects of unstructured pruning, consistently improving classification accuracy and reducing regression error across all datasets. This underscores the necessity of integrating resampling into pruning workflows, especially for methods prone to performance variability. Robust convergence diagnostics, through Gelman-Rubin analysis, confirmed better convergence of pruned BNNS for regression tasks at high pruning levels, further highlighting the importance of structured pruning methods.\nThe reef exploration datasets, Exp 325 and Exp 310 highlighted challenges such as class imbalance and data sparsity inherent in marine environments. Pruning and resampling techniques effectively maintained classification accuracy, while"}, {"title": null, "content": "Furthermore, achieving reliable results requires robust convergence analysis, as failure to converge or improper diagnostic evaluation can lead to misleading inferences. Our analysis of the Gelman-Rubin rates for the pruned models has confirmed that the models we have generated have stability in their posterior samples. Therefore, we can have confidence in our model's predictive performance. These findings have important implications for optimising deep learning models in resourceconstrained environments. The results demonstrate that structured pruning methods, particularly signal-to-noise pruning, should be preferred for both"}]}