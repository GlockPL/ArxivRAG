{"title": "Retrieval-Augmented Test Generation: How Far Are We?", "authors": ["JIHO SHIN", "REEM ALEITHAN", "HADI HEMMATI", "SONG WANG"], "abstract": "Retrieval Augmented Generation (RAG) has shown notable advancements in software engineering tasks. Despite its potential, RAG's application in unit test generation remains under-explored. To bridge this gap, we take the initiative to investigate the efficacy of RAG-based LLMs in test generation. As RAGs can leverage various knowledge sources to enhance their performance, we also explore the impact of different sources of RAGs' knowledge bases on unit test generation to provide insights into their practical benefits and limitations. Specifically, we examine RAG built upon three types of domain knowledge: 1) API documentation, 2) GitHub issues, and 3) StackOverflow Q&As. Each source offers essential knowledge for creating tests from different perspectives, i.e., API documentations provide official API usage guidelines, GitHub issues offer resolutions of issues related to the APIs from the library developers, and StackOverflow Q&As present community-driven solutions and best practices. For our experiment, we focus on five widely used and typical Python-based machine learning (ML) projects, i.e., TensorFlow, PyTorch, Scikit-learn, Google JAX, and XGBoost to build, train, and deploy complex neural networks efficiently. We conducted experiments using the top 10% most widely used APIs across these projects, involving a total of 188 APIs. We investigate the effectiveness of four state-of-the-art LLMs (open and closed-sourced), i.e., GPT-3.5-Turbo, GPT-40, Mistral MoE 8x22B, and Llamma 3.1 405B. Additionally, we compare three prompting strategies in generating unit test cases for the experimental APIs, i.e., zero-shot, a Basic RAG, and an API-level RAG on the three external sources. Finally, we compare the cost of different sources of knowledge used for the RAG. We conduct both qualitative and quantitative evaluations to investigate the generated test cases. For the quantitative analysis, we assess the syntactical and dynamic correctness of the generated tests. We observe that RAG does not improve the syntactical or dynamic correctness of unit test cases. However, using Basic RAG could improve the line coverage by an average of 8.94% and API-level RAG by 9.68%. We investigate the token cost of different RAGs with different prompting strategies. We find that RAGs using GitHub issue documents have the highest token cost. We also find that using limiting the number of test cases for cost efficiency significantly reduces the cost. Finally, we perform a manual analysis over a subset of the generated tests to evaluate how different strategies impact the software under test in more depth. We find that RAG helps cover unique lines by providing unique states of programs that cannot be generated directly by LLMs. Our study suggests that RAG has great potential in improving unit test line coverage when the right documents with unique examples of program states are given. Proposing new retrieval techniques that can search these documents will further improve RAG-based unit test generation for future work.", "sections": [{"title": "1 Introduction", "content": "Retrieval Augmented Generation (RAG) has seen increasing use in various software development tasks, i.e., code generation [32], code search [11], commit message generation [43], code suggestions [4], assertion generation [25], and automated program repair [25, 36] and shown great improvements. Despite its notable advancements, the potential of RAG in unit test generation remains under-explored. To bridge this gap, in this work, we take the initiative to investigate the efficacy of RAG in test generation. As RAG can be built up on different knowledge bases, we also explore the impact of different document sources on test generation to provide insights into its practical benefits and limitations.\nSpecifically, we consider the effect of three types of domain knowledge for the RAG used in this work: 1) API documentation 2) GitHub issues, and 3) StackOverflow Q&As. We include these sources as they could provide examples of API usage for creating tests. Each source offers knowledge from different perspectives; API documentation with official usage guidelines, GitHub issues offer resolution from the library developers, and StackOverflow Q&As present community-driven solutions and best practices.\nFor our experiments, we focus on five prominent Python-based machine learning (ML) projects, i.e., TensorFlow, PyTorch, Scikit-learn, Google JAX, and XGBoost, which are crucial in modern software development as they provide the foundational tools, frameworks, and abstractions needed to build, train, and deploy complex neural networks efficiently. We evaluate the effectiveness of four state-of-the-art LLMs, i.e., GPT-3.5-Turbo, GPT-40, Mistral MoE 8x22B, and Llamma 3.1 405B. For the RAG database, we utilize three external knowledge sources, i.e., API documentation, GitHub Issues, and StackOverflow Q&As. Specifically, we first extract all APIs of each project from its official API documentation. Subsequently, we collect all relevant StackOverflow Q&As and GitHub issues using specific tags and keywords related to a specific API from StackOverflow and the issue repository. In total, we inspect the unit test generated for 188 APIs across these ML projects, with 19.2K GitHub issues and 22K StackOverflow Q&As.\nWe start by quantitatively evaluating the generated tests based on their Abstract Syntax Tree (AST) parse rates, execution rates, and pass rates. Next, we execute all generated tests to determine the line coverage for each project. Additionally, we analyze the cost implications of various scenarios by calculating the total input tokens used and output tokens generated. Finally, we conduct a manual analysis on a subset of the generated tests to evaluate the impact of different strategies on the software under test in greater depth.\nOur findings indicate that LLMs generate syntactically correct test cases with a minimum average parse rate of 85.37%, an execution rate of 67.85%, and a pass rate of 58.21%. When evaluating RAG-based unit test case generation, we observe promising results compared to zero-shot prompting. Specifically, Basic RAG improves line coverage by an average of 8.94%, while API-level RAG achieves an average improvement of 9.68%. We also analyze the token cost associated with different RAGs. Our findings show that RAG using GitHub issue documents incurs the highest token cost. To enhance cost efficiency, we recommend limiting the number of test cases generated. This approach significantly reduces the overall cost without compromising the quality of the tests. Finally, we perform a manual analysis on a subset of the generated tests to evaluate the impact of different"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Retrieval Augmented Generation (RAG)", "content": "Retrieval Augmented Generation (RAG) is a prompting strategy that enhances the responses of Large Language Models (LLMs) by incorporating additional knowledge sources beyond the pre-trained knowledge base [2]. In RAG, the query intended for the LLM is used to search and retrieve relevant documents from a database [2, 5]. This process leverages external knowledge, such as documents from various sources, to produce more contextually relevant and accurate responses compared to traditional generation-only models [5].\nOne advantage of RAG is that it eliminates the need for retraining, as the LLM can search and access the latest information to generate more reliable output through retrieval-based generation [42]. RAG techniques have been successful in numerous software-related tasks, including code generation [32], code search [11], commit message generation [43], code suggestions [4], assertion generation [25], and automated program repair [25, 36]. However, the effectiveness of RAG relies heavily on the quality of the retrieved knowledge [19]. Therefore, it is essential to assess the impact of various knowledge resources on the performance of RAG-based techniques."}, {"title": "2.2 Prompt Engineering", "content": "Prompt engineering refers to the method by which humans communicate with LLMs. The effective application of prompt engineering is directly associated with obtaining more precise and accurate responses when interacting with LLMs. As LLMs become increasingly popular across various domains, researchers are investigating novel applications of LLMs within the field of software engineering [14, 21-23, 39]. The widely utilized ChatGPT has demonstrated promising results in software engineering tasks, including code generation [23], code review [12], vulnerability detection [33], unit test generation [31, 41], and even research related to software engineering [18].\nVarious prompt engineering techniques have been proposed, including zero-shot and few-shot prompting [3, 15, 17]. In zero-shot prompting, the LLM is prompted with the desired function without any prior examples. Conversely, in few-shot prompting, the LLM is provided with (n) examples of the desired output given a sample input. Generally, few-shot prompting is employed"}, {"title": "2.3 Unit Test Generation", "content": "In the past decade, numerous attempts have been made to automatically generate tests [1, 7, 9, 10, 16, 27, 30]. Derakhshanfar et al. [7] studied the generation of tests using search-based crash reproduction, i.e., replicating crashes from stack traces. Shin et al. [31] proposed a novel approach to unit test generation by leveraging Transformer-based code models, specifically CodeT5, and applying domain adaptation at the project level to improve test quality and coverage. The results show significant improvements in line coverage and mutation scores compared to existing methods like GPT-4 and A3Test. Yuan et al. [41] evaluated ChatGPT's capability in generating unit tests, finding that while it produces tests with comparable coverage and readability to manually written ones, it suffers from correctness issues. To address this, they propose ChatTester, an approach that improves the correctness of ChatGPT-generated tests through iterative refinement. Some studied test generation to increase their code coverage using search-based algorithms, symbolic execution, and genetic algorithms [9, 16, 34, 35, 40].\nIn the machine learning domain, several studies explored generating test cases differently. Zou et al. [44] proposed Ramos a novel hierarchical heuristic test generation approach for ML projects. Their study employed a hierarchical structure of machine learning models to generate more models by mutation techniques. Arunkaleeshwaran et al. [24] proposed MUTester to generate unit test cases for ML projects. Their approach mined API usage patterns from API-related documents from StackOverflow code snippets and usage scenarios [13, 28] to enhance search-based unit test case generation."}, {"title": "3 Methodology", "content": "This section details the methodology of our study, i.e., the research questions, the data collection, the retrieval augmented test generation framework, baseline LLMs, and the evaluation metrics. Figure 1 depicts the overall flow of our study."}, {"title": "3.1 Research Questions", "content": "To investigate the effectiveness of RAG-based LLMs for unit test generation for ML projects, we have devised the following five research questions (RQs) to tackle this:\nRQ1: How effective are current LLMs in generating unit test cases?\nMotivation. This RQ investigates the ability of state-of-the-art LLMs to generate unit test cases without document augmentation. We evaluate four leading models using zero-shot prompting on five ML projects, assessing the effectiveness of both closed and open-source LLMs with a basic prompting strategy. This serves as a baseline for subsequent research questions.\nRQ2: How effective is the Basic RAG approach, when using domain knowledge mined from API documents, GitHub issues, and StackOverflow Q&As as the external source, in improving LLMs ability in unit test case generation?\nMotivation. This RQ examines the impact of the Basic RAG approach on LLMs using documents"}, {"title": "3.2 Data Collection", "content": ""}, {"title": "3.2.1 Studied Projects", "content": "These five projects were selected due to their widespread popularity and adoption in both academia and industry, their large and active communities with extensive documentation, and their significant roles in the field of machine learning. These factors make them ideal candidates for a study aimed at improving unit test case generation using RAG.\n(1) TensorFlow (abbr. tf) is an open-source ML project developed by Google. It's designed for research and production, providing a comprehensive ecosystem of tools, libraries, and community resources to build and deploy ML models and deep learning applications.\n(2) PyTorch (abbr torch) is an open-source ML project developed by Facebook's AI Research lab. It's widely used for deep learning applications, offering dynamic computation graphs and a user-friendly interface, making it popular among researchers and developers for building and training neural networks."}, {"title": "3.2.2 Experiment Data Collection", "content": "The data we need to collect includes: 1) the list of API names, 2) the content of API documentation, 3) the content of GitHub issues, and 4) the content of StackOverflow Q&As. Initially, we gathered all API lists from their respective API documents. For each API, we extract the following information to compose the API documentation: 1) the signature information with parameter details (name, position, and type), 2) the natural language description detailing the functionality of the API and its parameters, and 3) example code demonstrating correct usage patterns, if available.\nFor the second RAG source, we mine closed issues from their GitHub repositories, specifically extracting the title, description body, and associated comments. For the final RAG source, we collect relevant StackOverflow questions using the tag feature and the associated voted answers. Each API document, GitHub issue thread, and StackOverflow Q&A pair is treated as a separate document chunk for the RAG database. We truncate these documents to a 5K token length to reduce costs and comply with the context windows of different LLMs.\nWe apply a simple string-matching heuristic to filter out documents irrelevant to API usage. Specifically, we only include GitHub issues and StackOverflow Q&A documents that mention any of the APIs for the RAG document. We link the mentioned APIs to each document and use this mapping information for the API-level RAG approach. More details on the API-level RAG are discussed in Section 3.3. Any APIs not mentioned in either source (issues and Q&As) are excluded from data collection. This results in a many-to-many relationship between issues and Q&A documents with APIs, while API documents maintain a one-to-one relationship.\nAfter mapping all documents to an API, we rank them by the number of documents. To ensure a balanced score, the ranking scores are calculated using the harmonic mean of the number of issue documents and Q&A documents. The distribution of the final number of APIs after filtering and the number of documents is presented in Table 1. To ensure that APIs have sufficient documentation and make the experiment cost-feasible, we select the top 10% APIs from the ranking as the target APIs, involving a total of 188 APIs."}, {"title": "3.3 Retrieval-Augmented Test Generation", "content": "We use the RAG approach to search relevant software engineering documents and augment the query to assist the LLMs in generating sounder and practical unit test cases for ML projects. We consider two settings for the RAG approach, i.e., the Basic RAG and the API-level RAG."}, {"title": "3.3.1 RAG Database Creation", "content": "The Basic RAG approach stores all documents in a single database, reflecting the most common cases of RAG-based LLMs [4, 25, 37]. To investigate the impact of different RAG sources, we construct four types of Basic RAG databases: 1) all API documents combined, 2) all GitHub issues combined, 3) all StackOverflow Q&A combined, and 4) a combination of the aforementioned three document types. As mentioned previously, we discard all documents irrelevant to API usage. When filtering these documents, we additionally associate documents with APIs using simple string-matching heuristics. We use this mapping information to construct the RAG database for each API, which strongly constrains the search to relevant documents. This method provides a solid constraint for retrieving only pertinent documents. Consequently, we create a database for each API to search within when generating unit test cases for a particular API, defining the API-level RAG approach.\nWe construct the vector databases for the RAG using the chromadb7 library, an open-source vector database designed for storing vector embeddings. It employs the nearest neighbor embedding search for efficient semantic search, utilizing Sentence Transformers [29] to embed the documents."}, {"title": "3.3.2 Prompting LLMs", "content": "We task an LLM with generating unit test cases for each API to ensure maximum coverage of its functionality. Additionally, we require generating new tests only if they cover new lines, allowing the model to determine the number of test cases needed autonomously. The generated test suite must utilize the unittest library and include all necessary imports and main functions to ensure the code is directly executable. Figure 2 illustrates the prompt used for zero-shot prompting.\nWhen augmenting the prompt with documents retrieved from the RAG database, we instruct the model to use the provided documents to make the test case more compilable, and passable and cover more lines. The additional prompt we use for augmented generation is listed in Fig. 3. For Basic RAG prompting, we set the number of retrieved documents to three, as previous research [4] indicated that metric improvements plateaued beyond this number. Specifically, we retrieve the top three API documents for Basic API-documentation RAG, the top three issue documents for Basic"}, {"title": "3.3.3 LLM Baselines", "content": "The following LLM baselines are chosen due to their recency, state-of-the-art performance, and diverse architectures. These models represent a range of capabilities and architecture designs, from the highly versatile and widely adopted GPT series to the specialized mixture of experts (MoE) approach in Mistral, and the efficient, Llama model. We also diversified the open and closed-source models where the GPTs being the two closed and Mistral and Llama being the open-sourced models.\n(1) GPT-3.5-turbo is an LLM developed by OpenAI, which is specialized for its enhanced efficiency and cost-effectiveness. The model was shown to perform strongly due to its auto-regressive architecture and vast training data and parameters.\n(2) GPT-40 is a successor LLM of GPT-3.5-turbo with additional multi-modal capabilities, increased context window, and trained on more data and parameters to balance efficiency and improved performance.\n(3) Mistral 8x22B instruct is a Mixture of Experts (MoE) model with 176 billion parameters, designed for high efficiency and performance. It features a large context window of up to 65k tokens, enabling it to handle extensive and complex interactions. This model excels in tasks requiring strong reasoning, coding, and multilingual capabilities.\n(4) Llama 3.1 405B instruct [8] is an LLM developed by Meta. It is a state-of-the-art language model that utilizes an optimized transformer architecture. It is fine-tuned using supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety."}, {"title": "3.4 Evaluation Metrics", "content": "We evaluate the generated test cases both statically and dynamically, utilizing the following commonly used metrics.\n(1) Parse rate (abbr. PR%): Here we calculate the AST parsability to check the syntactical correctness of the generated unit tests. The parse rate is calculated at the test suite level since we prompt the models to generate a directly runnable standalone test suite.\n(2) Execution rate (abbr. EX%): If the test suites are parsable, we execute the test cases to check the execution correctness. We subtract the methods that induce error from the total test cases and divide by the total number of parsable test cases."}, {"title": "4 Experiment Design and Results", "content": ""}, {"title": "4.1 RQ1: Performance of LLMs with Zero-Shot Prompting", "content": "Design. In this RQ, we investigate the general performance of LLMs' capability using zero-shot prompting to generate unit test cases. Specifically, we prompt the models to generate unit test cases for each API without augmenting them with external sources. We extract only the Python code part from the response as LLMs tend to generate verbose natural language explanations.\nWe parse the generated test suite using the ast built-in module and calculate the parse rate. For the parsable code, we save it to a Python file and execute it. We then parse the log to identify unit test cases that result in errors or failures, thereby calculating the execution and pass rates. For line coverage, we use coverage.py12 to obtain the coverage report. We derive the coverage score of the class defining the API from the coverage report to assess the coverage of the affected class.\nIt is important to note that these coverage reports do not explore the library's back-end implementation, such as the C++ implementation of TensorFlow, which many of these libraries rely on for faster computation. Investigations into the back-end implementation must incorporate each library's specialized testing framework, which requires a new level of expertise. Therefore, we leave this aspect for future work.\nResult. Table 2 presents the results of LLMs in generating unit test cases across five projects. Most state-of-the-art (SOTA) LLMs are capable of generating syntactically correct unit test cases, with parse rates ranging from 63% to 100%. Additionally, most LLMs were able to generate a substantial number of executable unit test cases, with some exceptions.\nGPT-40 exhibited a low execution rate of 42% for the xgb library. This was primarily due to its generation of complex unit tests that required references to unimported packages, failing to execute most test cases. Similarly, Mistral failed to generate executable test cases in certain instances, with a minimum execution rate of 26%. This failure is attributed to the model's inability to generate directly runnable test suites from the prompt. Mistral tends to produce a sequence of unstructured code fragments and natural language, including unnecessary elements such as the code base of the actual API and its explanation. These individual code pieces lack the necessary structure and dependencies, such as the main class, unit test functions, and imports, leading to low executability.\nGPT-40 consistently achieved the best overall results, ranking first in parse rate and coverage, second for execution rate on xgb, and third for execution rate on sklearn. It also ranked second for pass rate on sklearn, xgb, and jax. GPT-3.5-turbo and Mistral generally ranked in the middle (second to third), with GPT-3.5-turbo securing second place three times and Mistral twice for code coverage. Both models demonstrated good parse rates and relatively high execution and pass rates."}, {"title": "4.2 RQ2: Impact of External Sources in Basic RAG", "content": "Design. This RQ aims to investigate how different external sources, namely API documentation, GitHub issues, and StackOverflow Q&As, can enhance the Basic RAG in generating sound and practical unit test cases. To determine which documents are most effective in complementing test case generation, we construct four vector databases: (1) API documentation only, (2) GitHub issues only, (3) StackOverflow Q&As only, and (4) a combination of all three.\nSince Basic RAG does not differentiate between document types, all documents from the five projects are aggregated into each of the four databases. Similar to the previous RQ, we assess the parse rate, execution rate, pass rate, and line coverage.\nTo further investigate the impact of Basic RAG on code coverage performance, we undertake the following steps: (1) count the wins of each RAG approach versus zero-shot, (2) count the number of wins for each Basic RAG approach against each other, and (3) perform a pairwise ranking of the Basic RAG approaches with the four databases and the zero-shot using the Friedman tests [6] among zero-shot, Basic RAG combined, Basic RAG API documents, Basic RAG GitHub issues, and Basic RAG StackOverflow Q&As.\nResult. Table 3 presents the results of LLMs in generating unit test cases using the Basic RAG approach. For comparison, we also report the results of zero-shot prompting. Overall, we observe that employing Basic RAG enhances code coverage compared to zero-shot prompting. However, the"}, {"title": "4.3 RQ3: Impact of External Sources in API-Level RAG", "content": "Design. This RQ, similar to RQ2, aims to investigate the effects of different external sources within an API-level RAG setting. Given the availability of mapping information for documents to each API, we create a vector database for each API.\nAs with the previous RQs, we calculate the four metrics, count the wins for each RAG approach, and perform the Friedman tests on the code coverage for the four API RAGs versus zero-shot prompting. Additionally, we conduct a Friedman test comparing the basic and API RAG approaches, resulting in nine rank comparisons.\nResult. Table 4 presents the results of LLMs in generating unit test cases using the API-level RAG approach. For comparative purposes, zero-shot prompting results are also included. Consistent with the findings in RQ2, it is evident that the API-level approach enhances performance in terms of code coverage compared to zero-shot prompting. A similar decline in parse rate was observed for GPT-3.5-turbo and Mistral, attributable to the same underlying factors.\nFigures 4-b and 4-d illustrate the win counts of API-level RAG versus zero-shot prompting and within each API-level RAG. Out of 20 cases, the combined API-level RAG outperformed zero-shot"}, {"title": "4.4 RQ4: Cost-effectiveness of RAG", "content": "Design. This RQ aims to investigate the cost-effectiveness of utilizing different external sources for RAG in the context of unit test case generation. Given that this RQ focuses on the cost-effectiveness of RAG using different sources rather than different LLMs, we examine a more confined setting: GPT-40 applied to the TensorFlow and PyTorch libraries using zero-shot prompting, Basic RAG combined, Basic RAG with API documentation, Basic RAG with GitHub issues, and Basic RAG with StackOverflow Q&As. GPT-40 was selected due to its status as the state-of-the-art and most performant model. The TensorFlow and PyTorch projects were chosen for their relevance and widespread use in machine learning. We exclusively investigate the Basic RAG approach because 1) both RAGs utilize the same set of external sources, and 2) the generality of the Basic RAG implementation allows for application beyond APIs.\nThe cost is determined by the number of tokens in the input and output of the LLM, with output tokens being more costly. In previous RQs, the number of test cases per test suite was not limited, allowing each LLM to determine the number for maximum coverage, which may have resulted in varying costs per model. To ensure a fair comparison, we limit the number of test cases that could influence the output tokens generated by the LLM. Specifically, we conduct the generation process"}, {"title": "4.5 RQ5: Qualitative Analysis of RAG", "content": "Design. This RQ aims to investigate the effect of different RAG sources by closely examining examples manually. Specifically, we randomly select ten APIs from each TensorFlow and PyTorch respectively. The experimental design for this RQ follows the previous methodology to reduce the"}, {"title": "5 Threats to Validity", "content": "Internal Validity. The primary threat to internal validity arises from the nondeterministic nature of LLMs. To mitigate this, we set the temperature parameter to zero, thereby constraining randomness and ensuring the most deterministic response possible [26]. Additionally, we conducted a preliminary study by running a subset of the experiment five times (using only the TensorFlow and PyTorch libraries) and found that the average difference in metrics was less than 2.5%. There may also be limitations in the prompts used for the LLMs. To address these, we conducted multiple series of prompt optimizations before applying them to the entire dataset, ensuring the prompts were as effective as possible.\nConstruct Validity. The primary threat to construct validity lies in the evaluation metrics employed. The test adequacy metric used in this study is line coverage. Although line coverage is widely utilized to assess the effectiveness of test cases, it may not strongly correlate with the detection of actual bugs, which is the ultimate goal of testing. Mutation score is another widely used metric in the domain, designed to detect mutations (potential defects) in the software under test. However, in our case, mutating and building the source projects is too time-consuming, rendering our study infeasible. Therefore, further investigation into this matter necessitates additional future studies.\nConclusion Validity. We conducted a thorough manual analysis to investigate the effect of different RAG approaches on the generated unit test cases. However, we only examined a subset of our datasets, which could pose a potential threat to the validity of our claims.\nExternal Validity. The main threats to external validity in this study include limitations related to (a) LLMs, (b) the projects, (c) the dataset, and (d) the programming language investigated. Regarding LLMs, we utilized the most widely used state-of-the-art models in various sizes, considering our budget constraints. Including other LLM baselines might alter the observations from this study and will be worth exploring when they become available in the future. We selected five projects for this study, but these do not encompass all projects available. Concerning the dataset, we endeavored to mine all relevant information from API documentation, GitHub, and StackOverflow. Nevertheless, there may be missing information that we failed to mine. We also limited the documents to those relevant to the APIs using string-matching heuristics.\nTo make the experiment manageable in terms of budget and execution time, we used the top 10% of the found documents as sufficient for the APIs. This approach excluded a significant amount of data from the experiment; however, with more resources, this could be further improved in future studies. Despite these limitations, we have devised sufficient projects, instances, and documents to demonstrate significant observations from the study."}, {"title": "6 Conclusion", "content": "Our paper explores the effectiveness of different RAG approaches in generating unit test cases. It compares Basic RAG and API-level RAG, utilizing external sources like GitHub issues, StackOverflow Q&As, and API documentation. The results indicate that GitHub issues are particularly effective, often outperforming other sources and zero-shot prompting in code coverage. The combined RAG approaches generally show improvements over individual sources. The study employs the Friedman statistical tests to validate the findings, highlighting the potential of RAG methods to enhance software testing practices. From the manual analysis, we found that while LLMs can generate unit test cases for general scenarios without additional documents, the inclusion of documents with unique examples and settings enables the generation of test cases that cover unique lines, underscoring the importance of effective document search methods and additional context for internal classes. For future work, we suggest enhancing retrieval techniques by developing better search algorithms to find special cases for testing. Additionally, exploring the impact of"}]}