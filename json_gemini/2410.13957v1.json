{"title": "Goal Inference from Open-Ended Dialog", "authors": ["Rachel Ma", "Jingyi Qu", "Andreea Bobu", "Dylan Hadfield-Menell"], "abstract": "We present an online method for embodied agents\nto learn and accomplish diverse user goals. While offline meth-\nods like RLHF can represent various goals but require large\ndatasets, our approach achieves similar flexibility with online\nefficiency. We extract natural language goal representations\nfrom conversations with Large Language Models (LLMs). We\nprompt an LLM to role play as a human with different goals\nand use the corresponding likelihoods to run Bayesian inference\nover potential goals. As a result, our method can represent\nuncertainty over complex goals based on unrestricted dialog.\nWe evaluate our method in grocery shopping and home robot\nassistance domains using a text-based interface and AI2Thor\nsimulation respectively. Results show our method outperforms\nablation baselines that lack either explicit goal representation\nor probabilistic inference.", "sections": [{"title": "I. INTRODUCTION", "content": "AI agents and robots must quickly learn and carry out\nmany different user tasks in real-time. For example, a home\nrobot assistant may need to adapt to various household\npreferences and routines. Imagine a scenario where your\nrobot assistant is tasked with gathering ingredients to bake\na cake for you. Depending on who you are, you may want\ndifferent ingredients. If you only want a basic cake, your\nideal recipe is eggs, milk, sugar, and flour. However, if you\nare allergic to gluten, you would want gluten-free flour. If\nyou prefer strawberry cake, then you would want to get\nstrawberries. Every human has a different set of preferences.\nThis level of variation is challenging, if not impossible,\nfor system designers to anticipate in advance. In order for\nagents/robots to perform or assist with tasks for humans, they\nmust first be able to learn the preferences of the humans that\nthey are trying to help.\nTo address this challenge, we propose a new method\nGOOD (Goals for Open-ended Dialogue) that combines\nthe best parts of offline and online approaches. It uses\nLarge Language Models (LLMs) to infer natural language\nrepresentations of user goals. This allows our method to\nrepresent a flexible, open-ended set of possible goals during\nan online interaction. As a result, the method combines\nthe flexibility and representation power of offline preference\ntuning methods [1, 2] with the data efficiency and uncertainty\nquantification of online methods that learn rewards based on\na set of engineered features [3, 4]. This allows our method\nto represent uncertainty over goals that may not have been\nexplicitly engineered or anticipated in advance. In order to\nlearn goals efficiently, we use natural language dialog with\nthe user instead of, e.g., best-of-k comparisons present in\n[5, 6, 7, 8]."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "A. Preference Learning and NL Probabilistic Reasoning with\nLLMs\nOffline preference tuning methods [1, 2] are data heavy\nbut are generalizable to many domains and tasks. Online"}, {"title": "III. METHOD", "content": "The key idea behind our method is tracking possible\nhuman goals and how likely they are as the dialog between\nthe human and the agent goes on. To track the possible\nspace of human goals, we instantiate a finite goal set to\nwhich we can add goals if our inference finds that the human\npreferences in the conversation so far are not represented in\nthe goals, or remove unlikely goals. Given the updated goal\nset, we infer the likeliest goals and select actions based on\nthem. We continue the conversation rounds until the task is\ncompleted.\nA. Preliminaries\nTypical Bayesian preference learning methods interpret\nhuman input u as evidence for the person's goal. Given a\nnew input u, these methods model the likelihood P(u | g)\nusing, for example, models from econometrics and cognitive\nscience, then perform goal belief updates as follows:\n$$P(g|u) = \\frac{P(u | g)P(g)}{\\sum_{\\hat{g}\\in G} P(u | \\hat{g})P(\\hat{g}) }$$ \nThis formulation presents two challenges for open-ended\ngoal inference. First, how should we flexibly represent the\ngoals themselves? Typical methods ([18], [19]) define goals\nas x, y locations in navigation or continuous parameters"}, {"title": "IV. EXPERIMENTS", "content": "In our experiments, we show that our method enables\nthe robot to perform goal inference to estimate the human's\npreference, and then accomplish that task according to it. We\nconduct experiments for various tasks and human preferences\nin a text based Grocery Shopping agent domain and an\nAI2Thor robot simulation domain [27]. We compare our\nmethod with two ablation baselines in both of these domains.\nWe also run an isolated inference module experiment on a\nmultiple choice question dataset to see the accuracy of our\nproposed inference method and to compare the performance\nbetween two models of different number of parameters.\nA. Isolated Inference Intrinsic Evaluation\nWe check the performance of our proposed goal inference\nmethod by testing on a multiple choice dataset [28]. We also\ncompare the performances between using Llama3 8B Instruct\nand Llama3 70B Instruct models on Hugging Face [29], and\ncheck the accuracy with having the \u201cUnspecified Goal\" in the\ngoal list. Only open models such as Llama can be used for\nthe inference module because all the logits are available, not\njust the ones that correspond to the output of text generation.\nEach question from the multiple choice dataset has 5\nchoices. With GPT-40-mini, we generate four rephrasings\nfor each choice. These 20 choices are the goals in the\ngoal list. For the with \"Unspecified Goal\" comparison, the\n\"Unspecified Goal\" is in the goal list, so there are 21 goals.\nThe sum of the logits that correspond to the words of the"}, {"title": "V. DISCUSSION AND CONCLUSION", "content": "We introduced a language-assisted framework for open-\nended goal inference that allows for great flexibility in\ntasks where human preferences are unclear or challenging\nto specify. We have shown how Language Models can be\nleveraged to easily edit the support of possible human goals\nand maintain beliefs over them. We demonstrated that our\npipeline can efficiently propose and add new goals based\non conversations with the human, and remove goals that are\ndeemed unlikely, undesirable, or unsafe. We also see from\nour isolated inference evaluation, that overall that the 8B\nInstruct model performs better compared to the 70B Instruct\nmodel.\nFuture work involves conducting human user studies and\nsee if the LLM evaluations are reflected. While our method\nshows promise, it currently relies on synthetic conversations"}]}