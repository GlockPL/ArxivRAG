{"title": "FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features for Highly Controllable Text-Driven Image Translation", "authors": ["Xiang Gao", "Jiaying Liu"], "abstract": "Large-scale text-to-image diffusion models have been a revolutionary milestone in the evolution of generative AI and multimodal technology, allowing extraordinary image generation based on natural-language text prompts. However, the issue of lacking controllability of such models restricts their practical applicability for real-life content creation, for which attention has been focused on leveraging a reference image to control text-to-image synthesis. Due to the close correlation between the reference image and the generated image, this problem can also be regarded as the task of manipulating (or editing) the reference image as per the text, namely text-driven image-to-image translation. This paper contributes a novel, concise, and efficient approach that adapts the pre-trained large-scale text-to-image (T2I) diffusion model to the image-to-image (12I) paradigm in a plug-and-play manner, realizing high-quality and versatile text-driven I2I translation without any model training, model fine-tuning, or online optimization process. To guide T2I generation with a reference image, we propose to model diverse guiding factors with correspondingly different frequency bands of diffusion features in the DCT spectral space, and accordingly devise a novel frequency band substitution layer that dynamically substitutes a certain DCT frequency band of the diffusion features with the corresponding counterpart of the reference image along the reverse sampling process. We demonstrate that our method flexibly enables highly controllable text-driven I2I translation both in the guiding factor and guiding intensity of the reference image, simply by tuning the type and bandwidth of the substituted frequency band, respectively. Extensive qualitative and quantitative experiments verify the superiority of our approach over related methods in I2I translation visual quality, versatility, and controllability.", "sections": [{"title": "I. INTRODUCTION", "content": "As a typical application of the booming multimodal technology, text-driven I2I translation is an appealing computer vision problem that aims to translate a source (reference) image as per a text prompt. It can also be treated as a more controllable form of text-to-image (T2I) synthesis: controlling the T2I generation with a reference image. Since the advent of CLIP [1] bridging vision and language with large-scale contrastive pre-training, attempts have been made to instruct image manipulation with text by combining CLIP with generative models. VQGAN-CLIP [2] pioneers text-driven image translation by optimizing\nVQGAN [3] latent image embedding using the CLIP text-image similarity loss. DiffusionCLIP [4] fine-tunes diffusion model [5] under the CLIP loss to manipulate an image as per a text. DiffuseIT [6] combines VIT-based structure loss [7] and CLIP-based semantic loss to guide the diffusion model's reverse sampling process via manifold constrained gradient [8], synthesizing translated image that complies with the target text while maintaining the structure of the reference image. However, these methods are not competitive in visual quality of the generation results, due to the limited model capacity of the backbone generative model as well as unstability caused by the online fine-tuning process.\nTo promote image translation visual quality, efforts have been made to train large models on massive data. Instruct-Pix2Pix [9] employs GPT-3 [10] and Stable Diffusion [11] to synthesize huge amounts of paired training data, based on which trains a supervised text-driven I2I mapping for general image manipulation task. Design Booster [12] trains a latent\ndiffusion model [11] conditioned on a joint representation that fuses both text embedding and image embedding, realizing layout-preserved text-driven I2I translation. Nevertheless, these methods are computationally intensive due to the need for training large models on immense data.\nTo circumvent formidable training costs, research has been focused on leveraging off-the-shelf large-scale T2I diffusion models for text-driven I2I translation. This type of methods further divide into fine-tuning-based methods and inversion-based methods.\nThe former type of fine-tuning-based methods represented by SINE [13] and Imagic [14] fine-tune the pre-trained T2I diffusion model to reconstruct the reference image before manipulating it with a target text. These methods require separate fine-tuning of the entire diffusion model for each time of image editing, which is less efficient and prone to underfitting or overfitting to the reference image.\nThe latter type of inversion-based methods invert the reference image into the diffusion model's Gaussian noise space and then generate the translated image via the reverse sampling process guided by the target text. A pivotal challenge of this pipeline is that the sampling trajectory may severely deviate from the inversion trajectory due to the error accumulation caused by the classifier-free guidance technique [15], which impairs the correlation between the reference and the translated image. To remedy this issue, Null-text Inversion [16] optimizes the unconditional null-text embedding to calibrate the sampling trajectory step by step. Prompt Tuning Inversion [17] proposes to minimize trajectory divergence by learning to encode the reference image into a learnable prompt embedding. Similarly, StyleDiffusion [18] opts to optimize the \"value\" embedding of the cross-attention layer to encode the visual information of the reference image. Pix2Pix-zero [19] penalizes trajectory deviation by matching cross-attention maps between the two trajectories with least-square loss. These methods apply per-step online optimization to calibrate the whole sampling trajectory, introducing additional computational cost and time overhead. Moreover, most of these methods rely on the cross-attention control technique introduced by Prompt-to-Prompt [20] for image structure preservation. This makes them rely on a paired source text of the reference image for word-to-word alignment, which is not flexible or even available in most cases. Plug-and-Play (PAP) [21] proposes to use feature maps and self-attention maps extracted from the denoising U-Net internal layers to maintain image structure, realizing optimization-free text-driven I2I translation. However, the algorithm is sensitive to specific layers for feature extraction and feature injection, and the feature extraction process is also time-consuming.\nIn this paper, we propose a concise and efficient approach termed FBSDiff, realizing plug-and-play and highly controllable text-driven I2I translation from a frequency-domain perspective. To guide T2I generation with a reference image, a key missing ingredient of existing methods is the mechanism to control the guiding factor (e.g., image appearance, image layout, and image contours) and guiding intensity of the reference image. Since different guiding factors are difficult to isolate in the spatial domain, we consider decomposing\nthem in the frequency domain by modeling different guiding factors with different frequency bands of diffusion features in the Discrete Cosine Transform (DCT) spectral space. Based on this motivation, we propose an inversion-based text-driven I2I translation framework featured with a novel frequency band substitution mechanism, realizing plug-and-play and highly controllable reference image guidance by dynamically substituting a certain DCT frequency band of diffusion features with the corresponding counterpart of the reference image along the reverse sampling process. As displayed in Fig. 1, T2I generation with appearance and layout control, pure layout control, and contour control of the reference image can be respectively realized by transplanting low-frequency band, mid-frequency band, and high-frequency band between diffusion features, enabling controllable and versatile text-driven I2I translation.\nThe strengths of our method are fourfold: (I) dynamic reference image control: it allows plug-and-play text-driven I2I translation at inference time; (II) conciseness and efficiency: our method dispenses with the need for the paired source text of the reference image as well as cumbersome attention modulation process as compared with existing advanced methods, all while achieving leading I2I translation performance; (III) more methodologically generic : our method transplants frequency band of the denoised features along the reverse sampling trajectory, requiring no access to any internal features of the denoising network, and thus decouples with the specific diffusion model backbone architecture as contrasted with existing methods; (IV) high controllability: our method allows flexible control over the guiding factor and guiding intensity of the reference image simply by tuning the type and bandwidth of the substituted frequency band. The effectiveness of our method is fully demonstrated both qualitatively and quantitatively. To summarize, we make the following key contributions:\n\u2022\tWe provide new insights about controllable diffusion process from a novel frequency-domain perspective.\n\u2022\tWe propose a novel frequency band substitution technique, realizing plug-and-play text-driven I2I translation without any model training, fine-tuning, or online optimization process.\n\u2022\tWe contribute a concise and efficient text-driven I2I framework that is free from source text and cumbersome attention modulations, highly controllable in both guiding factor and guiding intensity of the reference image, and invariant to the architecture of the used diffusion model backbone, all while achieving superior I2I translation performance compared with existing advanced methods."}, {"title": "II. RELATED WORK", "content": "Since the advent of DDPM [5], diffusion model has soon dominated the family of generative models [22]. Afterwards, much progress have been made to improve diffusion model in both methodology and application. DDIM [23] and its variants [24], [24] substantially accelerate diffusion model sampling process to tens of times with only marginal drop\nin generation quality, promoting its practicability dramatically. Palette [25] extends diffusion model from unconditional image generation to the realm of conditional image synthesis, opening the door of diffusion-based image-to-image translation.\nWith the advancement of multimodal technology, large-scale T2I diffusion models [26], [27], [28] are proposed to generate high-resolution images with open-domain text prompts, bringing content creation to an unprecedented level. To lower computational overhead of large-scale T2I model, Latent Diffusion Model (LDM) [11] proposes to transfer diffusion model from high-dimension pixel space to low-dimensional feature space, contributing the most widely used architecture in AIGC industry. To introduce more controllability to T2I synthesis, ControlNet [29] and T2i-adapter [30] add spatial control to T2I diffusion models by training a control module of the denoising U-Net conditioned on certain image priors (e.g., canny edges, depth maps, human key points, etc.). SDXL [31] and DiTs [32] propose Transformer [33] based backbone denoising network, improving T2I diffusion model to larger capacity. Up to now, diffusion model has been applied to a wide variety of vision fields such as image super-resolution [34], image inpainting [35], image colorization [36], semantic segmentation [37], point cloud generation [38], video synthesis [39], 3D reconstruction [40], etc, and is still making rapid progress in theory and potential applications.\nNeural network models are mostly applied to tackle vision tasks in spatial or temporal domain, some research reveals that model performance can also be boosted from frequency domain. For example, Ghosh et al. [41] introduce DCT to convolutional neural network for image classification, accelerating network convergence speed. Xie et al. [42] propose a frequency-aware dynamic network for lightweight image super-resolution. Cai et al. [43] impose Fourier frequency spectrum consistency to image translation tasks, achieving better identity preservation ability. FreeU [44] improves T2I generation quality by selectively enhancing or depressing"}, {"title": "III. METHOD", "content": "In this section, we first describe the overall model architecture of our FBSDiff, then elaborate on our proposed\nEstablished on the pre-trained Latent Diffusion Model (LDM), FBSDiff adapts it from T2I generation to the realm of text-driven I2I translation with our proposed plug-and-play ref-erence image guidance mechanism: dynamic frequency band substitution, which efficiently and flexibly realizes controllable guiding factor and guiding intensity of the reference image to the T2I generation result.\nAs Fig. 2 shows, FBSDiff comprises three diffusion trajectories: (i) inversion trajectory (zo \u2192 ZTinv); (ii) reconstruction trajectory (ZTinv = zT \u2192 zo \u2248 zo); (iii) sampling trajectory (zT \u2192 zo). Starting from the initial feature zo = E(x) extracted from the reference image x by the LDM encoder E, a Tinv-step DDIM inversion is employed to project zo into the Gaussian noise latent space conditioned on the null-text embedding v\u00f8, based on the assumption that the ODE process can be reversed in the limit of small steps:\nz_{t+1} = \\sqrt{\\bar{a}_{t+1}}f_{\\theta}(z_t, t, v_{\\emptyset}) + \\sqrt{1 - \\bar{a}_{t+1}}\\epsilon_{\\theta}(z_t, t, v_{\\emptyset}), (1)\nf_{\\theta}(z_t, t, v_{\\emptyset}) = \\frac{z_t - \\sqrt{1 - \\bar{a}_{t}}\\epsilon_{\\theta}(z_t, t, v_{\\emptyset})}{\\sqrt{\\bar{a}_t}}\n(2)\nwhere {at} are schedule parameters that follows the same setting as DDPM [5], \u20ac\u03b8 is the denoising U-Net of the pre-trained LDM. The Gaussian noise zTiny obtained after the Tinv-step DDIM inversion is directly used as the initial noise feature of the subsequent reconstruction trajectory, which is a T-step DDIM sampling process that reconstructs zo \u2248 zo from the inverted noise feature zT = zTiny:\nz_{t-1} = \\sqrt{\\bar{a}_{t-1}}f_{\\theta}(z_t, t, v_{\\emptyset}) + \\sqrt{1 - \\bar{a}_{t-1}}\\epsilon_{\\theta}(z_t, t, v_{\\emptyset}),\n(3)\nin which fo(zt,t,v\u00f8) follows the same form as Eq. 2. The length of the reconstruction trajectory could be much smaller than that of the inversion trajectory (i.e., T\u226aTinv) to save inference time. The reconstruction trajectory is conditioned on the same null-text embedding v\u00f8 as the inversion trajectory to ensure feature reconstructability (i.e., zo \u2248 zo).\nMeanwhile, an equal-length sampling trajectory is applied in parallel with the reconstruction trajectory for T2I synthesis. The sampling trajectory is also a T-step DDIM sampling process that progressively denoises a randomly initialized Gaussian noise feature \u017er ~ N(0, I) into zo conditioned on the text embedding v of the target text prompt. To amplify the effect of text guidance, we employ classifier-free guidance technique [15] by interpolating conditional (target text) and unconditional (null text) noise prediction at each time step with a guidance scale w during the sampling trajectory:\n\\tilde{z}_{t-1} = \\sqrt{\\bar{a}_{t-1}}f_{\\theta}(\\tilde{z}_t, t, v, v_{\\emptyset}) + \\sqrt{1 - \\bar{a}_{t-1}}\\epsilon_{\\theta}(\\tilde{z}_t, t, v, v_{\\emptyset}), (4)\nf_{\\theta}(\\tilde{z}_t, t, v, v_{\\emptyset}) = \\frac{\\tilde{z}_t - \\sqrt{1 - \\bar{a}_t}\\epsilon_{\\theta}(\\tilde{z}_t, t, v, v_{\\emptyset})}{\\sqrt{\\bar{a}_t}}\n(5)\n\\epsilon_{\\theta}(\\tilde{z}_t, t, v, v_{\\emptyset}) = \\omega \\cdot \\epsilon_{\\theta}(\\tilde{z}_t, t, v) + (1 - \\omega) \\cdot \\epsilon_{\\theta}(\\tilde{z}_t, t, v_{\\emptyset}). (6)\nDue to the inherent property of DDIM inversion and DDIM sampling, the reconstruction trajectory forms a deterministic denoising mapping towards the reference image, where its intermediate denoising results {zt} can be used as pivotal guidance features to calibrate the corresponding counterpart {\u017et} along the sampling trajectory. Therefore, the correlation between the reference image and the generated image can be established, and thus realizing text-driven I2I translation. Specifically, we implement feature calibration by inserting a plug-and-play frequency band substitution (FBS) layer in between the reconstruction trajectory and the sampling trajectory. FBS layer substitutes a certain frequency band of \u017et in the sampling trajectory with the corresponding frequency band of zt in the reconstruction trajectory along the reverse sampling process. Such frequency band substitution effectively and efficiently imposes guidance of the reference image to the T2I synthesis process. Both the guiding factor (e.g., image appearance, image layout, image contours) and guiding intensity can be flexibly controllable simply by tuning the type and bandwidth of the substituted frequency band, respectively.\nTo improve I2I translation visual quality, we partition the sampling trajectory into a calibration phase and a non-calibration phase, separated by the time step AT. In the former calibration phase (\u017eT \u2192 \u017exT), dynamic frequency band substitution is applied at each time step for smooth calibration of the sampling process; in the latter non-calibration phase (\u017exT-1 \u2192 \u017e0), we remove FBS layer to avoid over-constrained sampling result, fully unleashing the generative power of the pre-trained T2I model to improve image generation quality. Here A denotes the ratio of the length of the non-calibration phase to that of the entire sampling trajectory.\nAt last, the final result \u017e0 of the sampling trajectory is decoded back to the image space via the LDM decoder D, producing the final translated image x, i.e., x = D(\u017eo).\nAs Fig. 3 illustrates, the FBS layer takes in a pair of diffusion features zt and \u017et, converts them from the spatial domain into the frequency domain via 2D-DCT, then transplants a certain frequency band in the DCT spectrum of \u017et to the same position in the DCT spectrum of zt. Finally, 2D-IDCT is applied to transform the manipulated DCT spectrum of \u017et back into the spatial domain as the final calibrated feature.\nIn 2D DCT spectrum, elements with smaller coordinates (nearer to the top-left origin) encode lower-frequency information while larger-coordinate elements (nearer to the bottom-right corner) correspond to higher-frequency components. Most of the DCT spectral energy is occupied by a small proportion of low-frequency elements near the top-left origin.\nIn FBS layer, the sum of 2D coordinates is used as the threshold to extract DCT frequency bands of different types and bandwidths through binary masking. Specifically, we design three types of binary masks which are respectively termed the low-pass mask (Mask\u0131p), high-pass mask (Maskhp), and mid-pass mask (Maskmp):\n\\begin{cases}\nMask_{lp}(x, y) =1 & \\text{if } x + y \\leq th_{lp} \\text{ else } 0, \\\\\nMask_{hp}(x,y) =1 & \\text{if } x + y > th_{hp} \\text{ else } 0, \\\\\nMask_{mp}(x,y) =1 & \\text{if } th_{mp1} < x + y < th_{mp2} \\text{ else } 0,\n\\end{cases}\nwhere thip is the threshold of the low-pass filtering; thhp is the threshold of the high-pass filtering; thmp1 and thmp2 are respectively the lower bound and upper bound of the mid-pass filtering. Given a binary mask Mask \u2208 {Mask\u0131p, Maskhp, Maskmp}, the frequency band substitution operation in the FBS layer can be formulated as:\n\\tilde{z}_t =IDCT(DCT(z_t) \\cdot Mask\u2217+\nDCT(\\tilde{z}_t) \\cdot (1 - Mask\u2217)),\n(7)\nwhere DCT and IDCT refer to the 2D-DCT and 2D-IDCT transformations respectively, which are introduced in detail in the Appendix section. The use of the low-pass mask Mask\u0131p, high-pass mask Maskhp, and mid-pass mask Maskmp respectively corresponds to the extraction and substitution of the low-frequency band, high-frequency band, and mid-frequency band, which controls different guiding factors of the reference image to the T2I generated result:\n\u2022\tLow-frequency band substitution enables low-frequency information guidance of the reference image x, realizing image appearance (e.g., color, luminance) and layout control over the generated image x;\n\u2022\tHigh-frequency band substitution enables high-frequency information guidance of x, realizing image contour control over the generated image x;\n\u2022\tMid-frequency band substitution enables mid-frequency information guidance of the reference image x. By filtering out higher-frequency contour information and lower-frequency appearance information in the DCT spectrum, the substitution of the mid-frequency band realizes only image layout control over the generated image x.\nThe DCT masking type and the corresponding thresholds used in the FBS layer are hyper-parameters of our method, which could be flexibly modulated to enable control over diverse guiding factors and continuous guiding intensity of the reference image x to the T2I generated image x.\nWe use the pre-trained Stable Diffusion v1.5 as the backbone diffusion model and set the classifier-free guidance scale w = 7.5. We use 1000-step DDIM inversion to ensure high-quality reconstruction, i.e., Tinv=1000, and use 50-step DDIM sampling for both the reconstruction and sampling trajectory, i.e., T=50. Along the sampling trajectory, we allocate 55% time steps to the calibration phase and the remaining 45% steps for the non-calibration phase, i.e., \u03bb=0.45. For the default DCT masking thresholds used in the FBS layer, we set thip=80 for the low-frequency band substitution (low-FBS); thhp=5 for the high-frequency band substitution (high-FBS); thmp1=5, thmp2=80 for the mid-frequency band substitution (mid-FBS). The complete algorithm of FBSDiff is presented in Alg. 1."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we first present and analyze the qualitative results of our method as well as qualitative comparison with related advanced methods; then we delve into the frequency band substitution mechanism in detail with ablation studies; finally, we show quantitative evaluations of our method and related approaches."}]}