{"title": "FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features for Highly Controllable Text-Driven Image Translation", "authors": ["Xiang Gao", "Jiaying Liu"], "abstract": "Large-scale text-to-image diffusion models have been a revolutionary milestone in the evolution of generative AI and multimodal technology, allowing extraordinary image generation based on natural-language text prompts. However, the issue of lacking controllability of such models restricts their practical applicability for real-life content creation, for which attention has been focused on leveraging a reference image to control text-to-image synthesis. Due to the close correlation between the reference image and the generated image, this problem can also be regarded as the task of manipulating (or editing) the reference image as per the text, namely text-driven image-to-image translation. This paper contributes a novel, concise, and efficient approach that adapts the pre-trained large-scale text-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a plug-and-play manner, realizing high-quality and versatile text-driven I2I translation without any model training, model fine-tuning, or online optimization process. To guide T2I generation with a reference image, we propose to model diverse guiding factors with correspondingly different frequency bands of diffusion features in the DCT spectral space, and accordingly devise a novel frequency band substitution layer that dynamically substitutes a certain DCT frequency band of the diffusion features with the corresponding counterpart of the reference image along the reverse sampling process. We demonstrate that our method flexibly enables highly controllable text-driven I2I translation both in the guiding factor and guiding intensity of the reference image, simply by tuning the type and bandwidth of the substituted frequency band, respectively. Extensive qualitative and quantitative experiments verify the superiority of our approach over related methods in I2I translation visual quality, versatility, and controllability.", "sections": [{"title": "I. INTRODUCTION", "content": "As a typical application of the booming multimodal technology, text-driven I2I translation is an appealing computer vision problem that aims to translate a source (reference) image as per a text prompt. It can also be treated as a more controllable form of text-to-image (T2I) synthesis: controlling the T2I generation with a reference image. Since the advent of CLIP [1] bridging vision and language with large-scale contrastive pre-training, attempts have been made to instruct image manipulation with text by combining CLIP with generative models. VQGAN-CLIP [2] pioneers text-driven image translation by optimizing VQGAN [3] latent image embedding using the CLIP text-image similarity loss. DiffusionCLIP [4] fine-tunes diffusion model [5] under the CLIP loss to manipulate an image as per a text. DiffuseIT [6] combines VIT-based structure loss [7] and CLIP-based semantic loss to guide the diffusion model's reverse sampling process via manifold constrained gradient [8], synthesizing translated image that complies with the target text while maintaining the structure of the reference image. However, these methods are not competitive in visual quality of the generation results, due to the limited model capacity of the backbone generative model as well as unstability caused by the online fine-tuning process.\nTo promote image translation visual quality, efforts have been made to train large models on massive data. Instruct-Pix2Pix [9] employs GPT-3 [10] and Stable Diffusion [11] to synthesize huge amounts of paired training data, based on which trains a supervised text-driven I2I mapping for general image manipulation task. Design Booster [12] trains a latent diffusion model [11] conditioned on a joint representation that fuses both text embedding and image embedding, realizing layout-preserved text-driven I2I translation. Nevertheless, these methods are computationally intensive due to the need for training large models on immense data.\nTo circumvent formidable training costs, research has been focused on leveraging off-the-shelf large-scale T2I diffusion models for text-driven I2I translation. This type of methods further divide into fine-tuning-based methods and inversion-based methods.\nThe former type of fine-tuning-based methods represented by SINE [13] and Imagic [14] fine-tune the pre-trained T2I diffusion model to reconstruct the reference image before manipulating it with a target text. These methods require separate fine-tuning of the entire diffusion model for each time of image editing, which is less efficient and prone to underfitting or overfitting to the reference image.\nThe latter type of inversion-based methods invert the reference image into the diffusion model's Gaussian noise space and then generate the translated image via the reverse sampling process guided by the target text. A pivotal challenge of this pipeline is that the sampling trajectory may severely deviate from the inversion trajectory due to the error accumulation caused by the classifier-free guidance technique [15], which impairs the correlation between the reference and the translated image. To remedy this issue, Null-text Inversion [16] optimizes the unconditional null-text embedding to calibrate the sampling trajectory step by step. Prompt Tuning Inversion [17] proposes to minimize trajectory divergence by learning to encode the reference image into a learnable prompt embedding. Similarly, StyleDiffusion [18] opts to optimize the \"value\" embedding of the cross-attention layer to encode the visual information of the reference image. Pix2Pix-zero [19] penalizes trajectory deviation by matching cross-attention maps between the two trajectories with least-square loss. These methods apply per-step online optimization to calibrate the whole sampling trajectory, introducing additional computational cost and time overhead. Moreover, most of these methods rely on the cross-attention control technique introduced by Prompt-to-Prompt [20] for image structure preservation. This makes them rely on a paired source text of the reference image for word-to-word alignment, which is not flexible or even available in most cases. Plug-and-Play (PAP) [21] proposes to use feature maps and self-attention maps extracted from the denoising U-Net internal layers to maintain image structure, realizing optimization-free text-driven I2I translation. However, the algorithm is sensitive to specific layers for feature extraction and feature injection, and the feature extraction process is also time-consuming.\nIn this paper, we propose a concise and efficient approach termed FBSDiff, realizing plug-and-play and highly controllable text-driven I2I translation from a frequency-domain perspective. To guide T2I generation with a reference image, a key missing ingredient of existing methods is the mechanism to control the guiding factor (e.g., image appearance, image layout, and image contours) and guiding intensity of the reference image. Since different guiding factors are difficult to isolate in the spatial domain, we consider decomposing them in the frequency domain by modeling different guiding factors with different frequency bands of diffusion features in the Discrete Cosine Transform (DCT) spectral space. Based on this motivation, we propose an inversion-based text-driven I2I translation framework featured with a novel frequency band substitution mechanism, realizing plug-and-play and highly controllable reference image guidance by dynamically substituting a certain DCT frequency band of diffusion features with the corresponding counterpart of the reference image along the reverse sampling process. As displayed in Fig. 1, T2I generation with appearance and layout control, pure layout control, and contour control of the reference image can be respectively realized by transplanting low-frequency band, mid-frequency band, and high-frequency band between diffusion features, enabling controllable and versatile text-driven I2I translation.\nThe strengths of our method are fourfold: (I) dynamic reference image control: it allows plug-and-play text-driven I2I translation at inference time; (II) conciseness and efficiency: our method dispenses with the need for the paired source text of the reference image as well as cumbersome attention modulation process as compared with existing advanced methods, all while achieving leading I2I translation performance; (III) more methodologically generic : our method transplants frequency band of the denoised features along the reverse sampling trajectory, requiring no access to any internal features of the denoising network, and thus decouples with the specific diffusion model backbone architecture as contrasted with existing methods; (IV) high controllability: our method allows flexible control over the guiding factor and guiding intensity of the reference image simply by tuning the type and bandwidth of the substituted frequency band. The effectiveness of our method is fully demonstrated both qualitatively and quantitatively. To summarize, we make the following key contributions:\n\u2022 We provide new insights about controllable diffusion process from a novel frequency-domain perspective.\n\u2022 We propose a novel frequency band substitution technique, realizing plug-and-play text-driven I2I translation without any model training, fine-tuning, or online optimization process.\n\u2022 We contribute a concise and efficient text-driven I2I framework that is free from source text and cumbersome attention modulations, highly controllable in both guiding factor and guiding intensity of the reference image, and invariant to the architecture of the used diffusion model backbone, all while achieving superior I2I translation performance compared with existing advanced methods."}, {"title": "II. RELATED WORK", "content": "A. Diffusion Model\nSince the advent of DDPM [5], diffusion model has soon dominated the family of generative models [22]. Afterwards, much progress have been made to improve diffusion model in both methodology and application. DDIM [23] and its variants [24], [24] substantially accelerate diffusion model sampling process to tens of times with only marginal drop in generation quality, promoting its practicability dramatically. Palette [25] extends diffusion model from unconditional image generation to the realm of conditional image synthesis, opening the door of diffusion-based image-to-image translation. With the advancement of multimodal technology, large-scale T2I diffusion models [26], [27], [28] are proposed to generate high-resolution images with open-domain text prompts, bringing content creation to an unprecedented level. To lower computational overhead of large-scale T2I model, Latent Diffusion Model (LDM) [11] proposes to transfer diffusion model from high-dimension pixel space to low-dimensional feature space, contributing the most widely used architecture in AIGC industry. To introduce more controllability to T2I synthesis, ControlNet [29] and T2i-adapter [30] add spatial control to T2I diffusion models by training a control module of the denoising U-Net conditioned on certain image priors (e.g., canny edges, depth maps, human key points, etc.). SDXL [31] and DiTs [32] propose Transformer [33] based backbone denoising network, improving T2I diffusion model to larger capacity. Up to now, diffusion model has been applied to a wide variety of vision fields such as image super-resolution [34], image inpainting [35], image colorization [36], semantic segmentation [37], point cloud generation [38], video synthesis [39], 3D reconstruction [40], etc, and is still making rapid progress in theory and potential applications.\nB. Computer Vision in Frequency Perspective\nNeural network models are mostly applied to tackle vision tasks in spatial or temporal domain, some research reveals that model performance can also be boosted from frequency domain. For example, Ghosh et al. [41] introduce DCT to convolutional neural network for image classification, accelerating network convergence speed. Xie et al. [42] propose a frequency-aware dynamic network for lightweight image super-resolution. Cai et al. [43] impose Fourier frequency spectrum consistency to image translation tasks, achieving better identity preservation ability. FreeU [44] improves T2I generation quality by selectively enhancing or depressing different frequency components of diffusion features inside the denoising U-Net model. ILVR [45] proposes to fuse the low-frequency information of the forward diffusion process into the reverse sampling process for conditioned image synthesis. Our method differs with ILVR in that ILVR simulates low-pass filtering with simple feature downsampling and upsampling and performs information fusion in the spatial domain, while our method explicitly extracts and transplants frequency bands of diffusion features in the DCT domain. FCDiffusion [46] shares similar spirit of frequency-based control of T2I diffusion model with our method. However, FCDiffusion relies on training multiple frequency control branches to realize versatile control effects, while our method achieves versatility and high controllability in both guiding factor and guiding intensity of the reference image in a training-free and plug-and-play manner."}, {"title": "III. METHOD", "content": "In this section, we first describe the overall model architecture of our FBSDiff, then elaborate on our proposed frequency band substitution mechanism, and finally summarize our algorithm and describe implementation details. For the diffusion model background, please refer to the Appendix.\nA. Overall Architecture\nEstablished on the pre-trained Latent Diffusion Model (LDM), FBSDiff adapts it from T2I generation to the realm of text-driven I2I translation with our proposed plug-and-play reference image guidance mechanism: dynamic frequency band substitution, which efficiently and flexibly realizes controllable guiding factor and guiding intensity of the reference image to the T2I generation result.\nAs Fig. 2 shows, FBSDiff comprises three diffusion trajectories: (i) inversion trajectory $(z_0 \\rightarrow z_T=z_{T_{inv}})$; (ii) reconstruction trajectory $(z_{T_{inv}}=z_T \\rightarrow z_0 \\approx z_0)$; (iii) sampling trajectory $(\\tilde{z}_T \\rightarrow \\tilde{z}_0)$. Starting from the initial feature $z_0 = E(x)$ extracted from the reference image $x$ by the LDM encoder $E$, a $T_{inv}$-step DDIM inversion is employed to project $z_0$ into the Gaussian noise latent space conditioned on the null-text embedding $v_{\\emptyset}$, based on the assumption that the ODE process can be reversed in the limit of small steps:\n$z_{t+1} = \\sqrt{\\bar{\\alpha}_{t+1}}f_0(z_t, t, v_{\\emptyset}) + \\sqrt{1 - \\bar{\\alpha}_{t+1}}\\epsilon_{\\theta}(z_t, t, v_{\\emptyset}),$ (1)\n$f_0(z_t, t, v_{\\emptyset}) = \\frac{z_t - \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_{\\theta}(z_t, t, v_{\\emptyset})}{\\sqrt{\\alpha}_t},$ (2)\nwhere ${\\alpha_t}$ are schedule parameters that follows the same setting as DDPM [5], $\\epsilon_{\\theta}$ is the denoising U-Net of the pre-trained LDM. The Gaussian noise $z_{T_{inv}}$ obtained after the $T_{inv}$-step DDIM inversion is directly used as the initial noise feature of the subsequent reconstruction trajectory, which is a T-step DDIM sampling process that reconstructs $z_0 \\approx z_0$ from the inverted noise feature $z_T = z_{T_{inv}}$:\n$z_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}f_0(z_t, t, v_{\\emptyset}) + \\sqrt{1 - \\bar{\\alpha}_{t-1}}\\epsilon_{\\theta}(z_t, t, v_{\\emptyset}),$ (3)\nin which $f_0(z_t, t, v_{\\emptyset})$ follows the same form as Eq. 2. The length of the reconstruction trajectory could be much smaller than that of the inversion trajectory (i.e., $T \\ll T_{inv}$) to save inference time. The reconstruction trajectory is conditioned on the same null-text embedding $v_{\\emptyset}$ as the inversion trajectory to ensure feature reconstructability (i.e., $z_0 \\approx z_0$).\nMeanwhile, an equal-length sampling trajectory is applied in parallel with the reconstruction trajectory for T2I synthesis. The sampling trajectory is also a T-step DDIM sampling process that progressively denoises a randomly initialized Gaussian noise feature $\\tilde{z}_T \\sim N(0, I)$ into $\\tilde{z}_0$ conditioned on the text embedding $v$ of the target text prompt. To amplify the effect of text guidance, we employ classifier-free guidance technique [15] by interpolating conditional (target text) and unconditional (null text) noise prediction at each time step with a guidance scale $w$ during the sampling trajectory:\n$\\tilde{z}_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}f_0(\\tilde{z}_t, t, v, v_{\\emptyset}) + \\sqrt{1 - \\bar{\\alpha}_{t-1}}\\epsilon_{\\theta}(\\tilde{z}_t, t, v, v_{\\emptyset}),$ (4)\n$f_0(\\tilde{z}_t, t, v, v_{\\emptyset}) = \\frac{\\tilde{z}_t - \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_{\\theta}(\\tilde{z}_t, t, v, v_{\\emptyset})}{\\sqrt{\\alpha}_t},$ (5)\n$\\epsilon_{\\theta}(\\tilde{z}_t, t, v, v_{\\emptyset}) = w \\cdot \\epsilon_{\\theta}(\\tilde{z}_t, t, v) + (1 - w) \\cdot \\epsilon_{\\theta}(\\tilde{z}_t, t, v_{\\emptyset}).$ (6)\nDue to the inherent property of DDIM inversion and DDIM sampling, the reconstruction trajectory forms a deterministic denoising mapping towards the reference image, where its intermediate denoising results $\\{z_t\\}_{t=1}^{T}$ can be used as pivotal guidance features to calibrate the corresponding counterpart $\\{\\tilde{z}_t\\}_{t=1}^{T}$ along the sampling trajectory. Therefore, the correlation between the reference image and the generated image can be established, and thus realizing text-driven I2I translation. Specifically, we implement feature calibration by inserting a plug-and-play frequency band substitution (FBS) layer in between the reconstruction trajectory and the sampling trajectory. FBS layer substitutes a certain frequency band of $\\tilde{z}_t$ in the sampling trajectory with the corresponding frequency band of $z_t$ in the reconstruction trajectory along the reverse sampling process. Such frequency band substitution effectively and efficiently imposes guidance of the reference image to the T2I synthesis process. Both the guiding factor (e.g., image appearance, image layout, image contours) and guiding intensity can be flexibly controllable simply by tuning the type and bandwidth of the substituted frequency band, respectively.\nTo improve I2I translation visual quality, we partition the sampling trajectory into a calibration phase and a non-calibration phase, separated by the time step $\\lambda T$. In the former calibration phase $(\\tilde{z}_T \\rightarrow \\tilde{z}_{\\lambda T})$, dynamic frequency band substitution is applied at each time step for smooth calibration of the sampling process; in the latter non-calibration phase $(\\tilde{z}_{\\lambda T-1} \\rightarrow \\tilde{z}_0)$, we remove FBS layer to avoid over-constrained sampling result, fully unleashing the generative power of the pre-trained T2I model to improve image generation quality. Here $\\lambda$ denotes the ratio of the length of the non-calibration phase to that of the entire sampling trajectory.\nAt last, the final result $\\tilde{z}_0$ of the sampling trajectory is decoded back to the image space via the LDM decoder $D$, producing the final translated image $\\tilde{x}$, i.e., $\\tilde{x} = D(\\tilde{z}_0)$.\nB. Frequency Band Substitution Layer\nAs Fig. 3 illustrates, the FBS layer takes in a pair of diffusion features $z_t$ and $\\tilde{z}_t$, converts them from the spatial domain into the frequency domain via 2D-DCT, then transplants a certain frequency band in the DCT spectrum of $\\tilde{z}_t$ to the same position in the DCT spectrum of $z_t$. Finally, 2D-IDCT is applied to transform the manipulated DCT spectrum of $\\tilde{z}_t$ back into the spatial domain as the final calibrated feature.\nIn 2D DCT spectrum, elements with smaller coordinates (nearer to the top-left origin) encode lower-frequency information while larger-coordinate elements (nearer to the bottom-right corner) correspond to higher-frequency components. Most of the DCT spectral energy is occupied by a small proportion of low-frequency elements near the top-left origin.\nIn FBS layer, the sum of 2D coordinates is used as the threshold to extract DCT frequency bands of different types and bandwidths through binary masking. Specifically, we design three types of binary masks which are respectively termed the low-pass mask ($Mask_{lp}$), high-pass mask ($Mask_{hp}$), and mid-pass mask ($Mask_{mp}$):\n$\\begin{cases}\nMask_{lp}(x, y) = 1 &\\text{if } x + y \\leq th_{lp} \\\\ Mask_{lp}(x, y) = 0 &\\text{else}\n\\end{cases}$,\n$\\begin{cases}\nMask_{hp}(x, y) = 1 &\\text{if } x + y > th_{hp} \\\\ Mask_{hp}(x, y) = 0 &\\text{else}\n\\end{cases}$,\n$\\begin{cases}\nMask_{mp}(x, y) = 1 &\\text{if } th_{mp1} < x + y < th_{mp2} \\\\ Mask_{mp}(x, y) = 0 &\\text{else}\n\\end{cases}$,\nwhere $th_{lp}$ is the threshold of the low-pass filtering; $th_{hp}$ is the threshold of the high-pass filtering; $th_{mp1}$ and $th_{mp2}$ are respectively the lower bound and upper bound of the mid-pass filtering. Given a binary mask $Mask \\in \\{Mask_{lp}, Mask_{hp}, Mask_{mp}\\}$, the frequency band substitution operation in the FBS layer can be formulated as:\n$\\tilde{z}_t = IDCT(DCT(z_t) \\cdot Mask_* + DCT(\\tilde{z}_t) \\cdot (1 - Mask_*)),$ (7)\nwhere $DCT$ and $IDCT$ refer to the 2D-DCT and 2D-IDCT transformations respectively, which are introduced in detail in the Appendix section. The use of the low-pass mask $Mask_{lp}$, high-pass mask $Mask_{hp}$, and mid-pass mask $Mask_{mp}$ respectively corresponds to the extraction and substitution of the low-frequency band, high-frequency band, and mid-frequency band, which controls different guiding factors of the reference image to the T2I generated result:\n\u2022 Low-frequency band substitution enables low-frequency information guidance of the reference image $x$, realizing image appearance (e.g., color, luminance) and layout control over the generated image $x$;\n\u2022 High-frequency band substitution enables high-frequency information guidance of $x$, realizing image contour control over the generated image $x$;\n\u2022 Mid-frequency band substitution enables mid-frequency information guidance of the reference image $x$. By filtering out higher-frequency contour information and lower-frequency appearance information in the DCT spectrum, the substitution of the mid-frequency band realizes only image layout control over the generated image $x$.\nThe DCT masking type and the corresponding thresholds used in the FBS layer are hyper-parameters of our method, which could be flexibly modulated to enable control over diverse guiding factors and continuous guiding intensity of the reference image $x$ to the T2I generated image $x$.\nC. Implementation Details\nWe use the pre-trained Stable Diffusion v1.5 as the backbone diffusion model and set the classifier-free guidance scale $w = 7.5$. We use 1000-step DDIM inversion to ensure high-quality reconstruction, i.e., $T_{inv}$=1000, and use 50-step DDIM sampling for both the reconstruction and sampling trajectory, i.e., T=50. Along the sampling trajectory, we allocate 55% time steps to the calibration phase and the remaining 45% steps for the non-calibration phase, i.e., $\\lambda$=0.45. For the default DCT masking thresholds used in the FBS layer, we set $th_{lp}$=80 for the low-frequency band substitution (low-FBS); $th_{hp}$=5 for the high-frequency band substitution (high-FBS); $th_{mp1}$=5, $th_{mp2}$=80 for the mid-frequency band substitution (mid-FBS). The complete algorithm of FBSDiff is presented in Alg. 1."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we first present and analyze the qualitative results of our method as well as qualitative comparison with related advanced methods; then we delve into the frequency band substitution mechanism in detail with ablation studies; finally, we show quantitative evaluations of our method and related approaches.\nA. Qualitative Results\nExample text-driven I2I translation results of our method are shown in Fig. 4. Our method effectively decomposes different guiding factors of the reference image to the generated image by dynamically transplanting different types of DCT frequency bands of diffusion features. The low-FBS transfers low-frequency information of the reference image into the sampling trajectory, producing the generated image that inherits the original image appearance and layout. In the mode of high-FBS that dynamically transplants high-frequency components of diffusion features, the generated image is aligned with the reference image in high-frequency contours while the low-frequency appearance is not restricted. The mid-FBS mainly imposes image layout control to the generated image since the lower-frequency appearance information and the higher-frequency contour information of the reference image are filtered out in the DCT domain. For all three types of frequency band substitution, the image translation results exhibit high visual quality and high text fidelity for both real-world and artistic-style reference images.\nThe control of our method over different guiding factors of the reference image is more clearly demonstrated in Fig. 5. The T2I generated image maintains the appearance and layout of the reference image with low-FBS; preserves detailed image contours of the reference image with high-FBS; and inherits pure image layout with mid-FBS.\nWe qualitatively compare our method with SOTA text-driven I2I translation methods including Plug-and-Play (PAP) [21], Null-text Inversion (Null-text) [16], Pix2Pix-zero [19], InstructPix2Pix (InsPix2Pix) [9], Prompt Tuning Inversion (PT-inversion) [17], StyleDiffusion [18], and VQGAN-CLIP (VQCLIP) [2], results are displayed in Fig. 7. The top panel of Fig. 7 shows that our method with low-FBS achieves better appearance consistency between the reference image and the translated result than related approaches, and is thus better suited to image creation scenario which favors inheriting the appearance and style from an existing image as much as possible. The bottom panel of Fig. 7 shows that existing SOTA text-driven I2I methods struggle at producing I2I results with large appearance change from the reference image, while our method with high-FBS excels in generating images with significantly different appearance, and is thus more suitable to image creation scenario where appearance divergence is pursued. Among the compared approaches, our method is the only one that enables flexible control over different guiding factors of the reference image, and is also the only approach that simultaneously dispenses with model training, fine-tuning, online optimization, and attention modulations.\nAn advantage of our approach over related methods is sampling diversity. As displayed in Fig. 6, our FBSDiff can produce diverse text-guided I2I results by randomly sampling $\\tilde{z}_T$ from isotropic Gaussian distribution, while other inversion-based methods [16], [21], [17], [19], [18] lack such sampling diversity due to directly initializing $\\tilde{z}_T$ with the inverted feature embedding of the reference image.\nThe importance of frequency band substitution (FBS) for reference image control is clearly shown in Fig. 8, from which we see that low-FBS establishes appearance and layout correlations between the reference and the generated images, while removing frequency band substitution leads to results with completely no correlation with the reference images. Moreover, as Fig. 9 displays, our method robustly adapts to varying degrees of semantic gap between the reference image and the target text prompt. The translated image of our method can still comply with the target text accurately with satisfying visual quality even in the case of very large image-text semantic discrepancy.\nBesides the controllability in the guiding factors of the reference image, our method also allows continuous control over the guiding intensity simply by modulating the bandwidth of the substituted frequency band. Results displayed in Fig. 10 demonstrate the image appearance and layout guiding intensity control of our method by adjusting the low-pass filtering threshold $th_{lp}$ in the mode of low-FBS. Enlarging the value of $th_{lp}$ widens the bandwidth of the transplanted frequency band and thus increases the amount of the low-frequency guiding information of the reference image, leading to the translated image with more resemblance to the reference image. Conversely, lowering the value of $th_{lp}$ narrows the bandwidth of the substituted frequency band, which reduces the amount of the low-frequency guiding information and thus brings more variations between the reference image and the translated result.\nLikewise, results in Fig. 11 demonstrate the image contour guiding intensity control of our method by adjusting the mid-pass filtering upper bound threshold $th_{mp2}$ in the mode of mid-FBS. When increasing the value of $th_{mp2}$, more high-frequency components of the reference image, namely more high-frequency guiding information, are included in the transplanted frequency band and transferred to the sampling trajectory, which results in more consistent image contours between the reference image and the translated image. On the contrary, decreasing the value of $th_{mp2}$ shrinks the transplanted high-frequency guiding information and thus leads to weaker image contour consistency.\nB. Ablation Study\nTo verify the rationality and effectiveness of our proposed method, we also explore and compare with other designs of frequency band substitution, including substituting the frequency band only once at $\\lambda T$ time step rather than along the whole calibration phase (which we denote as Once Substitution) and substituting the full DCT spectrum rather other only a partial frequency band of it (which we refer to as Full Substitution).\nThe image translation results of different designs of frequency band substitution (FBS) are displayed in Fig. 12. It shows that Once Substitution produces severely noisy results rather than reasonable images, which indicates that step-by-step FBS along the whole calibration phase is of crucial importance for smooth and stable information fusion. Since image content is basically formed at the early stage of the diffusion sampling process, removing per-step feature calibration of FBS in the early sampling stage will inevitably lead to large divergence between the sampling trajectory and the reconstruction trajectory. In this case, substituting a frequency band at the intermediate time step $\\lambda T$ will cause completely incoherent 2D DCT spectrum, and thus leads to abnormal image translation results after converting the diffusion features back to the spatial domain.\nBesides, it also shows that Full Substitution fails to manipulate the reference image as per the text prompt. This is because substituting the full DCT spectrum is equivalent to complete feature replacement, which makes the sampling trajectory totally the same as the reconstruction trajectory during the calibration phase, the early part of the diffusion sampling process that dominates the forming of image content. Therefore, the generated image content is forced to be the same as the reference image after the calibration phase and is difficult to be modified noticeably during the subsequent non-calibration phase, the latter part of the diffusion sampling process that focuses on refining fine-grained image details rather than coarse-grained image content. Thus, the sampling results of Full Substitution closely resembles the reference image, lacking editability and text fidelity.\nC. Quantitative Evaluations\nTo quantitative method evaluation, we evaluate methods separately on the text-driven I2I translation task that pursues image appearance consistency and the task that pursues image appearance divergence. For the former task, we assess models' appearance and layout preservation ability by measuring Structure Similarity (\u2191), Perceptual Similarity (\u2191), and Style Distance (\u2193) between the reference image and the translated image pair. For the latter task, we assess models' contour preservation and appearance alteration capabilities by measuring Structure Similarity (\u2191) and Style Distance (\u2191) between I2I translation pairs. For Structure Similarity measurement, we use DINO-ViT self-similarity distance [7] as the metric for Structure Distance between two images, and define Structure Similarity as 1 - Structure Distance. We use LPIPS [47] metric to measure Perceptual Similarity, and use AdaIN style loss [48] to measure Style Distance between I2I pairs. Besides, CLIP Similarity (\u2191) metric is used to measure semantic consistency between the target text prompt and the translated image, i.e., text fidelity of the I2I translation results. Finally, we evaluate Aesthetic Score (\u2191) of the translated images via the pre-trained LAION Aesthetics Predictor V2 model.\nWe sample reference images from the LAION Aesthetics 6.5+ dataset for quantitative evaluation. For the above-mentioned two tasks, we separately sample 500 reference images for each task and manually design 2 editing text prompts for each reference image, resulting in 1000 evaluation samples (pairs of reference image and target text) for each task. For evaluation of our method, we use low-FBS to handle the task pursuing appearance consistency and use high-FBS for the task pursuing appearance divergence. The average values of all the evaluation metrics are reported in Tab. I. Our method achieves top rankings for all the metrics in both two tasks, indicating superiority of our method in layout and appearance preservation with low-FBS, as well as simultaneous contour preservation and appearance modification with high-FBS. Moreover, the competitive results in CLIP Similarity and Aesthetic Score reflect that our method can generate I2I translation results with high text fidelity and visual quality.\nWe compare our FBSDiff with related text-driven I2I translation methods in method properties, results are summarized in Tab. II. Among the compared approaches, our method is the only one that possesses all the following advantages:\n\u2022 Dispense with model training;\n\u2022 Dispense with model fine-tuning;\n\u2022 Dispense with online-optimization at inference time;\n\u2022 Dispense with paired source text of the reference image;\n\u2022 Dispense with attention modulation operations inside the denoising network;\n\u2022 Invariant to the specific architecture of the backbone diffusion model.\nFor quantitative evaluation reported in Tab. I, we visualize partial results to highlight the superiority of our method over related approaches. For I2I task pursuing image appearance consistency, we display the scatter plot about Structure Similarity (\u2191) and CLIP Similarity (\u2191), and the scatter plot about (1-LPIPS)(\u2191) and CLIP Similarity (\u2191) in Fig. 13. Results show that our method with low-FBS achieves the most top-right position in both two scatter plots, indicating the best trade-off achieved by our method (low-FBS) in I2I translation appearance consistency and text fidelity. For I2I task pursuing image appearance divergence, we display the scatter plot about Structure Similarity (\u2191) and CLIP Similarity (\u2191), and the scatter plot about AdaIN Style Loss (\u2191) and CLIP Similarity"}, {"title": "V. CONCLUSION", "content": "This paper proposes FBSDiff, a plug-and-play method adapting pre-trained T2I diffusion model to highly controllable text-driven I2I translation. At the heart of our method is decomposing different guiding factors of the reference image in the diffusion feature DCT space, and dynamically transplanting a certain DCT frequency band from diffusion features along the reconstruction trajectory into the corresponding features along the sampling trajectory, which is realized via our proposed frequency band substitution layer. Experiments demonstrate that our method allows flexible control over both guiding factors and guiding intensity of the reference image simply by tuning the type and bandwidth of the substituted frequency band, respectively. In summary, our FBSDiff provides a novel solution to text-driven I2I translation from a frequency-domain perspective, integrating advantages in versatility, high controllability, high visual quality, and plug-and-play efficiency."}, {"title": "APPENDIX A DIFFUSION MODEL BACKGROUND", "content": "The Denoising Diffusion Probabilistic Model (DDPM) is a latent variable model that comprises a forward noising diffusion process and a reverse denoising diffusion process. Starting with a given data distribution $x_0 \\sim q(x_0)$", "follows": "n$q(x_t|x_{t-1"}, "N(x_t; \\sqrt{\\alpha_t}x_{t-1}, (1 - \\alpha_t)I),$ (8)\nwhere $\\alpha_t \\in (0,1)$, and $\\alpha_t \\geq \\alpha_{t+1}$. Using the notation $\\bar{\\alpha}_t := \\Pi_{i=1}^{t} \\alpha_i$, we can derive the marginal distribution $q(x_t|x_0)$ as follows:\n$q(x_t|x_0) := N(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1 - \\bar{\\alpha}_t)I),$ (9)\nwhere $\\sqrt{\\bar{\\alpha}_t}$ approaches to 0. With the above forward noising diffusion process, the source data distribution will be transformed into an isotropic Gaussian distribution.\\"]}