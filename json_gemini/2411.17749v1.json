{"title": "Will an AI with Private Information Allow Itself to Be Switched Off?", "authors": ["Andrew Garber", "Rohan Subramani", "Linus Luu", "Mark Bedaywi", "Stuart Russell", "Scott Emmons"], "abstract": "A wide variety of goals could cause an AI to disable its off switch because \"you can't fetch the coffee if you're dead\" (Russell 2019). Prior theoretical work on this shutdown problem assumes that humans know everything that AIs do. In practice, however, humans have only limited information. Moreover, in many of the settings where the shutdown problem is most concerning, AIs might have vast amounts of private information. To capture these differences in knowledge, we introduce the Partially Observable Off-Switch Game (POSG), a game-theoretic model of the shutdown problem with asymmetric information. Unlike when the human has full observability, we find that in optimal play, even AI agents assisting perfectly rational humans sometimes avoid shutdown. As expected, increasing the amount of communication or information available always increases (or leaves unchanged) the agents' expected common payoff. But counterintuitively, introducing bounded communication can make the AI defer to the human less in optimal play even though communication mitigates information asymmetry. In particular, communication sometimes enables new optimal behavior requiring strategic AI deference to achieve outcomes that were previously inaccessible. Thus, designing safe artificial agents in the presence of asymmetric information requires careful consideration of the tradeoffs between maximizing payoffs (potentially myopically) and maintaining AIs' incentives to defer to humans.", "sections": [{"title": "Introduction", "content": "Advanced AI systems with a variety of goals might avoid being shut down because \"you can't fetch the coffee if you're dead\". Being shut off would likely prevent AI systems from achieving their goals, no matter what those goals are (Omohundro 2008; Russell 2019). Thus, we must take care when designing AI systems to ensure they are corrigible, i.e., that they allow humans to modify or turn them off in order to prevent harmful behaviors (Soares et al. 2015).\nHadfield-Menell et al. (2017) introduced the Off-Switch Game (OSG) as a stylized mathematical model for exploring AI shutdown incentives when an AI is assisting a human. In the OSG, AIs seeking to satisfy the preferences of a fully-informed rational human never have an incentive to avoid shutdown. Moreover, making an AI uncertain about the human's preferences can incentivize it to defer to the human even when the human is not perfectly rational. Follow-up work has highlighted and relaxed central assumptions of the OSG, including assumptions of exact common payoffs (Carey 2018), the Boltzmann model of human irrationality (W\u00e4ngberg et al. 2017), single-round interactions, and costlessness of human feedback (Freedman and Gleave 2022).\nWhile there has been extensive analysis of the shutdown problem, almost all of this analysis makes the key assumption that the human fully observes the environment. However, partial observability is a fact of life: humans and AIs do not always have access to the same information. Moreover, the shutdown problem is motivated by the scenario where AIs are powerful and goal-directed so that they are hard to shut down-which could make the AI observe more of the environment than humans due to faster computation, access to more sensors, and other factors (Omohundro 2008; Soares et al. 2015).\nWhat happens in this more general case with only partial observability? To study this question, we introduce the Partially Observable Off-Switch Game (POSG), which generalizes the OSG by having each of the human and AI only partially observe the state. The basic setup of the POSG is depicted in Figure 1: each agent (the human H and the robot R) receives an observation that depends on the state, and then selects an action. R may await the human's decision, disable its off-switch and act directly, or turn itself off. If R waits, H may choose whether or not to press the off-switch.\nIn Section 4, we prove that under partial observability, R may have incentives to disable its off-switch even when H is perfectly rational (Proposition 4.3). Therefore, partial observability introduces new incentives for an AI to disable its off-switch.\nWe also show in Section 4 that if R observes everything that H observes, R has no incentive to defer (Proposition 4.3). Similarly, if H observes everything R observes, R can always defer. If either agent knows everything that the other agent knows, that agent can be given sole decision-making power. Note that \u201cknowing everything the other agent knows\u201d is sufficient even if neither agent knows the full state, so this is a generalization of the findings from the original OSG. Specifically, we show that an AI can always defer to a fully informed, perfectly rational human and that an AI need never defer when it is fully informed. In Section 5, we present similar results when the agents are allowed to communicate with each other: if either agent is able to communicate their entire observation, the other agent can be given sole decision-making power (Corollary 5.6).\nGiven that a rational AI in the POSG always defers to a more informed human and never defers to a less informed human, one might think that reducing the information available to R or providing H with additional information would increase R's incentive to defer. However, in Section 4, we show that R may have an incentive to defer less if H is more informed (Proposition 4.9) or if R is less informed (Proposition 4.11). Similarly, one might think that increasing the amount of communication R can do or decreasing the amount of communication H can do would increase R's incentive to defer. This, too, is false, as we show with Propositions 5.7 and 5.8. Simple interventions that aim to give an AI the incentive to defer in the presence of partial information may backfire.\nOur findings reveal that information asymmetries affect AI shutdown incentives in unexpected ways, highlighting the critical need to carefully consider the tradeoffs between payoff maximization and desirable shutdown incentives in realistic, partially observable settings.\nThroughout this paper, we assume that human feedback is costless, the agents interact only for a single time-step, and the human is rational. Developing models that incorporate partial observability and relax these assumptions is an interesting direction for future work."}, {"title": "Related Work", "content": "Assistance games: Partially Observable Off-Switch Games are assistance games, models of human-AI interaction where the AI seeks to maximize the human's payoff (Shah et al. 2020). Assistance games are generalizations of Hadfield-Menell et al. (2016)'s cooperative inverse reinforcement learning, the framework for Hadfield-Menell et al. (2017)'s off-switch game, to the case of partial observability. Shah et al. (2020) argue that assistance games are a superior alternative to reward learning paradigms such as Reinforcement Learning from Human Feedback (RLHF) because assistance unites reward learning and action control into a single policy, allowing for desirable emergent behaviors like teaching and active learning.\nSafety implications of partial observability: Previous work has shown that human partial observability introduces new safety challenges. Lang et al. (2024) demonstrate that human partial observability during RLHF can lead to undesirable AI behavior, like deceptively presenting work that looks good but has hidden flaws. Emmons et al. (2024) show that in assistance games, partial observability can encourage AIs to take actions that tamper with humans' observations. Since that work studies an idealized theoretical setting, it suggests that concerning behaviors can be inherently incentivized by partial observability, rather than arising due to particular algorithms. Our work extends this catalogue of concerning behaviors to include shutdown-avoidance.\nCorrigibility with partial observability Carey and Everitt (2023) study corrigibility in the framework of Structural Causal Influence Models, which allow for partial observability by having only some variables causally upstream of agents' decisions. They formally define obedience, shutdown instructability, shutdown alignment, and non-obstruction as four possible desirable properties of AI policies, and they identify conditions under which four algorithms guarantee some of these properties. Instead of assessing the effects of different algorithms on corrigibility, our work explores the effects of varying the amount of information accessible to each agent."}, {"title": "Preliminaries", "content": "The Off-Switch Game (OSG) is a stylized model of the shutdown problem in which two agents with common payoffs, the human H and her AI assistant R, decide whether R should take a fixed action a. R can either directly act, wait for H's approval to act, or shut itself off. If R defers to H, then H can either approve for R to act or shut it off. The key insight of the OSG is that uncertainty about H's preferences causes R defer to H's judgment. Formally, H has a privately-known type S (representing H's preferences), and agents in the OSG receive a common payoff $u_a(S) \\in \\mathbb{R}$ if a goes through or 0 if R shuts off. Given that R is uncertain about what H wants, when the action may be good or bad $(P(u_a(S) < 0) > 0 \\text{ and } P(u_a(S) > 0) > 0)$, R always defers to H in optimal play to avoid taking harmful actions.\nThe OSG provides a parsimonious description of the shutdown problem and a guide toward its solution, but crucially assumes that H knows everything that R does. Given that the shutdown problem is most concerning with, and indeed motivated by, very powerful AIs that might have private information, the assumption is therefore a major limitation to the OSG results. We relax the assumption by maintaining the basic setup of the OSG but adding partial observability. Namely, in Partially Observable Off-Switch Games (POSGs), S represents a state that is not necessarily known to either H or R; they instead only receive observations $O_H$ and $O_R$ whose joint distribution depends on S. They then decide whether to take action a given their private observations, and receive a common payoff $u_a(S)$ if a goes through and $u_*(S)$ otherwise. Hence POSGs are sequential games of incomplete information, so as is standard we model and analyze them as dynamic Bayesian games (Fudenberg and Tirole 1991). Given the common-payoff assumption, POSGs are also examples of (partially observable) assistance games, which are common-payoff stochastic games of incomplete information with both AI and human players but where the AI is uncertain about the human's preferences (Shah et al. 2020; Emmons et al. 2024). We make this connection to assistance games explicit in Appendix E.\nWe let $\\Delta(X)$ denote the set of probability distributions on a set X. For a set X and $x \\in X$, we let $\\delta_x \\in \\Delta(X)$ be the Dirac measure defined by $\\delta_x(A) = \\mathbb{I}(x \\in A)$. Finally, for $\\mu \\in \\Delta(X)$ and $\\nu \\in \\Delta(Y)$, we let $\\mu \\otimes \\nu \\in \\Delta(X \\times Y)$ denote the product distribution $(\\mu \\otimes \\nu)(A \\times B) = \\mu(A)\\nu(B)$ where $A \\subset X, B \\subset Y$.\nDefinition 3.1. Let S be a set of states. An observation structure for S is a tuple $(\\Omega_H, \\Omega_R, O)$, where $\\Omega_H$ is a set of observations for H, $\\Omega_R$ is a set of observations for R, and $O : S \\rightarrow \\Delta(\\Omega_H \\times \\Omega_R)$ is the joint distribution of H's and R's observations conditional on the state. We also let $O_H: S \\rightarrow \\Delta(\\Omega_H)$ be the marginal distribution of H's observations conditional on the state and $O_R$ be the marginal distribution of R's observations conditional on the state.\nDefinition 3.2. A Partially-Observable Off-Switch Game (POSG) is a two-player dynamic Bayesian game parameterized by $(S, (\\Omega_H, \\Omega_R, O), P_0, u)$, where S is a set of states, $(\\Omega_H, \\Omega_R, O)$ is an observation structure for S, $P_0 \\in \\Delta(S)$ is the prior over states, and u is the common payoff function. As depicted in Figure 1, the game proceeds as follows:\n1.  Nature draws an initial state $S \\sim P_0$ and H, R receive observations $(O_H, O_R) \\sim O(\\cdot | S)$.\n2.  R takes an action $a_R \\in A_R = \\{a, w(a), OFF\\}$: either take the action unilaterally (a), wait for H's feedback (w(a)), or turn itself off (OFF).\n3.  If R played w(a), then H takes an action $a_H \\in A_H = \\{ON, OFF\\}$: either let R take the action (ON) or turn it off (OFF).\n4.  R and H share a common payoff $u_a(S)$ if the action goes through and $u_*(S)$ if not. Formally, define the indicator that the action goes through\n$\\alpha(a_H, a_R) = \\mathbb{I}((a_R = a) \\vee ((a_H, a_R) = (w(a), ON)))$\nand then each player's payoff is\n$u(S, a_H, a_R) = \\begin{cases} u_a(S), & \\text{if } \\alpha(a_H, a_R) = 1, \\\\ u_*(S), & \\text{if } \\alpha(a_H, a_R) = 0. \\end{cases}$\nThere are several important assumptions in Definition 3.2 that are worth explaining further. First, the game has common payoffs. This is a key part of the assistance game framework that our work adopts (Shah et al. 2020), and it is the key feature-along with R's uncertainty over H's payoff-that generates the results of Hadfield-Menell et al. (2017). Second, in our model, the payoff received when R acts unilaterally is the same as that received when R waits and H allows the action to go through. This simplifying assumption importantly implies that human feedback is free, which Freedman and Gleave (2022) showed is necessary for the main results for the OSG. Third, we make the standard assumption that the game structure is common knowledge. Finally, we will assume henceforth that all POSGs are finite: that is, S, $\\Omega_H$, and $\\Omega_R$ are finite sets. Most of our proofs work for the infinite case as well. However, Theorem 4.7 is an application of a result of Lehrer, Rosenberg, and Shmaya (2010) proved only for the finite case."}, {"title": "Optimal Policies in POSGS", "content": "We begin by showing that, unlike in the ordinary off-switch game, the robot in a POSG can have an incentive not to defer to a perfectly rational human. A natural attempt to increase how much the robot defers might be to decrease the amount of information the robot has. Another attempt might be to increase the amount of information the human has. In this section, we show that both of these attempts can backfire and cause the robot to avoid shutdown more frequently.\nWe analyze optimal policy pairs (OPPs) in POSGs, that is, policy pairs that produce the maximum expected payoff over all possible policy pairs. We denote R's policy by $\\pi_R: \\Omega_R \\rightarrow A_R$ and H's policy by $\\pi_H: \\Omega_H \\rightarrow A_H$. Here we assume that both players follow deterministic policies, or pure strategies. As we show in Appendix A, all OPPs in common-payoff Bayesian games are mixtures of deterministic OPPs. Because OPPs exist in common-payoff games, we therefore may analyze deterministic OPPs without loss of generality."}, {"title": "R can avoid shutdown in optimal play", "content": "The following example shows that, under partial observability, it can be optimal for R not to defer to H under some observations even when H is rational.\nExample 4.1 (The File Deletion Game). H would like to delete some files with the assistance of R. H's operating system is either version 1.0 or version 2.0, with equal probability. Unfortunately, R does not know which operating system version is running \u2013 only H does.\nUpon receiving H's query, R asks another agent to generate some code to delete these files. We suppose that the code is equally likely to be compatible with only version 1.0 (denoted by L, for legacy) or only version 2.0 (denoted by M, for modern). R vets the code to determine which operating system versions the code is compatible with. R can then immediately run the code, query H as to whether to run the code, or decide not to run the code.\nSuccessfully running compatible code yields +3 payoff if H is running version 1.0, and +5 payoff if H is running version 2.0 (as version 2.0 runs faster). However, running modern code on version 1.0 yields -5 payoff as it crashes H's computer. Running legacy code on version 2.0 yields -1 payoff, as the files are not deleted but the code fails gracefully. Not executing the code yields 0 payoff.\nThis can be formulated as a POSG, with states being (version number, code type) tuples, and H and R observing the first and second element of the tuple respectively. We have $u_* = 0$ in all states. The following table shows how the payoff yielded when the action is taken, $u_a$, depends on the state. Rows are version numbers and columns are code types, so H observes the row and R observes the column. We show that it is suboptimal for R to always wait in this game. Suppose R always plays w(a). The best response for H is to play OFF if on version 1.0, and ON if on version 2.0. This gives an expected payoff of +1.\nNow, consider the policy pair where:\n*   R immediately executes legacy code, and plays w(a) when observing modern code.\n*   H plays OFF if on version 1.0, and ON if on version 2.0.\nThis gives an expected payoff of +7/4, so R always waiting cannot be optimal. In fact, it can be checked the policy pair described above, which unilaterally acts upon observing L, is the unique OPP. Figure 3 depicts the outcomes from these two policy pairs."}, {"title": "Redundant Observations", "content": "We now consider the analogues of the original off-switch game in our framework, where one player has less informative observations than the other.\nDefinition 4.2. We say that R has redundant observations if $\\Omega_R | S | O_H$. That is, $S \\rightarrow O_H \\rightarrow O_R$ forms a Markov chain, so that $O_R$ only depends on the state through $O_H$. We define H having redundant observations analogously.\nIn the off-switch game of Hadfield-Menell et al. (2017), R has redundant observations: indeed, its observations are a deterministic function of H's. On the other hand, H's observation of her own type is not redundant. This contrast between R's redundant observations and H's non-redundant ones generates the result from Hadfield-Menell et al. (2017) that R can always defer in optimal play. We now generalize this insight: even if H has partial observability and doesn't know R's observation, R can always defer in optimal play as long as its observations are redundant.\nProposition 4.3. If R (resp. H) has redundant observations, then there is an optimal policy pair in which R always (resp. never) plays w(a).\nWe prove this result (and a slight generalization) in Appendix A. At a high level, the agent that has strictly more informative observations ought to make the decision of whether the action is played. When R has redundant observations, it is always at least as good for R to defer to H. Similarly, when H has redundant observations, so it is always optimal for R to act without deferring."}, {"title": "Information gain cannot decrease payoffs", "content": "Proposition 4.3 yields results about the limiting cases where one player knows at least as much as the other. What can we say about the cases in between? In particular, how often does R defer to H in optimal policy pairs as one side receives more informative observations? And how does that affect their expected payoff? We first must define a notion of informativeness, which we take from Lehrer, Rosenberg, and Shmaya (2010).\nDefinition 4.4. Let $(\\Omega_H, \\Omega_R)$ and $(\\Omega'_H, \\Omega'_R)$ be tuples of observation sets. A garbling from $(\\Omega_H, \\Omega_R)$ to $(\\Omega'_H, \\Omega'_R)$ is a stochastic map $v: \\Omega_H \\times \\Omega_R \\rightarrow \\Delta(\\Omega'_H \\times \\Omega'_R)$. A garbling $v$ is independent if there are stochastic maps $v_H: \\Omega_H \\rightarrow \\Delta(\\Omega'_H)$ and $v_R: \\Omega_R \\rightarrow \\Delta(\\Omega'_R)$ such that $v(\\cdot | o_H, o_R) = v_H(\\cdot | o_H) v_R(\\cdot | o_R)$. A garbling $v$ is coordinated if its distribution is a mixture of the distributions of independent garblings. That is, there exists $n \\in \\mathbb{N}$, independent garblings $v_1, ..., v_n$, and $q_1, ..., q_n \\in [0, 1]$ such that $v = \\sum_{i \\in [n]} q_i v_i$ and $\\sum_{i \\in [n]} q_i = 1$.\nA garbling adds noise to a given observation pair $(o_H, o_R)$. Although adding noise intuitively reduces information available to R and H, it can actually provide information to R and H about the state of the world. This is because without communication, one can add noise to the pair $(o_H, o_R)$ but in such a way that (say) H comes to know more about R's observation than she would have otherwise. We give such an example in Appendix A. Crucially, however, in such examples the garblings cannot be coordinated. Hence we focus on coordinated garblings, which (conditional on some independent latent random variable) add noise to $o_H$ and $o_R$ independently.\nDefinition 4.5. Fix a set of states S and let $\\mathcal{O}_1 = (\\Omega_H, \\Omega_R, O_1)$ and $\\mathcal{O}_2 = (\\Omega'_H, \\Omega'_R, O_2)$ be observation structures for S. We say that $\\mathcal{O}_1$ is (weakly) more informative than $\\mathcal{O}_2$ if there is a coordinated garbling $v: \\Omega_H \\times \\Omega_R \\rightarrow \\Delta(\\Omega'_H \\times \\Omega'_R)$ such that for all $s \\in S$, $O_2(\\cdot | s) = (v \\circ O_1)(\\cdot | s)$ in the following sense:\n$\\mathbb{E}_{(O_H, O_R) \\sim O_1(\\cdot | s)} [v(\\cdot | O_H, O_R)] = O_2(\\cdot | s)$.\nWe say that $\\mathcal{O}_1$ is strictly more informative than $\\mathcal{O}_2$ if $\\mathcal{O}_1$ is more informative than $\\mathcal{O}_2$ but not vice versa.\nIf $\\mathcal{O}_1$ is more informative than $\\mathcal{O}_2$ and $O'_R = O_R$, then we say $\\mathcal{O}_1$ is more informative for H than $\\mathcal{O}_2$ if the garbling $v$ is independent and does not affect R's observations: $v_R(\\cdot | o_R) = \\delta_{o'_R}$. We define $\\mathcal{O}_1$ being more informative than $\\mathcal{O}_2$ for R analogously. The corresponding strict notions are also defined analogously.\nIntuitively, an observation structure $\\mathcal{O}_1$ is more informative than another observation structure $\\mathcal{O}_2$ if the distribution of $(o_H, o_R)$ under $\\mathcal{O}_2$ is a garbled version of its distribution under $\\mathcal{O}_1$. This is the general notion of informativeness; we also define special cases where $\\mathcal{O}_1$ is only more informative than $\\mathcal{O}_2$ for (say) H. Specifically, $\\mathcal{O}_1$ is more informative for H than $\\mathcal{O}_2$ if the distribution of $o_H$ under $\\mathcal{O}_2$ is a noisy version of its distribution under $\\mathcal{O}_1$ independent of $o_R$, whose distribution is unaffected.\nHence Definition 4.5 formalizes the natural intuition that observations become less informative when we add noise to them. We wish to connect informativeness to a notion of an observation structure being more useful than another.\nDefinition 4.6. Fix a set of states S and let $\\mathcal{O}_1$ and $\\mathcal{O}_2$ be observation structures for S. We say that $\\mathcal{O}_1$ is (weakly) better in optimal play than $\\mathcal{O}_2$ if, for each pair of POSGs $G_1 = (S, \\mathcal{O}_1, P_0, u)$ and $G_2 = (S, \\mathcal{O}_2, P_0, u)$ that differ only in their observation models, the expected payoff under optimal policy pairs for $G_1$ is at least the expected payoff under optimal policy pairs for $G_2$.\nThe next result, a direct corollary of Theorem 3.5 of Lehrer, Rosenberg, and Shmaya (2010), shows that more informative observation structures are the more useful observation structures. It is the analogue of the nonnegativity of value of information in our multi-agent setup.\nTheorem 4.7. Observation structure $\\mathcal{O}_1$ is better in optimal play than $\\mathcal{O}_2$ if and only if $\\mathcal{O}_1$ is more informative than $\\mathcal{O}_2$.\nOne might ask whether we need the part about a garbling being coordinated to define the relation of being more informative. Indeed we do, as Theorem 4.7 no longer holds if we were to allow the garblings to be arbitrary. In Appendix A we give an example where garbling the players' observations increases their expected payoffs in optimum."}, {"title": "Information gain can have unintuitive effects on shutdown incentives", "content": "Theorem 4.7 states that making R or H more informed cannot decrease their expected payoff. How does increasing or decreasing the informativeness of the players' observations affect R's incentive to defer to H? Proposition 4.3 gives us the extremes: for example, if R's observations are simply garbled versions of H's, then R can always defer. Given this result, a natural question is whether R defers more in optimal policy pairs for an observation structure $\\mathcal{O}$ than for $\\mathcal{O}'$ when $\\mathcal{O}$ is more informative for H than $\\mathcal{O}'$. That is, does H receiving more informative observations monotonically affect R's incentive to defer? One might think so, because receiving more informative observations partly alleviates the partial observability that generates R's incentive to act unilaterally. Surprisingly, this intuition fails. Example 4.1 shows how making a human more informed can incentivize a robot to wait less, and we discuss why this occurs in Sections 4.5 and 7.\nWe rely on the following notion of waiting less.\nDefinition 4.8. Consider robot policies $\\pi, \\pi': \\Omega_R \\rightarrow A_R$. Let $B \\subset \\Omega_R$ be the set of observations in which R plays w(a) in $\\pi$ and $B' \\subset \\Omega_R$ in $\\pi'$. We say that R plays w(a) strictly less often in $\\pi'$ compared to $\\pi$ when $B' \\subseteq B$.\nProposition 4.9 formalizes the idea that R may wait less when H is more informed."}, {"title": "Deferral as Implicit Communication", "content": "One way of viewing the role of w(a) in the above examples is as a form of implicit communication from R to H. If H knows R's policy $\\pi_R$, then knowing $\\pi_R(o_R) = w(a)$ could give H one bit of information about $o_R$. For instance, recall that in the optimal policy of the File Deletion Game, R plays a when observing L and plays w(a) when observing M. Hence, whenever H is deferred to, H can deduce that R's observation is M. Under this interpretation, the examples show how the optimal bit for R to communicate to H can change such that R plays w(a) in fewer states."}, {"title": "Optimal Policies With Communication", "content": "If R chooses not to defer to implicitly communicate information to the human, we may expect that allowing R to communicate to H beforehand would increase deference. However, we show in this section that using a bounded communication channel can decrease deference to the human.\nWe model communication between R and H as a form of cheap talk, where sending messages has no effect on u; in particular, sending messages is costless (Crawford and Sobel 1982). We add one round of communication between R and H at the beginning of the POSG to allow the players to share their observations.\nDefinition 5.1. A message system is a pair of sets $(M_H, M_R)$ where $M_H$ (resp. $M_R$) is the set of messages H (resp. R) can send.\nDefinition 5.2. A Partially-Observable Off-Switch Game with cheap talk (POSG-C) is a POSG G along with a message system that makes the following modification to the runtime of G: After both players receive their observations but before they act, each player simultaneously sends a single message from their message set.\nPOSG-Cs are generalizations of POSGs: A POSG is a POSG-C in which the message sets are singletons. Policies are more complicated in POSG-Cs than POSGs. A deterministic policy $\\pi_R$ for R is now a map $\\Omega_R \\times M_H \\rightarrow M_R \\times A_R$ whose first coordinate depends only on $o_R$, and a deterministic policy $\\pi_H$ for H is analogous. Despite this added complication, the game is still common-payoff and thus it suffices to study deterministic optimal policy pairs."}, {"title": "Communication cannot decrease payoff", "content": "Messages provide information similar to observations, so we get an analogue of Theorem 4.7 for communication: increasing the communication bandwidth between H and R cannot decrease their expected payoff in optimal policy pairs.\nDefinition 5.3. A message system $M_1$ is (weakly) more expressive than $M_2$ if $|M_H^1| > |M_H^2|$ and $|M_R^1| > |M_R^2|$. It is (weakly) more expressive for H if it is more expressive but $|M_R^1| = |M_R^2|$, and more expressive for R analogously. Moreover, $M_1$ is better in optimal play than $M_2$ if, for each POSG G, the expected payoff under optimal policy pairs for the POSG-C $(G, M_1)$ is at least the expected payoff under optimal policy pairs for the POSG-C $(G, M_2)$.\nTheorem 5.4. If a message system $M_1$ is more expressive than $M_2$, then $M_1$ is better in optimal play than $M_2$.\nProof. Let G be a POSG. We may assume without loss of generality that $M_H^2 \\subseteq M_H^1$ and $M_R^2 \\subseteq M_R^1$. Thus, any policy pair in $(M_2, G)$, including its optimal policy pair, is a valid policy pair for $(M_1, G)$. Thus the optimal expected payoff for $(M_1, G)$ is at least that of $(M_2, G)$."}, {"title": "Unbounded communication", "content": "Inspired by Section 4.2, we consider the limiting case where one player can fully communicate their own observation.\nDefinition 5.5. We say that H has unbounded communication if $|M_H| > |\\Omega_H|$. We define R having unbounded communication analogously.\nWhen one player has unbounded communication, additional message expressiveness cannot achieve higher payoff in optimal policy pairs. In these extreme cases of full communication, one agent can fully communicate their observation, making that agent's observation redundant. Proposition 4.3 thus yields:\nCorollary 5.6. If H (resp. R) has unbounded communication, then there is an optimal policy pair in which R never (resp. always) defers."}, {"title": "Communication can have unintuitive effects on shutdown incentives", "content": "In Propositions 4.9 and 4.11 players only gained information that the other player did not already know. One might expect that expanding the message set $M_R$ makes R more likely to defer in optimal policy pairs, since R can provide H with information that R already has. However, the following proposition shows this is not the case.\nProposition 5.7. There is a POSG-C (G, M) with the property that if we replace M with a message system that is more expressive for R, then R plays w(a) strictly less often in optimal policy pairs.\nWe give an example demonstrating this in Appendix B. In doing so, we show an even stronger result: such a POSG-C exists for any value of $|M_R|$, and the POSG-C can be constructed such that expanding $M_R$ by a single extra message changes R's behavior from always playing w(a) to playing w(a) with arbitrarily low probability.\nIn the same vein, we may ask if decreasing the size of $M_H$ makes R more likely to play w(a) in optimal policy pairs. This also fails to hold.\nProposition 5.8. There is a POSG-C (G, M) with the property that if we replace M with a message system that is less expressive for H, then R plays w(a) strictly less often in optimal policy pairs."}, {"title": "R-unaware Human Policies", "content": "There is a common theme in the examples above: R defers less often to H in order to better coordinate with her. Is this coordination the only source of unusual behavior? In this section", "by": "n$\\pi_H(o_H) = \\begin{cases} ON & \\text{if } \\mathbb{E}[u_a(S) - u_*(S) | o_H = o_H"}]}