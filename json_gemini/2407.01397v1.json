{"title": "Mask and Compress: Efficient Skeleton-based Action Recognition in Continual Learning", "authors": ["Matteo Mosconi", "Andriy Sorokin", "Aniello Panariello", "Angelo Porrello", "Jacopo Bonato", "Marco Cotogni", "Luigi Sabetta", "Simone Calderara", "Rita Cucchiara"], "abstract": "The use of skeletal data allows deep learning models to perform action recognition efficiently and effectively. Herein, we believe that exploring this problem within the context of Continual Learning is crucial. While numerous studies focus on skeleton-based action recognition from a traditional offline perspective, only a handful venture into online approaches. In this respect, we introduce CHARON (Continual Human Action Recognition On skeletoNs), which maintains consistent performance while operating within an efficient framework. Through techniques like uniform sampling, interpolation, and a memory-efficient training stage based on masking, we achieve improved recognition accuracy while minimizing computational overhead. Our experiments on Split NTU-60 and the proposed Split NTU-120 datasets demonstrate that CHARON sets a new benchmark in this domain. The code is available at https://github.com/Sperimental3/CHARON.", "sections": [{"title": "1 Introduction", "content": "Human Action Recognition (HAR) has become critical in various domains such as surveillance [27,29], rehabilitative healthcare [51], and sports analysis [23,39]. Early HAR approaches focused on exploiting RGB or gray-scale videos due to their widespread availability. However, recent advancements have explored alternative modalities, including skeletal joints [10,25,51], depth [36], point clouds [15], acceleration [24], and WiFi signals [42]. Among these, skeleton-based action recognition stands out as particularly efficient and concise, especially for actions not involving objects or scene context. Skeleton sequences capture the trajectory of key points (i.e., joints) in the human body (e.g., elbows, knees, wrists) [48]. As joints can be represented by 2D or 3D spatial coordinates,"}, {"title": "2 Related works", "content": "Skeleton-based Action Recognition. In early skeleton-based action recognition works, sequences were treated as time series, thus processed employing Recurrent Neural Networks (RNNs) [11,53,18,8] to capture dynamics over time. These approaches struggled to integrate the spatial context of joints and proved slow and challenging to parallelize. Following works exploited Convolutional Neural Networks (CNNs) [21,20], treating skeletal data in various ways to make them compatible with CNNs; some handle coordinates as image channels [10,25], while others reshape skeletons by combining joints in space and time [20].\nHowever, these models faced a common limitation: they failed to effectively represent the relationships between skeletal joints moving together in time. Graph Convolutional Networks (GCNs) resolve such shortcomings by exploiting nodes (i.e., joints) temporally and spatially [50,12,13,7,40]. Subsequently, the emergence of ViT [9] marked the introduction of transformer-based architectures into computer vision, leading to solutions that integrate self-attention layers into convolutional architectures. One such work, STTFormer [31], divides the sequence in tuples of joints and retains some concepts of CNNs (i.e., pooling aggregation) for in-time features processing. Nonetheless, such an approach under-exploits the sparsity and redundancy of skeletal data. In recent years, masking approaches [47,49] have been employed to take advantage of these characteristics for pre-training models. In contrast, our proposal adopts the reconstruction objective even during the optimization of the downstream task. Such a choice brings the benefit of reducing the training requirements of the whole pipeline, avoiding the pre-training phase."}, {"title": "Continual Learning", "content": "The Continual Learning setting makes a more realistic assumption w.r.t. standard learning paradigms. Specifically, data arrival is continuous and incremental. A subset of CL is Class-Incremental Learning (Class-IL) [44], where the dataset is re-arranged into multiple subsequent tasks, each containing a unique and disjoint set of classes. In this setting, the task identity is not known during inference.\nClassical CL methods employ a regularization term that penalizes the alterations of weights to avoid forgetting [22,52,37]. Rehearsal methods [34,32,5,1], on the other hand, employ a limited memory buffer in which they store samples from past tasks and replay them. Another paradigm is represented by dynamic architectures [35,3] in which new network components are instantiated for each incoming task; unfortunately, this often leads to a rapid increase in the number of parameters. This approach has been employed by the authors of Else-Net [26] to tackle skeleton-based HAR in Class-IL. They use the first 50 classes of NTU RGB+D 60 to pre-train their network, and perform incremental training across 10 tasks, each focusing on a different class. We retain that such a benchmark diverges from classical CL ones, as it is simplified and far from real-world scenarios. In our work, we utilize the same setting presented by the authors of [4], who split NTU RGB+D 60 into 6 tasks, each involving multiple classes."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Preliminaries", "content": "Class-Incremental Learning. In Class-IL, a deep model $f(\\cdot; \\theta)$ parametrized by $\\theta$ is presented with a sequence of tasks $T_i$ with $i\\in \\{1, \\dots, T\\}$, with $T$ denoting the number of tasks. The i-th task provides $N_i$ data entries $\\{(x_i^{(n)}, y_i^{(n)})\\}_{n=1}^{N_i}$ with $y_i^{(n)} \\in Y_i$; importantly, each task relies on a set of classes disjoint from others s.t. $Y_i \\cap Y_j = \\emptyset \\Leftrightarrow i \\neq j$. The objective of Class-IL is to minimize the empirical risk over all tasks:\n$\\mathcal{L}_{Class-IL} = \\sum_{i=1}^T \\mathbb{E}_{(x,y)\\sim T_i} [\\mathcal{L}(f(x; \\theta), y)],$ (1)\nwhere $\\mathcal{L}$ is the loss function (e.g., the cross entropy for classification) and $y$ is the ground truth label. Since the model observes one task at a time, tailored strategies are required to prevent catastrophic forgetting. Specifically, some rehearsal approaches [5,33] employ an additional regularization term $\\mathcal{L}_M$ exploiting samples stored in the memory buffer. The objective at the current task $T_e$ is:\n$\\mathcal{L}_{Class-IL} = \\mathbb{E}_{(x,y)\\sim T_e} [\\mathcal{L}(f(x;\\theta), y)] + \\mathcal{L}_M.$ (2)\nSpatio-Temporal Tuples Transformer (STTFormer). We adopt as main backbone of our architecture STTFormer [31], a transformer-based model designed for skeleton-based action recognition. It exploits self-attention to capture"}, {"title": "3.2 CHARON", "content": "In this section, we present CHARON, which encompasses three components: i) a technique to populate the memory buffer, employing linear interpolation to decompress memory samples; ii) an efficient training phase with masked inputs; iii) a linear probing stage, which refines the classifier and updates the logits stored in the memory buffer. We depict these elements in Fig. 1.\nEfficient buffer. A raw skeleton sequence $x \\in \\mathbb{R}^{C\\times F\\times V}$ collects the $C$ coordinates (e.g., xyz in Split NTU-60 and Split NTU-120) of $V$ joints at $F$ time instants. Unlike RGB video frames, skeletal data inherently reside in Euclidean"}, {"title": "Training phase", "content": "As we mentioned above, a transformer-based architecture founded on [31] is adopted as our backbone. We build upon it to derive an encoder-decoder framework inspired by masked autoencoders [17]. Notably, this allows us to reduce the computational effort during training, as depicted in Fig. 2. Specifically, given a sample $x$ coming from the current task or the buffer, the first step consists of a linear projection, followed by positional encoding to inject temporal dependencies. Afterward, we feed the encoder $e(\\cdot; \\theta_e)$ with a temporally masked sample $\\tilde{x} \\in \\mathbb{R}^{C\\times \\lfloor(1-\\eta)\\cdot F\\rfloor\\times V}$ obtained by dropping a random subset of frames from the input sequence, where $\\eta \\in [0, 1)$ is the masking ratio.\nThe encoder projects the input into the latent space, obtaining features $h = e(\\tilde{x}; \\theta_e)$. From this point, the architecture devises two branches: the first one (recognition) features a fully connected layer $f(\\cdot; \\theta_f)$ to yield pre-softmax logits $z = f(h;\\theta_f)$. The second branch (reconstruction) realizes the self-supervised"}, {"title": "Linear probing", "content": "As described above, the model is trained with partial skeleton sequences. While providing an efficient training strategy, there is a factor that could hinder the overall performance during evaluation. Indeed, we argue that the classification heads $f(\\cdot; \\theta_f)$ could be subject to possible misalignment due to the different conditions we have at training (masking on) and test time (masking off). To address this issue, highlighted in Fig. 3, we devise an auxiliary linear probing stage at the end of each task, which lasts for a few epochs (i.e., 10% of the number employed for the main training stage). During this phase, only the parameters of the classifier are allowed to change, while the encoder remains frozen. In doing so, we feed each full (i.e., not masked) sample $x \\in \\mathbb{R}^{C\\times F\\times V}$ to the encoder.\nIn formal terms, as for the main training phase, the encoder projects the input $x$ into the latent space obtaining hidden features $h = e(x; \\theta_e)$. The fully connected linear layer $f(\\cdot; \\theta_f)$ produces then the logits $z = f(h;\\theta_f)$ to which a cross-entropy loss is finally applied. In this phase, we still employ the regularization from [5]. Thus, the resulting objective $\\mathcal{L}_{lp}$ can be written as:\n$\\mathcal{L}_{lp} = \\mathcal{L}_{CE}(z, y) + \\alpha\\cdot ||f(h_M; \\theta_f) - z_M||_2 + \\beta\\cdot \\mathcal{L}_{ce}(f(h_M; \\theta_f), y_M).$ (8)"}, {"title": "4 Experimental analysis", "content": ""}, {"title": "4.1 Datasets", "content": "Split NTU-60 and Split NTU-120. NTU is one of the most popular benchmarks for action recognition on skeletal data. Initially comprising 60 classes and 56578 samples in its original version [38], and later expanded to 120 classes and 113945 samples [28], this dataset encompasses a diverse range of actions involving up to two individuals. The data collection process involves three Kinect cameras [54], positioned with different angles w.r.t. the subject. They provide RGB videos, IR videos, depth map sequences, and 3D skeletal data. Participants of various ages have contributed to the datasets construction, ensuring its broad applicability and relevance.\nWe adopt the extraction process employed by [31]. As original raw sequences contain a varying number of frames, we apply bilinear interpolation to obtain fixed-length sequences $x$ (i.e., 120 frames) s.t. $x \\in \\mathbb{R}^{(C=3)\\times (F=120)\\times (V=25)\\times (B=2)}$. The axis identified by $B$ regards the poses of the potentially two subjects involved in the action."}, {"title": "4.2 Implementation details", "content": "The custom version we adopt for STTFormer [31] reduces the width of intermediate layers to obtain a more lightweight model. We set the number of frames in each tuple $n = 6$ as in the original paper. Following the asymmetric design proposed in [17], we employ 8 layers for the encoder and 3 for the decoder. We refer the reader to the supplementary material for further details. Additionally, we employ an $\\alpha$ of 0.3 and a $\\beta$ of 0.8 for Eqs. (7) and (8), while we use a $\\gamma$ of 0.5 in approaches using the reconstruction regularization (Eqs. (4) to (6)). We adopt a batch size of 16 for all our experiments with a vanilla SGD optimizer and a learning rate of 0.05. Each task of the incremental setting lasts for 30 epochs. With the same hyperparameters as above, we perform 3 epochs for the linear probing phase. Finally, concerning data augmentation, we follow the original STTFormer implementation, applying a simple random rotation to each input sample."}, {"title": "4.3 Results", "content": "For the experimental comparison, we indicate with Joint Training (JT) the upper bound of our approach. It consists of training the model on the unified dataset (i.e., without splitting it into tasks). For the lower bound, we adopt an incremental training approach that does not employ tailored techniques against catastrophic forgetting. We refer to it as Fine Tuning (FT).\nIn Tab. 1 we report the results for buffer sizes $M_{size}$ of dimensions 500 and 2000. Following other works [5,26,4], we measure the recognition performance in terms of Final Average Accuracy (FAA), defined as:\n$FAA = \\frac{1}{T} \\sum_{i=1}^T a_{T_i}$ (9)\nwhere $a_{T_i}$ is the accuracy of the i-th task after the model has seen all T of them. Additionally, we repeat each experiment three times, thus reporting the mean and standard deviation of the FAA."}, {"title": "On the sampling interval", "content": "To further evaluate the effectiveness of our buffer strategy, we conduct a comparative study on varying sampling interval $s$ (which we recall indicates the step length in the uniform sampling procedure). Given $s\\in \\mathbb{N}^+$, we obtain the compression ratio as:\n$compression ratio = \\frac{s-1}{s} 100.$ (10)\nWe report in Fig. 4 (left) the FAA at varying sampling interval $s$ for both the buffer sizes tested. For each tested sampling interval, we scale the buffer size accordingly (as documented in Sec. 3.2). For instance, when $s = 10$, a memory with a nominal capacity of 500 examples could hence contain at most $s\\cdot500 = 5000$ (compressed) examples. As can be appreciated, the sampling interval $s = 5$ (i.e., 80% of compression) yields the best results in terms of final accuracy. Namely, when sampling one skeletal pose every five frames, the memory buffer"}, {"title": "On the masking ratio", "content": "We herein assess the impact of the masking ratio, which indicates the number of frames discarded before feeding the input sequence to the model. The results are illustrated in Fig. 4 (right) and reveal an increase in performance up to a value of 30%. For higher masking ratios, performance begins to decline, despite the notable efficiency gains (see Fig. 2). In quantitative terms, even with 50% of masking, CHARON achieves an acceptable final average accuracy of around 68%, while it decreases to $\\approx$ 66% with a masking ratio equal to 60%. Interestingly, both of these results are still higher than those of DER++, the second-best method reported in Tab. 1."}, {"title": "4.4 Ablations", "content": "We herein report the ablative studies; all the experiments are performed on the XView modality of Split NTU-60.\nOn the importance of the reconstruction-based objective. Our approach not only seeks good classification capabilities but also devises an auxiliary reconstruction term targeting the entire input sequence. To shed further light on the effects of such an auxiliary objective, we provide an ablative experiment in which we discard both the decoder module and the subsequent reconstruction loss. In doing so, we still apply random masking (testing two ratios equal to 30% and 60%) and linear probing at the end of each task."}, {"title": "Masking strategy and positioning", "content": "Our approach adopts a masking strategy that builds upon random guessing to drop frames, thus following most of the literature dealing with masked autoencoders. Herein, we want to compare our approach with a deterministic strategy, that drops one frame every $k$. We also assess different possible positions to introduce the masking operation. Specifically, post indicates that masking is placed after splitting the sequence into tuples (see Sec. 3.1), as carried out by our approach. Results for the combinations of these two alternatives are reported in Tab. 3: as can be observed, the random strategy with post-hoc masking emerges as the best configuration."}, {"title": "5 Conclusions", "content": "Skeleton-based action recognition is a relevant task in modern human-centric Artificial Intelligence. We addressed such a long-standing computer vision task from the perspective of incremental learning, thus enabling those applications (e.g., sports analysis, rehabilitative healthcare) where the set of actions to be recognized may change over time. Differently from existing proposals dealing with action recognition, our work appoints efficiency as a crucial aspect of an ideal incremental learner.\nOur method, named CHARON, could be considered a step forward, as it achieves state-of-the-art performance with a remarkable reduction of the computational footprint (in terms of both memory and training time). In a few words, these capabilities derive from a proper application of input sub-sampling and random masking. Importantly, our experiments show that the addition of a reconstruction-based auxiliary objective grants further robustness in the presence of higher masking ratios, thus encompassing settings demanding efficiency. In future studies, we are going to deepen the concepts discussed in this paper, to apply our proposal even in the case of extreme masking (e.g., up to 95%)."}]}