{"title": "Mask and Compress: Efficient Skeleton-based Action Recognition in Continual Learning", "authors": ["Matteo Mosconi", "Andriy Sorokin", "Aniello Panariello", "Angelo Porrello", "Jacopo Bonato", "Marco Cotogni", "Luigi Sabetta", "Simone Calderara", "Rita Cucchiara"], "abstract": "The use of skeletal data allows deep learning models to perform action recognition efficiently and effectively. Herein, we believe that exploring this problem within the context of Continual Learning is crucial. While numerous studies focus on skeleton-based action recognition from a traditional offline perspective, only a handful venture into online approaches. In this respect, we introduce CHARON (Continual Human Action Recognition On skeletoNs), which maintains consistent performance while operating within an efficient framework. Through techniques like uniform sampling, interpolation, and a memory-efficient training stage based on masking, we achieve improved recognition accuracy while minimizing computational overhead. Our experiments on Split NTU-60 and the proposed Split NTU-120 datasets demonstrate that CHARON sets a new benchmark in this domain. The code is available at https://github.com/Sperimental3/CHARON.", "sections": [{"title": "1 Introduction", "content": "Human Action Recognition (HAR) has become critical in various domains such as surveillance [27,29], rehabilitative healthcare [51], and sports analysis [23,39]. Early HAR approaches focused on exploiting RGB or gray-scale videos due to their widespread availability. However, recent advancements have explored alternative modalities, including skeletal joints [10,25,51], depth [36], point clouds [15], acceleration [24], and WiFi signals [42]. Among these, skeleton-based action recognition stands out as particularly efficient and concise, especially for actions not involving objects or scene context. Skeleton sequences capture the trajectory of key points (i.e., joints) in the human body (e.g., elbows, knees, wrists) [48]. As joints can be represented by 2D or 3D spatial coordinates,"}, {"title": "2 Related works", "content": "Skeleton-based Action Recognition. In early skeleton-based action recognition works, sequences were treated as time series, thus processed employing Recurrent Neural Networks (RNNs) [11,53,18,8] to capture dynamics over time. These approaches struggled to integrate the spatial context of joints and proved slow and challenging to parallelize. Following works exploited Convolutional Neural Networks (CNNs) [21,20], treating skeletal data in various ways to make them compatible with CNNs; some handle coordinates as image channels [10,25], while others reshape skeletons by combining joints in space and time [20].\nHowever, these models faced a common limitation: they failed to effectively represent the relationships between skeletal joints moving together in time. Graph Convolutional Networks (GCNs) resolve such shortcomings by exploiting nodes (i.e., joints) temporally and spatially [50,12,13,7,40]. Subsequently, the emergence of ViT [9] marked the introduction of transformer-based architectures into computer vision, leading to solutions that integrate self-attention layers into convolutional architectures. One such work, STTFormer [31], divides the sequence in tuples of joints and retains some concepts of CNNs (i.e., pooling aggregation) for in-time features processing. Nonetheless, such an approach under-exploits the sparsity and redundancy of skeletal data. In recent years, masking approaches [47,49] have been employed to take advantage of these char-acteristics for pre-training models. In contrast, our proposal adopts the recon-struction objective even during the optimization of the downstream task. Such a choice brings the benefit of reducing the training requirements of the whole pipeline, avoiding the pre-training phase."}, {"title": "Continual Learning", "content": "The Continual Learning setting makes a more realistic assumption w.r.t. standard learning paradigms. Specifically, data arrival is con-tinuous and incremental. A subset of CL is Class-Incremental Learning (Class-IL) [44], where the dataset is re-arranged into multiple subsequent tasks, each containing a unique and disjoint set of classes. In this setting, the task identity is not known during inference.\nClassical CL methods employ a regularization term that penalizes the alter-ations of weights to avoid forgetting [22,52,37]. Rehearsal methods [34,32,5,1], on the other hand, employ a limited memory buffer in which they store samples from past tasks and replay them. Another paradigm is represented by dynamic architectures [35,3] in which new network components are instantiated for each incoming task; unfortunately, this often leads to a rapid increase in the number of parameters. This approach has been employed by the authors of Else-Net [26] to tackle skeleton-based HAR in Class-IL. They use the first 50 classes of NTU RGB+D 60 to pre-train their network, and perform incremental training across 10 tasks, each focusing on a different class. We retain that such a benchmark diverges from classical CL ones, as it is simplified and far from real-world sce-narios. In our work, we utilize the same setting presented by the authors of [4], who split NTU RGB+D 60 into 6 tasks, each involving multiple classes."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Preliminaries", "content": "Class-Incremental Learning. In Class-IL, a deep model f(\u00b7; \u03b8) parametrized by \u03b8 is presented with a sequence of tasks \\(T_i\\) with \\(i \\in \\{1, ..., T\\}\\), with T denoting the number of tasks. The i-th task provides \\(N_i\\) data entries \\(\\{(x^{(n)}, y^{(n)})\\}_{n=1}^{N_i} \\) with \\(y^{(n)} \\in Y_i\\); importantly, each task relies on a set of classes disjoint from others s.t. \\(Y_i \\cap Y_j = \\emptyset \\Leftrightarrow i \\neq j\\). The objective of Class-IL is to minimize the empirical risk over all tasks:\n\\[\\mathcal{L}_{Class-IL} = \\sum_{i=1}^{T} \\mathbb{E}_{(x,y) \\sim T_i} [\\mathcal{L}(f(x; \\theta), y)],\\]\nwhere \\(\\mathcal{L}\\) is the loss function (e.g., the cross entropy for classification) and y is the ground truth label. Since the model observes one task at a time, tailored strate-gies are required to prevent catastrophic forgetting. Specifically, some rehearsal approaches [5,33] employ an additional regularization term \\(\\mathcal{L}_M\\) exploiting sam-ples stored in the memory buffer. The objective at the current task \\(T_e\\) is:\n\\[\\mathcal{L}_{Class-IL} = \\mathbb{E}_{(x,y) \\sim T_e} [\\mathcal{L}(f(x; \\theta), y)] + \\mathcal{L}_M.\\]\nSpatio-Temporal Tuples Transformer (STTFormer). We adopt as main backbone of our architecture STTFormer [31], a transformer-based model de-signed for skeleton-based action recognition. It exploits self-attention to capture"}, {"title": "3.2 CHARON", "content": "In this section, we present CHARON, which encompasses three components: i) a technique to populate the memory buffer, employing linear interpolation to decompress memory samples; ii) an efficient training phase with masked inputs; iii) a linear probing stage, which refines the classifier and updates the logits stored in the memory buffer. We depict these elements in Fig. 1.\nEfficient buffer. A raw skeleton sequence \\(x \\in \\mathbb{R}^{C \\times F \\times V}\\) collects the C coordinates (e.g., xyz in Split NTU-60 and Split NTU-120) of V joints at F time instants. Unlike RGB video frames, skeletal data inherently reside in Euclidean"}, {"title": "4 Experimental analysis", "content": ""}, {"title": "4.1 Datasets", "content": "Split NTU-60 and Split NTU-120. NTU is one of the most popular bench-marks for action recognition on skeletal data. Initially comprising 60 classes and 56578 samples in its original version [38], and later expanded to 120 classes and 113945 samples [28], this dataset encompasses a diverse range of actions in-volving up to two individuals. The data collection process involves three Kinect cameras [54], positioned with different angles w.r.t. the subject. They provide RGB videos, IR videos, depth map sequences, and 3D skeletal data. Participants of various ages have contributed to the datasets construction, ensuring its broad applicability and relevance.\nWe adopt the extraction process employed by [31]. As original raw sequences contain a varying number of frames, we apply bilinear interpolation to obtain fixed-length sequences x (i.e., 120 frames) s.t. \\(x \\in \\mathbb{R}^{(C=3) \\times (F=120) \\times (V=25) \\times (B=2)}\\). The axis identified by B regards the poses of the potentially two subjects involved in the action."}, {"title": "4.2 Implementation details", "content": "The custom version we adopt for STTFormer [31] reduces the width of interme-diate layers to obtain a more lightweight model. We set the number of frames in each tuple n = 6 as in the original paper. Following the asymmetric design proposed in [17], we employ 8 layers for the encoder and 3 for the decoder. We refer the reader to the supplementary material for further details. Additionally, we employ an \u03b1 of 0.3 and a \u03b2 of 0.8 for Eqs. (7) and (8), while we use a \u03b3 of 0.5 in approaches using the reconstruction regularization (Eqs. (4) to (6)). We adopt a batch size of 16 for all our experiments with a vanilla SGD opti-mizer and a learning rate of 0.05. Each task of the incremental setting lasts for 30 epochs. With the same hyperparameters as above, we perform 3 epochs for the linear probing phase. Finally, concerning data augmentation, we follow the original STTFormer implementation, applying a simple random rotation to each input sample."}, {"title": "4.3 Results", "content": "For the experimental comparison, we indicate with Joint Training (JT) the up-per bound of our approach. It consists of training the model on the unified dataset (i.e., without splitting it into tasks). For the lower bound, we adopt an incremental training approach that does not employ tailored techniques against catastrophic forgetting. We refer to it as Fine Tuning (FT).\nIn Tab. 1 we report the results for buffer sizes \\(M_{size}\\) of dimensions 500 and 2000. Following other works [5,26,4], we measure the recognition performance in terms of Final Average Accuracy (FAA), defined as:\n\\[FAA = \\frac{1}{T} \\sum_{i=1}^{T} a_{T_i}\\]\nwhere \\(a_{T_i}\\) is the accuracy of the i-th task after the model has seen all T of them. Additionally, we repeat each experiment three times, thus reporting the mean and standard deviation of the FAA."}, {"title": "4.4 Ablations", "content": "We herein report the ablative studies; all the experiments are performed on the XView modality of Split NTU-60.\nOn the importance of the reconstruction-based objective. Our approach not only seeks good classification capabilities but also devises an auxiliary re-construction term targeting the entire input sequence. To shed further light on the effects of such an auxiliary objective, we provide an ablative experiment in which we discard both the decoder module and the subsequent reconstruction loss. In doing so, we still apply random masking (testing two ratios equal to 30% and 60%) and linear probing at the end of each task.\nThe results of these ablative studies are reported in Tab. 2: remarkably, CHARON experiences a significant performance drop when removing the de-"}, {"title": "5 Conclusions", "content": "Skeleton-based action recognition is a relevant task in modern human-centric Artificial Intelligence. We addressed such a long-standing computer vision task from the perspective of incremental learning, thus enabling those applications (e.g., sports analysis, rehabilitative healthcare) where the set of actions to be recognized may change over time. Differently from existing proposals dealing with action recognition, our work appoints efficiency as a crucial aspect of an ideal incremental learner.\nOur method, named CHARON, could be considered a step forward, as it achieves state-of-the-art performance with a remarkable reduction of the com-putational footprint (in terms of both memory and training time). In a few words, these capabilities derive from a proper application of input sub-sampling and random masking. Importantly, our experiments show that the addition of a reconstruction-based auxiliary objective grants further robustness in the pres-ence of higher masking ratios, thus encompassing settings demanding efficiency. In future studies, we are going to deepen the concepts discussed in this paper, to apply our proposal even in the case of extreme masking (e.g., up to 95%)."}]}