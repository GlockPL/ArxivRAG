{"title": "SCALING OPTIMAL LR ACROSS TOKEN HORIZON", "authors": ["Johan Bjorck", "Alon Benhaim", "Vishrav Chaudhary", "Furu Wei", "Xia Song"], "abstract": "State-of-the-art LLMs are powered by scaling \u2013 scaling model size, dataset size\nand cluster size. It is economically infeasible to extensively tune hyperparameter\nfor the largest runs. Instead, approximately optimal hyperparameters must be in-\nferred or transferred from smaller experiments. Hyperparameter transfer across\nmodel sizes has been studied in Yang et al. (2022). However, hyperparameter\ntransfer across dataset size or token horizon - has not been studied yet. To\nremedy this we conduct a large scale empirical study on how optimal learning\nrate (LR) depends on token horizon in LLM training. We first demonstrate that\nthe optimal LR changes significantly with token horizon \u2013 longer training neces-\nsitates smaller LR. Secondly we demonstrate that the the optimal LR follows a scal-\ning law, and that the optimal LR for longer horizons can be accurately estimated\nfrom shorter horizons via our scaling laws. We also provide a rule-of-thumb for\ntransferring LR across token horizons with zero overhead over current practices.\nLastly we provide evidence that LLama-1 used too high LR, and estimate the per-\nformance hit from this. We thus argue that hyperparameter transfer across data\nsize is an important and overlooked component of LLM training.", "sections": [{"title": "INTRODUCTION", "content": "State-of-the-art LLMs are scaled in multiple dimensions. The models are becoming increasingly\nlarge, e.g. Grok-1.5 has 314 billion (B) parameters (xAI, 2024). The clusters used to trained them\nare growing in size, e.g. the recently operational Memphis super-cluster contains over 100,000\nH100 GPUs (Alcorn, 2024). Lastly the training datasets are growing, e.g. LLama-3 was trained\non 15 Trillion (T) tokens (Dubey et al., 2024). At these scales it is infeasible to extensively tune\nhyperparameters. Practitioners must instead resort to hyperparameter transfer, a process where\napproximately optimal hyperparameters for large scale experiments are inferred from experiments\nat a smaller scale. The perhaps most famous work on hyperparameter transfer is muP (Yang et al.,\n2022) - a methodology for transferring optimal hyperparameter from a small model to a large model.\nWhile hyperparameter transfer across model size is a well studied problem, transfer across dataset\nsize remains unstudied. This paper aims to remedy this shortcoming in the literature."}, {"title": "BACKGROUND", "content": "LLM Scaling Laws. LLMs are typically decoder-only transformers Vaswani (2017) trained via\nnext token prediction on web-scale text Radford et al. (2019). It has been empirically observed that\nthe performance of LLMs scale well with model size \u2013 with the largest model showing emergent\ncapabilities Wei et al. (2022). Kaplan et al. (2020) shows that LLM performance roughly scales as a\npower-law in the model size N. Performance here is measured by validation loss L, which is well\nknown to correlate strongy with downstream metrics. Specifically they propose the following law\n(using constants Ne, \u03b1N) for models trained on sufficiently large datasets:\n$L(N) = (Nc/N)\u221dN$ \nHoffmann et al. (2022) also showed that the performance scales well with dataset size and that the\noptimal performance for a given FLOPs budget is obtained by scaling model and dataset size jointly.\nThe current paradigm thus scales data in addition to model size \u2013 e.g. Llama-1 (Touvron et al., 2023)\nwas trained on 1.4 trillion tokens while LLama-3 used 15 trillion tokens (Dubey et al., 2024). In this\npaper we use notation from Kaplan et al. (2020), denoting the dataset size (i.e. number of tokens)\nby D and the model size (i.e. parameter count) by N. As is common in the literature, we will fit\nscaling laws to empirical observations. To measure goodness-of-fit we will use the R2 measure\nvalue ranges from 0 to 1, where 1 indicates a perfect fit and 0 indicates no fit at all.\nits\nHyperparameter transfer. Hyperparameters can strongly influence the performance of LLMs, and\nLR is a critical hyperparameter. The muP paper Yang et al. (2022) first popularized hyperparameter\ntransfer - the process of finding optimal hyperparameters from a small proxy experiment. Core\nto muP is the muP-parametrization a way of parametrizing an LLM such that the learning rate\nwhich is optimal for a small model is also optimal for a larger model. The muP parametrization\nintroduces some changes to the network, e.g. the attention scaling factor is changed from 1/\u221ada to\u221ada\nThe original muP paper shows that LR and many other hyperparameters transfer from small models\nto large models under muP parametrization. This is further corroborated in Lingle (2024)."}, {"title": "EXPERIMENTS", "content": "Experimental Setup. Our setup follows standard training of decoder-only transformer LLMs. We\nuse the Megatron codebase Shoeybi et al. (2019), the model sizes of Table 7, and hyperparamaters\nroughly following GPT-3 Brown (2020) \u2013 weight decay of 0.1, gradient clipping of 1.0, and cosine\nlearning decay schedule. The full list of hyperparameters can be viewed in Table 6 in Appendix A.\nWe make three adjustments compared the GPT-3, mainly to prevent divergence of the model for\nhigh learning rate. 1) The GPT-3 paper does not specify the batch-size schedule, and thus not the\ncomplete warm-up schedule. Given this we use the maxima of 1% of the training steps following"}, {"title": "INITIAL ABLATIONS", "content": "In our first experiment we consider the 350m model from Table 7. We perform an ablation\nstudy where we vary the LR and token horizon and measure the final validation loss. We\nconsider {25, 50, 100, 200, 400} billion tokens. We will start with the base LR of 3 \u00d7 10-4\nfrom Table 7 and then multiply it by factors {0.25, 0.5, 1, 2, 4}. The LRs we consider are thus\n{7.5\u00d710-5, 1.5\u00d710-4, 3\u00d710-4,6\u00d710-4,1.2\u00d710-3}. For each combination of LR and token horizon\nwe train a model and record its final validation loss. To make sure we have the best possible reso-\nlution around the minima we find the minimizer LR* among the LR multipliers {0.25, 0.5, 1, 2, 4},\nand then further add the LRs halfway between the minimizer and its two nearest neighbor \u2013 a proce-\ndure we repeat twice. From these losses we fit a curve which estimates how the final validation loss\ndepends on the LR. For each token horizon we fit a second degree polynomial in log(LR) which\nprovides an excellent fit. The R\u00b2 of the fit are 0.995 or better, see Table 9 in the Appendix for exact\nvalues. The validation losses and fitted curve are shown in Figure 2. We also estimate the optimal\nLR by taking the minimizer of the fitted curve. From Figure 2 we make two observations: 1) the op-\ntimal LR decreases with longer token horizons, 2) the quadratic fit provides excellent agreement\nwith experimental data."}, {"title": "SCALING LAWS", "content": "We have seen that longer token horizons require smaller LR. We now investigate if this insight\nallows us to do hyperaprameter-transfer \u2013 i.e. finding the optimal LR to a long token horizon from\nexperiments on a shorter horizon. To do this we will fit scaling laws to our empirical investigations.\nGiven some fixed model architecture and training recipe, let LR*(D) denote the optimal LR for\nsome token horizon D. Following Kaplan et al. (2020) we will use the following functional form\n$LR* (D) = BD^{-B}$ \nHere B and B are two constants which might e.g. depend on the model architecture. Taking the\nlogarithm of both sides of Equation (3) we get\n$log [LR* (D)] = log B \u2013 B log D$\nWe thus have a linear equation in the unknowns log B, B, and can fit the unknown variables with\nleast squares from our empirical data. Fitting this constant with least squares give reasonable fits\nthe curves for four models are shown in Figure 4. The R\u00b2 of these fits are in the range 0.99-0.96\n(see Table 8 in Appendix A for exact values), a rather high number considering the measurement\nvariance of Section 4.4. In Figure 12 in the Appendix we also show the fits for the 1.3B and 2.7B\nmodels, but these use shorter token horizons.\nTo evaluate the scaling laws we cannot solely rely on R2 as that is essentially a measure of training\nloss. We instead need to evaluate the the fit on some held-out data. To simulate hyperparameter"}, {"title": "ADDITIONAL EXPERIMENTS", "content": "MUP PARAMETRIZATION\nWhile we have demonstrated that the optimal LR decreases with token horizons it is natural to ask\ndoes muP alleviate this issue? I.e. Does muP transfer across token horizons? We here investigate\nif that is the case. To do this we consider the 125m model from Table 7, and use muP parameteri-"}, {"title": "JOINTLY FITTING TOKEN HORIZON AND MODEL SIZE", "content": "In section 3.2 we saw empirically that for a given N and constants B, \u1e9e that may be dependent on\n\u039d,\n$LR*(N, D) = B(N)D^{-B(N)}$\nAs seen in Figure 6b, we observe a linear relationship between log N and LR* for any given D.\nThis suggests the relationship:\n$LR*(N, D) = A(D)N^{-a(D)}$\nThe constants A, a may depend on D.\nThe key question that arises is how the general notion of model size N can be incorporated into\nthe joint scaling law. Moreover, the scaling law formula from Eq. 5 for constant D has to be\nrepresentable by Eq. 4. It is anticipated to align with the latter, consisting of distinct power laws,\neach with specific parameters for different N and D values. Consequently, the objective is to identify\na function that fulfills these criteria\n$LR*(N, D) = B(N)D^{-B(N)} = A(D)N^{-a(D)}$\nDataset size D for different Model Size N. As seen in Figure 6a, since the lines are parallel for\nany given N, the slope \u1e9e (Eq. 4) is independent of the model size N. Therefore we can assume that\n\u03b2(\u039d) = \u03b2.\nModel size N for different Dataset size D. As seen in Figure 6b, since the lines are parallel for\nany given D, the slope a (Eq. 5) is independent of the dataset size D. Therefore we can assume that\na(D) = a is constant.\nUsing the fact that a, \u03b2 are constant and multiplying Eq. 6 by N\u00ba D\u1e9e:\n$B(N)N^\u03b1 = A(D)D^\u03b2$"}, {"title": "A CASE-STUDY ON LLAMA", "content": "We now consider evaluating if the LRs used by Llama-1 Touvron et al. (2023) are \"correct\" accord-\ning to our scaling laws. To do this we adopt the LLama-1 architecture (RMSnorm, Rope embed-"}, {"title": "UNCERTAINTY QUANTIFICATION", "content": "To ensure that our results are reliable we here consider quantifying the variance of our experiments.\nWe will use two different methodologies. First, following Hoffmann et al. (2022) we will consider\nthe bootstrap method. We consider the 350m model of Table 7, randomly remove 20% of the\ndatapoints and then fit the optimal LR at each token horizon and the scaling law of Equation (3).\nWe repeat this procedure 10 times and then measure the mean and std of the optimal LR and the\nconstants of Equation (3). The results are given in Table 4. We see that the standard deviations are\nrelatively small.\nThe second methodology we use is simply to rerun a small-scale experiment with multiple seeds.\nWe use the 350m model from Table 7, a 100B token horizon, three learning rates and two seeds. We"}, {"title": "EFFECT OF BATCH SIZE", "content": "It is well known that batch size B affects the optimal LR (Goyal, 2017). While we we primarily\nfocus on the setting of a fixed batch size, we here consider modifying the batch size for the 1.3B\nmodel of Table 7. Specifically, we double the batch size to 1m tokens and train for 25B and 50B\ntokens in total with different LRs. In Figure 10 in the Appendix we show the final validation loss as\na function of the LR, and estimate the optimal LR. In Figure 9 we show the optimal LR as a function\ntokens for the 1.3B model using a batch size of 0.5 or 1m tokens. We see that the optimal LR is\nhigher with larger batch size as expected from Goyal (2017). More importantly we note that the\nlinear fits are roughly parallel. This suggests that the optimal LR depends on token horizon the same\nway irrespective of the batch size, i.e. that we can factorize Equation (3) as LR(B, D) = f(B)D-B."}, {"title": "RELATED WORK", "content": "The study of scaling laws for LLMs originated in Kaplan et al. (2020); Henighan et al. (2020) but is\nstill an active area of research. Scaling laws is researched in the context of mainstream topics such\npost-training Lin et al. (2024); Zhang et al. (2024a); Gao et al. (2023), model architecture Krajewski\net al. (2024); Alabdulmohsin et al. (2024); Frantar et al. (2023) and multi-modality Aghajanyan\net al. (2023); Cherti et al. (2023). Other areas include inference Sardana & Frankle (2023), multi-\nlinguality Fernandes et al. (2023), synthethic data Dohmatob et al. (2024) and model reuse Wang\net al. (2023). There are also more theoretical studies Michaud et al. (2024); Caballero et al. (2022)\nand applications to recommendation systems Zhang et al. (2024b), game-playing Neumann & Gros\n(2022) and hyperparameter optimization Kadra et al. (2024).\nLR has long been known as an important hyperparameter, and hence there is ample work on how to\nselect it. Goyal (2017) demonstrates that LR needs to be increased when the batch size increases.\nLarger models are known to require smaller LR, and e.g. Kaplan et al. (2020) suggests the formula\nLR(N) \u2248 0.003239 \u2013 0.0001395 log(N) for tuning LR as a function of model size. The muP\nparemetrization (Yang et al., 2022) is a principled method for selecting hyperparameters when the"}, {"title": "LIMITATIONS", "content": "To limit the scope and computational requirements of our study we have intentionally focused on\na narrow area scaling token horizons and changing LR with otherwise fixed LLM recipes. With\nthis limited scope there are naturally many limitations to our study. We have only extended the\nscaling laws to roughly 1T tokens. Many SOTA LLMs are trained significantly longer, e.g. Llama-3\nwas trained on 15T tokens (Dubey et al., 2024). Unfortunately, doing ablations experiments at the\n15T tokens horizon is not computationally feasible for us. Another limitation is that we haven't\ndistinguished between repeated and fresh tokens. It is known that one can repeat unique tokens a\ncouple of times with very limited downside (Muennighoff et al., 2024), but exactly how this interacts\nwith optimal LR is left for future studies. Data quality is also known to be very important for LLMs\n(Abdin et al., 2024), and it likely also interacts with the LR. It is well-known that optimal LR depends\non model size, and this study has only scratched the surface of this important topics. Beyond just\nmodel size, model architectural modifications like mixture of experts (Shazeer et al., 2017), different\nattention types (Ainslie et al., 2023), state-space models (Gu & Dao, 2023) could plausibly interact\nwith both the LR and token horizon. There are also multiple hyperparameters which we do not\nstudy, which are known to interact with LR. These include batch size (Goyal, 2017) and weight\ndecays (Bjorck et al., 2021), and their full interaction with LR is not clear from our study. At last,\nwhile We focus on text-only LLMs there many exciting applications are multimodal LLMs which\ncombine images and textual tokens (Huang et al., 2023). How the LR depends on the number of text\ntokens and image tokens is an important question left for future work."}, {"title": "ADVICE FOR PRACTIONERS", "content": "Our experiments show that the optimal LR decreases with token horizon. This necessitates hy-\nperparameter transfer where the optimal LR is found at a shorter horizon and then is modified to be\napproximately optimal at longer horizons. For practioners with ample compute we recommend find-\ning the best LR at multiple short horizons using the quadratic fitting of Section 3.1. Thereafter the\nconstants in Equation (3) can be found, and the optimal LR at longer horizons can be estimated. For\npractitioners with less computational resources who are working on larger models (say >= 350m)\nwe recommend simply using Equation (9) where we have already found \u03b2 = 0.34 to generalize"}, {"title": "CONCLUSIONS", "content": "We have investigated how learning rate (LR) and token horizon interacts when training LLMs. First,\nwe have show that the optimal LR decreases as the token horizon gets longer. This finding is robust\nacross model sizes. Secondly we have shown that the optimal LR follows reliable scaling laws,\nand that fitting these allows for hyperparameter transfer accross token horizons. As a case study\nwe have applied our methods to the training of LLama, and have shown evidence which suggests\nthat Llama was training with a LR which was significantly larger than optimal. We argue that this\ndemonstrates how hyperparameter transfer accross token horizons is an imporant and understudied\naspect of LLM training. There are many unanswered questions left in this direction, and future\nstudies with additional computational resources can study how token horizons and model size jointly\naffects optimal LR."}]}