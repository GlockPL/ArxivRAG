{"title": "Multi-Point Positional Insertion Tuning for Small Object Detection", "authors": ["Kanoko Goto", "Takumi Karasawa", "Takumi Hirose", "Rei Kawakami", "Nakamasa Inoue"], "abstract": "Small object detection aims to localize and classify small objects within images. With recent advances in large-scale vision-language pretraining, finetuning pretrained object detection models has emerged as a promising approach. However, finetuning large models is computationally and memory expensive. To address this issue, this paper introduces multi-point positional insertion (MPI) tuning, a parameter-efficient finetuning (PEFT) method for small object detection. Specifically, MPI incorporates multiple positional embeddings into a frozen pretrained model, enabling the efficient detection of small objects by providing precise positional information to latent features. Through experiments, we demonstrated the effectiveness of the proposed method on the SODA-D dataset. MPI performed comparably to conventional PEFT methods, including CoOp and VPT, while significantly reducing the number of parameters that need to be tuned.", "sections": [{"title": "I. INTRODUCTION", "content": "Object detection has become a crucial component in real-world applications such as autonomous driving and surveillance owing to the remarkable advancements in deep neural networks. To accurately detect small objects within images, various methods have been developed, such as super-resolution methods [1], [2], similarity learning [3]-[5], and context exploration [6]. However, detecting extremely small objects is still challenging, mainly because of the insufficiency of training data, as manual annotation of the bounding boxes for these objects is time-consuming and costly.\nTo address this issue, a recent trend has relied on large-scale pretraining. Specifically, for object detection, some studies have proposed vision-language models pretrained on large-scale datasets, such as GLIP [7], [8] and Grounding DINO [9], [10]. These models are open-set object detectors capable of accepting natural language text or a sequence of object names as inputs. They can also be adapted as closed-set object detectors for detecting objects within a predefined category set by finetuning them on limited labeled datasets [9], [10]. Therefore, they are expected to be effective for small object detection.\nWhen finetuning large models, a primary challenge remains in terms of parameter efficiency, as optimizing a large number of parameters is computationally and memory expensive. To improve parameter efficiency, adapter tuning [11]-[15] and prompt tuning [16]-[21] are known to be effective. These methods insert lightweight learnable modules into a frozen pretrained model, allowing the model to adapt to new tasks with a minimal increase in the number of learnable parameters while avoiding overfitting.\nInspired by these studies, this paper introduces a novel parameter-efficient finetuning (PEFT) method for small object detection. Specifically, we propose multi-point positional insertion (MPI) tuning, which incorporates multiple positional embeddings into a pretrained frozen model, as shown in Figure 1, enabling the efficient detection of small objects by providing precise positional information to latent features. In our experiments, we demonstrated the effectiveness and parameter efficiency of MPI tuning on the SODA-D dataset [22]."}, {"title": "II. RELATED WORK", "content": "Object detection. Over the last decade, numerous object detection models have been proposed [23]\u2013[27]. There have been two major architectures: convolutional architectures, e.g., RetinaNet [23] and Sparse RCNN [26], and transformer-based architectures, e.g., DETR [27] and Deformable DETR [28]. Recently, vision-language pretrained models such as GLIP [7], [8] and Grounding DINO (GDINO) [9], [10] have demonstrated effectiveness in open-set object detection and visual grounding. For small object detection, convolutional architectures, such as CFINet [3] using coarse-to-fine region proposals, remain the primary approach [29]-[32].\nPEFT. Adapter tuning inserts lightweight learnable modules into a frozen pretrained model [11]\u2013[15], [33]. For instance, encoder adapter tuning [15] incorporates small multilayer perceptrons (MLPs) into each encoder layers. Layer adapter tuning [33] inserts small modules between each layer and the downstream head. Prompt-based finetuning has also garnered attention because of its success in the field of natural language processing [16]\u2013[21]. Examples include context optimization (CoOp) [16] for text prompt tuning and visual prompt tuning (VPT) [18]. This study focuses on PEFT for small object detection, where encoding precise spatial positions within images is crucial."}, {"title": "III. METHOD", "content": "This section presents MPI tuning, a PEFT method for small object detection. MPI tuning inserts positional embeddings into multiple points within a frozen pretrained model. This approach provides precise positional information for latent features, enabling efficient adaptation for detecting small objects."}, {"title": "A. Notation and settings", "content": "Object detection aims to localize and classify objects within images. Specifically, the objective is to provide bounding boxes and categories for each object given an input image and predefined object categories. This study discusses the parameter efficiency of finetuning given a pretrained object detector $f$. We assume that $f$ is a deep neural network and involves latent features. Given an input $x$, we denote by $H(x) = \\{h_i(x)\\}_{i=1}^N$ the set of latent features in the neural network, where $N$ is the number of latent features."}, {"title": "B. Multi-point positional insertion tuning", "content": "MPI tuning inserts a multi-head positional (MHP) encoder, which is a lightweight learnable module that incorporates positional information into latent features. The MHP encoder produces $N$ output embeddings $P = \\{p_i\\}_{i=1}^N$, each of which is added to the latent vanilla features $h_i(x)$ as follows:\n$h'_i(x) = h_i(x) + p_i$, (1)\nwhere $h'_i(x)$ denotes the adapted latent features. In the finetuning phase, the adapted features are used instead of the vanilla"}, {"title": "C. Architecture", "content": "Figure 2 shows the architecture of the MHP encoder, which consists of the following three components: 1) sinusoidal positional embeddings, 2) tiny MLPs, and 3) a multi-head mixer."}, {"title": "Sinusoidal positional embeddings", "content": "The input of the MHP encoder is the sinusoidal positional embeddings $e = (e_1, e_2, \u2026, e_L) \\in \\mathbb{R}^{D \\times L}$, defined by\n$e_{l,2k} = sin(\\frac{l}{C^{\\frac{2k}{D}}}),  e_{l,2k+1} = cos(\\frac{l}{C^{\\frac{2k}{D}}}),$ (2)\nwhere $D$ is the dimension, $l = 0, 1, \u2026\u2026\u2026, L \u2212 1$ is the position index, $k = 0, 2,\u2026, D/2 \u2013 1$ is the element index, and $C$ is a constant. We use $D = 64$, $L = 80,000$, and $C = 10,000$ as the default values."}, {"title": "Tiny MLPs", "content": "The sinusoidal positional embeddings are fed into $M$ tiny MLPs. As shown in Figure 2b, each tiny MLP consists of two blocks of a linear layer, LayerNorm [35], and a Swish-Gated Linear Unit (SwiGLU) activation [36]. Both linear layers maintain dimension $D = 64$. This produces output embeddings $\\bar{e}^{(j)} = (\\bar{e}_1^{(j)},\\bar{e}_2^{(j)},...,\\bar{e}_L^{(j)}) \\in \\mathbb{R}^{D \\times L}$ for $j = 1,2,..., M$."}, {"title": "Multi-head mixer", "content": "Finally, the multi-head mixer produces the embeddings $P = \\{p_i\\}_{i=1}^N$ used in Eq. (1) from the embeddings $E = \\{\\bar{e}^{(j)}\\}_{j=1}^M$ obtained from the tiny MLPs. When $M = N$, we can straightforwardly map each embedding $\\bar{e}^{(j)}$ to its corresponding $p_i$ using a one-to-one correspondence, such that $p_i = g_i(\\bar{e}^{(j)})$, where $g_i$ is a simple transformation function such as a linear function. However, for parameter efficiency, reducing $M$ such that $M < N$ is beneficial. To this end, the multi-head mixer generates $N$ embeddings through a linear combination of the embeddings in $E$. Specifically, it generates $p_i$ as follows:\n$p_i = g_i(\\sum_{j=1}^M A_{ij} \\bar{e}^{(j)})$, (3)"}, {"title": "D. Application to Grounding DINO", "content": "This subsection describes the application of MPI tuning to GDINO [9], [10], which is the model used in our experiments. Figure 3 shows the architecture of GDINO, which consists of the following five components: a BERT (text encoder) [37], a Swin transformer (image encoder) [38], a feature enhancer, a query selector, and a decoder. Because inserting positional information into all latent features can be redundant due to the complexity of this architecture, we selected $N = 26$ points. These are highlighted in green colors in Figure 3.\nBERT and Swin. The first two points correspond to the outputs of the BERT and Swin transformer (Figure 3a). They help learn the positions of the raw input data.\nFeature enhancer. Each feature enhancer block has two points, one after the self-attention module and the other after the deformable self-attention module, as shown in Figure 3b. This results in twelve points because GDINO has six feature enhancer blocks.\nDecoder. Each decoder block has two points for the cross-attention module, as shown in Figure 3c. This results in twelve points because GDINO has six decoder blocks."}, {"title": "E. Loss function", "content": "The finetuning loss function is the sum of the localization and counteractive losses as in [9], [27]. The detection prompt [7]-[9] that concatenates object category names is used as the text input. We use the implementation provided with MM-GDINO [10]."}, {"title": "IV. EXPERIMENTS", "content": "Experimental settings\nDatasets. The SODA-D dataset [22] was used for finetuning and evaluation. It consists of 24,704 high-quality and high-resolution images of street scenes, along with 277,596 bounding box annotations for small objects across nine object categories. The official training and test splits were used. Parameter efficient finetuning experiments were conducted using the MM-Grounding DINO [10] model, which is pretrained on the union of the following four datasets: O365 [39], GoldG [40], GRIT [41] and V3Det [42].\nEvaluation metrics. The mean average precision (mAP) computed across multiple intersection over union (IoU) thresholds from 0.50 to 0.95 with an interval of 0.05 was used as a primary evaluation metric, reported along with mAPs at IoU thresholds of 0.50 and 0.75 referred to as mAP50 and mAP75, respectively. We also reported mAPs for extremely small objects (mAPes), relatively small objects (mAPrs), generally small objects (mAPgs), and normal objects (mAPN). To evaluate the parameter efficiency, the number of learnable parameters (#Params) was reported.\nBaselines. We selected four baselines: zero-shot detection, CoOp [16], VPT [18] and adapter tuning [15]. The zero-shot"}, {"title": "B. Experimental results", "content": "Main results. Table I compares MPI tuning with the conventional PEFT methods. As shown, it achieved results comparable to CoOp and VPT using a learnable decoder head, while reducing the number of parameters to 0.50 million. Compared with the zero-shot baseline, the detection performance was significantly improved, highlighting the effectiveness and parameter efficiency of MPI tuning. Compared with the full training reported as a reference, there is still room for performance improvement. Full finetuning of GDINO performed better than CFINet [3], which is a convolutional neural network designed for small object detection; however, it is parameter inefficient. To achieve the accuracy of these methods with better parameter efficiency, modules that further enhance the learning efficiency and effectiveness are required in future studies.\nAblation study. Table II summarizes the results of an ablation study on the incorporation of the positional information. As shown, incorporating positional information into the feature enhancer was the most effective. This is because, with GDINO, the fusion of text and image features is the most important process. By inserting a learnable module at this stage, the model can be adapted efficiently to small object detection.\nNumber of tiny MLPs. Table III summarizes the results of a hyperparameter study in which the number of tiny MLPS"}, {"title": "V. CONCLUSION", "content": "We proposed MPI tuning, a novel PEFT method for small object detection. The MHP encoder was introduced to incorporate positional information into the latent features in a frozen pretrained model. In experiments, MPI tuning was applied to GDINO. Its effectiveness was demonstrated on the SODA-D dataset in comparison with conventional PEFT methods."}]}