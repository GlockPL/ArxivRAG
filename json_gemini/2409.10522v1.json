{"title": "Bridging User Dynamics: Transforming Sequential Recommendations with Schr\u00f6dinger Bridge and Diffusion Models", "authors": ["Wenjia Xie", "Rui Zhou", "Hao Wang", "Tingjia Shen", "Enhong Chen"], "abstract": "Sequential recommendation has attracted increasing attention due to its ability to accurately capture the dynamic changes in user interests. We have noticed that generative models, especially diffusion models, which have achieved significant results in fields like image and audio, hold considerable promise in the field of sequential recommendation. However, existing sequential recommendation methods based on diffusion models are constrained by a prior distribution limited to Gaussian distribution, hindering the possibility of introducing user-specific information for each recommendation and leading to information loss. To address these issues, we introduce the Schr\u00f6dinger Bridge into diffusion-based sequential recommendation models, creating the SdifRec model. This allows us to replace the Gaussian prior of the diffusion model with the user's current state, directly modeling the process from a user's current state to the target recommendation. Additionally, to better utilize collaborative information in recommendations, we propose an extended version of SdifRec called con-SdifRec, which utilizes user clustering information as a guiding condition to further enhance the posterior distribution. Finally, extensive experiments on multiple public benchmark datasets have demonstrated the effectiveness of SdifRec and con-SdifRec through comparison with several state-of-the-art methods. Further in-depth analysis has validated their efficiency and robustness.", "sections": [{"title": "1 Introduction", "content": "In recent years, due to the outstanding performance and significant business value, sequential recommendation (SR) has attracted increasing attention [6, 19, 49, 74]. Distinct from the traditional collaborative filtering or certain graph-based approaches, SR systems underscore the dynamic behaviors inherent to users themselves, rather than depending solely on structured data [8, 65]. This confers enhanced personalization and its ability to more precisely track the shifts in users' interests and demands. Prominent deep learning-based SR models utilize the CNN, RNN, and GNN architecture to model users' preferences from historical interaction records, such as Caser [60], GRU4Rec [23], and SR-GNN [69]. After that, SASRec [29] has been a pioneering work that introduces Transformer [62] into SR to capture dependencies with powerful modeling capability. BERT4Rec [59] further adopts BERT architecture [12] and utilizes a masked language model to predict the target item.\nWith the rapid development of generative models, some studies have applied them to SR and achieved significant improvements. For example, SVAE [51] effectively models the probability distribution of the most likely future preferences by combining variational autoencoders (VAE) [32] and GRU [9]. MFGAN [48] decouples factors in SR based on the Generative Adversarial Network (GAN) [16] and trains the model using policy gradients. However, these methods are constrained by the expressive power and generative quality of VAE and GAN themselves [1, 53] and face the issue of posterior collapse [44], where the generated hidden representations often lack critical information about user preferences. As a result, we have turned our attention to a new paradigm of generative models, diffusion models [24], which have recently achieved exciting achievements in fields such as image and text generation [13, 39].\nThere have already been a few works based on diffusion models in SR, and they have achieved satisfactory results, such as DiffRec [66] and Diffurec [41]. These methods follow the principles of diffusion models, initially perturbing the embedding of the target item through a forward diffusion process into a known prior distribution, that is Gaussian noise. Subsequently, they restore the Gaussian distribution iteratively through a reverse denoising process, also referred to as the sampling phase, to recover meaningful representations and recommend items that are most similar to it.\nHowever, adhering to the paradigm of diffusion models, the prior distribution of these SR methods based on diffusion models is confined to a Gaussian distribution. Thus they can only utilize historical interactions as conditional information for the model. This constrains the potential of diffusion models, as only target items undergo the diffusion model processing. Additionally, information in SR is often sparse yet crucial [21]; during the process of adding noise to the pure noise state, the information is further compromised, making the model prone to collapse. Therefore, we aim to modify the diffusion model by substituting the Gaussian prior with meaningful historical interaction information, directly modeling the process of user interaction history to target items. We have more clearly illustrated the differences between our motivation and existing diffusion-based methods in the Figure 1.\nConsequently, obtaining the intermediate states required for diffusion models and inferring the sampling function that fits them presents a significant challenge. To address this, we introduce the Schr\u00f6dinger Bridge [37, 54] into diffusion-based sequential recommendation, which considers how to find the transfer path with the minimum required cost given the initial and marginal distributions. On a technical level, the determination of a Schr\u00f6dinger bridge capable of connecting two distributions is intricate. Therefore, we use a tractable Schr\u00f6dinger bridge to simplify the process of establishing the connection and derive the sampling function from it, thus constructing our SdifRec model. Specifically, we first employ a Transformer model to process the historical interaction sequence, obtaining the current state representation of the user, which is considered as the initial distribution. The embedding of the targeted recommended item is regarded as the marginal distribution.\nSubsequently, we introduce a Schr\u00f6dinger Bridge to establish the connection between these distributions, thereby eliminating the necessity of using Gaussian noise as the prior, a common practice in typical diffusion models. Furthermore, we design a connectivity model to reconstruct the representation of the target recommendation item at random moments. During the inference process, we initiate from the user's current state representation rather than Gaussian noise and iteratively apply the well-trained connectivity model to reveal the user's interests in the next moment. Finally, by computing and ranking the similarity between the user's next moment of interest and candidates, we recommend the target item.\nBased on the propsoed SdifRec, the issues posed by prior constraints are effectively resolved. Moreover, we have extended our focus to the respective strengths of SR and graph-based recommendation methods. SR can better model the dynamic evolution of user interests while the latter can more sensitively capture collaborative information between users and items. To combine the advantages of both forms, we propose an enhanced version of SdifRec called con-SdifRec. It utilizes user static representations obtained from pre-trained LightGCN to cluster users and uses the cluster information as conditional guidance for posterior distribution generation.\nIn summary, the main contributions of this paper include:\n\u2022 We are the first to introduce the Schr\u00f6dinger Bridge into diffusion-based SR work, thereby presenting the SdifRec model. It directly models the connection between the user's current state and the target item, rather than relying on the conventional Gaussian distribution prior used in diffusion-based models.\n\u2022 To capitalize on the strengths of both sequential recommendation and graph-based recommendation methods, we propose an extended version of SdifRec, termed con-SdifRec. It effectively utilizes collaborative information as conditional guidance to generate posterior distribution with extra information.\n\u2022 We have conducted extensive experiments on three public benchmark datasets, comparing SdifRec with several state-of-the-art methods. The results have demonstrated significant improvements of SdifRec and con-SdifRec over baselines across various settings, verifying their efficiency and robustness."}, {"title": "2 Related Work", "content": "2.1 Sequential Recommendation\nSR is a technique that suggests the subsequent item of potential interest, based on a user's historical interaction records [18, 20, 55, 68, 75, 76]. This approach was initially implemented using techniques such as Markov Chain and Matrix Factorization [22]. However, with the advent of neural networks, deep learning methods like GRU4Rec [23] have been employed to utilize Gated Recurrent Units (GRUs) [9] to capture sequential dependencies within sequences of user behavior. Caser [60] and NextItNet [77] introduce Convolutional Neural Networks (CNNs) [35] to learn local patterns in user behavior sequences. Graph neural networks (GNNs) have also gained attention for their ability to capture higher-order relationships among items like SR-GNN [69] and GCE-GNN [67]. After Transformer [63, 64] appears, SASRec [29] is a pioneering work that introduces the architecture to the field of SR, becoming a mainstream framework. Additionally, BERT4Rec [59] draws inspiration from the BERT architecture and employs bidirectional encoders to capture bidirectional dependencies in sequences, using a masked language model to predict the user's next action.\nIn recent years, with the development of generative models, an increasing number of studies have begun to apply generative models such as VAE [63] and GAN [10] to the field of SR, resulting in significant progress, such as MVAE [42], ACVAE [70], RecGAN [4], MFGAN [28]. Nevertheless, models grounded in GANs typically necessitate adversarial training between the generator and discriminator. This process can often be unstable, leading to suboptimal performance [3, 47]. Conversely, models founded on VAEs impose stringent assumptions about the posterior, which may constrain the quality of their generated hidden representations [31, 57]. As a result, a few works in SR have turned their attention to the new paradigm of generative models - diffusion models [71]. Among them, DiffuRec [41] and DiffRec [14] directly apply diffusion models to the field of SR. DiffuASR [43] utilizes user preference information as conditional guidance for personalized recommendations. DreamRec [72] employs classifier-free guidance diffusion models to further leverage the conditional information of user preferences. Yet they are all troubled by the limitations imposed by the prior distribution."}, {"title": "2.2 Diffusion Models", "content": "Diffusion models, inspired by non-equilibrium thermodynamics, have been introduced and demonstrate remarkable results in fields such as computer vision [38], sequence modeling [40, 61], and audio processing [5, 33]. Currently, the mainstream diffusion models are mostly variations of the Denoising Diffusion Probabilistic Models (DDPM) by Ho et al [24]. and the Score-Based Generative Model (SGMs) [58] proposed by Song et al. The latter uses Stochastic Differential Equations (SDE) to describe the data generation process, while DDPM can be seen as its discretized version with specific time step values. Given the broader conceptual framework of the SGMs, our subsequent discussions will be based on this form.\nClassifier-guided diffusion is a subsequent work in diffusion models, mainly divided into classifier guided diffusion [13] and classifier-free guided diffusion [25]. The former requires training an additional classifier, and the quality of the classifier greatly affects the quality of the generated results. Classifier-free guided diffusion is an improvement on it, which constructs an implicit classifier gradient by discarding conditional information. This lays the foundation for subsequent work latent diffusion [50], which is a method of conducting diffusion processes in the latent space, thereby significantly reducing computational complexity. The use of classifiers also enables controllable generation [52], as demonstrated by the prominent work GLIDE [17], which creates images based on textual descriptions."}, {"title": "2.3 Schr\u00f6dinger Bridge", "content": "The Schr\u00f6dinger Bridge problem [54] was first proposed in 1931, by Schr\u00f6dinger, which is closely related to optimal control theory in mathematics [36], optimal transport problems [7], and the path integral methods in physics [30]. Researchers like Valentin have applied the Schr\u00f6dinger Bridge to Score-Based Generative Modeling using a method akin to Iterative Proportional Fitting [11]. They iteratively adjusted elements within the joint probability distribution to align with the target marginal distribution. Following this, Shi and others applied this method to path optimization problems [56]."}, {"title": "3 Preliminary", "content": "3.1 Score-Based Generative Model\nIn this section, we will first introduce a Score-Based Generative Model (SGMs) [58], specifically a diffusion model represented in the form of Stochastic Differential Equations (SDEs). SGMs model the forward diffusion process using the stochastic differential equation:\n$$dx = f(x, t)dt + g(t)dw, x_0 := x(0) \\sim p_0 = P_{target}$$\nwhere $t \\in [0, T]$, and $w$ signifies Brownian motion, $P_{target}$ represents target distribution. The function $f(\\cdot, t) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ is a vector-valued function called the drift coefficient of $x(t)$, and $g(\\cdot) : \\mathbb{R} \\rightarrow \\mathbb{R}$ is a scalar function known as the diffusion coefficient of $x(t)$. The functions $f$ and $g$ determine the type of prior distribution $P_{prior}$ to which the forward process will diffuse, and they are typically designed to make the prior distribution a Gaussian distribution. As a remarkable result from Anderson (1982) [2], the reverse of the diffusion process is also a diffusion process, given by the following reverse-time SDE:\n$$dx = [f(x, t) - g(t)^2 \\nabla_x \\log p_t(x)]dt + g(t)d\\bar{w},$$\n$$x_T := x(T) \\sim p_T \\approx P_{prior},$$\nwhere $\\bar{w}$ is a standard Wiener process in reverse time. The term $\\nabla_x \\log p_t(x)$, which represents the score function of the marginal density $p_t$, is the only unknown term in this reverse process. SGMs learns its approximate target $s_\\theta (x(t), t)$ through denoising score matching (DSM) [26], with $s_\\theta$ referred to as the denoising model:\n$$\\theta^* = \\arg \\min_\\theta \\mathbb{E}_{t\\sim U(0,T)}\\lambda(t) \\mathbb{E}_{x(0)}\\mathbb{E}_{x(t)|x(0)} [||s_\\theta (x(t), t) - \\nabla_x \\log p_{0t}(x|x(0))||^2] .$$\nHere, $\\lambda(t)$ is a positive weighting coefficient, $t \\sim U(0, T)$. The joint distribution $p_{0t}(x|x_0)$ is the conditional transition distribution from $x_0$ to $x(t)$, which is determined by the pre-defined forward SDE.\nTo summarize, SGMs first utilize the diffusion process defined in Equation (1) to obtain the distribution $x(t)$ at intermediate time steps. Then, they minimize the loss defined in Equation (3) to train the denoising model $s_\\theta$ and sample iteratively using the formula defined in Equation (2) to obtain the final result."}, {"title": "3.2 Schr\u00f6dinger Bridge Problem", "content": "We aim to obtain the corresponding intermediate state after replacing Gaussian noise with user interaction information as the prior distribution of the SGMs. To achieve this, we introduce the Schr\u00f6dinger Bridge to model the process. The Schr\u00f6dinger Bridge (SB) problem is the optimization of path measures $p_{SB} \\in \\mathbb{P}(\\mathcal{C})$ with constrained boundaries:\n$$p_{SB} = \\arg \\min_{p} {KL(p||q) : P_0 = P_{target}, P_T = P_{prior}},$$\nwhere $q \\in \\mathbb{P}(\\mathcal{C})$ is a reference path measure. The above equation can be understood as finding a stochastic process with the minimum cost under the constraints of given initial and final state distributions. A common approach to solving Equation (4) is the Iterative Proportional Fitting (IPF) [15] method:\n$$P_{2n+1} = \\arg \\min_{p} {KL(p||p_{2n}) : P_T = P_{prior}},$$\n$$P_{2n+2} = \\arg \\min_{p} {KL(P||P_{2n+1}): P_0 = P_{target}},$$"}, {"title": "3.3 Problem Definition", "content": "Let $I$ be the set of discrete items in the dataset, $U$ be the set of users and $V$ be the set of items. For each user $u \\in U, o_{1:n-1} = [o_1, o_2,..., o_{n-1}]$ represents his historical interaction sequence sorted by timestamp. During the training process of a sequential recommender, learning involves maximizing the probability of the target item $o_n$, that is $p (v_n | o_1, o_2,\\cdots, o_{n-1})$. In the inference process, the generative sequential recommender predicts the probability of recommending the next item $v_{n+1}$ based on the entire sequence $[o_1, o_2,..., o_n]$, that is $p (v_{n+1} | o_1, o_2, , o_{T-1}, o_n)$.\nWe utilize item embedding $e_j$ to signify the semantic representation of the latent features encapsulated within item $v_j$. So, $e_{1:n-1}$ corresponds to the embedding of the historical interaction sequence, $e_n$ represents the embedding representation of the target item, which is also considered as the target distribution $x_0$ of our Schr\u00f6dinger bridge. $h_u$ represents the hidden identifier of the current state of user $u$, which is also considered as the initial distribution $x_1$ of our Schr\u00f6dinger bridge, Here let $T = 1$. $x(t)$ Represents the state at time $t$ of the Schr\u00f6dinger bridge connecting $x_0$ and $x_1$. In the following passage, we will use $x_1$ instead of Gaussian noise as the prior distribution of the diffusion model, and model the process from $x_1$ to $x_0$ using the Schr\u00f6dinger Bridge."}, {"title": "4 Methodology of SdifRec", "content": "In this section, we provide a detailed explanation of the proposed SdifRec, includes how to succinctly obtain the Schr\u00f6dinger Bridge connecting the user's historical interactions and the recommended items, as well as how to obtain the corresponding sampling process."}, {"title": "4.1 Build the Schr\u00f6dinger Bridge", "content": "Initially, we input the embedded representation of the interaction sequence $e_{1:n-1} = [e_1, e_2, ..., e_{n-1}]$ into a Transformer architecture similar to SASRec. To ensure its dimensionality matches that of the target item embedding $e_n$ (this is because we will subsequently compute their weighted sum), we select the last output from the Transformer as the current hidden state $h_u$ of the user. This process is represented by the approximator $h_\\theta()$:\n$$h_u = h_\\theta(e_{1:n-1}).$$\nNext, we consider $h_u$ as the initial distribution $x_1$ of the Schr\u00f6dinger bridge, and $e_n$ as the marginal distribution, which is our target distribution $x_0$. How to obtain the intermediate states of the Schr\u00f6dinger bridge from the initial and target distributions remains a challenging task, and this stochastic process needs to satisfy Equation (1). One feasible approach is to utilize the IPF method described in Equation (5), which is a common method for solving the Schr\u00f6dinger Bridge problem. However, it's worth noting that after parameter reparametrization, the traditional diffusion models require only one step to obtain the intermediate state from the initial state [24]. Therefore, employing the IPF method would significantly increase computational costs.\nTo deal with this, we consider the initial and target distributions as Gaussian distributions with specific means and variances and use a tractable Schr\u00f6dinger Bridge model to simplify this problem. Specifically, we assume that the initial state follows the distribution $P_{initial} = \\mathcal{N}(x_1, e^{-2\\int_0^1 f(t) dt} \\sigma_1^2 I)$, and the target state follows the distribution $P_{target} = \\mathcal{N}(x_0, \\epsilon^2 I)$. So, solving the Schr\u00f6dinger Bridge by Equation (5) to satisfy Equation (1) can be represented by the following partial differential equation:\n$$\\frac{\\partial \\Psi}{\\partial t} = -\\nabla_x \\cdot (\\Psi f) + Tr \\left(\\frac{g^2}{2} \\nabla^2 \\Psi\\right)$$\ns.t. $\\Psi_0 \\Psi_0 = P_{target}, \\Psi_T \\Psi_T = P_{initial}$. Here we present a lemma.\nLEMMA 1. The result of Equation (7) can be obtained by:\n$$\\Psi_t = \\mathcal{N}(\\alpha_t a, (\\alpha_t \\sigma_0^2 + \\bar{\\alpha}_t^2 \\sigma_1^2)I),$$\n$$\\bar{\\Psi}_t = \\mathcal{N}(\\bar{\\alpha}_t b, (\\alpha_t \\sigma_0^2 + \\bar{\\alpha}_t^2 \\sigma_1^2)I),$$\nwhere $t \\in [0, 1]$, that is, let $T = 1$, and the values of $a, b, etc.$ can be obtained from the following equation:\n$$a = x_0 + \\frac{\\alpha_1}{\\alpha_t} (x_1 - \\bar{\\alpha}_t x_0),$$\n$$b = x_1 + \\frac{\\bar{\\alpha}_0}{\\bar{\\alpha}_t} (x_0 - \\alpha_t x_1),$$\n$$\\sigma^2 = \\epsilon^2 + \\int_0^t g^2(\\tau) d\\tau,$$\nand\n$$\\alpha_t = e^{-\\int_0^t f(\\tau) d\\tau},$$\n$$\\bar{\\alpha}_t = e^{-\\int_t^1 f(\\tau) d\\tau},$$\n$$\\sigma_t^2 = \\int_0^t e^{-2\\int_{\\tau}^t f(s) ds} g^2(\\tau) d\\tau, \\bar{\\sigma}_t^2 = \\int_t^1 e^{-2\\int_t^{\\tau} f(s) ds} g^2(\\tau) d\\tau.$$\nWhen $\\epsilon \\rightarrow 0$,, converge to: $\\bar{\\Psi}_t = \\mathcal{N} (\\alpha_t x_0, \\alpha_t \\sigma_0^2 I)$, $\\Psi_t = \\mathcal{N} (\\bar{\\alpha}_t x_1, \\alpha_t \\sigma_1^2 I)$.\nPROOF. Due to space limitations, we provide a brief proof here. According to It\u00f4's lemma [27], it can be derived that for the SDE satisfying Equation (1), there is\n$$dx(t) \\overset{x(t) \\sim}{=} \\mathcal{N}\\left(0, \\left(\\int_0^t e^{-\\int_{\\tau}^t f(s) ds} g(\\tau) d\\bar{w}\\right)^2\\right),$$\nwhich leads to the result\n$$\\bar{\\Psi}_{t|0}(x(t)|x_0 = \\mathcal{N}\\left(\\alpha_t x_0, \\alpha_t \\int_0^t \\frac{g(\\tau)^2}{\\alpha_\\tau^2} d\\tau I\\right).$$ \nThen we conclude that $\\bar{\\Psi}_{t|0}(x(t)|x_0 = \\mathcal{N}(\\alpha_t x_0, \\alpha_t \\sigma_0^2 I)$.\nOn the other hand, we can let $s = 1-t$ and conduct similar derivations for $\\Psi$, which finally leads to the result $\\Psi_{t|1}(x(t)|x_1 = \\mathcal{N}(\\bar{\\alpha}_t x_1, \\alpha_t \\sigma_1^2 I)$. Then\n$$P_{data} = \\bar{\\Psi}_0 \\Psi_0 = \\mathcal{N}(x_0, \\epsilon^2 I), P_{prior} = \\bar{\\Psi}_1 \\Psi_1 = \\mathcal{N}(x_1, \\alpha_1 \\epsilon^2 I).$$ \nWe parameterize them as follows:\n$$\\bar{\\Psi}_0 = \\mathcal{N}(a, \\sigma^2 I), \\Psi_1 = \\mathcal{N}(b, \\bar{\\alpha}_1 \\epsilon^2 I).$$ \nSince the conditional transitions $\\bar{\\Psi}_{t|0}, \\Psi_{t|1}$ are known Gaussian, the marginals at any $t \\in [0, 1]$ are also Gaussian :\n$$\\bar{\\Psi}_t = \\mathcal{N}(\\alpha_t a, (\\alpha_t \\sigma^2 + \\bar{\\alpha}_t \\sigma_t^2)I), \\Psi_t = \\mathcal{N}(\\bar{\\alpha}_t b, (\\alpha_t \\sigma_0^2 + \\bar{\\alpha}_t \\sigma_t^2)I).$$ \nThen we can solve the coefficients a, b, $\\sigma$ by boundary conditions."}, {"title": "4.2 Model Training", "content": "In the training phase of the Schr\u00f6dinger Bridge, we build our connectivity model $f_\\theta$ with inputs initial distribution $x_1$, intermediate time distribution $x_t$ and time embedding $t$ to reconstruct the target distribution $x_0$. Since we have already used a transformer model to obtain the user's current state vector $h_u$, we define $f_\\theta$ as a simple MLP. In terms of details, based on our experiments, we have observed that directly providing $x_t$ may lead to the model overly relying on the latent $x_0$ within $x_t$, resulting in suboptimal performance. Therefore, we introduce a parameter $\\alpha$, which follows a normal distribution with mean and variance specified by hyperparameters $\\mu$ and $\\sigma$. We then multiply element-wise between $x_t$ and $\\alpha$ before feeding it into the model. Therefore, the approximate value of $x_0$ obtained using the connectivity model is:\n$$x_0 = f_\\theta (\\alpha x_t, t, x_1) .$$\nIn practice, to allow the model to better learn the importance of time steps, we amplify $t$ by an amplification factor of $\\lambda$, where $\\lambda$ is a hyperparameter. Furthermore, due to better compatibility with cross-entropy loss and its suitability for SR [45]. We have discarded the more commonly used loss function resembling Mean Squared Error (MSE) in the diffusion model. Instead, we have redefined the loss function as follows:\n$$\\mathcal{L}_{CE} = -\\frac{1}{|U|} \\sum_{i\\in U} \\log \\frac{exp(x_0 e_n)}{\\sum_{j\\in U} exp(x_0 e_i)}$$\nThe detailed training process can be found in Algorithm ??. The well-trained connectivity model $f_\\theta$, assists us in generating an approximate value for $x_0$ to be used in the subsequent sampling process during the model inference stage."}, {"title": "4.3 Model Inferencing", "content": "In the inference phase of the Schr\u00f6dinger Bridge, our goal is to iteratively generate the target item embedding from the initial distribution $x_1$ obtained from the historical interaction sequence. The sampling process is performed using the following equation at time $s \\in [0, 1]$:\n$$x_t = \\frac{\\alpha_t \\sigma}{\\alpha_s \\sigma_s} x_s + \\frac{\\alpha_t }{\\sqrt{\\alpha_s + 1}} f_\\theta (x_s, s, x_1)$$\n$$+ \\frac{\\alpha_t \\sigma_s}{\\alpha_s \\sqrt{\\alpha_s + 1}} \\epsilon, \\epsilon \\sim \\mathcal{N}(0, I)$$\n$$x_t = \\frac{\\bar{\\alpha}_t \\bar{\\sigma}_t}{\\bar{\\alpha}_s \\bar{\\sigma}_s} x_s + \\frac{\\bar{\\alpha}_t }{\\sqrt{\\bar{\\alpha}_s + 1}} f_\\theta (x_s, s, x_1)$$\n$$+ \\frac{\\bar{\\alpha}_t \\bar{\\sigma}_s}{\\bar{\\alpha}_s \\sqrt{\\bar{\\alpha}_s + 1}} \\epsilon, \\epsilon \\sim \\mathcal{N}(0, I)$$\nEquations (22) and (23) correspond to the SDE and ODE (similarly proposed in the work of Song [58]) forms of the Schr\u00f6dinger Bridge model, where $t \\in [0, s]$, $f_\\theta$ is the connectivity model introduced in Section 4.3. By selecting the appropriate time steps and iteratively running the above process until $t = 0$, we can obtain the target item embedding $x_0$. In our work, we opted for a relatively simple uniform sampling of time. In fact, there are multiple ways to choose the time schedule, and we leave the exploration of different approaches for future work. Finally, the recommendation list is generated by selecting the K items from the item set $V$ that are closest to the target item embedding $x_0$.\nSo far, we have completed the introduction of the main part of SdifRec. Next, we will present the cluster center guidance paradigm that we have proposed for SdifRec, namely con-SdifRec, which is an effective improvement."}, {"title": "5 Method of Condition-guiding", "content": "In the domain of SR, user's historical interaction sequences serve as inputs to model various user dynamic behaviors. In contrast, some graph-based recommendation methods excel at extracting collaborative information between users and items. Consequently, in this section, our proposed method, con-SdifRec, inspired by classifier-free guided diffusion, tends to cluster collaborative user information and integrate it as a guiding condition for the sampling process, aiming to constraint the user with Group homogeneity, thus harnessing both of these information types simultaneously."}, {"title": "5.1 Introducing Conditional Information", "content": "To gain collaborative user information, we start by using a pre-trained LightGCN to obtain static user representations $U_1, U_2, ..., U_{|U|}$. Subsequently, we further enhance the quality of collaborative information through clustering. Specifically, we initialize $k$ cluster centers {$Z_1, Z_2, ..., Z_k$}, which will be jointly optimized during the training process. We calculate the cosine similarity $cosine(u_i, z_j) = \\frac{u_i z_j}{(||u_i||_2 ||z_j||_2)}$ between users and each cluster center to assign user clustering information. which is represented as a one-hot encoding $c_i$, i.e.\n$$c_i [j] = \\begin{cases} 1 & \\text{if } j = \\arg \\underset{l\\in \\{1,2,...,k\\}} {\\text{max }} cosine(u_i, z_l), \\\\ 0 & \\text{otherwise}. \\end{cases}$$\nSubsequently, we provide $c_i$ as conditional guidance information to con-SdifRec, enabling the computation of the user's current state $h_u^{con}$ enriched with collaborative information. We need to modify the approximate function $h_\\theta$ mentioned in Section 4.2 so that it can receive conditional information. To achieve this, we pass the conditional vector through an MLP layer to obtain scaling vector $\\beta$ and bias vector $\\gamma$, them input $\\beta \\odot e_{1:n-1} + \\gamma$ into the transformer, and to differentiate, we designate the approximate function capable of receiving conditional information as $h_\\theta$:\n$$h_u^{con} = h_\\theta(e_{1:n-1}, c_i)$$\n$$=Transformer(\\beta \\odot e_1 + \\gamma, \\beta \\odot e_2 + \\gamma, . . ., \\beta \\odot e_{n-1} + \\gamma).$$\nWe also require an additional unconditional model:\n$$h_u^{uncon} = h_\\theta (e_{1:n-1}, 0) = Transformer(e_1, e_2, ..., e_{n-1}).$$ \nThe remaining forward process remains the same as SdifRec, where we still use $h_u^{con}$ or $h_u^{uncon}$ as $x_1$, which serves as the input for the connectivity model $f_\\theta$ to obtain the target distribution with conditional information 0."}, {"title": "5.2 Joint Training and Conditional Sampling", "content": "During the training phase, we have referenced the form of classifier-free guided diffusion and jointly trained models with and without conditional information. Specifically, we use $h_u^{uncon}$ without conditional information as the input for the connectivity model $f_\\theta$ with a probability of $p$. Conversely, with a probability of 1-$p$, we utilized $h_u^{con}$, which incorporates conditional information, as input for the connectivity model.\nDuring the sampling phase, we use the hyperparameter $\\omega$ to control the strength of the influence of guidance signal $c_i$, and replace $f_\\theta (x_s, s, x_1 = h_u)$ with\n$$f_\\theta (x_s, s, h_u^{con}, h_u^{uncon}) = (1+ \\omega)f_\\theta (x_s,s, h_u^{con}) - \\omega f_\\theta (x_s,s, h_u^{uncon})$$\nto complete Equation (18) or (19)'s sampling process. The rest remains the same as SdifRec, and we still obtain the final target embeddings through iterative sampling. Overall, we illustrated our SdifRec and con-SdifRec in Figure 2.\nIt is worth noting that here we provide the form of conditional guidance, and the available conditions are not limited to using clustering conditions obtained from fixed user representations. Multimodal information such as text embeddings and other side information can also serve as guidance conditions, which we leave for future research on diffusion-based SR."}, {"title": "6 Experiment", "content": "6.1 Experiment Settings\n6.1.1 Datasets. We selected three real-world datasets widely used in the sequential recommendation to evaluate the performance of our SdifRec: Amazon Beauty and Amazon Toys are two subcategories of the Amazon \u00b9 dataset [46], encompassing data collected from May 1996 to July 2014 on the Amazon online store. Yelp 2 [73] is a large-scale social media and business review dataset widely used for research and development."}, {"title": "6.1.2 Baselines", "content": "We compared SdifRec with eight state-of-the-art sequential recommendation methods", "methods": "nThe four conventional sequential methods include:\n\u2022 GRU4REC [23", "Caser[60": "applies CNN with vertical and horizontal convolutional layers to capture long and short-term user preferences.\n\u2022 SASRec[29", "59": "proposes a bidirectional Transformer with a cloze task predicting the masked target items for SR.\nThe four generative sequential methods include:\n\u2022 ACVAE [70", "MFGAN[28": "utilizes multi-factor generative adversarial network(GAN) to consider information from various factors.\n\u2022 DiffuRec[41"}]}