{"title": "INSTRUCT-TUNING PRETRAINED CAUSAL LANGUAGE MODELS\nFOR ANCIENT GREEK PAPYROLOGY AND EPIGRAPHY", "authors": ["Eric Cullhed"], "abstract": "This article presents an experiment in fine-tuning a pretrained causal language model (Meta's Llama\n3.1 8B Instruct) for aiding in three fundamental tasks of philological research: chronological and\ngeographic attribution as well as text restoration in ancient Greek inscriptions and documentary\npapyri. Using a prompt-based instruct approach, the fine-tuned models surpass the state of the art\nin key metrics. For inscriptions, the models achieve a lower average character error rate (CER) of\n22.5% (vs. 26.3%), while closely matching top-1 accuracy (60.9% vs. 61.8%) and top-20 accuracy\n(77.5% vs. 78.3%) for sequences up to 10 characters. They also provide a practical advantage by\nignoring spaces during reconstruction, aligning better with the scriptio continua format of ancient\nwritten artifacts. In geographic attribution, the model outperforms previous benchmarks with a top-1\naccuracy of 75.0% (vs. 70.8%) and a top-3 accuracy of 83.7% (vs. 82.1%). For dating, it achieves\nan average deviation of 26.2 years (vs. 29.3) and a median deviation of 1 year (vs. 3) from the actual\ndate range. The models also set new baselines for documentary papyri, with a CER of 16.3%, a\ntop-1 accuracy of 71.3%, and top-20 of 85.0% in text reconstruction; a top-1 accuracy of 66.4% and\ntop-3 of 79.9% in geographic attribution; and, in chronological attribution, a deviation of 21.7 years\nfrom the actual termini post/ante quem, with a median deviation of 0 years.", "sections": [{"title": "1 Introduction", "content": "A significant number of documentary texts in ancient Greek-such as announcements, laws, receipts, contracts, and\nletters-have survived from the Archaic period through to the early Byzantine era. These texts were inscribed or\nwritten on various materials, often on durable stone blocks or sheets of papyrus, which have survived particularly well\nin drier climates.\nA key task of philological research is to assign dates to these documents and attribute them to their places of origin.\nInsights can typically be drawn from the provenance of the artifact in question, its physical features and writing style,\nbut typically the analysis also relies on clues from the textual content, such as the information conveyed or specific\nlinguistic features. Another important task is the reconstruction of damaged or missing letters in the fragments, which\nare frequently marred by gaps, abrasions, and scribal errors. Grammatical and stylistic criteria, as well as parallels from\nsimilar documents\u2014now readily searchable through databases like www.papyri.infoand inscriptions.packhum.\norg can be invoked to support or challenge specific conjectures or emendations. However, the conception of such\nsuggestions has traditionally relied on the linguistic creativity and inventiveness of individual scholars, and on their\npersonal expertise acquired over many years of reading and working with similar ancient texts. Conjectural \u201chigher\"\ncriticism is performed ope ingenii (\u201cwith the aid of one's ingenuity\"), as textual critics put it. It is a branch of\nphilological research that often seems to align more closely with the realm of art than with that of science, or at least\""}, {"title": "", "content": "with a scientific field where, to borrow a phrase from T.S. Eliot [1, p. 103], \u201cthere is no method except to be very\nintelligent.\"\nThis traditional division within textual criticism-between the objective, technical analysis of manuscript data and the\nsubjective, intuitive insight of an astute critic-is undergoing a significant transformation. Machine learning has the\npotential to contribute across all areas of philology, including those traditionally characterized by interpretation, guess-\nwork, and creativity. Language models can be pretrained on the entire surviving textual record of ancient civilizations\nand fine-tuned to parse, classify, and analyze various aspects of words, phrases, and texts in these languages. They\ncan be applied to explore relationships between manuscripts, identify prosodic, rhetorical, and narrative patterns, de-\ntect intertextual relationships, recognize themes, analyze sentiments, summarize ideas, and translate texts (see review\nby Sommerschield and colleagues [2]). More immediately relevant to our purposes, they can be trained to date and\ngeographically attribute texts, detect errors, generate corrections, and fill in missing words and phrases ([3, 4, 5, 6, 7]).\nIn the field of Greek documentary texts, Assal and colleagues [4] remain the leading pioneers. They trained a trans-\nformer model with a sparse attention mechanism (BigBird), combining word- and character-level tokenization, to date,\nattribute geographically, and reconstruct damaged sequences in ancient Greek inscriptions. On their test set of 7,811\ninscriptions, they reported the following results:"}, {"title": "", "content": "Text Restoration: The model achieved a top-1 accuracy of 61.8% for restoring sequences of 1 to 10 characters,\ncompared to 25.3% for the human experts they recruited. The character error rate (CER) for the model was\n26.3%, while the CER for the philologists was 59.6%. Additionally, 78.3% of the correct cases were found\nwithin the top 20 results.\nGeographical Attribution: The model reached a top-1 accuracy of 70.8%, with the correct location appearing\nwithin the top 3 in 82.1% of cases.\nChronological Attribution: The model's predictions were, on average, 29.3 years away from the actual dating\nspan (termini post and ante quem), with a median error of only 3 years.\nThe modest aim of this experiment was to advance research in machine learning and textual criticism in three respects:\nFirst, the model published by Assael and colleagues [4] counts spaces as characters in the restoration task. For example,\nthe sequence \u03ba\u03b1\u03b9 \u03bf \u03bb\u03bf\u03b3\n\u03bf\u03c2 \u03c4\u03bf\u03bd \u03b8\u03b5\u03bf\u03bd would not be decoded as \u03ba\u03b1\u03b9 \u03bf \u03bb\u03bf\u03b3\u03bf\u03c2 \u03b7\u03bd \u03c0\u03c1\u03bf\u03c2 \u03c4\u03bf\u03bd \u03b8\u03b5\u03bf\u03bd. Instead,\nthe input would need to be \u03ba\u03b1\u03b9 \u03bf \u03bb\u03bf\u03b3\n\u03bf\u03c2 \u03c4\u03bf\u03bd \u03b8\u03b5\u03bf\u03bd, forcing us to guess beforehand that damaged\ncharacters 3 and 6 are empty spaces. In ancient inscriptions and papyri\u2014typically written in scriptio continua or at\nleast without evenly marked spaces\u2014we often know the approximate number of missing letters but not the combined\nnumber of missing characters and word boundaries. This study aims to develop a model that tracks only characters,\nbetter reflecting the real-world situations philologists face when working with damaged textual artifacts.\nSecond, given the rapid advancements in deep learning and Natural Language Processing over the last two years, it\nis worth exploring how newer, larger pretrained causal language models such as Meta's LLaMA, Google's Gemma,\nMicrosoft's Phi, or OpenAI's GPT-40-can assist textual critics. One promising approach could involve enabling\nmasked language modeling in these models and retraining them to fill in missing sequences of text within a bidirec-\ntional context (see the first step of the recipe developed by [8], LLM2Vec). However, in this preliminary study, I have\nopted for a much simpler approach that is immediately applicable and open to experimentation with larger and newer\nmodels in the future: using a straightforward instruct template prompting the model to generate the date, location, or\nreconstruction at the end of a sequence.\nThird, I aimed to expand the application of deep learning for textual criticism of ancient Greek documentary texts to\ninclude papyri in addition to inscriptions. While Pavlopoulos and colleagues [9] developed a regression model that\ndates Greek papyri with an average error of 54 years, there has otherwise been little strictly text-based deep-learning\nresearch on documentary papyri."}, {"title": "2 Method", "content": "Date spans, geographic attributions, and texts for the ancient Greek inscriptions were sourced from the Packard Hu-\nmanities Institute database (inscriptions.packhum.org/)using the text processing pipeline developed and shared\nby Assael and colleagues [4] (github.com/sommerschield/iphi). The texts of Greek documentary papyri were\nobtained from the Duke Databank of Documentary Papyri (DDbDP), with their chronological and geographical meta-\ndata sourced from the Heidelberger Gesamtverzeichnis der griechischen Papyrusurkunden \u00c4gyptens (HGV), both\naccessible via https://papyri.info."}, {"title": "", "content": "To facilitate comparison with the results reported by Assael and colleagues [4], the datasets were cleaned of diacritics.\nCommas were removed and all other punctuation marks converted to a high dot \".\". The data was then split into a train\n(95%) and test sets (5%).\nDates for both papyri and inscriptions were formatted as exact integers corresponding to the midpoint of their\nrespective date spans. For dates with only a terminus post quem, they were set 25 years after the given date,\nwhile those with only a terminus ante quem were set 25 years before. Place name attributions for the papyri\nwere normalized semi-manually due to inconsistencies across database entries. Although more detailed processing\nwould be beneficial in the future, the current approach \u2013 admittedly rough (see https://github.com/ericu9500/\nPapyriAndInscriptions/train_data/data/places_and_dates.tsv) was deemed sufficient for the experi-\nmental proof-of-concept purposes of this study.\nAll text editions were included in two versions: one in which philologists' reconstructions of lost or uncertain charac-\nters were integrated, and another where these characters were replaced with \u201c-\u201d. Individual missing characters were\nrepresented by a single \"-\", while longer stretches of missing text were indicated by 10 consecutive \"-\".\nText masks were generated before tokenization by randomly selecting spans of 3 to 20 characters, ensuring that no\nmore than 50% of a sequence of intact characters was masked. Longer spans of intact characters were favored by\nassigning weights proportional to the square of their lengths, increasing the probability of selecting longer spans for\nmasking. These spans were replaced with a simple placeholder in the format \u201c[6 letters missing]", "prompts": "Date this papyrus fragment/inscription to an exact year!", "Assign this papyrus frag-\nment/inscription to an exact place!\", or \\\"Reconstruct the missing letters in this papyrus fragment/inscription!\". The\nuser input consisted of a version of the text, while the assistant's response provided the corresponding date, place, or\nmasked sequence of characters, including spaces and punctuation marks. The text was tokenized using the model's\nTik Token-based tokenizer.\nThe model, LLaMA 3.1 8B-Instruct [10": "was fine-tuned on 4 A100 GPUs with 40 GB and batch size of 8 or 80 GB\nand batch size of 20 depending on availability at the time, utilizing the toolkit Torchtune (github.com/pytorch/\ntorchtune). In an initial calibration round, exploratory fine-tuning was conducted with three different data combina-\ntions to identify the best approach: one model trained solely on papyri (4 epochs, approximately 80 hours), another\nsolely on inscriptions (4 epochs, approximately 120 hours), and a third on both inscriptions and papyri (3 epochs,\napproximately 132 hours). For each data combination, two versions were trained: one using the entire sequence of\nsystem, user, and assistant messages, and another with the prompt masked out for loss computation.\nSubsequently, the best-performing models were further trained separately for the three tasks until their performance\nplateaued on the test set. During this stage, entries shorter than 75 tokens or longer than 847 tokens were filtered\nout, and data augmentation was applied by randomly altering the order of sentences and adding noise through the\nreplacement of 5%, 10%, 15%, 20%, and 25% of preserved characters with \"-\". Also, masked sequences of 1-20 (and\nnot only 3-20) characters were included. In the final stage, the fine-tuned models were re-merged with the base model\nusing the TIES method developed by Yadav and colleagues [11] with the toolkit Mergekit (github.com/arcee-ai/\nmergekit).\nThe models were tested using the same metrics proposed by Assael and colleagues [4]: CER (Character Error Rate),\ntop-1 and top-20 accuracy for normalized reconstructions (spaces, punctuation marks and numerals ignored), top-1\nand top-3 accuracy for geographical attributions, and average and median deviation from the date ranges provided\nin the database. Entries shorter than 90 characters were excluded. For inscriptions, the test was conducted on 7,811\nsamples for restoration, 2,870 samples for geographical attribution, and 1,856 samples for dating. For papyri, the test\nwas conducted on 7,811 samples for restoration, 1,990 samples for geographical attribution, and 2,295 samples for\ndating. Dates, places, and reconstructions were evaluated based on top-1, top-3, and top-20 rankings, respectively, out\nof 45 beams. For reconstructions, a repetition penalty of 1.2 was applied to encourage varied predictions."}, {"title": "3 Results", "content": null}, {"title": "3.1 Calibration", "content": "For the reconstruction task, training with the prompt masked out was clearly superior, resulting in a difference of 8%\nCER for the inscriptions and 7% for the papyri after 4 epochs, all other factors being equal. The transfer effect of\ntraining the same model on both papyri and inscriptions was slightly negative, with a 5% difference for inscriptions\nand 0.1% difference for papyri after 3 epochs."}, {"title": "", "content": "For the geographical attribution task, training with the prompt masked out was also superior, yielding a 10.8% differ-\nence in top-1 accuracy for inscriptions and a 4.9% difference for papyri after 4 epochs. The transfer effect of training\nthe same model on both papyri and inscriptions was negative, with a significant 39.2% difference for inscriptions and\na smaller 1.8% difference for papyri after 3 epochs.\nFor the dating task, again, training with the prompt masked out led to better performance, with an average difference\nof 16 years for inscriptions and 3 years for papyri after 4 epochs. The transfer effect of training the same model on\nboth papyri and inscriptions was slightly positive for papyri (-2 years) but negative for inscriptions (0.3 years), after 3\nepochs.\nThe top models in this round were as follows:"}, {"title": "", "content": "Reconstruction: Three epochs for inscriptions (CER 28.1%, top-1 accuracy 52.8%, and top-20 accuracy\n68.4%) and four epochs for papyri (CER 21.0%, top-1 accuracy 64.9%, and top-20 accuracy 78.3%).\nGeographical Attribution: Three epochs for inscriptions (top-1 accuracy 71.0%, top-3 accuracy 81.4%) and\nfour epochs for papyri (top-1 accuracy 62.3%, top-3 accuracy 77.9%).\nDating: Three epochs for inscriptions (average distance 40.5 years, median distance 5 years) and three epochs\n(trained on both papyri and inscriptions) for papyri (average distance 22.11 years, median distance 0 years)."}, {"title": "3.1.1 Continued training", "content": "The calibration round provided several key insights: it is beneficial to mask out the prompt; the effects of transfer\nlearning between papyri and inscriptions were slight or negative; chronological and geographical attributions plateaued\nquicker than reconstructions; and there was early deterioration in reconstruction performance, highlighting the need\nfor data augmentation techniques. As a result, three separate models were trained for each task and each corpus.\nDue to slower progress with the inscriptions dataset, characterized by more short entries, texts corresponding to less\nthan 75 or more than 847 tokens were filtered out. Data augmentation techniques, as detailed in the method section\nabove, were applied. Additionally, the decision to mask 3\u201320 characters instead of 1\u201320 resulted in long and rambling\nreconstructions when the test set required the reconstruction of 1\u20132 characters. Therefore, samples with 1\u20132 characters\nmasked were also included in this phase of training.\nFor the reconstruction task, performance plateaued after 6 additional epochs (for a total of 9 epochs) for the inscriptions,\nachieving a CER of 22.3%, top-1 accuracy of 61.0%, and top-20 accuracy of 74.2%. Training on the papyrus dataset\nplateaued after 7 additional epochs (for a total of 11 epochs), with a CER of 16.3%, top-1 accuracy of 69.9%, and\ntop-20 accuracy of 80.7%.\nFor the geographical attribution task, performance plateaued after 4 additional epochs (for a total of 7 epochs) for the\ninscriptions, achieving a top-1 accuracy of 75.0% and top-3 accuracy of 83.0%. Training on the papyrus dataset for\ngeographical attribution plateaued after 5 additional epochs (for a total of 9 epochs), with a top-1 accuracy of 66.4%\nand top-3 accuracy of 79.9%.\nFor the dating task, performance plateaued after 1 additional epoch (for a total of 4 epochs) for the inscriptions, with\nan average deviation of 26.2 years and a median deviation of 1 year from the actual dating span. Training on the\npapyrus dataset for dating also plateaued after 1 additional epoch (for a total of 4 epochs), with an average deviation\nof 21.7 years and a median deviation of 0 years from the actual dating span."}, {"title": "3.1.2 Re-merging", "content": "In the final step, the effects of re-merging the fine-tuned models with the original base model were assessed. The\nTrIm, Elect Sign & Merge method (TIES-Merging), where parameters that changed only slightly during fine-tuning\nare trimmed ([11]), was applied, using the toolkit Mergekit (github.com/arcee-ai/mergekit). This process led to\na deterioration of 10% in geographical attribution accuracy for inscriptions and 5% for papyri, along with an increase\nin the average error for dating by 4 years for inscriptions and 3 years for papyri."}, {"title": "4 Discussion", "content": "Table 1 summarizes the results and allows for a comparison between the models developed in this study and the figures\nreported by Assael and colleagues [4]. Notably, the datasets used for testing Epigr_1_Llama3.1-8b-Instruct were\nsmaller than those employed by Assael and colleagues [4], with 1,856 samples for dating and 2,870 for geographical\nattribution, compared to their dataset of 7,811 samples (though it is possible that not all of their entries in the test set\nincluded chronological or geographical data). A full, comparable replication of their study using a model like LLaMA"}, {"title": "5 Conclusion", "content": "This experiment demonstrated the effectiveness of fine-tuning pretrained causal language models for dating, geo-\ngraphic attribution, and text restoration in ancient Greek inscriptions and documentary papyri using a simple chat\ntemplate. While the results are preliminary, they suggest that this simple approach not only comes close to but, in\nsome respects, surpasses the current state of the art. The simplicity and scalability of this method, particularly the\nease of fine-tuning newer models on the same data without much modification, is a key advantage. There is signifi-\ncant potential for further improvement through more refined data cleaning, data augmentation and masking strategies,\noptimized training parameters, and experimentation with smaller or larger and more advanced base models as they\nbecome available in the future.\nAlthough more specialized models may eventually outperform this simple method, instruct-tuned pretrained causal\nlanguage models can offer an efficient and easily implementable moving baseline. Researchers developing machine\nlearning methods for textual criticism could either aim to surpass this baseline or incorporate it as part of a more com-\nprehensive solution. Rather than viewing different models as being in competition, a more productive approach may"}, {"title": "", "content": "involve combining them. A system that integrates several models, supported by an adjudicator or ranker model, could\npotentially achieve the best results. This collaborative approach, taking advantage of the complementary strengths of\nvarious forms of AI, holds great promise for developing powerful tools to advance philological research."}, {"title": "6 Appendix: The preliminary transcriptions of the two Uppsala papyri used", "content": null}, {"title": "6.1 Pap.Ups. 106", "content": "\u03b5\u03c4\u03bf\u03c5\u03c3 \u03c4\u03b5\u03c4\u03b1\u03c1\u03c4\u03bf\u03c5 \u03b1\u03c5\u03c4\u03bf\u03ba\u03c1\u03b1\u03c4\u03bf\u03c1\u03bf\u03c3 \u03ba\u03b1\u03b9\u03c3\u03b1\u03c1\u03bf\u03c3 \u03bf\u03c5\u03b5\u03c3\u03c0\u03b1\u03c3\u03b9\u03b1\u03bd\u03bf\u03c5 \u03c3\u03b5\u03b2\u03b1\u03c3\u03c4\u03bf\u03c5\n\u03c0\u03b1\u03c5\u03c3\u03b9\u03c1\u03af\u03c9\u03bd \u03b1\u03c0\u03bf\u03bb\u03bb\u03c9\u03bd\u03b9\u03bf\u03c5 \u03c4\u03bf\u03c5 \u03c0\u03b1\u03c5\u03c3\u03b9\u03c1\u03b9\u03c9\u03bd\u03bf\u03c3 \u03bc\u03b7\u03c4\u03c1\u03bf\u03c3\n\u03ba\u03b1\u03b9 \u03bc\u03b5\u03c4\u03b7\u03bb\u03bb\u03b1\u03c7\u03c5\u03b9\u03b1\u03c3 \u03b1\u03c5\u03c4\u03bf\u03c5 \u03b3\u03c5\u03bd\u03b1\u03b9\u03ba\u03bf\u03c3\n\u03c3\u03c5\u03b3\u03c7\u03c9\u03c1\u03b5\u03b9\u03bd \u03b5\u03b9\u03bd\u03b1\u03b9\n\u03c3\u03c5\u03bd\u03b5\u03c3\u03c4\u03b9\u03bd\n\u03bf\u03b9\u03ba\u03b9\u03b1\u03bd\n\u03bf\u03bc\u03bf\u03bb\u03bf\u03b3\u03b5\u03b9\n-\u03c4\u03c9\u03b9 \u03b3\u03b5\u03b3\u03bf\u03bd\u03bf\u03c4\u03b9 \u03b1\u03c5\u03c4\u03c9\u03b9 \u03b5\u03ba \u03c4\u03b7\u03c3 \u03b3\u03b5\u03bd\u03bf\u03bc\u03b5\u03bd\u03b7\u03c3\n\u03b1\u03c0\u03bf \u03c4\u03b7\u03c3 \u03b1\u03c5\u03c4\u03b7\u03c3 \u03c0\u03bf\u03bb\u03b5\u03c9\u03c3 \u03b5\u03bd \u03b1\u03b3\u03c5\u03b9\u03b1\u03b9\n-\u03c3 \u03b1\u03c5\u03c4\u03c9\u03b9 \u03b5\u03be \u03b7\u03c3\n-\u03c4\u03b7\u03c3 \u03b1\u03c5\u03c4\u03b7\u03c3 \u03b3\u03b5\u03bd\u03b5\u03b1\u03c3 \u03c4\u03b7\u03bd \u03c5\u03c0\u03b1\u03c1\u03c7\u03bf\u03c5\u03c3\u03b1\u03bd \u03b1\u03c5\u03c4\u03c9\u03b9\n-\u03ba\u03b1\u1f76 \u03b1\u03b9\u03b8\u03c1\u03b9\u03bf\u03bd \u03ba\u03b1\u03b9 \u03b1\u03c5\u03bb\u03b7 \u03b1\u03c0\u03b5\u03c1 \u03bf \u03c5\u03b9\u03bf\u03c3 \u03b4\u03b9\u03bf\u03ba\u03bf\u03c1\u03bf\u03c3\n-\u03b5\u03b3\u03c1\u03b1\u03c8\u03b5\u03bd \u03c4\u03bf\u03c5 \u03b4 \u03b1\u03c5\u03c4\u03bf\u03c5 \u03b4\u03b9\u03bf\u03c3\u03ba\u03bf\u03c1\u03bf\u03c5 \u03b5\u03af\u03bd\u03b1\u03b9\n\u03ba\u03b1\u03b9 \u03c0\u03c1\u03bf \u03ba\u03b1\u03c4\u03b5\u03bd\u03b3\u03b5\u03b3\u03c5\u03b7\u03c4\u03b1\u03b9 \u03c4\u03b1 \u03b4\u03b9\u03ba\u03b1\u03b9\u03b1\n\u03bd\u03b7\u03c3 \u03ba\u03b1\u03c4\u03b1 \u03c4\u03bf\u03c5\u03c3 \u03c4\u03b7\u03c3 \u03c7\u03c9\u03c1\u03b1\u03c3 \u03bd\u03bf\u03bc\u03bf\u03c5\u03c3\u00b7 \u03b5\u03b1\u03bd \u03b4\u03b5 \u03bc\u03b7\n\u03c5\u03c0 \u03b1\u03c5\u03c4\u03bf\u03c5 \u03c4\u03b7\u03b9 \u03c4\u03bf\u03c5 \u03b4\u03b9\u03bf\u03c3\u03ba\u03bf\u03c1\u03bf\u03c5 \u03c3\u03b7\u03bc\u03b1\u03b9\u03bd\u03bf\u03bc\u03b5\u03bd\u03b7\u03b9\n-\u03b5\u03bd\u03bf\u03b9\u03ba\u03b9\u03c3\u03bc\u03c9\u03b9 \u03c4\u03bf\u03c5 \u03b7\u03bc\u03b9\u03c3\u03bf\u03c5\u03c3 \u03bc\u03b5\u03c1\u03bf\u03c5\u03c3 \u03c4\u03b7\u03c3 \u03c0\u03c1\u03bf\u03ba\u03b5\u03b9\u03bc\u03b5\u03bd\u03b7\u03c3 \u03bf\u03b9\u03ba\u03b9\u03b1\u03c3\n\u03b4\u03b9\u03bf\u03c3\u03ba\u03bf\u03c1\u03bf\u03c3 \u03c4\u03b7\u03bd \u03c4\u03bf\u03cd\u03c4\u03c9\u03bd \u03b1\u03c0\u03bf\u03c7\u03b7\u03bd"}, {"title": "", "content": "-\u03bc\u03b7\u03b4 \u03c5\u03c0\u03b5\u03bd\u03b1\u03bd\u03c4\u03af\u03bf\u03bd \u03c4\u03bf\u03c5\u03c4\u03bf\u03b9\u03c3 \u03b5\u03c0\u03b9\u03c4\u03b5\u03bb\u03b5\u03b9\u03bd \u03bc\u03b7\u03b4\u03b5\n\u03b1\u03bd\u03b1\u03c3\u03ba\u03b5\u03c5\u03b7\u03b9 \u03ba\u03b1\u03c4 \u03b1\u03c5\u03c4\u03b7\u03c3 \u03c4\u03af\u03b8\u03b5\u03c3\u03b8\u03b1\u03b9 \u03bf\u03bc\u03bf\u03bb\u03bf\u03b3\u03af\u03b1\u03bd \u03bc\u03b7\u03b4\u03b5\n\u03b5\u03c0\u03b9\u03c4\u03b5\u03bb\u03b5\u03c3\u03b1\u03b9 \u03b7 \u03c7\u03c9\u03c1\u03b9\u03c3 \u03c4\u03bf\u03c5 \u03ba\u03c5\u03c1\u03b9\u03b1 \u03b5\u03b9\u03bd\u03b1\u03b9 \u03c4\u03b1 \u03b4\u03b9\u03bf\u03bc\u03bf\u03bb\u03bf\u03b3\u03b7\u03bc\u03b5\u03bd\u03b1 \u03c0\u03b1\u03c1\u03b1\u03b2\u03b1\u03b9\u03bd\u03b5\u03b9\u03bd, \u03b5\u03ba\u03c4\u03b5\u03b9\u03bd\u03b5\u03b9\u03bd \u03b4\u03b5 \u03c4\u03bf\u03bd \u03c0\u03b1\u03c1\u03b1\u03b2\u03b7\u03c3\u03bf\u03bc\u03b5\u03bd\u03bf\u03bd \u03c4\u03c9\u03b9\n\u03c5\u03b9\u03c9\u03b9 \u03b4\u03b9\u03bf\u03c3\u03ba\u03bf\u03c1\u03c9\u03b9 \u03b7 \u03c4\u03bf\u03b9\u03c3 \u03c0\u03b1\u03c1 \u03b1\u03c5\u03c4\u03bf\u03c5 \u03ba\u03b1\u03b8 \u03b5\u03ba\u03b1\u03c3\u03c4\u03b7\u03bd \u03b5\u03c6\u03bf\u03b4\u03bf\u03bd \u03c4\u03bf \u03c4\u03b5 \u03b2\u03bb\u03b1\u03b2\u03bf\u03c3 \u03ba\u03b1\u03b9 \u03b5\u03c0\u03b9\u03c4\u03b9\u03bc\u03bf\u03bd \u03b1\u03c1\u03b3\u03c5\u03c1\u03af\u03bf\u03c5 \u03b4\u03c1\u03b1\u03c7\u03bc\u03b1\u03c3 \u039f \u03ba\u03b1\u03b9 \u03b5\u03b9\u03c3 \u03c4\u03bf\n\u03b4\u03b7\u03bc\u03bf\u03c3\u03b9\u03bf\u03bd \u03c4\u03b1\u03c3 \u03b9\u03c3\u03b1\u03c3 \u03ba\u03b1\u03b9 \u03bc\u03b7\u03b8\u03b5\u03bd \u03b7\u03c3\u03c3\u03bf\u03bd\u00b7 \u03b4 - - - - -\u03b9\u03c9\u03bd \u03bf\u03bc\u03bf\u03bb\u03bf\u03b3\u03af\u03b1\u03bd \u03c3\u03c5\u03bd\u03b5\u03c7\u03c9\u03c1\u03b7\u03c3\u03b5\u03bd\u00b7\nI am grateful to Disa Lindholm for her previous transcription of this text."}, {"title": "6.2 Pap.Ups. 18", "content": "\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03b7\u03b9\n\u03b1\u03c0\u03bf \u03c4\u03b7\u03c3\n\u03c9\u03bd \u03bb\u03b1\u03bc\u03c0\u03c1\u03b1\u03c3 \u03c0\u03bf\u03bb\u03b5\u03c9\u03c3 \u03c0\u03b1\u03c1\u03b1 \u03b1 -\n\u03c0\u03b1\u03ba\u03c4\u03bf\u03c5\u03bc\u03b7\u03b9\u03b1\u03c3 \u03b4\u03b9\u03bf\u03bd\u03c5\u03c3\u03bf\u03b4\u03c9\u03c1\u03b1\u03c3\n\u03b5\u03bd\u03b4\u03b5\u03b9\u03be\u03b1\u03c3 \u03c4\u03c9 \u03bc\u03b5\u03b3\u03b1\u03bb\u03b5\u03af\u03c9 \u03b1\u03c5\u03c4\u03bf\u03c5 \u03c9\u03c3 \u03c0\u03c1\u03c9\u03b7\u03bd\n-\u03c3 \u03c0\u03c1\u03b5\u03c3\u03b2\u03c5\u03c4\u03b5\u03c1\u03bf\u03c5-\u03b1\u03b9\u03c4\u03bf\u03c5. \u03c5\u03c0\u03bf \u03b4\u03b5 \u03b5\u03c4\u03b5\u03c1\u03bf\u03c5 \u03b1\u03c5\u03c4\u03bf\u03c5 \u03b1\u03b4\u03b5\u03bb\u03c6\u03bf\u03c5\n\u03b1\u03c0\u03bf \u03c4\u03b7\u03c3 \u03b1\u03c5\u03c4\u03b7\u03c3 \u03c0\u03bf\u03bb\u03b5\u03c9\u03c3. \u03b5\u03bd\u03b5\u03c4\u03c5\u03c7\u03bf\u03bd \u03b5\u03c0\u03b9 \u03c5\u03c0\u03bf\u03bc\u03bd\u03b7\u03bc\u03b1\u03c4\u03c9\u03bd \u03c4\u03c9 \u03ba\u03c5\u03c1\u03af\u03c9 \u03b7\u03bc\u03c9\u03bd \u03c4\u03c9 \u03b4\u03b9\u03b1\u03c3\u03b7\u03bc\u03bf\u03c4\u03b1\u03c4\u03c9\n\u03b7\u03b3\u03b5\u03bc\u03bf\u03bd\u03b9 \u03c3\u03b1\u03c4\u03c1\u03b9\u03c9 \u03b1\u03c1\u03c1\u03b9\u03b1\u03bd\u03c9 \u03b4\u03b9\u03b1 \u03c6\u03b9\u03bb\u03b5\u03bf\u03c5 \u03b1\u03c0\u03bf\u03c3\u03c5\u03c3\u03c4\u03b1\u03b8\u03ad\u03bd\u03c4\u03bf\u03c3 \u03c5\u03c0 \u03b5\u03bc\u03bf\u03c5\n\u03c4\u03b5\u03c4\u03b5\u03bb\u03b5\u03c5\u03c4\u03b7\u03ba\u03bf\u03c4\u03bf\u03c3 \u03c4\u03bf\u03c5 \u03b5\u03bd\u03bf\u03c3 \u03c4\u03c9\u03bd \u03b1\u03c6-\n\u03b7\u03c1\u03c9\u03bd\u03bf\u03c3 \u03c4- -\n- -\u03c4\u03bf\u03c3 \u03b5\u03bd \u03c4\u03c9 \u03b4\u03b5\u03c3\u03bc\u03c9\u03c4\u03b7\u03c1\u03af\u03c9 \u03b1\u03c5\u03c4\u03b7\u03bd- -\u03b7\u03bd-\n\u03ba\u03b1\u03c4\u03bf\u03b9\u03c7\u03bf\u03bc\u03ad\u03bd\u03bf\u03c5 \u03c5\u03b9\u03bf\u03c5 \u03bc\u03bf\u03c5 \u03b7\u03bd\u03c0\u03b5\u03c1 \u03b1\u03c0\u03bf\u03c3\u03c0\u03b1\u03c3\u03b1\u03b9 \u03b5\u03b9\u03b1-\n\u03b5\u03c6\u03c1\u03b1\u03c3-\n-\u03c0\u03c1\u03bf\u03bd\u03bf\u03b9\u03b1\u03c3 \u03c0- - - - - -\u03ba\u03c9 \u03c0\u03c1\u03bf \u03b4\u03b9\u03ba\u03b7\u03c3 \u03c0\u03c1\u03bf\u03c7\u03c9- - - -\u03b1\u03bd\u03b1\u03b3\u03ba\u03b1\u03b9\u03c9\u03c3 \u03c4\u03bf \u03bc\u03b5-\n-\u03c3\u03c6\u03b1\u03bb\u03b5\u03c1\u03b1\u03bd \u03ba\u03b1\u03b8\u03b9\u03c3\u03c4\u03b7 \u03c9-\n\u03bc\u03b5 \u03b5\u03bd-\n-\u03ba\u03b1\u03b9-\n-\u03ba\u03bb\u03b7\u03c1\u03bf\u03bd\u03bf\u03bc\u03b9\u03b1\u03c3 \u03c4\u03bf\u03c5\n\u03ba\u03b1\u03bf\u03c3 \u03b1\u03c4\u03b9\u03bb\u03b9\u03bf\u03c3 \u03b1\u03c1\u03c4\u03b5\u03bc\u03af\u03b4\u03c9\u03c1\u03bf\u03c3\n\u03bd\u03b5\u03b9 \u03c4\u03bf\u03b9\u03bd\u03c5\u03bd- - -\u03b7\u03bd \u03b1\u03c0' \u03b1\u03c5\u03c4\u03bf\u03c5 \u03c4\u03bf \u03bc\u03b5\u03bd \u03b4\u03b7- - -\u03bc\u03b7\u03b4\u03b5\u03bd \u03b2\u03b9\u03b1\u03b9\u03bf\u03bd \u03b1-\n\u03b5\u03c0\u03b9\u03bc\u03b5\u03bb\u03b5\u03b9\u03b1 \u03b1\u03be\u03b9\u03bf\u03c5\u03c3\u03b1-\n\u03bd\u03bf\u03bc\u03bf\u03c5\u03c3 \u03b5-\n-\u03b3\u03b5\u03bd\u03b7\u03bc\u03b1\u03c4\u03c9\u03bd-\n-\u03b1\u03bd\u03c4\u03b9\u03b4\u03b9\u03ba\u03bf\u03c5 \u03ba\n\u03c0\u03c1\u03bf\u03c3 \u03c4\u03bf \u03b5\u03af\u03bd\u03b1\u03b9\n\u03c7\u03c1\u03b7\u03bc\u03b1\u03c4\u03c9\u03bd \u03ba\u03b1\u03c4-\n-\u03c6\u03b1\u03bd\u03c4\u03b1 \u03c4\u03b7 \u03b7\u03b3\u03b5\u03bc\u03bf\u03bd\u03b9\u03b1 \u03ba-\n\u03bf\u03c5 \u03ba\u03b1\u03b9 \u03b5\u03c4\u03bf\u03c5\u03c3- \u00b7 -\u03c4\u03c9\u03bd \u03ba\u03c5\u03c1\u03b9\u03c9\u03bd \u03b7\u03bc\u03c9\u03bd \u03b4\u03b9\u03bf\u03ba\u03bb\u03b7\u03c4\u03b9\u03b1\u03bd\u03bf\u03c5 \u03ba\u03b1\u03b9 \u03bc\u03b1\u03be\u03b9\u03bc\u03b9\u03b1\u03bd\u03bf\u03c5 \u03c3\u03b5\u03b2\u03b1\u03c3\u03c4\u03af\u03c9\u03bd \u03ba\u03b1\u03b9 \u03b5\u03c4\u03bf\u03c5\u03c3\n-\u03c4\u03c9\u03bd-\n\u03b9\u03b1 \u03c4\u03c9\u03bd \u03ba\u03c5\u03c1\u03b9\u03c9\u03bd \u03b7\u03bc\u03c9\u03bd \u03ba\u03c9\u03bd\u03c3\u03c4\u03b1\u03bd\u03c4\u03b9\u03bf\u03c5 \u03ba\u03b1\u03b9 \u03bc\u03b1\u03be\u03b9\u03bc\u03b9\u03b1\u03bd\u03bf\u03c5-\n\u03c5\u03c0\u03bf\u03bc\u03bd\u03b7\u03bc\u03b1\u03c4\u03bf\u03c3-\n-\u03c4\u03c9\u03bd \u03ba\u03b1\u03b9\u03c3\u03b1\u03c1\u03c9\u03bd \u03c0\u03b1\u03c7\u03c9\u03bd-\n-\u03c4\u03c9\u03bd \u03ba\u03c5\u03c1\u03b9\u03c9\u03bd \u03b7\u03bc\u03c9\u03bd \u03b4\u03b9\u03bf\u03ba\u03bb\u03b7\u03c4\u03b9\u03b1\u03bd\u03bf\u03c5 \u03c4\u03bf \u03b7 \u03ba\u03b1\u03b9 \u03bc\u03b1\u03be\u03b9\u03bc\u03b9\u03b1\u03bd\u03bf\u03c5 \u03c4\u03bf \u03b6 \u03c3\u03b5\u03b2\u03b1\u03c3\u03c4\u03c9\u03bd-\n-\u03b5\u03bd \u03c4\u03c9 \u03c3\u03b7\u03ba\u03c1\u03b7\u03c4\u03c9- - -\u03c0\u03b1\u03c7\u03c9\u03bd \u03b5-\n\u03ba\u03b1\u03c4\u03b1-\n-\u03c0\u03c1\u03bf \u03b4\u03b9\u03ba\u03b7\u03c3-\n-\u03bf\u03c3 \u03bf \u03b4\u03b9\u03b1\u03c3\u03b7\u03bc\u03bf\u03c4\u03b1\u03c4\u03bf\u03c3 \u03b7\u03b3\u03b5\u03bc\u03c9\u03bd \u03b5-\n-\u03b4\u03b9\u03b1 \u03c4\u03bf\u03c5- - -\u03c3\u03c4\u03bf\u03c5-\n-\u03b9 \u03c5\u03c0\u03bf\u03c3\u03c4\u03b7\u03c3\u03b5\u03c4\u03b1\u03b9-\nI am grateful to Disa Lindholm for her previous transcription of this text."}, {"title": "6.3 Code availability", "content": "The scripts for formatting training and test data, configuration files for training and merging, and notebooks for test-\ning the models and calculating scores based on premade tests can be found at https://github.com/ericu9500/\nPapyriAndInscriptions. The formatted dataset and the best-performing models are available at https://\nhuggingface.co/collections/Ericu950/papyri-and-inscriptions-66ed3af86b665725dcc28ca5."}, {"title": "7 Competing Interests", "content": "The author declares no competing interests."}, {"title": "8 Funding", "content": "The computations and storage during training were enabled by resources provided by the National Academic Infras-\ntructure for Supercomputing in Sweden (NAISS), partially funded by the Swedish Research Council through grant\nagreement no. 2022-06725."}]}