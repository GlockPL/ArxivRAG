{"title": "Gene Regulatory Network Inference from Pre-trained Single-Cell\nTranscriptomics Transformer with Joint Graph Learning", "authors": ["Sindhura Kommu", "Yizhi Wang", "Yue Wang", "Xuan Wang"], "abstract": "Inferring gene regulatory networks (GRNs) from\nsingle-cell RNA sequencing (scRNA-seq) data is\na complex challenge that requires capturing the\nintricate relationships between genes and their\nregulatory interactions. In this study, we tackle\nthis challenge by leveraging the single-cell BERT-\nbased pre-trained transformer model (scBERT),\ntrained on extensive unlabeled scRNA-seq data, to augment structured biological knowledge from\nexisting GRNs. We introduce a novel joint graph\nlearning approach scTransNet that combines the\nrich contextual representations learned by pre-\ntrained single-cell language models with the struc-\ntured knowledge encoded in GRNs using graph\nneural networks (GNNs). By integrating these\ntwo modalities, our approach effectively reasons\nover both the gene expression level constraints\nprovided by the scRNA-seq data and the struc-\ntured biological knowledge inherent in GRNs.\nWe evaluate scTransNet on human cell bench-\nmark datasets from the BEELINE study with cell\ntype-specific ground truth networks. The results\ndemonstrate superior performance over current\nstate-of-the-art baselines, offering a deeper under-\nstanding of cellular regulatory mechanisms.", "sections": [{"title": "1. Introduction", "content": "Single-cell RNA sequencing (scRNA-seq) has transformed\nthe exploration of gene expression patterns at the individual\ncell level (Jovic et al., 2022), offering an unprecedented op-\nportunity to unravel the intricate regulatory mechanisms gov-\nerning cellular identity and function (Pratapa et al., 2020).\nOne such promising application is the inference of gene\nregulatory networks (GRNs) which represent the complex\ninterplay between transcription factors (TFs) and their down-\nstream target genes (Akers & Murali, 2021; Cramer, 2019).\nA precise understanding of GRNs is crucial for understand-\ning cellular processes, molecular functions, and ultimately,\ndeveloping effective therapeutic interventions (Biswas et al.,\n2021).\nHowever, inferring GRNs from scRNA-seq data is challeng-\ning due to cell heterogeneity (Wagner et al., 2016), cell cycle\neffects (Buettner et al., 2015), and high sparsity caused by\ndropout events (Kharchenko et al., 2014), which can impact\naccuracy and robustness. Additionally, the availability of\nlabeled scRNA-seq data corresponding to a GRN is limited,\nmaking it challenging to train models from scratch. Tra-\nditional unsupervised or self-supervised models, while not\nreliant on label information, often struggle to effectively\nhandle the noise, dropouts, high sparsity, and high dimen-\nsionality characteristics of scRNA-seq data (Moerman et al.,\n2019; Matsumoto et al., 2017; Zeng et al., 2023). Super-\nvised methods are also proposed for GRN reconstruction\n(Zhao et al., 2022; Shu et al., 2022; KC et al., 2019; Chen &\nLiu, 2022a) but struggle to handle batch effects and fail to\nleverage latent gene-gene interaction information effectively\nlimiting their generalization capabilities.\nRecent advancements in large language models (LLMs) and\nthe pre-training followed by fine-tuning paradigm (Devlin\net al., 2019; OpenAI, 2023) have significantly contributed to\nthe development of transformer-based architectures tailored\nfor scRNA-seq data analysis (Yang et al., 2022; Cui et al.,\n2024; Chen et al., 2023; Theodoris et al., 2023). These mod-\nels effectively leverage vast amounts of unlabeled scRNA-\nseq data to learn contextual representations and capture\nintricate latent interactions between genes. To address the\nlimitations of the current methods, we effectively leverage\none of these large-scale pre-trained transformer models,\nnamely scBERT (Yang et al., 2022), which has been pre-\ntrained on large-scale unlabelled scRNA-seq data to learn\ndomain-irrelevant gene expression patterns and interactions\nfrom the whole genome expression. By fine-tuning scBERT\non user specific scRNA-seq datasets, we can mitigate batch\neffects and capture latent gene-gene interactions for down-"}, {"title": "2. Related Work", "content": "Several methods have been developed to infer GRNs from\nscRNA-seq data, broadly categorized into unsupervised and\nsupervised methods.\nUnsupervised methods primarily include information\ntheory-based, model-based, and machine learning-based\napproaches. Information theory-based methods, such as\nmutual information (MI) (Margolin et al., 2006), Pearson\ncorrelation coefficient (PCC) (Salleh et al., 2015; Raza &\nJaiswal, 2013), and partial information decomposition and\ncontext (PIDC) (Chan et al., 2017), conduct correlation anal-\nyses under the assumption that the strength of the correlation\nbetween genes is is positively correlated with the likelihood\nof regulation between them. Model-based approaches, such\nas SCODE (Matsumoto et al., 2017), involve fitting gene ex-\npression profiles to models that describe gene relationships,\nwhich are then used to reconstruct GRNs (Shu et al., 2021;\nTsai et al., 2020).\nMachine learning-based unsupervised methods, like GE-\nNIE3 (Huynh-Thu et al., 2010) and GRNBoost2 (Moerman\net al., 2019), utilize tree-based algorithms to infer GRNs.\nThese methods are integrated into tools like SCENIC (Aibar\net al., 2017; Van de Sande et al., 2020), employing tree rules\nto learn regulatory relationships by iteratively excluding\none gene at a time to determine its associations with other\ngenes. Despite not requiring labeled data, these unsuper-\nvised methods often struggle with the noise, dropouts, high\nsparsity, and high dimensionality typical of scRNA-seq data.\nAdditionally, the computational expense and scalability is-\nsues of these tree-based methods, due to the necessity of\nsegmenting input data and iteratively establishing multiple\nmodels, present further challenges for large datasets.\nSupervised methods, including DGRNS (Zhao et al., 2022),\nconvolutional neural network for co-expression (CNNC)\n(Yuan & Bar-Joseph, 2019), and DeepDRIM (Chen et al.,\n2021), have been developed to address the increasing scale\nand inherent complexity of scRNA-seq data. Compared\nwith unsupervised learning, supervised models are capable\nof detecting much more subtle differences between positive\nand negative pairs (Yuan & Bar-Joseph, 2019).\nDGRNS (Zhao et al., 2022) combines recurrent neural net-\nworks (RNNs) for extracting temporal features and convolu-\ntional neural networks (CNNs) for extracting spatial features\nto infer GRNs. CNNC (Yuan & Bar-Joseph, 2019) converts\nthe identification of gene regulation into an image classifi-\ncation task by transforming the expression values of gene\npairs into histograms and using a CNN for classification.\nHowever, the performance of CNNC (Yuan & Bar-Joseph,\n2019) is hindered by the issue of transitive interactions. To\naddress this, DeepDRIM (Chen et al., 2021) considers the\ninformation from neighboring genes and converts TF-gene\npairs and neighboring genes into histograms as additional\ninputs, thereby reducing the occurrence of transitive inter-\nactions to some extent. Despite their success there exist\ncertain limitations to the employment of CNN model-based\napproaches for GRN reconstruction. First of all, the genera-\ntion of image data not only gives rise to unanticipated noise\nbut also conceals certain original data features. Addition-\nally, this process is time-consuming, and since it changes the\nformat of scRNA-seq data, the predictions made by these\nCNN-based computational approaches cannot be wholly\nexplained.\nIn addition to CNN-based methods, there are also other\napproaches such as GNE (Kc et al., 2019) and GRN-\nTransformer (Shu et al., 2022). GNE (gene network embed-\nding) (Kc et al., 2019) is a deep learning method based on\nmultilayer perceptron (MLP) for GRN inference applied to\nmicroarray data. It utilizes one-hot gene ID vectors from the\ngene topology to capture topological information, which is\noften inefficient due to the highly sparse nature of the result-\ning one-hot feature vector. GRN-Transformer (Shu et al.,\n2022) constructs a weakly supervised learning framework\nbased on axial transformer to infer cell-type-specific GRNs\nfrom scRNA-seq data and generic GRNs derived from the\nbulk data.\nMore recently, graph neural networks (GNNs) (Wu et al.,\n2020), which are effective in capturing the topology of gene\nnetworks, have been introduced into GRN prediction meth-\nods. For instance, GENELink (Chen & Liu, 2022b) treats\nGRN inference as a link prediction problem and uses graph\nattention networks to predict the probability of interaction\nbetween two gene nodes. However, existing methods often\nsuffer from limitations such as improper handling of batch\neffects, difficulty in leveraging latent gene-gene interaction"}, {"title": "3. Approach", "content": "As shown in (Figure 1), our approach contains four parts:\nBERT encoding, Attentive Pooling, GRN encoding with\nGNNs and Output layer. The input scRNA-seq datasets are\nprocessed into a cell-by-gene matrix, $X \\in \\mathbb{R}^{N \\times T}$, where\neach element represents the read count of an RNA molecule.\nSpecifically, for scRNA-seq data, the element denotes the\nRNA abundance for gene $t \\in \\{0,1,...,T\\}$ in cell $n \\in\n\\{0, 1, ..., N\\}$. In subsequent sections, we will refer to this\nmatrix as the raw count matrix. Let us denote the sequence\nof gene tokens as $\\{9_1, ..., 9_\\tau\\}$, where T is the total number\nof genes."}, {"title": "3.1. BERT Encoding Layer", "content": "(Yang et al., 2022; Cui et al., 2024; Chen et al., 2023;\nTheodoris et al., 2023) show that pre-trained transformer\nmodels have a strong understanding of gene-gene interac-\ntions across cells and have achieved state-of-the-art results\non a variety of single-cell processing tasks. We use scBERT\n(Yang et al., 2022) as the backbone, which is a successful\npre-trained model with the advantage of capturing long-\ndistance dependency as it uses Performer (Choromanski\net al., 2022) to improve the scalability of the model to toler-\nate over 16,000 gene inputs.\nThe scBERT model adopts the advanced paradigm of BERT\nand tailors the architecture to solve single-cell data analysis.\nThe connections of this model with BERT are given as fol-\nlows. First, scBERT follows BERT's revolutionary method\nto conduct self-supervised pre-training (Devlin et al., 2019)\nand uses Transformer as the model backbone (Choromanski\net al., 2022). Second, the design of embeddings in scBERT\nis similar to BERT in some aspects while having unique\nfeatures to leverage gene knowledge. From this perspective,\nthe gene expression embedding could be viewed as the to-\nken embedding of BERT. As shuffling the columns of the\ninput does not change its meaning (like the extension of\nBERT to understand tabular data with TaBERT (Yin et al.,\n2020)), absolute positions are meaningless for gene. In-\nstead gene2vec is used to produce gene embeddings, which\ncould be viewed as relative embeddings (Du et al., 2019)\nthat capture the semantic similarities between any of two\ngenes. Third, Transformer with global receptive field could\neffectively learn global representation and long-range de-\npendency without absolute position information, achieving\nexcellent performance on non-sequential data (such as im-\nages, tables) (Parmar et al., 2018; Yin et al., 2020).\nIn spite of the gene embedding, there is also a challenge on\nhow to utilize the transcription level of each gene, which is\nactually a single continuous variable. The gene expression\ncould also be considered as the occurrence of each gene that\nhas already been well-documented in a biological system.\nDrawing from bag-of-words (Zhang et al., 2010) insight,\nthe conventionally used term-frequency-analysis method is\napplied that discretizes the continuous expression variables\nby binning, and converts them into 200-dimensional vectors,\nwhich are then used as token embeddings for the scBERT\nmodel.\nFor each token $g_t$ in a cell, we construct its input represen-\ntation as:\n$h_t = emb_{gene2vec}(g_t) + emb_{expr}(g_t)$\nwhere $emb_{gene2vec}(g_t)$ represents gene2vec embedding\n(Du et al., 2019) of gene $g_t$ analogous to position embedding\nin BERT and $emb_{expr}(g_t)$ represents expression embedding\nof the gene expression of $g_t$ analogous to token embedding\nin BERT.\nSuch input representations are then fed into L successive\nTransformer encoder blocks, i.e.,\n$h_l = Transformer(h^{l-1}), l = 1, 2, ..., L,$\nso as to generate deep, context-aware representations for\ngenes. The final hidden states $\\{h_L\\}_{t=1}^{T}$ are taken as the\noutput of this layer (Devlin et al., 2019; Vaswani et al.,\n2023)."}, {"title": "3.2. Attentive Pooling", "content": "After extracting the BERT encodings we further utilize the\nattention scores across cells from the model to select the\nmost representative cells for pooling of each gene represen-\ntation. For each input gene token $g_t$ we get the embeddings\nfor all cells denoted as $\\{h_t(n)\\}_{n=1}^{N}$, where N is the number\nof cells.\nThe quadratic computational complexity of the BERT\nmodel, with the Transformer as its foundational unit, does\nnot scale efficiently for long sequences. Given that the num-\nber of genes in scRNA-seq data can exceed 20,000, this lim-\nitation becomes significant. To address this issue, scBERT\nemploys a matrix decomposition variant of the Transformer,\nknown as Performer (Choromanski et al., 2022), to handle\nlonger sequence lengths. In a regular Transformer, the dot-\nproduct attention mechanism maps Q, K, and V, which are\nthe encoded representations of the input queries, keys, and\nvalues for each unit. The bidirectional attention matrix is\nformulated as follows:\n$Att(Q, K,V) = D^{-1}(QK^T)V,$\n$D = diag(QK^T1_L)$\nwhere $Q = W_qX, K = W_KX, V = W_vX$ are linear\ntransformations of the input X; $W_Q, W_K$ and $W_v$ are the\nweight matrices as parameters; $1_L$ is the all-ones vector of\nlength L; and diag(.) is a diagonal matrix with the input\nvector as the diagonal.\nThe attention matrix in Performer is described as follows:\n$Att(Q, K, V) = \\tilde{D}^{-1}(Q'((K')^TV)),$\n$\\tilde{D} = diag(Q' ((K')^T1_L))$\nwhere $Q' = \\phi(Q), K' = \\phi(K)$, and the function $\\phi(x)$ is\ndefined as:\n$\\phi(X) = \\frac{c}{\\sqrt{m}}sin(\\omega^TX)$\nwhere c is a positive constant, $\\omega$ is a random feature matrix,\nand m is the dimensionality of the matrix.\nThe attention weights can be obtained from equation 3,\nmodified by replacing V with $V^0$, where $V^0$ contains one-\nhot indicators for each position index. All the attention\nmatrices are integrated into one matrix by taking an element-\nwise average across all attention matrices in multi-head\nmulti-layer Performers. In this average attention matrix for\neach cell, $A(i, j)$ represents how much attention from gene\ni was paid to gene j. To focus on the importance of genes\nto each cell n, the attention matrix is summed along the\ncolumns into an attention-sum vector $a_n$, and its length is\nequal to the number of genes. These attention scores of\ngene $g_t$ are obtained across cells and normalized denoted as\n$\\{a_n\\}_{n=1}^N$\nThese normalized scores are used for weighted aggrega-\ntion of gene embeddings across cells. We aggregate each\ncell's gene representations together into one gene-level cell"}, {"title": "3.3. GRN encoding with GNNS", "content": "embedding such that the updated matrix is of the form\n$Z \\in \\mathbb{R}^{T \\times d}$, where d is the dimension of the output gene\nembedding.\n$Z_g[t] = \\Sigma_{n=1}^{N}h_t(n) a_n$\nIn this module, we use raw count matrix as the features of\nthe genes. Subsequently, we utilize graph convolutional\nnetwork (GCN)-based interaction graph encoders to learn\ngene features by leveraging the underlying structure of the\ngene interaction graph.\nLet us denote the prior network as $G = \\{V, E\\}$, where V is\nthe set of nodes and E is the set of edges. To perform the\nreasoning on this prior gene regulatory network G, our GNN\nmodule builds on the graph attention framework (GAT)\n(Velickovic et al., 2017), which induces node representations\nvia iterative message passing between neighbors on the\ngraph. In each layer of this GNN, the current representation\nof the node embeddings $\\{v_1^{l-1}, ..., v_T^{l-1}\\}$ is fed into the layer\nto perform a round of information propagation between\nnodes in the graph and yield pre-fused node embeddings for\neach node:\n$\\tilde{v_T^{l-1}}\\ = GNN(\\{v_1^{l-1}, ..., v_T^{l-1}\\}$\nfor $l = 1, ..., M$\nSpecifically, for each layer l, we update the representation\n$\\hat{v_t^l}$ of each node by\n$\\hat{v_t^l} = f_n(\\Sigma_{v_s\\in n_v \\cup \\{v_t\\}} a_{st}m_{st}) + v_t^{l-1}$\nwhere $n_v$ represents the neighborhood of an arbitrary node\n$v_t$, $m_{st}$ denotes the message one of its neighbors $v_s$ passes\nto $v_t$, $a_{st}$ is an attention weight that scales the message $m_{st}$,\nand $f_n$ is a 2-layer MLP. The messages $m_{st}$ between nodes\nallow entity information from a node to affect the model's\nrepresentation of its neighbors, and are computed in the\nfollowing manner:\n$r_{st} = f_r(\\vec{r_{st}}, u_s, u_t)$\n$m_{st} = f_m (v^{l-1}, u_s, r_{st})$\nwhere $u_s$, $u_t$ are node type embeddings, $\\vec{r_{st}}$ is a relationem-\nbedding for the relation connecting $v_s$ and $v_t$, $f_r$ is a 2-\nlayer MLP, and $f_m$ is a linear transformation. The attention\nweights $a_{st}$ scale the contribution of each neighbor's mes-\nsage by its importance, and are computed as follows:\n$q_s = f_q(v^{l-1}, u_s)$\n$k_t = f_k(v^{l-1}, u_t, r_{st})$"}, {"title": "3.4. Final Output Layer", "content": "$\\alpha_{st} = \\frac{exp(\\sigma_{st})}{\\Sigma_{u_s \\in n_v \\cup \\{v_t\\}} exp(\\sigma_{st})}, \\sigma_{st} = \\frac{q_sk_t}{\\sqrt{D}}$\nwhere $f_q$ and $f_k$ are linear transformations and $u_s$, $u_t$, $r_{st}$\nare defined the same as above.\nIn the final output layer, we concatenate the input gene\nrepresentations $Z_g$ from the BERT encoding layer with the\ngraph representation of each gene from GNN to get the final\ngene embedding.\nWe input these final embeddings of pairwise genes i and j\ninto two channels with the same structure. Each channel is\ncomposed of MLPs to further encode representations to low-\ndimensional vectors which serve for downstream similarity\nmeasurement or causal inference between genes."}, {"title": "4. Experimental Setup", "content": "4.1. Benchmark scRNA-seq datasets\nThe performance of scTransNet is evaluated on two human\ncell types using single-cell RNA-sequencing (scRNA-seq)\ndatasets from the BEELINE study (Pratapa et al., 2020):\nhuman embryonic stem cells (hESC (Yuan & Bar-Joseph,\n2019)) and human mature hepatocytes (hHEP (Camp et al.,\n2017)). The cell-type-specific ChIP-seq ground-truth net-\nworks are used as a reference for these datasets. The scRNA-\nseq datasets are preprocessed following the approach de-\nscribed in the (Pratapa et al., 2020), focusing on inferring\ninteractions outgoing from transcription factors (TFs). The\nmost significantly varying genes are selected, including\nall TFs with a corrected P-value (Bonferroni method) of\nvariance below 0.01. Specifically, 500 and 1000 of\nthe most varying genes are chosen for gene regulatory network\n(GRN) inference. The scRNA-seq datasets can be accessed\nfrom the Gene Expression Omnibus (GEO) with accession\nnumbers GSE81252 (hHEP) and GSE75748 (hESC). The\nevaluation compares the inferred gene regulatory networks\nto known ChIP-seq ground-truth networks specific to these\ncell types.\n4.2. Implementation and Training details\nData Preparation We utilized the benchmark networks\nthat containing labeled directed regulatory dependencies\nbetween gene pairs. These dependencies were classified as\npositive samples (labeled 1) if present in the network, and\nnegative samples (labeled 0) if absent. Due to the inherent\nnetwork density, the number of negative samples signifi-\ncantly outnumbered positive samples. To address the class\nimbalance, known transcription factor (TF)-gene pairs are\nsplit into training (2/3), and test (1/3) sets. Positive training\nsamples were randomly selected from the known TF-gene"}, {"title": "4.3. Baseline Methods", "content": "pairs. Moreover, 10% of TF-gene pairs are randomly se-\nlected from training samples for validation. The remaining\npositive pairs formed the positive test set. Negative samples\nwere generated using the following strategies: 1) Unlabeled\ninteractions: All unobserved TF-gene interactions outside\nthe labeled files were considered negative instances. 2)\nHard negative sampling: To enhance model learning during\ntraining, we employed a uniformly random negative sam-\npling strategy within the training set. This involved creating\n\"hard negative samples\" by pairing each positive sample\n(g1, g2) with a negative sample (g1, g3), where both share\nthe same gene g1. This approach injects more discrimina-\ntive information and accelerates training. 3) Information\nleakage prevention: Negative test samples were randomly\nselected from the remaining negative instances after gen-\nerating the training and validation sets. This ensured no\ninformation leakage from the test set to the training process.\nThe positive-to-negative sample ratio in each dataset was\nadjusted to reflect the network density i.e.\n$\\frac{Positive}{Negative} = \\frac{Network Density}{1- Network Density}$\nModel Training To account for the class imbalance, we\nadopted two performance metrics: Area Under the Receiver\nOperating Characteristic Curve (AUROC) and Area Under\nthe Precision-Recall Curve (AUPRC). The supervised model\nwas trained for 100 iterations with a learning rate of 0.003.\nThe Graph Neural Network (GNN) architecture comprised\ntwo layers with hidden layer sizes of 256 and 128 units,\nrespectively.\nEvaluation All reported results are based solely on predic-\ntions from the held-out test set. To ensure a fair comparison,\nidentical training and validation sets were utilized for all\nevaluated supervised methods. This approach eliminates\npotential bias introduced by different data splits.\nTo assess the effectiveness of our model in predicting GRNs,\nwe compare our model scTransNet against the existing base-\nline methods commonly used for inferring GRNs, as fol-\nlows:\n\u2022 GNNLink (Mao et al., 2023) is a graph neural network\nmodel that uses a GCN-based interaction graph encoder\nto capture gene expression patterns.\n\u2022 GENELink (Chen & Liu, 2022b) proposes a graph atten-\ntion network (GAT) approach to infer potential GRNs by\nleveraging the graph structure of gene regulatory interac-\ntions.\n\u2022 GNE (gene network embedding) (Kc et al., 2019) pro-\nposes a multilayer perceptron (MLP) approach to encode"}, {"title": "5. Results", "content": "5.1. Performance on benchmark datasets\nThe results (see Figure 2) demonstrate that scTransNet\noutperforms state-of-the-art baseline methods across all\nfour benchmark datasets, achieving superior performance\nin terms of both AUROC and AUPRC evaluation metrics.\nNotably, scTransNet's AUROC values are approximately\n5.4% and 7.4% higher on average compared to the second-\nbest methods, namely GNNLink (Mao et al., 2023) and\nGENELink (Chen & Liu, 2022b), respectively. Similarly, sc-\nTransNet's AUPRC values show an impressive improvement\nof approximately 7.4% and 16% on average over GNNLink\nand GENELink, respectively.\nTo gain further insights, we analyzed scTransNet's final\ngene regulatory network (GRN) predictions and compared\nthem with those from GENELink. Our analysis revealed"}, {"title": "5.2. Discussion and Ablations", "content": "that scTransNet effectively captured all the gene regulatory\ninteractions predicted by GENELink. This finding suggests\nthat by incorporating joint learning, scTransNet does not\nintroduce additional noise to the predictive power of the\ngraph representations. Instead, it enhances the predictive\ncapability through the scBERT encoder in its architecture.\nFigure 3 provides a visualization of a partial subgraph of\nthe ground truth GRN, highlighting the predictions made\nby scTransNet that were not captured by GENELink, which\nsolely relies on graphs for predicting gene-gene interactions.\nAdditionally, the figure visualizes the ground truth labels\nthat scTransNet failed to capture. In summary, the compara-\ntive analysis demonstrates that scTransNet effectively cap-\ntures all the regulatory interactions predicted by GENELink\nwhile leveraging joint learning to improve predictive perfor-\nmance. The visualization illustrates the additional interac-\ntions scTransNet could predict beyond GENELink, as well\nas the ground truth interactions it missed, providing insights\ninto the strengths and limitations of the proposed method.\nTo evaluate the effectiveness of jointly learning from pre-\ntrained scRNA-seq language models (Yang et al., 2022),\nwhich capture rich contextual representations, and Gene\nRegulatory Networks (GRNs), which encode structured bi-\nological knowledge, we compare the average Area Under\nthe Receiver Operating Characteristic Curve (AUROC) and\nArea Under the Precision-Recall Curve (AUPRC) metrics\nwith and without these encoders across the four human cell\ntype benchmark datasets (Pratapa et al., 2020). The aver-\nage AUROC and AUPRC scores are calculated across both\nthe TFs+500 highly variable genes and TFs+1000 highly\nvariable genes datasets for each human cell data type (i.e,\nhESC (human embryonic stem cells) and hHEP (human\nmature hepatocytes)). Additionally, we validate the impor-\ntance of incorporating Attentive Pooling (Section 3.2) by\ncontrasting the results when using average pooling of gene\nembeddings across cells instead of attentive pooling. Consis-\ntent parameter settings are employed across all four human\ncell benchmark datasets, with Cell-type-specific ChIP-seq\nnetwork data serving as the ground truth.\nEffect of Graph Neural Network Component: The re-\nsults demonstrate the significant impact of incorporating"}, {"title": "6. Conclusion and Future Work", "content": "the Graph Neural Network (GNN) encoder component in\nthe proposed method. With the GNN encoder", "former": "The removal of the scBERT encoder also leads\nto a drop in performance", "Mechanism": "The impact of\nincorporating Attentive Pooling is evaluated by comparing\nthe AUROC and AUPRC metrics with and without attentive\npooling across four datasets. As shown in Table 1", "modules": "a GNN encoder to capture the\nnetwork topology from known```json\n{"}, {"title": "Gene Regulatory Network Inference from Pre-trained Single-Cell\nTranscriptomics Transformer with Joint Graph Learning", "authors": ["Sindhura Kommu", "Yizhi Wang", "Yue Wang", "Xuan Wang"], "abstract": "Inferring gene regulatory networks (GRNs) from\nsingle-cell RNA sequencing (scRNA-seq) data is\na complex challenge that requires capturing the\nintricate relationships between genes and their\nregulatory interactions. In this study, we tackle\nthis challenge by leveraging the single-cell BERT-\nbased pre-trained transformer model (scBERT),\ntrained on extensive unlabeled scRNA-seq data, to augment structured biological knowledge from\nexisting GRNs. We introduce a novel joint graph\nlearning approach scTransNet that combines the\nrich contextual representations learned by pre-\ntrained single-cell language models with the struc-\ntured knowledge encoded in GRNs using graph\nneural networks (GNNs). By integrating these\ntwo modalities, our approach effectively reasons\nover both the gene expression level constraints\nprovided by the scRNA-seq data and the struc-\ntured biological knowledge inherent in GRNs.\nWe evaluate scTransNet on human cell bench-\nmark datasets from the BEELINE study with cell\ntype-specific ground truth networks. The results\ndemonstrate superior performance over current\nstate-of-the-art baselines, offering a deeper under-\nstanding of cellular regulatory mechanisms.", "sections": [{"title": "1. Introduction", "content": "Single-cell RNA sequencing (scRNA-seq) has transformed\nthe exploration of gene expression patterns at the individual\ncell level (Jovic et al., 2022), offering an unprecedented op-\nportunity to unravel the intricate regulatory mechanisms gov-\nerning cellular identity and function (Pratapa et al., 2020).\nOne such promising application is the inference of gene\nregulatory networks (GRNs) which represent the complex\ninterplay between transcription factors (TFs) and their down-\nstream target genes (Akers & Murali, 2021; Cramer, 2019).\nA precise understanding of GRNs is crucial for understand-\ning cellular processes, molecular functions, and ultimately,\ndeveloping effective therapeutic interventions (Biswas et al.,\n2021).\nHowever, inferring GRNs from scRNA-seq data is challeng-\ning due to cell heterogeneity (Wagner et al., 2016), cell cycle\neffects (Buettner et al., 2015), and high sparsity caused by\ndropout events (Kharchenko et al., 2014), which can impact\naccuracy and robustness. Additionally, the availability of\nlabeled scRNA-seq data corresponding to a GRN is limited,\nmaking it challenging to train models from scratch. Tra-\nditional unsupervised or self-supervised models, while not\nreliant on label information, often struggle to effectively\nhandle the noise, dropouts, high sparsity, and high dimen-\nsionality characteristics of scRNA-seq data (Moerman et al.,\n2019; Matsumoto et al., 2017; Zeng et al., 2023). Super-\nvised methods are also proposed for GRN reconstruction\n(Zhao et al., 2022; Shu et al., 2022; KC et al., 2019; Chen &\nLiu, 2022a) but struggle to handle batch effects and fail to\nleverage latent gene-gene interaction information effectively\nlimiting their generalization capabilities.\nRecent advancements in large language models (LLMs) and\nthe pre-training followed by fine-tuning paradigm (Devlin\net al., 2019; OpenAI, 2023) have significantly contributed to\nthe development of transformer-based architectures tailored\nfor scRNA-seq data analysis (Yang et al., 2022; Cui et al.,\n2024; Chen et al., 2023; Theodoris et al., 2023). These mod-\nels effectively leverage vast amounts of unlabeled scRNA-\nseq data to learn contextual representations and capture\nintricate latent interactions between genes. To address the\nlimitations of the current methods, we effectively leverage\none of these large-scale pre-trained transformer models,\nnamely scBERT (Yang et al., 2022), which has been pre-\ntrained on large-scale unlabelled scRNA-seq data to learn\ndomain-irrelevant gene expression patterns and interactions\nfrom the whole genome expression. By fine-tuning scBERT\non user specific scRNA-seq datasets, we can mitigate batch\neffects and capture latent gene-gene interactions for down-"}, {"title": "2. Related Work", "content": "Several methods have been developed to infer GRNs from\nscRNA-seq data, broadly categorized into unsupervised and\nsupervised methods.\nUnsupervised methods primarily include information\ntheory-based, model-based, and machine learning-based\napproaches. Information theory-based methods, such as\nmutual information (MI) (Margolin et al., 2006), Pearson\ncorrelation coefficient (PCC) (Salleh et al., 2015; Raza &\nJaiswal, 2013), and partial information decomposition and\ncontext (PIDC) (Chan et al., 2017), conduct correlation anal-\nyses under the assumption that the strength of the correlation\nbetween genes is is positively correlated with the likelihood\nof regulation between them. Model-based approaches, such\nas SCODE (Matsumoto et al., 2017), involve fitting gene ex-\npression profiles to models that describe gene relationships,\nwhich are then used to reconstruct GRNs (Shu et al., 2021;\nTsai et al., 2020).\nMachine learning-based unsupervised methods, like GE-\nNIE3 (Huynh-Thu et al., 2010) and GRNBoost2 (Moerman\net al., 2019), utilize tree-based algorithms to infer GRNs.\nThese methods are integrated into tools like SCENIC (Aibar\net al., 2017; Van de Sande et al., 2020), employing tree rules\nto learn regulatory relationships by iteratively excluding\none gene at a time to determine its associations with other\ngenes. Despite not requiring labeled data, these unsuper-\nvised methods often struggle with the noise, dropouts, high\nsparsity, and high dimensionality typical of scRNA-seq data.\nAdditionally, the computational expense and scalability is-\nsues of these tree-based methods, due to the necessity of\nsegmenting input data and iteratively establishing multiple\nmodels, present further challenges for large datasets.\nSupervised methods, including DGRNS (Zhao et al., 2022),\nconvolutional neural network for co-expression (CNNC)\n(Yuan & Bar-Joseph, 2019), and DeepDRIM (Chen et al.,\n2021), have been developed to address the increasing scale\nand inherent complexity of scRNA-seq data. Compared\nwith unsupervised learning, supervised models are capable\nof detecting much more subtle differences between positive\nand negative pairs (Yuan & Bar-Joseph, 2019).\nDGRNS (Zhao et al., 2022) combines recurrent neural net-\nworks (RNNs) for extracting temporal features and convolu-\ntional neural networks (CNNs) for extracting spatial features\nto infer GRNs. CNNC (Yuan & Bar-Joseph, 2019) converts\nthe identification of gene regulation into an image classifi-\ncation task by transforming the expression values of gene\npairs into histograms and using a CNN for classification.\nHowever, the performance of CNNC (Yuan & Bar-Joseph,\n2019) is hindered by the issue of transitive interactions. To\naddress this, DeepDRIM (Chen et al., 2021) considers the\ninformation from neighboring genes and converts TF-gene\npairs and neighboring genes into histograms as additional\ninputs, thereby reducing the occurrence of transitive inter-\nactions to some extent. Despite their success there exist\ncertain limitations to the employment of CNN model-based\napproaches for GRN reconstruction. First of all, the genera-\ntion of image data not only gives rise to unanticipated noise\nbut also conceals certain original data features. Addition-\nally, this process is time-consuming, and since it changes the\nformat of scRNA-seq data, the predictions made by these\nCNN-based computational approaches cannot be wholly\nexplained.\nIn addition to CNN-based methods, there are also other\napproaches such as GNE (Kc et al., 2019) and GRN-\nTransformer (Shu et al., 2022). GNE (gene network embed-\nding) (Kc et al., 2019) is a deep learning method based on\nmultilayer perceptron (MLP) for GRN inference applied to\nmicroarray data. It utilizes one-hot gene ID vectors from the\ngene topology to capture topological information, which is\noften inefficient due to the highly sparse nature of the result-\ning one-hot feature vector. GRN-Transformer (Shu et al.,\n2022) constructs a weakly supervised learning framework\nbased on axial transformer to infer cell-type-specific GRNs\nfrom scRNA-seq data and generic GRNs derived from the\nbulk data.\nMore recently, graph neural networks (GNNs) (Wu et al.,\n2020), which are effective in capturing the topology of gene\nnetworks, have been introduced into GRN prediction meth-\nods. For instance, GENELink (Chen & Liu, 2022b) treats\nGRN inference as a link prediction problem and uses graph\nattention networks to predict the probability of interaction\nbetween two gene nodes. However, existing methods often\nsuffer from limitations such as improper handling of batch\neffects, difficulty in leveraging latent gene-gene interaction"}, {"title": "3. Approach", "content": "As shown in (Figure 1), our approach contains four parts:\nBERT encoding, Attentive Pooling, GRN encoding with\nGNNs and Output layer. The input scRNA-seq datasets are\nprocessed into a cell-by-gene matrix, $X \\in \\mathbb{R}^{N \\times T}$, where\neach element represents the read count of an RNA molecule.\nSpecifically, for scRNA-seq data, the element denotes the\nRNA abundance for gene $t \\in \\{0,1,...,T\\}$ in cell $n \\in\n\\{0, 1, ..., N\\}$. In subsequent sections, we will refer to this\nmatrix as the raw count matrix. Let us denote the sequence\nof gene tokens as $\\{9_1, ..., 9_\\tau\\}$, where T is the total number\nof genes."}, {"title": "3.1. BERT Encoding Layer", "content": "(Yang et al., 2022; Cui et al., 2024; Chen et al., 2023;\nTheodoris et al., 2023) show that pre-trained transformer\nmodels have a strong understanding of gene-gene interac-\ntions across cells and have achieved state-of-the-art results\non a variety of single-cell processing tasks. We use scBERT\n(Yang et al., 2022) as the backbone, which is a successful\npre-trained model with the advantage of capturing long-\ndistance dependency as it uses Performer (Choromanski\net al., 2022) to improve the scalability of the model to toler-\nate over 16,000 gene inputs.\nThe scBERT model adopts the advanced paradigm of BERT\nand tailors the architecture to solve single-cell data analysis.\nThe connections of this model with BERT are given as fol-\nlows. First, scBERT follows BERT's revolutionary method\nto conduct self-supervised pre-training (Devlin et al., 2019)\nand uses Transformer as the model backbone (Choromanski\net al., 2022). Second, the design of embeddings in scBERT\nis similar to BERT in some aspects while having unique\nfeatures to leverage gene knowledge. From this perspective,\nthe gene expression embedding could be viewed as the to-\nken embedding of BERT. As shuffling the columns of the\ninput does not change its meaning (like the extension of\nBERT to understand tabular data with TaBERT (Yin et al.,\n2020)), absolute positions are meaningless for gene. In-\nstead gene2vec is used to produce gene embeddings, which\ncould be viewed as relative embeddings (Du et al., 2019)\nthat capture the semantic similarities between any of two\ngenes. Third, Transformer with global receptive field could\neffectively learn global representation and long-range de-\npendency without absolute position information, achieving\nexcellent performance on non-sequential data (such as im-\nages, tables) (Parmar et al., 2018; Yin et al., 2020).\nIn spite of the gene embedding, there is also a challenge on\nhow to utilize the transcription level of each gene, which is\nactually a single continuous variable. The gene expression\ncould also be considered as the occurrence of each gene that\nhas already been well-documented in a biological system.\nDrawing from bag-of-words (Zhang et al., 2010) insight,\nthe conventionally used term-frequency-analysis method is\napplied that discretizes the continuous expression variables\nby binning, and converts them into 200-dimensional vectors,\nwhich are then used as token embeddings for the scBERT\nmodel.\nFor each token $g_t$ in a cell, we construct its input represen-\ntation as:\n$h_t = emb_{gene2vec}(g_t) + emb_{expr}(g_t)$\nwhere $emb_{gene2vec}(g_t)$ represents gene2vec embedding\n(Du et al., 2019) of gene $g_t$ analogous to position embedding\nin BERT and $emb_{expr}(g_t)$ represents expression embedding\nof the gene expression of $g_t$ analogous to token embedding\nin BERT.\nSuch input representations are then fed into L successive\nTransformer encoder blocks, i.e.,\n$h_l = Transformer(h^{l-1}), l = 1, 2, ..., L,$\nso as to generate deep, context-aware representations for\ngenes. The final hidden states $\\{h_L\\}_{t=1}^{T}$ are taken as the\noutput of this layer (Devlin et al., 2019; Vaswani et al.,\n2023)."}, {"title": "3.2. Attentive Pooling", "content": "After extracting the BERT encodings we further utilize the\nattention scores across cells from the model to select the\nmost representative cells for pooling of each gene represen-\ntation. For each input gene token $g_t$ we get the embeddings\nfor all cells denoted as $\\{h_t(n)\\}_{n=1}^{N}$, where N is the number\nof cells.\nThe quadratic computational complexity of the BERT\nmodel, with the Transformer as its foundational unit, does\nnot scale efficiently for long sequences. Given that the num-\nber of genes in scRNA-seq data can exceed 20,000, this lim-\nitation becomes significant. To address this issue, scBERT\nemploys a matrix decomposition variant of the Transformer,\nknown as Performer (Choromanski et al., 2022), to handle\nlonger sequence lengths. In a regular Transformer, the dot-\nproduct attention mechanism maps Q, K, and V, which are\nthe encoded representations of the input queries, keys, and\nvalues for each unit. The bidirectional attention matrix is\nformulated as follows:\n$Att(Q, K,V) = D^{-1}(QK^T)V,$\n$D = diag(QK^T1_L)$\nwhere $Q = W_qX, K = W_KX, V = W_vX$ are linear\ntransformations of the input X; $W_Q, W_K$ and $W_v$ are the\nweight matrices as parameters; $1_L$ is the all-ones vector of\nlength L; and diag(.) is a diagonal matrix with the input\nvector as the diagonal.\nThe attention matrix in Performer is described as follows:\n$Att(Q, K, V) = \\tilde{D}^{-1}(Q'((K')^TV)),$\n$\\tilde{D} = diag(Q' ((K')^T1_L))$\nwhere $Q' = \\phi(Q), K' = \\phi(K)$, and the function $\\phi(x)$ is\ndefined as:\n$\\phi(X) = \\frac{c}{\\sqrt{m}}sin(\\omega^TX)$\nwhere c is a positive constant, $\\omega$ is a random feature matrix,\nand m is the dimensionality of the matrix.\nThe attention weights can be obtained from equation 3,\nmodified by replacing V with $V^0$, where $V^0$ contains one-\nhot indicators for each position index. All the attention\nmatrices are integrated into one matrix by taking an element-\nwise average across all attention matrices in multi-head\nmulti-layer Performers. In this average attention matrix for\neach cell, $A(i, j)$ represents how much attention from gene\ni was paid to gene j. To focus on the importance of genes\nto each cell n, the attention matrix is summed along the\ncolumns into an attention-sum vector $a_n$, and its length is\nequal to the number of genes. These attention scores of\ngene $g_t$ are obtained across cells and normalized denoted as\n$\\{a_n\\}_{n=1}^N$\nThese normalized scores are used for weighted aggrega-\ntion of gene embeddings across cells. We aggregate each\ncell's gene representations together into one gene-level cell"}, {"title": "3.3. GRN encoding with GNNS", "content": "embedding such that the updated matrix is of the form\n$Z \\in \\mathbb{R}^{T \\times d}$, where d is the dimension of the output gene\nembedding.\n$Z_g[t] = \\Sigma_{n=1}^{N}h_t(n) a_n$\nIn this module, we use raw count matrix as the features of\nthe genes. Subsequently, we utilize graph convolutional\nnetwork (GCN)-based interaction graph encoders to learn\ngene features by leveraging the underlying structure of the\ngene interaction graph.\nLet us denote the prior network as $G = \\{V, E\\}$, where V is\nthe set of nodes and E is the set of edges. To perform the\nreasoning on this prior gene regulatory network G, our GNN\nmodule builds on the graph attention framework (GAT)\n(Velickovic et al., 2017), which induces node representations\nvia iterative message passing between neighbors on the\ngraph. In each layer of this GNN, the current representation\nof the node embeddings $\\{v_1^{l-1}, ..., v_T^{l-1}\\}$ is fed into the layer\nto perform a round of information propagation between\nnodes in the graph and yield pre-fused node embeddings for\neach node:\n$\\tilde{v_T^{l-1}}\\ = GNN(\\{v_1^{l-1}, ..., v_T^{l-1}\\}$\nfor $l = 1, ..., M$\nSpecifically, for each layer l, we update the representation\n$\\hat{v_t^l}$ of each node by\n$\\hat{v_t^l} = f_n(\\Sigma_{v_s\\in n_v \\cup \\{v_t\\}} a_{st}m_{st}) + v_t^{l-1}$\nwhere $n_v$ represents the neighborhood of an arbitrary node\n$v_t$, $m_{st}$ denotes the message one of its neighbors $v_s$ passes\nto $v_t$, $a_{st}$ is an attention weight that scales the message $m_{st}$,\nand $f_n$ is a 2-layer MLP. The messages $m_{st}$ between nodes\nallow entity information from a node to affect the model's\nrepresentation of its neighbors, and are computed in the\nfollowing manner:\n$r_{st} = f_r(\\vec{r_{st}}, u_s, u_t)$\n$m_{st} = f_m (v^{l-1}, u_s, r_{st})$\nwhere $u_s$, $u_t$ are node type embeddings, $\\vec{r_{st}}$ is a relationem-\nbedding for the relation connecting $v_s$ and $v_t$, $f_r$ is a 2-\nlayer MLP, and $f_m$ is a linear transformation. The attention\nweights $a_{st}$ scale the contribution of each neighbor's mes-\nsage by its importance, and are computed as follows:\n$q_s = f_q(v^{l-1}, u_s)$\n$k_t = f_k(v^{l-1}, u_t, r_{st})$"}, {"title": "3.4. Final Output Layer", "content": "$\\alpha_{st} = \\frac{exp(\\sigma_{st})}{\\Sigma_{u_s \\in n_v \\cup \\{v_t\\}} exp(\\sigma_{st})}, \\sigma_{st} = \\frac{q_sk_t}{\\sqrt{D}}$\nwhere $f_q$ and $f_k$ are linear transformations and $u_s$, $u_t$, $r_{st}$\nare defined the same as above.\nIn the final output layer, we concatenate the input gene\nrepresentations $Z_g$ from the BERT encoding layer with the\ngraph representation of each gene from GNN to get the final\ngene embedding.\nWe input these final embeddings of pairwise genes i and j\ninto two channels with the same structure. Each channel is\ncomposed of MLPs to further encode representations to low-\ndimensional vectors which serve for downstream similarity\nmeasurement or causal inference between genes."}, {"title": "4. Experimental Setup", "content": "4.1. Benchmark scRNA-seq datasets\nThe performance of scTransNet is evaluated on two human\ncell types using single-cell RNA-sequencing (scRNA-seq)\ndatasets from the BEELINE study (Pratapa et al., 2020):\nhuman embryonic stem cells (hESC (Yuan & Bar-Joseph,\n2019)) and human mature hepatocytes (hHEP (Camp et al.,\n2017)). The cell-type-specific ChIP-seq ground-truth net-\nworks are used as a reference for these datasets. The scRNA-\nseq datasets are preprocessed following the approach de-\nscribed in the (Pratapa et al., 2020), focusing on inferring\ninteractions outgoing from transcription factors (TFs). The\nmost significantly varying genes are selected, including\nall TFs with a corrected P-value (Bonferroni method) of\nvariance below 0.01. Specifically, 500 and 1000 of\nthe most varying genes are chosen for gene regulatory network\n(GRN) inference. The scRNA-seq datasets can be accessed\nfrom the Gene Expression Omnibus (GEO) with accession\nnumbers GSE81252 (hHEP) and GSE75748 (hESC). The\nevaluation compares the inferred gene regulatory networks\nto known ChIP-seq ground-truth networks specific to these\ncell types.\n4.2. Implementation and Training details\nData Preparation We utilized the benchmark networks\nthat containing labeled directed regulatory dependencies\nbetween gene pairs. These dependencies were classified as\npositive samples (labeled 1) if present in the network, and\nnegative samples (labeled 0) if absent. Due to the inherent\nnetwork density, the number of negative samples signifi-\ncantly outnumbered positive samples. To address the class\nimbalance, known transcription factor (TF)-gene pairs are\nsplit into training (2/3), and test (1/3) sets. Positive training\nsamples were randomly selected from the known TF-gene"}, {"title": "4.3. Baseline Methods", "content": "pairs. Moreover, 10% of TF-gene pairs are randomly se-\nlected from training samples for validation. The remaining\npositive pairs formed the positive test set. Negative samples\nwere generated using the following strategies: 1) Unlabeled\ninteractions: All unobserved TF-gene interactions outside\nthe labeled files were considered negative instances. 2)\nHard negative sampling: To enhance model learning during\ntraining, we employed a uniformly random negative sam-\npling strategy within the training set. This involved creating\n\"hard negative samples\" by pairing each positive sample\n(g1, g2) with a negative sample (g1, g3), where both share\nthe same gene g1. This approach injects more discrimina-\ntive information and accelerates training. 3) Information\nleakage prevention: Negative test samples were randomly\nselected from the remaining negative instances after gen-\nerating the training and validation sets. This ensured no\ninformation leakage from the test set to the training process.\nThe positive-to-negative sample ratio in each dataset was\nadjusted to reflect the network density i.e.\n$\\frac{Positive}{Negative} = \\frac{Network Density}{1- Network Density}$\nModel Training To account for the class imbalance, we\nadopted two performance metrics: Area Under the Receiver\nOperating Characteristic Curve (AUROC) and Area Under\nthe Precision-Recall Curve (AUPRC). The supervised model\nwas trained for 100 iterations with a learning rate of 0.003.\nThe Graph Neural Network (GNN) architecture comprised\ntwo layers with hidden layer sizes of 256 and 128 units,\nrespectively.\nEvaluation All reported results are based solely on predic-\ntions from the held-out test set. To ensure a fair comparison,\nidentical training and validation sets were utilized for all\nevaluated supervised methods. This approach eliminates\npotential bias introduced by different data splits.\nTo assess the effectiveness of our model in predicting GRNs,\nwe compare our model scTransNet against the existing base-\nline methods commonly used for inferring GRNs, as fol-\nlows:\n\u2022 GNNLink (Mao et al., 2023) is a graph neural network\nmodel that uses a GCN-based interaction graph encoder\nto capture gene expression patterns.\n\u2022 GENELink (Chen & Liu, 2022b) proposes a graph atten-\ntion network (GAT) approach to infer potential GRNs by\nleveraging the graph structure of gene regulatory interac-\ntions.\n\u2022 GNE (gene network embedding) (Kc et al., 2019) pro-\nposes a multilayer perceptron (MLP) approach to encode"}, {"title": "5. Results", "content": "5.1. Performance on benchmark datasets\nThe results (see Figure 2) demonstrate that scTransNet\noutperforms state-of-the-art baseline methods across all\nfour benchmark datasets, achieving superior performance\nin terms of both AUROC and AUPRC evaluation metrics.\nNotably, scTransNet's AUROC values are approximately\n5.4% and 7.4% higher on average compared to the second-\nbest methods, namely GNNLink (Mao et al., 2023) and\nGENELink (Chen & Liu, 2022b), respectively. Similarly, sc-\nTransNet's AUPRC values show an impressive improvement\nof approximately 7.4% and 16% on average over GNNLink\nand GENELink, respectively.\nTo gain further insights, we analyzed scTransNet's final\ngene regulatory network (GRN) predictions and compared\nthem with those from GENELink. Our analysis revealed"}, {"title": "5.2. Discussion and Ablations", "content": "that scTransNet effectively captured all the gene regulatory\ninteractions predicted by GENELink. This finding suggests\nthat by incorporating joint learning, scTransNet does not\nintroduce additional noise to the predictive power of the\ngraph representations. Instead, it enhances the predictive\ncapability through the scBERT encoder in its architecture.\nFigure 3 provides a visualization of a partial subgraph of\nthe ground truth GRN, highlighting the predictions made\nby scTransNet that were not captured by GENELink, which\nsolely relies on graphs for predicting gene-gene interactions.\nAdditionally, the figure visualizes the ground truth labels\nthat scTransNet failed to capture. In summary, the compara-\ntive analysis demonstrates that scTransNet effectively cap-\ntures all the regulatory interactions predicted by GENELink\nwhile leveraging joint learning to improve predictive perfor-\nmance. The visualization illustrates the additional interac-\ntions scTransNet could predict beyond GENELink, as well\nas the ground truth interactions it missed, providing insights\ninto the strengths and limitations of the proposed method.\nTo evaluate the effectiveness of jointly learning from pre-\ntrained scRNA-seq language models (Yang et al., 2022),\nwhich capture rich contextual representations, and Gene\nRegulatory Networks (GRNs), which encode structured bi-\nological knowledge, we compare the average Area Under\nthe Receiver Operating Characteristic Curve (AUROC) and\nArea Under the Precision-Recall Curve (AUPRC) metrics\nwith and without these encoders across the four human cell\ntype benchmark datasets (Pratapa et al., 2020). The aver-\nage AUROC and AUPRC scores are calculated across both\nthe TFs+500 highly variable genes and TFs+1000 highly\nvariable genes datasets for each human cell data type (i.e,\nhESC (human embryonic stem cells) and hHEP (human\nmature hepatocytes)). Additionally, we validate the impor-\ntance of incorporating Attentive Pooling (Section 3.2) by\ncontrasting the results when using average pooling of gene\nembeddings across cells instead of attentive pooling. Consis-"}, {"title": "6. Conclusion and Future Work", "content": "tent parameter settings are employed across all four human\ncell benchmark datasets", "Component": "The re-\nsults demonstrate the significant impact of incorporating\nthe Graph Neural Network (GNN) encoder component in\nthe proposed method. With the GNN encoder", "Trans-\nformer": "The removal of the scBERT encoder also leads\nto a drop in performance", "Mechanism": "The impact of\nincorporating Attentive Pooling is evaluated by comparing\nthe AUROC and AUPRC metrics with and without attentive\npooling across four datasets. As shown in Table 1", "modules": "a GNN encoder to capture the\nnetwork topology from"}]}]}