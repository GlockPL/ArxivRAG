{"title": "Gene Regulatory Network Inference from Pre-trained Single-Cell Transcriptomics Transformer with Joint Graph Learning", "authors": ["Sindhura Kommu", "Yizhi Wang", "Yue Wang", "Xuan Wang"], "abstract": "Inferring gene regulatory networks (GRNs) from single-cell RNA sequencing (scRNA-seq) data is a complex challenge that requires capturing the intricate relationships between genes and their regulatory interactions. In this study, we tackle this challenge by leveraging the single-cell BERT-based pre-trained transformer model (scBERT), trained on extensive unlabeled scRNA-seq data, to augment structured biological knowledge from existing GRNs. We introduce a novel joint graph learning approach scTransNet that combines the rich contextual representations learned by pre-trained single-cell language models with the structured knowledge encoded in GRNs using graph neural networks (GNNs). By integrating these two modalities, our approach effectively reasons over both the gene expression level constraints provided by the scRNA-seq data and the structured biological knowledge inherent in GRNs. We evaluate scTransNet on human cell benchmark datasets from the BEELINE study with cell type-specific ground truth networks. The results demonstrate superior performance over current state-of-the-art baselines, offering a deeper understanding of cellular regulatory mechanisms.", "sections": [{"title": "1. Introduction", "content": "Single-cell RNA sequencing (scRNA-seq) has transformed the exploration of gene expression patterns at the individual cell level (Jovic et al., 2022), offering an unprecedented opportunity to unravel the intricate regulatory mechanisms governing cellular identity and function (Pratapa et al., 2020).\nOne such promising application is the inference of gene regulatory networks (GRNs) which represent the complex interplay between transcription factors (TFs) and their downstream target genes (Akers & Murali, 2021; Cramer, 2019). A precise understanding of GRNs is crucial for understanding cellular processes, molecular functions, and ultimately, developing effective therapeutic interventions (Biswas et al., 2021).\nHowever, inferring GRNs from scRNA-seq data is challenging due to cell heterogeneity (Wagner et al., 2016), cell cycle effects (Buettner et al., 2015), and high sparsity caused by dropout events (Kharchenko et al., 2014), which can impact accuracy and robustness. Additionally, the availability of labeled scRNA-seq data corresponding to a GRN is limited, making it challenging to train models from scratch. Traditional unsupervised or self-supervised models, while not reliant on label information, often struggle to effectively handle the noise, dropouts, high sparsity, and high dimensionality characteristics of scRNA-seq data (Moerman et al., 2019; Matsumoto et al., 2017; Zeng et al., 2023). Supervised methods are also proposed for GRN reconstruction (Zhao et al., 2022; Shu et al., 2022; KC et al., 2019; Chen & Liu, 2022a) but struggle to handle batch effects and fail to leverage latent gene-gene interaction information effectively limiting their generalization capabilities.\nRecent advancements in large language models (LLMs) and the pre-training followed by fine-tuning paradigm (Devlin et al., 2019; OpenAI, 2023) have significantly contributed to the development of transformer-based architectures tailored for scRNA-seq data analysis (Yang et al., 2022; Cui et al., 2024; Chen et al., 2023; Theodoris et al., 2023). These models effectively leverage vast amounts of unlabeled scRNA-seq data to learn contextual representations and capture intricate latent interactions between genes. To address the limitations of the current methods, we effectively leverage one of these large-scale pre-trained transformer models, namely scBERT (Yang et al., 2022), which has been pre-trained on large-scale unlabelled scRNA-seq data to learn domain-irrelevant gene expression patterns and interactions from the whole genome expression. By fine-tuning scBERT on user specific scRNA-seq datasets, we can mitigate batch effects and capture latent gene-gene interactions for downstream tasks.\nWe propose an innovative knowledge-aware supervised GRN inference framework, scTransNet (see Figure 1), which integrates pre-trained single-cell language models with structured knowledge of GRNs. Our approach combines gene representations learned from scBERT with graph representations derived from the corresponding GRNs, creating a unified context-aware and knowledge-aware representation (Feng et al., 2020). This joint learning approach enables us to surpass the accuracy of current state-of-the-art methods in supervised GRN inference. By harnessing the power of pre-trained transformer models and incorporating biological knowledge from diverse data sources, such as gene expression data and gene regulatory networks, our approach paves the way for more precise and robust GRN inference. Ultimately, this methodology offers deeper insights into cellular regulatory mechanisms, advancing our understanding of gene regulation."}, {"title": "2. Related Work", "content": "Several methods have been developed to infer GRNs from scRNA-seq data, broadly categorized into unsupervised and supervised methods.\nUnsupervised methods primarily include information theory-based, model-based, and machine learning-based approaches. Information theory-based methods, such as mutual information (MI) (Margolin et al., 2006), Pearson correlation coefficient (PCC) (Salleh et al., 2015; Raza & Jaiswal, 2013), and partial information decomposition and context (PIDC) (Chan et al., 2017), conduct correlation analyses under the assumption that the strength of the correlation between genes is is positively correlated with the likelihood of regulation between them. Model-based approaches, such as SCODE (Matsumoto et al., 2017), involve fitting gene expression profiles to models that describe gene relationships, which are then used to reconstruct GRNs (Shu et al., 2021; Tsai et al., 2020).\nMachine learning-based unsupervised methods, like GENIE3 (Huynh-Thu et al., 2010) and GRNBoost2 (Moerman et al., 2019), utilize tree-based algorithms to infer GRNs. These methods are integrated into tools like SCENIC (Aibar et al., 2017; Van de Sande et al., 2020), employing tree rules to learn regulatory relationships by iteratively excluding one gene at a time to determine its associations with other genes. Despite not requiring labeled data, these unsupervised methods often struggle with the noise, dropouts, high sparsity, and high dimensionality typical of scRNA-seq data. Additionally, the computational expense and scalability issues of these tree-based methods, due to the necessity of segmenting input data and iteratively establishing multiple models, present further challenges for large datasets.\nSupervised methods, including DGRNS (Zhao et al., 2022), convolutional neural network for co-expression (CNNC) (Yuan & Bar-Joseph, 2019), and DeepDRIM (Chen et al., 2021), have been developed to address the increasing scale and inherent complexity of scRNA-seq data. Compared with unsupervised learning, supervised models are capable of detecting much more subtle differences between positive and negative pairs (Yuan & Bar-Joseph, 2019).\nDGRNS (Zhao et al., 2022) combines recurrent neural networks (RNNs) for extracting temporal features and convolutional neural networks (CNNs) for extracting spatial features to infer GRNs. CNNC (Yuan & Bar-Joseph, 2019) converts the identification of gene regulation into an image classification task by transforming the expression values of gene pairs into histograms and using a CNN for classification. However, the performance of CNNC (Yuan & Bar-Joseph, 2019) is hindered by the issue of transitive interactions. To address this, DeepDRIM (Chen et al., 2021) considers the information from neighboring genes and converts TF-gene pairs and neighboring genes into histograms as additional inputs, thereby reducing the occurrence of transitive interactions to some extent. Despite their success there exist certain limitations to the employment of CNN model-based approaches for GRN reconstruction. First of all, the generation of image data not only gives rise to unanticipated noise but also conceals certain original data features. Additionally, this process is time-consuming, and since it changes the format of scRNA-seq data, the predictions made by these CNN-based computational approaches cannot be wholly explained.\nIn addition to CNN-based methods, there are also other approaches such as GNE (Kc et al., 2019) and GRN-Transformer (Shu et al., 2022). GNE (gene network embedding) (Kc et al., 2019) is a deep learning method based on multilayer perceptron (MLP) for GRN inference applied to microarray data. It utilizes one-hot gene ID vectors from the gene topology to capture topological information, which is often inefficient due to the highly sparse nature of the resulting one-hot feature vector. GRN-Transformer (Shu et al., 2022) constructs a weakly supervised learning framework based on axial transformer to infer cell-type-specific GRNs from scRNA-seq data and generic GRNs derived from the bulk data.\nMore recently, graph neural networks (GNNs) (Wu et al., 2020), which are effective in capturing the topology of gene networks, have been introduced into GRN prediction methods. For instance, GENELink (Chen & Liu, 2022b) treats GRN inference as a link prediction problem and uses graph attention networks to predict the probability of interaction between two gene nodes. However, existing methods often suffer from limitations such as improper handling of batch effects, difficulty in leveraging latent gene-gene interaction information, and making simplistic assumptions, which can impair their generalization and robustness."}, {"title": "3. Approach", "content": "As shown in (Figure 1), our approach contains four parts: BERT encoding, Attentive Pooling, GRN encoding with GNNs and Output layer. The input scRNA-seq datasets are processed into a cell-by-gene matrix, $X \\in \\mathbb{R}^{N \\times T}$, where each element represents the read count of an RNA molecule. Specifically, for scRNA-seq data, the element denotes the RNA abundance for gene $t \\in \\{0,1,...,T\\}$ in cell $n \\in \\{0, 1, ..., N\\}$. In subsequent sections, we will refer to this matrix as the raw count matrix. Let us denote the sequence of gene tokens as $\\{g_1, \u2026, g_T\\}$, where T is the total number of genes."}, {"title": "3.1. BERT Encoding Layer", "content": "(Yang et al., 2022; Cui et al., 2024; Chen et al., 2023; Theodoris et al., 2023) show that pre-trained transformer models have a strong understanding of gene-gene interactions across cells and have achieved state-of-the-art results on a variety of single-cell processing tasks. We use scBERT (Yang et al., 2022) as the backbone, which is a successful pre-trained model with the advantage of capturing long-distance dependency as it uses Performer (Choromanski et al., 2022) to improve the scalability of the model to tolerate over 16,000 gene inputs.\nThe scBERT model adopts the advanced paradigm of BERT and tailors the architecture to solve single-cell data analysis.\nThe connections of this model with BERT are given as follows. First, scBERT follows BERT's revolutionary method to conduct self-supervised pre-training (Devlin et al., 2019) and uses Transformer as the model backbone (Choromanski et al., 2022). Second, the design of embeddings in scBERT is similar to BERT in some aspects while having unique features to leverage gene knowledge. From this perspective, the gene expression embedding could be viewed as the token embedding of BERT. As shuffling the columns of the input does not change its meaning (like the extension of BERT to understand tabular data with TaBERT (Yin et al., 2020)), absolute positions are meaningless for gene. Instead gene2vec is used to produce gene embeddings, which could be viewed as relative embeddings (Du et al., 2019) that capture the semantic similarities between any of two genes. Third, Transformer with global receptive field could effectively learn global representation and long-range dependency without absolute position information, achieving excellent performance on non-sequential data (such as images, tables) (Parmar et al., 2018; Yin et al., 2020).\nIn spite of the gene embedding, there is also a challenge on how to utilize the transcription level of each gene, which is actually a single continuous variable. The gene expression could also be considered as the occurrence of each gene that has already been well-documented in a biological system. Drawing from bag-of-words (Zhang et al., 2010) insight, the conventionally used term-frequency-analysis method is applied that discretizes the continuous expression variables by binning, and converts them into 200-dimensional vectors, which are then used as token embeddings for the scBERT model.\nFor each token $g_t$ in a cell, we construct its input representation as:\n$h_t = emb_{gene2vec}(g_t) + emb_{expr}(g_t)$  (1)\nwhere $emb_{gene2vec}(g_t)$ represents gene2vec embedding (Du et al., 2019) of gene $g_t$ analogous to position embedding in BERT and $emb_{expr}(g_t)$ represents expression embedding of the gene expression of $g_t$ analogous to token embedding in BERT.\nSuch input representations are then fed into L successive Transformer encoder blocks, i.e.,\n$h_l = Transformer(h^{l-1}), l = 1, 2, ..., L$, (2)\nso as to generate deep, context-aware representations for genes. The final hidden states $\\{h_t^L\\}_{t=1}^T$ are taken as the output of this layer (Devlin et al., 2019; Vaswani et al., 2023)."}, {"title": "3.2. Attentive Pooling", "content": "After extracting the BERT encodings we further utilize the attention scores across cells from the model to select the most representative cells for pooling of each gene representation. For each input gene token $g_t$ we get the embeddings for all cells denoted as $\\{h_t(n)\\}_{n=1}^N$, where N is the number of cells.\nThe quadratic computational complexity of the BERT model, with the Transformer as its foundational unit, does not scale efficiently for long sequences. Given that the number of genes in scRNA-seq data can exceed 20,000, this limitation becomes significant. To address this issue, scBERT employs a matrix decomposition variant of the Transformer, known as Performer (Choromanski et al., 2022), to handle longer sequence lengths. In a regular Transformer, the dot-product attention mechanism maps Q, K, and V, which are the encoded representations of the input queries, keys, and values for each unit. The bidirectional attention matrix is formulated as follows:\n$Att(Q, K,V) = D^{-1}(QK^T)V$, (3)\n$D = diag(QK^T1_L)$\nwhere Q = WqX, K = W_KX, V = WvX are linear transformations of the input X; $W_Q$, $W_K$ and $W_V$ are the weight matrices as parameters; $1_L$ is the all-ones vector of length L; and diag(.) is a diagonal matrix with the input vector as the diagonal.\nThe attention matrix in Performer is described as follows:\n$Att(Q, K, V) = \\tilde{D}^{-1}(Q'((K')^TV))$, (4)\n$\\tilde{D} = diag(Q' ((K')^T1_L))$\nwhere $Q' = \\phi(Q)$, $K' = \\phi(K)$, and the function $\\phi(x)$ is defined as:\n$\\phi(X) = \\frac{1}{\\sqrt{m}} e^{iw^TX}$ (5)\nwhere c is a positive constant, w is a random feature matrix, and m is the dimensionality of the matrix.\nThe attention weights can be obtained from equation 3, modified by replacing V with $V^{\\circ}$, where $V^{\\circ}$ contains one-hot indicators for each position index. All the attention matrices are integrated into one matrix by taking an element-wise average across all attention matrices in multi-head multi-layer Performers. In this average attention matrix for each cell, $A(i, j)$ represents how much attention from gene i was paid to gene j. To focus on the importance of genes to each cell n, the attention matrix is summed along the columns into an attention-sum vector $a_n$, and its length is equal to the number of genes. These attention scores of gene $g_t$ are obtained across cells and normalized denoted as $\\{a_n^t\\}_{n=1}^N$\nThese normalized scores are used for weighted aggregation of gene embeddings across cells. We aggregate each cell's gene representations together into one gene-level cell embedding such that the updated matrix is of the form $Z \\in \\mathbb{R}^{T \\times d}$, where d is the dimension of the output gene embedding.\n$Z_g[t] = \\sum_{n=1}^N h_t(n) \\alpha_t^n$ (6)"}, {"title": "3.3. GRN encoding with GNNS", "content": "In this module, we use raw count matrix as the features of the genes. Subsequently, we utilize graph convolutional network (GCN)-based interaction graph encoders to learn gene features by leveraging the underlying structure of the gene interaction graph.\nLet us denote the prior network as $G = \\{V, E\\}$, where V is the set of nodes and E is the set of edges. To perform the reasoning on this prior gene regulatory network G, our GNN module builds on the graph attention framework (GAT) (Velickovic et al., 2017), which induces node representations via iterative message passing between neighbors on the graph. In each layer of this GNN, the current representation of the node embeddings $\\{v_1^{l-1}, ..., v_T^{l-1}\\}$ is fed into the layer to perform a round of information propagation between nodes in the graph and yield pre-fused node embeddings for each node:\n$\\{\\tilde{v_1^l}, ..., \\tilde{v_T^l}\\} = GNN(\\{v_1^{l-1}, ..., v_T^{l-1}\\})$ (7)\nfor l = 1, ..., M\nSpecifically, for each layer l, we update the representation $\\tilde{v_t^l}$ of each node by\n$\\tilde{v_t^l} = f_n(\\sum_{v_s \\in N_t \\cup \\{v_t\\}} \\alpha_{st}m_{st}) + v_t^{l-1}$ (8)\nwhere $N_t$ represents the neighborhood of an arbitrary node $v_t$, $m_{st}$ denotes the message one of its neighbors $v_s$ passes to $v_t$, $\\alpha_{st}$ is an attention weight that scales the message $m_{st}$, and $f_n$ is a 2-layer MLP. The messages $m_{st}$ between nodes allow entity information from a node to affect the model's representation of its neighbors, and are computed in the following manner:\n$r_{st} = f_r(\\tilde{r_{st}}, u_s, u_t)$ (9)\n$m_{st} = f_m(v_s^{l-1}, u_s, r_{st})$ (10)\nwhere $u_s, u_t$ are node type embeddings, $\\tilde{r_{st}}$ is a relationembedding for the relation connecting $v_s$ and $v_t$, $f_r$ is a 2-layer MLP, and $f_m$ is a linear transformation. The attention weights $\\alpha_{st}$ scale the contribution of each neighbor's message by its importance, and are computed as follows:\n$q_s = f_q(v_s^{l-1}, u_s)$ (11)\n$k_t = f_k(v_t^{l-1}, u_t, r_{st})$ (12)\n$\\alpha_{st} = \\frac{exp(\\delta_{st})}{\\sum_{u_s \\in N_t \\cup \\{u_t\\}} exp(\\delta_{st})}, \\delta_{st} = \\frac{q_s k_t}{\\sqrt{D}}$ (13)\nwhere $f_q$ and $f_k$ are linear transformations and $u_s, u_t, r_{st}$ are defined the same as above."}, {"title": "3.4. Final Output Layer", "content": "In the final output layer, we concatenate the input gene representations $Z_g$ from the BERT encoding layer with the graph representation of each gene from GNN to get the final gene embedding.\nWe input these final embeddings of pairwise genes i and j into two channels with the same structure. Each channel is composed of MLPs to further encode representations to low-dimensional vectors which serve for downstream similarity measurement or causal inference between genes."}, {"title": "4. Experimental Setup", "content": ""}, {"title": "4.1. Benchmark scRNA-seq datasets", "content": "The performance of scTransNet is evaluated on two human cell types using single-cell RNA-sequencing (scRNA-seq) datasets from the BEELINE study (Pratapa et al., 2020): human embryonic stem cells (hESC (Yuan & Bar-Joseph, 2019)) and human mature hepatocytes (hHEP (Camp et al., 2017)). The cell-type-specific ChIP-seq ground-truth networks are used as a reference for these datasets. The scRNA-seq datasets are preprocessed following the approach described in the (Pratapa et al., 2020), focusing on inferring interactions outgoing from transcription factors (TFs). The most significantly varying genes are selected, including all TFs with a corrected P-value (Bonferroni method) of variance below 0.01. Specifically, 500 and 1000 of the most varying genes are chosen for gene regulatory network (GRN) inference. The scRNA-seq datasets can be accessed from the Gene Expression Omnibus (GEO) with accession numbers GSE81252 (hHEP) and GSE75748 (hESC). The evaluation compares the inferred gene regulatory networks to known ChIP-seq ground-truth networks specific to these cell types."}, {"title": "4.2. Implementation and Training details", "content": "Data Preparation We utilized the benchmark networks that containing labeled directed regulatory dependencies between gene pairs. These dependencies were classified as positive samples (labeled 1) if present in the network, and negative samples (labeled 0) if absent. Due to the inherent network density, the number of negative samples significantly outnumbered positive samples. To address the class imbalance, known transcription factor (TF)-gene pairs are split into training (2/3), and test (1/3) sets. Positive training samples were randomly selected from the known TF-gene pairs. Moreover, 10% of TF-gene pairs are randomly selected from training samples for validation. The remaining positive pairs formed the positive test set. Negative samples were generated using the following strategies: 1) Unlabeled interactions: All unobserved TF-gene interactions outside the labeled files were considered negative instances. 2) Hard negative sampling: To enhance model learning during training, we employed a uniformly random negative sampling strategy within the training set. This involved creating \"hard negative samples\" by pairing each positive sample (g1, g2) with a negative sample (g1, g3), where both share the same gene g1. This approach injects more discriminative information and accelerates training. 3) Information leakage prevention: Negative test samples were randomly selected from the remaining negative instances after generating the training and validation sets. This ensured no information leakage from the test set to the training process. The positive-to-negative sample ratio in each dataset was adjusted to reflect the network density i.e.\n$\\frac{Positive}{Negative} = \\frac{Network Density}{1- Network Density}$  (14)\nModel Training To account for the class imbalance, we adopted two performance metrics: Area Under the Receiver Operating Characteristic Curve (AUROC) and Area Under the Precision-Recall Curve (AUPRC). The supervised model was trained for 100 iterations with a learning rate of 0.003. The Graph Neural Network (GNN) architecture comprised two layers with hidden layer sizes of 256 and 128 units, respectively.\nEvaluation All reported results are based solely on predictions from the held-out test set. To ensure a fair comparison, identical training and validation sets were utilized for all evaluated supervised methods. This approach eliminates potential bias introduced by different data splits."}, {"title": "4.3. Baseline Methods", "content": "To assess the effectiveness of our model in predicting GRNs, we compare our model scTransNet against the existing baseline methods commonly used for inferring GRNs, as follows:\n\u2022 GNNLink (Mao et al., 2023) is a graph neural network model that uses a GCN-based interaction graph encoder to capture gene expression patterns.\n\u2022 GENELink (Chen & Liu, 2022b) proposes a graph attention network (GAT) approach to infer potential GRNs by leveraging the graph structure of gene regulatory interactions.\n\u2022 GNE (gene network embedding) (Kc et al., 2019) proposes a multilayer perceptron (MLP) approach to encode both gene expression profiles and network topology for predicting gene dependencies.\n\u2022 CNNC (Yuan & Bar-Joseph, 2019) proposes inferring GRNs using deep convolutional neural networks (CNNs).\n\u2022 DeepDRIM (Chen et al., 2021) is a supervised deep neural network that utilizes images representing the expression distribution of joint gene pairs as input for binary classification of regulatory relationships, considering both target TF-gene pairs and potential neighbor genes.\n\u2022 GRN-transformer (Shu et al., 2022) is a weakly supervised learning method that utilizes axial transformers to infer cell type-specific GRNs from single-cell RNA-seq data and generic GRNs.\n\u2022 Pearson correlation coefficient (PCC) (Salleh et al., 2015; Raza & Jaiswal, 2013) is a traditional statistical method for measuring the linear correlation between two variables, often used as a baseline for GRN inference.\n\u2022 Mutual information (MI) (Margolin et al., 2006) is an information-theoretic measure of the mutual dependence between two random variables, also used as a baseline for GRN inference.\n\u2022 SCODE (Matsumoto et al., 2017) is a computational method for inferring GRNs from single-cell RNA-seq data using a Bayesian framework.\n\u2022 GRNBoost2 (Moerman et al., 2019) is a gradient boosting-based method for GRN inference.\n\u2022 GENIE3 (Huynh-Thu et al., 2010) is a random forest-based machine learning method that constructs GRNS based on regression weight coefficients, and won the DREAM5 In Silico Network Challenge in 2010.\nThese methods represent a diverse range of approaches, including traditional statistical methods, machine learning techniques, and deep learning models, for inferring gene regulatory networks from various types of data, such as bulk and single-cell RNA-seq, as well as incorporating additional information like network topology and chromatin accessibility."}, {"title": "5. Results", "content": ""}, {"title": "5.1. Performance on benchmark datasets", "content": "The results (see Figure 2) demonstrate that scTransNet outperforms state-of-the-art baseline methods across all four benchmark datasets, achieving superior performance in terms of both AUROC and AUPRC evaluation metrics. Notably, scTransNet's AUROC values are approximately 5.4% and 7.4% higher on average compared to the second-best methods, namely GNNLink (Mao et al., 2023) and GENELink (Chen & Liu, 2022b), respectively. Similarly, sc-TransNet's AUPRC values show an impressive improvement of approximately 7.4% and 16% on average over GNNLink and GENELink, respectively.\nTo gain further insights, we analyzed scTransNet's final gene regulatory network (GRN) predictions and compared them with those from GENELink. Our analysis revealed that scTransNet effectively captured all the gene regulatory interactions predicted by GENELink. This finding suggests that by incorporating joint learning, scTransNet does not introduce additional noise to the predictive power of the graph representations. Instead, it enhances the predictive capability through the scBERT encoder in its architecture.\nFigure 3 provides a visualization of a partial subgraph of the ground truth GRN, highlighting the predictions made by scTransNet that were not captured by GENELink, which solely relies on graphs for predicting gene-gene interactions. Additionally, the figure visualizes the ground truth labels that scTransNet failed to capture. In summary, the comparative analysis demonstrates that scTransNet effectively captures all the regulatory interactions predicted by GENELink while leveraging joint learning to improve predictive performance. The visualization illustrates the additional interactions scTransNet could predict beyond GENELink, as well as the ground truth interactions it missed, providing insights into the strengths and limitations of the proposed method."}, {"title": "5.2. Discussion and Ablations", "content": "To evaluate the effectiveness of jointly learning from pre-trained scRNA-seq language models (Yang et al., 2022), which capture rich contextual representations, and Gene Regulatory Networks (GRNs), which encode structured biological knowledge, we compare the average Area Under the Receiver Operating Characteristic Curve (AUROC) and Area Under the Precision-Recall Curve (AUPRC) metrics with and without these encoders across the four human cell type benchmark datasets (Pratapa et al., 2020). The average AUROC and AUPRC scores are calculated across both the TFs+500 highly variable genes and TFs+1000 highly variable genes datasets for each human cell data type (i.e, hESC (human embryonic stem cells) and hHEP (human mature hepatocytes)). Additionally, we validate the importance of incorporating Attentive Pooling (Section 3.2) by contrasting the results when using average pooling of gene embeddings across cells instead of attentive pooling. Consistent parameter settings are employed across all four human cell benchmark datasets, with Cell-type-specific ChIP-seq network data serving as the ground truth.\nEffect of Graph Neural Network Component: The results demonstrate the significant impact of incorporating the Graph Neural Network (GNN) encoder component in the proposed method. With the GNN encoder, the average AUROC value across all the human cell type datasets is 87.5%, and the average AUPRC value is 68.7%. In contrast, without the GNN encoder, the average AUROC drops to 83.6%, and the average AUPRC decreases to 63.4%. The inclusion of the GNN encoder leads to an improvement of 4.6% in the average AUROC and a notable 8.3% increase in the average AUPRC. These results highlight the consistent performance enhancement provided by the GNN encoder across both AUROC and AUPRC metrics for the human cell type benchmark datasets. The GNN encoder plays a crucial role in the architecture as the task is formulated as a supervised Gene Regulatory Network (GRN) inference problem, aiming to identify potential gene regulatory dependencies given prior knowledge of the GRN. The GNN models the regulatory interactions as a graph, learning node representations that effectively encode the network topology and gene interdependencies present in the GRN, which serves as the primary source of biological knowledge. The results in Table 1 justify the use of this structural graph representation for understanding the complex regulatory networks in single-cell transcriptomics data.\nEffect of Pre-trained Single-Cell Transcriptomics Transformer: The removal of the scBERT encoder also leads to a drop in performance, with the average AUROC decreasing from 87.5% to 85.3%, and the average AUPRC declining from 68.7% to 66.2% across both cell types (see Table 1). The inclusion of scBERT representations improves the AUROC by 2.6% and the AUPRC by 3.8%. While the improvement is less significant compared to the GNN encoder, this is expected as the contextual representations from scRNA-seq data are learned through pre-training on millions of unlabeled single cells and then fine-tuned for the specific cell type. In addition to rich contextual representations, scBERT captures long-range dependencies between genes by leveraging self-attention mechanisms and pretraining on large-scale unlabeled scRNA-seq data (Pratapa et al., 2020). This comprehensive understanding of gene-gene interactions and semantic relationships allows for effective modeling of complex, non-linear gene regulatory patterns that extend beyond immediate neighbors in the gene regulatory network.\nThe contextual representations learned by the pre-trained Transformer facilitate the identification of intricate regulatory relationships that might be overlooked by traditional methods focused on local neighborhoods or predefined gene sets. The ability to capture global context and long-range dependencies is a key advantage of pre-trained single-cell Transformer models for deciphering the intricate gene regulatory mechanisms governing cellular states and identities. The improvement shown in Table 1 justifies the effectiveness of this approach.\nEffect of Attentive Pooling Mechanism: The impact of incorporating Attentive Pooling is evaluated by comparing the AUROC and AUPRC metrics with and without attentive pooling across four datasets. As shown in Table 1, the inclusion of attentive pooling results in a slight improvement, with a 1.6% increase in the average AUROC and a 9.6% increase in the average AUPRC. While the improvement is not significant, the experiments confirm that attentive pooling offers some support for the gene regulation task. We believe that the significance of attentive pooling will be more pronounced when scaling the method to larger datasets. The cell type data is sparse and of low quality. However, the attention weights learned from scBERT (Pratapa et al., 2020) demonstrate that the marker genes are automatically learned for each cell. Consequently, attentive pooling helps to effectively focus on high-quality cell data by removing noise. By employing an attentive pooling mechanism, scTransNet selectively focuses on the most informative cells for each gene, mitigating noise and filtering out irrelevant information, thereby enhancing the quality of the input data used for GRN inference."}, {"title": "6. Conclusion and Future Work", "content": "In this work, we propose scTransNet, a joint graph learning inference framework that integrates prior knowledge from known Gene Regulatory Networks (GRNs) with contextual representations learned by pre-trained single-cell transcriptomics Transformers. Our approach aims to effectively boost GRN prediction by leveraging the complementary strengths of structured biological knowledge and rich contextual representations. We evaluate our method on four human cell scRNA-seq benchmark datasets and demonstrate consistent improvements over current baselines in predicting gene-gene regulatory interactions. Our framework comprises four key modules: a GNN encoder to capture the network topology from known GRNs, a scBERT encoder to learn contextual representations from scRNA-seq data, an Attentive Pooling mechanism to focus on informative cells, and a Final Output layer for prediction. The synergistic combination of these modules is verified to be effective in accurately inferring gene regulatory dependencies.\nMoving forward, we plan to incorporate the knowledge integration process directly into the fine-tuning of the Transformer model, aiming to fuse information across layers more effectively. Additionally, we will evaluate our approach on various other datasets, including simulated datasets, to further validate its robustness and generalizability. Beyond GRN inference, we intend to investigate the advantages of jointly learning single-cell Transformers and structured biological knowledge for other cell-related tasks. These tasks include cell type annotation, identifying echo archetypes (Luca et al., 2021), and enhancing the interpretability of single-cell models. By leveraging the complementary strengths of contextual representations and structured knowledge, we aim to advance the understanding and analysis of complex cellular processes and regulatory mechanisms."}, {"title": "Impact Statement", "content": "\"This paper"}]}