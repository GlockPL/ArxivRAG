{"title": "DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any Architecture", "authors": ["Qianlong Xiang", "Miao Zhang", "Yuzhang Shang", "Jianlong Wu", "Yan Yan", "Liqiang Nie"], "abstract": "Diffusion models (DMs) have demonstrated exceptional generative capabilities across various areas, while they are hindered by slow inference speeds and high computational demands during deployment. The most common way to accelerate DMs involves reducing the number of denoising steps during generation, achieved through faster sampling solvers or knowledge distillation (KD). In contrast to prior approaches, we propose a novel method that transfers the capability of large pretrained DMs to faster architectures. Specifically, we employ KD in a distinct manner to compress DMs by distilling their generative ability into more rapid variants. Furthermore, considering that the source data is either unaccessible or too enormous to store for current generative models, we introduce a new paradigm for their distillation without source data, termed Data-Free Knowledge Distillation for Diffusion Models (DKDM). Generally, our established DKDM framework comprises two main components: 1) a DKDM objective that uses synthetic denois-ing data produced by pretrained DMs to optimize faster DMs without source data, and 2) a dynamic iterative distillation method that flexibly organizes the synthesis of denoising data, preventing it from slowing down the optimization process as the generation is slow. To our knowledge, this is the first attempt at using KD to distill DMs into any architecture in a data-free manner. Importantly, our DKDM is orthogonal to most existing acceleration methods, such as denoising step reduction, quantization and pruning. Experiments show that our DKDM is capable of deriving 2x faster DMs with performance remaining on par with the baseline. Notably, our DKDM enables pretrained DMs to function as \"datasets\" for training new DMs.", "sections": [{"title": "1 Introduction", "content": "The advent of Diffusion Models (DMs) heralds a new era in the generative modeling domain, garnering widespread acclaim for their exceptional capability in producing samples of remarkable quality. These models have rapidly ascended to a pivotal role across a spectrum of generative applications, notably in the fields of image, video and audio. However, the generation via those models is significantly slow because the sampling process involves iterative noise estimation over thousands of time steps, which poses a challenge for practical deployment, particularly for consumer devices.\nTo accelerate diffusion models, as illustrated in Figure 1 (b)&(c), existing methods can be categorized into two pathways: reducing denoising steps and speeding up inference process of denoising networks for each step. Compared with the standard generation of DMs as shown in Figure 1 (a), existing methods of the first category focus on reducing denoising steps of the lengthy sampling process. The second category focuses on reducing the inference time of"}, {"title": "2 Preliminaries on Diffusion Models", "content": "In diffusion models [Ho et al., 2020], a Markov chain is defined to add noises to data and then diffusion models learn the reverse process to generate data from noises.\nForward Process. Given a sample $x^{0} \\sim q(x^{0})$ from the data distribution, the forward process iteratively adds Gaussian noise for T diffusion steps with the predefined noise schedule $(\\beta_{1}, ..., \\beta_{T})$:\n$q(x^{t} | x^{t-1}) = N(x^{t}; \\sqrt{1 - \\beta_{t}}x^{t-1}, \\beta_{t}I)$,\n$q(x^{1:T}|x^{0}) = \\prod_{t=1}^{T} q(x^{t}|x^{t-1})$,\nuntil a completely noise $x^{T} \\sim N(0, I)$ is obtained. According to Ho et al. [2020], adding noise t times sequentially to the original sample $x^{0}$ to generate a noisy sample $x^{t}$ can be simplified to a one-step calculation as follows:\n$q(x^{t}|x^{0}) = N(x^{t}; \\sqrt{\\bar{\\alpha_{t}}}x^{0}, (1 - \\bar{\\alpha_{t}})I)$,\n$x^{t} = \\sqrt{\\bar{\\alpha_{t}}}x^{0} + \\sqrt{1 - \\bar{\\alpha_{t}}}\\epsilon,$\nwhere $\\alpha_{t} := 1 - \\beta_{t}, \\bar{\\alpha_{t}} := \\prod_{i=1}^{t} \\alpha_{t}$ and $\\epsilon \\sim N(0, I)$.\nReverse Process. The posterior $q(x^{t-1}|x^{t})$ depends on the data distribution, which is tractable conditioned on $x^{0}$:\n$q(x^{t-1}|x^{t}, x^{0}) = N(x^{t-1}; \\tilde{\\mu}_{t}(x^{t}, x^{0}), \\tilde{\\beta}_{t}I)$,\nwhere $\\tilde{\\mu}_{t}(x^{t}, x^{0})$ and $\\tilde{\\beta}_{t}$ can be calculated by:\n$\\tilde{\\mu}_{t}(x^{t}, x^{0}) := \\frac{1}{\\sqrt{\\alpha_{t}}}\\frac{\\beta_{t}}{1 - \\bar{\\alpha}_{t}} \\sqrt{\\alpha_{t-1}}x^{0} + \\frac{\\sqrt{\\alpha_{t}}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_{t}}x^{t}.$\n$\\tilde{\\beta}_{t} := \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_{t}}\\beta_{t}.$\nSince $x^{0}$ in the data is not accessible during generation, a neural network parameterized by $\\theta$ is used for approximation:\n$p_{\\theta}(x^{t-1}|x^{t}) = N(x^{t-1}; \\mu_{\\theta}(x^{t},t), \\Sigma_{\\theta}(x, t) I)$.\nOptimization. To optimize this network, the variational bound on negative log likelihood $E[-\\log p_{\\theta}]$ is estimated by:\n$L_{vlb} = E_{x^{0},\\epsilon,t}[D_{KL}(q(x^{t-1}|x^{t}, x^{0})||p_{\\theta}(x^{t-1}|x^{t})]$.\nHo et al. [2020] found that predicting $\\epsilon$ is a more efficient way when parameterizing $p_{\\theta}(x^{t}, t)$ in practice, which can be derived by Equation (1) and Equation (2):\n$\\mu_{\\theta}(x^{t},t) = \\frac{1}{\\sqrt{\\alpha_{t}}}(x^{t} - \\frac{\\beta_{t}}{\\sqrt{1 - \\bar{\\alpha}_{t}}}\\epsilon_{\\theta}(x^{t}, t)).$\nThus, a reweighted loss function is designed as the objective to optimize $L_{vlb}$:\n$L_{simple} = E_{x^{0},\\epsilon,t}[||\\epsilon - \\epsilon_{\\theta}(x^{t}, t)||^{2}]$.\nImprovement. In original DDPMs, $L_{simple}$ offers no signal for learning $\\epsilon_{\\theta}(x^{t}, t)$ and Ho et al. [2020] fixed it to $\\beta_{t}$ or $\\tilde{\\beta}_{t}$. Nichol and Dhariwal [2021] found it to be sub-optimal and proposed to parameterize $\\Sigma_{\\theta}(x, t)$ as a neural network whose output $v$ is interpolated as:\n$\\Sigma_{\\theta}(x,t) = \\exp(v\\log \\beta_{t} + (1 - v)\\log \\tilde{\\beta}_{t})$.\nTo optimize $\\Sigma_{\\theta}(x, t)$, Nichol and Dhariwal [2021] use $L_{vlb}$, in which a stop-gradient is applied to the $p_{\\theta}(x^{t}, t)$ because it is optimized by $L_{simple}$. The final hybrid objective is defined as:\n$L_{hybrid} = L_{simple} + \\lambda L_{vlb}$,\nwhere $\\lambda$ is used for balance between the two objectives."}, {"title": "3 Data-Free Knowledge Distillation for Diffusion Models", "content": "In this section, we introduce a novel paradigm, termed Data-Free Knowledge Distillation for Diffusion Models (DKDM). Section 3.1 details the DKDM paradigm, focusing on two principal challenges: the formulation of the optimization objective and the acquisition of denoising data for distillation. Section 3.2 describes our proposed optimization objective tailored for DKDM. Section 3.3 details our proposed method for effective collection of denoising data."}, {"title": "3.1 DKDM Paradigm", "content": "The DKDM paradigm represents a novel data-free KD approach for DMs. Unlike conventional KD methods, DKDM aims to leverages KD to transfer the generative capabilities of DMs to models with any architecture, while eliminating the need for access to large or proprietary datasets. This approach poses two primary challenges: 1) optimizing DMs through synthetic denoising data instead of source data, and 2) devising methods to flexibly collect denoising data for KD as the generation is slow.\nIn standard training of DMs, as depicted in Figure 2 (a), a training sample $x^{0} \\sim D$ is selected along with a timestep $t \\sim [1, 1000]$ and random noise $\\epsilon \\sim N(0, I)$. The input $x^{t}$ is computed using Equation (1), and the denoising network is optimized according to Equation (6) to generate outputs close to $\\epsilon$. However, without dataset access, DKDM cannot obtain training data $(x^{t}, t, \\epsilon)$ to employ this standard method. A straightforward approach for DKDM, termed the intuitive baseline and depicted in Figure 2 (b), involves using DMs pretrained on D to generate a synthetic dataset $D'$, which is then used to train new DMs with varying architectures. Despite its simplicity, creating $D'$ is time-intensive and impractical for large datasets.\nWe propose an effective framework for DKDM paradigm, outlined in Figure 2 (c), which incorporates a DKDM Objective (described in Section 3.2) and a strategy for collecting denoising data $B_{t}$ during optimization (detailed in Section 3.3). This framework addresses the challenges of distillation without source dataset and reduces the costs associated with the intuitive baseline, since the synthetic $B_{t}$ requires much less computation than $D'$."}, {"title": "3.2 DKDM Objective", "content": "Given a dataset D, the original optimization objective for a diffusion model with parameters $\\theta$ involves minimizing the KL divergence $E_{x^{0},\\epsilon,t}[D_{KL}(q(x^{t-1}|x^{t},x^{0})||p_{\\theta}(x^{t-1}|x^{t}))]$. Our proposed DKDM objective encompasses two primary goals: (1) eliminating the diffusion posterior $q(x^{t-1}|x^{t},x^{0})$ and (2) removing the diffusion prior $x^{0} \\sim q(x^{t}|x^{0})$ from the KL divergence, since they both are dependent on $x^{0}$ from the dataset D.\nEliminating the diffusion posterior $q(x^{t-1}|x^{t}, x^{0})$. In our framework, we introduce a teacher DM with parameters $\\Theta_{T}$, trained on dataset D. This model can generate samples that conform to the learned distribution $D'$. Optimized with the objective (6), the distribution $D'$ within a well-learned teacher model closely matches D. Our goal is for a student DM, parameterized by $\\Theta_{S}$, to replicate $D'$ instead of $D'$, thereby obviating the need for q during optimization.\nSpecifically, the pretrained teacher model is optimized via the hybrid objective in Equation (6), which indicates that both the KL divergence $D_{KL}(q(x^{t-1}|x^{t}, x^{0})||p_{\\Theta_{T}}(x^{t-1}|x^{t}))$ and the mean squared error $E_{x^{t},\\epsilon,t}[||\\epsilon-\\epsilon_{\\Theta_{T}}(x, t)||^{2}]$ are minimized. Given the similarity in distribution between the teacher model and the dataset, we propose a DKDM objective that optimizes the student model through minimizing $D_{KL}(p_{\\Theta_{T}}(x^{t-1}|x^{t})||p_{\\Theta_{S}}(x^{t-1}|x^{t}))$ and $E_{x^{t}}[||\\epsilon_{\\Theta_{T}}(x_{t}, t) - \\epsilon_{\\Theta_{S}}(x_{t}, t)||^{2}]$. Indirectly,"}, {"title": "3.3 Efficient Collection of Denoising Data", "content": "In this section, we present our efficient strategy for gathering denoising data for distillation, illustrated in Figure 3. We begin by introducing a basic iterative distillation method that allows the student model to learn from the teacher model at each denoising step, instead of requiring the teacher to denoise multiple times within every training iteration to create a batch of noisy samples for the student to learn once. Subsequently, to enhance the diversity of noise levels within the batch samples, we develop an advanced method termed shuffled iterative distillation, which allows the student to learn denoising patterns across varying time steps. Lastly, we refine our approach to dynamic iterative distillation, significantly augmenting the diversity of data in the denoising batch. This adaptation ensures that the student model acquires knowledge from a broader array of samples over time, avoiding repetitive learning from identical samples.\nIterative Distillation. We introduce a method called iterative distillation, which closely aligns the optimization process with the generation procedure. In this approach, the teacher model consistently denoises, while the student model continuously learns from this denoising. Each output from the teacher's denoising step is incorporated into some batch for optimization, ensuring the student model learns from every output. Specifically, during each training iteration, the teacher performs $g_{\\Theta_{T}}(x^{t}, t)$, which is a single-step denoising, instead of $G_{\\Theta_{T}}(\\epsilon, T)$, which would involve t-step denoising. Initially, a batch $B_{1} = \\{x_{t}^{i}\\}$ is formed from a set of sampled noises $\\epsilon \\sim N(0, I)$. After one step of distillation, the batch $B_{2} = \\{x_{t-1}^{i}\\}$ is used for training. This process is iterated until $B_{t} = \\{x_{0}^{i}\\}$ is reached, indicating that the batch has nearly become real samples with no noise. The cycle then"}, {"title": "4 Experiments", "content": "This section details extensive experiments that demonstrate the effectiveness of our proposed DKDM. In Section 4.1, we establish appropriate metrics and baselines for evaluation. Section 4.2 compares the performance of baselines and our DKDM with different architectures. Additionally, we show that our DKDM can be combined with other methods to accelerate DMs. Finally, Section 4.3 describes an ablation study that validates the effectiveness of our proposed dynamic iterative distillation."}, {"title": "4.1 Experiment Setting", "content": "Datasets and teacher diffusion models. Our DKDM paradigm inherently eliminates the necessity for datasets. However, the pretrained teacher models employed are trained on specific datasets. We utilize three distinct pretrained DMs as teacher models, following the configurations introduced by Ning et al. [2023], and these models are all based on convolutional architecture. These models have been pretrained on CIFAR10 at a resolution of 32 \u00d7 32, CelebA at 64 \u00d7 64 and ImageNet at 32 \u00d7 32.\nMetrics. The distance between the generated samples and the reference samples can be estimated by the Fr\u00e9chet Inception Distance (FID) score [Heusel et al., 2017]. In our experiments, we utilize the FID score as the primary metric for evaluation. Additionally, we report sFID, Inception Score (IS) as secondary metrics. Following previous work , we generate 50K samples for DMs and we use the full training set in the corresponding dataset to compute the metrics. Without additional contextual states, all the samples are generated through 50 Improved DDPM sampling steps and the speed is measured by the average time taken to generate 256 images on a single NVIDIA A100 GPU. All of our metrics are calculated by ADM TensorFlow evaluation suite.\nBaseline. As the DKDM is a new paradigm proposed in this paper, previous methods are not suitable to serve as baselines. Therefore, in the data-free scenario, we take the intuitive baseline depicted in Figure 2 (a) as the baseline. Specifically, the teacher model consumes a lot of time to generate a substantial number of high-quality samples, equivalent in quantity to the source dataset, through 1,000 DDPM denoising steps. These samples then serve as the synthetic dataset (D') for the training of randomly initialized student models, following the standard training (Algorithm 2 in Appendix A). We use the performance obtained from this method as our baseline for comparative analysis."}, {"title": "4.2 Main Results", "content": "Effectiveness. Table 1 shows the performance comparison of our DKDM and baselines. Our DKDM consistently outperforms the baselines, demonstrating superior generative quality when maintaining identical architectures for the derived DMs. This performance validates the efficacy of our proposed DKDM objective and dynamic iterative distillation approach. The improvement over baselines is attributed to the complexity of the reverse diffusion process, which baselines struggle to learn, whereas the knowledge from pretrained teacher models is easier to learn, highlighting the advantage of our DKDM. Additionally, DKDM facilitates the distillation of generative capabilities into faster and more compact models, as evidenced by the 2\u00d7 faster architectures evaluated. The parameter count and generative speed are detailed in Table 2, with further information on hyperparameters and network architecture available in Appendix B.1. Appendix C includes some exemplary generated samples, illustrating that the student DMs, derived through DKDM, are capable of producing high-quality images. Nevertheless, a limitation noted in Table 1 is that the performance of these student DMs falls behind their teacher counterparts, which will be discussed further in Section 5.\nWe further evaluated the performance of these student models across a diverse range of architectures. Specifically, we tested five different model sizes by directly specifying the architecture, bypassing complex methods like neural architecture search. Both the teacher and student models employ Convolutional Neural Networks (CNNs) and the results are shown in Figure 4(a). Detailed descriptions of these architectures are available in AppendixB.2. Typically, we distilled a 14M model from a 57M teacher model, maintaining competitive performance and doubling the generation speed. Additionally, the 44M and 33M student models demonstrated similar speeds, suggesting that DKDM could benefit from integration with efficient architectural design techniques to further enhance the speed and quality of DMs. This aspect, however, is beyond our current scope and is designated for future research.\nCross-Architecture Distillation. Our DKDM transcends specific model architectures, enabling the distillation of generative capabilities from CNN-based DMs to Vision Transformers (ViT) and vice versa. We utilized DiT [Peebles and Xie, 2023] for ViT-based DMs to further affirm the superiority of our approach. Detailed structural descriptions are available in Appendix B.3. For experimental purposes, we pretrained a small ViT-based DM to serve as the teacher. As shown in Table 3, DKDM effectively facilitates cross-architecture distillation, yielding superior performance compared to baselines. Additionally, our results suggest that CNNs are more effective as compressed DMs than ViTs.\nCombination with Orthogonal Methods. The DMs derived by our DKDM are compatible with various orthogonal methods, such as denoising step reduction, quantization, pruning and so on. Here we conducted experiments to integrate the DDIM method, which reduces denoising steps, as illustrated in Figure 4 (b). This integration demonstrates that DDIM can further accelerate our derived student models, although with some performance trade-offs."}, {"title": "4.3 Ablation Study", "content": "To validate our designed paradigm DKDM, we tested FID score of our progressively designed methods, including iterative, shuffled iterative, and dynamic iterative distillation, over 200K training iterations. For the dynamic iterative distillation, the parameter p was set to 0.4. The results, shown in Figure 5 (a), demonstrate that our dynamic iterative distillation strategy not only converges more rapidly but also delivers superior performance. The convergence curve for our method closely"}, {"title": "5 Discussion and Future Work", "content": "The primary concept of our proposed DKDM paradigm is illustrated in Figure 6. In this paradigm, the teacher DM $\\Theta_{T}$, is trained on a real dataset D, which follows the distribution $D'$. There are two critical relationships between D and $D'$. First, the distribution $D'$ of a well-trained teacher closely approximates D. Second, the FID scores, when computed using D as a reference, correlate with those using $D'$, as demonstrated by the linear fitting in Figure 6. This correlation underpins the effectiveness of the DKDM paradigm. By transferring the distribution $D'$ from the teacher DM to a lighter student DM, DKDM enables the student to generate data whose distribution closely approximates D.\nHowever, in practice, there is invariably some discrepancy between D and $D'$, limiting the performance of the student model. We report these scores, denoted as FID' and sFID', calculated over the distribution $D'$ instead of D in Table 4. The results indicate that the FID' and sFID' scores of the student closely mirror those of the teacher, suggesting effective optimization. Nevertheless, these scores are inferior to those of the teacher, primarily due to the gap between D and $D'$. A potential solution to enhance DKDM involves improving the generative capabilities of the teacher, which we leave as a direction for future work."}, {"title": "6 Conclusions", "content": "In this paper, we introduce DKDM, a novel paradigm designed to efficiently distill the generative abilities of pretrained diffusion models into more compact and faster models with any architecture. Our experiments demonstrate the effectiveness of DKDM across three datasets, showcasing its ability to compress models to various sizes and architectures. A key advantage of our method is its ability to perform efficient distillation without direct data access, significantly facilitating ongoing research and development in the field. Moreover, our DKDM is compatible with most existing methods for accelerating diffusion models and can be integrated with them."}, {"title": "D Analysis: Random Discard", "content": "During our exploration, we discovered that the utilization of the Random Discard technique proves to be a straightforward yet highly effective approach for enhancing the distillation process. The idea behind it involves the random elimination of some batch of noisy samples generated by the teacher model during the iterative distillation. For instance, in iterative distillation during the initial five training iterations, batches B1, B3, B4 may be discarded, while B2, B5 are utilized for the student's learning. We present an analysis of the impact of random discarding in our devised methodologies. Specifically, we introduce the parameter p to denote the probability of discarding certain noisy samples. Subsequently, we apply varying discard probabilities to the iterative distillation, shuffled iterative distillation, and dynamic iterative distillation, and assess their respective performance alterations over a training duration of 200k iterations. The outcomes are presented in Figure 8. It is noteworthy that both iterative distillation and shuffled iterative distillation face limitations in constructing flexible batches, where random discard emerges as a noteworthy solution to enhance their efficacy. Conversely, for dynamic iterative distillation, when p attains a sufficiently large value, it becomes apparent that random discard fails to confer additional advantages. This observation underscores the inherent stability of our dynamic iterative distillation method and we ultimately omitted random discard from the final implementation. This is beneficial because of its inefficiency in requiring the teacher model to prepare a larger number of noisy samples."}, {"title": "E Social Impact", "content": "As an effective method for compressing and accelerating diffusion models, we believe that DKDM has the potential to significantly reduce the deployment costs associated with these models, thereby facilitating more widespread use of diffusion models for generating desired content. However, it is imperative to acknowledge that, as generative models, diffusion models, while offering creative applications across various scenarios, may also engender consequences such as the production of dangerous or biased content.\nOur DKDM is capable of mimicking the generative capabilities of a wide array of existing diffusion models without accessing the source datasets, which leads to derived models inheriting the flaws and limitations of these pre-existing models. For instance, if the training data of a pre-trained diffusion model contains sensitive or personal information collected without explicit consent, derived models may still risk leaking this data. Consequently, the potential societal harms of our approach primarily hinge on the negative impacts brought about by the existing diffusion models themselves. Addressing how to mitigate the adverse effects inherent in diffusion models remains a critical area of research."}]}