{"title": "Smirk: An Atomically Complete Tokenizer for Molecular Foundation Models", "authors": ["Alexius Wadell", "Anoushka Bhutani", "Venkatasubramanian Viswanathan"], "abstract": "Foundation models have become an important part of scientific discovery, with molecular foundation models leading the way in accelerating molecular design, material science, and cheminformatics. However, current models are constrained by closed-vocabulary tokenizers that fail to capture the full diversity of molecular structures. In this work, we systematically evaluate thirteen chemistry-specific tokenizers for their coverage of the SMILES language, uncovering substantial gaps. Using N-gram language models, we accessed the impact of tokenizer choice on model performance and quantified the information loss of unknown tokens. We introduce two new tokenizers, smirk and smirk-gpe, which can represent the entirety of the OpenSMILES specification while avoiding the pitfalls of existing tokenizers. Our work highlights the importance of open-vocabulary modeling for molecular foundation models and the need for chemically diverse benchmarks for cheminformatics. The proposed tokenizer provides a systematic", "sections": [{"title": "Introduction", "content": "framework to incorporate nuclear, electronic, and geometric degrees of freedom, which are critical in navigating chemical spaces for applications in pharmacology, agriculture, biology, and energy storage.\nPredicting chemical properties accurately and quickly is of great interest to numerous industries, in particular, to accelerate the discovery and design of next-generation energy storage devices. 1,2 Recently, machine learning has emerged as a computationally tractable and accurate method that can mitigate the high cost of experimentation or molecular simulation. To this end, numerous techniques have been devised, ranging from graph-neural\nnetworks, 4 equivariant networks, 5,6 or interatomic potentials.7 With the success of the transformer architecture for natural language processing, 8-10 recent efforts have sought to apply the architecture to chemical property prediction, 11-14 inverse molecular design, 15 retrosynthesis 16 and general chemistry tasks.17 At their core, these models operate by encoding molecules as text, which is then fed into models designed for natural language process-ing. 8-10 In the case of encoder-only architectures, the objective is to develop a chemically meaningful embedding from the provided input encoding; 11,12 while decoder-based models can be trained to predict the products of a chemical reaction 18,19 or design molecules with a particular property. 15\nBefore the transformer can do anything, the input molecule must be converted into a sequence of tokens that can then be fed into the model. For natural language process-ing, tokenization is predominantly handled by subword methods, namely byte-pair encoding (BPE). 20-22 Subword tokenization was developed to enable open vocabulary modeling with a fixed vocabulary size by interpolating between character-level and word-level units. 20,22,23 In contrast, chemistry language models have converged on \u201catomic-wise\u201d tokenization schemes splitting molecules into atom-level \u201cwords\" using a regular expression and then looking up each \"words\u201d in fixed vocabulary of tokens. 11,12,15,16,18,19 In particular, Schwaller et al. while developing an encoder/decoder recurrent neural network to predict the products of chem-ical reactions, 24 proposed the following regular expression to split the SMILE string into atom-level \"words\" for tokenization:\n(\\[[^\\]]+]|Br?|C1?|N|0|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.\n|=|#|-|\\+|\\\\/|:|~|@|\\?>|\\*|\\$|\\%[0-9]{2}|[0-9])\nFor example, dimethyl carbonate, a commonly used electrolyte, would be encoded as COC(=O)OC and tokenized as COC = 0 ) O C. Since then, their tokenization scheme has been used extensively throughout the literature for models 11,15,16,19,25\u201328 and cheminformatic frameworks. 12,16,29\nBagal et al. used generative pre-training (GPT) with masked self-attention to trained MolGPT, a decoder-only transformer model, to generate novel drug-like molecules. 15 Molecules were encoded as SMILES strings and pre-tokenized using the scheme proposed by Schwaller et al. and then tokenized using a dictionary vocabulary of 26 or 94 tokens for the MOSES or GuacaMol datasets, respectively.30 As a closed-vocabulary model, the MolGPT cannot represent or generate SMILES encodings not contained in its vocabulary. For example, both vocabularies lack tokens for : and $, preventing the tokenization or generation of gallium arsenide ([Ga+]$[As-]) among others.\nRoss et al. adopted the above pre-tokenization scheme for MoLFormer, a decoder-only model pre-trained on 1.1 billion molecules from the ZINC and PubChem datasets before being fine-tuned for property prediction and classification tasks.11 Their vocabulary consists of 2,362 tokens, including 101 tokens for carbon, constructed from the ZINC and PubChem datasets.\nLi and Fourches introducing Smiles-Pair Encoding (SPE), a variant of the byte-pair en-coding (BPE) to combine frequently co-occurring SMILES segments tokens into a single meta-token. Under SPE, molecules are first pre-tokenized using the regular expression pro-posed by Schwaller et al. and then tokenized using an initial dictionary of 77 tokens. A sequence of merge rules is applied to the tokenized sequence to replace pairs of tokens with a single meta-token for a total vocabulary of 3079 tokens. Unlike BPE, which operates at the character or byte level, SPE operates at the pre-token level. Thus avoiding the need for merges for two character elements (i.e., Cl, Br) or chemically invalid partial tokens (i.e., 1 or r). By combining pairs of tokens, the final tokenization can be significantly shorter, Li and Fourches reported a mean token length of 6 with SPE vs. 40 for atom-level tokenization on the ChEMBL25 dataset. 25\nTransPolymer13 applied an encoder-only transformer to the task of polymer property predictions. Polymer components were encoded using SMILES, with additional descriptor tokens to model the polymer systems. TransPolymer used RoBERTa's pre-trained BPE\ntokenizer with a novel regular expression to pre-tokenize SMILES strings. Unlike the reg-ularization expression proposed by Schwaller et al., TransPolymer does not differentiate between bracketed and non-bracketed elements. As a result, TransPolymer uses the same token for carbon bonded to aromatic oxygen and cobalt: Co.\nChemBERTa 12 used a BPE tokenizer with a vocabulary of 767 tokens for their base model. Additionally, Chithrananda et al. trained a variant using DeepChem's SmilesTok-enizer31 based on the tokenization scheme proposed by Schwaller et al.. They found a slight improvement in downstream performance using SmilesTokenizer, but advocated for addi-tional benchmarking of both tokenizers. 12 ChemBERTa-2 used \u201ca dictionary of the most common SMILES characters\u201d containing 591 tokens.27 As of the time of publication, the ChemBERTa-2 tokenizer is not publicly available and thus was not included in our analysis (September 2024).\nReactionT518 trained a T532 model for reaction prediction using an unigram tokenizer. 33 They used separate datasets for training their model (ORD34) and tokenizer (ZINC35). Unigram tokenizers are capable of open-vocabulary modeling; however, due to a limited alphabet, both ReactionT5 tokenizers emitted unknown tokens (fig. 1c). SELFormer14 is a ROBERTa model for molecular property prediction using SELFIES 36 to encode molecules as text. SELFormer opted for a byte-level BPE tokenizer instead of an atom-level or character-level tokenizer. Typically, BPE enables an open-vocabulary model of language. However, SELFormer's vocabulary is incomplete, resulting in unknown tokens even for valid input encodings (fig. 1c)."}, {"title": "Methods", "content": "In this section, we describe the metrics used to evaluate the impact of tokenizer choice for chemistry language models and our proposed tokenizers, smirk and smirk-gpe. Further details can be found in the Supporting Information."}, {"title": "Tokenizer Coverage of SMILES", "content": "We evaluated fifteen chemistry-specific tokenizers for their ability to tokenizer chemical prim-itives (i.e., elements, bonds, rings) and existing chemistry benchmarks (MoleculeNet 37). We tabulated the out-of-vocab frequency for each tokenizer on each set of SMILES (fig. 1c) or the percentage of SMILES strings that, when tokenized, included the unknown token. For example, MolGPT's tokenizer for the MOSES dataset includes F but not [F-], and thus tokenizes fluoride as [UNK].15 Since the creation of SMILES, 38 numerous expansions and variants of the SMILES language have emerged. 38-43 We used the OpenSMILES specifica-tion to select our SMILES primitives as it provides a formal definition of its grammar and is generally consistent with other standards. 39\nWe started by enumerating all 126 single-atom OpenSMILES strings, spanning the 118 elements and eight aromatic symbols. Items were marked as out-of-vocabulary if the to-kenization included the unknown token. We expanded this initial set to include isotopes, chiral symbols (limited to @ and @@), oxidation states, and the product of all three (Charged, Chiral Isotopes in fig. 1c). We used the oxidation states for an element as a proxy for the set of possible charged states of an atom; striking a balance between what is chemically permissible and the OpenSMILES requirement to support any change from -15 to +15.39\nWe limited our evaluation of chiral symbols to and @@, due to the rarity of the other chiral symbols: tetrahedral (@TH, allene (@AH), square-planar (@SP), trigonal-bipyramidal (@TB), and octahedral (@OH). For rings, we checked all single-digit (c1ccccc1) and double-digit (c%00ccccc%00) benzene rings as permitted by the OpenSMILES specification. For bonds, we evaluated tokenizer's on exemplar molecules consisting of two carbon atoms (C:C) with the desired bond.\nAs shown in fig. 1c, most existing chemistry models cannot represent all elements. Sup-port for quad C$C and aromatic C:C bonds was also limited even for models using Schwaller et al.'s regular expression; despite both symbols being explicitly included in the regular ex-pression. The coverage of isotopes (e.g., [12C]) and chirality (e.g., [C@]) is similarly limited,\nwith most models successfully tokenizing less than half of both sets. Interestingly, cover-age of MoleculeNet remains robust, highlighting the need for cheminformatics benchmark datasets covering a wider range of chemical spaces.\nFor the models using the tokenization scheme proposed by Schwaller et al. out-of-vocabulary molecules are a direct result of tokenizing all bracketed atoms as a single pre-token. Brack-eted atoms are information-rich, encoding isotopes, chiral centers, charge, species, hydrogen bonds and more. 39 Unless a tokenizer's vocabulary contains an entry for every possible com-bination of these features, it will emit unknown tokens; triggering a combinatorial explosion in vocabulary size. For example, carbon would require approximately 75,600 tokens to en-code all possible combinations of isotopes (14), oxidation states (9), aromaticity (2), chirality (60), and hydrogen bonds (5); 32x times larger that MoLFormer's entire vocabulary.\nFor the SELFormer and ReactionT5 tokenizers, out-of-vocabulary molecules result from an incomplete vocabulary, specifically an incomplete alphabet. All three models lack a token for U or u, preventing the representation of gold (Au), plutonium (Pu), copper (Cu), ruthe-nium (Ru), europium (Eu), lutetium (Lu) or uranium (U). Consequently, neither tokenizer achieves full coverage of the OpenSMILES specification, despite both tokenizers (BPE and Unigram, respectively) being capable of open-vocabulary modeling. 14,18"}, {"title": "smirk", "content": "To overcome the potential for out-of-vocab tokens, we propose further tokenizing bracketed compounds into their constituent glyphs; for example, OC[C\u00a9\u00a9H] [OH] would be tokenized as OC[C@@H] [OH]. Thus eliminating the need for an immense vocabulary enumerating all possible bracketed atoms. Our scheme also eliminates the training step of current chemistry-specific tokenizers; the set of glyphs used by OpenSMILES is fixed and documented by the specification. 39 In contrast to Schwaller et al., we elected to have separate tokens for @@ (clockwise chiral) and @ (anticlockwise chiral) and to tokenize two digit rings as % 1 2 instead of %12."}, {"title": "Glyph Pair Encoding", "content": "The resulting vocabulary consists of 167 tokens and, by construction, can faithfully to-kenize any OpenSMILES encoded molecule (fig. 1c). Like existing atom-wise tokenizers, smirk is implemented using a pre-tokenization step to split the input SMILES encoding, combined with a dictionary model to map individual glyphs to integers. Unlike current atom-wise tokenizers, our pre-tokenization scheme consists of two steps: first, splitting the SMILES string using a regular expression similar to Schwaller et al., and then decompos-ing bracketed atoms using another regular expression. The two-step process is necessary to distinguish between Cn representing carbon bounded to aromatic nitrogen and [Cn] for copernicium. Both regular expressions are described and documented in the supplementary information. We have implemented the proposed tokenization scheme in Rust using Hug-gingFace's Tokenizers 44 library and have made the code and prebuilt-wheels openly available at https://github.com/BattModels/smirk and on Py\u0420\u0406.\nThe above scheme is not without its drawbacks. The average number of tokens per molecule, or fertility, is higher for smirk than current atom-wise tokenizers (fig. 4); using at least three tokens [ Au] instead of one for any bracketed atom. Recent works have found a correlation between lower fertility and higher model performance, suggesting that smirk's increase in fertility is not desirable. 45,46 However, due to the relative rarity of bracketed atoms on average smirk's fertility is not grossly larger than that of other atom-wise tokenizers (fig. 4).\nTo address this, we also implemented smirk-gpe which further compresses smirk's to-kenization using a variant of BPE that operates at the glyph level instead of the byte or character level of existing BPE implementations. 44,47 As with BPE, merges between adja-cent tokens are replaced with a single meta-token using merge rules learned from a training corpus. 21,22 Unlike BPE, merges rules operate on token ids and not pairs of strings, thus merge(C, n) \u2260 Cn. Li and Fourches's SmilesPE tokenizer operates similarly, but similar to existing chemistry-specific tokenizers does not subdivide bracketed elements. As such, Smile-SPE avoids the need to differentiate between a merged Cn and Cn, but risks emitting the unknown token for bracketed atoms. Similar issues arise between [OH] and [C@OH1]; the former OH is an oxygen-hydrogen bond, while the latter indicates an octahedral chiral center.\nThe smirk-gpe tokenizer was trained on 262,035,601 molecules from the training split of our pre-training dataset derived from the Enamine REAL Space dataset. 48 To reduce the number of unique \u201cwords\u201d the training algorithm considers, we split SMILES strings on rings, brackets, non-bonds, and bracketed atoms using a regular expression. We set a target vocabulary size of 50k tokens; however, training stopped at 2.3k tokens after exhausting all possible merges. We have not explored the impact of vocabulary size 49 or corpus size 45 on tokenization performance."}, {"title": "N-Grams as a Metric for Tokenizer Performance", "content": "Despite the ubiquity of tokenization in language modeling, assessing the contribution of a tokenizer to overall model performance remains a nascent area of research. 20,45,50,51 In part due to the high cost and confounding nature of assessing tokenizer performance via overall model performance. 45 To overcome this, prior works have proposed a variety of intrinsic met-rics, primarily targeting sub-word tokenization. 45,46,49,52 For example, fertility is the average number of tokens per word. 49,52 Reducing fertility has immediate benefits as the cost of com-puting dot-product attention grows quadratically with sequence length. 53 Recent works have found a correlation between lower fertility and higher model performance, 45,46 suggesting a more compressive tokenizer may improve overall model performance.\nGowda and May, recommend limiting the size of the vocabulary such that 95% of tokens have at least 100 occurrences. 49 For our corpus of 1.6B molecules, this works out to a surprise of - lnp \u2248 15 nats, assuming rare tokens occur at most once per molecule. As shown in fig. 2, while the majority of observed tokens are far more frequent, I < 15; only a fraction of the total vocabularies are utilized. This reflects the targeted use case: Llama, GPT, Gemma, and TransPolymer use tokenizers designed for natural language, not chemistry. Atom-wise\ntokenizers used vocabularies ranging from 36 (MolGPT-MOSES) to 2,362 (MoLFormer) tokens, but aside from smirk-gpe, utilized fewer than 100 tokens. This is partially intentional; Smirk supports all 127 elements, most of which are not found in the corpus. However, it also reflects the combinatorial explosion caused by tokenizing all bracketed species as a single token; Chemformer has 26 carbon-containing tokens, and MoLFormer has 101. Summary statistics for all tokenizers are presented in the supplementary information.\nExisting tokenizer metrics were developed in the context of open-vocabulary modeling, primarily using sub-word tokenization. 45,46,49,52 As such, these metrics focus on evaluating the trade-off between vocabulary size, 49 mono/multi-lingual performance 46,54 or the size of a tokenizer's training corpus. .45 However, chemistry language models are largely driven by closed-vocabulary \u201catom\"-wise tokenization schemes that lack complete coverage of the SMILES language specification (fig. 1c). Individual tokens are not fungible; removing C would likely be catastrophic, but most models lack a token for the quadruple bond yet are demonstrably performant. We propose using n-grams as a proxy language model for a transformer-based model to directly assess a tokenizer's impact on a language model's overall performance. N-grams are a statistical language model for estimating the likelihood of a token xi, given the preceding n 1 tokens Xi-n+1,...,Xi\u22121:\n$P_n(X_i|X_{i-n+1},..., X_{i-1}) = \\frac{C(X_{i-n+1},..., X_i) + 1}{C(X_{i-n+1},..., X_{i-1}) + V}$ (1)\nWhere Pn is the likelihood of xi being the nth token, C is the count of the n-gram in the training corpus, and V is the vocabulary size. Here, we have opted for add-one smoothing to account for zero-counts tokens due to it's simplicity and lack of additional hyper-parameters. 55 For our pre-training corpus, we used 1.6B SMILES strings from the Enamine REAL Space database, 48 split 80/10/10 into training, validation, and test sets.\nFigure 3a, shows the average per molecule cross-entropy loss as evaluated on the validation set for N-grams ranging from 1 to 5. We found consistently improving performance up to N = 5 for all tokenizers. We did not evaluate the performance of larger N-grams, as we found a steep jump in computational cost for N = 5.\nUsing linear least squares regression, we found an increase of 0.62 \u00b1 0.02 nats per ad-ditional token of fertility (p < 13-12, n = 15); suggesting that a higher fertility may harm model performance (fig. 4). That is for every additional fertility token, the model's perplex-ity approximately doubles. Our finding are consistent with Goldman et al.'s conclusion that more compressive (lower fertility) tokenizer are correlated with better downstream perfor-mance. 45\nTo approximate the fine-tuning step, we evaluated the pre-training N-gram models on a downstream dataset from the MoleculeNet benchmark, again split 80/10/10 using the recommended random or scaffold splits for each constituent dataset.37 To approximate the typical workflow of training a chemistry language model, we evaluated the pre-training N-\""}, {"title": "Information Loss from Unknown Tokens", "content": "gram models on the validation set of the fine-tuning dataset (fig. 3b). Model performance was largely saturated at the trigram level, except for the smirk, character, and MolGPT-MOSES models. As the N-gram models were not fine-tuned on the downstream dataset, we caution that fig. 3b is likely not representative of a fine-tuned model using each tokenizer. Rather, fig. 3b measures the distributional shift from the pre-training to downstream datasets for each tokenizer. Figure 3 does not capture the impact of unknown tokens only the ability of an N-gram model to predict the next, possibly unknown, token.\nWe suspect complete coverage of the OpenSMILES specification is desirable for a chemistry language model. However, current models have achieved competitive performance despite lacking full support for the specification (fig. 1c). Partially, this reflects the current datasets available for chemistry models; for example, our corpus of 2.1B molecules lacks any molecules containing quadruple bonds. However, given the performance of existing models, the infor-mation lost from masking out unknown glyphs in a SMILES string may be negligible. To quantify this, we used a bidirectional character-level N-gram model as our reference tok-enizer R and evaluated the KL-divergence in its predictions with and without sections of the SMILES string masked out. Specifically, we estimated the probability of a token xi using from the joint probability of the preceding and succeeding n 1 tokens:\n$B_n(X_i|X_{i-n+1},..., X_{i-1}, X_{i+1},..., X_{i+n-1}) \\propto \\frac{C(X_{i-n+1},...,x_i) + 1}{C(X_{i-n+1},..., X_{i-1}) + V} \\frac{C(X_i,..., X_{i+n-1}) + 1}{C(X_{i+1},..., X_{i+n-1}) + V}$ (2)\nNext, we identify the tokens {xj| j \u2208 M} that would be tokenized as unknown by the tokenizer being evaluated E. By marginalizing R's distribution of Bn over the tokens that E would tokenize as unknown, we obtain B, the distribution for xi without knowledge of the unknown tokens. The KL-divergence between Bn and B is then the information lost to the unknown tokens: DKL(Bn||B'n) = \u03a3\u0392\u00b7 (log Bn \u2013 log B\u201e). Figure 6 shows the shift from B5 to B for several tokenizers for an example SMILES string. A detailed derivation of Bn and Bn is presented in the supplementary information.\nFor example, a tokenizer lacking a token for oxygen (0) would tokenize C1CCOC1 as C 1 C C [UNK] C1. Hypothetically, if oxygen were the only heterocyclic compound, no information would be lost by replacing the O with the [UNK] token; resulting in B\u2081 = \u0392\u03b7, and DKL(Bn||B) = 0. Otherwise, DKL(Bn||B) > 0, reflecting the increased uncertainty in the identity of the masked token. Notably, this metric reflects the information lost regarding the identity of the unknown token, which may or may not impact predictions for downstream tasks. Here, uncertainty over the identity of O should result in a higher uncertainty about the molecule's molecular weight but not the number of rings. Using a reference N-gram model and tokenizer is necessary to provide a baseline for comparison, as the tokenizer under evaluation does not have any concept of masked tokens. As a data-driven method, the reference N-gram model is limited by the distribution of the training corpus. As shown"}, {"title": "Discussion", "content": "in fig. 3, the character-level N-gram models achieved a reasonable level of performance and were consistently in the middle of the pack for both datasets. Additionally, by using a character-level tokenizer, we can marginalize precisely the tokens that would be marked as unknown by the tokenizer under evaluation.\nWe introduce two new tokenizers, smirk and smirk-gpe, for chemistry language models that can represent the entirety of the OpenSMILES specification. In contrast to prior works, smirk splits bracketed atoms into their constituent elements, enabling full coverage with limited vocabulary (fig. 1c) Additionally, we've implemented a BPE variant, smirk-gpe, ca-pable of merging frequently co-occurring SMILES tokens into a single meta-token. While both tokenizers achieve full coverage of the OpenSMILES specification, differences between\nSMILES flavors can still lead to unknown tokens. For example, the MoleculeNet's HIV benchmark includes 0Cc1cc[te]c1, despite [te] not being a valid aromatic symbol. How-ever, the overall information loss from unknown tokens remains negligible, as [te] was the sole ambiguity in our corpus of 1.6B molecules. Ongoing efforts to standardize the SMILES language will clarify and eliminate these ambiguities. 41,42 In the meantime, smirk delivers nearly identical tokenization for most molecules, with a marginal increase in fertility relative to chemistry-specific tokenizers (fig. 4).\nAdditionally, we introduced the use of N-grams as a proxy language model for a transformer-based model, which allows us to assess the impact of a tokenizer's particular vocabulary directly. As noted by prior works, establishing a causal link between tokenizer metrics and model performance remains an immensely expensive task. 45,46,49 N-gram language models can serve as an intermediate step between intrinsic metrics and extrinsic evaluations. Ad-mittedly, N-grams have largely been replaced by neural and transformer-based language models; 20 however, their lack of hyperparameters makes them ideal as a minimally intrusive baseline language model. Using N-grams as a proxy language model, we have quantified the potential impact of unknown tokens on model performance (fig. 6). The information loss for a particular molecule can be large (fig. 5). However, due to the rarity of unknown tokens, the impact on downstream performance remains small for most tokenizers (fig. 6).\nWe argue that eliminating unknown tokens is a desirable property for SMILES tokenizers and have demonstrated that smirk performs nearly identically to existing chemistry tokeniz-ers (figs. 2 and 4). However, we neglected the core questions of whether a chemistry-specific tokenizer is necessary or if general-purpose tokenizers are sufficient. Xu et al. achieved state-of-the-art performance for polymer property prediction, using a pre-trained tokenizer devel-oped for natural language processing. 9,13 Sagawa and Kojima's unigram tokenizer displays similar performance and is capable of open-vocabulary modeling, provided an expanded starting alphabet is used (fig. 1c). Our results suggest that chemistry-specific tokenizers may result in more robust models (fig. 3b), but not necessarily higher performance (fig. 3a).\nAtom-wise tokenization does improve interpretability by allowing researchers to prob the interatomic attention maps directly 19 and avoids the risk of generation artifacts 57 from splitting an element symbol across multiple tokens.\nUltimately, molecular foundation models must deliver reliable and accurate predictions across the entire molecular design space. Current, chemistry language models 11,12,15,16,18,19 employ tokenizers that inadvertently mask out entire atoms, triggering a potentially signifi-cant loss of information (fig. 5). This risk is not purely theoretical, cisplatin (fig. 1b) is an effective chemotherapy drug, but it's isomer is not. 58 Omitting the chiral marker both im-plies a tetrahedral, instead of square planar, geometry, and eliminates medical relevant stere-ochemistry. More broadly, coordination complexes, including cis-[CoCl2(en)2]+ and trans-[CoCl2(en)2] complexes that lead the development of coordination chemistry. 59 Bracketed atoms are found in the SMILES encodings for Vitamin B12, Tricalcium Sillicate (component of cement), Amoxicillin (antibiotic), Diammonium Phosphate (fertilizer) Vaska's Complex (homogenous catalyst),60 18F-Flurodeoxyglucose (radiotracer used for cancer detection). 61 A foundation model for chemistry must encode the entire breath of chemical space or risk obscuring critical features. Mitigating this risk demands transitioning to tokenizers capable of encoding the entirety of chemical space. Both smirk and numerous other open-vocabulary tokenizers 9,62-64 meet this threshold, are permissively licensed and well documented. The impact on model performance appears negligible, with clear advantages to robustness and interpretability."}, {"title": "Code and Data Availability", "content": "The code and data for this paper are currently under review and will be made available upon acceptance at the locations listed.\nThe source code for smirk is openly available at https://github.com/BattModels/smirk, along with example notebooks demonstrating the tokenizer's use with common train-"}]}