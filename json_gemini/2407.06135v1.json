{"title": "ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation", "authors": ["Ethan Chern", "Jiadi Su", "Yan Ma", "Pengfei Liu"], "abstract": "Previous open-source large multimodal models (LMMs) have faced several limitations: (1) they often lack native integration, requiring adapters to align visual representations with pre-trained large language models (LLMs); (2) many are restricted to single-modal generation; (3) while some support multimodal generation, they rely on separate diffusion models for visual modeling and generation. To mitigate these limitations, we present ANOLE, an open, autoregressive, native large multimodal model for interleaved image-text generation. We build ANOLE from Meta AI's Chameleon, adopting an innovative fine-tuning strategy that is both data-efficient and parameter-efficient. ANOLE demonstrates high-quality, coherent multimodal generation capabilities. We have open-sourced our model, training framework, and instruction tuning data.", "sections": [{"title": "Introduction", "content": "Since the introduction of Meta AI's LLaMA (Touvron et al., 2023) in February 2023, autoregressive open-source large language models (LLMs) such as LLaMA (Touvron et al., 2023), Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), Mistral (Jiang et al., 2023), Phi (Gunasekar et al., 2023), Gemma (Team et al., 2024), Olmo (Groeneveld et al., 2024) and LLM360 (Liu et al., 2023c) have democratized and advanced the development of LLMs. Efforts in open-sourcing large multimodal models (LMMs) are also ongoing, though at a slower pace compared to LLMs. Notable open-sourced LMMs include LLaVA (Liu et al., 2023b,a), CogVLM (Wang et al., 2023), DreamLLM (Dong et al., 2023), Emu2 (Sun et al., 2024) and Cambrian (Tong et al., 2024). However, current open-source LMMs have several significant limitations: (1) Many focus solely on multimodal understanding without multimodal generation (Liu et al., 2023b,a; Wang et al., 2023), (2) most are not natively multimodal (i.e., not trained on multimodal data from the pretraining stage) and rely on pretrained LLMs as their backbone (Liu et al., 2023b,a; Wang et al., 2023; Dong et al., 2023; Sun et al., 2024), and (3) those with vision generation capabilities require an additional diffusion model for vision modeling and generation (Dong et al., 2023; Sun et al., 2024). This reliance on extra mechanisms can introduce complexity and inefficiency in both training and inference time.\nGiven the limitations in current open-source LMMs, the AI community eagerly anticipates the emergence of truly open, autoregressive, native LMMs with multimodal generation capabilities. The goal is to be able to develop LMMs in the same way we do with LLMs. To address this gap, we introduce ANOLE. As shown in Fig. 1, ANOLE can generate a high-quality, coherent recipe for cooking eggs in just a few seconds.\nANOLE is built on top of Chameleon (Team, 2024) by Meta AI. Chameleon represents a significant advancement in multimodal AI, showcasing the potential of early-fusion, token-based autoregressive approaches for multimodal modeling. According to their paper, Chameleon has demonstrated impressive capabilities in understanding and generating interleaved sequences of images and text, pushing the boundaries of what is possible in multimodal AI. The latest open-source release of Chameleon has shown strong performance in text understanding, text generation, and multimodal comprehension. However, the current open-source version of Chameleon does not support image generation or multimodal generation. We build ANOLE to facilitate Chameleon's capabilities in vision and multimodal generation without compromising its strengths in text generation and multimodal comprehension. ANOLE addresses the limitations of the current open-source release of Chameleon, making the full potential of multimodal generation accessible to the broader research community. The key contributions of our work are fourfold:\n1. Full Open-Source Implementation: ANOLE has facilitated the vision and multimodal generation capabilities from Chameleon through an innovative fine-tuning approach, unlocking the model's most crucial technological aspects. This comprehensive open-source release allows researchers and developers to fully utilize and build upon it.\n2. Data and Parameter Efficient Fine-Tuning: Our method fine-tunes fewer than 40M parameters, requiring only about 6,000 samples to effectively facilitate vision and multimodal generation capabilities. This demonstrates a highly efficient approach to facilitate complex functionality in LMMs.\n3. Training, Multimodal Inferece, and Qualitative Evaluation: We provide a training and multimodal inference framework for unified tokenizer-based multimodal models. This infrastructure significantly lowers the barrier to entry for developing and experimenting with autoregressive LMMs, making it accessible to a wider range of researchers. Additionally, we conduct qualitative analysis to demonstrate the potential of autoregressive LMMs.\n4. Rich Resources for Accessibility: To further support the adoption and advancement of autoregressive LMMs, we offer an extensive collection of data resources and detailed tutorials. These materials are designed to facilitate easier onboarding and experimentation for researchers at various levels of expertise.\nBy addressing these critical aspects, ANOLE represents a significant step forward in democratizing access to advanced multimodal AI technologies. Our work not only builds upon the foundations laid by the original Chameleon model but also paves the way for more inclusive and collaborative research in the field of multimodal \u0391\u0399.\nMoreover, ANOLE sparks a series of important and intriguing research questions for the community to explore. For example:\n\u2022 Investigating the performance limits of vision generation using unified tokenizer-based multimodal models, in comparison to established methods like diffusion models.\n\u2022 Developing efficient techniques for interleaved image-text decoding, which are essential for real-world applications such as textbook and comic generation."}, {"title": "Related Works", "content": "LMMs have experienced significant advancements, often leveraging pretrained LLMs as their foundation backbone. One common approach involves combining a CLIP-pretrained vision encoder with diffusion models as the decoder, and a pretrained LLM (e.g., Vicuna) as the backbone (Dong et al., 2023; Tian et al., 2024; Sun et al., 2024). This approach achieves impressive results in both image understanding and generation by leveraging the robust representations provided by CLIP and diffusion models. However, incorporating CLIP and diffusion models increases the overall complexity of the model architecture, resulting in additional training overhead and reduced inference efficiency.\nIn contrast, purely token-based methods exclude diffusion models and CLIP, instead using token-based representations for multimodal understanding and generation. This approach traces back to BEiT (Bao et al., 2021), with OpenAI's DALL-E (Ramesh et al., 2021) exemplifying text-to-image generation based on similar principles. These methods rely heavily on vector quantization (VQ) models (Van Den Oord et al., 2017; Esser et al., 2021), which combine ResNet-based encoders and decoders with a discrete codebook. During image encoding, the VQ model transforms the image from pixel space to latent space representations, then maps these representations to codebook IDs using a nearest-neighbor search. These IDs serve as input tokens for a Transformer model, which models conditional probabilities and predicts sequences. The VQ decoder then reconstructs images from the generated sequences. This autoregressive, discrete image encoding and decoding approach has been validated in multiple studies for producing high-quality images (Zhu et al., 2024; Yu et al., 2024), effectively modeling inter-image dependencies (Bai et al., 2023), and enhancing image consistency (Pan et al., 2024). LWM (Liu et al., 2024) and Chameleon (Team, 2024) extend this concept to image-text multimodal tasks, using streamlined architectures to handle tasks involving both images and text. Compared to other methods, unified token-based modeling significantly reduces model complexity, facilitating seamless inference and the generation of interleaved image-text sequences without additional components.\nANOLE, building on the foundation of Chameleon, facilitates Chameleon's image and multimodal generation capabilities with efficient fine-tuning. ANOLE retains the inherent advantages of Chameleon's architecture while producing high-quality images and maintaining coherent image-text sequences. Tab. 1 highlights ANOLE'S characteristics as an autoregressive, diffusion-free, native token-based model, emphasizing its capability to support multimodal generation, as well as its simplicity and efficiency compared to more complex frameworks."}, {"title": "ANOLE", "content": "ANOLE adopts the same approach and architecture as Chameleon, utilizing an early-fusion, token-based, autoregressive approach to model multimodal sequences (text and images) without the use of diffusion models, relying solely on transformers. Token-based approaches (Team, 2024; Lu et al., 2022; Yu et al., 2023; Liu et al., 2024) achieve modality fusion at the input-token level. Firstly, modality-specific tokenizers tokenize samples from each modality. Then, these token sequences are concatenated to form a single multimodal token sequence, which is subsequently fed into an autoregressive transformer for modeling."}, {"title": "Facilitating the image generation and multimodal generation capabilities from Chameleon", "content": "Fig. 2 shows the innovative fine-tuning process of how ANOLE is fine-tuned from Chameleon. Based on available information and our testings, the latest release of Chameleon have demonstrated strong performance in text understanding, text generation, and multimodal understanding. ANOLE, build on top of Chameleon, aiming to facilitate the image generation and multimodal generation capabilities from Chameleon. Chameleon's pre-training data natively includes both text and image modalities, theoretically equipping it with image generation capabilities. Our goal is to facilitate this ability without compromising its text understanding, generation, and multimodal comprehension. To achieve this, we froze most of Chameleon's parameters and fine-tuned only the logits corresponding to image token ids in transformer's output head layer.\nFollowing the principle of \"less is more\u201d (Zhou et al., 2024), the current version of ANOLE, ANOLE-7b-v0.1, was developed using a small amount of image data (5,859 images from LAION-5B art (Schuhmann et al., 2022)) and was fine-tuned on just a few parameters (less than 40M) in a short time (around 30 minutes on 8 A100 GPUs). Despite these limitations, ANOLE-7b-v0.1 expresses impressive image (Tab. 2 and Tab. 3) and multimodal generation capabilities (Fig. 1, Fig. 3 and Fig. 4)."}, {"title": "Evaluation", "content": "We conduct qualitative analysis on ANOLE's capabilities on image generation and interleaved image-text generation."}, {"title": "Image Generation", "content": "Tab. 2 demonstrates the text-to-image capabilities of ANOLE. We highlight the following points: (1) The images generated by ANOLE are of high quality and closely adhere to the given instructions. For instance, ANOLE accurately captures the essence of \"A steaming cup of coffee next to a fresh croissant on a cozy cafe table,\u201d showcasing the steam, coffee, and croissant elements precisely. (2) ANOLE demonstrates remarkable versatility in generating diverse types of images. It can create realistic depictions, as seen in the coffee and ice cream images, as well as imaginative scenes, like the dinosaur strolling in Times Square. This variety highlights ANOLE's ability to blend realism with creativity seamlessly."}, {"title": "Interleaved Image-Text Generation", "content": "Fig. 1, Fig. 3 and Fig. 4 demonstrate the multimodal generation capabilities of ANOLE. We emphasize that (1) The generated text is well-organized and provides comprehensive details, which is essential for accurate interleaved image-text generation. This is evident in Fig. 4, where the detailed introduction of Gyumri is accompanied by relevant images, effectively capturing the city's architectural design and cultural highlights. (2) ANOLE demonstrates seamless integration between images and text, ensuring that the visual and textual elements complement each other perfectly. Fig. 3 highlights this capability by introducing traditional Chinese dishes with corresponding images, making the information both engaging and informative."}, {"title": "Conclusion & Future Directions", "content": "We introduce ANOLE, an open, autoregressive, native LMM for interleaved image-text generation that demonstrates advanced multimodal generation abilities. ANOLE facilitates image and multimodal generation capabilities from Chameleon by fine-tuning on just 6,000 samples with 40M parameters. We are committed to continually upgrading ANOLE to enhance its capabilities. Our future directions include (1) enhancing ANOLE's precise instruction-following capability, (2) extending its context length, (3) improving its multimodal understanding capabilities, and (4) applying ANOLE to downstream tasks requiring multimodal generation abilities."}, {"title": "Limitations & Disclaimer", "content": "ANOLE is intended for research use only. Our model weights follow the same license as Chameleon. The fine-tuning images we used are from LAION-5B art, and thus follow the same license as LAION. ANOLE is still under development and has many limitations that need to be addressed. Importantly, we have not aligned the image generation capabilities of the ANOLE to ensure safety and harmlessness. Therefore, we encourage users to interact with ANOLE with caution and report any concerning behaviors to help improve the model's safety and ethical considerations."}]}