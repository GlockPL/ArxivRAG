{"title": "Boosting Efficiency in Task-Agnostic Exploration through Causal Knowledge", "authors": ["Yupei Yang", "Biwei Huang", "Shikui Tu", "Lei Xu"], "abstract": "The effectiveness of model training heavily relies on the quality of available training resources. However, budget constraints often impose limitations on data collection efforts. To tackle this challenge, we introduce causal exploration in this paper, a strategy that leverages the underlying causal knowledge for both data collection and model training. We, in particular, focus on enhancing the sample efficiency and reliability of the world model learning within the domain of task-agnostic reinforcement learning. During the exploration phase, the agent actively selects actions expected to yield causal insights most beneficial for world model training. Concurrently, the causal knowledge is acquired and incrementally refined with the ongoing collection of data. We demonstrate that causal exploration aids in learning accurate world models using fewer data and provide theoretical guarantees for its convergence. Empirical experiments, on both synthetic data and real-world applications, further validate the benefits of causal exploration.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks have been incredibly successful in various domains, such as milestone achievements in Go games and control tasks [Silver et al., 2016; Tassa et al., 2018]. One key factor contributing to such remarkable performance is the availability of high-quality data for model training. However, in many practical applications, it remains data-hungry due to limited data collection efforts imposed by budget constraints [Fang et al., 2017; Yoo and Kweon, 2019; Robine et al., 2023]. This highlights the essential need to enhance both the sampling and model learning efficiency.\nIn this paper, we introduce causal exploration to tackle this challenge, a novel framework that makes use of the underlying causal knowledge to boost the data collection and model training processes. On one hand, acquiring and understanding causal knowledge unveils the fundamental mechanisms behind the data generation process, thereby reducing the exploration space. In contrast to random data collection, causal exploration allows systematic action planning based on the identified causal structures. On the other hand, causal knowledge reflects the cause-and-effect dependencies among variables. Through the incorporation of causal structural constraints into the model, we can acquire causal dynamics models that eliminate redundant dependencies, as opposed to non-causal dense models, which have shown to provide more accurate estimations [Seitzer et al., 2021; Huang et al., 2022; Wang et al., 2022].\nSpecifically, we focus on boosting the sample efficiency and reliability of the world model in the realm of task-agnostic reinforcement learning (RL). Different from methods that learn a fixed task from scratch, task-agnostic RL agent first learns a global world model that gathers information about the true environment on the data collected during exploration. Then based on the predictions of the learned model, the agent could make quick adaptations to various downstream tasks in a zero-shot manner given task-specific reward functions. This learning setup exhibits excellent generalization performance but also imposes high requirements on the accuracy of the model [Pathak et al., 2019]. However, the data collection and world model learning processes are usually expensive due to extensive environment interactions, especially in large state spaces where discovering the optimal policy can be highly challenging [Burda et al., 2018].\nTo address this problem, our causal exploration-based approach revolves around three primary aspects. First, we employ constraint-based methods to discover causal relationships among environment variables. Second, we formulate the dynamics model under causal structural constraints to enhance its reliability. Third, we propose several ways based on causal knowledge to improve the sample efficiency during exploration. In particular, for causal discovery, we present an efficient online method that selectively eliminates noisy samples and strategically gathers informative data points in an incremental way. Moreover, we learn to actively explore towards novel states that are expected to contribute most to model training. The learning process of the exploration policy is driven by intrinsic rewards, which measure both the agent's level of surprise at the outcome and the quality of the training caused by the selected data. During exploration, causal knowledge and the world model are continuously refined with the ongoing collection of data. Our key contributions are summarized below.\n\u2022 In order to enhance the sample efficiency and reliability of model training with causal knowledge, we introduce a novel concept: causal exploration, and focus particularly on the domain of task-agnostic reinforcement learning.\n\u2022 To efficiently learn and use causal structural constraints, we develop an online method for causal discovery and formulate the world model with explicit structural embeddings. During exploration, we train the dynamics model under a novel weight-sharing-decomposition schema that can avoid additional computational burden.\n\u2022 Theoretically, we show that, given strong convexity and smoothness assumptions, our approach attains a superior convergence rate compared to non-causal methods. Empirical experiments further demonstrate the robustness of our online causal discovery method and validate the effectiveness of causal exploration across a range of demanding reinforcement learning environments."}, {"title": "2 Preliminary", "content": "We consider task-agnostic RL within the framework of a Markov decision process characterized by state space S and action space A. In addition, to integrate causal information, we make the following assumptions throughout this paper.\nAssumption 1. (Causal Factorization). Both the state space and action space can be factorized. That is, $S = S_1 \\times ... x S_n \\in R^n$ and $A = A_1 \\times ... \\times A_c \\in R^C$.\nAssumption 2. (Causal Sufficiency). The state variables $s_t$ are fully observable without any hidden confounders.\nAssumption 3. (Faithfulness Condition). For a causal graph G and the associated probability distribution P, every true conditional independence relation in P is entailed by the Causal Markov Condition applied to G.\nGiven these commonly made assumptions for causal discovery methods, we next define transition causality over the transition variables from {$s_{t-1}, a_{t-1}$} to $s_t$.\nDefinition 1. (Transition Causality). Under the Markov condition, the causal structures are over the state-action variables $U = \\{S_{i,t-1}\\}_{i=1}^n \\cup \\{A_{j,t-1}\\}_{j=1}^c$ and $V = \\{S_{i,t}\\}_{i=1}^n$, which can be represented by a directed acyclic graph $G = (\\{U, V\\}, E)$ and its adjacency matrix $D$. Here, E denotes the edge set and $D \\in \\{0,1\\}^{|U| \\times |V|}$\nNote that all edges are from U to V. If $S_{i,t-1} \\in U$ has a causal edge to $s_{j,t} \\in V$, then we call $s_{i,t-1}$ a parent of $s_{j,t}$ and have D(i, j) = 1. Take the example in Figure 1: we have D(1,1) = 1 because $s_{1,t-1}$ is a parent of $s_{1,t}$, while D(2,1) = 0 because $s_{2,t-1}$ does not have a causal edge to $S_{1,t}$. Here, we assume that the structural constraints are invariant over time t. The causal identification theory under these appropriate definitions and assumptions has been given in existing work."}, {"title": "3 Related Work", "content": "Task-agnostic RL. Over recent decades, task-agnostic exploration strategies have been an active research area to attain generalization [Aubret et al., 2019]. Existing methods focus on designing appropriate forms of intrinsic rewards, which can be broadly categorized into three types: (1) the number of times a state has been visited, which helps to guide the agent towards unexplored regions [Bellemare et al., 2016; Machado et al., 2020]; (2) curiosity about the environment dynamics, which is usually formalized as prediction errors of future states [Pathak et al., 2017; Kim et al., 2018]; and (3) information gain, which aims to improve the agent's knowledge about the environment by maximizing the mutual information [Duan et al., 2016; Shyam et al., 2019]. However, existing works usually pay little attention to the support that causal structures can offer in improving exploration efficiency.\nRL with causal discovery. The intersection of causal discovery and RL has become a popular trend in recent years. [Zhu et al., 2019] uses RL to search for the causal graph with the best score. [Hu et al., 2022] proposes to learn a hierarchical causal structure for subgoal-based policy learning. [Ding et al., 2022] and [Mutti et al., 2023] focus on providing tractable formulations of systematic generalization in RL tasks by employing a causal viewpoint. [Yu et al., 2023] learns a causal world model to generate explanations for the decision-making process. Theoretical evidence about the advantages of using a causal world-model in offline RL is given in [Zhu et al., 2022]. However, most of these works focus on either extracting underlying causal graphs from given data in a particular environment or using random exploration strategies for data collection, instead of directly utilizing causal structure as guidance to improve exploration efficiency in task-agnostic RL [Kosoy et al., 2022]. Experimental comparisons with some existing works that are close to us have been conducted in Section 6."}, {"title": "4 Discovering and Utilizing Causality for Learning World Models", "content": "After establishing proper assumptions and definitions, we proceed to introduce the methodology part for causal exploration. In this section, we initially assume that the causal structures are known and show how to explicitly incorporate causal knowledge into the world model and utilize it for model training. Next, we give an estimation procedure for causal structures."}, {"title": "4.1 Causal Constraints for Forward Model", "content": "During task-agnostic exploration, the agent learns a world model $f_{w_0}$ with parameter w, serving as an abstraction of the ground truth transitions in the environment. In other words, the world model $f_{w_0}$ is designed to enable agents to predict future state $\\hat{s}_t$ based on current state $s_{t-1}$ and action $a_{t-1}$, represented by $\\hat{s}_t = f_{w_0}(s_{t-1}, a_{t-1}, e_t)$, where $e_t$ is the corresponding random noise. However, based on the understanding that causal structures within environmental variables are typically sparse rather than dense, as suggested by [Huang et al., 2022], such a framework could contain unnecessary dependencies. For instance, in the context of Figure 1, the variable $s_{2,t-1}$ does not causally affect $s_{1,t}$, and is thus identified as a non-parent node. Consequently, we only need to take a subset of $(s_{t-1}, a_{t-1})$ as inputs of the model.\nTo reflect these constraints, we explicitly consider the causal structures over state and action variables to model the one-step transition dynamics, formulated as:\n$\\hat{s}_t = f_{w_0}(\\prod_{i=1}^{n} D_i(s_{t-1}, a_{t-1}), e_{i,t}),$ (1)\nwhere denotes element-wise product, $D_i$ and $e_{i,t}$ are the i-th column of causal matrix D and noise term $e_t$, respectively. However, a naive implementation requires training n world models since each factored dimension has its unique parents. It is likely to result in an explosive growth in computational complexity as the state dimension and network size increase.\nWe propose a sharing-decomposition schema to address such a problem. It is unnecessary for all of these n networks to be totally different. Instead, each of these models could share the first several layers as a common embedding. Then following the sharing module, we design predictive networks for each dimension. Suppose $\\omega_i$ is the network parameter for the i-th dimension, it is a combination of the shared parameter $\\sigma$ and decomposed parameter $\\theta_i$, written as\n$\\omega_i = \\sigma \\cup \\theta_i.$ (2)\nDuring the training time, each model focuses on a different aspect of the state in the decomposition part $\\theta_i$ but shares a common knowledge $\\sigma$. The number of shared layers is a hyperparameter that allows for a trade-off between the sharing and decomposition parts. By training forward models under this schema, our approach can both utilize causal information of the ground environment dynamics to generate accurate predictions and achieve a significant reduction in model"}, {"title": "4.2 Efficient Online Causal Relationship Discovery", "content": "In this section, we show how to identify the causal adjacency matrix D. According to Definition 1, this can be transformed into determining whether there exists an edge between each pair of nodes in the causal graph G. To achieve this goal, we start from a complete graph and then iteratively remove unnecessary edges using Conditional Independence Tests (CIT). Given that the edges follow the temporal order without instantaneous connection, we extend the PC algorithm [Spirtes et al., 2000] to handle time-lagged causal relationships based on Kernel-based Conditional Independence (KCI) test [Zhang et al., 2012] to identify the causal adjacency matrix D.\nWhile causal discovery algorithms typically necessitate the collection of substantial causal information through data, it's important to note that accumulating more samples does not always confer an advantage: as the sample size increases, the time cost of causal algorithms also rises. Therefore, prioritizing the enhancement of data quality over quantity becomes paramount. In order to reduce the cost of the identification process, we design an efficient online causal relationship discovery method: instead of using all of the coming data for causal identification, we selectively collect representative data points during exploration in an incremental way. Specifically, we use the minibatch similarity and sample diversity criteria introduced in [Yoon et al., 2021] as our selection strategies, which are defined as\n$\\text{Similarity} = \\frac{\\nabla f_{w_0}(b_i) \\nabla f_{w_0}(B_t)}{||\\nabla f_{w_0}(b_i) || \\cdot ||\\nabla f_{w_0}(B_t)||},$ (4)\n$\\text{Diversity} = \\frac{\\sum_{i \\neq j}^{t-1} \\nabla f_{w_0}(b_i) \\nabla f_{w_0}(b_j)}{\\sum_{i=1}^{t-1} ||\\nabla f_{w_0}(b_i) || \\cdot || \\nabla f_{w_0}(b_j)||} .$"}, {"title": "5 Boosting Efficiency through Causal Exploration", "content": "We now return to the fundamental question: how to enhance the data collection efficiency during causal exploration, thereby improving the performance of both causal discovery and model learning. To attain this goal, a commonly applied concept from active learning is the selection of samples that make the largest contributions to the model's training loss. These samples are typically considered as a subset that the model is least familiar with. Hence, the prediction loss is used here as the intrinsic reward to guide exploration with a scaling weight \u03b7:\n$r_{t-1} = - \\frac{1}{2}||\\hat{s}_t - s_t||^2 \\cdot \\eta.$ (5)\nThis prediction loss can also be viewed as a validation of the agent's causal beliefs. The larger the prediction error, the more surprised the agent is by the actual outcome, implying a greater deviation from the estimated values based on the causal structure and the world model. The faster the error rate drops, the more learning progress signals we acquire.\nHowever, not all the novel states have a positive impact on the model. On the contrary, some noisy data may contribute significantly to prediction errors but can lead the model to an awful direction, which necessitates the agent to also pay attention to the inherent quality of the data during exploration.\nTo reflect this, we introduce active reward [Fang et al., 2017] that measures the data quality as another intrinsic motivation. Once a new sample is collected at time step t, active reward is then calculated as the change of the model's prediction ability before and after training. We use the prediction accuracy on a test set $D_h$ generated from episodes unseen before training to reflect the world model's performance and formulate active reward as\n$r_{t-1} = L_f(w_{t-2}) - L_f(w_{t-1}),$ (6)\nwhere $L_f(\u00b7)$ is the mean prediction error on $D_h$ and $w$ denotes parameters of the trained world model at time t. The value of active reward reflects beneficial or detrimental training caused by the selected data. If the reward is always positive, it indicates that the agent has been selecting beneficial samples for training the world model. We combine prediction loss and active reward with a regularization weight \u03b2:\n$r_{t-1} = r_i-1 + \\beta r_a-1.$ (7)\nDuring causal exploration, the agent keeps searching for causal informative data by maximizing the expected rewards, which is\n$a_{t} = \\arg \\underset{a \\in A}{\\max} \\underset{\\tau \\in \\pi_t}{\\mathbb{E}} [\\sum_{t} \\gamma^t r_t],$ (8)\nwhere \\tau represents the trajectory generated by the exploration policy \\pi and \u03b3 is the discount factor. Meanwhile, the world model minimizes the prediction loss. Since both of them contain the prediction error in equation (3) and (5), we can draw a conclusion that the learning of world models and causal exploration facilitate each other.\n5.1 Theoretical Analysis on Causal Exploration\nIn this subsection, we first present a mathematical criterion for evaluating the impact of causal exploration on sampling efficiency, and then we theoretically demonstrate the benefits of causal exploration in learning world models, especially when the causal graph is sparse.\nCausal efficiency ratio. Let k denote the optimization step in world model training, f represent the world model, and"}, {"title": "6 Experiments", "content": "To evaluate the effectiveness of causal exploration in complex scenarios, we conduct a series of experiments on both synthetic datasets and real-world applications, including the traffic light control task and MuJoCo control suites [Todorov et al., 2012]. Meanwhile, we compare our proposed causal exploration with the following baseline methods:\n1. Curiosity [Burda et al., 2018] and Plan2Explore [Sekar et al., 2020]: non-causal exploration methods that learn dense models with different forms of intrinsic rewards.\n2. CID [Seitzer et al., 2021]: focusing on detecting causal influences between actions $a_{t-1}$ and future states $S_t$ with random data collection. Causal exploration extends to discover dependencies among state transitions as well.\n3. ASR [Huang et al., 2022]: a differential approach that learns the causal matrix D as free parameters together with the world model f. Instead, the identification of D is completed by causal discovery methods in our approach.\n4. CDL [Wang et al., 2022]: learning causal dynamics models to approximate conditional mutual information rather than directly implementing CIT on the observed data."}, {"title": "6.1 Synthetic Datasets", "content": "Environments. We build our simulated environment following the state space model with controls. When the agent takes an action at based on the current state, the environment provides feedback $s_{t+1}$ at the next time. We denote the generative environment as\n$S_1 \\sim N(0, I),$ (12)\n$S_t \\sim N(h(s_{t-1}, a_{t-1}), \\Sigma),$\nwhere \u03a3 is the covariance matrix and h is the mean value as the ground truth transition function implemented by deep neural networks under causal graph G. Specifically, the linear condition consists of a single-layer network, and the nonlinear function is three-layer MLPs with sigmoid activation.\nGiven that the sparsity of the causal graph is an important factor affecting the performance of our method, we choose to demonstrate the superiority of our approach on relatively low-density causal structures. That is, G is generated by randomly connecting edges with a probability of p. Such sparse causal structures allow us to evaluate the ability of our method to accurately learn world models with limited data, which is a common scenario in many real-world applications. Below, we provide discussions on the benefits of causal exploration based on the empirical results.\nCorrectness of the identified causal structure. We compare our proposed method with different causal discovery methods. To be specific, CID and CDL apply score-based causal discovery, while ASR combines causal structure with neural networks for differentiable training. However, CID only considers causal action influence and lacks an explicit causal structure, so we do not use it for comparison. Figure 5 illustrates an example of the causal matrix discovered by different methods, where the depth of the color indicates the number of times each edge is detected as a causal connection. Figure 3 further shows the averaged F1-score of these"}, {"title": "7 Conclusion and Future Work", "content": "In this paper, we introduce causal exploration, a methodology designed to incorporate causal information from data for the purpose of learning world models efficiently. In particular, we employ causal exploration within the domain of task-agnostic reinforcement learning and design a sharing-decomposition schema to leverage causal structural knowledge for the world model. A series of experiments in both simulated environments and real-world tasks demonstrate the superiority of causal exploration, which highlights the importance of rich causal prior knowledge for efficient data collection and model learning. We would like to point out that this study primarily focuses on scenarios where states are fully observed. Future research directions involve addressing more complex scenarios including unobserved latent state variables and developing improved fault-tolerant mechanisms to enhance robustness. This work provides a promising direction for future endeavors in efficient exploration."}, {"title": "A Proof of Theorem 1", "content": "Recall that we consider solving a linear regression problem under m strong convex and M-smooth. MSE loss function $L_f(w)$ in the formulation $||w^\\top \\cdot (S_{t-1}, a_{t-1}) - S_t||^2$ with $S_t = w^{* \\top} \\cdot (S_{t-1}, a_{t-1})$ is minimized by gradient descent method with\n$w(k) = w(k \u2212 1) \u2212 \u03b1\u2207L_f(w(k \u2212 1))$ (13)\nwhere \u03b1 is the corresponding step size.\nDenote $w^\\circ(k)$ as the network parameters taking causal constraints with respect to $w(k)$ that does not gather causal information. Theorem 1 shows that causal exploration gets a prediction error bound $\u03b4_k$ times lower at the k-th step, where d is a density measurement of the causal adjacency matrix D.\nBelow, we present a two-step proof for the convergence of causal exploration. Lemma 2 provides an upper bound for convergence without using any causal information. Lemma 3 demonstrates that utilizing causal structure information results in $w^\\circ(k)$ being closer to the optimal value w\u2217 compared to $w(k)$ at the same optimization steps. Combining Lemma 2 and Lemma 3, we derive the convergence rate for causal exploration.\nLemma 2. Suppose $L_f(w)$ is m-strongly convex and M-smooth. We have\n$||w(k) \u2013 w^*||^2 \u2264 (1 - \\frac{m}{M})^k||w_0 \u2013 w^*||^2.$ (14)\nProof. According to gradient descent method (13), we get\n$||w(k) \u2013 w^*||^2 = ||w(k \u2212 1) \u2212 \u03b1\u2207L_f(w(k \u2212 1)) \u2013 w^*||^2$\n$= ||w(k \u2212 1) \u2013 w^*||^2 + \u03b1^2||\u2207L_f(w(k \u2212 1))||^2$\n$- 2\u03b1\u2207L_f(w(k \u2212 1))(w(k \u2212 1) \u2013 w^*).$\nBy strong convexity\n$\\nabla L_f(w)(w \u2013 w^*) \u2265 L_f(w) - L_f(w^*) + \\frac{m}{2} ||w - w^*||^2,$\nwe further obtain (15)\n$||w(k) \u2013 w^*||^2 \u2264 ||w(k \u2212 1) \u2212 w^*||^2 \u2013 2\u03b1(L_f(w(k \u2212 1))\n- L_f(w^*) + \\frac{m}{2} ||w(k-1) \u2013 w^*||^2) + \u03b1^2||\u2207L_f(w(k \u2212 1))||^2$\n$= ||w(k \u2212 1) \u2013 w^*||^2 \u2013 2\u03b1(L_f(w(k \u2212 1)) \u2013 L_f(w^*))$\n$- \u03b1m||w(k \u2212 1) \u2013 w^*||^2 + \u03b1^2||\u2207L_f(w(k \u2212 1))||^2$\n$\u2264 ||w(k \u2212 1) \u2013 w^*||^2 \u2013 2\u03b1(L_f(w(k \u2212 1)) \u2013 L_f(w^*))$\n$\u2013 \u03b1m||w(k \u2212 1) \u2013 w^*||^2$\n$+ \u03b1^2M(L_f(w(k \u2212 1)) \u2013 L_f(w^*))$\n$\u2264 ||w(k \u2212 1) \u2013 w^*||^2 \u2013 \u03b1m||w(k \u2212 1) \u2013 w^*||^2$\n$+ 2\u03b1(\u03b1M \u2212 1)(L_f(w(k \u2212 1)) \u2013 L_f(w^*)).$\nConsider $\u03b1 = \\frac{1}{M}$, we get (16)\n$||w(k) \u2013 w^*||^2 \u2264 (1- \\frac{m}{M}) ||w(k-1) - w^*||^2.$ (17)\nUsing the above equation repeatedly, we obtain\n$||w(k) \u2013 w^*||^2 \u2264 (1 - \\frac{m}{M})||w(k \u2013 1) \u2013 w^* ||^2$\n$\u2264 (1 - \\frac{m}{M})^2||w(k \u2013 2) \u2013 w^* ||^2$ (18)\n$<\u2026\u2026 \u2264 (1-\\frac{m}{M})^k||w_0 \u2013 w^*||^2.$\nLemma 3. Suppose $w^\\circ(k)$ and w(k) are the network parameters with/without causal structure respectively and w\u2217 is the optimum. It holds that\n$||w^\\circ(k) \u2013 w^* ||^2 < \u03b4_k||w(k) \u2013 w^*||^2.$ (19)\nProof. According to Definition 1, we have\n$w^\\circ(k) = D \\odot w(k),$ (20)\nwhere D is the binary causal matrix. Hence, we rewrite w(k) as\n$w(k) = D \\odot w(k) + (1 \u2212 D) \\odot w(k) = w^\\circ(k) + w^\\bot.$ (21)\nNote that we have w\u2217 = D w\u2217. Then we obtain\n$||w(k) \u2013 w* ||^2 = \\sum_{i,j}(w_{ij} (k) \u2013 w^*_{ij})^2$\n$= \\sum_{i,j} [D_{ij} \u00d7 w_{ij} (k) \u2013 w^*_{ij} + (1 \u2212 D_{ij}) \u00d7 w_{ij}(k)]^2$\n$= \\sum_{i,j} [(D_{ij} \u00d7 w_{ij} (k) \u2013 w^*_{ij})^2 + ((1 \u2013 D_{ij}) \u00d7 w_{ij}(k))^2]$\n$= \\sum_{i,j}(D_{ij} \u00d7 w_{ij} (k) \u2013 w^*_{ij})^2 + \\sum_{i,j}((1 - D_{ij}) \u00d7 w_{ij}(k))^2$\n$= \\sum_{i,j} (w_{ij} (k) \u2013 w^*_{ij})^2 + \\sum_{i,j}(w_{ij})^2$\n$= ||w^\\circ(k) \u2013 w*||^2 + ||w^\\bot||^2 \u2265 (1 + p_k)||w^\\circ(k) \u2013 w*||^2,$ (22)\nwhere $p_k \u2208 [0, +\u221e)$ is the lower bound of the ratio between $||w^\\bot||^2$ and $||w^\\circ(k) -w^*||^2$ whose value is related to the sparsity of causal matrix D. By setting $\u03b4_k = \\frac{1}{1+p_k}$, we complete the proof of Lemma 3.\nThe convexity of $L_f (w)$ implies (23)\n$L_f(w^\\circ(k)) - L_f(w^*) \u2264 \\frac{M}{2} ||w^\\circ(k) \u2013 w^*||^2.$\nBy applying Lemma 2 and Lemma 3 into (23) and denote\n\u03b4 = max{\u03b4_0, \u03b4_1,..., \u03b4_\u03ba}, (24)\nwe can obtain Theorem 1."}, {"title": "B Synthetic environment", "content": "B.1 More experiment details\nImplementation. Double DQN [Van Hasselt et al., 2016] is used to train the exploration policy of agents, where both the evaluation network and the target network are three-layer"}, {"title": "B.2 Generalization to underestimation scenarios", "content": "In some data-hungry scenarios, there may be insufficient data for causal discovery, leading to underestimation of the causal structure, which makes continuous data collection and causal structure correction important components. In other words, the causal structure inferred from causal discovery algorithms may deviate from the ground truth ones, which is particularly prone to occur under conditions of limited sample size or during the initial stages of exploration. Consequently, we conduct an evaluation of our proposed algorithm's performance"}, {"title": "B.3 Generalization to scenarios with causal structural changes", "content": "In real-world scenarios, causal structure between variables can often change due to sudden disturbance. For instance, causal relationships between economic variables like stock prices, interest rates, and inflation can be subject to rapid changes caused by market crashes or policy changes.\nTo evaluate the effectiveness of our approach in handling such mutation, we conduct experiments in a scenario where the causal structure changes randomly once. We use our simulation model to generate the data and compare our method to a non-causal approach. Figure 11(c) illustrates the advantages of our approach in tackling such a challenging task.\nOur sharing-decomposition schema enables the agents to quickly adapt to structural changes and make appropriate adjustments. This also demonstrates the robustness of our method, which allows for timely correction of errors in the causal structure. By sharing the same decomposition modules across different time steps and tasks, our method can effectively leverage previous knowledge and transfer it to new situations, while also being flexible enough to accommodate changes in the causal structure. In addition, the ability to adapt to changing causal structures can improve the generalization ability of our method, making it more applicable to a wider range of real-world tasks.\nIn our future research, we plan to expand our work to situations where changes occur within the model. In these cases, during the model learning phase, it becomes crucial to effectively detect these changes and promptly update the model. Additionally, when it comes to policy learning, a key challenge is determining the most suitable model to utilize. We may encounter entirely new models that have not been encountered before, adding an additional layer of complexity to our research."}, {"title": "C Traffic Signal Control", "content": "Traffic signal control is an important means of mitigating congestion in traffic management. Compared to using fixed-duration traffic signals, an RL agent learns a policy to determine real-time traffic signal states based on current road conditions. The state observed by the agent at each time consists of five dimensions of information, namely the number of vehicles, queue length, average waiting time in each lane plus current and next traffic signal states. Action here is to decide whether to change the traffic signal state or not. For example, suppose the traffic signal is red at time t, if the agent takes action 1, then it will change to green at the next time t + 1, otherwise, it will remain red. Following the work in IntelliLight [Wei et al., 2018], the traffic environment in our experiment is a three-lane intersection.\nExperiment details and analysis. We first only use prediction-based causal exploration to learn forward dynamic world models under the same traffic environment in IntelliLight. Then, the agent learns a policy for traffic signal control task in our learned world models, which avoids the high-cost interaction with real traffic environment. For consistency and easy comparison, we use the same DQN network from IntelliLight to train our causal exploration agent."}, {"title": "D More Results of Mujoco tasks", "content": "We use PPO algorithm [Schulman et al., 2017] for optimization during both the task-agnostic exploration and policy learning stages and adopt the hyperparameters from Ta-ble 3 of PPO with a trajectory length of 2048, an Adam stepsize of 3e-4, a minibatch size of 64, a discount factor (\u03b3 = 0.99), a GAE parameter (\u03bb = 0.95), and a clipping parameter (\u03f5 = 0.2). Both the actor-critic network and the world model are 2-(hidden)-layer neural networks, consisting of 256 and 64 hidden nodes respectively. Activation functions are Tanh and ReLU here.\nPerformance of causal exploration on some other MuJoCo tasks are provided in Figure 15. Predictions given by world models under causal structural constraints are more accurate and stable than those of other methods. The learned world model of causal exploration provides the agent with more information in the following policy learning stage, resulting in higher scores achieved in a shorter time. 14 illustrates the identified causal structures during exploration, which explains for the performance gain.\nWe also conduct several experiments to test the performance of causal exploration with a different form of intrinsic reward. To be specific, we formulate our world model as \u03bc\u03c9\u03c2 (st, at), \u03c3\u03c9\u03c2 (st, at) to model the transition probability as p(st+1 | st, at) ~ N(st+1; \u03bc\u03c9\u03c2, \u03c3\u03c9\u03c2). Then, the negative log-likelihood is used both for the world model learning and causal exploration, which is a replacement for equation (3) and (5), and is formulated as:\nL(\u03bc\u03c9\u03c2, \u03c3\u03c9\u03c2) = - \\frac{(s_{t+1} - \u03bc\u03c9\u03c2(s_t, a_t))^2}{2\u03c3\u03c9\u03c2(s_t, a_t)} - \\frac{1}{2} log \u03c3\u03c9\u03c2(st, at), (25)\n= \u03a3 L(\u03bc\u03c9\u03c2, \u03c3\u03c9\u03c2).\nCorresponding results are shown in However, various forms of intrinsic rewards don't exhibit significant differences in performance. In some tasks, the introduction of an additional covariance network even lead to performance not as favorable as when directly using regression loss."}]}